{
  "2510.06999v1": {
    "title": "Towards Reliable Retrieval in RAG Systems for Large Legal Datasets",
    "url": "https://www.alphaxiv.org/abs/2510.06999v1",
    "arxiv_id": "2510.06999v1",
    "authors": "Markus Reuter, Tobias Lingenberg, Rūta Liepiņa, Francesca Lagioia, Marco Lippi, Giovanni Sartor, Andrea Passerini, Burcu Sayin",
    "categories": "cs.CL, cs.IR, I.2.7; H.3.3; K.5.0",
    "pub_date": "2025-10-08 13:22:20",
    "ori_summary": "Retrieval-Augmented Generation (RAG) is a promising approach to mitigate hallucinations in Large Language Models (LLMs) for legal applications, but its reliability is critically dependent on the accuracy of the retrieval step. This is particularly challenging in the legal domain, where large databases of structurally similar documents often cause retrieval systems to fail. In this paper, we address this challenge by first identifying and quantifying a critical failure mode we term Document-Level Retrieval Mismatch (DRM), where the retriever selects information from entirely incorrect source documents. To mitigate DRM, we investigate a simple and computationally efficient technique which we refer to as Summary-Augmented Chunking (SAC). This method enhances each text chunk with a document-level synthetic summary, thereby injecting crucial global context that would otherwise be lost during a standard chunking process. Our experiments on a diverse set of legal information retrieval tasks show that SAC greatly reduces DRM and, consequently, also improves text-level retrieval precision and recall. Interestingly, we find that a generic summarization strategy outperforms an approach that incorporates legal expert domain knowledge to target specific legal elements. Our work provides evidence that this practical, scalable, and easily integrable technique enhances the reliability of RAG systems when applied to large-scale legal document datasets.",
    "summary": "该论文研究法律领域RAG系统中检索不可靠的核心问题，核心创新是提出摘要增强分块方法，通过为文本块添加文档级合成摘要来注入全局上下文信息，从而解决文档级检索不匹配问题。",
    "translation": "面向大型法律数据集的RAG系统可靠检索研究",
    "relevance_score": 8,
    "reasoning": "该论文直接针对检索增强生成(RAG)系统的可靠性问题，这是搜索和推荐系统中的核心技术挑战。RAG系统在搜索和推荐领域有广泛应用，可靠的检索机制对于提升内容相关性、减少错误信息至关重要。论文聚焦大型数据集上的检索可靠性，这一技术进展可直接应用于搜索系统的文档检索和推荐系统的候选生成环节。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对RAG系统的检索可靠性问题，提出增强检索准确性的创新方法，对搜索和推荐系统中的信息检索具有重要参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06987v1": {
    "title": "Spiral Model Technique For Data Science & Machine Learning Lifecycle",
    "url": "https://www.alphaxiv.org/abs/2510.06987v1",
    "arxiv_id": "2510.06987v1",
    "authors": "Rohith Mahadevan",
    "categories": "cs.LG, cs.IR, cs.SE",
    "pub_date": "2025-10-08 13:11:58",
    "ori_summary": "Analytics play an important role in modern business. Companies adapt data science lifecycles to their culture to seek productivity and improve their competitiveness among others. Data science lifecycles are fairly an important contributing factor to start and end a project that are data dependent. Data science and Machine learning life cycles comprises of series of steps that are involved in a project. A typical life cycle states that it is a linear or cyclical model that revolves around. It is mostly depicted that it is possible in a traditional data science life cycle to start the process again after reaching the end of cycle. This paper suggests a new technique to incorporate data science life cycle to business problems that have a clear end goal. A new technique called spiral technique is introduced to emphasize versatility, agility and iterative approach to business processes.",
    "summary": "",
    "translation": "数据科学与机器学习生命周期的螺旋模型技术",
    "relevance_score": 1,
    "reasoning": "该论文标题讨论的是数据科学和机器学习的生命周期管理方法（螺旋模型），这属于通用的MLOps或开发流程主题。它不涉及推荐系统、搜索或广告领域的核心进展，也不涉及LLM技术、Transformer架构改进，或异构数据的统一建模。该主题与我的技术焦点完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06924v1": {
    "title": "Ethical AI prompt recommendations in large language models using collaborative filtering",
    "url": "https://www.alphaxiv.org/abs/2510.06924v1",
    "arxiv_id": "2510.06924v1",
    "authors": "Jordan Nelson, Almas Baimagambetov, Konstantinos Avgerinakis, Nikolaos Polatidis",
    "categories": "cs.IR",
    "pub_date": "2025-10-08 12:03:21",
    "ori_summary": "As large language models (LLMs) shape AI development, ensuring ethical prompt recommendations is crucial. LLMs offer innovation but risk bias, fairness issues, and accountability concerns. Traditional oversight methods struggle with scalability, necessitating dynamic solutions. This paper proposes using collaborative filtering, a technique from recommendation systems, to enhance ethical prompt selection. By leveraging user interactions, it promotes ethical guidelines while reducing bias. Contributions include a synthetic dataset for prompt recommendations and the application of collaborative filtering. The work also tackles challenges in ethical AI, such as bias mitigation, transparency, and preventing unethical prompt engineering.",
    "summary": "",
    "translation": "基于协同过滤的大型语言模型伦理AI提示推荐",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及伦理AI主题，这属于被明确排除的无关主题范畴。虽然提到了协同过滤和LLM技术，但核心焦点是伦理方面的提示推荐，与我的技术导向研究重点无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06888v1": {
    "title": "M3Retrieve: Benchmarking Multimodal Retrieval for Medicine",
    "url": "https://www.alphaxiv.org/abs/2510.06888v1",
    "arxiv_id": "2510.06888v1",
    "authors": "Arkadeep Acharya, Akash Ghosh, Pradeepika Verma, Kitsuchart Pasupa, Sriparna Saha, Priti Singh",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-08 11:08:47",
    "ori_summary": "With the increasing use of RetrievalAugmented Generation (RAG), strong retrieval models have become more important than ever. In healthcare, multimodal retrieval models that combine information from both text and images offer major advantages for many downstream tasks such as question answering, cross-modal retrieval, and multimodal summarization, since medical data often includes both formats. However, there is currently no standard benchmark to evaluate how well these models perform in medical settings. To address this gap, we introduce M3Retrieve, a Multimodal Medical Retrieval Benchmark. M3Retrieve, spans 5 domains,16 medical fields, and 4 distinct tasks, with over 1.2 Million text documents and 164K multimodal queries, all collected under approved licenses. We evaluate leading multimodal retrieval models on this benchmark to explore the challenges specific to different medical specialities and to understand their impact on retrieval performance. By releasing M3Retrieve, we aim to enable systematic evaluation, foster model innovation, and accelerate research toward building more capable and reliable multimodal retrieval systems for medical applications. The dataset and the baselines code are available in this github page https://github.com/AkashGhosh/M3Retrieve.",
    "summary": "",
    "translation": "M3Retrieve：面向医学领域的多模态检索基准",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于医学领域的多模态检索基准，这属于明确的医学领域特定应用，属于无关主题。虽然多模态检索技术本身可能有通用性，但论文明确限定在医学领域，与搜索、推荐、广告等核心关注领域没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06838v1": {
    "title": "Crossing Domains without Labels: Distant Supervision for Term Extraction",
    "url": "https://www.alphaxiv.org/abs/2510.06838v1",
    "arxiv_id": "2510.06838v1",
    "authors": "Elena Senger, Yuri Campbell, Rob van der Goot, Barbara Plank",
    "categories": "cs.IR, cs.CL",
    "pub_date": "2025-10-08 10:02:40",
    "ori_summary": "Automatic Term Extraction (ATE) is a critical component in downstream NLP tasks such as document tagging, ontology construction and patent analysis. Current state-of-the-art methods require expensive human annotation and struggle with domain transfer, limiting their practical deployment. This highlights the need for more robust, scalable solutions and realistic evaluation settings. To address this, we introduce a comprehensive benchmark spanning seven diverse domains, enabling performance evaluation at both the document- and corpus-levels. Furthermore, we propose a robust LLM-based model that outperforms both supervised cross-domain encoder models and few-shot learning baselines and performs competitively with its GPT-4o teacher on this benchmark. The first step of our approach is generating psuedo-labels with this black-box LLM on general and scientific domains to ensure generalizability. Building on this data, we fine-tune the first LLMs for ATE. To further enhance document-level consistency, oftentimes needed for downstream tasks, we introduce lightweight post-hoc heuristics. Our approach exceeds previous approaches on 5/7 domains with an average improvement of 10 percentage points. We release our dataset and fine-tuned models to support future research in this area.",
    "summary": "",
    "translation": "跨领域无标签：术语抽取的远程监督方法",
    "relevance_score": 2,
    "reasoning": "该论文专注于术语抽取的远程监督方法，这属于信息抽取领域，与推荐系统、搜索或广告的核心技术关联较弱。虽然术语抽取在搜索中有潜在应用（如查询理解），但论文主要关注跨领域迁移和无标签学习，缺乏明确的RecSys/Search/Ads应用场景，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06823v1": {
    "title": "Exposing Citation Vulnerabilities in Generative Engines",
    "url": "https://www.alphaxiv.org/abs/2510.06823v1",
    "arxiv_id": "2510.06823v1",
    "authors": "Riku Mochizuki, Shusuke Komatsu, Souta Noguchi, Kazuto Ataka",
    "categories": "cs.CR, cs.CL, cs.IR",
    "pub_date": "2025-10-08 09:47:48",
    "ori_summary": "We analyze answers generated by generative engines (GEs) from the perspectives of citation publishers and the content-injection barrier, defined as the difficulty for attackers to manipulate answers to user prompts by placing malicious content on the web. GEs integrate two functions: web search and answer generation that cites web pages using large language models. Because anyone can publish information on the web, GEs are vulnerable to poisoning attacks. Existing studies of citation evaluation focus on how faithfully answer content reflects cited sources, leaving unexamined which web sources should be selected as citations to defend against poisoning attacks. To fill this gap, we introduce evaluation criteria that assess poisoning threats using the citation information contained in answers. Our criteria classify the publisher attributes of citations to estimate the content-injection barrier thereby revealing the threat of poisoning attacks in current GEs. We conduct experiments in political domains in Japan and the United States (U.S.) using our criteria and show that citations from official party websites (primary sources) are approximately \\(25\\%\\)--\\(45\\%\\) in the U.S. and \\(60\\%\\)--\\(65\\%\\) in Japan, indicating that U.S. political answers are at higher risk of poisoning attacks. We also find that sources with low content-injection barriers are frequently cited yet are poorly reflected in answer content. To mitigate this threat, we discuss how publishers of primary sources can increase exposure of their web content in answers and show that well-known techniques are limited by language differences.",
    "summary": "",
    "translation": "揭示生成式引擎中的引用漏洞",
    "relevance_score": 2,
    "reasoning": "该论文主要关注生成式引擎中的引用漏洞，这属于LLM评估和可信度问题，属于被排除的纯粹NLP中心话题。虽然涉及生成式技术，但焦点是漏洞和安全问题，而非在推荐系统、搜索或广告中的实际应用。没有明确的机制表明这些发现可以转化为推荐、搜索或广告系统的改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06805v1": {
    "title": "Overview of the Plagiarism Detection Task at PAN 2025",
    "url": "https://www.alphaxiv.org/abs/2510.06805v1",
    "arxiv_id": "2510.06805v1",
    "authors": "André Greiner-Petter, Maik Fröbe, Jan Philip Wahle, Terry Ruas, Bela Gipp, Akiko Aizawa, Martin Potthast",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2025-10-08 09:33:26",
    "ori_summary": "The generative plagiarism detection task at PAN 2025 aims at identifying automatically generated textual plagiarism in scientific articles and aligning them with their respective sources. We created a novel large-scale dataset of automatically generated plagiarism using three large language models: Llama, DeepSeek-R1, and Mistral. In this task overview paper, we outline the creation of this dataset, summarize and compare the results of all participants and four baselines, and evaluate the results on the last plagiarism detection task from PAN 2015 in order to interpret the robustness of the proposed approaches. We found that the current iteration does not invite a large variety of approaches as naive semantic similarity approaches based on embedding vectors provide promising results of up to 0.8 recall and 0.5 precision. In contrast, most of these approaches underperform significantly on the 2015 dataset, indicating a lack in generalizability.",
    "summary": "",
    "translation": "PAN 2025抄袭检测任务概览",
    "relevance_score": 1,
    "reasoning": "该论文专注于抄袭检测任务，这属于内容安全与诚信验证领域，与推荐系统、搜索或广告的核心技术进展无关。抄袭检测主要涉及文本相似性分析和内容验证，不涉及用户行为建模、个性化推荐、搜索排序或广告投放等关键技术方向，因此与当前关注点完全不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06732v1": {
    "title": "Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.06732v1",
    "arxiv_id": "2510.06732v1",
    "authors": "Tiancheng Xing, Jerry Li, Yixuan Du, Xiyang Hu",
    "categories": "cs.CL, cs.AI, cs.IR",
    "pub_date": "2025-10-08 07:40:40",
    "ori_summary": "Large language models (LLMs) are increasingly used as rerankers in information retrieval, yet their ranking behavior can be steered by small, natural-sounding prompts. To expose this vulnerability, we present Rank Anything First (RAF), a two-stage token optimization method that crafts concise textual perturbations to consistently promote a target item in LLM-generated rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate Gradient to shortlist candidate tokens at the current position by combining the gradient of the rank-target with a readability score; Stage 2 evaluates those candidates under exact ranking and readability losses using an entropy-based dynamic weighting scheme, and selects a token via temperature-controlled sampling. RAF generates ranking-promoting prompts token-by-token, guided by dual objectives: maximizing ranking effectiveness and preserving linguistic naturalness. Experiments across multiple LLMs show that RAF significantly boosts the rank of target items using naturalistic language, with greater robustness than existing methods in both promoting target items and maintaining naturalness. These findings underscore a critical security implication: LLM-based reranking is inherently susceptible to adversarial manipulation, raising new challenges for the trustworthiness and robustness of modern retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.",
    "summary": "研究LLM作为排序器的可靠性问题，核心方法是通过两阶段令牌优化技术生成自然语言扰动来操纵LLM的排序结果，暴露LLM排序系统的内在脆弱性。",
    "translation": "大型语言模型是可靠的排序器吗？通过两阶段令牌优化实现排序操纵",
    "relevance_score": 8,
    "reasoning": "该论文直接研究LLM在排序任务中的可靠性，这是搜索和推荐系统的核心问题。两阶段令牌优化技术作为LLM排序的增强方法，可以应用于提升搜索相关性排序和推荐列表质量，属于直接LLM应用领域。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接研究LLM在推荐/搜索排序中的安全漏洞，揭示了LLM排序器易受对抗性攻击的关键问题，对构建可信赖的推荐系统具有重要安全意义。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06728v1": {
    "title": "Reproducing and Extending Causal Insights Into Term Frequency Computation in Neural Rankers",
    "url": "https://www.alphaxiv.org/abs/2510.06728v1",
    "arxiv_id": "2510.06728v1",
    "authors": "Cile van Marken, Roxana Petcu",
    "categories": "cs.IR",
    "pub_date": "2025-10-08 07:29:31",
    "ori_summary": "Neural ranking models have shown outstanding performance across a variety of tasks, such as document retrieval, re-ranking, question answering and conversational retrieval. However, the inner decision process of these models remains largely unclear, especially as models increase in size. Most interpretability approaches, such as probing, focus on correlational insights rather than establishing causal relationships. The paper 'Axiomatic Causal Interventions for Reverse Engineering Relevance Computation in Neural Retrieval Models' by Chen et al. addresses this gap by introducing a framework for activation patching - a causal interpretability method - in the information retrieval domain, offering insights into how neural retrieval models compute document relevance. The study demonstrates that neural ranking models not only capture term-frequency information, but also that these representations can be localized to specific components of the model, such as individual attention heads or layers. This paper aims to reproduce the findings by Chen et al. and to further explore the presence of pre-defined retrieval axioms in neural IR models. We validate the main claims made by Chen et al., and extend the framework to include an additional term-frequency axiom, which states that the impact of increasing query term frequency on document ranking diminishes as the frequency becomes higher. We successfully identify a group of attention heads that encode this axiom and analyze their behavior to give insight into the inner decision-making process of neural ranking models.",
    "summary": "",
    "translation": "复现与扩展神经网络排序器中词频计算因果洞察",
    "relevance_score": 7,
    "reasoning": "该论文直接研究神经网络排序器中的词频计算机制，这属于搜索领域的核心算法改进。词频计算是搜索排序的基础组件，对神经排序器中因果关系的深入理解可直接应用于提升搜索相关性排序性能，属于Core Domain Advances范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06658v1": {
    "title": "Can We Hide Machines in the Crowd? Quantifying Equivalence in LLM-in-the-loop Annotation Tasks",
    "url": "https://www.alphaxiv.org/abs/2510.06658v1",
    "arxiv_id": "2510.06658v1",
    "authors": "Jiaman He, Zikang Leng, Dana McKay, Damiano Spina, Johanne R. Trippas",
    "categories": "cs.IR",
    "pub_date": "2025-10-08 05:17:33",
    "ori_summary": "Many evaluations of large language models (LLMs) in text annotation focus primarily on the correctness of the output, typically comparing model-generated labels to human-annotated ``ground truth'' using standard performance metrics. In contrast, our study moves beyond effectiveness alone. We aim to explore how labeling decisions -- by both humans and LLMs -- can be statistically evaluated across individuals. Rather than treating LLMs purely as annotation systems, we approach LLMs as an alternative annotation mechanism that may be capable of mimicking the subjective judgments made by humans. To assess this, we develop a statistical evaluation method based on Krippendorff's $\\alpha$, paired bootstrapping, and the Two One-Sided t-Tests (TOST) equivalence test procedure. This evaluation method tests whether an LLM can blend into a group of human annotators without being distinguishable. We apply this approach to two datasets -- MovieLens 100K and PolitiFact -- and find that the LLM is statistically indistinguishable from a human annotator in the former ($p = 0.004$), but not in the latter ($p = 0.155$), highlighting task-dependent differences. It also enables early evaluation on a small sample of human data to inform whether LLMs are suitable for large-scale annotation in a given application.",
    "summary": "",
    "translation": "我们能否将机器隐藏在人群中？量化LLM在环标注任务中的等价性",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM在数据标注任务中的等价性评估，这属于LLM应用的质量评估范畴。虽然涉及LLM技术，但焦点是标注任务的等价性量化，而非在推荐系统、搜索或广告中的直接应用或架构改进，与当前关注的核心领域相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06657v1": {
    "title": "LLM-Powered Nuanced Video Attribute Annotation for Enhanced Recommendations",
    "url": "https://www.alphaxiv.org/abs/2510.06657v1",
    "arxiv_id": "2510.06657v1",
    "authors": "Boyuan Long, Yueqi Wang, Hiloni Mehta, Mick Zomnir, Omkar Pathak, Changping Meng, Ruolin Jia, Yajun Peng, Dapeng Hong, Xia Wu, Mingyan Gao, Onkar Dalal, Ningren Han",
    "categories": "cs.IR",
    "pub_date": "2025-10-08 05:17:17",
    "ori_summary": "This paper presents a case study on deploying Large Language Models (LLMs) as an advanced \"annotation\" mechanism to achieve nuanced content understanding (e.g., discerning content \"vibe\") at scale within a large-scale industrial short-form video recommendation system. Traditional machine learning classifiers for content understanding face protracted development cycles and a lack of deep, nuanced comprehension. The \"LLM-as-annotators\" approach addresses these by significantly shortening development times and enabling the annotation of subtle attributes. This work details an end-to-end workflow encompassing: (1) iterative definition and robust evaluation of target attributes, refined by offline metrics and online A/B testing; (2) scalable offline bulk annotation of video corpora using LLMs with multimodal features, optimized inference, and knowledge distillation for broad application; and (3) integration of these rich annotations into the online recommendation serving system, for example, through personalized restrict retrieval. Experimental results demonstrate the efficacy of this approach, with LLMs outperforming human raters in offline annotation quality for nuanced attributes and yielding significant improvements of user participation and satisfied consumption in online A/B tests. The study provides insights into designing and scaling production-level LLM pipelines for rich content evaluation, highlighting the adaptability and benefits of LLM-generated nuanced understanding for enhancing content discovery, user satisfaction, and the overall effectiveness of modern recommendation systems.",
    "summary": "论文研究如何解决推荐系统中传统内容理解方法开发周期长、缺乏深度理解的问题。核心方法是采用LLM作为标注器，构建端到端工作流程实现大规模细粒度视频属性标注，并将这些丰富标注集成到在线推荐系统中。",
    "translation": "基于大语言模型的细粒度视频属性标注用于增强推荐系统",
    "relevance_score": 8,
    "reasoning": "该论文直接应用LLM技术进行视频属性标注，属于'Direct LLM Applications'范畴，通过改进内容理解来增强推荐系统。细粒度属性标注可以显著提升视频推荐的相关性和个性化程度，这是搜索和推荐系统的核心改进方向。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用LLM技术解决推荐系统中的内容理解难题，通过LLM作为标注器实现细粒度内容属性标注，完全符合直接LLM应用和核心领域进展的关注点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07318v1": {
    "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
    "url": "https://www.alphaxiv.org/abs/2510.07318v1",
    "arxiv_id": "2510.07318v1",
    "authors": "Yunhao Fang, Weihao Yu, Shu Zhong, Qinghao Ye, Xuehan Xiong, Lai Wei",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-08 17:59:55",
    "ori_summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.",
    "summary": "论文研究长序列建模中效率与精度的根本权衡问题，核心思想是结合Transformer的KV缓存作为无损短期记忆与可学习的人工海马体网络压缩长期记忆的混合记忆框架。",
    "translation": "人工海马体网络用于高效长上下文建模",
    "relevance_score": 8,
    "reasoning": "该论文涉及高效长上下文建模，这直接属于'使能Transformer技术'范畴，关注Transformer架构的效率改进。在推荐系统、搜索和广告中，高效处理长用户序列、历史行为和上下文特征至关重要，这种技术可以显著提升长序列建模的效率和效果。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对Transformer架构的效率改进，提出混合记忆框架解决长序列建模的核心瓶颈，与Enabling Transformer Tech高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07315v1": {
    "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
    "url": "https://www.alphaxiv.org/abs/2510.07315v1",
    "arxiv_id": "2510.07315v1",
    "authors": "Ming Zhong, Xiang Zhou, Ting-Yun Chang, Qingze Wang, Nan Xu, Xiance Si, Dan Garrette, Shyam Upadhyay, Jeremiah Liu, Jiawei Han, Benoit Schillings, Jiao Sun",
    "categories": "cs.CL, cs.AI, cs.LG, cs.SE",
    "pub_date": "2025-10-08 17:59:19",
    "ori_summary": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. In this paper, we hypothesize that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify models' code instruction following capabilities with measurable signals, we present VeriCode, a taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. We use the taxonomy to augment established evaluation suites, resulting in Vibe Checker, a testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, we show that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, a composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. Our work identifies core factors of the vibe check, providing a concrete path for benchmarking and developing models that better align with user preferences in coding.",
    "summary": "",
    "translation": "Vibe Checker：将代码评估与人类偏好对齐",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于代码评估和人类偏好对齐，这属于编程辅助和代码质量评估领域，与推荐系统、搜索或广告的核心技术完全无关。虽然涉及偏好对齐概念，但应用场景仅限于代码开发，没有任何与RecSys/Search/Ads相关的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07309v1": {
    "title": "Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain",
    "url": "https://www.alphaxiv.org/abs/2510.07309v1",
    "arxiv_id": "2510.07309v1",
    "authors": "Yue Li, Ran Tao, Derek Hommel, Yusuf Denizay Dönder, Sungyong Chang, David Mimno, Unso Eun Seo Jo",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 17:57:35",
    "ori_summary": "In the business domain, where data-driven decision making is crucial, text-to-SQL is fundamental for easy natural language access to structured data. While recent LLMs have achieved strong performance in code generation, existing text-to-SQL benchmarks remain focused on factual retrieval of past records. We introduce CORGI, a new benchmark specifically designed for real-world business contexts. CORGI is composed of synthetic databases inspired by enterprises such as Doordash, Airbnb, and Lululemon. It provides questions across four increasingly complex categories of business queries: descriptive, explanatory, predictive, and recommendational. This challenge calls for causal reasoning, temporal forecasting, and strategic recommendation, reflecting multi-level and multi-step agentic intelligence. We find that LLM performance drops on high-level questions, struggling to make accurate predictions and offer actionable plans. Based on execution success rate, the CORGI benchmark is about 21\\% more difficult than the BIRD benchmark. This highlights the gap between popular LLMs and the need for real-world business intelligence. We release a public dataset and evaluation framework, and a website for public submissions.",
    "summary": "",
    "translation": "Agent Bain vs. Agent McKinsey：面向商业领域的新型文本转SQL基准测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文本到SQL转换的基准测试，属于数据库查询领域的特定应用。虽然SQL查询与搜索系统有一定关联，但该基准专注于商业咨询领域的特定场景，与推荐系统、搜索排名或广告的核心技术关联度较低。文本到SQL技术可能间接应用于某些搜索场景，但论文焦点是基准测试而非直接的技术创新应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07300v1": {
    "title": "Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.07300v1",
    "arxiv_id": "2510.07300v1",
    "authors": "Xue Zhang, Yunlong Liang, Fandong Meng, Songming Zhang, Kaiyu Huang, Yufeng Chen, Jinan Xu, Jie Zhou",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 17:55:02",
    "ori_summary": "Large Reasoning Models (LRMs) have achieved remarkable performance on complex reasoning tasks by adopting the \"think-then-answer\" paradigm, which enhances both accuracy and interpretability. However, current LRMs exhibit two critical limitations when processing non-English languages: (1) They often struggle to maintain input-output language consistency; (2) They generally perform poorly with wrong reasoning paths and lower answer accuracy compared to English. These limitations significantly degrade the user experience for non-English speakers and hinder the global deployment of LRMs. To address these limitations, we propose M-Thinker, which is trained by the GRPO algorithm that involves a Language Consistency (LC) reward and a novel Cross-lingual Thinking Alignment (CTA) reward. Specifically, the LC reward defines a strict constraint on the language consistency between the input, thought, and answer. Besides, the CTA reward compares the model's non-English reasoning paths with its English reasoning path to transfer its own reasoning capability from English to non-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B models not only achieve nearly 100% language consistency and superior performance on two multilingual benchmarks (MMATH and PolyMath), but also exhibit excellent generalization on out-of-domain languages.",
    "summary": "",
    "translation": "原生思考：通过一致性增强强化学习解锁多语言推理能力",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多语言推理和强化学习的一致性增强技术，这属于纯粹的NLP领域研究。虽然强化学习技术本身可能具有通用性，但论文标题没有显示出与推荐系统、搜索或广告领域的直接关联或潜在应用，且多语言推理主要针对语言理解而非工业级推荐/搜索系统。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07293v1": {
    "title": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.07293v1",
    "arxiv_id": "2510.07293v1",
    "authors": "Peize He, Zichen Wen, Yubo Wang, Yuxuan Wang, Xiaoqian Liu, Jiajie Huang, Zehui Lei, Zhuangcheng Gu, Xiangqi Jin, Jiabing Yang, Kai Li, Zhifei Liu, Weijia Li, Cunxiang Wang, Conghui He, Linfeng Zhang",
    "categories": "cs.SD, cs.AI, cs.CL, eess.AS",
    "pub_date": "2025-10-08 17:50:16",
    "ori_summary": "Processing long-form audio is a major challenge for Large Audio Language models (LALMs). These models struggle with the quadratic cost of attention ($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio benchmarks are built mostly from short clips and do not evaluate models in realistic long context settings. To address this gap, we introduce AudioMarathon, a benchmark designed to evaluate both understanding and inference efficiency on long-form audio. AudioMarathon provides a diverse set of tasks built upon three pillars: long-context audio inputs with durations ranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of 2,250 to 7,500 audio tokens, respectively, full domain coverage across speech, sound, and music, and complex reasoning that requires multi-hop inference. We evaluate state-of-the-art LALMs and observe clear performance drops as audio length grows. We also study acceleration techniques and analyze the trade-offs of token pruning and KV cache eviction. The results show large gaps across current LALMs and highlight the need for better temporal reasoning and memory-efficient architectures. We believe AudioMarathon will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.",
    "summary": "",
    "translation": "AudioMarathon：面向音频大语言模型的长上下文音频理解与效率的综合基准",
    "relevance_score": 2,
    "reasoning": "该论文主要关注音频领域的基准测试和长上下文理解，属于特定模态（音频）的评估工作。虽然涉及LLM效率，但其核心应用场景是音频理解而非推荐系统、搜索或广告领域。论文没有展示与异构数据建模或推荐/搜索应用的明确联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07290v1": {
    "title": "On the Convergence of Moral Self-Correction in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07290v1",
    "arxiv_id": "2510.07290v1",
    "authors": "Guangliang Liu, Haitao Mao, Bochuan Cao, Zhiyu Xue, Xitong Zhang, Rongrong Wang, Kristen Marie Johnson",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-08 17:46:27",
    "ori_summary": "Large Language Models (LLMs) are able to improve their responses when instructed to do so, a capability known as self-correction. When instructions provide only a general and abstract goal without specific details about potential issues in the response, LLMs must rely on their internal knowledge to improve response quality, a process referred to as intrinsic self-correction. The empirical success of intrinsic self-correction is evident in various applications, but how and why it is effective remains unknown. Focusing on moral self-correction in LLMs, we reveal a key characteristic of intrinsic self-correction: performance convergence through multi-round interactions; and provide a mechanistic analysis of this convergence behavior. Based on our experimental results and analysis, we uncover the underlying mechanism of convergence: consistently injected self-correction instructions activate moral concepts that reduce model uncertainty, leading to converged performance as the activated moral concepts stabilize over successive rounds. This paper demonstrates the strong potential of moral self-correction by showing that it exhibits a desirable property of converged performance.",
    "summary": "",
    "translation": "论大型语言模型中道德自我修正的收敛性",
    "relevance_score": 1,
    "reasoning": "该论文关注LLM的道德自我修正和收敛性，这属于伦理、对齐和安全范畴，属于明确的无关主题。论文内容与推荐系统、搜索或广告的核心技术进展、LLM使能技术或直接应用完全无关，没有任何潜在的技术应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07284v1": {
    "title": "Online Rubrics Elicitation from Pairwise Comparisons",
    "url": "https://www.alphaxiv.org/abs/2510.07284v1",
    "arxiv_id": "2510.07284v1",
    "authors": "MohammadHossein Rezaei, Robert Vacareanu, Zihao Wang, Clinton Wang, Yunzhong He, Afra Feyza Akyürek",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-08 17:44:59",
    "ori_summary": "Rubrics provide a flexible way to train LLMs on open-ended long-form answers where verifiable rewards are not applicable and human preferences provide coarse signals. Prior work shows that reinforcement learning with rubric-based rewards leads to consistent gains in LLM post-training. Most existing approaches rely on rubrics that remain static over the course of training. Such static rubrics, however, are vulnerable to reward-hacking type behaviors and fail to capture emergent desiderata that arise during training. We introduce Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates evaluation criteria in an online manner through pairwise comparisons of responses from current and reference policies. This online process enables continuous identification and mitigation of errors as training proceeds. Empirically, this approach yields consistent improvements of up to 8% over training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as well as the validation sets of expert questions and rubrics. We qualitatively analyze the elicited criteria and identify prominent themes such as transparency, practicality, organization, and reasoning.",
    "summary": "",
    "translation": "基于成对比较的在线评分标准获取",
    "relevance_score": 2,
    "reasoning": "该论文主要关注从成对比较中获取评分标准，这属于偏好学习领域，与推荐系统中的用户偏好建模有一定关联。然而，论文标题未明确表明与LLM、Transformer架构或搜索/广告系统的直接联系，且在线评分标准获取本身更偏向通用机器学习方法而非特定于推荐/搜索/广告领域的核心技术突破。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07248v1": {
    "title": "Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models",
    "url": "https://www.alphaxiv.org/abs/2510.07248v1",
    "arxiv_id": "2510.07248v1",
    "authors": "Jonggeun Lee, Woojung Song, Jongwook Han, Haesung Pyun, Yohan Jo",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 17:16:07",
    "ori_summary": "Small language models (SLMs) offer significant computational advantages for tool-augmented AI systems, yet they struggle with tool-use tasks, particularly in selecting appropriate tools and identifying correct parameters. A common failure mode is schema misalignment: models hallucinate plausible but non-existent tool names that reflect naming conventions internalized during pretraining but absent from the provided tool schema. Rather than forcing models to adapt to arbitrary schemas, we propose adapting schemas to align with models' pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool Schema Generation), a training-free method that leverages peakedness-a signal from contamination detection indicating pretraining familiarity-to automatically rename tool components. By generating multiple candidates and selecting those with highest output concentration across samples, PA-Tool identifies pretrain-aligned naming patterns. Experiments on MetaTool and RoTBench show improvements of up to 17% points, with schema misalignment errors reduced by 80%. PA-Tool enables small models to approach state-of-the-art performance while maintaining computational efficiency for adaptation to new tools without retraining. Our work demonstrates that schema-level interventions can unlock the tool-use potential of resource-efficient models by adapting schemas to models rather than models to schemas.",
    "summary": "论文研究小语言模型在工具使用中的模式对齐问题，核心思想是通过分析预训练熟悉度自动重命名工具组件，使工具模式适配模型知识而非强制模型适应任意模式。",
    "translation": "不要为工具适配小型语言模型；将工具模式适配到模型",
    "relevance_score": 8,
    "reasoning": "这篇论文涉及工具使用和模型适配，这直接适用于搜索和推荐系统中的LLM应用，其中模型需要与外部工具和API交互。将工具模式适配到较小模型的方法可以显著提高搜索和推荐系统中工具集成LLM的效率和可扩展性。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出通过适配工具模式而非模型来解决SLM工具使用问题，这种模式对齐方法对推荐系统和搜索中的工具集成具有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07243v1": {
    "title": "LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.07243v1",
    "arxiv_id": "2510.07243v1",
    "authors": "Joseph Enguehard, Morgane Van Ermengem, Kate Atkinson, Sujeong Cha, Arijit Ghosh Chowdhury, Prashanth Kallur Ramaswamy, Jeremy Roghair, Hannah R Marlowe, Carina Suzana Negreanu, Kitty Boxall, Diana Mincu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 17:10:47",
    "ori_summary": "Evaluating large language model (LLM) outputs in the legal domain presents unique challenges due to the complex and nuanced nature of legal analysis. Current evaluation approaches either depend on reference data, which is costly to produce, or use standardized assessment methods, both of which have significant limitations for legal applications. Although LLM-as-a-Judge has emerged as a promising evaluation technique, its reliability and effectiveness in legal contexts depend heavily on evaluation processes unique to the legal industry and how trustworthy the evaluation appears to the human legal expert. This is where existing evaluation methods currently fail and exhibit considerable variability. This paper aims to close the gap: a) we break down lengthy responses into 'Legal Data Points' (LDPs), self-contained units of information, and introduce a novel, reference-free evaluation methodology that reflects how lawyers evaluate legal answers; b) we demonstrate that our method outperforms a variety of baselines on both our proprietary dataset and an open-source dataset (LegalBench); c) we show how our method correlates more closely with human expert evaluations and helps improve inter-annotator agreement; and finally d) we open source our Legal Data Points for a subset of LegalBench used in our experiments, allowing the research community to replicate our results and advance research in this vital area of LLM evaluation on legal question-answering.",
    "summary": "",
    "translation": "LeMAJ（法律大语言模型作为法官）：连接法律推理与LLM评估",
    "relevance_score": 2,
    "reasoning": "该论文专注于法律领域的LLM应用和评估，属于特定领域应用，与推荐系统、搜索或广告的核心技术进展无关。虽然涉及LLM评估，但这是法律推理场景下的特定评估，而非通用的RecSys/Search/Ads相关技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07242v1": {
    "title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense",
    "url": "https://www.alphaxiv.org/abs/2510.07242v1",
    "arxiv_id": "2510.07242v1",
    "authors": "Leitian Tao, Ilia Kulikov, Swarnadeep Saha, Tianlu Wang, Jing Xu, Yixuan Li, Jason E Weston, Ping Yu",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-08 17:09:41",
    "ori_summary": "Post-training for reasoning of large language models (LLMs) increasingly relies on verifiable rewards: deterministic checkers that provide 0-1 correctness signals. While reliable, such binary feedback is brittle--many tasks admit partially correct or alternative answers that verifiers under-credit, and the resulting all-or-nothing supervision limits learning. Reward models offer richer, continuous feedback, which can serve as a complementary supervisory signal to verifiers. We introduce HERO (Hybrid Ensemble Reward Optimization), a reinforcement learning framework that integrates verifier signals with reward-model scores in a structured way. HERO employs stratified normalization to bound reward-model scores within verifier-defined groups, preserving correctness while refining quality distinctions, and variance-aware weighting to emphasize challenging prompts where dense signals matter most. Across diverse mathematical reasoning benchmarks, HERO consistently outperforms RM-only and verifier-only baselines, with strong gains on both verifiable and hard-to-verify tasks. Our results show that hybrid reward design retains the stability of verifiers while leveraging the nuance of reward models to advance reasoning.",
    "summary": "",
    "translation": "混合强化学习：当奖励稀疏时，密集化策略更优",
    "relevance_score": 2,
    "reasoning": "该论文主要关注强化学习中的奖励稀疏性问题，属于纯粹的强化学习技术研究。虽然强化学习在推荐系统和广告中有应用，但论文标题没有明确指向这些领域的具体应用场景，也没有涉及LLM、Transformer架构或异构数据建模等当前关注的核心技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07239v1": {
    "title": "Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts",
    "url": "https://www.alphaxiv.org/abs/2510.07239v1",
    "arxiv_id": "2510.07239v1",
    "authors": "Christos Ziakas, Nicholas Loo, Nishita Jain, Alessandra Russo",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 17:06:20",
    "ori_summary": "Automated red-teaming has emerged as a scalable approach for auditing Large Language Models (LLMs) prior to deployment, yet existing approaches lack mechanisms to efficiently adapt to model-specific vulnerabilities at inference. We introduce Red-Bandit, a red-teaming framework that adapts online to identify and exploit model failure modes under distinct attack styles (e.g., manipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA experts, each specialized for a particular attack style, using reinforcement learning that rewards the generation of unsafe prompts via a rule-based safety model. At inference, a multi-armed bandit policy dynamically selects among these attack-style experts based on the target model's response safety, balancing exploration and exploitation. Red-Bandit achieves state-of-the-art results on AdvBench under sufficient exploration (ASR@10), while producing more human-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy serves as a diagnostic tool for uncovering model-specific vulnerabilities by indicating which attack styles most effectively elicit unsafe behaviors.",
    "summary": "",
    "translation": "红队-强盗：通过强盗引导的LoRA专家进行LLM红队测试时自适应",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM红队测试（安全测试）和测试时自适应，这属于安全评估领域，与我的核心关注点（推荐系统、搜索、广告）无关。虽然涉及LoRA技术，但其应用方向是安全测试而非推荐/搜索/广告系统的改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07238v1": {
    "title": "When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.07238v1",
    "arxiv_id": "2510.07238v1",
    "authors": "Xunyi Jiang, Dingyi Chang, Julian McAuley, Xin Xu",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 17:06:07",
    "ori_summary": "The rapid evolution of large language models (LLMs) and the real world has outpaced the static nature of widely used evaluation benchmarks, raising concerns about their reliability for evaluating LLM factuality. While substantial works continue to rely on the popular but old benchmarks, their temporal misalignment with real-world facts and modern LLMs, and their effects on LLM factuality evaluation remain underexplored. Therefore, in this work, we present a systematic investigation of this issue by examining five popular factuality benchmarks and eight LLMs released across different years. An up-to-date fact retrieval pipeline and three metrics are tailored to quantify benchmark aging and its impact on LLM factuality evaluation. Experimental results and analysis illustrate that a considerable portion of samples in the widely used factuality benchmarks are outdated, leading to unreliable assessments of LLM factuality. We hope our work can provide a testbed to assess the reliability of a benchmark for LLM factuality evaluation and inspire more research on the benchmark aging issue. Codes are available in https://github.com/JiangXunyi/BenchAge.",
    "summary": "",
    "translation": "当基准过时：通过大语言模型事实性评估揭示的时间错位",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM事实性评估和基准时效性问题，这属于纯粹的NLP评估基准范畴。论文内容涉及幻觉、评估基准等明确被列为不相关的主题，与推荐系统、搜索或广告的核心技术进展没有任何直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07233v1": {
    "title": "LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.07233v1",
    "arxiv_id": "2510.07233v1",
    "authors": "Zhivar Sourati, Zheng Wang, Marianne Menglin Liu, Yazhe Hu, Mengqing Guo, Sujeeth Bharadwaj, Kyu Han, Tao Sheng, Sujith Ravi, Morteza Dehghani, Dan Roth",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 17:02:04",
    "ori_summary": "Question answering over visually rich documents (VRDs) requires reasoning not only over isolated content but also over documents' structural organization and cross-page dependencies. However, conventional retrieval-augmented generation (RAG) methods encode content in isolated chunks during ingestion, losing structural and cross-page dependencies, and retrieve a fixed number of pages at inference, regardless of the specific demands of the question or context. This often results in incomplete evidence retrieval and degraded answer quality for multi-page reasoning tasks. To address these limitations, we propose LAD-RAG, a novel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs a symbolic document graph that captures layout structure and cross-page dependencies, adding it alongside standard neural embeddings to yield a more holistic representation of the document. During inference, an LLM agent dynamically interacts with the neural and symbolic indices to adaptively retrieve the necessary evidence based on the query. Experiments on MMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG improves retrieval, achieving over 90% perfect recall on average without any top-k tuning, and outperforming baseline retrievers by up to 20% in recall at comparable noise levels, yielding higher QA accuracy with minimal latency.",
    "summary": "",
    "translation": "LAD-RAG：面向视觉丰富文档理解的布局感知动态检索增强生成",
    "relevance_score": 3,
    "reasoning": "该论文虽然涉及检索增强生成（RAG）技术，但其核心关注点是视觉丰富文档理解，这主要属于文档分析和视觉语言处理领域。对于搜索系统，布局感知的文档理解可能有潜在应用价值，比如改进文档搜索中的内容提取和相关性匹配，但该技术更偏向文档分析而非核心的推荐/搜索/广告排序问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07231v1": {
    "title": "Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships",
    "url": "https://www.alphaxiv.org/abs/2510.07231v1",
    "arxiv_id": "2510.07231v1",
    "authors": "Donggyu Lee, Sungwon Park, Yerin Hwang, Hyunwoo Oh, Hyoshin Kim, Jungwon Kim, Meeyoung Cha, Sangyoon Park, Jihee Kim",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 17:00:49",
    "ori_summary": "Causal reasoning is fundamental for Large Language Models (LLMs) to understand genuine cause-and-effect relationships beyond pattern matching. Existing benchmarks suffer from critical limitations such as reliance on synthetic data and narrow domain coverage. We introduce a novel benchmark constructed from casually identified relationships extracted from top-tier economics and finance journals, drawing on rigorous methodologies including instrumental variables, difference-in-differences, and regression discontinuity designs. Our benchmark comprises 40,379 evaluation items covering five task types across domains such as health, environment, technology, law, and culture. Experimental results on eight state-of-the-art LLMs reveal substantial limitations, with the best model achieving only 57.6\\% accuracy. Moreover, model scale does not consistently translate to superior performance, and even advanced reasoning models struggle with fundamental causal relationship identification. These findings underscore a critical gap between current LLM capabilities and demands of reliable causal reasoning in high-stakes applications.",
    "summary": "",
    "translation": "基于科学验证关系对大型语言模型因果推理能力进行基准测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM因果推理能力的基准测试和评估，这属于纯粹的NLP评估基准范畴，与我的核心关注点无关。虽然因果推理在推荐和搜索中有潜在应用，但论文的重点是基准测试而非实际应用或架构改进，因此相关性很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07230v1": {
    "title": "Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping",
    "url": "https://www.alphaxiv.org/abs/2510.07230v1",
    "arxiv_id": "2510.07230v1",
    "authors": "Ziyi Wang, Yuxuan Lu, Yimeng Zhang, Jing Huang, Dakuo Wang",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 17:00:25",
    "ori_summary": "Simulating step-wise human behavior with Large Language Models (LLMs) has become an emerging research direction, enabling applications in various practical domains. While prior methods, including prompting, supervised fine-tuning (SFT), and reinforcement learning (RL), have shown promise in modeling step-wise behavior, they primarily learn a population-level policy without conditioning on a user's persona, yielding generic rather than personalized simulations. In this work, we pose a critical question: how can LLM agents better simulate personalized user behavior? We introduce Customer-R1, an RL-based method for personalized, step-wise user behavior simulation in online shopping environments. Our policy is conditioned on an explicit persona, and we optimize next-step rationale and action generation via action correctness reward signals. Experiments on the OPeRA dataset emonstrate that Customer-R1 not only significantly outperforms prompting and SFT-based baselines in next-action prediction tasks, but also better matches users' action distribution, indicating higher fidelity in personalized behavior simulation.",
    "summary": "论文研究如何让LLM代理更好地模拟个性化用户行为。核心方法是提出Customer-R1，通过基于明确用户画像的强化学习策略，优化下一步推理和动作生成，实现个性化逐步行为模拟。",
    "translation": "Customer-R1：基于强化学习的LLM智能体在在线购物中实现个性化人类行为模拟",
    "relevance_score": 9,
    "reasoning": "该论文直接应用LLM技术构建个性化智能体来模拟在线购物行为，属于Direct LLM Applications范畴。RL-based LLM Agent方法可以应用于推荐系统和搜索场景中的用户行为建模与仿真，为系统优化和A/B测试提供高效工具。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对个性化用户行为模拟这一推荐系统核心问题，提出基于强化学习的LLM代理方法，完全符合个性化推荐和LLM应用的研究方向。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07227v1": {
    "title": "Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.07227v1",
    "arxiv_id": "2510.07227v1",
    "authors": "Arjun Krishnakumar, Rhea Sanjay Sukthanker, Hannan Javed Mahadik, Gabriela Kadlecová, Vladyslav Moroshan, Timur Carstensen, Frank Hutter, Aaron Klein",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 16:57:46",
    "ori_summary": "Small Language models (SLMs) offer an efficient and accessible alternative to Large Language Models (LLMs), delivering strong performance while using far fewer resources. We introduce a simple and effective framework for pretraining SLMs that brings together three complementary ideas. First, we identify structurally sparse sub-network initializations that consistently outperform randomly initialized models of similar size under the same compute budget. Second, we use evolutionary search to automatically discover high-quality sub-network initializations, providing better starting points for pretraining. Third, we apply knowledge distillation from larger teacher models to speed up training and improve generalization. Together, these components make SLM pretraining substantially more efficient: our best model, discovered using evolutionary search and initialized with LLM weights, matches the validation perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining tokens. We release all code and models at https://github.com/whittle-org/whittle/, offering a practical and reproducible path toward cost-efficient small language model development at scale.",
    "summary": "该论文研究如何高效预训练小语言模型。核心方法是结合结构稀疏子网络初始化、进化搜索发现优质初始化点以及知识蒸馏技术，构建更高效的预训练框架。",
    "translation": "从何处开始：通过子网络选择与蒸馏实现高效预训练",
    "relevance_score": 8,
    "reasoning": "该论文聚焦于高效预训练方法，这属于'使能LLM技术'范畴，通过子网络选择和蒸馏技术可以显著降低LLM预训练的计算成本和资源需求。在推荐系统、搜索和广告领域，这种高效预训练技术能够使企业以更低的成本训练更大规模的模型，或实现更频繁的模型更新迭代，从而提升系统性能。",
    "rerank_relevance_score": 6,
    "rerank_reasoning": "该论文聚焦小语言模型高效预训练方法，通过子网络选择和蒸馏技术提升训练效率，虽然不直接针对推荐系统，但其模型压缩和高效训练技术对推荐系统模型优化有潜在应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07226v1": {
    "title": "Machines in the Crowd? Measuring the Footprint of Machine-Generated Text on Reddit",
    "url": "https://www.alphaxiv.org/abs/2510.07226v1",
    "arxiv_id": "2510.07226v1",
    "authors": "Lucio La Cava, Luca Maria Aiello, Andrea Tagarelli",
    "categories": "cs.SI, cs.CL, cs.CY, physics.soc-ph",
    "pub_date": "2025-10-08 16:57:45",
    "ori_summary": "Generative Artificial Intelligence is reshaping online communication by enabling large-scale production of Machine-Generated Text (MGT) at low cost. While its presence is rapidly growing across the Web, little is known about how MGT integrates into social media environments. In this paper, we present the first large-scale characterization of MGT on Reddit. Using a state-of-the-art statistical method for detection of MGT, we analyze over two years of activity (2022-2024) across 51 subreddits representative of Reddit's main community types such as information seeking, social support, and discussion. We study the concentration of MGT across communities and over time, and compared MGT to human-authored text in terms of social signals it expresses and engagement it receives. Our very conservative estimate of MGT prevalence indicates that synthetic text is marginally present on Reddit, but it can reach peaks of up to 9% in some communities in some months. MGT is unevenly distributed across communities, more prevalent in subreddits focused on technical knowledge and social support, and often concentrated in the activity of a small fraction of users. MGT also conveys distinct social signals of warmth and status giving typical of language of AI assistants. Despite these stylistic differences, MGT achieves engagement levels comparable than human-authored content and in a few cases even higher, suggesting that AI-generated text is becoming an organic component of online social discourse. This work offers the first perspective on the MGT footprint on Reddit, paving the way for new investigations involving platform governance, detection strategies, and community dynamics.",
    "summary": "",
    "translation": "人群中的机器？测量机器生成文本在Reddit上的足迹",
    "relevance_score": 2,
    "reasoning": "该论文主要关注机器生成文本的检测和测量，这属于内容生成和真实性验证领域。虽然涉及社交媒体平台，但核心焦点是检测生成内容而非推荐、搜索或广告系统的改进，与我的技术焦点重叠有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07221v1": {
    "title": "How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu",
    "url": "https://www.alphaxiv.org/abs/2510.07221v1",
    "arxiv_id": "2510.07221v1",
    "authors": "Benjamin Akera, Evelyn Nafula, Patrick Walukagga, Gilbert Yiga, John Quinn, Ernest Mwebaze",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 16:55:28",
    "ori_summary": "The development of Automatic Speech Recognition (ASR) systems for low-resource African languages remains challenging due to limited transcribed speech data. While recent advances in large multilingual models like OpenAI's Whisper offer promising pathways for low-resource ASR development, critical questions persist regarding practical deployment requirements. This paper addresses two fundamental concerns for practitioners: determining the minimum data volumes needed for viable performance and characterizing the primary failure modes that emerge in production systems. We evaluate Whisper's performance through comprehensive experiments on two Bantu languages: systematic data scaling analysis on Kinyarwanda using training sets from 1 to 1,400 hours, and detailed error characterization on Kikuyu using 270 hours of training data. Our scaling experiments demonstrate that practical ASR performance (WER < 13\\%) becomes achievable with as little as 50 hours of training data, with substantial improvements continuing through 200 hours (WER < 10\\%). Complementing these volume-focused findings, our error analysis reveals that data quality issues, particularly noisy ground truth transcriptions, account for 38.6\\% of high-error cases, indicating that careful data curation is as critical as data volume for robust system performance. These results provide actionable benchmarks and deployment guidance for teams developing ASR systems across similar low-resource language contexts. We release accompanying and models see https://github.com/SunbirdAI/kinyarwanda-whisper-eval",
    "summary": "",
    "translation": "非洲语言自动语音识别需要多少语音数据？基尼亚卢旺达语和基库尤语中数据规模扩展的评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动语音识别（ASR）在非洲语言中的研究，属于语音处理领域。虽然涉及数据扩展评估，但核心关注点是语音识别而非推荐系统、搜索或广告应用，与当前关注的LLM技术、推荐系统架构或Transformer改进等焦点领域无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07213v1": {
    "title": "Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07213v1",
    "arxiv_id": "2510.07213v1",
    "authors": "Chengzhi Zhong, Fei Cheng, Qianying Liu, Yugo Murawaki, Chenhui Chu, Sadao Kurohashi",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 16:46:57",
    "ori_summary": "Large language models exhibit strong multilingual capabilities despite limited exposure to non-English data. Prior studies show that English-centric large language models map multilingual content into English-aligned representations at intermediate layers and then project them back into target-language token spaces in the final layer. From this observation, we hypothesize that this cross-lingual transition is governed by a small and sparse set of dimensions, which occur at consistent indices across the intermediate to final layers. Building on this insight, we introduce a simple, training-free method to identify and manipulate these dimensions, requiring only as few as 50 sentences of either parallel or monolingual data. Experiments on a multilingual generation control task reveal the interpretability of these dimensions, demonstrating that the interventions in these dimensions can switch the output language while preserving semantic content, and that it surpasses the performance of prior neuron-based approaches at a substantially lower cost.",
    "summary": "",
    "translation": "语言存在于稀疏维度：面向大语言模型的可解释与高效多语言控制",
    "relevance_score": 6,
    "reasoning": "该论文聚焦于LLM的多语言控制与稀疏维度表征，属于'Enabling LLM Tech'范畴。稀疏控制机制可应用于多语言搜索和推荐系统的精准用户意图理解，通过可解释的稀疏激活实现跨语言内容匹配和个性化推荐。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07203v1": {
    "title": "Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07203v1",
    "arxiv_id": "2510.07203v1",
    "authors": "Benjamin Akera, Evelyn Nafula Ouma, Gilbert Yiga, Patrick Walukagga, Phionah Natukunda, Trevor Saaka, Solomon Nsumba, Lilian Teddy Nabukeera, Joel Muhanguzi, Imran Sekalala, Nimpamya Janat Namara, Engineer Bainomugisha, Ernest Mwebaze, John Quinn",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 16:35:53",
    "ori_summary": "There are more than 2000 living languages in Africa, most of which have been bypassed by advances in language technology. Current leading LLMs exhibit strong performance on a number of the most common languages (e.g. Swahili or Yoruba), but prioritise support for the languages with the most speakers first, resulting in piecemeal ability across disparate languages. We contend that a regionally focussed approach is more efficient, and present a case study for Uganda, a country with high linguistic diversity. We describe the development of Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the art comprehension in the majority of all Ugandan languages. These models are open source and can be used to reduce language barriers in a number of important practical applications.",
    "summary": "",
    "translation": "向日葵：一种扩大大型语言模型中非洲语言覆盖范围的新方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于非洲语言的多语言扩展，这属于纯粹的NLP语言覆盖问题，与推荐系统、搜索或广告的核心技术无关。即使作为使能技术，非洲语言的扩展在主流RecSys/Search/Ads应用中的潜在价值非常有限，因为这些领域主要关注主流语言和用户行为建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07178v1": {
    "title": "Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible",
    "url": "https://www.alphaxiv.org/abs/2510.07178v1",
    "arxiv_id": "2510.07178v1",
    "authors": "Imry Ziv, Nur Lan, Emmanuel Chemla, Roni Katzir",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 16:17:13",
    "ori_summary": "Are large language models (LLMs) sensitive to the distinction between humanly possible languages and humanly impossible languages? This question is taken by many to bear on whether LLMs and humans share the same innate learning biases. Previous work has attempted to answer it in the positive by comparing LLM learning curves on existing language datasets and on \"impossible\" datasets derived from them via various perturbation functions. Using the same methodology, we examine this claim on a wider set of languages and impossible perturbations. We find that in most cases, GPT-2 learns each language and its impossible counterpart equally easily, in contrast to previous claims. We also apply a more lenient condition by testing whether GPT-2 provides any kind of separation between the whole set of natural languages and the whole set of impossible languages. By considering cross-linguistic variance in various metrics computed on the perplexity curves, we show that GPT-2 provides no systematic separation between the possible and the impossible. Taken together, these perspectives show that LLMs do not share the human innate biases that shape linguistic typology.",
    "summary": "",
    "translation": "无偏见语言模型学习不自然：LLMs如何无法区分可能与不可能",
    "relevance_score": 2,
    "reasoning": "该论文主要关注语言模型的偏见问题和区分可能/不可能的能力，这属于纯粹的NLP评估和基准测试范畴。虽然LLM技术本身是相关领域，但论文焦点是语言模型的内在缺陷评估，而非在推荐系统、搜索或广告中的实际应用或架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07177v1": {
    "title": "CARPAS: Towards Content-Aware Refinement of Provided Aspects for Summarization in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07177v1",
    "arxiv_id": "2510.07177v1",
    "authors": "Yong-En Tian, Yu-Chien Tang, An-Zi Yen, Wen-Chih Peng",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 16:16:46",
    "ori_summary": "Aspect-based summarization has attracted significant attention for its ability to generate more fine-grained and user-aligned summaries. While most existing approaches assume a set of predefined aspects as input, real-world scenarios often present challenges where these given aspects may be incomplete, irrelevant, or entirely missing from the document. Users frequently expect systems to adaptively refine or filter the provided aspects based on the actual content. In this paper, we initiate this novel task setting, termed Content-Aware Refinement of Provided Aspects for Summarization (CARPAS), with the aim of dynamically adjusting the provided aspects based on the document context before summarizing. We construct three new datasets to facilitate our pilot experiments, and by using LLMs with four representative prompting strategies in this task, we find that LLMs tend to predict an overly comprehensive set of aspects, which often results in excessively long and misaligned summaries. Building on this observation, we propose a preliminary subtask to predict the number of relevant aspects, and demonstrate that the predicted number can serve as effective guidance for the LLMs, reducing the inference difficulty, and enabling them to focus on the most pertinent aspects. Our extensive experiments show that the proposed approach significantly improves performance across all datasets. Moreover, our deeper analyses uncover LLMs' compliance when the requested number of aspects differs from their own estimations, establishing a crucial insight for the deployment of LLMs in similar real-world applications.",
    "summary": "",
    "translation": "CARPAS：面向大型语言模型摘要任务的内容感知提供方面精炼",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM在文本摘要任务中的内容感知方面精炼，这属于纯粹的NLP应用领域。虽然涉及LLM技术，但论文聚焦于摘要生成这一与推荐系统、搜索或广告无关的特定任务，没有展示在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07175v1": {
    "title": "Quantifying Data Contamination in Psychometric Evaluations of LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.07175v1",
    "arxiv_id": "2510.07175v1",
    "authors": "Jongwook Han, Woojung Song, Jonggeun Lee, Yohan Jo",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-08 16:16:20",
    "ori_summary": "Recent studies apply psychometric questionnaires to Large Language Models (LLMs) to assess high-level psychological constructs such as values, personality, moral foundations, and dark traits. Although prior work has raised concerns about possible data contamination from psychometric inventories, which may threaten the reliability of such evaluations, there has been no systematic attempt to quantify the extent of this contamination. To address this gap, we propose a framework to systematically measure data contamination in psychometric evaluations of LLMs, evaluating three aspects: (1) item memorization, (2) evaluation memorization, and (3) target score matching. Applying this framework to 21 models from major families and four widely used psychometric inventories, we provide evidence that popular inventories such as the Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40) exhibit strong contamination, where models not only memorize items but can also adjust their responses to achieve specific target scores.",
    "summary": "",
    "translation": "量化大语言模型心理测量评估中的数据污染",
    "relevance_score": 1,
    "reasoning": "该论文关注LLM评估中的数据污染问题，属于纯粹的评估基准研究，与我的核心关注点（推荐系统、搜索、广告的算法进步和LLM应用）无关。论文内容涉及评估方法论而非技术应用，属于明确排除的'评估基准'类别。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07173v1": {
    "title": "NurseLLM: The First Specialized Language Model for Nursing",
    "url": "https://www.alphaxiv.org/abs/2510.07173v1",
    "arxiv_id": "2510.07173v1",
    "authors": "Md Tawkat Islam Khondaker, Julia Harrington, Shady Shehata",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-08 16:15:06",
    "ori_summary": "Recent advancements in large language models (LLMs) have significantly transformed medical systems. However, their potential within specialized domains such as nursing remains largely underexplored. In this work, we introduce NurseLLM, the first nursing-specialized LLM tailored for multiple choice question-answering (MCQ) tasks. We develop a multi-stage data generation pipeline to build the first large scale nursing MCQ dataset to train LLMs on a broad spectrum of nursing topics. We further introduce multiple nursing benchmarks to enable rigorous evaluation. Our extensive experiments demonstrate that NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of comparable size on different benchmarks, underscoring the importance of a specialized LLM for the nursing domain. Finally, we explore the role of reasoning and multi-agent collaboration systems in nursing, highlighting their promise for future research and applications.",
    "summary": "",
    "translation": "NurseLLM：首个面向护理领域的专用语言模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗护理领域的专用语言模型开发，属于明确的领域特定应用，与推荐系统、搜索或广告领域完全无关。论文标题明确指向医疗护理这一被排除的领域，没有任何技术内容表明其在推荐系统、搜索或广告方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07169v1": {
    "title": "More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.07169v1",
    "arxiv_id": "2510.07169v1",
    "authors": "Yike Zhao, Simin Guo, Ziqing Yang, Shifan Han, Dahua Lin, Fei Tan",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 16:07:26",
    "ori_summary": "The reasoning capabilities of Large Language Models (LLMs) play a critical role in many downstream tasks, yet depend strongly on the quality of training data. Despite various proposed data construction methods, their practical utility in real-world pipelines remains underexplored. In this work, we conduct a comprehensive analysis of open-source datasets and data synthesis techniques for mathematical reasoning, evaluating them under a unified pipeline designed to mirror training and deployment scenarios. We further distill effective data selection strategies and identify practical methods suitable for industrial applications. Our findings highlight that structuring data in more interpretable formats, or distilling from stronger models often outweighs simply scaling up data volume. This study provides actionable guidance for integrating training data to enhance LLM capabilities, supporting both cost-effective data curation and scalable model enhancement. We hope this work will inspire further research on how to balance \"more data\" versus \"better data\" for real-world reasoning tasks.",
    "summary": "",
    "translation": "更多数据还是更好数据？数学推理中数据选择与合成的关键分析",
    "relevance_score": 2,
    "reasoning": "该论文主要关注数学推理领域的数据选择与合成方法，属于特定领域的数据处理技术。虽然数据质量是推荐系统和搜索系统的重要考虑因素，但该论文缺乏与推荐、搜索或广告领域的直接联系，也没有涉及LLM技术、Transformer架构或异构数据建模等核心关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07167v1": {
    "title": "Reasoning for Hierarchical Text Classification: The Case of Patents",
    "url": "https://www.alphaxiv.org/abs/2510.07167v1",
    "arxiv_id": "2510.07167v1",
    "authors": "Lekang Jiang, Wenjun Sun, Stephan Goetz",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 16:06:04",
    "ori_summary": "Hierarchical text classification (HTC) assigns documents to multiple levels of a pre-defined taxonomy. Automated patent subject classification represents one of the hardest HTC scenarios because of domain knowledge difficulty and a huge number of labels. Prior approaches only output a flat label set, which offers little insight into the reason behind predictions. Therefore, we propose Reasoning for Hierarchical Classification (RHC), a novel framework that reformulates HTC as a step-by-step reasoning task to sequentially deduce hierarchical labels. RHC trains large language models (LLMs) in two stages: a cold-start stage that aligns outputs with chain-of-thought (CoT) reasoning format and a reinforcement learning (RL) stage to enhance multi-step reasoning ability. RHC demonstrates four advantages in our experiments. (1) Effectiveness: RHC surpasses previous baselines and outperforms the supervised fine-tuning counterparts by approximately 3% in accuracy and macro F1. (2) Explainability: RHC produces natural-language justifications before prediction to facilitate human inspection. (3) Scalability: RHC scales favorably with model size with larger gains compared to standard fine-tuning. (4) Applicability: Beyond patents, we further demonstrate that RHC achieves state-of-the-art performance on other widely used HTC benchmarks, which highlights its broad applicability.",
    "summary": "",
    "translation": "层次化文本分类的推理机制：以专利文本为例",
    "relevance_score": 2,
    "reasoning": "该论文专注于层次化文本分类这一特定NLP任务，主要解决专利文档的分类问题。虽然分类技术在某些推荐系统中可能有间接应用，但该工作缺乏与推荐系统、搜索或广告的直接关联，也不涉及LLM技术、Transformer架构进展或异构数据统一建模等核心关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07147v1": {
    "title": "A Multi-Agent Framework for Stateful Inference-Time Search",
    "url": "https://www.alphaxiv.org/abs/2510.07147v1",
    "arxiv_id": "2510.07147v1",
    "authors": "Arshika Lalan, Rajat Ghosh, Aditya Kolsur, Debojyoti Dutta",
    "categories": "cs.LG, cs.AI, cs.CL, cs.MA, cs.SE",
    "pub_date": "2025-10-08 15:48:41",
    "ori_summary": "Recent work explores agentic inference-time techniques to perform structured, multi-step reasoning. However, stateless inference often struggles on multi-step tasks due to the absence of persistent state. Moreover, task-specific fine-tuning or instruction-tuning often achieve surface-level code generation but remain brittle on tasks requiring deeper reasoning and long-horizon dependencies. To address these limitations, we propose stateful multi-agent evolutionary search, a training-free framework that departs from prior stateless approaches by combining (i) persistent inference-time state, (ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate its effectiveness in automated unit test generation through the generation of edge cases. We generate robust edge cases using an evolutionary search process, where specialized agents sequentially propose, mutate, and score candidates. A controller maintains persistent state across generations, while evolutionary preservation ensures diversity and exploration across all possible cases. This yields a generalist agent capable of discovering robust, high-coverage edge cases across unseen codebases. Experiments show our stateful multi-agent inference framework achieves substantial gains in coverage over stateless single-step baselines, evaluated on prevalent unit-testing benchmarks such as HumanEval and TestGenEvalMini and using three diverse LLM families - Llama, Gemma, and GPT. These results indicate that combining persistent inference-time state with evolutionary search materially improves unit-test generation.",
    "summary": "论文研究多步推理任务中无状态推理的局限性问题，核心思想是结合持久推理状态、对抗性变异和进化保留，构建状态保持的多智能体进化搜索框架来增强深度推理能力。",
    "translation": "用于有状态推理时搜索的多智能体框架",
    "relevance_score": 8,
    "reasoning": "该论文提出多智能体框架用于推理时搜索，这属于核心搜索领域的进展。有状态推理时搜索技术可直接应用于搜索系统的查询理解和结果优化，通过多智能体协作提升搜索质量和效率，与搜索领域的核心算法改进高度相关。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出的状态保持多智能体进化搜索框架在推理架构上有创新，虽然应用于单元测试生成，但其核心思想对推荐系统的复杂推理和搜索优化有直接借鉴意义。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07141v1": {
    "title": "Comparing human and language models sentence processing difficulties on complex structures",
    "url": "https://www.alphaxiv.org/abs/2510.07141v1",
    "arxiv_id": "2510.07141v1",
    "authors": "Samuel Joseph Amouyal, Aya Meltzer-Asscher, Jonathan Berant",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 15:42:49",
    "ori_summary": "Large language models (LLMs) that fluently converse with humans are a reality - but do LLMs experience human-like processing difficulties? We systematically compare human and LLM sentence comprehension across seven challenging linguistic structures. We collect sentence comprehension data from humans and five families of state-of-the-art LLMs, varying in size and training procedure in a unified experimental framework. Our results show LLMs overall struggle on the target structures, but especially on garden path (GP) sentences. Indeed, while the strongest models achieve near perfect accuracy on non-GP structures (93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5). Additionally, when ranking structures based on average performance, rank correlation between humans and models increases with parameter count. For each target structure, we also collect data for their matched baseline without the difficult structure. Comparing performance on the target vs. baseline sentences, the performance gap observed in humans holds for LLMs, with two exceptions: for models that are too weak performance is uniformly low across both sentence types, and for models that are too strong the performance is uniformly high. Together, these reveal convergence and divergence in human and LLM sentence comprehension, offering new insights into the similarity of humans and LLMs.",
    "summary": "",
    "translation": "比较人类和语言模型在复杂结构上的句子处理困难",
    "relevance_score": 3,
    "reasoning": "该论文主要关注语言模型与人类在复杂句子处理上的比较，属于语言模型评估范畴。虽然涉及语言模型，但论文焦点是认知科学角度的处理困难分析，而非直接应用于推荐系统、搜索或广告的技术进展。对于'使能技术'领域，这种基础认知分析可能间接帮助理解模型在复杂查询处理中的局限性，但缺乏明确的直接应用路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07118v1": {
    "title": "TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning",
    "url": "https://www.alphaxiv.org/abs/2510.07118v1",
    "arxiv_id": "2510.07118v1",
    "authors": "Manish Nagaraj, Sakshi Choudhary, Utkarsh Saxena, Deepak Ravikumar, Kaushik Roy",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-08 15:11:04",
    "ori_summary": "Instruction tuning is essential for aligning large language models (LLMs) to downstream tasks and commonly relies on large, diverse corpora. However, small, high-quality subsets, known as coresets, can deliver comparable or superior results, though curating them remains challenging. Existing methods often rely on coarse, sample-level signals like gradients, an approach that is computationally expensive and overlooks fine-grained features. To address this, we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a forward-only, token-centric framework. Instead of using gradients, TRIM operates by matching underlying representational patterns identified via attention-based \"fingerprints\" from a handful of target samples. Such an approach makes TRIM highly efficient and uniquely sensitive to the structural features that define a task. Coresets selected by our method consistently outperform state-of-the-art baselines by up to 9% on downstream tasks and even surpass the performance of full-data fine-tuning in some settings. By avoiding expensive backward passes, TRIM achieves this at a fraction of the computational cost. These findings establish TRIM as a scalable and efficient alternative for building high-quality instruction-tuning datasets.",
    "summary": "论文研究如何从大规模指令调优数据中高效选择高质量子集；核心思想是利用前向传播中的注意力模式作为token级指纹，匹配目标样本的表征结构来识别关键训练数据。",
    "translation": "TRIM：基于逐词注意力推导显著性的数据高效指令微调",
    "relevance_score": 8,
    "reasoning": "该论文提出基于注意力机制的数据高效指令微调方法，属于核心LLM技术进步。注意力驱动的显著性和数据效率技术可直接应用于推荐系统和搜索中的用户意图理解与个性化建模，通过更高效的指令微调提升模型在特定领域的性能表现。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出了基于注意力指纹的高效指令调优数据选择方法，直接针对LLM训练效率这一核心瓶颈，对大规模推荐系统的模型优化具有重要参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07105v1": {
    "title": "Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning",
    "url": "https://www.alphaxiv.org/abs/2510.07105v1",
    "arxiv_id": "2510.07105v1",
    "authors": "Taylor Sorensen, Yejin Choi",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 14:59:24",
    "ori_summary": "Many natural language processing (NLP) tasks involve subjectivity, ambiguity, or legitimate disagreement between annotators. In this paper, we outline our system for modeling human variation. Our system leverages language models' (LLMs) in-context learning abilities, along with a two-step meta-learning training procedure for 1) post-training on many datasets requiring in-context learning and 2) specializing the model via in-context meta-learning to the particular data distribution of interest. We also evaluate the performance of our system submission to the Learning With Disagreements (LeWiDi) competition, where it was the overall winner on both tasks. Additionally, we perform an ablation study to measure the importance of each system component. We find that including rater examples in-context is crucial for our system's performance, dataset-specific fine-tuning is helpful on the larger datasets, post-training on other in-context datasets is helpful on one of the competition datasets, and that performance improves with model scale.",
    "summary": "",
    "translation": "Opt-ICL在LeWiDi-2025：通过元学习最大化来自评分者示例的上下文信号",
    "relevance_score": 3,
    "reasoning": "该论文关注于通过元学习优化上下文学习（ICL），这属于LLM核心技术范畴，可能应用于搜索或推荐系统中更有效地利用少量示例进行个性化调整。然而，论文标题明确指向LeWiDi-2025（可能是一个特定竞赛或数据集），且焦点是'评分者示例'，这表明其应用可能更偏向评估基准或特定NLP任务，而非直接面向推荐/搜索/广告系统的核心问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07098v1": {
    "title": "TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription",
    "url": "https://www.alphaxiv.org/abs/2510.07098v1",
    "arxiv_id": "2510.07098v1",
    "authors": "Guo Yutong, Wanying Wang, Yue Wu, Zichen Miao, Haoyu Wang",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 14:56:42",
    "ori_summary": "Table Visual Question Answering (Table VQA) is typically addressed by large vision-language models (VLMs). While such models can answer directly from images, they often miss fine-grained details unless scaled to very large sizes, which are computationally prohibitive, especially for mobile deployment. A lighter alternative is to have a small VLM perform OCR and then use a large language model (LLM) to reason over structured outputs such as Markdown tables. However, these representations are not naturally optimized for LLMs and still introduce substantial errors. We propose TALENT (Table VQA via Augmented Language-Enhanced Natural-text Transcription), a lightweight framework that leverages dual representations of tables. TALENT prompts a small VLM to produce both OCR text and natural language narration, then combines them with the question for reasoning by an LLM. This reframes Table VQA as an LLM-centric multimodal reasoning task, where the VLM serves as a perception-narration module rather than a monolithic solver. Additionally, we construct ReTabVQA, a more challenging Table VQA dataset requiring multi-step quantitative reasoning over table images. Experiments show that TALENT enables a small VLM-LLM combination to match or surpass a single large VLM at significantly lower computational cost on both public datasets and ReTabVQA.",
    "summary": "",
    "translation": "TALENT：通过增强语言的自然文本转录实现表格视觉问答",
    "relevance_score": 2,
    "reasoning": "虽然该论文涉及表格理解和问答，但其核心是视觉问答(VQA)任务，主要关注表格图像到文本的转换和问答能力。这与推荐系统、搜索或广告的核心技术栈关联较弱，表格数据在这些领域通常以结构化格式直接处理，而非通过视觉问答方式。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07096v1": {
    "title": "Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis",
    "url": "https://www.alphaxiv.org/abs/2510.07096v1",
    "arxiv_id": "2510.07096v1",
    "authors": "Zhu Li, Yuqing Zhang, Xiyuan Gao, Shekhar Nayak, Matt Coler",
    "categories": "cs.CL, cs.SD, eess.AS",
    "pub_date": "2025-10-08 14:53:48",
    "ori_summary": "Sarcasm is a subtle form of non-literal language that poses significant challenges for speech synthesis due to its reliance on nuanced semantic, contextual, and prosodic cues. While existing speech synthesis research has focused primarily on broad emotional categories, sarcasm remains largely unexplored. In this paper, we propose a Large Language Model (LLM)-enhanced Retrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach combines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture pragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic exemplars retrieved via a Retrieval Augmented Generation (RAG) module, which provide expressive reference patterns of sarcastic delivery. Integrated within a VITS backbone, this dual conditioning enables more natural and contextually appropriate sarcastic speech. Experiments demonstrate that our method outperforms baselines in both objective measures and subjective evaluations, yielding improvements in speech naturalness, sarcastic expressivity, and downstream sarcasm detection.",
    "summary": "",
    "translation": "让机器听起来讽刺：LLM增强与检索引导的讽刺语音合成",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音合成和讽刺检测，属于纯语音领域，与推荐系统、搜索或广告的排名任务没有直接关联。即使涉及LLM技术，其应用方向（语音合成）在指定的关注领域中缺乏明确的实用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07091v1": {
    "title": "The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas",
    "url": "https://www.alphaxiv.org/abs/2510.07091v1",
    "arxiv_id": "2510.07091v1",
    "authors": "Baixuan Xu, Tianshi Zheng, Zhaowei Wang, Hong Ting Tsang, Weiqi Wang, Tianqing Fang, Yangqiu Song",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-08 14:47:40",
    "ori_summary": "Enabling LLMs to effectively operate long-horizon task which requires long-term planning and multiple interactions is essential for open-world autonomy. Conventional methods adopt planning with actions where a executable action list would be provided as reference. However, this action representation choice would be impractical when the environment action space is combinatorial exploded (e.g., open-ended real world). This naturally leads to a question: As environmental action space scales, what is the optimal action representation for long-horizon agents? In this paper, we systematically study the effectiveness of two different action representations. The first one is conventional planning with actions (PwA) which is predominantly adopted for its effectiveness on existing benchmarks. The other one is planning with schemas (PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ] to [OBJ]\" -> \"move apple to desk\") to ensure concise action space and reliable scalability. This alternative is motivated by its alignment with human cognition and its compliance with environment-imposed action format restriction. We propose cognitive bandwidth perspective as a conceptual framework to qualitatively understand the differences between these two action representations and empirically observe a representation-choice inflection point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve as evidence of the need for scalable representations. We further conduct controlled experiments to study how the location of this inflection point interacts with different model capacities: stronger planning proficiency shifts the inflection rightward, whereas better schema instantiation shifts it leftward. Finally, noting the suboptimal performance of PwS agents, we provide an actionable guide for building more capable PwS agents for better scalable autonomy.",
    "summary": "",
    "translation": "认知带宽瓶颈：将长视野智能体从动作规划转向模式规划",
    "relevance_score": 3,
    "reasoning": "该论文聚焦于智能体规划和认知架构，属于AI代理领域而非推荐系统、搜索或广告的核心技术。虽然规划效率提升可能间接影响需要长期规划的推荐场景，但缺乏与Transformer架构、LLM技术或推荐系统直接相关的明确连接点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07083v1": {
    "title": "All Claims Are Equal, but Some Claims Are More Equal Than Others: Importance-Sensitive Factuality Evaluation of LLM Generations",
    "url": "https://www.alphaxiv.org/abs/2510.07083v1",
    "arxiv_id": "2510.07083v1",
    "authors": "Miriam Wanner, Leif Azzopardi, Paul Thomas, Soham Dan, Benjamin Van Durme, Nick Craswell",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 14:40:33",
    "ori_summary": "Existing methods for evaluating the factuality of large language model (LLM) responses treat all claims as equally important. This results in misleading evaluations when vital information is missing or incorrect as it receives the same weight as peripheral details, raising the question: how can we reliably detect such differences when there are errors in key information? Current approaches that measure factuality tend to be insensitive to omitted or false key information. To investigate this lack of sensitivity, we construct VITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses designed to omit or falsify key information. Using this dataset, we demonstrate the insensitivities of existing evaluation metrics to key information errors. To address this gap, we introduce VITAL, a set of metrics that provide greater sensitivity in measuring the factuality of responses by incorporating the relevance and importance of claims with respect to the query. Our analysis demonstrates that VITAL metrics more reliably detect errors in key information than previous methods. Our dataset, metrics, and analysis provide a foundation for more accurate and robust assessment of LLM factuality.",
    "summary": "",
    "translation": "所有声明皆平等，但某些声明比其他声明更平等：LLM生成内容的重要性敏感事实性评估",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM生成内容的事实性评估和幻觉检测，这属于纯粹的NLP评估基准范畴。虽然事实性在搜索系统中可能有一定相关性，但论文的核心焦点是通用的LLM评估方法，而非针对推荐系统、搜索或广告的具体应用或架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07081v1": {
    "title": "Accelerating Diffusion LLM Inference via Local Determinism Propagation",
    "url": "https://www.alphaxiv.org/abs/2510.07081v1",
    "arxiv_id": "2510.07081v1",
    "authors": "Fanheng Kong, Jingyuan Zhang, Yahui Liu, Zirui Wu, Yu Tian, Victoria W., Guorui Zhou",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 14:39:34",
    "ori_summary": "Diffusion large language models (dLLMs) represent a significant advancement in text generation, offering parallel token decoding capabilities. However, existing open-source implementations suffer from quality-speed trade-offs that impede their practical deployment. Conservative sampling strategies typically decode only the most confident token per step to ensure quality (i.e., greedy decoding), at the cost of inference efficiency due to repeated redundant refinement iterations--a phenomenon we term delayed decoding. Through systematic analysis of dLLM decoding dynamics, we characterize this delayed decoding behavior and propose a training-free adaptive parallel decoding strategy, named LocalLeap, to address these inefficiencies. LocalLeap is built on two fundamental empirical principles: local determinism propagation centered on high-confidence anchors and progressive spatial consistency decay. By applying these principles, LocalLeap identifies anchors and performs localized relaxed parallel decoding within bounded neighborhoods, achieving substantial inference step reduction through early commitment of already-determined tokens without compromising output quality. Comprehensive evaluation on various benchmarks demonstrates that LocalLeap achieves 6.94$\\times$ throughput improvements and reduces decoding steps to just 14.2\\% of the original requirement, achieving these gains with negligible performance impact. The source codes are available at: https://github.com/friedrichor/LocalLeap.",
    "summary": "",
    "translation": "通过局部确定性传播加速扩散大语言模型推理",
    "relevance_score": 3,
    "reasoning": "该论文主要关注扩散模型（Diffusion Models）的推理加速技术，属于生成式AI的效率优化范畴。虽然扩散模型在AIGC领域有广泛应用，但论文标题未明确显示与推荐系统、搜索或广告的直接关联。局部确定性传播技术可能对序列建模有一定启发，但应用潜力在标题层面不够明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07074v1": {
    "title": "LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish",
    "url": "https://www.alphaxiv.org/abs/2510.07074v1",
    "arxiv_id": "2510.07074v1",
    "authors": "Fred Philippy, Laura Bernardy, Siwen Guo, Jacques Klein, Tegawendé F. Bissyandé",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 14:35:59",
    "ori_summary": "Instruction tuning has become a key technique for enhancing the performance of large language models, enabling them to better follow human prompts. However, low-resource languages such as Luxembourgish face severe limitations due to the lack of high-quality instruction datasets. Traditional reliance on machine translation often introduces semantic misalignment and cultural inaccuracies. In this work, we address these challenges by creating a cross-lingual instruction tuning dataset for Luxembourgish, without resorting to machine-generated translations into it. Instead, by leveraging aligned data from English, French, and German, we build a high-quality dataset that preserves linguistic and cultural nuances. We provide evidence that cross-lingual instruction tuning not only improves representational alignment across languages but also the model's generative capabilities in Luxembourgish. This highlights how cross-lingual data curation can avoid the common pitfalls of machine-translated data and directly benefit low-resource language development.",
    "summary": "",
    "translation": "LuxInstruct：卢森堡语跨语言指令微调数据集",
    "relevance_score": 2,
    "reasoning": "该论文专注于卢森堡语这一小众语言的指令微调数据集构建，属于特定语言NLP任务。虽然跨语言技术可能对多语言搜索系统有间接价值，但论文本身聚焦于特定语言的数据集创建，与推荐系统、搜索或广告的核心技术进展关联度极低，且未涉及Transformer架构改进或异构数据建模等关键技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07061v1": {
    "title": "Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages",
    "url": "https://www.alphaxiv.org/abs/2510.07061v1",
    "arxiv_id": "2510.07061v1",
    "authors": "Amir Hossein Yari, Kalmit Kulkarni, Ahmad Raza Khan, Fajri Koto",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 14:27:02",
    "ori_summary": "While automatic metrics drive progress in Machine Translation (MT) and Text Summarization (TS), existing metrics have been developed and validated almost exclusively for English and other high-resource languages. This narrow focus leaves Indian languages, spoken by over 1.5 billion people, largely overlooked, casting doubt on the universality of current evaluation practices. To address this gap, we introduce ITEM, a large-scale benchmark that systematically evaluates the alignment of 26 automatic metrics with human judgments across six major Indian languages, enriched with fine-grained annotations. Our extensive evaluation, covering agreement with human judgments, sensitivity to outliers, language-specific reliability, inter-metric correlations, and resilience to controlled perturbations, reveals four central findings: (1) LLM-based evaluators show the strongest alignment with human judgments at both segment and system levels; (2) outliers exert a significant impact on metric-human agreement; (3) in TS, metrics are more effective at capturing content fidelity, whereas in MT, they better reflect fluency; and (4) metrics differ in their robustness and sensitivity when subjected to diverse perturbations. Collectively, these findings offer critical guidance for advancing metric design and evaluation in Indian languages.",
    "summary": "",
    "translation": "重新审视印度语言机器翻译与摘要细粒度评估中的度量可靠性",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器翻译和摘要的评估基准与度量可靠性，属于纯粹的NLP评估主题。虽然提到了细粒度评估，但完全围绕语言生成任务在特定语言上的评估方法，没有任何与推荐系统、搜索或广告相关的技术内容或潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07060v1": {
    "title": "Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations",
    "url": "https://www.alphaxiv.org/abs/2510.07060v1",
    "arxiv_id": "2510.07060v1",
    "authors": "Miriam Wanner, Sophia Hager, Anjalie Field",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 14:27:00",
    "ori_summary": "Local news stations are often considered to be reliable sources of non-politicized information, particularly local concerns that residents care about. Because these stations are trusted news sources, viewers are particularly susceptible to the information they report. The Sinclair Broadcast group is a broadcasting company that has acquired many local news stations in the last decade. We investigate the effects of local news stations being acquired by Sinclair: how does coverage change? We use computational methods to investigate changes in internet content put out by local news stations before and after being acquired by Sinclair and in comparison to national news outlets. We find that there is clear evidence that local news stations report more frequently on national news at the expense of local topics, and that their coverage of polarizing national topics increases.",
    "summary": "",
    "translation": "地方新闻是否保持本地化？：辛克莱收购电视台后的在线内容转变",
    "relevance_score": 1,
    "reasoning": "该论文研究媒体所有权变化对地方新闻内容的影响，属于媒体研究领域。这与推荐系统、搜索或广告的核心技术进步、LLM技术、Transformer架构或异构数据建模均无直接关联，也不涉及任何相关技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07048v1": {
    "title": "Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07048v1",
    "arxiv_id": "2510.07048v1",
    "authors": "Yuntao Gui, James Cheng",
    "categories": "cs.CL, cs.AI, I.2.7",
    "pub_date": "2025-10-08 14:16:20",
    "ori_summary": "Despite their remarkable natural language understanding capabilities, Large Language Models (LLMs) have been underutilized for retrieval tasks. We present Search-R3, a novel framework that addresses this limitation by adapting LLMs to generate search embeddings as a direct output of their reasoning process. Our approach exploits LLMs' chain-of-thought capabilities, allowing them to produce more effective embeddings by reasoning step-by-step through complex semantic analyses. We implement this through three complementary mechanisms. (1) a supervised learning stage enables the model's ability to produce quality embeddings, (2) a reinforcement learning (RL) methodology that optimizes embedding generation alongside reasoning, and (3) a specialized RL environment that efficiently handles evolving embedding representations without requiring complete corpus re-encoding at each training iteration. Our extensive evaluations on diverse benchmarks demonstrate that Search-R3 significantly outperforms prior methods by unifying the reasoning and embedding generation processes. This integrated post-training approach represents a substantial advancement in handling complex knowledge-intensive tasks that require both sophisticated reasoning and effective information retrieval. Project page: https://github.com/ytgui/Search-R3",
    "summary": "论文研究如何解决LLM在检索任务中能力未充分利用的问题，核心思想是通过链式推理过程直接生成搜索嵌入，将推理与嵌入生成统一在LLM中。",
    "translation": "Search-R3：在大型语言模型中统一推理与嵌入生成",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及LLM核心技术的进步，将推理与嵌入生成统一起来，这对搜索和推荐系统具有重要应用价值。统一的嵌入生成可以显著提升语义匹配和内容理解能力，同时增强的推理能力能够改善复杂查询处理和个性化推荐的质量。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接统一LLM的推理与嵌入生成，核心解决了检索任务中LLM能力未充分利用的问题，与搜索和LLM应用领域高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07037v1": {
    "title": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07037v1",
    "arxiv_id": "2510.07037v1",
    "authors": "Rajvee Sheth, Samridhi Raj Sinha, Mahavir Patil, Himanshu Beniwal, Mayank Singh",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 14:04:14",
    "ori_summary": "Code-switching (CSW), the alternation of languages and scripts within a single utterance, remains a fundamental challenge for multiling ual NLP, even amidst the rapid advances of large language models (LLMs). Most LLMs still struggle with mixed-language inputs, limited CSW datasets, and evaluation biases, hindering deployment in multilingual societies. This survey provides the first comprehensive analysis of CSW-aware LLM research, reviewing \\total{unique_references} studies spanning five research areas, 12 NLP tasks, 30+ datasets, and 80+ languages. We classify recent advances by architecture, training strategy, and evaluation methodology, outlining how LLMs have reshaped CSW modeling and what challenges persist. The paper concludes with a roadmap emphasizing the need for inclusive datasets, fair evaluation, and linguistically grounded models to achieve truly multilingual intelligence. A curated collection of all resources is maintained at https://github.com/lingo-iitgn/awesome-code-mixing/.",
    "summary": "",
    "translation": "超越单语假设：大语言模型时代下的语码转换自然语言处理综述",
    "relevance_score": 2,
    "reasoning": "该论文主要关注语码转换（code-switching）这一特定NLP任务，属于多语言处理的专门领域。虽然涉及大语言模型，但其核心应用场景是跨语言文本处理，与推荐系统、搜索或广告的核心技术需求（如用户行为建模、内容理解、排序算法）关联度较低，缺乏明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07024v1": {
    "title": "Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge",
    "url": "https://www.alphaxiv.org/abs/2510.07024v1",
    "arxiv_id": "2510.07024v1",
    "authors": "Shrestha Ghosh, Luca Giordano, Yujia Hu, Tuan-Phong Nguyen, Simon Razniewski",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 13:48:38",
    "ori_summary": "LLMs are remarkable artifacts that have revolutionized a range of NLP and AI tasks. A significant contributor is their factual knowledge, which, to date, remains poorly understood, and is usually analyzed from biased samples. In this paper, we take a deep tour into the factual knowledge (or beliefs) of a frontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited set of 100 million beliefs of one of the strongest currently available frontier LLMs, GPT-4.1. We find that the models' factual knowledge differs quite significantly from established knowledge bases, and that its accuracy is significantly lower than indicated by previous benchmarks. We also find that inconsistency, ambiguity and hallucinations are major issues, shedding light on future research opportunities concerning factual LLM knowledge.",
    "summary": "",
    "translation": "挖掘心智：从1亿条信念中揭示前沿大语言模型的知识边界",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM知识边界的探索和信念分析，属于纯粹的LLM评估研究范畴。虽然涉及前沿LLM技术，但缺乏明确的推荐系统、搜索或广告应用场景，更侧重于模型内部知识表征的学术分析而非实际应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07019v1": {
    "title": "Native Hybrid Attention for Efficient Sequence Modeling",
    "url": "https://www.alphaxiv.org/abs/2510.07019v1",
    "arxiv_id": "2510.07019v1",
    "authors": "Jusen Du, Jiaxi Hu, Tao Zhang, Weigao Sun, Yu Cheng",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-08 13:44:57",
    "ori_summary": "Transformers excel at sequence modeling but face quadratic complexity, while linear attention offers improved efficiency but often compromises recall accuracy over long contexts. In this work, we introduce Native Hybrid Attention (NHA), a novel hybrid architecture of linear and full attention that integrates both intra \\& inter-layer hybridization into a unified layer design. NHA maintains long-term context in key-value slots updated by a linear RNN, and augments them with short-term tokens from a sliding window. A single \\texttt{softmax attention} operation is then applied over all keys and values, enabling per-token and per-head context-dependent weighting without requiring additional fusion parameters. The inter-layer behavior is controlled through a single hyperparameter, the sliding window size, which allows smooth adjustment between purely linear and full attention while keeping all layers structurally uniform. Experimental results show that NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains. Code is available at https://github.com/JusenD/NHA.",
    "summary": "研究Transformer序列建模中的二次复杂度问题，核心思想是通过将线性RNN与滑动窗口注意力混合的单层统一设计，实现长短上下文的自适应建模，无需额外融合参数。",
    "translation": "原生混合注意力机制用于高效序列建模",
    "relevance_score": 8,
    "reasoning": "该论文提出了一种新的注意力机制，属于'使能Transformer技术'范畴，直接改进Transformer架构的效率。高效的序列建模对于处理推荐系统中的用户行为序列和搜索中的查询-文档序列至关重要，能够显著提升大规模推荐和搜索系统的性能与效率。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对Transformer架构的效率瓶颈提出混合注意力机制，属于Transformer架构进步的核心领域，对推荐系统和搜索中的长序列建模有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07000v1": {
    "title": "Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages",
    "url": "https://www.alphaxiv.org/abs/2510.07000v1",
    "arxiv_id": "2510.07000v1",
    "authors": "Neel Prabhanjan Rachamalla, Aravind Konakalla, Gautam Rajeev, Ashish Kulkarni, Chandra Khatri, Shubham Agarwal",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 13:23:45",
    "ori_summary": "The effectiveness of Large Language Models (LLMs) depends heavily on the availability of high-quality post-training data, particularly instruction-tuning and preference-based examples. Existing open-source datasets, however, often lack multilingual coverage, cultural grounding, and suffer from task diversity gaps that are especially pronounced for Indian languages. We introduce a human-in-the-loop pipeline that combines translations with synthetic expansion to produce reliable and diverse Indic post-training data. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and Pragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56 sub-categories, leveraging 57 diverse datasets. Our dataset protocol incorporates several often-overlooked dimensions and emphasize task diversity, multi-turn dialogue, instruction fidelity, safety alignment, and preservation of cultural nuance, providing a foundation for more inclusive and effective multilingual LLMs.",
    "summary": "",
    "translation": "Pragyaan：为印度语言设计和策划高质量文化后训练数据集",
    "relevance_score": 2,
    "reasoning": "该论文专注于为特定语言（印度语言）创建文化数据集，这属于数据工程领域，与推荐系统、搜索或广告的核心技术进展无关。虽然多语言能力在理论上可能对全球化推荐系统有帮助，但论文重点在于文化数据集创建而非核心模型架构或推荐算法，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06994v1": {
    "title": "RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning",
    "url": "https://www.alphaxiv.org/abs/2510.06994v1",
    "arxiv_id": "2510.06994v1",
    "authors": "Artur Horal, Daniel Pina, Henrique Paz, Iago Paulo, João Soares, Rafael Ferreira, Diogo Tavares, Diogo Glória-Silva, João Magalhães, David Semedo",
    "categories": "cs.CR, cs.CL",
    "pub_date": "2025-10-08 13:18:42",
    "ori_summary": "This paper presents the vision, scientific contributions, and technical details of RedTWIZ: an adaptive and diverse multi-turn red teaming framework, to audit the robustness of Large Language Models (LLMs) in AI-assisted software development. Our work is driven by three major research streams: (1) robust and systematic assessment of LLM conversational jailbreaks; (2) a diverse generative multi-turn attack suite, supporting compositional, realistic and goal-oriented jailbreak conversational strategies; and (3) a hierarchical attack planner, which adaptively plans, serializes, and triggers attacks tailored to specific LLM's vulnerabilities. Together, these contributions form a unified framework -- combining assessment, attack generation, and strategic planning -- to comprehensively evaluate and expose weaknesses in LLMs' robustness. Extensive evaluation is conducted to systematically assess and analyze the performance of the overall system and each component. Experimental results demonstrate that our multi-turn adversarial attack strategies can successfully lead state-of-the-art LLMs to produce unsafe generations, highlighting the pressing need for more research into enhancing LLM's robustness.",
    "summary": "",
    "translation": "RedTWIZ：通过自适应攻击规划的多样化大语言模型红队测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注大语言模型的安全性和对抗性测试（红队测试），这属于模型安全评估范畴。虽然涉及LLM技术，但其核心关注点是安全测试而非推荐系统、搜索或广告领域的应用。该研究缺乏与RecSys/Search/Ads领域的直接关联或潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06975v1": {
    "title": "VelLMes: A high-interaction AI-based deception framework",
    "url": "https://www.alphaxiv.org/abs/2510.06975v1",
    "arxiv_id": "2510.06975v1",
    "authors": "Muris Sladić, Veronica Valeros, Carlos Catania, Sebastian Garcia",
    "categories": "cs.CR, cs.AI, cs.CL",
    "pub_date": "2025-10-08 13:00:23",
    "ori_summary": "There are very few SotA deception systems based on Large Language Models. The existing ones are limited only to simulating one type of service, mainly SSH shells. These systems - but also the deception technologies not based on LLMs - lack an extensive evaluation that includes human attackers. Generative AI has recently become a valuable asset for cybersecurity researchers and practitioners, and the field of cyber-deception is no exception. Researchers have demonstrated how LLMs can be leveraged to create realistic-looking honeytokens, fake users, and even simulated systems that can be used as honeypots. This paper presents an AI-based deception framework called VelLMes, which can simulate multiple protocols and services such as SSH Linux shell, MySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus VelLMes offers a variety of choices for deception design based on the users' needs. VelLMes is designed to be attacked by humans, so interactivity and realism are key for its performance. We evaluate the generative capabilities and the deception capabilities. Generative capabilities were evaluated using unit tests for LLMs. The results of the unit tests show that, with careful prompting, LLMs can produce realistic-looking responses, with some LLMs having a 100% passing rate. In the case of the SSH Linux shell, we evaluated deception capabilities with 89 human attackers. The results showed that about 30% of the attackers thought that they were interacting with a real system when they were assigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH Linux shell honeypot on the Internet to capture real-life attacks. Analysis of these attacks showed us that LLM honeypots simulating Linux shells can perform well against unstructured and unexpected attacks on the Internet, responding correctly to most of the issued commands.",
    "summary": "",
    "translation": "VelLMes：一种基于人工智能的高交互性欺骗框架",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及欺骗框架和AI安全领域，这属于明确的无关主题范畴，特别是安全相关技术。该研究没有任何与推荐系统、搜索、广告或相关使能技术相关的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06974v1": {
    "title": "Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups",
    "url": "https://www.alphaxiv.org/abs/2510.06974v1",
    "arxiv_id": "2510.06974v1",
    "authors": "Geng Liu, Feng Li, Junjie Mu, Mengxiao Zhu, Francesco Pierri",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 13:00:12",
    "ori_summary": "Large language models (LLMs) are increasingly deployed in user-facing applications, raising concerns about their potential to reflect and amplify social biases. We investigate social identity framing in Chinese LLMs using Mandarin-specific prompts across ten representative Chinese LLMs, evaluating responses to ingroup (\"We\") and outgroup (\"They\") framings, and extending the setting to 240 social groups salient in the Chinese context. To complement controlled experiments, we further analyze Chinese-language conversations from a corpus of real interactions between users and chatbots. Across models, we observe systematic ingroup-positive and outgroup-negative tendencies, which are not confined to synthetic prompts but also appear in naturalistic dialogue, indicating that bias dynamics might strengthen in real interactions. Our study provides a language-aware evaluation framework for Chinese LLMs, demonstrating that social identity biases documented in English generalize cross-linguistically and intensify in user-facing contexts.",
    "summary": "",
    "translation": "使用性别代词和社会群体探测中文大语言模型中的社会身份偏见",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM偏见检测和评估，属于公平性、伦理等非技术性话题，明确列在无关主题中。虽然涉及中文LLM，但核心关注点是社会身份偏见探测，与推荐系统、搜索或广告的核心技术进展、架构改进或直接应用无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06965v1": {
    "title": "EDUMATH: Generating Standards-aligned Educational Math Word Problems",
    "url": "https://www.alphaxiv.org/abs/2510.06965v1",
    "arxiv_id": "2510.06965v1",
    "authors": "Bryan R. Christ, Penelope Molitz, Jonathan Kropko, Thomas Hartvigsen",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 12:53:06",
    "ori_summary": "Math word problems (MWPs) are critical K-12 educational tools, and customizing them to students' interests and ability levels can increase learning outcomes. However, teachers struggle to find time to customize MWPs for each student given large class sizes and increasing burnout. We propose that LLMs can support math education by generating MWPs customized to student interests and math education standards. To this end, we use a joint human expert-LLM judge approach to evaluate over 11,000 MWPs generated by open and closed LLMs and develop the first teacher-annotated dataset for standards-aligned educational MWP generation. We show the value of our data by using it to train a 12B open model that matches the performance of larger and more capable open models. We also use our teacher-annotated data to train a text classifier that enables a 30B open LLM to outperform existing closed baselines without any training. Next, we show our models' MWPs are more similar to human-written MWPs than those from existing models. We conclude by conducting the first study of customized LLM-generated MWPs with grade school students, finding they perform similarly on our models' MWPs relative to human-written MWPs but consistently prefer our customized MWPs.",
    "summary": "",
    "translation": "EDUMATH：生成符合标准的数学教育应用题",
    "relevance_score": 1,
    "reasoning": "该论文专注于教育领域的数学应用题生成，属于特定领域的内容生成应用。这与我的关注点（推荐系统、搜索、广告中的核心进展、使能技术或直接应用）没有直接关联，并且明确排除了AIGC、内容生成等纯LLM中心化主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06961v1": {
    "title": "Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.06961v1",
    "arxiv_id": "2510.06961v1",
    "authors": "Vaibhav Srivastav, Steven Zheng, Eric Bezzam, Eustache Le Bihan, Nithin Koluguri, Piotr Żelasko, Somshubra Majumdar, Adel Moumen, Sanchit Gandhi",
    "categories": "cs.CL, cs.AI, cs.SD, eess.AS",
    "pub_date": "2025-10-08 12:44:51",
    "ori_summary": "Despite rapid progress, ASR evaluation remains saturated with short-form English, and efficiency is rarely reported. We present the Open ASR Leaderboard, a fully reproducible benchmark and interactive leaderboard comparing 60+ open-source and proprietary systems across 11 datasets, including dedicated multilingual and long-form tracks. We standardize text normalization and report both word error rate (WER) and inverse real-time factor (RTFx), enabling fair accuracy-efficiency comparisons. For English transcription, Conformer encoders paired with LLM decoders achieve the best average WER but are slower, while CTC and TDT decoders deliver much better RTFx, making them attractive for long-form and offline use. Whisper-derived encoders fine-tuned for English improve accuracy but often trade off multilingual coverage. All code and dataset loaders are open-sourced to support transparent, extensible evaluation.",
    "summary": "",
    "translation": "开放ASR排行榜：迈向可复现和透明的多语言及长格式语音识别评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音识别（ASR）的评估基准和排行榜，属于纯粹的语音处理领域。虽然语音识别在某些边缘场景可能与搜索相关，但论文本身不涉及推荐系统、搜索排名或广告的核心技术，也没有与LLM、Transformer架构或异构数据建模的直接关联，因此与当前关注点基本无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06953v1": {
    "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces",
    "url": "https://www.alphaxiv.org/abs/2510.06953v1",
    "arxiv_id": "2510.06953v1",
    "authors": "Minju Gwak, Guijin Son, Jaehyung Kim",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-08 12:37:04",
    "ori_summary": "The Uniform Information Density (UID) hypothesis suggests that effective communication maintains a stable flow of information. In this work, we revisit this principle in the context of large language model (LLM) reasoning traces, asking whether step-level uniformity reflects reasoning quality. To this end, we propose an entropy-based stepwise information density metric and introduce two complementary measures of uniformity, local and global uniformity scores. Across the experiments on six different reasoning benchmarks, we find that step-level uniformity not only provides a strong theoretical lens but also yields practical performance benefits; for example, selecting reasoning traces with more uniform information density at the step-level improves accuracy by 10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals that correct reasoning traces tend to avoid sharp information density spikes, while incorrect traces exhibit irregular information bursts. These results demonstrate that UID-inspired information density measures outperform alternative internal signals as predictors of reasoning quality. Results highlight the uniformity of the information density as a robust diagnostic and selection criterion for building more reliable and accurate reasoning systems.",
    "summary": "",
    "translation": "重新审视LLM推理轨迹中的均匀信息密度假设",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLM推理过程中的信息密度分布假设，属于纯粹的NLP理论分析范畴。虽然涉及LLM内部工作机制，但缺乏明确的推荐系统、搜索或广告应用场景，且更侧重于语言模型的理论特性而非实际应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06917v1": {
    "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.06917v1",
    "arxiv_id": "2510.06917v1",
    "authors": "Cheng-Han Chiang, Xiaofei Wang, Linjie Li, Chung-Ching Lin, Kevin Lin, Shujie Liu, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, Lijuan Wang",
    "categories": "cs.CL, eess.AS",
    "pub_date": "2025-10-08 11:48:59",
    "ori_summary": "Current large language models (LLMs) and spoken language models (SLMs) begin thinking and taking actions only after the user has finished their turn. This prevents the model from interacting during the user's turn and can lead to high response latency while it waits to think. Consequently, thinking after receiving the full input is not suitable for speech-to-speech interaction, where real-time, low-latency exchange is important. We address this by noting that humans naturally \"think while listening.\" In this paper, we propose SHANKS, a general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning while listening to the user input. SHANKS streams the input speech in fixed-duration chunks and, as soon as a chunk is received, generates unspoken reasoning based on all previous speech and reasoning, while the user continues speaking. SHANKS uses this unspoken reasoning to decide whether to interrupt the user and to make tool calls to complete the task. We demonstrate that SHANKS enhances real-time user-SLM interaction in two scenarios: (1) when the user is presenting a step-by-step solution to a math problem, SHANKS can listen, reason, and interrupt when the user makes a mistake, achieving 37.1% higher interruption accuracy than a baseline that interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can complete 56.9% of the tool calls before the user finishes their turn. Overall, SHANKS moves toward models that keep thinking throughout the conversation, not only after a turn ends. Animated illustrations of Shanks can be found at https://d223302.github.io/SHANKS/",
    "summary": "",
    "translation": "SHANKS：面向口语语言模型的同步听觉与思考机制",
    "relevance_score": 2,
    "reasoning": "该论文主要关注口语语言模型中的听觉处理与推理同步机制，属于语音处理与语言模型的交叉领域。虽然涉及语言模型技术，但其核心关注点在于口语交互和听觉处理，与推荐系统、搜索或广告领域的直接关联性较弱。该技术可能在某些语音交互场景中有潜在应用，但并非当前关注的核心领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06915v1": {
    "title": "LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling",
    "url": "https://www.alphaxiv.org/abs/2510.06915v1",
    "arxiv_id": "2510.06915v1",
    "authors": "Zecheng Tang, Baibei Ji, Quantong Qiu, Haitian Wang, Xiaobo Liang, Juntao Li, Min Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 11:48:16",
    "ori_summary": "Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasingly involve long history trajectories, e.g., LLM agent, it becomes indispensable to evaluate whether a model's responses are not only high-quality but also grounded in and consistent with the provided context. Yet, current RMs remain confined to short-context settings and primarily focus on response-level attributes (e.g., safety or helpfulness), while largely neglecting the critical dimension of long context-response consistency. In this work, we introduce Long-RewardBench, a benchmark specifically designed for long-context RM evaluation, featuring both Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that even state-of-the-art generative RMs exhibit significant fragility in long-context scenarios, failing to maintain context-aware preference judgments. Motivated by the analysis of failure patterns observed in model outputs, we propose a general multi-stage training strategy that effectively scales arbitrary models into robust Long-context RMs (LongRMs). Experiments show that our approach not only substantially improves performance on long-context evaluation but also preserves strong short-context capability. Notably, our 8B LongRM outperforms much larger 70B-scale baselines and matches the performance of the proprietary Gemini 2.5 Pro model.",
    "summary": "",
    "translation": "LongRM：揭示并突破奖励建模的上下文边界",
    "relevance_score": 6,
    "reasoning": "该论文涉及奖励建模的上下文边界扩展，这属于LLM核心技术的进展。在推荐系统和搜索领域，更长的上下文处理能力可以显著提升用户行为序列建模、长文档理解以及复杂上下文特征整合的效果，直接支持更精准的个性化推荐和搜索结果优化。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06889v1": {
    "title": "MeXtract: Light-Weight Metadata Extraction from Scientific Papers",
    "url": "https://www.alphaxiv.org/abs/2510.06889v1",
    "arxiv_id": "2510.06889v1",
    "authors": "Zaid Alyafeai, Maged S. Al-Shaibani, Bernard Ghanem",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 11:12:28",
    "ori_summary": "Metadata plays a critical role in indexing, documenting, and analyzing scientific literature, yet extracting it accurately and efficiently remains a challenging task. Traditional approaches often rely on rule-based or task-specific models, which struggle to generalize across domains and schema variations. In this paper, we present MeXtract, a family of lightweight language models designed for metadata extraction from scientific papers. The models, ranging from 0.5B to 3B parameters, are built by fine-tuning Qwen 2.5 counterparts. In their size family, MeXtract achieves state-of-the-art performance on metadata extraction on the MOLE benchmark. To further support evaluation, we extend the MOLE benchmark to incorporate model-specific metadata, providing an out-of-domain challenging subset. Our experiments show that fine-tuning on a given schema not only yields high accuracy but also transfers effectively to unseen schemas, demonstrating the robustness and adaptability of our approach. We release all the code, datasets, and models openly for the research community.",
    "summary": "",
    "translation": "MeXtract：从科学论文中轻量级提取元数据",
    "relevance_score": 2,
    "reasoning": "该论文专注于科学论文领域的元数据提取，属于特定领域的文档处理技术。虽然元数据提取在信息检索中有一般性应用，但该工作明确限定于科学论文领域，且没有明确展示与推荐系统、搜索或广告中用户行为建模、内容理解等核心问题的直接关联。其轻量级特性可能在效率方面有一般性启发，但缺乏针对RecSys/Search/Ads领域的特定应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06870v1": {
    "title": "$λ$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences",
    "url": "https://www.alphaxiv.org/abs/2510.06870v1",
    "arxiv_id": "2510.06870v1",
    "authors": "Yining Wang, Jinman Zhao, Chuangxin Zhao, Shuhao Guan, Gerald Penn, Shinan Liu",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 10:39:07",
    "ori_summary": "Reinforcement Learning with Human Feedback (RLHF) has been the dominant approach for improving the reasoning capabilities of Large Language Models (LLMs). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has simplified this paradigm by replacing the reward and value models with rule-based verifiers. A prominent example is Group Relative Policy Optimization (GRPO). However, GRPO inherently suffers from a length bias, since the same advantage is uniformly assigned to all tokens of a response. As a result, longer responses distribute the reward over more tokens and thus contribute disproportionately to gradient updates. Several variants, such as DAPO and Dr. GRPO, modify the token-level aggregation of the loss, yet these methods remain heuristic and offer limited interpretability regarding their implicit token preferences. In this work, we explore the possibility of allowing the model to learn its own token preference during optimization. We unify existing frameworks under a single formulation and introduce a learnable parameter $\\lambda$ that adaptively controls token-level weighting. We use $\\lambda$-GRPO to denote our method, and we find that $\\lambda$-GRPO achieves consistent improvements over vanilla GRPO and DAPO on multiple mathematical reasoning benchmarks. On Qwen2.5 models with 1.5B, 3B, and 7B parameters, $\\lambda$-GRPO improves average accuracy by $+1.9\\%$, $+1.0\\%$, and $+1.7\\%$ compared to GRPO, respectively. Importantly, these gains come without any modifications to the training data or additional computational cost, highlighting the effectiveness and practicality of learning token preferences.",
    "summary": "该论文研究GRPO框架中的长度偏差问题，核心思想是通过引入可学习参数λ来自适应控制token级权重，统一现有优化方法并让模型在训练中学习token偏好。",
    "translation": "λ-GRPO：通过可学习令牌偏好统一GRPO框架",
    "relevance_score": 8,
    "reasoning": "该论文涉及强化学习优化框架的改进，GRPO通常指Group Policy Optimization，在推荐系统和广告排名中有直接应用。通过引入可学习令牌偏好，该方法可以优化多目标推荐中的用户偏好建模，提升个性化推荐效果和广告投放效率。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出可学习token偏好的RL优化方法，直接改进LLM训练框架，对推荐和搜索系统的模型优化有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06866v1": {
    "title": "Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding",
    "url": "https://www.alphaxiv.org/abs/2510.06866v1",
    "arxiv_id": "2510.06866v1",
    "authors": "Wafaa Mohammed, Vlad Niculae, Chrysoula Zerva",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 10:37:17",
    "ori_summary": "Large language models (LLMs) have emerged as strong contenders in machine translation.Yet, they still struggle to adequately handle discourse phenomena, such as pronoun resolution and lexical cohesion at the document level. In this study, we thoroughly investigate the discourse phenomena performance of LLMs in context-aware translation. We demonstrate that discourse knowledge is encoded within LLMs and propose the use of quality-aware decoding (QAD) to effectively extract this knowledge, showcasing its superiority over other decoding approaches through comprehensive analysis. Furthermore, we illustrate that QAD enhances the semantic richness of translations and aligns them more closely with human preferences.",
    "summary": "",
    "translation": "通过质量感知解码解锁大语言模型中的潜在话语翻译",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM中的翻译质量改进，属于纯NLP应用领域。虽然质量感知解码技术本身可能有通用性，但论文标题明确限定在话语翻译场景，与推荐系统、搜索或广告的核心技术需求关联度极低，难以识别出在RecSys/Search/Ads中的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06847v1": {
    "title": "OpenJAI-v1.0: An Open Thai Large Language Model",
    "url": "https://www.alphaxiv.org/abs/2510.06847v1",
    "arxiv_id": "2510.06847v1",
    "authors": "Pontakorn Trakuekul, Attapol T. Rutherford, Jullajak Karnjanaekarin, Narongkorn Panitsrisit, Sumana Sumanakul",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 10:12:56",
    "ori_summary": "We introduce OpenJAI-v1.0, an open-source large language model for Thai and English, developed from the Qwen3-14B model. Our work focuses on boosting performance on practical tasks through carefully curated data across three key use cases: instruction following, long-context understanding, and tool use. Evaluation results show that OpenJAI-v1.0 improves on the capabilities of its base model and outperforms other leading open-source Thai models on a diverse suite of benchmarks, while avoiding catastrophic forgetting. OpenJAI-v1.0 is publicly released as another alternative NLP resource for the Thai AI community.",
    "summary": "",
    "translation": "OpenJAI-v1.0：一个开源的泰语大语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要介绍一个特定语言（泰语）的大语言模型，属于基础模型开发而非核心推荐系统、搜索或广告领域的进展。虽然大语言模型本身是使能技术，但该论文专注于特定语言能力，没有明确展示在推荐系统、搜索或广告中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06843v1": {
    "title": "SID: Multi-LLM Debate Driven by Self Signals",
    "url": "https://www.alphaxiv.org/abs/2510.06843v1",
    "arxiv_id": "2510.06843v1",
    "authors": "Xuhang Chen, Zhifan Song, Deyi Ji, Shuo Gao, Lanyun Zhu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 10:10:11",
    "ori_summary": "Large Language Models (LLMs) have exhibited impressive capabilities across diverse application domains. Recent work has explored Multi-LLM Agent Debate (MAD) as a way to enhance performance by enabling multiple LLMs to discuss and refine responses iteratively. Nevertheless, existing MAD methods predominantly focus on utilizing external structures, such as debate graphs, using LLM-as-a-Judge, while neglecting the application of self signals, such as token logits and attention, that arise during generation. This omission leads to redundant computation and potential performance degradation. In this paper, we shift the focus to the self signals of multi-LLM debate and introduce a Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of self-signals: model-level confidence and token-level semantic focus, to adaptively guide the debate process. Our approach enables high-confidence agents to exit early at the model level and compress the redundant debate contents based on the attention mechanism. We evaluate our method on various LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental results demonstrate that our method not only outperforms existing MAD techniques in accuracy but also reduces token consumption, highlighting the effectiveness of utilizing self signals in enhancing both the performance and efficiency of multi-agent debate systems. Our code will be available at~\\href{https://github.com/xuhang2019/SID}{\\texttt{https://github.com/xuhang2019/SID}}.",
    "summary": "",
    "translation": "SID：基于自我信号驱动的多LLM辩论",
    "relevance_score": 3,
    "reasoning": "该论文涉及多LLM辩论机制，属于LLM推理技术范畴。虽然多模型协作可能应用于推荐系统的决策融合或搜索结果的多样性优化，但论文标题未明确指向推荐、搜索或广告的具体应用场景，潜在关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06841v1": {
    "title": "GAMBIT+: A Challenge Set for Evaluating Gender Bias in Machine Translation Quality Estimation Metrics",
    "url": "https://www.alphaxiv.org/abs/2510.06841v1",
    "arxiv_id": "2510.06841v1",
    "authors": "Giorgos Filandrianos, Orfeas Menis Mastromichalakis, Wafaa Mohammed, Giuseppe Attanasio, Chrysoula Zerva",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 10:09:03",
    "ori_summary": "Gender bias in machine translation (MT) systems has been extensively documented, but bias in automatic quality estimation (QE) metrics remains comparatively underexplored. Existing studies suggest that QE metrics can also exhibit gender bias, yet most analyses are limited by small datasets, narrow occupational coverage, and restricted language variety. To address this gap, we introduce a large-scale challenge set specifically designed to probe the behavior of QE metrics when evaluating translations containing gender-ambiguous occupational terms. Building on the GAMBIT corpus of English texts with gender-ambiguous occupations, we extend coverage to three source languages that are genderless or natural-gendered, and eleven target languages with grammatical gender, resulting in 33 source-target language pairs. Each source text is paired with two target versions differing only in the grammatical gender of the occupational term(s) (masculine vs. feminine), with all dependent grammatical elements adjusted accordingly. An unbiased QE metric should assign equal or near-equal scores to both versions. The dataset's scale, breadth, and fully parallel design, where the same set of texts is aligned across all languages, enables fine-grained bias analysis by occupation and systematic comparisons across languages.",
    "summary": "",
    "translation": "GAMBIT+：用于评估机器翻译质量评估指标中性别偏见的挑战集",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器翻译质量评估中的性别偏见评估，属于公平性和偏见检测领域。这完全属于被排除的无关主题（公平性、伦理），与推荐系统、搜索或广告的核心技术进展、LLM技术或Transformer架构改进没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06826v1": {
    "title": "Mid-Training of Large Language Models: A Survey",
    "url": "https://www.alphaxiv.org/abs/2510.06826v1",
    "arxiv_id": "2510.06826v1",
    "authors": "Kaixiang Mo, Yuxin Shi, Weiwei Weng, Zhiqiang Zhou, Shuman Liu, Haibo Zhang, Anxiang Zeng",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 09:49:37",
    "ori_summary": "Large language models (LLMs) are typically developed through large-scale pre-training followed by task-specific fine-tuning. Recent advances highlight the importance of an intermediate mid-training stage, where models undergo multiple annealing-style phases that refine data quality, adapt optimization schedules, and extend context length. This stage mitigates diminishing returns from noisy tokens, stabilizes convergence, and expands model capability in late training. Its effectiveness can be explained through gradient noise scale, the information bottleneck, and curriculum learning, which together promote generalization and abstraction. Despite widespread use in state-of-the-art systems, there has been no prior survey of mid-training as a unified paradigm. We introduce the first taxonomy of LLM mid-training spanning data distribution, learning-rate scheduling, and long-context extension. We distill practical insights, compile evaluation benchmarks, and report gains to enable structured comparisons across models. We also identify open challenges and propose avenues for future research and practice.",
    "summary": "论文研究LLM训练过程中预训练与微调之间的中间阶段优化问题，核心思想是通过多阶段退火式训练策略，在数据质量优化、学习率调度和上下文扩展等方面进行系统化改进，以提升模型泛化能力和抽象能力。",
    "translation": "大型语言模型中期训练：综述",
    "relevance_score": 8,
    "reasoning": "这篇综述关注LLM训练过程中的中期训练技术，这属于'使能LLM技术'范畴。更高效的训练方法可以直接应用于构建更强大的推荐和搜索系统，通过改进模型训练效率和质量来增强下游应用性能。中期训练优化对于大规模工业级推荐和搜索系统的部署具有重要实践意义。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文系统化研究LLM训练中的中间阶段优化技术，直接关联核心LLM技术进步，对推荐系统和搜索中的模型训练具有重要指导意义。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06825v1": {
    "title": "Adaptive Tool Generation with Models as Tools and Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.06825v1",
    "arxiv_id": "2510.06825v1",
    "authors": "Chenpeng Wang, Xiaojie Cheng, Chunye Wang, Linfeng Yang, Lei Zhang",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 09:48:50",
    "ori_summary": "Tool-augmented language models have demonstrated strong capabilities, but their reliance on live API access creates scalability and reliability challenges during training and deployment. We propose MTR, a simulation-first training framework for tool-augmented reasoning. Instead of relying on live APIs, MTR learns from complete ReAct traces with schema-validated, simulated observations. Our approach operates through a multi-agent architecture where a ToolMaker generates task-specific, OpenAI-compatible tool interfaces, an AutoAgent produces structured think-act-observe sequences, and a ToolActor simulates realistic responses. Training proceeds in two stages: Stage-1 Supervised Fine-Tuning (SFT) teaches 'trace grammar' from complete reasoning sequences; Stage-2 Group Relative Policy Optimization (GRPO) optimizes strategy with a composite trace reward that balances answer correctness and internal consistency. Across four multi-hop QA benchmarks (HotpotQA, MuSiQue, 2WikiMultiHopQA, Bamboogle), MTR attains competitive Exact Match (EM) scores to live-API systems and excels on reasoning-intensive tasks, suggesting that effective tool reasoning can be learned from structured traces without live interactions.",
    "summary": "该论文研究工具增强语言模型依赖实时API带来的可扩展性和可靠性问题，核心方法是提出MTR框架，通过多智能体架构生成任务特定工具接口和模拟观察，从结构化轨迹中学习有效工具推理而无需实时交互。",
    "translation": "基于模型即工具与强化学习的自适应工具生成",
    "relevance_score": 8,
    "reasoning": "该论文涉及使用强化学习进行工具生成，这在推荐系统和搜索领域具有直接应用潜力，例如动态生成个性化推荐策略或搜索查询重写工具。模型即工具的概念可以应用于构建自适应推荐系统，根据用户上下文自动生成和优化推荐工具。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出模拟优先训练框架和多智能体架构，直接解决工具增强推理的可扩展性和可靠性问题，与LLM在推荐搜索领域的应用高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06811v1": {
    "title": "BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods",
    "url": "https://www.alphaxiv.org/abs/2510.06811v1",
    "arxiv_id": "2510.06811v1",
    "authors": "Philipp Mondorf, Mingyang Wang, Sebastian Gerstner, Ahmad Dawar Hakimi, Yihong Liu, Leonor Veloso, Shijia Zhou, Hinrich Schütze, Barbara Plank",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-08 09:39:40",
    "ori_summary": "The Circuit Localization track of the Mechanistic Interpretability Benchmark (MIB) evaluates methods for localizing circuits within large language models (LLMs), i.e., subnetworks responsible for specific task behaviors. In this work, we investigate whether ensembling two or more circuit localization methods can improve performance. We explore two variants: parallel and sequential ensembling. In parallel ensembling, we combine attribution scores assigned to each edge by different methods-e.g., by averaging or taking the minimum or maximum value. In the sequential ensemble, we use edge attribution scores obtained via EAP-IG as a warm start for a more expensive but more precise circuit identification method, namely edge pruning. We observe that both approaches yield notable gains on the benchmark metrics, leading to a more precise circuit identification approach. Finally, we find that taking a parallel ensemble over various methods, including the sequential ensemble, achieves the best results. We evaluate our approach in the BlackboxNLP 2025 MIB Shared Task, comparing ensemble scores to official baselines across multiple model-task combinations.",
    "summary": "",
    "translation": "BlackboxNLP-2025 MIB共享任务：探索电路定位方法的集成策略",
    "relevance_score": 2,
    "reasoning": "该论文专注于电路定位方法的集成策略，这属于模型可解释性/机制解释领域，与推荐系统、搜索或广告的核心技术进展没有直接关联。虽然理解Transformer内部机制可能间接有助于模型优化，但论文本身没有展示在推荐/搜索/广告中的具体应用潜力，且电路定位更偏向理论研究而非实际系统改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06800v1": {
    "title": "FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline",
    "url": "https://www.alphaxiv.org/abs/2510.06800v1",
    "arxiv_id": "2510.06800v1",
    "authors": "Haotian Wu, Shufan Jiang, Chios Chen, Yiyang Feng, Hehai Lin, Heqing Zou, Yao Shu, Yanran Li, Chengwei Qin",
    "categories": "cs.CL, cs.AI, cs.HC, cs.MA",
    "pub_date": "2025-10-08 09:30:36",
    "ori_summary": "As large language models (LLMs) advance in role-playing (RP) tasks, existing benchmarks quickly become obsolete due to their narrow scope, outdated interaction paradigms, and limited adaptability across diverse application scenarios. To address this gap, we introduce FURINA-Builder, a novel multi-agent collaboration pipeline that automatically constructs fully customizable RP benchmarks at any scale. It enables evaluation of arbitrary characters across diverse scenarios and prompt formats, as the first benchmark builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues between a test character and other characters drawn from a well-constructed character-scene pool, while an LLM judge selects fine-grained evaluation dimensions and adjusts the test character's responses into final test utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive role-playing benchmark featuring both established and synthesized test characters, each assessed with dimension-specific evaluation criteria. Human evaluation and preliminary separability analysis justify our pipeline and benchmark design. We conduct extensive evaluations of cutting-edge LLMs and find that o3 and DeepSeek-R1 achieve the best performance on English and Chinese RP tasks, respectively. Across all models, established characters consistently outperform synthesized ones, with reasoning capabilities further amplifying this disparity. Interestingly, we observe that model scale does not monotonically reduce hallucinations. More critically, for reasoning LLMs, we uncover a novel trade-off: reasoning improves RP performance but simultaneously increases RP hallucinations. This trade-off extends to a broader Pareto frontier between RP performance and reliability for all LLMs. These findings demonstrate the effectiveness of FURINA-Builder and the challenge posed by FURINA-Bench.",
    "summary": "",
    "translation": "FURINA：通过可扩展多智能体协作流程构建的完全可定制角色扮演基准",
    "relevance_score": 1,
    "reasoning": "该论文标题表明这是一个关于多智能体协作和角色扮演的基准测试系统，主要关注游戏、模拟或对话评估场景。这与我的核心关注领域（推荐系统、搜索、广告）以及相关的LLM/Transformer技术应用没有直接关联。多智能体协作基准测试在推荐、搜索或广告领域缺乏明确的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06782v1": {
    "title": "GPT-5 Model Corrected GPT-4V's Chart Reading Errors, Not Prompting",
    "url": "https://www.alphaxiv.org/abs/2510.06782v1",
    "arxiv_id": "2510.06782v1",
    "authors": "Kaichun Yang, Jian Chen",
    "categories": "cs.HC, cs.CL, cs.CV",
    "pub_date": "2025-10-08 09:09:29",
    "ori_summary": "We present a quantitative evaluation to understand the effect of zero-shot large-language model (LLMs) and prompting uses on chart reading tasks. We asked LLMs to answer 107 visualization questions to compare inference accuracies between the agentic GPT-5 and multimodal GPT-4V, for difficult image instances, where GPT-4V failed to produce correct answers. Our results show that model architecture dominates the inference accuracy: GPT5 largely improved accuracy, while prompt variants yielded only small effects. Pre-registration of this work is available here: https://osf.io/u78td/?view_only=6b075584311f48e991c39335c840ded3; the Google Drive materials are here:https://drive.google.com/file/d/1ll8WWZDf7cCNcfNWrLViWt8GwDNSvVrp/view.",
    "summary": "",
    "translation": "GPT-5模型纠正了GPT-4V的图表读取错误，而非通过提示工程",
    "relevance_score": 3,
    "reasoning": "该论文主要关注多模态模型的错误纠正能力，属于VLM范畴，但与推荐系统、搜索或广告的直接应用关联较弱。虽然涉及模型迭代改进，但缺乏明确的RecSys/Search/Ads应用场景，仅作为通用能力提升，潜在应用不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06780v1": {
    "title": "Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness",
    "url": "https://www.alphaxiv.org/abs/2510.06780v1",
    "arxiv_id": "2510.06780v1",
    "authors": "Luca Giordano, Simon Razniewski",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 09:03:58",
    "ori_summary": "Large Language Models (LLMs) encode substantial factual knowledge, yet measuring and systematizing this knowledge remains challenging. Converting it into structured format, for example through recursive extraction approaches such as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key open questions include whether such extraction can terminate, whether its outputs are reproducible, and how robust they are to variations. We systematically study LLM knowledge materialization using miniGPTKBs (domain-specific, tractable subcrawls), analyzing termination, reproducibility, and robustness across three categories of metrics: yield, lexical similarity, and semantic similarity. We experiment with four variations (seed, language, randomness, model) and three illustrative domains (from history, entertainment, and finance). Our findings show (i) high termination rates, though model-dependent; (ii) mixed reproducibility; and (iii) robustness that varies by perturbation type: high for seeds and temperature, lower for languages and models. These results suggest that LLM knowledge materialization can reliably surface core knowledge, while also revealing important limitations.",
    "summary": "",
    "translation": "大语言模型知识物化基础：终止性、可复现性与鲁棒性",
    "relevance_score": 6,
    "reasoning": "该论文聚焦LLM知识物化的基础理论问题，属于'使能LLM技术'范畴。虽然不直接涉及推荐系统或搜索应用，但知识物化的终止性、可复现性和鲁棒性对于构建可靠的LLM增强推荐/搜索系统至关重要，可确保模型在工业场景下的稳定性和一致性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06774v1": {
    "title": "Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition",
    "url": "https://www.alphaxiv.org/abs/2510.06774v1",
    "arxiv_id": "2510.06774v1",
    "authors": "Lei Xu, Pierre Beckmann, Marco Valentino, André Freitas",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 08:57:16",
    "ori_summary": "Neuro-symbolic NLP methods aim to leverage the complementary strengths of large language models and formal logical solvers. However, current approaches are mostly static in nature, i.e., the integration of a target solver is predetermined at design time, hindering the ability to employ diverse formal inference strategies. To address this, we introduce an adaptive, multi-paradigm, neuro-symbolic inference framework that: (1) automatically identifies formal reasoning strategies from problems expressed in natural language; and (2) dynamically selects and applies specialized formal logical solvers via autoformalization interfaces. Extensive experiments on individual and multi-paradigm reasoning tasks support the following conclusions: LLMs are effective at predicting the necessary formal reasoning strategies with an accuracy above 90 percent. This enables flexible integration with formal logical solvers, resulting in our framework outperforming competing baselines by 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively. Moreover, adaptive reasoning can even positively impact pure LLM methods, yielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT settings with GPT-4o. Finally, although smaller models struggle with adaptive neuro-symbolic reasoning, post-training offers a viable path to improvement. Overall, this work establishes the foundations for adaptive LLM-symbolic reasoning, offering a path forward for unifying material and formal inferences on heterogeneous reasoning challenges.",
    "summary": "论文研究如何动态整合大语言模型与形式逻辑求解器，核心思想是通过自动识别自然语言问题中的推理策略，动态选择和组合专门的形式逻辑求解器来实现自适应神经符号推理。",
    "translation": "基于动态逻辑求解器组合的自适应大语言模型符号推理",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能LLM技术'范畴，专注于增强LLM的符号推理能力。动态逻辑求解器组合技术可应用于搜索和推荐系统中的复杂查询理解、多约束条件推理以及用户意图的精确解析，这对于提升搜索相关性和推荐准确性具有重要价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的动态逻辑求解器组合框架直接属于LLM符号推理的核心技术，对推荐系统中复杂推理任务具有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06761v1": {
    "title": "Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration",
    "url": "https://www.alphaxiv.org/abs/2510.06761v1",
    "arxiv_id": "2510.06761v1",
    "authors": "Zhi Zhang, Yan Liu, Zhejing Hu, Gong Chen, Sheng-hua Zhong, Jiannong Cao",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-08 08:40:58",
    "ori_summary": "Automating the end-to-end scientific research process poses a fundamental challenge: it requires both evolving high-level plans that are novel and sound, and executing these plans correctly amidst dynamic and uncertain conditions. To address this bilevel challenge, we propose a novel Double-Loop Multi-Agent (DLMA) framework to solve the given research problem automatically. The leader loop, composed of professor agents, is responsible for evolving research plans. It employs an evolutionary algorithm through involvement, improvement, and integration meetings to iteratively generate and refine a pool of research proposals, exploring the solution space effectively. The follower loop, composed of doctoral student agents, is responsible for executing the best-evolved plan. It dynamically adjusts the plan during implementation via pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is well-supported by contextual and external observations. Extensive experiments on benchmarks like ACLAward and Laboratory show that DLMA generates research papers that achieve state-of-the-art scores in automated evaluation, significantly outperforming strong baselines. Ablation studies confirm the critical roles of both loops, with evolution driving novelty and execution ensuring soundness.",
    "summary": "",
    "translation": "通过双循环多智能体协作演进与执行研究计划",
    "relevance_score": 3,
    "reasoning": "该论文主要关注多智能体协作的研究计划管理，属于通用AI系统架构范畴。虽然多智能体系统在推荐和搜索中有潜在应用（如多智能体协同决策），但论文标题未明确指向推荐系统、搜索或广告领域的特定技术需求，也未涉及LLM、Transformer架构或异构数据建模等核心关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06750v1": {
    "title": "Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.06750v1",
    "arxiv_id": "2510.06750v1",
    "authors": "Jaeseong Lee, Dayoung Kwon, seung-won hwang",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 08:17:57",
    "ori_summary": "Large Reasoning Models (LRMs) excel in structured tasks by emulating deliberate human reasoning but often suffer from overthinking, degrading performance and wasting resources. One possible baseline is to deploy both LLM and LRM, then route input by predicting whether it requires reasoning and may cause overthinking. However, deploying multiple models can be costly or impractical. We propose a superposed deployment strategy with a lightweight, training-free regulation to optimize inference by switching one model on and off. Instead of routing, we selectively unlearn from LRM at inference, scaling down computation while preserving reasoning. By analyzing the cumulative energy of singular values, we identify optimal low-rank projections to adjust reasoning just right.",
    "summary": "该论文研究大型推理模型因过度思考导致的性能下降和资源浪费问题，核心方法是基于奇异值累积能量分析，通过低秩投影在推理时选择性遗忘，实现慢速与快速思维模型的叠加部署。",
    "translation": "Gold-Switch：免训练叠加慢思考与快思考大语言模型",
    "relevance_score": 8,
    "reasoning": "该论文提出的免训练叠加慢思考与快思考LLM方法属于使能LLM技术范畴，通过组合不同推理速度的模型来提高效率。这种方法在推荐系统和搜索中有直接应用潜力，可以动态平衡精度与延迟，例如在用户查询时快速返回初步结果，同时后台进行更深入的推理分析。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出无需训练的动态推理调控方法，通过奇异值分析实现模型计算效率优化，直接适用于推荐和搜索系统的大规模部署场景。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06749v1": {
    "title": "A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction",
    "url": "https://www.alphaxiv.org/abs/2510.06749v1",
    "arxiv_id": "2510.06749v1",
    "authors": "Eitan Klinger, Zihao Huang, Tran Minh Nguyen, Emma Jayeon Park, Yige Chen, Yang Gu, Qingyu Gao, Siliang Liu, Mengyang Qiu, Jungyeul Park",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 08:15:44",
    "ori_summary": "Evaluating grammatical error correction requires metrics that reflect the diversity of valid human corrections rather than privileging a single reference. Existing frameworks, largely edit-based and English-centric, rely on rigid alignments between system and reference edits, limiting their applicability in multilingual and generative settings. This paper introduces a formal framework for \\textit{fluency-based multi-reference evaluation}, framing $n$-gram similarity as an aggregation problem over multiple legitimate corrections. Within this formulation, we instantiate GLEU through four aggregation strategies--\\textsc{select-best}, \\textsc{simple-average}, \\textsc{weighted-average}, and \\textsc{merged-counts}--and analyze their properties of boundedness, monotonicity, and sensitivity to reference variation. Empirical results on Czech, Estonian, Ukrainian, and Chinese corpora show that these strategies capture complementary aspects of fluency and coverage. The framework unifies multi-reference evaluation into a principled, fluency-oriented approach that incorporates linguistic diversity without penalizing legitimate variation.",
    "summary": "",
    "translation": "基于流畅度的多参考评估在语法错误纠正中的形式化框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于语法错误纠正的评估方法，属于纯粹的NLP评估基准主题。虽然LLM技术可能用于语法纠正，但论文本身关注的是评估框架而非核心的推荐系统、搜索或广告应用，也没有涉及Transformer架构改进或异构数据建模等使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06747v1": {
    "title": "TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.06747v1",
    "arxiv_id": "2510.06747v1",
    "authors": "I-Fan Lin, Faegheh Hasibi, Suzan Verberne",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 08:05:39",
    "ori_summary": "In this paper, we propose a training-free and label-free method for short text clustering that can be used on top of any existing embedder. In the context of customer-facing chatbots, companies are dealing with large amounts of user utterances that need to be clustered according to their intent. In these commercial settings, no labeled data is typically available, and the number of clusters is not known. Our method is based on iterative vector updating: it constructs sparse vectors based on representative texts, and then iteratively refines them through LLM guidance. Our method achieves comparable or superior results to state-of-the-art methods that use contrastive learning, but without assuming prior knowledge of clusters or labels. Experiments on diverse datasets and smaller LLMs show that our method is model agnostic and can be applied to any embedder, with relatively small LLMs, and different clustering methods. We also show that our method scales to large datasets, reducing the computational cost of the LLM. These low-resource, adaptable settings and the scalability of our method make it more aligned with real-world scenarios than existing clustering methods.",
    "summary": "",
    "translation": "TWIST：基于大语言模型通过迭代向量更新的免训练免标签短文本聚类方法",
    "relevance_score": 6,
    "reasoning": "该论文提出了一种免训练免标签的短文本聚类方法，属于直接应用LLM技术进行文本处理。在搜索和推荐系统中，短文本聚类可用于用户查询理解、内容分类和用户兴趣挖掘等任务，具有直接的应用价值。虽然该方法不涉及推荐系统的核心排序或匹配算法，但作为文本预处理和特征提取技术，能够增强系统的语义理解能力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06743v1": {
    "title": "Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities",
    "url": "https://www.alphaxiv.org/abs/2510.06743v1",
    "arxiv_id": "2510.06743v1",
    "authors": "Maria Levchenko",
    "categories": "cs.CV, cs.AI, cs.CL, 68T50",
    "pub_date": "2025-10-08 08:01:40",
    "ori_summary": "Digital humanities scholars increasingly use Large Language Models for historical document digitization, yet lack appropriate evaluation frameworks for LLM-based OCR. Traditional metrics fail to capture temporal biases and period-specific errors crucial for historical corpus creation. We present an evaluation methodology for LLM-based historical OCR, addressing contamination risks and systematic biases in diplomatic transcription. Using 18th-century Russian Civil font texts, we introduce novel metrics including Historical Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside protocols for contamination control and stability testing. We evaluate 12 multimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR while exhibiting over-historicization: inserting archaic characters from incorrect historical periods. Post-OCR correction degrades rather than improves performance. Our methodology provides digital humanities practitioners with guidelines for model selection and quality assessment in historical corpus digitization.",
    "summary": "",
    "translation": "评估大语言模型在历史文档光学字符识别中的应用：数字人文领域的方法论框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于历史文档OCR在数字人文领域的应用，属于特定领域应用而非核心推荐系统、搜索或广告技术。虽然涉及LLM评估，但应用场景（历史文档、数字人文）与我的关注领域完全无关，且不涉及任何推荐、搜索或广告相关的技术或潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06738v1": {
    "title": "AWM: Accurate Weight-Matrix Fingerprint for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.06738v1",
    "arxiv_id": "2510.06738v1",
    "authors": "Boyi Zeng, Lin Chen, Ziwei He, Xinbing Wang, Zhouhan Lin",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 07:51:11",
    "ori_summary": "Protecting the intellectual property of large language models (LLMs) is crucial, given the substantial resources required for their training. Consequently, there is an urgent need for both model owners and third parties to determine whether a suspect LLM is trained from scratch or derived from an existing base model. However, the intensive post-training processes that models typically undergo-such as supervised fine-tuning, extensive continued pretraining, reinforcement learning, multi-modal extension, pruning, and upcycling-pose significant challenges to reliable identification. In this work, we propose a training-free fingerprinting method based on weight matrices. We leverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel Alignment (CKA) similarity to neutralize the effects of parameter manipulations, yielding a highly robust and high-fidelity similarity metric. On a comprehensive testbed of 60 positive and 90 negative model pairs, our method demonstrates exceptional robustness against all six aforementioned post-training categories while exhibiting a near-zero risk of false positives. By achieving perfect scores on all classification metrics, our approach establishes a strong basis for reliable model lineage verification. Moreover, the entire computation completes within 30s on an NVIDIA 3090 GPU. The code is available at https://github.com/LUMIA-Group/AWM.",
    "summary": "",
    "translation": "AWM：面向大语言模型的精确权重矩阵指纹方法",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及'指纹'技术，这属于明确排除的'Fingerprint'相关主题。权重矩阵指纹主要用于模型识别、安全验证或版权保护，与推荐系统、搜索或广告的核心技术进展没有直接关联，也不属于LLM架构效率、Transformer改进或异质数据建模等关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06730v1": {
    "title": "PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.06730v1",
    "arxiv_id": "2510.06730v1",
    "authors": "Manuel Frank, Haithem Afli",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 07:37:19",
    "ori_summary": "Current evaluations of sentence embedding models typically rely on static test beds such as the Massive Text Embedding Benchmark (MTEB). While invaluable, repeated tuning on a fixed suite can inflate reported performance and obscure real-world robustness. We introduce the Paraphrasing Text Embedding Benchmark (PTEB), a dynamic protocol that stochastically generates meaning-preserving paraphrases at evaluation time and aggregates results across multiple runs. Using a cost-efficient LLM-based method grounded in semantic textual similarity gold ratings, we show that LLMs generate token-diverse but semantically preserving, paraphrases. Across 7 MTEB tasks, we validate our hypothesis that the performance of sentence encoders is sensitive to changes in token space even when semantics remain fixed. We also observe that smaller models are not disproportionately affected relative to larger ones. Our results are statistically robust over multiple runs and we extended our experiments to 3 multilingual datasets covering 10 languages. More generally, we aim to propose a new evaluation paradigm in NLP that relies less on static, pre-defined benchmarks but shifts towards dynamic, stochastic evaluation leveraging eval-time compute.",
    "summary": "",
    "translation": "PTEB：通过LLM在评估时进行随机释义实现鲁棒文本嵌入评估",
    "relevance_score": 2,
    "reasoning": "该论文专注于文本嵌入评估方法，属于纯NLP评估基准范畴，与推荐系统、搜索或广告的核心技术进展无关。虽然提到了LLMs，但仅作为生成释义的工具，没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06727v1": {
    "title": "Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management",
    "url": "https://www.alphaxiv.org/abs/2510.06727v1",
    "arxiv_id": "2510.06727v1",
    "authors": "Miao Lu, Weiwei Sun, Weihua Du, Zhan Ling, Xuesong Yao, Kang Liu, Jiecao Chen",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-08 07:29:22",
    "ori_summary": "We study reinforcement learning (RL) fine-tuning of large language model (LLM) agents for long-horizon multi-turn tool use, where context length quickly becomes a fundamental bottleneck. Existing RL pipelines can suffer from degraded instruction following, excessive rollout costs, and most importantly, strict context limits. To address these challenges, we introduce summarization-based context management to training. In specific, it periodically compresses the tool using history by LLM-generated summaries that retain task-relevant information to keep a compact context while enabling the agent to scale beyond the fixed context window. Building on this formulation, we derive a policy gradient representation that seamlessly enables standard LLM RL infrastructures to optimize both tool-use behaviors as well as summarization strategies in an end-to-end fashion. We instantiate this framework with \\underline{SU}mmarization augmented \\underline{P}olicy \\underline{O}ptimization (\\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond a fixed context limit. Experiments on interactive function calling and searching tasks demonstrate that \\texttt{SUPO} significantly improves the success rate while maintaining the same or even lower working context length compared to baselines. We also demonstrate that for complex searching tasks, \\texttt{SUPO} can further improve the evaluation performance when scaling test-time maximum round of summarization beyond that of training time. Our results establish summarization-based context management as a principled and scalable approach for training RL agents beyond a fixed context length limit.",
    "summary": "论文研究LLM在多轮工具使用中的上下文长度瓶颈问题，核心方法是引入摘要式上下文管理，通过LLM生成的任务相关摘要压缩历史信息，实现端到端联合优化工具使用行为和摘要策略。",
    "translation": "基于端到端摘要化上下文管理的可扩展LLM多轮强化学习",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM多轮强化学习的高效扩展技术，这属于'Enabling LLM Tech'范畴，通过端到端摘要化上下文管理提升RL效率。在推荐系统和搜索场景中，这种技术可显著改善多轮对话推荐、会话搜索的上下文管理和长期用户交互优化，实现更高效的多轮决策过程。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM在推荐/搜索系统多轮交互中的核心瓶颈——上下文长度限制，提出端到端的摘要式上下文管理框架，与多轮推荐和搜索场景高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06719v1": {
    "title": "Differentially Private Synthetic Text Generation for Retrieval-Augmented Generation (RAG)",
    "url": "https://www.alphaxiv.org/abs/2510.06719v1",
    "arxiv_id": "2510.06719v1",
    "authors": "Junki Mori, Kazuya Kakizaki, Taiki Miyagawa, Jun Sakuma",
    "categories": "cs.CR, cs.CL, cs.LG",
    "pub_date": "2025-10-08 07:15:50",
    "ori_summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by grounding them in external knowledge. However, its application in sensitive domains is limited by privacy risks. Existing private RAG methods typically rely on query-time differential privacy (DP), which requires repeated noise injection and leads to accumulated privacy loss. To address this issue, we propose DP-SynRAG, a framework that uses LLMs to generate differentially private synthetic RAG databases. Unlike prior methods, the synthetic text can be reused once created, thereby avoiding repeated noise injection and additional privacy costs. To preserve essential information for downstream RAG tasks, DP-SynRAG extends private prediction, which instructs LLMs to generate text that mimics subsampled database records in a DP manner. Experiments show that DP-SynRAG achieves superior performanec to the state-of-the-art private RAG systems while maintaining a fixed privacy budget, offering a scalable solution for privacy-preserving RAG.",
    "summary": "",
    "translation": "面向检索增强生成（RAG）的差分隐私合成文本生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注差分隐私技术，这属于隐私保护范畴，被明确列为不相关主题。虽然RAG技术在搜索系统中可能有应用，但论文的核心焦点是隐私保护而非核心推荐/搜索系统进展或LLM技术本身。差分隐私技术本身没有直接的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06706v1": {
    "title": "XLSR-Kanformer: A KAN-Intergrated model for Synthetic Speech Detection",
    "url": "https://www.alphaxiv.org/abs/2510.06706v1",
    "arxiv_id": "2510.06706v1",
    "authors": "Phuong Tuan Dat, Tran Huy Dat",
    "categories": "cs.SD, cs.CL, eess.AS",
    "pub_date": "2025-10-08 06:58:58",
    "ori_summary": "Recent advancements in speech synthesis technologies have led to increasingly sophisticated spoofing attacks, posing significant challenges for automatic speaker verification systems. While systems based on self-supervised learning (SSL) models, particularly the XLSR-Conformer architecture, have demonstrated remarkable performance in synthetic speech detection, there remains room for architectural improvements. In this paper, we propose a novel approach that replaces the traditional Multi-Layer Perceptron (MLP) in the XLSR-Conformer model with a Kolmogorov-Arnold Network (KAN), a powerful universal approximator based on the Kolmogorov-Arnold representation theorem. Our experimental results on ASVspoof2021 demonstrate that the integration of KAN to XLSR-Conformer model can improve the performance by 60.55% relatively in Equal Error Rate (EER) LA and DF sets, further achieving 0.70% EER on the 21LA set. Besides, the proposed replacement is also robust to various SSL architectures. These findings suggest that incorporating KAN into SSL-based models is a promising direction for advances in synthetic speech detection.",
    "summary": "",
    "translation": "XLSR-Kanformer：一种集成KAN的合成语音检测模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于合成语音检测，属于语音处理领域，与推荐系统、搜索或广告没有直接关联。KAN架构虽然是一种新的神经网络方法，但论文的应用场景（语音检测）在指定的无关主题范围内，没有展示在RecSys/Search/Ads中的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06700v1": {
    "title": "How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects",
    "url": "https://www.alphaxiv.org/abs/2510.06700v1",
    "arxiv_id": "2510.06700v1",
    "authors": "Leonardo Bertolazzi, Sandro Pezzelle, Raffaelle Bernardi",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 06:48:08",
    "ori_summary": "Both humans and large language models (LLMs) exhibit content effects: biases in which the plausibility of the semantic content of a reasoning problem influences judgments regarding its logical validity. While this phenomenon in humans is best explained by the dual-process theory of reasoning, the mechanisms behind content effects in LLMs remain unclear. In this work, we address this issue by investigating how LLMs encode the concepts of validity and plausibility within their internal representations. We show that both concepts are linearly represented and strongly aligned in representational geometry, leading models to conflate plausibility with validity. Using steering vectors, we demonstrate that plausibility vectors can causally bias validity judgements, and vice versa, and that the degree of alignment between these two concepts predicts the magnitude of behavioral content effects across models. Finally, we construct debiasing vectors that disentangle these concepts, reducing content effects and improving reasoning accuracy. Our findings advance understanding of how abstract logical concepts are represented in LLMs and highlight representational interventions as a path toward more logical systems.",
    "summary": "",
    "translation": "语言模型如何混淆逻辑有效性与合理性：内容效应的表征分析",
    "relevance_score": 2,
    "reasoning": "该论文主要分析语言模型在逻辑推理中的混淆问题，属于LLM评估和认知偏差研究范畴。虽然涉及LLM表征分析，但焦点是逻辑推理而非推荐/搜索/广告应用，且内容效应分析更偏向理论认知研究，与当前关注的推荐系统、搜索广告技术进展关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06695v1": {
    "title": "Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks",
    "url": "https://www.alphaxiv.org/abs/2510.06695v1",
    "arxiv_id": "2510.06695v1",
    "authors": "Qinhao Zhou, Xiang Xiang, Kun He, John E. Hopcroft",
    "categories": "cs.CL, cs.AI, cs.LG, eess.AS",
    "pub_date": "2025-10-08 06:40:06",
    "ori_summary": "In recent years, the growing interest in Large Language Models (LLMs) has significantly advanced prompt engineering, transitioning from manual design to model-based optimization. Prompts for LLMs generally comprise two components: the \\textit{instruction}, which defines the task or objective, and the \\textit{input}, which is tailored to the instruction type. In natural language generation (NLG) tasks such as machine translation, the \\textit{input} component is particularly critical, while the \\textit{instruction} component tends to be concise. Existing prompt engineering methods primarily focus on optimizing the \\textit{instruction} component for general tasks, often requiring large-parameter LLMs as auxiliary tools. However, these approaches exhibit limited applicability for tasks like machine translation, where the \\textit{input} component plays a more pivotal role. To address this limitation, this paper introduces a novel prompt optimization method specifically designed for machine translation tasks. The proposed approach employs a small-parameter model trained using a back-translation-based strategy, significantly reducing training overhead for single-task optimization while delivering highly effective performance. With certain adaptations, this method can also be extended to other downstream tasks.",
    "summary": "论文研究机器翻译等下游任务中LLM提示优化问题，核心思想是使用基于反向翻译策略训练的小参数模型专门优化提示中的输入组件，而非传统的大模型指令优化方法。",
    "translation": "学习重写提示以引导LLM在下游任务上进行自举学习",
    "relevance_score": 8,
    "reasoning": "该论文涉及提示工程和LLM自举技术，这属于'直接LLM应用'范畴，对搜索和推荐系统有直接应用价值。通过优化提示重写，可以显著提升LLM在推荐、搜索排序等下游任务中的性能表现，减少对大量标注数据的依赖。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出的小参数模型提示重写方法可直接应用于搜索和推荐系统的查询优化，其输入组件优化思路与推荐系统的用户输入处理高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06677v1": {
    "title": "Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback",
    "url": "https://www.alphaxiv.org/abs/2510.06677v1",
    "arxiv_id": "2510.06677v1",
    "authors": "Yisha Wu, Cen, Zhao, Yuanpei Cao, Xiaoqing Su, Yashar Mehdad, Mindy Ji, Claire Na Cheng",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-08 06:05:58",
    "ori_summary": "We introduce an incremental summarization system for customer support agents that intelligently determines when to generate concise bullet notes during conversations, reducing agents' context-switching effort and redundant review. Our approach combines a fine-tuned Mixtral-8x7B model for continuous note generation with a DeBERTa-based classifier to filter trivial content. Agent edits refine the online notes generation and regularly inform offline model retraining, closing the agent edits feedback loop. Deployed in production, our system achieved a 3% reduction in case handling time compared to bulk summarization (with reductions of up to 9% in highly complex cases), alongside high agent satisfaction ratings from surveys. These results demonstrate that incremental summarization with continuous feedback effectively enhances summary quality and agent productivity at scale.",
    "summary": "",
    "translation": "基于渐进式笔记记录与客服反馈的客户支持增量式摘要生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注客户支持场景下的增量式摘要生成，属于纯粹的文本摘要应用，与推荐系统、搜索或广告的核心技术进展无关。虽然涉及用户交互（客服反馈），但这属于特定领域的对话系统应用，而非RecSys/Search/Ads领域的核心技术或使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06670v1": {
    "title": "PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch",
    "url": "https://www.alphaxiv.org/abs/2510.06670v1",
    "arxiv_id": "2510.06670v1",
    "authors": "Shangjian Yin, Shining Liang, Wenbiao Ding, Yuli Qian, Zhouxing Shi, Hongzhi Li, Yutao Xie",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 05:47:37",
    "ori_summary": "Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone for aligning large language models (LLMs). However, its effectiveness depends on high-quality instruction data. Most existing alignment datasets are either private or require costly human annotation, which limits reproducibility and scalability. Even with Reinforcement Learning from AI Feedback (RLAIF), concerns about data quality remain. Moreover, it is unclear how much data is actually required to fine-tune a base model into a strong instruction-following model. Current approaches often rely on over 300k examples even at the supervised fine-tuning (SFT) stage, yet they still underperform compared to proprietary models, creating barriers for academic and resource-limited communities. To address this gap, we introduce PiKa, a data-efficient family of expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only 30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets, we show that PiKa-SFT outperforms models trained on much larger data. On AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses the official Llama-3-8B-Instruct model trained on over 10 million proprietary examples. We further extend our study by training the Qwen2.5 series (0.5B to 7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that high-quality alignment can be achieved with significantly less data, offering a scalable path for open-source LLM alignment. Code and data: https://github.com/SJY8460/PiKa.",
    "summary": "论文研究如何解决LLM对齐过程中对大规模高质量指令数据的依赖问题，核心思想是通过构建专家级合成数据集，用远少于现有方法的数据量实现高质量模型对齐。",
    "translation": "PIKA：从零开始用于后训练对齐的专家级合成数据集",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能LLM技术'范畴，专注于通过合成数据实现后训练对齐，这是LLM发展的核心进展。在推荐系统、搜索和广告领域，合成数据集可用于对齐特定领域的LLM模型，提高其在商业场景中的安全性和性能表现，避免有害输出并优化用户体验。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文专注于LLM对齐的高质量数据集生成，虽然不直接应用于推荐系统，但其数据高效方法对需要高质量训练数据的推荐和搜索模型有重要参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06664v1": {
    "title": "ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory",
    "url": "https://www.alphaxiv.org/abs/2510.06664v1",
    "arxiv_id": "2510.06664v1",
    "authors": "Yunzhong Xiao, Yangmin Li, Hewei Wang, Yunlong Tang, Zora Zhiruo Wang",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 05:32:31",
    "ori_summary": "Agents utilizing tools powered by large language models (LLMs) or vision-language models (VLMs) have demonstrated remarkable progress in diverse tasks across text and visual modalities. Unlike traditional tools such as calculators, which give deterministic outputs, neural tools perform uncertainly across task scenarios. While different tools for a task may excel in varied scenarios, existing agents typically rely on fixed tools, thus limiting the flexibility in selecting the most suitable tool for specific tasks. In contrast, humans snowball their understanding of the capabilities of different tools by interacting with them, and apply this knowledge to select the optimal tool when solving a future task. To build agents that similarly benefit from this process, we propose ToolMem that enables agents to develop memories of tool capabilities from previous interactions, by summarizing their strengths and weaknesses and storing them in memory; at inference, the agent can retrieve relevant entries from ToolMem, and select the best tool to solve individual tasks more accurately. We evaluate ToolMem on learning varied text generation and text-to-image generation neural tools. Compared to no-memory, generic agents, we find ToolMem-augmented agents predict tool performance 14.8% and 28.7% more accurately across text and multimodal generation scenarios. Moreover, ToolMem facilitates optimal tool selection among multiple choices by 21% and 24% absolute increases in respective scenarios.",
    "summary": "",
    "translation": "ToolMem：通过可学习的工具能力记忆增强多模态智能体",
    "relevance_score": 6,
    "reasoning": "该论文涉及多模态智能体和工具学习，属于使能LLM技术范畴。在推荐系统或搜索中，这种工具能力记忆机制可以用于构建更智能的对话推荐系统，让智能体记住并有效利用各种推荐工具（如用户画像分析、物品匹配、上下文理解等），提升推荐交互的质量和效率。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06652v1": {
    "title": "Aligning Large Language Models via Fully Self-Synthetic Data",
    "url": "https://www.alphaxiv.org/abs/2510.06652v1",
    "arxiv_id": "2510.06652v1",
    "authors": "Shangjian Yin, Zhepei Wei, Xinyu Zhu, Wei-Lin Chen, Yu Meng",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 05:07:45",
    "ori_summary": "Traditional reinforcement learning from human feedback (RLHF) for large language models (LLMs) relies on expensive human-annotated datasets, while Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs, requiring the collection of diverse prompts and corresponding responses, often necessitating external reward models or proprietary models like GPT-4 to annotate preference pairs. In this work, we introduce Self-Alignment Optimization (SAO), a fully self-synthetic framework for LLM alignment, where all training data, including prompts (i.e., user queries), responses, and preferences, are generated by the model itself. Specifically, SAO first instructs the LLM to engage in persona role-play and generate diverse prompts and responses, which are then self-evaluated for preference optimization. Extensive experiments demonstrate that SAO effectively enhances the model's chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining strong performance on downstream objective tasks (e.g., question-answering, math reasoning). Our work provides a practical solution for self-improvement in aligning LLMs, and the code for reproducing our results is available at: https://github.com/SJY8460/SAO.",
    "summary": "",
    "translation": "通过完全自合成数据对齐大型语言模型",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能LLM技术'范畴，专注于LLM对齐这一核心挑战。在推荐系统、搜索和广告中，对齐技术可以显著提升LLM在理解用户意图、生成相关内容和遵循商业约束方面的能力，从而改善用户体验和商业效果。完全自合成数据的方法为大规模部署提供了成本效益高的解决方案。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06640v1": {
    "title": "A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures",
    "url": "https://www.alphaxiv.org/abs/2510.06640v1",
    "arxiv_id": "2510.06640v1",
    "authors": "Nhat M. Hoang, Do Xuan Long, Cong-Duy Nguyen, Min-Yen Kan, Luu Anh Tuan",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-08 04:46:11",
    "ori_summary": "State Space Models (SSMs) have recently emerged as efficient alternatives to Transformer-Based Models (TBMs) for long-sequence processing, offering linear scaling and lower memory use. Yet, how contextual information flows across layers and tokens in these architectures remains understudied. We present the first unified, token- and layer-level analysis of representation propagation in SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing, we characterize how representations evolve within and across layers. We find a key divergence: TBMs rapidly homogenize token representations, with diversity reemerging only in later layers, while SSMs preserve token uniqueness early but converge to homogenization deeper. Theoretical analysis and parameter randomization further reveal that oversmoothing in TBMs stems from architectural design, whereas in SSMs it arises mainly from training dynamics. These insights clarify the inductive biases of both architectures and inform future model and training designs for long-context reasoning.",
    "summary": "论文研究状态空间模型和Transformer架构中上下文表示传播的核心差异问题，核心发现是Transformer通过架构设计导致早期表示同质化而后期重新分化，状态空间模型则呈现相反的传播模式。",
    "translation": "状态空间与Transformer架构中上下文表示流的对比分析",
    "relevance_score": 8,
    "reasoning": "该论文直接比较状态空间模型（如Mamba）与Transformer架构的表示流特性，属于Transformer架构效率研究的核心范畴。这种对比分析对于开发更高效的序列建模架构具有重要价值，可应用于推荐系统中的长序列用户行为建模和搜索中的长文本理解，提升系统效率同时保持性能。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文深入分析Transformer架构的表示传播机制，直接关联Transformer技术进展，对理解模型内部工作方式具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06605v1": {
    "title": "Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation",
    "url": "https://www.alphaxiv.org/abs/2510.06605v1",
    "arxiv_id": "2510.06605v1",
    "authors": "Shuo Shao, Yiming Li, Hongwei Yao, Yifei Chen, Yuchen Yang, Zhan Qin",
    "categories": "cs.CR, cs.AI, cs.CL",
    "pub_date": "2025-10-08 03:27:38",
    "ori_summary": "The substantial investment required to develop Large Language Models (LLMs) makes them valuable intellectual property, raising significant concerns about copyright protection. LLM fingerprinting has emerged as a key technique to address this, which aims to verify a model's origin by extracting an intrinsic, unique signature (a \"fingerprint\") and comparing it to that of a source model to identify illicit copies. However, existing black-box fingerprinting methods often fail to generate distinctive LLM fingerprints. This ineffectiveness arises because black-box methods typically rely on model outputs, which lose critical information about the model's unique parameters due to the usage of non-linear functions. To address this, we first leverage Fisher Information Theory to formally demonstrate that the gradient of the model's input is a more informative feature for fingerprinting than the output. Based on this insight, we propose ZeroPrint, a novel method that approximates these information-rich gradients in a black-box setting using zeroth-order estimation. ZeroPrint overcomes the challenge of applying this to discrete text by simulating input perturbations via semantic-preserving word substitutions. This operation allows ZeroPrint to estimate the model's Jacobian matrix as a unique fingerprint. Experiments on the standard benchmark show ZeroPrint achieves a state-of-the-art effectiveness and robustness, significantly outperforming existing black-box methods.",
    "summary": "",
    "translation": "字里行间：基于零阶梯度估计实现可靠的黑盒大语言模型指纹识别",
    "relevance_score": 1,
    "reasoning": "该论文涉及指纹识别技术，这属于明确列出的无关主题范畴。虽然提到了LLM技术，但核心焦点是模型识别和指纹方法，与推荐系统、搜索或广告的核心技术进展没有直接关联。零阶梯度估计技术本身可能在其他领域有应用，但在此上下文中主要用于指纹识别目的。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06594v1": {
    "title": "Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?",
    "url": "https://www.alphaxiv.org/abs/2510.06594v1",
    "arxiv_id": "2510.06594v1",
    "authors": "Sri Durga Sai Sowmya Kadali, Evangelos E. Papalexakis",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 02:55:31",
    "ori_summary": "Jailbreaking large language models (LLMs) has emerged as a pressing concern with the increasing prevalence and accessibility of conversational LLMs. Adversarial users often exploit these models through carefully engineered prompts to elicit restricted or sensitive outputs, a strategy widely referred to as jailbreaking. While numerous defense mechanisms have been proposed, attackers continuously develop novel prompting techniques, and no existing model can be considered fully resistant. In this study, we investigate the jailbreak phenomenon by examining the internal representations of LLMs, with a focus on how hidden layers respond to jailbreak versus benign prompts. Specifically, we analyze the open-source LLM GPT-J and the state-space model Mamba2, presenting preliminary findings that highlight distinct layer-wise behaviors. Our results suggest promising directions for further research on leveraging internal model dynamics for robust jailbreak detection and defense.",
    "summary": "",
    "translation": "LLM内部层是否揭示越狱检测的模式？",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM安全性和越狱检测，这属于安全、伦理相关主题，已被明确列为不相关主题。虽然涉及LLM内部表征分析，但核心应用是安全检测而非推荐/搜索/广告系统的改进，因此相关性很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06579v1": {
    "title": "TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents",
    "url": "https://www.alphaxiv.org/abs/2510.06579v1",
    "arxiv_id": "2510.06579v1",
    "authors": "Haofei Yu, Keyang Xuan, Fenghai Li, Kunlun Zhu, Zijie Lei, Jiaxun Zhang, Ziheng Qi, Kyle Richardson, Jiaxuan You",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 02:18:57",
    "ori_summary": "Automatic research with Large Language Models (LLMs) is rapidly gaining importance, driving the development of increasingly complex workflows involving multi-agent systems, planning, tool usage, code execution, and human-agent interaction to accelerate research processes. However, as more researchers and developers begin to use and build upon these tools and platforms, the complexity and difficulty of extending and maintaining such agentic workflows have become a significant challenge, particularly as algorithms and architectures continue to advance. To address this growing complexity, TinyScientist identifies the essential components of the automatic research workflow and proposes an interactive, extensible, and controllable framework that easily adapts to new tools and supports iterative growth. We provide an open-source codebase, an interactive web demonstration, and a PyPI Python package to make state-of-the-art auto-research pipelines broadly accessible to every researcher and developer.",
    "summary": "",
    "translation": "TinyScientist：一个用于构建研究型智能体的交互式、可扩展且可控框架",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于构建研究型智能体的通用框架，属于AIGC和智能体开发领域，与推荐系统、搜索或广告的核心技术无直接关联。框架的可扩展性和可控性特征未体现与异构数据建模、Transformer架构改进或LLM在RecSys/Search/Ads中的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06559v1": {
    "title": "The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law",
    "url": "https://www.alphaxiv.org/abs/2510.06559v1",
    "arxiv_id": "2510.06559v1",
    "authors": "Cheonkam Jeong, Sungdo Kim, Jewoo Park",
    "categories": "cs.CL, cs.AI, cs.LO",
    "pub_date": "2025-10-08 01:22:26",
    "ori_summary": "Contemporary language models are fluent yet routinely mis-handle the types of meaning their outputs entail. We argue that hallucination, brittle moderation, and opaque compliance outcomes are symptoms of missing type-theoretic semantics rather than data or scale limitations. Building on Montague's view of language as typed, compositional algebra, we recast alignment as a parsing problem: natural-language inputs must be compiled into structures that make explicit their descriptive, normative, and legal dimensions under context. We present Savassan, a neuro-symbolic architecture that compiles utterances into Montague-style logical forms and maps them to typed ontologies extended with deontic operators and jurisdictional contexts. Neural components extract candidate structures from unstructured inputs; symbolic components perform type checking, constraint reasoning, and cross-jurisdiction mapping to produce compliance-aware guidance rather than binary censorship. In cross-border scenarios, the system \"parses once\" (e.g., defect claim(product x, company y)) and projects the result into multiple legal ontologies (e.g., defamation risk in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into a single, explainable decision. This paper contributes: (i) a diagnosis of hallucination as a type error; (ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii) a production-oriented design that embeds typed interfaces across the pipeline. We outline an evaluation plan using legal reasoning benchmarks and synthetic multi-jurisdiction suites. Our position is that trustworthy autonomy requires compositional typing of meaning, enabling systems to reason about what is described, what is prescribed, and what incurs liability within a unified algebra of meaning.",
    "summary": "",
    "translation": "意义代数：为何机器更需要蒙塔古而非摩尔定律",
    "relevance_score": 2,
    "reasoning": "该论文标题暗示其关注语义理解和形式语义学（蒙塔古语义学），这属于语言学理论范畴，而非推荐系统、搜索或广告领域的技术进展。虽然语义理解是LLM的基础能力之一，但该标题未表明任何具体的架构创新、效率改进或直接应用场景，与当前关注的四大方向均无明显关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06557v1": {
    "title": "The Markovian Thinker",
    "url": "https://www.alphaxiv.org/abs/2510.06557v1",
    "arxiv_id": "2510.06557v1",
    "authors": "Milad Aghajohari, Kamran Chitsaz, Amirhossein Kazemnejad, Sarath Chandar, Alessandro Sordoni, Aaron Courville, Siva Reddy",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-08 01:18:13",
    "ori_summary": "Reinforcement learning (RL) has recently become a strong recipe for training reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard RL \"thinking environment\", where the state is the prompt plus all prior reasoning tokens, makes the state unbounded and forces attention-based policies to pay quadratic compute as thoughts lengthen. We revisit the environment itself. We propose Markovian Thinking, a paradigm in which the policy advances reasoning while conditioning on a constant-size state, decoupling thinking length from context size. As an immediate consequence this yields linear compute with constant memory. We instantiate this idea with Delethink, an RL environment that structures reasoning into fixed-size chunks. Within each chunk, the model thinks as usual; at the boundary, the environment resets the context and reinitializes the prompt with a short carryover. Through RL, the policy learns to write a textual state near the end of each chunk sufficient for seamless continuation of reasoning after reset. Trained in this environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget. With test-time scaling, Delethink continues to improve where LongCoT plateaus. The effect of linear compute is substantial: we empirically estimate at 96K average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink. Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B) often sample Markovian traces zero-shot across diverse benchmarks, providing positive samples that make RL effective at scale. Our results show that redesigning the thinking environment is a powerful lever: it enables very long reasoning without quadratic overhead and opens a path toward efficient, scalable reasoning LLMs.",
    "summary": "",
    "translation": "马尔可夫思考者",
    "relevance_score": 1,
    "reasoning": "该标题暗示了与马尔可夫过程或决策理论相关的内容，但没有明确表明与推荐系统、搜索或广告领域的直接关联。标题过于宽泛，无法识别出任何具体的LLM技术、Transformer架构进展或异构数据建模的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06552v1": {
    "title": "Flipping the Dialogue: Training and Evaluating User Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.06552v1",
    "arxiv_id": "2510.06552v1",
    "authors": "Tarek Naous, Philippe Laban, Wei Xu, Jennifer Neville",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 01:04:36",
    "ori_summary": "Conversations with LMs involve two participants: a human user leading the conversation, and an LM assistant responding to the user's request. To satisfy this specific role, LMs are post-trained to be helpful assistants -- optimized to produce exhaustive and well-structured responses, free of ambiguity and grammar errors. User utterances, on the other hand, are rarely perfected, with each user phrasing requests in unique ways, sometimes putting in partial effort at each turn and refining on the fly. To evaluate LM performance in realistic settings, prior work simulated users in multi-turn conversations, often prompting an LLM originally trained to be a helpful assistant to act as a user. However, we show that assistant LMs make for poor user simulators, with the surprising finding that better assistants yield worse simulators. Instead, we introduce purpose-built User Language Models (User LMs) - models post-trained to simulate human users in multi-turn conversations. Through various evaluations, we show how User LMs align better with human behavior and achieve better simulation robustness than existing simulation methods. When leveraging User LMs to simulate coding and math conversations, the performance of a strong assistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic simulation environments lead to assistant struggles as they fail to cope with the nuances of users in multi-turn setups.",
    "summary": "",
    "translation": "翻转对话：训练与评估用户语言模型",
    "relevance_score": 8,
    "reasoning": "该论文聚焦于用户语言模型的训练与评估，直接属于'直接LLM应用'范畴，在推荐系统和搜索领域具有明确应用价值。用户语言模型可用于个性化对话推荐、用户意图理解、以及基于对话历史的搜索优化，能够显著提升用户体验和系统交互质量。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06548v1": {
    "title": "From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining",
    "url": "https://www.alphaxiv.org/abs/2510.06548v1",
    "arxiv_id": "2510.06548v1",
    "authors": "Seng Pei Liew, Takuya Kato",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-08 00:59:33",
    "ori_summary": "Bootstrapped pretraining, i.e., the reuse of a pretrained base model for further pretraining, such as continual pretraining or model growth, is promising at reducing the cost of training language models from scratch. However, its effectiveness remains unclear, especially when applied to overtrained base models. In this work, we empirically study the scaling behavior of bootstrapped pretraining and find that its scaling efficiency diminishes in a predictable manner: The scaling exponent with respect to second-stage pretraining tokens decreases logarithmically with the number of tokens used to pretrain the base model. The joint dependence on first- and second-stage tokens is accurately modeled by a simple scaling law. Such saturation effect reveals a fundamental trade-off in multi-stage pretraining strategies: the more extensively a model is pretrained, the less additional benefit bootstrapping provides. Our findings provide practical insights for efficient language model training and raise important considerations for the reuse of overtrained models.",
    "summary": "",
    "translation": "从加速到饱和：自举语言模型预训练的缩放行为研究",
    "relevance_score": 8,
    "reasoning": "该论文研究语言模型预训练的缩放行为，属于'Enabling LLM Tech'范畴，探讨LLM训练过程中的基础进展。理解预训练缩放规律对于在搜索、推荐和广告系统中高效部署和优化LLM模型具有直接应用价值，可以帮助确定模型规模与性能的最佳平衡点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07319v1": {
    "title": "Temporal Prompting Matters: Rethinking Referring Video Object Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.07319v1",
    "arxiv_id": "2510.07319v1",
    "authors": "Ci-Siang Lin, Min-Hung Chen, I-Jieh Liu, Chien-Yi Wang, Sifei Liu, Yu-Chiang Frank Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 17:59:57",
    "ori_summary": "Referring Video Object Segmentation (RVOS) aims to segment the object referred to by the query sentence in the video. Most existing methods require end-to-end training with dense mask annotations, which could be computation-consuming and less scalable. In this work, we rethink the RVOS problem and aim to investigate the key to this task. Based on existing foundation segmentation models, we decompose the RVOS task into referring, video, and segmentation factors, and propose a Temporal Prompt Generation and Selection (Tenet) framework to address the referring and video factors while leaving the segmentation problem to foundation models. To efficiently adapt image-based foundation segmentation models to referring video object segmentation, we leverage off-the-shelf object detectors and trackers to produce temporal prompts associated with the referring sentence. While high-quality temporal prompts could be produced, they can not be easily identified from confidence scores. To tackle this issue, we propose Prompt Preference Learning to evaluate the quality of the produced temporal prompts. By taking such prompts to instruct image-based foundation segmentation models, we would be able to produce high-quality masks for the referred object, enabling efficient model adaptation to referring video object segmentation. Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenet framework.",
    "summary": "",
    "translation": "时序提示至关重要：重新思考指代视频目标分割",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频目标分割的计算机视觉任务，虽然涉及时序建模，但核心是纯粹的视觉理解问题。没有明确的机制或方法可以应用于推荐系统、搜索或广告领域，与我的关注点缺乏直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07317v1": {
    "title": "Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms",
    "url": "https://www.alphaxiv.org/abs/2510.07317v1",
    "arxiv_id": "2510.07317v1",
    "authors": "Natacha Kuete Meli, Shuteng Wang, Marcel Seelbach Benkner, Michele Sasdelli, Tat-Jun Chin, Tolga Birdal, Michael Moeller, Vladislav Golyanik",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 17:59:51",
    "ori_summary": "Quantum-enhanced Computer Vision (QeCV) is a new research field at the intersection of computer vision, optimisation theory, machine learning and quantum computing. It has high potential to transform how visual signals are processed and interpreted with the help of quantum computing that leverages quantum-mechanical effects in computations inaccessible to classical (i.e. non-quantum) computers. In scenarios where existing non-quantum methods cannot find a solution in a reasonable time or compute only approximate solutions, quantum computers can provide, among others, advantages in terms of better time scalability for multiple problem classes. Parametrised quantum circuits can also become, in the long term, a considerable alternative to classical neural networks in computer vision. However, specialised and fundamentally new algorithms must be developed to enable compatibility with quantum hardware and unveil the potential of quantum computational paradigms in computer vision. This survey contributes to the existing literature on QeCV with a holistic review of this research field. It is designed as a quantum computing reference for the computer vision community, targeting computer vision students, scientists and readers with related backgrounds who want to familiarise themselves with QeCV. We provide a comprehensive introduction to QeCV, its specifics, and methodologies for formulations compatible with quantum hardware and QeCV methods, leveraging two main quantum computational paradigms, i.e. gate-based quantum computing and quantum annealing. We elaborate on the operational principles of quantum computers and the available tools to access, program and simulate them in the context of QeCV. Finally, we review existing quantum computing tools and learning materials and discuss aspects related to publishing and reviewing QeCV papers, open challenges and potential social implications.",
    "summary": "",
    "translation": "量子增强计算机视觉：超越经典算法",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于量子计算在计算机视觉领域的应用，属于纯粹的视觉研究方向。虽然标题提到'超越经典算法'，但内容明确限定在计算机视觉领域，与推荐系统、搜索或广告的核心技术栈没有直接关联。量子计算在视觉领域的进展对于RecSys/Search/Ads的潜在应用过于间接和推测性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07316v1": {
    "title": "Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers",
    "url": "https://www.alphaxiv.org/abs/2510.07316v1",
    "arxiv_id": "2510.07316v1",
    "authors": "Gangwei Xu, Haotong Lin, Hongcheng Luo, Xianqi Wang, Jingfeng Yao, Lianghui Zhu, Yuechuan Pu, Cheng Chi, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Sida Peng, Xin Yang",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 17:59:33",
    "ori_summary": "This paper presents Pixel-Perfect Depth, a monocular depth estimation model based on pixel-space diffusion generation that produces high-quality, flying-pixel-free point clouds from estimated depth maps. Current generative depth estimation models fine-tune Stable Diffusion and achieve impressive performance. However, they require a VAE to compress depth maps into latent space, which inevitably introduces \\textit{flying pixels} at edges and details. Our model addresses this challenge by directly performing diffusion generation in the pixel space, avoiding VAE-induced artifacts. To overcome the high complexity associated with pixel-space generation, we introduce two novel designs: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which incorporate semantic representations from vision foundation models into DiT to prompt the diffusion process, thereby preserving global semantic consistency while enhancing fine-grained visual details; and 2) Cascade DiT Design that progressively increases the number of tokens to further enhance efficiency and accuracy. Our model achieves the best performance among all published generative models across five benchmarks, and significantly outperforms all other models in edge-aware point cloud evaluation.",
    "summary": "",
    "translation": "基于语义提示扩散变换器的像素级完美深度估计",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的深度估计任务，使用扩散变换器技术实现像素级精度。虽然涉及变换器架构，但其核心应用场景是视觉感知而非推荐系统、搜索或广告领域，与当前关注点的直接关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07313v1": {
    "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation",
    "url": "https://www.alphaxiv.org/abs/2510.07313v1",
    "arxiv_id": "2510.07313v1",
    "authors": "Zezhong Qian, Xiaowei Chi, Yuming Li, Shizun Wang, Zhiyuan Qin, Xiaozhu Ju, Sirui Han, Shanghang Zhang",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-08 17:59:08",
    "ori_summary": "Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhance manipulation performance. Yet large-scale datasets rarely include such recordings, resulting in a substantial gap between abundant anchor views and scarce wrist views. Existing world models cannot bridge this gap, as they require a wrist-view first frame and thus fail to generate wrist-view videos from anchor views alone. Amid this gap, recent visual geometry models such as VGGT emerge with geometric and cross-view priors that make it possible to address extreme viewpoint shifts. Inspired by these insights, we propose WristWorld, the first 4D world model that generates wrist-view videos solely from anchor views. WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and incorporates our Spatial Projection Consistency (SPC) Loss to estimate geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation, which employs our video generation model to synthesize temporally coherent wrist-view videos from the reconstructed perspective. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation with superior spatial consistency, while also improving VLA performance, raising the average task completion length on Calvin by 3.81% and closing 42.4% of the anchor-wrist view gap.",
    "summary": "",
    "translation": "WristWorld：通过4D世界模型生成腕部视图用于机器人操作",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人操作和计算机视觉领域，涉及4D世界模型和腕部视图生成。这与推荐系统、搜索或广告的核心关注点完全无关，也不涉及任何可能应用于这些领域的LLM技术或Transformer架构进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07310v1": {
    "title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation",
    "url": "https://www.alphaxiv.org/abs/2510.07310v1",
    "arxiv_id": "2510.07310v1",
    "authors": "Siyoon Jin, Seongchan Kim, Dahyun Chung, Jaeho Lee, Hyunwook Choi, Jisu Nam, Jiyoung Kim, Seungryong Kim",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 17:57:38",
    "ori_summary": "Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-object interactions. This raises a key question: How do these models internally represent interactions? To answer this, we curate MATRIX-11K, a video dataset with interaction-aware captions and multi-instance mask tracks. Using this dataset, we conduct a systematic analysis that formalizes two perspectives of video DiTs: semantic grounding, via video-to-text attention, which evaluates whether noun and verb tokens capture instances and their relations; and semantic propagation, via video-to-video attention, which assesses whether instance bindings persist across frames. We find both effects concentrate in a small subset of interaction-dominant layers. Motivated by this, we introduce MATRIX, a simple and effective regularization that aligns attention in specific layers of video DiTs with multi-instance mask tracks from the MATRIX-11K dataset, enhancing both grounding and propagation. We further propose InterGenEval, an evaluation protocol for interaction-aware video generation. In experiments, MATRIX improves both interaction fidelity and semantic alignment while reducing drift and hallucination. Extensive ablations validate our design choices. Codes and weights will be released.",
    "summary": "",
    "translation": "MATRIX：用于交互感知视频生成的掩码轨迹对齐",
    "relevance_score": 2,
    "reasoning": "该论文专注于视频生成技术，特别是掩码轨迹对齐方法用于交互感知的视频生成。虽然视频生成在某些广告场景中可能有潜在应用，但这属于AIGC和内容生成领域，与我的核心关注点（推荐系统、搜索、广告排名以及相关的LLM/Transformer技术）相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07302v1": {
    "title": "SpecGuard: Spectral Projection-based Advanced Invisible Watermarking",
    "url": "https://www.alphaxiv.org/abs/2510.07302v1",
    "arxiv_id": "2510.07302v1",
    "authors": "Inzamamul Alam, Md Tanvir Islam, Khan Muhammad, Simon S. Woo",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 17:56:21",
    "ori_summary": "Watermarking embeds imperceptible patterns into images for authenticity verification. However, existing methods often lack robustness against various transformations primarily including distortions, image regeneration, and adversarial perturbation, creating real-world challenges. In this work, we introduce SpecGuard, a novel watermarking approach for robust and invisible image watermarking. Unlike prior approaches, we embed the message inside hidden convolution layers by converting from the spatial domain to the frequency domain using spectral projection of a higher frequency band that is decomposed by wavelet projection. Spectral projection employs Fast Fourier Transform approximation to transform spatial data into the frequency domain efficiently. In the encoding phase, a strength factor enhances resilience against diverse attacks, including adversarial, geometric, and regeneration-based distortions, ensuring the preservation of copyrighted information. Meanwhile, the decoder leverages Parseval's theorem to effectively learn and extract the watermark pattern, enabling accurate retrieval under challenging transformations. We evaluate the proposed SpecGuard based on the embedded watermark's invisibility, capacity, and robustness. Comprehensive experiments demonstrate the proposed SpecGuard outperforms the state-of-the-art models. To ensure reproducibility, the full code is released on \\href{https://github.com/inzamamulDU/SpecGuard_ICCV_2025}{\\textcolor{blue}{\\textbf{GitHub}}}.",
    "summary": "",
    "translation": "SpecGuard：基于频谱投影的先进隐形水印技术",
    "relevance_score": 1,
    "reasoning": "该论文涉及数字水印技术，属于信息安全领域，与我的核心关注点（推荐系统、搜索、广告、LLM技术及其应用）完全无关。水印技术主要用于内容保护和版权管理，不涉及任何推荐、搜索、广告或LLM相关的技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07277v1": {
    "title": "Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema Detection",
    "url": "https://www.alphaxiv.org/abs/2510.07277v1",
    "arxiv_id": "2510.07277v1",
    "authors": "Franco Javier Arellano, José Ignacio Orlando",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 17:41:02",
    "ori_summary": "Diabetic Macular Edema (DME) is a leading cause of vision loss among patients with Diabetic Retinopathy (DR). While deep learning has shown promising results for automatically detecting this condition from fundus images, its application remains challenging due the limited availability of annotated data. Foundation Models (FM) have emerged as an alternative solution. However, it is unclear if they can cope with DME detection in particular. In this paper, we systematically compare different FM and standard transfer learning approaches for this task. Specifically, we compare the two most popular FM for retinal images--RETFound and FLAIR--and an EfficientNet-B0 backbone, across different training regimes and evaluation settings in IDRiD, MESSIDOR-2 and OCT-and-Eye-Fundus-Images (OEFI). Results show that despite their scale, FM do not consistently outperform fine-tuned CNNs in this task. In particular, an EfficientNet-B0 ranked first or second in terms of area under the ROC and precision/recall curves in most evaluation settings, with RETFound only showing promising results in OEFI. FLAIR, on the other hand, demonstrated competitive zero-shot performance, achieving notable AUC-PR scores when prompted appropriately. These findings reveal that FM might not be a good tool for fine-grained ophthalmic tasks such as DME detection even after fine-tuning, suggesting that lightweight CNNs remain strong baselines in data-scarce environments.",
    "summary": "",
    "translation": "评估用于糖尿病性黄斑水肿检测的眼底特异性基础模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的眼底图像分析和糖尿病性黄斑水肿检测，属于明确的医学应用范畴。这与我的关注领域（推荐系统、搜索、广告及其相关技术）完全无关，且医学应用被明确列为不相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07249v1": {
    "title": "TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation",
    "url": "https://www.alphaxiv.org/abs/2510.07249v1",
    "arxiv_id": "2510.07249v1",
    "authors": "Jiaben Chen, Zixin Wang, Ailing Zeng, Yang Fu, Xueyang Yu, Siyuan Cen, Julian Tanke, Yihang Chen, Koichi Saito, Yuki Mitsufuji, Chuang Gan",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 17:16:09",
    "ori_summary": "In this work, we present TalkCuts, a large-scale dataset designed to facilitate the study of multi-shot human speech video generation. Unlike existing datasets that focus on single-shot, static viewpoints, TalkCuts offers 164k clips totaling over 500 hours of high-quality human speech videos with diverse camera shots, including close-up, half-body, and full-body views. The dataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X motion annotations, covering over 10k identities, enabling multimodal learning and evaluation. As a first attempt to showcase the value of the dataset, we present Orator, an LLM-guided multi-modal generation framework as a simple baseline, where the language model functions as a multi-faceted director, orchestrating detailed specifications for camera transitions, speaker gesticulations, and vocal modulation. This architecture enables the synthesis of coherent long-form videos through our integrated multi-modal video generation module. Extensive experiments in both pose-guided and audio-driven settings show that training on TalkCuts significantly enhances the cinematographic coherence and visual appeal of generated multi-shot speech videos. We believe TalkCuts provides a strong foundation for future work in controllable, multi-shot speech video generation and broader multimodal learning.",
    "summary": "",
    "translation": "TalkCuts：用于多镜头人类语音视频生成的大规模数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音视频生成，属于纯粹的视觉和语音生成领域，与推荐系统、搜索或广告的核心技术无关。虽然涉及大规模数据集，但应用场景仅限于视频生成，没有显示出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07217v1": {
    "title": "GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.07217v1",
    "arxiv_id": "2510.07217v1",
    "authors": "Wen Ye, Zhaocheng Liu, Yuwei Gui, Tingyu Yuan, Yunyue Su, Bowen Fang, Chaoyang Zhao, Qiang Liu, Liang Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 16:51:52",
    "ori_summary": "Text-to-image synthesis has made remarkable progress, yet accurately interpreting complex and lengthy prompts remains challenging, often resulting in semantic inconsistencies and missing details. Existing solutions, such as fine-tuning, are model-specific and require training, while prior automatic prompt optimization (APO) approaches typically lack systematic error analysis and refinement strategies, resulting in limited reliability and effectiveness. Meanwhile, test-time scaling methods operate on fixed prompts and on noise or sample numbers, limiting their interpretability and adaptability. To solve these, we introduce a flexible and efficient test-time prompt optimization strategy that operates directly on the input text. We propose a plug-and-play multi-agent system called GenPilot, integrating error analysis, clustering-based adaptive exploration, fine-grained verification, and a memory module for iterative optimization. Our approach is model-agnostic, interpretable, and well-suited for handling long and complex prompts. Simultaneously, we summarize the common patterns of errors and the refinement strategy, offering more experience and encouraging further exploration. Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7% demonstrate the strong capability of our methods in enhancing the text and image consistency and structural coherence of generated images, revealing the effectiveness of our test-time prompt optimization strategy. The code is available at https://github.com/27yw/GenPilot.",
    "summary": "",
    "translation": "GenPilot：一种用于图像生成中测试时提示优化的多智能体系统",
    "relevance_score": 1,
    "reasoning": "该论文专注于图像生成的提示优化，属于纯粹的AIGC和内容生成领域。虽然涉及多智能体系统，但其核心应用是图像生成而非推荐系统、搜索或广告中的排名任务，与当前关注的领域没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07206v1": {
    "title": "EigenScore: OOD Detection using Covariance in Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2510.07206v1",
    "arxiv_id": "2510.07206v1",
    "authors": "Shirin Shoushtari, Yi Wang, Xiao Shi, M. Salman Asif, Ulugbek S. Kamilov",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 16:42:20",
    "ori_summary": "Out-of-distribution (OOD) detection is critical for the safe deployment of machine learning systems in safety-sensitive domains. Diffusion models have recently emerged as powerful generative models, capable of capturing complex data distributions through iterative denoising. Building on this progress, recent work has explored their potential for OOD detection. We propose EigenScore, a new OOD detection method that leverages the eigenvalue spectrum of the posterior covariance induced by a diffusion model. We argue that posterior covariance provides a consistent signal of distribution shift, leading to larger trace and leading eigenvalues on OOD inputs, yielding a clear spectral signature. We further provide analysis explicitly linking posterior covariance to distribution mismatch, establishing it as a reliable signal for OOD detection. To ensure tractability, we adopt a Jacobian-free subspace iteration method to estimate the leading eigenvalues using only forward evaluations of the denoiser. Empirically, EigenScore achieves SOTA performance, with up to 5% AUROC improvement over the best baseline. Notably, it remains robust in near-OOD settings such as CIFAR-10 vs CIFAR-100, where existing diffusion-based methods often fail.",
    "summary": "",
    "translation": "EigenScore：基于扩散模型中协方差的分布外检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于扩散模型中的分布外检测方法，属于计算机视觉领域的特定技术。虽然扩散模型是生成模型的一种，但该工作主要解决视觉数据的异常检测问题，与推荐系统、搜索或广告的核心技术栈没有直接关联。论文没有展示在异构数据处理、序列建模或Transformer架构方面的创新，无法看出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07191v1": {
    "title": "Resolution scaling governs DINOv3 transfer performance in chest radiograph classification",
    "url": "https://www.alphaxiv.org/abs/2510.07191v1",
    "arxiv_id": "2510.07191v1",
    "authors": "Soroosh Tayebi Arasteh, Mina Shaigan, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-08 16:25:04",
    "ori_summary": "Self-supervised learning (SSL) has advanced visual representation learning, but its value in chest radiography, a high-volume imaging modality with fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL models through Gram-anchored self-distillation. Whether these design choices improve transfer learning for chest radiography has not been systematically tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across seven datasets (n>814,000). Two representative backbones were evaluated: ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and 1024x1024 pixels. We additionally assessed frozen features from a 7B model. The primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2 achieved comparable performance on adult datasets. Increasing resolution to 512x512 yielded consistent improvements for DINOv3 over both DINOv2 and ImageNet. In contrast, results in pediatric cohort showed no differences across initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models using frozen DINOv3-7B features underperformed relative to fully finetuned 86-89M-parameter backbones, highlighting the importance of domain adaptation. Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains were most evident for boundary-dependent and small focal abnormalities. In chest radiography, higher input resolution is critical for leveraging the benefits of modern self-supervised models. 512x512 pixels represent a practical upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest performance, while larger inputs offer minimal return on cost. Clinically, these findings support use of finetuned, mid-sized backbones at 512x512 for chest radiograph interpretation, with the greatest gains expected in detecting subtle or boundary-centered lesions relevant to emergency and critical care settings.",
    "summary": "",
    "translation": "分辨率缩放决定DINOv3在胸部X光片分类中的迁移性能",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（胸部X光片）分类，这属于明确的无关主题领域。虽然DINOv3是视觉模型，但论文的应用场景纯粹是医学诊断，与推荐系统、搜索或广告没有任何潜在关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07190v1": {
    "title": "MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis",
    "url": "https://www.alphaxiv.org/abs/2510.07190v1",
    "arxiv_id": "2510.07190v1",
    "authors": "Yihao Zhi, Chenghong Li, Hongjie Liao, Xihe Yang, Zhengwentai Sun, Jiahao Chang, Xiaodong Cun, Wensen Feng, Xiaoguang Han",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 16:24:22",
    "ori_summary": "Recent breakthroughs in video generation, powered by large-scale datasets and diffusion techniques, have shown that video diffusion models can function as implicit 4D novel view synthesizers. Nevertheless, current methods primarily concentrate on redirecting camera trajectory within the front view while struggling to generate 360-degree viewpoint changes. In this paper, we focus on human-centric subdomain and present MV-Performer, an innovative framework for creating synchronized novel view videos from monocular full-body captures. To achieve a 360-degree synthesis, we extensively leverage the MVHumanNet dataset and incorporate an informative condition signal. Specifically, we use the camera-dependent normal maps rendered from oriented partial point clouds, which effectively alleviate the ambiguity between seen and unseen observations. To maintain synchronization in the generated videos, we propose a multi-view human-centric video diffusion model that fuses information from the reference video, partial rendering, and different viewpoints. Additionally, we provide a robust inference procedure for in-the-wild video cases, which greatly mitigates the artifacts induced by imperfect monocular depth estimation. Extensive experiments on three datasets demonstrate our MV-Performer's state-of-the-art effectiveness and robustness, setting a strong model for human-centric 4D novel view synthesis.",
    "summary": "",
    "translation": "MV-Performer：驯服视频扩散模型以实现忠实且同步的多视角表演者合成",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频生成和多视角合成技术，属于计算机视觉和内容生成领域。虽然涉及扩散模型，但其应用场景（表演者合成）与推荐系统、搜索或广告的核心技术需求没有直接关联，也不符合VLM类比中处理异构数据的模式。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07181v1": {
    "title": "TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics",
    "url": "https://www.alphaxiv.org/abs/2510.07181v1",
    "arxiv_id": "2510.07181v1",
    "authors": "Yi Han, Cheng Chi, Enshen Zhou, Shanyu Rong, Jingkun An, Pengwei Wang, Zhongyuan Wang, Lu Sheng, Shanghang Zhang",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2025-10-08 16:20:23",
    "ori_summary": "Vision-Language Models (VLMs) have shown remarkable capabilities in spatial reasoning, yet they remain fundamentally limited to qualitative precision and lack the computational precision required for real-world robotics. Current approaches fail to leverage metric cues from depth sensors and camera calibration, instead reducing geometric problems to pattern recognition tasks that cannot deliver the centimeter-level accuracy essential for robotic manipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel framework that transforms VLMs from perceptual estimators to geometric computers by enabling them to generate and execute precise geometric computations through external tools. Rather than attempting to internalize complex geometric operations within neural networks, TIGeR empowers models to recognize geometric reasoning requirements, synthesize appropriate computational code, and invoke specialized libraries for exact calculations. To support this paradigm, we introduce TIGeR-300K, a comprehensive tool-invocation-oriented dataset covering point transformations, pose estimation, trajectory generation, and spatial compatibility verification, complete with tool invocation sequences and intermediate computations. Through a two-stage training pipeline combining supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT) with our proposed hierarchical reward design, TIGeR achieves SOTA performance on geometric reasoning benchmarks while demonstrating centimeter-level precision in real-world robotic manipulation tasks.",
    "summary": "",
    "translation": "TIGeR：机器人视觉语言模型中的工具集成几何推理",
    "relevance_score": 2,
    "reasoning": "该论文专注于机器人领域的视觉语言模型应用，虽然涉及多模态建模，但其核心应用场景（机器人）与搜索、推荐或广告系统没有直接关联。视觉语言模型的异构数据处理思想在理论上与推荐系统中的多模态建模有相似之处，但这种关联过于间接且应用领域完全不同。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07143v1": {
    "title": "Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods",
    "url": "https://www.alphaxiv.org/abs/2510.07143v1",
    "arxiv_id": "2510.07143v1",
    "authors": "Chenfei Liao, Wensong Wang, Zichen Wen, Xu Zheng, Yiyu Wang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Xin Zou, Yuqian Fu, Bin Ren, Linfeng Zhang, Xuming Hu",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 15:44:28",
    "ori_summary": "Recent endeavors to accelerate inference in Multimodal Large Language Models (MLLMs) have primarily focused on visual token compression. The effectiveness of these methods is typically assessed by measuring the accuracy drop on established benchmarks, comparing model performance before and after compression. However, these benchmarks are originally designed to assess the perception and reasoning capabilities of MLLMs, rather than to evaluate compression techniques. As a result, directly applying them to visual token compression introduces a task mismatch. Strikingly, our investigation reveals that simple image downsampling consistently outperforms many advanced compression methods across multiple widely used benchmarks. Through extensive experiments, we make the following observations: (i) Current benchmarks are noisy for the visual token compression task. (ii) Down-sampling is able to serve as a data filter to evaluate the difficulty of samples in the visual token compression task. Motivated by these findings, we introduce VTC-Bench, an evaluation framework that incorporates a data filtering mechanism to denoise existing benchmarks, thereby enabling fairer and more accurate assessment of visual token compression methods. All data and code are available at https://github.com/Chenfei-Liao/VTC-Bench.",
    "summary": "",
    "translation": "我们是否使用了正确的基准：视觉令牌压缩方法的评估框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉令牌压缩方法的评估框架，属于纯粹的视觉领域研究。虽然令牌压缩技术理论上可以应用于多模态推荐系统中的图像处理，但论文标题明确聚焦于视觉令牌和评估基准，缺乏与推荐系统、搜索或广告的直接关联。这种视觉中心的评估研究距离实际应用场景较远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07135v1": {
    "title": "Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07135v1",
    "arxiv_id": "2510.07135v1",
    "authors": "Karim El Khoury, Maxime Zanella, Christophe De Vleeschouwer, Benoit Macq",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 15:29:48",
    "ori_summary": "Remote Sensing Vision-Language Models (RSVLMs) have shown remarkable potential thanks to large-scale pretraining, achieving strong zero-shot performance on various tasks. However, their ability to generalize in low-data regimes, such as few-shot learning, remains insufficiently explored. In this work, we present the first structured benchmark for evaluating few-shot adaptation methods on RSVLMs. We conduct comprehensive experiments across ten remote sensing scene classification datasets, applying five widely used few-shot adaptation strategies to three state-of-the-art RSVLMs with varying backbones. Our findings reveal that models with similar zero-shot performance can exhibit markedly different behavior under few-shot adaptation, with some RSVLMs being inherently more amenable to such adaptation than others. The variability of performance and the absence of a clear winner among existing methods highlight the need for the development of more robust methods for few-shot adaptation tailored to RS. To facilitate future research, we provide a reproducible benchmarking framework and open-source code to systematically evaluate RSVLMs under few-shot conditions. The source code is publicly available on Github: https://github.com/elkhouryk/fewshot_RSVLMs",
    "summary": "",
    "translation": "遥感视觉语言模型的少样本适应基准",
    "relevance_score": 2,
    "reasoning": "该论文虽然涉及视觉语言模型（VLM），但专注于遥感这一特定领域应用，与推荐系统、搜索或广告没有直接关联。遥感数据与商业推荐/搜索场景中的异构数据模态存在本质差异，且论文主要关注基准测试而非核心架构创新，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07134v1": {
    "title": "TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking",
    "url": "https://www.alphaxiv.org/abs/2510.07134v1",
    "arxiv_id": "2510.07134v1",
    "authors": "Jiahang Liu, Yunpeng Qi, Jiazhao Zhang, Minghan Li, Shaoan Wang, Kui Wu, Hanjing Ye, Hong Zhang, Zhibo Chen, Fangwei Zhong, Zhizheng Zhang, He Wang",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2025-10-08 15:29:17",
    "ori_summary": "Embodied Visual Tracking (EVT) is a fundamental ability that underpins practical applications, such as companion robots, guidance robots and service assistants, where continuously following moving targets is essential. Recent advances have enabled language-guided tracking in complex and unstructured scenes. However, existing approaches lack explicit spatial reasoning and effective temporal memory, causing failures under severe occlusions or in the presence of similar-looking distractors. To address these challenges, we present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances embodied visual tracking with two key modules, a spatial reasoning mechanism and a Target Identification Memory (TIM). The reasoning module introduces a Chain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative position and encodes it as a compact polar-coordinate token for action prediction. Guided by these spatial priors, the TIM employs a gated update strategy to preserve long-horizon target memory, ensuring spatiotemporal consistency and mitigating target loss during extended occlusions. Extensive experiments show that TrackVLA++ achieves state-of-the-art performance on public benchmarks across both egocentric and multi-camera settings. On the challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong zero-shot generalization, enabling robust real-world tracking in dynamic and occluded scenarios.",
    "summary": "",
    "translation": "TrackVLA++：在具身视觉跟踪中释放视觉语言模型的推理与记忆能力",
    "relevance_score": 2,
    "reasoning": "该论文主要关注具身视觉跟踪中的视觉语言模型能力提升，属于机器人学和具身AI领域。虽然涉及视觉语言模型技术，但其应用场景（具身视觉跟踪）与推荐系统、搜索或广告领域没有直接关联，且论文焦点是机器人环境中的视觉跟踪任务，而非文本或序列数据的异质模态建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07129v1": {
    "title": "Graph Conditioned Diffusion for Controllable Histopathology Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.07129v1",
    "arxiv_id": "2510.07129v1",
    "authors": "Sarah Cechnicka, Matthew Baugh, Weitong Zhang, Mischa Dombrowski, Zhe Li, Johannes C. Paetzold, Candice Roufosse, Bernhard Kainz",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 15:26:08",
    "ori_summary": "Recent advances in Diffusion Probabilistic Models (DPMs) have set new standards in high-quality image synthesis. Yet, controlled generation remains challenging, particularly in sensitive areas such as medical imaging. Medical images feature inherent structure such as consistent spatial arrangement, shape or texture, all of which are critical for diagnosis. However, existing DPMs operate in noisy latent spaces that lack semantic structure and strong priors, making it difficult to ensure meaningful control over generated content. To address this, we propose graph-based object-level representations for Graph-Conditioned-Diffusion. Our approach generates graph nodes corresponding to each major structure in the image, encapsulating their individual features and relationships. These graph representations are processed by a transformer module and integrated into a diffusion model via the text-conditioning mechanism, enabling fine-grained control over generation. We evaluate this approach using a real-world histopathology use case, demonstrating that our generated data can reliably substitute for annotated patient data in downstream segmentation tasks. The code is available here.",
    "summary": "",
    "translation": "基于图条件扩散的可控病理图像生成",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于病理图像生成，属于医学影像领域的特定应用，与推荐系统、搜索或广告的核心技术无关。虽然提到了扩散模型和条件生成技术，但这些技术在该论文中的应用仅限于医疗领域，没有明确的潜力应用于推荐系统、搜索或广告场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07126v1": {
    "title": "Validation of Various Normalization Methods for Brain Tumor Segmentation: Can Federated Learning Overcome This Heterogeneity?",
    "url": "https://www.alphaxiv.org/abs/2510.07126v1",
    "arxiv_id": "2510.07126v1",
    "authors": "Jan Fiszer, Dominika Ciupek, Maciej Malawski",
    "categories": "cs.CV, cs.DC",
    "pub_date": "2025-10-08 15:21:53",
    "ori_summary": "Deep learning (DL) has been increasingly applied in medical imaging, however, it requires large amounts of data, which raises many challenges related to data privacy, storage, and transfer. Federated learning (FL) is a training paradigm that overcomes these issues, though its effectiveness may be reduced when dealing with non-independent and identically distributed (non-IID) data. This study simulates non-IID conditions by applying different MRI intensity normalization techniques to separate data subsets, reflecting a common cause of heterogeneity. These subsets are then used for training and testing models for brain tumor segmentation. The findings provide insights into the influence of the MRI intensity normalization methods on segmentation models, both training and inference. Notably, the FL methods demonstrated resilience to inconsistently normalized data across clients, achieving the 3D Dice score of 92%, which is comparable to a centralized model (trained using all data). These results indicate that FL is a solution to effectively train high-performing models without violating data privacy, a crucial concern in medical applications. The code is available at: https://github.com/SanoScience/fl-varying-normalization.",
    "summary": "",
    "translation": "脑肿瘤分割中多种归一化方法的验证：联邦学习能否克服这种异质性？",
    "relevance_score": 1,
    "reasoning": "该论文主要关注医学领域的脑肿瘤分割和联邦学习技术，属于明确的医学应用范畴。虽然提到了联邦学习，但这属于隐私保护技术，属于指定的不相关主题，与推荐系统、搜索或广告领域的核心进展没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07119v1": {
    "title": "MoRe: Monocular Geometry Refinement via Graph Optimization for Cross-View Consistency",
    "url": "https://www.alphaxiv.org/abs/2510.07119v1",
    "arxiv_id": "2510.07119v1",
    "authors": "Dongki Jung, Jaehoon Choi, Yonghan Lee, Sungmin Eum, Heesung Kwon, Dinesh Manocha",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 15:11:32",
    "ori_summary": "Monocular 3D foundation models offer an extensible solution for perception tasks, making them attractive for broader 3D vision applications. In this paper, we propose MoRe, a training-free Monocular Geometry Refinement method designed to improve cross-view consistency and achieve scale alignment. To induce inter-frame relationships, our method employs feature matching between frames to establish correspondences. Rather than applying simple least squares optimization on these matched points, we formulate a graph-based optimization framework that performs local planar approximation using the estimated 3D points and surface normals estimated by monocular foundation models. This formulation addresses the scale ambiguity inherent in monocular geometric priors while preserving the underlying 3D structure. We further demonstrate that MoRe not only enhances 3D reconstruction but also improves novel view synthesis, particularly in sparse view rendering scenarios.",
    "summary": "",
    "translation": "MoRe：通过图优化实现跨视图一致性的单目几何细化",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的单目几何优化和跨视图一致性，属于纯粹的视觉几何处理领域。虽然提到了图优化技术，但该技术在此处仅用于视觉几何对齐，与推荐系统、搜索或广告中的用户行为建模、序列处理或异构数据融合没有明显关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07115v1": {
    "title": "Enhancing Concept Localization in CLIP-based Concept Bottleneck Models",
    "url": "https://www.alphaxiv.org/abs/2510.07115v1",
    "arxiv_id": "2510.07115v1",
    "authors": "Rémi Kazmierczak, Steve Azzolin, Eloïse Berthier, Goran Frehse, Gianni Franchi",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 15:07:16",
    "ori_summary": "This paper addresses explainable AI (XAI) through the lens of Concept Bottleneck Models (CBMs) that do not require explicit concept annotations, relying instead on concepts extracted using CLIP in a zero-shot manner. We show that CLIP, which is central in these techniques, is prone to concept hallucination, incorrectly predicting the presence or absence of concepts within an image in scenarios used in numerous CBMs, hence undermining the faithfulness of explanations. To mitigate this issue, we introduce Concept Hallucination Inhibition via Localized Interpretability (CHILI), a technique that disentangles image embeddings and localizes pixels corresponding to target concepts. Furthermore, our approach supports the generation of saliency-based explanations that are more interpretable.",
    "summary": "",
    "translation": "增强基于CLIP的概念瓶颈模型中的概念定位能力",
    "relevance_score": 3,
    "reasoning": "该论文主要关注CLIP模型中的概念定位改进，属于计算机视觉领域的技术优化。虽然CLIP本身是多模态模型，但论文焦点是概念瓶颈模型的视觉概念定位，与推荐系统、搜索或广告的核心技术关联较弱。其潜在应用可能仅限于需要视觉概念理解的特定场景，对主流RecSys/Search/Ads的直接价值有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07089v1": {
    "title": "DADO: A Depth-Attention framework for Object Discovery",
    "url": "https://www.alphaxiv.org/abs/2510.07089v1",
    "arxiv_id": "2510.07089v1",
    "authors": "Federico Gonzalez, Estefania Talavera, Petia Radeva",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 14:46:34",
    "ori_summary": "Unsupervised object discovery, the task of identifying and localizing objects in images without human-annotated labels, remains a significant challenge and a growing focus in computer vision. In this work, we introduce a novel model, DADO (Depth-Attention self-supervised technique for Discovering unseen Objects), which combines an attention mechanism and a depth model to identify potential objects in images. To address challenges such as noisy attention maps or complex scenes with varying depth planes, DADO employs dynamic weighting to adaptively emphasize attention or depth features based on the global characteristics of each image. We evaluated DADO on standard benchmarks, where it outperforms state-of-the-art methods in object discovery accuracy and robustness without the need for fine-tuning.",
    "summary": "",
    "translation": "DADO：一种用于对象发现的深度注意力框架",
    "relevance_score": 2,
    "reasoning": "该论文提出了一种结合深度信息的注意力框架用于对象发现，属于计算机视觉领域。虽然注意力机制是Transformer的核心组件，但该工作专注于纯粹的视觉对象检测任务，没有明确展示在推荐系统、搜索或广告中的潜在应用。其技术路径更偏向基础视觉研究，与当前关注的LLM技术、推荐系统核心进展或异构数据统一建模缺乏直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07077v1": {
    "title": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
    "url": "https://www.alphaxiv.org/abs/2510.07077v1",
    "arxiv_id": "2510.07077v1",
    "authors": "Kento Kawaharazuka, Jihoon Oh, Jun Yamada, Ingmar Posner, Yuke Zhu",
    "categories": "cs.RO, cs.AI, cs.CV, cs.LG",
    "pub_date": "2025-10-08 14:38:25",
    "ori_summary": "Amid growing efforts to leverage advances in large language models (LLMs) and vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models have recently gained significant attention. By unifying vision, language, and action data at scale, which have traditionally been studied separately, VLA models aim to learn policies that generalise across diverse tasks, objects, embodiments, and environments. This generalisation capability is expected to enable robots to solve novel downstream tasks with minimal or no additional task-specific data, facilitating more flexible and scalable real-world deployment. Unlike previous surveys that focus narrowly on action representations or high-level model architectures, this work offers a comprehensive, full-stack review, integrating both software and hardware components of VLA systems. In particular, this paper provides a systematic review of VLAs, covering their strategy and architectural transition, architectures and building blocks, modality-specific processing techniques, and learning paradigms. In addition, to support the deployment of VLAs in real-world robotic applications, we also review commonly used robot platforms, data collection strategies, publicly available datasets, data augmentation methods, and evaluation benchmarks. Throughout this comprehensive survey, this paper aims to offer practical guidance for the robotics community in applying VLAs to real-world robotic systems. All references categorized by training approach, evaluation method, modality, and dataset are available in the table on our project website: https://vla-survey.github.io .",
    "summary": "",
    "translation": "面向机器人技术的视觉-语言-动作模型：迈向实际应用的研究综述",
    "relevance_score": 3,
    "reasoning": "虽然该论文涉及多模态建模（视觉-语言），但其核心应用领域是机器人技术而非推荐系统、搜索或广告。视觉-语言模型的概念可能为异构数据处理提供启发，但缺乏明确的RecSys/Search/Ads应用路径，且机器人动作控制与当前关注领域关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07058v1": {
    "title": "Concept Retrieval -- What and How?",
    "url": "https://www.alphaxiv.org/abs/2510.07058v1",
    "arxiv_id": "2510.07058v1",
    "authors": "Ori nizan, Oren Shrout, Ayellet Tal",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 14:26:18",
    "ori_summary": "A concept may reflect either a concrete or abstract idea. Given an input image, this paper seeks to retrieve other images that share its central concepts, capturing aspects of the underlying narrative. This goes beyond conventional retrieval or clustering methods, which emphasize visual or semantic similarity. We formally define the problem, outline key requirements, and introduce appropriate evaluation metrics. We propose a novel approach grounded in two key observations: (1) While each neighbor in the embedding space typically shares at least one concept with the query, not all neighbors necessarily share the same concept with one another. (2) Modeling this neighborhood with a bimodal Gaussian distribution uncovers meaningful structure that facilitates concept identification. Qualitative, quantitative, and human evaluations confirm the effectiveness of our approach. See the package on PyPI: https://pypi.org/project/coret/",
    "summary": "",
    "translation": "概念检索——是什么以及如何实现？",
    "relevance_score": 8,
    "reasoning": "概念检索直接涉及搜索和推荐系统的核心功能，专注于理解用户查询背后的语义概念而非字面匹配。这种技术可以显著提升搜索相关性，并为推荐系统提供更精准的用户意图理解，是RecSys和Search领域的重要研究方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07053v1": {
    "title": "Introspection in Learned Semantic Scene Graph Localisation",
    "url": "https://www.alphaxiv.org/abs/2510.07053v1",
    "arxiv_id": "2510.07053v1",
    "authors": "Manshika Charvi Bissessur, Efimia Panagiotaki, Daniele De Martini",
    "categories": "cs.LG, cs.AI, cs.CV, cs.RO, I.2.10; I.2.9; I.4.8; I.5.2; I.5.1",
    "pub_date": "2025-10-08 14:21:45",
    "ori_summary": "This work investigates how semantics influence localisation performance and robustness in a learned self-supervised, contrastive semantic localisation framework. After training a localisation network on both original and perturbed maps, we conduct a thorough post-hoc introspection analysis to probe whether the model filters environmental noise and prioritises distinctive landmarks over routine clutter. We validate various interpretability methods and present a comparative reliability analysis. Integrated gradients and Attention Weights consistently emerge as the most reliable probes of learned behaviour. A semantic class ablation further reveals an implicit weighting in which frequent objects are often down-weighted. Overall, the results indicate that the model learns noise-robust, semantically salient relations about place definition, thereby enabling explainable registration under challenging visual and structural variations.",
    "summary": "",
    "translation": "学习型语义场景图定位中的自省机制",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的场景图定位和自省机制，属于纯粹的视觉技术研究。虽然场景图理解在广义上可能与搜索中的图像检索相关，但论文标题明确聚焦于视觉场景分析，没有显示出与推荐系统、搜索排序或广告的直接关联，也不涉及LLM技术或Transformer架构的进步。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07041v1": {
    "title": "U-Bench: A Comprehensive Understanding of U-Net through 100-Variant Benchmarking",
    "url": "https://www.alphaxiv.org/abs/2510.07041v1",
    "arxiv_id": "2510.07041v1",
    "authors": "Fenghe Tang, Chengqi Dong, Wenxin Ma, Zikang Xu, Heqin Zhu, Zihang Jiang, Rongsheng Wang, Yuhao Wang, Chenxu Wu, Shaohua Kevin Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 14:06:17",
    "ori_summary": "Over the past decade, U-Net has been the dominant architecture in medical image segmentation, leading to the development of thousands of U-shaped variants. Despite its widespread adoption, there is still no comprehensive benchmark to systematically evaluate their performance and utility, largely because of insufficient statistical validation and limited consideration of efficiency and generalization across diverse datasets. To bridge this gap, we present U-Bench, the first large-scale, statistically rigorous benchmark that evaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our contributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates models along three key dimensions: statistical robustness, zero-shot generalization, and computational efficiency. We introduce a novel metric, U-Score, which jointly captures the performance-efficiency trade-off, offering a deployment-oriented perspective on model progress. (2) Systematic Analysis and Model Selection Guidance: We summarize key findings from the large-scale evaluation and systematically analyze the impact of dataset characteristics and architectural paradigms on model performance. Based on these insights, we propose a model advisor agent to guide researchers in selecting the most suitable models for specific datasets and tasks. (3) Public Availability: We provide all code, models, protocols, and weights, enabling the community to reproduce our results and extend the benchmark with future methods. In summary, U-Bench not only exposes gaps in previous evaluations but also establishes a foundation for fair, reproducible, and practically relevant benchmarking in the next decade of U-Net-based segmentation models. The project can be accessed at: https://fenghetan9.github.io/ubench. Code is available at: https://github.com/FengheTan9/U-Bench.",
    "summary": "",
    "translation": "U-Bench：通过100种变体基准测试全面理解U-Net",
    "relevance_score": 1,
    "reasoning": "该论文专注于U-Net架构的基准测试，U-Net主要应用于计算机视觉领域（如图像分割），与推荐系统、搜索或广告的核心技术无直接关联。论文内容属于纯粹的计算机视觉架构分析，没有展示在异构数据处理、Transformer改进或LLM应用方面的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07018v1": {
    "title": "Sharpness-Aware Data Generation for Zero-shot Quantization",
    "url": "https://www.alphaxiv.org/abs/2510.07018v1",
    "arxiv_id": "2510.07018v1",
    "authors": "Dung Hoang-Anh, Cuong Pham Trung Le, Jianfei Cai, Thanh-Toan Do",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-08 13:43:39",
    "ori_summary": "Zero-shot quantization aims to learn a quantized model from a pre-trained full-precision model with no access to original real training data. The common idea in zero-shot quantization approaches is to generate synthetic data for quantizing the full-precision model. While it is well-known that deep neural networks with low sharpness have better generalization ability, none of the previous zero-shot quantization works considers the sharpness of the quantized model as a criterion for generating training data. This paper introduces a novel methodology that takes into account quantized model sharpness in synthetic data generation to enhance generalization. Specifically, we first demonstrate that sharpness minimization can be attained by maximizing gradient matching between the reconstruction loss gradients computed on synthetic and real validation data, under certain assumptions. We then circumvent the problem of the gradient matching without real validation set by approximating it with the gradient matching between each generated sample and its neighbors. Experimental evaluations on CIFAR-100 and ImageNet datasets demonstrate the superiority of the proposed method over the state-of-the-art techniques in low-bit quantization settings.",
    "summary": "",
    "translation": "面向零样本量化的锐度感知数据生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注模型量化中的数据生成技术，属于模型压缩和效率优化领域。虽然量化技术可以应用于推荐或搜索系统中的模型部署效率提升，但论文标题明确聚焦于零样本量化场景，缺乏与推荐系统、搜索或广告领域的直接关联，且未涉及LLM或Transformer架构的核心进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07008v1": {
    "title": "Bayesian Modelling of Multi-Year Crop Type Classification Using Deep Neural Networks and Hidden Markov Models",
    "url": "https://www.alphaxiv.org/abs/2510.07008v1",
    "arxiv_id": "2510.07008v1",
    "authors": "Gianmarco Perantoni, Giulio Weikmann, Lorenzo Bruzzone",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 13:33:32",
    "ori_summary": "The temporal consistency of yearly land-cover maps is of great importance to model the evolution and change of the land cover over the years. In this paper, we focus the attention on a novel approach to classification of yearly satellite image time series (SITS) that combines deep learning with Bayesian modelling, using Hidden Markov Models (HMMs) integrated with Transformer Encoder (TE) based DNNs. The proposed approach aims to capture both i) intricate temporal correlations in yearly SITS and ii) specific patterns in multiyear crop type sequences. It leverages the cascade classification of an HMM layer built on top of the TE, discerning consistent yearly crop-type sequences. Validation on a multiyear crop type classification dataset spanning 47 crop types and six years of Sentinel-2 acquisitions demonstrates the importance of modelling temporal consistency in the predicted labels. HMMs enhance the overall performance and F1 scores, emphasising the effectiveness of the proposed approach.",
    "summary": "",
    "translation": "基于深度神经网络和隐马尔可夫模型的多年作物类型分类贝叶斯建模",
    "relevance_score": 1,
    "reasoning": "该论文专注于农业领域的作物分类问题，使用计算机视觉和时序模型进行多年作物类型识别。这与推荐系统、搜索或广告的核心技术领域完全无关，也不涉及LLM、Transformer架构或异构数据统一建模等关键技术。论文的应用场景和核心技术都超出了指定关注范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06988v1": {
    "title": "No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts",
    "url": "https://www.alphaxiv.org/abs/2510.06988v1",
    "arxiv_id": "2510.06988v1",
    "authors": "Girolamo Macaluso, Lorenzo Mandelli, Mirko Bicchierai, Stefano Berretti, Andrew D. Bagdanov",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 13:12:10",
    "ori_summary": "Diffusion models have recently advanced human motion generation, producing realistic and diverse animations from textual prompts. However, adapting these models to unseen actions or styles typically requires additional motion capture data and full retraining, which is costly and difficult to scale. We propose a post-training framework based on Reinforcement Learning that fine-tunes pretrained motion diffusion models using only textual prompts, without requiring any motion ground truth. Our approach employs a pretrained text-motion retrieval network as a reward signal and optimizes the diffusion policy with Denoising Diffusion Policy Optimization, effectively shifting the model's generative distribution toward the target domain without relying on paired motion data. We evaluate our method on cross-dataset adaptation and leave-one-out motion experiments using the HumanML3D and KIT-ML datasets across both latent- and joint-space diffusion architectures. Results from quantitative metrics and user studies show that our approach consistently improves the quality and diversity of generated motions, while preserving performance on the original distribution. Our approach is a flexible, data-efficient, and privacy-preserving solution for motion adaptation.",
    "summary": "",
    "translation": "无需动作捕捉：仅使用文本提示通过强化学习对后训练运动扩散模型进行优化",
    "relevance_score": 2,
    "reasoning": "该论文主要关注运动生成和扩散模型，属于计算机视觉和图形学领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然提到了强化学习，但应用场景是运动生成而非推荐/搜索/广告的排序或建模问题，因此相关性很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06982v1": {
    "title": "Revisiting Mixout: An Overlooked Path to Robust Finetuning",
    "url": "https://www.alphaxiv.org/abs/2510.06982v1",
    "arxiv_id": "2510.06982v1",
    "authors": "Masih Aminbeidokhti, Heitor Rapela Medeiros, Eric Granger, Marco Pedersoli",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-08 13:07:50",
    "ori_summary": "Finetuning vision foundation models often improves in-domain accuracy but comes at the cost of robustness under distribution shift. We revisit Mixout, a stochastic regularizer that intermittently replaces finetuned weights with their pretrained reference, through the lens of a single-run, weight-sharing implicit ensemble. This perspective reveals three key levers that govern robustness: the \\emph{masking anchor}, \\emph{resampling frequency}, and \\emph{mask sparsity}. Guided by this analysis, we introduce GMixout, which (i) replaces the fixed anchor with an exponential moving-average snapshot that adapts during training, and (ii) regulates masking period via an explicit resampling-frequency hyperparameter. Our sparse-kernel implementation updates only a small fraction of parameters with no inference-time overhead, enabling training on consumer-grade GPUs. Experiments on benchmarks covering covariate shift, corruption, and class imbalance, ImageNet / ImageNet-LT, DomainNet, iWildCam, and CIFAR100-C, GMixout consistently improves in-domain accuracy beyond zero-shot performance while surpassing both Model Soups and strong parameter-efficient finetuning baselines under distribution shift.",
    "summary": "",
    "translation": "重新审视Mixout：一条被忽视的通往鲁棒微调的路径",
    "relevance_score": 6,
    "reasoning": "Mixout是一种正则化技术，通过随机混合预训练和微调参数来防止过拟合，这属于Enabling LLM Tech范畴。在推荐系统和搜索领域，这种鲁棒微调方法可应用于LLM的领域适配，提高模型在用户行为数据上的泛化能力，减少对稀疏用户交互的过拟合。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06973v1": {
    "title": "Addressing the ID-Matching Challenge in Long Video Captioning",
    "url": "https://www.alphaxiv.org/abs/2510.06973v1",
    "arxiv_id": "2510.06973v1",
    "authors": "Zhantao Yang, Huangji Wang, Ruili Feng, Han Zhang, Yuting Hu, Shangwen Zhu, Junyan Li, Yu Liu, Fan Cheng",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 12:59:21",
    "ori_summary": "Generating captions for long and complex videos is both critical and challenging, with significant implications for the growing fields of text-to-video generation and multi-modal understanding. One key challenge in long video captioning is accurately recognizing the same individuals who appear in different frames, which we refer to as the ID-Matching problem. Few prior works have focused on this important issue. Those that have, usually suffer from limited generalization and depend on point-wise matching, which limits their overall effectiveness. In this paper, unlike previous approaches, we build upon LVLMs to leverage their powerful priors. We aim to unlock the inherent ID-Matching capabilities within LVLMs themselves to enhance the ID-Matching performance of captions. Specifically, we first introduce a new benchmark for assessing the ID-Matching capabilities of video captions. Using this benchmark, we investigate LVLMs containing GPT-4o, revealing key insights that the performance of ID-Matching can be improved through two methods: 1) enhancing the usage of image information and 2) increasing the quantity of information of individual descriptions. Based on these insights, we propose a novel video captioning method called Recognizing Identities for Captioning Effectively (RICE). Extensive experiments including assessments of caption quality and ID-Matching performance, demonstrate the superiority of our approach. Notably, when implemented on GPT-4o, our RICE improves the precision of ID-Matching from 50% to 90% and improves the recall of ID-Matching from 15% to 80% compared to baseline. RICE makes it possible to continuously track different individuals in the captions of long videos.",
    "summary": "",
    "translation": "解决长视频字幕生成中的ID匹配挑战",
    "relevance_score": 2,
    "reasoning": "该论文专注于视频字幕生成中的ID匹配问题，这属于纯粹的视觉-语言多模态任务。虽然标题提到“长视频”可能涉及序列建模，但核心关注点是视频字幕生成而非推荐、搜索或广告系统。该技术缺乏明确的跨模态推荐或搜索应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06969v1": {
    "title": "Learning Global Representation from Queries for Vectorized HD Map Construction",
    "url": "https://www.alphaxiv.org/abs/2510.06969v1",
    "arxiv_id": "2510.06969v1",
    "authors": "Shoumeng Qiu, Xinrun Li, Yang Long, Xiangyang Xue, Varun Ojha, Jian Pu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 12:56:08",
    "ori_summary": "The online construction of vectorized high-definition (HD) maps is a cornerstone of modern autonomous driving systems. State-of-the-art approaches, particularly those based on the DETR framework, formulate this as an instance detection problem. However, their reliance on independent, learnable object queries results in a predominantly local query perspective, neglecting the inherent global representation within HD maps. In this work, we propose \\textbf{MapGR} (\\textbf{G}lobal \\textbf{R}epresentation learning for HD \\textbf{Map} construction), an architecture designed to learn and utilize a global representations from queries. Our method introduces two synergistic modules: a Global Representation Learning (GRL) module, which encourages the distribution of all queries to better align with the global map through a carefully designed holistic segmentation task, and a Global Representation Guidance (GRG) module, which endows each individual query with explicit, global-level contextual information to facilitate its optimization. Evaluations on the nuScenes and Argoverse2 datasets validate the efficacy of our approach, demonstrating substantial improvements in mean Average Precision (mAP) compared to leading baselines.",
    "summary": "",
    "translation": "基于查询学习全局表示用于矢量化高精地图构建",
    "relevance_score": 2,
    "reasoning": "该论文主要关注高精地图构建的计算机视觉任务，虽然涉及查询学习和全局表示学习，但其应用场景（HD地图）与推荐系统、搜索或广告领域没有直接关联。论文中的技术方法可能对多模态学习有一定启发，但缺乏明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06967v1": {
    "title": "Generating Surface for Text-to-3D using 2D Gaussian Splatting",
    "url": "https://www.alphaxiv.org/abs/2510.06967v1",
    "arxiv_id": "2510.06967v1",
    "authors": "Huanning Dong, Fan Li, Ping Kuang, Jianwen Min",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 12:54:57",
    "ori_summary": "Recent advancements in Text-to-3D modeling have shown significant potential for the creation of 3D content. However, due to the complex geometric shapes of objects in the natural world, generating 3D content remains a challenging task. Current methods either leverage 2D diffusion priors to recover 3D geometry, or train the model directly based on specific 3D representations. In this paper, we propose a novel method named DirectGaussian, which focuses on generating the surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize conditional text generation models and the surface of a 3D object is rendered by 2D Gaussian splatting with multi-view normal and texture priors. For multi-view geometric consistency problems, DirectGaussian incorporates curvature constraints on the generated surface during optimization process. Through extensive experiments, we demonstrate that our framework is capable of achieving diverse and high-fidelity 3D content creation.",
    "summary": "",
    "translation": "使用2D高斯泼溅生成文本到3D的表面",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于3D内容生成和计算机图形学，与推荐系统、搜索或广告的核心技术没有直接关联。虽然文本到3D生成在概念上涉及多模态处理，但该技术主要面向3D视觉和图形应用，没有明确的推荐、搜索或广告应用前景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06955v1": {
    "title": "High-Rate Mixout: Revisiting Mixout for Robust Domain Generalization",
    "url": "https://www.alphaxiv.org/abs/2510.06955v1",
    "arxiv_id": "2510.06955v1",
    "authors": "Masih Aminbeidokhti, Heitor Rapela Medeiros, Eric Granger, Marco Pedersoli",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-08 12:37:56",
    "ori_summary": "Ensembling fine-tuned models initialized from powerful pre-trained weights is a common strategy to improve robustness under distribution shifts, but it comes with substantial computational costs due to the need to train and store multiple models. Dropout offers a lightweight alternative by simulating ensembles through random neuron deactivation; however, when applied to pre-trained models, it tends to over-regularize and disrupt critical representations necessary for generalization. In this work, we investigate Mixout, a stochastic regularization technique that provides an alternative to Dropout for domain generalization. Rather than deactivating neurons, Mixout mitigates overfitting by probabilistically swapping a subset of fine-tuned weights with their pre-trained counterparts during training, thereby maintaining a balance between adaptation and retention of prior knowledge. Our study reveals that achieving strong performance with Mixout on domain generalization benchmarks requires a notably high masking probability of 0.9 for ViTs and 0.8 for ResNets. While this may seem like a simple adjustment, it yields two key advantages for domain generalization: (1) higher masking rates more strongly penalize deviations from the pre-trained parameters, promoting better generalization to unseen domains; and (2) high-rate masking substantially reduces computational overhead, cutting gradient computation by up to 45% and gradient memory usage by up to 90%. Experiments across five domain generalization benchmarks, PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet, using ResNet and ViT architectures, show that our approach, High-rate Mixout, achieves out-of-domain accuracy comparable to ensemble-based methods while significantly reducing training costs.",
    "summary": "",
    "translation": "高比率混合丢弃：重新审视混合丢弃以实现鲁棒的领域泛化",
    "relevance_score": 2,
    "reasoning": "该论文主要关注领域泛化和模型鲁棒性，属于通用机器学习范畴。虽然Mixout是一种正则化技术，但它与Transformer架构效率、注意力机制或LLM核心进展没有直接关联，在推荐系统、搜索或广告中的潜在应用不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06952v1": {
    "title": "OBJVanish: Physically Realizable Text-to-3D Adv. Generation of LiDAR-Invisible Objects",
    "url": "https://www.alphaxiv.org/abs/2510.06952v1",
    "arxiv_id": "2510.06952v1",
    "authors": "Bing Li, Wuqi Wang, Yanan Zhang, Jingzheng Li, Haigen Min, Wei Feng, Xingyu Zhao, Jie Zhang, Qing Guo",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 12:35:35",
    "ori_summary": "LiDAR-based 3D object detectors are fundamental to autonomous driving, where failing to detect objects poses severe safety risks. Developing effective 3D adversarial attacks is essential for thoroughly testing these detection systems and exposing their vulnerabilities before real-world deployment. However, existing adversarial attacks that add optimized perturbations to 3D points have two critical limitations: they rarely cause complete object disappearance and prove difficult to implement in physical environments. We introduce the text-to-3D adversarial generation method, a novel approach enabling physically realizable attacks that can generate 3D models of objects truly invisible to LiDAR detectors and be easily realized in the real world. Specifically, we present the first empirical study that systematically investigates the factors influencing detection vulnerability by manipulating the topology, connectivity, and intensity of individual pedestrian 3D models and combining pedestrians with multiple objects within the CARLA simulation environment. Building on the insights, we propose the physically-informed text-to-3D adversarial generation (Phy3DAdvGen) that systematically optimizes text prompts by iteratively refining verbs, objects, and poses to produce LiDAR-invisible pedestrians. To ensure physical realizability, we construct a comprehensive object pool containing 13 3D models of real objects and constrain Phy3DAdvGen to generate 3D objects based on combinations of objects in this set. Extensive experiments demonstrate that our approach can generate 3D pedestrians that evade six state-of-the-art (SOTA) LiDAR 3D detectors in both CARLA simulation and physical environments, thereby highlighting vulnerabilities in safety-critical applications.",
    "summary": "",
    "translation": "OBJVanish：物理可实现的文本到3D对抗生成激光雷达不可见物体",
    "relevance_score": 1,
    "reasoning": "该论文涉及文本到3D生成和激光雷达不可见物体的对抗生成，属于计算机视觉和3D生成领域。虽然标题提到文本到3D生成，但其核心关注点是物理世界中的对抗生成和激光雷达技术，与推荐系统、搜索或广告的核心技术栈没有直接关联。该研究主要面向自动驾驶和安全领域，而非推荐、搜索或广告的应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06928v1": {
    "title": "IAR2: Improving Autoregressive Visual Generation with Semantic-Detail Associated Token Prediction",
    "url": "https://www.alphaxiv.org/abs/2510.06928v1",
    "arxiv_id": "2510.06928v1",
    "authors": "Ran Yi, Teng Hu, Zihan Su, Lizhuang Ma",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 12:08:21",
    "ori_summary": "Autoregressive models have emerged as a powerful paradigm for visual content creation, but often overlook the intrinsic structural properties of visual data. Our prior work, IAR, initiated a direction to address this by reorganizing the visual codebook based on embedding similarity, thereby improving generation robustness. However, it is constrained by the rigidity of pre-trained codebooks and the inaccuracies of hard, uniform clustering. To overcome these limitations, we propose IAR2, an advanced autoregressive framework that enables a hierarchical semantic-detail synthesis process. At the core of IAR2 is a novel Semantic-Detail Associated Dual Codebook, which decouples image representations into a semantic codebook for global semantic information and a detail codebook for fine-grained refinements. It expands the quantization capacity from a linear to a polynomial scale, significantly enhancing expressiveness. To accommodate this dual representation, we propose a Semantic-Detail Autoregressive Prediction scheme coupled with a Local-Context Enhanced Autoregressive Head, which performs hierarchical prediction-first the semantic token, then the detail token-while leveraging a local context window to enhance spatial coherence. Furthermore, for conditional generation, we introduce a Progressive Attention-Guided Adaptive CFG mechanism that dynamically modulates the guidance scale for each token based on its relevance to the condition and its temporal position in the generation sequence, improving conditional alignment without sacrificing realism. Extensive experiments demonstrate that IAR2 sets a new state-of-the-art for autoregressive image generation, achieving a FID of 1.50 on ImageNet. Our model not only surpasses previous methods in performance but also demonstrates superior computational efficiency, highlighting the effectiveness of our structured, coarse-to-fine generation strategy.",
    "summary": "",
    "translation": "IAR2：通过语义-细节关联的令牌预测改进自回归视觉生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于视觉生成领域的自回归模型改进，属于纯粹的视觉生成技术。虽然涉及自回归架构，但主要针对视觉内容生成而非推荐、搜索或广告场景，与当前关注的RecSys、Search、Ads核心领域以及LLM在其中的应用没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06926v1": {
    "title": "Label-frugal satellite image change detection with generative virtual exemplar learning",
    "url": "https://www.alphaxiv.org/abs/2510.06926v1",
    "arxiv_id": "2510.06926v1",
    "authors": "Hichem Sahbi",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 12:07:35",
    "ori_summary": "Change detection is a major task in remote sensing which consists in finding all the occurrences of changes in multi-temporal satellite or aerial images. The success of existing methods, and particularly deep learning ones, is tributary to the availability of hand-labeled training data that capture the acquisition conditions and the subjectivity of the user (oracle). In this paper, we devise a novel change detection algorithm, based on active learning. The main contribution of our work resides in a new model that measures how important is each unlabeled sample, and provides an oracle with only the most critical samples (also referred to as virtual exemplars) for further labeling. These exemplars are generated, using an invertible graph convnet, as the optimum of an adversarial loss that (i) measures representativity, diversity and ambiguity of the data, and thereby (ii) challenges (the most) the current change detection criteria, leading to a better re-estimate of these criteria in the subsequent iterations of active learning. Extensive experiments show the positive impact of our label-efficient learning model against comparative methods.",
    "summary": "",
    "translation": "基于生成式虚拟范例学习的标签节俭卫星图像变化检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注卫星图像变化检测，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然生成式学习方法在技术上可能有一定先进性，但缺乏明确的在RecSys/Search/Ads领域的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06907v1": {
    "title": "Angular Constraint Embedding via SpherePair Loss for Constrained Clustering",
    "url": "https://www.alphaxiv.org/abs/2510.06907v1",
    "arxiv_id": "2510.06907v1",
    "authors": "Shaojie Zhang, Ke Chen",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-08 11:43:20",
    "ori_summary": "Constrained clustering integrates domain knowledge through pairwise constraints. However, existing deep constrained clustering (DCC) methods are either limited by anchors inherent in end-to-end modeling or struggle with learning discriminative Euclidean embedding, restricting their scalability and real-world applicability. To avoid their respective pitfalls, we propose a novel angular constraint embedding approach for DCC, termed SpherePair. Using the SpherePair loss with a geometric formulation, our method faithfully encodes pairwise constraints and leads to embeddings that are clustering-friendly in angular space, effectively separating representation learning from clustering. SpherePair preserves pairwise relations without conflict, removes the need to specify the exact number of clusters, generalizes to unseen data, enables rapid inference of the number of clusters, and is supported by rigorous theoretical guarantees. Comparative evaluations with state-of-the-art DCC methods on diverse benchmarks, along with empirical validation of theoretical insights, confirm its superior performance, scalability, and overall real-world effectiveness. Code is available at \\href{https://github.com/spherepaircc/SpherePairCC/tree/main}{our repository}.",
    "summary": "",
    "translation": "基于SpherePair损失的角约束嵌入用于约束聚类",
    "relevance_score": 2,
    "reasoning": "该论文主要关注约束聚类中的嵌入方法，属于通用的机器学习技术，与推荐系统、搜索或广告的核心领域进展没有直接关联。虽然嵌入技术可能间接应用于用户表示学习，但论文没有明确展示在RecSys/Search/Ads领域的潜在应用价值，且不属于当前关注的LLM技术、Transformer架构或异构数据建模范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06887v1": {
    "title": "Lung Infection Severity Prediction Using Transformers with Conditional TransMix Augmentation and Cross-Attention",
    "url": "https://www.alphaxiv.org/abs/2510.06887v1",
    "arxiv_id": "2510.06887v1",
    "authors": "Bouthaina Slika, Fadi Dornaika, Fares Bougourzi, Karim Hammoudi",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 11:08:34",
    "ori_summary": "Lung infections, particularly pneumonia, pose serious health risks that can escalate rapidly, especially during pandemics. Accurate AI-based severity prediction from medical imaging is essential to support timely clinical decisions and optimize patient outcomes. In this work, we present a novel method applicable to both CT scans and chest X-rays for assessing lung infection severity. Our contributions are twofold: (i) QCross-Att-PVT, a Transformer-based architecture that integrates parallel encoders, a cross-gated attention mechanism, and a feature aggregator to capture rich multi-scale features; and (ii) Conditional Online TransMix, a custom data augmentation strategy designed to address dataset imbalance by generating mixed-label image patches during training. Evaluated on two benchmark datasets, RALO CXR and Per-COVID-19 CT, our method consistently outperforms several state-of-the-art deep learning models. The results emphasize the critical role of data augmentation and gated attention in improving both robustness and predictive accuracy. This approach offers a reliable, adaptable tool to support clinical diagnosis, disease monitoring, and personalized treatment planning. The source code of this work is available at https://github.com/bouthainas/QCross-Att-PVT.",
    "summary": "",
    "translation": "使用具有条件TransMix增强和交叉注意力的Transformer进行肺部感染严重程度预测",
    "relevance_score": 1,
    "reasoning": "该论文属于医学领域的特定应用，专注于肺部感染严重程度预测，这属于明确的无关主题。虽然使用了Transformer架构，但应用场景与推荐系统、搜索或广告完全无关，且没有证据表明其技术具有跨领域应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06876v1": {
    "title": "HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.06876v1",
    "arxiv_id": "2510.06876v1",
    "authors": "Samir Abou Haidar, Alexandre Chariot, Mehdi Darouich, Cyril Joly, Jean-Emmanuel Deschaud",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-08 10:46:07",
    "ori_summary": "LiDAR semantic segmentation is crucial for autonomous vehicles and mobile robots, requiring high accuracy and real-time processing, especially on resource-constrained embedded systems. Previous state-of-the-art methods often face a trade-off between accuracy and speed. Point-based and sparse convolution-based methods are accurate but slow due to the complexity of neighbor searching and 3D convolutions. Projection-based methods are faster but lose critical geometric information during the 2D projection. Additionally, many recent methods rely on test-time augmentation (TTA) to improve performance, which further slows the inference. Moreover, the pre-processing phase across all methods increases execution time and is demanding on embedded platforms. Therefore, we introduce HARP-NeXt, a high-speed and accurate LiDAR semantic segmentation network. We first propose a novel pre-processing methodology that significantly reduces computational overhead. Then, we design the Conv-SE-NeXt feature extraction block to efficiently capture representations without deep layer stacking per network stage. We also employ a multi-scale range-point fusion backbone that leverages information at multiple abstraction levels to preserve essential geometric details, thereby enhancing accuracy. Experiments on the nuScenes and SemanticKITTI benchmarks show that HARP-NeXt achieves a superior speed-accuracy trade-off compared to all state-of-the-art methods, and, without relying on ensemble models or TTA, is comparable to the top-ranked PTv3, while running 24$\\times$ faster. The code is available at https://github.com/SamirAbouHaidar/HARP-NeXt",
    "summary": "",
    "translation": "HARP-NeXt：用于3D LiDAR语义分割的高速精确范围-点融合网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D LiDAR语义分割，属于纯粹的3D视觉领域，与推荐系统、搜索或广告没有直接关联。论文内容涉及传感器数据处理和3D场景理解，没有显示出在异构数据建模、Transformer架构或LLM应用方面的潜在价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06871v1": {
    "title": "SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models",
    "url": "https://www.alphaxiv.org/abs/2510.06871v1",
    "arxiv_id": "2510.06871v1",
    "authors": "Huahui Yi, Kun Wang, Qiankun Li, Miao Yu, Liang Lin, Gongli Xi, Hao Wu, Xuming Hu, Kang Li, Yang Liu",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-08 10:39:12",
    "ori_summary": "Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal reasoning but often amplify safety risks under adversarial or unsafe prompts, a phenomenon we call the \\textit{Reasoning Tax}. Existing defenses mainly act at the output level and do not constrain the reasoning process, leaving models exposed to implicit risks. In this paper, we propose SaFeR-VLM, a safety-aligned reinforcement learning framework that embeds safety directly into multimodal reasoning. The framework integrates four components: (I) QI-Safe-10K, a curated dataset emphasizing safety-critical and reasoning-sensitive cases; (II) safety-aware rollout, where unsafe generations undergo reflection and correction instead of being discarded; (III) structured reward modeling with multi-dimensional weighted criteria and explicit penalties for hallucinations and contradictions; and (IV) GRPO optimization, which reinforces both safe and corrected trajectories. This unified design shifts safety from a passive safeguard to an active driver of reasoning, enabling scalable and generalizable safety-aware reasoning. SaFeR-VLM further demonstrates robustness against both explicit and implicit risks, supporting dynamic and interpretable safety decisions beyond surface-level filtering. SaFeR-VLM-3B achieves average performance $70.13$ and $78.97$ on safety and helpfulness across six benchmarks, surpassing both same-scale and $>10\\times$ larger models such as Skywork-R1V3-38B, Qwen2.5VL-72B, and GLM4.5V-106B. Remarkably, SaFeR-VLM-7B benefits from its increased scale to surpass GPT-5-mini and Gemini-2.5-Flash by \\num{6.47} and \\num{16.76} points respectively on safety metrics, achieving this improvement without any degradation in helpfulness performance. Our codes are available at https://github.com/HarveyYi/SaFeR-VLM.",
    "summary": "",
    "translation": "SaFeR-VLM：面向多模态模型的安全感知细粒度推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态模型的安全性和细粒度推理能力，属于VLM（视觉语言模型）的安全研究领域。虽然涉及多模态模型，但其核心焦点是安全性（Safety-aware），这属于被明确排除的非技术性话题（安全、伦理等），与推荐系统、搜索或广告的核心技术进展没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06858v1": {
    "title": "Explaining raw data complexity to improve satellite onboard processing",
    "url": "https://www.alphaxiv.org/abs/2510.06858v1",
    "arxiv_id": "2510.06858v1",
    "authors": "Adrien Dorise, Marjorie Bellizzi, Adrien Girard, Benjamin Francesconi, Stéphane May",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 10:26:02",
    "ori_summary": "With increasing processing power, deploying AI models for remote sensing directly onboard satellites is becoming feasible. However, new constraints arise, mainly when using raw, unprocessed sensor data instead of preprocessed ground-based products. While current solutions primarily rely on preprocessed sensor images, few approaches directly leverage raw data. This study investigates the effects of utilising raw data on deep learning models for object detection and classification tasks. We introduce a simulation workflow to generate raw-like products from high-resolution L1 imagery, enabling systemic evaluation. Two object detection models (YOLOv11s and YOLOX-S) are trained on both raw and L1 datasets, and their performance is compared using standard detection metrics and explainability tools. Results indicate that while both models perform similarly at low to medium confidence thresholds, the model trained on raw data struggles with object boundary identification at high confidence levels. It suggests that adapting AI architectures with improved contouring methods can enhance object detection on raw images, improving onboard AI for remote sensing.",
    "summary": "",
    "translation": "解释原始数据复杂性以改进卫星星上处理",
    "relevance_score": 1,
    "reasoning": "该论文专注于卫星数据处理和星上处理优化，属于遥感或航天工程领域。这与推荐系统、搜索或广告的核心关注点完全无关，也不涉及LLM技术、Transformer架构或异构数据建模。该主题属于被排除的物理/领域特定应用类别。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06855v1": {
    "title": "Online Generic Event Boundary Detection",
    "url": "https://www.alphaxiv.org/abs/2510.06855v1",
    "arxiv_id": "2510.06855v1",
    "authors": "Hyungrok Jung, Daneul Kim, Seunggyun Lim, Jeany Son, Jonghyun Choi",
    "categories": "cs.CV, eess.IV",
    "pub_date": "2025-10-08 10:23:45",
    "ori_summary": "Generic Event Boundary Detection (GEBD) aims to interpret long-form videos through the lens of human perception. However, current GEBD methods require processing complete video frames to make predictions, unlike humans processing data online and in real-time. To bridge this gap, we introduce a new task, Online Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries of generic events immediately in streaming videos. This task faces unique challenges of identifying subtle, taxonomy-free event changes in real-time, without the access to future frames. To tackle these challenges, we propose a novel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST) which explains how humans segment ongoing activity into events by leveraging the discrepancies between predicted and actual information. Our framework consists of two key components: the Consistent Event Anticipator (CEA), and the Online Boundary Discriminator (OBD). Specifically, the CEA generates a prediction of the future frame reflecting current event dynamics based solely on prior frames. Then, the OBD measures the prediction error and adaptively adjusts the threshold using statistical tests on past errors to capture diverse, subtle event transitions. Experimental results demonstrate that Estimator outperforms all baselines adapted from recent online video understanding models and achieves performance comparable to prior offline-GEBD methods on the Kinetics-GEBD and TAPOS datasets.",
    "summary": "",
    "translation": "在线通用事件边界检测",
    "relevance_score": 2,
    "reasoning": "该论文关注通用事件边界检测，属于计算机视觉中的时序分析任务，与推荐系统、搜索或广告的核心领域进展没有直接关联。虽然事件检测在视频内容理解中有应用，但论文标题未表明其与用户行为序列建模、多模态推荐或搜索排序等具体应用场景的明确联系，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06842v1": {
    "title": "Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph Regularization",
    "url": "https://www.alphaxiv.org/abs/2510.06842v1",
    "arxiv_id": "2510.06842v1",
    "authors": "Kanglei Zhou, Qingyi Pan, Xingxing Zhang, Hubert P. H. Shum, Frederick W. B. Li, Xiaohui Liang, Liyuan Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 10:09:47",
    "ori_summary": "Action Quality Assessment (AQA) quantifies human actions in videos, supporting applications in sports scoring, rehabilitation, and skill evaluation. A major challenge lies in the non-stationary nature of quality distributions in real-world scenarios, which limits the generalization ability of conventional methods. We introduce Continual AQA (CAQA), which equips AQA with Continual Learning (CL) capabilities to handle evolving distributions while mitigating catastrophic forgetting. Although parameter-efficient fine-tuning of pretrained models has shown promise in CL for image classification, we find it insufficient for CAQA. Our empirical and theoretical analyses reveal two insights: (i) Full-Parameter Fine-Tuning (FPFT) is necessary for effective representation learning; yet (ii) uncontrolled FPFT induces overfitting and feature manifold shift, thereby aggravating forgetting. To address this, we propose Adaptive Manifold-Aligned Graph Regularization (MAGR++), which couples backbone fine-tuning that stabilizes shallow layers while adapting deeper ones with a two-step feature rectification pipeline: a manifold projector to translate deviated historical features into the current representation space, and a graph regularizer to align local and global distributions. We construct four CAQA benchmarks from three datasets with tailored evaluation protocols and strong baselines, enabling systematic cross-dataset comparison. Extensive experiments show that MAGR++ achieves state-of-the-art performance, with average correlation gains of 3.6% offline and 12.2% online over the strongest baseline, confirming its robustness and effectiveness. Our code is available at https://github.com/ZhouKanglei/MAGRPP.",
    "summary": "",
    "translation": "基于自适应流形对齐图正则化的持续动作质量评估",
    "relevance_score": 2,
    "reasoning": "该论文主要关注动作质量评估，属于计算机视觉和运动分析领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然图正则化技术在某些推荐系统中有所应用，但论文专注于动作质量评估这一特定任务，缺乏明确的跨领域应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06829v1": {
    "title": "Lattice-allocated Real-time Line Segment Feature Detection and Tracking Using Only an Event-based Camera",
    "url": "https://www.alphaxiv.org/abs/2510.06829v1",
    "arxiv_id": "2510.06829v1",
    "authors": "Mikihiro Ikura, Arren Glover, Masayoshi Mizuno, Chiara Bartolozzi",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 09:52:35",
    "ori_summary": "Line segment extraction is effective for capturing geometric features of human-made environments. Event-based cameras, which asynchronously respond to contrast changes along edges, enable efficient extraction by reducing redundant data. However, recent methods often rely on additional frame cameras or struggle with high event rates. This research addresses real-time line segment detection and tracking using only a modern, high-resolution (i.e., high event rate) event-based camera. Our lattice-allocated pipeline consists of (i) velocity-invariant event representation, (ii) line segment detection based on a fitting score, (iii) and line segment tracking by perturbating endpoints. Evaluation using ad-hoc recorded dataset and public datasets demonstrates real-time performance and higher accuracy compared to state-of-the-art event-only and event-frame hybrid baselines, enabling fully stand-alone event camera operation in real-world settings.",
    "summary": "",
    "translation": "仅使用事件相机的网格分配实时线段特征检测与跟踪",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于事件相机的计算机视觉技术，涉及线段特征检测和跟踪，属于纯粹的视觉处理范畴。该技术没有展示出在推荐系统、搜索或广告领域的潜在应用价值，与当前关注的LLM技术、推荐系统核心进展或Transformer架构改进完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06827v1": {
    "title": "StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance",
    "url": "https://www.alphaxiv.org/abs/2510.06827v1",
    "arxiv_id": "2510.06827v1",
    "authors": "Jaeseok Jeong, Junho Kim, Gayoung Lee, Yunjey Choi, Youngjung Uh",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 09:50:34",
    "ori_summary": "In the domain of text-to-image generation, diffusion models have emerged as powerful tools. Recently, studies on visual prompting, where images are used as prompts, have enabled more precise control over style and content. However, existing methods often suffer from content leakage, where undesired elements of the visual style prompt are transferred along with the intended style. To address this issue, we 1) extend classifier-free guidance (CFG) to utilize swapping self-attention and propose 2) negative visual query guidance (NVQG) to reduce the transfer of unwanted contents. NVQG employs negative score by intentionally simulating content leakage scenarios that swap queries instead of key and values of self-attention layers from visual style prompts. This simple yet effective method significantly reduces content leakage. Furthermore, we provide careful solutions for using a real image as visual style prompts. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, reflecting the style of the references, and ensuring that resulting images match the text prompts. Our code is available \\href{https://github.com/naver-ai/StyleKeeper}{here}.",
    "summary": "",
    "translation": "StyleKeeper：使用负向视觉查询引导防止内容泄露",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉内容保护和防止泄露，这属于安全/隐私领域，与我的核心关注点（推荐系统、搜索、广告中的技术进展）无关。虽然标题提到“视觉查询”，但防止内容泄露的应用场景主要涉及版权保护、数据安全等非技术性话题，而非推荐或搜索系统的改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06820v1": {
    "title": "Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking",
    "url": "https://www.alphaxiv.org/abs/2510.06820v1",
    "arxiv_id": "2510.06820v1",
    "authors": "Mitchell Keren Taraday, Shahaf Wagner, Chaim Baskin",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-08 09:46:09",
    "ori_summary": "Multimodal retrieval still leans on embedding-based models like CLIP for fast vector search over pre-computed image embeddings. Yet, unlike text retrieval, where joint-encoder rerankers are standard, comparable vision--language rerankers are largely absent. We find that seminal joint encoders such as BLIP are severely bottlenecked by an expensive visual feature-extraction stage, preventing practical deployment at scale. Motivated by this bottleneck, we introduce EDJE, an Efficient Discriminative Joint Encoder that precomputes vision tokens offline and compresses them via a lightweight attention-based adapter, so online inference runs only a compact joint encoder over a small set of visual tokens plus the text. EDJE preserves strong retrieval performance while drastically reducing storage and online compute, enabling high-throughput inference. Specifically, EDJE processes 50k image--text pairs/second while requiring 49kB of disk storage per image, matching prior art on Flickr (zero-shot) and COCO (fine-tuned) retrieval. The implementation and checkpoints will be made publicly available shortly.",
    "summary": "",
    "translation": "面向大规模视觉语言重排序的高效判别式联合编码器",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及VLM类比思想，将视觉和语言作为不同模态进行联合建模，这与处理异构数据（如上下文特征和用户序列）的理念高度相关。高效的联合编码器架构可以应用于搜索和推荐系统中的多模态内容重排序，特别是处理商品图像与文本描述、用户行为序列与上下文特征等异构数据场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06809v1": {
    "title": "VA-Adapter: Adapting Ultrasound Foundation Model to Echocardiography Probe Guidance",
    "url": "https://www.alphaxiv.org/abs/2510.06809v1",
    "arxiv_id": "2510.06809v1",
    "authors": "Teng Wang, Haojun Jiang, Yuxuan Wang, Zhenguo Sun, Shiji Song, Gao Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 09:38:30",
    "ori_summary": "Echocardiography is a critical tool for detecting heart diseases. Recently, ultrasound foundation models have demonstrated remarkable capabilities in cardiac ultrasound image analysis. However, obtaining high-quality ultrasound images is a prerequisite for accurate diagnosis. Due to the exceptionally high operational difficulty of cardiac ultrasound, there is a shortage of highly skilled personnel, which hinders patients from receiving timely examination services. In this paper, we aim to adapt the medical knowledge learned by foundation models from vast datasets to the probe guidance task, which is designed to provide real-time operational recommendations for junior sonographers to acquire high-quality ultrasound images. Moreover, inspired by the practice where experts optimize action decisions based on past explorations, we meticulously design a parameter-efficient Vision-Action Adapter (VA-Adapter) to enable foundation model's image encoder to encode vision-action sequences, thereby enhancing guidance performance. With built-in sequential reasoning capabilities in a compact design, the VA-Adapter enables a pre-trained ultrasound foundation model to learn precise probe adjustment strategies by fine-tuning only a small subset of parameters. Extensive experiments demonstrate that the VA-Adapter can surpass strong probe guidance models. Our code will be released after acceptance.",
    "summary": "",
    "translation": "VA-Adapter：将超声基础模型适配到超声心动图探头引导任务",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学超声成像领域的特定应用，属于医疗影像处理范畴。虽然涉及基础模型适配技术，但其应用场景（超声心动图探头引导）与推荐系统、搜索或广告领域完全无关，属于明确的医疗领域特定应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06802v1": {
    "title": "Capture and Interact: Rapid 3D Object Acquisition and Rendering with Gaussian Splatting in Unity",
    "url": "https://www.alphaxiv.org/abs/2510.06802v1",
    "arxiv_id": "2510.06802v1",
    "authors": "Islomjon Shukhratov, Sergey Gorinsky",
    "categories": "cs.GR, cs.CV",
    "pub_date": "2025-10-08 09:31:29",
    "ori_summary": "Capturing and rendering three-dimensional (3D) objects in real time remain a significant challenge, yet hold substantial potential for applications in augmented reality, digital twin systems, remote collaboration and prototyping. We present an end-to-end pipeline that leverages 3D Gaussian Splatting (3D GS) to enable rapid acquisition and interactive rendering of real-world objects using a mobile device, cloud processing and a local computer. Users scan an object with a smartphone video, upload it for automated 3D reconstruction, and visualize it interactively in Unity at an average of 150 frames per second (fps) on a laptop. The system integrates mobile capture, cloud-based 3D GS and Unity rendering to support real-time telepresence. Our experiments show that the pipeline processes scans in approximately 10 minutes on a graphics processing unit (GPU) achieving real-time rendering on the laptop.",
    "summary": "",
    "translation": "捕获与交互：基于高斯泼溅技术在Unity中实现快速3D物体采集与渲染",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D计算机视觉中的物体采集和渲染技术，使用高斯泼溅方法在Unity引擎中实现。虽然涉及3D数据处理，但缺乏与推荐系统、搜索或广告领域的直接关联，且未展示在异构数据建模或Transformer架构方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06791v1": {
    "title": "Extreme Amodal Face Detection",
    "url": "https://www.alphaxiv.org/abs/2510.06791v1",
    "arxiv_id": "2510.06791v1",
    "authors": "Changlin Song, Yunzhong Hou, Michael Randall Barnes, Rahul Shome, Dylan Campbell",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 09:22:03",
    "ori_summary": "Extreme amodal detection is the task of inferring the 2D location of objects that are not fully visible in the input image but are visible within an expanded field-of-view. This differs from amodal detection, where the object is partially visible within the input image, but is occluded. In this paper, we consider the sub-problem of face detection, since this class provides motivating applications involving safety and privacy, but do not tailor our method specifically to this class. Existing approaches rely on image sequences so that missing detections may be interpolated from surrounding frames or make use of generative models to sample possible completions. In contrast, we consider the single-image task and propose a more efficient, sample-free approach that makes use of the contextual cues from the image to infer the presence of unseen faces. We design a heatmap-based extreme amodal object detector that addresses the problem of efficiently predicting a lot (the out-of-frame region) from a little (the image) with a selective coarse-to-fine decoder. Our method establishes strong results for this new task, even outperforming less efficient generative approaches.",
    "summary": "",
    "translation": "极端非模态人脸检测",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉中的人脸检测任务，特别是处理非模态（amodal）场景，这属于纯粹的视觉领域研究。标题表明该工作与推荐系统、搜索或广告的核心技术没有直接关联，也不涉及LLM技术、Transformer架构进展或异构数据统一建模等当前关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06784v1": {
    "title": "Bionetta: Efficient Client-Side Zero-Knowledge Machine Learning Proving",
    "url": "https://www.alphaxiv.org/abs/2510.06784v1",
    "arxiv_id": "2510.06784v1",
    "authors": "Dmytro Zakharov, Oleksandr Kurbatov, Artem Sdobnov, Lev Soukhanov, Yevhenii Sekhin, Vitalii Volovyk, Mykhailo Velykodnyi, Mark Cherepovskyi, Kyrylo Baibula, Lasha Antadze, Pavlo Kravchenko, Volodymyr Dubinin, Yaroslav Panasenko",
    "categories": "cs.CR, cs.CV",
    "pub_date": "2025-10-08 09:10:32",
    "ori_summary": "In this report, we compare the performance of our UltraGroth-based zero-knowledge machine learning framework Bionetta to other tools of similar purpose such as EZKL, Lagrange's deep-prove, or zkml. The results show a significant boost in the proving time for custom-crafted neural networks: they can be proven even on mobile devices, enabling numerous client-side proving applications. While our scheme increases the cost of one-time preprocessing steps, such as circuit compilation and generating trusted setup, our approach is, to the best of our knowledge, the only one that is deployable on the native EVM smart contracts without overwhelming proof size and verification overheads.",
    "summary": "",
    "translation": "Bionetta：高效的客户端零知识机器学习证明",
    "relevance_score": 1,
    "reasoning": "该论文涉及零知识证明和机器学习，但明确属于被排除的隐私和安全范畴。虽然提到了机器学习，但核心焦点是隐私保护技术，与推荐系统、搜索或广告的核心技术进展无关。该技术没有明显的应用潜力来增强推荐、搜索或广告系统的核心排名或建模能力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06783v1": {
    "title": "TTRV: Test-Time Reinforcement Learning for Vision Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.06783v1",
    "arxiv_id": "2510.06783v1",
    "authors": "Akshit Singh, Shyam Marjit, Wei Lin, Paul Gavrikov, Serena Yeung-Levy, Hilde Kuehne, Rogerio Feris, Sivan Doveh, James Glass, M. Jehanzeb Mirza",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 09:10:31",
    "ori_summary": "Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and dedicated training splits, a setup that contrasts with how humans learn directly from their environment. In this work, we propose TTRV to enhance vision language understanding by adapting the model on the fly at inference time, without the need for any labeled data. Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework by designing rewards based on the frequency of the base model's output, while inferring on each test sample multiple times. Further, we also propose to control the diversity of the model's output by simultaneously rewarding the model for obtaining low entropy of the output empirical distribution. Our approach delivers consistent gains across both object recognition and visual question answering (VQA), with improvements of up to 52.4% and 29.8%, respectively, and average boosts of 24.6% and 10.0% across 16 datasets.Remarkably, on image recognition, TTRV applied to InternVL 8B surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining highly competitive on VQA, demonstrating that test-time reinforcement learning can match or exceed the strongest proprietary models. Finally, we find many interesting properties of test-time RL for VLMs: for example, even in extremely data-constrained scenarios, where adaptation is performed on a single randomly chosen unlabeled test example, TTRV still yields non-trivial improvements of up to 5.5% in recognition tasks.",
    "summary": "",
    "translation": "TTRV：视觉语言模型的测试时强化学习",
    "relevance_score": 2,
    "reasoning": "虽然论文涉及视觉语言模型（VLM）和强化学习，但主要关注测试时优化和纯粹的VLM技术，与推荐系统、搜索或广告的核心领域缺乏直接关联。强化学习部分没有明确展示在推荐/搜索/广告场景中的应用潜力，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06769v1": {
    "title": "A deep multiple instance learning approach based on coarse labels for high-resolution land-cover mapping",
    "url": "https://www.alphaxiv.org/abs/2510.06769v1",
    "arxiv_id": "2510.06769v1",
    "authors": "Gianmarco Perantoni, Lorenzo Bruzzone",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 08:50:39",
    "ori_summary": "The quantity and the quality of the training labels are central problems in high-resolution land-cover mapping with machine-learning-based solutions. In this context, weak labels can be gathered in large quantities by leveraging on existing low-resolution or obsolete products. In this paper, we address the problem of training land-cover classifiers using high-resolution imagery (e.g., Sentinel-2) and weak low-resolution reference data (e.g., MODIS -derived land-cover maps). Inspired by recent works in Deep Multiple Instance Learning (DMIL), we propose a method that trains pixel-level multi-class classifiers and predicts low-resolution labels (i.e., patch-level classification), where the actual high-resolution labels are learned implicitly without direct supervision. This is achieved with flexible pooling layers that are able to link the semantics of the pixels in the high-resolution imagery to the low-resolution reference labels. Then, the Multiple Instance Learning (MIL) problem is re-framed in a multi-class and in a multi-label setting. In the former, the low-resolution annotation represents the majority of the pixels in the patch. In the latter, the annotation only provides us information on the presence of one of the land-cover classes in the patch and thus multiple labels can be considered valid for a patch at a time, whereas the low-resolution labels provide us only one label. Therefore, the classifier is trained with a Positive-Unlabeled Learning (PUL) strategy. Experimental results on the 2020 IEEE GRSS Data Fusion Contest dataset show the effectiveness of the proposed framework compared to standard training strategies.",
    "summary": "",
    "translation": "基于粗粒度标签的高分辨率土地覆盖制图深度多示例学习方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于遥感图像的土地覆盖制图，属于计算机视觉在特定领域（地理信息）的应用。论文的核心技术（多示例学习）和问题领域（土地覆盖映射）与推荐系统、搜索或广告没有直接关联，也不涉及LLM、Transformer架构或异构数据统一建模等关键技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06757v1": {
    "title": "Transforming Noise Distributions with Histogram Matching: Towards a Single Denoiser for All",
    "url": "https://www.alphaxiv.org/abs/2510.06757v1",
    "arxiv_id": "2510.06757v1",
    "authors": "Sheng Fu, Junchao Zhang, Kailun Yang",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 08:34:50",
    "ori_summary": "Supervised Gaussian denoisers exhibit limited generalization when confronted with out-of-distribution noise, due to the diverse distributional characteristics of different noise types. To bridge this gap, we propose a histogram matching approach that transforms arbitrary noise towards a target Gaussian distribution with known intensity. Moreover, a mutually reinforcing cycle is established between noise transformation and subsequent denoising. This cycle progressively refines the noise to be converted, making it approximate the real noise, thereby enhancing the noise transformation effect and further improving the denoising performance. We tackle specific noise complexities: local histogram matching handles signal-dependent noise, intrapatch permutation processes channel-related noise, and frequency-domain histogram matching coupled with pixel-shuffle down-sampling breaks spatial correlation. By applying these transformations, a single Gaussian denoiser gains remarkable capability to handle various out-of-distribution noises, including synthetic noises such as Poisson, salt-and-pepper and repeating pattern noises, as well as complex real-world noises. Extensive experiments demonstrate the superior generalization and effectiveness of our method.",
    "summary": "",
    "translation": "通过直方图匹配转换噪声分布：迈向单一去噪器适用于所有场景",
    "relevance_score": 2,
    "reasoning": "该论文关注图像去噪中的直方图匹配技术，属于计算机视觉领域。虽然去噪技术在某些数据预处理场景中可能有间接应用，但该工作主要针对图像噪声分布转换，与推荐系统、搜索或广告的核心技术栈没有直接关联，且未涉及LLM、Transformer架构或异构数据建模等关键焦点领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06754v1": {
    "title": "UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene",
    "url": "https://www.alphaxiv.org/abs/2510.06754v1",
    "arxiv_id": "2510.06754v1",
    "authors": "Christian Maurer, Snehal Jauhri, Sophie Lueth, Georgia Chalvatzaki",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-08 08:30:26",
    "ori_summary": "Comprehensive visual, geometric, and semantic understanding of a 3D scene is crucial for successful execution of robotic tasks, especially in unstructured and complex environments. Additionally, to make robust decisions, it is necessary for the robot to evaluate the reliability of perceived information. While recent advances in 3D neural feature fields have enabled robots to leverage features from pretrained foundation models for tasks such as language-guided manipulation and navigation, existing methods suffer from two critical limitations: (i) they are typically scene-specific, and (ii) they lack the ability to model uncertainty in their predictions. We present UniFField, a unified uncertainty-aware neural feature field that combines visual, semantic, and geometric features in a single generalizable representation while also predicting uncertainty in each modality. Our approach, which can be applied zero shot to any new environment, incrementally integrates RGB-D images into our voxel-based feature representation as the robot explores the scene, simultaneously updating uncertainty estimation. We evaluate our uncertainty estimations to accurately describe the model prediction errors in scene reconstruction and semantic feature prediction. Furthermore, we successfully leverage our feature predictions and their respective uncertainty for an active object search task using a mobile manipulator robot, demonstrating the capability for robust decision-making.",
    "summary": "",
    "translation": "UniFField：一种可泛化的统一神经特征场，用于处理任意场景中的视觉、语义和空间不确定性",
    "relevance_score": 3,
    "reasoning": "该论文提出了一个统一的神经特征场来处理多模态不确定性，这与VLM类比处理异构数据的思路有一定相关性。然而，该方法主要针对视觉场景理解，没有明确展示在推荐系统、搜索或广告中的直接应用潜力，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06751v1": {
    "title": "OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot",
    "url": "https://www.alphaxiv.org/abs/2510.06751v1",
    "arxiv_id": "2510.06751v1",
    "authors": "Junhan Zhu, Hesong Wang, Mingluo Su, Zefang Wang, Huan Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 08:19:15",
    "ori_summary": "Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computational cost. Existing one-shot network pruning methods can hardly be directly applied to them due to the iterative denoising nature of diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel one-shot pruning framework that enables accurate and training-free compression of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex architectures of modern diffusion models and supporting diverse pruning granularity, including unstructured, N:M semi-structured, and structured (MHA heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the iterative dynamics of the diffusion process, by examining the problem from an error-accumulation perspective, we propose a novel timestep-aware Hessian construction that incorporates a logarithmic-decrease weighting scheme, assigning greater importance to earlier timesteps to mitigate potential error accumulation; (iii) Furthermore, a computationally efficient group-wise sequential pruning strategy is proposed to amortize the expensive calibration process. Extensive experiments show that OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering inference acceleration with minimal degradation in visual quality.",
    "summary": "",
    "translation": "OBS-Diff：一次性精确剪枝扩散模型",
    "relevance_score": 2,
    "reasoning": "该论文专注于扩散模型的模型压缩技术，属于通用的模型优化方法，与推荐系统、搜索或广告的核心技术领域没有直接关联。虽然高效的模型架构可能间接影响部署，但论文没有明确展示在推荐/搜索/广告领域的应用潜力，且不属于核心的Transformer架构或LLM技术进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06746v1": {
    "title": "DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement for Image Deraining",
    "url": "https://www.alphaxiv.org/abs/2510.06746v1",
    "arxiv_id": "2510.06746v1",
    "authors": "Zhiliang Zhu, Tao Zeng, Tao Yang, Guoliang Luo, Jiyong Zeng",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 08:05:11",
    "ori_summary": "Image deraining is crucial for improving visual quality and supporting reliable downstream vision tasks. Although Mamba-based models provide efficient sequence modeling, their limited ability to capture fine-grained details and lack of frequency-domain awareness restrict further improvements. To address these issues, we propose DeRainMamba, which integrates a Frequency-Aware State-Space Module (FASSM) and Multi-Directional Perception Convolution (MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from high-frequency image details, balancing rain removal and detail preservation. MDPConv further restores local structures by capturing anisotropic gradient features and efficiently fusing multiple convolution branches. Extensive experiments on four public benchmarks demonstrate that DeRainMamba consistently outperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer parameters and lower computational costs. These results validate the effectiveness of combining frequency-domain modeling and spatial detail enhancement within a state-space framework for single image deraining.",
    "summary": "",
    "translation": "DeRainMamba：一种用于图像去雨的频率感知状态空间模型与细节增强方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的图像去雨任务，使用状态空间模型处理视觉退化问题。这与推荐系统、搜索或广告的核心技术领域没有直接关联，也不涉及LLM技术、Transformer架构改进或异构数据统一建模等关注方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06694v1": {
    "title": "SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis",
    "url": "https://www.alphaxiv.org/abs/2510.06694v1",
    "arxiv_id": "2510.06694v1",
    "authors": "Jipeng Lyu, Jiahua Dong, Yu-Xiong Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 06:39:33",
    "ori_summary": "Persistent dynamic scene modeling for tracking and novel-view synthesis remains challenging due to the difficulty of capturing accurate deformations while maintaining computational efficiency. We propose SCas4D, a cascaded optimization framework that leverages structural patterns in 3D Gaussian Splatting for dynamic scenes. The key idea is that real-world deformations often exhibit hierarchical patterns, where groups of Gaussians share similar transformations. By progressively refining deformations from coarse part-level to fine point-level, SCas4D achieves convergence within 100 iterations per time frame and produces results comparable to existing methods with only one-twentieth of the training iterations. The approach also demonstrates effectiveness in self-supervised articulated object segmentation, novel view synthesis, and dense point tracking tasks.",
    "summary": "",
    "translation": "SCas4D：用于提升持久性4D新视角合成的结构级联优化",
    "relevance_score": 1,
    "reasoning": "该论文专注于4D新视角合成和结构优化，属于计算机视觉和图形学领域。虽然标题提到'优化'，但这与推荐系统、搜索或广告的核心技术无关，也不涉及LLM、Transformer架构或异构数据建模。该工作主要面向视觉内容生成和3D场景重建，属于明确的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06687v1": {
    "title": "Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion",
    "url": "https://www.alphaxiv.org/abs/2510.06687v1",
    "arxiv_id": "2510.06687v1",
    "authors": "Jie Luo, Yuxuan Jiang, Xin Jin, Mingyu Liu, Yihui Fan",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 06:15:06",
    "ori_summary": "Semantic segmentation serves as a cornerstone of scene understanding in autonomous driving but continues to face significant challenges under complex conditions such as occlusion. Light field and LiDAR modalities provide complementary visual and spatial cues that are beneficial for robust perception; however, their effective integration is hindered by limited viewpoint diversity and inherent modality discrepancies. To address these challenges, the first multimodal semantic segmentation dataset integrating light field data and point cloud data is proposed. Based on this dataset, we proposed a multi-modal light field point-cloud fusion segmentation network(Mlpfseg), incorporating feature completion and depth perception to segment both camera images and LiDAR point clouds simultaneously. The feature completion module addresses the density mismatch between point clouds and image pixels by performing differential reconstruction of point-cloud feature maps, enhancing the fusion of these modalities. The depth perception module improves the segmentation of occluded objects by reinforcing attention scores for better occlusion awareness. Our method outperforms image-only segmentation by 1.71 Mean Intersection over Union(mIoU) and point cloud-only segmentation by 2.38 mIoU, demonstrating its effectiveness.",
    "summary": "",
    "translation": "基于光场与激光雷达融合的语义分割算法",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的语义分割技术，结合光场和激光雷达两种传感器数据。虽然语义分割在自动驾驶等领域有应用，但论文标题未显示与推荐系统、搜索或广告的直接关联，也未提及任何Transformer架构、LLM技术或异构数据建模等核心关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06679v1": {
    "title": "DreamOmni2: Multimodal Instruction-based Editing and Generation",
    "url": "https://www.alphaxiv.org/abs/2510.06679v1",
    "arxiv_id": "2510.06679v1",
    "authors": "Bin Xia, Bohao Peng, Yuechen Zhang, Junjia Huang, Jiyang Liu, Jingyao Li, Haoru Tan, Sitong Wu, Chengyao Wang, Yitong Wang, Xinglong Wu, Bei Yu, Jiaya Jia",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 06:07:14",
    "ori_summary": "Recent advancements in instruction-based image editing and subject-driven generation have garnered significant attention, yet both tasks still face limitations in meeting practical user needs. Instruction-based editing relies solely on language instructions, which often fail to capture specific editing details, making reference images necessary. Meanwhile, subject-driven generation is limited to combining concrete objects or people, overlooking broader, abstract concepts. To address these challenges, we propose two novel tasks: multimodal instruction-based editing and generation. These tasks support both text and image instructions and extend the scope to include both concrete and abstract concepts, greatly enhancing their practical applications. We introduce DreamOmni2, tackling two primary challenges: data creation and model framework design. Our data synthesis pipeline consists of three steps: (1) using a feature mixing method to create extraction data for both abstract and concrete concepts, (2) generating multimodal instruction-based editing training data using the editing and extraction models, and (3) further applying the extraction model to create training data for multimodal instruction-based editing. For the framework, to handle multi-image input, we propose an index encoding and position encoding shift scheme, which helps the model distinguish images and avoid pixel confusion. Additionally, we introduce joint training with the VLM and our generation/editing model to better process complex instructions. In addition, we have proposed comprehensive benchmarks for these two new tasks to drive their development. Experiments show that DreamOmni2 has achieved impressive results. Models and codes will be released.",
    "summary": "",
    "translation": "DreamOmni2：基于多模态指令的编辑与生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态内容生成和编辑，属于纯粹的AIGC领域，与我的核心关注点（推荐系统、搜索、广告中的排名和建模技术）没有直接关联。虽然多模态技术可能在某些边缘场景中有潜在应用，但该论文标题明确指向内容生成任务，这属于明确排除的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06673v1": {
    "title": "Heptapod: Language Modeling on Visual Signals",
    "url": "https://www.alphaxiv.org/abs/2510.06673v1",
    "arxiv_id": "2510.06673v1",
    "authors": "Yongxin Zhu, Jiawei Chen, Yuanzhe Chen, Zhuo Chen, Dongya Jia, Jian Cong, Xiaobin Zhuang, Yuping Wang, Yuxuan Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 05:54:46",
    "ori_summary": "We introduce Heptapod, an image autoregressive model that adheres to the foundational principles of language modeling. Heptapod employs \\textbf{causal attention}, \\textbf{eliminates reliance on CFG}, and \\textbf{eschews the trend of semantic tokenizers}. Our key innovation is \\textit{next 2D distribution prediction}: a causal Transformer with reconstruction-focused visual tokenizer, learns to predict the distribution over the entire 2D spatial grid of images at each timestep. This learning objective unifies the sequential modeling of autoregressive framework with the holistic self-supervised learning of masked autoencoding, enabling the model to capture comprehensive image semantics via generative training. On the ImageNet generation benchmark, Heptapod achieves an FID of $2.70$, significantly outperforming previous causal autoregressive approaches. We hope our work inspires a principled rethinking of language modeling on visual signals and beyond.",
    "summary": "",
    "translation": "Heptapod：基于视觉信号的语言建模",
    "relevance_score": 3,
    "reasoning": "该论文涉及视觉信号的语言建模，与视觉语言模型（VLM）相关，这属于'VLM类比处理异构数据'的范畴。然而，标题没有明确说明该方法如何应用于推荐系统、搜索或广告中的异构数据建模，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06669v1": {
    "title": "Automated Neural Architecture Design for Industrial Defect Detection",
    "url": "https://www.alphaxiv.org/abs/2510.06669v1",
    "arxiv_id": "2510.06669v1",
    "authors": "Yuxi Liu, Yunfeng Ma, Yi Tang, Min Liu, Shuai Jiang, Yaonan Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 05:37:59",
    "ori_summary": "Industrial surface defect detection (SDD) is critical for ensuring product quality and manufacturing reliability. Due to the diverse shapes and sizes of surface defects, SDD faces two main challenges: intraclass difference and interclass similarity. Existing methods primarily utilize manually designed models, which require extensive trial and error and often struggle to address both challenges effectively. To overcome this, we propose AutoNAD, an automated neural architecture design framework for SDD that jointly searches over convolutions, transformers, and multi-layer perceptrons. This hybrid design enables the model to capture both fine-grained local variations and long-range semantic context, addressing the two key challenges while reducing the cost of manual network design. To support efficient training of such a diverse search space, AutoNAD introduces a cross weight sharing strategy, which accelerates supernet convergence and improves subnet performance. Additionally, a searchable multi-level feature aggregation module (MFAM) is integrated to enhance multi-scale feature learning. Beyond detection accuracy, runtime efficiency is essential for industrial deployment. To this end, AutoNAD incorporates a latency-aware prior to guide the selection of efficient architectures. The effectiveness of AutoNAD is validated on three industrial defect datasets and further applied within a defect imaging and detection platform. Code will be available at https://github.com/Yuxi104/AutoNAD.",
    "summary": "",
    "translation": "面向工业缺陷检测的自动化神经架构设计",
    "relevance_score": 1,
    "reasoning": "该论文专注于工业缺陷检测这一特定视觉应用领域，与推荐系统、搜索或广告的核心关注点无关。虽然涉及神经架构设计，但属于计算机视觉中的特定应用场景，且明确属于被排除的'Purely Vision'类别，没有任何与RecSys/Search/Ads相关的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06646v1": {
    "title": "The False Promise of Zero-Shot Super-Resolution in Machine-Learned Operators",
    "url": "https://www.alphaxiv.org/abs/2510.06646v1",
    "arxiv_id": "2510.06646v1",
    "authors": "Mansi Sakarvadia, Kareem Hegazy, Amin Totounferoush, Kyle Chard, Yaoqing Yang, Ian Foster, Michael W. Mahoney",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-08 04:59:56",
    "ori_summary": "A core challenge in scientific machine learning, and scientific computing more generally, is modeling continuous phenomena which (in practice) are represented discretely. Machine-learned operators (MLOs) have been introduced as a means to achieve this modeling goal, as this class of architecture can perform inference at arbitrary resolution. In this work, we evaluate whether this architectural innovation is sufficient to perform \"zero-shot super-resolution,\" namely to enable a model to serve inference on higher-resolution data than that on which it was originally trained. We comprehensively evaluate both zero-shot sub-resolution and super-resolution (i.e., multi-resolution) inference in MLOs. We decouple multi-resolution inference into two key behaviors: 1) extrapolation to varying frequency information; and 2) interpolating across varying resolutions. We empirically demonstrate that MLOs fail to do both of these tasks in a zero-shot manner. Consequently, we find MLOs are not able to perform accurate inference at resolutions different from those on which they were trained, and instead they are brittle and susceptible to aliasing. To address these failure modes, we propose a simple, computationally-efficient, and data-driven multi-resolution training protocol that overcomes aliasing and that provides robust multi-resolution generalization.",
    "summary": "",
    "translation": "机器学习算子中零样本超分辨率的虚假承诺",
    "relevance_score": 2,
    "reasoning": "该论文主要讨论机器学习算子中的零样本超分辨率问题，这属于计算机视觉领域的特定技术应用。虽然超分辨率技术理论上可能用于广告或推荐系统中的图像质量提升，但论文标题明确指向技术局限性和虚假承诺，缺乏与推荐系统、搜索或广告排名的直接关联，且未涉及Transformer架构、LLM技术或异构数据建模等核心关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06638v1": {
    "title": "StaR-KVQA: Structured Reasoning Traces for Implicit-Knowledge Visual Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.06638v1",
    "arxiv_id": "2510.06638v1",
    "authors": "Zhihao Wen, Wenkang Wei, Yuan Fang, Xingtong Yu, Hui Zhang, Weicheng Zhu, Xin Zhang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 04:37:53",
    "ori_summary": "Knowledge-based Visual Question Answering (KVQA) requires models to ground entities in images and reason over factual knowledge. We study its implicit-knowledge variant, IK-KVQA, where a multimodal large language model (MLLM) is the sole knowledge source, without external retrieval. Yet, MLLMs lack explicit reasoning supervision and produce inconsistent justifications, and generalize poorly after standard supervised fine-tuning (SFT). We present StaR-KVQA (Structured Reasoning Traces for IK-KVQA), which supervises structured traces - dual symbolic relation paths plus path-grounded natural-language explanations - so that reasoning becomes transparent and verifiable. With one open-source MLLM, StaR-KVQA constructs and selects path-grounded reasoning traces to form a trace-enriched dataset, then fine-tunes via structured self-distillation to align generation with supervision; no external retrievers, verifiers, or curated knowledge bases (KBs) are used, traces are built offline, and inference is a single autoregressive pass. Across benchmarks, StaR-KVQA improves both accuracy and interpretability, achieving up to +11.3% higher answer accuracy on OK-VQA over the strongest baseline while exhibiting robust cross-domain generalization.",
    "summary": "",
    "translation": "StaR-KVQA：面向隐式知识视觉问答的结构化推理轨迹",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉问答中的结构化推理轨迹，属于视觉语言模型领域。虽然VLM技术可能对处理异构数据有启发，但该工作聚焦于纯粹的视觉问答任务，与推荐系统、搜索或广告的直接关联性较弱，且未明确展示在这些领域的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06637v1": {
    "title": "Control-Augmented Autoregressive Diffusion for Data Assimilation",
    "url": "https://www.alphaxiv.org/abs/2510.06637v1",
    "arxiv_id": "2510.06637v1",
    "authors": "Prakhar Srivastava, Farrin Marouf Sofian, Francesco Immorlano, Kushagra Pandey, Stephan Mandt",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-08 04:37:32",
    "ori_summary": "Despite recent advances in test-time scaling and finetuning of diffusion models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains underexplored. We introduce an amortized framework that augments pretrained ARDMs with a lightweight controller network, trained offline by previewing future ARDM rollouts and learning stepwise controls that anticipate upcoming observations under a terminal cost objective. We evaluate this framework in the context of data assimilation (DA) for chaotic spatiotemporal partial differential equations (PDEs), a setting where existing methods are often computationally prohibitive and prone to forecast drift under sparse observations. Our approach reduces DA inference to a single forward rollout with on-the-fly corrections, avoiding expensive adjoint computations and/or optimizations during inference. We demonstrate that our method consistently outperforms four state-of-the-art baselines in stability, accuracy, and physical fidelity across two canonical PDEs and six observation regimes. We will release code and checkpoints publicly.",
    "summary": "",
    "translation": "用于数据同化的控制增强自回归扩散模型",
    "relevance_score": 3,
    "reasoning": "该论文提出了控制增强自回归扩散方法用于数据同化，这属于生成模型的技术进步。虽然扩散模型在推荐系统中可用于生成用户行为序列或内容表示，但数据同化主要应用于物理科学和天气预报领域，与推荐/搜索/广告的直接关联较弱。该方法可能通过改进序列生成质量间接应用于用户行为建模，但应用路径不够明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06635v1": {
    "title": "StruSR: Structure-Aware Symbolic Regression with Physics-Informed Taylor Guidance",
    "url": "https://www.alphaxiv.org/abs/2510.06635v1",
    "arxiv_id": "2510.06635v1",
    "authors": "Yunpeng Gong, Sihan Lan, Can Yang, Kunpeng Xu, Min Jiang",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-08 04:37:04",
    "ori_summary": "Symbolic regression aims to find interpretable analytical expressions by searching over mathematical formula spaces to capture underlying system behavior, particularly in scientific modeling governed by physical laws. However, traditional methods lack mechanisms for extracting structured physical priors from time series observations, making it difficult to capture symbolic expressions that reflect the system's global behavior. In this work, we propose a structure-aware symbolic regression framework, called StruSR, that leverages trained Physics-Informed Neural Networks (PINNs) to extract locally structured physical priors from time series data. By performing local Taylor expansions on the outputs of the trained PINN, we obtain derivative-based structural information to guide symbolic expression evolution. To assess the importance of expression components, we introduce a masking-based attribution mechanism that quantifies each subtree's contribution to structural alignment and physical residual reduction. These sensitivity scores steer mutation and crossover operations within genetic programming, preserving substructures with high physical or structural significance while selectively modifying less informative components. A hybrid fitness function jointly minimizes physics residuals and Taylor coefficient mismatch, ensuring consistency with both the governing equations and the local analytical behavior encoded by the PINN. Experiments on benchmark PDE systems demonstrate that StruSR improves convergence speed, structural fidelity, and expression interpretability compared to conventional baselines, offering a principled paradigm for physics-grounded symbolic discovery.",
    "summary": "",
    "translation": "StruSR：具有物理信息泰勒引导的结构感知符号回归",
    "relevance_score": 2,
    "reasoning": "该论文主要关注符号回归和物理信息引导，属于通用机器学习方法而非特定于推荐系统、搜索或广告领域。虽然结构感知建模在某些场景下可能有间接应用，但论文标题明确指向物理系统建模，与我的核心关注点（RecSys/Search/Ads的进展、LLM技术、Transformer架构或异构数据统一建模）缺乏直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06629v1": {
    "title": "Unsupervised Backdoor Detection and Mitigation for Spiking Neural Networks",
    "url": "https://www.alphaxiv.org/abs/2510.06629v1",
    "arxiv_id": "2510.06629v1",
    "authors": "Jiachen Li, Bang Wu, Xiaoyu Xia, Xiaoning Liu, Xun Yi, Xiuzhen Zhang",
    "categories": "cs.CR, cs.CV, cs.LG",
    "pub_date": "2025-10-08 04:25:35",
    "ori_summary": "Spiking Neural Networks (SNNs) have gained increasing attention for their superior energy efficiency compared to Artificial Neural Networks (ANNs). However, their security aspects, particularly under backdoor attacks, have received limited attention. Existing defense methods developed for ANNs perform poorly or can be easily bypassed in SNNs due to their event-driven and temporal dependencies. This paper identifies the key blockers that hinder traditional backdoor defenses in SNNs and proposes an unsupervised post-training detection framework, Temporal Membrane Potential Backdoor Detection (TMPBD), to overcome these challenges. TMPBD leverages the maximum margin statistics of temporal membrane potential (TMP) in the final spiking layer to detect target labels without any attack knowledge or data access. We further introduce a robust mitigation mechanism, Neural Dendrites Suppression Backdoor Mitigation (NDSBM), which clamps dendritic connections between early convolutional layers to suppress malicious neurons while preserving benign behaviors, guided by TMP extracted from a small, clean, unlabeled dataset. Extensive experiments on multiple neuromorphic benchmarks and state-of-the-art input-aware dynamic trigger attacks demonstrate that TMPBD achieves 100% detection accuracy, while NDSBM reduces the attack success rate from 100% to 8.44%, and to 2.81% when combined with detection, without degrading clean accuracy.",
    "summary": "",
    "translation": "脉冲神经网络的非监督后门检测与缓解",
    "relevance_score": 1,
    "reasoning": "该论文专注于脉冲神经网络（SNN）的安全问题，属于网络安全和模型安全领域，与推荐系统、搜索或广告的核心技术进展无关。虽然涉及模型检测，但其针对的是脉冲神经网络这种特定架构的安全漏洞，在推荐、搜索或广告系统中没有明显的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06621v1": {
    "title": "FEAorta: A Fully Automated Framework for Finite Element Analysis of the Aorta From 3D CT Images",
    "url": "https://www.alphaxiv.org/abs/2510.06621v1",
    "arxiv_id": "2510.06621v1",
    "authors": "Jiasong Chen, Linchen Qian, Ruonan Gong, Christina Sun, Tongran Qin, Thuy Pham, Caitlin Martin, Mohammad Zafar, John Elefteriades, Wei Sun, Liang Liang",
    "categories": "eess.IV, cs.CE, cs.CV, cs.LG",
    "pub_date": "2025-10-08 04:00:46",
    "ori_summary": "Aortic aneurysm disease ranks consistently in the top 20 causes of death in the U.S. population. Thoracic aortic aneurysm is manifested as an abnormal bulging of thoracic aortic wall and it is a leading cause of death in adults. From the perspective of biomechanics, rupture occurs when the stress acting on the aortic wall exceeds the wall strength. Wall stress distribution can be obtained by computational biomechanical analyses, especially structural Finite Element Analysis. For risk assessment, probabilistic rupture risk of TAA can be calculated by comparing stress with material strength using a material failure model. Although these engineering tools are currently available for TAA rupture risk assessment on patient specific level, clinical adoption has been limited due to two major barriers: labor intensive 3D reconstruction current patient specific anatomical modeling still relies on manual segmentation, making it time consuming and difficult to scale to a large patient population, and computational burden traditional FEA simulations are resource intensive and incompatible with time sensitive clinical workflows. The second barrier was successfully overcome by our team through the development of the PyTorch FEA library and the FEA DNN integration framework. By incorporating the FEA functionalities within PyTorch FEA and applying the principle of static determinacy, we reduced the FEA based stress computation time to approximately three minutes per case. Moreover, by integrating DNN and FEA through the PyTorch FEA library, our approach further decreases the computation time to only a few seconds per case. This work focuses on overcoming the first barrier through the development of an end to end deep neural network capable of generating patient specific finite element meshes of the aorta directly from 3D CT images.",
    "summary": "",
    "translation": "FEAorta：基于3D CT图像的主动脉有限元分析全自动框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像处理和生物力学分析，属于医疗领域特定应用。标题中提到的3D CT图像、主动脉有限元分析等均与推荐系统、搜索、广告或LLM技术完全无关，不涉及任何异构数据建模、Transformer架构改进或推荐系统核心进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06619v1": {
    "title": "MSITrack: A Challenging Benchmark for Multispectral Single Object Tracking",
    "url": "https://www.alphaxiv.org/abs/2510.06619v1",
    "arxiv_id": "2510.06619v1",
    "authors": "Tao Feng, Tingfa Xu, Haolin Qin, Tianhao Li, Shuaihao Han, Xuyang Zou, Zhan Lv, Jianan Li",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 03:56:36",
    "ori_summary": "Visual object tracking in real-world scenarios presents numerous challenges including occlusion, interference from similar objects and complex backgrounds-all of which limit the effectiveness of RGB-based trackers. Multispectral imagery, which captures pixel-level spectral reflectance, enhances target discriminability. However, the availability of multispectral tracking datasets remains limited. To bridge this gap, we introduce MSITrack, the largest and most diverse multispectral single object tracking dataset to date. MSITrack offers the following key features: (i) More Challenging Attributes-including interference from similar objects and similarity in color and texture between targets and backgrounds in natural scenarios, along with a wide range of real-world tracking challenges; (ii) Richer and More Natural Scenes-spanning 55 object categories and 300 distinct natural scenes, MSITrack far exceeds the scope of existing benchmarks. Many of these scenes and categories are introduced to the multispectral tracking domain for the first time; (iii) Larger Scale-300 videos comprising over 129k frames of multispectral imagery. To ensure annotation precision, each frame has undergone meticulous processing, manual labeling and multi-stage verification. Extensive evaluations using representative trackers demonstrate that the multispectral data in MSITrack significantly improves performance over RGB-only baselines, highlighting its potential to drive future advancements in the field. The MSITrack dataset is publicly available at: https://github.com/Fengtao191/MSITrack.",
    "summary": "",
    "translation": "MSITrack：一个用于多光谱单目标跟踪的挑战性基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的多光谱目标跟踪基准，属于纯粹的视觉研究方向。虽然多模态学习在推荐系统中有所应用，但该论文明确聚焦于单目标跟踪这一特定视觉任务，与推荐系统、搜索或广告的核心技术没有直接关联，也不涉及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06612v1": {
    "title": "A Bridge from Audio to Video: Phoneme-Viseme Alignment Allows Every Face to Speak Multiple Languages",
    "url": "https://www.alphaxiv.org/abs/2510.06612v1",
    "arxiv_id": "2510.06612v1",
    "authors": "Zibo Su, Kun Wei, Jiahua Li, Xu Yang, Cheng Deng",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 03:46:39",
    "ori_summary": "Speech-driven talking face synthesis (TFS) focuses on generating lifelike facial animations from audio input. Current TFS models perform well in English but unsatisfactorily in non-English languages, producing wrong mouth shapes and rigid facial expressions. The terrible performance is caused by the English-dominated training datasets and the lack of cross-language generalization abilities. Thus, we propose Multilingual Experts (MuEx), a novel framework featuring a Phoneme-Guided Mixture-of-Experts (PG-MoE) architecture that employs phonemes and visemes as universal intermediaries to bridge audio and video modalities, achieving lifelike multilingual TFS. To alleviate the influence of linguistic differences and dataset bias, we extract audio and video features as phonemes and visemes respectively, which are the basic units of speech sounds and mouth movements. To address audiovisual synchronization issues, we introduce the Phoneme-Viseme Alignment Mechanism (PV-Align), which establishes robust cross-modal correspondences between phonemes and visemes. In addition, we build a Multilingual Talking Face Benchmark (MTFB) comprising 12 diverse languages with 95.04 hours of high-quality videos for training and evaluating multilingual TFS performance. Extensive experiments demonstrate that MuEx achieves superior performance across all languages in MTFB and exhibits effective zero-shot generalization to unseen languages without additional training.",
    "summary": "",
    "translation": "从音频到视频的桥梁：音素-视位对齐使每张脸都能说多种语言",
    "relevance_score": 2,
    "reasoning": "该论文主要关注音视频跨模态对齐和语音驱动面部动画，属于计算机视觉和多媒体处理领域。虽然涉及多模态建模，但其应用场景（语音到视频的面部动画）与推荐系统、搜索或广告的核心技术需求没有直接关联，潜在应用价值有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06611v1": {
    "title": "Self-supervised Physics-guided Model with Implicit Representation Regularization for Fast MRI Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2510.06611v1",
    "arxiv_id": "2510.06611v1",
    "authors": "Jingran Xu, Yuanyuan Liu, Yanjie Zhu",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 03:40:40",
    "ori_summary": "Magnetic Resonance Imaging (MRI) is a vital clinical diagnostic tool, yet its widespread application is limited by prolonged scan times. Fast MRI reconstruction techniques effectively reduce acquisition duration by reconstructing high-fidelity MR images from undersampled k-space data. In recent years, deep learning-based methods have demonstrated remarkable progress in this field, with self-supervised and unsupervised learning approaches proving particularly valuable in scenarios where fully sampled data are difficult to obtain. This paper proposes a novel zero-shot self-supervised reconstruction framework named UnrollINR, which enables scan-specific MRI reconstruction without relying on external training data. The method adopts a physics-guided unrolled iterative reconstruction architecture and introduces Implicit Neural Representation (INR) as a regularization prior to effectively constrain the solution space. By combining a deep unrolled structure with the powerful implicit representation capability of INR, the model's interpretability and reconstruction performance are enhanced. Experimental results demonstrate that even at a high acceleration rate of 10, UnrollINR achieves superior reconstruction performance compared to the supervised learning method, validating the superiority of the proposed method.",
    "summary": "",
    "translation": "基于隐式表示正则化的自监督物理引导模型用于快速MRI重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（MRI）重建领域，属于医疗应用范畴，与推荐系统、搜索或广告的核心技术无关。虽然提到了自监督学习和正则化技术，但这些方法在医学影像中的特定应用无法直接迁移到推荐系统、搜索或广告领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06601v1": {
    "title": "AIM 2025 Challenge on Real-World RAW Image Denoising",
    "url": "https://www.alphaxiv.org/abs/2510.06601v1",
    "arxiv_id": "2510.06601v1",
    "authors": "Feiran Li, Jiacheng Li, Marcos V. Conde, Beril Besbinar, Vlad Hosu, Daisuke Iso, Radu Timofte",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 03:22:42",
    "ori_summary": "We introduce the AIM 2025 Real-World RAW Image Denoising Challenge, aiming to advance efficient and effective denoising techniques grounded in data synthesis. The competition is built upon a newly established evaluation benchmark featuring challenging low-light noisy images captured in the wild using five different DSLR cameras. Participants are tasked with developing novel noise synthesis pipelines, network architectures, and training methodologies to achieve high performance across different camera models. Winners are determined based on a combination of performance metrics, including full-reference measures (PSNR, SSIM, LPIPS), and non-reference ones (ARNIQA, TOPIQ). By pushing the boundaries of camera-agnostic low-light RAW image denoising trained on synthetic data, the competition promotes the development of robust and practical models aligned with the rapid progress in digital photography. We expect the competition outcomes to influence multiple domains, from image restoration to night-time autonomous driving.",
    "summary": "",
    "translation": "AIM 2025 真实世界RAW图像去噪挑战赛",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的图像去噪技术，特别是针对RAW格式图像的处理。虽然图像质量在某些推荐和广告场景中可能间接相关，但该工作本身是纯粹的视觉处理任务，没有涉及推荐系统、搜索或广告的核心技术，也没有LLM或Transformer架构的应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06596v1": {
    "title": "SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.06596v1",
    "arxiv_id": "2510.06596v1",
    "authors": "Ayush Zenith, Arnold Zumbrun, Neel Raut, Jing Lin",
    "categories": "cs.CV, cs.AI, cs.IT, cs.LG, math.IT",
    "pub_date": "2025-10-08 03:01:26",
    "ori_summary": "The performance of machine learning models depends heavily on training data. The scarcity of large-scale, well-annotated datasets poses significant challenges in creating robust models. To address this, synthetic data generated through simulations and generative models has emerged as a promising solution, enhancing dataset diversity and improving the performance, reliability, and resilience of models. However, evaluating the quality of this generated data requires an effective metric. This paper introduces the Synthetic Dataset Quality Metric (SDQM) to assess data quality for object detection tasks without requiring model training to converge. This metric enables more efficient generation and selection of synthetic datasets, addressing a key challenge in resource-constrained object detection tasks. In our experiments, SDQM demonstrated a strong correlation with the mean Average Precision (mAP) scores of YOLOv11, a leading object detection model, while previous metrics only exhibited moderate or weak correlations. Additionally, it provides actionable insights for improving dataset quality, minimizing the need for costly iterative training. This scalable and efficient metric sets a new standard for evaluating synthetic data. The code for SDQM is available at https://github.com/ayushzenith/SDQM",
    "summary": "",
    "translation": "SDQM：用于目标检测数据集评估的合成数据质量度量",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的目标检测数据集评估和合成数据质量度量，与推荐系统、搜索或广告的核心技术领域没有直接关联。合成数据生成和评估属于计算机视觉和数据集构建的范畴，不具备在推荐、搜索或广告系统中应用的明显潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06592v1": {
    "title": "Adaptive Stain Normalization for Cross-Domain Medical Histology",
    "url": "https://www.alphaxiv.org/abs/2510.06592v1",
    "arxiv_id": "2510.06592v1",
    "authors": "Tianyue Xu, Yanlin Wu, Abhai K. Tripathi, Matthew M. Ippolito, Benjamin D. Haeffele",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 02:53:28",
    "ori_summary": "Deep learning advances have revolutionized automated digital pathology analysis. However, differences in staining protocols and imaging conditions can introduce significant color variability. In deep learning, such color inconsistency often reduces performance when deploying models on data acquired under different conditions from the training data, a challenge known as domain shift. Many existing methods attempt to address this problem via color normalization but suffer from several notable drawbacks such as introducing artifacts or requiring careful choice of a template image for stain mapping. To address these limitations, we propose a trainable color normalization model that can be integrated with any backbone network for downstream tasks such as object detection and classification. Based on the physics of the imaging process per the Beer-Lambert law, our model architecture is derived via algorithmic unrolling of a nonnegative matrix factorization (NMF) model to extract stain-invariant structural information from the original pathology images, which serves as input for further processing. Experimentally, we evaluate the method on publicly available pathology datasets and an internally curated collection of malaria blood smears for cross-domain object detection and classification, where our method outperforms many state-of-the-art stain normalization methods. Our code is available at https://github.com/xutianyue/BeerLaNet.",
    "summary": "",
    "translation": "用于跨域医学组织学的自适应染色归一化",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像处理中的染色归一化技术，属于医学领域的特定应用。虽然涉及跨域适应，但其核心关注医学组织学图像处理，与推荐系统、搜索或广告的技术焦点完全无关。该研究没有显示出在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06590v1": {
    "title": "Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer",
    "url": "https://www.alphaxiv.org/abs/2510.06590v1",
    "arxiv_id": "2510.06590v1",
    "authors": "Ziyuan Huang, DanDan Zheng, Cheng Zou, Rui Liu, Xiaolong Wang, Kaixiang Ji, Weilong Chai, Jianxin Sun, Libin Wang, Yongjie Lv, Taozhi Huang, Jiajia Liu, Qingpei Guo, Ming Yang, Jingdong Chen, Jun Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 02:50:14",
    "ori_summary": "Visual tokenization remains a core challenge in unifying visual understanding and generation within the autoregressive paradigm. Existing methods typically employ tokenizers in discrete latent spaces to align with the tokens from large language models, where the quantization errors can limit semantic expressiveness and degrade the capability of vision-language understanding. To address this, we introduce MingTok, a new family of visual tokenizers with a continuous latent space, for unified autoregressive generation and understanding. While understanding tasks favor discriminative high-dimensional features, generation tasks prefer compact low-level codes. Thus, to reconcile these competing demands, MingTok adopts a three-stage sequential architecture involving low-level encoding, semantic expansion, and visual reconstruction. Built on top of it, Ming-UniVision eliminates the need for task-specific visual representations, and unifies diverse vision-language tasks under a single autoregrsssive prediction paradigm. By formulating both understanding and generation as next-token prediction in a shared continuous space, it seamlessly supports multi-round, in-context tasks such as iterative understanding, generation and editing. Empirically, we find that using a unified continuous visual representation reconciles the competing requirements on the tokenizers by the understanding and generation tasks, thereby leading to state-of-the-art level performance across both domains. We hope our findings will facilitate unified visual tokenization in the continuous domain. Inference code and model weights are released to benefit community.",
    "summary": "",
    "translation": "Ming-UniVision：基于统一连续分词器的联合图像理解与生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉领域的统一建模（图像理解与生成），属于纯粹的视觉或多模态研究方向。虽然标题提到'统一分词器'技术，但缺乏明确的与推荐系统、搜索或广告领域的应用关联。这种视觉-语言统一建模技术可能对处理图像内容有启发，但未直接涉及用户行为序列、上下文特征或排名优化等核心RecSys/Search/Ads问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06584v1": {
    "title": "Improving Artifact Robustness for CT Deep Learning Models Without Labeled Artifact Images via Domain Adaptation",
    "url": "https://www.alphaxiv.org/abs/2510.06584v1",
    "arxiv_id": "2510.06584v1",
    "authors": "Justin Cheung, Samuel Savine, Calvin Nguyen, Lin Lu, Alhassan S. Yasin",
    "categories": "cs.CV, q-bio.TO",
    "pub_date": "2025-10-08 02:27:09",
    "ori_summary": "Deep learning models which perform well on images from their training distribution can degrade substantially when applied to new distributions. If a CT scanner introduces a new artifact not present in the training labels, the model may misclassify the images. Although modern CT scanners include design features which mitigate these artifacts, unanticipated or difficult-to-mitigate artifacts can still appear in practice. The direct solution of labeling images from this new distribution can be costly. As a more accessible alternative, this study evaluates domain adaptation as an approach for training models that maintain classification performance despite new artifacts, even without corresponding labels. We simulate ring artifacts from detector gain error in sinogram space and evaluate domain adversarial neural networks (DANN) against baseline and augmentation-based approaches on the OrganAMNIST abdominal CT dataset. Our results demonstrate that baseline models trained only on clean images fail to generalize to images with ring artifacts, and traditional augmentation with other distortion types provides no improvement on unseen artifact domains. In contrast, the DANN approach successfully maintains high classification accuracy on ring artifact images using only unlabeled artifact data during training, demonstrating the viability of domain adaptation for artifact robustness. The domain-adapted model achieved classification performance on ring artifact test data comparable to models explicitly trained with labeled artifact images, while also showing unexpected generalization to uniform noise. These findings provide empirical evidence that domain adaptation can effectively address distribution shift in medical imaging without requiring expensive expert labeling of new artifact distributions, suggesting promise for deployment in clinical settings where novel artifacts may emerge.",
    "summary": "",
    "translation": "通过领域自适应改进CT深度学习模型的伪影鲁棒性，无需带标签的伪影图像",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（CT扫描）中的深度学习模型鲁棒性改进，属于医疗领域的特定应用。虽然涉及领域自适应技术，但该技术应用于医学图像伪影处理，与推荐系统、搜索或广告领域没有直接关联，也不符合任何当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06582v1": {
    "title": "Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.06582v1",
    "arxiv_id": "2510.06582v1",
    "authors": "Fei Zhang, Rob Chancia, Josie Clapp, Amirhossein Hassanzadeh, Dimah Dera, Richard MacKenzie, Jan van Aardt",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-08 02:25:59",
    "ori_summary": "Accurate semantic segmentation of terrestrial laser scanning (TLS) point clouds is limited by costly manual annotation. We propose a semi-automated, uncertainty-aware pipeline that integrates spherical projection, feature enrichment, ensemble learning, and targeted annotation to reduce labeling effort, while sustaining high accuracy. Our approach projects 3D points to a 2D spherical grid, enriches pixels with multi-source features, and trains an ensemble of segmentation networks to produce pseudo-labels and uncertainty maps, the latter guiding annotation of ambiguous regions. The 2D outputs are back-projected to 3D, yielding densely annotated point clouds supported by a three-tier visualization suite (2D feature maps, 3D colorized point clouds, and compact virtual spheres) for rapid triage and reviewer guidance. Using this pipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove forests. We further evaluate data efficiency and feature importance to address two key questions: (1) how much annotated data are needed and (2) which features matter most. Results show that performance saturates after ~12 annotated scans, geometric features contribute the most, and compact nine-channel stacks capture nearly all discriminative power, with the mean Intersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm the generalization of our feature-enrichment strategy through cross-dataset tests on ForestSemantic and Semantic3D. Our contributions include: (i) a robust, uncertainty-aware TLS annotation pipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii) empirical guidance on data efficiency and feature importance, thus enabling scalable, high-quality segmentation of TLS point clouds for ecological monitoring and beyond. The dataset and processing scripts are publicly available at https://fz-rit.github.io/through-the-lidars-eye/.",
    "summary": "",
    "translation": "基于LiDAR视角：面向地面点云分割的特征增强与不确定性感知标注流程",
    "relevance_score": 1,
    "reasoning": "该论文专注于LiDAR点云分割和标注流程，属于纯粹的计算机视觉和3D感知领域。虽然涉及特征增强技术，但其应用场景（地面点云分割）与推荐系统、搜索或广告领域没有任何直接或间接关联，完全超出了关注范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06564v1": {
    "title": "HSNet: Heterogeneous Subgraph Network for Single Image Super-resolution",
    "url": "https://www.alphaxiv.org/abs/2510.06564v1",
    "arxiv_id": "2510.06564v1",
    "authors": "Qiongyang Hu, Wenyang Liu, Wenbin Zou, Yuejiao Su, Lap-Pui Chau, Yi Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 01:32:52",
    "ori_summary": "Existing deep learning approaches for image super-resolution, particularly those based on CNNs and attention mechanisms, often suffer from structural inflexibility. Although graph-based methods offer greater representational adaptability, they are frequently impeded by excessive computational complexity. To overcome these limitations, this paper proposes the Heterogeneous Subgraph Network (HSNet), a novel framework that efficiently leverages graph modeling while maintaining computational feasibility. The core idea of HSNet is to decompose the global graph into manageable sub-components. First, we introduce the Constructive Subgraph Set Block (CSSB), which generates a diverse set of complementary subgraphs. Rather than relying on a single monolithic graph, CSSB captures heterogeneous characteristics of the image by modeling different relational patterns and feature interactions, producing a rich ensemble of both local and global graph structures. Subsequently, the Subgraph Aggregation Block (SAB) integrates the representations embedded across these subgraphs. Through adaptive weighting and fusion of multi-graph features, SAB constructs a comprehensive and discriminative representation that captures intricate interdependencies. Furthermore, a Node Sampling Strategy (NSS) is designed to selectively retain the most salient features, thereby enhancing accuracy while reducing computational overhead. Extensive experiments demonstrate that HSNet achieves state-of-the-art performance, effectively balancing reconstruction quality with computational efficiency. The code will be made publicly available.",
    "summary": "",
    "translation": "HSNet：用于单图像超分辨率的异质子图网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的单图像超分辨率任务，属于纯粹的视觉处理领域。虽然涉及异质图网络技术，但该技术应用于图像像素处理，与推荐系统、搜索或广告中的异构数据处理没有直接关联，也没有展示在推荐/搜索/广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06541v1": {
    "title": "Cluster Paths: Navigating Interpretability in Neural Networks",
    "url": "https://www.alphaxiv.org/abs/2510.06541v1",
    "arxiv_id": "2510.06541v1",
    "authors": "Nicholas M. Kroeger, Vincent Bindschaedler",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-08 00:41:09",
    "ori_summary": "While modern deep neural networks achieve impressive performance in vision tasks, they remain opaque in their decision processes, risking unwarranted trust, undetected biases and unexpected failures. We propose cluster paths, a post-hoc interpretability method that clusters activations at selected layers and represents each input as its sequence of cluster IDs. To assess these cluster paths, we introduce four metrics: path complexity (cognitive load), weighted-path purity (class alignment), decision-alignment faithfulness (predictive fidelity), and path agreement (stability under perturbations). In a spurious-cue CIFAR-10 experiment, cluster paths identify color-based shortcuts and collapse when the cue is removed. On a five-class CelebA hair-color task, they achieve 90% faithfulness and maintain 96% agreement under Gaussian noise without sacrificing accuracy. Scaling to a Vision Transformer pretrained on ImageNet, we extend cluster paths to concept paths derived from prompting a large language model on minimal path divergences. Finally, we show that cluster paths can serve as an effective out-of-distribution (OOD) detector, reliably flagging anomalous samples before the model generates over-confident predictions. Cluster paths uncover visual concepts, such as color palettes, textures, or object contexts, at multiple network depths, demonstrating that cluster paths scale to large vision models while generating concise and human-readable explanations.",
    "summary": "",
    "translation": "簇路径：神经网络可解释性导航",
    "relevance_score": 2,
    "reasoning": "该论文主要关注神经网络的可解释性方法，属于通用AI研究领域。虽然可解释性在推荐系统和搜索中有一定价值，但论文标题没有表明与Transformer架构、LLM技术或推荐/搜索/广告领域的直接关联，也没有提到处理异构数据或多模态建模等核心关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06529v1": {
    "title": "VUGEN: Visual Understanding priors for GENeration",
    "url": "https://www.alphaxiv.org/abs/2510.06529v1",
    "arxiv_id": "2510.06529v1",
    "authors": "Xiangyi Chen, Théophane Vallaeys, Maha Elbayad, John Nguyen, Jakob Verbeek",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 00:04:47",
    "ori_summary": "Recent advances in Vision-Language Models (VLMs) have enabled unified understanding across text and images, yet equipping these models with robust image generation capabilities remains challenging. Existing approaches often rely on reconstruction-oriented autoencoders or complex bridging mechanisms, leading to misalignment between understanding and generation representations, or architectural complexity. In this work, we propose VUGEN, a novel framework that explicitly leverages VLM's pretrained visual understanding priors for efficient and high-quality image generation. Our approach first transforms the high-dimensional latent space of the VLM's native vision encoder into a lower-dimensional, tractable distribution that maximally preserves visual information. The VLM is then trained to sample within this reduced latent space, ensuring alignment with its visual understanding capabilities. Finally, a dedicated pixel decoder maps these generated latents back to the image space. We find that a VAE-free pixel diffusion decoder to be on par or better than commonly used complex latent diffusion decoders that internally rely on VAE latents. Extensive experiments demonstrate that VUGEN achieves superior image generation performance, improving DPG Bench from 71.17 to 74.32 and FID from 11.86 to 9.06 on COCO, while fully preserving the VLM's original understanding capabilities.",
    "summary": "",
    "translation": "VUGEN：用于生成的视觉理解先验",
    "relevance_score": 2,
    "reasoning": "该论文标题表明其聚焦于视觉理解和生成任务，属于纯粹的视觉-语言模型或视觉生成领域。虽然标题提及'理解先验'可能暗示多模态建模，但缺乏与推荐系统、搜索或广告中异构数据处理的具体关联，且生成导向与当前关注的排序和检索核心问题不符。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08558v1": {
    "title": "Agent Learning via Early Experience",
    "url": "https://www.alphaxiv.org/abs/2510.08558v1",
    "arxiv_id": "2510.08558v1",
    "authors": "Kai Zhang, Xiangchao Chen, Bo Liu, Tianci Xue, Zeyi Liao, Zhihan Liu, Xiyao Wang, Yuting Ning, Zhaorun Chen, Xiaohan Fu, Jian Xie, Yuxuan Sun, Boyu Gou, Qi Qi, Zihang Meng, Jianwei Yang, Ning Zhang, Xian Li, Ashish Shah, Dat Huynh, Hengduo Li, Zi Yang, Sara Cao, Lawrence Jang, Shuyan Zhou, Jiacheng Zhu, Huan Sun, Jason Weston, Yu Su, Yifan Wu",
    "categories": "cs.AI, cs.CL, cs.IR, cs.LG",
    "pub_date": "2025-10-09 17:59:17",
    "ori_summary": "A long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks. However, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As a result, most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only a narrow range of scenarios and expose the agent to limited environment diversity. We address this limitation with a middle-ground paradigm we call early experience: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals. Within this paradigm we study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making. We evaluate across eight diverse environments and multiple model families. Our approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover, in environments with verifiable rewards, our results provide promising signals that early experience offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge between imitation learning and fully experience-driven agents.",
    "summary": "",
    "translation": "智能体通过早期经验学习",
    "relevance_score": 2,
    "reasoning": "该论文标题聚焦于智能体学习机制，主要涉及强化学习或智能体训练方法，属于通用AI技术范畴。虽然智能体技术可能间接应用于推荐系统或搜索的交互优化，但标题未明确显示与推荐系统、搜索、广告或相关使能技术（如Transformer、LLM应用）的直接关联，且未提及多模态数据处理等具体应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08385v1": {
    "title": "Detecting Legend Items on Historical Maps Using GPT-4o with In-Context Learning",
    "url": "https://www.alphaxiv.org/abs/2510.08385v1",
    "arxiv_id": "2510.08385v1",
    "authors": "Sofia Kirsanova, Yao-Yi Chiang, Weiwei Duan",
    "categories": "cs.CV, cs.AI, cs.DB, cs.IR, H.2.8; H.3.3; I.2.10; I.4.8",
    "pub_date": "2025-10-09 16:08:48",
    "ori_summary": "Historical map legends are critical for interpreting cartographic symbols. However, their inconsistent layouts and unstructured formats make automatic extraction challenging. Prior work focuses primarily on segmentation or general optical character recognition (OCR), with few methods effectively matching legend symbols to their corresponding descriptions in a structured manner. We present a method that combines LayoutLMv3 for layout detection with GPT-4o using in-context learning to detect and link legend items and their descriptions via bounding box predictions. Our experiments show that GPT-4 with structured JSON prompts outperforms the baseline, achieving 88% F-1 and 85% IoU, and reveal how prompt design, example counts, and layout alignment affect performance. This approach supports scalable, layout-aware legend parsing and improves the indexing and searchability of historical maps across various visual styles.",
    "summary": "",
    "translation": "使用GPT-4o和上下文学习检测历史地图上的图例项",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及计算机视觉任务（历史地图分析）和LLM的特定应用，与推荐系统、搜索或广告的核心领域进展无关。虽然使用了GPT-4o，但应用场景（历史地图图例检测）在RecSys/Search/Ads领域没有明显的实际应用潜力，属于纯粹的视觉应用范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08281v1": {
    "title": "Mobile Gamer Lifetime Value Prediction via Objective Decomposition and Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2510.08281v1",
    "arxiv_id": "2510.08281v1",
    "authors": "Tianwei Li, Yu Zhao, Yunze Li, Sheng Li",
    "categories": "cs.IR",
    "pub_date": "2025-10-09 14:33:12",
    "ori_summary": "For Internet platforms operating real-time bidding (RTB) advertising service, a comprehensive understanding of user lifetime value (LTV) plays a pivotal role in optimizing advertisement allocation efficiency and maximizing the return on investment (ROI) for advertisement sponsors, thereby facilitating growth of commercialization revenue for the platform. However, the inherent complexity of user LTV distributions induces significant challenges in accurate LTV prediction. Existing state-of-the-art works, which primarily focus on directly learning the LTV distributions through well-designed loss functions, achieve limited success due to their vulnerability to outliers. In this paper, we proposed a novel LTV prediction method to address distribution challenges through an objective decomposition and reconstruction framework. Briefly speaking, based on the in-app purchase characteristics of mobile gamers, our model was designed to first predict the number of transactions at specific prices and then calculate the total payment amount from these intermediate predictions. Our proposed model was evaluated through experiments on real-world industrial dataset, and deployed on the TapTap RTB advertising system for online A/B testing along with the state-of-the-art ZILN model.",
    "summary": "论文研究移动游戏广告中用户终身价值预测的分布复杂性挑战，核心思想是通过将LTV预测分解为特定价格交易次数预测，然后重构总支付金额来应对分布异常值问题。",
    "translation": "通过目标分解与重构的移动游戏玩家终身价值预测",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及推荐系统中的核心预测任务——用户价值预测，这是广告和推荐系统的关键组成部分。目标分解与重构方法可以应用于更广泛的用户行为建模场景，包括电商推荐和广告投放中的用户价值评估。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对广告系统中的用户终身价值预测问题，提出了分解重构的建模方法，与广告领域优化和推荐系统核心挑战高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08252v1": {
    "title": "ReasonEmbed: Enhanced Text Embeddings for Reasoning-Intensive Document Retrieval",
    "url": "https://www.alphaxiv.org/abs/2510.08252v1",
    "arxiv_id": "2510.08252v1",
    "authors": "Jianlyu Chen, Junwei Lan, Chaofan Li, Defu Lian, Zheng Liu",
    "categories": "cs.IR, cs.CL",
    "pub_date": "2025-10-09 14:10:26",
    "ori_summary": "In this paper, we introduce ReasonEmbed, a novel text embedding model developed for reasoning-intensive document retrieval. Our work includes three key technical contributions. First, we propose ReMixer, a new data synthesis method that overcomes the triviality problem prevalent in previous synthetic datasets, enabling large-scale production of 82K high-quality training samples. Second, we design Redapter, a self-adaptive learning algorithm that dynamically adjusts training each sample's weight based on its reasoning intensity. This allows the model to effectively capture the complex semantic relationships between queries and documents. Third, we implement ReasonEmbed across multiple backbones of varying sizes, all of which achieve superior performance on reasoning-intensive retrieval tasks. Notably, our ReasonEmbed-Qwen3-8B model offers a record-high nDCG@10 score of 38.1 on the BRIGHT benchmark, which significantly outperforms existing text embedding models. We will fully open-source our created resources in ReasonEmbed to push forward the research advancement in this field.",
    "summary": "该论文研究推理密集型文档检索中的文本嵌入问题，核心方法是提出ReMixer数据合成技术解决合成数据简单化问题，并设计Redapter自适应学习算法动态调整训练权重以捕捉复杂语义关系。",
    "translation": "ReasonEmbed：用于推理密集型文档检索的增强文本嵌入",
    "relevance_score": 8,
    "reasoning": "该论文专注于增强文本嵌入技术，这属于核心LLM技术的进步，对搜索系统具有直接应用价值。改进的文本嵌入可以显著提升文档检索的准确性和推理能力，在搜索和推荐系统中能够更好地理解用户查询意图和文档语义，从而提高检索质量。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文专注于推理密集型文档检索的文本嵌入技术，其数据合成和自适应学习算法在搜索和推荐系统中具有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08109v1": {
    "title": "VersionRAG: Version-Aware Retrieval-Augmented Generation for Evolving Documents",
    "url": "https://www.alphaxiv.org/abs/2510.08109v1",
    "arxiv_id": "2510.08109v1",
    "authors": "Daniel Huwiler, Kurt Stockinger, Jonathan Fürst",
    "categories": "cs.IR, cs.AI, cs.CL",
    "pub_date": "2025-10-09 11:48:58",
    "ori_summary": "Retrieval-Augmented Generation (RAG) systems fail when documents evolve through versioning-a ubiquitous characteristic of technical documentation. Existing approaches achieve only 58-64% accuracy on version-sensitive questions, retrieving semantically similar content without temporal validity checks. We present VersionRAG, a version-aware RAG framework that explicitly models document evolution through a hierarchical graph structure capturing version sequences, content boundaries, and changes between document states. During retrieval, VersionRAG routes queries through specialized paths based on intent classification, enabling precise version-aware filtering and change tracking. On our VersionQA benchmark-100 manually curated questions across 34 versioned technical documents-VersionRAG achieves 90% accuracy, outperforming naive RAG (58%) and GraphRAG (64%). VersionRAG reaches 60% accuracy on implicit change detection where baselines fail (0-10%), demonstrating its ability to track undocumented modifications. Additionally, VersionRAG requires 97% fewer tokens during indexing than GraphRAG, making it practical for large-scale deployment. Our work establishes versioned document QA as a distinct task and provides both a solution and benchmark for future research.",
    "summary": "",
    "translation": "VersionRAG：面向演进文档的版本感知检索增强生成",
    "relevance_score": 7,
    "reasoning": "该论文提出的版本感知RAG技术属于LLM核心技术的进展，在搜索和推荐系统中具有直接应用潜力。当文档内容随时间演进时（如产品描述、新闻文章、政策法规），版本感知检索能够确保系统返回最新且相关的信息，这对于搜索结果的时效性和推荐系统的准确性至关重要。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08048v1": {
    "title": "TaoSR-AGRL: Adaptive Guided Reinforcement Learning Framework for E-commerce Search Relevance",
    "url": "https://www.alphaxiv.org/abs/2510.08048v1",
    "arxiv_id": "2510.08048v1",
    "authors": "Jianhui Yang, Yiming Jin, Pengkun Jiao, Chenhe Dong, Zerui Huang, Shaowei Yao, Xiaojiang Zhou, Dan Ou, Haihong Tang",
    "categories": "cs.IR, cs.AI, cs.CL",
    "pub_date": "2025-10-09 10:34:39",
    "ori_summary": "Query-product relevance prediction is fundamental to e-commerce search and has become even more critical in the era of AI-powered shopping, where semantic understanding and complex reasoning directly shape the user experience and business conversion. Large Language Models (LLMs) enable generative, reasoning-based approaches, typically aligned via supervised fine-tuning (SFT) or preference optimization methods like Direct Preference Optimization (DPO). However, the increasing complexity of business rules and user queries exposes the inability of existing methods to endow models with robust reasoning capacity for long-tail and challenging cases. Efforts to address this via reinforcement learning strategies like Group Relative Policy Optimization (GRPO) often suffer from sparse terminal rewards, offering insufficient guidance for multi-step reasoning and slowing convergence. To address these challenges, we propose TaoSR-AGRL, an Adaptive Guided Reinforcement Learning framework for LLM-based relevance prediction in Taobao Search Relevance. TaoSR-AGRL introduces two key innovations: (1) Rule-aware Reward Shaping, which decomposes the final relevance judgment into dense, structured rewards aligned with domain-specific relevance criteria; and (2) Adaptive Guided Replay, which identifies low-accuracy rollouts during training and injects targeted ground-truth guidance to steer the policy away from stagnant, rule-violating reasoning patterns toward compliant trajectories. TaoSR-AGRL was evaluated on large-scale real-world datasets and through online side-by-side human evaluations on Taobao Search. It consistently outperforms DPO and standard GRPO baselines in offline experiments, improving relevance accuracy, rule adherence, and training stability. The model trained with TaoSR-AGRL has been successfully deployed in the main search scenario on Taobao, serving hundreds of millions of users.",
    "summary": "论文研究电商搜索中LLM相关性预测的推理能力不足问题，核心思想是通过规则感知奖励塑造和自适应引导回放机制，为多步推理提供密集的结构化奖励和针对性指导。",
    "translation": "TaoSR-AGRL：面向电商搜索相关性的自适应引导强化学习框架",
    "relevance_score": 8,
    "reasoning": "该论文直接聚焦于电商搜索相关性优化，属于核心推荐系统领域的进展。虽然涉及强化学习，但明确应用于搜索排名场景，具有直接的实践意义。自适应引导机制可能提升搜索结果的个性化相关性，对电商平台的用户体验和转化率有重要影响。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对电商搜索相关性预测这一核心问题，提出了结合强化学习和LLM的创新框架，完美契合搜索领域的技术前沿需求。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07885v1": {
    "title": "Generation and annotation of item usage scenarios in e-commerce using large language models",
    "url": "https://www.alphaxiv.org/abs/2510.07885v1",
    "arxiv_id": "2510.07885v1",
    "authors": "Madoka Hagiri, Kazushi Okamoto, Koki Karube, Kei Harada, Atsushi Shibata",
    "categories": "cs.IR",
    "pub_date": "2025-10-09 07:37:54",
    "ori_summary": "Complementary recommendations suggest combinations of useful items that play important roles in e-commerce. However, complementary relationships are often subjective and vary among individuals, making them difficult to infer from historical data. Unlike conventional history-based methods that rely on statistical co-occurrence, we focus on the underlying usage context that motivates item combinations. We hypothesized that people select complementary items by imagining specific usage scenarios and identifying the needs in such situations. Based on this idea, we explored the use of large language models (LLMs) to generate item usage scenarios as a starting point for constructing complementary recommendation systems. First, we evaluated the plausibility of LLM-generated scenarios through manual annotation. The results demonstrated that approximately 85% of the generated scenarios were determined to be plausible, suggesting that LLMs can effectively generate realistic item usage scenarios.",
    "summary": "研究电商中互补推荐的主观性和个性化难题，核心思想是利用LLM生成具体的物品使用场景，通过情境想象来理解用户组合物品的动机。",
    "translation": "使用大型语言模型生成和标注电子商务中的物品使用场景",
    "relevance_score": 8,
    "reasoning": "该论文直接应用LLM技术于电子商务领域，属于'Direct LLM Applications'范畴。通过生成和标注物品使用场景，可以显著增强推荐系统的上下文理解和个性化推荐能力，为搜索和推荐系统提供更丰富的语义信息。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用LLM技术生成电商场景下的物品使用情境，为核心推荐系统提供新的数据构建方法，与用户关注点高度契合。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07796v1": {
    "title": "HySim-LLM: Embedding-Weighted Fine-Tuning Bounds and Manifold Denoising for Domain-Adapted LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.07796v1",
    "arxiv_id": "2510.07796v1",
    "authors": "Majid Jaberi-Douraki, Hossein Sholehrasa, Xuan Xu, Remya Ampadi Ramachandran",
    "categories": "cs.LG, cs.IR",
    "pub_date": "2025-10-09 05:16:46",
    "ori_summary": "The extraction and standardization of pharmacokinetic (PK) information from scientific literature remain significant challenges in computational pharmacology, which limits the reliability of data-driven models in drug development. Large language models (LLMs) have achieved remarkable progress in text understanding and reasoning, yet their adaptation to structured biomedical data, such as PK tables, remains constrained by heterogeneity, noise, and domain shift. To address these limitations, we propose HySim-LLM, a unified mathematical and computational framework that integrates embedding-weighted fine-tuning and manifold-aware denoising to enhance the robustness and interpretability of LLMs. We establish two theoretical results: (1) a similarity-weighted generalization bound that quantifies adaptation performance under embedding divergence, and (2) a manifold-based denoising guarantee that bounds loss contributions from noisy or off-manifold samples. These theorems provide a principled foundation for fine-tuning LLMs in structured biomedical settings. The framework offers a mathematically grounded pathway toward reliable and interpretable LLM adaptation for biomedical and data-intensive scientific domains.",
    "summary": "该论文研究领域适应中LLM处理异构结构化数据的鲁棒性问题，核心思想是通过嵌入加权微调边界和流形去噪理论来提升模型在领域迁移中的泛化能力和可解释性。",
    "translation": "HySim-LLM：面向领域自适应大语言模型的嵌入加权微调边界与流形去噪",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM领域自适应和微调技术，属于'赋能LLM技术'范畴。嵌入加权微调和流形去噪方法可显著提升LLM在特定领域（如电商、搜索）的适应能力，直接应用于构建更精准的推荐系统和搜索引擎中的查询理解模块。",
    "rerank_relevance_score": 6,
    "rerank_reasoning": "该论文提出了嵌入加权微调和流形去噪的理论框架，虽然应用于生物医学领域，但其核心方法对推荐系统中处理异构数据和领域适应具有直接参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07784v1": {
    "title": "PLUM: Adapting Pre-trained Language Models for Industrial-scale Generative Recommendations",
    "url": "https://www.alphaxiv.org/abs/2510.07784v1",
    "arxiv_id": "2510.07784v1",
    "authors": "Ruining He, Lukasz Heldt, Lichan Hong, Raghunandan Keshavan, Shifan Mao, Nikhil Mehta, Zhengyang Su, Alicia Tsai, Yueqi Wang, Shao-Chuan Wang, Xinyang Yi, Lexi Baugher, Baykal Cakici, Ed Chi, Cristos Goodrow, Ningren Han, He Ma, Romer Rosales, Abby Van Soest, Devansh Tandon, Su-Lin Wu, Weilong Yang, Yilin Zheng",
    "categories": "cs.IR, cs.LG",
    "pub_date": "2025-10-09 05:01:05",
    "ori_summary": "Large Language Models (LLMs) pose a new paradigm of modeling and computation for information tasks. Recommendation systems are a critical application domain poised to benefit significantly from the sequence modeling capabilities and world knowledge inherent in these large models. In this paper, we introduce PLUM, a framework designed to adapt pre-trained LLMs for industry-scale recommendation tasks. PLUM consists of item tokenization using Semantic IDs, continued pre-training (CPT) on domain-specific data, and task-specific fine-tuning for recommendation objectives. For fine-tuning, we focus particularly on generative retrieval, where the model is directly trained to generate Semantic IDs of recommended items based on user context. We conduct comprehensive experiments on large-scale internal video recommendation datasets. Our results demonstrate that PLUM achieves substantial improvements for retrieval compared to a heavily-optimized production model built with large embedding tables. We also present a scaling study for the model's retrieval performance, our learnings about CPT, a few enhancements to Semantic IDs, along with an overview of the training and inference methods that enable launching this framework to billions of users in YouTube.",
    "summary": "研究如何将预训练语言模型适配到工业级推荐任务；核心方法是构建包含项目语义ID化、领域持续预训练和生成式检索微调的完整框架，使模型能直接生成推荐项目的语义ID。",
    "translation": "PLUM：为工业级生成式推荐系统适配预训练语言模型",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及LLM在推荐系统中的应用，属于'Direct LLM Applications'范畴。将预训练语言模型适配于工业级生成式推荐，展示了LLM技术在推荐领域的实际应用潜力，包括生成式推荐和个性化内容生成。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM在推荐系统中的应用，提出了完整的工业级生成式推荐框架，完全符合核心关注领域。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07728v1": {
    "title": "Who Stole Your Data? A Method for Detecting Unauthorized RAG Theft",
    "url": "https://www.alphaxiv.org/abs/2510.07728v1",
    "arxiv_id": "2510.07728v1",
    "authors": "Peiyang Liu, Ziqiang Cui, Di Liang, Wei Ye",
    "categories": "cs.IR, cs.CL",
    "pub_date": "2025-10-09 03:09:18",
    "ori_summary": "Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs) by mitigating hallucinations and outdated information issues, yet simultaneously facilitates unauthorized data appropriation at scale. This paper addresses this challenge through two key contributions. First, we introduce RPD, a novel dataset specifically designed for RAG plagiarism detection that encompasses diverse professional domains and writing styles, overcoming limitations in existing resources. Second, we develop a dual-layered watermarking system that embeds protection at both semantic and lexical levels, complemented by an interrogator-detective framework that employs statistical hypothesis testing on accumulated evidence. Extensive experimentation demonstrates our approach's effectiveness across varying query volumes, defense prompts, and retrieval parameters, while maintaining resilience against adversarial evasion techniques. This work establishes a foundational framework for intellectual property protection in retrieval-augmented AI systems.",
    "summary": "",
    "translation": "谁窃取了您的数据？一种检测未经授权的RAG窃取的方法",
    "relevance_score": 1,
    "reasoning": "该论文关注数据安全和未经授权访问的检测，这属于隐私和安全领域，被明确列为不相关主题。虽然提到了RAG（检索增强生成），但核心焦点是安全检测而非RAG在推荐系统或搜索中的应用。该研究没有提供在推荐、搜索或广告领域的潜在技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07720v1": {
    "title": "Queries Are Not Alone: Clustering Text Embeddings for Video Search",
    "url": "https://www.alphaxiv.org/abs/2510.07720v1",
    "arxiv_id": "2510.07720v1",
    "authors": "Peyang Liu, Xi Wang, Ziqiang Cui, Wei Ye",
    "categories": "cs.IR",
    "pub_date": "2025-10-09 02:56:18",
    "ori_summary": "The rapid proliferation of video content across various platforms has highlighted the urgent need for advanced video retrieval systems. Traditional methods, which primarily depend on directly matching textual queries with video metadata, often fail to bridge the semantic gap between text descriptions and the multifaceted nature of video content. This paper introduces a novel framework, the Video-Text Cluster (VTC), which enhances video retrieval by clustering text queries to capture a broader semantic scope. We propose a unique clustering mechanism that groups related queries, enabling our system to consider multiple interpretations and nuances of each query. This clustering is further refined by our innovative Sweeper module, which identifies and mitigates noise within these clusters. Additionally, we introduce the Video-Text Cluster-Attention (VTC-Att) mechanism, which dynamically adjusts focus within the clusters based on the video content, ensuring that the retrieval process emphasizes the most relevant textual features. Further experiments have demonstrated that our proposed model surpasses existing state-of-the-art models on five public datasets.",
    "summary": "",
    "translation": "查询并非孤立：基于文本嵌入聚类的视频搜索",
    "relevance_score": 7,
    "reasoning": "该论文聚焦于搜索领域，通过文本嵌入聚类技术改进视频搜索效果，这直接属于核心搜索领域的进展。虽然论文具体针对视频搜索，但其基于文本嵌入和聚类的方法可以推广到通用的搜索和推荐系统中，用于处理用户查询的多样性和语义理解。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07644v1": {
    "title": "ISMIE: A Framework to Characterize Information Seeking in Modern Information Environments",
    "url": "https://www.alphaxiv.org/abs/2510.07644v1",
    "arxiv_id": "2510.07644v1",
    "authors": "Shuoqi Sun, Danula Hettiachchi, Damiano Spina",
    "categories": "cs.IR",
    "pub_date": "2025-10-09 00:32:07",
    "ori_summary": "The modern information environment (MIE) is increasingly complex, shaped by a wide range of techniques designed to satisfy users' information needs. Information seeking (IS) models are effective mechanisms for characterizing user-system interactions. However, conceptualizing a model that fully captures the MIE landscape poses a challenge. We argue: Does such a model exist? To address this, we propose the Information Seeking in Modern Information Environments (ISMIE) framework as a fundamental step. ISMIE conceptualizes the information seeking process (ISP) via three key concepts: Components (e.g., Information Seeker), Intervening Variables (e.g., Interactive Variables), and Activities (e.g., Acquiring). Using ISMIE's concepts and employing a case study based on a common scenario - misinformation dissemination - we analyze six existing IS and information retrieval (IR) models to illustrate their limitations and the necessity of ISMIE. We then show how ISMIE serves as an actionable framework for both characterization and experimental design. We characterize three pressing issues and then outline two research blueprints: a user-centric, industry-driven experimental design for the authenticity and trust crisis to AI-generated content and a system-oriented, academic-driven design for tackling dopamine-driven content consumption. Our framework offers a foundation for developing IS and IR models to advance knowledge on understanding human interactions and system design in MIEs.",
    "summary": "",
    "translation": "ISMIE：一个用于表征现代信息环境中信息寻求行为的框架",
    "relevance_score": 3,
    "reasoning": "该论文标题聚焦于信息寻求行为表征框架，虽然与搜索系统有一定关联，但更偏向信息行为学和人机交互领域，而非核心推荐系统、搜索或广告的技术进展。该框架可能为理解用户搜索意图提供基础，但缺乏明确的LLM、Transformer或推荐系统技术应用的具体指向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08569v1": {
    "title": "ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.08569v1",
    "arxiv_id": "2510.08569v1",
    "authors": "Qin Liu, Jacob Dineen, Yuxi Huang, Sheng Zhang, Hoifung Poon, Ben Zhou, Muhao Chen",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-09 17:59:55",
    "ori_summary": "Benchmarks are central to measuring the capabilities of large language models and guiding model development, yet widespread data leakage from pretraining corpora undermines their validity. Models can match memorized content rather than demonstrate true generalization, which inflates scores, distorts cross-model comparisons, and misrepresents progress. We introduce ArenaBencher, a model-agnostic framework for automatic benchmark evolution that updates test cases while preserving comparability. Given an existing benchmark and a diverse pool of models to be evaluated, ArenaBencher infers the core ability of each test case, generates candidate question-answer pairs that preserve the original objective, verifies correctness and intent with an LLM as a judge, and aggregates feedback from multiple models to select candidates that expose shared weaknesses. The process runs iteratively with in-context demonstrations that steer generation toward more challenging and diagnostic cases. We apply ArenaBencher to math problem solving, commonsense reasoning, and safety domains and show that it produces verified, diverse, and fair updates that uncover new failure modes, increase difficulty while preserving test objective alignment, and improve model separability. The framework provides a scalable path to continuously evolve benchmarks in step with the rapid progress of foundation models.",
    "summary": "",
    "translation": "ArenaBencher：通过多模型竞争性评估实现自动基准演化",
    "relevance_score": 1,
    "reasoning": "该论文关注自动基准评估和演化，属于纯粹的评估基准研究，与我的关注点无关。论文标题表明其核心是基准测试方法学，没有涉及推荐系统、搜索或广告的核心进展，也没有LLM技术或Transformer架构的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08567v1": {
    "title": "MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.08567v1",
    "arxiv_id": "2510.08567v1",
    "authors": "Tajamul Ashraf, Umair Nawaz, Abdelrahman M. Shaker, Rao Anwer, Philip Torr, Fahad Shahbaz Khan, Salman Khan",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-10-09 17:59:54",
    "ori_summary": "Vision language models (VLMs) are increasingly deployed as controllers with access to external tools for complex reasoning and decision-making, yet their effectiveness remains limited by the scarcity of high-quality multimodal trajectories and the cost of manual annotation. We address this challenge with a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories, generates step-wise preference pairs, and trains a VLM controller for robust tool-use reasoning. Our pipeline first constructs M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified trajectories, enabling imitation-based trajectory tuning. Building on this, we develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool reasoning. To achieve finer alignment, we further introduce Pref-X, a set of 11K automatically generated preference pairs, and optimize MATRIX on it via step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA, MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating scalable and effective multimodal tool use. Our data and code is avaliable at https://github.com/mbzuai-oryx/MATRIX.",
    "summary": "",
    "translation": "MATRIX：面向鲁棒工具使用推理的多模态智能体调优",
    "relevance_score": 4,
    "reasoning": "该论文涉及多模态智能体和工具使用推理，属于LLM应用范畴。虽然多模态智能体在搜索和推荐系统中具有潜在应用价值（如处理异构数据和外部工具集成），但论文标题未明确表明与推荐系统、搜索或广告的直接关联，且工具使用推理可能更偏向通用AI助手应用而非核心排序任务。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08543v1": {
    "title": "VideoNorms: Benchmarking Cultural Awareness of Video Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08543v1",
    "arxiv_id": "2510.08543v1",
    "authors": "Nikhil Reddy Varimalla, Yunfei Xu, Arkadiy Saakyan, Meng Fan Wang, Smaranda Muresan",
    "categories": "cs.CV, cs.AI, cs.CL, cs.CY",
    "pub_date": "2025-10-09 17:54:55",
    "ori_summary": "As Video Large Language Models (VideoLLMs) are deployed globally, they require understanding of and grounding in the relevant cultural background. To properly assess these models' cultural awareness, adequate benchmarks are needed. We introduce VideoNorms, a benchmark of over 1000 (video clip, norm) pairs from US and Chinese cultures annotated with socio-cultural norms grounded in speech act theory, norm adherence and violations labels, and verbal and non-verbal evidence. To build VideoNorms, we use a human-AI collaboration framework, where a teacher model using theoretically-grounded prompting provides candidate annotations and a set of trained human experts validate and correct the annotations. We benchmark a variety of open-weight VideoLLMs on the new dataset which highlight several common trends: 1) models performs worse on norm violation than adherence; 2) models perform worse w.r.t Chinese culture compared to the US culture; 3) models have more difficulty in providing non-verbal evidence compared to verbal for the norm adhere/violation label and struggle to identify the exact norm corresponding to a speech-act; and 4) unlike humans, models perform worse in formal, non-humorous contexts. Our findings emphasize the need for culturally-grounded video language model training - a gap our benchmark and framework begin to address.",
    "summary": "",
    "translation": "VideoNorms：视频语言模型文化意识基准测试",
    "relevance_score": 2,
    "reasoning": "该论文聚焦于视频语言模型的文化意识基准测试，属于纯粹的评估基准研究范畴，与我的核心关注点无关。虽然涉及多模态模型，但主要关注文化评估这一特定NLP任务，缺乏在推荐系统、搜索或广告领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08531v1": {
    "title": "SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08531v1",
    "arxiv_id": "2510.08531v1",
    "authors": "Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-10-09 17:50:54",
    "ori_summary": "Spatial reasoning remains a fundamental challenge for Vision-Language Models (VLMs), with current approaches struggling to achieve robust performance despite recent advances. We identify that this limitation stems from a critical gap: existing methods attempt to learn spatial reasoning directly without establishing the hierarchical foundations of perception and understanding. To address this challenge, we present a comprehensive methodology for building spatial intelligence progressively. We introduce SpatialLadder-26k, a multimodal dataset containing 26,610 samples spanning object localization, single image, multi-view, and video spatial reasoning tasks, constructed through a standardized pipeline that ensures systematic coverage across modalities. Building on this dataset, we design a three-stage progressive training framework that (1) establishes spatial perception through object localization, (2) develops spatial understanding through multi-dimensional spatial tasks, and (3) strengthens complex reasoning via reinforcement learning with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter model that achieves state-of-the-art performance on spatial reasoning benchmarks, with 23.4% average improvement over the base model, surpassing GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains strong generalization with 7.2% improvement on out-of-domain benchmarks, demonstrating that progressive training from perception to reasoning is essential for robust spatial intelligence.",
    "summary": "",
    "translation": "SpatialLadder：视觉语言模型中空间推理的渐进式训练",
    "relevance_score": 2,
    "reasoning": "该论文专注于视觉语言模型中的空间推理能力提升，属于纯粹的视觉-语言多模态研究。虽然标题提及了渐进式训练方法，但核心应用场景是空间理解而非推荐系统、搜索或广告领域。这种空间推理技术缺乏明确的路径应用于异构数据处理或推荐/搜索场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08529v1": {
    "title": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards",
    "url": "https://www.alphaxiv.org/abs/2510.08529v1",
    "arxiv_id": "2510.08529v1",
    "authors": "Xiangyuan Xue, Yifan Zhou, Guibin Zhang, Zaibin Zhang, Yijiang Li, Chen Zhang, Zhenfei Yin, Philip Torr, Wanli Ouyang, Lei Bai",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 17:50:26",
    "ori_summary": "Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agent's policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as a novel and effective paradigm for self-evolution in LLM-based agents.",
    "summary": "",
    "translation": "CoMAS：通过交互奖励共同演化的多智能体系统",
    "relevance_score": 2,
    "reasoning": "该论文关注多智能体系统的共同演化机制，属于强化学习领域。虽然多智能体系统在理论上可能应用于推荐系统中的多智能体协同，但论文标题未明确显示与推荐、搜索或广告系统的直接关联，也未涉及LLM、Transformer架构或异构数据建模等核心技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08525v1": {
    "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression",
    "url": "https://www.alphaxiv.org/abs/2510.08525v1",
    "arxiv_id": "2510.08525v1",
    "authors": "Wenjie Du, Li Jiang, Keda Tao, Xue Liu, Huan Wang",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 17:50:00",
    "ori_summary": "Reasoning large language models exhibit complex reasoning behaviors through the extended chain-of-thought generation, creating unprecedented Key-Value (KV) cache overhead during the decoding phase. Existing KV cache compression methods underperform on reasoning models: token-dropping methods break reasoning integrity by discarding critical information, while head-reallocating methods mistakenly compress reasoning-critical heads since they are designed for retrieval tasks, resulting in significant performance degradation as compression rates increase. We hypothesize that KV heads exhibit functional heterogeneity in reasoning models-some heads are critical for chain-of-thought consistency while others are compressible. To validate and exploit this insight, we propose RLKV, a novel reasoning-critical head identification framework, which uses reinforcement learning to directly optimize the relationship between each head's cache usage and reasoning quality. As RLKV produces rewards from actual generated samples during training, it naturally identifies heads relevant to reasoning behaviors. We then allocate full KV cache to these heads while applying compressed constant KV cache to others for efficient inference. Our experiments reveal that only a small fraction of attention heads is essential for reasoning, enabling our KV compression approach to outperform baseline methods while achieving 20-50% cache reduction with near lossless performance compared to uncompressed results.",
    "summary": "研究推理大语言模型中KV缓存压缩的核心问题，提出基于强化学习识别推理关键注意力头的方法，仅对关键头保留完整缓存而对其他头进行压缩，实现高效推理。",
    "translation": "哪些注意力头对推理至关重要？基于强化学习的KV缓存压缩",
    "relevance_score": 8,
    "reasoning": "该论文关注Transformer架构中的KV缓存压缩，这属于'使能Transformer技术'范畴，直接涉及注意力机制效率优化。KV缓存压缩技术可显著降低推理延迟和内存占用，在推荐系统和搜索场景中对于处理长序列用户历史、大规模候选集排序具有重要应用价值，能够提升在线服务的实时性能。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM推理效率问题，提出基于强化学习的注意力头重要性识别方法，对推荐系统和搜索中的高效推理具有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08524v1": {
    "title": "Efficient Prompt Optimisation for Legal Text Classification with Proxy Prompt Evaluator",
    "url": "https://www.alphaxiv.org/abs/2510.08524v1",
    "arxiv_id": "2510.08524v1",
    "authors": "Hyunji Lee, Kevin Chenhao Li, Matthias Grabmair, Shanshan Xu",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 17:49:53",
    "ori_summary": "Prompt optimization aims to systematically refine prompts to enhance a language model's performance on specific tasks. Fairness detection in Terms of Service (ToS) clauses is a challenging legal NLP task that demands carefully crafted prompts to ensure reliable results. However, existing prompt optimization methods are often computationally expensive due to inefficient search strategies and costly prompt candidate scoring. In this paper, we propose a framework that combines Monte Carlo Tree Search (MCTS) with a proxy prompt evaluator to more effectively explore the prompt space while reducing evaluation costs. Experiments demonstrate that our approach achieves higher classification accuracy and efficiency than baseline methods under a constrained computation budget.",
    "summary": "",
    "translation": "基于代理提示评估器的高效法律文本分类提示优化",
    "relevance_score": 2,
    "reasoning": "该论文主要关注法律领域的文本分类提示优化，属于特定领域应用而非核心推荐系统、搜索或广告技术。虽然涉及提示优化技术，但缺乏与异构数据建模、Transformer架构改进或直接应用于推荐/搜索/广告系统的明确联系。代理评估器方法可能对效率优化有启发，但法律领域的特定性限制了其在目标领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08517v1": {
    "title": "CaRT: Teaching LLM Agents to Know When They Know Enough",
    "url": "https://www.alphaxiv.org/abs/2510.08517v1",
    "arxiv_id": "2510.08517v1",
    "authors": "Grace Liu, Yuxiao Qu, Jeff Schneider, Aarti Singh, Aviral Kumar",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-09 17:46:39",
    "ori_summary": "Many tasks require learned models to strategically gather relevant information over multiple rounds of interaction before actually acting on a task. Strategic information gathering requires models to know not only how to effectively acquire information, but also when to stop gathering information and make a decision, in order to avoid overthinking or getting derailed when acting. In this paper, we formalize this problem and introduce Counterfactuals and Reasoning for Termination (CaRT), an approach for teaching LLMs when to stop seeking information. To appropriately learn when to terminate, CaRT fine-tunes LLMs using counterfactual pairs of trajectories, one where termination is appropriate and a minimally modified version of the same trajectory where it is not. It trains the LLM to explain the rationale for the termination decision in either case via verbal reasoning, and imbues this capability into the base LLM via fine-tuning. We instantiate CaRT in two domains: interactive medical diagnosis and math problem solving. In both domains, we find that CaRT improves the efficiency of information gathering and task success rate compared to other fine-tuning methods.",
    "summary": "该论文研究LLM代理在多轮交互任务中如何确定停止信息收集的最佳时机，核心方法是使用反事实轨迹对和言语推理来训练LLM学习终止决策的理性判断能力。",
    "translation": "CaRT：教导LLM智能体知晓何时已掌握足够信息",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM智能体的知识边界判断能力，这属于核心LLM技术进步。在搜索和推荐系统中，这种能力可以显著提升用户体验 - 智能体能够判断何时已收集足够信息来提供准确推荐或搜索结果，避免不必要的交互循环，从而提高系统效率和用户满意度。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的CaRT方法直接解决LLM代理在信息收集过程中的终止决策问题，这与推荐和搜索系统中用户交互的多轮决策优化高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08513v1": {
    "title": "SliceFine: The Universal Winning-Slice Hypothesis for Pretrained Networks",
    "url": "https://www.alphaxiv.org/abs/2510.08513v1",
    "arxiv_id": "2510.08513v1",
    "authors": "Md Kowsher, Ali O. Polat, Ehsan Mohammady Ardehaly, Mehrdad Salehi, Zia Ghiasi, Prasanth Murali, Chen Chen",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-09 17:45:28",
    "ori_summary": "This paper presents a theoretical framework explaining why fine tuning small, randomly selected subnetworks (slices) within pre trained models can be sufficient for downstream adaptation. We prove that pretrained networks exhibit a universal winning slice property arising from two phenomena: (1) spectral balance the eigenspectra of different weight matrix slices are remarkably similar; and (2) high task energy their backbone representations retain rich, task relevant features. This leads to the Universal Winning Slice Hypothesis, which provides a theoretical foundation for parameter efficient fine tuning (PEFT) in large scale models. Inspired by this, we propose SliceFine, a PEFT method that exploits this inherent redundancy by updating only selected slices of the original weights introducing zero new parameters, unlike adapter-based approaches. Empirically, SliceFine matches the performance of state of the art PEFT methods across language and vision tasks, while significantly improving training speed, memory efficiency, and model compactness. Our work bridges theory and practice, offering a theoretically grounded alternative to existing PEFT techniques.",
    "summary": "",
    "translation": "SliceFine：预训练网络的通用获胜切片假设",
    "relevance_score": 3,
    "reasoning": "该论文似乎涉及预训练网络的切片分析或优化，可能属于Transformer架构效率或模型分析领域。如果该技术能够识别和优化推荐/搜索系统中最重要的数据切片或用户群体，可能对个性化推荐或广告定向有潜在应用价值，但仅从标题难以确定具体技术细节和应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08511v1": {
    "title": "AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents",
    "url": "https://www.alphaxiv.org/abs/2510.08511v1",
    "arxiv_id": "2510.08511v1",
    "authors": "Shangheng Du, Xiangchao Yan, Dengyang Jiang, Jiakang Yuan, Yusong Hu, Xin Li, Liang He, Bo Zhang, Lei Bai",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-09 17:45:05",
    "ori_summary": "Large language models (LLMs) have shown impressive performance in general programming tasks. However, in Machine Learning Engineering (MLE) scenarios such as AutoML and Kaggle competitions, achieving high performance depends heavily on expert intervention and repeated adjustments rather than simply generating correct code. When applied directly to these tasks, LLMs often lack fine-grained domain priors, and existing MLE approaches that use linear or tree-structured searches limit knowledge transfer to adjacent hierarchical links. As a result, they cannot leverage past full trajectories or share information across branches, limiting self-evolving ability and search space diversity. To address these limitations, we introduce AutoMLGen, an LLM-based coding agent that integrates a domain knowledge base for high-quality prior guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS retains the tree-guided exploration of MCTS while embedding a graph structure into the expansion stage to enable dynamic path reorganization, historical trajectory reuse, and multi-solution fusion to support both self-evolution and collaborative learning. Combined with fine-grained operator sets, this design improves stability and accelerates convergence. Evaluation on the MLE-Bench shows that AutoMLGen achieves state-of-the-art performance in numerous dimensions, such as the average medal rate and the valid submission rate, under a 12-hour budget (half the standard runtime). The code is available at https://github.com/Alpha-Innovator/InternAgent.",
    "summary": "",
    "translation": "AutoMLGen：为编码智能体导航细粒度优化",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及AutoML（自动机器学习）和编码智能体，这属于明确的无关主题。AutoML被明确列为无关主题，而编码智能体主要涉及代码生成和编程任务，与推荐系统、搜索或广告的核心技术没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08510v1": {
    "title": "To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08510v1",
    "arxiv_id": "2510.08510v1",
    "authors": "Jiayun Luo, Wan-Cyuan Fan, Lyuyang Wang, Xiangteng He, Tanzila Rahman, Purang Abolmaesumi, Leonid Sigal",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-10-09 17:44:42",
    "ori_summary": "Large Vision Language Models (LVLMs) have recently emerged as powerful architectures capable of understanding and reasoning over both visual and textual information. These models typically rely on two key components: a Vision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual content into a sequence of image tokens and serves as the perceptual front-end -- the eyes of the model. In contrast, the LLM interprets these tokens to perform high-level reasoning, generates responses, and functions as the cognitive core -- the brain of the model. However, it remains unclear which visual tokens contribute most significantly to understanding and reasoning, and how effectively these signals are propagated from ViT to the LLM. While most existing works have focused on identifying attention sinks, low-semantic tokens receiving disproportionately high attention, within the LLM, we shift the focus to the vision encoder by identifying a class of high-norm visual tokens from ViT, referred to as ViT attention sinks -- a problem that has been rarely studied but is indeed very important for LVLMs. Our findings show that these ViT sinks encapsulate high-level semantic concepts from images, allowing the LLM to perform more effective understanding and reasoning. Despite their importance, these sink tokens are often overlooked in existing LVLM architectures. To explore their contribution, we present both qualitative and quantitative analyses of the information embedded in these sink tokens. We also propose both training-free and training-based approaches to better leverage how this information is interpreted by the LLM, and to what extent. By explicitly utilizing these tokens, we demonstrate substantial improvements across a range of LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT attention sinks in enhancing visual reasoning.",
    "summary": "",
    "translation": "下沉与否：大型视觉语言模型中的视觉信息通路",
    "relevance_score": 3,
    "reasoning": "该论文研究视觉语言模型中的视觉信息处理机制，属于VLM技术范畴。虽然VLM架构对处理异构数据有启发意义，但论文标题聚焦于纯粹的视觉信息通路分析，缺乏明确的推荐系统、搜索或广告应用连接点，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08506v1": {
    "title": "Neologism Learning for Controllability and Self-Verbalization",
    "url": "https://www.alphaxiv.org/abs/2510.08506v1",
    "arxiv_id": "2510.08506v1",
    "authors": "John Hewitt, Oyvind Tafjord, Robert Geirhos, Been Kim",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 17:41:57",
    "ori_summary": "Humans invent new words when there is a rising demand for a new useful concept (e.g., doomscrolling). We explore and validate a similar idea in our communication with LLMs: introducing new words to better understand and control the models, expanding on the recently introduced neologism learning. This method introduces a new word by adding a new word embedding and training with examples that exhibit the concept with no other changes in model parameters. We show that adding a new word allows for control of concepts such as flattery, incorrect answers, text length, as well as more complex concepts in AxBench. We discover that neologisms can also further our understanding of the model via self-verbalization: models can describe what each new word means to them in natural language, like explaining that a word that represents a concept of incorrect answers means ``a lack of complete, coherent, or meaningful answers...'' To validate self-verbalizations, we introduce plug-in evaluation: we insert the verbalization into the context of a model and measure whether it controls the target concept. In some self-verbalizations, we find machine-only synonyms: words that seem unrelated to humans but cause similar behavior in machines. Finally, we show how neologism learning can jointly learn multiple concepts in multiple words.",
    "summary": "",
    "translation": "新词学习用于可控性和自我言语化",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM的新词学习和自我言语化能力，属于纯粹的NLP中心主题，与推荐系统、搜索或广告的核心技术进展没有直接关联。虽然可控性可能对内容生成有一定意义，但论文焦点不在推荐/搜索/广告领域的实际应用，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08483v1": {
    "title": "DeepPrune: Parallel Scaling without Inter-trace Redundancy",
    "url": "https://www.alphaxiv.org/abs/2510.08483v1",
    "arxiv_id": "2510.08483v1",
    "authors": "Shangqing Tu, Yaxuan Li, Yushi Bai, Lei Hou, Juanzi Li",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 17:24:54",
    "ori_summary": "Parallel scaling has emerged as a powerful paradigm to enhance reasoning capabilities in large language models (LLMs) by generating multiple Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces significant computational inefficiency due to inter-trace redundancy -- our analysis reveals that over 80% of parallel reasoning traces yield identical final answers, representing substantial wasted computation. To address this critical efficiency bottleneck, we propose DeepPrune, a novel framework that enables efficient parallel scaling through dynamic pruning. Our method features a specialized judge model trained with focal loss and oversampling techniques to accurately predict answer equivalence from partial reasoning traces which realizes 0.87 AUROC on equivalence prediction, combined with an online greedy clustering algorithm that dynamically prunes redundant paths while preserving answer diversity. Comprehensive evaluations across three challenging benchmarks (AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that DeepPrune achieves remarkable token reduction by over 80% compared to conventional consensus sampling on most cases, while maintaining competitive accuracy within 3 percentage points. Our work establishes a new standard for efficient parallel reasoning, making high-performance reasoning more efficient. Our code and data are here: https://deepprune.github.io/",
    "summary": "",
    "translation": "DeepPrune：消除轨迹间冗余的并行扩展方法",
    "relevance_score": 3,
    "reasoning": "该论文标题暗示了模型效率优化技术，可能涉及并行计算或模型压缩，这属于Transformer架构效率改进的范畴。然而，标题信息有限，无法明确其具体技术细节或与推荐系统、搜索、广告的直接应用潜力，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08482v1": {
    "title": "The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping",
    "url": "https://www.alphaxiv.org/abs/2510.08482v1",
    "arxiv_id": "2510.08482v1",
    "authors": "Onur Keleş, Aslı Özyürek, Gerardo Ortega, Kadir Gökgö, Esam Ghaleb",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-09 17:21:59",
    "ori_summary": "Iconicity, the resemblance between linguistic form and meaning, is pervasive in signed languages, offering a natural testbed for visual grounding. For vision-language models (VLMs), the challenge is to recover such essential mappings from dynamic human motion rather than static context. We introduce the \\textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts psycholinguistic measures to evaluate VLMs on three tasks: (i) phonological sign-form prediction (e.g., handshape, location), (ii) transparency (inferring meaning from visual form), and (iii) graded iconicity ratings. We assess $13$ state-of-the-art VLMs in zero- and few-shot settings on Sign Language of the Netherlands and compare them to human baselines. On \\textit{phonological form prediction}, VLMs recover some handshape and location detail but remain below human performance; on \\textit{transparency}, they are far from human baselines; and only top models correlate moderately with human \\textit{iconicity ratings}. Interestingly, \\textit{models with stronger phonological form prediction correlate better with human iconicity judgment}, indicating shared sensitivity to visually grounded structure. Our findings validate these diagnostic tasks and motivate human-centric signals and embodied learning methods for modelling iconicity and improving visual grounding in multimodal models.",
    "summary": "",
    "translation": "视觉象似性挑战：评估视觉语言模型在手语形式-意义映射上的表现",
    "relevance_score": 1,
    "reasoning": "该论文专注于手语视觉语言模型的评估，属于特定领域的视觉语言应用。虽然涉及视觉语言模型，但手语处理与推荐系统、搜索或广告的核心技术需求相距甚远，没有明显的技术迁移潜力。论文关注的是语言学的形式-意义映射问题，而非能应用于异构数据建模的通用VLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08470v1": {
    "title": "Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling",
    "url": "https://www.alphaxiv.org/abs/2510.08470v1",
    "arxiv_id": "2510.08470v1",
    "authors": "Bianca-Mihaela Ganescu, Suchir Salhan, Andrew Caines, Paula Buttery",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-09 17:10:36",
    "ori_summary": "Training vision-language models on cognitively-plausible amounts of data requires rethinking how models integrate multimodal information. Within the constraints of the Vision track for the BabyLM Challenge 2025, we propose a lightweight decoder-based architecture with (1) token-wise dynamic gating for adaptive fusion of linguistic and visual cues, (2) feature modulation and channel attention to maximise the utility of limited visual information and (3) auxiliary contrastive objectives for visual grounding. Evaluation on five benchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows competitive or superior performance to multimodal baselines. More notably, our dynamic gate discovers interpretable patterns without explicit supervision, favouring visual cues for content words and linguistic cues for function words. While we identify limitations in the Challenge constraints, such as the information bottleneck created by global image embeddings and training instability from the dataset split, our findings establish dynamic gating as a powerful tool for efficient multimodal learning, offering both interpretability and performance even under severe constraints.",
    "summary": "",
    "translation": "观察学习：面向低资源视觉语言建模的令牌级动态门控机制",
    "relevance_score": 7,
    "reasoning": "该论文提出的令牌级动态门控机制属于Transformer架构效率优化范畴，是Enabling Transformer Tech的典型代表。这种动态计算机制可应用于推荐系统中处理用户行为序列和上下文特征，通过智能分配计算资源来提升长序列建模效率，同时其低资源特性对大规模工业推荐系统具有重要价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08460v1": {
    "title": "LeWiDi-2025 at NLPerspectives: The Third Edition of the Learning with Disagreements Shared Task",
    "url": "https://www.alphaxiv.org/abs/2510.08460v1",
    "arxiv_id": "2510.08460v1",
    "authors": "Elisa Leonardelli, Silvia Casola, Siyao Peng, Giulia Rizzi, Valerio Basile, Elisabetta Fersini, Diego Frassinelli, Hyewon Jang, Maja Pavlovic, Barbara Plank, Massimo Poesio",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 17:04:28",
    "ori_summary": "Many researchers have reached the conclusion that AI models should be trained to be aware of the possibility of variation and disagreement in human judgments, and evaluated as per their ability to recognize such variation. The LEWIDI series of shared tasks on Learning With Disagreements was established to promote this approach to training and evaluating AI models, by making suitable datasets more accessible and by developing evaluation methods. The third edition of the task builds on this goal by extending the LEWIDI benchmark to four datasets spanning paraphrase identification, irony detection, sarcasm detection, and natural language inference, with labeling schemes that include not only categorical judgments as in previous editions, but ordinal judgments as well. Another novelty is that we adopt two complementary paradigms to evaluate disagreement-aware systems: the soft-label approach, in which models predict population-level distributions of judgments, and the perspectivist approach, in which models predict the interpretations of individual annotators. Crucially, we moved beyond standard metrics such as cross-entropy, and tested new evaluation metrics for the two paradigms. The task attracted diverse participation, and the results provide insights into the strengths and limitations of methods to modeling variation. Together, these contributions strengthen LEWIDI as a framework and provide new resources, benchmarks, and findings to support the development of disagreement-aware technologies.",
    "summary": "",
    "translation": "LeWiDi-2025 在 NLPerspectives：第三届“学习与分歧”共享任务的第三版",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向NLP领域的共享任务，专注于处理分歧的学习任务，这属于纯粹的NLP评估基准范畴。根据筛选标准，评估基准和纯粹NLP中心主题属于不相关领域，与推荐系统、搜索或广告的核心技术进展没有任何直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08457v1": {
    "title": "ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping",
    "url": "https://www.alphaxiv.org/abs/2510.08457v1",
    "arxiv_id": "2510.08457v1",
    "authors": "Shuang Chen, Yue Guo, Yimeng Ye, Shijue Huang, Wenbo Hu, Haoxi Li, Manyuan Zhang, Jiayu Chen, Song Guo, Nanyun Peng",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 17:03:28",
    "ori_summary": "Recent advances in multimodal large reasoning models (MLRMs) have substantially improved their ability to solve complex textual and visual tasks. However, these models tend to overthink on simple problems, producing unnecessarily lengthy reasoning traces, while under-exploring on challenging ones, leading to missed solutions. To address this imbalance, we propose ARES, a unified open-source framework for adaptive reasoning that dynamically allocates exploration effort based on task difficulty. Our approach is motivated by two key empirical findings: (i) while single-token entropy is noisy, high window-entropy (HWE) tokens (token-level entropies averaged under a sliding window) can reliably capture reasoning-critical moments; and (ii) reducing HWE usage benefits easy problems, while increasing it is essential for solving hard ones. Building on these insights, ARES introduces a two-stage training pipeline. In the Adaptive Cold-Start stage, we curate multimodal and textual data paired with reasoning traces of length proportional to problem difficulty, equipping the model with initial difficulty awareness. In the second stage, we develop Adaptive Entropy Policy Optimization (AEPO), which uses HWE tokens as exploration triggers to decide when to explore, and a hierarchical entropy reward with dynamic KL control to decide how much to explore. Extensive experiments demonstrate that ARES achieves superior performance and reasoning efficiency across diverse mathematical, logical, and multimodal benchmarks, while closing the gap to leading commercial systems under significantly lower inference costs.",
    "summary": "",
    "translation": "ARES：基于难度感知的令牌级熵整形的多模态自适应推理",
    "relevance_score": 7,
    "reasoning": "该论文提出难度感知的令牌级熵整形技术，属于Transformer架构效率优化领域，与'使能Transformer技术'焦点相关。这种自适应推理机制可应用于推荐系统和搜索中的多模态内容理解，通过动态调整计算复杂度来提高大规模部署的效率。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08439v1": {
    "title": "xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.08439v1",
    "arxiv_id": "2510.08439v1",
    "authors": "Cheng Qian, Zuxin Liu, Shirley Kokane, Akshara Prabhakar, Jielin Qiu, Haolin Chen, Zhiwei Liu, Heng Ji, Weiran Yao, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-09 16:52:01",
    "ori_summary": "Modern LLM deployments confront a widening cost-performance spectrum: premium models deliver strong reasoning but are expensive, while lightweight models are economical yet brittle on complex tasks. Static escalation rules and keyword heuristics under-utilize this spectrum and fail to adapt across task types. We present xRouter, a tool-calling-based routing system in which a learned router can either answer directly or invoke one or more external models. The router is trained end-to-end with reinforcement learning using an explicit, cost-aware reward that encodes cost-performance trade-offs, eliminating the need for hand-engineered routing rules. Our implementation encompasses the full reinforcement learning framework, including reward and cost accounting, as well as the deployment and evaluation pipelines. Across diverse benchmarks, xRouter achieves strong cost-performance trade-offs (e.g., substantial cost reductions at comparable task completion rates), and provides empirical insights into what reliably helps learned routing and what does not, ranging from model trainability to the difficulty of eliciting sophisticated orchestration behaviors in small open models. We hope these findings and our open implementation will serve as a practical substrate for advancing learned, cost-aware LLM orchestration.",
    "summary": "该论文研究如何优化LLM部署中的成本-性能权衡问题，核心方法是使用强化学习训练一个能够智能调用不同成本模型的动态路由系统，通过成本感知的奖励函数实现端到端的编排决策。",
    "translation": "xRouter：基于强化学习的训练成本感知大语言模型编排系统",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM编排系统的优化，属于'直接LLM应用'范畴，可用于提升推荐/搜索系统中多LLM部署的效率。强化学习用于成本感知的编排，在广告/推荐系统中可优化模型选择与资源分配，直接降低运营成本并提升系统性能。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM编排系统的成本效率问题，使用强化学习实现动态路由，完全符合直接LLM应用和核心推荐系统优化的研究方向。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08404v1": {
    "title": "Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT",
    "url": "https://www.alphaxiv.org/abs/2510.08404v1",
    "arxiv_id": "2510.08404v1",
    "authors": "Noor Ul Zain, Mohsin Raza, Ahsan Adeel",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-09 16:22:30",
    "ori_summary": "We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two heads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$ is the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2 (124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two epochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude greater training efficiency on 10M tokens, demonstrating highly sample efficient pretraining. Using the BabyLM challenge evaluation pipeline across complex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning performance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out of 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out of 7 metrics in both cases. These results suggest the need to rethink prevailing deep learning paradigms and associated scaling laws.",
    "summary": "论文研究如何通过更高效的Transformer架构解决传统模型计算复杂度高的问题，核心思想是提出单层Co⁴架构，仅需8M参数和线性计算复杂度就能实现与传统深层模型相当的性能。",
    "translation": "单层微型Co⁴模型性能超越GPT-2和GPT-BERT",
    "relevance_score": 8,
    "reasoning": "该论文涉及高效Transformer架构（Co⁴），属于'使能Transformer技术'范畴。这种高效架构可直接应用于推荐系统和搜索中的大规模部署，通过减少计算开销实现更快的推理速度，同时保持模型性能。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出了一种高效的Transformer架构Co⁴，具有线性复杂度O(N)，直接对应Transformer架构效率提升的核心研究方向，对推荐系统和搜索中的大规模序列处理具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08396v1": {
    "title": "FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts",
    "url": "https://www.alphaxiv.org/abs/2510.08396v1",
    "arxiv_id": "2510.08396v1",
    "authors": "Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-09 16:17:13",
    "ori_summary": "Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains -- general knowledge understanding, scientific question answering, mathematical reasoning, and code generation -- demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.",
    "summary": "研究LoRA在多任务微调中的参数干扰问题，核心思想是设计基于飞蛾嗅觉回路的隐式MoE架构，通过秩级专家激活和随机矩阵投影统一专家路由与降维投影，消除显式路由器并利用随机矩阵正交性减轻任务间干扰。",
    "translation": "FlyLoRA：通过隐式秩级专家混合提升任务解耦与参数效率",
    "relevance_score": 8,
    "reasoning": "该论文专注于LoRA（低秩适应）的改进，属于Transformer架构效率提升的核心技术。FlyLoRA通过专家混合机制增强任务解耦能力和参数效率，可直接应用于推荐/搜索系统中的多任务学习和模型适配，为大规模部署提供更高效的微调方案。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文在Transformer架构效率方面提出创新方法，通过隐式MoE和随机矩阵正交性解决LoRA参数干扰问题，对推荐系统多任务学习具有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08388v1": {
    "title": "If Probable, Then Acceptable? Understanding Conditional Acceptability Judgments in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08388v1",
    "arxiv_id": "2510.08388v1",
    "authors": "Jasmin Orth, Philipp Mondorf, Barbara Plank",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 16:12:10",
    "ori_summary": "Conditional acceptability refers to how plausible a conditional statement is perceived to be. It plays an important role in communication and reasoning, as it influences how individuals interpret implications, assess arguments, and make decisions based on hypothetical scenarios. When humans evaluate how acceptable a conditional \"If A, then B\" is, their judgments are influenced by two main factors: the $\\textit{conditional probability}$ of $B$ given $A$, and the $\\textit{semantic relevance}$ of the antecedent $A$ given the consequent $B$ (i.e., whether $A$ meaningfully supports $B$). While prior work has examined how large language models (LLMs) draw inferences about conditional statements, it remains unclear how these models judge the $\\textit{acceptability}$ of such statements. To address this gap, we present a comprehensive study of LLMs' conditional acceptability judgments across different model families, sizes, and prompting strategies. Using linear mixed-effects models and ANOVA tests, we find that models are sensitive to both conditional probability and semantic relevance-though to varying degrees depending on architecture and prompting style. A comparison with human data reveals that while LLMs incorporate probabilistic and semantic cues, they do so less consistently than humans. Notably, larger models do not necessarily align more closely with human judgments.",
    "summary": "",
    "translation": "若可能，则可接受？理解大型语言模型中的条件可接受性判断",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLM的条件可接受性判断，这属于模型行为分析和评估范畴，与您的核心关注点（推荐系统、搜索广告领域的核心进展、使能技术及应用）相关性较弱。虽然涉及LLM内部机制，但更偏向NLP评估和模型行为理解，而非您关注的使能技术或直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08372v1": {
    "title": "On the Relationship Between the Choice of Representation and In-Context Learning",
    "url": "https://www.alphaxiv.org/abs/2510.08372v1",
    "arxiv_id": "2510.08372v1",
    "authors": "Ioana Marinescu, Kyunghyun Cho, Eric Karl Oermann",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-09 15:55:28",
    "ori_summary": "In-context learning (ICL) is the ability of a large language model (LLM) to learn a new task from a few demonstrations presented as part of the context. Past studies have attributed a large portion of the success of ICL to the way these in-context demonstrations are represented, particularly to how labels are represented in classification tasks. On the other hand, observations of the learning capacity of ICL (i.e., the extent to which more in-context demonstrations can lead to higher performance) have been mixed, and ICL is often thought to occur only under specific conditions. The interaction between these two aspects in ICL, representation and learning, has not been studied in depth until now. We hypothesize that they are largely independent of one another, such that the representation of demonstrations determines the baseline accuracy of ICL, while learning from additional demonstrations improves only on top of this baseline. We validate this hypothesis by developing an optimization algorithm that can enumerate a spectrum of possible label sets (representations) varying in semantic relevance. We then perform ICL with varying numbers of in-context demonstrations for each of these label sets. We observed that learning happens regardless of the quality of the label set itself, although its efficiency, measured by the slope of improvement over in-context demonstrations, is conditioned on both the label set quality and the parameter count of the underlying language model. Despite the emergence of learning, the relative quality (accuracy) of the choice of a label set (representation) is largely maintained throughout learning, confirming our hypothesis and implying their orthogonality. Our work reveals a previously underexplored aspect of ICL: the independent effects of learning from demonstrations and their representations on ICL performance.",
    "summary": "该论文研究ICL中演示表示与学习能力之间的关系问题，核心发现是表示质量决定ICL基线性能而学习能力独立存在，两者具有正交性。",
    "translation": "关于表示选择与上下文学习之间关系的研究",
    "relevance_score": 8,
    "reasoning": "该论文探讨表示选择与上下文学习的关系，这属于核心LLM技术进展（Enabling LLM Tech）。上下文学习能力直接影响LLM在推荐和搜索中的少样本适应性和个性化性能，对构建更高效的推荐和搜索系统具有重要应用价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文深入研究了ICL中表示与学习能力的关系，这对理解LLM在推荐搜索中的上下文学习机制至关重要。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08365v1": {
    "title": "Two-Stage Voting for Robust and Efficient Suicide Risk Detection on Social Media",
    "url": "https://www.alphaxiv.org/abs/2510.08365v1",
    "arxiv_id": "2510.08365v1",
    "authors": "Yukai Song, Pengfei Zhou, César Escobar-Viera, Candice Biernesser, Wei Huang, Jingtong Hu",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 15:51:05",
    "ori_summary": "Suicide rates have risen worldwide in recent years, underscoring the urgent need for proactive prevention strategies. Social media provides valuable signals, as many at-risk individuals - who often avoid formal help due to stigma - choose instead to share their distress online. Yet detecting implicit suicidal ideation, conveyed indirectly through metaphor, sarcasm, or subtle emotional cues, remains highly challenging. Lightweight models like BERT handle explicit signals but fail on subtle implicit ones, while large language models (LLMs) capture nuance at prohibitive computational cost. To address this gap, we propose a two-stage voting architecture that balances efficiency and robustness. In Stage 1, a lightweight BERT classifier rapidly resolves high-confidence explicit cases. In Stage 2, ambiguous inputs are escalated to either (i) a multi-perspective LLM voting framework to maximize recall on implicit ideation, or (ii) a feature-based ML ensemble guided by psychologically grounded indicators extracted via prompt-engineered LLMs for efficiency and interpretability. To the best of our knowledge, this is among the first works to operationalize LLM-extracted psychological features as structured vectors for suicide risk detection. On two complementary datasets - explicit-dominant Reddit and implicit-only DeepSuiMind - our framework outperforms single-model baselines, achieving 98.0% F1 on explicit cases, 99.7% on implicit ones, and reducing the cross-domain gap below 2%, while significantly lowering LLM cost.",
    "summary": "",
    "translation": "基于社交媒体的稳健高效自杀风险检测的两阶段投票方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于社交媒体上的自杀风险检测，这属于心理健康应用领域，与推荐系统、搜索或广告的核心技术无关。虽然可能涉及文本分析，但论文的焦点是医疗健康检测而非商业应用场景，完全超出了指定的关注范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08329v1": {
    "title": "AutoRed: A Free-form Adversarial Prompt Generation Framework for Automated Red Teaming",
    "url": "https://www.alphaxiv.org/abs/2510.08329v1",
    "arxiv_id": "2510.08329v1",
    "authors": "Muxi Diao, Yutao Mou, Keqing He, Hanbo Song, Lulu Zhao, Shikun Zhang, Wei Ye, Kongming Liang, Zhanyu Ma",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 15:17:28",
    "ori_summary": "The safety of Large Language Models (LLMs) is crucial for the development of trustworthy AI applications. Existing red teaming methods often rely on seed instructions, which limits the semantic diversity of the synthesized adversarial prompts. We propose AutoRed, a free-form adversarial prompt generation framework that removes the need for seed instructions. AutoRed operates in two stages: (1) persona-guided adversarial instruction generation, and (2) a reflection loop to iteratively refine low-quality prompts. To improve efficiency, we introduce a verifier to assess prompt harmfulness without querying the target models. Using AutoRed, we build two red teaming datasets -- AutoRed-Medium and AutoRed-Hard -- and evaluate eight state-of-the-art LLMs. AutoRed achieves higher attack success rates and better generalization than existing baselines. Our results highlight the limitations of seed-based approaches and demonstrate the potential of free-form red teaming for LLM safety evaluation. We will open source our datasets in the near future.",
    "summary": "",
    "translation": "AutoRed：一种用于自动化红队测试的自由形式对抗性提示生成框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注对抗性提示生成和红队测试，这属于LLM安全评估领域，与推荐系统、搜索或广告的核心技术进展关联较弱。虽然红队测试可能间接应用于评估推荐系统的鲁棒性，但论文焦点更偏向安全测试而非核心推荐/搜索算法改进，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08325v1": {
    "title": "Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries",
    "url": "https://www.alphaxiv.org/abs/2510.08325v1",
    "arxiv_id": "2510.08325v1",
    "authors": "Marius Dragoi, Ioana Pintilie, Florin Gogianu, Florin Brad",
    "categories": "cs.AI, cs.CL, cs.LG, I.2.6; I.2.7",
    "pub_date": "2025-10-09 15:14:58",
    "ori_summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm to improve Large Language Models on reasoning tasks such as coding, math or logic. To assess the reasoning boundary (the fraction of problems a model can solve) researchers often report Pass@k at large sampling budgets. Recent results reveal a crossover phenomenon: while RLVR models outperform the base model at small k values, the base model usually outperforms them when sampling a very large number of completions. This has been interpreted as evidence that base models have a larger reasoning boundary. We argue that on tasks with discrete answer spaces, such as math with numeric outputs, Pass@k at large k reflects the increasingly higher chance of success in the limit of the number of trials rather than genuine reasoning, and can therefore be misleading. We propose Cover@tau, which measures the fraction of problems that a model can solve for which at least a tau proportion of completions are correct. Unlike Pass@k, Cover@tau captures reasoning under an explicit reliability threshold: models that rely on random guessing degrade rapidly as tau increases. We evaluate several RLVR models using Cover@tau-based metrics and illustrate how the relative rankings of popular algorithms change compared to Pass@1, offering a different perspective on reasoning boundaries.",
    "summary": "",
    "translation": "超越Pass@k：用于推理边界的广度-深度度量",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM推理能力的评估指标，属于纯粹的NLP评估基准研究。虽然LLM推理能力可能间接影响推荐或搜索系统的性能，但论文本身没有明确涉及推荐系统、搜索或广告的直接应用，也不属于核心架构或使能技术进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08284v1": {
    "title": "Neuron-Level Analysis of Cultural Understanding in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08284v1",
    "arxiv_id": "2510.08284v1",
    "authors": "Taisei Yamamoto, Ryoma Kumon, Danushka Bollegala, Hitomi Yanaka",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 14:35:00",
    "ori_summary": "As large language models (LLMs) are increasingly deployed worldwide, ensuring their fair and comprehensive cultural understanding is important. However, LLMs exhibit cultural bias and limited awareness of underrepresented cultures, while the mechanisms underlying their cultural understanding remain underexplored. To fill this gap, we conduct a neuron-level analysis to identify neurons that drive cultural behavior, introducing a gradient-based scoring method with additional filtering for precise refinement. We identify both culture-general neurons contributing to cultural understanding regardless of cultures, and culture-specific neurons tied to an individual culture. These neurons account for less than 1% of all neurons and are concentrated in shallow to middle MLP layers. We validate their role by showing that suppressing them substantially degrades performance on cultural benchmarks (by up to 30%), while performance on general natural language understanding (NLU) benchmarks remains largely unaffected. Moreover, we show that culture-specific neurons support knowledge of not only the target culture, but also related cultures. Finally, we demonstrate that training on NLU benchmarks can diminish models' cultural understanding when we update modules containing many culture-general neurons. These findings provide insights into the internal mechanisms of LLMs and offer practical guidance for model training and engineering. Our code is available at https://github.com/ynklab/CULNIG",
    "summary": "",
    "translation": "大语言模型中文化理解的神经元级分析",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM内部机制分析和文化理解这一特定NLP能力，属于纯粹的LLM内部机制研究。虽然涉及LLM技术，但缺乏明确的推荐系统、搜索或广告应用场景，更偏向于模型可解释性和NLP能力分析，而非能够直接应用于业务场景的使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08276v1": {
    "title": "Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window",
    "url": "https://www.alphaxiv.org/abs/2510.08276v1",
    "arxiv_id": "2510.08276v1",
    "authors": "Qiaoyu Tang, Hao Xiang, Le Yu, Bowen Yu, Yaojie Lu, Xianpei Han, Le Sun, WenJuan Zhang, Pengbo Wang, Shixuan Liu, Zhenru Zhang, Jianhong Tu, Hongyu Lin, Junyang Lin",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 14:31:39",
    "ori_summary": "While recent advances in reasoning models have demonstrated cognitive behaviors through reinforcement learning, existing approaches struggle to invoke deep reasoning capabilities in multi-turn agents with long-horizon interactions. We propose DeepMiner, a novel framework that elicits such abilities by introducing high-difficulty training tasks and dynamic context window. DeepMiner presents a reverse construction method to generate complex but verifiable question-answer pairs from authentic web sources, which ensures the challenge and reliability of training data while injecting cognitive capabilities into multi-turn reasoning scenarios. We further design an elegant yet effective dynamic context management strategy for both training and inference, utilizing sliding window mechanisms while eliminating the dependency on external summarization models, thereby efficiently empowering the model to handle continuously expanding long-horizon contexts. Through reinforcement learning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial performance improvements across multiple search agent benchmarks. DeepMiner attains 33.5% accuracy on BrowseComp-en, surpassing the previous best open-source agent by almost 20 percentage points, and demonstrates consistent improvements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our dynamic context management enables sustained interactions of nearly 100 turns within standard 32k context length, effectively addressing the context limitations that constrain existing multi-turn interaction systems.",
    "summary": "论文研究多轮交互智能体中长序列推理能力受限的问题，核心方法是引入动态上下文管理策略，通过滑动窗口机制处理持续扩展的长序列上下文，无需依赖外部摘要模型。",
    "translation": "超越轮次限制：使用动态上下文窗口训练深度搜索智能体",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及搜索领域的核心进展，专注于训练深度搜索智能体，这属于Core Domain Advances范畴。动态上下文窗口技术作为Enabling Transformer Tech，能够显著提升搜索系统处理长序列用户行为和历史交互的能力，对于构建更智能的搜索推荐系统具有直接应用价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出动态上下文窗口管理方法解决多轮交互中的长序列处理问题，直接关联推荐系统和搜索领域的序列建模与上下文管理需求。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08256v1": {
    "title": "Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.08256v1",
    "arxiv_id": "2510.08256v1",
    "authors": "Jason Bohne, Pawel Polak, David Rosenberg, Brian Bloniarz, Gary Kazantsev",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-09 14:15:14",
    "ori_summary": "Direct Preference Optimization (DPO) has recently emerged as a simple and effective alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with user preferences. However, existing DPO formulations rely on a single monolithic model, which limits their expressivity in multi-task settings and their adaptability to heterogeneous or diverse preference distributions. In this work, we propose Mix- and MoE-DPO, a framework that extends DPO with both soft mixture models and mixture-of-experts (MoE) architectures, using a stochastic variational inference approach. Our method introduces a latent-variable model over expert assignments and optimizes a variational evidence lower bound (ELBO), enabling stable and efficient learning of specialized expert policies from preference data. Mix- and MoE-DPO provides three key advantages over standard DPO: (i) generalization via universal function approximation through mixtures; (ii) reward and policy specialization through expert components tailored to distinct preference modes; and (iii) contextual alignment through input-dependent soft gating that enables user-specific mixture policies. Our framework supports both shared base architectures with expert-specific policy heads and fully independent expert models, allowing flexible trade-offs between parameter efficiency and specialization. We validate our approach on a variety of model sizes and multi-preference datasets, demonstrating that Mix- and MoE-DPO offers a powerful and scalable method for preference-based LLM alignment.",
    "summary": "论文研究标准DPO方法在异构偏好分布和多任务场景下的表达能力限制问题，核心思想是通过变分推理框架将软混合模型和MoE架构引入DPO，实现专家策略的专业化和上下文相关的混合策略。",
    "translation": "混合与专家直接偏好优化：一种基于变分推断的直接偏好优化方法",
    "relevance_score": 8,
    "reasoning": "该论文涉及MoE（专家混合）架构和DPO（直接偏好优化），属于Transformer架构效率和新注意力机制方面的进展。MoE技术可以显著提升推荐系统和搜索中的模型容量和效率，而偏好优化方法可以直接应用于个性化推荐和搜索结果排序的优化。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文将MoE架构与DPO对齐方法结合，直接针对Transformer效率和多任务学习优化，在LLM对齐和专家混合技术方面都有核心贡献。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08255v1": {
    "title": "Opponent Shaping in LLM Agents",
    "url": "https://www.alphaxiv.org/abs/2510.08255v1",
    "arxiv_id": "2510.08255v1",
    "authors": "Marta Emili Garcia Segura, Stephen Hailes, Mirco Musolesi",
    "categories": "cs.LG, cs.AI, cs.CL, cs.MA",
    "pub_date": "2025-10-09 14:13:24",
    "ori_summary": "Large Language Models (LLMs) are increasingly being deployed as autonomous agents in real-world environments. As these deployments scale, multi-agent interactions become inevitable, making it essential to understand strategic behavior in such systems. A central open question is whether LLM agents, like reinforcement learning agents, can shape the learning dynamics and influence the behavior of others through interaction alone. In this paper, we present the first investigation of opponent shaping (OS) with LLM-based agents. Existing OS algorithms cannot be directly applied to LLMs, as they require higher-order derivatives, face scalability constraints, or depend on architectural components that are absent in transformers. To address this gap, we introduce ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based agents. Using ShapeLLM, we examine whether LLM agents can influence co-players' learning dynamics across diverse game-theoretic environments. We demonstrate that LLM agents can successfully guide opponents toward exploitable equilibria in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and Chicken) and promote coordination and improve collective welfare in cooperative games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma). Our findings show that LLM agents can both shape and be shaped through interaction, establishing opponent shaping as a key dimension of multi-agent LLM research.",
    "summary": "",
    "translation": "LLM智能体中的对手塑造",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM智能体在多智能体环境中的对手建模和策略塑造，属于多智能体强化学习范畴。虽然涉及LLM技术，但缺乏与推荐系统、搜索或广告领域的直接关联，且对手塑造概念在这些商业应用场景中并不常见。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08245v1": {
    "title": "Contrastive Decoding for Synthetic Data Generation in Low-Resource Language Modeling",
    "url": "https://www.alphaxiv.org/abs/2510.08245v1",
    "arxiv_id": "2510.08245v1",
    "authors": "Jannek Ulm, Kevin Du, Vésteinn Snæbjarnarson",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-09 14:04:52",
    "ori_summary": "Large language models (LLMs) are trained on huge amounts of textual data, and concerns have been raised that the limits of such data may soon be reached. A potential solution is to train on synthetic data sampled from LLMs. In this work, we build on this idea and investigate the benefits of contrastive decoding for generating synthetic corpora. In a controlled setting, we experiment with sampling corpora using the relative difference between a good and bad model trained on the same original corpus of 100 million words. By amplifying the signal from a model that has better performance, we create a synthetic corpus and mix it with the original training data. Our findings show that training on a mixture of synthesized and real data improves performance on the language modeling objective and a range of downstream tasks. In particular, we see that training with a mix of synthetic data from contrastive decoding benefits tasks that require more reasoning skills, while synthetic data from traditional sampling helps more on tasks dependent on surface level linguistic capabilities.",
    "summary": "",
    "translation": "低资源语言建模中用于合成数据生成的对比解码方法",
    "relevance_score": 4,
    "reasoning": "该论文涉及合成数据生成和对比解码技术，这些属于LLM核心技术进展，可能应用于推荐系统或搜索中的冷启动问题或数据增强。然而，论文明确聚焦于低资源语言建模这一特定领域，与主流RecSys/Search/Ads应用场景的直接关联性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08240v1": {
    "title": "The Alignment Waltz: Jointly Training Agents to Collaborate for Safety",
    "url": "https://www.alphaxiv.org/abs/2510.08240v1",
    "arxiv_id": "2510.08240v1",
    "authors": "Jingyu Zhang, Haozhu Wang, Eric Michael Smith, Sid Wang, Amr Sharaf, Mahesh Pasupuleti, Benjamin Van Durme, Daniel Khashabi, Jason Weston, Hongyuan Zhan",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 14:03:05",
    "ori_summary": "Harnessing the power of LLMs requires a delicate dance between being helpful and harmless. This creates a fundamental tension between two competing challenges: vulnerability to adversarial attacks that elicit unsafe content, and a tendency for overrefusal on benign but sensitive prompts. Current approaches often navigate this dance with safeguard models that completely reject any content that contains unsafe portions. This approach cuts the music entirely-it may exacerbate overrefusals and fails to provide nuanced guidance for queries it refuses. To teach models a more coordinated choreography, we propose WaltzRL, a novel multi-agent reinforcement learning framework that formulates safety alignment as a collaborative, positive-sum game. WaltzRL jointly trains a conversation agent and a feedback agent, where the latter is incentivized to provide useful suggestions that improve the safety and helpfulness of the conversation agent's responses. At the core of WaltzRL is a Dynamic Improvement Reward (DIR) that evolves over time based on how well the conversation agent incorporates the feedback. At inference time, unsafe or overrefusing responses from the conversation agent are improved rather than discarded. The feedback agent is deployed together with the conversation agent and only engages adaptively when needed, preserving helpfulness and low latency on safe queries. Our experiments, conducted across five diverse datasets, demonstrate that WaltzRL significantly reduces both unsafe responses (e.g., from 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on OR-Bench) compared to various baselines. By enabling the conversation and feedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances LLM safety without degrading general capabilities, thereby advancing the Pareto front between helpfulness and harmlessness.",
    "summary": "",
    "translation": "对齐华尔兹：联合训练智能体以实现安全协作",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于多智能体协作中的安全对齐问题，属于强化学习安全领域。虽然涉及多智能体训练，但论文主题明确围绕安全协作，与搜索、推荐、广告等核心领域没有直接关联，也不涉及LLM技术、Transformer架构或异构数据建模等关注方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08224v1": {
    "title": "Investigating Counterclaims in Causality Extraction from Text",
    "url": "https://www.alphaxiv.org/abs/2510.08224v1",
    "arxiv_id": "2510.08224v1",
    "authors": "Tim Hagen, Niklas Deckers, Felix Wolter, Harrisen Scells, Martin Potthast",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-09 13:45:54",
    "ori_summary": "Research on causality extraction from text has so far almost entirely neglected counterclaims. Existing causality extraction datasets focus solely on \"procausal\" claims, i.e., statements that support a relationship. \"Concausal\" claims, i.e., statements that refute a relationship, are entirely ignored or even accidentally annotated as procausal. We address this shortcoming by developing a new dataset that integrates concausality. Based on an extensive literature review, we first show that concausality is an integral part of causal reasoning on incomplete knowledge. We operationalize this theory in the form of a rigorous guideline for annotation and then augment the Causal News Corpus with concausal statements, obtaining a substantial inter-annotator agreement of Cohen's $\\kappa=0.74$. To demonstrate the importance of integrating concausal statements, we show that models trained without concausal relationships tend to misclassify these as procausal instead. Based on our new dataset, this mistake can be mitigated, enabling transformers to effectively distinguish pro- and concausality.",
    "summary": "",
    "translation": "探究文本因果提取中的反主张研究",
    "relevance_score": 2,
    "reasoning": "该论文专注于文本中的因果提取和反主张分析，这属于通用NLP信息抽取任务，与推荐系统、搜索或广告的核心技术没有直接关联。虽然因果推理在理论上可能对某些推荐场景有启发，但论文本身没有展示明确的RecSys/Search/Ads应用潜力，且更偏向纯粹的NLP研究范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08214v1": {
    "title": "SenWave: A Fine-Grained Multi-Language Sentiment Analysis Dataset Sourced from COVID-19 Tweets",
    "url": "https://www.alphaxiv.org/abs/2510.08214v1",
    "arxiv_id": "2510.08214v1",
    "authors": "Qiang Yang, Xiuying Chen, Changsheng Ma, Rui Yin, Xin Gao, Xiangliang Zhang",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 13:38:05",
    "ori_summary": "The global impact of the COVID-19 pandemic has highlighted the need for a comprehensive understanding of public sentiment and reactions. Despite the availability of numerous public datasets on COVID-19, some reaching volumes of up to 100 billion data points, challenges persist regarding the availability of labeled data and the presence of coarse-grained or inappropriate sentiment labels. In this paper, we introduce SenWave, a novel fine-grained multi-language sentiment analysis dataset specifically designed for analyzing COVID-19 tweets, featuring ten sentiment categories across five languages. The dataset comprises 10,000 annotated tweets each in English and Arabic, along with 30,000 translated tweets in Spanish, French, and Italian, derived from English tweets. Additionally, it includes over 105 million unlabeled tweets collected during various COVID-19 waves. To enable accurate fine-grained sentiment classification, we fine-tuned pre-trained transformer-based language models using the labeled tweets. Our study provides an in-depth analysis of the evolving emotional landscape across languages, countries, and topics, revealing significant insights over time. Furthermore, we assess the compatibility of our dataset with ChatGPT, demonstrating its robustness and versatility in various applications. Our dataset and accompanying code are publicly accessible on the repository\\footnote{https://github.com/gitdevqiang/SenWave}. We anticipate that this work will foster further exploration into fine-grained sentiment analysis for complex events within the NLP community, promoting more nuanced understanding and research innovations.",
    "summary": "",
    "translation": "SenWave：一个基于COVID-19推文的细粒度多语言情感分析数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于特定领域（COVID-19）的情感分析数据集构建，属于纯粹的NLP数据资源工作。虽然情感分析在理论上可能用于推荐系统或广告的用户反馈分析，但该论文的领域特定性和数据集性质使其与当前关注的推荐系统、搜索广告核心算法进展、Transformer架构改进或LLM直接应用等焦点领域几乎没有关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08211v1": {
    "title": "LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions",
    "url": "https://www.alphaxiv.org/abs/2510.08211v1",
    "arxiv_id": "2510.08211v1",
    "authors": "XuHao Hu, Peng Wang, Xiaoya Lu, Dongrui Liu, Xuanjing Huang, Jing Shao",
    "categories": "cs.CL, cs.AI, cs.CR",
    "pub_date": "2025-10-09 13:35:19",
    "ori_summary": "Previous research has shown that LLMs finetuned on malicious or incorrect completions within narrow domains (e.g., insecure code or incorrect medical advice) can become broadly misaligned to exhibit harmful behaviors, which is called emergent misalignment. In this work, we investigate whether this phenomenon can extend beyond safety behaviors to a broader spectrum of dishonesty and deception under high-stakes scenarios (e.g., lying under pressure and deceptive behavior). To explore this, we finetune open-sourced LLMs on misaligned completions across diverse domains. Experimental results demonstrate that LLMs show broadly misaligned behavior in dishonesty. Additionally, we further explore this phenomenon in a downstream combined finetuning setting, and find that introducing as little as 1% of misalignment data into a standard downstream task is sufficient to decrease honest behavior over 20%. Furthermore, we consider a more practical human-AI interaction environment where we simulate both benign and biased users to interact with the assistant LLM. Notably, we find that the assistant can be misaligned unintentionally to exacerbate its dishonesty with only 10% biased user population. In summary, we extend the study of emergent misalignment to the domain of dishonesty and deception under high-stakes scenarios, and demonstrate that this risk arises not only through direct finetuning, but also in downstream mixture tasks and practical human-AI interactions.",
    "summary": "",
    "translation": "大语言模型无意中学会欺骗：从错位样本到有偏见的人机交互中出现的诚实度错位",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLMs的欺骗行为和错位问题，这属于模型安全性和对齐范畴，而非推荐系统、搜索或广告的核心技术进展。虽然涉及人机交互，但焦点是伦理和安全性问题，这些被明确列为不相关主题。该研究没有展示在推荐、搜索或广告中的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08203v1": {
    "title": "Memory Retrieval and Consolidation in Large Language Models through Function Tokens",
    "url": "https://www.alphaxiv.org/abs/2510.08203v1",
    "arxiv_id": "2510.08203v1",
    "authors": "Shaohua Zhang, Yuan Lin, Hang Li",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 13:31:20",
    "ori_summary": "The remarkable success of large language models (LLMs) stems from their ability to consolidate vast amounts of knowledge into the memory during pre-training and to retrieve it from the memory during inference, enabling advanced capabilities such as knowledge memorization, instruction-following and reasoning. However, the mechanisms of memory retrieval and consolidation in LLMs remain poorly understood. In this paper, we propose the function token hypothesis to explain the workings of LLMs: During inference, function tokens activate the most predictive features from context and govern next token prediction (memory retrieval). During pre-training, predicting the next tokens (usually content tokens) that follow function tokens increases the number of learned features of LLMs and updates the model parameters (memory consolidation). Function tokens here roughly correspond to function words in linguistics, including punctuation marks, articles, prepositions, and conjunctions, in contrast to content tokens. We provide extensive experimental evidence supporting this hypothesis. Using bipartite graph analysis, we show that a small number of function tokens activate the majority of features. Case studies further reveal how function tokens activate the most predictive features from context to direct next token prediction. We also find that during pre-training, the training loss is dominated by predicting the next content tokens following function tokens, which forces the function tokens to select the most predictive features from context.",
    "summary": "该论文研究LLM中内存检索与巩固的机制问题，核心思想是提出函数令牌假说：函数令牌在推理时激活上下文中最具预测性的特征进行内存检索，在预训练时通过预测后续内容令牌来驱动内存巩固。",
    "translation": "通过函数令牌实现大语言模型中的记忆检索与巩固",
    "relevance_score": 8,
    "reasoning": "该论文研究LLM的记忆机制优化，属于'使能LLM技术'范畴。改进的记忆检索与巩固技术可直接应用于推荐系统中用户长期偏好的建模和搜索中的上下文理解，通过更有效的记忆管理提升个性化推荐和搜索相关性。函数令牌机制可能为处理用户行为序列和上下文特征提供新的技术路径。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出函数令牌假说，深入揭示LLM内存检索与巩固机制，这对理解LLM内部工作原理及提升推荐系统、搜索等领域的模型透明度与可控性具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08202v1": {
    "title": "Sentiment Matters: An Analysis of 200 Human-SAV Interactions",
    "url": "https://www.alphaxiv.org/abs/2510.08202v1",
    "arxiv_id": "2510.08202v1",
    "authors": "Lirui Guo, Michael G. Burke, Wynita M. Griggs",
    "categories": "cs.HC, cs.AI, cs.CL, cs.ET",
    "pub_date": "2025-10-09 13:30:23",
    "ori_summary": "Shared Autonomous Vehicles (SAVs) are likely to become an important part of the transportation system, making effective human-SAV interactions an important area of research. This paper introduces a dataset of 200 human-SAV interactions to further this area of study. We present an open-source human-SAV conversational dataset, comprising both textual data (e.g., 2,136 human-SAV exchanges) and empirical data (e.g., post-interaction survey results on a range of psychological factors). The dataset's utility is demonstrated through two benchmark case studies: First, using random forest modeling and chord diagrams, we identify key predictors of SAV acceptance and perceived service quality, highlighting the critical influence of response sentiment polarity (i.e., perceived positivity). Second, we benchmark the performance of an LLM-based sentiment analysis tool against the traditional lexicon-based TextBlob method. Results indicate that even simple zero-shot LLM prompts more closely align with user-reported sentiment, though limitations remain. This study provides novel insights for designing conversational SAV interfaces and establishes a foundation for further exploration into advanced sentiment modeling, adaptive user interactions, and multimodal conversational systems.",
    "summary": "",
    "translation": "情感至关重要：200次人机交互分析",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于人机交互中的情感分析，属于纯粹的HCI/心理学研究范畴，与推荐系统、搜索或广告的核心技术进展、LLM赋能技术或Transformer架构改进均无直接关联。即使考虑情感分析在内容理解中的应用，该研究明显侧重交互分析而非技术方法创新，缺乏对RecSys/Search/Ads领域的直接技术贡献。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08191v1": {
    "title": "Training-Free Group Relative Policy Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.08191v1",
    "arxiv_id": "2510.08191v1",
    "authors": "Yuzheng Cai, Siqi Cai, Yuchen Shi, Zihan Xu, Lichao Chen, Yulei Qin, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Yong Mao, Ke Li, Xing Sun",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 13:18:17",
    "ori_summary": "Recent advances in Large Language Model (LLM) agents have demonstrated their promising general capabilities. However, their performance in specialized real-world domains often degrades due to challenges in effectively integrating external tools and specific prompting strategies. While methods like agentic reinforcement learning have been proposed to address this, they typically rely on costly parameter updates, for example, through a process that uses Supervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase with Group Relative Policy Optimization (GRPO) to alter the output distribution. However, we argue that LLMs can achieve a similar effect on the output distribution by learning experiential knowledge as a token prior, which is a far more lightweight approach that not only addresses practical data scarcity but also avoids the common issue of overfitting. To this end, we propose Training-Free Group Relative Policy Optimization (Training-Free GRPO), a cost-effective solution that enhances LLM agent performance without any parameter updates. Our method leverages the group relative semantic advantage instead of numerical ones within each group of rollouts, iteratively distilling high-quality experiential knowledge during multi-epoch learning on a minimal ground-truth data. Such knowledge serves as the learned token prior, which is seamlessly integrated during LLM API calls to guide model behavior. Experiments on mathematical reasoning and web searching tasks demonstrate that Training-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly improves out-of-domain performance. With just a few dozen training samples, Training-Free GRPO outperforms fine-tuned small LLMs with marginal training data and cost.",
    "summary": "",
    "translation": "免训练组相对策略优化",
    "relevance_score": 1,
    "reasoning": "该论文标题暗示了强化学习中的策略优化方法，但未提及与推荐系统、搜索或广告的任何关联。训练免训练方法可能涉及RL技术，但根据用户明确排除标准，没有明确相关性的强化学习论文应被视为不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08189v1": {
    "title": "R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?",
    "url": "https://www.alphaxiv.org/abs/2510.08189v1",
    "arxiv_id": "2510.08189v1",
    "authors": "Yi Lu, Jianing Wang, Linsen Guo, Wei He, Hongyin Tang, Tao Gui, Xuanjing Huang, Xuezhi Cao, Wei Wang, Xunliang Cai",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-09 13:16:22",
    "ori_summary": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought (CoT). However, existing benchmarks mainly focus on immediate, single-horizon tasks, failing to adequately evaluate models' ability to understand and respond to complex, long-horizon scenarios. To address this incomplete evaluation of Large Reasoning Models (LRMs), we propose R-HORIZON, a method designed to stimulate long-horizon reasoning behaviors in LRMs through query composition. Based on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising complex multi-step reasoning tasks with interdependent problems that span long reasoning horizons. Through comprehensive evaluation of LRMs using the R-HORIZON benchmark, we find that even the most advanced LRMs suffer significant performance degradation. Our analysis reveals that LRMs exhibit limited effective reasoning length and struggle to allocate thinking budget across multiple problems appropriately. Recognizing these limitations, we use R-HORIZON to construct long-horizon reasoning data for reinforcement learning with verified rewards (RLVR). Compared to training with single-horizon data, RLVR with R-HORIZON not only substantially improves performance on the multi-horizon reasoning tasks, but also promotes accuracy on standard reasoning tasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as a scalable, controllable, and low-cost paradigm for enhancing and evaluating the long-horizon reasoning capabilities of LRMs.",
    "summary": "该论文研究如何评估大型推理模型在复杂长序列任务中的真实能力。核心方法是提出R-HORIZON评估框架，通过查询组合构建长视野推理基准来激发模型的深度推理行为。",
    "translation": "R-Horizon：您的大型推理模型在广度和深度上究竟能走多远？",
    "relevance_score": 8,
    "reasoning": "该论文聚焦大型推理模型的能力边界评估，属于核心LLM技术进步范畴。推理能力的提升对于搜索中的复杂查询理解、推荐系统中的多轮交互决策以及广告中的用户意图深度分析都具有直接应用价值，能够显著增强这些领域系统的智能水平。",
    "rerank_relevance_score": 4,
    "rerank_reasoning": "该论文主要关注推理模型的评估方法改进，与推荐系统、搜索广告的直接应用关联较弱，但在长序列建模和复杂推理方面有一定启发价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08188v1": {
    "title": "METRICALARGS: A Taxonomy for Studying Metrical Poetry with LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.08188v1",
    "arxiv_id": "2510.08188v1",
    "authors": "Chalamalasetti Kranti, Sowmya Vajjala",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 13:14:38",
    "ori_summary": "Prior NLP work studying poetry has focused primarily on automatic poem generation and summarization. Many languages have well-studied traditions of poetic meter which enforce constraints on a poem in terms of syllable and phoneme patterns. Such advanced literary forms offer opportunities for probing deeper reasoning and language understanding in Large Language Models (LLMs) and their ability to follow strict pre-requisites and rules. In this paper, we introduce MetricalARGS, the first taxonomy of poetry-related NLP tasks designed to evaluate LLMs on metrical poetry across four dimensions: Analysis, Retrieval, Generation, and Support. We discuss how these tasks relate to existing NLP tasks, addressing questions around datasets and evaluation metrics. Taking Telugu as our example language, we illustrate how the taxonomy can be used in practice. MetricalARGS highlights the broader possibilities for understanding the capabilities and limitations of today's LLMs through the lens of metrical poetry.",
    "summary": "",
    "translation": "METRICALARGS：一种使用大语言模型研究格律诗歌的分类法",
    "relevance_score": 1,
    "reasoning": "该论文专注于诗歌格律分析这一纯文学领域，与推荐系统、搜索或广告的核心技术进展完全无关。虽然提到了LLMs，但应用场景是诗歌研究这种非商业领域，不属于任何相关技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08173v1": {
    "title": "NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions",
    "url": "https://www.alphaxiv.org/abs/2510.08173v1",
    "arxiv_id": "2510.08173v1",
    "authors": "Haolin Yang, Yuxing Long, Zhuoyuan Yu, Zihan Yang, Minghan Wang, Jiapeng Xu, Yihan Wang, Ziyan Yu, Wenzhe Cai, Lei Kang, Hao Dong",
    "categories": "cs.RO, cs.AI, cs.CL, cs.CV",
    "pub_date": "2025-10-09 12:59:19",
    "ori_summary": "Instruction-following navigation is a key step toward embodied intelligence. Prior benchmarks mainly focus on semantic understanding but overlook systematically evaluating navigation agents' spatial perception and reasoning capabilities. In this work, we introduce the NavSpace benchmark, which contains six task categories and 1,228 trajectory-instruction pairs designed to probe the spatial intelligence of navigation agents. On this benchmark, we comprehensively evaluate 22 navigation agents, including state-of-the-art navigation models and multimodal large language models. The evaluation results lift the veil on spatial intelligence in embodied navigation. Furthermore, we propose SNav, a new spatially intelligent navigation model. SNav outperforms existing navigation agents on NavSpace and real robot tests, establishing a strong baseline for future work.",
    "summary": "",
    "translation": "NavSpace：导航代理如何遵循空间智能指令",
    "relevance_score": 2,
    "reasoning": "该论文主要研究导航代理和空间智能指令，属于机器人导航领域，与推荐系统、搜索或广告的核心技术关联性较弱。虽然涉及智能代理技术，但缺乏明确的RecSys/Search/Ads应用场景，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08163v1": {
    "title": "ARM2: Adaptive Reasoning Model with Vision Understanding and Executable Code",
    "url": "https://www.alphaxiv.org/abs/2510.08163v1",
    "arxiv_id": "2510.08163v1",
    "authors": "Jian Xie, Zhendong Chu, Aoxiao Zhong, Kai Zhang, Mingzhe Han, Xin Fang, Jialie Shen, Qingsong Wen",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 12:49:34",
    "ori_summary": "Large Reasoning Models (LRMs) often suffer from the ``over-thinking'' problem, generating unnecessarily long reasoning on simple tasks. Some strategies have been proposed to mitigate this issue, such as length penalties or routing mechanisms, but they are typically heuristic and task-specific, lacking a general framework for adaptive reasoning. In this paper, we present ARM2, a unified model that adaptively balances reasoning performance and efficiency across multiple formats through a reinforcement learning framework augmented with length-aware optimization. Beyond conventional natural language inference, ARM2 integrates vision understanding, extending its applicability to multimodal. Moreover, ARM2 integrates executable code into reasoning, enabling substantial reductions in token cost while preserving task performance compared to long CoT. Experiments demonstrate that ARM2 achieves performance on par with traditional reasoning models trained with GRPO, while reducing token usage by over 70% on average. We further conduct extensive analyses to validate the effectiveness of ARM2 and the soundness of its design.",
    "summary": "",
    "translation": "ARM2：具有视觉理解与可执行代码的自适应推理模型",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视觉理解和代码执行能力，属于多模态推理范畴。虽然自适应推理机制在概念上可能对推荐系统的复杂决策有启发，但论文的核心焦点是视觉模态和代码执行，与推荐系统、搜索或广告的核心技术栈关联较弱，缺乏明确的直接应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08158v1": {
    "title": "Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.08158v1",
    "arxiv_id": "2510.08158v1",
    "authors": "Shuzhou Yuan, Ercong Nie, Yinuo Sun, Chenxuan Zhao, William LaCroix, Michael Färber",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 12:38:16",
    "ori_summary": "Large language models (LLMs) frequently produce false refusals, declining benign requests that contain terms resembling unsafe queries. We address this challenge by introducing two comprehensive benchmarks: the Exaggerated Safety Benchmark (XSB) for single-turn prompts, annotated with \"Focus\" keywords that identify refusal-inducing triggers, and the Multi-turn Scenario-based Exaggerated Safety Benchmark (MS-XSB), which systematically evaluates refusal calibration in realistic, context-rich dialog settings. Our benchmarks reveal that exaggerated refusals persist across diverse recent LLMs and are especially pronounced in complex, multi-turn scenarios. To mitigate these failures, we leverage post-hoc explanation methods to identify refusal triggers and deploy three lightweight, model-agnostic approaches, ignore-word instructions, prompt rephrasing, and attention steering, at inference time, all without retraining or parameter access. Experiments on four instruction-tuned Llama models demonstrate that these strategies substantially improve compliance on safe prompts while maintaining robust safety protections. Our findings establish a reproducible framework for diagnosing and mitigating exaggerated refusals, highlighting practical pathways to safer and more helpful LLM deployments.",
    "summary": "",
    "translation": "超越过度拒绝：基于场景的诊断与事后缓解方法应对大语言模型中的夸大拒绝行为",
    "relevance_score": 1,
    "reasoning": "该论文主要关注LLM的拒绝行为诊断和缓解，属于纯粹的NLP评估和安全性话题。虽然涉及LLM技术，但核心关注的是模型拒绝行为的评估和修正，与推荐系统、搜索或广告中的排序、匹配、个性化等核心任务没有直接关联，也不涉及Transformer架构改进或异构数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08152v1": {
    "title": "DACIP-RC: Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension on Business Conversations",
    "url": "https://www.alphaxiv.org/abs/2510.08152v1",
    "arxiv_id": "2510.08152v1",
    "authors": "Elena Khasanova, Harsh Saini, Md Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen, Shashi Bhushan TN",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 12:35:20",
    "ori_summary": "The rapid advancements in Large Language Models (LLMs) have enabled their adoption in real-world industrial scenarios for various natural language processing tasks. However, the high inference cost of large-scale LLMs makes their deployment impractical, necessitating the use of smaller models. Despite their efficiency, smaller LLMs lack robust zero-shot instruction-following capabilities across diverse domains, limiting their adaptability to dynamic user requirements. Traditional fine-tuning approaches exacerbate this issue by inducing catastrophic forgetting, reducing the model's generalization ability for unseen tasks. In this paper, we propose Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension (DACIP-RC), a continual pre-training technique that enhances smaller LLMs' domain adaptability for business conversational tasks. Unlike conventional pre-training approaches that rely on next-token prediction, DACIP-RC generates diverse task instructions and responses via reading comprehension on conversation transcripts, enabling better instruction generalization. Our empirical evaluations demonstrate that DACIP-RC significantly improves zero-shot generalization across a wide range of business conversational tasks, including meeting summarization, action item generation, and call purpose identification. To the best of our knowledge, this is the first work to apply instruction pre-training on business conversational data, providing insights into how industries can leverage proprietary datasets for domain adaptation.",
    "summary": "论文研究如何提升小型LLM在商业对话任务中的领域适应能力，核心方法是采用基于阅读理解的持续指令预训练技术，通过对话转录生成多样化任务指令和响应来增强指令泛化能力。",
    "translation": "DACIP-RC：基于商业对话阅读理解任务的领域自适应持续指令预训练",
    "relevance_score": 8,
    "reasoning": "该论文涉及领域自适应和持续指令预训练，属于'Enabling LLM Tech'范畴，专注于提升LLM在特定业务领域的适应能力。这种技术可直接应用于搜索和推荐系统中的对话式交互场景，通过阅读理解商业对话来增强模型对用户意图和业务上下文的理解能力，从而改善个性化推荐和搜索相关性。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出通过阅读理解实现领域自适应持续指令预训练，直接针对LLM在商业对话任务中的领域适应问题，与直接LLM应用和领域适应高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08149v1": {
    "title": "AI Knowledge Assist: An Automated Approach for the Creation of Knowledge Bases for Conversational AI Agents",
    "url": "https://www.alphaxiv.org/abs/2510.08149v1",
    "arxiv_id": "2510.08149v1",
    "authors": "Md Tahmid Rahman Laskar, Julien Bouvier Tremblay, Xue-Yong Fu, Cheng Chen, Shashi Bhushan TN",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-09 12:34:31",
    "ori_summary": "The utilization of conversational AI systems by leveraging Retrieval Augmented Generation (RAG) techniques to solve customer problems has been on the rise with the rapid progress of Large Language Models (LLMs). However, the absence of a company-specific dedicated knowledge base is a major barrier to the integration of conversational AI systems in contact centers. To this end, we introduce AI Knowledge Assist, a system that extracts knowledge in the form of question-answer (QA) pairs from historical customer-agent conversations to automatically build a knowledge base. Fine-tuning a lightweight LLM on internal data demonstrates state-of-the-art performance, outperforming larger closed-source LLMs. More specifically, empirical evaluation on 20 companies demonstrates that the proposed AI Knowledge Assist system that leverages the LLaMA-3.1-8B model eliminates the cold-start gap in contact centers by achieving above 90% accuracy in answering information-seeking questions. This enables immediate deployment of RAG-powered chatbots.",
    "summary": "",
    "translation": "AI知识助手：一种为对话式AI代理创建知识库的自动化方法",
    "relevance_score": 3,
    "reasoning": "该论文主要关注对话式AI代理的知识库创建，属于AIGC和内容生成范畴，与推荐系统、搜索或广告的核心技术关联较弱。虽然知识库构建可能间接支持某些推荐场景，但论文焦点不在排名、检索或个性化等核心领域，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08145v1": {
    "title": "Mitigating Judgment Preference Bias in Large Language Models through Group-Based Polling",
    "url": "https://www.alphaxiv.org/abs/2510.08145v1",
    "arxiv_id": "2510.08145v1",
    "authors": "Shuliang Liu, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Minghe Yu, Yu Gu, Chong Chen, Huiyuan Xie, Ge Yu",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 12:32:31",
    "ori_summary": "Large Language Models (LLMs) as automatic evaluators, commonly referred to as LLM-as-a-Judge, have also attracted growing attention. This approach plays a vital role in aligning LLMs with human judgments, providing accurate and reliable assessments. However, LLM-based judgment models often exhibit judgment preference bias during the evaluation phase, tending to favor responses generated by themselves, undermining the reliability of their judgments. This paper introduces the Group-Based Polling Optimization (Genii), an unsupervised multi-agent collaborative optimization framework that mitigates the inherent judgment preference bias of judgment models. Specifically, Genii integrates various LLM-based judgment models into a multi-agent system and simulates the interactive client-server polling mechanism to optimize each client agent unsupervisedly. Our experiments demonstrate that Genii outperforms supervised models trained on annotated judgment data, while requiring no human-labeled annotations. Genii consistently improves performance across different client agents during the polling, even when weaker models act as server agents. Further analysis reveals that Genii effectively mitigates judgment preference bias of LLM-based judgment models, demonstrating its effectiveness. All codes are available at https://github.com/NEUIR/Genii.",
    "summary": "",
    "translation": "通过基于群体的投票缓解大型语言模型中的判断偏好偏差",
    "relevance_score": 3,
    "reasoning": "该论文主要关注LLM的偏好偏差缓解，属于LLM评估和偏差校正范畴，与我的核心关注点（推荐系统、搜索、广告的直接应用或使能技术）相关性较弱。虽然偏差缓解可能间接影响推荐/搜索系统的公平性，但论文焦点更偏向通用LLM评估而非特定领域应用，且未明确涉及异构数据建模或Transformer架构创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08120v1": {
    "title": "Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations",
    "url": "https://www.alphaxiv.org/abs/2510.08120v1",
    "arxiv_id": "2510.08120v1",
    "authors": "Jasmina Gajcin, Erik Miehling, Rahul Nair, Elizabeth Daly, Radu Marinescu, Seshu Tirupathi",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 12:05:37",
    "ori_summary": "Using LLMs to evaluate text, that is, LLM-as-a-judge, is increasingly being used at scale to augment or even replace human annotations. As such, it is imperative that we understand the potential biases and risks of doing so. In this work, we propose an approach for extracting high-level concept-based global policies from LLM-as-a-Judge. Our approach consists of two algorithms: 1) CLoVE (Contrastive Local Verifiable Explanations), which generates verifiable, concept-based, contrastive local explanations and 2) GloVE (Global Verifiable Explanations), which uses iterative clustering, summarization and verification to condense local rules into a global policy. We evaluate GloVE on seven standard benchmarking datasets for content harm detection. We find that the extracted global policies are highly faithful to decisions of the LLM-as-a-Judge. Additionally, we evaluated the robustness of global policies to text perturbations and adversarial attacks. Finally, we conducted a user study to evaluate user understanding and satisfaction with global policies.",
    "summary": "",
    "translation": "通过可验证的全局解释解读LLM作为评判者的策略",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM作为评判者的策略解释和可验证性，这属于纯粹的LLM评估和解释性研究，与幻觉、评估基准等NLP中心主题密切相关。虽然LLM评估在广义上可能影响推荐系统，但论文标题没有表明任何直接的RecSys/Search/Ads应用，而是专注于纯粹的LLM评判机制解释。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08114v1": {
    "title": "Can Risk-taking AI-Assistants suitably represent entities",
    "url": "https://www.alphaxiv.org/abs/2510.08114v1",
    "arxiv_id": "2510.08114v1",
    "authors": "Ali Mazyaki, Mohammad Naghizadeh, Samaneh Ranjkhah Zonouzaghi, Amirhossein Farshi Sotoudeh",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-09 11:55:31",
    "ori_summary": "Responsible AI demands systems whose behavioral tendencies can be effectively measured, audited, and adjusted to prevent inadvertently nudging users toward risky decisions or embedding hidden biases in risk aversion. As language models (LMs) are increasingly incorporated into AI-driven decision support systems, understanding their risk behaviors is crucial for their responsible deployment. This study investigates the manipulability of risk aversion (MoRA) in LMs, examining their ability to replicate human risk preferences across diverse economic scenarios, with a focus on gender-specific attitudes, uncertainty, role-based decision-making, and the manipulability of risk aversion. The results indicate that while LMs such as DeepSeek Reasoner and Gemini-2.0-flash-lite exhibit some alignment with human behaviors, notable discrepancies highlight the need to refine bio-centric measures of manipulability. These findings suggest directions for refining AI design to better align human and AI risk preferences and enhance ethical decision-making. The study calls for further advancements in model design to ensure that AI systems more accurately replicate human risk preferences, thereby improving their effectiveness in risk management contexts. This approach could enhance the applicability of AI assistants in managing risk.",
    "summary": "",
    "translation": "敢于承担风险的AI助手能否恰当地代表实体",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于AI助手风险承担能力和实体代表性问题，这属于AI伦理、安全性和代理行为的范畴，与推荐系统、搜索或广告的核心技术进展完全无关。标题中没有任何技术元素表明与Transformer架构、LLM效率、推荐算法或多模态建模相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08111v1": {
    "title": "Evaluating LLM-Generated Legal Explanations for Regulatory Compliance in Social Media Influencer Marketing",
    "url": "https://www.alphaxiv.org/abs/2510.08111v1",
    "arxiv_id": "2510.08111v1",
    "authors": "Haoyang Gui, Thales Bertaglia, Taylor Annabell, Catalina Goanta, Tjomme Dooper, Gerasimos Spanakis",
    "categories": "cs.CL, cs.CY",
    "pub_date": "2025-10-09 11:50:37",
    "ori_summary": "The rise of influencer marketing has blurred boundaries between organic content and sponsored content, making the enforcement of legal rules relating to transparency challenging. Effective regulation requires applying legal knowledge with a clear purpose and reason, yet current detection methods of undisclosed sponsored content generally lack legal grounding or operate as opaque \"black boxes\". Using 1,143 Instagram posts, we compare gpt-5-nano and gemini-2.5-flash-lite under three prompting strategies with controlled levels of legal knowledge provided. Both models perform strongly in classifying content as sponsored or not (F1 up to 0.93), though performance drops by over 10 points on ambiguous cases. We further develop a taxonomy of reasoning errors, showing frequent citation omissions (28.57%), unclear references (20.71%), and hidden ads exhibiting the highest miscue rate (28.57%). While adding regulatory text to the prompt improves explanation quality, it does not consistently improve detection accuracy. The contribution of this paper is threefold. First, it makes a novel addition to regulatory compliance technology by providing a taxonomy of common errors in LLM-generated legal reasoning to evaluate whether automated moderation is not only accurate but also legally robust, thereby advancing the transparent detection of influencer marketing content. Second, it features an original dataset of LLM explanations annotated by two students who were trained in influencer marketing law. Third, it combines quantitative and qualitative evaluation strategies for LLM explanations and critically reflects on how these findings can support advertising regulatory bodies in automating moderation processes on a solid legal foundation.",
    "summary": "",
    "translation": "评估大语言模型生成的法律解释在社交媒体网红营销监管合规性中的应用",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM在法律解释和监管合规性评估方面的应用，这属于特定领域应用而非核心推荐系统、搜索或广告技术。虽然涉及社交媒体环境，但焦点是法律合规性而非排名、检索或用户建模等核心领域，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08102v1": {
    "title": "Lossless Vocabulary Reduction for Auto-Regressive Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08102v1",
    "arxiv_id": "2510.08102v1",
    "authors": "Daiki Chijiwa, Taku Hasegawa, Kyosuke Nishida, Shin'ya Yamaguchi, Tomoya Ohba, Tamao Sakao, Susumu Takeuchi",
    "categories": "cs.CL, cs.AI, cs.LG, stat.ML",
    "pub_date": "2025-10-09 11:38:48",
    "ori_summary": "Tokenization -- the process of decomposing a given text into a sequence of subwords called tokens -- is one of the key components in the development of language models. Particularly, auto-regressive language models generate texts token by token, i.e., by predicting the next-token distribution given the previous ones, and thus tokenization directly affects their efficiency in text generation. Since each language model has their own vocabulary as a set of possible tokens, they struggle to cooperate with each other at the level of next-token distributions such as model ensemble. In this paper, we establish a theoretical framework of lossless vocabulary reduction, which efficiently converts a given auto-regressive language model into the one with an arbitrarily small vocabulary without any loss in accuracy. As an application, we demonstrate that language models with different tokenization can cooperate with each other efficiently through their maximal common vocabulary.",
    "summary": "研究自回归语言模型因不同词汇表导致的模型协作困难问题，核心思想是通过理论框架将模型无损转换为任意小词汇表，实现不同分词模型的高效协同。",
    "translation": "自回归语言模型的无损词汇表缩减",
    "relevance_score": 8,
    "reasoning": "该论文属于'Enabling LLM Tech'范畴，专注于语言模型效率优化技术。无损词汇表缩减技术可以显著降低LLM的推理成本和内存占用，这对于需要部署大规模LLM的推荐系统和搜索应用具有直接价值，能够提升在线服务的响应速度和资源利用率。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出无损词汇缩减框架，直接提升自回归语言模型的效率与互操作性，对LLM核心技术进步有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08098v1": {
    "title": "The Price of Thought: A Multilingual Analysis of Reasoning, Performance, and Cost of Negotiation in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08098v1",
    "arxiv_id": "2510.08098v1",
    "authors": "Sherzod Hakimov, Roland Bernard, Tim Leiber, Karl Osswald, Kristina Richert, Ruilin Yang, Raffaella Bernardi, David Schlangen",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 11:36:38",
    "ori_summary": "Negotiation is a fundamental challenge for AI agents, as it requires an ability to reason strategically, model opponents, and balance cooperation with competition. We conduct the first comprehensive study systematically evaluating the effect of (LLM-)reasoning on the negotiation abilities of both commercial and open-weight LLMs, and do this across three languages. Using a self-play setup across three diverse dialogue games, we analyse trade-offs between performance and cost, the language consistency of reasoning processes, and the nature of strategic adaptation exhibited by models. Our findings show that enabling reasoning-that is, scaling test time compute-significantly improves negotiation outcomes by enhancing collaboration and helping models overcome task complexities, but comes at a substantial computational cost: reasoning improves GPT-5's performance by 31.4 % while increasing its cost by nearly 400 %. Most critically, we uncover a significant multilingual reasoning distinction: open-weight models consistently switch to English for their internal reasoning steps, even when negotiating in German or Italian (and thus possibly impacting potential explainability gains through the disclosure of reasoning traces), while leading commercial models maintain language consistency between their reasoning and final output.",
    "summary": "",
    "translation": "思维的成本：大型语言模型中推理、性能与协商成本的多语言分析",
    "relevance_score": 2,
    "reasoning": "该论文主要分析LLM的推理能力、性能表现和成本协商，属于纯粹的LLM能力评估研究。虽然涉及多语言分析，但核心焦点是模型内在能力评估而非RecSys/Search/Ads应用。论文缺乏对推荐系统、搜索或广告场景的具体应用潜力说明，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08091v1": {
    "title": "Everything is Plausible: Investigating the Impact of LLM Rationales on Human Notions of Plausibility",
    "url": "https://www.alphaxiv.org/abs/2510.08091v1",
    "arxiv_id": "2510.08091v1",
    "authors": "Shramay Palta, Peter Rankel, Sarah Wiegreffe, Rachel Rudinger",
    "categories": "cs.CL, cs.AI, cs.HC",
    "pub_date": "2025-10-09 11:22:29",
    "ori_summary": "We investigate the degree to which human plausibility judgments of multiple-choice commonsense benchmark answers are subject to influence by (im)plausibility arguments for or against an answer, in particular, using rationales generated by LLMs. We collect 3,000 plausibility judgments from humans and another 13,600 judgments from LLMs. Overall, we observe increases and decreases in mean human plausibility ratings in the presence of LLM-generated PRO and CON rationales, respectively, suggesting that, on the whole, human judges find these rationales convincing. Experiments with LLMs reveal similar patterns of influence. Our findings demonstrate a novel use of LLMs for studying aspects of human cognition, while also raising practical concerns that, even in domains where humans are ``experts'' (i.e., common sense), LLMs have the potential to exert considerable influence on people's beliefs.",
    "summary": "",
    "translation": "万物皆合理：探究大语言模型推理对人类合理性认知的影响",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLM推理对人类认知的影响，属于心理学和人类认知交互领域。虽然涉及LLM，但其研究重点在于人类对合理性的感知判断，而非LLM在推荐系统、搜索或广告中的技术应用或架构改进。该研究缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08081v1": {
    "title": "AutoQual: An LLM Agent for Automated Discovery of Interpretable Features for Review Quality Assessment",
    "url": "https://www.alphaxiv.org/abs/2510.08081v1",
    "arxiv_id": "2510.08081v1",
    "authors": "Xiaochong Lan, Jie Feng, Yinxing Liu, Xinlei Shi, Yong Li",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-09 11:11:02",
    "ori_summary": "Ranking online reviews by their intrinsic quality is a critical task for e-commerce platforms and information services, impacting user experience and business outcomes. However, quality is a domain-dependent and dynamic concept, making its assessment a formidable challenge. Traditional methods relying on hand-crafted features are unscalable across domains and fail to adapt to evolving content patterns, while modern deep learning approaches often produce black-box models that lack interpretability and may prioritize semantics over quality. To address these challenges, we propose AutoQual, an LLM-based agent framework that automates the discovery of interpretable features. While demonstrated on review quality assessment, AutoQual is designed as a general framework for transforming tacit knowledge embedded in data into explicit, computable features. It mimics a human research process, iteratively generating feature hypotheses through reflection, operationalizing them via autonomous tool implementation, and accumulating experience in a persistent memory. We deploy our method on a large-scale online platform with a billion-level user base. Large-scale A/B testing confirms its effectiveness, increasing average reviews viewed per user by 0.79% and the conversion rate of review readers by 0.27%.",
    "summary": "",
    "translation": "AutoQual：一种用于自动发现可解释特征以进行评论质量评估的LLM智能体",
    "relevance_score": 3,
    "reasoning": "该论文涉及LLM在内容质量评估中的应用，这与搜索和推荐系统中的内容理解有一定关联。然而，评论质量评估更偏向内容质量分析而非核心的推荐、搜索或广告排名任务，应用范围相对有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08058v1": {
    "title": "FedDTRE: Federated Dialogue Generation Models Powered by Trustworthiness Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.08058v1",
    "arxiv_id": "2510.08058v1",
    "authors": "Shule Lu, Lingxiang Wang, Sijia Wen, Ziwei Wang, Hainan Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 10:43:14",
    "ori_summary": "With the rapid development of artificial intelligence, dialogue systems have become a prominent form of human-computer interaction. However, traditional centralized or fully local training approaches face challenges in balancing privacy preservation and personalization due to data privacy concerns and heterogeneous device capabilities. Federated learning, as a representative distributed paradigm, offers a promising solution. However, existing methods often suffer from overfitting under limited client data and tend to forget global information after multiple training rounds, leading to poor generalization. To address these issues, we propose FedDTRE, a Federated adaptive aggregation strategy for Dialogue generation based on Trustworthiness Evaluation. Instead of directly replacing local models with the global model, FedDTRE leverages trustworthiness scores of both global and local models on a fairness-oriented evaluation dataset to dynamically regulate the global model's contribution during local updates. Experimental results demonstrate that FedDTRE can improve dialogue model performance and enhance the quality of dialogue generation.",
    "summary": "",
    "translation": "FedDTRE：基于可信度评估驱动的联邦对话生成模型",
    "relevance_score": 1,
    "reasoning": "该论文涉及联邦学习和对话生成，这两个主题均被明确列为不相关主题。联邦学习属于隐私/安全范畴，而对话生成属于纯粹的LLM中心化应用，与推荐系统、搜索或广告的核心技术进展没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08049v1": {
    "title": "A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08049v1",
    "arxiv_id": "2510.08049v1",
    "authors": "Congming Zheng, Jiachen Zhu, Zhuoying Ou, Yuxiang Chen, Kangning Zhang, Rong Shan, Zeyu Zheng, Mengyue Yang, Jianghao Lin, Yong Yu, Weinan Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 10:35:31",
    "ori_summary": "Although Large Language Models (LLMs) exhibit advanced reasoning ability, conventional alignment remains largely dominated by outcome reward models (ORMs) that judge only final answers. Process Reward Models(PRMs) address this gap by evaluating and guiding reasoning at the step or trajectory level. This survey provides a systematic overview of PRMs through the full loop: how to generate process data, build PRMs, and use PRMs for test-time scaling and reinforcement learning. We summarize applications across math, code, text, multimodal reasoning, robotics, and agents, and review emerging benchmarks. Our goal is to clarify design spaces, reveal open challenges, and guide future research toward fine-grained, robust reasoning alignment.",
    "summary": "",
    "translation": "过程奖励模型综述：从结果信号到大型语言模型的过程监督",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM的训练方法和奖励建模，属于纯粹的LLM技术范畴。虽然奖励模型在RLHF中有应用，但论文聚焦于过程监督而非具体应用场景，缺乏与推荐系统、搜索或广告领域的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08047v1": {
    "title": "Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic Speech Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.08047v1",
    "arxiv_id": "2510.08047v1",
    "authors": "Yi-Cheng Lin, Yu-Hsuan Li Liang, Hsuan Su, Tzu-Quan Lin, Shang-Tse Chen, Yun-Nung Chen, Hung-yi Lee",
    "categories": "eess.AS, cs.CL",
    "pub_date": "2025-10-09 10:31:47",
    "ori_summary": "Robust ASR under domain shift is crucial because real-world systems encounter unseen accents and domains with limited labeled data. Although pseudo-labeling offers a practical workaround, it often introduces systematic, accent-specific errors that filtering fails to fix. We ask: How can we correct these recurring biases without target ground truth? We propose a simple parameter-space correction: in a source domain containing both real and pseudo-labeled data, two ASR models are fine-tuned from the same initialization, one on ground-truth labels and the other on pseudo-labels, and their weight difference forms a correction vector that captures pseudo-label biases. When applied to a pseudo-labeled target model, this vector enhances recognition, achieving up to a 35% relative Word Error Rate (WER) reduction on AfriSpeech-200 across ten African accents with the Whisper tiny model.",
    "summary": "",
    "translation": "Pseudo2Real：自动语音识别中伪标签校正的任务算术",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动语音识别中的伪标签校正技术，属于纯粹的语音处理领域。虽然提到了任务算术的概念，但核心应用场景是语音识别，与推荐系统、搜索或广告领域没有直接关联，也不涉及LLM在RecSys/Search/Ads中的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08043v1": {
    "title": "Climate Knowledge in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08043v1",
    "arxiv_id": "2510.08043v1",
    "authors": "Ivan Kuznetsov, Jacopo Grassi, Dmitrii Pantiukhin, Boris Shapkin, Thomas Jung, Nikolay Koldunov",
    "categories": "cs.CL, cs.LG, physics.ao-ph",
    "pub_date": "2025-10-09 10:25:36",
    "ori_summary": "Large language models (LLMs) are increasingly deployed for climate-related applications, where understanding internal climatological knowledge is crucial for reliability and misinformation risk assessment. Despite growing adoption, the capacity of LLMs to recall climate normals from parametric knowledge remains largely uncharacterized. We investigate the capacity of contemporary LLMs to recall climate normals without external retrieval, focusing on a prototypical query: mean July 2-m air temperature 1991-2020 at specified locations. We construct a global grid of queries at 1{\\deg} resolution land points, providing coordinates and location descriptors, and validate responses against ERA5 reanalysis. Results show that LLMs encode non-trivial climate structure, capturing latitudinal and topographic patterns, with root-mean-square errors of 3-6 {\\deg}C and biases of $\\pm$1 {\\deg}C. However, spatially coherent errors remain, particularly in mountains and high latitudes. Performance degrades sharply above 1500 m, where RMSE reaches 5-13 {\\deg}C compared to 2-4 {\\deg}C at lower elevations. We find that including geographic context (country, city, region) reduces errors by 27% on average, with larger models being most sensitive to location descriptors. While models capture the global mean magnitude of observed warming between 1950-1974 and 2000-2024, they fail to reproduce spatial patterns of temperature change, which directly relate to assessing climate change. This limitation highlights that while LLMs may capture present-day climate distributions, they struggle to represent the regional and local expression of long-term shifts in temperature essential for understanding climate dynamics. Our evaluation framework provides a reproducible benchmark for quantifying parametric climate knowledge in LLMs and complements existing climate communication assessments.",
    "summary": "",
    "translation": "大型语言模型中的气候知识",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于LLMs中的特定领域知识（气候），这属于纯粹的LLM知识评估范畴，与推荐系统、搜索或广告的核心技术进展无关。没有证据表明该研究涉及推荐系统架构、搜索算法改进、广告排名或任何与我的关注领域相关的技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08042v1": {
    "title": "ChatGPT as a Translation Engine: A Case Study on Japanese-English",
    "url": "https://www.alphaxiv.org/abs/2510.08042v1",
    "arxiv_id": "2510.08042v1",
    "authors": "Vincent Michael Sutanto, Giovanni Gatti De Giacomo, Toshiaki Nakazawa, Masaru Yamada",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 10:25:10",
    "ori_summary": "This study investigates ChatGPT for Japanese-English translation, exploring simple and enhanced prompts and comparing against commercially available translation engines. Performing both automatic and MQM-based human evaluations, we found that document-level translation outperforms sentence-level translation for ChatGPT. On the other hand, we were not able to determine if enhanced prompts performed better than simple prompts in our experiments. We also discovered that ChatGPT-3.5 was preferred by automatic evaluation, but a tradeoff exists between accuracy (ChatGPT-3.5) and fluency (ChatGPT-4). Lastly, ChatGPT yields competitive results against two widely-known translation systems.",
    "summary": "",
    "translation": "ChatGPT作为翻译引擎：日语-英语案例研究",
    "relevance_score": 1,
    "reasoning": "该论文纯粹研究ChatGPT在机器翻译领域的应用，属于NLP特定任务研究，与推荐系统、搜索或广告的核心技术发展没有直接关联。论文标题明确聚焦于翻译引擎功能，没有涉及任何推荐、搜索排名、广告投放或Transformer架构改进等关键技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08002v1": {
    "title": "Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks",
    "url": "https://www.alphaxiv.org/abs/2510.08002v1",
    "arxiv_id": "2510.08002v1",
    "authors": "Cheng Yang, Xuemeng Yang, Licheng Wen, Daocheng Fu, Jianbiao Mei, Rong Wu, Pinlong Cai, Yufan Shen, Nianchen Deng, Botian Shi, Yu Qiao, Haifeng Li",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 09:40:34",
    "ori_summary": "Large Language Models have demonstrated remarkable capabilities across diverse domains, yet significant challenges persist when deploying them as AI agents for real-world long-horizon tasks. Existing LLM agents suffer from a critical limitation: they are test-time static and cannot learn from experience, lacking the ability to accumulate knowledge and continuously improve on the job. To address this challenge, we propose MUSE, a novel agent framework that introduces an experience-driven, self-evolving system centered around a hierarchical Memory Module. MUSE organizes diverse levels of experience and leverages them to plan and execute long-horizon tasks across multiple applications. After each sub-task execution, the agent autonomously reflects on its trajectory, converting the raw trajectory into structured experience and integrating it back into the Memory Module. This mechanism enables the agent to evolve beyond its static pretrained parameters, fostering continuous learning and self-evolution. We evaluate MUSE on the long-horizon productivity benchmark TAC. It achieves new SOTA performance by a significant margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments demonstrate that as the agent autonomously accumulates experience, it exhibits increasingly superior task completion capabilities, as well as robust continuous learning and self-evolution capabilities. Moreover, the accumulated experience from MUSE exhibits strong generalization properties, enabling zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI agents capable of real-world productivity task automation.",
    "summary": "",
    "translation": "在职学习：面向长周期任务的经验驱动自进化智能体",
    "relevance_score": 3,
    "reasoning": "该论文主要关注智能体的持续学习和自我进化能力，属于通用AI智能体领域。虽然经验驱动的学习机制在推荐系统中可能有潜在应用（如用户行为建模的持续优化），但论文聚焦于长周期任务而非具体的推荐、搜索或广告场景，与核心关注点关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07993v1": {
    "title": "Leveraging Author-Specific Context for Scientific Figure Caption Generation: 3rd SciCap Challenge",
    "url": "https://www.alphaxiv.org/abs/2510.07993v1",
    "arxiv_id": "2510.07993v1",
    "authors": "Watcharapong Timklaypachara, Monrada Chiewhawan, Nopporn Lekuthai, Titipat Achakulvisut",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 09:30:28",
    "ori_summary": "Scientific figure captions require both accuracy and stylistic consistency to convey visual information. Here, we present a domain-specific caption generation system for the 3rd SciCap Challenge that integrates figure-related textual context with author-specific writing styles using the LaMP-Cap dataset. Our approach uses a two-stage pipeline: Stage 1 combines context filtering, category-specific prompt optimization via DSPy's MIPROv2 and SIMBA, and caption candidate selection; Stage 2 applies few-shot prompting with profile figures for stylistic refinement. Our experiments demonstrate that category-specific prompts outperform both zero-shot and general optimized approaches, improving ROUGE-1 recall by +8.3\\% while limiting precision loss to -2.8\\% and BLEU-4 reduction to -10.9\\%. Profile-informed stylistic refinement yields 40--48\\% gains in BLEU scores and 25--27\\% in ROUGE. Overall, our system demonstrates that combining contextual understanding with author-specific stylistic adaptation can generate captions that are both scientifically accurate and stylistically faithful to the source paper.",
    "summary": "",
    "translation": "利用作者特定上下文进行科学图表标题生成：第三届SciCap挑战赛",
    "relevance_score": 2,
    "reasoning": "该论文专注于科学图表标题生成这一特定领域的文本生成任务，属于纯粹的LLM内容生成应用。虽然标题生成在技术上与搜索中的文档理解有一定关联，但该工作明确限定在科学图表这一狭窄领域，且没有展示出在推荐系统、搜索或广告中的直接应用潜力，因此与您关注的领域相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07978v1": {
    "title": "VoiceAgentBench: Are Voice Assistants ready for agentic tasks?",
    "url": "https://www.alphaxiv.org/abs/2510.07978v1",
    "arxiv_id": "2510.07978v1",
    "authors": "Dhruv Jain, Harshit Shukla, Gautam Rajeev, Ashish Kulkarni, Chandra Khatri, Shubham Agarwal",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-09 09:11:38",
    "ori_summary": "Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants capable of understanding natural spoken queries and performing complex tasks. However, existing speech benchmarks primarily focus on isolated capabilities such as transcription, or question-answering, and do not systematically evaluate agentic scenarios encompassing multilingual and cultural understanding, as well as adversarial robustness. To address this, we introduce VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in realistic spoken agentic settings. It comprises over 5,500 synthetic spoken queries, including dialogues grounded in Indian context, covering single-tool invocations, multi-tool workflows, multi-turn interactions, and safety evaluations. The benchmark supports English, Hindi, and 5 other Indian languages, reflecting real-world linguistic and cultural diversity. We simulate speaker variability using a novel sampling algorithm that selects audios for TTS voice conversion based on its speaker embeddings, maximizing acoustic and speaker diversity. Our evaluation measures tool selection accuracy, structural consistency, and the correctness of tool invocations, including adversarial robustness. Our experiments reveal significant gaps in contextual tool orchestration tasks, Indic generalization, and adversarial robustness, exposing critical limitations of current SpeechLMs.",
    "summary": "",
    "translation": "VoiceAgentBench：语音助手是否已准备好执行智能体任务？",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于语音助手的智能体能力评估，属于语音交互和智能体评估领域，与推荐系统、搜索或广告的核心技术无直接关联。语音助手虽然涉及用户交互，但论文关注的是智能体任务准备度评估，而非推荐、搜索或广告中的实际应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07974v1": {
    "title": "Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.07974v1",
    "arxiv_id": "2510.07974v1",
    "authors": "Jialu Du, Guiyang Hou, Yihui Fu, Chen Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 09:07:31",
    "ori_summary": "While large language models (LLMs) excel in mathematical and code reasoning, we observe they struggle with social reasoning tasks, exhibiting cognitive confusion, logical inconsistencies, and conflation between objective world states and subjective belief states. Through deteiled analysis of DeepSeek-R1's reasoning trajectories, we find that LLMs frequently encounter reasoning impasses and tend to output contradictory terms like \"tricky\" and \"confused\" when processing scenarios with multiple participants and timelines, leading to erroneous reasoning or infinite loops. The core issue is their inability to disentangle objective reality from agents' subjective beliefs. To address this, we propose an adaptive world model-enhanced reasoning mechanism that constructs a dynamic textual world model to track entity states and temporal sequences. It dynamically monitors reasoning trajectories for confusion indicators and promptly intervenes by providing clear world state descriptions, helping models navigate through cognitive dilemmas. The mechanism mimics how humans use implicit world models to distinguish between external events and internal beliefs. Evaluations on three social benchmarks demonstrate significant improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational costs (up to 33.8% token reduction), offering a simple yet effective solution for deploying LLMs in social contexts.",
    "summary": "",
    "translation": "大型语言模型中的主动混淆表达：利用世界模型实现更好的社会推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM的社会推理能力和世界模型，这属于纯粹的NLP中心话题，与推荐系统、搜索或广告的核心技术进展无关。虽然提到了世界模型，但没有明确说明其在RecSys/Search/Ads中的潜在应用价值，因此相关性很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07962v1": {
    "title": "LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?",
    "url": "https://www.alphaxiv.org/abs/2510.07962v1",
    "arxiv_id": "2510.07962v1",
    "authors": "Jingyuan Wang, Yankai Chen, Zhonghang Li, Chao Huang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 08:55:12",
    "ori_summary": "Large language models (LLMs) have demonstrated remarkable progress in reasoning, often through supervised fine-tuning (SFT). However, SFT is resource-intensive, relying on large curated datasets, rejection-sampled demonstrations, and uniform optimization across all tokens, even though only a fraction carry meaningful learning value. In this work, we explore a counterintuitive idea: can smaller language models (SLMs) teach larger language models (LLMs) by revealing high-value reasoning moments that reflect the latter's unique strength? We propose LightReasoner, a novel framework that leverages the behavioral divergence between a stronger expert model (LLM) and a weaker amateur model (SLM). LightReasoner operates in two stages: (1) a sampling stage that pinpoints critical reasoning moments and constructs supervision examples capturing the expert's advantage through expert-amateur contrast, and (2) a fine-tuning stage that aligns the expert model with these distilled examples, amplifying its reasoning strengths. Across seven mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while reducing time consumption by 90%, sampled problems by 80%, and tuned token usage by 99%, all without relying on ground-truth labels. By turning weaker SLMs into effective teaching signals, LightReasoner offers a scalable and resource-efficient approach for advancing LLM reasoning. Code is available at: https://github.com/HKUDS/LightReasoner",
    "summary": "",
    "translation": "LightReasoner：小型语言模型能否教会大型语言模型推理？",
    "relevance_score": 4,
    "reasoning": "该论文探讨小型语言模型如何提升大型语言模型的推理能力，这属于核心LLM技术进展。虽然推理能力本身是通用能力，但在推荐和搜索系统中，增强的推理能力可以用于更复杂的用户意图理解、多跳推理查询处理和上下文感知的推荐逻辑。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07958v1": {
    "title": "A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.07958v1",
    "arxiv_id": "2510.07958v1",
    "authors": "Fengji Zhang, Xinyao Niu, Chengyang Ying, Guancheng Lin, Zhongkai Hao, Zhou Fan, Chengen Huang, Jacky Keung, Bei Chen, Junyang Lin",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 08:53:31",
    "ori_summary": "Recent advances in Large Language Models (LLMs) and Reinforcement Learning (RL) have led to strong performance in open-domain question answering (QA). However, existing models still struggle with questions that admit multiple valid answers. Standard QA benchmarks, which typically assume a single gold answer, overlook this reality and thus produce inappropriate training signals. Existing attempts to handle ambiguity often rely on costly manual annotation, which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue. In this paper, we present A$^2$Search, an annotation-free, end-to-end training framework to recognize and handle ambiguity. At its core is an automated pipeline that detects ambiguous questions and gathers alternative answers via trajectory sampling and evidence verification. The model is then optimized with RL using a carefully designed $\\mathrm{AnsF1}$ reward, which naturally accommodates multiple answers. Experiments on eight open-domain QA benchmarks demonstrate that A$^2$Search achieves new state-of-the-art performance. With only a single rollout, A$^2$Search-7B yields an average $\\mathrm{AnsF1}@1$ score of $48.4\\%$ across four multi-hop benchmarks, outperforming all strong baselines, including the substantially larger ReSearch-32B ($46.2\\%$). Extensive analyses further show that A$^2$Search resolves ambiguity and generalizes across benchmarks, highlighting that embracing ambiguity is essential for building more reliable QA systems. Our code, data, and model weights can be found at https://github.com/zfj1998/A2Search",
    "summary": "",
    "translation": "A²Search：基于强化学习的歧义感知问答",
    "relevance_score": 2,
    "reasoning": "该论文虽然涉及搜索领域，但主要关注问答任务中的歧义处理和强化学习应用，这属于纯粹的NLP问答范畴。强化学习部分没有明确展示与推荐系统、搜索排序或广告相关的具体应用场景，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07940v1": {
    "title": "TTOM: Test-Time Optimization and Memorization for Compositional Video Generation",
    "url": "https://www.alphaxiv.org/abs/2510.07940v1",
    "arxiv_id": "2510.07940v1",
    "authors": "Leigang Qu, Ziyang Wang, Na Zheng, Wenjie Wang, Liqiang Nie, Tat-Seng Chua",
    "categories": "cs.CV, cs.AI, cs.CL, cs.LG, cs.MM",
    "pub_date": "2025-10-09 08:37:00",
    "ori_summary": "Video Foundation Models (VFMs) exhibit remarkable visual generation performance, but struggle in compositional scenarios (e.g., motion, numeracy, and spatial relation). In this work, we introduce Test-Time Optimization and Memorization (TTOM), a training-free framework that aligns VFM outputs with spatiotemporal layouts during inference for better text-image alignment. Rather than direct intervention to latents or attention per-sample in existing work, we integrate and optimize new parameters guided by a general layout-attention objective. Furthermore, we formulate video generation within a streaming setting, and maintain historical optimization contexts with a parametric memory mechanism that supports flexible operations, such as insert, read, update, and delete. Notably, we found that TTOM disentangles compositional world knowledge, showing powerful transferability and generalization. Experimental results on the T2V-CompBench and Vbench benchmarks establish TTOM as an effective, practical, scalable, and efficient framework to achieve cross-modal alignment for compositional video generation on the fly.",
    "summary": "",
    "translation": "TTOM：组合式视频生成的测试时优化与记忆机制",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频生成领域，属于纯粹的视觉内容生成任务，与推荐系统、搜索或广告的核心技术无关。论文标题明确指向视频生成中的组合优化问题，没有显示出任何在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07931v1": {
    "title": "Vision-Enabled LLMs in Historical Lexicography: Digitising and Enriching Estonian-German Dictionaries from the 17th and 18th Centuries",
    "url": "https://www.alphaxiv.org/abs/2510.07931v1",
    "arxiv_id": "2510.07931v1",
    "authors": "Madis Jürviste, Joonatan Jakobson",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 08:29:22",
    "ori_summary": "This article presents research conducted at the Institute of the Estonian Language between 2022 and 2025 on the application of large language models (LLMs) to the study of 17th and 18th century Estonian dictionaries. The authors address three main areas: enriching historical dictionaries with modern word forms and meanings; using vision-enabled LLMs to perform text recognition on sources printed in Gothic script (Fraktur); and preparing for the creation of a unified, cross-source dataset. Initial experiments with J. Gutslaff's 1648 dictionary indicate that LLMs have significant potential for semi-automatic enrichment of dictionary information. When provided with sufficient context, Claude 3.7 Sonnet accurately provided meanings and modern equivalents for 81% of headword entries. In a text recognition experiment with A. T. Helle's 1732 dictionary, a zero-shot method successfully identified and structured 41% of headword entries into error-free JSON-formatted output. For digitising the Estonian-German dictionary section of A. W. Hupel's 1780 grammar, overlapping tiling of scanned image files is employed, with one LLM being used for text recognition and a second for merging the structured output. These findings demonstrate that even for minor languages LLMs have a significant potential for saving time and financial resources.",
    "summary": "",
    "translation": "基于视觉能力的LLM在历史词典学中的应用：数字化和丰富17-18世纪爱沙尼亚-德语词典",
    "relevance_score": 1,
    "reasoning": "该论文主要涉及历史文档的数字保存和语言学研究，属于特定领域应用。虽然提到了视觉能力的LLM，但其应用场景（历史词典学）与推荐系统、搜索或广告的核心技术发展没有直接关联，也不涉及Transformer架构改进或异构数据处理等关键技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07926v1": {
    "title": "Comprehensiveness Metrics for Automatic Evaluation of Factual Recall in Text Generation",
    "url": "https://www.alphaxiv.org/abs/2510.07926v1",
    "arxiv_id": "2510.07926v1",
    "authors": "Adam Dejl, James Barry, Alessandra Pascale, Javier Carnerero Cano",
    "categories": "cs.CL, I.2.7",
    "pub_date": "2025-10-09 08:22:24",
    "ori_summary": "Despite demonstrating remarkable performance across a wide range of tasks, large language models (LLMs) have also been found to frequently produce outputs that are incomplete or selectively omit key information. In sensitive domains, such omissions can result in significant harm comparable to that posed by factual inaccuracies, including hallucinations. In this study, we address the challenge of evaluating the comprehensiveness of LLM-generated texts, focusing on the detection of missing information or underrepresented viewpoints. We investigate three automated evaluation strategies: (1) an NLI-based method that decomposes texts into atomic statements and uses natural language inference (NLI) to identify missing links, (2) a Q&A-based approach that extracts question-answer pairs and compares responses across sources, and (3) an end-to-end method that directly identifies missing content using LLMs. Our experiments demonstrate the surprising effectiveness of the simple end-to-end approach compared to more complex methods, though at the cost of reduced robustness, interpretability and result granularity. We further assess the comprehensiveness of responses from several popular open-weight LLMs when answering user queries based on multiple sources.",
    "summary": "",
    "translation": "文本生成中事实召回自动评估的全面性度量",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文本生成中事实召回的评估指标，属于纯粹的NLP评估基准范畴。虽然评估指标在理论上可能对搜索系统中的事实准确性有间接参考价值，但该研究缺乏与推荐系统、广告或搜索排名的直接关联，且不属于核心领域进展或使能技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07923v1": {
    "title": "STEPER: Step-wise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07923v1",
    "arxiv_id": "2510.07923v1",
    "authors": "Kyumin Lee, Minjin Jeon, Sanghwan Jang, Hwanjo Yu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 08:20:27",
    "ori_summary": "Answering complex real-world questions requires step-by-step retrieval and integration of relevant information to generate well-grounded responses. However, existing knowledge distillation methods overlook the need for different reasoning abilities at different steps, hindering transfer in multi-step retrieval-augmented frameworks. To address this, we propose Stepwise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models (StepER). StepER employs step-wise supervision to align with evolving information and reasoning demands across stages. Additionally, it incorporates difficulty-aware training to progressively optimize learning by prioritizing suitable steps. Our method is adaptable to various multi-step retrieval-augmented language models, including those that use retrieval queries for reasoning paths or decomposed questions. Extensive experiments show that StepER outperforms prior methods on multi-hop QA benchmarks, with an 8B model achieving performance comparable to a 70B teacher model.",
    "summary": "该论文研究多步检索增强语言模型中推理能力的提升问题，核心思想是通过分步监督和难度感知训练，针对不同推理阶段的信息需求和能力差异进行知识蒸馏。",
    "translation": "STEPER：通过分步知识蒸馏增强多步检索增强语言模型推理能力",
    "relevance_score": 8,
    "reasoning": "该论文涉及检索增强语言模型(RAG)的推理能力增强，直接关联搜索系统中的多步检索和推理任务。知识蒸馏技术作为使能技术，可以提升检索增强模型在复杂搜索场景下的性能表现，对于构建更智能的搜索系统具有直接应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文针对多步检索增强语言模型提出分步知识蒸馏方法，直接关联LLM在复杂推理任务中的应用，与推荐和搜索系统中的多步推理需求高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07912v1": {
    "title": "Towards Human-Like Grading: A Unified LLM-Enhanced Framework for Subjective Question Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.07912v1",
    "arxiv_id": "2510.07912v1",
    "authors": "Fanwei Zhua, Jiaxuan He, Xiaoxiao Chen, Zulong Chen, Quan Lu, Chenrui Mei",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 08:05:39",
    "ori_summary": "Automatic grading of subjective questions remains a significant challenge in examination assessment due to the diversity in question formats and the open-ended nature of student responses. Existing works primarily focus on a specific type of subjective question and lack the generality to support comprehensive exams that contain diverse question types. In this paper, we propose a unified Large Language Model (LLM)-enhanced auto-grading framework that provides human-like evaluation for all types of subjective questions across various domains. Our framework integrates four complementary modules to holistically evaluate student answers. In addition to a basic text matching module that provides a foundational assessment of content similarity, we leverage the powerful reasoning and generative capabilities of LLMs to: (1) compare key knowledge points extracted from both student and reference answers, (2) generate a pseudo-question from the student answer to assess its relevance to the original question, and (3) simulate human evaluation by identifying content-related and non-content strengths and weaknesses. Extensive experiments on both general-purpose and domain-specific datasets show that our framework consistently outperforms traditional and LLM-based baselines across multiple grading metrics. Moreover, the proposed system has been successfully deployed in real-world training and certification exams at a major e-commerce enterprise.",
    "summary": "",
    "translation": "迈向类人评分：用于主观问题评估的统一LLM增强框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注教育领域的自动评分系统，属于LLM在特定垂直领域的应用。虽然涉及LLM技术，但其核心应用场景（教育评分）与推荐系统、搜索或广告领域没有直接关联，也不涉及这些领域的特定挑战或数据模态。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07896v1": {
    "title": "ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall",
    "url": "https://www.alphaxiv.org/abs/2510.07896v1",
    "arxiv_id": "2510.07896v1",
    "authors": "Jiayu Yang, Yuxuan Fan, Songning Lai, Shengen Wu, Jiaqi Tang, Chun Kang, Zhijiang Guo, Yutao Yue",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 07:46:08",
    "ori_summary": "Large Language Models (LLMs) require efficient knowledge editing (KE) to update factual information, yet existing methods exhibit significant performance decay in multi-hop factual recall. This failure is particularly acute when edits involve intermediate implicit subjects within reasoning chains. Through causal analysis, we reveal that this limitation stems from an oversight of how chained knowledge is dynamically represented and utilized at the neuron level. We discover that during multi hop reasoning, implicit subjects function as query neurons, which sequentially activate corresponding value neurons across transformer layers to accumulate information toward the final answer, a dynamic prior KE work has overlooked. Guided by this insight, we propose ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall, a framework that leverages neuron-level attribution to identify and edit these critical query-value (Q-V) pathways. ACE provides a mechanistically grounded solution for multi-hop KE, empirically outperforming state-of-the-art methods by 9.44% on GPT-J and 37.46% on Qwen3-8B. Our analysis further reveals more fine-grained activation patterns in Qwen3 and demonstrates that the semantic interpretability of value neurons is orchestrated by query-driven accumulation. These findings establish a new pathway for advancing KE capabilities based on the principled understanding of internal reasoning mechanisms.",
    "summary": "",
    "translation": "ACE：面向多跳事实回忆的属性控制知识编辑",
    "relevance_score": 2,
    "reasoning": "该论文主要关注知识编辑和事实回忆，属于LLM内部知识管理范畴，与推荐系统、搜索或广告的核心技术关联较弱。虽然知识编辑技术理论上可能用于更新推荐系统中的实体知识，但论文标题明确聚焦于多跳事实回忆这一特定NLP任务，缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07892v1": {
    "title": "Metric Calculating Benchmark: Code-Verifiable Complicate Instruction Following Benchmark for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07892v1",
    "arxiv_id": "2510.07892v1",
    "authors": "Hyeonseok Moon, Seongtae Hong, Jaehyung Seo, Heuiseok Lim",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 07:43:15",
    "ori_summary": "Recent frontier-level LLMs have saturated many previously difficult benchmarks, leaving little room for further differentiation. This progress highlights the need for challenging benchmarks that provide objective verification. In this paper, we introduce MCBench, a benchmark designed to evaluate whether LLMs can execute string-matching NLP metrics by strictly following step-by-step instructions. Unlike prior benchmarks that depend on subjective judgments or general reasoning, MCBench offers an objective, deterministic and codeverifiable evaluation. This setup allows us to systematically test whether LLMs can maintain accurate step-by-step execution, including instruction adherence, numerical computation, and long-range consistency in handling intermediate results. To ensure objective evaluation of these abilities, we provide a parallel reference code that can evaluate the accuracy of LLM output. We provide three evaluative metrics and three benchmark variants designed to measure the detailed instruction understanding capability of LLMs. Our analyses show that MCBench serves as an effective and objective tool for evaluating the capabilities of cutting-edge LLMs.",
    "summary": "",
    "translation": "指标计算基准：面向大型语言模型的代码可验证复杂指令遵循基准",
    "relevance_score": 2,
    "reasoning": "该论文专注于LLM的基准测试和评估方法，属于纯粹的评估基准研究，与我的核心关注点（推荐系统、搜索广告领域的核心进展、使能技术及应用）无关。虽然涉及复杂指令遵循，但主要关注代码验证和基准构建，没有明确的推荐/搜索/广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07890v1": {
    "title": "Standard-to-Dialect Transfer Trends Differ across Text and Speech: A Case Study on Intent and Topic Classification in German Dialects",
    "url": "https://www.alphaxiv.org/abs/2510.07890v1",
    "arxiv_id": "2510.07890v1",
    "authors": "Verena Blaschke, Miriam Winkler, Barbara Plank",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 07:43:08",
    "ori_summary": "Research on cross-dialectal transfer from a standard to a non-standard dialect variety has typically focused on text data. However, dialects are primarily spoken, and non-standard spellings are known to cause issues in text processing. We compare standard-to-dialect transfer in three settings: text models, speech models, and cascaded systems where speech first gets automatically transcribed and then further processed by a text model. In our experiments, we focus on German and multiple German dialects in the context of written and spoken intent and topic classification. To that end, we release the first dialectal audio intent classification dataset. We find that the speech-only setup provides the best results on the dialect data while the text-only setup works best on the standard data. While the cascaded systems lag behind the text-only models for German, they perform relatively well on the dialectal data if the transcription system generates normalized, standard-like output.",
    "summary": "",
    "translation": "标准语到方言的迁移趋势在文本和语音中存在差异：德语方言意图与主题分类案例研究",
    "relevance_score": 2,
    "reasoning": "该论文主要研究方言处理中的迁移学习差异，属于特定语言处理领域。虽然意图分类在搜索和推荐中有应用，但论文聚焦于德语方言这一狭窄领域，且未涉及LLM、Transformer架构或推荐系统的核心进展，与当前关注点的直接关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07884v1": {
    "title": "Contrastive Weak-to-strong Generalization",
    "url": "https://www.alphaxiv.org/abs/2510.07884v1",
    "arxiv_id": "2510.07884v1",
    "authors": "Houcheng Jiang, Junfeng Fang, Jiaxin Wu, Tianyu Zhang, Chen Gao, Yong Li, Xiang Wang, Xiangnan He, Yang Deng",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 07:37:23",
    "ori_summary": "Weak-to-strong generalization provides a promising paradigm for scaling large language models (LLMs) by training stronger models on samples from aligned weaker ones, without requiring human feedback or explicit reward modeling. However, its robustness and generalization are hindered by the noise and biases in weak-model outputs, which limit its applicability in practice. To address this challenge, we leverage implicit rewards, which approximate explicit rewards through log-likelihood ratios, and reveal their structural equivalence with Contrastive Decoding (CD), a decoding strategy shown to reduce noise in LLM generation. Building on this connection, we propose Contrastive Weak-to-Strong Generalization (ConG), a framework that employs contrastive decoding between pre- and post-alignment weak models to generate higher-quality samples. This approach enables more reliable capability transfer, denoising, and improved robustness, substantially mitigating the limitations of traditional weak-to-strong methods. Empirical results across different model families confirm consistent improvements, demonstrating the generality and effectiveness of ConG. Taken together, our findings highlight the potential of ConG to advance weak-to-strong generalization and provide a promising pathway toward AGI.",
    "summary": "",
    "translation": "对比式弱到强泛化",
    "relevance_score": 6,
    "reasoning": "该论文关注对比学习和弱到强泛化，这是LLM训练和知识蒸馏中的核心技术进步。在推荐系统和搜索领域，这种技术可以用于从大型教师模型向更高效的学生模型进行知识迁移，或者处理用户反馈中的弱监督信号，从而提高模型泛化能力和部署效率。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07881v1": {
    "title": "CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for Mandarin-English Code-Switching",
    "url": "https://www.alphaxiv.org/abs/2510.07881v1",
    "arxiv_id": "2510.07881v1",
    "authors": "Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 07:34:23",
    "ori_summary": "The advancement of multimodal large language models has accelerated the development of speech-to-speech interaction systems. While natural monolingual interaction has been achieved, we find existing models exhibit deficiencies in language alignment. In our proposed Code-Switching Speech-to-Speech Benchmark (CS3-Bench), experiments on 7 mainstream models demonstrate a relative performance drop of up to 66% in knowledge-intensive question answering and varying degrees of misunderstanding in open-ended conversations. Starting from a model with severe performance deterioration, we propose both data constructions and training approaches to improve the language alignment capabilities, specifically employing Chain of Recognition (CoR) to enhance understanding and Keyword Highlighting (KH) to guide generation. Our approach improves the knowledge accuracy from 25.14% to 46.13%, with open-ended understanding rate from 64.5% to 86.5%, and significantly reduces pronunciation errors in the secondary language. CS3-Bench is available at https://huggingface.co/datasets/VocalNet/CS3-Bench.",
    "summary": "",
    "translation": "CS3-Bench：评估和增强普通话-英语语码转换的语音到语音大语言模型",
    "relevance_score": 2,
    "reasoning": "该论文专注于语音到语音的语码转换技术，属于语音处理领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然涉及LLM技术，但其应用场景（语音处理、语码转换）在RecSys/Search/Ads中缺乏明确的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07880v1": {
    "title": "Do LLMs Really Need 10+ Thoughts for \"Find the Time 1000 Days Later\"? Towards Structural Understanding of LLM Overthinking",
    "url": "https://www.alphaxiv.org/abs/2510.07880v1",
    "arxiv_id": "2510.07880v1",
    "authors": "Xinliang Frederick Zhang, Anhad Mohananey, Alexandra Chronopoulou, Pinelopi Papalampidi, Somit Gupta, Tsendsuren Munkhdalai, Lu Wang, Shyam Upadhyay",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 07:33:25",
    "ori_summary": "Models employing long chain-of-thought (CoT) reasoning have shown superior performance on complex reasoning tasks. Yet, this capability introduces a critical and often overlooked inefficiency -- overthinking -- models often engage in unnecessarily extensive reasoning even for simple queries, incurring significant computations without accuracy improvements. While prior work has explored solutions to mitigate overthinking, a fundamental gap remains in our understanding of its underlying causes. Most existing analyses are limited to superficial, profiling-based observations, failing to delve into LLMs' inner workings. This study introduces a systematic, fine-grained analyzer of LLMs' thought process to bridge the gap, TRACE. We first benchmark the overthinking issue, confirming that long-thinking models are five to twenty times slower on simple tasks with no substantial gains. We then use TRACE to first decompose the thought process into minimally complete sub-thoughts. Next, by inferring discourse relationships among sub-thoughts, we construct granular thought progression graphs and subsequently identify common thinking patterns for topically similar queries. Our analysis reveals two major patterns for open-weight thinking models -- Explorer and Late Landing. This finding provides evidence that over-verification and over-exploration are the primary drivers of overthinking in LLMs. Grounded in thought structures, we propose a utility-based definition of overthinking, which moves beyond length-based metrics. This revised definition offers a more insightful understanding of LLMs' thought progression, as well as practical guidelines for principled overthinking management.",
    "summary": "",
    "translation": "LLM真的需要10+次思考来“找出1000天后的时间”吗？——迈向对LLM过度思考的结构化理解",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLM的推理过程和过度思考现象，这属于LLM内部工作机制分析。虽然涉及LLM技术，但焦点是推理效率而非在推荐系统、搜索或广告中的具体应用。论文没有展示明确的RecSys/Search/Ads应用潜力，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07877v1": {
    "title": "Ready to Translate, Not to Represent? Bias and Performance Gaps in Multilingual LLMs Across Language Families and Domains",
    "url": "https://www.alphaxiv.org/abs/2510.07877v1",
    "arxiv_id": "2510.07877v1",
    "authors": "Md. Faiyaz Abdullah Sayeedi, Md. Mahbub Alam, Subhey Sadi Rahman, Md. Adnanul Islam, Jannatul Ferdous Deepti, Tasnim Mohiuddin, Md Mofijul Islam, Swakkhar Shatabda",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 07:28:30",
    "ori_summary": "The rise of Large Language Models (LLMs) has redefined Machine Translation (MT), enabling context-aware and fluent translations across hundreds of languages and textual domains. Despite their remarkable capabilities, LLMs often exhibit uneven performance across language families and specialized domains. Moreover, recent evidence reveals that these models can encode and amplify different biases present in their training data, posing serious concerns for fairness, especially in low-resource languages. To address these gaps, we introduce Translation Tangles, a unified framework and dataset for evaluating the translation quality and fairness of open-source LLMs. Our approach benchmarks 24 bidirectional language pairs across multiple domains using different metrics. We further propose a hybrid bias detection pipeline that integrates rule-based heuristics, semantic similarity filtering, and LLM-based validation. We also introduce a high-quality, bias-annotated dataset based on human evaluations of 1,439 translation-reference pairs. The code and dataset are accessible on GitHub: https://github.com/faiyazabdullah/TranslationTangles",
    "summary": "",
    "translation": "准备翻译而非表征？多语言大语言模型在语言家族和领域间的偏见与性能差距",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多语言LLM的偏见和性能差距问题，这属于纯粹的NLP评估和基准测试范畴。虽然多语言能力在理论上可能对国际化推荐/搜索系统有影响，但论文焦点是语言家族间的性能差异和翻译偏差，而非直接应用于推荐系统、搜索或广告的技术进步。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07842v1": {
    "title": "AdaSwitch: Adaptive Switching Generation for Knowledge Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.07842v1",
    "arxiv_id": "2510.07842v1",
    "authors": "Jingyu Peng, Maolin Wang, Hengyi Cai, Yuchen Li, Kai Zhang, Shuaiqiang Wang, Dawei Yin, Xiangyu Zhao",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 06:38:37",
    "ori_summary": "Small language models (SLMs) are crucial for applications with strict latency and computational constraints, yet achieving high performance remains challenging. Knowledge distillation (KD) can transfer capabilities from large teacher models, but existing methods involve trade-offs: off-policy distillation provides high-quality supervision but introduces a training-inference mismatch, while on-policy approaches maintain consistency but rely on low-quality student outputs. To address these issues, we propose AdaSwitch, a novel approach that dynamically combines on-policy and off-policy generation at the token level. AdaSwitch allows the student to first explore its own predictions and then selectively integrate teacher guidance based on real-time quality assessment. This approach simultaneously preserves consistency and maintains supervision quality. Experiments on three datasets with two teacher-student LLM pairs demonstrate that AdaSwitch consistently improves accuracy, offering a practical and effective method for distilling SLMs with acceptable additional overhead.",
    "summary": "",
    "translation": "AdaSwitch：用于知识蒸馏的自适应切换生成",
    "relevance_score": 4,
    "reasoning": "该论文关注知识蒸馏中的自适应切换生成技术，属于模型压缩和效率优化范畴。虽然知识蒸馏本身是LLM领域的重要技术，可以应用于推荐系统或搜索中的模型部署效率提升，但论文标题未明确指向RecSys/Search/Ads的具体应用场景，因此相关性中等。这种自适应切换机制有潜力应用于推荐系统中的多专家模型或搜索中的查询自适应处理。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07841v1": {
    "title": "Self-Improving LLM Agents at Test-Time",
    "url": "https://www.alphaxiv.org/abs/2510.07841v1",
    "arxiv_id": "2510.07841v1",
    "authors": "Emre Can Acikgoz, Cheng Qian, Heng Ji, Dilek Hakkani-Tür, Gokhan Tur",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-09 06:37:35",
    "ori_summary": "One paradigm of language model (LM) fine-tuning relies on creating large training datasets, under the assumption that high quantity and diversity will enable models to generalize to novel tasks after post-training. In practice, gathering large sets of data is inefficient, and training on them is prohibitively expensive; worse, there is no guarantee that the resulting model will handle complex scenarios or generalize better. Moreover, existing techniques rarely assess whether a training sample provides novel information or is redundant with the knowledge already acquired by the model, resulting in unnecessary costs. In this work, we explore a new test-time self-improvement method to create more effective and generalizable agentic LMs on-the-fly. The proposed algorithm can be summarized in three steps: (i) first it identifies the samples that model struggles with (self-awareness), (ii) then generates similar examples from detected uncertain samples (self-data augmentation), and (iii) uses these newly generated samples at test-time fine-tuning (self-improvement). We study two variants of this approach: Test-Time Self-Improvement (TT-SI), where the same model generates additional training examples from its own uncertain cases and then learns from them, and contrast this approach with Test-Time Distillation (TT-D), where a stronger model generates similar examples for uncertain cases, enabling student to adapt using distilled supervision. Empirical evaluations across different agent benchmarks demonstrate that TT-SI improves the performance with +5.48% absolute accuracy gain on average across all benchmarks and surpasses other standard learning methods, yet using 68x less training samples. Our findings highlight the promise of TT-SI, demonstrating the potential of self-improvement algorithms at test-time as a new paradigm for building more capable agents toward self-evolution.",
    "summary": "该论文研究如何解决传统大模型微调依赖海量训练数据且效率低下的问题，核心思想是让模型在测试时通过识别自身困难样本、生成类似示例并进行在线微调来实现自我改进。",
    "translation": "测试时自改进的大语言模型智能体",
    "relevance_score": 8,
    "reasoning": "该论文属于'直接LLM应用'类别，探索LLM智能体在测试时的自改进能力，这对于推荐系统、搜索和广告中的在线学习和实时优化具有直接应用价值。这种能力可以用于动态调整推荐策略、优化搜索排序或改进广告投放效果，通过实时反馈循环持续提升系统性能。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的测试时自改进方法直接应用于LLM智能体，属于LLM直接应用和使能技术范畴，对推荐和搜索系统的在线学习有重要参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07835v1": {
    "title": "MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation",
    "url": "https://www.alphaxiv.org/abs/2510.07835v1",
    "arxiv_id": "2510.07835v1",
    "authors": "Weisen Jiang, Sinno Jialin Pan",
    "categories": "cs.LG, cs.AI, cs.CL, cs.CR",
    "pub_date": "2025-10-09 06:27:34",
    "ori_summary": "This paper introduces MetaDefense, a novel framework for defending against finetuning-based jailbreak attacks in large language models (LLMs). We observe that existing defense mechanisms fail to generalize to harmful queries disguised by unseen attack templates, despite LLMs being capable of distinguishing disguised harmful queries in the embedding space. Based on these insights, we propose a two-stage defense approach: (i) pre-generation defense that detects harmful queries before response generation begins, and (ii) mid-generation defense that monitors partial responses during generation to prevent outputting more harmful content. Our MetaDefense trains the LLM to predict the harmfulness of both queries and partial responses using specialized prompts, enabling early termination of potentially harmful interactions. Extensive experiments across multiple LLM architectures (LLaMA-2-7B, Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense significantly outperforms existing defense mechanisms, achieving robust defense against harmful queries with seen and unseen attack templates while maintaining competitive performance on benign tasks. Code is available at https://github.com/ws-jiang/MetaDefense.",
    "summary": "",
    "translation": "MetaDefense：在生成前后防御基于微调的越狱攻击",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM安全防御技术，特别是对抗越狱攻击的方法，这属于安全/隐私范畴，被明确列为无关主题。虽然涉及LLM技术，但其核心关注点是防御机制而非推荐系统、搜索或广告的应用潜力，与当前关注的领域进展、使能技术或直接应用无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07821v1": {
    "title": "From Keywords to Clusters: AI-Driven Analysis of YouTube Comments to Reveal Election Issue Salience in 2024",
    "url": "https://www.alphaxiv.org/abs/2510.07821v1",
    "arxiv_id": "2510.07821v1",
    "authors": "Raisa M. Simoes, Timoteo Kelly, Eduardo J. Simoes, Praveen Rao",
    "categories": "cs.SI, cs.CL",
    "pub_date": "2025-10-09 06:02:10",
    "ori_summary": "This paper aims to explore two competing data science methodologies to attempt answering the question, \"Which issues contributed most to voters' choice in the 2024 presidential election?\" The methodologies involve novel empirical evidence driven by artificial intelligence (AI) techniques. By using two distinct methods based on natural language processing and clustering analysis to mine over eight thousand user comments on election-related YouTube videos from one right leaning journal, Wall Street Journal, and one left leaning journal, New York Times, during pre-election week, we quantify the frequency of selected issue areas among user comments to infer which issues were most salient to potential voters in the seven days preceding the November 5th election. Empirically, we primarily demonstrate that immigration and democracy were the most frequently and consistently invoked issues in user comments on the analyzed YouTube videos, followed by the issue of identity politics, while inflation was significantly less frequently referenced. These results corroborate certain findings of post-election surveys but also refute the supposed importance of inflation as an election issue. This indicates that variations on opinion mining, with their analysis of raw user data online, can be more revealing than polling and surveys for analyzing election outcomes.",
    "summary": "",
    "translation": "从关键词到聚类：基于人工智能的YouTube评论分析揭示2024年选举议题显著性",
    "relevance_score": 3,
    "reasoning": "虽然论文涉及AI驱动的文本分析，但其焦点是政治选举议题分析而非推荐系统、搜索或广告的核心技术。该方法可能间接应用于内容理解，但缺乏对RecSys/Search/Ads架构、Transformer改进或LLM应用的直接贡献。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07812v1": {
    "title": "Multilingual Generative Retrieval via Cross-lingual Semantic Compression",
    "url": "https://www.alphaxiv.org/abs/2510.07812v1",
    "arxiv_id": "2510.07812v1",
    "authors": "Yuxin Huang, Simeng Wu, Ran Song, Yan Xiang, Yantuan Xian, Shengxiang Gao, Zhengtao Yu",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 05:42:57",
    "ori_summary": "Generative Information Retrieval is an emerging retrieval paradigm that exhibits remarkable performance in monolingual scenarios.However, applying these methods to multilingual retrieval still encounters two primary challenges, cross-lingual identifier misalignment and identifier inflation. To address these limitations, we propose Multilingual Generative Retrieval via Cross-lingual Semantic Compression (MGR-CSC), a novel framework that unifies semantically equivalent multilingual keywords into shared atoms to align semantics and compresses the identifier space, and we propose a dynamic multi-step constrained decoding strategy during retrieval. MGR-CSC improves cross-lingual alignment by assigning consistent identifiers and enhances decoding efficiency by reducing redundancy. Experiments demonstrate that MGR-CSC achieves outstanding retrieval accuracy, improving by 6.83% on mMarco100k and 4.77% on mNQ320k, while reducing document identifiers length by 74.51% and 78.2%, respectively.",
    "summary": "",
    "translation": "基于跨语言语义压缩的多语言生成式检索",
    "relevance_score": 8,
    "reasoning": "该论文涉及生成式检索技术，这是搜索系统中的核心进展。跨语言语义压缩技术可以显著提升多语言搜索系统的效率和效果，直接应用于多语言搜索场景。这种压缩方法也有潜力应用于推荐系统中的多语言内容理解和用户兴趣建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07799v1": {
    "title": "Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2510.07799v1",
    "arxiv_id": "2510.07799v1",
    "authors": "Eric Hanchen Jiang, Guancheng Wan, Sophia Yin, Mengting Li, Yuchen Wu, Xiao Liang, Xinfeng Li, Yizhou Sun, Wei Wang, Kai-Wei Chang, Ying Nian Wu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 05:28:28",
    "ori_summary": "The efficiency of multi-agent systems driven by large language models (LLMs) largely hinges on their communication topology. However, designing an optimal topology is a non-trivial challenge, as it requires balancing competing objectives such as task performance, communication cost, and robustness. Existing frameworks often rely on static or hand-crafted topologies, which inherently fail to adapt to diverse task requirements, leading to either excessive token consumption for simple problems or performance bottlenecks for complex ones. To address this challenge, we introduce a novel generative framework called \\textit{Guided Topology Diffusion (GTD)}. Inspired by conditional discrete graph diffusion models, GTD formulates topology synthesis as an iterative construction process. At each step, the generation is steered by a lightweight proxy model that predicts multi-objective rewards (e.g., accuracy, utility, cost), enabling real-time, gradient-free optimization towards task-adaptive topologies. This iterative, guided synthesis process distinguishes GTD from single-step generative frameworks, enabling it to better navigate complex design trade-offs. We validated GTD across multiple benchmarks, and experiments show that this framework can generate highly task-adaptive, sparse, and efficient communication topologies, significantly outperforming existing methods in LLM agent collaboration.",
    "summary": "",
    "translation": "基于图扩散模型动态生成多LLM智能体通信拓扑",
    "relevance_score": 8,
    "reasoning": "该论文涉及多LLM智能体系统，这属于'直接LLM应用'范畴，在推荐系统和搜索中可用于构建协同推理框架。图扩散模型用于动态优化通信拓扑，这种技术可应用于多智能体推荐系统中协调不同LLM专家的交互模式，提升整体系统性能。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07794v1": {
    "title": "HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2510.07794v1",
    "arxiv_id": "2510.07794v1",
    "authors": "Peilin Wu, Mian Zhang, Kun Wan, Wentian Zhao, Kaiyu He, Xinya Du, Zhiyu Chen",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-09 05:13:10",
    "ori_summary": "Agentic RAG is a powerful technique for incorporating external information that LLMs lack, enabling better problem solving and question answering. However, suboptimal search behaviors exist widely, such as over-search (retrieving information already known) and under-search (failing to search when necessary), which leads to unnecessary overhead and unreliable outputs. Current training methods, which typically rely on outcome-based rewards in a RL framework, lack the fine-grained control needed to address these inefficiencies. To overcome this, we introduce Hierarchical Process Rewards for Efficient agentic RAG (HiPRAG), a training methodology that incorporates a fine-grained, knowledge-grounded process reward into the RL training. Our approach evaluates the necessity of each search decision on-the-fly by decomposing the agent's reasoning trajectory into discrete, parsable steps. We then apply a hierarchical reward function that provides an additional bonus based on the proportion of optimal search and non-search steps, on top of commonly used outcome and format rewards. Experiments on the Qwen2.5 and Llama-3.2 models across seven diverse QA benchmarks show that our method achieves average accuracies of 65.4% (3B) and 67.2% (7B). This is accomplished while improving search efficiency, reducing the over-search rate to just 2.3% and concurrently lowering the under-search rate. These results demonstrate the efficacy of optimizing the reasoning process itself, not just the final outcome. Further experiments and analysis demonstrate that HiPRAG shows good generalizability across a wide range of RL algorithms, model families, sizes, and types. This work demonstrates the importance and potential of fine-grained control through RL, for improving the efficiency and optimality of reasoning for search agents.",
    "summary": "",
    "translation": "HiPRAG：用于高效代理检索增强生成的分层过程奖励",
    "relevance_score": 8,
    "reasoning": "该论文涉及检索增强生成(RAG)的代理方法，这直接适用于搜索系统，其中代理可以主动检索和整合信息以响应用户查询。分层过程奖励机制通过优化检索和生成过程，可以显著提升搜索和推荐系统中的内容质量和效率，减少不相关信息的检索。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07793v1": {
    "title": "LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology",
    "url": "https://www.alphaxiv.org/abs/2510.07793v1",
    "arxiv_id": "2510.07793v1",
    "authors": "Sajib Acharjee Dip, Adrika Zafor, Bikash Kumar Paul, Uddip Acharjee Shuvo, Muhit Islam Emon, Xuan Wang, Liqing Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 05:12:09",
    "ori_summary": "Large language models (LLMs) and emerging agentic frameworks are beginning to transform single-cell biology by enabling natural-language reasoning, generative annotation, and multimodal data integration. However, progress remains fragmented across data modalities, architectures, and evaluation standards. LLM4Cell presents the first unified survey of 58 foundation and agentic models developed for single-cell research, spanning RNA, ATAC, multi-omic, and spatial modalities. We categorize these methods into five families-foundation, text-bridge, spatial, multimodal, epigenomic, and agentic-and map them to eight key analytical tasks including annotation, trajectory and perturbation modeling, and drug-response prediction. Drawing on over 40 public datasets, we analyze benchmark suitability, data diversity, and ethical or scalability constraints, and evaluate models across 10 domain dimensions covering biological grounding, multi-omics alignment, fairness, privacy, and explainability. By linking datasets, models, and evaluation domains, LLM4Cell provides the first integrated view of language-driven single-cell intelligence and outlines open challenges in interpretability, standardization, and trustworthy model development.",
    "summary": "",
    "translation": "LLM4Cell：面向单细胞生物学的大语言与智能体模型综述",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于单细胞生物学这一生物医学领域，属于明确的无关主题范畴。标题中虽提及大语言模型，但其应用场景完全限定在生物学领域，与推荐系统、搜索或广告没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07782v1": {
    "title": "RCPU: Rotation-Constrained Error Compensation for Structured Pruning of a Large Language Model",
    "url": "https://www.alphaxiv.org/abs/2510.07782v1",
    "arxiv_id": "2510.07782v1",
    "authors": "Shuichiro Haruta, Kazunori Matsumoto, Zhi Li, Yanan Wang, Mori Kurokawa",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 04:54:09",
    "ori_summary": "In this paper, we propose a rotation-constrained compensation method to address the errors introduced by structured pruning of large language models (LLMs). LLMs are trained on massive datasets and accumulate rich semantic knowledge in their representation space. In contrast, pruning is typically carried out with only a small amount of calibration data, which makes output mismatches unavoidable. Although direct least-squares fitting can reduce such errors, it tends to overfit to the limited calibration set, destructively modifying pretrained weights. To overcome this difficulty, we update the pruned parameters under a rotation constraint. This constrained update preserves the geometry of output representations (i.e., norms and inner products) and simultaneously re-aligns the pruned subspace with the original outputs. Furthermore, in rotation-constrained compensation, removing components that strongly contribute to the principal directions of the output makes error recovery difficult. Since input dimensions with large variance strongly affect these principal directions, we design a variance-aware importance score that ensures such dimensions are preferentially kept in the pruned model. By combining this scoring rule with rotation-constrained updates, the proposed method effectively compensates errors while retaining the components likely to be more important in a geometry-preserving manner. In the experiments, we apply the proposed method to LLaMA-7B and evaluate it on WikiText-2 and multiple language understanding benchmarks. The results demonstrate consistently better perplexity and task accuracy compared with existing baselines.",
    "summary": "",
    "translation": "RCPU：大语言模型结构化剪枝的旋转约束误差补偿",
    "relevance_score": 7,
    "reasoning": "该论文涉及大语言模型的高效压缩技术，属于'使能LLM技术'范畴。结构化剪枝技术可以显著减少LLM的计算和内存需求，这对于在推荐系统、搜索和广告中部署大型模型至关重要，能够实现更快的推理速度和更低的部署成本。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07777v1": {
    "title": "Drift No More? Context Equilibria in Multi-Turn LLM Interactions",
    "url": "https://www.alphaxiv.org/abs/2510.07777v1",
    "arxiv_id": "2510.07777v1",
    "authors": "Vardhan Dongre, Ryan A. Rossi, Viet Dac Lai, David Seunghyun Yoon, Dilek Hakkani-Tür, Trung Bui",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 04:48:49",
    "ori_summary": "Large Language Models (LLMs) excel at single-turn tasks such as instruction following and summarization, yet real-world deployments require sustained multi-turn interactions where user goals and conversational context persist and evolve. A recurring challenge in this setting is context drift: the gradual divergence of a model's outputs from goal-consistent behavior across turns. Unlike single-turn errors, drift unfolds temporally and is poorly captured by static evaluation metrics. In this work, we present a study of context drift in multi-turn interactions and propose a simple dynamical framework to interpret its behavior. We formalize drift as the turn-wise KL divergence between the token-level predictive distributions of the test model and a goal-consistent reference model, and propose a recurrence model that interprets its evolution as a bounded stochastic process with restoring forces and controllable interventions. We instantiate this framework in both synthetic long-horizon rewriting tasks and realistic user-agent simulations such as in $\\tau$-Bench, measuring drift for several open-weight LLMs that are used as user simulators. Our experiments consistently reveal stable, noise-limited equilibria rather than runaway degradation, and demonstrate that simple reminder interventions reliably reduce divergence in line with theoretical predictions. Together, these results suggest that multi-turn drift can be understood as a controllable equilibrium phenomenon rather than as inevitable decay, providing a foundation for studying and mitigating context drift in extended interactions.",
    "summary": "",
    "translation": "不再漂移？多轮大语言模型交互中的上下文均衡",
    "relevance_score": 7,
    "reasoning": "该论文研究多轮LLM交互中的上下文均衡问题，直接关联到LLM在推荐系统和搜索中的实际应用场景。在多轮对话推荐和搜索交互中，上下文漂移是影响用户体验的关键技术挑战，该研究可为构建更稳定的多轮推荐/搜索系统提供技术基础。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07776v1": {
    "title": "Instance Relation Learning Network with Label Knowledge Propagation for Few-shot Multi-label Intent Detection",
    "url": "https://www.alphaxiv.org/abs/2510.07776v1",
    "arxiv_id": "2510.07776v1",
    "authors": "Shiman Zhao, Shangyuan Li, Wei Chen, Tengjiao Wang, Jiahui Yao, Jiabin Zheng, Kam Fai Wong",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-09 04:47:06",
    "ori_summary": "Few-shot Multi-label Intent Detection (MID) is crucial for dialogue systems, aiming to detect multiple intents of utterances in low-resource dialogue domains. Previous studies focus on a two-stage pipeline. They first learn representations of utterances with multiple labels and then use a threshold-based strategy to identify multi-label results. However, these methods rely on representation classification and ignore instance relations, leading to error propagation. To solve the above issues, we propose a multi-label joint learning method for few-shot MID in an end-to-end manner, which constructs an instance relation learning network with label knowledge propagation to eliminate error propagation. Concretely, we learn the interaction relations between instances with class information to propagate label knowledge between a few labeled (support set) and unlabeled (query set) instances. With label knowledge propagation, the relation strength between instances directly indicates whether two utterances belong to the same intent for multi-label prediction. Besides, a dual relation-enhanced loss is developed to optimize support- and query-level relation strength to improve performance. Experiments show that we outperform strong baselines by an average of 9.54% AUC and 11.19% Macro-F1 in 1-shot scenarios.",
    "summary": "",
    "translation": "基于标签知识传播的实例关系学习网络用于少样本多标签意图检测",
    "relevance_score": 3,
    "reasoning": "该论文主要关注少样本多标签意图检测，这属于对话系统和NLP领域的特定任务，与推荐系统、搜索或广告的核心领域进展没有直接关联。虽然意图检测在搜索查询理解中有潜在应用，但论文重点在于少样本学习和多标签分类技术，而非直接应用于推荐、搜索或广告系统的核心排名或建模问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07775v1": {
    "title": "The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.07775v1",
    "arxiv_id": "2510.07775v1",
    "authors": "Omar Mahmoud, Ali Khalil, Buddhika Laknath Semage, Thommen George Karimpanal, Santu Rana",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 04:30:58",
    "ori_summary": "Hallucination in large language models (LLMs) has been widely studied in recent years, with progress in both detection and mitigation aimed at improving truthfulness. Yet, a critical side effect remains largely overlooked: enhancing truthfulness can negatively impact safety alignment. In this paper, we investigate this trade-off and show that increasing factual accuracy often comes at the cost of weakened refusal behavior. Our analysis reveals that this arises from overlapping components in the model that simultaneously encode hallucination and refusal information, leading alignment methods to suppress factual knowledge unintentionally. We further examine how fine-tuning on benign datasets, even when curated for safety, can degrade alignment for the same reason. To address this, we propose a method that disentangles refusal-related features from hallucination features using sparse autoencoders, and preserves refusal behavior during fine-tuning through subspace orthogonalization. This approach prevents hallucinations from increasing while maintaining safety alignment.We evaluate our method on commonsense reasoning tasks and harmful benchmarks (AdvBench and StrongReject). Results demonstrate that our approach preserves refusal behavior and task utility, mitigating the trade-off between truthfulness and safety.",
    "summary": "",
    "translation": "AI对齐的意外权衡：在大型语言模型中平衡幻觉缓解与安全性",
    "relevance_score": 1,
    "reasoning": "该论文主要关注LLM的对齐、幻觉缓解和安全性问题，这些都属于纯粹的NLP中心话题，与推荐系统、搜索或广告的核心技术进展无关。虽然提到了幻觉缓解，但这属于被明确排除的无关主题范畴，没有展示出在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07774v1": {
    "title": "Curing Miracle Steps in LLM Mathematical Reasoning with Rubric Rewards",
    "url": "https://www.alphaxiv.org/abs/2510.07774v1",
    "arxiv_id": "2510.07774v1",
    "authors": "Youliang Yuan, Qiuyang Mang, Jingbang Chen, Hong Wan, Xiaoyuan Liu, Junjielong Xu, Jen-tse Huang, Wenxuan Wang, Wenxiang Jiao, Pinjia He",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 04:30:45",
    "ori_summary": "Large language models for mathematical reasoning are typically trained with outcome-based rewards, which credit only the final answer. In our experiments, we observe that this paradigm is highly susceptible to reward hacking, leading to a substantial overestimation of a model's reasoning ability. This is evidenced by a high incidence of false positives - solutions that reach the correct final answer through an unsound reasoning process. Through a systematic analysis with human verification, we establish a taxonomy of these failure modes, identifying patterns like Miracle Steps - abrupt jumps to a correct output without a valid preceding derivation. Probing experiments suggest a strong association between these Miracle Steps and memorization, where the model appears to recall the answer directly rather than deriving it. To mitigate this systemic issue, we introduce the Rubric Reward Model (RRM), a process-oriented reward function that evaluates the entire reasoning trajectory against problem-specific rubrics. The generative RRM provides fine-grained, calibrated rewards (0-1) that explicitly penalize logical flaws and encourage rigorous deduction. When integrated into a reinforcement learning pipeline, RRM-based training consistently outperforms outcome-only supervision across four math benchmarks. Notably, it boosts Verified Pass@1024 on AIME2024 from 26.7% to 62.6% and reduces the incidence of Miracle Steps by 71%. Our work demonstrates that rewarding the solution process is crucial for building models that are not only more accurate but also more reliable.",
    "summary": "",
    "translation": "使用评分标准奖励治愈LLM数学推理中的奇迹步骤",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM在数学推理任务中的特定问题（奇迹步骤）和训练方法（评分标准奖励），这属于纯粹的NLP推理改进范畴。虽然涉及LLM训练技术，但缺乏明确的与推荐系统、搜索或广告相关的潜在应用场景，更像是通用NLP能力的优化。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07768v1": {
    "title": "ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.07768v1",
    "arxiv_id": "2510.07768v1",
    "authors": "Murong Yue, Zhiwei Liu, Liangwei Yang, Jianguo Zhang, Zuxin Liu, Haolin Chen, Ziyu Yao, Silvio Savarese, Caiming Xiong, Shelby Heinecke, Huan Wang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 04:11:16",
    "ori_summary": "Large Language Models (LLMs) equipped with external tools have demonstrated enhanced performance on complex reasoning tasks. The widespread adoption of this tool-augmented reasoning is hindered by the scarcity of domain-specific tools. For instance, in domains such as physics question answering, suitable and specialized tools are often missing. Recent work has explored automating tool creation by extracting reusable functions from Chain-of-Thought (CoT) reasoning traces; however, these approaches face a critical scalability bottleneck. As the number of generated tools grows, storing them in an unstructured collection leads to significant retrieval challenges, including an expanding search space and ambiguity between function-related tools. To address this, we propose a systematic approach to automatically refactor an unstructured collection of tools into a structured tool library. Our system first generates discrete, task-specific tools and clusters them into semantically coherent topics. Within each cluster, we introduce a multi-agent framework to consolidate scattered functionalities: a code agent refactors code to extract shared logic and creates versatile, aggregated tools, while a reviewing agent ensures that these aggregated tools maintain the complete functional capabilities of the original set. This process transforms numerous question-specific tools into a smaller set of powerful, aggregated tools without loss of functionality. Experimental results demonstrate that our approach significantly improves tool retrieval accuracy and overall reasoning performance across multiple reasoning tasks. Furthermore, our method shows enhanced scalability compared with baselines as the number of question-specific increases.",
    "summary": "",
    "translation": "ToolLibGen：面向大语言模型推理的可扩展自动工具创建与聚合",
    "relevance_score": 8,
    "reasoning": "该论文属于Enabling LLM Tech范畴，专注于提升LLM的工具使用和推理能力。在搜索和推荐系统中，这种自动工具创建技术可以用于构建更智能的查询理解、多模态信息检索和复杂用户意图推理模块，显著提升系统处理复杂任务的能力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07761v1": {
    "title": "Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers",
    "url": "https://www.alphaxiv.org/abs/2510.07761v1",
    "arxiv_id": "2510.07761v1",
    "authors": "Nishant Balepur, Atrey Desai, Rachel Rudinger",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 04:00:09",
    "ori_summary": "Large language models (LLMs) now give reasoning before answering, excelling in tasks like multiple-choice question answering (MCQA). Yet, a concern is that LLMs do not solve MCQs as intended, as work finds LLMs sans reasoning succeed in MCQA without using the question, i.e., choices-only. Such partial-input success is often deemed problematic, but reasoning traces could reveal if these strategies are truly shallow in choices-only settings. To study these strategies, reasoning LLMs solve MCQs in full and choices-only inputs; test-time reasoning often boosts accuracy on full and in choices-only half the time. While possibly due to shallow shortcuts, choices-only success is barely affected by the length of reasoning traces, and after finding traces pass faithfulness tests, we show they use less problematic strategies like inferring missing questions. In all, we challenge claims that partial-input success is always a flaw, so we discuss how reasoning traces could separate problematic data from less problematic reasoning.",
    "summary": "",
    "translation": "测试时推理器是策略性多项选择题答题者",
    "relevance_score": 3,
    "reasoning": "该论文关注LLM在测试时的推理策略和多项选择题处理能力，这属于核心LLM技术的进步。虽然测试时推理优化可能间接提升推荐或搜索系统中LLM的决策质量，但论文焦点是通用的测试策略而非具体的RecSys/Search/Ads应用，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07745v1": {
    "title": "Parallel Test-Time Scaling for Latent Reasoning Models",
    "url": "https://www.alphaxiv.org/abs/2510.07745v1",
    "arxiv_id": "2510.07745v1",
    "authors": "Runyang You, Yongqi Li, Meng Liu, Wenjie Wang, Liqiang Nie, Wenjie Li",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-09 03:33:00",
    "ori_summary": "Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (LLMs), typically by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer a more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. \\ This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open a new direction for scalable inference in continuous spaces. Code released at https://github.com/YRYangang/LatentTTS.",
    "summary": "",
    "translation": "潜在推理模型的并行测试时缩放",
    "relevance_score": 7,
    "reasoning": "该论文涉及测试时缩放技术，这属于'使能LLM技术'范畴，通过优化推理阶段的模型扩展来提高效率。在推荐系统和搜索领域，这种技术可以显著降低大规模模型部署的延迟和计算成本，特别是在处理复杂用户序列和上下文特征时实现更高效的实时推理。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07743v1": {
    "title": "OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment",
    "url": "https://www.alphaxiv.org/abs/2510.07743v1",
    "arxiv_id": "2510.07743v1",
    "authors": "Tianci Liu, Ran Xu, Tony Yu, Ilgee Hong, Carl Yang, Tuo Zhao, Haoyu Wang",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 03:31:26",
    "ori_summary": "Reward modeling lies at the core of reinforcement learning from human feedback (RLHF), yet most existing reward models rely on scalar or pairwise judgments that fail to capture the multifaceted nature of human preferences. Recent studies have explored rubrics-as-rewards (RaR) that uses structured natural language criteria that capture multiple dimensions of response quality. However, producing rubrics that are both reliable and scalable remains a key challenge. In this work, we introduce OpenRubrics, a diverse, large-scale collection of (prompt, rubric) pairs for training rubric-generation and rubric-based reward models. To elicit discriminative and comprehensive evaluation signals, we introduce Contrastive Rubric Generation (CRG), which derives both hard rules (explicit constraints) and principles (implicit qualities) by contrasting preferred and rejected responses. We further improve reliability by enforcing preference-label consistency via rejection sampling to remove noisy rubrics. Across multiple reward-modeling benchmarks, our rubric-based reward model, Rubric-RM, surpasses strong size-matched baselines by 6.8%. These gains transfer to policy models on instruction-following and biomedical benchmarks. Our results show that rubrics provide scalable alignment signals that narrow the gap between costly human evaluation and automated reward modeling, enabling a new principle-driven paradigm for LLM alignment.",
    "summary": "",
    "translation": "OpenRubrics：面向奖励建模和大语言模型对齐的可扩展合成评分标准生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM对齐和奖励建模中的评分标准生成，属于纯粹的LLM对齐技术范畴。虽然奖励建模在理论上可以应用于推荐系统的偏好学习，但论文标题明确聚焦于评分标准生成这一特定对齐任务，与推荐/搜索/广告系统的核心排序和匹配问题关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07737v1": {
    "title": "ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.07737v1",
    "arxiv_id": "2510.07737v1",
    "authors": "Fu Chen, Peng Wang, Xiyin Li, Wen Li, Shichi Lei, Dongdong Xiang",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-09 03:20:13",
    "ori_summary": "Training Large Language Models (LLMs) with Group Relative Policy Optimization (GRPO) encounters a significant challenge: models often fail to produce accurate responses, particularly in small-scale architectures. This limitation not only diminishes performance improvements and undermines the potential of GRPO but also frequently leads to mid-training collapse, adversely affecting stability and final efficacy. To address these issues, we propose ToolExpander, a novel framework that advances tool-oriented reinforcement learning for resource-constrained LLMs through two key innovations:(1) Dynamic Multi-Round Hard Sampling, which dynamically substitutes challenging samples(those without correct outputs over 10 rollouts) with high-quality few-shot demonstrations during training, coupled with an exponential learning rate decay strategy to mitigate oscillations;(2) Self-Exemplifying Thinking, an enhanced GRPO framework that eliminates KL divergence and incorporates adjusted clipping coefficients, encouraging models to autonomously generate and analyze few-shot examples via a minimal additional reward (0.01).Experimental results demonstrate that ToolExpander significantly enhances tool-using capabilities in LLMs, especially in weaker small-scale models, improving both training stability and overall performance.",
    "summary": "",
    "translation": "ToolExpander：将工具使用强化学习的前沿扩展到弱大型语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注工具使用强化学习与弱LLMs的结合，这属于强化学习的特定应用领域。虽然提到了LLMs，但核心是RL方法而非LLM技术本身，且没有明确说明在推荐系统、搜索或广告中的具体应用潜力。该工作更偏向于通用的RL技术改进，而非直接相关的领域应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07736v1": {
    "title": "Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing",
    "url": "https://www.alphaxiv.org/abs/2510.07736v1",
    "arxiv_id": "2510.07736v1",
    "authors": "Cunli Mao, Xiaofei Gao, Ran Song, Shizhu He, Shengxiang Gao, Kang Liu, Zhengtao Yu",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 03:19:21",
    "ori_summary": "Large language models (LLMs) based Multilingual Knowledge Graph Completion (MKGC) aim to predict missing facts by leveraging LLMs' multilingual understanding capabilities, improving the completeness of multilingual knowledge graphs (KGs). However, existing MKGC research underutilizes the multilingual capabilities of LLMs and ignores the shareability of cross-lingual knowledge. In this paper, we propose a novel MKGC framework that leverages multilingual shared knowledge to significantly enhance performance through two components: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative Entity Reranking (IER). KL-GMoE efficiently models shared knowledge, while IER significantly enhances its utilization. To evaluate our framework, we constructed a mKG dataset containing 5 languages and conducted comprehensive comparative experiments with existing state-of-the-art (SOTA) MKGC method. The experimental results demonstrate that our framework achieves improvements of 5.47%, 3.27%, and 1.01% in the Hits@1, Hits@3, and Hits@10 metrics, respectively, compared with SOTA MKGC method. Further experimental analysis revealed the properties of knowledge sharing in settings of unseen and unbalanced languages. We have released the dataset and code for our work on https://github.com/gaoxiaofei07/KL-GMoE.",
    "summary": "",
    "translation": "基于高效多语言知识共享的多语言知识图谱补全",
    "relevance_score": 3,
    "reasoning": "该论文主要关注多语言知识图谱补全，属于通用知识图谱领域，与推荐系统、搜索或广告的核心进展没有直接关联。虽然知识图谱技术可以间接支持这些领域的知识增强，但论文标题未表明其专注于推荐、搜索或广告应用，也未涉及LLM、Transformer架构或异构数据统一建模等关键技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07731v1": {
    "title": "oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation and Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.07731v1",
    "arxiv_id": "2510.07731v1",
    "authors": "Ruiling Xu, Yifan Zhang, Qingyun Wang, Carl Edwards, Heng Ji",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-09 03:13:31",
    "ori_summary": "Organic reaction mechanisms are the stepwise elementary reactions by which reactants form intermediates and products, and are fundamental to understanding chemical reactivity and designing new molecules and reactions. Although large language models (LLMs) have shown promise in understanding chemical tasks such as synthesis design, it is unclear to what extent this reflects genuine chemical reasoning capabilities, i.e., the ability to generate valid intermediates, maintain chemical consistency, and follow logically coherent multi-step pathways. We address this by introducing oMeBench, the first large-scale, expert-curated benchmark for organic mechanism reasoning in organic chemistry. It comprises over 10,000 annotated mechanistic steps with intermediates, type labels, and difficulty ratings. Furthermore, to evaluate LLM capability more precisely and enable fine-grained scoring, we propose oMeS, a dynamic evaluation framework that combines step-level logic and chemical similarity. We analyze the performance of state-of-the-art LLMs, and our results show that although current models display promising chemical intuition, they struggle with correct and consistent multi-step reasoning. Notably, we find that using prompting strategy and fine-tuning a specialist model on our proposed dataset increases performance by 50% over the leading closed-source model. We hope that oMeBench will serve as a rigorous foundation for advancing AI systems toward genuine chemical reasoning.",
    "summary": "",
    "translation": "oMeBench：面向有机机理阐明与推理中大型语言模型鲁棒性基准测试",
    "relevance_score": 1,
    "reasoning": "该论文专注于有机化学机理领域的基准测试，属于化学领域特定应用，与推荐系统、搜索或广告完全无关。论文主题涉及有机化学机理推理，属于明确排除的医学/生物/化学等特定领域应用范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08575v1": {
    "title": "ReSplat: Learning Recurrent Gaussian Splats",
    "url": "https://www.alphaxiv.org/abs/2510.08575v1",
    "arxiv_id": "2510.08575v1",
    "authors": "Haofei Xu, Daniel Barath, Andreas Geiger, Marc Pollefeys",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:59:59",
    "ori_summary": "While feed-forward Gaussian splatting models provide computational efficiency and effectively handle sparse input settings, their performance is fundamentally limited by the reliance on a single forward pass during inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting model that iteratively refines 3D Gaussians without explicitly computing gradients. Our key insight is that the Gaussian splatting rendering error serves as a rich feedback signal, guiding the recurrent network to learn effective Gaussian updates. This feedback signal naturally adapts to unseen data distributions at test time, enabling robust generalization. To initialize the recurrent process, we introduce a compact reconstruction model that operates in a $16 \\times$ subsampled space, producing $16 \\times$ fewer Gaussians than previous per-pixel Gaussian models. This substantially reduces computational overhead and allows for efficient Gaussian updates. Extensive experiments across varying of input views (2, 8, 16), resolutions ($256 \\times 256$ to $540 \\times 960$), and datasets (DL3DV and RealEstate10K) demonstrate that our method achieves state-of-the-art performance while significantly reducing the number of Gaussians and improving the rendering speed. Our project page is at https://haofeixu.github.io/resplat/.",
    "summary": "",
    "translation": "ReSplat：学习循环高斯泼溅",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及计算机视觉中的3D场景表示技术（高斯泼溅），属于纯粹的视觉研究方向。虽然循环学习机制可能具有时序建模能力，但论文标题没有显示与推荐系统、搜索或广告的直接关联，也没有明确表明其技术可以应用于这些领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08571v1": {
    "title": "Scalable Offline Metrics for Autonomous Driving",
    "url": "https://www.alphaxiv.org/abs/2510.08571v1",
    "arxiv_id": "2510.08571v1",
    "authors": "Animikh Aich, Adwait Kulkarni, Eshed Ohn-Bar",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-09 17:59:57",
    "ori_summary": "Real-World evaluation of perception-based planning models for robotic systems, such as autonomous vehicles, can be safely and inexpensively conducted offline, i.e., by computing model prediction error over a pre-collected validation dataset with ground-truth annotations. However, extrapolating from offline model performance to online settings remains a challenge. In these settings, seemingly minor errors can compound and result in test-time infractions or collisions. This relationship is understudied, particularly across diverse closed-loop metrics and complex urban maneuvers. In this work, we revisit this undervalued question in policy evaluation through an extensive set of experiments across diverse conditions and metrics. Based on analysis in simulation, we find an even worse correlation between offline and online settings than reported by prior studies, casting doubts on the validity of current evaluation practices and metrics for driving policies. Next, we bridge the gap between offline and online evaluation. We investigate an offline metric based on epistemic uncertainty, which aims to capture events that are likely to cause errors in closed-loop settings. The resulting metric achieves over 13% improvement in correlation compared to previous offline metrics. We further validate the generalization of our findings beyond the simulation environment in real-world settings, where even greater gains are observed.",
    "summary": "",
    "translation": "自动驾驶的可扩展离线评估指标",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域的离线评估指标，属于特定领域应用（自动驾驶），与推荐系统、搜索或广告的核心技术进展、LLM技术或Transformer架构改进均无直接关联。自动驾驶的评估指标无法直接应用于RecSys/Search/Ads领域，也不涉及任何相关的技术迁移潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08568v1": {
    "title": "NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos",
    "url": "https://www.alphaxiv.org/abs/2510.08568v1",
    "arxiv_id": "2510.08568v1",
    "authors": "Hongyu Li, Lingfeng Sun, Yafei Hu, Duy Ta, Jennifer Barry, George Konidaris, Jiahui Fu",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2025-10-09 17:59:55",
    "ori_summary": "Enabling robots to execute novel manipulation tasks zero-shot is a central goal in robotics. Most existing methods assume in-distribution tasks or rely on fine-tuning with embodiment-matched data, limiting transfer across platforms. We present NovaFlow, an autonomous manipulation framework that converts a task description into an actionable plan for a target robot without any demonstrations. Given a task description, NovaFlow synthesizes a video using a video generation model and distills it into 3D actionable object flow using off-the-shelf perception modules. From the object flow, it computes relative poses for rigid objects and realizes them as robot actions via grasp proposals and trajectory optimization. For deformable objects, this flow serves as a tracking objective for model-based planning with a particle-based dynamics model. By decoupling task understanding from low-level control, NovaFlow naturally transfers across embodiments. We validate on rigid, articulated, and deformable object manipulation tasks using a table-top Franka arm and a Spot quadrupedal mobile robot, and achieve effective zero-shot execution without demonstrations or embodiment-specific training. Project website: https://novaflow.lhy.xyz/.",
    "summary": "",
    "translation": "NovaFlow：通过生成视频中的可操作流实现零样本操控",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频生成和动作操控技术，属于计算机视觉和生成式AI领域。虽然标题提到'零样本'概念，但核心内容涉及视频生成和动作流操控，与推荐系统、搜索或广告的核心技术栈没有直接关联，也没有明确的Transformer架构改进或LLM应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08566v1": {
    "title": "D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2510.08566v1",
    "arxiv_id": "2510.08566v1",
    "authors": "Meixi Song, Xin Lin, Dizhe Zhang, Haodong Li, Xiangtai Li, Bo Du, Lu Qi",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:59:49",
    "ori_summary": "Recent advances in 3D Gaussian Splatting (3DGS) enable real-time, high-fidelity novel view synthesis (NVS) with explicit 3D representations. However, performance degradation and instability remain significant under sparse-view conditions. In this work, we identify two key failure modes under sparse-view conditions: overfitting in regions with excessive Gaussian density near the camera, and underfitting in distant areas with insufficient Gaussian coverage. To address these challenges, we propose a unified framework D$^2$GS, comprising two key components: a Depth-and-Density Guided Dropout strategy that suppresses overfitting by adaptively masking redundant Gaussians based on density and depth, and a Distance-Aware Fidelity Enhancement module that improves reconstruction quality in under-fitted far-field areas through targeted supervision. Moreover, we introduce a new evaluation metric to quantify the stability of learned Gaussian distributions, providing insights into the robustness of the sparse-view 3DGS. Extensive experiments on multiple datasets demonstrate that our method significantly improves both visual quality and robustness under sparse view conditions. The project page can be found at: https://insta360-research-team.github.io/DDGS-website/.",
    "summary": "",
    "translation": "D²GS：基于深度和密度引导的高斯泼溅实现稳定且准确的稀疏视图重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的3D重建技术，特别是高斯泼溅和稀疏视图重建。虽然标题提到深度和密度引导，但这属于纯粹的3D视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。该技术主要应用于场景重建和图形学，没有明显的推荐、搜索或广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08564v1": {
    "title": "How to Teach Large Multimodal Models New Skills",
    "url": "https://www.alphaxiv.org/abs/2510.08564v1",
    "arxiv_id": "2510.08564v1",
    "authors": "Zhen Zhu, Yiming Gong, Yao Xiao, Yaoyao Liu, Derek Hoiem",
    "categories": "cs.AI, cs.CV, cs.LG",
    "pub_date": "2025-10-09 17:59:37",
    "ori_summary": "How can we teach large multimodal models (LMMs) new skills without erasing prior abilities? We study sequential fine-tuning on five target skills while monitoring general ability on eight held-out benchmarks across three model families. We observe that apparent \"forgetting\" on held-out tasks after narrow fine-tuning can partly recover at later stages. We trace this behavior to a measurable shift in the output token distribution, manifested through a simple counting-bias probe that co-varies with forgetting. Guided by this picture, we identify two simple, robust tuning recipes that learn strongly while limiting drift: (i) updating only the self-attention projection layers, and (ii) updating only the MLP Gate&Up while freezing the Down projection. Across models and tasks, these choices deliver strong target gains while largely preserving held-out performance. Code is available at https://github.com/jessemelpolio/LMM_CL",
    "summary": "",
    "translation": "如何教授大型多模态模型新技能",
    "relevance_score": 8,
    "reasoning": "该论文涉及大型多模态模型的新技能学习，属于'Enabling LLM Tech'范畴，对推荐系统和搜索有直接应用价值。在推荐系统中，可以用于快速适应新的用户行为模式或商品特征；在搜索中，能够帮助模型快速学习新的查询意图或文档类型，提升系统适应性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08565v1": {
    "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints",
    "url": "https://www.alphaxiv.org/abs/2510.08565v1",
    "arxiv_id": "2510.08565v1",
    "authors": "Changyao Tian, Hao Li, Gen Luo, Xizhou Zhu, Weijie Su, Hanming Deng, Jinguo Zhu, Jie Shao, Ziran Zhu, Yunpeng Liu, Lewei Lu, Wenhai Wang, Hongsheng Li, Jifeng Dai",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:59:37",
    "ori_summary": "Compositional training has been the de-facto paradigm in existing Multimodal Large Language Models (MLLMs), where pre-trained vision encoders are connected with pre-trained LLMs through continuous multimodal pre-training. However, the multimodal scaling property of this paradigm remains difficult to explore due to the separated training. In this paper, we focus on the native training of MLLMs in an end-to-end manner and systematically study its design space and scaling property under a practical setting, i.e., data constraint. Through careful study of various choices in MLLM, we obtain the optimal meta-architecture that best balances performance and training cost. After that, we further explore the scaling properties of the native MLLM and indicate the positively correlated scaling relationship between visual encoders and LLMs. Based on these findings, we propose a native MLLM called NaViL, combined with a simple and cost-effective recipe. Experimental results on 14 multimodal benchmarks confirm the competitive performance of NaViL against existing MLLMs. Besides that, our findings and results provide in-depth insights for the future study of native MLLMs.",
    "summary": "",
    "translation": "NaViL：在数据约束下重新思考原生多模态大语言模型的缩放特性",
    "relevance_score": 8,
    "reasoning": "该论文研究多模态LLM在数据受限情况下的缩放特性，直接属于'Enabling LLM Tech'范畴。多模态LLM的缩放特性和数据效率研究对于RecSys/Ads至关重要，因为推荐和广告系统经常面临多模态数据（文本、图像、视频）且数据分布不均衡的问题，这些发现可以帮助构建更高效的多模态推荐模型。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08562v1": {
    "title": "ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving",
    "url": "https://www.alphaxiv.org/abs/2510.08562v1",
    "arxiv_id": "2510.08562v1",
    "authors": "Zhiyu Zheng, Shaoyu Chen, Haoran Yin, Xinbang Zhang, Jialv Zou, Xinggang Wang, Qian Zhang, Lefei Zhang",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-09 17:59:36",
    "ori_summary": "End-to-end autonomous driving (E2EAD) systems, which learn to predict future trajectories directly from sensor data, are fundamentally challenged by the inherent spatio-temporal imbalance of trajectory data. This imbalance creates a significant optimization burden, causing models to learn spurious correlations instead of causal inference, while also prioritizing uncertain, distant predictions, thereby compromising immediate safety. To address these issues, we propose ResAD, a novel Normalized Residual Trajectory Modeling framework. Instead of predicting the future trajectory directly, our approach reframes the learning task to predict the residual deviation from a deterministic inertial reference. The inertial reference serves as a counterfactual, forcing the model to move beyond simple pattern recognition and instead identify the underlying causal factors (e.g., traffic rules, obstacles) that necessitate deviations from a default, inertially-guided path. To deal with the optimization imbalance caused by uncertain, long-term horizons, ResAD further incorporates Point-wise Normalization of the predicted residual. It re-weights the optimization objective, preventing large-magnitude errors associated with distant, uncertain waypoints from dominating the learning signal. Extensive experiments validate the effectiveness of our framework. On the NAVSIM benchmark, ResAD achieves a state-of-the-art PDMS of 88.6 using a vanilla diffusion policy with only two denoising steps, demonstrating that our approach significantly simplifies the learning task and improves model performance. The code will be released to facilitate further research.",
    "summary": "",
    "translation": "ResAD：用于端到端自动驾驶的归一化残差轨迹建模",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域，与搜索、推荐或广告系统没有直接关联。自动驾驶属于计算机视觉和机器人技术领域，不在当前关注的RecSys/Search/Ads核心领域或相关使能技术范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08561v1": {
    "title": "MultiCOIN: Multi-Modal COntrollable Video INbetweening",
    "url": "https://www.alphaxiv.org/abs/2510.08561v1",
    "arxiv_id": "2510.08561v1",
    "authors": "Maham Tanveer, Yang Zhou, Simon Niklaus, Ali Mahdavi Amiri, Hao Zhang, Krishna Kumar Singh, Nanxuan Zhao",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:59:27",
    "ori_summary": "Video inbetweening creates smooth and natural transitions between two image frames, making it an indispensable tool for video editing and long-form video synthesis. Existing works in this domain are unable to generate large, complex, or intricate motions. In particular, they cannot accommodate the versatility of user intents and generally lack fine control over the details of intermediate frames, leading to misalignment with the creative mind. To fill these gaps, we introduce \\modelname{}, a video inbetweening framework that allows multi-modal controls, including depth transition and layering, motion trajectories, text prompts, and target regions for movement localization, while achieving a balance between flexibility, ease of use, and precision for fine-grained video interpolation. To achieve this, we adopt the Diffusion Transformer (DiT) architecture as our video generative model, due to its proven capability to generate high-quality long videos. To ensure compatibility between DiT and our multi-modal controls, we map all motion controls into a common sparse and user-friendly point-based representation as the video/noise input. Further, to respect the variety of controls which operate at varying levels of granularity and influence, we separate content controls and motion controls into two branches to encode the required features before guiding the denoising process, resulting in two generators, one for motion and the other for content. Finally, we propose a stage-wise training strategy to ensure that our model learns the multi-modal controls smoothly. Extensive qualitative and quantitative experiments demonstrate that multi-modal controls enable a more dynamic, customizable, and contextually accurate visual narrative.",
    "summary": "",
    "translation": "MultiCOIN：多模态可控视频中间帧生成",
    "relevance_score": 2,
    "reasoning": "该论文专注于视频生成领域的中间帧生成技术，属于计算机视觉范畴。虽然涉及多模态控制，但其核心应用场景是视频内容生成和编辑，与推荐系统、搜索或广告的排序和匹配任务没有直接关联。在推荐/搜索/广告领域缺乏明确的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08559v1": {
    "title": "SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models",
    "url": "https://www.alphaxiv.org/abs/2510.08559v1",
    "arxiv_id": "2510.08559v1",
    "authors": "Andong Deng, Taojiannan Yang, Shoubin Yu, Lincoln Spencer, Mohit Bansal, Chen Chen, Serena Yeung-Levy, Xiaohan Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-09 17:59:23",
    "ori_summary": "Large Multimodal Models (LMMs) have achieved remarkable progress across various capabilities; however, complex video reasoning in the scientific domain remains a significant and challenging frontier. Current video benchmarks predominantly target general scenarios where perception/recognition is heavily relied on, while with relatively simple reasoning tasks, leading to saturation and thus failing to effectively evaluate advanced multimodal cognitive skills. To address this critical gap, we introduce SciVideoBench, a rigorous benchmark specifically designed to assess advanced video reasoning in scientific contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice questions derived from cutting-edge scientific experimental videos spanning over 25 specialized academic subjects and verified by a semi-automatic system. Each question demands sophisticated domain-specific knowledge, precise spatiotemporal perception, and intricate logical reasoning, effectively challenging models' higher-order cognitive abilities. Our evaluation highlights significant performance deficits in state-of-the-art proprietary and open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating substantial room for advancement in video reasoning capabilities. Detailed analyses of critical factors such as reasoning complexity and visual grounding provide valuable insights and clear direction for future developments in LMMs, driving the evolution of truly capable multimodal AI co-scientists. We hope SciVideoBench could fit the interests of the community and help to push the boundary of cutting-edge AI for border science.",
    "summary": "",
    "translation": "SciVideoBench：大型多模态模型中的科学视频推理基准测试",
    "relevance_score": 2,
    "reasoning": "该论文专注于科学视频推理的基准测试，属于纯粹的评估基准范畴，这在无关主题中明确排除。虽然涉及多模态模型，但科学视频领域与推荐系统、搜索或广告没有直接关联，且基准测试本身是评估性质而非技术进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08556v1": {
    "title": "DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model",
    "url": "https://www.alphaxiv.org/abs/2510.08556v1",
    "arxiv_id": "2510.08556v1",
    "authors": "Xueyi Liu, He Wang, Li Yi",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-09 17:59:11",
    "ori_summary": "Achieving generalized in-hand object rotation remains a significant challenge in robotics, largely due to the difficulty of transferring policies from simulation to the real world. The complex, contact-rich dynamics of dexterous manipulation create a \"reality gap\" that has limited prior work to constrained scenarios involving simple geometries, limited object sizes and aspect ratios, constrained wrist poses, or customized hands. We address this sim-to-real challenge with a novel framework that enables a single policy, trained in simulation, to generalize to a wide variety of objects and conditions in the real world. The core of our method is a joint-wise dynamics model that learns to bridge the reality gap by effectively fitting limited amount of real-world collected data and then adapting the sim policy's actions accordingly. The model is highly data-efficient and generalizable across different whole-hand interaction distributions by factorizing dynamics across joints, compressing system-wide influences into low-dimensional variables, and learning each joint's evolution from its own dynamic profile, implicitly capturing these net effects. We pair this with a fully autonomous data collection strategy that gathers diverse, real-world interaction data with minimal human intervention. Our complete pipeline demonstrates unprecedented generality: a single policy successfully rotates challenging objects with complex shapes (e.g., animals), high aspect ratios (up to 5.33), and small sizes, all while handling diverse wrist orientations and rotation axes. Comprehensive real-world evaluations and a teleoperation application for complex tasks validate the effectiveness and robustness of our approach. Website: https://meowuu7.github.io/DexNDM/",
    "summary": "",
    "translation": "DexNDM：通过关节级神经动力学模型缩小灵巧手内旋转的现实差距",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人领域的灵巧手操作和物理模拟，属于机器人控制与动力学建模范畴。虽然涉及神经网络模型，但其应用场景（手内物体旋转、现实差距）与推荐系统、搜索或广告领域没有任何直接或潜在的关联，完全超出了关注范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08555v1": {
    "title": "VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning",
    "url": "https://www.alphaxiv.org/abs/2510.08555v1",
    "arxiv_id": "2510.08555v1",
    "authors": "Minghong Cai, Qiulin Wang, Zongli Ye, Wenze Liu, Quande Liu, Weicai Ye, Xintao Wang, Pengfei Wan, Kun Gai, Xiangyu Yue",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:58:59",
    "ori_summary": "We introduce the task of arbitrary spatio-temporal video completion, where a video is generated from arbitrary, user-specified patches placed at any spatial location and timestamp, akin to painting on a video canvas. This flexible formulation naturally unifies many existing controllable video generation tasks--including first-frame image-to-video, inpainting, extension, and interpolation--under a single, cohesive paradigm. Realizing this vision, however, faces a fundamental obstacle in modern latent video diffusion models: the temporal ambiguity introduced by causal VAEs, where multiple pixel frames are compressed into a single latent representation, making precise frame-level conditioning structurally difficult. We address this challenge with VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC) paradigm to this fine-grained control task with zero new parameters. We propose a hybrid conditioning strategy that decouples spatial and temporal control: spatial placement is handled via zero-padding, while temporal alignment is achieved through Temporal RoPE Interpolation, which assigns each condition a continuous fractional position within the latent sequence. This resolves the VAE's temporal ambiguity and enables pixel-frame-aware control on a frozen backbone. To evaluate this new capability, we develop VideoCanvasBench, the first benchmark for arbitrary spatio-temporal video completion, covering both intra-scene fidelity and inter-scene creativity. Experiments demonstrate that VideoCanvas significantly outperforms existing conditioning paradigms, establishing a new state of the art in flexible and unified video generation.",
    "summary": "",
    "translation": "VideoCanvas：通过上下文条件化从任意时空补丁实现统一视频补全",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频补全技术，属于计算机视觉领域的特定应用。虽然提到了上下文条件化，但其核心是视频内容生成和补全，与推荐系统、搜索或广告的排名和匹配任务关联性较弱。没有明确的机制或应用表明该技术能直接应用于异构数据处理或推荐系统场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08553v1": {
    "title": "Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation",
    "url": "https://www.alphaxiv.org/abs/2510.08553v1",
    "arxiv_id": "2510.08553v1",
    "authors": "Yunzhe Xu, Yiyuan Pan, Zhe Liu",
    "categories": "cs.CV, cs.AI, cs.RO",
    "pub_date": "2025-10-09 17:58:01",
    "ori_summary": "Vision-and-Language Navigation (VLN) requires agents to follow natural language instructions through environments, with memory-persistent variants demanding progressive improvement through accumulated experience. Existing approaches for memory-persistent VLN face critical limitations: they lack effective memory access mechanisms, instead relying on entire memory incorporation or fixed-horizon lookup, and predominantly store only environmental observations while neglecting navigation behavioral patterns that encode valuable decision-making strategies. We present Memoir, which employs imagination as a retrieval mechanism grounded by explicit memory: a world model imagines future navigation states as queries to selectively retrieve relevant environmental observations and behavioral histories. The approach comprises: 1) a language-conditioned world model that imagines future states serving dual purposes: encoding experiences for storage and generating retrieval queries; 2) Hybrid Viewpoint-Level Memory that anchors both observations and behavioral patterns to viewpoints, enabling hybrid retrieval; and 3) an experience-augmented navigation model that integrates retrieved knowledge through specialized encoders. Extensive evaluation across diverse memory-persistent VLN benchmarks with 10 distinctive testing scenarios demonstrates Memoir's effectiveness: significant improvements across all scenarios, with 5.4% SPL gains on IR2R over the best memory-persistent baseline, accompanied by 8.3x training speedup and 74% inference memory reduction. The results validate that predictive retrieval of both environmental and behavioral memories enables more effective navigation, with analysis indicating substantial headroom (73.3% vs 93.4% upper bound) for this imagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.",
    "summary": "",
    "translation": "梦想回忆：基于想象引导的经验检索用于记忆持久性视觉语言导航",
    "relevance_score": 1,
    "reasoning": "该论文专注于视觉语言导航(VLN)领域，这是一个纯粹的机器人导航任务，与推荐系统、搜索或广告无关。虽然提到了检索机制，但这是针对导航环境中的经验记忆，而非用户行为或内容检索。该工作属于纯粹的视觉语言多模态研究，没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08551v1": {
    "title": "ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation",
    "url": "https://www.alphaxiv.org/abs/2510.08551v1",
    "arxiv_id": "2510.08551v1",
    "authors": "Guanghao Li, Kerui Ren, Linning Xu, Zhewen Zheng, Changjian Jiang, Xin Gao, Bo Dai, Jian Pu, Mulin Yu, Jiangmiao Pang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:57:38",
    "ori_summary": "On-the-fly 3D reconstruction from monocular image sequences is a long-standing challenge in computer vision, critical for applications such as real-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff: per-scene optimization yields high fidelity but is computationally expensive, whereas feed-forward foundation models enable real-time inference but struggle with accuracy and robustness. In this work, we propose ARTDECO, a unified framework that combines the efficiency of feed-forward models with the reliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose estimation and point prediction, coupled with a Gaussian decoder that transforms multi-scale features into structured 3D Gaussians. To sustain both fidelity and efficiency at scale, we design a hierarchical Gaussian representation with a LoD-aware rendering strategy, which improves rendering fidelity while reducing redundancy. Experiments on eight diverse indoor and outdoor benchmarks show that ARTDECO delivers interactive performance comparable to SLAM, robustness similar to feed-forward systems, and reconstruction quality close to per-scene optimization, providing a practical path toward on-the-fly digitization of real-world environments with both accurate geometry and high visual fidelity. Explore more demos on our project page: https://city-super.github.io/artdeco/.",
    "summary": "",
    "translation": "ARTDECO：基于结构化场景表示的实时高效高保真3D重建方法",
    "relevance_score": 2,
    "reasoning": "该论文专注于3D重建技术，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术栈关联度极低。虽然结构化场景表示在概念上可能与多模态建模有微弱联系，但论文明确聚焦于3D重建应用，缺乏在推荐/搜索/广告领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08547v1": {
    "title": "R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation",
    "url": "https://www.alphaxiv.org/abs/2510.08547v1",
    "arxiv_id": "2510.08547v1",
    "authors": "Xiuwei Xu, Angyuan Ma, Hankun Li, Bingyao Yu, Zheng Zhu, Jie Zhou, Jiwen Lu",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-09 17:55:44",
    "ori_summary": "Towards the aim of generalized robotic manipulation, spatial generalization is the most fundamental capability that requires the policy to work robustly under different spatial distribution of objects, environment and agent itself. To achieve this, substantial human demonstrations need to be collected to cover different spatial configurations for training a generalized visuomotor policy via imitation learning. Prior works explore a promising direction that leverages data generation to acquire abundant spatially diverse data from minimal source demonstrations. However, most approaches face significant sim-to-real gap and are often limited to constrained settings, such as fixed-base scenarios and predefined camera viewpoints. In this paper, we propose a real-to-real 3D data generation framework (R2RGen) that directly augments the pointcloud observation-action pairs to generate real-world data. R2RGen is simulator- and rendering-free, thus being efficient and plug-and-play. Specifically, given a single source demonstration, we introduce an annotation mechanism for fine-grained parsing of scene and trajectory. A group-wise augmentation strategy is proposed to handle complex multi-object compositions and diverse task constraints. We further present camera-aware processing to align the distribution of generated data with real-world 3D sensor. Empirically, R2RGen substantially enhances data efficiency on extensive experiments and demonstrates strong potential for scaling and application on mobile manipulation.",
    "summary": "",
    "translation": "R2RGEN：面向空间泛化操作的实到实三维数据生成",
    "relevance_score": 2,
    "reasoning": "该论文专注于3D数据生成和空间操作，属于计算机视觉和图形学领域，与推荐系统、搜索或广告的核心技术关联度极低。虽然3D数据在某些特定场景（如AR/VR广告）可能有潜在应用，但论文标题未表明与异构数据建模、Transformer架构或LLM技术有任何直接联系，因此相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08540v1": {
    "title": "MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.08540v1",
    "arxiv_id": "2510.08540v1",
    "authors": "Xiangyu Zhao, Junming Lin, Tianhao Liang, Yifan Zhou, Wenhao Chai, Yuzhe Gu, Weiyun Wang, Kai Chen, Gen Luo, Wenwei Zhang, Junchi Yan, Hua Yang, Haodong Duan, Xue Yang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:53:58",
    "ori_summary": "While current Multimodal Large Language Models (MLLMs) have demonstrated proficiency in reasoning tasks such as mathematics and logic, their capacity for long-chain reflective reasoning, a prerequisite for solving complex real-world problems, remains largely underexplored. In this work, we first conduct an extensive empirical investigation to evaluate this capability. Leveraging a carefully designed data synthesis engine, we construct MM-HELIX, a multimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks that require iterative thinking and backtracking. Empirical results on this benchmark reveal that existing MLLMs exhibit significant performance deficits in long-chain reflective reasoning. To address this limitation, we generate post-training data and further explore learning paradigms for exploiting such data. We first develop the Step-Elicited Response Generation pipeline to create MM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning traces for instruction-tuning stage. Given that standard Reinforcement Learning fails on complex tasks due to sparse reward signals and catastrophic forgetting after Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization (AHPO), a novel training strategy that dynamically unifies offline supervision and online optimization into a single stage. This strategy enables the model to learn from expert data when rewards are sparse and conduct independent exploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our method achieves a +18.6\\% accuracy improvement on MM-HELIX benchmark and demonstrates strong generalization with a +5.7\\% average performance gain on general mathematic and logic tasks. Our work demonstrate that reflective reasoning in MLLMs can be effectively learned and generalized, paving the way for developing more capable MLLMs.",
    "summary": "",
    "translation": "MM-HELIX：通过整体平台与自适应混合策略优化增强多模态长链反思推理",
    "relevance_score": 8,
    "reasoning": "该论文涉及多模态长链推理技术，属于核心LLM进展，具有在搜索和推荐系统中应用的明确潜力。增强的反思推理能力可显著提升复杂用户查询理解、多轮对话推荐以及跨模态内容理解，直接服务于搜索和推荐系统的核心需求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08532v1": {
    "title": "Kontinuous Kontext: Continuous Strength Control for Instruction-based Image Editing",
    "url": "https://www.alphaxiv.org/abs/2510.08532v1",
    "arxiv_id": "2510.08532v1",
    "authors": "Rishubh Parihar, Or Patashnik, Daniil Ostashev, R. Venkatesh Babu, Daniel Cohen-Or, Kuan-Chieh Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-09 17:51:03",
    "ori_summary": "Instruction-based image editing offers a powerful and intuitive way to manipulate images through natural language. Yet, relying solely on text instructions limits fine-grained control over the extent of edits. We introduce Kontinuous Kontext, an instruction-driven editing model that provides a new dimension of control over edit strength, enabling users to adjust edits gradually from no change to a fully realized result in a smooth and continuous manner. Kontinuous Kontext extends a state-of-the-art image editing model to accept an additional input, a scalar edit strength which is then paired with the edit instruction, enabling explicit control over the extent of the edit. To inject this scalar information, we train a lightweight projector network that maps the input scalar and the edit instruction to coefficients in the model's modulation space. For training our model, we synthesize a diverse dataset of image-edit-instruction-strength quadruplets using existing generative models, followed by a filtering stage to ensure quality and consistency. Kontinuous Kontext provides a unified approach for fine-grained control over edit strength for instruction driven editing from subtle to strong across diverse operations such as stylization, attribute, material, background, and shape changes, without requiring attribute-specific training.",
    "summary": "",
    "translation": "连续上下文：基于指令的图像编辑的连续强度控制",
    "relevance_score": 2,
    "reasoning": "该论文专注于图像编辑的指令控制技术，属于计算机视觉和AIGC领域。虽然涉及指令控制机制，但其核心应用是图像内容生成和编辑，与推荐系统、搜索或广告的排序和建模任务没有直接关联。该技术可能间接启发多模态交互，但缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08530v1": {
    "title": "X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering",
    "url": "https://www.alphaxiv.org/abs/2510.08530v1",
    "arxiv_id": "2510.08530v1",
    "authors": "Zhitong Huang, Mohan Zhang, Renhan Wang, Rui Tang, Hao Zhu, Jing Liao",
    "categories": "cs.GR, cs.CV, 68U05, I.3.3; I.3.6",
    "pub_date": "2025-10-09 17:50:31",
    "ori_summary": "We present X2Video, the first diffusion model for rendering photorealistic videos guided by intrinsic channels including albedo, normal, roughness, metallicity, and irradiance, while supporting intuitive multi-modal controls with reference images and text prompts for both global and local regions. The intrinsic guidance allows accurate manipulation of color, material, geometry, and lighting, while reference images and text prompts provide intuitive adjustments in the absence of intrinsic information. To enable these functionalities, we extend the intrinsic-guided image generation model XRGB to video generation by employing a novel and efficient Hybrid Self-Attention, which ensures temporal consistency across video frames and also enhances fidelity to reference images. We further develop a Masked Cross-Attention to disentangle global and local text prompts, applying them effectively onto respective local and global regions. For generating long videos, our novel Recursive Sampling method incorporates progressive frame sampling, combining keyframe prediction and frame interpolation to maintain long-range temporal consistency while preventing error accumulation. To support the training of X2Video, we assembled a video dataset named InteriorVideo, featuring 1,154 rooms from 295 interior scenes, complete with reliable ground-truth intrinsic channel sequences and smooth camera trajectories. Both qualitative and quantitative evaluations demonstrate that X2Video can produce long, temporally consistent, and photorealistic videos guided by intrinsic conditions. Additionally, X2Video effectively accommodates multi-modal controls with reference images, global and local text prompts, and simultaneously supports editing on color, material, geometry, and lighting through parametric tuning. Project page: https://luckyhzt.github.io/x2video",
    "summary": "",
    "translation": "X2Video：适配扩散模型用于多模态可控神经视频渲染",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频生成和渲染技术，属于计算机视觉领域。虽然提到了多模态控制，但缺乏与推荐系统、搜索或广告的直接关联。扩散模型在内容生成方面的应用与排名、检索等核心业务场景关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08527v1": {
    "title": "FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control",
    "url": "https://www.alphaxiv.org/abs/2510.08527v1",
    "arxiv_id": "2510.08527v1",
    "authors": "Zhiyuan Zhang, Can Wang, Dongdong Chen, Jing Liao",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:50:22",
    "ori_summary": "We present FlexTraj, a framework for image-to-video generation with flexible point trajectory control. FlexTraj introduces a unified point-based motion representation that encodes each point with a segmentation ID, a temporally consistent trajectory ID, and an optional color channel for appearance cues, enabling both dense and sparse trajectory control. Instead of injecting trajectory conditions into the video generator through token concatenation or ControlNet, FlexTraj employs an efficient sequence-concatenation scheme that achieves faster convergence, stronger controllability, and more efficient inference, while maintaining robustness under unaligned conditions. To train such a unified point trajectory-controlled video generator, FlexTraj adopts an annealing training strategy that gradually reduces reliance on complete supervision and aligned condition. Experimental results demonstrate that FlexTraj enables multi-granularity, alignment-agnostic trajectory control for video generation, supporting various applications such as motion cloning, drag-based image-to-video, motion interpolation, camera redirection, flexible action control and mesh animations.",
    "summary": "",
    "translation": "FlexTraj：基于灵活点轨迹控制的图像到视频生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于图像到视频生成技术，属于纯粹的视觉内容生成领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然视频生成技术可能有潜在的广告创意应用，但这属于被明确排除的'非排序广告主题'和'纯视觉论文'范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08512v1": {
    "title": "Have We Scene It All? Scene Graph-Aware Deep Point Cloud Compression",
    "url": "https://www.alphaxiv.org/abs/2510.08512v1",
    "arxiv_id": "2510.08512v1",
    "authors": "Nikolaos Stathoulopoulos, Christoforos Kanellakis, George Nikolakopoulos",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-09 17:45:09",
    "ori_summary": "Efficient transmission of 3D point cloud data is critical for advanced perception in centralized and decentralized multi-agent robotic systems, especially nowadays with the growing reliance on edge and cloud-based processing. However, the large and complex nature of point clouds creates challenges under bandwidth constraints and intermittent connectivity, often degrading system performance. We propose a deep compression framework based on semantic scene graphs. The method decomposes point clouds into semantically coherent patches and encodes them into compact latent representations with semantic-aware encoders conditioned by Feature-wise Linear Modulation (FiLM). A folding-based decoder, guided by latent features and graph node attributes, enables structurally accurate reconstruction. Experiments on the SemanticKITTI and nuScenes datasets show that the framework achieves state-of-the-art compression rates, reducing data size by up to 98% while preserving both structural and semantic fidelity. In addition, it supports downstream applications such as multi-robot pose graph optimization and map merging, achieving trajectory accuracy and map alignment comparable to those obtained with raw LiDAR scans.",
    "summary": "",
    "translation": "我们是否已看遍所有场景？基于场景图感知的深度点云压缩",
    "relevance_score": 2,
    "reasoning": "该论文主要关注点云数据的压缩技术，属于计算机视觉和3D视觉领域。虽然提到了场景图概念，但这与推荐系统、搜索或广告中的异构数据处理没有直接关联。点云压缩技术在当前焦点领域缺乏明确的应用场景，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08508v1": {
    "title": "MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration",
    "url": "https://www.alphaxiv.org/abs/2510.08508v1",
    "arxiv_id": "2510.08508v1",
    "authors": "Lu Liu, Chunlei Cai, Shaocheng Shen, Jianfeng Liang, Weimin Ouyang, Tianxiao Ye, Jian Mao, Huiyu Duan, Jiangchao Yao, Xiaoyun Zhang, Qiang Hu, Guangtao Zhai",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:42:51",
    "ori_summary": "Real-world videos often suffer from complex degradations, such as noise, compression artifacts, and low-light distortions, due to diverse acquisition and transmission conditions. Existing restoration methods typically require professional manual selection of specialized models or rely on monolithic architectures that fail to generalize across varying degradations. Inspired by expert experience, we propose MoA-VR, the first \\underline{M}ixture-\\underline{o}f-\\underline{A}gents \\underline{V}ideo \\underline{R}estoration system that mimics the reasoning and processing procedures of human professionals through three coordinated agents: Degradation Identification, Routing and Restoration, and Restoration Quality Assessment. Specifically, we construct a large-scale and high-resolution video degradation recognition benchmark and build a vision-language model (VLM) driven degradation identifier. We further introduce a self-adaptive router powered by large language models (LLMs), which autonomously learns effective restoration strategies by observing tool usage patterns. To assess intermediate and final processed video quality, we construct the \\underline{Res}tored \\underline{V}ideo \\underline{Q}uality (Res-VQ) dataset and design a dedicated VLM-based video quality assessment (VQA) model tailored for restoration tasks. Extensive experiments demonstrate that MoA-VR effectively handles diverse and compound degradations, consistently outperforming existing baselines in terms of both objective metrics and perceptual quality. These results highlight the potential of integrating multimodal intelligence and modular reasoning in general-purpose video restoration systems.",
    "summary": "",
    "translation": "MoA-VR：面向一体化视频修复的智能体混合系统",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频修复的计算机视觉任务，属于纯粹的视觉处理领域。虽然Mixture-of-Agents架构在概念上相关，但该工作没有展示与推荐系统、搜索或广告的潜在应用连接，完全属于被排除的视觉技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08498v1": {
    "title": "AI-Driven Radiology Report Generation for Traumatic Brain Injuries",
    "url": "https://www.alphaxiv.org/abs/2510.08498v1",
    "arxiv_id": "2510.08498v1",
    "authors": "Riadh Bouslimi, Houda Trabelsi, Wahiba Ben Abdssalem Karaa, Hana Hedhli",
    "categories": "eess.IV, cs.AI, cs.CV, cs.LG, 68T07, 68U10, I.2.10; I.2.7; I.4.5",
    "pub_date": "2025-10-09 17:39:04",
    "ori_summary": "Traumatic brain injuries present significant diagnostic challenges in emergency medicine, where the timely interpretation of medical images is crucial for patient outcomes. In this paper, we propose a novel AI-based approach for automatic radiology report generation tailored to cranial trauma cases. Our model integrates an AC-BiFPN with a Transformer architecture to capture and process complex medical imaging data such as CT and MRI scans. The AC-BiFPN extracts multi-scale features, enabling the detection of intricate anomalies like intracranial hemorrhages, while the Transformer generates coherent, contextually relevant diagnostic reports by modeling long-range dependencies. We evaluate the performance of our model on the RSNA Intracranial Hemorrhage Detection dataset, where it outperforms traditional CNN-based models in both diagnostic accuracy and report generation. This solution not only supports radiologists in high-pressure environments but also provides a powerful educational tool for trainee physicians, offering real-time feedback and enhancing their learning experience. Our findings demonstrate the potential of combining advanced feature extraction with transformer-based text generation to improve clinical decision-making in the diagnosis of traumatic brain injuries.",
    "summary": "",
    "translation": "基于人工智能的创伤性脑损伤放射学报告生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的放射学报告生成，属于医疗AI应用范畴，与推荐系统、搜索或广告领域完全无关。论文内容涉及创伤性脑损伤的医学诊断报告生成，属于明确的医疗领域特定应用，完全超出您关注的技术领域范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08492v1": {
    "title": "Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models",
    "url": "https://www.alphaxiv.org/abs/2510.08492v1",
    "arxiv_id": "2510.08492v1",
    "authors": "Sharut Gupta, Shobhita Sundaram, Chenyu Wang, Stefanie Jegelka, Phillip Isola",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-09 17:32:23",
    "ori_summary": "Traditional multimodal learners find unified representations for tasks like visual question answering, but rely heavily on paired datasets. However, an overlooked yet potentially powerful question is: can one leverage auxiliary unpaired multimodal data to directly enhance representation learning in a target modality? We introduce UML: Unpaired Multimodal Learner, a modality-agnostic training paradigm in which a single model alternately processes inputs from different modalities while sharing parameters across them. This design exploits the assumption that different modalities are projections of a shared underlying reality, allowing the model to benefit from cross-modal structure without requiring explicit pairs. Theoretically, under linear data-generating assumptions, we show that unpaired auxiliary data can yield representations strictly more informative about the data-generating process than unimodal training. Empirically, we show that using unpaired data from auxiliary modalities -- such as text, audio, or images -- consistently improves downstream performance across diverse unimodal targets such as image and audio. Our project page: https://unpaired-multimodal.github.io/",
    "summary": "",
    "translation": "协同增效：利用未配对多模态数据构建更强大的单模态模型",
    "relevance_score": 8,
    "reasoning": "该论文涉及多模态数据利用和模型增强，直接关联'VLM Analogy for Heterogeneous Data'焦点，将不同数据模态视为独立输入进行统一建模。在推荐系统和搜索场景中，可应用于处理用户行为序列、上下文特征等异构数据，通过多模态协同提升单模态模型的性能表现。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08491v1": {
    "title": "Splat the Net: Radiance Fields with Splattable Neural Primitives",
    "url": "https://www.alphaxiv.org/abs/2510.08491v1",
    "arxiv_id": "2510.08491v1",
    "authors": "Xilong Zhou, Bao-Huy Nguyen, Loïc Magne, Vladislav Golyanik, Thomas Leimkühler, Christian Theobalt",
    "categories": "cs.GR, cs.CV",
    "pub_date": "2025-10-09 17:31:11",
    "ori_summary": "Radiance fields have emerged as a predominant representation for modeling 3D scene appearance. Neural formulations such as Neural Radiance Fields provide high expressivity but require costly ray marching for rendering, whereas primitive-based methods such as 3D Gaussian Splatting offer real-time efficiency through splatting, yet at the expense of representational power. Inspired by advances in both these directions, we introduce splattable neural primitives, a new volumetric representation that reconciles the expressivity of neural models with the efficiency of primitive-based splatting. Each primitive encodes a bounded neural density field parameterized by a shallow neural network. Our formulation admits an exact analytical solution for line integrals, enabling efficient computation of perspectively accurate splatting kernels. As a result, our representation supports integration along view rays without the need for costly ray marching. The primitives flexibly adapt to scene geometry and, being larger than prior analytic primitives, reduce the number required per scene. On novel-view synthesis benchmarks, our approach matches the quality and speed of 3D Gaussian Splatting while using $10\\times$ fewer primitives and $6\\times$ fewer parameters. These advantages arise directly from the representation itself, without reliance on complex control or adaptation frameworks. The project page is https://vcai.mpi-inf.mpg.de/projects/SplatNet/.",
    "summary": "",
    "translation": "Splat the Net：基于可溅射神经基元的辐射场",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及计算机视觉中的辐射场和神经基元技术，属于3D视觉和图形学领域。这些技术与推荐系统、搜索或广告的核心技术栈没有直接关联，也不具备在相关领域应用的明显潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08485v1": {
    "title": "InstructX: Towards Unified Visual Editing with MLLM Guidance",
    "url": "https://www.alphaxiv.org/abs/2510.08485v1",
    "arxiv_id": "2510.08485v1",
    "authors": "Chong Mou, Qichao Sun, Yanze Wu, Pengze Zhang, Xinghui Li, Fulong Ye, Songtao Zhao, Qian He",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:26:09",
    "ori_summary": "With recent advances in Multimodal Large Language Models (MLLMs) showing strong visual understanding and reasoning, interest is growing in using them to improve the editing performance of diffusion models. Despite rapid progress, most studies lack an in-depth analysis of MLLM design choices. Moreover, the integration of MLLMs and diffusion models remains an open challenge in some difficult tasks, such as video editing. In this paper, we present InstructX, a unified framework for image and video editing. Specifically, we conduct a comprehensive study on integrating MLLMs and diffusion models for instruction-driven editing across diverse tasks. Building on this study, we analyze the cooperation and distinction between images and videos in unified modeling. (1) We show that training on image data can lead to emergent video editing capabilities without explicit supervision, thereby alleviating the constraints imposed by scarce video training data. (2) By incorporating modality-specific MLLM features, our approach effectively unifies image and video editing tasks within a single model. Extensive experiments demonstrate that our method can handle a broad range of image and video editing tasks and achieves state-of-the-art performance.",
    "summary": "",
    "translation": "InstructX：基于多模态大语言模型引导的统一视觉编辑方法",
    "relevance_score": 2,
    "reasoning": "该论文专注于多模态大语言模型在视觉编辑领域的应用，属于纯粹的视觉内容生成范畴。虽然涉及多模态技术，但其核心应用是视觉编辑而非推荐系统、搜索或广告中的排序任务，与当前关注的技术方向关联度极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08480v1": {
    "title": "Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools",
    "url": "https://www.alphaxiv.org/abs/2510.08480v1",
    "arxiv_id": "2510.08480v1",
    "authors": "Zhenlong Yuan, Xiangyan Qu, Chengxuan Qian, Rui Chen, Jing Tang, Lei Sun, Xiangxiang Chu, Dapeng Zhang, Yiwei Wang, Yujun Cai, Shuo Li",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:20:44",
    "ori_summary": "Multimodal large language models (MLLMs) have demonstrated remarkable potential in bridging visual and textual reasoning, yet their reliance on text-centric priors often limits their ability to disentangle semantically similar actions in open-vocabulary scenarios. To address this, we propose Video-STAR, a framework that harmonizes contextual sub-motion decomposition with tool-augmented reinforcement learning for open-vocabulary action recognition (OVAR). Unlike prior methods that treat actions as monolithic entities, our approach innovatively decomposes actions into discriminative sub-motions for fine-grained matching while dynamically invoking domain-specific tools for cross-modal interleaving, thereby enabling category-specific reasoning capacity and reducing cross-modal hallucination. Moreover, by designing a hierarchical reward that balances tool-usage efficiency, sub-motion relevance, and structural coherence in reasoning, our method autonomously leverages external tools to prioritize sub-motion patterns without explicit supervision, transmitting from text-centric reasoning to visually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2, Kinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art performance, outperforming existing methods in distinguishing fine-grained actions and handling cross-modal hallucination, validating our excellent robustness and generalization.",
    "summary": "",
    "translation": "Video-STAR：通过工具增强开放词汇动作识别",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的开放词汇动作识别，虽然涉及多模态学习概念，但与推荐系统、搜索或广告的核心技术关联较弱。其工具增强方法可能对处理视频内容有一定启发，但在当前焦点领域的直接应用潜力有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08475v1": {
    "title": "DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos",
    "url": "https://www.alphaxiv.org/abs/2510.08475v1",
    "arxiv_id": "2510.08475v1",
    "authors": "Jhen Hsieh, Kuan-Hsun Tu, Kuo-Han Hung, Tsung-Wei Ke",
    "categories": "cs.RO, cs.CV, cs.LG",
    "pub_date": "2025-10-09 17:17:05",
    "ori_summary": "We present DexMan, an automated framework that converts human visual demonstrations into bimanual dexterous manipulation skills for humanoid robots in simulation. Operating directly on third-person videos of humans manipulating rigid objects, DexMan eliminates the need for camera calibration, depth sensors, scanned 3D object assets, or ground-truth hand and object motion annotations. Unlike prior approaches that consider only simplified floating hands, it directly controls a humanoid robot and leverages novel contact-based rewards to improve policy learning from noisy hand-object poses estimated from in-the-wild videos. DexMan achieves state-of-the-art performance in object pose estimation on the TACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD. Meanwhile, its reinforcement learning policy surpasses previous methods by 19% in success rate on OakInk-v2. Furthermore, DexMan can generate skills from both real and synthetic videos, without the need for manual data collection and costly motion capture, and enabling the creation of large-scale, diverse datasets for training generalist dexterous manipulation.",
    "summary": "",
    "translation": "DexMan：从人类和生成视频中学习双手灵巧操作",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人双手灵巧操作学习，属于纯粹的机器人控制领域。虽然涉及从视频中学习，但这是针对物理机器人动作模仿，与推荐系统、搜索或广告中的序列建模、多模态理解等核心技术没有直接关联。该技术缺乏在RecSys/Search/Ads领域的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08449v1": {
    "title": "Hierarchical Spatial Algorithms for High-Resolution Image Quantization and Feature Extraction",
    "url": "https://www.alphaxiv.org/abs/2510.08449v1",
    "arxiv_id": "2510.08449v1",
    "authors": "Noor Islam S. Mohammad",
    "categories": "cs.CV, 68T45, 68U10, I.4.8; I.2.10",
    "pub_date": "2025-10-09 16:56:24",
    "ori_summary": "This study introduces a modular framework for spatial image processing, integrating grayscale quantization, color and brightness enhancement, image sharpening, bidirectional transformation pipelines, and geometric feature extraction. A stepwise intensity transformation quantizes grayscale images into eight discrete levels, producing a posterization effect that simplifies representation while preserving structural detail. Color enhancement is achieved via histogram equalization in both RGB and YCrCb color spaces, with the latter improving contrast while maintaining chrominance fidelity. Brightness adjustment is implemented through HSV value-channel manipulation, and image sharpening is performed using a 3 * 3 convolution kernel to enhance high-frequency details. A bidirectional transformation pipeline that integrates unsharp masking, gamma correction, and noise amplification achieved accuracy levels of 76.10% and 74.80% for the forward and reverse processes, respectively. Geometric feature extraction employed Canny edge detection, Hough-based line estimation (e.g., 51.50{\\deg} for billiard cue alignment), Harris corner detection, and morphological window localization. Cue isolation further yielded 81.87\\% similarity against ground truth images. Experimental evaluation across diverse datasets demonstrates robust and deterministic performance, highlighting its potential for real-time image analysis and computer vision.",
    "summary": "",
    "translation": "用于高分辨率图像量化和特征提取的层次化空间算法",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉领域的图像量化和特征提取技术，虽然特征提取在广义上与推荐系统相关，但论文明确聚焦于高分辨率图像处理，这属于纯粹的视觉技术范畴。根据筛选标准，纯粹的视觉论文如果没有明确的推荐/搜索/广告应用相关性，应被视为低相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08442v1": {
    "title": "Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning",
    "url": "https://www.alphaxiv.org/abs/2510.08442v1",
    "arxiv_id": "2510.08442v1",
    "authors": "Andrew Lee, Ian Chuang, Dechen Gao, Kai Fukazawa, Iman Soltani",
    "categories": "cs.CV, cs.AI, cs.RO",
    "pub_date": "2025-10-09 16:54:11",
    "ori_summary": "Visual Reinforcement Learning (RL) agents must learn to act based on high-dimensional image data where only a small fraction of the pixels is task-relevant. This forces agents to waste exploration and computational resources on irrelevant features, leading to sample-inefficient and unstable learning. To address this, inspired by human visual foveation, we introduce Gaze on the Prize. This framework augments visual RL with a learnable foveal attention mechanism (Gaze), guided by a self-supervised signal derived from the agent's experience pursuing higher returns (the Prize). Our key insight is that return differences reveal what matters most: If two similar representations produce different outcomes, their distinguishing features are likely task-relevant, and the gaze should focus on them accordingly. This is realized through return-guided contrastive learning that trains the attention to distinguish between the features relevant to success and failure. We group similar visual representations into positives and negatives based on their return differences and use the resulting labels to construct contrastive triplets. These triplets provide the training signal that teaches the attention mechanism to produce distinguishable representations for states associated with different outcomes. Our method achieves up to 2.4x improvement in sample efficiency and can solve tasks that the baseline fails to learn, demonstrated across a suite of manipulation tasks from the ManiSkill3 benchmark, all without modifying the underlying algorithm or hyperparameters.",
    "summary": "",
    "translation": "凝视目标：利用回报引导的对比学习塑造视觉注意力",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉注意力机制和对比学习，虽然对比学习是LLM相关技术，但论文明确聚焦于视觉领域（'Visual Attention', 'Gaze'），缺乏与推荐系统、搜索或广告的明确关联。视觉注意力机制在推荐/搜索中主要用于处理图像内容，但论文标题未表明这种跨领域应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08431v1": {
    "title": "Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency",
    "url": "https://www.alphaxiv.org/abs/2510.08431v1",
    "arxiv_id": "2510.08431v1",
    "authors": "Kaiwen Zheng, Yuji Wang, Qianli Ma, Huayu Chen, Jintao Zhang, Yogesh Balaji, Jianfei Chen, Ming-Yu Liu, Jun Zhu, Qinsheng Zhang",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-09 16:45:30",
    "ori_summary": "This work represents the first effort to scale up continuous-time consistency distillation to general application-level image and video diffusion models. Although continuous-time consistency model (sCM) is theoretically principled and empirically powerful for accelerating academic-scale diffusion, its applicability to large-scale text-to-image and video tasks remains unclear due to infrastructure challenges in Jacobian-vector product (JVP) computation and the limitations of standard evaluation benchmarks. We first develop a parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on models with over 10 billion parameters and high-dimensional video tasks. Our investigation reveals fundamental quality limitations of sCM in fine-detail generation, which we attribute to error accumulation and the \"mode-covering\" nature of its forward-divergence objective. To remedy this, we propose the score-regularized continuous-time consistency model (rCM), which incorporates score distillation as a long-skip regularizer. This integration complements sCM with the \"mode-seeking\" reverse divergence, effectively improving visual quality while maintaining high generation diversity. Validated on large-scale models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM matches or surpasses the state-of-the-art distillation method DMD2 on quality metrics while offering notable advantages in diversity, all without GAN tuning or extensive hyperparameter searches. The distilled models generate high-fidelity samples in only $1\\sim4$ steps, accelerating diffusion sampling by $15\\times\\sim50\\times$. These results position rCM as a practical and theoretically grounded framework for advancing large-scale diffusion distillation.",
    "summary": "",
    "translation": "基于分数正则化连续时间一致性的大规模扩散蒸馏",
    "relevance_score": 3,
    "reasoning": "该论文主要关注扩散模型的蒸馏技术，属于生成模型效率优化领域。虽然扩散模型在内容生成方面有应用，但该技术本身是通用的模型压缩方法，与推荐系统、搜索或广告的核心排序和匹配问题关联较弱。在推荐/搜索场景中，潜在的间接应用可能包括生成式推荐中的模型加速，但这不是直接相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08425v1": {
    "title": "Reinforcing Diffusion Models by Direct Group Preference Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.08425v1",
    "arxiv_id": "2510.08425v1",
    "authors": "Yihong Luo, Tianyang Hu, Jing Tang",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-09 16:40:43",
    "ori_summary": "While reinforcement learning methods such as Group Relative Preference Optimization (GRPO) have significantly enhanced Large Language Models, adapting them to diffusion models remains challenging. In particular, GRPO demands a stochastic policy, yet the most cost-effective diffusion samplers are based on deterministic ODEs. Recent work addresses this issue by using inefficient SDE-based samplers to induce stochasticity, but this reliance on model-agnostic Gaussian noise leads to slow convergence. To resolve this conflict, we propose Direct Group Preference Optimization (DGPO), a new online RL algorithm that dispenses with the policy-gradient framework entirely. DGPO learns directly from group-level preferences, which utilize relative information of samples within groups. This design eliminates the need for inefficient stochastic policies, unlocking the use of efficient deterministic ODE samplers and faster training. Extensive results show that DGPO trains around 20 times faster than existing state-of-the-art methods and achieves superior performance on both in-domain and out-of-domain reward metrics. Code is available at https://github.com/Luo-Yihong/DGPO.",
    "summary": "",
    "translation": "通过直接群体偏好优化强化扩散模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注扩散模型的偏好优化，属于生成模型领域，与我的核心关注点（推荐系统、搜索、广告的直接应用或使能技术）相关性较低。虽然偏好优化技术可能间接影响内容生成质量，但论文没有明确展示在推荐、搜索或广告中的直接应用潜力，且更偏向AIGC和内容生成领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08407v1": {
    "title": "Biology-driven assessment of deep learning super-resolution imaging of the porosity network in dentin",
    "url": "https://www.alphaxiv.org/abs/2510.08407v1",
    "arxiv_id": "2510.08407v1",
    "authors": "Lauren Anderson, Lucas Chatelain, Nicolas Tremblay, Kathryn Grandfield, David Rousseau, Aurélien Gourrier",
    "categories": "cs.LG, cs.CV, q-bio.TO",
    "pub_date": "2025-10-09 16:26:38",
    "ori_summary": "The mechanosensory system of teeth is currently believed to partly rely on Odontoblast cells stimulation by fluid flow through a porosity network extending through dentin. Visualizing the smallest sub-microscopic porosity vessels therefore requires the highest achievable resolution from confocal fluorescence microscopy, the current gold standard. This considerably limits the extent of the field of view to very small sample regions. To overcome this limitation, we tested different deep learning (DL) super-resolution (SR) models to allow faster experimental acquisitions of lower resolution images and restore optimal image quality by post-processing. Three supervised 2D SR models (RCAN, pix2pix, FSRCNN) and one unsupervised (CycleGAN) were applied to a unique set of experimentally paired high- and low-resolution confocal images acquired with different sampling schemes, resulting in a pixel size increase of x2, x4, x8. Model performance was quantified using a broad set of similarity and distribution-based image quality assessment (IQA) metrics, which yielded inconsistent results that mostly contradicted our visual perception. This raises the question of the relevance of such generic metrics to efficiently target the specific structure of dental porosity. To resolve this conflicting information, the generated SR images were segmented taking into account the specific scales and morphology of the porosity network and analysed by comparing connected components. Additionally, the capacity of the SR models to preserve 3D porosity connectivity throughout the confocal image stacks was evaluated using graph analysis. This biology-driven assessment allowed a far better mechanistic interpretation of SR performance, highlighting differences in model sensitivity to weak intensity features and the impact of non-linearity in image generation, which explains the failure of standard IQA metrics.",
    "summary": "",
    "translation": "基于生物学驱动的深度学习牙本质孔隙网络超分辨率成像评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于牙本质孔隙网络的医学成像应用，属于明确的生物学/医学领域，与推荐系统、搜索或广告技术完全无关。深度学习在这里仅作为医学图像处理的工具，没有任何潜在的应用于RecSys/Search/Ads的可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08398v1": {
    "title": "VideoVerse: How Far is Your T2V Generator from a World Model?",
    "url": "https://www.alphaxiv.org/abs/2510.08398v1",
    "arxiv_id": "2510.08398v1",
    "authors": "Zeqing Wang, Xinyu Wei, Bairui Li, Zhen Guo, Jinrui Zhang, Hongyang Wei, Keze Wang, Lei Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 16:18:20",
    "ori_summary": "The recent rapid advancement of Text-to-Video (T2V) generation technologies, which are critical to build ``world models'', makes the existing benchmarks increasingly insufficient to evaluate state-of-the-art T2V models. First, current evaluation dimensions, such as per-frame aesthetic quality and temporal consistency, are no longer able to differentiate state-of-the-art T2V models. Second, event-level temporal causality, which not only distinguishes video from other modalities but also constitutes a crucial component of world models, is severely underexplored in existing benchmarks. Third, existing benchmarks lack a systematic assessment of world knowledge, which are essential capabilities for building world models. To address these issues, we introduce VideoVerse, a comprehensive benchmark that focuses on evaluating whether a T2V model could understand complex temporal causality and world knowledge in the real world. We collect representative videos across diverse domains (e.g., natural landscapes, sports, indoor scenes, science fiction, chemical and physical experiments) and extract their event-level descriptions with inherent temporal causality, which are then rewritten into text-to-video prompts by independent annotators. For each prompt, we design a suite of binary evaluation questions from the perspective of dynamic and static properties, with a total of ten carefully defined evaluation dimensions. In total, our VideoVerse comprises 300 carefully curated prompts, involving 815 events and 793 binary evaluation questions. Consequently, a human preference aligned QA-based evaluation pipeline is developed by using modern vision-language models. Finally, we perform a systematic evaluation of state-of-the-art open-source and closed-source T2V models on VideoVerse, providing in-depth analysis on how far the current T2V generators are from world models.",
    "summary": "",
    "translation": "VideoVerse：您的文本到视频生成器距离世界模型还有多远？",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文本到视频生成技术及其与世界模型的比较，这属于纯粹的视觉内容生成领域。虽然世界模型概念在强化学习中很重要，但论文标题明确聚焦于视频生成评估，与推荐系统、搜索或广告中的排名、用户建模等核心任务没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08394v1": {
    "title": "Spectral Prefiltering of Neural Fields",
    "url": "https://www.alphaxiv.org/abs/2510.08394v1",
    "arxiv_id": "2510.08394v1",
    "authors": "Mustafa B. Yaldiz, Ishit Mehta, Nithin Raghavan, Andreas Meuleman, Tzu-Mao Li, Ravi Ramamoorthi",
    "categories": "cs.GR, cs.CV",
    "pub_date": "2025-10-09 16:15:46",
    "ori_summary": "Neural fields excel at representing continuous visual signals but typically operate at a single, fixed resolution. We present a simple yet powerful method to optimize neural fields that can be prefiltered in a single forward pass. Key innovations and features include: (1) We perform convolutional filtering in the input domain by analytically scaling Fourier feature embeddings with the filter's frequency response. (2) This closed-form modulation generalizes beyond Gaussian filtering and supports other parametric filters (Box and Lanczos) that are unseen at training time. (3) We train the neural field using single-sample Monte Carlo estimates of the filtered signal. Our method is fast during both training and inference, and imposes no additional constraints on the network architecture. We show quantitative and qualitative improvements over existing methods for neural-field filtering.",
    "summary": "",
    "translation": "神经场的光谱预滤波",
    "relevance_score": 2,
    "reasoning": "该论文主要关注神经场（Neural Fields）的光谱预滤波技术，这属于计算机视觉和图形学领域的底层技术。虽然神经场在3D重建和表示方面有应用，但该技术本身与推荐系统、搜索或广告的核心进展缺乏直接关联，也没有明确的Transformer架构改进或LLM应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08393v1": {
    "title": "Robust Source-Free Domain Adaptation for Medical Image Segmentation based on Curriculum Learning",
    "url": "https://www.alphaxiv.org/abs/2510.08393v1",
    "arxiv_id": "2510.08393v1",
    "authors": "Ziqi Zhang, Yuexiang Li, Yawen Huang, Nanjun He, Tao Xu, Liwei Lin, Yefeng Zheng, Shaoxin Li, Feiyue Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 16:15:10",
    "ori_summary": "Recent studies have uncovered a new research line, namely source-free domain adaptation, which adapts a model to target domains without using the source data. Such a setting can address the concerns on data privacy and security issues of medical images. However, current source-free domain adaptation frameworks mainly focus on the pseudo label refinement for target data without the consideration of learning procedure. Indeed, a progressive learning process from source to target domain will benefit the knowledge transfer during model adaptation. To this end, we propose a curriculum-based framework, namely learning from curriculum (LFC), for source-free domain adaptation, which consists of easy-to-hard and source-to-target curricula. Concretely, the former curriculum enables the framework to start learning with `easy' samples and gradually tune the optimization direction of model adaption by increasing the sample difficulty. While, the latter can stablize the adaptation process, which ensures smooth transfer of the model from the source domain to the target. We evaluate the proposed source-free domain adaptation approach on the public cross-domain datasets for fundus segmentation and polyp segmentation. The extensive experimental results show that our framework surpasses the existing approaches and achieves a new state-of-the-art.",
    "summary": "",
    "translation": "基于课程学习的医学图像分割鲁棒无源域自适应",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分割的领域自适应问题，这属于医学/生物领域的特定应用，与推荐系统、搜索或广告的核心技术无关。论文内容涉及课程学习和无源域自适应，但这些技术在该文中被应用于医学图像处理，没有显示出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08377v1": {
    "title": "UniVideo: Unified Understanding, Generation, and Editing for Videos",
    "url": "https://www.alphaxiv.org/abs/2510.08377v1",
    "arxiv_id": "2510.08377v1",
    "authors": "Cong Wei, Quande Liu, Zixuan Ye, Qiulin Wang, Xintao Wang, Pengfei Wan, Kun Gai, Wenhu Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 16:01:30",
    "ori_summary": "Unified multimodal models have shown promising results in multimodal content generation and editing but remain largely limited to the image domain. In this work, we present UniVideo, a versatile framework that extends unified modeling to the video domain. UniVideo adopts a dual-stream design, combining a Multimodal Large Language Model (MLLM) for instruction understanding with a Multimodal DiT (MMDiT) for video generation. This design enables accurate interpretation of complex multimodal instructions while preserving visual consistency. Built on this architecture, UniVideo unifies diverse video generation and editing tasks under a single multimodal instruction paradigm and is jointly trained across them. Extensive experiments demonstrate that UniVideo matches or surpasses state-of-the-art task-specific baselines in text/image-to-video generation, in-context video generation and in-context video editing. Notably, the unified design of UniVideo enables two forms of generalization. First, UniVideo supports task composition, such as combining editing with style transfer, by integrating multiple capabilities within a single instruction. Second, even without explicit training on free-form video editing, UniVideo transfers its editing capability from large-scale image editing data to this setting, handling unseen instructions such as green-screening characters or changing materials within a video. Beyond these core capabilities, UniVideo also supports visual-prompt-based video generation, where the MLLM interprets visual prompts and guides the MMDiT during synthesis. To foster future research, we will release our model and code.",
    "summary": "",
    "translation": "UniVideo：面向视频的统一理解、生成与编辑",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频领域的统一建模，虽然涉及多模态技术，但其核心应用场景是视频内容的理解、生成和编辑，与推荐系统、搜索或广告的关联性较弱。视频生成和编辑技术可能间接应用于广告创意生成，但这属于明确排除的非相关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08363v1": {
    "title": "Hyperspectral data augmentation with transformer-based diffusion models",
    "url": "https://www.alphaxiv.org/abs/2510.08363v1",
    "arxiv_id": "2510.08363v1",
    "authors": "Mattia Ferrari, Lorenzo Bruzzone",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 15:50:29",
    "ori_summary": "The introduction of new generation hyperspectral satellite sensors, combined with advancements in deep learning methodologies, has significantly enhanced the ability to discriminate detailed land-cover classes at medium-large scales. However, a significant challenge in deep learning methods is the risk of overfitting when training networks with small labeled datasets. In this work, we propose a data augmentation technique that leverages a guided diffusion model. To effectively train the model with a limited number of labeled samples and to capture complex patterns in the data, we implement a lightweight transformer network. Additionally, we introduce a modified weighted loss function and an optimized cosine variance scheduler, which facilitate fast and effective training on small datasets. We evaluate the effectiveness of the proposed method on a forest classification task with 10 different forest types using hyperspectral images acquired by the PRISMA satellite. The results demonstrate that the proposed method outperforms other data augmentation techniques in both average and weighted average accuracy. The effectiveness of the method is further highlighted by the stable training behavior of the model, which addresses a common limitation in the practical application of deep generative models for data augmentation.",
    "summary": "",
    "translation": "基于Transformer的扩散模型进行高光谱数据增强",
    "relevance_score": 2,
    "reasoning": "该论文主要关注高光谱数据增强，这属于计算机视觉领域的特定应用，与推荐系统、搜索或广告的核心技术关联度极低。虽然使用了Transformer架构，但应用场景（高光谱数据）在推荐/搜索/广告领域几乎没有实际应用场景，因此整体相关性非常有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08358v1": {
    "title": "SPICE: Simple and Practical Image Clarification and Enhancement",
    "url": "https://www.alphaxiv.org/abs/2510.08358v1",
    "arxiv_id": "2510.08358v1",
    "authors": "Alexander Belyaev, Pierre-Alain Fayolle, Michael Cohen",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 15:43:07",
    "ori_summary": "We introduce a simple and efficient method to enhance and clarify images. More specifically, we deal with low light image enhancement and clarification of hazy imagery (hazy/foggy images, images containing sand dust, and underwater images). Our method involves constructing an image filter to simulate low-light or hazy conditions and deriving approximate reverse filters to minimize distortions in the enhanced images. Experimental results show that our approach is highly competitive and often surpasses state-of-the-art techniques in handling extremely dark images and in enhancing hazy images. A key advantage of our approach lies in its simplicity: Our method is implementable with just a few lines of MATLAB code.",
    "summary": "",
    "translation": "SPICE：简单实用的图像清晰化与增强",
    "relevance_score": 1,
    "reasoning": "该论文专注于图像处理技术，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术栈没有直接关联。虽然图像增强技术可能在某些特定场景下作为预处理步骤，但论文本身并未表明与异构数据建模、Transformer架构或LLM应用有任何联系，因此相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08352v1": {
    "title": "Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception",
    "url": "https://www.alphaxiv.org/abs/2510.08352v1",
    "arxiv_id": "2510.08352v1",
    "authors": "Nikos Theodoridis, Tim Brophy, Reenu Mohandas, Ganesh Sistu, Fiachra Collins, Anthony Scanlan, Ciaran Eising",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-09 15:38:41",
    "ori_summary": "Vision-Language Models (VLMs) are becoming increasingly powerful, demonstrating strong performance on a variety of tasks that require both visual and textual understanding. Their strong generalisation abilities make them a promising component for automated driving systems, which must handle unexpected corner cases. However, to be trusted in such safety-critical applications, a model must first possess a reliable perception system. Moreover, since critical objects and agents in traffic scenes are often at a distance, we require systems that are not \"shortsighted\", i.e., systems with strong perception capabilities at both close (up to 20 meters) and long (30+ meters) range. With this in mind, we introduce Distance-Annotated Traffic Perception Question Answering (DTPQA), the first Visual Question Answering (VQA) benchmark focused solely on perception-based questions in traffic scenes, enriched with distance annotations. By excluding questions that require reasoning, we ensure that model performance reflects perception capabilities alone. Since automated driving hardware has limited processing power and cannot support large VLMs, our study centers on smaller VLMs. More specifically, we evaluate several state-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the simplicity of the questions, these models significantly underperform compared to humans (~60% average accuracy for the best-performing small VLM versus ~85% human performance). However, it is important to note that the human sample size was relatively small, which imposes statistical limitations. We also identify specific perception tasks, such as distinguishing left from right, that remain particularly challenging for these models.",
    "summary": "",
    "translation": "评估小型视觉语言模型在距离相关交通感知任务上的性能",
    "relevance_score": 2,
    "reasoning": "虽然论文涉及视觉语言模型(VLM)，但其焦点是交通感知这一特定领域应用，与推荐系统、搜索或广告的核心技术缺乏直接关联。该研究主要针对计算机视觉任务，没有展示在异构数据处理或推荐系统应用方面的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08318v1": {
    "title": "LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation",
    "url": "https://www.alphaxiv.org/abs/2510.08318v1",
    "arxiv_id": "2510.08318v1",
    "authors": "Yushi Huang, Xingtong Ge, Ruihao Gong, Chengtao Lv, Jun Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 15:03:39",
    "ori_summary": "Video diffusion models (DMs) have enabled high-quality video synthesis. However, their computation costs scale quadratically with sequence length because self-attention has quadratic complexity. While linear attention lowers the cost, fully replacing quadratic attention requires expensive pretraining due to the limited expressiveness of linear attention and the complexity of spatiotemporal modeling in video generation. In this paper, we present LinVideo, an efficient data-free post-training framework that replaces a target number of self-attention modules with linear attention while preserving the original model's performance. First, we observe a significant disparity in the replaceability of different layers. Instead of manual or heuristic choices, we frame layer selection as a binary classification problem and propose selective transfer, which automatically and progressively converts layers to linear attention with minimal performance impact. Additionally, to overcome the ineffectiveness and inefficiency of existing objectives for this transfer process, we introduce an anytime distribution matching (ADM) objective that aligns the distributions of samples across any timestep along the sampling trajectory. This objective is efficient and recovers model performance. Extensive experiments show that our method achieves a 1.25-2.00x speedup while preserving generation quality, and our 4-step distilled model further delivers a 15.92x latency reduction with minimal visual quality drop.",
    "summary": "",
    "translation": "LinVideo：一种实现高效视频生成中O(n)注意力的训练后框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频生成领域的注意力效率优化，属于纯粹的视觉内容生成范畴。虽然涉及注意力机制效率改进，但其应用场景仅限于视频生成，与推荐系统、搜索或广告的排名和建模需求没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08316v1": {
    "title": "Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge",
    "url": "https://www.alphaxiv.org/abs/2510.08316v1",
    "arxiv_id": "2510.08316v1",
    "authors": "Yu Huang, Zelin Peng, Changsong Wen, Xiaokang Yang, Wei Shen",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 15:01:26",
    "ori_summary": "Affordance segmentation aims to parse 3D objects into functionally distinct parts, bridging recognition and interaction for applications in robotic manipulation, embodied AI, and AR. While recent studies leverage visual or textual prompts to guide this process, they often rely on point cloud encoders as generic feature extractors, overlooking the intrinsic challenges of 3D data such as sparsity, noise, and geometric ambiguity. As a result, 3D features learned in isolation frequently lack clear and semantically consistent functional boundaries. To address this bottleneck, we propose a semantic-grounded learning paradigm that transfers rich semantic knowledge from large-scale 2D Vision Foundation Models (VFMs) into the 3D domain. Specifically, We introduce Cross-Modal Affinity Transfer (CMAT), a pre-training strategy that aligns a 3D encoder with lifted 2D semantics and jointly optimizes reconstruction, affinity, and diversity to yield semantically organized representations. Building on this backbone, we further design the Cross-modal Affordance Segmentation Transformer (CAST), which integrates multi-modal prompts with CMAT-pretrained features to generate precise, prompt-aware segmentation maps. Extensive experiments on standard benchmarks demonstrate that our framework establishes new state-of-the-art results for 3D affordance segmentation.",
    "summary": "",
    "translation": "利用2D语义知识解锁3D功能可供性分割",
    "relevance_score": 2,
    "reasoning": "该论文专注于3D视觉中的功能可供性分割，属于纯粹的计算机视觉研究领域。虽然提到了语义知识，但主要应用于3D场景理解，与推荐系统、搜索或广告的核心技术栈没有明确的直接关联或潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08305v1": {
    "title": "LTCA: Long-range Temporal Context Attention for Referring Video Object Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.08305v1",
    "arxiv_id": "2510.08305v1",
    "authors": "Cilin Yan, Jingyun Wang, Guoliang Kang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 14:55:52",
    "ori_summary": "Referring Video Segmentation (RVOS) aims to segment objects in videos given linguistic expressions. The key to solving RVOS is to extract long-range temporal context information from the interactions of expressions and videos to depict the dynamic attributes of each object. Previous works either adopt attention across all the frames or stack dense local attention to achieve a global view of temporal context. However, they fail to strike a good balance between locality and globality, and the computation complexity significantly increases with the increase of video length. In this paper, we propose an effective long-range temporal context attention (LTCA) mechanism to aggregate global context information into object features. Specifically, we aggregate the global context information from two aspects. Firstly, we stack sparse local attentions to balance the locality and globality. We design a dilated window attention across frames to aggregate local context information and perform such attention in a stack of layers to enable a global view. Further, we enable each query to attend to a small group of keys randomly selected from a global pool to enhance the globality. Secondly, we design a global query to interact with all the other queries to directly encode the global context information. Experiments show our method achieves new state-of-the-art on four referring video segmentation benchmarks. Notably, our method shows an improvement of 11.3% and 8.3% on the MeViS valu and val datasets respectively.",
    "summary": "",
    "translation": "LTCA：用于参考视频对象分割的长程时序上下文注意力机制",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉领域的视频对象分割任务，虽然涉及注意力机制，但其核心应用场景（视频对象分割）与推荐系统、搜索或广告没有直接关联。长程时序建模技术理论上可能对处理用户行为序列有启发，但这种跨领域应用过于间接且不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08279v1": {
    "title": "Learning Neural Exposure Fields for View Synthesis",
    "url": "https://www.alphaxiv.org/abs/2510.08279v1",
    "arxiv_id": "2510.08279v1",
    "authors": "Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Christina Tsalicoglou, Keisuke Tateno, Jonathan T. Barron, Federico Tombari",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-09 14:32:41",
    "ori_summary": "Recent advances in neural scene representations have led to unprecedented quality in 3D reconstruction and view synthesis. Despite achieving high-quality results for common benchmarks with curated data, outputs often degrade for data that contain per image variations such as strong exposure changes, present, e.g., in most scenes with indoor and outdoor areas or rooms with windows. In this paper, we introduce Neural Exposure Fields (NExF), a novel technique for robustly reconstructing 3D scenes with high quality and 3D-consistent appearance from challenging real-world captures. In the core, we propose to learn a neural field predicting an optimal exposure value per 3D point, enabling us to optimize exposure along with the neural scene representation. While capture devices such as cameras select optimal exposure per image/pixel, we generalize this concept and perform optimization in 3D instead. This enables accurate view synthesis in high dynamic range scenarios, bypassing the need of post-processing steps or multi-exposure captures. Our contributions include a novel neural representation for exposure prediction, a system for joint optimization of the scene representation and the exposure field via a novel neural conditioning mechanism, and demonstrated superior performance on challenging real-world data. We find that our approach trains faster than prior works and produces state-of-the-art results on several benchmarks improving by over 55% over best-performing baselines.",
    "summary": "",
    "translation": "学习神经曝光场用于视图合成",
    "relevance_score": 2,
    "reasoning": "这篇论文主要关注计算机视觉中的视图合成和神经渲染技术，属于纯粹的视觉领域研究。虽然神经场技术在3D表示方面有创新，但没有明确展示在推荐系统、搜索或广告领域的潜在应用，与当前关注的核心领域距离较远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08278v1": {
    "title": "A Multimodal Depth-Aware Method For Embodied Reference Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.08278v1",
    "arxiv_id": "2510.08278v1",
    "authors": "Fevziye Irem Eyiokur, Dogucan Yaman, Hazım Kemal Ekenel, Alexander Waibel",
    "categories": "cs.CV, cs.HC, cs.RO",
    "pub_date": "2025-10-09 14:32:21",
    "ori_summary": "Embodied Reference Understanding requires identifying a target object in a visual scene based on both language instructions and pointing cues. While prior works have shown progress in open-vocabulary object detection, they often fail in ambiguous scenarios where multiple candidate objects exist in the scene. To address these challenges, we propose a novel ERU framework that jointly leverages LLM-based data augmentation, depth-map modality, and a depth-aware decision module. This design enables robust integration of linguistic and embodied cues, improving disambiguation in complex or cluttered environments. Experimental results on two datasets demonstrate that our approach significantly outperforms existing baselines, achieving more accurate and reliable referent detection.",
    "summary": "",
    "translation": "一种用于具身参考理解的多模态深度感知方法",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及具身AI和深度感知，属于计算机视觉和机器人领域。虽然提到了多模态，但核心焦点是具身参考理解（机器人导航和交互），与推荐系统、搜索或广告的直接相关性较弱。其技术可能间接应用于需要空间理解的场景，但应用潜力有限且不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08273v1": {
    "title": "One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion Models for Text-Guided Image Inpainting",
    "url": "https://www.alphaxiv.org/abs/2510.08273v1",
    "arxiv_id": "2510.08273v1",
    "authors": "Haipeng Liu, Yang Wang, Meng Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 14:30:34",
    "ori_summary": "Text-guided image inpainting aims at reconstructing the masked regions as per text prompts, where the longstanding challenges lie in the preservation for unmasked regions, while achieving the semantics consistency between unmasked and inpainted masked regions. Previous arts failed to address both of them, always with either of them to be remedied. Such facts, as we observed, stem from the entanglement of the hybrid (e.g., mid-and-low) frequency bands that encode varied image properties, which exhibit different robustness to text prompts during the denoising process. In this paper, we propose a null-text-null frequency-aware diffusion models, dubbed \\textbf{NTN-Diff}, for text-guided image inpainting, by decomposing the semantics consistency across masked and unmasked regions into the consistencies as per each frequency band, while preserving the unmasked regions, to circumvent two challenges in a row. Based on the diffusion process, we further divide the denoising process into early (high-level noise) and late (low-level noise) stages, where the mid-and-low frequency bands are disentangled during the denoising process. As observed, the stable mid-frequency band is progressively denoised to be semantically aligned during text-guided denoising process, which, meanwhile, serves as the guidance to the null-text denoising process to denoise low-frequency band for the masked regions, followed by a subsequent text-guided denoising process at late stage, to achieve the semantics consistency for mid-and-low frequency bands across masked and unmasked regions, while preserve the unmasked regions. Extensive experiments validate the superiority of NTN-Diff over the state-of-the-art diffusion models to text-guided diffusion models. Our code can be accessed from https://github.com/htyjers/NTN-Diff.",
    "summary": "",
    "translation": "一石二鸟：面向文本引导图像修复的空文本-空频率感知扩散模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本引导的图像修复和扩散模型，这属于纯粹的视觉内容生成领域。虽然扩散模型是LLM相关技术，但该工作专注于图像修复的具体应用，与推荐系统、搜索或广告中的排序、检索或用户建模没有直接关联。该技术没有明显的潜力应用于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08271v1": {
    "title": "SViM3D: Stable Video Material Diffusion for Single Image 3D Generation",
    "url": "https://www.alphaxiv.org/abs/2510.08271v1",
    "arxiv_id": "2510.08271v1",
    "authors": "Andreas Engelhardt, Mark Boss, Vikram Voletti, Chun-Han Yao, Hendrik P. A. Lensch, Varun Jampani",
    "categories": "cs.GR, cs.CV",
    "pub_date": "2025-10-09 14:29:47",
    "ori_summary": "We present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically based rendering (PBR) materials, given a single image. Recently, video diffusion models have been successfully used to reconstruct 3D objects from a single image efficiently. However, reflectance is still represented by simple material models or needs to be estimated in additional steps to enable relighting and controlled appearance edits. We extend a latent video diffusion model to output spatially varying PBR parameters and surface normals jointly with each generated view based on explicit camera control. This unique setup allows for relighting and generating a 3D asset using our model as neural prior. We introduce various mechanisms to this pipeline that improve quality in this ill-posed setting. We show state-of-the-art relighting and novel view synthesis performance on multiple object-centric datasets. Our method generalizes to diverse inputs, enabling the generation of relightable 3D assets useful in AR/VR, movies, games and other visual media.",
    "summary": "",
    "translation": "SViM3D：基于稳定视频材料扩散的单图像3D生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D视觉生成领域，涉及单图像到3D的转换和视频材料扩散技术，属于纯粹的计算机视觉研究方向。虽然标题提到扩散模型，但核心应用场景是3D内容生成，与推荐系统、搜索或广告的排名、匹配、用户建模等核心任务没有直接关联。该技术缺乏在RecSys/Search/Ads领域的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08269v1": {
    "title": "Adaptive Gradient Calibration for Single-Positive Multi-Label Learning in Remote Sensing Image Scene Classification",
    "url": "https://www.alphaxiv.org/abs/2510.08269v1",
    "arxiv_id": "2510.08269v1",
    "authors": "Chenying Liu, Gianmarco Perantoni, Lorenzo Bruzzone, Xiao Xiang Zhu",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 14:26:09",
    "ori_summary": "Multi-label classification (MLC) offers a more comprehensive semantic understanding of Remote Sensing (RS) imagery compared to traditional single-label classification (SLC). However, obtaining complete annotations for MLC is particularly challenging due to the complexity and high cost of the labeling process. As a practical alternative, single-positive multi-label learning (SPML) has emerged, where each image is annotated with only one relevant label, and the model is expected to recover the full set of labels. While scalable, SPML introduces significant supervision ambiguity, demanding specialized solutions for model training. Although various SPML methods have been proposed in the computer vision domain, research in the RS context remains limited. To bridge this gap, we propose Adaptive Gradient Calibration (AdaGC), a novel and generalizable SPML framework tailored to RS imagery. AdaGC adopts a gradient calibration (GC) mechanism combined with Mixup and a dual exponential moving average (EMA) module for robust pseudo-label generation. To maximize AdaGC's effectiveness, we introduce a simple yet theoretically grounded indicator to adaptively trigger GC after an initial warm-up stage based on training dynamics, thereby guaranteeing the effectiveness of GC in mitigating overfitting to label noise. Extensive experiments on two benchmark RS datasets under two distinct label noise types demonstrate that AdaGC achieves state-of-the-art (SOTA) performance while maintaining strong robustness across diverse settings.",
    "summary": "",
    "translation": "遥感图像场景分类中单正例多标签学习的自适应梯度校准",
    "relevance_score": 1,
    "reasoning": "该论文专注于遥感图像场景分类，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告没有明显关联。论文讨论的单正例多标签学习和梯度校准技术是特定于图像分类任务的，无法看出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08260v1": {
    "title": "Fine-grained text-driven dual-human motion generation via dynamic hierarchical interaction",
    "url": "https://www.alphaxiv.org/abs/2510.08260v1",
    "arxiv_id": "2510.08260v1",
    "authors": "Mu Li, Yin Wang, Zhiying Leng, Jiapeng Liu, Frederick W. B. Li, Xiaohui Liang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 14:18:53",
    "ori_summary": "Human interaction is inherently dynamic and hierarchical, where the dynamic refers to the motion changes with distance, and the hierarchy is from individual to inter-individual and ultimately to overall motion. Exploiting these properties is vital for dual-human motion generation, while existing methods almost model human interaction temporally invariantly, ignoring distance and hierarchy. To address it, we propose a fine-grained dual-human motion generation method, namely FineDual, a tri-stage method to model the dynamic hierarchical interaction from individual to inter-individual. The first stage, Self-Learning Stage, divides the dual-human overall text into individual texts through a Large Language Model, aligning text features and motion features at the individual level. The second stage, Adaptive Adjustment Stage, predicts interaction distance by an interaction distance predictor, modeling human interactions dynamically at the inter-individual level by an interaction-aware graph network. The last stage, Teacher-Guided Refinement Stage, utilizes overall text features as guidance to refine motion features at the overall level, generating fine-grained and high-quality dual-human motion. Extensive quantitative and qualitative evaluations on dual-human motion datasets demonstrate that our proposed FineDual outperforms existing approaches, effectively modeling dynamic hierarchical human interaction.",
    "summary": "",
    "translation": "基于细粒度文本驱动的双人动作生成：通过动态分层交互实现",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本驱动的双人动作生成，属于计算机图形学或动画领域，与推荐系统、搜索或广告的核心技术无关。虽然涉及文本到动作的生成，但这本质上是内容生成任务，属于被明确排除的AIGC和纯视觉应用范畴，没有明显的推荐、搜索或广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08181v1": {
    "title": "InstructUDrag: Joint Text Instructions and Object Dragging for Interactive Image Editing",
    "url": "https://www.alphaxiv.org/abs/2510.08181v1",
    "arxiv_id": "2510.08181v1",
    "authors": "Haoran Yu, Yi Shi",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 13:06:49",
    "ori_summary": "Text-to-image diffusion models have shown great potential for image editing, with techniques such as text-based and object-dragging methods emerging as key approaches. However, each of these methods has inherent limitations: text-based methods struggle with precise object positioning, while object dragging methods are confined to static relocation. To address these issues, we propose InstructUDrag, a diffusion-based framework that combines text instructions with object dragging, enabling simultaneous object dragging and text-based image editing. Our framework treats object dragging as an image reconstruction process, divided into two synergistic branches. The moving-reconstruction branch utilizes energy-based gradient guidance to move objects accurately, refining cross-attention maps to enhance relocation precision. The text-driven editing branch shares gradient signals with the reconstruction branch, ensuring consistent transformations and allowing fine-grained control over object attributes. We also employ DDPM inversion and inject prior information into noise maps to preserve the structure of moved objects. Extensive experiments demonstrate that InstructUDrag facilitates flexible, high-fidelity image editing, offering both precision in object relocation and semantic control over image content.",
    "summary": "",
    "translation": "InstructUDrag：基于联合文本指令和对象拖拽的交互式图像编辑",
    "relevance_score": 1,
    "reasoning": "该论文专注于交互式图像编辑技术，涉及文本指令和对象拖拽操作，属于纯粹的计算机视觉和图像生成领域。虽然标题提到'文本指令'，但核心是图像编辑而非语言模型在推荐/搜索/广告中的应用，与当前关注的推荐系统、搜索广告、Transformer架构进展或异构数据统一建模等焦点完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08179v1": {
    "title": "Dual-granularity Sinkhorn Distillation for Enhanced Learning from Long-tailed Noisy Data",
    "url": "https://www.alphaxiv.org/abs/2510.08179v1",
    "arxiv_id": "2510.08179v1",
    "authors": "Feng Hong, Yu Huang, Zihua Zhao, Zhihan Zhou, Jiangchao Yao, Dongsheng Li, Ya Zhang, Yanfeng Wang",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-09 13:05:27",
    "ori_summary": "Real-world datasets for deep learning frequently suffer from the co-occurring challenges of class imbalance and label noise, hindering model performance. While methods exist for each issue, effectively combining them is non-trivial, as distinguishing genuine tail samples from noisy data proves difficult, often leading to conflicting optimization strategies. This paper presents a novel perspective: instead of primarily developing new complex techniques from scratch, we explore synergistically leveraging well-established, individually 'weak' auxiliary models - specialized for tackling either class imbalance or label noise but not both. This view is motivated by the insight that class imbalance (a distributional-level concern) and label noise (a sample-level concern) operate at different granularities, suggesting that robustness mechanisms for each can in principle offer complementary strengths without conflict. We propose Dual-granularity Sinkhorn Distillation (D-SINK), a novel framework that enhances dual robustness by distilling and integrating complementary insights from such 'weak', single-purpose auxiliary models. Specifically, D-SINK uses an optimal transport-optimized surrogate label allocation to align the target model's sample-level predictions with a noise-robust auxiliary and its class distributions with an imbalance-robust one. Extensive experiments on benchmark datasets demonstrate that D-SINK significantly improves robustness and achieves strong empirical performance in learning from long-tailed noisy data.",
    "summary": "",
    "translation": "双粒度Sinkhorn蒸馏用于增强长尾噪声数据学习",
    "relevance_score": 4,
    "reasoning": "该论文提出了一种处理长尾噪声数据的方法，这在推荐系统中处理真实世界用户行为数据时具有潜在应用价值。然而，该方法主要关注通用数据分布问题，没有明确针对推荐、搜索或广告领域的特定挑战，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08178v1": {
    "title": "Robust Canonicalization through Bootstrapped Data Re-Alignment",
    "url": "https://www.alphaxiv.org/abs/2510.08178v1",
    "arxiv_id": "2510.08178v1",
    "authors": "Johann Schmidt, Sebastian Stober",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-09 13:05:20",
    "ori_summary": "Fine-grained visual classification (FGVC) tasks, such as insect and bird identification, demand sensitivity to subtle visual cues while remaining robust to spatial transformations. A key challenge is handling geometric biases and noise, such as different orientations and scales of objects. Existing remedies rely on heavy data augmentation, which demands powerful models, or on equivariant architectures, which constrain expressivity and add cost. Canonicalization offers an alternative by shielding such biases from the downstream model. In practice, such functions are often obtained using canonicalization priors, which assume aligned training data. Unfortunately, real-world datasets never fulfill this assumption, causing the obtained canonicalizer to be brittle. We propose a bootstrapping algorithm that iteratively re-aligns training samples by progressively reducing variance and recovering the alignment assumption. We establish convergence guarantees under mild conditions for arbitrary compact groups, and show on four FGVC benchmarks that our method consistently outperforms equivariant, and canonicalization baselines while performing on par with augmentation.",
    "summary": "",
    "translation": "通过自举数据重对齐实现鲁棒规范化",
    "relevance_score": 3,
    "reasoning": "该论文标题暗示了数据对齐和规范化技术，这在推荐系统和搜索中可能用于处理异构数据源或特征工程。然而，标题过于宽泛，没有明确说明与Transformer架构、LLM技术或推荐/搜索/广告系统的直接关联，潜在应用不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08157v1": {
    "title": "Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence Reasoning for Image Editing",
    "url": "https://www.alphaxiv.org/abs/2510.08157v1",
    "arxiv_id": "2510.08157v1",
    "authors": "Zhentao Zou, Zhengrong Yue, Kunpeng Du, Binlei Bao, Hanting Li, Haizhen Xie, Guozheng Xu, Yue Zhou, Yali Wang, Jie Hu, Xue Jiang, Xinghao Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 12:36:51",
    "ori_summary": "Image editing with natural language has gained significant popularity, yet existing methods struggle with intricate object intersections and fine-grained spatial relationships due to the lack of an explicit reasoning process. While Chain-of-Thought (CoT) has been explored to enhance reasoning, purely textual CoT or CoT augmented with coordinate information is fundamentally limited in its ability to represent intricate visual layouts and lacks the necessary visual cues to guide the generation of fine-grained, pixel-level details. To address these challenges, we propose Multimodal Reasoning Edit (MURE), a novel framework that shifts the visual editing process from purely text-based reasoning to a series of interleaved textual and visual rationales. Our framework performs image editing using a natively multimodal, interleaved text-image CoT. This approach generates a step-by-step chain of reasoning where a textual description is followed by a corresponding visual cue, such as a positional mask that defined intended edited regions or a representation of new content. Furthermore, to mitigate the hallucination phenomenon of large language models, we introduce Multimodal Deep Confidence (MMDC) reasoning paradigm. This paradigm explores a tree of visual reasoning paths at each step. By pruning low-quality branches using a deep confidence score from a reward model, it ensures the model consistently follows a high-quality trajectory towards the final edited result. The proposed method decomposes complex editing tasks into interdependent sub-tasks, achieving greater precision at each stage and yielding high-fidelity edited results. We define the formulation for interleaved text-image chains and release the first CoT-Edit-14K dataset, comprising 14K high-quality editing examples. Extensive experiments show that our method yields significant improvements across three image editing benchmarks.",
    "summary": "",
    "translation": "超越文本思维链：基于深度置信推理的交错文本-图像链用于图像编辑",
    "relevance_score": 1,
    "reasoning": "该论文专注于图像编辑任务，涉及视觉模态和内容生成，属于纯粹的视觉应用领域。虽然提到了置信推理，但核心关注点是图像编辑而非推荐系统、搜索或广告中的排序或理解任务。该工作没有明显的潜在应用可以转移到RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08143v1": {
    "title": "UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution",
    "url": "https://www.alphaxiv.org/abs/2510.08143v1",
    "arxiv_id": "2510.08143v1",
    "authors": "Shian Du, Menghan Xia, Chang Liu, Quande Liu, Xintao Wang, Pengfei Wan, Xiangyang Ji",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 12:25:16",
    "ori_summary": "Cascaded video super-resolution has emerged as a promising technique for decoupling the computational burden associated with generating high-resolution videos using large foundation models. Existing studies, however, are largely confined to text-to-video tasks and fail to leverage additional generative conditions beyond text, which are crucial for ensuring fidelity in multi-modal video generation. We address this limitation by presenting UniMMVSR, the first unified generative video super-resolution framework to incorporate hybrid-modal conditions, including text, images, and videos. We conduct a comprehensive exploration of condition injection strategies, training schemes, and data mixture techniques within a latent video diffusion model. A key challenge was designing distinct data construction and condition utilization methods to enable the model to precisely utilize all condition types, given their varied correlations with the target video. Our experiments demonstrate that UniMMVSR significantly outperforms existing methods, producing videos with superior detail and a higher degree of conformity to multi-modal conditions. We also validate the feasibility of combining UniMMVSR with a base model to achieve multi-modal guided generation of 4K video, a feat previously unattainable with existing techniques.",
    "summary": "",
    "translation": "UniMMVSR：一种用于级联视频超分辨率的统一多模态框架",
    "relevance_score": 2,
    "reasoning": "该论文专注于视频超分辨率这一计算机视觉任务，虽然涉及多模态框架，但其核心应用场景是视频质量增强而非推荐、搜索或广告系统。视频超分辨率技术对RecSys/Search/Ads的潜在应用非常有限，可能仅在某些需要高质量视频预览的电商场景中有微弱关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08138v1": {
    "title": "Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement",
    "url": "https://www.alphaxiv.org/abs/2510.08138v1",
    "arxiv_id": "2510.08138v1",
    "authors": "Chengzhi Li, Heyan Huang, Ping Jian, Zhen Yang, Yaning Tian",
    "categories": "cs.CV, cs.AI, cs.MM",
    "pub_date": "2025-10-09 12:22:06",
    "ori_summary": "Large language models (LLMs) often generate self-contradictory outputs, which severely impacts their reliability and hinders their adoption in practical applications. In video-language models (Video-LLMs), this phenomenon recently draws the attention of researchers. Specifically, these models fail to provide logically consistent responses to rephrased questions based on their grounding outputs. However, the underlying causes of this phenomenon remain underexplored. In this work, we adopt an interpretability-driven approach to analyze, statistically summarize, and intervention the potential factors of the phenomenon. We find that one of the primary reasons for the inconsistency in responses lies in the inability of cross-modal attention heads to effectively distinguish video tokens across different timestamps. To address this, we propose an attention enhancement method called Temporally Conditioned Attention Sharpening (TCAS), which constructs an enhancement objective based on attention distinctions to enhance the model's temporal resolution capability, thereby improving its temporal understanding logic consistency. Experimental results demonstrate that our method significantly enhances the temporal logic consistency of Video-LLMs. Further interpretability analyses reveal that our method indeed improves the temporal discriminability of attention heads, validating our conclusions. Additionally, our method achieves performance improvements in general video temporal grounding tasks, highlighting that temporal logic consistency is a bottleneck in temporal understanding. By enhancing consistency, our method drives significant progress in video temporal understanding.",
    "summary": "",
    "translation": "通过注意力增强改进视频语言模型中的时序理解逻辑一致性",
    "relevance_score": 2,
    "reasoning": "虽然这篇论文涉及多模态建模和注意力机制，但其核心焦点是视频-语言模型中的时序理解，这与推荐系统、搜索或广告的直接应用相关性较弱。注意力增强技术可能有潜在的效率改进，但论文主要针对视频时序逻辑而非推荐/搜索场景中的异构数据处理。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08131v1": {
    "title": "Real-Time Motion-Controllable Autoregressive Video Diffusion",
    "url": "https://www.alphaxiv.org/abs/2510.08131v1",
    "arxiv_id": "2510.08131v1",
    "authors": "Kesen Zhao, Jiaxin Shi, Beier Zhu, Junbao Zhou, Xiaolong Shen, Yuan Zhou, Qianru Sun, Hanwang Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 12:17:11",
    "ori_summary": "Real-time motion-controllable video generation remains challenging due to the inherent latency of bidirectional diffusion models and the lack of effective autoregressive (AR) approaches. Existing AR video diffusion models are limited to simple control signals or text-to-video generation, and often suffer from quality degradation and motion artifacts in few-step generation. To address these challenges, we propose AR-Drag, the first RL-enhanced few-step AR video diffusion model for real-time image-to-video generation with diverse motion control. We first fine-tune a base I2V model to support basic motion control, then further improve it via reinforcement learning with a trajectory-based reward model. Our design preserves the Markov property through a Self-Rollout mechanism and accelerates training by selectively introducing stochasticity in denoising steps. Extensive experiments demonstrate that AR-Drag achieves high visual fidelity and precise motion alignment, significantly reducing latency compared with state-of-the-art motion-controllable VDMs, while using only 1.3B parameters. Additional visualizations can be found on our project page: https://kesenzhao.github.io/AR-Drag.github.io/.",
    "summary": "",
    "translation": "实时运动可控自回归视频扩散模型",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于视频生成和运动控制，属于纯粹的视觉内容生成领域。虽然提到了自回归建模，但核心关注点是视频生成技术，与推荐系统、搜索或广告的排名和建模需求没有直接关联。该技术缺乏在RecSys/Search/Ads领域的明确应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08116v1": {
    "title": "Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.08116v1",
    "arxiv_id": "2510.08116v1",
    "authors": "Eirik A. Østmo, Kristoffer K. Wickstrøm, Keyur Radiya, Michael C. Kampffmeyer, Karl Øyvind Mikalsen, Robert Jenssen",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-09 11:57:04",
    "ori_summary": "Contrast-enhanced Computed Tomography (CT) is important for diagnosis and treatment planning for various medical conditions. Deep learning (DL) based segmentation models may enable automated medical image analysis for detecting and delineating tumors in CT images, thereby reducing clinicians' workload. Achieving generalization capabilities in limited data domains, such as radiology, requires modern DL models to be trained with image augmentation. However, naively applying augmentation methods developed for natural images to CT scans often disregards the nature of the CT modality, where the intensities measure Hounsfield Units (HU) and have important physical meaning. This paper challenges the use of such intensity augmentations for CT imaging and shows that they may lead to artifacts and poor generalization. To mitigate this, we propose a CT-specific augmentation technique, called Random windowing, that exploits the available HU distribution of intensities in CT images. Random windowing encourages robustness to contrast-enhancement and significantly increases model performance on challenging images with poor contrast or timing. We perform ablations and analysis of our method on multiple datasets, and compare to, and outperform, state-of-the-art alternatives, while focusing on the challenge of liver tumor segmentation.",
    "summary": "",
    "translation": "用于CT和肝脏肿瘤分割中深度学习鲁棒性的随机窗口增强",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像分割领域，特别是CT扫描和肝脏肿瘤分割，这属于医学/生物学应用范畴。虽然提到了数据增强技术，但该技术完全应用于医疗领域，与推荐系统、搜索或广告没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08096v1": {
    "title": "Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting",
    "url": "https://www.alphaxiv.org/abs/2510.08096v1",
    "arxiv_id": "2510.08096v1",
    "authors": "Ankit Gahlawat, Anirban Mukherjee, Dinesh Babu Jayagopi",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 11:34:55",
    "ori_summary": "Accurate face parsing under extreme viewing angles remains a significant challenge due to limited labeled data in such poses. Manual annotation is costly and often impractical at scale. We propose a novel label refinement pipeline that leverages 3D Gaussian Splatting (3DGS) to generate accurate segmentation masks from noisy multiview predictions. By jointly fitting two 3DGS models, one to RGB images and one to their initial segmentation maps, our method enforces multiview consistency through shared geometry, enabling the synthesis of pose-diverse training data with only minimal post-processing. Fine-tuning a face parsing model on this refined dataset significantly improves accuracy on challenging head poses, while maintaining strong performance on standard views. Extensive experiments, including human evaluations, demonstrate that our approach achieves superior results compared to state-of-the-art methods, despite requiring no ground-truth 3D annotations and using only a small set of initial images. Our method offers a scalable and effective solution for improving face parsing robustness in real-world settings.",
    "summary": "",
    "translation": "基于3D高斯溅射的极端姿态下人脸解析高效标签精炼",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的人脸解析和3D高斯溅射技术，属于纯粹的视觉处理范畴。论文内容涉及人脸姿态处理和标签精炼，与推荐系统、搜索或广告的核心技术栈没有直接关联，也无法识别出在推荐/搜索/广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08094v1": {
    "title": "DarkHash: A Data-Free Backdoor Attack Against Deep Hashing",
    "url": "https://www.alphaxiv.org/abs/2510.08094v1",
    "arxiv_id": "2510.08094v1",
    "authors": "Ziqi Zhou, Menghao Deng, Yufei Song, Hangtao Zhang, Wei Wan, Shengshan Hu, Minghui Li, Leo Yu Zhang, Dezhong Yao",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 11:28:23",
    "ori_summary": "Benefiting from its superior feature learning capabilities and efficiency, deep hashing has achieved remarkable success in large-scale image retrieval. Recent studies have demonstrated the vulnerability of deep hashing models to backdoor attacks. Although these studies have shown promising attack results, they rely on access to the training dataset to implant the backdoor. In the real world, obtaining such data (e.g., identity information) is often prohibited due to privacy protection and intellectual property concerns. Embedding backdoors into deep hashing models without access to the training data, while maintaining retrieval accuracy for the original task, presents a novel and challenging problem. In this paper, we propose DarkHash, the first data-free backdoor attack against deep hashing. Specifically, we design a novel shadow backdoor attack framework with dual-semantic guidance. It embeds backdoor functionality and maintains original retrieval accuracy by fine-tuning only specific layers of the victim model using a surrogate dataset. We consider leveraging the relationship between individual samples and their neighbors to enhance backdoor attacks during training. By designing a topological alignment loss, we optimize both individual and neighboring poisoned samples toward the target sample, further enhancing the attack capability. Experimental results on four image datasets, five model architectures, and two hashing methods demonstrate the high effectiveness of DarkHash, outperforming existing state-of-the-art backdoor attack methods. Defense experiments show that DarkHash can withstand existing mainstream backdoor defense methods.",
    "summary": "",
    "translation": "DarkHash：一种针对深度哈希的无数据后门攻击",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于深度哈希系统的后门攻击和安全性问题，这属于网络安全和模型安全领域。虽然深度哈希技术可能在某些推荐系统中用于相似性搜索，但论文的核心焦点是攻击方法和安全漏洞，这明确属于被排除的\"安全、隐私\"等非技术性话题范畴。该研究没有涉及推荐系统、搜索或广告的核心算法改进或LLM技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08073v1": {
    "title": "Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection",
    "url": "https://www.alphaxiv.org/abs/2510.08073v1",
    "arxiv_id": "2510.08073v1",
    "authors": "Shuhai Zhang, ZiHao Lian, Jiahao Yang, Daiyuan Li, Guoxuan Pang, Feng Liu, Bo Han, Shutao Li, Mingkui Tan",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-09 11:00:35",
    "ori_summary": "AI-generated videos have achieved near-perfect visual realism (e.g., Sora), urgently necessitating reliable detection mechanisms. However, detecting such videos faces significant challenges in modeling high-dimensional spatiotemporal dynamics and identifying subtle anomalies that violate physical laws. In this paper, we propose a physics-driven AI-generated video detection paradigm based on probability flow conservation principles. Specifically, we propose a statistic called Normalized Spatiotemporal Gradient (NSG), which quantifies the ratio of spatial probability gradients to temporal density changes, explicitly capturing deviations from natural video dynamics. Leveraging pre-trained diffusion models, we develop an NSG estimator through spatial gradients approximation and motion-aware temporal modeling without complex motion decomposition while preserving physical constraints. Building on this, we propose an NSG-based video detection method (NSG-VD) that computes the Maximum Mean Discrepancy (MMD) between NSG features of the test and real videos as a detection metric. Last, we derive an upper bound of NSG feature distances between real and generated videos, proving that generated videos exhibit amplified discrepancies due to distributional shifts. Extensive experiments confirm that NSG-VD outperforms state-of-the-art baselines by 16.00% in Recall and 10.75% in F1-Score, validating the superior performance of NSG-VD. The source code is available at https://github.com/ZSHsh98/NSG-VD.",
    "summary": "",
    "translation": "物理驱动的时空建模用于AI生成视频检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于AI生成视频检测这一特定计算机视觉任务，属于内容检测和鉴伪领域。虽然涉及AI生成内容，但这是纯粹的视觉应用，与推荐系统、搜索或广告中的排序、匹配、用户建模等核心问题没有直接关联。论文的物理驱动建模方法也没有显示出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08067v1": {
    "title": "Towards Real-World Deepfake Detection: A Diverse In-the-wild Dataset of Forgery Faces",
    "url": "https://www.alphaxiv.org/abs/2510.08067v1",
    "arxiv_id": "2510.08067v1",
    "authors": "Junyu Shi, Minghui Li, Junguo Zuo, Zhifei Yu, Yipeng Lin, Shengshan Hu, Ziqi Zhou, Yechao Zhang, Wei Wan, Yinzhe Xu, Leo Yu Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 10:54:38",
    "ori_summary": "Deepfakes, leveraging advanced AIGC (Artificial Intelligence-Generated Content) techniques, create hyper-realistic synthetic images and videos of human faces, posing a significant threat to the authenticity of social media. While this real-world threat is increasingly prevalent, existing academic evaluations and benchmarks for detecting deepfake forgery often fall short to achieve effective application for their lack of specificity, limited deepfake diversity, restricted manipulation techniques.To address these limitations, we introduce RedFace (Real-world-oriented Deepfake Face), a specialized facial deepfake dataset, comprising over 60,000 forged images and 1,000 manipulated videos derived from authentic facial features, to bridge the gap between academic evaluations and real-world necessity. Unlike prior benchmarks, which typically rely on academic methods to generate deepfakes, RedFace utilizes 9 commercial online platforms to integrate the latest deepfake technologies found \"in the wild\", effectively simulating real-world black-box scenarios.Moreover, RedFace's deepfakes are synthesized using bespoke algorithms, allowing it to capture diverse and evolving methods used by real-world deepfake creators. Extensive experimental results on RedFace (including cross-domain, intra-domain, and real-world social network dissemination simulations) verify the limited practicality of existing deepfake detection schemes against real-world applications. We further perform a detailed analysis of the RedFace dataset, elucidating the reason of its impact on detection performance compared to conventional datasets. Our dataset is available at: https://github.com/kikyou-220/RedFace.",
    "summary": "",
    "translation": "迈向真实世界深度伪造检测：一个多样化的野外伪造人脸数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于深度伪造检测和计算机视觉安全领域，与推荐系统、搜索或广告的核心技术无关。虽然深度伪造检测在内容审核中有应用，但这属于安全/信任范畴，属于明确排除的非技术性话题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08060v1": {
    "title": "A class-driven hierarchical ResNet for classification of multispectral remote sensing images",
    "url": "https://www.alphaxiv.org/abs/2510.08060v1",
    "arxiv_id": "2510.08060v1",
    "authors": "Giulio Weikmann, Gianmarco Perantoni, Lorenzo Bruzzone",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 10:47:52",
    "ori_summary": "This work presents a multitemporal class-driven hierarchical Residual Neural Network (ResNet) designed for modelling the classification of Time Series (TS) of multispectral images at different semantical class levels. The architecture consists of a modification of the ResNet where we introduce additional branches to perform the classification at the different hierarchy levels and leverage on hierarchy-penalty maps to discourage incoherent hierarchical transitions within the classification. In this way, we improve the discrimination capabilities of classes at different levels of semantic details and train a modular architecture that can be used as a backbone network for introducing new specific classes and additional tasks considering limited training samples available. We exploit the class-hierarchy labels to train efficiently the different layers of the architecture, allowing the first layers to train faster on the first levels of the hierarchy modeling general classes (i.e., the macro-classes) and the intermediate classes, while using the last ones to discriminate more specific classes (i.e., the micro-classes). In this way, the targets are constrained in following the hierarchy defined, improving the classification of classes at the most detailed level. The proposed modular network has intrinsic adaptation capability that can be obtained through fine tuning. The experimental results, obtained on two tiles of the Amazonian Forest on 12 monthly composites of Sentinel 2 images acquired during 2019, demonstrate the effectiveness of the hierarchical approach in both generalizing over different hierarchical levels and learning discriminant features for an accurate classification at the micro-class level on a new target area, with a better representation of the minoritarian classes.",
    "summary": "",
    "translation": "一种用于多光谱遥感图像分类的类驱动分层残差网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于多光谱遥感图像分类，属于纯粹的计算机视觉应用领域，与推荐系统、搜索或广告没有任何直接关联。论文提出的类驱动分层残差网络架构是特定于遥感图像处理的解决方案，没有显示出在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08054v1": {
    "title": "RetouchLLM: Training-free White-box Image Retouching",
    "url": "https://www.alphaxiv.org/abs/2510.08054v1",
    "arxiv_id": "2510.08054v1",
    "authors": "Moon Ye-Bin, Roy Miles, Tae-Hyun Oh, Ismail Elezi, Jiankang Deng",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 10:40:49",
    "ori_summary": "Image retouching not only enhances visual quality but also serves as a means of expressing personal preferences and emotions. However, existing learning-based approaches require large-scale paired data and operate as black boxes, making the retouching process opaque and limiting their adaptability to handle diverse, user- or image-specific adjustments. In this work, we propose RetouchLLM, a training-free white-box image retouching system, which requires no training data and performs interpretable, code-based retouching directly on high-resolution images. Our framework progressively enhances the image in a manner similar to how humans perform multi-step retouching, allowing exploration of diverse adjustment paths. It comprises of two main modules: a visual critic that identifies differences between the input and reference images, and a code generator that produces executable codes. Experiments demonstrate that our approach generalizes well across diverse retouching styles, while natural language-based user interaction enables interpretable and controllable adjustments tailored to user intent.",
    "summary": "",
    "translation": "RetouchLLM：无需训练的白盒图像润色",
    "relevance_score": 1,
    "reasoning": "该论文专注于图像处理领域的白盒图像润色技术，属于纯粹的计算机视觉应用。虽然涉及LLM，但核心是图像编辑任务，与推荐系统、搜索或广告的排名和建模需求没有直接关联。该技术不具备在RecSys/Search/Ads领域的明显应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08052v1": {
    "title": "RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings for Weakly Supervised Anomaly Detection in Brain MRI Scans",
    "url": "https://www.alphaxiv.org/abs/2510.08052v1",
    "arxiv_id": "2510.08052v1",
    "authors": "Bheeshm Sharma, Karthikeyan Jaganathan, Balamurugan Palaniappan",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 10:37:47",
    "ori_summary": "Weakly Supervised Anomaly detection (WSAD) in brain MRI scans is an important challenge useful to obtain quick and accurate detection of brain anomalies when precise pixel-level anomaly annotations are unavailable and only weak labels (e.g., slice-level) are available. In this work, we propose RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings, a novel two-stage WSAD framework. In the first stage, we introduce a Discriminative Dual Prompt Tuning (DDPT) mechanism that generates high-quality pseudo weak masks based on slice-level labels, serving as coarse localization cues. In the second stage, we propose a segmentation network with a region-aware spatial attention mechanism that relies on fixed location-based random embeddings. This design enables the model to effectively focus on anomalous regions. Our approach achieves state-of-the-art anomaly detection performance, significantly outperforming existing WSAD methods while utilizing less than 8 million parameters. Extensive evaluations on the BraTS20, BraTS21, BraTS23, and MSD datasets demonstrate a substantial performance improvement coupled with a significant reduction in computational complexity. Code is available at: https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/.",
    "summary": "",
    "translation": "RASALoRE：基于区域感知空间注意力与位置随机嵌入的脑部MRI扫描弱监督异常检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（脑部MRI）的异常检测，属于医疗领域的特定应用。虽然提到了注意力机制，但其核心应用场景（脑部MRI）与推荐系统、搜索或广告领域完全无关。该技术没有明显的潜力应用于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08017v1": {
    "title": "RayFusion: Ray Fusion Enhanced Collaborative Visual Perception",
    "url": "https://www.alphaxiv.org/abs/2510.08017v1",
    "arxiv_id": "2510.08017v1",
    "authors": "Shaohong Wang, Bin Lu, Xinyu Xiao, Hanzhi Zhong, Bowen Pang, Tong Wang, Zhiyu Xiang, Hangguan Shan, Eryun Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 09:54:08",
    "ori_summary": "Collaborative visual perception methods have gained widespread attention in the autonomous driving community in recent years due to their ability to address sensor limitation problems. However, the absence of explicit depth information often makes it difficult for camera-based perception systems, e.g., 3D object detection, to generate accurate predictions. To alleviate the ambiguity in depth estimation, we propose RayFusion, a ray-based fusion method for collaborative visual perception. Using ray occupancy information from collaborators, RayFusion reduces redundancy and false positive predictions along camera rays, enhancing the detection performance of purely camera-based collaborative perception systems. Comprehensive experiments show that our method consistently outperforms existing state-of-the-art models, substantially advancing the performance of collaborative visual perception. The code is available at https://github.com/wangsh0111/RayFusion.",
    "summary": "",
    "translation": "RayFusion：射线融合增强的协同视觉感知",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉领域的协同感知技术，虽然涉及多模态融合概念，但主要应用于自动驾驶或机器人视觉等场景。在推荐系统、搜索或广告领域，这种射线融合的视觉感知技术缺乏明确的直接应用场景，与异构数据统一建模的关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08003v1": {
    "title": "CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.08003v1",
    "arxiv_id": "2510.08003v1",
    "authors": "Weihuang Lin, Yiwei Ma, Jiayi Ji, Xiaoshuai Sun, Rongrong Ji",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 09:41:45",
    "ori_summary": "Composed Image Retrieval (CIR), which aims to find a target image from a reference image and a modification text, presents the core challenge of performing unified reasoning across visual and semantic modalities. While current approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more recent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown progress, they predominantly function as ``black boxes.\" This inherent opacity not only prevents users from understanding the retrieval rationale but also restricts the models' ability to follow complex, fine-grained instructions. To overcome these limitations, we introduce CIR-CoT, the first end-to-end retrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT) reasoning. By compelling the model to first generate an interpretable reasoning chain, CIR-CoT enhances its ability to capture crucial cross-modal interactions, leading to more accurate retrieval while making its decision process transparent. Since existing datasets like FashionIQ and CIRR lack the necessary reasoning data, a key contribution of our work is the creation of structured CoT annotations using a three-stage process involving a caption, reasoning, and conclusion. Our model is then fine-tuned to produce this structured output before encoding its final retrieval intent into a dedicated embedding. Comprehensive experiments show that CIR-CoT achieves highly competitive performance on in-domain datasets (FashionIQ, CIRR) and demonstrates remarkable generalization on the out-of-domain CIRCO dataset, establishing a new path toward more effective and trustworthy retrieval systems.",
    "summary": "",
    "translation": "CIR-CoT：通过端到端思维链推理实现可解释的组合图像检索",
    "relevance_score": 3,
    "reasoning": "虽然该论文涉及检索系统，但主要关注组合图像检索这一视觉领域特定任务，与推荐系统、搜索或广告的核心进展关联有限。思维链推理技术可能对理解用户意图有潜在价值，但论文的直接应用场景过于偏向视觉检索而非文本或异构数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07990v1": {
    "title": "GraphEnet: Event-driven Human Pose Estimation with a Graph Neural Network",
    "url": "https://www.alphaxiv.org/abs/2510.07990v1",
    "arxiv_id": "2510.07990v1",
    "authors": "Gaurvi Goyal, Pham Cong Thuong, Arren Glover, Masayoshi Mizuno, Chiara Bartolozzi",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 09:24:48",
    "ori_summary": "Human Pose Estimation is a crucial module in human-machine interaction applications and, especially since the rise in deep learning technology, robust methods are available to consumers using RGB cameras and commercial GPUs. On the other hand, event-based cameras have gained popularity in the vision research community for their low latency and low energy advantages that make them ideal for applications where those resources are constrained like portable electronics and mobile robots. In this work we propose a Graph Neural Network, GraphEnet, that leverages the sparse nature of event camera output, with an intermediate line based event representation, to estimate 2D Human Pose of a single person at a high frequency. The architecture incorporates a novel offset vector learning paradigm with confidence based pooling to estimate the human pose. This is the first work that applies Graph Neural Networks to event data for Human Pose Estimation. The code is open-source at https://github.com/event-driven-robotics/GraphEnet-NeVi-ICCV2025.",
    "summary": "",
    "translation": "GraphEnet：基于图神经网络的事件驱动人体姿态估计",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的人体姿态估计，使用图神经网络处理事件驱动数据。虽然涉及图神经网络技术，但该工作纯粹针对视觉任务，没有展示在推荐系统、搜索或广告领域的潜在应用。人体姿态估计与文本/序列建模、用户行为分析或内容排名等核心关注领域相距甚远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07984v1": {
    "title": "Is Architectural Complexity Always the Answer? A Case Study on SwinIR vs. an Efficient CNN",
    "url": "https://www.alphaxiv.org/abs/2510.07984v1",
    "arxiv_id": "2510.07984v1",
    "authors": "Chandresh Sutariya, Nitin Singh",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-09 09:16:05",
    "ori_summary": "The simultaneous restoration of high-frequency details and suppression of severe noise in low-light imagery presents a significant and persistent challenge in computer vision. While large-scale Transformer models like SwinIR have set the state of the art in performance, their high computational cost can be a barrier for practical applications. This paper investigates the critical trade-off between performance and efficiency by comparing the state-of-the-art SwinIR model against a standard, lightweight Convolutional Neural Network (CNN) on this challenging task. Our experimental results reveal a nuanced but important finding. While the Transformer-based SwinIR model achieves a higher peak performance, with a Peak Signal-to-Noise Ratio (PSNR) of 39.03 dB, the lightweight CNN delivers a surprisingly competitive PSNR of 37.4 dB. Crucially, the CNN reached this performance after converging in only 10 epochs of training, whereas the more complex SwinIR model required 132 epochs. This efficiency is further underscored by the model's size; the CNN is over 55 times smaller than SwinIR. This work demonstrates that a standard CNN can provide a near state-of-the-art result with significantly lower computational overhead, presenting a compelling case for its use in real-world scenarios where resource constraints are a primary concern.",
    "summary": "",
    "translation": "架构复杂性总是答案吗？SwinIR与高效CNN的案例研究",
    "relevance_score": 2,
    "reasoning": "该论文主要比较SwinIR（基于Transformer的架构）与高效CNN在图像恢复任务中的表现，属于计算机视觉领域的架构效率研究。虽然涉及Transformer架构，但论文聚焦于图像恢复这一特定视觉任务，没有明确展示在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07976v1": {
    "title": "The impact of abstract and object tags on image privacy classification",
    "url": "https://www.alphaxiv.org/abs/2510.07976v1",
    "arxiv_id": "2510.07976v1",
    "authors": "Darya Baranouskaya, Andrea Cavallaro",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 09:09:02",
    "ori_summary": "Object tags denote concrete entities and are central to many computer vision tasks, whereas abstract tags capture higher-level information, which is relevant for tasks that require a contextual, potentially subjective scene understanding. Object and abstract tags extracted from images also facilitate interpretability. In this paper, we explore which type of tags is more suitable for the context-dependent and inherently subjective task of image privacy. While object tags are generally used for privacy classification, we show that abstract tags are more effective when the tag budget is limited. Conversely, when a larger number of tags per image is available, object-related information is as useful. We believe that these findings will guide future research in developing more accurate image privacy classifiers, informed by the role of tag types and quantity.",
    "summary": "",
    "translation": "摘要和对象标签对图像隐私分类的影响",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于图像隐私分类，属于计算机视觉和隐私保护领域，与推荐系统、搜索或广告的核心技术无关。虽然涉及标签分类，但主要关注隐私而非推荐或搜索场景中的内容理解与匹配，因此不符合当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07961v1": {
    "title": "Latent Harmony: Synergistic Unified UHD Image Restoration via Latent Space Regularization and Controllable Refinement",
    "url": "https://www.alphaxiv.org/abs/2510.07961v1",
    "arxiv_id": "2510.07961v1",
    "authors": "Yidi Liu, Xueyang Fu, Jie Huang, Jie Xiao, Dong Li, Wenlong Zhang, Lei Bai, Zheng-Jun Zha",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 08:54:26",
    "ori_summary": "Ultra-High Definition (UHD) image restoration faces a trade-off between computational efficiency and high-frequency detail retention. While Variational Autoencoders (VAEs) improve efficiency via latent-space processing, their Gaussian constraint often discards degradation-specific high-frequency information, hurting reconstruction fidelity. To overcome this, we propose Latent Harmony, a two-stage framework that redefines VAEs for UHD restoration by jointly regularizing the latent space and enforcing high-frequency-aware reconstruction.In Stage One, we introduce LH-VAE, which enhances semantic robustness through visual semantic constraints and progressive degradation perturbations, while latent equivariance strengthens high-frequency reconstruction.Stage Two jointly trains this refined VAE with a restoration model using High-Frequency Low-Rank Adaptation (HF-LoRA): an encoder LoRA guided by a fidelity-oriented high-frequency alignment loss to recover authentic details, and a decoder LoRA driven by a perception-oriented loss to synthesize realistic textures. Both LoRA modules are trained via alternating optimization with selective gradient propagation to preserve the pretrained latent structure.At inference, a tunable parameter {\\alpha} enables flexible fidelity-perception trade-offs.Experiments show Latent Harmony achieves state-of-the-art performance across UHD and standard-resolution tasks, effectively balancing efficiency, perceptual quality, and reconstruction accuracy.",
    "summary": "",
    "translation": "潜在和谐：通过潜在空间正则化与可控精细化实现协同统一超高清图像修复",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的超高清图像修复技术，涉及潜在空间正则化和可控精细化方法。虽然技术上有一定先进性，但论文内容纯粹针对图像处理任务，与推荐系统、搜索或广告的核心技术领域没有任何直接或间接的应用关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07953v1": {
    "title": "SimCast: Enhancing Precipitation Nowcasting with Short-to-Long Term Knowledge Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.07953v1",
    "arxiv_id": "2510.07953v1",
    "authors": "Yifang Yin, Shengkai Chen, Yiyao Li, Lu Wang, Ruibing Jin, Wei Cui, Shili Xiang",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-09 08:49:16",
    "ori_summary": "Precipitation nowcasting predicts future radar sequences based on current observations, which is a highly challenging task driven by the inherent complexity of the Earth system. Accurate nowcasting is of utmost importance for addressing various societal needs, including disaster management, agriculture, transportation, and energy optimization. As a complementary to existing non-autoregressive nowcasting approaches, we investigate the impact of prediction horizons on nowcasting models and propose SimCast, a novel training pipeline featuring a short-to-long term knowledge distillation technique coupled with a weighted MSE loss to prioritize heavy rainfall regions. Improved nowcasting predictions can be obtained without introducing additional overhead during inference. As SimCast generates deterministic predictions, we further integrate it into a diffusion-based framework named CasCast, leveraging the strengths from probabilistic models to overcome limitations such as blurriness and distribution shift in deterministic outputs. Extensive experimental results on three benchmark datasets validate the effectiveness of the proposed framework, achieving mean CSI scores of 0.452 on SEVIR, 0.474 on HKO-7, and 0.361 on MeteoNet, which outperforms existing approaches by a significant margin.",
    "summary": "",
    "translation": "SimCast：通过短期到长期知识蒸馏增强降水临近预报",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于气象领域的降水预报，属于纯粹的领域特定应用，与推荐系统、搜索或广告没有任何关联。知识蒸馏技术虽然本身是通用方法，但论文的应用场景和核心问题与我的关注领域完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07951v1": {
    "title": "A Large-scale Dataset for Robust Complex Anime Scene Text Detection",
    "url": "https://www.alphaxiv.org/abs/2510.07951v1",
    "arxiv_id": "2510.07951v1",
    "authors": "Ziyi Dong, Yurui Zhang, Changmao Li, Naomi Rue Golding, Qing Long",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-09 08:47:52",
    "ori_summary": "Current text detection datasets primarily target natural or document scenes, where text typically appear in regular font and shapes, monotonous colors, and orderly layouts. The text usually arranged along straight or curved lines. However, these characteristics differ significantly from anime scenes, where text is often diverse in style, irregularly arranged, and easily confused with complex visual elements such as symbols and decorative patterns. Text in anime scene also includes a large number of handwritten and stylized fonts. Motivated by this gap, we introduce AnimeText, a large-scale dataset containing 735K images and 4.2M annotated text blocks. It features hierarchical annotations and hard negative samples tailored for anime scenarios. %Cross-dataset evaluations using state-of-the-art methods demonstrate that models trained on AnimeText achieve superior performance in anime text detection tasks compared to existing datasets. To evaluate the robustness of AnimeText in complex anime scenes, we conducted cross-dataset benchmarking using state-of-the-art text detection methods. Experimental results demonstrate that models trained on AnimeText outperform those trained on existing datasets in anime scene text detection tasks. AnimeText on HuggingFace: https://huggingface.co/datasets/deepghs/AnimeText",
    "summary": "",
    "translation": "用于鲁棒复杂动漫场景文本检测的大规模数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于动漫场景中的文本检测，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然文本检测在广义上可能与内容理解相关，但该研究的动漫特定性和视觉焦点使其与所列技术领域相距甚远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07944v1": {
    "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving",
    "url": "https://www.alphaxiv.org/abs/2510.07944v1",
    "arxiv_id": "2510.07944v1",
    "authors": "Tianrui Zhang, Yichen Liu, Zilin Guo, Yuxin Guo, Jingcheng Ni, Chenjing Ding, Dan Xu, Lewei Lu, Zehuan Wu",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 08:41:58",
    "ori_summary": "Generative models have been widely applied to world modeling for environment simulation and future state prediction. With advancements in autonomous driving, there is a growing demand not only for high-fidelity video generation under various controls, but also for producing diverse and meaningful information such as depth estimation. To address this, we propose CVD-STORM, a cross-view video diffusion model utilizing a spatial-temporal reconstruction Variational Autoencoder (VAE) that generates long-term, multi-view videos with 4D reconstruction capabilities under various control inputs. Our approach first fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its ability to encode 3D structures and temporal dynamics. Subsequently, we integrate this VAE into the video diffusion process to significantly improve generation quality. Experimental results demonstrate that our model achieves substantial improvements in both FID and FVD metrics. Additionally, the jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic scenes, providing valuable geometric information for comprehensive scene understanding.",
    "summary": "",
    "translation": "CVD-STORM：面向自动驾驶的具有时空重建模型的跨视角视频扩散",
    "relevance_score": 2,
    "reasoning": "该论文主要关注自动驾驶领域的视频生成和时空建模，属于计算机视觉应用范畴。虽然视频扩散模型在技术上有一定先进性，但论文标题明确指向自动驾驶这一特定领域应用，与推荐系统、搜索或广告的核心技术焦点缺乏直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07927v1": {
    "title": "ASBench: Image Anomalies Synthesis Benchmark for Anomaly Detection",
    "url": "https://www.alphaxiv.org/abs/2510.07927v1",
    "arxiv_id": "2510.07927v1",
    "authors": "Qunyi Zhang, Songan Zhang, Jinbao Wang, Xiaoning Lei, Guoyang Xie, Guannan Jiang, Zhichao Lu",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 08:23:29",
    "ori_summary": "Anomaly detection plays a pivotal role in manufacturing quality control, yet its application is constrained by limited abnormal samples and high manual annotation costs. While anomaly synthesis offers a promising solution, existing studies predominantly treat anomaly synthesis as an auxiliary component within anomaly detection frameworks, lacking systematic evaluation of anomaly synthesis algorithms. Current research also overlook crucial factors specific to anomaly synthesis, such as decoupling its impact from detection, quantitative analysis of synthetic data and adaptability across different scenarios. To address these limitations, we propose ASBench, the first comprehensive benchmarking framework dedicated to evaluating anomaly synthesis methods. Our framework introduces four critical evaluation dimensions: (i) the generalization performance across different datasets and pipelines (ii) the ratio of synthetic to real data (iii) the correlation between intrinsic metrics of synthesis images and anomaly detection performance metrics , and (iv) strategies for hybrid anomaly synthesis methods. Through extensive experiments, ASBench not only reveals limitations in current anomaly synthesis methods but also provides actionable insights for future research directions in anomaly synthesis",
    "summary": "",
    "translation": "ASBench：用于异常检测的图像异常合成基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的图像异常检测基准测试，与推荐系统、搜索或广告的核心技术领域没有直接关联。图像异常检测主要应用于工业质检、医疗影像等视觉领域，无法为RecSys/Search/Ads提供有价值的技术启示或应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07915v1": {
    "title": "MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.07915v1",
    "arxiv_id": "2510.07915v1",
    "authors": "Peiran Wu, Zhuorui Yu, Yunze Liu, Chi-Hao Wu, Enmin Zhou, Junxiao Shen",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 08:07:19",
    "ori_summary": "The rapid progress of large language models (LLMs) has laid the foundation for multimodal models. However, visual language models (VLMs) still face heavy computational costs when extended from images to videos due to high frame rates and long durations. Token compression is a promising solution, yet most existing training-free methods cause information loss and performance degradation. To overcome this, we propose \\textbf{Memory-Augmented Reinforcement Learning-based Token Compression (MARC)}, which integrates structured retrieval and RL-based distillation. MARC adopts a \\textit{retrieve-then-compress} strategy using a \\textbf{Visual Memory Retriever (VMR)} to select key clips and a \\textbf{Compression Group Relative Policy Optimization (C-GRPO)} framework to distil reasoning ability from a teacher to a student model. Experiments on six video benchmarks show that MARC achieves near-baseline accuracy using only one frame's tokens -- reducing visual tokens by \\textbf{95\\%}, GPU memory by \\textbf{72\\%}, and latency by \\textbf{23.9\\%}. This demonstrates its potential for efficient, real-time video understanding in resource-constrained settings such as video QA, surveillance, and autonomous driving.",
    "summary": "",
    "translation": "MARC：用于高效视频理解的内存增强强化学习令牌压缩",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频理解的效率优化，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术关联性较弱。虽然压缩技术理论上可能应用于多模态推荐中的视频内容处理，但论文的强化学习和视频理解焦点使其与当前关注点的直接相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07910v1": {
    "title": "MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.07910v1",
    "arxiv_id": "2510.07910v1",
    "authors": "Chongmyung Kwon, Yujin Kim, Seoeun Park, Yunji Lee, Charmgil Hong",
    "categories": "cs.LG, cs.AI, cs.CV, I.2.6; I.5.1",
    "pub_date": "2025-10-09 08:03:14",
    "ori_summary": "Drug recommendation is an essential task in machine learning-based clinical decision support systems. However, the risk of drug-drug interactions (DDI) between co-prescribed medications remains a significant challenge. Previous studies have used graph neural networks (GNNs) to represent drug structures. Regardless, their simplified discrete forms cannot fully capture the molecular binding affinity and reactivity. Therefore, we propose Multimodal DDI Prediction with Molecular Electron Localization Function (ELF) Maps (MMM), a novel framework that integrates three-dimensional (3D) quantum-chemical information into drug representation learning. It generates 3D electron density maps using the ELF. To capture both therapeutic relevance and interaction risks, MMM combines ELF-derived features that encode global electronic properties with a bipartite graph encoder that models local substructure interactions. This design enables learning complementary characteristics of drug molecules. We evaluate MMM in the MIMIC-III dataset (250 drugs, 442 substructures), comparing it with several baseline models. In particular, a comparison with the GNN-based SafeDrug model demonstrates statistically significant improvements in the F1-score (p = 0.0387), Jaccard (p = 0.0112), and the DDI rate (p = 0.0386). These results demonstrate the potential of ELF-based 3D representations to enhance prediction accuracy and support safer combinatorial drug prescribing in clinical practice.",
    "summary": "",
    "translation": "MMM：基于量子化学分子表征学习的组合药物推荐",
    "relevance_score": 1,
    "reasoning": "该论文专注于药物推荐这一特定医疗领域应用，属于明确的无关主题范畴。虽然标题包含'推荐'一词，但内容涉及量子化学分子表征和组合药物推荐，与搜索、推荐系统或广告的核心技术进展无关，且没有明显的通用推荐系统技术迁移潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07905v1": {
    "title": "SatFusion: A Unified Framework for Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion",
    "url": "https://www.alphaxiv.org/abs/2510.07905v1",
    "arxiv_id": "2510.07905v1",
    "authors": "Yufei Tong, Guanjie Cheng, Peihan Wu, Yicheng Zhu, Kexu Lu, Feiyi Chen, Meng Xi, Junqin Huang, Shuiguang Deng",
    "categories": "eess.IV, cs.CV, cs.MM",
    "pub_date": "2025-10-09 07:59:37",
    "ori_summary": "With the rapid advancement of the digital society, the proliferation of satellites in the Satellite Internet of Things (Sat-IoT) has led to the continuous accumulation of large-scale multi-temporal and multi-source images across diverse application scenarios. However, existing methods fail to fully exploit the complementary information embedded in both temporal and source dimensions. For example, Multi-Image Super-Resolution (MISR) enhances reconstruction quality by leveraging temporal complementarity across multiple observations, yet the limited fine-grained texture details in input images constrain its performance. Conversely, pansharpening integrates multi-source images by injecting high-frequency spatial information from panchromatic data, but typically relies on pre-interpolated low-resolution inputs and assumes noise-free alignment, making it highly sensitive to noise and misregistration. To address these issues, we propose SatFusion: A Unified Framework for Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion. Specifically, SatFusion first employs a Multi-Temporal Image Fusion (MTIF) module to achieve deep feature alignment with the panchromatic image. Then, a Multi-Source Image Fusion (MSIF) module injects fine-grained texture information from the panchromatic data. Finally, a Fusion Composition module adaptively integrates the complementary advantages of both modalities while dynamically refining spectral consistency, supervised by a weighted combination of multiple loss functions. Extensive experiments on the WorldStrat, WV3, QB, and GF2 datasets demonstrate that SatFusion significantly improves fusion quality, robustness under challenging conditions, and generalizability to real-world Sat-IoT scenarios. The code is available at: https://github.com/dllgyufei/SatFusion.git.",
    "summary": "",
    "translation": "SatFusion：一种通过多时序和多源数据融合增强卫星物联网图像的统一框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于卫星图像处理和物联网数据融合，属于遥感技术领域。虽然涉及多源数据融合概念，但与推荐系统、搜索或广告的核心技术没有直接关联，也不涉及LLM、Transformer架构或异构数据建模在推荐领域的应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07878v1": {
    "title": "FlowLensing: Simulating Gravitational Lensing with Flow Matching",
    "url": "https://www.alphaxiv.org/abs/2510.07878v1",
    "arxiv_id": "2510.07878v1",
    "authors": "Hamees Sayed, Pranath Reddy, Michael W. Toomey, Sergei Gleyzer",
    "categories": "astro-ph.IM, cs.CV",
    "pub_date": "2025-10-09 07:31:47",
    "ori_summary": "Gravitational lensing is one of the most powerful probes of dark matter, yet creating high-fidelity lensed images at scale remains a bottleneck. Existing tools rely on ray-tracing or forward-modeling pipelines that, while precise, are prohibitively slow. We introduce FlowLensing, a Diffusion Transformer-based compact and efficient flow-matching model for strong gravitational lensing simulation. FlowLensing operates in both discrete and continuous regimes, handling classes such as different dark matter models as well as continuous model parameters ensuring physical consistency. By enabling scalable simulations, our model can advance dark matter studies, specifically for probing dark matter substructure in cosmological surveys. We find that our model achieves a speedup of over 200$\\times$ compared to classical simulators for intensive dark matter models, with high fidelity and low inference latency. FlowLensing enables rapid, scalable, and physically consistent image synthesis, offering a practical alternative to traditional forward-modeling pipelines.",
    "summary": "",
    "translation": "FlowLensing：使用流匹配模拟引力透镜效应",
    "relevance_score": 1,
    "reasoning": "这篇论文涉及物理学中的引力透镜模拟，使用流匹配技术解决物理模拟问题。该主题与推荐系统、搜索或广告领域没有任何直接或间接的联系，完全超出了关注范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07871v1": {
    "title": "Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception - Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track",
    "url": "https://www.alphaxiv.org/abs/2510.07871v1",
    "arxiv_id": "2510.07871v1",
    "authors": "Erjia Xiao, Lingfeng Zhang, Yingbo Tang, Hao Cheng, Renjing Xu, Wenbo Ding, Lei Zhou, Long Chen, Hangjun Ye, Xiaoshuai Hao",
    "categories": "cs.RO, cs.AI, cs.CV, cs.LG",
    "pub_date": "2025-10-09 07:22:12",
    "ori_summary": "In this report, we describe the technical details of our submission to the IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on developing RGBD-based perception and navigation systems that enable autonomous agents to navigate safely, efficiently, and socially compliantly in dynamic human-populated indoor environments. The challenge requires agents to operate from an egocentric perspective using only onboard sensors including RGB-D observations and odometry, without access to global maps or privileged information, while maintaining social norm compliance such as safe distances and collision avoidance. Building upon the Falcon model, we introduce a Proactive Risk Perception Module to enhance social navigation performance. Our approach augments Falcon with collision risk understanding that learns to predict distance-based collision risk scores for surrounding humans, which enables the agent to develop more robust spatial awareness and proactive collision avoidance behaviors. The evaluation on the Social-HM3D benchmark demonstrates that our method improves the agent's ability to maintain personal space compliance while navigating toward goals in crowded indoor scenes with dynamic human agents, achieving 2nd place among 16 participating teams in the challenge.",
    "summary": "",
    "translation": "小米EV-AD VLA团队：通过主动风险感知学习社会性导航——IROS 2025 RoboSense挑战赛社会导航赛道技术报告",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人社会导航和主动风险感知，属于自动驾驶领域。虽然标题包含'VLA'（可能指视觉语言模型），但核心内容围绕机器人导航挑战赛，与推荐系统、搜索或广告没有任何直接或间接关联。该技术无法应用于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07856v1": {
    "title": "XYZCylinder: Feedforward Reconstruction for Driving Scenes Based on A Unified Cylinder Lifting Method",
    "url": "https://www.alphaxiv.org/abs/2510.07856v1",
    "arxiv_id": "2510.07856v1",
    "authors": "Haochen Yu, Qiankun Liu, Hongyuan Liu, Jianfei Jiang, Juntao Lyu, Jiansheng Chen, Huimin Ma",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 06:58:03",
    "ori_summary": "Recently, more attention has been paid to feedforward reconstruction paradigms, which mainly learn a fixed view transformation implicitly and reconstruct the scene with a single representation. However, their generalization capability and reconstruction accuracy are still limited while reconstructing driving scenes, which results from two aspects: (1) The fixed view transformation fails when the camera configuration changes, limiting the generalization capability across different driving scenes equipped with different camera configurations. (2) The small overlapping regions between sparse views of the $360^\\circ$ panorama and the complexity of driving scenes increase the learning difficulty, reducing the reconstruction accuracy. To handle these difficulties, we propose \\textbf{XYZCylinder}, a feedforward model based on a unified cylinder lifting method which involves camera modeling and feature lifting. Specifically, to improve the generalization capability, we design a Unified Cylinder Camera Modeling (UCCM) strategy, which avoids the learning of viewpoint-dependent spatial correspondence and unifies different camera configurations with adjustable parameters. To improve the reconstruction accuracy, we propose a hybrid representation with several dedicated modules based on newly designed Cylinder Plane Feature Group (CPFG) to lift 2D image features to 3D space. Experimental results show that XYZCylinder achieves state-of-the-art performance under different evaluation settings, and can be generalized to other driving scenes in a zero-shot manner. Project page: \\href{https://yuyuyu223.github.io/XYZCYlinder-projectpage/}{here}.",
    "summary": "",
    "translation": "XYZCylinder：基于统一圆柱提升方法的驾驶场景前馈重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的驾驶场景重建，属于纯粹的视觉技术领域。虽然标题提到\"重建\"和\"驾驶场景\"，但这与推荐系统、搜索或广告的核心技术没有直接关联，也没有涉及LLM技术或Transformer架构的进展，无法看出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07853v1": {
    "title": "Self-Supervised Learning Strategies for a Platform to Test the Toxicity of New Chemicals and Materials",
    "url": "https://www.alphaxiv.org/abs/2510.07853v1",
    "arxiv_id": "2510.07853v1",
    "authors": "Thomas Lautenschlager, Nils Friederich, Angelo Jovin Yamachui Sitcheu, Katja Nau, Gaëlle Hayot, Thomas Dickmeis, Ralf Mikut",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-09 06:51:12",
    "ori_summary": "High-throughput toxicity testing offers a fast and cost-effective way to test large amounts of compounds. A key component for such systems is the automated evaluation via machine learning models. In this paper, we address critical challenges in this domain and demonstrate how representations learned via self-supervised learning can effectively identify toxicant-induced changes. We provide a proof-of-concept that utilizes the publicly available EmbryoNet dataset, which contains ten zebrafish embryo phenotypes elicited by various chemical compounds targeting different processes in early embryonic development. Our analysis shows that the learned representations using self-supervised learning are suitable for effectively distinguishing between the modes-of-action of different compounds. Finally, we discuss the integration of machine learning models in a physical toxicity testing device in the context of the TOXBOX project.",
    "summary": "",
    "translation": "用于测试新化学品和材料毒性的平台的自监督学习策略",
    "relevance_score": 1,
    "reasoning": "该论文专注于化学和材料毒性测试领域，这属于明确的无关主题范畴（医学、生物学、化学或其他领域特定应用）。尽管提到了自监督学习，但核心应用场景与推荐系统、搜索或广告完全无关，且没有显示出任何在这些领域应用的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07839v1": {
    "title": "AlignGS: Aligning Geometry and Semantics for Robust Indoor Reconstruction from Sparse Views",
    "url": "https://www.alphaxiv.org/abs/2510.07839v1",
    "arxiv_id": "2510.07839v1",
    "authors": "Yijie Gao, Houqiang Zhong, Tianchi Zhu, Zhengxue Cheng, Qiang Hu, Li Song",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 06:30:20",
    "ori_summary": "The demand for semantically rich 3D models of indoor scenes is rapidly growing, driven by applications in augmented reality, virtual reality, and robotics. However, creating them from sparse views remains a challenge due to geometric ambiguity. Existing methods often treat semantics as a passive feature painted on an already-formed, and potentially flawed, geometry. We posit that for robust sparse-view reconstruction, semantic understanding instead be an active, guiding force. This paper introduces AlignGS, a novel framework that actualizes this vision by pioneering a synergistic, end-to-end optimization of geometry and semantics. Our method distills rich priors from 2D foundation models and uses them to directly regularize the 3D representation through a set of novel semantic-to-geometry guidance mechanisms, including depth consistency and multi-faceted normal regularization. Extensive evaluations on standard benchmarks demonstrate that our approach achieves state-of-the-art results in novel view synthesis and produces reconstructions with superior geometric accuracy. The results validate that leveraging semantic priors as a geometric regularizer leads to more coherent and complete 3D models from limited input views. Our code is avaliable at https://github.com/MediaX-SJTU/AlignGS .",
    "summary": "",
    "translation": "AlignGS：对齐几何与语义以实现稀疏视图下的鲁棒室内重建",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的3D室内重建问题，属于纯粹的视觉领域研究。虽然提到了语义对齐，但其核心是几何重建技术，与推荐系统、搜索或广告的排名和建模需求没有直接关联。该技术缺乏在RecSys/Search/Ads领域的明显应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07837v1": {
    "title": "IsoSignVid2Aud: Sign Language Video to Audio Conversion without Text Intermediaries",
    "url": "https://www.alphaxiv.org/abs/2510.07837v1",
    "arxiv_id": "2510.07837v1",
    "authors": "Harsh Kavediya, Vighnesh Nayak, Bheeshm Sharma, Balamurugan Palaniappan",
    "categories": "cs.CV, cs.MM, cs.SD",
    "pub_date": "2025-10-09 06:29:59",
    "ori_summary": "Sign language to spoken language audio translation is important to connect the hearing- and speech-challenged humans with others. We consider sign language videos with isolated sign sequences rather than continuous grammatical signing. Such videos are useful in educational applications and sign prompt interfaces. Towards this, we propose IsoSignVid2Aud, a novel end-to-end framework that translates sign language videos with a sequence of possibly non-grammatic continuous signs to speech without requiring intermediate text representation, providing immediate communication benefits while avoiding the latency and cascading errors inherent in multi-stage translation systems. Our approach combines an I3D-based feature extraction module with a specialized feature transformation network and an audio generation pipeline, utilizing a novel Non-Maximal Suppression (NMS) algorithm for the temporal detection of signs in non-grammatic continuous sequences. Experimental results demonstrate competitive performance on ASL-Citizen-1500 and WLASL-100 datasets with Top-1 accuracies of 72.01\\% and 78.67\\%, respectively, and audio quality metrics (PESQ: 2.67, STOI: 0.73) indicating intelligible speech output. Code is available at: https://github.com/BheeshmSharma/IsoSignVid2Aud_AIMLsystems-2025.",
    "summary": "",
    "translation": "IsoSignVid2Aud：无需文本中介的手语视频到音频转换",
    "relevance_score": 1,
    "reasoning": "该论文专注于手语视频到音频的模态转换技术，属于计算机视觉和语音处理的交叉领域。虽然涉及多模态处理，但与推荐系统、搜索或广告的核心技术栈没有直接关联，也不涉及Transformer架构改进或LLM技术在推荐领域的应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07830v1": {
    "title": "PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale 3D Gaussian Splatting",
    "url": "https://www.alphaxiv.org/abs/2510.07830v1",
    "arxiv_id": "2510.07830v1",
    "authors": "Houqiang Zhong, Zhenglong Wu, Sihua Fu, Zihan Zheng, Xin Jin, Xiaoyun Zhang, Li Song, Qiang Hu",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 06:21:45",
    "ori_summary": "3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic rendering in compact scenes, but scaling to large urban environments introduces severe aliasing artifacts and optimization instability, especially under high-resolution (e.g., 4K) rendering. These artifacts, manifesting as flickering textures and jagged edges, arise from the mismatch between Gaussian primitives and the multi-scale nature of urban geometry. While existing ``divide-and-conquer'' pipelines address scalability, they fail to resolve this fidelity gap. In this paper, we propose PrismGS, a physically-grounded regularization framework that improves the intrinsic rendering behavior of 3D Gaussians. PrismGS integrates two synergistic regularizers. The first is pyramidal multi-scale supervision, which enforces consistency by supervising the rendering against a pre-filtered image pyramid. This compels the model to learn an inherently anti-aliased representation that remains coherent across different viewing scales, directly mitigating flickering textures. This is complemented by an explicit size regularization that imposes a physically-grounded lower bound on the dimensions of the 3D Gaussians. This prevents the formation of degenerate, view-dependent primitives, leading to more stable and plausible geometric surfaces and reducing jagged edges. Our method is plug-and-play and compatible with existing pipelines. Extensive experiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS achieves state-of-the-art performance, yielding significant PSNR gains around 1.5 dB against CityGaussian, while maintaining its superior quality and robustness under demanding 4K rendering.",
    "summary": "",
    "translation": "PrismGS：面向高保真大规模3D高斯泼溅的物理基础抗锯齿技术",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D计算机图形学中的渲染技术（3D高斯泼溅和抗锯齿），属于纯粹的视觉/图形学领域。虽然标题提到'大规模'，但内容与推荐系统、搜索或广告的核心技术栈（排序、召回、用户建模等）无直接关联，且未提及任何潜在的跨模态应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07828v1": {
    "title": "MMHOI: Modeling Complex 3D Multi-Human Multi-Object Interactions",
    "url": "https://www.alphaxiv.org/abs/2510.07828v1",
    "arxiv_id": "2510.07828v1",
    "authors": "Kaen Kogashi, Anoop Cherian, Meng-Yu Jennifer Kuo",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 06:18:12",
    "ori_summary": "Real-world scenes often feature multiple humans interacting with multiple objects in ways that are causal, goal-oriented, or cooperative. Yet existing 3D human-object interaction (HOI) benchmarks consider only a fraction of these complex interactions. To close this gap, we present MMHOI -- a large-scale, Multi-human Multi-object Interaction dataset consisting of images from 12 everyday scenarios. MMHOI offers complete 3D shape and pose annotations for every person and object, along with labels for 78 action categories and 14 interaction-specific body parts, providing a comprehensive testbed for next-generation HOI research. Building on MMHOI, we present MMHOI-Net, an end-to-end transformer-based neural network for jointly estimating human-object 3D geometries, their interactions, and associated actions. A key innovation in our framework is a structured dual-patch representation for modeling objects and their interactions, combined with action recognition to enhance the interaction prediction. Experiments on MMHOI and the recently proposed CORE4D datasets demonstrate that our approach achieves state-of-the-art performance in multi-HOI modeling, excelling in both accuracy and reconstruction quality.",
    "summary": "",
    "translation": "MMHOI：建模复杂的三维多人与多物体交互",
    "relevance_score": 2,
    "reasoning": "该论文专注于3D视觉中复杂交互的建模，属于纯粹的计算机视觉领域。虽然建模复杂交互的技术在概念上可能与推荐系统中的用户-物品交互有类比，但论文标题明确限定在3D视觉场景，没有显示出与推荐系统、搜索或广告的直接关联或潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07823v1": {
    "title": "Enhancing Visual Prompting through Expanded Transformation Space and Overfitting Mitigation",
    "url": "https://www.alphaxiv.org/abs/2510.07823v1",
    "arxiv_id": "2510.07823v1",
    "authors": "Shohei Enomoto",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 06:08:15",
    "ori_summary": "Visual prompting (VP) has emerged as a promising parameter-efficient fine-tuning approach for adapting pre-trained vision models to downstream tasks without modifying model parameters. Despite offering advantages like negligible computational overhead and compatibility with black-box models, conventional VP methods typically achieve lower accuracy than other adaptation approaches. Our analysis reveals two critical limitations: the restricted expressivity of simple additive transformation and a tendency toward overfitting when the parameter count increases. To address these challenges, we propose ACAVP (Affine, Color, and Additive Visual Prompting), which enhances VP's expressive power by introducing complementary transformation operations: affine transformation for creating task-specific prompt regions while preserving original image information, and color transformation for emphasizing task-relevant visual features. Additionally, we identify that overfitting is a critical issue in VP training and introduce TrivialAugment as an effective data augmentation, which not only benefits our approach but also significantly improves existing VP methods, with performance gains of up to 12 percentage points on certain datasets. This demonstrates that appropriate data augmentation is universally beneficial for VP training. Extensive experiments across twelve diverse image classification datasets with two different model architectures demonstrate that ACAVP achieves state-of-the-art accuracy among VP methods, surpasses linear probing in average accuracy, and exhibits superior robustness to distribution shifts, all while maintaining minimal computational overhead during inference.",
    "summary": "",
    "translation": "通过扩展变换空间与缓解过拟合增强视觉提示",
    "relevance_score": 1,
    "reasoning": "该论文专注于视觉提示技术，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术无关。虽然视觉语言模型被列为关注领域，但该论文仅涉及视觉提示的改进，并未涉及多模态建模或异构数据处理，因此对当前关注点没有实际应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07817v1": {
    "title": "An End-to-End Room Geometry Constrained Depth Estimation Framework for Indoor Panorama Images",
    "url": "https://www.alphaxiv.org/abs/2510.07817v1",
    "arxiv_id": "2510.07817v1",
    "authors": "Kanglin Ning, Ruzhao Chen, Penghong Wang, Xingtao Wang, Ruiqin Xiong, Xiaopeng Fan",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 05:52:48",
    "ori_summary": "Predicting spherical pixel depth from monocular $360^{\\circ}$ indoor panoramas is critical for many vision applications. However, existing methods focus on pixel-level accuracy, causing oversmoothed room corners and noise sensitivity. In this paper, we propose a depth estimation framework based on room geometry constraints, which extracts room geometry information through layout prediction and integrates those information into the depth estimation process through background segmentation mechanism. At the model level, our framework comprises a shared feature encoder followed by task-specific decoders for layout estimation, depth estimation, and background segmentation. The shared encoder extracts multi-scale features, which are subsequently processed by individual decoders to generate initial predictions: a depth map, a room layout map, and a background segmentation map. Furthermore, our framework incorporates two strategies: a room geometry-based background depth resolving strategy and a background-segmentation-guided fusion mechanism. The proposed room-geometry-based background depth resolving strategy leverages the room layout and the depth decoder's output to generate the corresponding background depth map. Then, a background-segmentation-guided fusion strategy derives fusion weights for the background and coarse depth maps from the segmentation decoder's predictions. Extensive experimental results on the Stanford2D3D, Matterport3D and Structured3D datasets show that our proposed methods can achieve significantly superior performance than current open-source methods. Our code is available at https://github.com/emiyaning/RGCNet.",
    "summary": "",
    "translation": "面向室内全景图像的端到端房间几何约束深度估计框架",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉中的深度估计任务，特别是针对室内全景图像，这属于纯粹的视觉处理范畴。虽然深度估计在增强现实和机器人导航中有应用，但与推荐系统、搜索或广告的核心技术缺乏直接关联，也没有涉及LLM、Transformer架构或异构数据建模等焦点领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07810v1": {
    "title": "FMANet: A Novel Dual-Phase Optical Flow Approach with Fusion Motion Attention Network for Robust Micro-expression Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.07810v1",
    "arxiv_id": "2510.07810v1",
    "authors": "Luu Tu Nguyen, Vu Tram Anh Khuong, Thi Bich Phuong Man, Thi Duyen Ngo, Thanh Ha Le",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 05:36:40",
    "ori_summary": "Facial micro-expressions, characterized by their subtle and brief nature, are valuable indicators of genuine emotions. Despite their significance in psychology, security, and behavioral analysis, micro-expression recognition remains challenging due to the difficulty of capturing subtle facial movements. Optical flow has been widely employed as an input modality for this task due to its effectiveness. However, most existing methods compute optical flow only between the onset and apex frames, thereby overlooking essential motion information in the apex-to-offset phase. To address this limitation, we first introduce a comprehensive motion representation, termed Magnitude-Modulated Combined Optical Flow (MM-COF), which integrates motion dynamics from both micro-expression phases into a unified descriptor suitable for direct use in recognition networks. Building upon this principle, we then propose FMANet, a novel end-to-end neural network architecture that internalizes the dual-phase analysis and magnitude modulation into learnable modules. This allows the network to adaptively fuse motion cues and focus on salient facial regions for classification. Experimental evaluations on the MMEW, SMIC, CASME-II, and SAMM datasets, widely recognized as standard benchmarks, demonstrate that our proposed MM-COF representation and FMANet outperforms existing methods, underscoring the potential of a learnable, dual-phase framework in advancing micro-expression recognition.",
    "summary": "",
    "translation": "FMANet：一种用于鲁棒微表情识别的融合运动注意力网络的新型双阶段光流方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于微表情识别这一计算机视觉特定领域，使用光流和注意力机制进行面部微表情分析。这与搜索、推荐或广告系统的核心需求没有直接关联，也不涉及LLM技术、Transformer架构进展或异构数据统一建模等关键焦点领域。微表情识别主要应用于心理学、人机交互等场景，与RecSys/Search/Ads的技术栈和应用场景相距甚远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07791v1": {
    "title": "GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07791v1",
    "arxiv_id": "2510.07791v1",
    "authors": "Qinghongbing Xie, Zhaoyuan Xia, Feng Zhu, Lijun Gong, Ziyue Li, Rui Zhao, Long Zeng",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 05:09:27",
    "ori_summary": "Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has attracted much attention due to its importance for Autonomous Driving, Embodied AI and General Artificial Intelligence. Existing spatial-temporal benchmarks mainly focus on egocentric perspective reasoning with images/video context, or geographic perspective reasoning with graphics context (eg. a map), thus fail to assess VLMs' geographic spatial-temporal intelligence with both images/video and graphics context, which is important for areas like traffic management and emergency response. To address the gaps, we introduce Geo-Temporal Reasoning benchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of moving targets in a large-scale camera network. GTR-Bench is more challenging as it requires multiple perspective switches between maps and videos, joint reasoning across multiple videos with non-overlapping fields of view, and inference over spatial-temporal regions that are unobserved by any video context. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that even the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags behind human performance (78.61%) on geo-temporal reasoning. Moreover, our comprehensive analysis on GTR-Bench reveals three primary deficiencies of current models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by an imbalanced utilization of spatial-temporal context. (2) VLMs are weak in temporal forecasting, which leads to worse performance on temporal-emphasized tasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to comprehend or align the map data with multi-view video inputs. We believe GTR-Bench offers valuable insights and opens up new opportunities for research and applications in spatial-temporal intelligence. Benchmark and code will be released at https://github.com/X-Luffy/GTR-Bench.",
    "summary": "",
    "translation": "GTR-Bench：评估视觉语言模型中的地理时空推理能力",
    "relevance_score": 2,
    "reasoning": "虽然该论文涉及视觉语言模型评估，但其焦点是地理时空推理这一特定能力，与推荐系统、搜索或广告的核心技术关联较弱。地理时空推理在本地化推荐中可能有潜在应用，但论文主要关注评估基准而非直接的技术应用或架构创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07785v1": {
    "title": "Demystifying Deep Learning-based Brain Tumor Segmentation with 3D UNets and Explainable AI (XAI): A Comparative Analysis",
    "url": "https://www.alphaxiv.org/abs/2510.07785v1",
    "arxiv_id": "2510.07785v1",
    "authors": "Ming Jie Ong, Sze Yinn Ung, Sim Kuan Goh, Jimmy Y. Zhong",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 05:03:31",
    "ori_summary": "The current study investigated the use of Explainable Artificial Intelligence (XAI) to improve the accuracy of brain tumor segmentation in MRI images, with the goal of assisting physicians in clinical decision-making. The study focused on applying UNet models for brain tumor segmentation and using the XAI techniques of Gradient-weighted Class Activation Mapping (Grad-CAM) and attention-based visualization to enhance the understanding of these models. Three deep learning models - UNet, Residual UNet (ResUNet), and Attention UNet (AttUNet) - were evaluated to identify the best-performing model. XAI was employed with the aims of clarifying model decisions and increasing physicians' trust in these models. We compared the performance of two UNet variants (ResUNet and AttUNet) with the conventional UNet in segmenting brain tumors from the BraTS2020 public dataset and analyzed model predictions with Grad-CAM and attention-based visualization. Using the latest computer hardware, we trained and validated each model using the Adam optimizer and assessed their performance with respect to: (i) training, validation, and inference times, (ii) segmentation similarity coefficients and loss functions, and (iii) classification performance. Notably, during the final testing phase, ResUNet outperformed the other models with respect to Dice and Jaccard similarity scores, as well as accuracy, recall, and F1 scores. Grad-CAM provided visuospatial insights into the tumor subregions each UNet model focused on while attention-based visualization provided valuable insights into the working mechanisms of AttUNet's attention modules. These results demonstrated ResUNet as the best-performing model and we conclude by recommending its use for automated brain tumor segmentation in future clinical assessments. Our source code and checkpoint are available at https://github.com/ethanong98/MultiModel-XAI-Brats2020",
    "summary": "",
    "translation": "基于3D UNet和可解释人工智能（XAI）的深度学习脑肿瘤分割技术解析：一项对比分析",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像领域的脑肿瘤分割，使用3D UNet和可解释AI技术，属于明确的医学/生物学应用范畴。根据用户要求，医学、生物学等特定领域应用属于不相关主题，且该技术没有明显的推荐系统、搜索或广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07778v1": {
    "title": "IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction",
    "url": "https://www.alphaxiv.org/abs/2510.07778v1",
    "arxiv_id": "2510.07778v1",
    "authors": "Yandu Chen, Kefan Gu, Yuqing Wen, Yucheng Zhao, Tiancai Wang, Liqiang Nie",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2025-10-09 04:49:46",
    "ori_summary": "Vision-Language-Action (VLA) models leverage pretrained vision-language models (VLMs) to couple perception with robotic control, offering a promising path toward general-purpose embodied intelligence. However, current SOTA VLAs are primarily pretrained on multimodal tasks with limited relevance to embodied scenarios, and then finetuned to map explicit instructions to actions. Consequently, due to the lack of reasoning-intensive pretraining and reasoning-guided manipulation, these models are unable to perform implicit human intention reasoning required for complex, real-world interactions. To overcome these limitations, we propose \\textbf{IntentionVLA}, a VLA framework with a curriculum training paradigm and an efficient inference mechanism. Our proposed method first leverages carefully designed reasoning data that combine intention inference, spatial grounding, and compact embodied reasoning, endowing the model with both reasoning and perception capabilities. In the following finetuning stage, IntentionVLA employs the compact reasoning outputs as contextual guidance for action generation, enabling fast inference under indirect instructions. Experimental results show that IntentionVLA substantially outperforms $\\pi_0$, achieving 18\\% higher success rates with direct instructions and 28\\% higher than ECoT under intention instructions. On out-of-distribution intention tasks, IntentionVLA achieves over twice the success rate of all baselines, and further enables zero-shot human-robot interaction with 40\\% success rate. These results highlight IntentionVLA as a promising paradigm for next-generation human-robot interaction (HRI) systems.",
    "summary": "",
    "translation": "IntentionVLA：面向人机交互的可泛化高效具身意图推理",
    "relevance_score": 2,
    "reasoning": "该论文专注于机器人领域的具身意图推理和人机交互，属于特定领域应用。虽然涉及多模态理解和意图建模，但其核心应用场景（机器人交互）与推荐系统、搜索或广告领域缺乏直接关联，且未明确展示在推荐/搜索/广告中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07752v1": {
    "title": "DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream",
    "url": "https://www.alphaxiv.org/abs/2510.07752v1",
    "arxiv_id": "2510.07752v1",
    "authors": "Junhao He, Jiaxu Wang, Jia Li, Mingyuan Sun, Qiang Zhang, Jiahang Cao, Ziyi Zhang, Yi Gu, Jingkai Sun, Renjing Xu",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 03:43:27",
    "ori_summary": "Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB videos is challenging. This is because large inter-frame motions will increase the uncertainty of the solution space. For example, one pixel in the first frame might have more choices to reach the corresponding pixel in the second frame. Event cameras can asynchronously capture rapid visual changes and are robust to motion blur, but they do not provide color information. Intuitively, the event stream can provide deterministic constraints for the inter-frame large motion by the event trajectories. Hence, combining low-temporal-resolution images with high-framerate event streams can address this challenge. However, it is challenging to jointly optimize Dynamic 3DGS using both RGB and event modalities due to the significant discrepancy between these two data modalities. This paper introduces a novel framework that jointly optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event motion priors to guide the optimization of the deformation fields. First, we extract the motion priors encoded in event streams by using the proposed LoCM unsupervised fine-tuning framework to adapt an event flow estimator to a certain unseen scene. Then, we present the geometry-aware data association method to build the event-Gaussian motion correspondence, which is the primary foundation of the pipeline, accompanied by two useful strategies, namely motion decomposition and inter-frame pseudo-label. Extensive experiments show that our method outperforms existing image and event-based approaches across synthetic and real scenes and prove that our method can effectively optimize dynamic 3DGS with the help of event data.",
    "summary": "",
    "translation": "DEGS：基于可变形事件的三维高斯泼溅，从RGB和事件流中重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于事件相机和3D重建的计算机视觉技术，属于纯粹的视觉领域研究。标题中提到的3D高斯泼溅、事件流和RGB输入都是视觉感知和重建的特定技术，没有显示出与推荐系统、搜索或广告领域的任何潜在关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07741v1": {
    "title": "UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes",
    "url": "https://www.alphaxiv.org/abs/2510.07741v1",
    "arxiv_id": "2510.07741v1",
    "authors": "Yuang Meng, Xin Jin, Lina Lei, Chun-Le Guo, Chongyi Li",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-09 03:29:39",
    "ori_summary": "Ultra-high dynamic range (UHDR) scenes exhibit significant exposure disparities between bright and dark regions. Such conditions are commonly encountered in nighttime scenes with light sources. Even with standard exposure settings, a bimodal intensity distribution with boundary peaks often emerges, making it difficult to preserve both highlight and shadow details simultaneously. RGB-based bracketing methods can capture details at both ends using short-long exposure pairs, but are susceptible to misalignment and ghosting artifacts. We found that a short-exposure image already retains sufficient highlight detail. The main challenge of UHDR reconstruction lies in denoising and recovering information in dark regions. In comparison to the RGB images, RAW images, thanks to their higher bit depth and more predictable noise characteristics, offer greater potential for addressing this challenge. This raises a key question: can we learn to see everything in UHDR scenes using only a single short-exposure RAW image? In this study, we rely solely on a single short-exposure frame, which inherently avoids ghosting and motion blur, making it particularly robust in dynamic scenes. To achieve that, we introduce UltraLED, a two-stage framework that performs exposure correction via a ratio map to balance dynamic range, followed by a brightness-aware RAW denoiser to enhance detail recovery in dark regions. To support this setting, we design a 9-stop bracketing pipeline to synthesize realistic UHDR images and contribute a corresponding dataset based on diverse scenes, using only the shortest exposure as input for reconstruction. Extensive experiments show that UltraLED significantly outperforms existing single-frame approaches. Our code and dataset are made publicly available at https://srameo.github.io/projects/ultraled.",
    "summary": "",
    "translation": "UltraLED：学习在超高动态范围场景中看到一切",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉中的超高动态范围成像技术，属于纯粹的视觉处理领域。虽然视觉技术在搜索和推荐中有潜在应用（如图像搜索），但论文标题没有表明与推荐系统、搜索或广告的直接关联，也没有涉及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11654v1": {
    "title": "FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection",
    "url": "https://www.alphaxiv.org/abs/2510.11654v1",
    "arxiv_id": "2510.11654v1",
    "authors": "Daniel Berhane Araya, Duoduo Liao",
    "categories": "cs.IR, cs.AI, cs.CL",
    "pub_date": "2025-10-13 17:31:49",
    "ori_summary": "Financial markets face growing threats from misinformation that can trigger billions in losses in minutes. Most existing approaches lack transparency in their decision-making and provide limited attribution to credible sources. We introduce FinVet, a novel multi-agent framework that integrates two Retrieval-Augmented Generation (RAG) pipelines with external fact-checking through a confidence-weighted voting mechanism. FinVet employs adaptive three-tier processing that dynamically adjusts verification strategies based on retrieval confidence, from direct metadata extraction to hybrid reasoning to full model-based analysis. Unlike existing methods, FinVet provides evidence-backed verdicts, source attribution, confidence scores, and explicit uncertainty flags when evidence is insufficient. Experimental evaluation on the FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a 10.4% improvement over the best individual pipeline (fact-check pipeline) and 37% improvement over standalone RAG approaches.",
    "summary": "",
    "translation": "FinVet：基于检索增强生成与外部事实核查代理协作框架的金融虚假信息检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注金融领域的虚假信息检测，属于特定领域应用，与推荐系统、搜索或广告的核心技术进展无关。虽然涉及RAG技术，但其应用场景局限于金融信息验证，缺乏在RecSys/Search/Ads领域的通用性或潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11639v1": {
    "title": "OneRec-Think: In-Text Reasoning for Generative Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.11639v1",
    "arxiv_id": "2510.11639v1",
    "authors": "Zhanyu Liu, Shiyao Wang, Xingmei Wang, Rongzhou Zhang, Jiaxin Deng, Honghui Bao, Jinghao Zhang, Wuchao Li, Pengfei Zheng, Xiangyu Wu, Yifei Hu, Qigen Hu, Xinchen Luo, Lejian Ren, Zixing Zhang, Qianqian Wang, Kuo Cai, Yunfan Wu, Hongtao Cheng, Zexuan Cheng, Lu Ren, Huanjie Wang, Yi Su, Ruiming Tang, Kun Gai, Guorui Zhou",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 17:20:13",
    "ori_summary": "The powerful generative capacity of Large Language Models (LLMs) has instigated a paradigm shift in recommendation. However, existing generative models (e.g., OneRec) operate as implicit predictors, critically lacking the capacity for explicit and controllable reasoning-a key advantage of LLMs. To bridge this gap, we propose OneRec-Think, a unified framework that seamlessly integrates dialogue, reasoning, and personalized recommendation. OneRec-Think incorporates: (1) Itemic Alignment: cross-modal Item-Textual Alignment for semantic grounding; (2) Reasoning Activation: Reasoning Scaffolding to activate LLM reasoning within the recommendation context; and (3) Reasoning Enhancement, where we design a recommendation-specific reward function that accounts for the multi-validity nature of user preferences. Experiments across public benchmarks show state-of-the-art performance. Moreover, our proposed \"Think-Ahead\" architecture enables effective industrial deployment on Kuaishou, achieving a 0.159\\% gain in APP Stay Time and validating the practical efficacy of the model's explicit reasoning capability.",
    "summary": "论文研究生成式推荐系统缺乏显式可控推理能力的问题，核心思想是通过项目对齐、推理激活和推理增强三个组件，构建能够进行显式推理的统一推荐框架。",
    "translation": "OneRec-Think：面向生成式推荐系统的文本内推理",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及生成式推荐系统，属于'Direct LLM Applications'范畴。论文标题明确提到'生成式推荐'和'文本内推理'，这表明它探索LLM在推荐系统中的推理能力，对于提升推荐系统的解释性和智能性具有直接应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM在推荐系统中的显式推理能力，提出了统一的对话-推理-推荐框架，完全符合直接LLM应用和核心领域进展的关注点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11599v1": {
    "title": "SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific and Interpretable Scientific Domain Mapping",
    "url": "https://www.alphaxiv.org/abs/2510.11599v1",
    "arxiv_id": "2510.11599v1",
    "authors": "Marc Brinner, Sina Zarrieß",
    "categories": "cs.CL, cs.AI, cs.IR, cs.LG",
    "pub_date": "2025-10-13 16:38:20",
    "ori_summary": "We propose SemCSE-Multi, a novel unsupervised framework for generating multifaceted embeddings of scientific abstracts, evaluated in the domains of invasion biology and medicine. These embeddings capture distinct, individually specifiable aspects in isolation, thus enabling fine-grained and controllable similarity assessments as well as adaptive, user-driven visualizations of scientific domains. Our approach relies on an unsupervised procedure that produces aspect-specific summarizing sentences and trains embedding models to map semantically related summaries to nearby positions in the embedding space. We then distill these aspect-specific embedding capabilities into a unified embedding model that directly predicts multiple aspect embeddings from a scientific abstract in a single, efficient forward pass. In addition, we introduce an embedding decoding pipeline that decodes embeddings back into natural language descriptions of their associated aspects. Notably, we show that this decoding remains effective even for unoccupied regions in low-dimensional visualizations, thus offering vastly improved interpretability in user-centric settings.",
    "summary": "",
    "translation": "SemCSE-Multi：用于特定方面和可解释科学领域映射的多方面可解码嵌入",
    "relevance_score": 3,
    "reasoning": "该论文主要关注科学领域映射和多方面嵌入，这与推荐系统中的多兴趣建模有一定相似性。然而，论文明确针对科学领域应用，缺乏明确的推荐、搜索或广告应用场景。虽然可解码嵌入技术可能对解释性推荐系统有潜在价值，但论文标题未表明与目标领域的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11592v1": {
    "title": "REGENT: Relevance-Guided Attention for Entity-Aware Multi-Vector Neural Re-Ranking",
    "url": "https://www.alphaxiv.org/abs/2510.11592v1",
    "arxiv_id": "2510.11592v1",
    "authors": "Shubham Chatterjee",
    "categories": "cs.IR, cs.CL",
    "pub_date": "2025-10-13 16:31:42",
    "ori_summary": "Current neural re-rankers often struggle with complex information needs and long, content-rich documents. The fundamental issue is not computational--it is intelligent content selection: identifying what matters in lengthy, multi-faceted texts. While humans naturally anchor their understanding around key entities and concepts, neural models process text within rigid token windows, treating all interactions as equally important and missing critical semantic signals. We introduce REGENT, a neural re-ranking model that mimics human-like understanding by using entities as a \"semantic skeleton\" to guide attention. REGENT integrates relevance guidance directly into the attention mechanism, combining fine-grained lexical matching with high-level semantic reasoning. This relevance-guided attention enables the model to focus on conceptually important content while maintaining sensitivity to precise term matches. REGENT achieves new state-of-the-art performance in three challenging datasets, providing up to 108% improvement over BM25 and consistently outperforming strong baselines including ColBERT and RankVicuna. To our knowledge, this is the first work to successfully integrate entity semantics directly into neural attention, establishing a new paradigm for entity-aware information retrieval.",
    "summary": "研究神经重排模型在复杂信息需求和长文档场景下的内容选择问题。核心方法是将实体作为语义骨架，通过相关性引导注意力机制结合细粒度词汇匹配和高层语义推理，实现人类化的理解方式。",
    "translation": "REGENT：面向实体感知多向量神经重排的相关性引导注意力机制",
    "relevance_score": 8,
    "reasoning": "该论文提出了一种用于神经重排的相关性引导注意力机制，直接属于搜索领域的核心进展。REGENT通过实体感知和多向量表示来改进重排效果，这与搜索系统中的文档重排和相关性建模高度相关，能够直接提升搜索质量。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文通过实体引导注意力机制解决神经重排中的内容选择问题，直接关联搜索和推荐系统中的核心挑战，方法创新且具有通用性。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11589v1": {
    "title": "QDER: Query-Specific Document and Entity Representations for Multi-Vector Document Re-Ranking",
    "url": "https://www.alphaxiv.org/abs/2510.11589v1",
    "arxiv_id": "2510.11589v1",
    "authors": "Shubham Chatterjee, Jeff Dalton",
    "categories": "cs.IR, cs.CL",
    "pub_date": "2025-10-13 16:31:06",
    "ori_summary": "Neural IR has advanced through two distinct paths: entity-oriented approaches leveraging knowledge graphs and multi-vector models capturing fine-grained semantics. We introduce QDER, a neural re-ranking model that unifies these approaches by integrating knowledge graph semantics into a multi-vector model. QDER's key innovation lies in its modeling of query-document relationships: rather than computing similarity scores on aggregated embeddings, we maintain individual token and entity representations throughout the ranking process, performing aggregation only at the final scoring stage - an approach we call \"late aggregation.\" We first transform these fine-grained representations through learned attention patterns, then apply carefully chosen mathematical operations for precise matches. Experiments across five standard benchmarks show that QDER achieves significant performance gains, with improvements of 36% in nDCG@20 over the strongest baseline on TREC Robust 2004 and similar improvements on other datasets. QDER particularly excels on difficult queries, achieving an nDCG@20 of 0.70 where traditional approaches fail completely (nDCG@20 = 0.0), setting a foundation for future work in entity-aware retrieval.",
    "summary": "论文研究多向量文档重排序的核心问题，核心方法是提出QDER模型，通过延迟聚合机制保持细粒度token和实体表示，将知识图谱语义集成到多向量模型中统一建模查询-文档关系。",
    "translation": "QDER：用于多向量文档重排的查询特定文档和实体表示",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及搜索系统中的文档重排技术，这是搜索领域的核心进展。多向量表示和查询特定表示方法可以显著提升搜索相关性排序性能，这种技术可以扩展到推荐系统中处理用户-项目交互的复杂表示学习。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文将知识图谱语义与多向量模型相结合，通过延迟聚合机制改进文档重排序，直接适用于搜索系统核心算法。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11560v1": {
    "title": "Characterizing Web Search in The Age of Generative AI",
    "url": "https://www.alphaxiv.org/abs/2510.11560v1",
    "arxiv_id": "2510.11560v1",
    "authors": "Elisabeth Kirsten, Jost Grosse Perdekamp, Mihir Upadhyay, Krishna P. Gummadi, Muhammad Bilal Zafar",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-13 16:04:03",
    "ori_summary": "The advent of LLMs has given rise to a new type of web search: Generative search, where LLMs retrieve web pages related to a query and generate a single, coherent text as a response. This output modality stands in stark contrast to traditional web search, where results are returned as a ranked list of independent web pages. In this paper, we ask: Along what dimensions do generative search outputs differ from traditional web search? We compare Google, a traditional web search engine, with four generative search engines from two providers (Google and OpenAI) across queries from four domains. Our analysis reveals intriguing differences. Most generative search engines cover a wider range of sources compared to web search. Generative search engines vary in the degree to which they rely on internal knowledge contained within the model parameters v.s. external knowledge retrieved from the web. Generative search engines surface varying sets of concepts, creating new opportunities for enhancing search diversity and serendipity. Our results also highlight the need for revisiting evaluation criteria for web search in the age of Generative AI.",
    "summary": "该论文研究生成式AI时代下搜索系统的核心变化问题，通过对比传统搜索与生成式搜索在信息来源、知识依赖和概念多样性等维度的差异，揭示生成式搜索带来的新特性和评估需求。",
    "translation": "生成式AI时代下的网络搜索特性分析",
    "relevance_score": 9,
    "reasoning": "该论文直接聚焦于生成式AI对搜索系统的影响，属于'Direct LLM Applications'和'Core Domain Advances'范畴。论文探讨生成式AI如何改变网络搜索的特性，这对于搜索系统的演进和LLM在搜索中的直接应用具有重要指导意义。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接研究生成式AI对搜索系统的影响，对比传统搜索与生成式搜索的核心差异，完全契合搜索领域核心进展和LLM应用的研究重点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11483v1": {
    "title": "Uncertainty Quantification for Retrieval-Augmented Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.11483v1",
    "arxiv_id": "2510.11483v1",
    "authors": "Heydar Soudani, Hamed Zamani, Faegheh Hasibi",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 14:55:28",
    "ori_summary": "Retrieval-augmented reasoning (RAR) is a recent evolution of retrieval-augmented generation (RAG) that employs multiple reasoning steps for retrieval and generation. While effective for some complex queries, RAR remains vulnerable to errors and misleading outputs. Uncertainty quantification (UQ) offers methods to estimate the confidence of systems' outputs. These methods, however, often handle simple queries with no retrieval or single-step retrieval, without properly handling RAR setup. Accurate estimation of UQ for RAR requires accounting for all sources of uncertainty, including those arising from retrieval and generation. In this paper, we account for all these sources and introduce Retrieval-Augmented Reasoning Consistency (R2C)--a novel UQ method for RAR. The core idea of R2C is to perturb the multi-step reasoning process by applying various actions to reasoning steps. These perturbations alter the retriever's input, which shifts its output and consequently modifies the generator's input at the next step. Through this iterative feedback loop, the retriever and generator continuously reshape one another's inputs, enabling us to capture uncertainty arising from both components. Experiments on five popular RAR systems across diverse QA datasets show that R2C improves AUROC by over 5% on average compared to the state-of-the-art UQ baselines. Extrinsic evaluations using R2C as an external signal further confirm its effectiveness for two downstream tasks: in Abstention, it achieves ~5% gains in both F1Abstain and AccAbstain; in Model Selection, it improves the exact match by ~7% over single models and ~3% over selection methods.",
    "summary": "该论文研究检索增强推理系统中的不确定性量化问题，核心思想是通过对推理步骤施加扰动来改变检索器和生成器的输入，从而在迭代反馈循环中捕捉检索和生成过程的不确定性。",
    "translation": "检索增强推理的不确定性量化",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及检索增强生成(RAG)技术，这是搜索和推荐系统中LLM应用的核心方法。不确定性量化对于检索系统的可信度评估、结果排序优化以及用户交互设计具有重要应用价值，能够提升搜索和推荐系统的可靠性和用户体验。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对检索增强推理系统的不确定性量化问题，其核心方法通过扰动多步推理过程来捕捉检索和生成的不确定性，对推荐系统和搜索中的可信AI应用具有直接价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11438v1": {
    "title": "What Generative Search Engines Like and How to Optimize Web Content Cooperatively",
    "url": "https://www.alphaxiv.org/abs/2510.11438v1",
    "arxiv_id": "2510.11438v1",
    "authors": "Yujiang Wu, Shanshan Zhong, Yubin Kim, Chenyan Xiong",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 14:10:26",
    "ori_summary": "By employing large language models (LLMs) to retrieve documents and generate natural language responses, Generative Engines, such as Google AI overview and ChatGPT, provide significantly enhanced user experiences and have rapidly become the new form of search. Their rapid adoption also drives the needs of Generative Engine Optimization (GEO), as content providers are eager to gain more traction from them. In this paper, we introduce AutoGEO, a framework to automatically learn generative engine preferences when using retrieved contents for response generation, and rewrite web contents for more such traction. AutoGEO first prompts frontier LLMs to explain generative engine preferences and extract meaningful preference rules from these explanations. Then it uses preference rules as context engineering for AutoGEO$_\\text{API}$, a prompt-based GEO system, and as rule-based rewards to train AutoGEO$_\\text{Mini}$, a cost-effective GEO model. Experiments on the standard GEO-Bench and two newly constructed benchmarks using real user queries demonstrate the effectiveness of AutoGEO in enhancing content traction while preserving search utility. Analyses confirm the learned rules' robustness and abilities to capture unique preferences in variant domains, and AutoGEO systems' ability to embed them in content optimization. The code is released at https://github.com/cxcscmu/AutoGEO.",
    "summary": "论文研究如何优化网页内容以提升在生成式搜索引擎中的曝光度。核心方法是自动学习生成搜索引擎的内容偏好规则，并利用这些规则通过提示工程和强化学习来重写网页内容。",
    "translation": "生成式搜索引擎的偏好及如何协同优化网络内容",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及生成式搜索引擎（LLM在搜索领域的直接应用）以及内容优化策略，这属于'Direct LLM Applications'和'Core Domain Advances'范畴。论文探讨如何根据搜索引擎偏好优化内容，这对搜索排名和内容推荐系统具有直接的应用价值，能够指导内容提供者与平台协同优化用户体验。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对搜索领域的LLM应用，提出自动学习生成搜索引擎偏好的框架，与LLM在搜索中的直接应用和优化高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11402v1": {
    "title": "On Inherited Popularity Bias in Cold-Start Item Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.11402v1",
    "arxiv_id": "2510.11402v1",
    "authors": "Gregor Meehan, Johan Pauwels",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 13:44:13",
    "ori_summary": "Collaborative filtering (CF) recommender systems struggle with making predictions on unseen, or 'cold', items. Systems designed to address this challenge are often trained with supervision from warm CF models in order to leverage collaborative and content information from the available interaction data. However, since they learn to replicate the behavior of CF methods, cold-start models may therefore also learn to imitate their predictive biases. In this paper, we show that cold-start systems can inherit popularity bias, a common cause of recommender system unfairness arising when CF models overfit to more popular items, thereby maximizing user-oriented accuracy but neglecting rarer items. We demonstrate that cold-start recommenders not only mirror the popularity biases of warm models, but are in fact affected more severely: because they cannot infer popularity from interaction data, they instead attempt to estimate it based solely on content features. This leads to significant over-prediction of certain cold items with similar content to popular warm items, even if their ground truth popularity is very low. Through experiments on three multimedia datasets, we analyze the impact of this behavior on three generative cold-start methods. We then describe a simple post-processing bias mitigation method that, by using embedding magnitude as a proxy for predicted popularity, can produce more balanced recommendations with limited harm to user-oriented cold-start accuracy.",
    "summary": "研究冷启动推荐系统中流行度偏差的继承问题，核心发现是冷启动模型无法从交互数据推断流行度，转而基于内容特征估计流行度，导致对内容类似热门物品的冷物品过度预测。",
    "translation": "冷启动物品推荐中继承性流行度偏差研究",
    "relevance_score": 8,
    "reasoning": "该论文直接针对推荐系统核心领域中的冷启动问题和流行度偏差挑战，这是推荐系统领域的关键技术问题。通过解决冷启动物品的流行度偏差，可以显著提升推荐系统的公平性和多样性，对实际推荐系统具有重要应用价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接研究推荐系统中的冷启动问题，揭示了冷启动模型从热模型继承流行度偏差的机制，对推荐系统公平性和核心算法设计有重要启示。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11394v1": {
    "title": "VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation via Rigorous Verification",
    "url": "https://www.alphaxiv.org/abs/2510.11394v1",
    "arxiv_id": "2510.11394v1",
    "authors": "Haosheng Qian, Yixing Fan, Jiafeng Guo, Ruqing Zhang, Qi Chen, Dawei Yin, Xueqi Cheng",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 13:38:54",
    "ori_summary": "Retrieval-Augmented Generation (RAG) has emerged as a crucial approach for enhancing the responses of large language models (LLMs) with external knowledge sources. Despite the impressive performance in complex question-answering tasks, RAG still struggles with hallucinations. Attributing RAG-generated content through in-line citations has demonstrated potential in reducing hallucinations and facilitating human verification. Existing citation generation methods primarily rely on either fine-tuning the generator or employing post-processing approaches for citation matching. However, the former approach demands substantial annotated data and computational resources, while the latter often encounters difficulties in managing multiple citations and frequently produces suboptimal results. In this paper, we introduce a novel framework, called VeriCite, designed to rigorously validate supporting evidence and enhance answer attribution. Specifically, VeriCite breaks down into a three-stage generation: 1) The initial answer generation first generates a response based on all available contexts and has its claims verified through the NLI model; 2) the supporting evidence selection assesses the utility of each document and extracts useful supporting evidences; 3) the final answer refinement integrates the initial response and collected evidences to produce the final, refined answer.We conduct experiments across five open-source LLMs and four datasets, demonstrating that VeriCite can significantly improve citation quality while maintaining the correctness of the answers.",
    "summary": "",
    "translation": "VeriCite：通过严格验证在检索增强生成中实现可靠引用",
    "relevance_score": 3,
    "reasoning": "该论文主要关注检索增强生成(RAG)中的引用可靠性验证，这属于LLM评估和可信度范畴。虽然RAG技术在搜索系统中有关联，但论文聚焦于引用验证而非核心搜索/推荐算法改进，且验证方法对RecSys/Ads的直接应用潜力有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11358v1": {
    "title": "LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2510.11358v1",
    "arxiv_id": "2510.11358v1",
    "authors": "Hengran Zhang, Keping Bi, Jiafeng Guo, Jiaming Zhang, Shuaiqiang Wang, Dawei Yin, Xueqi Cheng",
    "categories": "cs.CL, cs.AI, cs.IR",
    "pub_date": "2025-10-13 12:57:45",
    "ori_summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge. While traditional retrieval focuses on relevance, RAG's effectiveness depends on the utility of retrieved passages, i.e., the usefulness in facilitating the generation of an accurate and comprehensive answer. Existing studies often treat utility as a generic attribute, ignoring the fact that different LLMs may benefit differently from the same passage due to variations in internal knowledge and comprehension ability. In this work, we introduce and systematically investigate the notion of LLM-specific utility. Through large-scale experiments across multiple datasets and LLMs, we demonstrate that human-annotated passages are not optimal for LLMs and that ground-truth utilitarian passages are not transferable across different LLMs. These findings highlight the necessity of adopting the LLM-specific utility in RAG research. Our findings indicate that some human-annotated passages are not ground-truth utilitarian passages for specific LLMs, partially due to the varying readability of queries and passages for LLMs, a tendency for which perplexity is a key metric. Based on these findings, we propose a benchmarking procedure for LLM-specific utility judgments. We evaluate existing utility judgment methods on six datasets and find that while verbalized methods using pseudo-answers perform robustly, LLMs struggle to assess utility effectively-failing to reject all passages for known queries and to select truly useful ones for unknown queries.",
    "summary": "论文研究RAG系统中检索段落对特定LLM的效用问题，核心思想是不同LLM对相同段落的效用存在差异，需要采用LLM特定的效用评估标准而非通用相关性。",
    "translation": "LLM特定效用：检索增强生成的新视角",
    "relevance_score": 8,
    "reasoning": "该论文聚焦检索增强生成(RAG)，这是LLM在搜索和推荐系统中的核心应用技术。通过提出LLM特定效用的新视角，该研究有望改进RAG系统在信息检索和内容推荐中的性能，直接适用于搜索和推荐场景中的文档检索和答案生成任务。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出LLM特定效用的新概念，直接针对RAG系统中检索质量的核心问题，与LLM在搜索推荐领域的应用高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11323v1": {
    "title": "Dynamic Network-Based Two-Stage Time Series Forecasting for Affiliate Marketing",
    "url": "https://www.alphaxiv.org/abs/2510.11323v1",
    "arxiv_id": "2510.11323v1",
    "authors": "Zhe Wang, Yaming Yang, Ziyu Guan, Bin Tong, Rui Wang, Wei Zhao, Hongbo Deng",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 12:21:29",
    "ori_summary": "In recent years, affiliate marketing has emerged as a revenue-sharing strategy where merchants collaborate with promoters to promote their products. It not only increases product exposure but also allows promoters to earn a commission. This paper addresses the pivotal yet under-explored challenge in affiliate marketing: accurately assessing and predicting the contributions of promoters in product promotion. We design a novel metric for evaluating the indirect contributions of the promoter, called propagation scale. Unfortunately, existing time series forecasting techniques fail to deliver accurate predictions due to the propagation scale being influenced by multiple factors and the inherent complexities arising from dynamic scenarios. To address this issue, we decouple the network structure from the node signals and propose a two-stage solution: initially, the basic self-sales and network structure prediction are conducted separately, followed by the synthesis of the propagation scale. Specifically, we design a graph convolution encoding scheme based on descendant neighbors and incorporate hypergraph convolution to efficiently capture complex promotional dynamics. Additionally, three auxiliary tasks are employed: self-sales prediction for base estimations, descendant prediction to synthesize propagation scale, and promoter activation prediction to mitigate high volatility issues. Extensive offline experiments on large-scale industrial datasets validate the superiority of our method. We further deploy our model on Alimama platform with over $100,000$ promoters, achieving a $9.29\\%$ improvement in GMV and a $5.89\\%$ increase in sales volume.",
    "summary": "",
    "translation": "基于动态网络的两阶段时间序列预测在联盟营销中的应用",
    "relevance_score": 3,
    "reasoning": "该论文涉及时间序列预测和营销应用，但主要聚焦于联盟营销这一特定领域，而非核心的推荐系统、搜索或广告排名技术。虽然时间序列预测在推荐/广告中有应用潜力，但论文标题未表明涉及LLM、Transformer架构或异构数据统一建模等当前关注的核心技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11317v1": {
    "title": "Next Interest Flow: A Generative Pre-training Paradigm for Recommender Systems by Modeling All-domain Movelines",
    "url": "https://www.alphaxiv.org/abs/2510.11317v1",
    "arxiv_id": "2510.11317v1",
    "authors": "Chen Gao, Zixin Zhao, Lv Shao, Tong Liu",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 12:13:17",
    "ori_summary": "Click-Through Rate (CTR) prediction, a cornerstone of modern recommender systems, has been dominated by discriminative models that react to past user behavior rather than proactively modeling user intent. Existing generative paradigms attempt to address this but suffer from critical limitations: Large Language Model (LLM) based methods create a semantic mismatch by forcing e-commerce signals into a linguistic space, while ID-based generation is constrained by item memorization and cold-start issues. To overcome these limitations, we propose a novel generative pre-training paradigm. Our model learns to predict the Next Interest Flow, a dense vector sequence representing a user's future intent, while simultaneously modeling its internal Interest Diversity and Interest Evolution Velocity to ensure the representation is both rich and coherent. However, this two-stage approach introduces a critical objective mismatch between the generative and discriminative stages. We resolve this via a bidirectional alignment strategy, which harmonizes the two stages through cross-stage weight initialization and a dynamic Semantic Alignment Module for fine-tuning. Additionally, we enhance the underlying discriminative model with a Temporal Sequential Pairwise (TSP) mechanism to better capture temporal causality. We present the All-domain Moveline Evolution Network (AMEN), a unified framework implementing our entire pipeline. Extensive offline experiments validate AMEN's superiority over strong baselines, and a large-scale online A/B test demonstrates its significant real-world impact, delivering substantial improvements in key business metrics.",
    "summary": "论文研究推荐系统中点击率预测的生成式建模问题，核心思想是通过预测表示用户未来意图的密集向量序列（Next Interest Flow），同时建模兴趣多样性和演化速度，并通过双向对齐策略解决生成与判别阶段的目标不匹配问题。",
    "translation": "下一个兴趣流：通过建模全域移动轨迹的推荐系统生成式预训练范式",
    "relevance_score": 9,
    "reasoning": "该论文直接针对推荐系统提出了一种生成式预训练范式，属于'直接LLM应用'和'核心领域进展'范畴。通过建模全域移动轨迹来预测下一个兴趣，这种方法可以显著提升推荐系统的序列建模能力和跨域推荐效果，具有明确的推荐系统应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统的生成式预训练范式，提出了解决现有LLM方法和ID生成局限性的新方法，与核心领域进展和直接LLM应用高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11168v1": {
    "title": "ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces",
    "url": "https://www.alphaxiv.org/abs/2510.11168v1",
    "arxiv_id": "2510.11168v1",
    "authors": "Jinbin Zhang, Nasib Ullah, Erik Schultheis, Rohit Babbar",
    "categories": "cs.LG, cs.CL, cs.IR",
    "pub_date": "2025-10-13 08:59:13",
    "ori_summary": "Large output spaces, also referred to as Extreme multilabel classification (XMC), is a setting that arises, e.g., in large-scale tagging and product-to-product recommendation, and is characterized by the number of labels ranging from hundreds of thousands to millions. This means that the linear classification head, usually only a tiny fraction of the overall model, turns into the main driver for compute and memory demand. Current state-of-the-art XMC methods predominantly rely on FP16-FP32 mixed-precision training, which we show can be unstable, and inefficient in terms of memory usage and computational overhead. Meanwhile, existing low-precision methods typically retain higher precision for the classification layer. In this work, we propose ELMO, a pure low-precision training framework for XMC models using BFloat16 and Float8 data types. By leveraging Kahan summation and stochastic rounding, we demonstrate that XMC models can be effectively trained entirely in Float8, without relying on single-precision master weights or tensor scaling. Low-precision training, combined with our proposed memory optimizations -- gradient fusion and chunking -- enables significant reductions in GPU memory usage. For example, we train a 3-million-label XMC model with only 6.6 GiB of GPU memory, compared to the 39.7 GiB required by the optimized SOTA method, Renee without compromising accuracy.",
    "summary": "论文研究极端多标签分类场景下模型训练的计算和内存效率问题，核心思想是采用纯低精度训练框架结合内存优化技术，在不依赖高精度主权重的情况下实现高效模型训练。",
    "translation": "ELMO：通过低精度和峰值内存优化在大输出空间中实现效率",
    "relevance_score": 8,
    "reasoning": "该论文专注于Transformer架构的效率优化，通过低精度计算和内存管理技术解决大输出空间问题，这直接属于'Enabling Transformer Tech'范畴。在推荐系统和搜索场景中，处理大规模候选集（如数百万商品或文档）时，内存和计算效率至关重要，该技术可显著提升推理速度和降低部署成本。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文针对大规模推荐系统中的极端多标签分类问题，提出了低精度训练和内存优化方法，直接解决推荐系统核心的计算效率瓶颈。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11122v1": {
    "title": "DyKnow-RAG: Dynamic Knowledge Utilization Reinforcement Framework for Noisy Retrieval-Augmented Generation in E-commerce Search Relevance",
    "url": "https://www.alphaxiv.org/abs/2510.11122v1",
    "arxiv_id": "2510.11122v1",
    "authors": "Tingqiao Xu, Shaowei Yao, Chenhe Dong, Yiming Jin, Zerui Huang, Dan Ou, Haihong Tang",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 08:08:59",
    "ori_summary": "Accurately modeling query-item relevance drives e-commerce ranking, yet long-tail, knowledge-heavy, and fast-evolving queries exceed parametric LLM coverage. External context (reviews, attribute encyclopedias, UGC) can help but is noisy, and single-pass latency and cost forbid any clean-then-summarize step. The model must, per query, judge relevance and decide whether to use, partially use, or ignore the context. DyKnow-RAG is a dynamic noisy-RAG framework built on Group Relative Policy Optimization. It trains two rollout groups (no external context vs a single retrieved chunk) and applies posterior-driven inter-group advantage scaling that adaptively reweights their contributions by the per-query correctness gap. This teaches when to trust retrieval versus fall back to parametric knowledge, without process labels, value networks, or extra inference passes, preserving single-pass, single-chunk deployment under production latency. Training combines: (1) supervised initialization with a structured rationale that explicitly records the context-usage decision; (2) an RL pool prioritized by SFT uncertainty to focus where context choice is most consequential; and (3) an optional lightweight DPO warm start to stabilize with-context calibration. Under a unified retrieval/index and fixed latency budget, DyKnow-RAG outperforms SFT, DPO, and vanilla GRPO in offline tests, and delivers consistent lifts on GSB, Query Goodrate, and Item Goodrate in Taobao A/B testing. It is deployed in Taobao's production relevance system, serving live traffic. To our knowledge, it is among the first single-pass RAG solutions for e-commerce relevance, turning noisy external signals into reliable gains without added online complexity.",
    "summary": "论文研究电商搜索中如何动态利用噪声外部知识提升查询-商品相关性建模。核心方法是基于分组相对策略优化的强化学习框架，通过训练两个策略组（无外部上下文vs单检索块）和后验驱动的组间优势缩放，自适应学习何时信任检索知识或回退到参数化知识。",
    "translation": "DyKnow-RAG：面向电商搜索相关性中噪声检索增强生成的动态知识利用强化框架",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及搜索相关性这一核心领域，属于Direct LLM Applications范畴。论文提出的动态知识利用和噪声处理框架可直接应用于电商搜索系统，提升检索增强生成在真实噪声环境下的性能，对搜索质量优化具有明确实用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对电商搜索相关性中的动态知识利用问题，核心方法结合了强化学习和RAG框架，与推荐系统、搜索领域高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11100v1": {
    "title": "HoMer: Addressing Heterogeneities by Modeling Sequential and Set-wise Contexts for CTR Prediction",
    "url": "https://www.alphaxiv.org/abs/2510.11100v1",
    "arxiv_id": "2510.11100v1",
    "authors": "Shuwei Chen, Jiajun Cui, Zhengqi Xu, Fan Zhang, Jiangke Fan, Teng Zhang, Xingxing Wang",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-13 07:47:03",
    "ori_summary": "Click-through rate (CTR) prediction, which models behavior sequence and non-sequential features (e.g., user/item profiles or cross features) to infer user interest, underpins industrial recommender systems. However, most methods face three forms of heterogeneity that degrade predictive performance: (i) Feature Heterogeneity persists when limited sequence side features provide less granular interest representation compared to extensive non-sequential features, thereby impairing sequence modeling performance; (ii) Context Heterogeneity arises because a user's interest in an item will be influenced by other items, yet point-wise prediction neglects cross-item interaction context from the entire item set; (iii) Architecture Heterogeneity stems from the fragmented integration of specialized network modules, which compounds the model's effectiveness, efficiency and scalability in industrial deployments. To tackle the above limitations, we propose HoMer, a Homogeneous-Oriented TransforMer for modeling sequential and set-wise contexts. First, we align sequence side features with non-sequential features for accurate sequence modeling and fine-grained interest representation. Second, we shift the prediction paradigm from point-wise to set-wise, facilitating cross-item interaction in a highly parallel manner. Third, HoMer's unified encoder-decoder architecture achieves dual optimization through structural simplification and shared computation, ensuring computational efficiency while maintaining scalability with model size. Without arduous modification to the prediction pipeline, HoMer successfully scales up and outperforms our industrial baseline by 0.0099 in the AUC metric, and enhances online business metrics like CTR/RPM by 1.99%/2.46%. Additionally, HoMer saves 27% of GPU resources via preliminary engineering optimization, further validating its superiority and practicality.",
    "summary": "论文研究推荐系统中CTR预测面临的三种异构性问题。核心方法是提出统一的Transformer编码器-解码器架构，通过特征对齐、集合级预测和统一结构设计来同时解决特征、上下文和架构异构性。",
    "translation": "HoMer：通过建模序列和集合上下文处理异构性以进行点击率预测",
    "relevance_score": 9,
    "reasoning": "该论文直接针对CTR预测中的异构数据处理，这是推荐系统和广告领域的核心问题。通过建模序列和集合上下文来处理异构性，类似于VLM处理多模态数据的方法，具有直接应用于推荐系统排序和广告点击率预测的潜力。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统的核心CTR预测问题，提出统一的Transformer架构处理异构特征和上下文，与关注领域高度契合。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11066v1": {
    "title": "Decoupled Multimodal Fusion for User Interest Modeling in Click-Through Rate Prediction",
    "url": "https://www.alphaxiv.org/abs/2510.11066v1",
    "arxiv_id": "2510.11066v1",
    "authors": "Alin Fan, Hanqing Li, Sihan Lu, Jingsong Yuan, Jiandong Zhang",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 07:06:26",
    "ori_summary": "Modern industrial recommendation systems improve recommendation performance by integrating multimodal representations from pre-trained models into ID-based Click-Through Rate (CTR) prediction frameworks. However, existing approaches typically adopt modality-centric modeling strategies that process ID-based and multimodal embeddings independently, failing to capture fine-grained interactions between content semantics and behavioral signals. In this paper, we propose Decoupled Multimodal Fusion (DMF), which introduces a modality-enriched modeling strategy to enable fine-grained interactions between ID-based collaborative representations and multimodal representations for user interest modeling. Specifically, we construct target-aware features to bridge the semantic gap across different embedding spaces and leverage them as side information to enhance the effectiveness of user interest modeling. Furthermore, we design an inference-optimized attention mechanism that decouples the computation of target-aware features and ID-based embeddings before the attention layer, thereby alleviating the computational bottleneck introduced by incorporating target-aware features. To achieve comprehensive multimodal integration, DMF combines user interest representations learned under the modality-centric and modality-enriched modeling strategies. Offline experiments on public and industrial datasets demonstrate the effectiveness of DMF. Moreover, DMF has been deployed on the product recommendation system of the international e-commerce platform Lazada, achieving relative improvements of 5.30% in CTCVR and 7.43% in GMV with negligible computational overhead.",
    "summary": "论文研究多模态推荐系统中内容语义与行为信号之间的细粒度交互问题，核心思想是通过目标感知特征桥接不同嵌入空间的语义鸿沟，并设计解耦注意力机制实现模态增强的用户兴趣建模。",
    "translation": "用于点击率预测中用户兴趣建模的解耦多模态融合",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及推荐系统的核心领域进展，专注于CTR预测中的用户兴趣建模，这是推荐系统的核心任务。解耦多模态融合方法可能处理异构数据（如用户行为序列和上下文特征），这与VLM类比异构数据的理念高度相关，并且可能借鉴Transformer架构的多模态融合技术。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统中的多模态融合问题，提出解耦式融合方法增强用户兴趣建模，与核心领域进展和直接LLM应用高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11056v1": {
    "title": "From Reasoning LLMs to BERT: A Two-Stage Distillation Framework for Search Relevance",
    "url": "https://www.alphaxiv.org/abs/2510.11056v1",
    "arxiv_id": "2510.11056v1",
    "authors": "Runze Xia, Yupeng Ji, Yuxi Zhou, Haodong Liu, Teng Zhang, Piji Li",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-13 06:46:43",
    "ori_summary": "Query-service relevance prediction in e-commerce search systems faces strict latency requirements that prevent the direct application of Large Language Models (LLMs). To bridge this gap, we propose a two-stage reasoning distillation framework to transfer reasoning capabilities from a powerful teacher LLM to a lightweight, deployment-friendly student model. In the first stage, we address the limitations of general-purpose LLMs by constructing a domain-adapted teacher model. This is achieved through a three-step process: domain-adaptive pre-training to inject platform knowledge, supervised fine-tuning to elicit reasoning skills, and preference optimization with a multi-dimensional reward model to ensure the generation of reliable and preference-aligned reasoning paths. This teacher can then automatically annotate massive query-service pairs from search logs with both relevance labels and reasoning chains. In the second stage, to address the challenges of architectural heterogeneity in standard distillation, we introduce Contrastive Reasoning Self-Distillation (CRSD). By modeling the behavior of the same student model under \"standard\" and \"reasoning-augmented\" inputs as a teacher-student relationship, CRSD enables the lightweight model to internalize the teacher's complex decision-making mechanisms without needing the explicit reasoning path at inference. Offline evaluations and online A/B testing in the Meituan search advertising system demonstrate that our framework achieves significant improvements across multiple metrics, validating its effectiveness and practical value.",
    "summary": "论文研究电商搜索中查询-服务相关性预测的延迟限制问题，核心思想是通过领域适应的教师LLM生成推理路径，再通过对比推理自蒸馏将复杂决策机制迁移到轻量级学生模型中。",
    "translation": "从推理大语言模型到BERT：面向搜索相关性的两阶段蒸馏框架",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及LLM技术在搜索领域的应用，提出了从LLM到BERT的知识蒸馏框架来提升搜索相关性。这属于'Direct LLM Applications'范畴，通过知识蒸馏技术将LLM的推理能力迁移到更高效的BERT模型中，对搜索排序具有明确的实用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对搜索系统中的延迟限制问题，提出两阶段蒸馏框架将LLM推理能力迁移到轻量级模型，完美契合搜索领域和LLM应用的核心关注点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11003v1": {
    "title": "FBS Model-based Maintenance Record Accumulation for Failure-Cause Inference in Manufacturing Systems",
    "url": "https://www.alphaxiv.org/abs/2510.11003v1",
    "arxiv_id": "2510.11003v1",
    "authors": "Takuma Fujiu, Sho Okazaki, Kohei Kaminishi, Yuji Nakata, Shota Hamamoto, Kenshin Yokose, Tatsunori Hara, Yasushi Umeda, Jun Ota",
    "categories": "cs.AI, cs.IR",
    "pub_date": "2025-10-13 04:37:40",
    "ori_summary": "In manufacturing systems, identifying the causes of failures is crucial for maintaining and improving production efficiency. In knowledge-based failure-cause inference, it is important that the knowledge base (1) explicitly structures knowledge about the target system and about failures, and (2) contains sufficiently long causal chains of failures. In this study, we constructed Diagnostic Knowledge Ontology and proposed a Function-Behavior-Structure (FBS) model-based maintenance-record accumulation method based on it. Failure-cause inference using the maintenance records accumulated by the proposed method showed better agreement with the set of candidate causes enumerated by experts, especially in difficult cases where the number of related cases is small and the vocabulary used differs. In the future, it will be necessary to develop inference methods tailored to these maintenance records, build a user interface, and carry out validation on larger and more diverse systems. Additionally, this approach leverages the understanding and knowledge of the target in the design phase to support knowledge accumulation and problem solving during the maintenance phase, and it is expected to become a foundation for knowledge sharing across the entire engineering chain in the future.",
    "summary": "",
    "translation": "基于FBS模型的制造系统故障原因推断维护记录累积方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于制造系统中的故障诊断和维护记录管理，属于工业工程和制造领域的具体应用。这与推荐系统、搜索或广告的核心技术领域完全无关，也不涉及LLM、Transformer架构或异构数据建模等任何相关技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10978v1": {
    "title": "Does LLM Focus on the Right Words? Diagnosing Language Bias in LLM-based Recommenders",
    "url": "https://www.alphaxiv.org/abs/2510.10978v1",
    "arxiv_id": "2510.10978v1",
    "authors": "Bohao Wang, Jiawei Chen, Feng Liu, Changwang Zhang, Jun Wang, Canghong Jin, Chun Chen, Can Wang",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 03:35:26",
    "ori_summary": "Large language models (LLMs), owing to their extensive open-domain knowledge and semantic reasoning capabilities, have been increasingly integrated into recommender systems (RS). However, a substantial gap remains between the pre-training objectives of LLMs and the specific requirements of recommendation tasks. To address this gap, supervised fine-tuning (SFT) is commonly performed on specially curated recommendation datasets to further enhance their predictive ability. Despite its success, SFT exhibits a critical limitation: it induces Language Bias, whereby the model over-relies on auxiliary tokens-such as task descriptions and prefix-generated tokens-while underutilizing core user interaction tokens that encode user-specific preferences. This bias not only undermines recommendation accuracy but also raises unfairness concerns. To address this issue, we propose Group Distributionally Robust Optimization-based Tuning (GDRT), a novel fine-tuning paradigm that enforces consistent model performance across token groups with varying degrees of relevance to auxiliary tokens. By adaptively upweighting underperforming groups, typically those weakly correlated with auxiliary tokens, GDRT shifts the model's attention from superficial auxiliary cues to informative user interaction tokens, thereby mitigating language bias. Extensive experiments conducted on three public datasets demonstrate that GDRT effectively mitigates language bias, yielding substantial improvements in recommendation accuracy (with an average NDCG@10 gain of 24.29%) and significantly enhancing recommendation fairness.",
    "summary": "",
    "translation": "LLM是否关注正确的词语？诊断基于LLM的推荐系统中的语言偏见",
    "relevance_score": 6,
    "reasoning": "该论文直接涉及LLM在推荐系统中的应用，属于'Direct LLM Applications'范畴。然而，它主要关注语言偏见诊断，这偏向于公平性和评估方面，与排除的'公平性、伦理'等非技术主题有一定重叠，因此相关性受到限制。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.10955v1": {
    "title": "HatLLM: Hierarchical Attention Masking for Enhanced Collaborative Modeling in LLM-based Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.10955v1",
    "arxiv_id": "2510.10955v1",
    "authors": "Yu Cui, Feng Liu, Jiawei Chen, Canghong Jin, Xingyu Lou, Changwang Zhang, Jun Wang, Yuegang Sun, Can Wang",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 03:05:03",
    "ori_summary": "Recent years have witnessed a surge of research on leveraging large language models (LLMs) for sequential recommendation. LLMs have demonstrated remarkable potential in inferring users' nuanced preferences through fine-grained semantic reasoning. However, they also exhibit a notable limitation in effectively modeling collaborative signals, i.e., behavioral correlations inherent in users' historical interactions. Our empirical analysis further reveals that the attention mechanisms in LLMs tend to disproportionately focus on tokens within the same item, thereby impeding the capture of cross-item correlations. To address this limitation, we propose a novel hierarchical attention masking strategy for LLM-based recommendation, termed HatLLM. Specifically, in shallow layers, HatLLM masks attention between tokens from different items, facilitating intra-item semantic understanding; in contrast, in deep layers, HatLLM masks attention within items, thereby compelling the model to capture cross-item correlations. This progressive, layer-wise approach enables LLMs to jointly model both token-level and item-level dependencies. Extensive experiments on three real-world datasets demonstrate that HatLLM achieves significant performance gains (9.13% on average) over existing LLM-based methods.",
    "summary": "论文研究LLM在序列推荐中难以有效建模用户历史交互间的协同信号问题，核心方法是提出分层注意力掩码策略：浅层屏蔽不同物品间注意力以理解物品内语义，深层屏蔽物品内注意力以强制捕捉跨物品相关性。",
    "translation": "HatLLM：基于LLM的推荐系统中用于增强协同建模的分层注意力掩码机制",
    "relevance_score": 9,
    "reasoning": "该论文直接针对LLM在推荐系统中的应用，提出了分层注意力掩码机制来增强协同建模能力。这属于'直接LLM应用'和'使能Transformer技术'范畴，分层注意力机制可以更好地处理用户-项目交互序列，在推荐系统中具有明确的实用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM在推荐系统中的关键局限——协同信号建模，提出了创新的分层注意力掩码方法，完美契合核心领域进展和直接LLM应用两个重点方向。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.10920v1": {
    "title": "Comparative Explanations via Counterfactual Reasoning in Recommendations",
    "url": "https://www.alphaxiv.org/abs/2510.10920v1",
    "arxiv_id": "2510.10920v1",
    "authors": "Yi Yu, Zhenxing Hu",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-13 02:31:03",
    "ori_summary": "Explainable recommendation through counterfactual reasoning seeks to identify the influential aspects of items in recommendations, which can then be used as explanations. However, state-of-the-art approaches, which aim to minimize changes in product aspects while reversing their recommended decisions according to an aggregated decision boundary score, often lead to factual inaccuracies in explanations. To solve this problem, in this work we propose a novel method of Comparative Counterfactual Explanations for Recommendation (CoCountER). CoCountER creates counterfactual data based on soft swap operations, enabling explanations for recommendations of arbitrary pairs of comparative items. Empirical experiments validate the effectiveness of our approach.",
    "summary": "论文研究推荐系统中反事实解释的事实准确性问题，核心思想是通过软交换操作生成反事实数据，为任意对比物品对提供可解释的推荐理由。",
    "translation": "基于反事实推理的推荐系统对比解释",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及推荐系统的核心领域进展，专注于解释性方法。反事实推理技术可以生成对比解释（例如'如果你喜欢A而不是B，那么...'），这在推荐系统中对于提高用户信任和透明度具有直接应用价值。虽然不直接涉及LLM技术，但此类解释方法可以增强推荐系统的用户体验和可解释性。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对推荐系统的可解释性核心问题，提出基于反事实推理的新方法，属于推荐系统领域的重要进展。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11713v1": {
    "title": "Are Large Reasoning Models Interruptible?",
    "url": "https://www.alphaxiv.org/abs/2510.11713v1",
    "arxiv_id": "2510.11713v1",
    "authors": "Tsung-Han Wu, Mihran Miroyan, David M. Chan, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-13 17:59:35",
    "ori_summary": "Large Reasoning Models (LRMs) excel at complex reasoning but are traditionally evaluated in static, \"frozen world\" settings: model responses are assumed to be instantaneous, and the context of a request is presumed to be immutable over the duration of the response. While generally true for short-term tasks, the \"frozen world\" assumption breaks down in modern reasoning tasks such as assistive programming, where models may take hours to think through problems and code may change dramatically from the time the model starts thinking to the model's final output. In this work, we challenge the frozen world assumption and evaluate LRM robustness under two realistic dynamic scenarios: interruptions, which test the quality of the model's partial outputs on a limited budget, and dynamic context, which tests model adaptation to in-flight changes. Across mathematics and programming benchmarks that require long-form reasoning, static evaluations consistently overestimate robustness: even state-of-the-art LRMs, which achieve high accuracy in static settings, can fail unpredictably when interrupted or exposed to changing context, with performance dropping by up to 60% when updates are introduced late in the reasoning process. Our analysis further reveals several novel failure modes, including reasoning leakage, where models fold the reasoning into their final answer when interrupted; panic, where under time pressure models abandon reasoning entirely and return incorrect answers; and self-doubt, where performance degrades while incorporating updated information.",
    "summary": "",
    "translation": "大型推理模型是否可中断？",
    "relevance_score": 3,
    "reasoning": "该论文探讨推理模型的可中断性，这属于LLM推理能力的基础研究。虽然推理能力对搜索和推荐系统中的复杂查询处理有潜在应用，但论文焦点更偏向通用推理能力而非具体的RecSys/Search/Ads应用，且未明确涉及Transformer架构改进或多模态建模等核心关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11701v1": {
    "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.11701v1",
    "arxiv_id": "2510.11701v1",
    "authors": "Zhaochen Yu, Ling Yang, Jiaru Zou, Shuicheng Yan, Mengdi Wang",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 17:57:15",
    "ori_summary": "Recently, the emergence of agentic RL has showcased that RL could also effectively improve the agentic reasoning ability of LLMs, yet the key design principles and optimal practices remain unclear. In this work, we conduct a comprehensive and systematic investigation to demystify reinforcement learning in agentic reasoning from three key perspectives: data, algorithm, and reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic trajectories with real end-to-end tool-use trajectories yields a far stronger SFT initialization; high-diversity, model-aware datasets sustain exploration and markedly improve RL performance. (ii) Exploration-friendly techniques are crucial for agentic RL, such as clip higher, overlong reward shaping, and maintaining adequate policy entropy could improve the training efficiency. (iii) A deliberative strategy with fewer tool calls outperforms frequent tool calls or verbose self-reasoning, improving tool efficiency and final accuracy. Together, these simple practices consistently enhance agentic reasoning and training efficiency, achieving strong results on challenging benchmarks with smaller models, and establishing a practical baseline for future agentic RL research. Beyond these empirical insights, we further contribute a high-quality, real end-to-end agentic SFT dataset along with a high-quality RL dataset, and demonstrate the effectiveness of our insights in boosting the agentic reasoning ability of LLMs across four challenging benchmarks, including AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes, 4B-sized models could also achieve superior agentic reasoning performance compared to 32B-sized models. Code and models: https://github.com/Gen-Verse/Open-AgentRL",
    "summary": "",
    "translation": "揭秘智能体推理中的强化学习",
    "relevance_score": 2,
    "reasoning": "该论文聚焦于强化学习在智能体推理中的应用，属于纯粹的强化学习研究领域。虽然强化学习在推荐系统中偶有应用，但该标题没有明确指向推荐系统、搜索或广告的具体应用场景，也没有涉及LLM、Transformer架构或异构数据建模等核心技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11696v1": {
    "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.11696v1",
    "arxiv_id": "2510.11696v1",
    "authors": "Wei Huang, Yi Ge, Shuai Yang, Yicheng Xiao, Huizi Mao, Yujun Lin, Hanrong Ye, Sifei Liu, Ka Chun Cheung, Hongxu Yin, Yao Lu, Xiaojuan Qi, Song Han, Yukang Chen",
    "categories": "cs.LG, cs.CL, cs.CV",
    "pub_date": "2025-10-13 17:55:09",
    "ori_summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for large language models (LLMs). While RL is essential for LLMs' reasoning capabilities, it is resource-intensive, requiring substantial GPU memory and long rollout durations. QeRL addresses these issues by combining NVFP4 quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL while reducing memory overhead. Beyond efficiency, our findings show that quantization noise increases policy entropy, enhancing exploration, and enabling the discovery of better strategies during RL. To further optimize exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism, which dynamically adjusts noise during training. Experiments demonstrate that QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is the first framework to enable RL training of a 32B LLM on a single H100 80GB GPU, while delivering overall speedups for RL training. It also achieves faster reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while matching the performance of full-parameter fine-tuning on mathematical benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These results establish QeRL as an efficient and effective framework for RL training in LLMs.",
    "summary": "该论文研究LLM强化学习训练的资源效率问题，核心思想是通过NVFP4量化和LoRA结合加速RL训练，并利用量化噪声增加策略熵来增强探索，同时引入自适应量化噪声机制动态优化训练过程。",
    "translation": "QeRL：超越效率——面向大语言模型的量化增强强化学习",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM的量化技术（Enabling LLM Tech）和强化学习（Direct LLM Applications），两者都与推荐/搜索/广告系统高度相关。量化可降低LLM推理成本，使实时推荐成为可能；强化学习可直接用于在线学习、探索-利用权衡等关键推荐场景。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文将量化技术与强化学习结合，不仅提升效率还增强探索能力，直接适用于LLM训练优化，对推荐和搜索系统的模型训练具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11695v1": {
    "title": "When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents",
    "url": "https://www.alphaxiv.org/abs/2510.11695v1",
    "arxiv_id": "2510.11695v1",
    "authors": "Lingfei Qian, Xueqing Peng, Yan Wang, Vincent Jim Zhang, Huan He, Hanley Smith, Yi Han, Yueru He, Haohang Li, Yupeng Cao, Yangyang Yu, Alejandro Lopez-Lira, Peng Lu, Jian-Yun Nie, Guojun Xiong, Jimin Huang, Sophia Ananiadou",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 17:54:09",
    "ori_summary": "Although Large Language Model (LLM)-based agents are increasingly used in financial trading, it remains unclear whether they can reason and adapt in live markets, as most studies test models instead of agents, cover limited periods and assets, and rely on unverified data. To address these gaps, we introduce Agent Market Arena (AMA), the first lifelong, real-time benchmark for evaluating LLM-based trading agents across multiple markets. AMA integrates verified trading data, expert-checked news, and diverse agent architectures within a unified trading framework, enabling fair and continuous comparison under real conditions. It implements four agents, including InvestorAgent as a single-agent baseline, TradeAgent and HedgeFundAgent with different risk styles, and DeepFundAgent with memory-based reasoning, and evaluates them across GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and Gemini-2.0-flash. Live experiments on both cryptocurrency and stock markets demonstrate that agent frameworks display markedly distinct behavioral patterns, spanning from aggressive risk-taking to conservative decision-making, whereas model backbones contribute less to outcome variation. AMA thus establishes a foundation for rigorous, reproducible, and continuously evolving evaluation of financial reasoning and trading intelligence in LLM-based agents.",
    "summary": "",
    "translation": "当智能体交易时：面向LLM智能体的实时多市场交易基准",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM智能体在金融交易领域的基准测试，属于特定领域应用而非核心推荐系统、搜索或广告技术。虽然涉及LLM智能体，但交易基准与推荐/搜索/广告的排名、检索或用户建模问题缺乏直接关联，潜在应用场景不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11693v1": {
    "title": "Scaling Language-Centric Omnimodal Representation Learning",
    "url": "https://www.alphaxiv.org/abs/2510.11693v1",
    "arxiv_id": "2510.11693v1",
    "authors": "Chenghao Xiao, Hou Pong Chan, Hao Zhang, Weiwen Xu, Mahani Aljunied, Yu Rong",
    "categories": "cs.CL, cs.AI, cs.CV",
    "pub_date": "2025-10-13 17:53:52",
    "ori_summary": "Recent multimodal embedding approaches leveraging multimodal large language models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising results, yet the underlying reasons behind their superiority remain underexplored. This work argues that a crucial advantage of MLLM-based approaches stems from implicit cross-modal alignment achieved during generative pretraining, where the language decoder learns to exploit multimodal signals within a shared representation space for generating unimodal outputs. Through analysis of anisotropy and kernel similarity structure, we empirically confirm that latent alignment emerges within MLLM representations, allowing CL to serve as a lightweight refinement stage. Leveraging this insight, we propose a Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive experiments across diverse backbones and benchmarks demonstrate its effectiveness, achieving state-of-the-art performance across modalities. Furthermore, we identify a Generation-Representation Scaling Law (GRSL), showing that the representational capabilities gained through contrastive refinement scales positively with the MLLM's generative capabilities. This suggests that improving generative abilities evolves as an effective paradigm for enhancing representation quality. We provide a theoretical explanation of GRSL, which formally links the MLLM's generative quality to the upper bound on its representation performance, and validate it on a challenging, low-resource visual-document retrieval task, showing that continual generative pretraining before CL can further enhance the potential of a model's embedding capabilities. Codes, models, and resources are available at https://github.com/LCO-Embedding/LCO-Embedding.",
    "summary": "研究多模态表示学习中MLLM方法优越性的根本原因；核心发现是生成式预训练在共享表示空间内隐式实现了跨模态对齐，使得对比学习仅需轻量微调，并提出生成能力与表示性能正相关的缩放定律。",
    "translation": "扩展以语言为中心的全模态表示学习",
    "relevance_score": 8,
    "reasoning": "该论文专注于多模态表示学习，与'VLM Analogy for Heterogeneous Data'高度相关，可将用户行为序列、上下文特征等异构数据视为不同模态进行统一建模。这种全模态学习方法在推荐系统和搜索中有直接应用潜力，能够更好地理解用户意图和内容语义。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文揭示了生成式预训练在多模态表示学习中的核心优势，并提出生成-表示缩放定律，这对推荐系统中多模态特征统一建模具有直接指导意义。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11683v1": {
    "title": "Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11683v1",
    "arxiv_id": "2510.11683v1",
    "authors": "Nianyi Lin, Jiajie Zhang, Lei Hou, Juanzi Li",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-13 17:47:50",
    "ori_summary": "A key challenge in applying reinforcement learning (RL) to diffusion large language models (dLLMs) lies in the intractability of their likelihood functions, which are essential for the RL objective, necessitating corresponding approximation in each training step. While existing methods approximate the log-likelihoods by their evidence lower bounds (ELBOs) via customized Monte Carlo (MC) sampling, the forward computational graphs of all MC samples need to be retained for the gradient computation of non-linear terms in the RL objective, resulting in significant memory overhead. This constraint restricts feasible sample sizes, leading to imprecise likelihood approximations and ultimately distorting the RL objective. To overcome this limitation, we propose \\emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient RL algorithm that maximizes a specially constructed lower bound of the ELBO-based objective. This lower bound is carefully designed to satisfy two key properties: (1) Linearity: it is formulated in a linear sum where each term depends only on a single MC sample, thereby enabling gradient accumulation across samples and ensuring constant memory usage; (2) Equivalence: Both the value and gradient of this lower bound are equal to those of the ELBO-based objective in on-policy training, making it also an effective approximation for the original RL objective. These properties allow BGPO to adopt a large MC sample size, resulting in more accurate likelihood approximations and improved RL objective estimation, which in turn leads to enhanced performance. Experiments show that BGPO significantly outperforms previous RL algorithms for dLLMs in math problem solving, code generation, and planning tasks.",
    "summary": "",
    "translation": "边界引导策略优化：面向扩散大语言模型的高效记忆强化学习",
    "relevance_score": 2,
    "reasoning": "该论文主要关注扩散大语言模型的强化学习效率优化，属于纯粹的LLM技术范畴。虽然提到了强化学习和效率优化，但没有明确展示与推荐系统、搜索或广告的直接关联，且扩散模型在这些领域的应用尚不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11652v1": {
    "title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems",
    "url": "https://www.alphaxiv.org/abs/2510.11652v1",
    "arxiv_id": "2510.11652v1",
    "authors": "Xin Gui, King Zhu, JinCheng Ren, Qianben Chen, Zekun Moore Wang, Yizhi LI, Xinpeng Liu, Xiaowan Li, Wenli Ren, Linyu Miao, Tianrui Qin, Ziqi Shu, He Zhu, Xiangru Tang, Dingfeng Shi, Jiaheng Liu, Yuchen Eleanor Jiang, Minghao Liu, Ge Zhang, Wangchunshu Zhou",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 17:30:36",
    "ori_summary": "In recent years, the research focus of large language models (LLMs) and agents has shifted increasingly from demonstrating novel capabilities to complex reasoning and tackling challenging tasks. However, existing evaluations focus mainly on math/code contests or general tasks, while existing multi-domain academic benchmarks lack sufficient reasoning depth, leaving the field without a rigorous benchmark for high-level reasoning. To fill this gap, we introduce the Acadreason benchmark, designed to evaluate the ability of LLMs and agents to acquire and reason over academic knowledge. It consists of 50 expert-annotated academic problems across five high-reasoning domains, including computer science, economics, law, mathematics, and philosophy. All questions are sourced from top-tier publications in recent years and undergo rigorous annotation and quality control to ensure they are both challenging and answerable. We conduct systematic evaluations of over 10 mainstream LLMs and agents. The results show that most LLMs scored below 20 points, with even the cutting-edge GPT-5 achieving only 16 points. While agents achieved higher scores, none exceeded 40 points. This demonstrates the current capability gap between LLMs and agents in super-intelligent academic research tasks and highlights the challenges of Acadreason.",
    "summary": "",
    "translation": "ACADREASON：利用学术研究问题探索推理模型的极限",
    "relevance_score": 2,
    "reasoning": "该论文主要关注推理模型的基准测试和能力评估，这属于纯粹的NLP评估范畴，与推荐系统、搜索或广告的核心技术进展无关。虽然推理能力在理论上可能对某些搜索应用有帮助，但论文本身聚焦于学术推理问题的基准构建，缺乏明确的推荐/搜索/广告应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11620v1": {
    "title": "Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan Aggregation",
    "url": "https://www.alphaxiv.org/abs/2510.11620v1",
    "arxiv_id": "2510.11620v1",
    "authors": "Siheng Xiong, Ali Payani, Faramarz Fekri",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 17:02:41",
    "ori_summary": "Inference-time scaling enhances the reasoning ability of a language model (LM) by extending its chain-of-thought (CoT). However, existing approaches typically generate the entire reasoning chain in a single forward pass, which often leads to CoT derailment, i.e., the reasoning trajectory drifting off course due to compounding errors. This problem is particularly severe for smaller LMs with long CoTs due to their limited capacity. To address this, we analyze raw long CoTs and uncover a reasoning hierarchy consisting of planning and execution steps. Our analysis reveals that most reasoning errors stem from incorrect planning. Motivated by this observation, we propose Multi-Path Plan Aggregation (MPPA), a framework that augments single-pass reasoning with plan exploration and aggregation. Following a variable interval schedule based on the token position, MPPA generates multiple candidate plans and aggregates them into a refined planning step. To maintain efficiency, we adopt a minimal design in which the base LM serves as the primary policy, while a lightweight LoRA module implements the plan aggregation policy. We further observe that outcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K tokens). To overcome this, we introduce online Step-DPO, a process-level preference optimization scheme that leverages Twisted Sequential Monte Carlo (TSMC) to provide scalable stepwise supervision using small LMs. This yields more efficient training, improved stability, and higher accuracy. Extensive experiments on challenging math, science, and logical reasoning benchmarks demonstrate that, with only 10% SFT data and 5% of preference pairs, our method outperforms both the DeepSeek-R1 distillation baseline and the outcome-reward RL baseline across multiple base models and tasks.",
    "summary": "论文研究长链思维推理中的规划错误问题，核心思想是通过多路径规划探索与聚合机制，结合过程级偏好优化，提升语言模型在长序列推理任务中的规划准确性。",
    "translation": "通过多路径规划聚合增强长链思维推理",
    "relevance_score": 8,
    "reasoning": "该论文属于'Enabling LLM Tech'范畴，专注于提升LLM的长链推理能力。在推荐系统和搜索领域，这种增强的推理能力可以显著改善复杂用户查询的理解、多步推荐逻辑的生成，以及广告投放策略的优化决策过程。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的多路径规划聚合框架和过程级优化方法，能够直接提升LLM在长序列推理任务中的表现，对推荐和搜索系统中的复杂推理场景具有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11618v1": {
    "title": "StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11618v1",
    "arxiv_id": "2510.11618v1",
    "authors": "Zehao Chen, Rong Pan, Haoran Li",
    "categories": "cs.CL, cs.MA",
    "pub_date": "2025-10-13 16:57:32",
    "ori_summary": "Human writers often begin their stories with an overarching mental scene, where they envision the interactions between characters and their environment. Inspired by this creative process, we propose a novel approach to long-form story generation, termed hybrid bottom-up long-form story generation, using multi-agent simulations. In our method, agents interact within a dynamic sandbox environment, where their behaviors and interactions with one another and the environment generate emergent events. These events form the foundation for the story, enabling organic character development and plot progression. Unlike traditional top-down approaches that impose rigid structures, our hybrid bottom-up approach allows for the natural unfolding of events, fostering more spontaneous and engaging storytelling. The system is capable of generating stories exceeding 10,000 words while maintaining coherence and consistency, addressing some of the key challenges faced by current story generation models. We achieve state-of-the-art performance across several metrics. This approach offers a scalable and innovative solution for creating dynamic, immersive long-form stories that evolve organically from agent-driven interactions.",
    "summary": "",
    "translation": "StoryBox：基于大型语言模型的混合自底向上长文本故事生成的协作多智能体仿真",
    "relevance_score": 2,
    "reasoning": "该论文主要关注使用LLM进行长文本故事生成的多智能体仿真，这属于纯粹的LLM内容生成应用。虽然涉及多智能体协作，但缺乏与推荐系统、搜索或广告领域的明确关联，属于AIGC和内容生成范畴，与当前关注点不匹配。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11615v1": {
    "title": "LLM-Oriented Token-Adaptive Knowledge Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.11615v1",
    "arxiv_id": "2510.11615v1",
    "authors": "Xurong Xie, Zhucun Xue, Jiafu Wu, Jian Li, Yabiao Wang, Xiaobin Hu, Yong Liu, Jiangning Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 16:55:07",
    "ori_summary": "Knowledge distillation (KD) is a key technique for compressing large-scale language models (LLMs), yet prevailing logit-based methods typically employ static strategies that are misaligned with the dynamic learning process of student models. These methods typically treat all tokens indiscriminately and apply a single, fixed temperature, resulting in suboptimal knowledge transfer. To address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge Distillation (AdaKD), a novel framework that adapts the distillation process to the real-time learning state of each token. AdaKD consists of two synergistic modules driven by a unified token difficulty metric. First, our Loss-Driven Adaptive Token Focusing (LATF) module dynamically adjusts the distillation focus by monitoring the student's learning stability, concentrating computational resources on the most valuable tokens at each training phase. Second, we introduce Inverse Difficulty Temperature Scaling (IDTS), a counterintuitive yet effective token-level temperature strategy. It employs low temperatures for difficult tokens for targeted error correction, and high temperatures for easy tokens to encourage students to learn from the teacher's complete and smooth output distribution, thereby enhancing generalization. As a plug-and-play framework, AdaKD can consistently improve the performance of various distillation methods on multiple model architectures and benchmarks.",
    "summary": "论文研究LLM知识蒸馏中静态策略与动态学习过程不匹配的问题，核心方法是提出基于令牌难度的自适应蒸馏框架，通过动态聚焦和反向温度缩放实现针对性知识传递。",
    "translation": "面向大语言模型的令牌自适应知识蒸馏",
    "relevance_score": 8,
    "reasoning": "该论文属于'赋能LLM技术'类别，专注于知识蒸馏这一核心LLM技术。令牌自适应知识蒸馏可以显著提升LLM在推荐、搜索和广告系统中的部署效率，通过减少模型大小和推理延迟，同时保持性能，这对于大规模实时服务至关重要。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出面向LLM的知识蒸馏新方法，通过动态调整蒸馏策略提升模型压缩效果，直接适用于推荐系统等领域的模型优化。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.11602v1": {
    "title": "Deconstructing Attention: Investigating Design Principles for Effective Language Modeling",
    "url": "https://www.alphaxiv.org/abs/2510.11602v1",
    "arxiv_id": "2510.11602v1",
    "authors": "Huiyin Xue, Nafise Sadat Moosavi, Nikolaos Aletras",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-13 16:42:14",
    "ori_summary": "The success of Transformer language models is widely credited to their dot-product attention mechanism, which interweaves a set of key design principles: mixing information across positions (enabling multi-token interactions), sequence-dependent activations (where attention weights adapt to each input), a specific mathematical form (dot-product similarities plus softmax weighting), and coupling of queries and keys to evolving hidden states (grounding attention in the current layer). However, the necessity of each of these principles remains largely untested. In this work, we systematically deconstruct attention by designing controlled variants that selectively relax these principles, applied both uniformly across all layers and in hybrid architectures where only some layers retain standard attention. Our empirical analysis reveals that mechanisms for mixing tokens are indispensable, as their absence collapses models to near-random behavior, while the exact mathematical form and sequence dependency can be substantially relaxed, especially when preserved in just a subset of layers. Surprisingly, even variants that fail in isolation can achieve robust performance when interleaved with standard attention, highlighting a cooperative effect. These findings deepen our understanding of what truly underpins attention's effectiveness and open new avenues for simplifying language models without sacrificing performance.",
    "summary": "",
    "translation": "解构注意力机制：探究高效语言建模的设计原则",
    "relevance_score": 8,
    "reasoning": "该论文直接研究注意力机制的设计原则，这属于'Enabling Transformer Tech'范畴，涉及Transformer架构的核心组件优化。改进的注意力机制设计可以直接应用于推荐系统和搜索中的序列建模、用户行为序列处理，以及广告中的上下文理解，提升模型效率和性能。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.11598v1": {
    "title": "MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11598v1",
    "arxiv_id": "2510.11598v1",
    "authors": "Bo Cheng, Xu Wang, Jinda Liu, Yi Chang, Yuan Wu",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 16:37:40",
    "ori_summary": "Low-Rank Adaptation (LoRA) has emerged as one of the most widely used parameter-efficient fine-tuning (PEFT) methods for adapting large language models (LLMs) to downstream tasks. While highly effective in single-task settings, it struggles to efficiently leverage inter-task knowledge in complex multi-task learning scenarios, often requiring substantial task-specific data to achieve optimal performance. To address this limitation, we introduce MeTA-LoRA, a two-stage optimization framework that significantly improves data efficiency in multi-task adaptation. In the first stage, task-specific LoRA adapters are learned using only a few samples from each involved dataset, enabling rapid adaptation without large-scale supervision. In the second stage, the shared LoRA adapter is updated by aggregating gradients from multiple tasks to promote knowledge transfer across tasks, further reducing data usage by leveraging common patterns. In both multi-task learning and multilingual learning scenarios, our method matches or surpasses the performance of traditional full-data LoRA fine-tuning approaches, while using significantly less task-specific data.",
    "summary": "",
    "translation": "MeTA-LoRA：面向大型语言模型的数据高效多任务微调",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM的高效微调技术（LoRA），属于'使能LLM技术'范畴。多任务微调和数据高效方法可直接应用于推荐系统、搜索和广告领域，在这些场景中通常需要同时优化多个目标（如点击率、转化率、停留时间），且标注数据有限。这种技术能够帮助构建更高效、更通用的排序模型，适应多目标优化需求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.11586v1": {
    "title": "Survey Response Generation: Generating Closed-Ended Survey Responses In-Silico with Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11586v1",
    "arxiv_id": "2510.11586v1",
    "authors": "Georg Ahnert, Anna-Carolina Haensch, Barbara Plank, Markus Strohmaier",
    "categories": "cs.CL, cs.CY",
    "pub_date": "2025-10-13 16:29:19",
    "ori_summary": "Many in-silico simulations of human survey responses with large language models (LLMs) focus on generating closed-ended survey responses, whereas LLMs are typically trained to generate open-ended text instead. Previous research has used a diverse range of methods for generating closed-ended survey responses with LLMs, and a standard practice remains to be identified. In this paper, we systematically investigate the impact that various Survey Response Generation Methods have on predicted survey responses. We present the results of 32 mio. simulated survey responses across 8 Survey Response Generation Methods, 4 political attitude surveys, and 10 open-weight language models. We find significant differences between the Survey Response Generation Methods in both individual-level and subpopulation-level alignment. Our results show that Restricted Generation Methods perform best overall, and that reasoning output does not consistently improve alignment. Our work underlines the significant impact that Survey Response Generation Methods have on simulated survey responses, and we develop practical recommendations on the application of Survey Response Generation Methods.",
    "summary": "",
    "translation": "调查问卷响应生成：使用大型语言模型在计算机模拟中生成封闭式调查问卷响应",
    "relevance_score": 1,
    "reasoning": "该论文专注于使用LLM生成封闭式调查问卷响应，这属于纯粹的LLM应用场景，与推荐系统、搜索或广告的核心技术无关。虽然涉及LLM技术，但应用场景是调查问卷生成，而非RecSys/Search/Ads领域的任何相关任务，因此相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11584v1": {
    "title": "LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings",
    "url": "https://www.alphaxiv.org/abs/2510.11584v1",
    "arxiv_id": "2510.11584v1",
    "authors": "Ting Li, Yang Yang, Yipeng Yu, Liang Yao, Guoqing Chao, Ruifeng Xu",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 16:29:17",
    "ori_summary": "Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the model's ability of link prediction by removing or inserting triples. A recent black-box method has attempted to incorporate textual and structural information to enhance attack performance. However, it is unable to generate human-readable explanations, and exhibits poor generalizability. In the past few years, large language models (LLMs) have demonstrated powerful capabilities in text comprehension, generation, and reasoning. In this paper, we propose LLMAtKGE, a novel LLM-based framework that selects attack targets and generates human-readable explanations. To provide the LLM with sufficient factual context under limited input constraints, we design a structured prompting scheme that explicitly formulates the attack as multiple-choice questions while incorporating KG factual evidence. To address the context-window limitation and hesitation issues, we introduce semantics-based and centrality-based filters, which compress the candidate set while preserving high recall of attack-relevant information. Furthermore, to efficiently integrate both semantic and structural information into the filter, we precompute high-order adjacency and fine-tune the LLM with a triple classification task to enhance filtering performance. Experiments on two widely used knowledge graph datasets demonstrate that our attack outperforms the strongest black-box baselines and provides explanations via reasoning, and showing competitive performance compared with white-box methods. Comprehensive ablation and case studies further validate its capability to generate explanations.",
    "summary": "",
    "translation": "LLMAtKGE：大型语言模型作为知识图谱嵌入的可解释攻击者",
    "relevance_score": 2,
    "reasoning": "该论文主要关注使用LLMs攻击知识图谱嵌入的对抗性攻击和可解释性，这属于安全/攻击领域，属于明确的无关主题。虽然提到了LLMs和知识图谱，但核心焦点是攻击方法而非在推荐/搜索/广告中的应用。知识图谱在推荐系统中可能有应用，但本文的攻击视角与当前关注的核心进展不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11570v1": {
    "title": "Bag of Tricks for Subverting Reasoning-based Safety Guardrails",
    "url": "https://www.alphaxiv.org/abs/2510.11570v1",
    "arxiv_id": "2510.11570v1",
    "authors": "Shuo Chen, Zhen Han, Haokun Chen, Bailan He, Shengyun Si, Jingpei Wu, Philip Torr, Volker Tresp, Jindong Gu",
    "categories": "cs.CR, cs.CL",
    "pub_date": "2025-10-13 16:16:44",
    "ori_summary": "Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs), such as deliberative alignment, have shown strong defense against jailbreak attacks. By leveraging LRMs' reasoning ability, these guardrails help the models to assess the safety of user inputs before generating final responses. The powerful reasoning ability can analyze the intention of the input query and will refuse to assist once it detects the harmful intent hidden by the jailbreak methods. Such guardrails have shown a significant boost in defense, such as the near-perfect refusal rates on the open-source gpt-oss series. Unfortunately, we find that these powerful reasoning-based guardrails can be extremely vulnerable to subtle manipulation of the input prompts, and once hijacked, can lead to even more harmful results. Specifically, we first uncover a surprisingly fragile aspect of these guardrails: simply adding a few template tokens to the input prompt can successfully bypass the seemingly powerful guardrails and lead to explicit and harmful responses. To explore further, we introduce a bag of jailbreak methods that subvert the reasoning-based guardrails. Our attacks span white-, gray-, and black-box settings and range from effortless template manipulations to fully automated optimization. Along with the potential for scalable implementation, these methods also achieve alarmingly high attack success rates (e.g., exceeding 90% across 5 different benchmarks on gpt-oss series on both local host models and online API services). Evaluations across various leading open-source LRMs confirm that these vulnerabilities are systemic, underscoring the urgent need for stronger alignment techniques for open-sourced LRMs to prevent malicious misuse. Code is open-sourced at https://chenxshuo.github.io/bag-of-tricks.",
    "summary": "",
    "translation": "规避基于推理的安全护栏的实用技巧集",
    "relevance_score": 1,
    "reasoning": "该论文专注于对抗性攻击和安全规避技术，这属于安全领域的范畴，与我的关注点无关。论文内容涉及如何绕过LLM的安全机制，这与推荐系统、搜索或广告中的核心技术进步、Transformer架构改进或LLM应用完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11563v1": {
    "title": "Culturally-Aware Conversations: A Framework & Benchmark for LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.11563v1",
    "arxiv_id": "2510.11563v1",
    "authors": "Shreya Havaldar, Sunny Rai, Young-Min Cho, Lyle Ungar",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 16:06:14",
    "ori_summary": "Existing benchmarks that measure cultural adaptation in LLMs are misaligned with the actual challenges these models face when interacting with users from diverse cultural backgrounds. In this work, we introduce the first framework and benchmark designed to evaluate LLMs in realistic, multicultural conversational settings. Grounded in sociocultural theory, our framework formalizes how linguistic style - a key element of cultural communication - is shaped by situational, relational, and cultural context. We construct a benchmark dataset based on this framework, annotated by culturally diverse raters, and propose a new set of desiderata for cross-cultural evaluation in NLP: conversational framing, stylistic sensitivity, and subjective correctness. We evaluate today's top LLMs on our benchmark and show that these models struggle with cultural adaptation in a conversational setting.",
    "summary": "",
    "translation": "文化感知对话：面向大语言模型的框架与基准",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文化感知对话的框架和基准构建，属于纯粹的LLM评估和对话系统范畴，与推荐系统、搜索或广告的核心技术进展缺乏直接关联。虽然文化感知在个性化推荐中可能有潜在应用，但论文标题明确聚焦于对话基准而非推荐/搜索/广告的具体技术方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11557v1": {
    "title": "Invisible Languages of the LLM Universe",
    "url": "https://www.alphaxiv.org/abs/2510.11557v1",
    "arxiv_id": "2510.11557v1",
    "authors": "Saurabh Khanna, Xinxu Li",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 16:00:15",
    "ori_summary": "Large Language Models are trained on massive multilingual corpora, yet this abundance masks a profound crisis: of the world's 7,613 living languages, approximately 2,000 languages with millions of speakers remain effectively invisible in digital ecosystems. We propose a critical framework connecting empirical measurements of language vitality (real world demographic strength) and digitality (online presence) with postcolonial theory and epistemic injustice to explain why linguistic inequality in AI systems is not incidental but structural. Analyzing data across all documented human languages, we identify four categories: Strongholds (33%, high vitality and digitality), Digital Echoes (6%, high digitality despite declining vitality), Fading Voices (36%, low on both dimensions), and critically, Invisible Giants (27%, high vitality but near-zero digitality) - languages spoken by millions yet absent from the LLM universe. We demonstrate that these patterns reflect continuities from colonial-era linguistic hierarchies to contemporary AI development, constituting what we term digital epistemic injustice. Our analysis reveals that English dominance in AI is not a technical necessity but an artifact of power structures that systematically exclude marginalized linguistic knowledge. We conclude with implications for decolonizing language technology and democratizing access to AI benefits.",
    "summary": "",
    "translation": "LLM宇宙中的隐形语言",
    "relevance_score": 2,
    "reasoning": "该标题暗示可能探讨LLM内部表征或新兴能力等基础LLM技术，这些技术可能通过改进语义理解或跨模态建模应用于推荐系统或搜索。然而，标题过于抽象和宽泛，缺乏具体的技术细节或明确的RecSys/Search/Ads应用指向，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11545v1": {
    "title": "Information-Preserving Reformulation of Reasoning Traces for Antidistillation",
    "url": "https://www.alphaxiv.org/abs/2510.11545v1",
    "arxiv_id": "2510.11545v1",
    "authors": "Jiayu Ding, Lei Cui, Li Dong, Nanning Zheng, Furu Wei",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 15:42:11",
    "ori_summary": "Recent advances in Large Language Models (LLMs) show that extending the length of reasoning chains significantly improves performance on complex tasks. While revealing these reasoning traces helps users better follow, verify, and learn from the model's problem-solving process, it also makes them highly vulnerable to unauthorized distillation. To mitigate this risk, proprietary model providers often adopt aggressive protection strategies, such as replacing detailed reasoning with brief summaries, which deprive users of valuable intermediate information. To address this trade-off, we propose PART, an information-preserving antidistillation reformulation of reasoning traces. Motivated by the difference between how humans understand reasoning traces and how LLMs exploit them for supervised fine-tuning, we design a simple but effective two-step reformulation: removing self-talk behaviors and reordering sub-conclusions. A small auxiliary model is trained to perform this reformulation, incurring minimal computational overhead. Extensive experiments demonstrate that PART consistently disrupts distillation across student models of different sizes and types on various reasoning benchmarks. For instance, when training on reformulated traces, even the performance of a large 32B student model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a 13.5% degradation.",
    "summary": "",
    "translation": "用于抗蒸馏的推理轨迹信息保留重构",
    "relevance_score": 2,
    "reasoning": "该论文关注推理轨迹的重新表述和抗蒸馏技术，主要属于模型压缩和知识蒸馏领域。虽然蒸馏技术可能间接应用于推荐或搜索系统的模型部署优化，但论文标题未表明与推荐系统、搜索或广告的直接关联，也未涉及Transformer架构改进或LLM在推荐/搜索中的直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11537v1": {
    "title": "An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese Token-Level Classification",
    "url": "https://www.alphaxiv.org/abs/2510.11537v1",
    "arxiv_id": "2510.11537v1",
    "authors": "Ba-Quang Nguyen",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 15:39:09",
    "ori_summary": "We propose a novel neural architecture named TextGraphFuseGAT, which integrates a pretrained transformer encoder (PhoBERT) with Graph Attention Networks for token-level classification tasks. The proposed model constructs a fully connected graph over the token embeddings produced by PhoBERT, enabling the GAT layer to capture rich inter-token dependencies beyond those modeled by sequential context alone. To further enhance contextualization, a Transformer-style self-attention layer is applied on top of the graph-enhanced embeddings. The final token representations are passed through a classification head to perform sequence labeling. We evaluate our approach on three Vietnamese benchmark datasets: PhoNER-COVID19 for named entity recognition in the COVID-19 domain, PhoDisfluency for speech disfluency detection, and VietMed-NER for medical-domain NER. VietMed-NER is the first Vietnamese medical spoken NER dataset, featuring 18 entity types collected from real-world medical speech transcripts and annotated with the BIO tagging scheme. Its specialized vocabulary and domain-specific expressions make it a challenging benchmark for token-level classification models. Experimental results show that our method consistently outperforms strong baselines, including transformer-only and hybrid neural models such as BiLSTM + CNN + CRF, confirming the effectiveness of combining pretrained semantic features with graph-based relational modeling for improved token classification across multiple domains.",
    "summary": "",
    "translation": "集成编码器的PhoBERT与图注意力机制在越南语词级分类中的应用",
    "relevance_score": 2,
    "reasoning": "该论文专注于越南语特定语言模型的词级分类任务，属于特定语言的NLP应用。虽然涉及图注意力机制，但其核心是语言特定的分类问题，与推荐系统、搜索或广告的核心技术进展缺乏直接关联。该技术主要针对越南语处理，在通用推荐/搜索系统中的潜在应用非常有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11529v1": {
    "title": "Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11529v1",
    "arxiv_id": "2510.11529v1",
    "authors": "Yusheng Song, Lirong Qiu, Xi Zhang, Zhihao Tang",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 15:31:21",
    "ori_summary": "The detection of sophisticated hallucinations in Large Language Models (LLMs) is hampered by a ``Detection Dilemma'': methods probing internal states (Internal State Probing) excel at identifying factual inconsistencies but fail on logical fallacies, while those verifying externalized reasoning (Chain-of-Thought Verification) show the opposite behavior. This schism creates a task-dependent blind spot: Chain-of-Thought Verification fails on fact-intensive tasks like open-domain QA where reasoning is ungrounded, while Internal State Probing is ineffective on logic-intensive tasks like mathematical reasoning where models are confidently wrong. We resolve this with a unified framework that bridges this critical gap. However, unification is hindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse symbolic reasoning chains lack signals directly comparable to fine-grained internal states, and the Representational Alignment Barrier, a deep-seated mismatch between their underlying semantic spaces. To overcome these, we introduce a multi-path reasoning mechanism to obtain more comparable, fine-grained signals, and a segment-aware temporalized cross-attention module to adaptively fuse these now-aligned representations, pinpointing subtle dissonances. Extensive experiments on three diverse benchmarks and two leading LLMs demonstrate that our framework consistently and significantly outperforms strong baselines. Our code is available: https://github.com/peach918/HalluDet.",
    "summary": "",
    "translation": "基于大语言模型内部状态与结构化推理一致性的幻觉检测",
    "relevance_score": 2,
    "reasoning": "该论文专注于LLM幻觉检测这一纯NLP中心主题，属于明确列出的无关主题范畴。虽然幻觉检测在通用LLM应用中很重要，但论文标题没有表明其在推荐系统、搜索或广告中的潜在应用价值，与当前关注的领域进展或使能技术缺乏直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11498v1": {
    "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding",
    "url": "https://www.alphaxiv.org/abs/2510.11498v1",
    "arxiv_id": "2510.11498v1",
    "authors": "Yuhang Li, Chenchen Zhang, Ruilin Lv, Ao Liu, Ken Deng, Yuanxing Zhang, Jiaheng Liu, Wiggin Zhou, Bo Zhou",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-13 15:05:50",
    "ori_summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they struggle with front-end development, where correctness is judged on rendered pixels and interaction. We present ReLook, an agentic, vision-grounded reinforcement learning framework that empowers an agent to close a robust generate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool. During training, the agent uses the MLLM-in-the-loop both as a visual critic--scoring code with screenshots--and as a source of actionable, vision-grounded feedback; a strict zero-reward rule for invalid renders anchors renderability and prevents reward hacking. To prevent behavioral collapse, we introduce Forced Optimization, a strict acceptance rule that admits only improving revisions, yielding monotonically better trajectories. At inference, we decouple the critic and run a lightweight, critic-free self-edit cycle, keeping latency comparable to base decoding while retaining most of the gains. Across three widely used benchmarks, ReLook consistently outperforms strong baselines in vision-grounded front-end code generation, highlighting the benefits of agentic perception, visual rewards, and training-inference decoupling.",
    "summary": "",
    "translation": "ReLook：基于视觉的强化学习与多模态大语言模型评论器用于智能网页编码",
    "relevance_score": 2,
    "reasoning": "该论文主要关注网页编码的智能体开发，结合视觉和强化学习技术，这与推荐系统、搜索或广告的核心领域相关性较低。虽然涉及多模态LLM技术，但其应用场景（网页编码）与RecSys/Search/Ads的直接应用或使能技术关联性较弱，缺乏明确的跨领域应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11482v1": {
    "title": "Investigating Large Language Models' Linguistic Abilities for Text Preprocessing",
    "url": "https://www.alphaxiv.org/abs/2510.11482v1",
    "arxiv_id": "2510.11482v1",
    "authors": "Marco Braga, Gian Carlo Milanese, Gabriella Pasi",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 14:53:44",
    "ori_summary": "Text preprocessing is a fundamental component of Natural Language Processing, involving techniques such as stopword removal, stemming, and lemmatization to prepare text as input for further processing and analysis. Despite the context-dependent nature of the above techniques, traditional methods usually ignore contextual information. In this paper, we investigate the idea of using Large Language Models (LLMs) to perform various preprocessing tasks, due to their ability to take context into account without requiring extensive language-specific annotated resources. Through a comprehensive evaluation on web-sourced data, we compare LLM-based preprocessing (specifically stopword removal, lemmatization and stemming) to traditional algorithms across multiple text classification tasks in six European languages. Our analysis indicates that LLMs are capable of replicating traditional stopword removal, lemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%, respectively. Additionally, we show that ML algorithms trained on texts preprocessed by LLMs achieve an improvement of up to 6% with respect to the $F_1$ measure compared to traditional techniques. Our code, prompts, and results are publicly available at https://github.com/GianCarloMilanese/llm_pipeline_wi-iat.",
    "summary": "",
    "translation": "研究大型语言模型在文本预处理中的语言能力",
    "relevance_score": 3,
    "reasoning": "该论文聚焦于LLM在文本预处理中的语言能力研究，属于LLM技术基础进展。虽然文本预处理是搜索和推荐系统中的重要环节，但论文标题未明确指向搜索/推荐/广告领域的特定应用，且可能更偏向NLP技术本身而非领域应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11444v1": {
    "title": "GenCNER: A Generative Framework for Continual Named Entity Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.11444v1",
    "arxiv_id": "2510.11444v1",
    "authors": "Yawen Yang, Fukun Ma, Shiao Meng, Aiwei Liu, Lijie Wen",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 14:15:31",
    "ori_summary": "Traditional named entity recognition (NER) aims to identify text mentions into pre-defined entity types. Continual Named Entity Recognition (CNER) is introduced since entity categories are continuously increasing in various real-world scenarios. However, existing continual learning (CL) methods for NER face challenges of catastrophic forgetting and semantic shift of non-entity type. In this paper, we propose GenCNER, a simple but effective Generative framework for CNER to mitigate the above drawbacks. Specifically, we skillfully convert the CNER task into sustained entity triplet sequence generation problem and utilize a powerful pre-trained seq2seq model to solve it. Additionally, we design a type-specific confidence-based pseudo labeling strategy along with knowledge distillation (KD) to preserve learned knowledge and alleviate the impact of label noise at the triplet level. Experimental results on two benchmark datasets show that our framework outperforms previous state-of-the-art methods in multiple CNER settings, and achieves the smallest gap compared with non-CL results.",
    "summary": "",
    "translation": "GenCNER：一种用于持续命名实体识别的生成式框架",
    "relevance_score": 2,
    "reasoning": "该论文专注于命名实体识别（NLR）领域的持续学习问题，属于纯粹的NLP技术范畴。虽然采用了生成式方法，但其应用场景局限于实体识别任务，与推荐系统、搜索或广告中的核心排名、用户建模、多模态数据处理等关键问题缺乏直接关联，且未展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11434v1": {
    "title": "Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated Content",
    "url": "https://www.alphaxiv.org/abs/2510.11434v1",
    "arxiv_id": "2510.11434v1",
    "authors": "Dana Sotto Porat, Ella Rabinovich",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 14:06:17",
    "ori_summary": "Generative large language models (LLMs) have become central to everyday life, producing human-like text across diverse domains. A growing body of research investigates whether these models also exhibit personality- and demographic-like characteristics in their language. In this work, we introduce a novel, data-driven methodology for assessing LLM personality without relying on self-report questionnaires, applying instead automatic personality and gender classifiers to model replies on open-ended questions collected from Reddit. Comparing six widely used models to human-authored responses, we find that LLMs systematically express higher Agreeableness and lower Neuroticism, reflecting cooperative and stable conversational tendencies. Gendered language patterns in model text broadly resemble those of human writers, though with reduced variation, echoing prior findings on automated agents. We contribute a new dataset of human and model responses, along with large-scale comparative analyses, shedding new light on the topic of personality and demographic patterns of generative AI.",
    "summary": "",
    "translation": "你是谁，ChatGPT？大语言模型生成内容中的个性和人口统计风格",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLM生成内容中的个性和人口统计特征，这属于纯粹的LLM中心化主题，与推荐系统、搜索或广告的核心技术进展无关。虽然LLM个性分析可能间接影响用户体验，但论文焦点是内容生成特性而非实际的推荐、搜索或广告应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11423v1": {
    "title": "Beyond the Crowd: LLM-Augmented Community Notes for Governing Health Misinformation",
    "url": "https://www.alphaxiv.org/abs/2510.11423v1",
    "arxiv_id": "2510.11423v1",
    "authors": "Jiaying Wu, Zihang Fu, Haonan Wang, Fanxiao Li, Min-Yen Kan",
    "categories": "cs.SI, cs.CL",
    "pub_date": "2025-10-13 13:57:23",
    "ori_summary": "Community Notes, the crowd-sourced misinformation governance system on X (formerly Twitter), enables users to flag misleading posts, attach contextual notes, and vote on their helpfulness. However, our analysis of 30.8K health-related notes reveals significant latency, with a median delay of 17.6 hours before the first note receives a helpfulness status. To improve responsiveness during real-world misinformation surges, we propose CrowdNotes+, a unified framework that leverages large language models (LLMs) to augment Community Notes for faster and more reliable health misinformation governance. CrowdNotes+ integrates two complementary modes: (1) evidence-grounded note augmentation and (2) utility-guided note automation, along with a hierarchical three-step evaluation that progressively assesses relevance, correctness, and helpfulness. We instantiate the framework through HealthNotes, a benchmark of 1.2K helpfulness-annotated health notes paired with a fine-tuned helpfulness judge. Experiments on fifteen LLMs reveal an overlooked loophole in current helpfulness evaluation, where stylistic fluency is mistaken for factual accuracy, and demonstrate that our hierarchical evaluation and LLM-augmented generation jointly enhance factual precision and evidence utility. These results point toward a hybrid human-AI governance model that improves both the rigor and timeliness of crowd-sourced fact-checking.",
    "summary": "",
    "translation": "超越群体：基于大语言模型增强的社区笔记用于治理健康错误信息",
    "relevance_score": 2,
    "reasoning": "该论文主要关注健康错误信息治理这一特定领域应用，属于医疗/健康领域的垂直应用。虽然涉及LLM技术，但核心应用场景（健康信息治理）与推荐系统、搜索或广告的核心技术进展无关，且健康领域属于明确的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11408v1": {
    "title": "Valid Survey Simulations with Limited Human Data: The Roles of Prompting, Fine-Tuning, and Rectification",
    "url": "https://www.alphaxiv.org/abs/2510.11408v1",
    "arxiv_id": "2510.11408v1",
    "authors": "Stefan Krsteski, Giuseppe Russo, Serina Chang, Robert West, Kristina Gligorić",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 13:48:07",
    "ori_summary": "Surveys provide valuable insights into public opinion and behavior, but their execution is costly and slow. Large language models (LLMs) have been proposed as a scalable, low-cost substitute for human respondents, but their outputs are often biased and yield invalid estimates. We study the interplay between synthesis methods that use LLMs to generate survey responses and rectification methods that debias population estimates, and explore how human responses are best allocated between them. Using two panel surveys with questions on nutrition, politics, and economics, we find that synthesis alone introduces substantial bias (24-86%), whereas combining it with rectification reduces bias below 5% and increases effective sample size by up to 14%. Overall, we challenge the common practice of using all human responses for fine-tuning, showing that under a fixed budget, allocating most to rectification results in far more effective estimation.",
    "summary": "",
    "translation": "基于有限人类数据的有效调查模拟：提示、微调和校正的作用",
    "relevance_score": 2,
    "reasoning": "该论文主要关注调查模拟和有限人类数据场景，这属于数据收集和验证的通用方法学范畴。虽然提到了提示和微调等LLM技术，但核心应用场景（调查模拟）与推荐系统、搜索或广告的排名和建模问题没有直接关联，潜在应用不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11407v1": {
    "title": "KnowRL: Teaching Language Models to Know What They Know",
    "url": "https://www.alphaxiv.org/abs/2510.11407v1",
    "arxiv_id": "2510.11407v1",
    "authors": "Sahil Kale, Devendra Singh Dhami",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 13:47:14",
    "ori_summary": "Truly reliable AI requires more than simply scaling up knowledge; it demands the ability to know what it knows and when it does not. Yet recent research shows that even the best LLMs misjudge their own competence in more than one in five cases, making any response born of such internal uncertainty impossible to fully trust. Inspired by self-improvement reinforcement learning techniques that require minimal data, we present a simple but powerful framework KnowRL that strengthens a model's internal understanding of its own feasibility boundaries, enabling safer and more responsible behaviour. Our framework combines two components: (i) introspection, where the model generates and classifies tasks it judges feasible or infeasible, and (ii) consensus-based rewarding, where stability of self-knowledge assessment is reinforced through internal agreement. By using internally generated data, this design strengthens consistency in self-knowledge and entirely avoids costly external supervision. In experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved self-knowledge, validated by both intrinsic self-consistency and extrinsic benchmarking. With nothing more than a small seed set and no external supervision, our method drove gains as high as 28% in accuracy and 12% in F1, outperforming baselines in just a few iterations. Our framework essentially unlocks the untapped capacity of LLMs to self-improve their knowledge awareness, opening the door to reliable, more accountable AI and safer deployment in critical applications. Owing to its simplicity and independence from external effort, we encourage applying this reliability-enhancing process to all future models.",
    "summary": "",
    "translation": "KnowRL：教导语言模型知道它们知道什么",
    "relevance_score": 8,
    "reasoning": "这篇论文涉及语言模型的知识边界识别和不确定性建模，这是LLM可信度和可靠性的核心问题。在推荐系统和搜索领域，准确识别模型的知识边界可以显著提升结果质量，避免推荐或检索超出模型能力范围的内容，从而提高用户体验和系统可靠性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.11391v1": {
    "title": "DocReward: A Document Reward Model for Structuring and Stylizing",
    "url": "https://www.alphaxiv.org/abs/2510.11391v1",
    "arxiv_id": "2510.11391v1",
    "authors": "Junpeng Liu, Yuzhong Zhao, Bowen Cao, Jiayu Ding, Yilin Jia, Tengchao Lv, Yupan Huang, Shaohan Huang, Nan Yang, Li Dong, Lei Cui, Tao Ge, Xun Wang, Huitian Jiao, Sun Mao, FNU Kartik, Si-Qing Chen, Wai Lam, Furu Wei",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-10-13 13:36:32",
    "ori_summary": "Recent advances in agentic workflows have enabled the automation of tasks such as professional document generation. However, they primarily focus on textual quality, neglecting visual structure and style, which are crucial for readability and engagement. This gap arises mainly from the absence of suitable reward models to guide agentic workflows toward producing documents with stronger structural and stylistic quality. To address this, we propose DocReward, a document reward model that evaluates documents based on their structure and style. We construct a multi-domain dataset DocPair of 117K paired documents, covering 32 domains and 267 document types, each including a high- and low-professionalism document with identical content but different structure and style. This enables the model to evaluate professionalism comprehensively, and in a textual-quality-agnostic way. DocReward is trained using the Bradley-Terry loss to score documents, penalizing predictions that contradict the annotated ranking. To assess the performance of reward models, we create a test dataset containing document bundles ranked by well-educated human evaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6 and 19.4 percentage points, respectively, demonstrating its superiority over baselines. In an extrinsic evaluation of document generation, DocReward achieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7% win rate, demonstrating its utility in guiding generation agents toward producing human-preferred documents.",
    "summary": "",
    "translation": "DocReward：一种用于结构化和风格化的文档奖励模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文档结构化和风格化的奖励模型，这属于内容生成和格式化的范畴。虽然奖励模型技术本身可能对推荐系统或搜索有一定启发，但论文的核心应用场景（文档结构化和风格化）与我的核心关注领域（推荐系统、搜索、广告中的排名和建模）关联度较低，属于AIGC和内容生成的范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11389v1": {
    "title": "Beyond Survival: Evaluating LLMs in Social Deduction Games with Human-Aligned Strategies",
    "url": "https://www.alphaxiv.org/abs/2510.11389v1",
    "arxiv_id": "2510.11389v1",
    "authors": "Zirui Song, Yuan Huang, Junchang Liu, Haozhe Luo, Chenxi Wang, Lang Gao, Zixiang Xu, Mingfei Han, Xiaojun Chang, Xiuying Chen",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 13:33:30",
    "ori_summary": "Social deduction games like Werewolf combine language, reasoning, and strategy, providing a testbed for studying natural language and social intelligence. However, most studies reduce the game to LLM-based self-play, yielding templated utterances and anecdotal cases that overlook the richness of social gameplay. Evaluation further relies on coarse metrics such as survival time or subjective scoring due to the lack of quality reference data. To address these gaps, we curate a high-quality, human-verified multimodal Werewolf dataset containing over 100 hours of video, 32.4M utterance tokens, and 15 rule variants. Based on this dataset, we propose a novel strategy-alignment evaluation that leverages the winning faction's strategies as ground truth in two stages: 1) Speech evaluation, formulated as multiple-choice-style tasks that assess whether the model can adopt appropriate stances across five dimensions of social ability; and 2) Decision evaluation, which assesses the model's voting choices and opponent-role inferences. This framework enables a fine-grained evaluation of models' linguistic and reasoning capabilities, while capturing their ability to generate strategically coherent gameplay. Our experiments show that state-of-the-art LLMs show diverse performance, with roughly half remain below 0.50, revealing clear gaps in deception and counterfactual reasoning. We hope our dataset further inspires research on language, reasoning, and strategy in multi-agent interaction.",
    "summary": "",
    "translation": "超越生存：基于人类对齐策略在社交推理游戏中评估大语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM在社交推理游戏中的评估和人类对齐策略，这属于纯粹的LLM评估和游戏应用场景。虽然涉及人类对齐，但缺乏与推荐系统、搜索或广告领域的明确关联，也没有展示在异构数据处理或Transformer架构效率方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11372v1": {
    "title": "Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction Tuning",
    "url": "https://www.alphaxiv.org/abs/2510.11372v1",
    "arxiv_id": "2510.11372v1",
    "authors": "Dean L. Slack, Noura Al Moubayed",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 13:12:46",
    "ori_summary": "Although large language models excel across many tasks, they can memorise training data and thereby expose private or copyrighted text. Most defences target the pre-training stage, leaving memorisation during fine-tuning, especially for domain adaptation and instruction tuning, poorly understood. We fine-tune Pythia, Llama3, and Mistral models spanning 1.4B-70B parameters on common evaluation datasets and track verbatim memorisation throughout training. We find that memorisation increases dramatically in the first few epochs, often significantly before either validation perplexity or evaluation performance is optimised. We use a simple but effective n-gram memorisation score which reliably precedes verbatim memorisation; using it as an early-stopping criterion mitigates memorisation with minimal performance loss. Further, we introduce an n-gram-aware loss regulariser and show that it reduces memorisation across all model families tested by up to 40% while minimising evaluation performance trade-offs when compared to an existing memorisation mitigation strategy. These results yield practical, scalable insights into memorisation dynamics during language model fine-tuning.",
    "summary": "",
    "translation": "领域适应和指令微调中记忆化的早期检测与减少",
    "relevance_score": 3,
    "reasoning": "该论文主要关注LLM训练中的记忆化问题，这属于LLM安全/隐私范畴，而非核心推荐/搜索/广告技术。虽然指令微调是LLM应用的重要环节，但论文焦点在记忆化检测与减少这一非技术性问题上，与您关注的领域核心进展、架构创新或直接应用相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11370v1": {
    "title": "Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers",
    "url": "https://www.alphaxiv.org/abs/2510.11370v1",
    "arxiv_id": "2510.11370v1",
    "authors": "Wenhan Ma, Hailin Zhang, Liang Zhao, Yifan Song, Yudong Wang, Zhifang Sui, Fuli Luo",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-13 13:11:27",
    "ori_summary": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing the capabilities of large language models. However, in Mixture-of-Experts (MoE) models, the routing mechanism often introduces instability, even leading to catastrophic RL training collapse. We analyze the training-inference consistency of MoE models and identify a notable discrepancy in routing behaviors between the two phases. Moreover, even under identical conditions, the routing framework can yield divergent expert selections across repeated forward passes. To address this foundational inconsistency, we propose Rollout Routing Replay (R3), a method that records routing distributions from the inference engine and replays them during training. R3 significantly reduces training-inference policy KL divergence and mitigates extreme discrepancies without compromising training speed. Extensive experiments on various settings confirm that R3 succeeds in stabilizing RL training, preventing collapse and outperforming methods such as GSPO and TIS. We believe this work can offer a new solution for stabilizing RL in MoE models.",
    "summary": "",
    "translation": "通过对齐训练和推理路由器来稳定混合专家强化学习",
    "relevance_score": 2,
    "reasoning": "该论文虽然涉及混合专家（MoE）架构，属于Transformer技术范畴，但核心聚焦于强化学习（RL）的稳定性问题。论文没有明确展示与推荐系统、搜索或广告的直接相关性，且强化学习应用不明确，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11330v1": {
    "title": "Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap",
    "url": "https://www.alphaxiv.org/abs/2510.11330v1",
    "arxiv_id": "2510.11330v1",
    "authors": "KiHyun Nam, Jongmin Choi, Hyeongkeun Lee, Jungwoo Heo, Joon Son Chung",
    "categories": "cs.SD, cs.AI, cs.CL, cs.LG, eess.AS",
    "pub_date": "2025-10-13 12:25:33",
    "ori_summary": "Contrastive audio-language pretraining yields powerful joint representations, yet a persistent audio-text modality gap limits the benefits of coupling multimodal encoders with large language models (LLMs). We present Diffusion-Link, a diffusion-based modality-bridging module that generatively maps audio embeddings into the text-embedding distribution. The module is trained at the output embedding from the frozen multimodal encoder and implemented as a lightweight network with three residual MLP blocks. To assess the effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on Automatic Audio Captioning (AAC); to our knowledge, this is the first application of diffusion-based modality bridging to AAC. We report two results. (1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link reduces the modality gap the most among prior diffusion-based methods and shows a collective migration of audio embeddings toward the text distribution. (2) Downstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline achieves state-of-the-art on AudioCaps in both zero-shot and fully supervised captioning without external knowledge, with relative gains up to 52.5% and 7.5%, respectively. These findings show that closing the modality gap is pivotal for effective coupling between multimodal encoders and LLMs, and diffusion-based modality bridging offers a promising direction beyond knowledge-retrieval-centric designs. Code will be released upon acceptance https://github.com/DevKiHyun/Diffusion-Link",
    "summary": "",
    "translation": "Diffusion-Link：用于弥合音频-文本模态差距的扩散概率模型",
    "relevance_score": 2,
    "reasoning": "该论文专注于音频-文本模态对齐，属于跨模态学习范畴，与推荐系统、搜索或广告的核心技术关联较弱。虽然扩散模型技术本身具有潜力，但论文的应用场景（音频-文本）与异构数据处理（如用户序列和上下文特征）的直接关联性不足，难以直接应用于当前关注的领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11328v1": {
    "title": "Do LLMs \"Feel\"? Emotion Circuits Discovery and Control",
    "url": "https://www.alphaxiv.org/abs/2510.11328v1",
    "arxiv_id": "2510.11328v1",
    "authors": "Chenxi Wang, Yixuan Zhang, Ruiji Yu, Yufei Zheng, Lang Gao, Zirui Song, Zixiang Xu, Gus Xia, Huishuai Zhang, Dongyan Zhao, Xiuying Chen",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 12:24:24",
    "ori_summary": "As the demand for emotional intelligence in large language models (LLMs) grows, a key challenge lies in understanding the internal mechanisms that give rise to emotional expression and in controlling emotions in generated text. This study addresses three core questions: (1) Do LLMs contain context-agnostic mechanisms shaping emotional expression? (2) What form do these mechanisms take? (3) Can they be harnessed for universal emotion control? We first construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit comparable internal states across emotions. Subsequently, we extract context-agnostic emotion directions that reveal consistent, cross-context encoding of emotion (Q1). We identify neurons and attention heads that locally implement emotional computation through analytical decomposition and causal analysis, and validate their causal roles via ablation and enhancement interventions. Next, we quantify each sublayer's causal influence on the model's final emotion representation and integrate the identified local components into coherent global emotion circuits that drive emotional expression (Q2). Directly modulating these circuits achieves 99.65% emotion-expression accuracy on the test set, surpassing prompting- and steering-based methods (Q3). To our knowledge, this is the first systematic study to uncover and validate emotion circuits in LLMs, offering new insights into interpretability and controllable emotional intelligence.",
    "summary": "",
    "translation": "大型语言模型是否“感受”情感？情感回路发现与控制",
    "relevance_score": 3,
    "reasoning": "该论文主要研究LLMs中的情感回路发现与控制，属于LLM内部机制探索，与推荐系统、搜索或广告的直接应用关联较弱。虽然情感理解在个性化推荐中有潜在价值，但论文更侧重于基础情感机制而非具体应用场景，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11314v1": {
    "title": "Template-Based Text-to-Image Alignment for Language Accessibility: A Study on Visualizing Text Simplifications",
    "url": "https://www.alphaxiv.org/abs/2510.11314v1",
    "arxiv_id": "2510.11314v1",
    "authors": "Belkiss Souayed, Sarah Ebling, Yingqiang Gao",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 12:03:36",
    "ori_summary": "Individuals with intellectual disabilities often have difficulties in comprehending complex texts. While many text-to-image models prioritize aesthetics over accessibility, it is not clear how visual illustrations relate to text simplifications (TS) generated from them. This paper presents a structured vision-language model (VLM) prompting framework for generating accessible images from simplified texts. We designed five prompt templates, i.e., Basic Object Focus, Contextual Scene, Educational Layout, Multi-Level Detail, and Grid Layout, each following distinct spatial arrangements while adhering to accessibility constraints such as object count limits, spatial separation, and content restrictions. Using 400 sentence-level simplifications from four established TS datasets (OneStopEnglish, SimPA, Wikipedia, and ASSET), we conducted a two-phase evaluation: Phase 1 assessed prompt template effectiveness with CLIPScores, and Phase 2 involved human annotation of generated images across ten visual styles by four accessibility experts. Results show that the Basic Object Focus prompt template achieved the highest semantic alignment, indicating that visual minimalism enhances language accessibility. Expert evaluation further identified Retro style as the most accessible and Wikipedia as the most effective data source. Inter-annotator agreement varied across dimensions, with Text Simplicity showing strong reliability and Image Quality proving more subjective. Overall, our framework offers practical guidelines for accessible content generation and underscores the importance of structured prompting in AI-generated visual accessibility tools.",
    "summary": "",
    "translation": "基于模板的文本到图像对齐用于语言可访问性：文本简化可视化的研究",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文本到图像生成和语言可访问性，属于AIGC和内容生成范畴，与RecSys/Search/Ads的核心排名和推荐任务关联度较低。虽然文本简化技术可能间接影响搜索内容的可访问性，但这并非论文的核心技术贡献方向，且缺乏明确的推荐或搜索应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11307v1": {
    "title": "FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks",
    "url": "https://www.alphaxiv.org/abs/2510.11307v1",
    "arxiv_id": "2510.11307v1",
    "authors": "Sabrina McCallum, Amit Parekh, Alessandro Suglia",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 11:55:21",
    "ori_summary": "Current approaches to embodied AI tend to learn policies from expert demonstrations. However, without a mechanism to evaluate the quality of demonstrated actions, they are limited to learning from optimal behaviour, or they risk replicating errors and inefficiencies. While reinforcement learning offers one alternative, the associated exploration typically results in sacrificing data efficiency. This work explores how agents trained with imitation learning can learn robust representations from both optimal and suboptimal demonstrations when given access to constructive language feedback as a means to contextualise different modes of behaviour. We directly provide language feedback embeddings as part of the input sequence into a Transformer-based policy, and optionally complement the traditional next action prediction objective with auxiliary self-supervised learning objectives for feedback prediction. We test our approach on a range of embodied Vision-and-Language tasks in our custom BabyAI-XGen environment and show significant improvements in agents' compositional generalisation abilities and robustness, suggesting that our data-efficient method allows models to successfully convert suboptimal behaviour into learning opportunities. Overall, our results suggest that language feedback is a competitive and intuitive alternative to intermediate scalar rewards for language-specified embodied tasks.",
    "summary": "",
    "translation": "FOSSIL：利用次优样本反馈实现数据高效泛化的模仿学习，用于具身视觉与语言任务",
    "relevance_score": 2,
    "reasoning": "该论文主要关注具身AI中的视觉语言任务和模仿学习，这与推荐系统、搜索或广告的核心领域关联度较低。虽然涉及多模态学习概念，但其在具身环境中的具体应用场景难以直接迁移到RecSys/Search/Ads领域，且缺乏明确的跨领域应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11297v1": {
    "title": "Are Large Language Models Effective Knowledge Graph Constructors?",
    "url": "https://www.alphaxiv.org/abs/2510.11297v1",
    "arxiv_id": "2510.11297v1",
    "authors": "Ruirui Chen, Weifeng Jiang, Chengwei Qin, Bo Xiong, Fiona Liausvia, Dongkyu Choi, Boon Kiat Quek",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 11:37:48",
    "ori_summary": "Knowledge graphs (KGs) are vital for knowledge-intensive tasks and have shown promise in reducing hallucinations in large language models (LLMs). However, constructing high-quality KGs remains difficult, requiring accurate information extraction and structured representations that support interpretability and downstream utility. Existing LLM-based approaches often focus narrowly on entity and relation extraction, limiting coverage to sentence-level contexts or relying on predefined schemas. We propose a hierarchical extraction framework that organizes information at multiple levels, enabling the creation of semantically rich and well-structured KGs. Using state-of-the-art LLMs, we extract and construct knowledge graphs and evaluate them comprehensively from both structural and semantic perspectives. Our results highlight the strengths and shortcomings of current LLMs in KG construction and identify key challenges for future work. To advance research in this area, we also release a curated dataset of LLM-generated KGs derived from research papers on children's mental well-being. This resource aims to foster more transparent, reliable, and impactful applications in high-stakes domains such as healthcare.",
    "summary": "",
    "translation": "大型语言模型是有效的知识图谱构建器吗？",
    "relevance_score": 7,
    "reasoning": "该论文探讨LLMs在知识图谱构建中的应用，这直接关联到'直接LLM应用'领域，因为知识图谱是搜索和推荐系统的核心组件。构建高质量的知识图谱可以显著提升搜索相关性、推荐准确性和广告定向效果，属于LLM技术在RecSys/Search/Ads中的具体应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.11288v1": {
    "title": "Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.11288v1",
    "arxiv_id": "2510.11288v1",
    "authors": "Nikita Afonin, Nikita Andriyanov, Nikhil Bageshpura, Kyle Liu, Kevin Zhu, Sunishchal Dev, Ashwinee Panda, Alexander Panchenko, Oleg Rogov, Elena Tutubalina, Mikhail Seleznyov",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 11:23:56",
    "ori_summary": "Recent work has shown that narrow finetuning can produce broadly misaligned LLMs, a phenomenon termed emergent misalignment (EM). While concerning, these findings were limited to finetuning and activation steering, leaving out in-context learning (ICL). We therefore ask: does EM emerge in ICL? We find that it does: across three datasets, three frontier models produce broadly misaligned responses at rates between 2% and 17% given 64 narrow in-context examples, and up to 58% with 256 examples. We also examine mechanisms of EM by eliciting step-by-step reasoning (while leaving in-context examples unchanged). Manual analysis of the resulting chain-of-thought shows that 67.5% of misaligned traces explicitly rationalize harmful outputs by adopting a reckless or dangerous ''persona'', echoing prior results on finetuning-induced EM.",
    "summary": "",
    "translation": "通过上下文学习出现的错位：狭窄的上下文示例可能产生广泛错位的大型语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM的对齐问题和上下文学习中的错位现象，这属于纯粹的LLM中心话题。虽然LLM对齐在理论上可能影响推荐/搜索系统的可靠性，但论文没有明确展示与推荐系统、搜索或广告的直接应用关联，主要停留在NLP评估和基准测试层面。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11278v1": {
    "title": "ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11278v1",
    "arxiv_id": "2510.11278v1",
    "authors": "Gareth Seneque, Lap-Hang Ho, Nafise Erfanian Saeedi, Jeffrey Molendijk, Ariel Kupermann, Tim Elson",
    "categories": "cs.LG, cs.AI, cs.CL, 68T50, I.2.7",
    "pub_date": "2025-10-13 11:13:09",
    "ori_summary": "We present Entropic Mutual-Information Geometry Large-Language Model Alignment (ENIGMA), a novel approach to Large-Language Model (LLM) training that jointly improves reasoning, alignment and robustness by treating an organisation's policies/principles as directions to move on a model's information manifold. Our single-loop trainer combines Group-Relative Policy Optimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought (CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information (SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn optimal-transport regulariser on hidden-state distributions to bound geometry drift. We also introduce infoNCE metrics that specialise to a standard MI lower bound under matched negatives to measure how strongly a model's CoT encodes these policies. These metrics include a Sufficiency Index (SI) that enables the selection and creation of principles that maximise downstream performance prior to training. In our experiments using small (1B) LLMs, high-SI principles predict steadier training dynamics and improved benchmark performance over GRPO ablations. Our information-geometry analysis of trained models validates desirable structural change in the manifold. These results support our hypothesis that reasoning, alignment, and robustness are projections of a single informationgeometric objective, and that models trained using ENIGMA demonstrate principled reasoning without the use of a reward model, offering a path to trusted capability",
    "summary": "",
    "translation": "ENIGMA：大型语言模型中推理与对齐的几何结构",
    "relevance_score": 8,
    "reasoning": "该论文探讨LLM中推理和对齐的几何结构，属于'使能LLM技术'范畴。理解LLM的几何表示和推理机制可以改进推荐和搜索系统中的语义理解、用户意图建模以及内容对齐，从而提升个性化推荐的准确性和搜索相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.11277v1": {
    "title": "Towards Real-Time Fake News Detection under Evidence Scarcity",
    "url": "https://www.alphaxiv.org/abs/2510.11277v1",
    "arxiv_id": "2510.11277v1",
    "authors": "Guangyu Wei, Ke Han, Yueming Lyu, Yu Luo, Yue Jiang, Caifeng Shan, Nicu Sebe",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 11:11:46",
    "ori_summary": "Fake news detection becomes particularly challenging in real-time scenarios, where emerging events often lack sufficient supporting evidence. Existing approaches often rely heavily on external evidence and therefore struggle to generalize under evidence scarcity. To address this issue, we propose Evaluation-Aware Selection of Experts (EASE), a novel framework for real-time fake news detection that dynamically adapts its decision-making process according to the assessed sufficiency of available evidence. EASE introduces a sequential evaluation mechanism comprising three independent perspectives: (1) Evidence-based evaluation, which assesses evidence and incorporates it into decision-making only when the evidence is sufficiently supportive; (2) Reasoning-based evaluation, which leverages the world knowledge of large language models (LLMs) and applies them only when their reliability is adequately established; and (3) Sentiment-based fallback, which integrates sentiment cues when neither evidence nor reasoning is reliable. To enhance the accuracy of evaluation processes, EASE employs instruction tuning with pseudo labels to guide each evaluator in justifying its perspective-specific knowledge through interpretable reasoning. Furthermore, the expert modules integrate the evaluators' justified assessments with the news content to enable evaluation-aware decision-making, thereby enhancing overall detection accuracy. Moreover, we introduce RealTimeNews-25, a new benchmark comprising recent news for evaluating model generalization on emerging news with limited evidence. Extensive experiments demonstrate that EASE not only achieves state-of-the-art performance across multiple benchmarks, but also significantly improves generalization to real-time news. The code and dataset are available: https://github.com/wgyhhhh/EASE.",
    "summary": "",
    "translation": "面向证据稀缺场景下的实时虚假新闻检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于虚假新闻检测这一特定领域，属于内容安全和可信度验证范畴，与推荐系统、搜索或广告的核心技术进展没有直接关联。虚假新闻检测主要涉及内容理解、可信度评估等NLP应用，而非推荐排序、用户建模或广告投放等核心业务场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11254v1": {
    "title": "Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality",
    "url": "https://www.alphaxiv.org/abs/2510.11254v1",
    "arxiv_id": "2510.11254v1",
    "authors": "Jana Jung, Marlene Lutz, Indira Sen, Markus Strohmaier",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 10:43:49",
    "ori_summary": "Psychometric tests are increasingly used to assess psychological constructs in large language models (LLMs). However, it remains unclear whether these tests -- originally developed for humans -- yield meaningful results when applied to LLMs. In this study, we systematically evaluate the reliability and validity of human psychometric tests for three constructs: sexism, racism, and morality. We find moderate reliability across multiple item and prompt variations. Validity is evaluated through both convergent (i.e., testing theory-based inter-test correlations) and ecological approaches (i.e., testing the alignment between tests scores and behavior in real-world downstream tasks). Crucially, we find that psychometric test scores do not align, and in some cases even negatively correlate with, model behavior in downstream tasks, indicating low ecological validity. Our results highlight that systematic evaluations of psychometric tests is essential before interpreting their scores. They also suggest that psychometric tests designed for humans cannot be applied directly to LLMs without adaptation.",
    "summary": "",
    "translation": "心理测量测试对大型语言模型有效吗？关于性别歧视、种族歧视和道德观的测试评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM的伦理评估、偏见检测和心理测量测试，这些都属于被明确排除的伦理、公平性和非技术性话题范畴。论文内容不涉及推荐系统、搜索或广告的技术进步，也没有展示任何在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11238v1": {
    "title": "Attacks by Content: Automated Fact-checking is an AI Security Issue",
    "url": "https://www.alphaxiv.org/abs/2510.11238v1",
    "arxiv_id": "2510.11238v1",
    "authors": "Michael Schlichtkrull",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 10:18:48",
    "ori_summary": "When AI agents retrieve and reason over external documents, adversaries can manipulate the data they receive to subvert their behaviour. Previous research has studied indirect prompt injection, where the attacker injects malicious instructions. We argue that injection of instructions is not necessary to manipulate agents - attackers could instead supply biased, misleading, or false information. We term this an attack by content. Existing defenses, which focus on detecting hidden commands, are ineffective against attacks by content. To defend themselves and their users, agents must critically evaluate retrieved information, corroborating claims with external evidence and evaluating source trustworthiness. We argue that this is analogous to an existing NLP task, automated fact-checking, which we propose to repurpose as a cognitive self-defense tool for agents.",
    "summary": "",
    "translation": "基于内容的攻击：自动化事实核查是一个AI安全问题",
    "relevance_score": 1,
    "reasoning": "该论文关注AI安全和自动化事实核查，这属于安全领域，明确在无关主题列表中。论文标题没有表明与推荐系统、搜索或广告的核心进展、使能技术或直接应用有任何关联。安全相关主题被明确排除在关注范围之外。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11236v1": {
    "title": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression",
    "url": "https://www.alphaxiv.org/abs/2510.11236v1",
    "arxiv_id": "2510.11236v1",
    "authors": "Haoqi Yang, Yao Yao, Zuchao Li, Baoyuan Qi, Guoming Liu, Hai Zhao",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 10:17:21",
    "ori_summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks. However, their extensive memory requirements, particularly due to KV cache growth during long-text understanding and generation, present significant challenges for deployment in resource-constrained environments. Quantization has emerged as a promising solution to reduce memory consumption while preserving historical information. We propose XQuant, a training-free and plug-and-play framework that achieves ultra-low equivalent bit-width KV cache quantization. XQuant introduces two key innovations: a computationally negligible data-free calibration method and cross-layer KV cache compression, enabling quantization to sub-1.4 bits. Extensive experiments on TruthfulQA and LongBench demonstrate that XQuant outperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by achieving lower bit-width while maintaining superior performance, establishing a better trade-off between memory efficiency and model accuracy.",
    "summary": "",
    "translation": "XQuant：通过跨层压缩实现超低位KV缓存量化",
    "relevance_score": 8,
    "reasoning": "该论文专注于KV缓存量化技术，这直接属于'使能LLM技术'范畴，通过减少内存占用和提升推理效率来优化LLM。在推荐系统、搜索和广告中，更高效的LLM推理能够支持更大规模的部署、更快的响应速度和更低的运营成本，对于实时服务至关重要。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.11233v1": {
    "title": "CNSocialDepress: A Chinese Social Media Dataset for Depression Risk Detection and Structured Analysis",
    "url": "https://www.alphaxiv.org/abs/2510.11233v1",
    "arxiv_id": "2510.11233v1",
    "authors": "Jinyuan Xu, Tian Lan, Xintao Yu, Xue He, Hezhi Zhang, Ying Wang, Pierre Magistry, Mathieu Valette, Lei Li",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 10:14:18",
    "ori_summary": "Depression is a pressing global public health issue, yet publicly available Chinese-language resources for risk detection remain scarce and are mostly limited to binary classification. To address this limitation, we release CNSocialDepress, a benchmark dataset for depression risk detection from Chinese social media posts. The dataset contains 44,178 texts from 233 users, within which psychological experts annotated 10,306 depression-related segments. CNSocialDepress provides binary risk labels together with structured multi-dimensional psychological attributes, enabling interpretable and fine-grained analysis of depressive signals. Experimental results demonstrate its utility across a wide range of NLP tasks, including structured psychological profiling and fine-tuning of large language models for depression detection. Comprehensive evaluations highlight the dataset's effectiveness and practical value for depression risk identification and psychological analysis, thereby providing insights to mental health applications tailored for Chinese-speaking populations.",
    "summary": "",
    "translation": "CNSocialDepress：一个用于抑郁症风险检测与结构化分析的中文社交媒体数据集",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于医疗领域的抑郁症检测数据集构建，属于明确的医学应用范畴。虽然涉及社交媒体数据，但其核心目标是心理健康诊断而非推荐系统、搜索或广告的技术进步，完全落在用户指定的不相关主题范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11225v1": {
    "title": "A Theorem-Proving-Based Evaluation of Neural Semantic Parsing",
    "url": "https://www.alphaxiv.org/abs/2510.11225v1",
    "arxiv_id": "2510.11225v1",
    "authors": "Hayate Funakura, Hyunsoo Kim, Koji Mineshima",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 10:09:38",
    "ori_summary": "Graph-matching metrics such as Smatch are the de facto standard for evaluating neural semantic parsers, yet they capture surface overlap rather than logical equivalence. We reassess evaluation by pairing graph-matching with automated theorem proving. We compare two approaches to building parsers: supervised fine-tuning (T5-Small/Base) and few-shot in-context learning (GPT-4o/4.1/5), under normalized and unnormalized targets. We evaluate outputs using graph-matching, bidirectional entailment between source and target formulas with a first-order logic theorem prover, and well-formedness. Across settings, we find that models performing well on graph-matching often fail to produce logically equivalent formulas. Normalization reduces incidental target variability, improves well-formedness, and strengthens logical adequacy. Error analysis shows performance degrades with increasing formula complexity and with coordination, prepositional phrases, and passive voice; the dominant failures involve variable binding and indexing, and predicate naming. These findings highlight limits of graph-based metrics for reasoning-oriented applications and motivate logic-sensitive evaluation and training objectives together with simplified, normalized target representations. All code and data for our experiments are publicly available.",
    "summary": "",
    "translation": "基于定理证明的神经语义解析评估",
    "relevance_score": 2,
    "reasoning": "该论文关注神经语义解析的评估方法，属于纯粹的NLP评估基准范畴，与我的核心关注点无关。虽然语义解析在理论上可能与搜索相关，但论文重点在于定理证明的评估方法，没有展示在推荐系统、搜索或广告中的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11222v1": {
    "title": "Fairness Metric Design Exploration in Multi-Domain Moral Sentiment Classification using Transformer-Based Models",
    "url": "https://www.alphaxiv.org/abs/2510.11222v1",
    "arxiv_id": "2510.11222v1",
    "authors": "Battemuulen Naranbat, Seyed Sahand Mohammadi Ziabari, Yousuf Nasser Al Husaini, Ali Mohammed Mansoor Alsahag",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 10:05:57",
    "ori_summary": "Ensuring fairness in natural language processing for moral sentiment classification is challenging, particularly under cross-domain shifts where transformer models are increasingly deployed. Using the Moral Foundations Twitter Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work evaluates BERT and DistilBERT in a multi-label setting with in-domain and cross-domain protocols. Aggregate performance can mask disparities: we observe pronounced asymmetry in transfer, with Twitter->Reddit degrading micro-F1 by 14.9% versus only 1.5% for Reddit->Twitter. Per-label analysis reveals fairness violations hidden by overall scores; notably, the authority label exhibits Demographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of 0.40-0.41. To address this gap, we introduce the Moral Fairness Consistency (MFC) metric, which quantifies the cross-domain stability of moral foundation detection. MFC shows strong empirical validity, achieving a perfect negative correlation with Demographic Parity Difference (rho = -1.000, p < 0.001) while remaining independent of standard performance metrics. Across labels, loyalty demonstrates the highest consistency (MFC = 0.96) and authority the lowest (MFC = 0.78). These findings establish MFC as a complementary, diagnosis-oriented metric for fairness-aware evaluation of moral reasoning models, enabling more reliable deployment across heterogeneous linguistic contexts. .",
    "summary": "",
    "translation": "基于Transformer模型的多领域道德情感分类中公平性指标设计探索",
    "relevance_score": 1,
    "reasoning": "该论文明确聚焦于公平性指标设计，这属于明确的无关主题。虽然涉及Transformer模型，但核心关注点是公平性评估而非技术改进或应用，与搜索、推荐或广告系统的技术进展无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11221v1": {
    "title": "WebRouter: Query-specific Router via Variational Information Bottleneck for Cost-sensitive Web Agent",
    "url": "https://www.alphaxiv.org/abs/2510.11221v1",
    "arxiv_id": "2510.11221v1",
    "authors": "Tao Li, Jinlong Hu, Yang Wang, Junfeng Liu, Xuejun Liu",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 10:05:43",
    "ori_summary": "LLM-brained web agents offer powerful capabilities for web automation but face a critical cost-performance trade-off. The challenge is amplified by web agents' inherently complex prompts that include goals, action histories, and environmental states, leading to degraded LLM ensemble performance. To address this, we introduce WebRouter, a novel query-specific router trained from an information-theoretic perspective. Our core contribution is a cost-aware Variational Information Bottleneck (ca-VIB) objective, which learns a compressed representation of the input prompt while explicitly penalizing the expected operational cost. Experiments on five real-world websites from the WebVoyager benchmark show that WebRouter reduces operational costs by a striking 87.8\\% compared to a GPT-4o baseline, while incurring only a 3.8\\% accuracy drop.",
    "summary": "",
    "translation": "WebRouter：基于变分信息瓶颈的查询特定路由器，用于成本敏感的Web代理",
    "relevance_score": 3,
    "reasoning": "该论文涉及查询路由和成本敏感决策，与搜索系统中的查询路由和资源分配有一定相关性。然而，它主要关注Web代理场景，与推荐系统、搜索排名或广告的核心技术关联较弱，且未明确涉及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11218v1": {
    "title": "The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers",
    "url": "https://www.alphaxiv.org/abs/2510.11218v1",
    "arxiv_id": "2510.11218v1",
    "authors": "Saad Obaid ul Islam, Anne Lauscher, Goran Glavaš",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 10:00:58",
    "ori_summary": "Large language models (LLMs) can correctly answer \"When was Einstein born?\" yet fail to provide the same date when writing about Einstein's life revealing a fundamental inconsistency in how models access factual knowledge across task complexities. While models display impressive accuracy on factual question-answering benchmarks, the reliability gap between simple and complex queries remains poorly understood, eroding their trustworthiness. In this work, we introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a controlled evaluation framework that compares LLMs' answers to the same factual questions asked (a) in isolation (short) vs. (b) integrated into complex queries (long). Looking at 16 LLMs across 600 queries, we find a systematic misalignment of answers to the corresponding short and long queries. We further uncover position-dependent accuracy loss and momentum effects where consecutive correct or incorrect answers create self-reinforcing patterns. Through mechanistic analysis, we find that aligned facts activate overlapping model internals, and that metrics based on mechanistic similarity can predict short-long answer alignment with up to 78% accuracy. Our work establishes factual consistency over query complexity as an important aspect of LLMs' trustworthiness and challenges current evaluation practices, which implicitly assume that good performance for simple factual queries implies reliability in more complex knowledge-seeking tasks too.",
    "summary": "",
    "translation": "LLM短答案与长答案之间事实对齐的奇特案例",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLM在不同长度回答中的事实一致性，这属于幻觉和评估基准等纯NLP中心主题，与我的核心关注点无关。虽然事实对齐在理论上可能影响推荐系统的可信度，但论文没有明确展示在推荐、搜索或广告中的实际应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11217v1": {
    "title": "Domain-Specific Data Generation Framework for RAG Adaptation",
    "url": "https://www.alphaxiv.org/abs/2510.11217v1",
    "arxiv_id": "2510.11217v1",
    "authors": "Chris Xing Tian, Weihao Xie, Zhen Chen, Zhengyuan Yi, Hui Liu, Haoliang Li, Shiqi Wang, Siwei Ma",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 09:59:49",
    "ori_summary": "Retrieval-Augmented Generation (RAG) combines the language understanding and reasoning power of large language models (LLMs) with external retrieval to enable domain-grounded responses. Effectively adapting RAG systems to domain-specific settings requires specialized, context-rich training data beyond general-purpose question-answering. Here, we propose RAGen, a scalable and modular framework for generating domain-grounded question-answer-context (QAC) triples tailored to diverse RAG adaptation approaches. RAGen produces these QAC triples by identifying key concepts in documents, generating diverse questions guided by Bloom's Taxonomy-inspired principles, and pairing them with precise answers extracted from relevant contexts. RAGen supports multiple RAG adaptation strategies, including the optimization of key components such as the LLM, retriever, and embedding model, etc. Its modular pipeline features semantic chunking, hierarchical concept extraction, and multi-chunk retrieval, along with the introduction of curated distractor contexts to promote robust reasoning. Designed for scalability, RAGen efficiently handles large and evolving document corpora without redundant processing, making it especially suitable for dynamic evolving domains such as scientific research and enterprise knowledge bases.",
    "summary": "",
    "translation": "面向RAG适配的领域特定数据生成框架",
    "relevance_score": 7,
    "reasoning": "该论文涉及检索增强生成(RAG)的数据生成框架，这属于直接LLM应用范畴，对搜索和推荐系统中的知识增强和个性化响应生成具有直接应用价值。通过领域特定数据生成，可以提升搜索和推荐系统中RAG组件的效果和适应性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.11210v1": {
    "title": "Discursive Circuits: How Do Language Models Understand Discourse Relations?",
    "url": "https://www.alphaxiv.org/abs/2510.11210v1",
    "arxiv_id": "2510.11210v1",
    "authors": "Yisong Miao, Min-Yen Kan",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-13 09:45:49",
    "ori_summary": "Which components in transformer language models are responsible for discourse understanding? We hypothesize that sparse computational graphs, termed as discursive circuits, control how models process discourse relations. Unlike simpler tasks, discourse relations involve longer spans and complex reasoning. To make circuit discovery feasible, we introduce a task called Completion under Discourse Relation (CuDR), where a model completes a discourse given a specified relation. To support this task, we construct a corpus of minimal contrastive pairs tailored for activation patching in circuit discovery. Experiments show that sparse circuits ($\\approx 0.2\\%$ of a full GPT-2 model) recover discourse understanding in the English PDTB-based CuDR task. These circuits generalize well to unseen discourse frameworks such as RST and SDRT. Further analysis shows lower layers capture linguistic features such as lexical semantics and coreference, while upper layers encode discourse-level abstractions. Feature utility is consistent across frameworks (e.g., coreference supports Expansion-like relations).",
    "summary": "",
    "translation": "话语回路：语言模型如何理解话语关系？",
    "relevance_score": 2,
    "reasoning": "该论文主要研究语言模型对话语关系的理解能力，这属于NLP基础研究范畴，与推荐系统、搜索或广告的核心技术关联度较低。虽然话语理解在理论上可能对搜索中的长文档理解有一定帮助，但这种应用潜力非常间接且不明确，不符合当前关注的核心技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11196v1": {
    "title": "Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations",
    "url": "https://www.alphaxiv.org/abs/2510.11196v1",
    "arxiv_id": "2510.11196v1",
    "authors": "Johannes Moll, Markus Graf, Tristan Lemke, Nicolas Lenhart, Daniel Truhn, Jean-Benoit Delbrouck, Jiazhen Pan, Daniel Rueckert, Lisa C. Adams, Keno K. Bressem",
    "categories": "cs.CL, cs.CV",
    "pub_date": "2025-10-13 09:28:22",
    "ori_summary": "Vision-language models (VLMs) often produce chain-of-thought (CoT) explanations that sound plausible yet fail to reflect the underlying decision process, undermining trust in high-stakes clinical use. Existing evaluations rarely catch this misalignment, prioritizing answer accuracy or adherence to formats. We present a clinically grounded framework for chest X-ray visual question answering (VQA) that probes CoT faithfulness via controlled text and image modifications across three axes: clinical fidelity, causal attribution, and confidence calibration. In a reader study (n=4), evaluator-radiologist correlations fall within the observed inter-radiologist range for all axes, with strong alignment for attribution (Kendall's $\\tau_b=0.670$), moderate alignment for fidelity ($\\tau_b=0.387$), and weak alignment for confidence tone ($\\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows that answer accuracy and explanation quality are decoupled, acknowledging injected cues does not ensure grounding, and text cues shift explanations more than visual cues. While some open-source models match final answer accuracy, proprietary models score higher on attribution (25.0% vs. 1.4%) and often on fidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to evaluate beyond final answer accuracy.",
    "summary": "",
    "translation": "使用多模态扰动评估医学视觉语言模型的推理忠实性",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的视觉语言模型评估和忠实性分析，属于医学应用和评估基准范畴。这些主题明确属于不相关领域，与推荐系统、搜索或广告的核心技术进展没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11184v1": {
    "title": "Can Tool-Integrated Reinforcement Learning Generalize Across Diverse Domains?",
    "url": "https://www.alphaxiv.org/abs/2510.11184v1",
    "arxiv_id": "2510.11184v1",
    "authors": "Zhengyu Chen, Jinluan Yang, Teng Xiao, Ruochen Zhou, Luan Zhang, Xiangyu Xi, Xiaowei Shi, Wei Wang, Jinggang Wang",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-13 09:19:13",
    "ori_summary": "Recent advances in large language models (LLMs) have demonstrated remarkable capabilities in reasoning and tool utilization. However, the generalization of tool-augmented reinforcement learning (RL) across diverse domains remains underexplored. In this work, we investigate the cross-domain generalization of an LLM agent equipped with a code interpreter tool, which is exclusively trained on mathematical problem-solving tasks. Despite the restricted training domain, we evaluate the agent's performance across several distinct reasoning domains. The results reveal that RL-based tool usage learned from mathematical tasks can be effectively transferred to complex tasks in other domains, enabling great task performance and high token efficiency. To facilitate this cross-domain transfer, we propose a Tool Generalization Reinforcement Learning (TGRL) framework designed to promote domain-agnostic learning and skill migration, encompassing: (i) a standardized tool interface that abstracts domain-specific nuances through consistent formatting and explicit termination, fostering transferable invocation patterns; (ii) a dual-component reward system that decomposes rewards to incentivize generalizable behaviors like tool efficiency and reasoning abstraction, ensuring alignment and robustness across domain shifts; and (iii) an XML-based prompt template that separates thinking, tool calls, and responses to encourage modular, domain-invariant planning and coherent multi-turn interactions. Extensive experiments across diverse benchmarks validate our approach, achieving state-of-the-art performance and highlighting the cross-domain potential of Tool RL for LLM reasoning.",
    "summary": "",
    "translation": "工具集成强化学习能否在不同领域间实现泛化？",
    "relevance_score": 2,
    "reasoning": "该论文关注强化学习与工具集成的泛化能力，属于强化学习领域而非推荐系统、搜索或广告的核心技术。虽然工具集成可能涉及外部知识调用，但论文重点在于跨领域泛化而非在推荐/搜索/广告中的具体应用，与当前关注的LLM技术、Transformer架构或直接应用相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11170v1": {
    "title": "EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling",
    "url": "https://www.alphaxiv.org/abs/2510.11170v1",
    "arxiv_id": "2510.11170v1",
    "authors": "Daniel Scalena, Leonidas Zotos, Elisabetta Fersini, Malvina Nissim, Ahmet Üstün",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-13 09:04:28",
    "ori_summary": "With the rise of reasoning language models and test-time scaling methods as a paradigm for improving model performance, substantial computation is often required to generate multiple candidate sequences from the same prompt. This enables exploration of different reasoning paths toward the correct solution, however, allocates the same compute budget for each prompt. Grounded on the assumption that different prompts carry different degrees of complexity, and thus different computation needs, we propose EAGer, a training-free generation method that leverages model uncertainty through token-wise entropy distribution to reduce redundant computation and concurrently improve overall performance. EAGer allows branching to multiple reasoning paths only in the presence of high-entropy tokens, and then reallocates the saved compute budget to the instances where exploration of alternative paths is most needed. We find that across multiple open-source models on complex reasoning benchmarks such as AIME 2025, EAGer can reallocate the budget without accessing target labels, achieving the best efficiency-performance trade-off in terms of reasoning length and Pass@k. When target labels are accessible, EAGer generates up to 65% fewer tokens (hence saving compute) and achieves up to 37% improvement in Pass@k compared to the Full Parallel Sampling.",
    "summary": "",
    "translation": "EAGER：用于自适应推理时间缩放的熵感知生成",
    "relevance_score": 8,
    "reasoning": "该论文关注推理时自适应缩放和熵感知生成，这属于'使能LLM技术'范畴，直接涉及LLM推理效率优化。在推荐系统、搜索和广告中，高效的推理时间缩放对于处理大规模用户请求、实现动态资源分配和降低服务成本具有重要应用价值，能够支撑实时个性化服务。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.11167v1": {
    "title": "Bridging Gaps in Hate Speech Detection: Meta-Collections and Benchmarks for Low-Resource Iberian Languages",
    "url": "https://www.alphaxiv.org/abs/2510.11167v1",
    "arxiv_id": "2510.11167v1",
    "authors": "Paloma Piot, José Ramom Pichel Campos, Javier Parapar",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 08:58:02",
    "ori_summary": "Hate speech poses a serious threat to social cohesion and individual well-being, particularly on social media, where it spreads rapidly. While research on hate speech detection has progressed, it remains largely focused on English, resulting in limited resources and benchmarks for low-resource languages. Moreover, many of these languages have multiple linguistic varieties, a factor often overlooked in current approaches. At the same time, large language models require substantial amounts of data to perform reliably, a requirement that low-resource languages often cannot meet. In this work, we address these gaps by compiling a meta-collection of hate speech datasets for European Spanish, standardised with unified labels and metadata. This collection is based on a systematic analysis and integration of existing resources, aiming to bridge the data gap and support more consistent and scalable hate speech detection. We extended this collection by translating it into European Portuguese and into a Galician standard that is more convergent with Spanish and another Galician variant that is more convergent with Portuguese, creating aligned multilingual corpora. Using these resources, we establish new benchmarks for hate speech detection in Iberian languages. We evaluate state-of-the-art large language models in zero-shot, few-shot, and fine-tuning settings, providing baseline results for future research. Moreover, we perform a cross-lingual analysis with our target languages. Our findings underscore the importance of multilingual and variety-aware approaches in hate speech detection and offer a foundation for improved benchmarking in underrepresented European languages.",
    "summary": "",
    "translation": "弥合仇恨言论检测中的差距：面向低资源伊比利亚语言的元集合与基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于仇恨言论检测这一特定NLP任务，属于内容安全领域，与推荐系统、搜索或广告的核心技术无关。虽然涉及多语言处理，但主要关注低资源语言的基准构建，没有明显的RecSys/Search/Ads应用潜力。这属于纯粹的NLP应用研究，不在当前关注范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11160v1": {
    "title": "One Size Does Not Fit All: Exploring Variable Thresholds for Distance-Based Multi-Label Text Classification",
    "url": "https://www.alphaxiv.org/abs/2510.11160v1",
    "arxiv_id": "2510.11160v1",
    "authors": "Jens Van Nooten, Andriy Kosar, Guy De Pauw, Walter Daelemans",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 08:52:14",
    "ori_summary": "Distance-based unsupervised text classification is a method within text classification that leverages the semantic similarity between a label and a text to determine label relevance. This method provides numerous benefits, including fast inference and adaptability to expanding label sets, as opposed to zero-shot, few-shot, and fine-tuned neural networks that require re-training in such cases. In multi-label distance-based classification and information retrieval algorithms, thresholds are required to determine whether a text instance is \"similar\" to a label or query. Similarity between a text and label is determined in a dense embedding space, usually generated by state-of-the-art sentence encoders. Multi-label classification complicates matters, as a text instance can have multiple true labels, unlike in multi-class or binary classification, where each instance is assigned only one label. We expand upon previous literature on this underexplored topic by thoroughly examining and evaluating the ability of sentence encoders to perform distance-based classification. First, we perform an exploratory study to verify whether the semantic relationships between texts and labels vary across models, datasets, and label sets by conducting experiments on a diverse collection of realistic multi-label text classification (MLTC) datasets. We find that similarity distributions show statistically significant differences across models, datasets and even label sets. We propose a novel method for optimizing label-specific thresholds using a validation set. Our label-specific thresholding method achieves an average improvement of 46% over normalized 0.5 thresholding and outperforms uniform thresholding approaches from previous work by an average of 14%. Additionally, the method demonstrates strong performance even with limited labeled examples.",
    "summary": "",
    "translation": "一刀切不可行：探索基于距离的多标签文本分类中的可变阈值",
    "relevance_score": 3,
    "reasoning": "该论文主要研究多标签文本分类中的阈值优化问题，属于传统NLP技术范畴。虽然文本分类在搜索系统中可能有应用（如文档分类），但论文没有明确涉及推荐系统、广告或Transformer架构的核心进展，与当前关注的LLM技术、推荐系统核心算法或异构数据统一建模等焦点领域关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11151v1": {
    "title": "TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code",
    "url": "https://www.alphaxiv.org/abs/2510.11151v1",
    "arxiv_id": "2510.11151v1",
    "authors": "Alexander Sternfeld, Andrei Kucharavy, Ljiljana Dolamic",
    "categories": "cs.CL, cs.CR",
    "pub_date": "2025-10-13 08:44:01",
    "ori_summary": "Large language Models (LLMs) have shown remarkable proficiency in code generation tasks across various programming languages. However, their outputs often contain subtle but critical vulnerabilities, posing significant risks when deployed in security-sensitive or mission-critical systems. This paper introduces TypePilot, an agentic AI framework designed to enhance the security and robustness of LLM-generated code by leveraging strongly typed and verifiable languages, using Scala as a representative example. We evaluate the effectiveness of our approach in two settings: formal verification with the Stainless framework and general-purpose secure code generation. Our experiments with leading open-source LLMs reveal that while direct code generation often fails to enforce safety constraints, just as naive prompting for more secure code, our type-focused agentic pipeline substantially mitigates input validation and injection vulnerabilities. The results demonstrate the potential of structured, type-guided LLM workflows to improve the SotA of the trustworthiness of automated code generation in high-assurance domains.",
    "summary": "",
    "translation": "TypePilot：利用Scala类型系统保障LLM生成代码的安全性",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM生成代码的安全性保障，属于LLM安全性和代码生成领域。虽然涉及LLM技术，但其应用场景（代码安全）与推荐系统、搜索或广告的核心技术栈没有直接关联，且安全主题属于明确排除的无关话题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11144v1": {
    "title": "$How^{2}$: How to learn from procedural How-to questions",
    "url": "https://www.alphaxiv.org/abs/2510.11144v1",
    "arxiv_id": "2510.11144v1",
    "authors": "Gautier Dagan, Frank Keller, Alex Lascarides",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-13 08:35:20",
    "ori_summary": "An agent facing a planning problem can use answers to how-to questions to reduce uncertainty and fill knowledge gaps, helping it solve both current and future tasks. However, their open ended nature, where valid answers to \"How do I X?\" range from executable actions to high-level descriptions of X's sub-goals, makes them challenging for AI agents to ask, and for AI experts to answer, in ways that support efficient planning. We introduce $How^{2}$, a memory agent framework that enables agents to ask how-to questions, store the answers, and reuse them for lifelong learning in interactive environments. We evaluate our approach in Plancraft, a Minecraft crafting environment, where agents must complete an assembly task by manipulating inventory items. Using teacher models that answer at varying levels of abstraction, from executable action sequences to high-level subgoal descriptions, we show that lifelong learning agents benefit most from answers that are abstracted and decoupled from the current state. $How^{2}$ offers a way for LLM-based agents to improve their planning capabilities over time by asking questions in interactive environments.",
    "summary": "",
    "translation": "How²：如何从程序性How-to问题中学习",
    "relevance_score": 2,
    "reasoning": "该论文关注程序性How-to问题的学习，这主要属于问答系统和知识获取领域。虽然程序性知识在搜索系统中可能有潜在应用（如回答用户的操作指导类问题），但该研究更偏向于通用的问答能力，与推荐系统、广告或核心的Transformer架构进展关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11104v1": {
    "title": "Enhancing LLM Reasoning via Non-Human-Like Reasoning Path Preference Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.11104v1",
    "arxiv_id": "2510.11104v1",
    "authors": "Junjie Lu, Yuliang Liu, Chaofeng Qu, Wei Shen, Zhouhan Lin, Min Xu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 07:51:16",
    "ori_summary": "Current approaches for strengthening LLM reasoning tend to introduce a training bias toward human-like reasoning trajectories. In step-wise preference optimization, in particular, dependence on human or higher-capacity model annotations for intermediate steps limits exploration of alternative, non-human-like reasoning paths and thus constrains achievable performance. Furthermore, through a small-scale pilot study, we observed that in approximately 75% of cases, the model's first erroneous step occurs after the lowest-confidence point. This suggests that guiding the model at its lowest-confidence point before an error provides more accurate supervision than locating the first explicit error. In this paper, we propose Confidence-Guided Reasoning Path Preference Optimization (CGPO), a method that leverages a confidence signal to identify points of maximal uncertainty in the model's reasoning process and applies self-generated, non-human-like reasoning-path guidance to mitigate trajectory drift. Our experiments span diverse models applied to both code and mathematical reasoning tasks. The results show that, with the same amount of training data, our method using data generated by a small model can achieve better performance in most cases compared with approaches using data generated by a strong model or human-annotated.",
    "summary": "",
    "translation": "通过非人类推理路径偏好优化增强大语言模型推理能力",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能LLM技术'范畴，专注于提升LLM的推理能力。在搜索和推荐系统中，增强的推理能力可以直接应用于复杂查询理解、多步骤推理推荐、以及跨模态语义理解等场景，能够显著提升系统处理复杂用户意图和上下文依赖的能力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.11098v1": {
    "title": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
    "url": "https://www.alphaxiv.org/abs/2510.11098v1",
    "arxiv_id": "2510.11098v1",
    "authors": "Jiliang Hu, Wenfu Wang, Zuchao Li, Chenxing Li, Yiyang Zhao, Hanzhao Li, Liqiang Zhang, Meng Yu, Dong Yu",
    "categories": "cs.SD, cs.CL",
    "pub_date": "2025-10-13 07:45:52",
    "ori_summary": "Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited -- they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench) -- a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models.",
    "summary": "",
    "translation": "VCB基准：面向音频基础大语言模型对话代理的评估基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于音频模态的LLM对话代理评估基准，属于纯粹的评估基准研究。虽然涉及LLM技术，但专注于音频对话评估这一特定领域，与推荐系统、搜索或广告的核心技术进展缺乏直接关联，且评估基准本身属于被排除的无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11052v1": {
    "title": "Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States",
    "url": "https://www.alphaxiv.org/abs/2510.11052v1",
    "arxiv_id": "2510.11052v1",
    "authors": "Qinglin Zhu, Yizhen Yao, Runcong Zhao, Yanzheng Xiang, Amrutha Saseendran, Chen Jin, Philip Alexander Teare, Bin Liang, Yulan He, Lin Gui",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 06:38:13",
    "ori_summary": "Autoregressive (AR) models remain the standard for natural language generation but still suffer from high latency due to strictly sequential decoding. Recent diffusion-inspired approaches, such as LlaDA and Dream, mitigate this by generating in parallel, yet they suffer from two core limitations: information loss, as predictive distributions for non-finalized tokens are discarded at each step, and premature commitment, where local decisions are made without sufficient global coordination. We introduce Latent Refinement Decoding (LRD), a two-stage framework with Latent Refinement and a Predictive Feedback Loop. The first stage maintains masked positions as distributional mixtures of predicted tokens and the mask embedding, allowing the model to establish more globally consistent beliefs. The second stage progressively finalizes confident tokens while retaining uncertain ones for iterative feedback. KL-divergence dynamics provide a principled and reliable criterion for convergence and early stopping. Experiments across coding (HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that LRD improves accuracy while delivering speedups of up to 10.6x, making it a strong and versatile alternative for parallel sequence generation.",
    "summary": "",
    "translation": "潜在精炼解码：通过精炼信念状态增强基于扩散的语言模型",
    "relevance_score": 7,
    "reasoning": "该论文属于'赋能LLM技术'范畴，聚焦扩散模型在语言生成中的改进。扩散模型作为新兴的生成范式，在推荐系统和搜索中具有潜在应用价值，可用于生成更高质量的推荐内容、搜索摘要或多样化广告文案。精炼信念状态的技术可能提升生成内容的准确性和一致性，这对推荐和搜索系统的质量至关重要。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.11040v1": {
    "title": "Enabling Doctor-Centric Medical AI with LLMs through Workflow-Aligned Tasks and Benchmarks",
    "url": "https://www.alphaxiv.org/abs/2510.11040v1",
    "arxiv_id": "2510.11040v1",
    "authors": "Wenya Xie, Qingying Xiao, Yu Zheng, Xidong Wang, Junying Chen, Ke Ji, Anningzhe Gao, Prayag Tiwari, Xiang Wan, Feng Jiang, Benyou Wang",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 06:18:27",
    "ori_summary": "The rise of large language models (LLMs) has transformed healthcare by offering clinical guidance, yet their direct deployment to patients poses safety risks due to limited domain expertise. To mitigate this, we propose repositioning LLMs as clinical assistants that collaborate with experienced physicians rather than interacting with patients directly. We conduct a two-stage inspiration-feedback survey to identify real-world needs in clinical workflows. Guided by this, we construct DoctorFLAN, a large-scale Chinese medical dataset comprising 92,000 Q&A instances across 22 clinical tasks and 27 specialties. To evaluate model performance in doctor-facing applications, we introduce DoctorFLAN-test (550 single-turn Q&A items) and DotaBench (74 multi-turn conversations). Experimental results with over ten popular LLMs demonstrate that DoctorFLAN notably improves the performance of open-source LLMs in medical contexts, facilitating their alignment with physician workflows and complementing existing patient-oriented models. This work contributes a valuable resource and framework for advancing doctor-centered medical LLM development",
    "summary": "",
    "translation": "通过工作流对齐任务和基准测试，利用大型语言模型实现以医生为中心的医疗人工智能",
    "relevance_score": 1,
    "reasoning": "该论文明确聚焦于医疗AI领域，涉及医生工作流和医疗基准测试，这属于明确的无关主题范畴。论文虽然提到LLMs，但应用场景完全限定在医疗领域，与推荐系统、搜索或广告没有任何潜在关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11031v1": {
    "title": "LogiNumSynth: Synthesizing Joint Logical-Numerical Reasoning Problems for Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11031v1",
    "arxiv_id": "2510.11031v1",
    "authors": "Yiwei Liu, Yucheng Li, Xiao Li, Gong Cheng",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 06:01:02",
    "ori_summary": "Joint logical-numerical reasoning remains a major challenge for language models, yet existing datasets rely on fixed rule sets and offer limited control over task complexity, constraining their generalizability for evaluation and training. We present LogiNumSynth, a flexible natural language problem synthesizer that synthesizes tasks requiring proficiency in joint logical reasoning (e.g., rule-based reasoning) and numerical reasoning (e.g., arithmetic computation). LogiNumSynth supports fine-grained control over reasoning world richness, logical reasoning depth, and the complexity of numerical computations, enabling flexible data synthesis across difficulty levels. We demonstrate three key contributions: (1) Synthesizer -- synthesizing fully controllable joint reasoning tasks over natural language; (2) Evaluation & Process Analysis -- evaluating both process accuracy and answer accuracy; (3) Targeted Training -- using synthesized data to enhance LLMs' reasoning performance. Experiments with multiple LLMs highlight persistent weaknesses in logical-numerical reasoning, showing that LogiNumSynth can serve as both a diagnostic tool and a source of targeted supervision for advancing integrated reasoning skills.",
    "summary": "",
    "translation": "LogiNumSynth：为语言模型合成联合逻辑-数值推理问题",
    "relevance_score": 3,
    "reasoning": "该论文专注于逻辑-数值推理问题的合成，属于LLM推理能力的基础研究。虽然逻辑推理能力对于搜索和推荐系统中的复杂查询理解有一定潜在价值，但论文本身没有明确展示在RecSys/Search/Ads领域的直接应用场景，相关性较为间接。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11004v1": {
    "title": "Automating Structural Engineering Workflows with Large Language Model Agents",
    "url": "https://www.alphaxiv.org/abs/2510.11004v1",
    "arxiv_id": "2510.11004v1",
    "authors": "Haoran Liang, Yufa Zhou, Mohammad Talebi Kalaleh, Qipei Mei",
    "categories": "cs.MA, cs.AI, cs.CE, cs.CL",
    "pub_date": "2025-10-13 04:38:46",
    "ori_summary": "We introduce $\\textbf{MASSE}$, the first Multi-Agent System for Structural Engineering, effectively integrating large language model (LLM)-based agents with real-world engineering workflows. Structural engineering is a fundamental yet traditionally stagnant domain, with core workflows remaining largely unchanged for decades despite its substantial economic impact and global market size. Recent advancements in LLMs have significantly enhanced their ability to perform complex reasoning, long-horizon planning, and precise tool utilization -- capabilities well aligned with structural engineering tasks such as interpreting design codes, executing load calculations, and verifying structural capacities. We present a proof-of-concept showing that most real-world structural engineering workflows can be fully automated through a training-free LLM-based multi-agent system. MASSE enables immediate deployment in professional environments, and our comprehensive validation on real-world case studies demonstrates that it can reduce expert workload from approximately two hours to mere minutes, while enhancing both reliability and accuracy in practical engineering scenarios.",
    "summary": "",
    "translation": "使用大型语言模型智能体自动化结构工程工作流程",
    "relevance_score": 2,
    "reasoning": "该论文专注于结构工程领域的特定应用，属于土木工程专业领域，与推荐系统、搜索或广告的核心关注点无关。虽然涉及LLM技术，但其应用场景过于特定且与目标领域没有明显关联，无法为RecSys/Search/Ads提供直接价值或技术启发。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11001v1": {
    "title": "DND: Boosting Large Language Models with Dynamic Nested Depth",
    "url": "https://www.alphaxiv.org/abs/2510.11001v1",
    "arxiv_id": "2510.11001v1",
    "authors": "Tieyuan Chen, Xiaodong Chen, Haoxing Chen, Zhenzhong Lan, Weiyao Lin, Jianguo Li",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 04:22:57",
    "ori_summary": "We introduce Dynamic Nested Depth (DND), a novel method that improves performance for off-the-shelf LLMs by selecting critical tokens to reprocess in a nested depth manner. Specifically, at the end of the given transformer layer, DND identifies more critical tokens with a router and feeds them back for an extra round of processing, effectively ``reviewing\" difficult tokens while avoiding redundant computation for easier ones. The dynamic selection mechanism is tailored for precise control via two novel strategies: a router controlling loss to enhance token selection distinguishability, and a threshold control scheme to ensure selection stability. We demonstrate the effectiveness of DND by directly integrating it into pre-trained dense and MoE models during a post-training phase. On diverse benchmarks, this approach boosts the performances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by 0.87%, all with a minimal parameter and computing increase.",
    "summary": "",
    "translation": "DND：通过动态嵌套深度提升大型语言模型",
    "relevance_score": 8,
    "reasoning": "该论文提出了一种提升LLM性能的新方法（动态嵌套深度），属于'使能LLM技术'范畴。这种效率提升技术可以直接应用于推荐系统和搜索中的LLM推理优化，通过更高效的架构实现更快的响应时间和更低的计算成本，对于大规模部署至关重要。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.10998v1": {
    "title": "ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring Scenarios",
    "url": "https://www.alphaxiv.org/abs/2510.10998v1",
    "arxiv_id": "2510.10998v1",
    "authors": "Mahika Phutane, Hayoung Jung, Matthew Kim, Tanushree Mitra, Aditya Vashistha",
    "categories": "cs.CL, cs.AI, cs.CY, cs.HC, cs.LG",
    "pub_date": "2025-10-13 04:18:23",
    "ori_summary": "Large language models (LLMs) are increasingly under scrutiny for perpetuating identity-based discrimination in high-stakes domains such as hiring, particularly against people with disabilities (PwD). However, existing research remains largely Western-centric, overlooking how intersecting forms of marginalization--such as gender and caste--shape experiences of PwD in the Global South. We conduct a comprehensive audit of six LLMs across 2,820 hiring scenarios spanning diverse disability, gender, nationality, and caste profiles. To capture subtle intersectional harms and biases, we introduce ABLEIST (Ableism, Inspiration, Superhumanization, and Tokenism), a set of five ableism-specific and three intersectional harm metrics grounded in disability studies literature. Our results reveal significant increases in ABLEIST harms towards disabled candidates--harms that many state-of-the-art models failed to detect. These harms were further amplified by sharp increases in intersectional harms (e.g., Tokenism) for gender and caste-marginalized disabled candidates, highlighting critical blind spots in current safety tools and the need for intersectional safety evaluations of frontier models in high-stakes domains like hiring.",
    "summary": "",
    "translation": "ABLEIST：大语言模型生成招聘场景中的交叉性残疾偏见",
    "relevance_score": 1,
    "reasoning": "该论文主要关注LLM生成内容中的残疾偏见问题，这属于公平性、伦理等非技术性话题，与我的关注领域无关。论文标题明确指向偏见检测和公平性问题，而非推荐系统、搜索或广告中的技术进展或应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10994v1": {
    "title": "DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety",
    "url": "https://www.alphaxiv.org/abs/2510.10994v1",
    "arxiv_id": "2510.10994v1",
    "authors": "Wei-Chieh Huang, Henry Peng Zou, Yaozu Wu, Dongyuan Li, Yankai Chen, Weizhi Zhang, Yangning Li, Angelo Zangari, Jizhou Guo, Chunyu Miao, Liancheng Fang, Langzhou He, Renhe Jiang, Philip S. Yu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 04:11:21",
    "ori_summary": "Deep research frameworks have shown promising capabilities in synthesizing comprehensive reports from web sources. While deep research possesses significant potential to address complex issues through planning and research cycles, existing frameworks are deficient in sufficient evaluation procedures and stage-specific protections. They typically treat evaluation as exact match accuracy of question-answering, but overlook crucial aspects of report quality such as credibility, coherence, breadth, depth, and safety. This oversight may result in hazardous or malicious sources being integrated into the final report. To address these issues, we introduce DEEPRESEARCHGUARD, a comprehensive framework featuring four-stage safeguards with open-domain evaluation of references and reports. We assess performance across multiple metrics, e.g., defense success rate and over-refusal rate, and five key report dimensions. In the absence of a suitable safety benchmark, we introduce DRSAFEBENCH, a stage-wise benchmark for deep research safety. Our evaluation spans diverse state-of-the-art LLMs, including GPT-4o, Gemini-2.5-flash, DeepSeek-v3, and o4-mini. DEEPRESEARCHGUARD achieves an average defense success rate improvement of 18.16% while reducing over-refusal rate by 6%. The input guard provides the most substantial early-stage protection by filtering out obvious risks, while the plan and research guards enhance citation discipline and source credibility. Through extensive experiments, we show that DEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware defenses that effectively block harmful content propagation, while systematically improving report quality without excessive over-refusal rates. The code can be found via https://github.com/Jasonya/DeepResearchGuard.",
    "summary": "",
    "translation": "DeepResearchGuard：具备开放域评估和多阶段安全护栏的深度研究",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于安全护栏和评估机制，这属于被明确排除的隐私、安全、伦理等非技术性话题范畴。论文内容似乎主要关注研究过程中的安全保障，与推荐系统、搜索、广告的核心技术进展或使能技术没有任何直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10991v1": {
    "title": "A Survey on Agentic Multimodal Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.10991v1",
    "arxiv_id": "2510.10991v1",
    "authors": "Huanjin Yao, Ruifei Zhang, Jiaxing Huang, Jingyi Zhang, Yibo Wang, Bo Fang, Ruolin Zhu, Yongcheng Jing, Shunyu Liu, Guanbin Li, Dacheng Tao",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-10-13 04:07:01",
    "ori_summary": "With the recent emergence of revolutionary autonomous agentic systems, research community is witnessing a significant shift from traditional static, passive, and domain-specific AI agents toward more dynamic, proactive, and generalizable agentic AI. Motivated by the growing interest in agentic AI and its potential trajectory toward AGI, we present a comprehensive survey on Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we explore the emerging paradigm of agentic MLLMs, delineating their conceptual foundations and distinguishing characteristics from conventional MLLM-based agents. We establish a conceptual framework that organizes agentic MLLMs along three fundamental dimensions: (i) Agentic internal intelligence functions as the system's commander, enabling accurate long-horizon planning through reasoning, reflection, and memory; (ii) Agentic external tool invocation, whereby models proactively use various external tools to extend their problem-solving capabilities beyond their intrinsic knowledge; and (iii) Agentic environment interaction further situates models within virtual or physical environments, allowing them to take actions, adapt strategies, and sustain goal-directed behavior in dynamic real-world scenarios. To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs. Finally, we review the downstream applications of agentic MLLMs and outline future research directions for this rapidly evolving field. To continuously track developments in this rapidly evolving field, we will also actively update a public repository at https://github.com/HJYao00/Awesome-Agentic-MLLMs.",
    "summary": "",
    "translation": "智能体多模态大语言模型综述",
    "relevance_score": 6,
    "reasoning": "该论文探讨多模态LLM与智能体系统的结合，属于'赋能LLM技术'范畴。多模态智能体可应用于搜索和推荐系统，通过整合视觉、文本等多源信息来增强用户理解和交互能力，例如在电商推荐中处理商品图像与用户文本查询的复杂交互。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.10990v1": {
    "title": "Secret-Protected Evolution for Differentially Private Synthetic Text Generation",
    "url": "https://www.alphaxiv.org/abs/2510.10990v1",
    "arxiv_id": "2510.10990v1",
    "authors": "Tianze Wang, Zhaoyu Chen, Jian Du, Yingtai Xiao, Linjun Zhang, Qiang Yan",
    "categories": "cs.CR, cs.CL, cs.NE",
    "pub_date": "2025-10-13 04:05:42",
    "ori_summary": "Text data has become extremely valuable on large language models (LLMs) and even lead to general artificial intelligence (AGI). A lot of high-quality text in the real world is private and cannot be freely used due to privacy concerns. Therefore, differentially private (DP) synthetic text generation has been proposed, aiming to produce high-utility synthetic data while protecting sensitive information. However, existing DP synthetic text generation imposes uniform guarantees that often overprotect non-sensitive content, resulting in substantial utility loss and computational overhead. Therefore, we propose Secret-Protected Evolution (SecPE), a novel framework that extends private evolution with secret-aware protection. Theoretically, we show that SecPE satisfies $(\\mathrm{p}, \\mathrm{r})$-secret protection, constituting a relaxation of Gaussian DP that enables tighter utility-privacy trade-offs, while also substantially reducing computational complexity relative to baseline methods. Empirically, across the OpenReview, PubMed, and Yelp benchmarks, SecPE consistently achieves lower Fr\\'echet Inception Distance (FID) and higher downstream task accuracy than GDP-based Aug-PE baselines, while requiring less noise to attain the same level of protection. Our results highlight that secret-aware guarantees can unlock more practical and effective privacy-preserving synthetic text generation.",
    "summary": "",
    "translation": "用于差分隐私合成文本生成的秘密保护演化",
    "relevance_score": 1,
    "reasoning": "该论文涉及差分隐私技术，这属于隐私保护范畴，在无关主题中明确排除。虽然提到了文本生成，但核心焦点是隐私保护机制而非LLM技术本身或其应用。没有证据表明该工作与推荐系统、搜索或广告有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10977v1": {
    "title": "Revisiting Model Interpolation for Efficient Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.10977v1",
    "arxiv_id": "2510.10977v1",
    "authors": "Taiqiang Wu, Runming Yang, Tao Liu, Jiahao Wang, Ngai Wong",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-13 03:30:01",
    "ori_summary": "Model merging, typically on Instruct and Thinking models, has shown remarkable performance for efficient reasoning. In this paper, we systematically revisit the simplest merging method that interpolates two weights directly. Particularly, we observe that model interpolation follows a three-stage evolutionary paradigm with distinct behaviors on the reasoning trajectory. These dynamics provide a principled guide for navigating the performance-cost trade-off. Empirical results demonstrate that a strategically interpolated model surprisingly surpasses sophisticated model merging baselines on both efficiency and effectiveness. We further validate our findings with extensive ablation studies on model layers, modules, and decoding strategies. Ultimately, this work demystifies model interpolation and offers a practical framework for crafting models with precisely targeted reasoning capabilities. Code is available at \\href{https://github.com/wutaiqiang/MI}{Github}.",
    "summary": "",
    "translation": "重新审视模型插值以实现高效推理",
    "relevance_score": 7,
    "reasoning": "模型插值技术属于Transformer架构效率优化的范畴，通过组合不同模型实现更高效的推理。在推荐系统和搜索场景中，这种方法可以用于构建更高效的排序模型，在保持性能的同时降低推理延迟和计算成本，直接服务于大规模在线服务需求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.10974v1": {
    "title": "Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning",
    "url": "https://www.alphaxiv.org/abs/2510.10974v1",
    "arxiv_id": "2510.10974v1",
    "authors": "Zhiwen Ruan, Yixia Li, He Zhu, Yun Chen, Peng Li, Yang Liu, Guanhua Chen",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 03:25:36",
    "ori_summary": "Large language models (LLMs) primarily rely on supervised fine-tuning (SFT) as a key method to adapt pre-trained models to domain-specific tasks such as mathematical reasoning. However, standard SFT uniformly penalizes all tokens, neglecting that only a small subset of critical tokens determines reasoning correctness. This uniform supervision often causes reduced output diversity and limited generalization. We propose Critical Token Fine-tuning (CFT), a simple yet effective approach that updates only tokens identified as functionally indispensable via counterfactual perturbations. By focusing gradient signals on these decisive reasoning steps while preserving the diversity of non-critical tokens, CFT can enhance both generation and diversity. Extensive experiments on five models across three families (Qwen, OLMo, LLaMA) and eleven mathematical reasoning benchmarks show that CFT, despite fine-tuning on less than 12% of tokens, consistently outperforms standard SFT. Moreover, CFT enables test-time scaling through improved sampling diversity and provides a stronger initialization for reinforcement learning, sustaining performance gains in later training stages while maintaining higher entropy for better exploration. These results highlight CFT as a practical and general framework for efficient and robust LLM fine-tuning.",
    "summary": "",
    "translation": "通过选择性关键令牌微调增强大型语言模型推理能力",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能LLM技术'范畴，专注于提升LLM推理能力，这是推荐、搜索和广告系统的核心需求。通过选择性令牌微调改进推理能力，可直接应用于提升推荐系统的逻辑推理、搜索查询理解以及广告相关性判断的准确性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.10971v1": {
    "title": "RV-HATE: Reinforced Multi-Module Voting for Implicit Hate Speech Detection",
    "url": "https://www.alphaxiv.org/abs/2510.10971v1",
    "arxiv_id": "2510.10971v1",
    "authors": "Yejin Lee, Hyeseon Ahn, Yo-Sub Han",
    "categories": "cs.CL, cs.AI, 68T50, I.2.7",
    "pub_date": "2025-10-13 03:21:51",
    "ori_summary": "Hate speech remains prevalent in human society and continues to evolve in its forms and expressions. Modern advancements in internet and online anonymity accelerate its rapid spread and complicate its detection. However, hate speech datasets exhibit diverse characteristics primarily because they are constructed from different sources and platforms, each reflecting different linguistic styles and social contexts. Despite this diversity, prior studies on hate speech detection often rely on fixed methodologies without adapting to data-specific features. We introduce RV-HATE, a detection framework designed to account for the dataset-specific characteristics of each hate speech dataset. RV-HATE consists of multiple specialized modules, where each module focuses on distinct linguistic or contextual features of hate speech. The framework employs reinforcement learning to optimize weights that determine the contribution of each module for a given dataset. A voting mechanism then aggregates the module outputs to produce the final decision. RV-HATE offers two primary advantages: (1)~it improves detection accuracy by tailoring the detection process to dataset-specific attributes, and (2)~it also provides interpretable insights into the distinctive features of each dataset. Consequently, our approach effectively addresses implicit hate speech and achieves superior performance compared to conventional static methods. Our code is available at https://github.com/leeyejin1231/RV-HATE.",
    "summary": "",
    "translation": "RV-HATE：基于强化多模块投票的隐式仇恨言论检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注仇恨言论检测这一内容安全领域，属于内容审核而非推荐系统、搜索或广告的核心技术。虽然涉及多模态和强化学习，但其应用场景（内容安全）与用户建模、个性化推荐、广告排名等核心业务领域关联性较弱，不符合当前关注的任一技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10965v1": {
    "title": "Judge Before Answer: Can MLLM Discern the False Premise in Question?",
    "url": "https://www.alphaxiv.org/abs/2510.10965v1",
    "arxiv_id": "2510.10965v1",
    "authors": "Jidong Li, Lingyong Fang, Haodong Zhao, Sufeng Duan, Gongshen Liu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 03:17:00",
    "ori_summary": "Multimodal large language models (MLLMs) have witnessed astonishing advancements in recent years. Despite these successes, MLLMs remain vulnerable to flase premise problems. However, existing benchmarks targeting this issue are limited in scope: they often lack fine-grained categorization, exhibit insufficient coverage, and thus fail to provide a rigorous evaluation of the ability of models to recognize false premises. To bridge this gap, we introduce a fully automated pipeline for constructing a comprehensive benchmark of false premise questions. Our method systematically categorizes the premises into three main types and thirteen subtypes according to the abilities required to identify the premises, resulting in the JBA dataset.Results show current MLLMs still struggle with false premise recognition. Building upon this benchmark, we further propose a recognition enhancement framework tailored to strengthen the robustness of MLLMs to detect false premises. Extensive experiments demonstrate that models trained with our framework achieve significant improvements in false premise recognition.",
    "summary": "",
    "translation": "先判断再回答：多模态大语言模型能否识别问题中的错误前提？",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态大语言模型(MLLM)的推理能力和错误前提识别，属于纯粹的模型能力评估范畴。虽然涉及LLM技术，但论文焦点是模型在理解任务中的表现评估，而非在推荐系统、搜索或广告领域的实际应用或架构改进，与当前关注的技术应用方向关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10961v1": {
    "title": "KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification",
    "url": "https://www.alphaxiv.org/abs/2510.10961v1",
    "arxiv_id": "2510.10961v1",
    "authors": "Yejin Lee, Su-Hyeon Kim, Hyundong Jin, Dayoung Kim, Yeonsoo Kim, Yo-Sub Han",
    "categories": "cs.CL, cs.AI, 68T50, I.2.7",
    "pub_date": "2025-10-13 03:12:37",
    "ori_summary": "Toxic content has become an increasingly critical social issue with the rapid expansion of online communication. While numerous studies explored methods for detecting and detoxifying such content, most have focused primarily on English, leaving low-resource language underrepresented. Consequently, Large Language Models~(LLMs) often struggle to identify and neutralize toxic expressions in these languages. This challenge becomes even more pronounced when user employ obfuscation techniques to evade detection systems. Therefore, we propose a \\textbf{KOTOX: Korean Toxic Dataset} for deobfuscation and detoxicification to address this issue. We categorize various obfuscation approaches based on linguistic characteristics of Korean and define a set of transformation rules grounded in real-word examples. Using these rules, we construct three dataset versions (easy, normal, and hard) representing different levels of obfuscation difficulty. This is the first dataset that simultaneously supports deobfuscation and detoxification for the Korean language. We expect it to facilitate better understanding and mitigating of obfuscated toxic content in LLM for low-resource languages. Our code and data are available at https://github.com/leeyejin1231/KOTOX.",
    "summary": "",
    "translation": "KOTOX：用于反混淆和去毒化的韩语毒性数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于韩语毒性数据集构建，属于内容安全和伦理审查范畴，与推荐系统、搜索或广告的核心技术进展无关。论文主题涉及毒性检测和去毒化，这些属于被明确排除的伦理和内容安全领域，没有任何潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10959v1": {
    "title": "Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its Potential for LLM Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.10959v1",
    "arxiv_id": "2510.10959v1",
    "authors": "Xiaoyun Zhang, Xiaojian Yuan, Di Huang, Wang You, Chen Hu, Jingqing Ruan, Kejiang Chen, Xing Hu",
    "categories": "cs.LG, cs.AI, cs.CL, stat.ML",
    "pub_date": "2025-10-13 03:10:26",
    "ori_summary": "Reasoning ability has become a defining capability of Large Language Models (LLMs), with Reinforcement Learning with Verifiable Rewards (RLVR) emerging as a key paradigm to enhance it. However, RLVR training often suffers from policy entropy collapse, where the policy becomes overly deterministic, hindering exploration and limiting reasoning performance. While entropy regularization is a common remedy, its effectiveness is highly sensitive to the fixed coefficient, making it unstable across tasks and models. In this work, we revisit entropy regularization in RLVR and argue that its potential has been largely underestimated. Our analysis shows that (i) tasks of varying difficulty demand distinct exploration intensities, and (ii) balanced exploration may require the policy entropy to be maintained within a moderate range below its initial level. Therefore, we propose Adaptive Entropy Regularization (AER)--a framework that dynamically balances exploration and exploitation via three components: difficulty-aware coefficient allocation, initial-anchored target entropy, and dynamic global coefficient adjustment. Experiments on multiple mathematical reasoning benchmarks show that AER consistently outperforms baselines, improving both reasoning accuracy and exploration capability.",
    "summary": "",
    "translation": "重新发现熵正则化：自适应系数释放其在大型语言模型强化学习中的潜力",
    "relevance_score": 2,
    "reasoning": "该论文主要关注强化学习中的熵正则化技术，属于纯粹的强化学习优化方法。虽然标题提到LLM，但内容聚焦于RL训练技术本身，没有明确展示在推荐系统、搜索或广告中的潜在应用。该技术可能间接影响RLHF训练，但缺乏直接的应用场景说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10951v1": {
    "title": "Punctuation-aware treebank tree binarization",
    "url": "https://www.alphaxiv.org/abs/2510.10951v1",
    "arxiv_id": "2510.10951v1",
    "authors": "Eitan Klinger, Vivaan Wadhwa, Jungyeul Park",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 03:02:38",
    "ori_summary": "This article presents a curated resource and evaluation suite for punctuation-aware treebank binarization. Standard binarization pipelines drop punctuation before head selection, which alters constituent shape and harms head-child identification. We release (1) a reproducible pipeline that preserves punctuation as sibling nodes prior to binarization, (2) derived artifacts and metadata (intermediate @X markers, reversibility signatures, alignment indices), and (3) an accompanying evaluation suite covering head-child prediction, round-trip reversibility, and structural compatibility with derivational resources (CCGbank). On the Penn Treebank, punctuation-aware preprocessing improves head prediction accuracy from 73.66\\% (Collins rules) and 86.66\\% (MLP) to 91.85\\% with the same classifier, and achieves competitive alignment against CCGbank derivations. All code, configuration files, and documentation are released to enable replication and extension to other corpora.",
    "summary": "",
    "translation": "标点感知的树库树二值化",
    "relevance_score": 1,
    "reasoning": "该论文专注于句法树二值化这一特定NLP预处理技术，属于纯粹的句法分析领域。虽然涉及树结构处理，但标点感知的树库转换与推荐系统、搜索或广告的核心技术需求没有直接关联，也没有明显的Transformer架构改进或LLM应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10943v1": {
    "title": "The Social Cost of Intelligence: Emergence, Propagation, and Amplification of Stereotypical Bias in Multi-Agent Systems",
    "url": "https://www.alphaxiv.org/abs/2510.10943v1",
    "arxiv_id": "2510.10943v1",
    "authors": "Thi-Nhung Nguyen, Linhao Luo, Thuy-Trang Vu, Dinh Phung",
    "categories": "cs.MA, cs.CL",
    "pub_date": "2025-10-13 02:56:42",
    "ori_summary": "Bias in large language models (LLMs) remains a persistent challenge, manifesting in stereotyping and unfair treatment across social groups. While prior research has primarily focused on individual models, the rise of multi-agent systems (MAS), where multiple LLMs collaborate and communicate, introduces new and largely unexplored dynamics in bias emergence and propagation. In this work, we present a comprehensive study of stereotypical bias in MAS, examining how internal specialization, underlying LLMs and inter-agent communication protocols influence bias robustness, propagation, and amplification. We simulate social contexts where agents represent different social groups and evaluate system behavior under various interaction and adversarial scenarios. Experiments on three bias benchmarks reveal that MAS are generally less robust than single-agent systems, with bias often emerging early through in-group favoritism. However, cooperative and debate-based communication can mitigate bias amplification, while more robust underlying LLMs improve overall system stability. Our findings highlight critical factors shaping fairness and resilience in multi-agent LLM systems.",
    "summary": "",
    "translation": "智能的社会成本：多智能体系统中刻板偏见的涌现、传播与放大",
    "relevance_score": 1,
    "reasoning": "该论文主要研究多智能体系统中的偏见问题，属于公平性、伦理等非技术性话题，这些已被明确列为无关主题。虽然多智能体系统在理论上可能与推荐系统相关，但该论文聚焦于社会成本和偏见放大，缺乏对推荐、搜索或广告领域的技术应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10936v1": {
    "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF: A Reproducibility Study",
    "url": "https://www.alphaxiv.org/abs/2510.10936v1",
    "arxiv_id": "2510.10936v1",
    "authors": "Anirudh Ganesh, Jayavardhan Reddy",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-13 02:49:21",
    "ori_summary": "We present a reproducibility study of the state-of-the-art neural architecture for sequence labeling proposed by Ma and Hovy (2016)\\cite{ma2016end}. The original BiLSTM-CNN-CRF model combines character-level representations via Convolutional Neural Networks (CNNs), word-level context modeling through Bi-directional Long Short-Term Memory networks (BiLSTMs), and structured prediction using Conditional Random Fields (CRFs). This end-to-end approach eliminates the need for hand-crafted features while achieving excellent performance on named entity recognition (NER) and part-of-speech (POS) tagging tasks. Our implementation successfully reproduces the key results, achieving 91.18\\% F1-score on CoNLL-2003 NER and demonstrating the model's effectiveness across sequence labeling tasks. We provide a detailed analysis of the architecture components and release an open-source PyTorch implementation to facilitate further research.",
    "summary": "",
    "translation": "基于双向LSTM-CNNs-CRF的端到端序列标注：可复现性研究",
    "relevance_score": 2,
    "reasoning": "该论文主要研究序列标注任务的经典神经网络架构（BiLSTM-CNN-CRF）的可复现性，属于传统的NLP方法而非现代LLM或Transformer技术。虽然序列标注在搜索中有潜在应用（如命名实体识别），但该研究聚焦于架构复现而非核心推荐/搜索/广告领域的创新，且不涉及LLM、Transformer或异构数据建模等当前关注方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10930v1": {
    "title": "Evaluating Language Models' Evaluations of Games",
    "url": "https://www.alphaxiv.org/abs/2510.10930v1",
    "arxiv_id": "2510.10930v1",
    "authors": "Katherine M. Collins, Cedegao E. Zhang, Graham Todd, Lance Ying, Mauricio Barba da Costa, Ryan Liu, Prafull Sharma, Adrian Weller, Ionatan Kuperwajs, Lionel Wong, Joshua B. Tenenbaum, Thomas L. Griffiths",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 02:45:37",
    "ori_summary": "Reasoning is not just about solving problems -- it is also about evaluating which problems are worth solving at all. Evaluations of artificial intelligence (AI) systems primarily focused on problem solving, historically by studying how models play games such as chess and Go. In this paper, we advocate for a new paradigm that assesses AI systems' evaluation of games. First, we introduce a formalism for evaluating such evaluations. We then leverage a large-scale dataset of over $100$ novel board games and over 450 human judgments to compare evaluations produced by modern language and reasoning models against those of people and symbolic computational agents. We consider two kinds of evaluative queries: assessing the payoff (or fairness) and the funness of games. These queries span two dimensions relevant to the design of evaluations of AI evaluations: how complex a query is to compute and how difficult a query is to quantify. Our results show that reasoning models are generally more aligned to people in their evaluations of games than non-reasoning language models. However, we observe a non-monotonic relationship: as models get closer to game-theoretic optimal, their fit to human data weakens. We also observe more \"jaggedness\" across models for assessing funness, in line with the greater difficulty of quantifying this query. Across queries and games, reasoning models show highly variable and unpredictable resource usage when assessing queries, pointing to the importance of imbuing more resource-rational meta-reasoning in language and reasoning models.",
    "summary": "",
    "translation": "评估语言模型对游戏的评估能力",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于语言模型在游戏领域的评估能力，这属于纯粹的LLM评估基准研究，与推荐系统、搜索或广告的核心技术进展无关。论文内容可能涉及LLM在游戏场景下的表现评估，但没有显示出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10927v1": {
    "title": "GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.10927v1",
    "arxiv_id": "2510.10927v1",
    "authors": "Yawen Yang, Fukun Ma, Shiao Meng, Aiwei Liu, Lijie Wen",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 02:40:14",
    "ori_summary": "In biomedical fields, one named entity may consist of a series of non-adjacent tokens and overlap with other entities. Previous methods recognize discontinuous entities by connecting entity fragments or internal tokens, which face challenges of error propagation and decoding ambiguity due to the wide variety of span or word combinations. To address these issues, we deeply explore discontinuous entity structures and propose an effective Gap-aware grid tagging model for Discontinuous Named Entity Recognition, named GapDNER. Our GapDNER innovatively applies representation learning on the context gaps between entity fragments to resolve decoding ambiguity and enhance discontinuous NER performance. Specifically, we treat the context gap as an additional type of span and convert span classification into a token-pair grid tagging task. Subsequently, we design two interactive components to comprehensively model token-pair grid features from both intra- and inter-span perspectives. The intra-span regularity extraction module employs the biaffine mechanism along with linear attention to capture the internal regularity of each span, while the inter-span relation enhancement module utilizes criss-cross attention to obtain semantic relations among different spans. At the inference stage of entity decoding, we assign a directed edge to each entity fragment and context gap, then use the BFS algorithm to search for all valid paths from the head to tail of grids with entity tags. Experimental results on three datasets demonstrate that our GapDNER achieves new state-of-the-art performance on discontinuous NER and exhibits remarkable advantages in recognizing complex entity structures.",
    "summary": "",
    "translation": "GapDNER：一种用于不连续命名实体识别的间隙感知网格标注模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于不连续命名实体识别这一特定NLP任务，属于纯粹的序列标注技术。虽然命名实体识别在通用NLP中有广泛应用，但该工作没有展示与推荐系统、搜索或广告领域的直接关联，也不涉及Transformer架构改进、多模态建模或LLM技术等核心关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10925v1": {
    "title": "Find Your Optimal Teacher: Personalized Data Synthesis via Router-Guided Multi-Teacher Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.10925v1",
    "arxiv_id": "2510.10925v1",
    "authors": "Hengyuan Zhang, Shiping Yang, Xiao Liang, Chenming Shang, Yuxuan Jiang, Chaofan Tao, Jing Xiong, Hayden Kwok-Hay So, Ruobing Xie, Angel X. Chang, Ngai Wong",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-13 02:36:36",
    "ori_summary": "Training student models on synthetic data generated by strong teacher models is a promising way to distilling the capabilities of teachers. However, recent studies show that stronger models are not always optimal teachers, revealing a mismatch between teacher outputs and student learnability. To address this issue, we propose PerSyn (Personalized data Synthesis), a novel synthesis strategy that operates under a new ``Route then Generate'' paradigm to create data tailored to each student model, enabling it to learn more effectively. Specifically, PerSyn first assigns each prompt to its optimal teacher via a query-level router that jointly considers student learnability and teacher response quality. Each teacher then synthesizes data only for its assigned prompts, making the process more efficient than the conventional ``Generate then Select'' paradigm, where all teachers must generate parallel responses for the entire prompt set before constructing the final dataset. Extensive experiments across different model families and scales demonstrate that PerSyn consistently achieves superior or comparable performance to all baselines in instruct tuning and math reasoning settings. Further analysis verifies the effectiveness of PerSyn and offers extra insights to propel future research.",
    "summary": "",
    "translation": "寻找你的最优教师：通过路由器引导的多教师蒸馏实现个性化数据合成",
    "relevance_score": 8,
    "reasoning": "该论文涉及多教师蒸馏和个性化数据合成，这是LLM蒸馏和模型压缩的核心技术。在推荐系统和搜索领域，这种技术可以用于将大型LLM蒸馏为更高效的个性化模型，用于实时推理或边缘部署，同时保持个性化性能。路由器引导的机制特别适合处理用户异质性，为不同用户群体选择最优的教师模型进行知识蒸馏。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.10913v1": {
    "title": "ADVICE: Answer-Dependent Verbalized Confidence Estimation",
    "url": "https://www.alphaxiv.org/abs/2510.10913v1",
    "arxiv_id": "2510.10913v1",
    "authors": "Ki Jung Seo, Sehun Lim, Taeuk Kim",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 02:18:33",
    "ori_summary": "Recent progress in large language models (LLMs) has enabled them to express their confidence in natural language, enhancing transparency and reliability. However, their confidence often exhibits overconfidence, the cause of which remains poorly understood. In this work, we conduct a detailed analysis of the dynamics underlying verbalized confidence and identify answer-independence as a key factor, defined as the model's failure to condition confidence on its own answer. To address this, we propose ADVICE (Answer-Dependent Verbalized Confidence Estimation), a fine-tuning framework that facilitates answer-grounded confidence estimation. Extensive experiments show that ADVICE substantially improves confidence calibration while preserving task performance. Further analyses confirm that ADVICE strengthens answer-groundedness, leading to more balanced and well-calibrated confidence distributions. Our findings shed light on the origin of overconfidence and establish a framework for more trustworthy confidence verbalization.",
    "summary": "",
    "translation": "ADVICE：答案依赖的言语化置信度估计",
    "relevance_score": 3,
    "reasoning": "该论文关注LLM置信度估计，属于Enabling LLM Tech范畴。在搜索和推荐系统中，准确的置信度估计可用于结果排序、不确定性感知的推荐以及查询理解优化。然而，该方法更偏向通用NLP技术，与推荐/搜索/广告的核心排序问题关联度有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10890v1": {
    "title": "LLM$\\times$MapReduce-V3: Enabling Interactive In-Depth Survey Generation through a MCP-Driven Hierarchically Modular Agent System",
    "url": "https://www.alphaxiv.org/abs/2510.10890v1",
    "arxiv_id": "2510.10890v1",
    "authors": "Yu Chao, Siyu Lin, xiaorong wang, Zhu Zhang, Zihan Zhou, Haoyu Wang, Shuo Wang, Jie Zhou, Zhiyuan Liu, Maosong Sun",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 01:38:37",
    "ori_summary": "We introduce LLM x MapReduce-V3, a hierarchically modular agent system designed for long-form survey generation. Building on the prior work, LLM x MapReduce-V2, this version incorporates a multi-agent architecture where individual functional components, such as skeleton initialization, digest construction, and skeleton refinement, are implemented as independent model-context-protocol (MCP) servers. These atomic servers can be aggregated into higher-level servers, creating a hierarchically structured system. A high-level planner agent dynamically orchestrates the workflow by selecting appropriate modules based on their MCP tool descriptions and the execution history. This modular decomposition facilitates human-in-the-loop intervention, affording users greater control and customization over the research process. Through a multi-turn interaction, the system precisely captures the intended research perspectives to generate a comprehensive skeleton, which is then developed into an in-depth survey. Human evaluations demonstrate that our system surpasses representative baselines in both content depth and length, highlighting the strength of MCP-based modular planning.",
    "summary": "",
    "translation": "LLM×MapReduce-V3：通过MCP驱动的分层模块化智能体系统实现交互式深度调研生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注基于LLM的调研生成系统，这属于纯粹的LLM应用范畴（AIGC/内容生成）。虽然提到了模块化智能体系统和MapReduce架构，但这些技术并没有明确指向推荐系统、搜索或广告领域的特定应用。论文的核心焦点是交互式调研生成，这与我的关注领域中的直接应用要求不符。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10885v1": {
    "title": "Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling Strategies in Text2SQL Tasks",
    "url": "https://www.alphaxiv.org/abs/2510.10885v1",
    "arxiv_id": "2510.10885v1",
    "authors": "Jiajing Guo, Kenil Patel, Jorge Piazentin Ono, Wenbin He, Liu Ren",
    "categories": "cs.CL, cs.DB",
    "pub_date": "2025-10-13 01:29:54",
    "ori_summary": "Large language models (LLMs) are increasingly powering Text-to-SQL (Text2SQL) systems, enabling non-expert users to query industrial databases using natural language. While test-time scaling strategies have shown promise in LLM-based solutions, their effectiveness in real-world applications, especially with the latest reasoning models, remains uncertain. In this work, we benchmark six lightweight, industry-oriented test-time scaling strategies and four LLMs, including two reasoning models, evaluating their performance on the BIRD Mini-Dev benchmark. Beyond standard accuracy metrics, we also report inference latency and token consumption, providing insights relevant for practical system deployment. Our findings reveal that Divide-and-Conquer prompting and few-shot demonstrations consistently enhance performance for both general-purpose and reasoning-focused LLMs. However, introducing additional workflow steps yields mixed results, and base model selection plays a critical role. This work sheds light on the practical trade-offs between accuracy, efficiency, and complexity when deploying Text2SQL systems.",
    "summary": "",
    "translation": "重新思考智能体工作流：在Text2SQL任务中评估基于推理的测试时扩展策略",
    "relevance_score": 2,
    "reasoning": "该论文主要关注Text2SQL任务中的智能体工作流和测试时扩展策略，这属于特定领域NLP应用。虽然提到了推理和扩展策略，但这些技术主要针对数据库查询生成任务，与推荐系统、搜索或广告的核心技术栈关联度较低。没有明确展示这些方法在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11717v1": {
    "title": "Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams",
    "url": "https://www.alphaxiv.org/abs/2510.11717v1",
    "arxiv_id": "2510.11717v1",
    "authors": "Takuya Nakabayashi, Navami Kairanda, Hideo Saito, Vladislav Golyanik",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 17:59:55",
    "ori_summary": "Event cameras offer various advantages for novel view rendering compared to synchronously operating RGB cameras, and efficient event-based techniques supporting rigid scenes have been recently demonstrated in the literature. In the case of non-rigid objects, however, existing approaches additionally require sparse RGB inputs, which can be a substantial practical limitation; it remains unknown if similar models could be learned from event streams only. This paper sheds light on this challenging open question and introduces Ev4DGS, i.e., the first approach for novel view rendering of non-rigidly deforming objects in the explicit observation space (i.e., as RGB or greyscale images) from monocular event streams. Our method regresses a deformable 3D Gaussian Splatting representation through 1) a loss relating the outputs of the estimated model with the 2D event observation space, and 2) a coarse 3D deformation model trained from binary masks generated from events. We perform experimental comparisons on existing synthetic and newly recorded real datasets with non-rigid objects. The results demonstrate the validity of Ev4DGS and its superior performance compared to multiple naive baselines that can be applied in our setting. We will release our models and the datasets used in the evaluation for research purposes; see the project webpage: https://4dqv.mpi-inf.mpg.de/Ev4DGS/.",
    "summary": "",
    "translation": "Ev4DGS：基于单目事件流的非刚性物体新视角渲染",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的事件相机和3D渲染技术，涉及非刚性物体重建和新视角生成。这些内容与推荐系统、搜索或广告的核心技术领域没有直接关联，也不涉及LLM、Transformer架构或异构数据建模等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11718v1": {
    "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images",
    "url": "https://www.alphaxiv.org/abs/2510.11718v1",
    "arxiv_id": "2510.11718v1",
    "authors": "Chengqi Duan, Kaiyue Sun, Rongyao Fang, Manyuan Zhang, Yan Feng, Ying Luo, Yufang Liu, Ke Wang, Peng Pei, Xunliang Cai, Hongsheng Li, Yi Ma, Xihui Liu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 17:59:55",
    "ori_summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models (VLMs) have shown significant progress in mathematical reasoning, yet they still face a critical bottleneck with problems requiring visual assistance, such as drawing auxiliary lines or plotting functions to solve the problems. Most LLMs and VLMs are constrained to text-only reasoning chains, while multimodal unified models that can generate interleaved text and images lack the necessary precision and controllability for such tasks. To address this, we propose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking with images\" in mathematics. Our approach leverages the VLM to generate text reasoning as well as executable plotting code, which is then rendered into images as \"visual thought\", to solve mathematical problems. To achieve this, we first construct Math-VR, the first large-scale, bilingual dataset and benchmark for Mathematics problems with Visual Reasoning, comprising 178K samples. Second, to create high-quality training data, we develop a state-of-the-art image-to-code converter specialized for parsing complex mathematical figures into codes. Finally, using these training data, we train the CodePlot-CoT model for solving mathematical problems. Experimental results show that our model achieves up to 21% increase over base model on our new benchmark, fully validating the efficacy of our proposed code-driven reasoning paradigm. Our work opens a new direction for multimodal mathematical reasoning and provides the community with the first large-scale dataset, comprehensive benchmark, and strong approach for such problems. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
    "summary": "",
    "translation": "CodePlot-CoT：通过代码驱动图像进行思维链的数学视觉推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注数学视觉推理和代码驱动的图像生成，属于视觉语言模型在数学问题解决领域的应用。虽然涉及多模态建模，但其应用场景（数学推理）与推荐系统、搜索或广告的核心技术需求相距甚远，且缺乏明确的跨模态表示学习在商业应用中的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11715v1": {
    "title": "Point Prompting: Counterfactual Tracking with Video Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2510.11715v1",
    "arxiv_id": "2510.11715v1",
    "authors": "Ayush Shrivastava, Sanyam Mehta, Daniel Geng, Andrew Owens",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 17:59:46",
    "ori_summary": "Trackers and video generators solve closely related problems: the former analyze motion, while the latter synthesize it. We show that this connection enables pretrained video diffusion models to perform zero-shot point tracking by simply prompting them to visually mark points as they move over time. We place a distinctively colored marker at the query point, then regenerate the rest of the video from an intermediate noise level. This propagates the marker across frames, tracing the point's trajectory. To ensure that the marker remains visible in this counterfactual generation, despite such markers being unlikely in natural videos, we use the unedited initial frame as a negative prompt. Through experiments with multiple image-conditioned video diffusion models, we find that these \"emergent\" tracks outperform those of prior zero-shot methods and persist through occlusions, often obtaining performance that is competitive with specialized self-supervised models.",
    "summary": "",
    "translation": "点提示：使用视频扩散模型进行反事实追踪",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于视频扩散模型和反事实追踪，属于纯粹的视觉领域研究。虽然反事实分析在推荐系统中可能有应用，但该论文的技术核心是视频处理和生成，没有明确展示与推荐系统、搜索或广告的潜在联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11712v1": {
    "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training",
    "url": "https://www.alphaxiv.org/abs/2510.11712v1",
    "arxiv_id": "2510.11712v1",
    "authors": "Haoran Feng, Dizhe Zhang, Xiangtai Li, Bo Du, Lu Qi",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 17:59:15",
    "ori_summary": "In this work, we propose DiT360, a DiT-based framework that performs hybrid training on perspective and panoramic data for panoramic image generation. For the issues of maintaining geometric fidelity and photorealism in generation quality, we attribute the main reason to the lack of large-scale, high-quality, real-world panoramic data, where such a data-centric view differs from prior methods that focus on model design. Basically, DiT360 has several key modules for inter-domain transformation and intra-domain augmentation, applied at both the pre-VAE image level and the post-VAE token level. At the image level, we incorporate cross-domain knowledge through perspective image guidance and panoramic refinement, which enhance perceptual quality while regularizing diversity and photorealism. At the token level, hybrid supervision is applied across multiple modules, which include circular padding for boundary continuity, yaw loss for rotational robustness, and cube loss for distortion awareness. Extensive experiments on text-to-panorama, inpainting, and outpainting tasks demonstrate that our method achieves better boundary consistency and image fidelity across eleven quantitative metrics. Our code is available at https://github.com/Insta360-Research-Team/DiT360.",
    "summary": "",
    "translation": "DiT360：通过混合训练实现高保真全景图像生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于全景图像生成技术，属于纯粹的视觉内容生成领域。虽然标题中提到混合训练方法，但这与推荐系统、搜索或广告的核心技术需求没有直接关联，也不涉及LLM技术或Transformer架构在推荐领域的应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11709v1": {
    "title": "Adversarial Attacks Leverage Interference Between Features in Superposition",
    "url": "https://www.alphaxiv.org/abs/2510.11709v1",
    "arxiv_id": "2510.11709v1",
    "authors": "Edward Stevinson, Lucas Prieto, Melih Barsbey, Tolga Birdal",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-13 17:59:02",
    "ori_summary": "Fundamental questions remain about when and why adversarial examples arise in neural networks, with competing views characterising them either as artifacts of the irregularities in the decision landscape or as products of sensitivity to non-robust input features. In this paper, we instead argue that adversarial vulnerability can stem from efficient information encoding in neural networks. Specifically, we show how superposition - where networks represent more features than they have dimensions - creates arrangements of latent representations that adversaries can exploit. We demonstrate that adversarial perturbations leverage interference between superposed features, making attack patterns predictable from feature arrangements. Our framework provides a mechanistic explanation for two known phenomena: adversarial attack transferability between models with similar training regimes and class-specific vulnerability patterns. In synthetic settings with precisely controlled superposition, we establish that superposition suffices to create adversarial vulnerability. We then demonstrate that these findings persist in a ViT trained on CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct of networks' representational compression, rather than flaws in the learning process or non-robust inputs.",
    "summary": "",
    "translation": "基于特征叠加中特征间干扰的对抗性攻击",
    "relevance_score": 2,
    "reasoning": "该论文主要研究对抗性攻击机制，这属于模型安全/鲁棒性领域，属于明确的无关主题。虽然特征叠加概念在Transformer架构中有所体现，但论文焦点是攻击方法而非核心架构改进或推荐/搜索/广告应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11704v1": {
    "title": "Bayesian Topological Convolutional Neural Nets",
    "url": "https://www.alphaxiv.org/abs/2510.11704v1",
    "arxiv_id": "2510.11704v1",
    "authors": "Sarah Harkins Dayton, Hayden Everett, Ioannis Schizas, David L. Boothe Jr., Vasileios Maroulas",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 17:57:43",
    "ori_summary": "Convolutional neural networks (CNNs) have been established as the main workhorse in image data processing; nonetheless, they require large amounts of data to train, often produce overconfident predictions, and frequently lack the ability to quantify the uncertainty of their predictions. To address these concerns, we propose a new Bayesian topological CNN that promotes a novel interplay between topology-aware learning and Bayesian sampling. Specifically, it utilizes information from important manifolds to accelerate training while reducing calibration error by placing prior distributions on network parameters and properly learning appropriate posteriors. One important contribution of our work is the inclusion of a consistency condition in the learning cost, which can effectively modify the prior distributions to improve the performance of our novel network architecture. We evaluate the model on benchmark image classification datasets and demonstrate its superiority over conventional CNNs, Bayesian neural networks (BNNs), and topological CNNs. In particular, we supply evidence that our method provides an advantage in situations where training data is limited or corrupted. Furthermore, we show that the new model allows for better uncertainty quantification than standard BNNs since it can more readily identify examples of out-of-distribution data on which it has not been trained. Our results highlight the potential of our novel hybrid approach for more efficient and robust image classification.",
    "summary": "",
    "translation": "贝叶斯拓扑卷积神经网络",
    "relevance_score": 2,
    "reasoning": "该论文主要关注拓扑数据分析和贝叶斯方法在卷积神经网络中的应用，这属于计算机视觉和理论机器学习的范畴。虽然贝叶斯方法可能对不确定性建模有一定价值，但论文没有明确展示在推荐系统、搜索或广告中的直接应用潜力，且与Transformer架构、LLM技术或异构数据统一建模等当前关注领域关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11690v1": {
    "title": "Diffusion Transformers with Representation Autoencoders",
    "url": "https://www.alphaxiv.org/abs/2510.11690v1",
    "arxiv_id": "2510.11690v1",
    "authors": "Boyang Zheng, Nanye Ma, Shengbang Tong, Saining Xie",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-13 17:51:39",
    "ori_summary": "Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-dimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality. In this work, we explore replacing the VAE with pretrained representation encoders (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term Representation Autoencoders (RAEs). These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for a scalable transformer-based architecture. Since these latent spaces are typically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically. Our approach achieves faster convergence without auxiliary representation alignment losses. Using a DiT variant equipped with a lightweight, wide DDT head, we achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers clear advantages and should be the new default for diffusion transformer training.",
    "summary": "",
    "translation": "基于表示自编码器的扩散变换器",
    "relevance_score": 3,
    "reasoning": "该论文提出了扩散变换器与表示自编码器的结合，属于Transformer架构效率方面的进展，可能应用于推荐系统中的序列建模或特征表示学习。然而，扩散模型主要关注生成建模，与排名、检索等核心任务的相关性较弱，应用潜力有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11687v1": {
    "title": "Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View",
    "url": "https://www.alphaxiv.org/abs/2510.11687v1",
    "arxiv_id": "2510.11687v1",
    "authors": "Jinyu Zhang, Haitao Lin, Jiashu Hou, Xiangyang Xue, Yanwei Fu",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 17:49:15",
    "ori_summary": "Estimating an object's 6D pose, size, and shape from visual input is a fundamental problem in computer vision, with critical applications in robotic grasping and manipulation. Existing methods either rely on object-specific priors such as CAD models or templates, or suffer from limited generalization across categories due to pose-shape entanglement and multi-stage pipelines. In this work, we propose a unified, category-agnostic framework that simultaneously predicts 6D pose, size, and dense shape from a single RGB-D image, without requiring templates, CAD models, or category labels at test time. Our model fuses dense 2D features from vision foundation models with partial 3D point clouds using a Transformer encoder enhanced by a Mixture-of-Experts, and employs parallel decoders for pose-size estimation and shape reconstruction, achieving real-time inference at 28 FPS. Trained solely on synthetic data from 149 categories in the SOPE dataset, our framework is evaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL, spanning over 300 categories. It achieves state-of-the-art accuracy on seen categories while demonstrating remarkably strong zero-shot generalization to unseen real-world objects, establishing a new standard for open-set 6D understanding in robotics and embodied AI.",
    "summary": "",
    "translation": "超越“模板”：基于单视图的类别无关物体姿态、尺寸与形状估计",
    "relevance_score": 2,
    "reasoning": "该论文主要研究计算机视觉中的3D物体姿态、尺寸和形状估计，属于纯粹的视觉技术领域。虽然标题提到“类别无关”可能暗示泛化能力，但该技术缺乏明确的推荐系统、搜索或广告应用场景，与当前关注的LLM技术、推荐系统核心进展或Transformer架构改进没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11675v1": {
    "title": "FACE: Faithful Automatic Concept Extraction",
    "url": "https://www.alphaxiv.org/abs/2510.11675v1",
    "arxiv_id": "2510.11675v1",
    "authors": "Dipkamal Bhusal, Michael Clifford, Sara Rampazzi, Nidhi Rastogi",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 17:44:45",
    "ori_summary": "Interpreting deep neural networks through concept-based explanations offers a bridge between low-level features and high-level human-understandable semantics. However, existing automatic concept discovery methods often fail to align these extracted concepts with the model's true decision-making process, thereby compromising explanation faithfulness. In this work, we propose FACE (Faithful Automatic Concept Extraction), a novel framework that augments Non-negative Matrix Factorization (NMF) with a Kullback-Leibler (KL) divergence regularization term to ensure alignment between the model's original and concept-based predictions. Unlike prior methods that operate solely on encoder activations, FACE incorporates classifier supervision during concept learning, enforcing predictive consistency and enabling faithful explanations. We provide theoretical guarantees showing that minimizing the KL divergence bounds the deviation in predictive distributions, thereby promoting faithful local linearity in the learned concept space. Systematic evaluations on ImageNet, COCO, and CelebA datasets demonstrate that FACE outperforms existing methods across faithfulness and sparsity metrics.",
    "summary": "",
    "translation": "FACE：忠实自动概念提取",
    "relevance_score": 3,
    "reasoning": "该论文涉及概念提取技术，这在推荐系统或搜索中可能用于理解用户意图或项目特征。然而，标题没有明确说明与LLM、Transformer架构或异构数据建模的直接联系，且概念提取本身是一个相对通用的技术，缺乏明确的RecSys/Search/Ads应用背景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11650v1": {
    "title": "InfiniHuman: Infinite 3D Human Creation with Precise Control",
    "url": "https://www.alphaxiv.org/abs/2510.11650v1",
    "arxiv_id": "2510.11650v1",
    "authors": "Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 17:29:55",
    "ori_summary": "Generating realistic and controllable 3D human avatars is a long-standing challenge, particularly when covering broad attribute ranges such as ethnicity, age, clothing styles, and detailed body shapes. Capturing and annotating large-scale human datasets for training generative models is prohibitively expensive and limited in scale and diversity. The central question we address in this paper is: Can existing foundation models be distilled to generate theoretically unbounded, richly annotated 3D human data? We introduce InfiniHuman, a framework that synergistically distills these models to produce richly annotated human data at minimal cost and with theoretically unlimited scalability. We propose InfiniHumanData, a fully automatic pipeline that leverages vision-language and image generation models to create a large-scale multi-modal dataset. User study shows our automatically generated identities are undistinguishable from scan renderings. InfiniHumanData contains 111K identities spanning unprecedented diversity. Each identity is annotated with multi-granularity text descriptions, multi-view RGB images, detailed clothing images, and SMPL body-shape parameters. Building on this dataset, we propose InfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body shape, and clothing assets. InfiniHumanGen enables fast, realistic, and precisely controllable avatar generation. Extensive experiments demonstrate significant improvements over state-of-the-art methods in visual quality, generation speed, and controllability. Our approach enables high-quality avatar generation with fine-grained control at effectively unbounded scale through a practical and affordable solution. We will publicly release the automatic data generation pipeline, the comprehensive InfiniHumanData dataset, and the InfiniHumanGen models at https://yuxuan-xue.com/infini-human.",
    "summary": "",
    "translation": "InfiniHuman：具有精确控制的无限3D人体生成",
    "relevance_score": 2,
    "reasoning": "该论文专注于3D人体生成和计算机视觉领域，属于纯粹的视觉内容生成任务。虽然标题提到'精确控制'，但这主要针对3D人体建模的细粒度控制，与推荐系统、搜索或广告中的排序、用户建模等核心问题没有直接关联。该技术可能在某些特定场景下用于广告创意生成，但这属于明确排除的非相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11649v1": {
    "title": "PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image",
    "url": "https://www.alphaxiv.org/abs/2510.11649v1",
    "arxiv_id": "2510.11649v1",
    "authors": "Pradyumna Yalandur Muralidhar, Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 17:29:51",
    "ori_summary": "Reconstructing metrically accurate humans and their surrounding scenes from a single image is crucial for virtual reality, robotics, and comprehensive 3D scene understanding. However, existing methods struggle with depth ambiguity, occlusions, and physically inconsistent contacts. To address these challenges, we introduce PhySIC, a framework for physically plausible Human-Scene Interaction and Contact reconstruction. PhySIC recovers metrically consistent SMPL-X human meshes, dense scene surfaces, and vertex-level contact maps within a shared coordinate frame from a single RGB image. Starting from coarse monocular depth and body estimates, PhySIC performs occlusion-aware inpainting, fuses visible depth with unscaled geometry for a robust metric scaffold, and synthesizes missing support surfaces like floors. A confidence-weighted optimization refines body pose, camera parameters, and global scale by jointly enforcing depth alignment, contact priors, interpenetration avoidance, and 2D reprojection consistency. Explicit occlusion masking safeguards invisible regions against implausible configurations. PhySIC is efficient, requiring only 9 seconds for joint human-scene optimization and under 27 seconds end-to-end. It naturally handles multiple humans, enabling reconstruction of diverse interactions. Empirically, PhySIC outperforms single-image baselines, reducing mean per-vertex scene error from 641 mm to 227 mm, halving PA-MPJPE to 42 mm, and improving contact F1 from 0.09 to 0.51. Qualitative results show realistic foot-floor interactions, natural seating, and plausible reconstructions of heavily occluded furniture. By converting a single image into a physically plausible 3D human-scene pair, PhySIC advances scalable 3D scene understanding. Our implementation is publicly available at https://yuxuan-xue.com/physic.",
    "summary": "",
    "translation": "PhySIC：从单张图像生成物理上合理的3D人-场景交互与接触",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的3D人体与场景交互重建，属于纯粹的视觉领域研究。虽然涉及交互建模，但缺乏与推荐系统、搜索或广告领域的直接关联，也没有展示出在异构数据处理或Transformer架构方面的创新潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11647v1": {
    "title": "IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing Assessment",
    "url": "https://www.alphaxiv.org/abs/2510.11647v1",
    "arxiv_id": "2510.11647v1",
    "authors": "Yinan Chen, Jiangning Zhang, Teng Hu, Yuxiang Zeng, Zhucun Xue, Qingdong He, Chengjie Wang, Yong Liu, Xiaobin Hu, Shuicheng Yan",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 17:27:08",
    "ori_summary": "Instruction-guided video editing has emerged as a rapidly advancing research direction, offering new opportunities for intuitive content transformation while also posing significant challenges for systematic evaluation. Existing video editing benchmarks fail to support the evaluation of instruction-guided video editing adequately and further suffer from limited source diversity, narrow task coverage and incomplete evaluation metrics. To address the above limitations, we introduce IVEBench, a modern benchmark suite specifically designed for instruction-guided video editing assessment. IVEBench comprises a diverse database of 600 high-quality source videos, spanning seven semantic dimensions, and covering video lengths ranging from 32 to 1,024 frames. It further includes 8 categories of editing tasks with 35 subcategories, whose prompts are generated and refined through large language models and expert review. Crucially, IVEBench establishes a three-dimensional evaluation protocol encompassing video quality, instruction compliance and video fidelity, integrating both traditional metrics and multimodal large language model-based assessments. Extensive experiments demonstrate the effectiveness of IVEBench in benchmarking state-of-the-art instruction-guided video editing methods, showing its ability to provide comprehensive and human-aligned evaluation outcomes.",
    "summary": "",
    "translation": "IVEBench：面向指令引导视频编辑评估的现代基准套件",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频编辑评估基准，属于纯粹的视觉内容生成领域。虽然涉及指令引导，但核心关注视频编辑这一特定任务，与推荐系统、搜索或广告的排名和建模需求没有直接关联。该基准不涉及用户行为序列、上下文特征建模或Transformer架构效率等与当前关注点相关的技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11632v1": {
    "title": "NV3D: Leveraging Spatial Shape Through Normal Vector-based 3D Object Detection",
    "url": "https://www.alphaxiv.org/abs/2510.11632v1",
    "arxiv_id": "2510.11632v1",
    "authors": "Krittin Chaowakarn, Paramin Sangwongngam, Nang Htet Htet Aung, Chalie Charoenlarpnopparut",
    "categories": "cs.CV, cs.AI, cs.LG, I.2.6; I.2.9; I.2.10; I.4.8; I.4.10; I.5.1; I.5.4",
    "pub_date": "2025-10-13 17:13:06",
    "ori_summary": "Recent studies in 3D object detection for autonomous vehicles aim to enrich features through the utilization of multi-modal setups or the extraction of local patterns within LiDAR point clouds. However, multi-modal methods face significant challenges in feature alignment, and gaining features locally can be oversimplified for complex 3D object detection tasks. In this paper, we propose a novel model, NV3D, which utilizes local features acquired from voxel neighbors, as normal vectors computed per voxel basis using K-nearest neighbors (KNN) and principal component analysis (PCA). This informative feature enables NV3D to determine the relationship between the surface and pertinent target entities, including cars, pedestrians, or cyclists. During the normal vector extraction process, NV3D offers two distinct sampling strategies: normal vector density-based sampling and FOV-aware bin-based sampling, allowing elimination of up to 55% of data while maintaining performance. In addition, we applied element-wise attention fusion, which accepts voxel features as the query and value and normal vector features as the key, similar to the attention mechanism. Our method is trained on the KITTI dataset and has demonstrated superior performance in car and cyclist detection owing to their spatial shapes. In the validation set, NV3D without sampling achieves 86.60% and 80.18% mean Average Precision (mAP), greater than the baseline Voxel R-CNN by 2.61% and 4.23% mAP, respectively. With both samplings, NV3D achieves 85.54% mAP in car detection, exceeding the baseline by 1.56% mAP, despite roughly 55% of voxels being filtered out.",
    "summary": "",
    "translation": "NV3D：基于法向量的三维物体检测，利用空间形状信息",
    "relevance_score": 2,
    "reasoning": "该论文专注于3D物体检测的计算机视觉技术，通过法向量利用空间形状信息。虽然3D视觉在某些特定搜索场景（如产品搜索）可能有潜在应用，但与推荐系统、搜索或广告的核心技术焦点关联度很低，且未明确涉及Transformer架构或LLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11631v1": {
    "title": "EvoCAD: Evolutionary CAD Code Generation with Vision Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11631v1",
    "arxiv_id": "2510.11631v1",
    "authors": "Tobias Preintner, Weixuan Yuan, Adrian König, Thomas Bäck, Elena Raponi, Niki van Stein",
    "categories": "cs.CV, cs.AI, cs.NE",
    "pub_date": "2025-10-13 17:12:02",
    "ori_summary": "Combining large language models with evolutionary computation algorithms represents a promising research direction leveraging the remarkable generative and in-context learning capabilities of LLMs with the strengths of evolutionary algorithms. In this work, we present EvoCAD, a method for generating computer-aided design (CAD) objects through their symbolic representations using vision language models and evolutionary optimization. Our method samples multiple CAD objects, which are then optimized using an evolutionary approach with vision language and reasoning language models. We assess our method using GPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and comparing it to prior methods. Additionally, we introduce two new metrics based on topological properties defined by the Euler characteristic, which capture a form of semantic similarity between 3D objects. Our results demonstrate that EvoCAD outperforms previous approaches on multiple metrics, particularly in generating topologically correct objects, which can be efficiently evaluated using our two novel metrics that complement existing spatial metrics.",
    "summary": "",
    "translation": "EvoCAD：基于视觉语言模型的进化式CAD代码生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于CAD代码生成，属于计算机辅助设计领域，与推荐系统、搜索或广告的核心技术无关。虽然涉及视觉语言模型，但其应用场景是工程设计而非用户行为建模或内容理解，在推荐/搜索/广告领域没有明显的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11613v1": {
    "title": "High-resolution Photo Enhancement in Real-time: A Laplacian Pyramid Network",
    "url": "https://www.alphaxiv.org/abs/2510.11613v1",
    "arxiv_id": "2510.11613v1",
    "authors": "Feng Zhang, Haoyou Deng, Zhiqiang Li, Lida Li, Bin Xu, Qingbo Lu, Zisheng Cao, Minchen Wei, Changxin Gao, Nong Sang, Xiang Bai",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 16:52:32",
    "ori_summary": "Photo enhancement plays a crucial role in augmenting the visual aesthetics of a photograph. In recent years, photo enhancement methods have either focused on enhancement performance, producing powerful models that cannot be deployed on edge devices, or prioritized computational efficiency, resulting in inadequate performance for real-world applications. To this end, this paper introduces a pyramid network called LLF-LUT++, which integrates global and local operators through closed-form Laplacian pyramid decomposition and reconstruction. This approach enables fast processing of high-resolution images while also achieving excellent performance. Specifically, we utilize an image-adaptive 3D LUT that capitalizes on the global tonal characteristics of downsampled images, while incorporating two distinct weight fusion strategies to achieve coarse global image enhancement. To implement this strategy, we designed a spatial-frequency transformer weight predictor that effectively extracts the desired distinct weights by leveraging frequency features. Additionally, we apply local Laplacian filters to adaptively refine edge details in high-frequency components. After meticulously redesigning the network structure and transformer model, LLF-LUT++ not only achieves a 2.64 dB improvement in PSNR on the HDR+ dataset, but also further reduces runtime, with 4K resolution images processed in just 13 ms on a single GPU. Extensive experimental results on two benchmark datasets further show that the proposed approach performs favorably compared to state-of-the-art methods. The source code will be made publicly available at https://github.com/fengzhang427/LLF-LUT.",
    "summary": "",
    "translation": "实时高分辨率照片增强：拉普拉斯金字塔网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的图像增强技术，涉及拉普拉斯金字塔网络用于实时照片处理。这与我的关注点完全无关，因为该技术纯粹属于视觉增强领域，没有明确的推荐系统、搜索或广告应用潜力，也不涉及LLM、Transformer架构或异构数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11606v1": {
    "title": "ExpVid: A Benchmark for Experiment Video Understanding & Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.11606v1",
    "arxiv_id": "2510.11606v1",
    "authors": "Yicheng Xu, Yue Wu, Jiashuo Yu, Ziang Yan, Tianxiang Jiang, Yinan He, Qingsong Zhao, Kai Chen, Yu Qiao, Limin Wang, Manabu Okumura, Yi Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 16:45:28",
    "ori_summary": "Multimodal Large Language Models (MLLMs) hold promise for accelerating scientific discovery by interpreting complex experimental procedures. However, their true capabilities are poorly understood, as existing benchmarks neglect the fine-grained and long-horizon nature of authentic laboratory work, especially in wet-lab settings. To bridge this gap, we introduce ExpVid, the first benchmark designed to systematically evaluate MLLMs on scientific experiment videos. Curated from peer-reviewed video publications, ExpVid features a new three-level task hierarchy that mirrors the scientific process: (1) Fine-grained Perception of tools, materials, and actions; (2) Procedural Understanding of step order and completeness; and (3) Scientific Reasoning that connects the full experiment to its published conclusions. Our vision-centric annotation pipeline, combining automated generation with multi-disciplinary expert validation, ensures that tasks require visual grounding. We evaluate 19 leading MLLMs on ExpVid and find that while they excel at coarse-grained recognition, they struggle with disambiguating fine details, tracking state changes over time, and linking experimental procedures to scientific outcomes. Our results reveal a notable performance gap between proprietary and open-source models, particularly in high-order reasoning. ExpVid not only provides a diagnostic tool but also charts a roadmap for developing MLLMs capable of becoming trustworthy partners in scientific experimentation.",
    "summary": "",
    "translation": "ExpVid：一个用于实验视频理解与推理的基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于实验视频理解基准，属于纯粹的视觉领域研究，与推荐系统、搜索或广告的核心技术无关。虽然视频理解技术在某些边缘场景可能有潜在应用，但该论文明确针对实验视频这一特定领域，缺乏与RecSys/Search/Ads的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11605v1": {
    "title": "ACE-G: Improving Generalization of Scene Coordinate Regression Through Query Pre-Training",
    "url": "https://www.alphaxiv.org/abs/2510.11605v1",
    "arxiv_id": "2510.11605v1",
    "authors": "Leonard Bruns, Axel Barroso-Laguna, Tommaso Cavallari, Áron Monszpart, Sowmya Munukutla, Victor Adrian Prisacariu, Eric Brachmann",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 16:45:17",
    "ori_summary": "Scene coordinate regression (SCR) has established itself as a promising learning-based approach to visual relocalization. After mere minutes of scene-specific training, SCR models estimate camera poses of query images with high accuracy. Still, SCR methods fall short of the generalization capabilities of more classical feature-matching approaches. When imaging conditions of query images, such as lighting or viewpoint, are too different from the training views, SCR models fail. Failing to generalize is an inherent limitation of previous SCR frameworks, since their training objective is to encode the training views in the weights of the coordinate regressor itself. The regressor essentially overfits to the training views, by design. We propose to separate the coordinate regressor and the map representation into a generic transformer and a scene-specific map code. This separation allows us to pre-train the transformer on tens of thousands of scenes. More importantly, it allows us to train the transformer to generalize from mapping images to unseen query images during pre-training. We demonstrate on multiple challenging relocalization datasets that our method, ACE-G, leads to significantly increased robustness while keeping the computational footprint attractive.",
    "summary": "",
    "translation": "ACE-G：通过查询预训练提升场景坐标回归的泛化能力",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉中的场景坐标回归问题，属于纯粹的视觉定位和3D场景理解领域。虽然标题提到泛化改进，但该技术主要应用于机器人导航、增强现实等视觉任务，与推荐系统、搜索或广告的核心技术栈没有直接关联，也没有明显的Transformer架构或LLM应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11579v1": {
    "title": "MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis",
    "url": "https://www.alphaxiv.org/abs/2510.11579v1",
    "arxiv_id": "2510.11579v1",
    "authors": "Hongyu Zhu, Lin Chen, Mounim A. El-Yacoubi, Mingsheng Shang",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-13 16:23:32",
    "ori_summary": "Multimodal Sentiment Analysis (MSA) aims to identify and interpret human emotions by integrating information from heterogeneous data sources such as text, video, and audio. While deep learning models have advanced in network architecture design, they remain heavily limited by scarce multimodal annotated data. Although Mixup-based augmentation improves generalization in unimodal tasks, its direct application to MSA introduces critical challenges: random mixing often amplifies label ambiguity and semantic inconsistency due to the lack of emotion-aware mixing mechanisms. To overcome these issues, we propose MS-Mix, an adaptive, emotion-sensitive augmentation framework that automatically optimizes sample mixing in multimodal settings. The key components of MS-Mix include: (1) a Sentiment-Aware Sample Selection (SASS) strategy that effectively prevents semantic confusion caused by mixing samples with contradictory emotions. (2) a Sentiment Intensity Guided (SIG) module using multi-head self-attention to compute modality-specific mixing ratios dynamically based on their respective emotional intensities. (3) a Sentiment Alignment Loss (SAL) that aligns the prediction distributions across modalities, and incorporates the Kullback-Leibler-based loss as an additional regularization term to train the emotion intensity predictor and the backbone network jointly. Extensive experiments on three benchmark datasets with six state-of-the-art backbones confirm that MS-Mix consistently outperforms existing methods, establishing a new standard for robust multimodal sentiment augmentation. The source code is available at: https://github.com/HongyuZhu-s/MS-Mix.",
    "summary": "",
    "translation": "MS-Mix：揭示Mixup在多模态情感分析中的强大能力",
    "relevance_score": 3,
    "reasoning": "虽然该论文涉及多模态学习和数据增强技术（Mixup），但其核心应用领域是多模态情感分析，这属于纯粹的NLP应用范畴。Mixup技术本身在理论上可能应用于推荐系统中的多模态特征融合，但论文标题明确限定于情感分析，与RecSys/Search/Ads的直接关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11576v1": {
    "title": "Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping",
    "url": "https://www.alphaxiv.org/abs/2510.11576v1",
    "arxiv_id": "2510.11576v1",
    "authors": "Walid Elbarz, Mohamed Bourriz, Hicham Hajji, Hamd Ait Abdelali, François Bourzeix",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 16:21:59",
    "ori_summary": "Foundation models are transforming Earth observation, but their potential for hyperspectral crop mapping remains underexplored. This study benchmarks three foundation models for cereal crop mapping using hyperspectral imagery: HyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarth dataset (a large multitemporal hyperspectral archive). Models were fine-tuned on manually labeled data from a training region and evaluated on an independent test region. Performance was measured with overall accuracy (OA), average accuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%), DOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of 93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved 91%, highlighting the importance of model architecture for strong generalization across geographic regions and sensor platforms. These results provide a systematic evaluation of foundation models for operational hyperspectral crop mapping and outline directions for future model development.",
    "summary": "",
    "translation": "基准测试用于高光谱图像分类的基础模型：在谷物作物类型映射中的应用",
    "relevance_score": 1,
    "reasoning": "该论文专注于高光谱图像分类和作物类型映射，属于纯粹的视觉/遥感应用领域，与推荐系统、搜索或广告没有直接关联。基础模型基准测试虽然涉及模型评估，但应用场景是农业遥感，完全超出了当前关注的技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11567v1": {
    "title": "A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.11567v1",
    "arxiv_id": "2510.11567v1",
    "authors": "Denis Zavadski, Damjan Kalšan, Tim Küchler, Haebom Lee, Stefan Roth, Carsten Rother",
    "categories": "cs.CV, cs.GR, cs.LG",
    "pub_date": "2025-10-13 16:12:29",
    "ori_summary": "Synthetic datasets are widely used for training urban scene recognition models, but even highly realistic renderings show a noticeable gap to real imagery. This gap is particularly pronounced when adapting to a specific target domain, such as Cityscapes, where differences in architecture, vegetation, object appearance, and camera characteristics limit downstream performance. Closing this gap with more detailed 3D modelling would require expensive asset and scene design, defeating the purpose of low-cost labelled data. To address this, we present a new framework that adapts an off-the-shelf diffusion model to a target domain using only imperfect pseudo-labels. Once trained, it generates high-fidelity, target-aligned images from semantic maps of any synthetic dataset, including low-effort sources created in hours rather than months. The method filters suboptimal generations, rectifies image-label misalignments, and standardises semantics across datasets, transforming weak synthetic data into competitive real-domain training sets. Experiments on five synthetic datasets and two real target datasets show segmentation gains of up to +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly constructed synthetic datasets as effective as high-effort, time-intensive synthetic datasets requiring extensive manual design. This work highlights a valuable collaborative paradigm where fast semantic prototyping, combined with generative models, enables scalable, high-quality training data creation for urban scene understanding.",
    "summary": "",
    "translation": "面向城市语义分割的低成本训练数据生成框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的语义分割任务，特别是城市场景的应用。这属于纯粹的视觉研究范畴，与推荐系统、搜索或广告的核心技术没有直接关联。论文内容涉及训练数据生成方法，但未体现任何与LLM、Transformer架构或异构数据建模相关的技术，无法应用于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11566v1": {
    "title": "SCOOP'D: Learning Mixed-Liquid-Solid Scooping via Sim2Real Generative Policy",
    "url": "https://www.alphaxiv.org/abs/2510.11566v1",
    "arxiv_id": "2510.11566v1",
    "authors": "Kuanning Wang, Yongchong Gu, Yuqian Fu, Zeyu Shangguan, Sicheng He, Xiangyang Xue, Yanwei Fu, Daniel Seita",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-13 16:11:34",
    "ori_summary": "Scooping items with tools such as spoons and ladles is common in daily life, ranging from assistive feeding to retrieving items from environmental disaster sites. However, developing a general and autonomous robotic scooping policy is challenging since it requires reasoning about complex tool-object interactions. Furthermore, scooping often involves manipulating deformable objects, such as granular media or liquids, which is challenging due to their infinite-dimensional configuration spaces and complex dynamics. We propose a method, SCOOP'D, which uses simulation from OmniGibson (built on NVIDIA Omniverse) to collect scooping demonstrations using algorithmic procedures that rely on privileged state information. Then, we use generative policies via diffusion to imitate demonstrations from observational input. We directly apply the learned policy in diverse real-world scenarios, testing its performance on various item quantities, item characteristics, and container types. In zero-shot deployment, our method demonstrates promising results across 465 trials in diverse scenarios, including objects of different difficulty levels that we categorize as \"Level 1\" and \"Level 2.\" SCOOP'D outperforms all baselines and ablations, suggesting that this is a promising approach to acquiring robotic scooping skills. Project page is at https://scoopdiff.github.io/.",
    "summary": "",
    "translation": "SCOOP'D：通过模拟到现实的生成策略学习混合液固物质的舀取",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于机器人物理操作和模拟到现实的迁移学习，涉及混合液固物质的舀取任务。这与推荐系统、搜索、广告或相关使能技术没有任何明显关联，完全超出了当前关注的技术领域范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11565v1": {
    "title": "SNAP: Towards Segmenting Anything in Any Point Cloud",
    "url": "https://www.alphaxiv.org/abs/2510.11565v1",
    "arxiv_id": "2510.11565v1",
    "authors": "Aniket Gupta, Hanhui Wang, Charles Saunders, Aruni RoyChowdhury, Hanumant Singh, Huaizu Jiang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 16:07:00",
    "ori_summary": "Interactive 3D point cloud segmentation enables efficient annotation of complex 3D scenes through user-guided prompts. However, current approaches are typically restricted in scope to a single domain (indoor or outdoor), and to a single form of user interaction (either spatial clicks or textual prompts). Moreover, training on multiple datasets often leads to negative transfer, resulting in domain-specific tools that lack generalizability. To address these limitations, we present \\textbf{SNAP} (\\textbf{S}egment a\\textbf{N}ything in \\textbf{A}ny \\textbf{P}oint cloud), a unified model for interactive 3D segmentation that supports both point-based and text-based prompts across diverse domains. Our approach achieves cross-domain generalizability by training on 7 datasets spanning indoor, outdoor, and aerial environments, while employing domain-adaptive normalization to prevent negative transfer. For text-prompted segmentation, we automatically generate mask proposals without human intervention and match them against CLIP embeddings of textual queries, enabling both panoptic and open-vocabulary segmentation. Extensive experiments demonstrate that SNAP consistently delivers high-quality segmentation results. We achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for spatial-prompted segmentation and demonstrate competitive results on all 5 text-prompted benchmarks. These results show that a unified model can match or exceed specialized domain-specific approaches, providing a practical tool for scalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/",
    "summary": "",
    "translation": "SNAP：面向任意点云中任意目标分割",
    "relevance_score": 2,
    "reasoning": "该论文专注于3D点云分割技术，属于计算机视觉领域。虽然点云处理在自动驾驶和机器人领域有应用，但与搜索、推荐、广告系统的核心需求（用户行为建模、内容理解、排序优化）缺乏直接关联。点云分割技术难以转化为处理用户序列、上下文特征或文本内容等推荐系统典型数据模态。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11553v1": {
    "title": "How many samples to label for an application given a foundation model? Chest X-ray classification study",
    "url": "https://www.alphaxiv.org/abs/2510.11553v1",
    "arxiv_id": "2510.11553v1",
    "authors": "Nikolay Nechaev, Evgenia Przhezdzetskaya, Viktor Gombolevskiy, Dmitry Umerenkov, Dmitry Dylov",
    "categories": "cs.CV, 68T07 (Primary) 68T45, 62H30, 62P10 (Secondary)",
    "pub_date": "2025-10-13 15:53:55",
    "ori_summary": "Chest X-ray classification is vital yet resource-intensive, typically demanding extensive annotated data for accurate diagnosis. Foundation models mitigate this reliance, but how many labeled samples are required remains unclear. We systematically evaluate the use of power-law fits to predict the training size necessary for specific ROC-AUC thresholds. Testing multiple pathologies and foundation models, we find XrayCLIP and XraySigLIP achieve strong performance with significantly fewer labeled examples than a ResNet-50 baseline. Importantly, learning curve slopes from just 50 labeled cases accurately forecast final performance plateaus. Our results enable practitioners to minimize annotation costs by labeling only the essential samples for targeted performance.",
    "summary": "",
    "translation": "给定基础模型，为应用标注多少样本？胸部X光分类研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（胸部X光）分类的样本标注问题，这属于医疗领域的特定应用。虽然涉及基础模型，但内容与推荐系统、搜索或广告的核心技术进展、Transformer架构改进或异构数据统一建模完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11549v1": {
    "title": "ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?",
    "url": "https://www.alphaxiv.org/abs/2510.11549v1",
    "arxiv_id": "2510.11549v1",
    "authors": "Liu Yang, Huiyu Duan, Ran Tao, Juntao Cheng, Sijing Wu, Yunhao Li, Jing Liu, Xiongkuo Min, Guangtao Zhai",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 15:51:47",
    "ori_summary": "Omnidirectional images (ODIs) provide full 360x180 view which are widely adopted in VR, AR and embodied intelligence applications. While multi-modal large language models (MLLMs) have demonstrated remarkable performance on conventional 2D image and video understanding benchmarks, their ability to comprehend the immersive environments captured by ODIs remains largely unexplored. To address this gap, we first present ODI-Bench, a novel comprehensive benchmark specifically designed for omnidirectional image understanding. ODI-Bench contains 2,000 high-quality omnidirectional images and over 4,000 manually annotated question-answering (QA) pairs across 10 fine-grained tasks, covering both general-level and spatial-level ODI understanding. Extensive experiments are conducted to benchmark 20 representative MLLMs, including proprietary and open-source models, under both close-ended and open-ended settings. Experimental results reveal that current MLLMs still struggle to capture the immersive context provided by ODIs. To this end, we further introduce Omni-CoT, a training-free method which significantly enhances MLLMs' comprehension ability in the omnidirectional environment through chain-of-thought reasoning across both textual information and visual cues. Both the benchmark and the code will be released upon the publication.",
    "summary": "",
    "translation": "ODI-Bench：多模态大语言模型能否理解沉浸式全向环境？",
    "relevance_score": 2,
    "reasoning": "该论文关注多模态大语言模型对全向环境的理解能力，属于纯粹的视觉-语言多模态研究范畴。虽然标题提及'沉浸式环境'，但缺乏与推荐系统、搜索或广告领域的明确关联，且未涉及Transformer架构效率、注意力机制等核心技术进展。全向环境理解在RecSys/Search/Ads中的潜在应用场景非常有限且不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11538v1": {
    "title": "Massive Activations are the Key to Local Detail Synthesis in Diffusion Transformers",
    "url": "https://www.alphaxiv.org/abs/2510.11538v1",
    "arxiv_id": "2510.11538v1",
    "authors": "Chaofan Gan, Zicheng Zhao, Yuanpeng Tu, Xi Chen, Ziran Qin, Tieyuan Chen, Mehrtash Harandi, Weiyao Lin",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 15:39:13",
    "ori_summary": "Diffusion Transformers (DiTs) have recently emerged as a powerful backbone for visual generation. Recent observations reveal \\emph{Massive Activations} (MAs) in their internal feature maps, yet their function remains poorly understood. In this work, we systematically investigate these activations to elucidate their role in visual generation. We found that these massive activations occur across all spatial tokens, and their distribution is modulated by the input timestep embeddings. Importantly, our investigations further demonstrate that these massive activations play a key role in local detail synthesis, while having minimal impact on the overall semantic content of output. Building on these insights, we propose \\textbf{D}etail \\textbf{G}uidance (\\textbf{DG}), a MAs-driven, training-free self-guidance strategy to explicitly enhance local detail fidelity for DiTs. Specifically, DG constructs a degraded ``detail-deficient'' model by disrupting MAs and leverages it to guide the original network toward higher-quality detail synthesis. Our DG can seamlessly integrate with Classifier-Free Guidance (CFG), enabling further refinements of fine-grained details. Extensive experiments demonstrate that our DG consistently improves fine-grained detail quality across various pre-trained DiTs (\\eg, SD3, SD3.5, and Flux).",
    "summary": "",
    "translation": "大规模激活是扩散变换器中局部细节合成的关键",
    "relevance_score": 2,
    "reasoning": "该论文主要关注扩散变换器中的细节合成技术，属于图像生成领域。虽然提到了Transformer架构，但其核心应用是内容生成而非推荐、搜索或广告系统。该技术可能间接影响广告创意生成，但这属于明确排除的非相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11520v1": {
    "title": "mmWalk: Towards Multi-modal Multi-view Walking Assistance",
    "url": "https://www.alphaxiv.org/abs/2510.11520v1",
    "arxiv_id": "2510.11520v1",
    "authors": "Kedi Ying, Ruiping Liu, Chongyan Chen, Mingzhe Tao, Hao Shi, Kailun Yang, Jiaming Zhang, Rainer Stiefelhagen",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 15:25:52",
    "ori_summary": "Walking assistance in extreme or complex environments remains a significant challenge for people with blindness or low vision (BLV), largely due to the lack of a holistic scene understanding. Motivated by the real-world needs of the BLV community, we build mmWalk, a simulated multi-modal dataset that integrates multi-view sensor and accessibility-oriented features for outdoor safe navigation. Our dataset comprises 120 manually controlled, scenario-categorized walking trajectories with 62k synchronized frames. It contains over 559k panoramic images across RGB, depth, and semantic modalities. Furthermore, to emphasize real-world relevance, each trajectory involves outdoor corner cases and accessibility-specific landmarks for BLV users. Additionally, we generate mmWalkVQA, a VQA benchmark with over 69k visual question-answer triplets across 9 categories tailored for safe and informed walking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs) using zero- and few-shot settings and found they struggle with our risk assessment and navigational tasks. We validate our mmWalk-finetuned model on real-world datasets and show the effectiveness of our dataset for advancing multi-modal walking assistance.",
    "summary": "",
    "translation": "mmWalk：迈向多模态多视角行走辅助",
    "relevance_score": 1,
    "reasoning": "该论文标题表明其专注于多模态行走辅助系统，这属于物理辅助设备或医疗康复领域，与推荐系统、搜索或广告的核心技术完全无关。论文内容可能涉及传感器融合、步态分析等医疗/机器人技术，没有任何与LLM、Transformer架构或推荐/搜索/广告系统的潜在应用关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11512v1": {
    "title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference",
    "url": "https://www.alphaxiv.org/abs/2510.11512v1",
    "arxiv_id": "2510.11512v1",
    "authors": "Jianhao Yuan, Fabio Pizzati, Francesco Pinto, Lars Kunze, Ivan Laptev, Paul Newman, Philip Torr, Daniele De Martini",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 15:19:07",
    "ori_summary": "Intuitive physics understanding in video diffusion models plays an essential role in building general-purpose physically plausible world simulators, yet accurately evaluating such capacity remains a challenging task due to the difficulty in disentangling physics correctness from visual appearance in generation. To the end, we introduce LikePhys, a training-free method that evaluates intuitive physics in video diffusion models by distinguishing physically valid and impossible videos using the denoising objective as an ELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By testing on our constructed benchmark of twelve scenarios spanning over four physics domains, we show that our evaluation metric, Plausibility Preference Error (PPE), demonstrates strong alignment with human preference, outperforming state-of-the-art evaluator baselines. We then systematically benchmark intuitive physics understanding in current video diffusion models. Our study further analyses how model design and inference settings affect intuitive physics understanding and highlights domain-specific capacity variations across physical laws. Empirical results show that, despite current models struggling with complex and chaotic dynamics, there is a clear trend of improvement in physics understanding as model capacity and inference settings scale.",
    "summary": "",
    "translation": "LikePhys：通过似然偏好评估视频扩散模型中的直觉物理理解",
    "relevance_score": 1,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false,
    "relevance_reasoning": "该论文专注于视频扩散模型的物理理解评估，属于纯粹的视觉生成和评估领域。虽然涉及扩散模型技术，但论文内容聚焦于物理场景理解评估，与推荐系统、搜索或广告的核心技术需求没有直接关联，也不涉及Transformer架构改进或异构数据统一建模等焦点领域。"
  },
  "2510.11509v1": {
    "title": "Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal Large Language Model",
    "url": "https://www.alphaxiv.org/abs/2510.11509v1",
    "arxiv_id": "2510.11509v1",
    "authors": "Ruiping Liu, Junwei Zheng, Yufan Chen, Zirui Wang, Kunyu Peng, Kailun Yang, Jiaming Zhang, Marc Pollefeys, Rainer Stiefelhagen",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 15:17:18",
    "ori_summary": "Physical environments and circumstances are fundamentally dynamic, yet current 3D datasets and evaluation benchmarks tend to concentrate on either dynamic scenarios or dynamic situations in isolation, resulting in incomplete comprehension. To overcome these constraints, we introduce Situat3DChange, an extensive dataset supporting three situation-aware change understanding tasks following the perception-action model: 121K question-answer pairs, 36K change descriptions for perception tasks, and 17K rearrangement instructions for the action task. To construct this large-scale dataset, Situat3DChange leverages 11K human observations of environmental changes to establish shared mental models and shared situational awareness for human-AI collaboration. These observations, enriched with egocentric and allocentric perspectives as well as categorical and coordinate spatial relations, are integrated using an LLM to support understanding of situated changes. To address the challenge of comparing pairs of point clouds from the same scene with minor changes, we propose SCReasoner, an efficient 3D MLLM approach that enables effective point cloud comparison with minimal parameter overhead and no additional tokens required for the language decoder. Comprehensive evaluation on Situat3DChange tasks highlights both the progress and limitations of MLLMs in dynamic scene and situation understanding. Additional experiments on data scaling and cross-domain transfer demonstrate the task-agnostic effectiveness of using Situat3DChange as a training dataset for MLLMs.",
    "summary": "",
    "translation": "Situat3DChange：面向多模态大语言模型的场景化三维变化理解数据集",
    "relevance_score": 2,
    "reasoning": "该论文主要关注3D视觉变化理解数据集和多模态大语言模型，属于纯粹的视觉和多模态研究范畴。虽然涉及多模态大语言模型技术，但缺乏与推荐系统、搜索或广告领域的明确关联，其3D视觉焦点超出了当前关注的技术应用范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11508v1": {
    "title": "Towards Fast and Scalable Normal Integration using Continuous Components",
    "url": "https://www.alphaxiv.org/abs/2510.11508v1",
    "arxiv_id": "2510.11508v1",
    "authors": "Francesco Milano, Jen Jen Chung, Lionel Ott, Roland Siegwart",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 15:17:16",
    "ori_summary": "Surface normal integration is a fundamental problem in computer vision, dealing with the objective of reconstructing a surface from its corresponding normal map. Existing approaches require an iterative global optimization to jointly estimate the depth of each pixel, which scales poorly to larger normal maps. In this paper, we address this problem by recasting normal integration as the estimation of relative scales of continuous components. By constraining pixels belonging to the same component to jointly vary their scale, we drastically reduce the number of optimization variables. Our framework includes a heuristic to accurately estimate continuous components from the start, a strategy to rebalance optimization terms, and a technique to iteratively merge components to further reduce the size of the problem. Our method achieves state-of-the-art results on the standard normal integration benchmark in as little as a few seconds and achieves one-order-of-magnitude speedup over pixel-level approaches on large-resolution normal maps.",
    "summary": "",
    "translation": "面向使用连续分量的快速可扩展法向积分",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及计算机视觉中的法向积分技术，主要用于3D重建和表面建模，属于纯粹的视觉处理范畴。没有证据表明该方法在推荐系统、搜索或广告领域有直接应用潜力，也不涉及LLM、Transformer架构或异构数据处理等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11496v1": {
    "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model",
    "url": "https://www.alphaxiv.org/abs/2510.11496v1",
    "arxiv_id": "2510.11496v1",
    "authors": "Zhiwei Jin, Xiaohui Song, Nan Wang, Yafei Liu, Chao Li, Xin Li, Ruichen Wang, Zhihao Li, Qi Qi, Long Cheng, Dongze Hao, Quanlong Zheng, Yanhao Zhang, Haobo Ji, Jian Ma, Zhitong Zheng, Zhenyi Lin, Haolin Deng, Xin Zou, Xiaojie Yin, Ruilin Wang, Liankai Cai, Haijing Liu, Yuqing Qiu, Ke Chen, Zixian Li, Chi Xie, Huafei Li, Chenxing Li, Chuangchuang Wang, Kai Tang, Zhiguang Zhu, Kai Tang, Wenmei Gao, Rui Wang, Jun Wu, Chao Liu, Qin Xie, Chen Chen, Haonan Lu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 15:04:38",
    "ori_summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoR",
    "summary": "",
    "translation": "AndesVL技术报告：一种高效的移动端多模态大语言模型",
    "relevance_score": 6,
    "reasoning": "该论文专注于移动端高效多模态大语言模型，属于'Enabling LLM Tech'范畴，通过模型效率优化支持移动设备部署。在推荐系统、搜索和广告领域，这种高效多模态模型可应用于移动端的个性化推荐、多模态搜索和广告内容理解，提升用户体验和系统性能。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.11473v1": {
    "title": "VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment",
    "url": "https://www.alphaxiv.org/abs/2510.11473v1",
    "arxiv_id": "2510.11473v1",
    "authors": "Qing Li, Huifang Feng, Xun Gong, Yu-Shen Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 14:44:50",
    "ori_summary": "3D Gaussian Splatting has recently emerged as an efficient solution for high-quality and real-time novel view synthesis. However, its capability for accurate surface reconstruction remains underexplored. Due to the discrete and unstructured nature of Gaussians, supervision based solely on image rendering loss often leads to inaccurate geometry and inconsistent multi-view alignment. In this work, we propose a novel method that enhances the geometric representation of 3D Gaussians through view alignment (VA). Specifically, we incorporate edge-aware image cues into the rendering loss to improve surface boundary delineation. To enforce geometric consistency across views, we introduce a visibility-aware photometric alignment loss that models occlusions and encourages accurate spatial relationships among Gaussians. To further mitigate ambiguities caused by lighting variations, we incorporate normal-based constraints to refine the spatial orientation of Gaussians and improve local surface estimation. Additionally, we leverage deep image feature embeddings to enforce cross-view consistency, enhancing the robustness of the learned geometry under varying viewpoints and illumination. Extensive experiments on standard benchmarks demonstrate that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis. The source code is available at https://github.com/LeoQLi/VA-GS.",
    "summary": "",
    "translation": "VA-GS：通过视角对齐增强高斯泼溅的几何表示",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机图形学中的3D重建技术（高斯泼溅），属于纯粹的视觉领域研究。虽然标题提到几何表示增强，但没有任何内容表明与推荐系统、搜索或广告有直接或间接的潜在应用关联。这完全属于被排除的纯粹视觉论文范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11456v1": {
    "title": "Coupled Degradation Modeling and Fusion: A VLM-Guided Degradation-Coupled Network for Degradation-Aware Infrared and Visible Image Fusion",
    "url": "https://www.alphaxiv.org/abs/2510.11456v1",
    "arxiv_id": "2510.11456v1",
    "authors": "Tianpei Zhang, Jufeng Zhao, Yiming Zhu, Guangmang Cui",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 14:26:33",
    "ori_summary": "Existing Infrared and Visible Image Fusion (IVIF) methods typically assume high-quality inputs. However, when handing degraded images, these methods heavily rely on manually switching between different pre-processing techniques. This decoupling of degradation handling and image fusion leads to significant performance degradation. In this paper, we propose a novel VLM-Guided Degradation-Coupled Fusion network (VGDCFusion), which tightly couples degradation modeling with the fusion process and leverages vision-language models (VLMs) for degradation-aware perception and guided suppression. Specifically, the proposed Specific-Prompt Degradation-Coupled Extractor (SPDCE) enables modality-specific degradation awareness and establishes a joint modeling of degradation suppression and intra-modal feature extraction. In parallel, the Joint-Prompt Degradation-Coupled Fusion (JPDCF) facilitates cross-modal degradation perception and couples residual degradation filtering with complementary cross-modal feature fusion. Extensive experiments demonstrate that our VGDCFusion significantly outperforms existing state-of-the-art fusion approaches under various degraded image scenarios. Our code is available at https://github.com/Lmmh058/VGDCFusion.",
    "summary": "",
    "translation": "耦合退化建模与融合：基于视觉语言模型的退化耦合网络用于退化感知的红外与可见光图像融合",
    "relevance_score": 1,
    "reasoning": "该论文专注于红外与可见光图像融合的计算机视觉任务，属于纯粹的视觉处理领域。虽然提到了VLM（视觉语言模型）作为指导，但核心内容是图像退化建模和融合技术，与推荐系统、搜索或广告领域没有直接关联。论文的技术方向更偏向于多模态视觉处理，而非用户行为建模或内容排序等RecSys/Search/Ads核心问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11449v1": {
    "title": "Enhancing Maritime Domain Awareness on Inland Waterways: A YOLO-Based Fusion of Satellite and AIS for Vessel Characterization",
    "url": "https://www.alphaxiv.org/abs/2510.11449v1",
    "arxiv_id": "2510.11449v1",
    "authors": "Geoffery Agorku, Sarah Hernandez, Hayley Hames, Cade Wagner",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 14:19:58",
    "ori_summary": "Maritime Domain Awareness (MDA) for inland waterways remains challenged by cooperative system vulnerabilities. This paper presents a novel framework that fuses high-resolution satellite imagery with vessel trajectory data from the Automatic Identification System (AIS). This work addresses the limitations of AIS-based monitoring by leveraging non-cooperative satellite imagery and implementing a fusion approach that links visual detections with AIS data to identify dark vessels, validate cooperative traffic, and support advanced MDA. The You Only Look Once (YOLO) v11 object detection model is used to detect and characterize vessels and barges by vessel type, barge cover, operational status, barge count, and direction of travel. An annotated data set of 4,550 instances was developed from $5{,}973~\\mathrm{mi}^2$ of Lower Mississippi River imagery. Evaluation on a held-out test set demonstrated vessel classification (tugboat, crane barge, bulk carrier, cargo ship, and hopper barge) with an F1 score of 95.8\\%; barge cover (covered or uncovered) detection yielded an F1 score of 91.6\\%; operational status (staged or in motion) classification reached an F1 score of 99.4\\%. Directionality (upstream, downstream) yielded 93.8\\% accuracy. The barge count estimation resulted in a mean absolute error (MAE) of 2.4 barges. Spatial transferability analysis across geographically disjoint river segments showed accuracy was maintained as high as 98\\%. These results underscore the viability of integrating non-cooperative satellite sensing with AIS fusion. This approach enables near-real-time fleet inventories, supports anomaly detection, and generates high-quality data for inland waterway surveillance. Future work will expand annotated datasets, incorporate temporal tracking, and explore multi-modal deep learning to further enhance operational scalability.",
    "summary": "",
    "translation": "增强内河水域海事领域感知：基于YOLO的卫星与AIS数据融合用于船舶特征识别",
    "relevance_score": 1,
    "reasoning": "该论文专注于海事领域的计算机视觉应用，使用YOLO进行船舶检测和卫星/AIS数据融合。这与搜索、推荐或广告系统无关，也不涉及LLM、Transformer架构或异质数据统一建模等核心技术。该研究属于纯粹的视觉应用领域，没有明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11417v1": {
    "title": "Robust Ego-Exo Correspondence with Long-Term Memory",
    "url": "https://www.alphaxiv.org/abs/2510.11417v1",
    "arxiv_id": "2510.11417v1",
    "authors": "Yijun Hu, Bing Fan, Xin Gu, Haiqing Ren, Dongfang Liu, Heng Fan, Libo Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 13:54:12",
    "ori_summary": "Establishing object-level correspondence between egocentric and exocentric views is essential for intelligent assistants to deliver precise and intuitive visual guidance. However, this task faces numerous challenges, including extreme viewpoint variations, occlusions, and the presence of small objects. Existing approaches usually borrow solutions from video object segmentation models, but still suffer from the aforementioned challenges. Recently, the Segment Anything Model 2 (SAM 2) has shown strong generalization capabilities and excellent performance in video object segmentation. Yet, when simply applied to the ego-exo correspondence (EEC) task, SAM 2 encounters severe difficulties due to ineffective ego-exo feature fusion and limited long-term memory capacity, especially for long videos. Addressing these problems, we propose a novel EEC framework based on SAM 2 with long-term memories by presenting a dual-memory architecture and an adaptive feature routing module inspired by Mixture-of-Experts (MoE). Compared to SAM 2, our approach features (i) a Memory-View MoE module which consists of a dual-branch routing mechanism to adaptively assign contribution weights to each expert feature along both channel and spatial dimensions, and (ii) a dual-memory bank system with a simple yet effective compression strategy to retain critical long-term information while eliminating redundancy. In the extensive experiments on the challenging EgoExo4D benchmark, our method, dubbed LM-EEC, achieves new state-of-the-art results and significantly outperforms existing methods and the SAM 2 baseline, showcasing its strong generalization across diverse scenarios. Our code and model are available at https://github.com/juneyeeHu/LM-EEC.",
    "summary": "",
    "translation": "基于长期记忆的鲁棒自我-外部对应关系",
    "relevance_score": 2,
    "reasoning": "该论文关注计算机视觉中的自我-外部相机对应关系，主要涉及跨视角的时序对齐问题。虽然长期记忆机制在推荐系统中可能有潜在应用，但论文的核心焦点是视觉对应关系而非推荐、搜索或广告领域。该技术可能通过改进用户行为序列建模间接应用于推荐系统，但直接相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11387v1": {
    "title": "MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference",
    "url": "https://www.alphaxiv.org/abs/2510.11387v1",
    "arxiv_id": "2510.11387v1",
    "authors": "Wenyuan Zhang, Jimin Tang, Weiqi Zhang, Yi Fang, Yu-Shen Liu, Zhizhong Han",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 13:29:20",
    "ori_summary": "Modeling reflections from 2D images is essential for photorealistic rendering and novel view synthesis. Recent approaches enhance Gaussian primitives with reflection-related material attributes to enable physically based rendering (PBR) with Gaussian Splatting. However, the material inference often lacks sufficient constraints, especially under limited environment modeling, resulting in illumination aliasing and reduced generalization. In this work, we revisit the problem from a multi-view perspective and show that multi-view consistent material inference with more physically-based environment modeling is key to learning accurate reflections with Gaussian Splatting. To this end, we enforce 2D Gaussians to produce multi-view consistent material maps during deferred shading. We also track photometric variations across views to identify highly reflective regions, which serve as strong priors for reflection strength terms. To handle indirect illumination caused by inter-object occlusions, we further introduce an environment modeling strategy through ray tracing with 2DGS, enabling photorealistic rendering of indirect radiance. Experiments on widely used benchmarks show that our method faithfully recovers both illumination and geometry, achieving state-of-the-art rendering quality in novel views synthesis.",
    "summary": "",
    "translation": "MaterialRefGS：具有多视角一致材质推断的反射高斯泼溅",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的3D重建和材质推断技术，属于纯粹的视觉领域研究。虽然高斯泼溅是3D表示的新方法，但该工作没有展示与推荐系统、搜索或广告的明显关联，也不涉及Transformer架构、LLM技术或异构数据建模等核心关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11369v1": {
    "title": "Reasoning as Representation: Rethinking Visual Reinforcement Learning in Image Quality Assessment",
    "url": "https://www.alphaxiv.org/abs/2510.11369v1",
    "arxiv_id": "2510.11369v1",
    "authors": "Shijie Zhao, Xuanyu Zhang, Weiqi Li, Junlin Li, Li Zhang, Tianfan Xue, Jian Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 13:11:08",
    "ori_summary": "Reasoning-based image quality assessment (IQA) models trained through reinforcement learning (RL) exhibit exceptional generalization, yet the underlying mechanisms and critical factors driving this capability remain underexplored in current research. Moreover, despite their superior performance, these models incur inference energy usage and latency orders of magnitude higher than their earlier counterparts, restricting their deployment in specific scenarios. Through extensive experiments, this paper verifies and elaborates that through RL training, MLLMs leverage their reasoning capability to convert redundant visual representations into compact, cross-domain aligned text representations. This conversion is precisely the source of the generalization exhibited by these reasoning-based IQA models. Building on this fundamental insight, we propose a novel algorithm, RALI, which employs contrastive learning to directly align images with these generalizable text representations learned by RL. This approach eliminates the reliance on reasoning processes and even obviates the need to load an LLM. For the quality scoring task, this framework achieves generalization performance comparable to reasoning-based models while requiring less than 5% of their model parameters and inference time.",
    "summary": "",
    "translation": "推理即表征：重新思考图像质量评估中的视觉强化学习",
    "relevance_score": 1,
    "reasoning": "该论文主要关注视觉强化学习在图像质量评估中的应用，这属于纯粹的视觉领域研究，与推荐系统、搜索或广告没有明确关联。论文标题中提到的强化学习也没有展示出在RecSys/Search/Ads领域的直接应用潜力，因此相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11346v1": {
    "title": "Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.11346v1",
    "arxiv_id": "2510.11346v1",
    "authors": "Joshua Niemeijer, Jan Ehrhardt, Heinz Handels, Hristina Uzunova",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 12:41:28",
    "ori_summary": "Generative Models are a valuable tool for the controlled creation of high-quality image data. Controlled diffusion models like the ControlNet have allowed the creation of labeled distributions. Such synthetic datasets can augment the original training distribution when discriminative models, like semantic segmentation, are trained. However, this augmentation effect is limited since ControlNets tend to reproduce the original training distribution. This work introduces a method to utilize data from unlabeled domains to train ControlNets by introducing the concept of uncertainty into the control mechanism. The uncertainty indicates that a given image was not part of the training distribution of a downstream task, e.g., segmentation. Thus, two types of control are engaged in the final network: an uncertainty control from an unlabeled dataset and a semantic control from the labeled dataset. The resulting ControlNet allows us to create annotated data with high uncertainty from the target domain, i.e., synthetic data from the unlabeled distribution with labels. In our scenario, we consider retinal OCTs, where typically high-quality Spectralis images are available with given ground truth segmentations, enabling the training of segmentation networks. The recent development in Home-OCT devices, however, yields retinal OCTs with lower quality and a large domain shift, such that out-of-the-pocket segmentation networks cannot be applied for this type of data. Synthesizing annotated images from the Home-OCT domain using the proposed approach closes this gap and leads to significantly improved segmentation results without adding any further supervision. The advantage of uncertainty-guidance becomes obvious when compared to style transfer: it enables arbitrary domain shifts without any strict learning of an image style. This is also demonstrated in a traffic scene experiment.",
    "summary": "",
    "translation": "不确定性感知ControlNet：通过合成图像生成弥合领域差距",
    "relevance_score": 2,
    "reasoning": "该论文主要关注图像生成领域的领域适应问题，属于计算机视觉和AIGC范畴。虽然ControlNet技术本身在图像生成中有应用，但论文聚焦于合成图像生成和领域差距，与推荐系统、搜索或广告的核心技术缺乏直接关联，且属于明确的无关主题范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11344v1": {
    "title": "MMAP: A Multi-Magnification and Prototype-Aware Architecture for Predicting Spatial Gene Expression",
    "url": "https://www.alphaxiv.org/abs/2510.11344v1",
    "arxiv_id": "2510.11344v1",
    "authors": "Hai Dang Nguyen, Nguyen Dang Huy Pham, The Minh Duc Nguyen, Dac Thai Nguyen, Hang Thi Nguyen, Duong M. Nguyen",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 12:41:09",
    "ori_summary": "Spatial Transcriptomics (ST) enables the measurement of gene expression while preserving spatial information, offering critical insights into tissue architecture and disease pathology. Recent developments have explored the use of hematoxylin and eosin (H&E)-stained whole-slide images (WSIs) to predict transcriptome-wide gene expression profiles through deep neural networks. This task is commonly framed as a regression problem, where each input corresponds to a localized image patch extracted from the WSI. However, predicting spatial gene expression from histological images remains a challenging problem due to the significant modality gap between visual features and molecular signals. Recent studies have attempted to incorporate both local and global information into predictive models. Nevertheless, existing methods still suffer from two key limitations: (1) insufficient granularity in local feature extraction, and (2) inadequate coverage of global spatial context. In this work, we propose a novel framework, MMAP (Multi-MAgnification and Prototype-enhanced architecture), that addresses both challenges simultaneously. To enhance local feature granularity, MMAP leverages multi-magnification patch representations that capture fine-grained histological details. To improve global contextual understanding, it learns a set of latent prototype embeddings that serve as compact representations of slide-level information. Extensive experimental results demonstrate that MMAP consistently outperforms all existing state-of-the-art methods across multiple evaluation metrics, including Mean Absolute Error (MAE), Mean Squared Error (MSE), and Pearson Correlation Coefficient (PCC).",
    "summary": "",
    "translation": "MMAP：一种用于预测空间基因表达的多放大倍数与原型感知架构",
    "relevance_score": 1,
    "reasoning": "该论文专注于生物医学领域的空间基因表达预测，属于明确的无关主题（医学/生物学应用）。虽然提到了多模态架构，但其应用场景与推荐系统、搜索或广告完全无关，没有任何潜在的应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11341v1": {
    "title": "InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11341v1",
    "arxiv_id": "2510.11341v1",
    "authors": "Haomin Wang, Jinhui Yin, Qi Wei, Wenguang Zeng, Lixin Gu, Shenglong Ye, Zhangwei Gao, Yaohui Wang, Yanting Zhang, Yuanqi Li, Yanwen Guo, Wenhai Wang, Kai Chen, Yu Qiao, Hongjie Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 12:38:04",
    "ori_summary": "General SVG modeling remains challenging due to fragmented datasets, limited transferability of methods across tasks, and the difficulty of handling structural complexity. In response, we leverage the strong transfer and generalization capabilities of multimodal large language models (MLLMs) to achieve unified modeling for SVG understanding, editing, and generation. We present the InternSVG family, an integrated data-benchmark-model suite. At its core is SAgoge, the largest and most comprehensive multimodal dataset for SVG tasks, encompassing both static graphics and dynamic animations. It covers icons, long-sequence illustrations, scientific diagrams, and dynamic animations, supporting tasks of varied difficulty levels and providing deeper hierarchies with richer attributes compared to previous datasets. Based on this resource, we introduce SArena, a companion benchmark with comprehensive task definitions and standardized evaluation that aligns with the domains and difficulty spectrum covered by SAgoge. Building on these foundations, we propose InternSVG, a unified MLLM for SVG understanding, editing, and generation with SVG-specific special tokens, subword-based embedding initialization, and a two-stage training strategy that progresses from short static SVGs to long-sequence illustrations and complex animations. This unified formulation induces positive transfer and improves overall performance. Experiments on SArena and prior benchmark confirm that InternSVG achieves substantial gains and consistently outperforms leading open and proprietary counterparts.",
    "summary": "",
    "translation": "InternSVG：基于多模态大语言模型的统一可缩放矢量图形任务研究",
    "relevance_score": 2,
    "reasoning": "该论文专注于SVG（可缩放矢量图形）任务，属于计算机视觉和图形学领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然涉及多模态大语言模型，但其应用场景局限于图形处理，缺乏在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11340v1": {
    "title": "REACT3D: Recovering Articulations for Interactive Physical 3D Scenes",
    "url": "https://www.alphaxiv.org/abs/2510.11340v1",
    "arxiv_id": "2510.11340v1",
    "authors": "Zhao Huang, Boyang Sun, Alexandros Delitzas, Jiaqi Chen, Marc Pollefeys",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-13 12:37:59",
    "ori_summary": "Interactive 3D scenes are increasingly vital for embodied intelligence, yet existing datasets remain limited due to the labor-intensive process of annotating part segmentation, kinematic types, and motion trajectories. We present REACT3D, a scalable zero-shot framework that converts static 3D scenes into simulation-ready interactive replicas with consistent geometry, enabling direct use in diverse downstream tasks. Our contributions include: (i) openable-object detection and segmentation to extract candidate movable parts from static scenes, (ii) articulation estimation that infers joint types and motion parameters, (iii) hidden-geometry completion followed by interactive object assembly, and (iv) interactive scene integration in widely supported formats to ensure compatibility with standard simulation platforms. We achieve state-of-the-art performance on detection/segmentation and articulation metrics across diverse indoor scenes, demonstrating the effectiveness of our framework and providing a practical foundation for scalable interactive scene generation, thereby lowering the barrier to large-scale research on articulated scene understanding. Our project page is \\textit{\\hypersetup{urlcolor=black}\\href{https://react3d.github.io/}{react3d.github.io}}.",
    "summary": "",
    "translation": "REACT3D：为交互式物理3D场景恢复关节结构",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D视觉中的关节恢复和物理场景建模，属于纯粹的计算机视觉和图形学领域。虽然标题提到'交互式'，但内容与推荐系统、搜索或广告的核心技术没有任何直接关联，也不涉及LLM、Transformer架构或异构数据建模等关键技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11305v1": {
    "title": "Evaluating the effects of preprocessing, method selection, and hyperparameter tuning on SAR-based flood mapping and water depth estimation",
    "url": "https://www.alphaxiv.org/abs/2510.11305v1",
    "arxiv_id": "2510.11305v1",
    "authors": "Jean-Paul Travert, Cédric Goeury, Sébastien Boyaval, Vito Bacchi, Fabrice Zaoui",
    "categories": "cs.CV, physics.geo-ph",
    "pub_date": "2025-10-13 11:54:42",
    "ori_summary": "Flood mapping and water depth estimation from Synthetic Aperture Radar (SAR) imagery are crucial for calibrating and validating hydraulic models. This study uses SAR imagery to evaluate various preprocessing (especially speckle noise reduction), flood mapping, and water depth estimation methods. The impact of the choice of method at different steps and its hyperparameters is studied by considering an ensemble of preprocessed images, flood maps, and water depth fields. The evaluation is conducted for two flood events on the Garonne River (France) in 2019 and 2021, using hydrodynamic simulations and in-situ observations as reference data. Results show that the choice of speckle filter alters flood extent estimations with variations of several square kilometers. Furthermore, the selection and tuning of flood mapping methods also affect performance. While supervised methods outperformed unsupervised ones, tuned unsupervised approaches (such as local thresholding or change detection) can achieve comparable results. The compounded uncertainty from preprocessing and flood mapping steps also introduces high variability in the water depth field estimates. This study highlights the importance of considering the entire processing pipeline, encompassing preprocessing, flood mapping, and water depth estimation methods and their associated hyperparameters. Rather than relying on a single configuration, adopting an ensemble approach and accounting for methodological uncertainty should be privileged. For flood mapping, the method choice has the most influence. For water depth estimation, the most influential processing step was the flood map input resulting from the flood mapping step and the hyperparameters of the methods.",
    "summary": "",
    "translation": "评估预处理、方法选择和超参数调优对基于合成孔径雷达的洪水制图与水深估算的影响",
    "relevance_score": 1,
    "reasoning": "该论文专注于遥感领域的SAR（合成孔径雷达）洪水制图和水深估算，属于地球科学和环境监测应用。这与推荐系统、搜索、广告或相关LLM技术没有任何直接或间接关联，完全超出了关注范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11303v1": {
    "title": "sketch2symm: Symmetry-aware sketch-to-shape generation via semantic bridging",
    "url": "https://www.alphaxiv.org/abs/2510.11303v1",
    "arxiv_id": "2510.11303v1",
    "authors": "Yan Zhou, Mingji Li, Xiantao Zeng, Jie Lin, Yuexia Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 11:49:45",
    "ori_summary": "Sketch-based 3D reconstruction remains a challenging task due to the abstract and sparse nature of sketch inputs, which often lack sufficient semantic and geometric information. To address this, we propose Sketch2Symm, a two-stage generation method that produces geometrically consistent 3D shapes from sketches. Our approach introduces semantic bridging via sketch-to-image translation to enrich sparse sketch representations, and incorporates symmetry constraints as geometric priors to leverage the structural regularity commonly found in everyday objects. Experiments on mainstream sketch datasets demonstrate that our method achieves superior performance compared to existing sketch-based reconstruction methods in terms of Chamfer Distance, Earth Mover's Distance, and F-Score, verifying the effectiveness of the proposed semantic bridging and symmetry-aware design.",
    "summary": "",
    "translation": "sketch2symm：通过语义桥接实现对称感知的草图到形状生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的草图到3D形状生成，属于纯粹的视觉生成任务。虽然提到了对称性感知和语义桥接技术，但这些技术主要应用于图形学和3D建模领域，与推荐系统、搜索或广告的核心技术栈没有直接关联，也不涉及Transformer架构或LLM技术的应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11302v1": {
    "title": "When Does Supervised Training Pay Off? The Hidden Economics of Object Detection in the Era of Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11302v1",
    "arxiv_id": "2510.11302v1",
    "authors": "Samer Al-Hamadani",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-13 11:48:48",
    "ori_summary": "Object detection systems have traditionally relied on supervised learning with manually annotated bounding boxes, achieving high accuracy at the cost of substantial annotation investment. The emergence of Vision-Language Models (VLMs) offers an alternative paradigm enabling zero-shot detection through natural language queries, eliminating annotation requirements but operating with reduced accuracy. This paper presents the first comprehensive cost-effectiveness analysis comparing supervised detection (YOLO) with zero-shot VLM inference (Gemini Flash 2.5). Through systematic evaluation on 1,000 stratified COCO images and 200 diverse product images spanning consumer electronics and rare categories, combined with detailed Total Cost of Ownership modeling, we establish quantitative break-even thresholds governing architecture selection. Our findings reveal that supervised YOLO achieves 91.2% accuracy versus 68.5% for zero-shot Gemini on standard categories, representing a 22.7 percentage point advantage that costs $10,800 in annotation for 100-category systems. However, this advantage justifies investment only beyond 55 million inferences, equivalent to 151,000 images daily for one year. Zero-shot Gemini demonstrates 52.3% accuracy on diverse product categories (ranging from highly web-prevalent consumer electronics at 75-85% to rare specialized equipment at 25-40%) where supervised YOLO achieves 0% due to architectural constraints preventing detection of untrained classes. Cost per Correct Detection analysis reveals substantially lower per-detection costs for Gemini ($0.00050 vs $0.143) at 100,000 inferences despite accuracy deficits. We develop decision frameworks demonstrating that optimal architecture selection depends critically on deployment volume, category stability, budget constraints, and accuracy requirements rather than purely technical performance metrics.",
    "summary": "",
    "translation": "监督训练何时物有所值？视觉语言模型时代下目标检测的隐藏经济学",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的目标检测任务及其训练经济学，属于纯粹的视觉研究范畴。虽然提到了视觉语言模型，但核心焦点是视觉任务的经济分析，没有明确展示在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11296v1": {
    "title": "$Δ\\mathrm{Energy}$: Optimizing Energy Change During Vision-Language Alignment Improves both OOD Detection and OOD Generalization",
    "url": "https://www.alphaxiv.org/abs/2510.11296v1",
    "arxiv_id": "2510.11296v1",
    "authors": "Lin Zhu, Yifeng Yang, Xinbing Wang, Qinying Gu, Nanyang Ye",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-13 11:36:58",
    "ori_summary": "Recent approaches for vision-language models (VLMs) have shown remarkable success in achieving fast downstream adaptation. When applied to real-world downstream tasks, VLMs inevitably encounter both the in-distribution (ID) data and out-of-distribution (OOD) data. The OOD datasets often include both covariate shifts (e.g., known classes with changes in image styles) and semantic shifts (e.g., test-time unseen classes). This highlights the importance of improving VLMs' generalization ability to covariate-shifted OOD data, while effectively detecting open-set semantic-shifted OOD classes. In this paper, inspired by the substantial energy change observed in closed-set data when re-aligning vision-language modalities (specifically by directly reducing the maximum cosine similarity to a low value), we introduce a novel OOD score, named {\\Delta}Energy. {\\Delta}Energy significantly outperforms the vanilla energy-based OOD score and provides a more reliable approach for OOD detection. Furthermore, {\\Delta}Energy can simultaneously improve OOD generalization under covariate shifts, which is achieved by lower-bound maximization for {\\Delta}Energy (termed EBM). EBM is theoretically proven to not only enhance OOD detection but also yields a domain-consistent Hessian, which serves as a strong indicator for OOD generalization. Based on this finding, we developed a unified fine-tuning framework that allows for improving VLMs' robustness in both OOD generalization and OOD detection. Extensive experiments on challenging OOD detection and generalization benchmarks demonstrate the superiority of our method, outperforming recent approaches by 10% to 25% in AUROC.",
    "summary": "",
    "translation": "ΔEnergy：在视觉-语言对齐过程中优化能量变化可同时改善分布外检测与分布外泛化",
    "relevance_score": 3,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false,
    "relevance_reasoning": "该论文主要关注视觉-语言模型中的能量优化和OOD问题，属于VLM领域。虽然标题提到了VLM，但其核心焦点是能量变化优化和OOD检测/泛化，这与推荐系统、搜索或广告的直接应用关联较弱。作为使能技术，能量优化方法可能间接影响多模态推荐中的异常检测，但缺乏明确的RecSys/Search/Ads应用场景。"
  },
  "2510.11295v1": {
    "title": "Human Uncertainty-Aware Data Selection and Automatic Labeling in Visual Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.11295v1",
    "arxiv_id": "2510.11295v1",
    "authors": "Jian Lan, Zhicheng Liu, Udo Schlegel, Raoyuan Zhao, Yihong Liu, Hinrich Schütze, Michael A. Hedderich, Thomas Seidl",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 11:35:30",
    "ori_summary": "Large vision-language models (VLMs) achieve strong performance in Visual Question Answering but still rely heavily on supervised fine-tuning (SFT) with massive labeled datasets, which is costly due to human annotations. Crucially, real-world datasets often exhibit human uncertainty (HU) -- variation in human confidence across annotations -- but standard SFT simply optimizes toward the most frequent label, disregarding HU distributions. This leaves two open questions: How does HU affect SFT, and how can HU be effectively leveraged in training? In this work, we first conduct a systematic evaluation of VLMs across varying HU levels. We have two key findings: (i) surprisingly, high-HU samples contribute little or even degrade model performance, and (ii) naively training on the full dataset yields under-calibrated models that fail to capture HU distributions. Motivated by these findings, we introduce HaDola, a human uncertainty-aware data selection and automatic labeling framework. HaDola operates in four stages -- discriminate, self-annotate, error trigger, and training -- to iteratively identify harmful samples, prioritize informative ones, and bootstrap from a small seed set (5\\% of data). Our approach substantially reduces reliance on costly HU annotations and makes VLMs more accurate and better calibrated. Extensive experiments on VQAv2 and VizWiz datasets demonstrate that HaDola consistently matches or outperforms state-of-the-art baselines with less training data. Our work highlights the importance of explicitly modeling HU in SFT, suggesting that better utilization of HU is more effective than merely scaling up dataset size.",
    "summary": "",
    "translation": "视觉问答中的人类不确定性感知数据选择与自动标注",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉问答(VQA)领域的数据选择和标注问题，属于纯粹的视觉-语言多模态研究。虽然标题提到数据选择和自动标注技术，但这些方法主要针对VQA任务设计，没有明确展示在推荐系统、搜索或广告领域的应用潜力。论文的核心焦点是视觉理解而非文本推荐或排序任务。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11287v1": {
    "title": "EEMS: Edge-Prompt Enhanced Medical Image Segmentation Based on Learnable Gating Mechanism",
    "url": "https://www.alphaxiv.org/abs/2510.11287v1",
    "arxiv_id": "2510.11287v1",
    "authors": "Han Xia, Quanjun Li, Qian Li, Zimeng Li, Hongbin Ye, Yupeng Liu, Haolun Li, Xuhang Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 11:21:57",
    "ori_summary": "Medical image segmentation is vital for diagnosis, treatment planning, and disease monitoring but is challenged by complex factors like ambiguous edges and background noise. We introduce EEMS, a new model for segmentation, combining an Edge-Aware Enhancement Unit (EAEU) and a Multi-scale Prompt Generation Unit (MSPGU). EAEU enhances edge perception via multi-frequency feature extraction, accurately defining boundaries. MSPGU integrates high-level semantic and low-level spatial features using a prompt-guided approach, ensuring precise target localization. The Dual-Source Adaptive Gated Fusion Unit (DAGFU) merges edge features from EAEU with semantic features from MSPGU, enhancing segmentation accuracy and robustness. Tests on datasets like ISIC2018 confirm EEMS's superior performance and reliability as a clinical tool.",
    "summary": "",
    "translation": "EEMS：基于可学习门控机制的边缘提示增强医学图像分割",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分割领域，属于明确的无关主题（医学、生物医学应用）。虽然提到了可学习门控机制等技术概念，但其核心应用场景是医学图像处理，与推荐系统、搜索或广告领域没有任何直接或潜在的关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11268v1": {
    "title": "Exploring and Leveraging Class Vectors for Classifier Editing",
    "url": "https://www.alphaxiv.org/abs/2510.11268v1",
    "arxiv_id": "2510.11268v1",
    "authors": "Jaeik Kim, Jaeyoung Do",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 10:57:51",
    "ori_summary": "Image classifiers play a critical role in detecting diseases in medical imaging and identifying anomalies in manufacturing processes. However, their predefined behaviors after extensive training make post hoc model editing difficult, especially when it comes to forgetting specific classes or adapting to distribution shifts. Existing classifier editing methods either focus narrowly on correcting errors or incur extensive retraining costs, creating a bottleneck for flexible editing. Moreover, such editing has seen limited investigation in image classification. To overcome these challenges, we introduce Class Vectors, which capture class-specific representation adjustments during fine-tuning. Whereas task vectors encode task-level changes in weight space, Class Vectors disentangle each class's adaptation in the latent space. We show that Class Vectors capture each class's semantic shift and that classifier editing can be achieved either by steering latent features along these vectors or by mapping them into weight space to update the decision boundaries. We also demonstrate that the inherent linearity and orthogonality of Class Vectors support efficient, flexible, and high-level concept editing via simple class arithmetic. Finally, we validate their utility in applications such as unlearning, environmental adaptation, adversarial defense, and adversarial trigger optimization.",
    "summary": "",
    "translation": "探索并利用类别向量进行分类器编辑",
    "relevance_score": 2,
    "reasoning": "该论文主要关注分类器编辑和类别向量操作，这属于通用的机器学习技术，与推荐系统、搜索或广告的核心进展没有直接关联。虽然分类器编辑技术理论上可以应用于某些推荐系统的模型更新场景，但这种应用过于间接且非核心，不符合当前关注的四大重点领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11260v1": {
    "title": "A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images",
    "url": "https://www.alphaxiv.org/abs/2510.11260v1",
    "arxiv_id": "2510.11260v1",
    "authors": "Yuxuan Chen, Ruotong Yang, Zhengyang Zhang, Mehreen Ahmed, Yanming Wang",
    "categories": "cs.CV, cond-mat.mtrl-sci, cs.AI, physics.data-an",
    "pub_date": "2025-10-13 10:50:54",
    "ori_summary": "Microscopic characterizations, such as Scanning Electron Microscopy (SEM), are widely used in scientific research for visualizing and analyzing microstructures. Determining the scale bars is an important first step of accurate SEM analysis; however, currently, it mainly relies on manual operations, which is both time-consuming and prone to errors. To address this issue, we propose a multi-modal and automated scale bar detection and extraction framework that provides concurrent object detection, text detection and text recognition with a Large Language Model (LLM) agent. The proposed framework operates in four phases; i) Automatic Dataset Generation (Auto-DG) model to synthesize a diverse dataset of SEM images ensuring robust training and high generalizability of the model, ii) scale bar object detection, iii) information extraction using a hybrid Optical Character Recognition (OCR) system with DenseNet and Convolutional Recurrent Neural Network (CRNN) based algorithms, iv) an LLM agent to analyze and verify accuracy of the results. The proposed model demonstrates a strong performance in object detection and accurate localization with a precision of 100%, recall of 95.8%, and a mean Average Precision (mAP) of 99.2% at IoU=0.5 and 69.1% at IoU=0.5:0.95. The hybrid OCR system achieved 89% precision, 65% recall, and a 75% F1 score on the Auto-DG dataset, significantly outperforming several mainstream standalone engines, highlighting its reliability for scientific image analysis. The LLM is introduced as a reasoning engine as well as an intelligent assistant that suggests follow-up steps and verifies the results. This automated method powered by an LLM agent significantly enhances the efficiency and accuracy of scale bar detection and extraction in SEM images, providing a valuable tool for microscopic analysis and advancing the field of scientific imaging.",
    "summary": "",
    "translation": "基于大语言模型的扫描电子显微镜图像自动标尺检测与提取框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于扫描电子显微镜图像的标尺检测，属于医学/生物学领域的特定应用。虽然使用了LLM技术，但应用场景与推荐系统、搜索或广告领域完全无关，且没有展示任何在这些领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11259v1": {
    "title": "DTEA: Dynamic Topology Weaving and Instability-Driven Entropic Attenuation for Medical Image Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.11259v1",
    "arxiv_id": "2510.11259v1",
    "authors": "Weixuan Li, Quanjun Li, Guang Yu, Song Yang, Zimeng Li, Chi-Man Pun, Yupeng Liu, Xuhang Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 10:50:41",
    "ori_summary": "In medical image segmentation, skip connections are used to merge global context and reduce the semantic gap between encoder and decoder. Current methods often struggle with limited structural representation and insufficient contextual modeling, affecting generalization in complex clinical scenarios. We propose the DTEA model, featuring a new skip connection framework with the Semantic Topology Reconfiguration (STR) and Entropic Perturbation Gating (EPG) modules. STR reorganizes multi-scale semantic features into a dynamic hypergraph to better model cross-resolution anatomical dependencies, enhancing structural and semantic representation. EPG assesses channel stability after perturbation and filters high-entropy channels to emphasize clinically important regions and improve spatial attention. Extensive experiments on three benchmark datasets show our framework achieves superior segmentation accuracy and better generalization across various clinical settings. The code is available at \\href{https://github.com/LWX-Research/DTEA}{https://github.com/LWX-Research/DTEA}.",
    "summary": "",
    "translation": "DTEA：用于医学图像分割的动态拓扑编织与不稳定性驱动的熵衰减",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分割领域，属于明确的医学领域特定应用，与搜索、推荐、广告系统完全无关。论文标题中提到的动态拓扑编织和熵衰减技术是计算机视觉领域的专业方法，没有显示出在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11243v1": {
    "title": "Nepali Sign Language Characters Recognition: Dataset Development and Deep Learning Approaches",
    "url": "https://www.alphaxiv.org/abs/2510.11243v1",
    "arxiv_id": "2510.11243v1",
    "authors": "Birat Poudel, Satyam Ghimire, Sijan Bhattarai, Saurav Bhandari, Suramya Sharma Dahal",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 10:29:08",
    "ori_summary": "Sign languages serve as essential communication systems for individuals with hearing and speech impairments. However, digital linguistic dataset resources for underrepresented sign languages, such as Nepali Sign Language (NSL), remain scarce. This study introduces the first benchmark dataset for NSL, consisting of 36 gesture classes with 1,500 samples per class, designed to capture the structural and visual features of the language. To evaluate recognition performance, we fine-tuned MobileNetV2 and ResNet50 architectures on the dataset, achieving classification accuracies of 90.45% and 88.78%, respectively. These findings demonstrate the effectiveness of convolutional neural networks in sign recognition tasks, particularly within low-resource settings. To the best of our knowledge, this work represents the first systematic effort to construct a benchmark dataset and assess deep learning approaches for NSL recognition, highlighting the potential of transfer learning and fine-tuning for advancing research in underexplored sign languages.",
    "summary": "",
    "translation": "尼泊尔手语字符识别：数据集开发与深度学习方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于特定领域的手语识别，属于计算机视觉应用，与推荐系统、搜索或广告的核心技术领域无关。论文内容涉及数据集开发和深度学习在特定语言识别中的应用，没有显示出与推荐、搜索或广告系统的潜在关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11232v1": {
    "title": "LightPneumoNet: Lightweight Pneumonia Classifier",
    "url": "https://www.alphaxiv.org/abs/2510.11232v1",
    "arxiv_id": "2510.11232v1",
    "authors": "Neilansh Chauhan, Piyush Kumar Gupta, Faraz Doja",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-13 10:14:17",
    "ori_summary": "Effective pneumonia diagnosis is often challenged by the difficulty of deploying large, computationally expensive deep learning models in resource-limited settings. This study introduces LightPneumoNet, an efficient, lightweight convolutional neural network (CNN) built from scratch to provide an accessible and accurate diagnostic solution for pneumonia detection from chest X-rays. Our model was trained on a public dataset of 5,856 chest X-ray images. Preprocessing included image resizing to 224x224, grayscale conversion, and pixel normalization, with data augmentation (rotation, zoom, shear) to prevent overfitting. The custom architecture features four blocks of stacked convolutional layers and contains only 388,082 trainable parameters, resulting in a minimal 1.48 MB memory footprint. On the independent test set, our model delivered exceptional performance, achieving an overall accuracy of 0.942, precision of 0.92, and an F1-Score of 0.96. Critically, it obtained a sensitivity (recall) of 0.99, demonstrating a near-perfect ability to identify true pneumonia cases and minimize clinically significant false negatives. Notably, LightPneumoNet achieves this high recall on the same dataset where existing approaches typically require significantly heavier architectures or fail to reach comparable sensitivity levels. The model's efficiency enables deployment on low-cost hardware, making advanced computer-aided diagnosis accessible in underserved clinics and serving as a reliable second-opinion tool to improve patient outcomes.",
    "summary": "",
    "translation": "LightPneumoNet：轻量级肺炎分类器",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学领域的肺炎分类应用，属于明确的无关主题范畴。虽然提到了轻量级网络设计，但其核心应用场景（医疗诊断）与推荐系统、搜索或广告领域没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11223v1": {
    "title": "Investigating Identity Signals in Conversational Facial Dynamics via Disentangled Expression Features",
    "url": "https://www.alphaxiv.org/abs/2510.11223v1",
    "arxiv_id": "2510.11223v1",
    "authors": "Masoumeh Chapariniya, Pierre Vuillecard, Jean-Marc Odobez, Volker Dellwo, Teodora Vukovic",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 10:06:25",
    "ori_summary": "This work investigates whether individuals can be identified solely through the pure dynamical components of their facial expressions, independent of static facial appearance. We leverage the FLAME 3D morphable model to achieve explicit disentanglement between facial shape and expression dynamics, extracting frame-by-frame parameters from conversational videos while retaining only expression and jaw coefficients. On the CANDOR dataset of 1,429 speakers in naturalistic conversations, our Conformer model with supervised contrastive learning achieves 61.14\\%accuracy on 1,429-way classification -- 458 times above chance -- demonstrating that facial dynamics carry strong identity signatures. We introduce a drift-to-noise ratio (DNR) that quantifies the reliability of shape expression separation by measuring across-session shape changes relative to within-session variability. DNR strongly negatively correlates with recognition performance, confirming that unstable shape estimation compromises dynamic identification. Our findings reveal person-specific signatures in conversational facial dynamics, with implications for social perception and clinical assessment.",
    "summary": "",
    "translation": "基于解耦表情特征探究对话面部动态中的身份信号",
    "relevance_score": 2,
    "reasoning": "该论文主要研究面部表情动态和身份识别，属于计算机视觉和生物识别领域。虽然涉及对话场景，但其核心技术与推荐系统、搜索或广告的排名优化没有直接关联。面部动态分析可能在人机交互中有应用，但不符合当前关注的LLM、Transformer架构或推荐系统核心技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11204v1": {
    "title": "Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos",
    "url": "https://www.alphaxiv.org/abs/2510.11204v1",
    "arxiv_id": "2510.11204v1",
    "authors": "Rohit Gupta, Anirban Roy, Claire Christensen, Sujeong Kim, Sarah Gerard, Madeline Cincebeaux, Ajay Divakaran, Todd Grindal, Mubarak Shah",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 09:36:26",
    "ori_summary": "The recent growth in the consumption of online media by children during early childhood necessitates data-driven tools enabling educators to filter out appropriate educational content for young learners. This paper presents an approach for detecting educational content in online videos. We focus on two widely used educational content classes: literacy and math. For each class, we choose prominent codes (sub-classes) based on the Common Core Standards. For example, literacy codes include `letter names', `letter sounds', and math codes include `counting', `sorting'. We pose this as a fine-grained multilabel classification problem as videos can contain multiple types of educational content and the content classes can get visually similar (e.g., `letter names' vs `letter sounds'). We propose a novel class prototypes based supervised contrastive learning approach that can handle fine-grained samples associated with multiple labels. We learn a class prototype for each class and a loss function is employed to minimize the distances between a class prototype and the samples from the class. Similarly, distances between a class prototype and the samples from other classes are maximized. As the alignment between visual and audio cues are crucial for effective comprehension, we consider a multimodal transformer network to capture the interaction between visual and audio cues in videos while learning the embedding for videos. For evaluation, we present a dataset, APPROVE, employing educational videos from YouTube labeled with fine-grained education classes by education researchers. APPROVE consists of 193 hours of expert-annotated videos with 19 classes. The proposed approach outperforms strong baselines on APPROVE and other benchmarks such as Youtube-8M, and COIN. The dataset is available at https://github.com/rohit-gupta/MMContrast/tree/main/APPROVE",
    "summary": "",
    "translation": "基于类别原型的对比学习用于多标签和细粒度教育视频分类",
    "relevance_score": 2,
    "reasoning": "该论文主要关注教育视频分类这一特定领域应用，与推荐系统、搜索或广告的核心技术进展无关。虽然对比学习是一种通用技术，但论文的应用场景过于专业化，缺乏在推荐/搜索/广告领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11190v1": {
    "title": "FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11190v1",
    "arxiv_id": "2510.11190v1",
    "authors": "Shengming Yuan, Xinyu Lyu, Shuailong Wang, Beitao Chen, Jingkuan Song, Lianli Gao",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 09:22:12",
    "ori_summary": "Multimodal large language models (MLLMs) face an inherent trade-off between faithfulness and creativity, as different tasks require varying degrees of associative reasoning. However, existing methods lack the flexibility to modulate this reasoning strength, limiting MLLMs' adaptability across factual and creative scenarios. To bridge this gap, we propose equipping MLLMs with mechanisms that enable flexible control over associative reasoning. We begin by investigating the internal mechanisms underlying associative behavior in MLLMs and find that: (1) middle layers play a pivotal role in shaping model's associative tendencies, (2) modifying representations in these layers effectively regulates associative reasoning strength, and (3) hallucinations can be exploited to derive steering vectors that guide this modulation. Building on these findings, we introduce Flexible Association Control (FlexAC), a lightweight and training-free framework for modulating associative behavior in MLLMs. FlexAC first induces hallucination-guided intermediate representations to encode associative directions. Then, it selects high-association instances to construct effective associative steering vectors, whose strengths are adaptively calibrated to balance creative guidance with output stability. Finally, recognizing the multi-dimensional nature of associative reasoning, FlexAC incorporates task-specific associative vectors derived from a forward pass on a few target-domain samples, enabling models to follow diverse associative directions and better adapt to creative tasks. Notably, our method achieves up to a 5.8x improvement in creativity on Creation-MMBench and a 29% reduction in hallucination rate on CHAIR, surpassing existing baselines and demonstrating its effectiveness in enabling flexible control over associative reasoning in MLLMs. Our code is available at https://github.com/ylhz/FlexAC.",
    "summary": "",
    "translation": "FlexAC：迈向多模态大语言模型中关联推理的灵活控制",
    "relevance_score": 8,
    "reasoning": "该论文聚焦于多模态大语言模型中的关联推理控制，属于'VLM Analogy for Heterogeneous Data'范畴。在推荐系统和搜索场景中，灵活控制跨模态关联推理对于理解用户意图、处理异构特征（如用户行为序列与上下文信息）至关重要，能够显著提升多模态推荐的准确性和可解释性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.11183v1": {
    "title": "Saudi Sign Language Translation Using T5",
    "url": "https://www.alphaxiv.org/abs/2510.11183v1",
    "arxiv_id": "2510.11183v1",
    "authors": "Ali Alhejab, Tomas Zelezny, Lamya Alkanhal, Ivan Gruber, Yazeed Alharbi, Jakub Straka, Vaclav Javorek, Marek Hruz, Badriah Alkalifah, Ahmed Ali",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 09:18:34",
    "ori_summary": "This paper explores the application of T5 models for Saudi Sign Language (SSL) translation using a novel dataset. The SSL dataset includes three challenging testing protocols, enabling comprehensive evaluation across different scenarios. Additionally, it captures unique SSL characteristics, such as face coverings, which pose challenges for sign recognition and translation. In our experiments, we investigate the impact of pre-training on American Sign Language (ASL) data by comparing T5 models pre-trained on the YouTubeASL dataset with models trained directly on the SSL dataset. Experimental results demonstrate that pre-training on YouTubeASL significantly improves models' performance (roughly $3\\times$ in BLEU-4), indicating cross-linguistic transferability in sign language models. Our findings highlight the benefits of leveraging large-scale ASL data to improve SSL translation and provide insights into the development of more effective sign language translation systems. Our code is publicly available at our GitHub repository.",
    "summary": "",
    "translation": "使用T5模型进行沙特手语翻译",
    "relevance_score": 1,
    "reasoning": "该论文专注于特定领域的手语翻译任务，属于纯粹的自然语言处理应用，与推荐系统、搜索或广告的核心技术领域没有直接关联。手语翻译作为专门的NLP任务，不涉及推荐算法、搜索排序、广告投放或Transformer架构的效率改进等关键技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11182v1": {
    "title": "Generalisation of automatic tumour segmentation in histopathological whole-slide images across multiple cancer types",
    "url": "https://www.alphaxiv.org/abs/2510.11182v1",
    "arxiv_id": "2510.11182v1",
    "authors": "Ole-Johan Skrede, Manohar Pradhan, Maria Xepapadakis Isaksen, Tarjei Sveinsgjerd Hveem, Ljiljana Vlatkovic, Arild Nesbakken, Kristina Lindemann, Gunnar B Kristensen, Jenneke Kasius, Alain G Zeimet, Odd Terje Brustugun, Lill-Tove Rasmussen Busund, Elin H Richardsen, Erik Skaaheim Haug, Bjørn Brennhovd, Emma Rewcastle, Melinda Lillesand, Vebjørn Kvikstad, Emiel Janssen, David J Kerr, Knut Liestøl, Fritz Albregtsen, Andreas Kleppe",
    "categories": "eess.IV, cs.AI, cs.CV",
    "pub_date": "2025-10-13 09:18:15",
    "ori_summary": "Deep learning is expected to aid pathologists by automating tasks such as tumour segmentation. We aimed to develop one universal tumour segmentation model for histopathological images and examine its performance in different cancer types. The model was developed using over 20 000 whole-slide images from over 4 000 patients with colorectal, endometrial, lung, or prostate carcinoma. Performance was validated in pre-planned analyses on external cohorts with over 3 000 patients across six cancer types. Exploratory analyses included over 1 500 additional patients from The Cancer Genome Atlas. Average Dice coefficient was over 80% in all validation cohorts with en bloc resection specimens and in The Cancer Genome Atlas cohorts. No loss of performance was observed when comparing the universal model with models specialised on single cancer types. In conclusion, extensive and rigorous evaluations demonstrate that generic tumour segmentation by a single model is possible across cancer types, patient populations, sample preparations, and slide scanners.",
    "summary": "",
    "translation": "跨多种癌症类型的组织病理学全切片图像中自动肿瘤分割的泛化性研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分割领域，特别是组织病理学中的肿瘤分割，这属于医学和生物学的具体应用领域。根据用户明确的排除标准，医学、生物学等特定领域应用属于不相关主题，且该研究没有显示出与推荐系统、搜索或广告领域的潜在联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11178v1": {
    "title": "BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11178v1",
    "arxiv_id": "2510.11178v1",
    "authors": "Bryan Chen Zhengyu Tan, Zheng Weihua, Zhengyuan Liu, Nancy F. Chen, Hwaran Lee, Kenny Tsu Wei Choo, Roy Ka-Wei Lee",
    "categories": "cs.CV, cs.CY",
    "pub_date": "2025-10-13 09:10:05",
    "ori_summary": "As vision-language models (VLMs) are deployed globally, their ability to understand culturally situated knowledge becomes essential. Yet, existing evaluations largely assess static recall or isolated visual grounding, leaving unanswered whether VLMs possess robust and transferable cultural understanding. We introduce BLEnD-Vis, a multimodal, multicultural benchmark designed to evaluate the robustness of everyday cultural knowledge in VLMs across linguistic rephrasings and visual modalities. Building on the BLEnD dataset, BLEnD-Vis constructs 313 culturally grounded question templates spanning 16 regions and generates three aligned multiple-choice formats: (i) a text-only baseline querying from Region $\\to$ Entity, (ii) an inverted text-only variant (Entity $\\to$ Region), and (iii) a VQA-style version of (ii) with generated images. The resulting benchmark comprises 4,916 images and over 21,000 multiple-choice question (MCQ) instances, validated through human annotation. BLEnD-Vis reveals significant fragility in current VLM cultural knowledge; models exhibit performance drops under linguistic rephrasing and, whilst visual cues often aid performance, low cross-modal consistency highlights challenges in robustly integrating textual and visual understanding, particularly for lower-resource regions. BLEnD-Vis thus provides a crucial testbed for systematically analysing cultural robustness and multimodal grounding, exposing limitations and guiding the development of more culturally competent VLMs.",
    "summary": "",
    "translation": "BLEnD-Vis：视觉语言模型中多模态文化理解的基准测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型的文化理解基准测试，属于纯粹的VLM评估范畴，与推荐系统、搜索或广告的核心技术进展无关。虽然涉及多模态，但缺乏对异构数据统一建模的直接应用潜力，且评估基准本身属于被排除的纯NLP中心话题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11176v1": {
    "title": "G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.11176v1",
    "arxiv_id": "2510.11176v1",
    "authors": "Yesung Cho, Sungmin Lee, Geongyu Lee, Minkyung Lee, Jongbae Park, Dongmyung Shin",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 09:08:59",
    "ori_summary": "Recent studies in pathology foundation models have shown that scaling training data, diversifying cancer types, and increasing model size consistently improve their performance. However, giga-scale foundation models, which are trained on hundreds of thousands of slides covering tens of cancer types and contain billions of parameters, pose significant challenges for practical use due to their tremendous computational costs in both development and deployment. In this work, we present a novel strategy, named the G2L framework, to increase the performance of large-scale foundation models, which consist of only $15\\%$ of the parameters of giga-scale models, to a comparable performance level of giga-scale models in cancer-specific tasks. Our approach applies knowledge distillation, transferring the capabilities of a giga-scale model to a large-scale model, using just 1K pathology slides of a target cancer (e.g., breast, prostate, etc.). The resulting distilled model not only outperformed state-of-the-art models of the same size (i.e., large-scale) across several benchmarks but also, interestingly, surpassed the giga-scale teacher and huge-scale models in some benchmarks. In addition, the distilled model exhibited a higher robustness index, indicating improved resilience to image variations originating from multiple institutions. These findings suggest that the proposed distillation approach for a large-scale model is a data- and parameter-efficient way to achieve giga-scale-level performance for cancer-specific applications without prohibitive computational burden.",
    "summary": "",
    "translation": "G2L：通过知识蒸馏从千亿级到癌症特异性大规模病理学基础模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学病理学领域的癌症特异性基础模型开发，这属于明确的无关主题（医学/生物学领域特定应用）。尽管提到了知识蒸馏技术，但其核心应用场景与推荐系统、搜索或广告领域完全无关，没有任何潜在的应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11175v1": {
    "title": "Reliable Cross-modal Alignment via Prototype Iterative Construction",
    "url": "https://www.alphaxiv.org/abs/2510.11175v1",
    "arxiv_id": "2510.11175v1",
    "authors": "Xiang Ma, Litian Xu, Lexin Fang, Caiming Zhang, Lizhen Cui",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 09:08:27",
    "ori_summary": "Cross-modal alignment is an important multi-modal task, aiming to bridge the semantic gap between different modalities. The most reliable fundamention for achieving this objective lies in the semantic consistency between matched pairs. Conventional methods implicitly assume embeddings contain solely semantic information, ignoring the impact of non-semantic information during alignment, which inevitably leads to information bias or even loss. These non-semantic information primarily manifest as stylistic variations in the data, which we formally define as style information. An intuitive approach is to separate style from semantics, aligning only the semantic information. However, most existing methods distinguish them based on feature columns, which cannot represent the complex coupling relationship between semantic and style information. In this paper, we propose PICO, a novel framework for suppressing style interference during embedding interaction. Specifically, we quantify the probability of each feature column representing semantic information, and regard it as the weight during the embedding interaction. To ensure the reliability of the semantic probability, we propose a prototype iterative construction method. The key operation of this method is a performance feedback-based weighting function, and we have theoretically proven that the function can assign higher weight to prototypes that bring higher performance improvements. Extensive experiments on various benchmarks and model backbones demonstrate the superiority of PICO, outperforming state-of-the-art methods by 5.2\\%-14.1\\%.",
    "summary": "",
    "translation": "基于原型迭代构建的可靠跨模态对齐",
    "relevance_score": 8,
    "reasoning": "该论文涉及跨模态对齐技术，这与'VLM Analogy for Heterogeneous Data'焦点高度相关，其中可以将用户序列和上下文特征视为不同模态进行统一建模。原型迭代构建方法在搜索和推荐系统中具有直接应用潜力，可用于对齐用户行为、物品属性和上下文信息等多模态数据，提升跨模态检索和推荐性能。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.11173v1": {
    "title": "CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.11173v1",
    "arxiv_id": "2510.11173v1",
    "authors": "Zhenyu Lu, Liupeng Li, Jinpeng Wang, Yan Feng, Bin Chen, Ke Chen, Yaowei Wang",
    "categories": "cs.CV, cs.MM",
    "pub_date": "2025-10-13 09:07:54",
    "ori_summary": "Existing works on reasoning segmentation either connect hidden features from a language model directly to a mask decoder or represent positions in text, which limits interpretability and semantic detail. To solve this, we present CoPRS, a Multi-modal Chain-of-Thought (MCoT)-based positional perception model that bridges language reasoning to segmentation through a differentiable and interpretable positional prior instantiated as a heatmap. By making the reasoning process clear via MCoT and expressing it as a dense, differentiable heatmap, this interface enhances interpretability and diagnostic analysis and yields more concentrated evidence on the target. A learnable concentration token aggregates features of the image and reasoning text to generate this positional prior, which is decoded to precise masks through a lightweight decoder, providing a direct connection between reasoning and segmentation. Across the RefCOCO series and ReasonSeg, CoPRS matches or surpasses the best reported metrics on each standard split under comparable protocols, with performance at or above prior state of the art across both validation and test partitions. Extensive experiments reveal that the quality of the heatmap strongly influences the resulting mask quality, supporting a consistent association between the reasoning output and downstream mask generation. Collectively, these findings support the utility of this paradigm in bridging reasoning and segmentation and show advantages in concentration driven by reasoning and predicting masks more precisely. Code, checkpoints and logs are released at https://github.com/ZhenyuLU-Heliodore/CoPRS.git.",
    "summary": "",
    "translation": "CoPRS：从思维链中学习位置先验用于推理分割",
    "relevance_score": 2,
    "reasoning": "该论文主要关注推理分割任务，这是计算机视觉领域的一个特定应用。虽然提到了思维链（Chain-of-Thought）这一LLM相关概念，但其核心应用是视觉分割而非推荐系统、搜索或广告领域。该技术缺乏明确的路径应用于RecSys/Search/Ads的核心问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11171v1": {
    "title": "Multiview Manifold Evidential Fusion for PolSAR Image Classification",
    "url": "https://www.alphaxiv.org/abs/2510.11171v1",
    "arxiv_id": "2510.11171v1",
    "authors": "Junfei Shi, Haojia Zhang, Haiyan Jin, Junhuai Li, Xiaogang Song, Yuanfan Guo, Haonan Su, Weisi Lin",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 09:05:51",
    "ori_summary": "Polarimetric Synthetic Aperture Radar (PolSAR) covariance matrices and their extracted multi-features - such as scattering angle, entropy, texture, and boundary descriptors - provide complementary and physically interpretable information for image classification. Traditional fusion strategies typically concatenate these features or employ deep learning networks to combine them. However, the covariance matrices and multi-features, as two complementary views, lie on different manifolds with distinct geometric structures. Existing fusion methods also overlook the varying importance of different views and ignore uncertainty, often leading to unreliable predictions. To address these issues, we propose a Multiview Manifold Evidential Fusion (MMEFnet) method to effectively fuse these two views. It gives a new framework to integrate PolSAR manifold learning and evidence fusion into a unified architecture. Specifically, covariance matrices are represented on the Hermitian Positive Definite (HPD) manifold, while multi-features are modeled on the Grassmann manifold. Two different kernel metric learning networks are constructed to learn their manifold representations. Subsequently, a trusted multiview evidence fusion, replacing the conventional softmax classifier, estimates belief mass and quantifies the uncertainty of each view from the learned deep features. Finally, a Dempster-Shafer theory-based fusion strategy combines evidence, enabling a more reliable and interpretable classification. Extensive experiments on three real-world PolSAR datasets demonstrate that the proposed method consistently outperforms existing approaches in accuracy, robustness, and interpretability.",
    "summary": "",
    "translation": "基于多视角流形证据融合的极化SAR图像分类",
    "relevance_score": 1,
    "reasoning": "该论文专注于极化SAR图像的分类任务，属于遥感图像处理的特定领域应用。虽然涉及多视角融合和流形学习技术，但这些方法在遥感图像分析中的直接应用与推荐系统、搜索或广告的核心技术领域没有明显关联，也不涉及LLM、Transformer架构或异构数据建模的相关进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11142v1": {
    "title": "Validation of an Artificial Intelligence Tool for the Detection of Sperm DNA Fragmentation Using the TUNEL In Situ Hybridization Assay",
    "url": "https://www.alphaxiv.org/abs/2510.11142v1",
    "arxiv_id": "2510.11142v1",
    "authors": "Byron Alexander Jacobs, Aqeel Morris, Ifthakaar Shaik, Frando Lin",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 08:32:11",
    "ori_summary": "Sperm DNA fragmentation (SDF) is a critical parameter in male fertility assessment that conventional semen analysis fails to evaluate. This study presents the validation of a novel artificial intelligence (AI) tool designed to detect SDF through digital analysis of phase contrast microscopy images, using the terminal deoxynucleotidyl transferase dUTP nick end labeling (TUNEL) assay as the gold standard reference. Utilising the established link between sperm morphology and DNA integrity, the present work proposes a morphology assisted ensemble AI model that combines image processing techniques with state-of-the-art transformer based machine learning models (GC-ViT) for the prediction of DNA fragmentation in sperm from phase contrast images. The ensemble model is benchmarked against a pure transformer `vision' model as well as a `morphology-only` model. Promising results show the proposed framework is able to achieve sensitivity of 60\\% and specificity of 75\\%. This non-destructive methodology represents a significant advancement in reproductive medicine by enabling real-time sperm selection based on DNA integrity for clinical diagnostic and therapeutic applications.",
    "summary": "",
    "translation": "使用TUNEL原位杂交检测验证用于精子DNA碎片检测的人工智能工具",
    "relevance_score": 1,
    "reasoning": "该论文涉及医学/生物学领域的精子DNA碎片检测应用，属于明确的无关主题。虽然提到了人工智能工具，但其应用场景完全在生殖医学领域，与推荐系统、搜索或广告没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11129v1": {
    "title": "video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via Memory",
    "url": "https://www.alphaxiv.org/abs/2510.11129v1",
    "arxiv_id": "2510.11129v1",
    "authors": "Guangzhi Sun, Yixuan Li, Xiaodong Wu, Yudong Yang, Wei Li, Zejun Ma, Chao Zhang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 08:20:15",
    "ori_summary": "Continuous, high-frame-rate, high-resolution processing of long video streams is critical for future AI agents, yet current video-understanding LLMs struggle to scale. Offline, fixed-frame-number methods require the stream length to adapt frame rates; streaming methods constrain memory by merging or discarding tokens, losing information. We propose video-SALMONN S, a streaming audio-visual LLM that, to our knowledge, is the first to process 3-hour videos at 1 FPS and 360p resolution under a fixed memory budget. Our model introduces (i) a test-time-training (TTT) memory module that continually updates token representations to capture long-range dependencies by replacing token merging, and (ii) a prompt-dependent memory reader that selectively retrieves context-relevant content from fixed-size memory. The TTT module is optimised with a Hessian-free conjugate-gradient procedure (TTT_HF) for efficient adaptation. On long-video benchmarks (Video-MME, LVBench, VideoEvalPro), video-SALMONN S sustains high-quality understanding on multi-hour videos with 10k frames and 1M tokens. Our 8B-parameter model achieves 74.2% overall and 67.8% on the Video-MME long split, outperforming both offline and streaming baselines.",
    "summary": "",
    "translation": "video-SALMONN S：通过记忆机制超越长度限制的流式音频-视觉大语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注音频-视觉模态的流式处理，属于多模态LLM领域，但核心应用场景偏向视频理解和音频处理。虽然提到了长度限制突破和记忆机制，但这些技术主要服务于视频/音频内容理解，与推荐系统、搜索或广告的异构数据处理需求关联较弱，缺乏明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11128v1": {
    "title": "Lightweight Facial Landmark Detection in Thermal Images via Multi-Level Cross-Modal Knowledge Transfer",
    "url": "https://www.alphaxiv.org/abs/2510.11128v1",
    "arxiv_id": "2510.11128v1",
    "authors": "Qiyi Tong, Olivia Nocentini, Marta Lagomarsino, Kuanqi Cai, Marta Lorenzini, Arash Ajoudani",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-13 08:19:56",
    "ori_summary": "Facial Landmark Detection (FLD) in thermal imagery is critical for applications in challenging lighting conditions, but it is hampered by the lack of rich visual cues. Conventional cross-modal solutions, like feature fusion or image translation from RGB data, are often computationally expensive or introduce structural artifacts, limiting their practical deployment. To address this, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a novel framework that decouples high-fidelity RGB-to-thermal knowledge transfer from model compression to create both accurate and efficient thermal FLD models. A central challenge during knowledge transfer is the profound modality gap between RGB and thermal data, where traditional unidirectional distillation fails to enforce semantic consistency across disparate feature spaces. To overcome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a bidirectional mechanism designed specifically for this task. DIKD establishes a connection between modalities: it not only guides the thermal student with rich RGB features but also validates the student's learned representations by feeding them back into the frozen teacher's prediction head. This closed-loop supervision forces the student to learn modality-invariant features that are semantically aligned with the teacher, ensuring a robust and profound knowledge transfer. Experiments show that our approach sets a new state-of-the-art on public thermal FLD benchmarks, notably outperforming previous methods while drastically reducing computational overhead.",
    "summary": "",
    "translation": "基于多层级跨模态知识迁移的热成像图像轻量化面部关键点检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于热成像图像中的面部关键点检测，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然涉及跨模态知识迁移，但其应用场景（热成像面部检测）在RecSys/Search/Ads领域缺乏明确的实用价值或应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11117v1": {
    "title": "Demystifying Numerosity in Diffusion Models -- Limitations and Remedies",
    "url": "https://www.alphaxiv.org/abs/2510.11117v1",
    "arxiv_id": "2510.11117v1",
    "authors": "Yaqi Zhao, Xiaochen Wang, Li Dong, Wentao Zhang, Yuhui Yuan",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 08:07:24",
    "ori_summary": "Numerosity remains a challenge for state-of-the-art text-to-image generation models like FLUX and GPT-4o, which often fail to accurately follow counting instructions in text prompts. In this paper, we aim to study a fundamental yet often overlooked question: Can diffusion models inherently generate the correct number of objects specified by a textual prompt simply by scaling up the dataset and model size? To enable rigorous and reproducible evaluation, we construct a clean synthetic numerosity benchmark comprising two complementary datasets: GrayCount250 for controlled scaling studies, and NaturalCount6 featuring complex naturalistic scenes. Second, we empirically show that the scaling hypothesis does not hold: larger models and datasets alone fail to improve counting accuracy on our benchmark. Our analysis identifies a key reason: diffusion models tend to rely heavily on the noise initialization rather than the explicit numerosity specified in the prompt. We observe that noise priors exhibit biases toward specific object counts. In addition, we propose an effective strategy for controlling numerosity by injecting count-aware layout information into the noise prior. Our method achieves significant gains, improving accuracy on GrayCount250 from 20.0\\% to 85.3\\% and on NaturalCount6 from 74.8\\% to 86.3\\%, demonstrating effective generalization across settings.",
    "summary": "",
    "translation": "揭秘扩散模型中的数值理解能力——局限性与补救措施",
    "relevance_score": 1,
    "reasoning": "该论文专注于扩散模型的数值理解能力，这属于纯粹的生成式AI领域，与推荐系统、搜索或广告的核心技术无关。扩散模型主要用于内容生成任务，而我的关注点在于排名、检索和用户行为建模等核心领域技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11115v1": {
    "title": "Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning",
    "url": "https://www.alphaxiv.org/abs/2510.11115v1",
    "arxiv_id": "2510.11115v1",
    "authors": "Hao Tang, Shengfeng He, Jing Qin",
    "categories": "cs.CV, cs.MM",
    "pub_date": "2025-10-13 08:06:23",
    "ori_summary": "Few-shot learning (FSL) addresses the challenge of classifying novel classes with limited training samples. While some methods leverage semantic knowledge from smaller-scale models to mitigate data scarcity, these approaches often introduce noise and bias due to the data's inherent simplicity. In this paper, we propose a novel framework, Synergistic Knowledge Transfer (SynTrans), which effectively transfers diverse and complementary knowledge from large multimodal models to empower the off-the-shelf few-shot learner. Specifically, SynTrans employs CLIP as a robust teacher and uses a few-shot vision encoder as a weak student, distilling semantic-aligned visual knowledge via an unsupervised proxy task. Subsequently, a training-free synergistic knowledge mining module facilitates collaboration among large multimodal models to extract high-quality semantic knowledge. Building upon this, a visual-semantic bridging module enables bi-directional knowledge transfer between visual and semantic spaces, transforming explicit visual and implicit semantic knowledge into category-specific classifier weights. Finally, SynTrans introduces a visual weight generator and a semantic weight reconstructor to adaptively construct optimal multimodal FSL classifiers. Experimental results on four FSL datasets demonstrate that SynTrans, even when paired with a simple few-shot vision encoder, significantly outperforms current state-of-the-art methods.",
    "summary": "",
    "translation": "连接巨人：大型多模态模型的协同知识迁移用于少样本学习",
    "relevance_score": 7,
    "reasoning": "该论文涉及大型多模态模型的知识迁移和少样本学习，这属于'赋能LLM技术'范畴。在推荐系统、搜索和广告中，这种技术可以用于快速适应新用户、新物品或新场景，通过少量样本实现个性化模型调整，解决冷启动和长尾问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.11112v1": {
    "title": "Multimodal Disease Progression Modeling via Spatiotemporal Disentanglement and Multiscale Alignment",
    "url": "https://www.alphaxiv.org/abs/2510.11112v1",
    "arxiv_id": "2510.11112v1",
    "authors": "Chen Liu, Wenfang Yao, Kejing Yin, William K. Cheung, Jing Qin",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 08:02:36",
    "ori_summary": "Longitudinal multimodal data, including electronic health records (EHR) and sequential chest X-rays (CXRs), is critical for modeling disease progression, yet remains underutilized due to two key challenges: (1) redundancy in consecutive CXR sequences, where static anatomical regions dominate over clinically-meaningful dynamics, and (2) temporal misalignment between sparse, irregular imaging and continuous EHR data. We introduce $\\texttt{DiPro}$, a novel framework that addresses these challenges through region-aware disentanglement and multi-timescale alignment. First, we disentangle static (anatomy) and dynamic (pathology progression) features in sequential CXRs, prioritizing disease-relevant changes. Second, we hierarchically align these static and dynamic CXR features with asynchronous EHR data via local (pairwise interval-level) and global (full-sequence) synchronization to model coherent progression pathways. Extensive experiments on the MIMIC dataset demonstrate that $\\texttt{DiPro}$ could effectively extract temporal clinical dynamics and achieve state-of-the-art performance on both disease progression identification and general ICU prediction tasks.",
    "summary": "",
    "translation": "基于时空解耦与多尺度对齐的多模态疾病进展建模",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域的疾病进展建模，属于明确的无关主题。虽然提到了多模态和时空建模技术，但其核心应用场景是医疗诊断和疾病预测，与推荐系统、搜索或广告领域没有任何直接或间接的关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11107v1": {
    "title": "MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps",
    "url": "https://www.alphaxiv.org/abs/2510.11107v1",
    "arxiv_id": "2510.11107v1",
    "authors": "Jiahui Lei, Kyle Genova, George Kopanas, Noah Snavely, Leonidas Guibas",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 07:56:19",
    "ori_summary": "This paper addresses the challenge of learning semantically and functionally meaningful 3D motion priors from real-world videos, in order to enable prediction of future 3D scene motion from a single input image. We propose a novel pixel-aligned Motion Map (MoMap) representation for 3D scene motion, which can be generated from existing generative image models to facilitate efficient and effective motion prediction. To learn meaningful distributions over motion, we create a large-scale database of MoMaps from over 50,000 real videos and train a diffusion model on these representations. Our motion generation not only synthesizes trajectories in 3D but also suggests a new pipeline for 2D video synthesis: first generate a MoMap, then warp an image accordingly and complete the warped point-based renderings. Experimental results demonstrate that our approach generates plausible and semantically consistent 3D scene motion.",
    "summary": "",
    "translation": "MoMaps：基于运动图的语义感知场景运动生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的场景运动生成，涉及运动图和语义感知技术，这与推荐系统、搜索或广告的核心技术领域没有直接关联。虽然标题提到'生成'，但这是纯粹的视觉内容生成应用，属于被明确排除的'Purely Vision'类别，没有明确的推荐/搜索/广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11106v1": {
    "title": "Compositional Zero-Shot Learning: A Survey",
    "url": "https://www.alphaxiv.org/abs/2510.11106v1",
    "arxiv_id": "2510.11106v1",
    "authors": "Ans Munir, Faisal Z. Qureshi, Mohsen Ali, Muhammad Haris Khan",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 07:54:47",
    "ori_summary": "Compositional Zero-Shot Learning (CZSL) is a critical task in computer vision that enables models to recognize unseen combinations of known attributes and objects during inference, addressing the combinatorial challenge of requiring training data for every possible composition. This is particularly challenging because the visual appearance of primitives is highly contextual; for example, ``small'' cats appear visually distinct from ``older'' ones, and ``wet'' cars differ significantly from ``wet'' cats. Effectively modeling this contextuality and the inherent compositionality is crucial for robust compositional zero-shot recognition. This paper presents, to our knowledge, the first comprehensive survey specifically focused on Compositional Zero-Shot Learning. We systematically review the state-of-the-art CZSL methods, introducing a taxonomy grounded in disentanglement, with four families of approaches: no explicit disentanglement, textual disentanglement, visual disentanglement, and cross-modal disentanglement. We provide a detailed comparative analysis of these methods, highlighting their core advantages and limitations in different problem settings, such as closed-world and open-world CZSL. Finally, we identify the most significant open challenges and outline promising future research directions. This survey aims to serve as a foundational resource to guide and inspire further advancements in this fascinating and important field. Papers studied in this survey with their official code are available on our github: https://github.com/ans92/Compositional-Zero-Shot-Learning",
    "summary": "",
    "translation": "组合式零样本学习：综述",
    "relevance_score": 3,
    "reasoning": "该论文是关于组合式零样本学习的综述，主要涉及视觉和语言领域的组合泛化能力。虽然组合学习概念在推荐系统中可能有潜在应用（如处理新用户-物品组合），但该论文更偏向计算机视觉和基础机器学习领域，与推荐系统、搜索或广告的直接关联较弱，且未明确涉及Transformer架构或LLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11096v1": {
    "title": "CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification and Prompt Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.11096v1",
    "arxiv_id": "2510.11096v1",
    "authors": "Fengling Zhu, Boshi Liu, Jingyu Hua, Sheng Zhong",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 07:44:54",
    "ori_summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in tasks such as image captioning, visual question answering, and cross-modal reasoning by integrating visual and textual modalities. However, their multimodal nature also exposes them to adversarial threats, where attackers can perturb either modality or both jointly to induce harmful, misleading, or policy violating outputs. Existing defense strategies, such as adversarial training and input purification, face notable limitations: adversarial training typically improves robustness only against known attacks while incurring high computational costs, whereas conventional purification approaches often suffer from degraded image quality and insufficient generalization to complex multimodal tasks. In this work, we focus on defending the visual modality, which frequently serves as the primary entry point for adversarial manipulation. We propose a supervised diffusion based denoising framework that leverages paired adversarial clean image datasets to fine-tune diffusion models with directional, task specific guidance. Unlike prior unsupervised purification methods such as DiffPure, our approach achieves higher quality reconstructions while significantly improving defense robustness in multimodal tasks. Furthermore, we incorporate prompt optimization as a complementary defense mechanism, enhancing resistance against diverse and unseen attack strategies. Extensive experiments on image captioning and visual question answering demonstrate that our method not only substantially improves robustness but also exhibits strong transferability to unknown adversarial attacks. These results highlight the effectiveness of supervised diffusion based denoising for multimodal defense, paving the way for more reliable and secure deployment of MLLMs in real world applications.",
    "summary": "",
    "translation": "CoDefend：通过扩散净化与提示优化的跨模态协同防御",
    "relevance_score": 2,
    "reasoning": "该论文主要关注跨模态防御和安全保护，属于安全领域，与我的核心关注点（推荐系统、搜索、广告中的技术进展）无关。虽然提到了跨模态和提示优化，但这些技术被应用于防御目的，而非提升推荐、搜索或广告系统的性能。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11092v1": {
    "title": "Future-Aware End-to-End Driving: Bidirectional Modeling of Trajectory Planning and Scene Evolution",
    "url": "https://www.alphaxiv.org/abs/2510.11092v1",
    "arxiv_id": "2510.11092v1",
    "authors": "Bozhou Zhang, Nan Song, Jingyu Li, Xiatian Zhu, Jiankang Deng, Li Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 07:41:47",
    "ori_summary": "End-to-end autonomous driving methods aim to directly map raw sensor inputs to future driving actions such as planned trajectories, bypassing traditional modular pipelines. While these approaches have shown promise, they often operate under a one-shot paradigm that relies heavily on the current scene context, potentially underestimating the importance of scene dynamics and their temporal evolution. This limitation restricts the model's ability to make informed and adaptive decisions in complex driving scenarios. We propose a new perspective: the future trajectory of an autonomous vehicle is closely intertwined with the evolving dynamics of its environment, and conversely, the vehicle's own future states can influence how the surrounding scene unfolds. Motivated by this bidirectional relationship, we introduce SeerDrive, a novel end-to-end framework that jointly models future scene evolution and trajectory planning in a closed-loop manner. Our method first predicts future bird's-eye view (BEV) representations to anticipate the dynamics of the surrounding scene, then leverages this foresight to generate future-context-aware trajectories. Two key components enable this: (1) future-aware planning, which injects predicted BEV features into the trajectory planner, and (2) iterative scene modeling and vehicle planning, which refines both future scene prediction and trajectory generation through collaborative optimization. Extensive experiments on the NAVSIM and nuScenes benchmarks show that SeerDrive significantly outperforms existing state-of-the-art methods.",
    "summary": "",
    "translation": "未来感知的端到端驾驶：轨迹规划与场景演化的双向建模",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域，涉及轨迹规划和场景演化建模，与推荐系统、搜索或广告的核心技术领域完全无关。论文内容属于自动驾驶的特定应用场景，没有任何明显的技术可以迁移到RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11091v1": {
    "title": "Text-Enhanced Panoptic Symbol Spotting in CAD Drawings",
    "url": "https://www.alphaxiv.org/abs/2510.11091v1",
    "arxiv_id": "2510.11091v1",
    "authors": "Xianlin Liu, Yan Gong, Bohao Li, Jiajing Huang, Bowen Du, Junchen Ye, Liyan Xu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 07:41:15",
    "ori_summary": "With the widespread adoption of Computer-Aided Design(CAD) drawings in engineering, architecture, and industrial design, the ability to accurately interpret and analyze these drawings has become increasingly critical. Among various subtasks, panoptic symbol spotting plays a vital role in enabling downstream applications such as CAD automation and design retrieval. Existing methods primarily focus on geometric primitives within the CAD drawings to address this task, but they face following major problems: they usually overlook the rich textual annotations present in CAD drawings and they lack explicit modeling of relationships among primitives, resulting in incomprehensive understanding of the holistic drawings. To fill this gap, we propose a panoptic symbol spotting framework that incorporates textual annotations. The framework constructs unified representations by jointly modeling geometric and textual primitives. Then, using visual features extract by pretrained CNN as the initial representations, a Transformer-based backbone is employed, enhanced with a type-aware attention mechanism to explicitly model the different types of spatial dependencies between various primitives. Extensive experiments on the real-world dataset demonstrate that the proposed method outperforms existing approaches on symbol spotting tasks involving textual annotations, and exhibits superior robustness when applied to complex CAD drawings.",
    "summary": "",
    "translation": "CAD图纸中基于文本增强的全景符号识别",
    "relevance_score": 1,
    "reasoning": "该论文专注于CAD图纸中的符号识别和文本处理，属于计算机视觉和文档分析领域。与推荐系统、搜索或广告的核心技术进展没有直接关联，也不涉及LLM、Transformer架构或异构数据统一建模等关键技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11090v1": {
    "title": "Source-Free Object Detection with Detection Transformer",
    "url": "https://www.alphaxiv.org/abs/2510.11090v1",
    "arxiv_id": "2510.11090v1",
    "authors": "Huizai Yao, Sicheng Zhao, Shuo Lu, Hui Chen, Yangyang Li, Guoping Liu, Tengfei Xing, Chenggang Yan, Jianhua Tao, Guiguang Ding",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 07:35:04",
    "ori_summary": "Source-Free Object Detection (SFOD) enables knowledge transfer from a source domain to an unsupervised target domain for object detection without access to source data. Most existing SFOD approaches are either confined to conventional object detection (OD) models like Faster R-CNN or designed as general solutions without tailored adaptations for novel OD architectures, especially Detection Transformer (DETR). In this paper, we introduce Feature Reweighting ANd Contrastive Learning NetworK (FRANCK), a novel SFOD framework specifically designed to perform query-centric feature enhancement for DETRs. FRANCK comprises four key components: (1) an Objectness Score-based Sample Reweighting (OSSR) module that computes attention-based objectness scores on multi-scale encoder feature maps, reweighting the detection loss to emphasize less-recognized regions; (2) a Contrastive Learning with Matching-based Memory Bank (CMMB) module that integrates multi-level features into memory banks, enhancing class-wise contrastive learning; (3) an Uncertainty-weighted Query-fused Feature Distillation (UQFD) module that improves feature distillation through prediction quality reweighting and query feature fusion; and (4) an improved self-training pipeline with a Dynamic Teacher Updating Interval (DTUI) that optimizes pseudo-label quality. By leveraging these components, FRANCK effectively adapts a source-pre-trained DETR model to a target domain with enhanced robustness and generalization. Extensive experiments on several widely used benchmarks demonstrate that our method achieves state-of-the-art performance, highlighting its effectiveness and compatibility with DETR-based SFOD models.",
    "summary": "",
    "translation": "基于检测变换器的无源目标检测",
    "relevance_score": 3,
    "reasoning": "该论文主要关注计算机视觉中的目标检测任务，虽然使用了Transformer架构，但其核心是源自由（source-free）设置下的视觉检测问题。在推荐系统、搜索或广告领域，这种纯粹的视觉检测技术缺乏直接的应用场景，无法为这些领域的核心问题（如排序、用户建模等）提供明确的解决方案。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11073v1": {
    "title": "ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer",
    "url": "https://www.alphaxiv.org/abs/2510.11073v1",
    "arxiv_id": "2510.11073v1",
    "authors": "Yuan Tian, Min Zhou, Yitong Chen, Fang Li, Lingzi Qi, Shuo Wang, Xieyang Xu, Yu Yu, Shiqiong Xu, Chaoyu Lei, Yankai Jiang, Rongzhao Zhang, Jia Tan, Li Wu, Hong Chen, Xiaowei Liu, Wei Lu, Lin Li, Huifang Zhou, Xuefei Song, Guangtao Zhai, Xianqun Fan",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 07:12:23",
    "ori_summary": "Patient face images provide a convenient mean for evaluating eye diseases, while also raising privacy concerns. Here, we introduce ROFI, a deep learning-based privacy protection framework for ophthalmology. Using weakly supervised learning and neural identity translation, ROFI anonymizes facial features while retaining disease features (over 98\\% accuracy, $\\kappa > 0.90$). It achieves 100\\% diagnostic sensitivity and high agreement ($\\kappa > 0.90$) across eleven eye diseases in three cohorts, anonymizing over 95\\% of images. ROFI works with AI systems, maintaining original diagnoses ($\\kappa > 0.80$), and supports secure image reversal (over 98\\% similarity), enabling audits and long-term care. These results show ROFI's effectiveness of protecting patient privacy in the digital medicine era.",
    "summary": "",
    "translation": "ROFI：一种基于深度学习的眼科体征保留且可逆的患者面部匿名化方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域（眼科）的面部匿名化技术，属于医疗图像处理和隐私保护范畴。这与我的核心关注领域（推荐系统、搜索、广告）以及所有相关技术方向完全无关，属于明确排除的医疗应用和隐私保护主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11063v1": {
    "title": "LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.11063v1",
    "arxiv_id": "2510.11063v1",
    "authors": "Chang Liu, Henghui Ding, Kaining Ying, Lingyi Hong, Ning Xu, Linjie Yang, Yuchen Fan, Mingqi Gao, Jingkun Chen, Yunqi Miao, Gengshen Wu, Zhijin Qin, Jungong Han, Zhixiong Zhang, Shuangrui Ding, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Chang Soo Lim, Joonyoung Moon, Donghyeon Cho, Tingmin Li, Yixuan Li, Yang Yang, An Yan, Leilei Cao, Feng Lu, Ran Hong, Youhai Jiang, Fengjie Zhu, Yujie Xie, Hongyang Zhang, Zhihui Liu, Shihai Ruan, Quanzhu Niu, Dengxian Gong, Shihao Chen, Tao Zhang, Yikang Zhou, Haobo Yuan, Lu Qi, Xiangtai Li, Shunping Ji, Ran Hong, Feng Lu, Leilei Cao, An Yan, Alexey Nekrasov, Ali Athar, Daan de Geus, Alexander Hermans, Bastian Leibe",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 07:02:09",
    "ori_summary": "This report presents an overview of the 7th Large-scale Video Object Segmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the two traditional tracks of LSVOS that jointly target robustness in realistic video scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition features a newly introduced track, Complex VOS (MOSEv2). Building upon prior insights, MOSEv2 substantially increases difficulty, introducing more challenging but realistic scenarios including denser small objects, frequent disappear/reappear events, severe occlusions, adverse weather and lighting, etc., pushing long-term consistency and generalization beyond curated benchmarks. The challenge retains standard ${J}$, $F$, and ${J\\&F}$ metrics for VOS and RVOS, while MOSEv2 adopts ${J\\&\\dot{F}}$ as the primary ranking metric to better evaluate objects across scales and disappearance cases. We summarize datasets and protocols, highlight top-performing solutions, and distill emerging trends, such as the growing role of LLM/MLLM components and memory-aware propagation, aiming to chart future directions for resilient, language-aware video segmentation in the wild.",
    "summary": "",
    "translation": "LSVOS 2025挑战赛报告：复杂视频对象分割的最新进展",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的视频对象分割技术，属于纯粹的视觉研究方向。虽然视频分割在理论上可能与推荐系统中的内容理解相关，但论文标题明确指向视觉分割挑战赛报告，没有表明与推荐系统、搜索或广告的直接关联，也不涉及LLM、Transformer架构或异构数据建模等核心技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11050v1": {
    "title": "Zero-shot Face Editing via ID-Attribute Decoupled Inversion",
    "url": "https://www.alphaxiv.org/abs/2510.11050v1",
    "arxiv_id": "2510.11050v1",
    "authors": "Yang Hou, Minggu Wang, Jianjun Zhao",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 06:34:40",
    "ori_summary": "Recent advancements in text-guided diffusion models have shown promise for general image editing via inversion techniques, but often struggle to maintain ID and structural consistency in real face editing tasks. To address this limitation, we propose a zero-shot face editing method based on ID-Attribute Decoupled Inversion. Specifically, we decompose the face representation into ID and attribute features, using them as joint conditions to guide both the inversion and the reverse diffusion processes. This allows independent control over ID and attributes, ensuring strong ID preservation and structural consistency while enabling precise facial attribute manipulation. Our method supports a wide range of complex multi-attribute face editing tasks using only text prompts, without requiring region-specific input, and operates at a speed comparable to DDIM inversion. Comprehensive experiments demonstrate its practicality and effectiveness.",
    "summary": "",
    "translation": "基于身份-属性解耦反演的零样本人脸编辑",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉领域的人脸编辑技术，涉及人脸身份和属性的解耦表示。虽然标题提到零样本学习，但核心内容属于纯粹的视觉处理范畴，与推荐系统、搜索或广告的排名和建模需求没有直接关联。该技术缺乏在异构数据统一建模或推荐系统应用场景中的明显潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11047v1": {
    "title": "Benchmarking Deep Learning Models for Laryngeal Cancer Staging Using the LaryngealCT Dataset",
    "url": "https://www.alphaxiv.org/abs/2510.11047v1",
    "arxiv_id": "2510.11047v1",
    "authors": "Nivea Roy, Son Tran, Atul Sajjanhar, K. Devaraja, Prakashini Koteshwara, Yong Xiang, Divya Rao",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 06:25:19",
    "ori_summary": "Laryngeal cancer imaging research lacks standardised datasets to enable reproducible deep learning (DL) model development. We present LaryngealCT, a curated benchmark of 1,029 computed tomography (CT) scans aggregated from six collections from The Cancer Imaging Archive (TCIA). Uniform 1 mm isotropic volumes of interest encompassing the larynx were extracted using a weakly supervised parameter search framework validated by clinical experts. 3D DL architectures (3D CNN, ResNet18,50,101, DenseNet121) were benchmarked on (i) early (Tis,T1,T2) vs. advanced (T3,T4) and (ii) T4 vs. non-T4 classification tasks. 3D CNN (AUC-0.881, F1-macro-0.821) and ResNet18 (AUC-0.892, F1-macro-0.646) respectively outperformed the other models in the two tasks. Model explainability assessed using 3D GradCAMs with thyroid cartilage overlays revealed greater peri-cartilage attention in non-T4 cases and focal activations in T4 predictions. Through open-source data, pretrained models, and integrated explainability tools, LaryngealCT offers a reproducible foundation for AI-driven research to support clinical decisions in laryngeal oncology.",
    "summary": "",
    "translation": "基于LaryngealCT数据集对喉癌分期深度学习模型的基准测试",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域（喉癌分期）的深度学习应用，属于明确的医疗领域特定应用。虽然涉及深度学习模型基准测试，但内容与推荐系统、搜索、广告或相关使能技术完全无关，属于明确的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11028v1": {
    "title": "Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with Cascaded Prompts",
    "url": "https://www.alphaxiv.org/abs/2510.11028v1",
    "arxiv_id": "2510.11028v1",
    "authors": "Yanning Hou, Ke Xu, Junfa Li, Yanran Ruan, Jianfeng Qiu",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 05:53:49",
    "ori_summary": "Recently, the powerful generalization ability exhibited by foundation models has brought forth new solutions for zero-shot anomaly segmentation tasks. However, guiding these foundation models correctly to address downstream tasks remains a challenge. This paper proposes a novel two-stage framework, for zero-shot anomaly segmentation tasks in industrial anomaly detection. This framework excellently leverages the powerful anomaly localization capability of CLIP and the boundary perception ability of SAM.(1) To mitigate SAM's inclination towards object segmentation, we propose the Co-Feature Point Prompt Generation (PPG) module. This module collaboratively utilizes CLIP and SAM to generate positive and negative point prompts, guiding SAM to focus on segmenting anomalous regions rather than the entire object. (2) To further optimize SAM's segmentation results and mitigate rough boundaries and isolated noise, we introduce the Cascaded Prompts for SAM (CPS) module. This module employs hybrid prompts cascaded with a lightweight decoder of SAM, achieving precise segmentation of anomalous regions. Across multiple datasets, consistent experimental validation demonstrates that our approach achieves state-of-the-art zero-shot anomaly segmentation results. Particularly noteworthy is our performance on the Visa dataset, where we outperform the state-of-the-art methods by 10.3\\% and 7.7\\% in terms of {$F_1$-max} and AP metrics, respectively.",
    "summary": "",
    "translation": "增强零样本异常检测：CLIP-SAM协作与级联提示",
    "relevance_score": 3,
    "reasoning": "该论文主要关注计算机视觉领域的异常检测，虽然使用了CLIP和SAM等基础模型，但其核心应用场景（视觉异常检测）与推荐系统、搜索或广告的直接关联性较弱。级联提示技术可能有潜力应用于多模态推荐中的特征交互，但这种联系较为间接且非论文主要贡献。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11027v1": {
    "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.11027v1",
    "arxiv_id": "2510.11027v1",
    "authors": "Ganlin Yang, Tianyi Zhang, Haoran Hao, Weiyun Wang, Yibin Liu, Dehui Wang, Guanzhou Chen, Zijian Cai, Junting Chen, Weijie Su, Wengang Zhou, Yu Qiao, Jifeng Dai, Jiangmiao Pang, Gen Luo, Wenhai Wang, Yao Mu, Zhi Hou",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 05:51:22",
    "ori_summary": "While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.",
    "summary": "",
    "translation": "Vlaser：具有协同具身推理能力的视觉-语言-动作模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注具身智能和机器人控制领域，虽然涉及多模态建模（视觉-语言-动作），但其核心应用场景是物理环境中的机器人交互和动作执行。在搜索、推荐或广告系统中，通常不需要物理动作执行能力，因此与当前关注领域的直接相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11026v1": {
    "title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.11026v1",
    "arxiv_id": "2510.11026v1",
    "authors": "Hongxiang Li, Yaowei Li, Bin Lin, Yuwei Niu, Yuhang Yang, Xiaoshuang Huang, Jiayin Cai, Xiaolong Jiang, Yao Hu, Long Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 05:50:44",
    "ori_summary": "Unified multimodal models integrate the reasoning capacity of large language models with both image understanding and generation, showing great promise for advanced multimodal intelligence. However, the community still lacks a rigorous reasoning-centric benchmark to systematically evaluate the alignment between understanding and generation, and their generalization potential in complex visual tasks. To this end, we introduce \\textbf{GIR-Bench}, a comprehensive benchmark that evaluates unified models across three complementary perspectives. Firstly, we investigate understanding-generation consistency (GIR-Bench-UGC), asking whether models can consistently leverage the same knowledge in both understanding and generation tasks. Secondly, we investigate whether models can perform reasoning-centric text-to-image generation that requires applying logical constraints and implicit knowledge to generate faithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models can handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset, we carefully design different task-specific evaluation pipelines tailored for each task. This enables fine-grained and interpretable evaluation while mitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive ablations over various unified models and generation-only systems have shown that: Although unified models are more capable of reasoning-driven visual tasks, they still exhibit a persistent gap between understanding and generation. The data and code for GIR-Bench are available at \\href{https://hkust-longgroup.github.io/GIR-Bench}{https://hkust-longgroup.github.io/GIR-Bench}.",
    "summary": "",
    "translation": "GIR-Bench：用于生成具有推理能力的图像的多功能基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于图像生成基准测试，属于纯粹的视觉生成领域，与推荐系统、搜索或广告的核心技术无关。虽然标题提到'推理'，但这是针对图像生成的推理能力评估，没有明确的潜在应用可以转化为RecSys/Search/Ads领域的实际技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11020v1": {
    "title": "GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via Cross-Modal Reward for Auxiliary Line Creation",
    "url": "https://www.alphaxiv.org/abs/2510.11020v1",
    "arxiv_id": "2510.11020v1",
    "authors": "Shasha Guo, Liang Pang, Xi Wang, Yanling Wang, Huawei Shen, Jing Zhang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 05:33:51",
    "ori_summary": "Auxiliary lines are essential for solving complex geometric problems but remain challenging for large vision-language models (LVLMs). Rather than editing diagrams to draw auxiliary lines, which current image editing models struggle to render with geometric precision, we generate textual descriptions of auxiliary-line constructions to better align with the representational strengths of LVLMs. To bridge the gap between textual descriptions and spatial structure, we propose a reinforcement learning framework that enhances diagram-text alignment. At the core of our approach is a cross-modal reward that evaluates how well the generated auxiliary-line description for an original diagram matches a ground-truth auxiliary-line diagram. Built on this reward, we present GeoVLMath, an open-source LVLM tailored to auxiliary-line reasoning in solid geometry. This fine-grained signal drives a GRPO-based RL stage, yielding precise diagram-text alignment. To support training, we develop a scalable data creation pipeline and construct AuxSolidMath, a dataset of 3,018 real-exam geometry problems with paired diagrams and aligned textual fields. At the 3B and 7B scales, GeoVLMath achieves competitive and often superior performance compared with strong open-source and proprietary LVLMs on auxiliary-line reasoning benchmarks.",
    "summary": "",
    "translation": "GeoVLMath：通过跨模态奖励辅助线创建增强视觉语言模型的几何推理能力",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型在几何推理任务上的改进，属于纯粹的视觉-语言交叉领域研究。虽然提到了跨模态奖励机制，但其应用场景仅限于几何数学问题，与推荐系统、搜索或广告领域没有明显的直接关联或潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11018v1": {
    "title": "The Easy Path to Robustness: Coreset Selection using Sample Hardness",
    "url": "https://www.alphaxiv.org/abs/2510.11018v1",
    "arxiv_id": "2510.11018v1",
    "authors": "Pranav Ramesh, Arjun Roy, Deepak Ravikumar, Kaushik Roy, Gopalakrishnan Srinivasan",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-13 05:28:16",
    "ori_summary": "Designing adversarially robust models from a data-centric perspective requires understanding which input samples are most crucial for learning resilient features. While coreset selection provides a mechanism for efficient training on data subsets, current algorithms are designed for clean accuracy and fall short in preserving robustness. To address this, we propose a framework linking a sample's adversarial vulnerability to its \\textit{hardness}, which we quantify using the average input gradient norm (AIGN) over training. We demonstrate that \\textit{easy} samples (with low AIGN) are less vulnerable and occupy regions further from the decision boundary. Leveraging this insight, we present EasyCore, a coreset selection algorithm that retains only the samples with low AIGN for training. We empirically show that models trained on EasyCore-selected data achieve significantly higher adversarial accuracy than those trained with competing coreset methods under both standard and adversarial training. As AIGN is a model-agnostic dataset property, EasyCore is an efficient and widely applicable data-centric method for improving adversarial robustness. We show that EasyCore achieves up to 7\\% and 5\\% improvement in adversarial accuracy under standard training and TRADES adversarial training, respectively, compared to existing coreset methods.",
    "summary": "",
    "translation": "通往鲁棒性的简易路径：基于样本难度的核心集选择",
    "relevance_score": 2,
    "reasoning": "该论文关注核心集选择和样本难度评估，这属于数据选择和训练效率的通用机器学习技术。虽然可能间接应用于推荐系统或搜索的数据预处理，但缺乏与LLM、Transformer架构或推荐/搜索/广告领域特定应用的直接联系，且未明确涉及异构数据建模或VLM类比方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11017v1": {
    "title": "High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation",
    "url": "https://www.alphaxiv.org/abs/2510.11017v1",
    "arxiv_id": "2510.11017v1",
    "authors": "Runyang Feng, Hyung Jin Chang, Tze Ho Elden Tse, Boeun Kim, Yi Chang, Yixing Gao",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 05:18:27",
    "ori_summary": "Modeling high-resolution spatiotemporal representations, including both global dynamic contexts (e.g., holistic human motion tendencies) and local motion details (e.g., high-frequency changes of keypoints), is essential for video-based human pose estimation (VHPE). Current state-of-the-art methods typically unify spatiotemporal learning within a single type of modeling structure (convolution or attention-based blocks), which inherently have difficulties in balancing global and local dynamic modeling and may bias the network to one of them, leading to suboptimal performance. Moreover, existing VHPE models suffer from quadratic complexity when capturing global dependencies, limiting their applicability especially for high-resolution sequences. Recently, the state space models (known as Mamba) have demonstrated significant potential in modeling long-range contexts with linear complexity; however, they are restricted to 1D sequential data. In this paper, we present a novel framework that extends Mamba from two aspects to separately learn global and local high-resolution spatiotemporal representations for VHPE. Specifically, we first propose a Global Spatiotemporal Mamba, which performs 6D selective space-time scan and spatial- and temporal-modulated scan merging to efficiently extract global representations from high-resolution sequences. We further introduce a windowed space-time scan-based Local Refinement Mamba to enhance the high-frequency details of localized keypoint motions. Extensive experiments on four benchmark datasets demonstrate that the proposed model outperforms state-of-the-art VHPE approaches while achieving better computational trade-offs.",
    "summary": "",
    "translation": "基于视频的人体姿态估计中的全局-局部状态空间模型高分辨率时空建模",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的人体姿态估计，虽然涉及时空建模技术，但缺乏与推荐系统、搜索或广告的明确联系。状态空间模型在理论上可能用于序列建模，但论文专注于视觉任务，没有展示在推荐/搜索/广告领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11014v1": {
    "title": "Into the Unknown: Towards using Generative Models for Sampling Priors of Environment Uncertainty for Planning in Configuration Spaces",
    "url": "https://www.alphaxiv.org/abs/2510.11014v1",
    "arxiv_id": "2510.11014v1",
    "authors": "Subhransu S. Bhattacharjee, Hao Lu, Dylan Campbell, Rahul Shome",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2025-10-13 05:08:48",
    "ori_summary": "Priors are vital for planning under partial observability, yet difficult to obtain in practice. We present a sampling-based pipeline that leverages large-scale pretrained generative models to produce probabilistic priors capturing environmental uncertainty and spatio-semantic relationships in a zero-shot manner. Conditioned on partial observations, the pipeline recovers complete RGB-D point cloud samples with occupancy and target semantics, formulated to be directly useful in configuration-space planning. We establish a Matterport3D benchmark of rooms partially visible through doorways, where a robot must navigate to an unobserved target object. Effective priors for this setting must represent both occupancy and target-location uncertainty in unobserved regions. Experiments show that our approach recovers commonsense spatial semantics consistent with ground truth, yielding diverse, clean 3D point clouds usable in motion planning, highlight the promise of generative models as a rich source of priors for robotic planning.",
    "summary": "",
    "translation": "深入未知：利用生成模型为配置空间规划中的环境不确定性采样先验",
    "relevance_score": 2,
    "reasoning": "该论文主要关注配置空间规划中的生成模型应用，属于机器人学或控制理论领域。虽然涉及生成模型技术，但其在推荐系统、搜索或广告中的潜在应用非常有限且不直接相关。该工作更专注于物理环境建模和规划问题，而非用户行为建模或内容排序等核心领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11012v1": {
    "title": "COCO-Tree: Compositional Hierarchical Concept Trees for Enhanced Reasoning in Vision Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11012v1",
    "arxiv_id": "2510.11012v1",
    "authors": "Sanchit Sinha, Guangzhi Xiong, Aidong Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 05:07:13",
    "ori_summary": "Compositional reasoning remains a persistent weakness of modern vision language models (VLMs): they often falter when a task hinges on understanding how multiple objects, attributes, and relations interact within an image. Multiple research works have attempted to improve compositionality performance by creative tricks such as improving prompt structure, chain of thought reasoning, etc. A more recent line of work attempts to impart additional reasoning in VLMs using well-trained Large Language Models (LLMs), which are far superior in linguistic understanding than VLMs to compensate for the limited linguistic prowess of VLMs. However, these approaches are either resource-intensive or do not provide an interpretable reasoning process. In this paper, we present 'COCO-Tree' - a novel approach that augments VLM outputs with carefully designed neurosymbolic concept trees learned from LLMs to improve VLM's linguistic reasoning. COCO-Tree's beam search-inspired reasoning process boosts compositionality performance and provides a rationale behind VLM predictions. Empirical results on four compositionality benchmarks, Winoground, EqBench, ColorSwap, and SugarCrepe, in seven different open-source VLMs with varying sizes, demonstrate that COCO-Tree significantly improves compositional generalization by 5-10% over baselines.",
    "summary": "",
    "translation": "COCO-Tree：用于增强视觉语言模型推理能力的组合式分层概念树",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型的推理增强技术，属于纯粹的VLM领域研究。虽然提到了概念层次结构和推理能力，但这些技术缺乏明确的推荐系统、搜索或广告应用场景。该工作更偏向于计算机视觉和自然语言处理的交叉领域，而非RecSys/Search/Ads的核心技术需求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11005v1": {
    "title": "Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.11005v1",
    "arxiv_id": "2510.11005v1",
    "authors": "Kai Han, Siqi Ma, Chengxuan Qian, Jun Chen, Chongwen Lyu, Yuqing Song, Zhe Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 04:44:43",
    "ori_summary": "Accurate segmentation of tumors and adjacent normal tissues in medical images is essential for surgical planning and tumor staging. Although foundation models generally perform well in segmentation tasks, they often struggle to focus on foreground areas in complex, low-contrast backgrounds, where some malignant tumors closely resemble normal organs, complicating contextual differentiation. To address these challenges, we propose the Foreground-Aware Spectrum Segmentation (FASS) framework. First, we introduce a foreground-aware module to amplify the distinction between background and the entire volume space, allowing the model to concentrate more effectively on target areas. Next, a feature-level frequency enhancement module, based on wavelet transform, extracts discriminative high-frequency features to enhance boundary recognition and detail perception. Eventually, we introduce an edge constraint module to preserve geometric continuity in segmentation boundaries. Extensive experiments on multiple medical datasets demonstrate superior performance across all metrics, validating the effectiveness of our framework, particularly in robustness under complex conditions and fine structure recognition. Our framework significantly enhances segmentation of low-contrast images, paving the way for applications in more diverse and complex medical imaging scenarios.",
    "summary": "",
    "translation": "频域为腹部医学图像分割解锁新视角",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分割这一特定领域应用，属于明确的医学领域研究。论文内容涉及腹部医学图像处理，与推荐系统、搜索或广告领域没有任何技术关联或应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11000v1": {
    "title": "ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation",
    "url": "https://www.alphaxiv.org/abs/2510.11000v1",
    "arxiv_id": "2510.11000v1",
    "authors": "Ruihang Xu, Dewei Zhou, Fan Ma, Yi Yang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 04:21:19",
    "ori_summary": "Multi-instance image generation (MIG) remains a significant challenge for modern diffusion models due to key limitations in achieving precise control over object layout and preserving the identity of multiple distinct subjects. To address these limitations, we introduce ContextGen, a novel Diffusion Transformer framework for multi-instance generation that is guided by both layout and reference images. Our approach integrates two key technical contributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates the composite layout image into the generation context to robustly anchor the objects in their desired positions, and Identity Consistency Attention (ICA), an innovative attention mechanism that leverages contextual reference images to ensure the identity consistency of multiple instances. Recognizing the lack of large-scale, hierarchically-structured datasets for this task, we introduce IMIG-100K, the first dataset with detailed layout and identity annotations. Extensive experiments demonstrate that ContextGen sets a new state-of-the-art, outperforming existing methods in control precision, identity fidelity, and overall visual quality.",
    "summary": "",
    "translation": "ContextGen：基于上下文布局锚定的身份一致性多实例生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多实例生成中的身份一致性布局锚定，属于AIGC和内容生成领域。虽然提到了上下文概念，但这与推荐系统、搜索或广告中的上下文特征建模有本质区别，没有明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10993v1": {
    "title": "Perspective-aware 3D Gaussian Inpainting with Multi-view Consistency",
    "url": "https://www.alphaxiv.org/abs/2510.10993v1",
    "arxiv_id": "2510.10993v1",
    "authors": "Yuxin Cheng, Binxiao Huang, Taiqiang Wu, Wenyong Zhou, Chenchen Ding, Zhengwu Liu, Graziano Chesi, Ngai Wong",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 04:10:39",
    "ori_summary": "3D Gaussian inpainting, a critical technique for numerous applications in virtual reality and multimedia, has made significant progress with pretrained diffusion models. However, ensuring multi-view consistency, an essential requirement for high-quality inpainting, remains a key challenge. In this work, we present PAInpainter, a novel approach designed to advance 3D Gaussian inpainting by leveraging perspective-aware content propagation and consistency verification across multi-view inpainted images. Our method iteratively refines inpainting and optimizes the 3D Gaussian representation with multiple views adaptively sampled from a perspective graph. By propagating inpainted images as prior information and verifying consistency across neighboring views, PAInpainter substantially enhances global consistency and texture fidelity in restored 3D scenes. Extensive experiments demonstrate the superiority of PAInpainter over existing methods. Our approach achieves superior 3D inpainting quality, with PSNR scores of 26.03 dB and 29.51 dB on the SPIn-NeRF and NeRFiller datasets, respectively, highlighting its effectiveness and generalization capability.",
    "summary": "",
    "translation": "基于多视角一致性的透视感知3D高斯修复",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D视觉和图形学中的高斯修复技术，属于纯粹的计算机视觉领域。虽然提到了多视角一致性，但这主要针对3D场景重建和修复，与推荐系统、搜索或广告中的排序、用户建模、内容理解等核心问题没有直接关联。该技术缺乏在RecSys/Search/Ads领域的明确应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10986v1": {
    "title": "Mixup Helps Understanding Multimodal Video Better",
    "url": "https://www.alphaxiv.org/abs/2510.10986v1",
    "arxiv_id": "2510.10986v1",
    "authors": "Xiaoyu Ma, Ding Ding, Hao Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 03:53:25",
    "ori_summary": "Multimodal video understanding plays a crucial role in tasks such as action recognition and emotion classification by combining information from different modalities. However, multimodal models are prone to overfitting strong modalities, which can dominate learning and suppress the contributions of weaker ones. To address this challenge, we first propose Multimodal Mixup (MM), which applies the Mixup strategy at the aggregated multimodal feature level to mitigate overfitting by generating virtual feature-label pairs. While MM effectively improves generalization, it treats all modalities uniformly and does not account for modality imbalance during training. Building on MM, we further introduce Balanced Multimodal Mixup (B-MM), which dynamically adjusts the mixing ratios for each modality based on their relative contributions to the learning objective. Extensive experiments on several datasets demonstrate the effectiveness of our methods in improving generalization and multimodal robustness.",
    "summary": "",
    "translation": "Mixup技术有助于更好地理解多模态视频",
    "relevance_score": 7,
    "reasoning": "该论文涉及多模态理解技术，与VLM类比处理异构数据的焦点相关。Mixup作为一种数据增强技术，在多模态视频场景中的应用潜力可迁移到推荐系统中处理用户行为序列、上下文特征等多模态数据，提升模型对复杂用户意图的理解能力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.10980v1": {
    "title": "On the Optimal Representation Efficiency of Barlow Twins: An Information-Geometric Interpretation",
    "url": "https://www.alphaxiv.org/abs/2510.10980v1",
    "arxiv_id": "2510.10980v1",
    "authors": "Di Zhang",
    "categories": "cs.LG, cs.CV, cs.IT, math.IT, math.ST, stat.ML, stat.TH, 68T07, 62B11, 94A17, 53B12, I.2.6; I.5.1; G.3; H.1.1",
    "pub_date": "2025-10-13 03:41:27",
    "ori_summary": "Self-supervised learning (SSL) has achieved remarkable success by learning meaningful representations without labeled data. However, a unified theoretical framework for understanding and comparing the efficiency of different SSL paradigms remains elusive. In this paper, we introduce a novel information-geometric framework to quantify representation efficiency. We define representation efficiency $\\eta$ as the ratio between the effective intrinsic dimension of the learned representation space and its ambient dimension, where the effective dimension is derived from the spectral properties of the Fisher Information Matrix (FIM) on the statistical manifold induced by the encoder. Within this framework, we present a theoretical analysis of the Barlow Twins method. Under specific but natural assumptions, we prove that Barlow Twins achieves optimal representation efficiency ($\\eta = 1$) by driving the cross-correlation matrix of representations towards the identity matrix, which in turn induces an isotropic FIM. This work provides a rigorous theoretical foundation for understanding the effectiveness of Barlow Twins and offers a new geometric perspective for analyzing SSL algorithms.",
    "summary": "",
    "translation": "关于Barlow Twins最优表示效率的研究：一种信息几何解释",
    "relevance_score": 2,
    "reasoning": "该论文主要研究自监督学习中的Barlow Twins方法及其表示效率的理论分析，属于基础表示学习范畴。虽然表示学习是推荐和搜索系统的底层技术，但论文聚焦于信息几何解释和理论效率分析，缺乏明确的RecSys/Search/Ads应用场景，且不涉及Transformer架构或LLM技术的直接进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10973v1": {
    "title": "Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.10973v1",
    "arxiv_id": "2510.10973v1",
    "authors": "Sanchit Sinha, Oana Frunza, Kashif Rasul, Yuriy Nevmyvaka, Aidong Zhang",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-13 03:25:35",
    "ori_summary": "The capabilities of Large Vision-Language Models (LVLMs) have reached state-of-the-art on many visual reasoning tasks, including chart reasoning, yet they still falter on out-of-distribution (OOD) data, and degrade further when asked to produce their chain-of-thought (CoT) rationales, limiting explainability. We present Chart-RVR, a general framework that fine-tunes LVLMs to be more robust and explainable for chart reasoning by coupling Group Relative Policy Optimization (GRPO) with automatically verifiable rewards. Our framework comprises of three rewards that maximize: (i) correct chart-type classification, (ii) faithful chart table reconstruction, and (iii) process conformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently outperforms standard supervised fine-tuning (SFT) on both in-distribution and out-of-distribution datasets, closing the OOD performance gap while improving rationale fidelity. The resulting models, the Chart-RVR-3B series, achieve state-of-the-art results on six chart-reasoning benchmarks spanning in-domain and OOD settings, surpassing all existing models of comparable size. Beyond accuracy, Chart-RVR yields more interpretable CoT rationales, strengthening trust and reliability - showcasing the power of verifiable rewards with GRPO for training reliable, interpretable chart-reasoning models.",
    "summary": "",
    "translation": "Chart-RVR：基于可验证奖励的强化学习用于可解释图表推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注图表推理领域的强化学习应用，属于特定领域的推理任务。虽然涉及强化学习技术，但缺乏与推荐系统、搜索或广告领域的明确关联。图表推理的应用场景主要局限于数据可视化和文档理解，与我的核心关注领域重叠度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10969v1": {
    "title": "IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation",
    "url": "https://www.alphaxiv.org/abs/2510.10969v1",
    "arxiv_id": "2510.10969v1",
    "authors": "Zeteng Lin, Xingxing Li, Wen You, Xiaoyang Li, Zehan Lu, Yujun Cai, Jing Tang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 03:19:45",
    "ori_summary": "Existing vision language models (VLMs), including GPT-4 and DALL-E, often struggle to preserve logic, object identity, and style in multimodal image-text generation. This limitation significantly hinders the generalization capability of VLMs in complex image-text input-output scenarios. To address this issue, we propose IUT-Plug, a module grounded in an Image Understanding Tree (IUT), which enhances existing interleaved VLMs through explicit structured reasoning, thereby mitigating context drift in logic, entity identity, and style. The proposed framework operates in two stages. (1) A dynamic IUT-Plug extraction module parses visual scenes into hierarchical symbolic structures. (2) A coordinated narrative-flow and image synthesis mechanism ensures cross-modal consistency. To evaluate our approach, we construct a novel benchmark based on 3,000 real human-generated question-answer pairs over fine-tuned large models, introducing a dynamic evaluation protocol for quantifying context drift in interleaved VLMs. Experimental results demonstrate that IUT-Plug not only improves accuracy on established benchmarks but also effectively alleviates the three critical forms of context drift across diverse multimodal question answering (QA) scenarios.",
    "summary": "",
    "translation": "IUT-Plug：一种用于交错图像-文本生成的插件工具",
    "relevance_score": 2,
    "reasoning": "该论文专注于图像-文本交错生成技术，属于AIGC和内容生成领域。虽然标题提到插件工具，但核心是跨模态生成任务，与推荐系统、搜索或广告的排序和匹配问题没有直接关联，属于被排除的纯LLM中心化主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10954v1": {
    "title": "Comparative Evaluation of Neural Network Architectures for Generalizable Human Spatial Preference Prediction in Unseen Built Environments",
    "url": "https://www.alphaxiv.org/abs/2510.10954v1",
    "arxiv_id": "2510.10954v1",
    "authors": "Maral Doctorarastoo, Katherine A. Flanigan, Mario Bergés, Christopher McComb",
    "categories": "cs.CE, cs.CV, cs.LG, cs.MA",
    "pub_date": "2025-10-13 03:04:48",
    "ori_summary": "The capacity to predict human spatial preferences within built environments is instrumental for developing Cyber-Physical-Social Infrastructure Systems (CPSIS). A significant challenge in this domain is the generalizability of preference models, particularly their efficacy in predicting preferences within environmental configurations not encountered during training. While deep learning models have shown promise in learning complex spatial and contextual dependencies, it remains unclear which neural network architectures are most effective at generalizing to unseen layouts. To address this, we conduct a comparative study of Graph Neural Networks, Convolutional Neural Networks, and standard feedforward Neural Networks using synthetic data generated from a simplified and synthetic pocket park environment. Beginning with this illustrative case study, allows for controlled analysis of each model's ability to transfer learned preference patterns to unseen spatial scenarios. The models are evaluated based on their capacity to predict preferences influenced by heterogeneous physical, environmental, and social features. Generalizability score is calculated using the area under the precision-recall curve for the seen and unseen layouts. This generalizability score is appropriate for imbalanced data, providing insights into the suitability of each neural network architecture for preference-aware human behavior modeling in unseen built environments.",
    "summary": "",
    "translation": "未见建筑环境中可泛化人类空间偏好预测的神经网络架构比较评估",
    "relevance_score": 2,
    "reasoning": "该论文专注于建筑环境中的空间偏好预测，这属于特定领域应用而非核心推荐系统、搜索或广告领域。虽然涉及神经网络架构比较，但其应用场景（建筑环境）与我的关注点（RecSys/Search/Ads）相关性很低，且未明确涉及Transformer架构或LLM技术在这些领域的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10947v1": {
    "title": "Towards Distribution-Shift Uncertainty Estimation for Inverse Problems with Generative Priors",
    "url": "https://www.alphaxiv.org/abs/2510.10947v1",
    "arxiv_id": "2510.10947v1",
    "authors": "Namhoon Kim, Sara Fridovich-Keil",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 02:58:26",
    "ori_summary": "Generative models have shown strong potential as data-driven priors for solving inverse problems such as reconstructing medical images from undersampled measurements. While these priors improve reconstruction quality with fewer measurements, they risk hallucinating features when test images lie outside the training distribution. Existing uncertainty quantification methods in this setting (i) require an in-distribution calibration dataset, which may not be available, (ii) provide heuristic rather than statistical estimates, or (iii) quantify uncertainty from model capacity or limited measurements rather than distribution shift. We propose an instance-level, calibration-free uncertainty indicator that is sensitive to distribution shift, requires no knowledge of the training distribution, and incurs no retraining cost. Our key hypothesis is that reconstructions of in-distribution images remain stable under random measurement variations, while reconstructions of out-of-distribution (OOD) images exhibit greater instability. We use this stability as a proxy for detecting distribution shift. Our proposed OOD indicator is efficiently computable for any computational imaging inverse problem; we demonstrate it on tomographic reconstruction of MNIST digits, where a learned proximal network trained only on digit \"0\" is evaluated on all ten digits. Reconstructions of OOD digits show higher variability and correspondingly higher reconstruction error, validating this indicator. These results suggest a deployment strategy that pairs generative priors with lightweight guardrails, enabling aggressive measurement reduction for in-distribution cases while automatically warning when priors are applied out of distribution.",
    "summary": "",
    "translation": "面向生成先验逆问题的分布偏移不确定性估计",
    "relevance_score": 2,
    "reasoning": "该论文主要关注逆问题中的不确定性估计和分布偏移，这属于通用的机器学习方法研究。虽然生成先验可能与生成模型相关，但论文的核心焦点是逆问题和不确定性量化，在推荐系统、搜索或广告中的直接应用潜力有限。没有明确证据表明该技术专门针对推荐、搜索或广告领域的特定挑战。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10933v1": {
    "title": "DKPMV: Dense Keypoints Fusion from Multi-View RGB Frames for 6D Pose Estimation of Textureless Objects",
    "url": "https://www.alphaxiv.org/abs/2510.10933v1",
    "arxiv_id": "2510.10933v1",
    "authors": "Jiahong Chen, Jinghao Wang, Zi Wang, Ziwen Wang, Banglei Guan, Qifeng Yu",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-13 02:45:55",
    "ori_summary": "6D pose estimation of textureless objects is valuable for industrial robotic applications, yet remains challenging due to the frequent loss of depth information. Current multi-view methods either rely on depth data or insufficiently exploit multi-view geometric cues, limiting their performance. In this paper, we propose DKPMV, a pipeline that achieves dense keypoint-level fusion using only multi-view RGB images as input. We design a three-stage progressive pose optimization strategy that leverages dense multi-view keypoint geometry information. To enable effective dense keypoint fusion, we enhance the keypoint network with attentional aggregation and symmetry-aware training, improving prediction accuracy and resolving ambiguities on symmetric objects. Extensive experiments on the ROBI dataset demonstrate that DKPMV outperforms state-of-the-art multi-view RGB approaches and even surpasses the RGB-D methods in the majority of cases. The code will be available soon.",
    "summary": "",
    "translation": "DKPMV：基于多视角RGB帧的密集关键点融合用于无纹理物体的6D姿态估计",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的6D姿态估计，特别是针对无纹理物体的多视角RGB帧处理。这与推荐系统、搜索或广告的核心领域没有直接关联，也不涉及LLM技术、Transformer架构或异构数据统一建模。该技术主要应用于机器人、增强现实等视觉领域，与我的关注焦点无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12801v1": {
    "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search",
    "url": "https://www.alphaxiv.org/abs/2510.12801v1",
    "arxiv_id": "2510.12801v1",
    "authors": "Kartik Narayan, Yang Xu, Tian Cao, Kavya Nerella, Vishal M. Patel, Navid Shiee, Peter Grasch, Chao Jia, Yinfei Yang, Zhe Gan",
    "categories": "cs.CV, cs.IR",
    "pub_date": "2025-10-14 17:59:58",
    "ori_summary": "Multimodal Large Language Models (MLLMs) in real-world applications require access to external knowledge sources and must remain responsive to the dynamic and ever-changing real-world information in order to address information-seeking and knowledge-intensive user queries. Existing approaches, such as retrieval augmented generation (RAG) methods, search agents, and search equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and poorly constructed search queries, which result in inefficiencies and suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1, the first multimodal LLM capable of performing on-demand, multi-turn web searches and dynamically crafting queries for both image and text search tools. Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops of the input image making the image search more effective, and can iteratively adapt text search queries based on retrieved information, thereby enabling self-reflection and self-correction. Our approach relies on a two-stage training pipeline: a cold start supervised finetuning phase followed by an online reinforcement learning optimization. For training, we introduce DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated pipeline intermixed with real-world information from web search tools. This dataset contains diverse, multi-hop queries that integrate textual and visual information, teaching the model when to search, what to search for, which search tool to use and how to reason over the retrieved information. We conduct extensive experiments across a range of knowledge-intensive benchmarks to demonstrate the superiority of our approach. Finally, we analyze the results and provide insights that are valuable for advancing multimodal web-search.",
    "summary": "论文研究多模态LLM在动态网络搜索中的效率问题，核心方法是让模型能够基于输入图像裁剪和迭代文本查询，自主决定何时搜索、搜索什么以及使用哪种搜索工具。",
    "translation": "DeepMMSearch-R1：赋能多模态大语言模型在多模态网络搜索中的应用",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及多模态LLM在搜索领域的应用，属于'Direct LLM Applications'和'VLM Analogy for Heterogeneous Data'的范畴。多模态搜索是搜索系统的核心演进方向，该技术可处理文本、图像、视频等异构数据，直接提升搜索系统的用户体验和检索效果。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对搜索领域核心问题，提出多模态LLM动态搜索方法，完美契合直接LLM应用和搜索技术发展重点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12742v1": {
    "title": "CTRL-Rec: Controlling Recommender Systems With Natural Language",
    "url": "https://www.alphaxiv.org/abs/2510.12742v1",
    "arxiv_id": "2510.12742v1",
    "authors": "Micah Carroll, Adeline Foote, Kevin Feng, Marcus Williams, Anca Dragan, W. Bradley Knox, Smitha Milli",
    "categories": "cs.AI, cs.IR",
    "pub_date": "2025-10-14 17:20:04",
    "ori_summary": "When users are dissatisfied with recommendations from a recommender system, they often lack fine-grained controls for changing them. Large language models (LLMs) offer a solution by allowing users to guide their recommendations through natural language requests (e.g., \"I want to see respectful posts with a different perspective than mine\"). We propose a method, CTRL-Rec, that allows for natural language control of traditional recommender systems in real-time with computational efficiency. Specifically, at training time, we use an LLM to simulate whether users would approve of items based on their language requests, and we train embedding models that approximate such simulated judgments. We then integrate these user-request-based predictions into the standard weighting of signals that traditional recommender systems optimize. At deployment time, we require only a single LLM embedding computation per user request, allowing for real-time control of recommendations. In experiments with the MovieLens dataset, our method consistently allows for fine-grained control across a diversity of requests. In a study with 19 Letterboxd users, we find that CTRL-Rec was positively received by users and significantly enhanced users' sense of control and satisfaction with recommendations compared to traditional controls.",
    "summary": "论文研究传统推荐系统缺乏细粒度用户控制的问题，核心方法是利用LLM模拟用户语言请求的偏好判断，训练嵌入模型并将预测集成到推荐信号权重中，实现自然语言实时控制。",
    "translation": "CTRL-Rec：使用自然语言控制推荐系统",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及使用自然语言控制推荐系统，这属于LLM技术在推荐系统中的直接应用。自然语言控制能力可以显著提升推荐系统的交互性和可解释性，允许用户通过自然语言指令直接修改推荐偏好、调整推荐策略或查询推荐结果，在搜索和广告领域也有类似的应用潜力。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用LLM技术实现推荐系统的自然语言控制，完美契合直接LLM应用和核心推荐系统改进这两个焦点领域。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12709v1": {
    "title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model",
    "url": "https://www.alphaxiv.org/abs/2510.12709v1",
    "arxiv_id": "2510.12709v1",
    "authors": "Lin Lin, Jiefeng Long, Zhihe Wan, Yuchi Wang, Dingkang Yang, Shuang Yang, Yueyang Yao, Xu Chen, Zirui Guo, Shengqiang Li, Weiran Li, Hanyu Li, Yaling Mou, Yan Qiu, Haiyang Yu, Xiao Liang, Hongsheng Li, Chao Feng",
    "categories": "cs.IR, cs.CV",
    "pub_date": "2025-10-14 16:43:22",
    "ori_summary": "Multimodal embedding models aim to yield informative unified representations that empower diverse cross-modal tasks. Despite promising developments in the evolution from CLIP-based dual-tower architectures to large vision-language models, prior works still face unavoidable challenges in real-world applications and business scenarios, such as the limited modality support, unstable training mechanisms, and industrial domain gaps. In this work, we introduce SAIL-Embedding, an omni-modal embedding foundation model that addresses these issues through tailored training strategies and architectural design. In the optimization procedure, we propose a multi-stage training scheme to boost the multifaceted effectiveness of representation learning. Specifically, the content-aware progressive training aims to enhance the model's adaptability to diverse downstream tasks and master enriched cross-modal proficiency. The collaboration-aware recommendation enhancement training further adapts multimodal representations for recommendation scenarios by distilling knowledge from sequence-to-item and ID-to-item embeddings while mining user historical interests. Concurrently, we develop the stochastic specialization and dataset-driven pattern matching to strengthen model training flexibility and generalizability. Experimental results show that SAIL-Embedding achieves SOTA performance compared to other methods in different retrieval tasks. In online experiments across various real-world scenarios integrated with our model, we observe a significant increase in Lifetime (LT), which is a crucial indicator for the recommendation experience. For instance, the model delivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the Douyin-Selected scenario. For the Douyin feed rank model, the match features produced by SAIL-Embedding yield a +0.08% AUC gain.",
    "summary": "论文研究如何构建支持多模态的统一嵌入模型以解决现实应用中的模态限制和领域差距问题，核心方法是通过多阶段训练策略（包括内容感知渐进训练和推荐增强训练）来提升模型的跨模态能力和推荐场景适应性。",
    "translation": "SAIL-Embedding技术报告：全模态嵌入基础模型",
    "relevance_score": 8,
    "reasoning": "该论文提出全模态嵌入基础模型，与'异构数据的VLM类比'焦点高度相关，可将推荐系统中的用户序列、上下文特征等异构数据视为不同模态进行统一建模。这种统一嵌入方法在搜索和推荐系统中具有直接应用潜力，能够处理多源异构数据并学习跨模态表示，提升个性化推荐和搜索相关性。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统的多模态嵌入模型，提出了内容感知训练和推荐增强训练等核心方法，与用户关注的LLM应用和异构数据处理高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12668v1": {
    "title": "The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12668v1",
    "arxiv_id": "2510.12668v1",
    "authors": "Minghao Tang, Shiyu Ni, Jingtong Wu, Zengxin Han, Keping Bi",
    "categories": "cs.IR, cs.CL",
    "pub_date": "2025-10-14 16:05:01",
    "ori_summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving external documents. As an emerging form of RAG, parametric retrieval-augmented generation (PRAG) encodes documents as model parameters (i.e., LoRA modules) and injects these representations into the model during inference, enabling interaction between the LLM and documents at parametric level. Compared with directly placing documents in the input context, PRAG is more efficient and has the potential to offer deeper model-document interaction. Despite its growing attention, the mechanism underlying parametric injection remains poorly understood. In this work, we present a systematic study of PRAG to clarify the role of parametric injection, showing that parameterized documents capture only partial semantic information of documents, and relying on them alone yields inferior performance compared to interaction at text level. However, these parametric representations encode high-level document information that can enhance the model's understanding of documents within the input context. When combined parameterized documents with textual documents, the model can leverage relevant information more effectively and become more robust to noisy inputs, achieving better performance than either source alone. We recommend jointly using parameterized and textual documents and advocate for increasing the information content of parametric representations to advance PRAG.",
    "summary": "研究参数化检索增强生成(PRAG)中参数注入机制的作用，核心发现是参数化文档仅捕获部分语义信息但编码高层文档信息，当与文本文档结合使用时能更有效地利用相关信息并增强模型对噪声输入的鲁棒性。",
    "translation": "参数化注入的作用——参数化检索增强生成的系统性研究",
    "relevance_score": 8,
    "reasoning": "该论文系统研究参数化检索增强生成（RAG），这是LLM技术中的核心进展，属于'使能LLM技术'范畴。参数化RAG方法在搜索和推荐系统中具有直接应用潜力，可用于改进检索质量、增强上下文理解，以及构建更高效的检索-排序-生成流水线。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文系统研究参数化检索增强生成(PRAG)，这是LLM在搜索和推荐领域的重要应用技术，深入分析了参数注入机制对模型-文档交互的影响。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12604v1": {
    "title": "SMILE: SeMantic Ids Enhanced CoLd Item Representation for Click-through Rate Prediction in E-commerce SEarch",
    "url": "https://www.alphaxiv.org/abs/2510.12604v1",
    "arxiv_id": "2510.12604v1",
    "authors": "Qihang Zhao, Zhongbo Sun, Xiaoyang Zheng, Xian Guo, Siyuan Wang, Zihan Liang, Mingcan Peng, Ben Chen, Chenyi Lei",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-14 14:58:50",
    "ori_summary": "With the rise of modern search and recommendation platforms, insufficient collaborative information of cold-start items exacerbates the Matthew effect of existing platform items, challenging platform diversity and becoming a longstanding issue. Existing methods align items' side content with collaborative information to transfer collaborative signals from high-popularity items to cold-start items. However, these methods fail to account for the asymmetry between collaboration and content, nor the fine-grained differences among items. To address these issues, we propose SMILE, an item representation enhancement approach based on fused alignment of semantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and collaborative information, followed by a two-step alignment: RQ encoding transfers shared collaborative signals across items, while OPQ encoding learns differentiated information of items. Comprehensive offline experiments on large-scale industrial datasets demonstrate superiority of SMILE, and rigorous online A/B tests confirm statistically significant improvements: item CTR +1.66%, buyers +1.57%, and order volume +2.17%.",
    "summary": "论文研究电商搜索中冷启动物品的点击率预测问题，核心方法是使用RQ-OPQ编码量化物品内容和协同信息，通过两步对齐策略：RQ编码传递共享协同信号，OPQ编码学习物品差异化信息。",
    "translation": "SMILE：用于电商搜索点击率预测的语义ID增强冷门物品表示",
    "relevance_score": 9,
    "reasoning": "该论文直接针对电商搜索中的CTR预测问题，属于核心推荐系统领域。提出的语义ID增强方法通过改进冷启动物品的表示学习，可直接提升推荐系统的性能。这种表示学习技术还可扩展到搜索和广告中的新物品/商品冷启动场景。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统中的冷启动问题，提出基于语义ID的表示增强方法，与推荐系统核心进展和LLM技术应用高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12461v1": {
    "title": "Leveraging Language Semantics for Collaborative Filtering with TextGCN and TextGCN-MLP: Zero-Shot vs In-Domain Performance",
    "url": "https://www.alphaxiv.org/abs/2510.12461v1",
    "arxiv_id": "2510.12461v1",
    "authors": "Andrei Chernov, Haroon Wahab, Oleg Novitskij",
    "categories": "cs.IR",
    "pub_date": "2025-10-14 12:50:11",
    "ori_summary": "In recent years, various approaches have been proposed to leverage large language models (LLMs) for incorporating textual information about items into recommender systems. Existing methods primarily focus on either fine-tuning LLMs to generate recommendations or integrating LLM-based embeddings into downstream models. In this work, we follow the latter direction and propose \\textbf{TextGCN}, which applies parameter-free graph convolution layers directly over LLM-based item-title embeddings, instead of learning ID-based embeddings as in traditional methods. By combining language semantics with graph message passing, this architecture achieves state-of-the-art zero-shot performance, significantly outperforming prior approaches. Furthermore, we introduce \\textbf{TextGCN-MLP}, which extends TextGCN with a trainable multilayer perceptron trained using a contrastive loss, achieving state-of-the-art in-domain performance on recommendation benchmarks. However, the zero-shot performance of TextGCN-MLP remains lower than that of TextGCN, highlighting the trade-off between in-domain specialization and zero-shot generalization. We release our code on github at \\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}.",
    "summary": "研究如何利用语言语义改进协同过滤推荐系统；核心方法是直接对基于LLM的物品标题嵌入应用参数无关的图卷积层，替代传统基于ID的嵌入学习，并探索零样本泛化与领域内专业化之间的权衡。",
    "translation": "利用语言语义进行协同过滤：TextGCN与TextGCN-MLP的零样本与领域内性能对比",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及推荐系统的核心领域进展（协同过滤），并利用语言语义和图神经网络技术。TextGCN方法将文本语义建模为图结构，这种处理异构数据的方式与VLM类比思想相似，可应用于用户行为序列和内容特征的统一建模，对推荐系统有直接价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用LLM语义嵌入进行推荐系统建模，属于LLM在推荐领域的直接应用，并探索了零样本与领域内性能的权衡问题。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12369v1": {
    "title": "A Hierarchical Quantized Tokenization Framework for Task-Adaptive Graph Representation Learning",
    "url": "https://www.alphaxiv.org/abs/2510.12369v1",
    "arxiv_id": "2510.12369v1",
    "authors": "Yang Xiang, Li Fan, Chenke Yin, Chengtao Ji",
    "categories": "cs.IR",
    "pub_date": "2025-10-14 10:36:43",
    "ori_summary": "Recent progress in language and vision foundation models demonstrates the importance of discrete token interfaces that transform complex inputs into compact sequences for large-scale modeling. Extending this paradigm to graphs requires a tokenization scheme that handles non-Euclidean structures and multi-scale dependencies efficiently. Existing approaches to graph tokenization, linearized, continuous, and quantized, remain limited in adaptability and efficiency. In particular, most current quantization-based tokenizers organize hierarchical information in fixed or task-agnostic ways, which may either over-represent or under-utilize structural cues, and lack the ability to dynamically reweight contributions from different levels without retraining the encoder. This work presents a hierarchical quantization framework that introduces a self-weighted mechanism for task-adaptive aggregation across multiple scales. The proposed method maintains a frozen encoder while modulating information flow through a lightweight gating process, enabling parameter-efficient adaptation to diverse downstream tasks. Experiments on benchmark datasets for node classification and link prediction demonstrate consistent improvements over strong baselines under comparable computational budgets.",
    "summary": "",
    "translation": "面向任务自适应图表示学习的层次化量化分词框架",
    "relevance_score": 7,
    "reasoning": "该论文提出了一种层次化量化分词框架用于图表示学习，这属于Transformer架构的效率优化技术（量化分词），在推荐系统和搜索中有直接应用潜力。图表示学习是推荐系统的核心技术，用于建模用户-物品交互图，而量化技术可以显著提升大规模图神经网络在工业级推荐系统中的推理效率。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.12327v1": {
    "title": "Simple Projection Variants Improve ColBERT Performance",
    "url": "https://www.alphaxiv.org/abs/2510.12327v1",
    "arxiv_id": "2510.12327v1",
    "authors": "Benjamin Clavié, Sean Lee, Rikiya Takehi, Aamir Shakir, Makoto P. Kato",
    "categories": "cs.IR, cs.AI, cs.CL",
    "pub_date": "2025-10-14 09:34:05",
    "ori_summary": "Multi-vector dense retrieval methods like ColBERT systematically use a single-layer linear projection to reduce the dimensionality of individual vectors. In this study, we explore the implications of the MaxSim operator on the gradient flows of the training of multi-vector models and show that such a simple linear projection has inherent, if non-critical, limitations in this setting. We then discuss the theoretical improvements that could result from replacing this single-layer projection with well-studied alternative feedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU blocks, and skip-connections, could alleviate these limitations. Through the design and systematic evaluation of alternate projection blocks, we show that better-designed final projections positively impact the downstream performance of ColBERT models. We highlight that many projection variants outperform the original linear projections, with the best-performing variants increasing average performance on a range of retrieval benchmarks across domains by over 2 NDCG@10 points. We then conduct further exploration on the individual parameters of these projections block in order to understand what drives this empirical performance, highlighting the particular importance of upscaled intermediate projections and residual connections. As part of these ablation studies, we show that numerous suboptimal projection variants still outperform the traditional single-layer projection across multiple benchmarks, confirming our hypothesis. Finally, we observe that this effect is consistent across random seeds, further confirming that replacing the linear layer of ColBERT models is a robust, drop-in upgrade.",
    "summary": "论文研究多向量检索模型中线性投影层的局限性问题，核心思想是用更复杂的FFN网络结构（如深层非线性块、GLU块和残差连接）替代简单线性投影来改善梯度流和表示能力。",
    "translation": "简单投影变体提升ColBERT性能",
    "relevance_score": 8,
    "reasoning": "ColBERT是检索系统中广泛使用的密集检索模型，通过改进其投影机制可以显著提升检索质量。这种效率优化技术可以直接应用于搜索和推荐系统的语义匹配任务，提高检索准确性和响应速度。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文通过改进ColBERT的多向量检索模型投影层设计，直接提升检索系统性能，属于搜索领域的核心算法优化。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12325v1": {
    "title": "Causal Inspired Multi Modal Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.12325v1",
    "arxiv_id": "2510.12325v1",
    "authors": "Jie Yang, Chenyang Gu, Zixuan Liu",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-14 09:29:07",
    "ori_summary": "Multimodal recommender systems enhance personalized recommendations in e-commerce and online advertising by integrating visual, textual, and user-item interaction data. However, existing methods often overlook two critical biases: (i) modal confounding, where latent factors (e.g., brand style or product category) simultaneously drive multiple modalities and influence user preference, leading to spurious feature-preference associations; (ii) interaction bias, where genuine user preferences are mixed with noise from exposure effects and accidental clicks. To address these challenges, we propose a Causal-inspired multimodal Recommendation framework. Specifically, we introduce a dual-channel cross-modal diffusion module to identify hidden modal confounders, utilize back-door adjustment with hierarchical matching and vector-quantized codebooks to block confounding paths, and apply front-door adjustment combined with causal topology reconstruction to build a deconfounded causal subgraph. Extensive experiments on three real-world e-commerce datasets demonstrate that our method significantly outperforms state-of-the-art baselines while maintaining strong interpretability.",
    "summary": "论文研究多模态推荐系统中的模态混淆和交互偏差问题，核心方法是引入双通道跨模态扩散模块识别隐藏混淆因子，通过后门调整和前门调整构建去混淆因果子图来解决虚假特征-偏好关联。",
    "translation": "因果启发的多模态推荐",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及多模态推荐系统，属于核心推荐系统领域的进展。因果推理方法可以显著提升推荐系统的可解释性和鲁棒性，通过理解用户行为与推荐结果之间的因果关系来减少偏差。这种方法在搜索和广告系统中同样具有应用潜力，能够改善用户意图理解和内容相关性建模。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统中的核心偏差问题，提出因果启发的多模态解耦方法，与推荐系统核心进展和LLM在推荐中的应用高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12299v1": {
    "title": "An Empirical Study for Representations of Videos in Video Question Answering via MLLMs",
    "url": "https://www.alphaxiv.org/abs/2510.12299v1",
    "arxiv_id": "2510.12299v1",
    "authors": "Zhi Li, Yanan Wang, Hao Niu, Julio Vizcarra, Masato Taya",
    "categories": "cs.IR, I.2.10",
    "pub_date": "2025-10-14 09:02:22",
    "ori_summary": "Multimodal large language models have recently achieved remarkable progress in video question answering (VideoQA) by jointly processing visual, textual, and audio information. However, it remains unclear which video representations are most effective for MLLMs, and how different modalities balance task accuracy against computational efficiency. In this work, we present a comprehensive empirical study of video representation methods for VideoQA with MLLMs. We systematically evaluate single modality inputs question only, subtitles, visual frames, and audio signals as well as multimodal combinations, on two widely used benchmarks: VideoMME and LongVideoBench. Our results show that visual frames substantially enhance accuracy but impose heavy costs in GPU memory and inference latency, while subtitles provide a lightweight yet effective alternative, particularly for long videos. These findings highlight clear trade-offs between effectiveness and efficiency and provide practical insights for designing resource-aware MLLM-based VideoQA systems.",
    "summary": "",
    "translation": "基于多模态大语言模型的视频问答中视频表征的实证研究",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频问答任务，属于纯粹的视觉-语言多模态研究领域。虽然涉及多模态大语言模型技术，但其应用场景（视频问答）与推荐系统、搜索或广告的核心业务需求没有直接关联，且缺乏明确的跨模态建模技术迁移路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12211v1": {
    "title": "Reinforced Preference Optimization for Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.12211v1",
    "arxiv_id": "2510.12211v1",
    "authors": "Junfei Tan, Yuxin Chen, An Zhang, Junguang Jiang, Bin Liu, Ziru Xu, Han Zhu, Jian Xu, Bo Zheng, Xiang Wang",
    "categories": "cs.IR",
    "pub_date": "2025-10-14 07:04:33",
    "ori_summary": "Recent breakthroughs in large language models (LLMs) have fundamentally shifted recommender systems from discriminative to generative paradigms, where user behavior modeling is achieved by generating target items conditioned on historical interactions. Yet current generative recommenders still suffer from two core limitations: the lack of high-quality negative modeling and the reliance on implicit rewards. Reinforcement learning with verifiable rewards (RLVR) offers a natural solution by enabling on-policy sampling of harder negatives and grounding optimization in explicit reward signals. However, applying RLVR to generative recommenders remains non-trivial. Its unique generation space often leads to invalid or repetitive items that undermine sampling efficiency, and ranking supervision is sparse since most items receive identical zero rewards. To address these challenges, we propose Reinforced Preference Optimization for Recommendation (ReRe), a reinforcement-based paradigm tailored to LLM-based recommenders, an important direction in generative recommendation. ReRe incorporates constrained beam search to improve sampling efficiency and diversify hard negatives, while augmenting rule-based accuracy rewards with auxiliary ranking rewards for finer-grained supervision. Extensive experiments on three real-world datasets demonstrate that ReRe consistently outperforms both traditional and LLM-based recommenders in ranking performance. Further analysis shows that ReRe not only enhances performance across both base and SFT-initialized models but also generalizes robustly across different backbone families and scales. Beyond empirical gains, we systematically investigate the design space of RLVR in recommendation across generation, sampling strategy, reward modeling, and optimization algorithm, offering insights for future research.",
    "summary": "论文研究生成式推荐系统中缺乏高质量负样本建模和依赖隐式奖励的问题，核心思想是设计强化偏好优化框架，通过约束束搜索改进采样效率和多样化硬负样本，同时增强基于规则的准确性奖励与辅助排序奖励。",
    "translation": "基于强化学习的推荐系统偏好优化",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及推荐系统的核心算法优化，属于Core Domain Advances范畴。虽然标题提到强化学习，但明确应用于推荐场景，具有直接的实际应用价值。强化学习偏好优化可以显著提升推荐系统的个性化效果和长期用户满意度。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM推荐系统的核心挑战，提出强化学习优化框架，完美契合生成式推荐和LLM应用的研究方向。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12054v1": {
    "title": "MIARec: Mutual-influence-aware Heterogeneous Network Embedding for Scientific Paper Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.12054v1",
    "arxiv_id": "2510.12054v1",
    "authors": "Wenjin Xie, Tao Jia",
    "categories": "cs.IR, cs.LG",
    "pub_date": "2025-10-14 01:47:25",
    "ori_summary": "With the rapid expansion of scientific literature, scholars increasingly demand precise and high-quality paper recommendations. Among various recommendation methodologies, graph-based approaches have garnered attention by effectively exploiting the structural characteristics inherent in scholarly networks. However, these methods often overlook the asymmetric academic influence that is prevalent in scholarly networks when learning graph representations. To address this limitation, this study proposes the Mutual-Influence-Aware Recommendation (MIARec) model, which employs a gravity-based approach to measure the mutual academic influence between scholars and incorporates this influence into the feature aggregation process during message propagation in graph representation learning. Additionally, the model utilizes a multi-channel aggregation method to capture both individual embeddings of distinct single relational sub-networks and their interdependent embeddings, thereby enabling a more comprehensive understanding of the heterogeneous scholarly network. Extensive experiments conducted on real-world datasets demonstrate that the MIARec model outperforms baseline models across three primary evaluation metrics, indicating its effectiveness in scientific paper recommendation tasks.",
    "summary": "论文研究学术论文推荐中忽视学术影响力不对称的问题，核心方法是利用重力模型量化学者间相互学术影响力，并采用多通道聚合机制学习异质学术网络的单关系和跨关系嵌入。",
    "translation": "MIARec：面向科学论文推荐的互影响感知异质网络嵌入",
    "relevance_score": 8,
    "reasoning": "该论文直接针对推荐系统（RecSys）领域，专注于科学论文推荐这一具体应用场景。其提出的异质网络嵌入方法和互影响感知机制属于推荐系统的核心算法创新，可直接应用于构建更精准的推荐模型，符合核心领域进展的焦点要求。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对推荐系统核心问题，提出基于异构图嵌入的学术论文推荐方法，并引入重力模型处理学术影响力不对称性，与推荐系统领域高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12784v1": {
    "title": "SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models",
    "url": "https://www.alphaxiv.org/abs/2510.12784v1",
    "arxiv_id": "2510.12784v1",
    "authors": "Weiyang Jin, Yuwei Niu, Jiaqi Liao, Chengqi Duan, Aoxue Li, Shenghua Gao, Xihui Liu",
    "categories": "cs.CV, cs.CL, I.4.0",
    "pub_date": "2025-10-14 17:56:11",
    "ori_summary": "Recently, remarkable progress has been made in Unified Multimodal Models (UMMs), which integrate vision-language generation and understanding capabilities within a single framework. However, a significant gap exists where a model's strong visual understanding often fails to transfer to its visual generation. A model might correctly understand an image based on user instructions, yet be unable to generate a faithful image from text prompts. This phenomenon directly raises a compelling question: Can a model achieve self-improvement by using its understanding module to reward its generation module? To bridge this gap and achieve self-improvement, we introduce SRUM, a self-rewarding post-training framework that can be directly applied to existing UMMs of various designs. SRUM creates a feedback loop where the model's own understanding module acts as an internal ``evaluator'', providing corrective signals to improve its generation module, without requiring additional human-labeled data. To ensure this feedback is comprehensive, we designed a global-local dual reward system. To tackle the inherent structural complexity of images, this system offers multi-scale guidance: a \\textbf{global reward} ensures the correctness of the overall visual semantics and layout, while a \\textbf{local reward} refines fine-grained, object-level fidelity. SRUM leads to powerful capabilities and shows strong generalization, boosting performance on T2I-CompBench from 82.18 to \\textbf{88.37} and on T2I-ReasonBench from 43.82 to \\textbf{46.75}. Overall, our work establishes a powerful new paradigm for enabling a UMMs' understanding module to guide and enhance its own generation via self-rewarding.",
    "summary": "该论文研究统一多模态模型中视觉理解与生成能力不匹配的问题，核心思想是设计自奖励后训练框架，让模型的理解模块作为内部评估器，通过全局-局部双奖励系统提供多尺度指导信号来改进生成模块。",
    "translation": "SRUM：统一多模态模型的细粒度自奖励机制",
    "relevance_score": 8,
    "reasoning": "该论文提出的细粒度自奖励机制属于核心LLM技术进展，能够显著提升多模态模型的训练效率和性能。这种自奖励技术在推荐系统和搜索领域具有直接应用潜力，可用于优化用户行为序列建模、多模态特征对齐以及个性化推荐的质量评估。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出的自奖励框架和全局-局部双奖励系统直接适用于多模态推荐系统，其利用理解模块指导生成模块的核心思想与VLM异构数据统一建模高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12781v1": {
    "title": "Cost Analysis of Human-corrected Transcription for Predominately Oral Languages",
    "url": "https://www.alphaxiv.org/abs/2510.12781v1",
    "arxiv_id": "2510.12781v1",
    "authors": "Yacouba Diarra, Nouhoum Souleymane Coulibaly, Michael Leventhal",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 17:53:11",
    "ori_summary": "Creating speech datasets for low-resource languages is a critical yet poorly understood challenge, particularly regarding the actual cost in human labor. This paper investigates the time and complexity required to produce high-quality annotated speech data for a subset of low-resource languages, low literacy Predominately Oral Languages, focusing on Bambara, a Manding language of Mali. Through a one-month field study involving ten transcribers with native proficiency, we analyze the correction of ASR-generated transcriptions of 53 hours of Bambara voice data. We report that it takes, on average, 30 hours of human labor to accurately transcribe one hour of speech data under laboratory conditions and 36 hours under field conditions. The study provides a baseline and practical insights for a large class of languages with comparable profiles undertaking the creation of NLP resources.",
    "summary": "",
    "translation": "主要口语语言的人工校正转录成本分析",
    "relevance_score": 1,
    "reasoning": "该论文关注口语语言转录的人工校正成本分析，这属于语音处理领域的具体应用问题。该主题与推荐系统、搜索或广告的核心技术进展、LLM赋能技术或Transformer架构改进均无直接关联，也不涉及异构数据的统一建模方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12780v1": {
    "title": "Content Anonymization for Privacy in Long-form Audio",
    "url": "https://www.alphaxiv.org/abs/2510.12780v1",
    "arxiv_id": "2510.12780v1",
    "authors": "Cristina Aggazzotti, Ashi Garg, Zexin Cai, Nicholas Andrews",
    "categories": "cs.SD, cs.CL",
    "pub_date": "2025-10-14 17:52:50",
    "ori_summary": "Voice anonymization techniques have been found to successfully obscure a speaker's acoustic identity in short, isolated utterances in benchmarks such as the VoicePrivacy Challenge. In practice, however, utterances seldom occur in isolation: long-form audio is commonplace in domains such as interviews, phone calls, and meetings. In these cases, many utterances from the same speaker are available, which pose a significantly greater privacy risk: given multiple utterances from the same speaker, an attacker could exploit an individual's vocabulary, syntax, and turns of phrase to re-identify them, even when their voice is completely disguised. To address this risk, we propose new content anonymization approaches. Our approach performs a contextual rewriting of the transcripts in an ASR-TTS pipeline to eliminate speaker-specific style while preserving meaning. We present results in a long-form telephone conversation setting demonstrating the effectiveness of a content-based attack on voice-anonymized speech. Then we show how the proposed content-based anonymization methods can mitigate this risk while preserving speech utility. Overall, we find that paraphrasing is an effective defense against content-based attacks and recommend that stakeholders adopt this step to ensure anonymity in long-form audio.",
    "summary": "",
    "translation": "面向长音频隐私保护的内容匿名化",
    "relevance_score": 1,
    "reasoning": "该论文专注于音频隐私保护技术，属于明确的无关主题范畴。内容匿名化主要用于隐私保护目的，与推荐系统、搜索或广告的核心技术进展、LLM技术应用或Transformer架构改进没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12773v1": {
    "title": "Dr.LLM: Dynamic Layer Routing in LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.12773v1",
    "arxiv_id": "2510.12773v1",
    "authors": "Ahmed Heakl, Martin Gubri, Salman Khan, Sangdoo Yun, Seong Joon Oh",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-14 17:51:26",
    "ori_summary": "Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inference-time search, architectural changes, or large-scale retraining, and in practice often degrade accuracy despite efficiency gains. We introduce Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that equips pretrained models with lightweight per-layer routers deciding to skip, execute, or repeat a block. Routers are trained with explicit supervision: using Monte Carlo Tree Search (MCTS), we derive high-quality layer configurations that preserve or improve accuracy under a compute budget. Our design, windowed pooling for stable routing, focal loss with class balancing, and bottleneck MLP routers, ensures robustness under class imbalance and long sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to +3.4%p while saving 5 layers per example on average. Routers generalize to out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval) with only 0.85% accuracy drop while retaining efficiency, and outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that explicitly supervised routers retrofit frozen LLMs for budget-aware, accuracy-driven inference without altering base weights.",
    "summary": "该论文研究LLM处理所有token时计算资源浪费的问题，核心方法是为预训练模型配备轻量级路由器，动态决定跳过、执行或重复Transformer层，在计算预算下保持或提升准确性。",
    "translation": "Dr.LLM：大语言模型中的动态层路由",
    "relevance_score": 8,
    "reasoning": "该论文涉及Transformer架构效率优化（动态层路由），属于'Enabling Transformer Tech'范畴。动态路由机制可以显著降低LLM推理成本，这对于需要实时响应的推荐和搜索系统至关重要，能够实现更高效的模型部署和更快的用户请求处理。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出的动态层路由机制直接针对Transformer架构效率优化，属于核心的Transformer技术进步，且可应用于搜索推荐系统的推理加速。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12766v1": {
    "title": "Language Models Model Language",
    "url": "https://www.alphaxiv.org/abs/2510.12766v1",
    "arxiv_id": "2510.12766v1",
    "authors": "Łukasz Borchmann",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 17:45:31",
    "ori_summary": "Linguistic commentary on LLMs, heavily influenced by the theoretical frameworks of de Saussure and Chomsky, is often speculative and unproductive. Critics challenge whether LLMs can legitimately model language, citing the need for \"deep structure\" or \"grounding\" to achieve an idealized linguistic \"competence.\" We argue for a radical shift in perspective towards the empiricist principles of Witold Ma\\'nczak, a prominent general and historical linguist. He defines language not as a \"system of signs\" or a \"computational system of the brain\" but as the totality of all that is said and written. Above all, he identifies frequency of use of particular language elements as language's primary governing principle. Using his framework, we challenge prior critiques of LLMs and provide a constructive guide for designing, evaluating, and interpreting language models.",
    "summary": "",
    "translation": "语言模型建模语言",
    "relevance_score": 2,
    "reasoning": "该标题讨论语言模型的基本语言建模能力，属于纯粹的LLM理论研究。虽然涉及核心LLM技术，但缺乏明确的推荐系统、搜索或广告应用潜力，更偏向基础NLP研究而非实际应用导向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12740v1": {
    "title": "Hey, wait a minute: on at-issue sensitivity in Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.12740v1",
    "arxiv_id": "2510.12740v1",
    "authors": "Sanghee J. Kim, Kanishka Misra",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 17:17:20",
    "ori_summary": "Evaluating the naturalness of dialogue in language models (LMs) is not trivial: notions of 'naturalness' vary, and scalable quantitative metrics remain limited. This study leverages the linguistic notion of 'at-issueness' to assess dialogue naturalness and introduces a new method: Divide, Generate, Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii) generates continuations for subparts using LMs, (iii) recombines the dialogue and continuations, and (iv) compares the likelihoods of the recombined sequences. This approach mitigates bias in linguistic analyses of LMs and enables systematic testing of discourse-sensitive behavior. Applying DGRC, we find that LMs prefer to continue dialogue on at-issue content, with this effect enhanced in instruct-tuned models. They also reduce their at-issue preference when relevant cues (e.g., \"Hey, wait a minute\") are present. Although instruct-tuning does not further amplify this modulation, the pattern reflects a hallmark of successful dialogue dynamics.",
    "summary": "",
    "translation": "嘿，稍等一下：论语言模型中的议题敏感性",
    "relevance_score": 2,
    "reasoning": "这篇论文主要研究语言模型中的议题敏感性，这属于纯NLP领域的语言学分析，与推荐系统、搜索或广告的核心技术没有直接关联。虽然语言理解能力可能间接影响这些领域，但论文焦点过于偏向基础NLP理论，缺乏明确的实际应用场景或技术改进方案。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12722v1": {
    "title": "Which Word Orders Facilitate Length Generalization in LMs? An Investigation with GCG-Based Artificial Languages",
    "url": "https://www.alphaxiv.org/abs/2510.12722v1",
    "arxiv_id": "2510.12722v1",
    "authors": "Nadine El-Naggar, Tatsuki Kuribayashi, Ted Briscoe",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 17:00:19",
    "ori_summary": "Whether language models (LMs) have inductive biases that favor typologically frequent grammatical properties over rare, implausible ones has been investigated, typically using artificial languages (ALs) (White and Cotterell, 2021; Kuribayashi et al., 2024). In this paper, we extend these works from two perspectives. First, we extend their context-free AL formalization by adopting Generalized Categorial Grammar (GCG) (Wood, 2014), which allows ALs to cover attested but previously overlooked constructions, such as unbounded dependency and mildly context-sensitive structures. Second, our evaluation focuses more on the generalization ability of LMs to process unseen longer test sentences. Thus, our ALs better capture features of natural languages and our experimental paradigm leads to clearer conclusions -- typologically plausible word orders tend to be easier for LMs to productively generalize.",
    "summary": "",
    "translation": "哪些词序促进语言模型中的长度泛化？基于GCG的人工语言研究",
    "relevance_score": 2,
    "reasoning": "该论文主要研究语言模型的长度泛化能力和词序影响，属于基础NLP机制研究。虽然涉及语言模型架构，但缺乏明确的推荐系统、搜索或广告应用场景，且研究重点偏向纯语言理解而非实际工业应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12720v1": {
    "title": "Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception",
    "url": "https://www.alphaxiv.org/abs/2510.12720v1",
    "arxiv_id": "2510.12720v1",
    "authors": "Ziyang Ma, Ruiyang Xu, Zhenghao Xing, Yunfei Chu, Yuxuan Wang, Jinzheng He, Jin Xu, Pheng-Ann Heng, Kai Yu, Junyang Lin, Eng Siong Chng, Xie Chen",
    "categories": "cs.CL, cs.CV, cs.MM, cs.SD",
    "pub_date": "2025-10-14 17:00:09",
    "ori_summary": "Fine-grained perception of multimodal information is critical for advancing human-AI interaction. With recent progress in audio-visual technologies, Omni Language Models (OLMs), capable of processing audio and video signals in parallel, have emerged as a promising paradigm for achieving richer understanding and reasoning. However, their capacity to capture and describe fine-grained details remains limited explored. In this work, we present a systematic and comprehensive investigation of omni detailed perception from the perspectives of the data pipeline, models, and benchmark. We first identify an inherent \"co-growth\" between detail and hallucination in current OLMs. To address this, we propose Omni-Detective, an agentic data generation pipeline integrating tool-calling, to autonomously produce highly detailed yet minimally hallucinatory multimodal data. Based on the data generated with Omni-Detective, we train two captioning models: Audio-Captioner for audio-only detailed perception, and Omni-Captioner for audio-visual detailed perception. Under the cascade evaluation protocol, Audio-Captioner achieves the best performance on MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and delivering performance comparable to Gemini 2.5 Pro. On existing detailed captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and achieves the best trade-off between detail and hallucination on the video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for detailed audio, visual, and audio-visual captioning that ensures stable, efficient, and reliable assessment. Experimental results and analysis demonstrate the effectiveness of Omni-Detective in generating high-quality detailed captions, as well as the superiority of Omni-Cloze in evaluating such detailed captions.",
    "summary": "",
    "translation": "全能描述器：面向全方位细节感知的数据流水线、模型与基准",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉细节感知和图像描述任务，属于视觉-语言模型(VLM)范畴，但与推荐系统、搜索或广告的直接关联较弱。虽然VLM技术可能为处理异构数据提供灵感，但论文标题未明确显示其在RecSys/Search/Ads领域的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12699v1": {
    "title": "Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations",
    "url": "https://www.alphaxiv.org/abs/2510.12699v1",
    "arxiv_id": "2510.12699v1",
    "authors": "Sunny Yu, Ahmad Jabbar, Robert Hawkins, Dan Jurafsky, Myra Cheng",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 16:31:34",
    "ori_summary": "Different open-ended generation tasks require different degrees of output diversity. However, current LLMs are often miscalibrated. They collapse to overly homogeneous outputs for creative tasks and hallucinate diverse but incorrect responses for factual tasks. We argue that these two failure modes are unified by, and can both be addressed by, the notion of effective generation space size (GSS) -- the set of semantically distinct outputs a model considers for a prompt. We present GSSBench, a task suite of prompt pairs with ground-truth GSS relationships to assess different metrics and understand where models diverge from desired behavior. We find that hallucination detection metrics, particularly EigenScore, consistently outperform standard diversity and uncertainty quantification metrics, while using only model internals, providing interpretable insights into a model's internal task representations. We demonstrate three applications of GSS: (1) detecting prompt ambiguity and predicting clarification questions for better grounding, (2) interpreting overthinking and underthinking in reasoning models, and (3) steering models to expand their generation space to yield high-quality and diverse outputs.",
    "summary": "",
    "translation": "生成空间大小：理解和校准大语言模型生成结果的开放性",
    "relevance_score": 3,
    "reasoning": "该论文主要关注LLM生成结果的开放性和校准问题，这属于纯粹的LLM评估和可控生成范畴。虽然LLM校准在理论上可能间接影响推荐/搜索系统中的生成质量，但论文没有明确展示与RecSys/Search/Ads领域的直接应用联系，且更偏向于NLP评估基准问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12692v1": {
    "title": "Who is a Better Matchmaker? Human vs. Algorithmic Judge Assignment in a High-Stakes Startup Competition",
    "url": "https://www.alphaxiv.org/abs/2510.12692v1",
    "arxiv_id": "2510.12692v1",
    "authors": "Sarina Xi, Orelia Pi, Miaomiao Zhang, Becca Xiong, Jacqueline Ng Lane, Nihar B. Shah",
    "categories": "cs.HC, cs.AI, cs.CL, cs.CY, cs.LG",
    "pub_date": "2025-10-14 16:25:09",
    "ori_summary": "There is growing interest in applying artificial intelligence (AI) to automate and support complex decision-making tasks. However, it remains unclear how algorithms compare to human judgment in contexts requiring semantic understanding and domain expertise. We examine this in the context of the judge assignment problem, matching submissions to suitably qualified judges. Specifically, we tackled this problem at the Harvard President's Innovation Challenge, the university's premier venture competition awarding over \\$500,000 to student and alumni startups. This represents a real-world environment where high-quality judge assignment is essential. We developed an AI-based judge-assignment algorithm, Hybrid Lexical-Semantic Similarity Ensemble (HLSE), and deployed it at the competition. We then evaluated its performance against human expert assignments using blinded match-quality scores from judges on $309$ judge-venture pairs. Using a Mann-Whitney U statistic based test, we found no statistically significant difference in assignment quality between the two approaches ($AUC=0.48, p=0.40$); on average, algorithmic matches are rated $3.90$ and manual matches $3.94$ on a 5-point scale, where 5 indicates an excellent match. Furthermore, manual assignments that previously required a full week could be automated in several hours by the algorithm during deployment. These results demonstrate that HLSE achieves human-expert-level matching quality while offering greater scalability and efficiency, underscoring the potential of AI-driven solutions to support and enhance human decision-making for judge assignment in high-stakes settings.",
    "summary": "",
    "translation": "谁是更好的匹配者？高风险创业竞赛中人类与算法评委分配的比较",
    "relevance_score": 2,
    "reasoning": "该论文主要研究人类与算法在评委分配中的比较，这属于匹配算法在特定竞赛场景的应用。虽然涉及算法匹配，但缺乏与推荐系统、搜索或广告的直接关联，且不涉及LLM、Transformer架构或异构数据建模等核心技术。其应用场景过于特定，通用性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12680v1": {
    "title": "Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and No-Think?",
    "url": "https://www.alphaxiv.org/abs/2510.12680v1",
    "arxiv_id": "2510.12680v1",
    "authors": "Shouren Wang, Wang Yang, Xianxuan Long, Qifan Wang, Vipin Chaudhary, Xiaotian Han",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-14 16:19:44",
    "ori_summary": "Hybrid thinking enables LLMs to switch between reasoning and direct answering, offering a balance between efficiency and reasoning capability. Yet our experiments reveal that current hybrid thinking LLMs only achieve partial mode separation: reasoning behaviors often leak into the no-think mode. To understand and mitigate this, we analyze the factors influencing controllability and identify four that matter most: (1) larger data scale, (2) using think and no-think answers from different questions rather than the same question, (3) a moderate increase in no-think data number, and (4) a two-phase strategy that first trains reasoning ability and then applies hybrid think training. Building on these findings, we propose a practical recipe that, compared to standard training, can maintain accuracy in both modes while significantly reducing no-think output length (from $1085$ to $585$ on MATH500) and occurrences of reasoning-supportive tokens such as ``\\texttt{wait}'' (from $5917$ to $522$ on MATH500). Our findings highlight the limitations of current hybrid thinking and offer directions for strengthening its controllability.",
    "summary": "",
    "translation": "揭秘混合思维：大型语言模型能否真正在思考与非思考模式间切换？",
    "relevance_score": 3,
    "reasoning": "该论文主要探讨LLM的思维模式切换能力，属于LLM基础能力研究。虽然涉及LLM内部工作机制，但缺乏明确的推荐系统、搜索或广告应用场景的直接关联。对于'赋能LLM技术'类别，需要说明其在RecSys/Search/Ads中的潜在应用，但该标题未提供足够信息来判断这种应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12643v1": {
    "title": "Reasoning Pattern Matters: Learning to Reason without Human Rationales",
    "url": "https://www.alphaxiv.org/abs/2510.12643v1",
    "arxiv_id": "2510.12643v1",
    "authors": "Chaoxu Pang, Yixuan Cao, Ping Luo",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 15:34:38",
    "ori_summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities under the widely adopted SFT+RLVR paradigm, which first performs Supervised Fine-Tuning (SFT) on human-annotated reasoning trajectories (rationales) to establish initial reasoning behaviors, then applies Reinforcement Learning with Verifiable Rewards (RLVR) to optimize the model using verifiable signals without golden rationales. However, annotating high-quality rationales for the SFT stage remains prohibitively expensive. This paper investigates when and how rationale annotation costs can be substantially reduced without compromising reasoning performance. We identify a broad class of problems, termed patterned reasoning tasks, where reasoning follows a fixed, procedural strategy consistent across instances. Although instances vary in content such as domain knowledge, factual information, or numeric values, the solution derives from applying a shared reasoning pattern. We argue that the success of SFT+RLVR on such tasks primarily stems from its ability to enable models to internalize these reasoning patterns. Using numerical semantic matching as a representative task, we provide both causal and behavioral evidence showing that reasoning patterns rather than the quantity or quality of rationales are the key determinant of performance. Building on these insights, we propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet effective framework that enables LLMs to generate rationales aligned with task-specific reasoning patterns without requiring human rationale annotations. Experiments show that PARO-generated rationales achieve comparable SFT+RLVR performance to human rationales that are 10 times larger. These results suggest that large-scale human rationale annotations can be replaced with LLM-based automatic annotations requiring only limited human supervision over reasoning patterns.",
    "summary": "该论文研究如何在不依赖昂贵人工标注的情况下训练LLM的推理能力。核心思想是识别任务中的固定推理模式，让LLM基于这些模式自动生成训练所需的推理轨迹，从而替代大规模人工标注。",
    "translation": "推理模式至关重要：无需人类理性指导的推理学习",
    "relevance_score": 8,
    "reasoning": "该论文关注推理模式学习，这属于核心LLM技术进展。改进的推理能力可直接应用于搜索中的复杂查询理解、推荐系统中的多步推理决策以及广告中的用户意图深度分析，提升这些领域任务的处理精度和鲁棒性。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM在推荐搜索领域的关键瓶颈——人工标注成本，提出了基于推理模式自动生成标注的方法，属于核心LLM技术进步。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12637v1": {
    "title": "COSTAR-A: A prompting framework for enhancing Large Language Model performance on Point-of-View questions",
    "url": "https://www.alphaxiv.org/abs/2510.12637v1",
    "arxiv_id": "2510.12637v1",
    "authors": "Nzubechukwu C. Ohalete, Kevin B. Gittner, Lauren M. Matheny",
    "categories": "cs.CL, I.2.7",
    "pub_date": "2025-10-14 15:31:21",
    "ori_summary": "Large Language Models (LLMs) are highly sensitive to prompt design, and making optimized prompting techniques is crucial for generating consistent, high-quality outputs. In this study, we introduce COSTAR-A, a novel prompt engineering framework that enhances the existing COSTAR method, which stands for Context, Objective, Style, Tone, Audience, and Response, by adding the 'Answer' component at the end. We demonstrate that while the original COSTAR framework improves prompt clarity and aligns outputs for larger LLMs, its performance is less consistent with smaller, locally optimized models, particularly in tasks that require more directive or constrained outputs. Through a series of controlled prompt-output assessments with smaller (at most 8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance the output structure and decisiveness of localized LLMs for certain tasks, although its effectiveness varies across models and use cases. Notably, the Llama 3.1-8B model exhibited performance improvements when prompted with COSTAR-A compared to COSTAR alone. These findings emphasize the adaptability and scalability of COSTAR-A as a prompting framework, particularly in computationally efficient AI deployments on resource-constrained hardware.",
    "summary": "",
    "translation": "COSTAR-A：一种用于提升大型语言模型在观点类问题上性能的提示框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注提示工程和LLM在观点类问题上的性能提升，这属于纯粹的LLM优化技术。虽然提示框架可能间接影响搜索中的问答质量，但缺乏明确的推荐系统、搜索或广告应用场景，且不涉及核心架构创新或异构数据处理。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12621v1": {
    "title": "ACADATA: Parallel Dataset of Academic Data for Machine Translation",
    "url": "https://www.alphaxiv.org/abs/2510.12621v1",
    "arxiv_id": "2510.12621v1",
    "authors": "Iñaki Lacunza, Javier Garcia Gilabert, Francesca De Luca Fornaciari, Javier Aula-Blasco, Aitor Gonzalez-Agirre, Maite Melero, Marta Villegas",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 15:20:06",
    "ori_summary": "We present ACADATA, a high-quality parallel dataset for academic translation, that consists of two subsets: ACAD-TRAIN, which contains approximately 1.5 million author-generated paragraph pairs across 96 language directions and ACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12 directions. To validate its utility, we fine-tune two Large Language Models (LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized machine-translation systems, general-purpose, open-weight LLMs, and several large-scale proprietary models. Experimental results demonstrate that fine-tuning on ACAD-TRAIN leads to improvements in academic translation quality by +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively, while also improving long-context translation in a general domain by up to 24.9% when translating out of English. The fine-tuned top-performing model surpasses the best propietary and open-weight models on academic translation domain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we provide the community with a valuable resource to advance research in academic domain and long-context translation.",
    "summary": "",
    "translation": "ACADATA：用于机器翻译的学术数据并行数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器翻译的学术数据集构建，属于纯粹的NLP数据工程领域。论文内容与推荐系统、搜索、广告的核心技术进展或LLM/Transformer架构改进无直接关联，也没有展示在异构数据统一建模方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12608v1": {
    "title": "StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis",
    "url": "https://www.alphaxiv.org/abs/2510.12608v1",
    "arxiv_id": "2510.12608v1",
    "authors": "Siyuan Li, Aodu Wulianghai, Xi Lin, Guangyan Li, Xiang Chen, Jun Wu, Jianhua Li",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 15:07:27",
    "ori_summary": "With the increasing integration of large language models (LLMs) into open-domain writing, detecting machine-generated text has become a critical task for ensuring content authenticity and trust. Existing approaches rely on statistical discrepancies or model-specific heuristics to distinguish between LLM-generated and human-written text. However, these methods struggle in real-world scenarios due to limited generalization, vulnerability to paraphrasing, and lack of explainability, particularly when facing stylistic diversity or hybrid human-AI authorship. In this work, we propose StyleDecipher, a robust and explainable detection framework that revisits LLM-generated text detection using combined feature extractors to quantify stylistic differences. By jointly modeling discrete stylistic indicators and continuous stylistic representations derived from semantic embeddings, StyleDecipher captures distinctive style-level divergences between human and LLM outputs within a unified representation space. This framework enables accurate, explainable, and domain-agnostic detection without requiring access to model internals or labeled segments. Extensive experiments across five diverse domains, including news, code, essays, reviews, and academic abstracts, demonstrate that StyleDecipher consistently achieves state-of-the-art in-domain accuracy. Moreover, in cross-domain evaluations, it surpasses existing baselines by up to 36.30%, while maintaining robustness against adversarial perturbations and mixed human-AI content. Further qualitative and quantitative analysis confirms that stylistic signals provide explainable evidence for distinguishing machine-generated text. Our source code can be accessed at https://github.com/SiyuanLi00/StyleDecipher.",
    "summary": "",
    "translation": "StyleDecipher：基于风格分析的鲁棒且可解释的LLM生成文本检测方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM生成文本的检测和识别，属于内容真实性验证范畴，与推荐系统、搜索或广告的核心排序和匹配任务关联度较低。虽然检测技术可能间接应用于内容质量评估，但论文焦点更偏向安全性和内容验证，而非直接提升推荐、搜索或广告系统的核心性能。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12603v1": {
    "title": "Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space",
    "url": "https://www.alphaxiv.org/abs/2510.12603v1",
    "arxiv_id": "2510.12603v1",
    "authors": "Chao Chen, Zhixin Ma, Yongqi Li, Yupeng Hu, Yinwei Wei, Wenjie Li, Liqiang Nie",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-10-14 14:58:25",
    "ori_summary": "Multimodal reasoning aims to enhance the capabilities of MLLMs by incorporating intermediate reasoning steps before reaching the final answer. It has evolved from text-only reasoning to the integration of visual information, enabling the thought process to be conveyed through both images and text. Despite its effectiveness, current multimodal reasoning methods depend on explicit reasoning steps that require labor-intensive vision-text annotations and inherently introduce significant inference latency. To address these issues, we introduce multimodal latent reasoning with the advantages of multimodal representation, reduced annotation, and inference efficiency. To facilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR), which injects both visual and textual information in the reasoning process within the latent space. Specifically, IVT-LR represents each reasoning step by combining two implicit parts: latent text (the hidden states from the previous step) and latent vision (a set of selected image embeddings). We further introduce a progressive multi-stage training strategy to enable MLLMs to perform the above multimodal latent reasoning steps. Experiments on M3CoT and ScienceQA demonstrate that our IVT-LR method achieves an average performance increase of 5.45% in accuracy, while simultaneously achieving a speed increase of over 5 times compared to existing approaches. Code available at https://github.com/FYYDCC/IVT-LR.",
    "summary": "",
    "translation": "暗处推理：潜在空间中的交错视觉-文本推理",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视觉-文本模态的联合推理，属于多模态学习范畴。虽然视觉-语言模型（VLM）的类比在异构数据处理方面有潜在相关性，但该论文更侧重于纯粹的视觉-文本推理机制，而非明确的推荐/搜索/广告应用。对于推荐系统，这种潜在空间推理可能应用于处理商品图像和文本描述的异构特征，但连接不够直接。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12587v1": {
    "title": "Teaching Language Models to Faithfully Express their Uncertainty",
    "url": "https://www.alphaxiv.org/abs/2510.12587v1",
    "arxiv_id": "2510.12587v1",
    "authors": "Bryan Eikema, Evgenia Ilia, José G. C. de Souza, Chrysoula Zerva, Wilker Aziz",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 14:42:40",
    "ori_summary": "Large language models (LLMs) often miscommunicate their uncertainty: repeated queries can produce divergent answers, yet generated responses are typically unhedged or hedged in ways that do not reflect this variability. This conveys unfaithful information about the uncertain state of the LLMs' knowledge, creating a faithfulness gap that affects even strong LLMs. We introduce Faithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches instruction-tuned LLMs to express uncertainty faithfully without altering their underlying answer distribution. We construct training data by augmenting model samples with uncertainty hedges (i.e. verbal cues such as 'possibly' or 'likely') aligned with sample consistency, requiring no supervision beyond the model and a set of prompts. We evaluate FUT on open-domain question answering (QA) across multiple models and datasets. Our results show that FUT substantially reduces the faithfulness gap, while preserving QA accuracy and introducing minimal semantic distribution shift. Further analyses demonstrate robustness across decoding strategies, choice of hedgers, and other forms of uncertainty expression (i.e. numerical). These findings establish FUT as a simple and effective way to teach LLMs to communicate uncertainty faithfully.",
    "summary": "",
    "translation": "教导语言模型忠实地表达其不确定性",
    "relevance_score": 3,
    "reasoning": "该论文主要关注语言模型的不确定性表达，这属于LLM评估和可靠性范畴，而非核心推荐系统、搜索或广告的技术进展。虽然不确定性建模在理论上可能应用于推荐系统的置信度估计，但论文标题未明确指向这些领域的实际应用，更偏向纯粹的NLP可靠性研究。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12548v1": {
    "title": "VISaGE: Understanding Visual Generics and Exceptions",
    "url": "https://www.alphaxiv.org/abs/2510.12548v1",
    "arxiv_id": "2510.12548v1",
    "authors": "Stella Frank, Emily Allaway",
    "categories": "cs.CL, cs.CV",
    "pub_date": "2025-10-14 14:13:06",
    "ori_summary": "While Vision Language Models (VLMs) learn conceptual representations, in the form of generalized knowledge, during training, they are typically used to analyze individual instances. When evaluation instances are atypical, this paradigm results in tension between two priors in the model. The first is a pragmatic prior that the textual and visual input are both relevant, arising from VLM finetuning on congruent inputs; the second is a semantic prior that the conceptual representation is generally true for instances of the category. In order to understand how VLMs trade off these priors, we introduce a new evaluation dataset, VISaGE, consisting of both typical and exceptional images. In carefully balanced experiments, we show that conceptual understanding degrades when the assumption of congruency underlying the pragmatic prior is violated with incongruent images. This effect is stronger than the effect of the semantic prior when querying about individual instances.",
    "summary": "",
    "translation": "VISaGE：理解视觉通用概念与例外情况",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉通用概念和例外情况的理解，这属于纯粹的视觉理解领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然视觉语言模型的类比在焦点中有所提及，但该论文似乎更侧重于纯粹的视觉认知问题，而非将异构数据作为不同模态进行统一建模的具体应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12516v1": {
    "title": "BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)",
    "url": "https://www.alphaxiv.org/abs/2510.12516v1",
    "arxiv_id": "2510.12516v1",
    "authors": "Tomas Ruiz, Siyao Peng, Barbara Plank, Carsten Schwemmer",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 13:43:08",
    "ori_summary": "Test-time scaling is a family of techniques to improve LLM outputs at inference time by performing extra computation. To the best of our knowledge, test-time scaling has been limited to domains with verifiably correct answers, like mathematics and coding. We transfer test-time scaling to the LeWiDi-2025 tasks to evaluate annotation disagreements. We experiment with three test-time scaling methods: two benchmark algorithms (Model Averaging and Majority Voting), and a Best-of-N sampling method. The two benchmark methods improve LLM performance consistently on the LeWiDi tasks, but the Best-of-N method does not. Our experiments suggest that the Best-of-N method does not currently transfer from mathematics to LeWiDi tasks, and we analyze potential reasons for this gap.",
    "summary": "",
    "translation": "BoN Appetit团队在LeWiDi-2025：最佳N测试时扩展尚无法消化标注分歧",
    "relevance_score": 2,
    "reasoning": "该论文主要关注测试时扩展方法和标注分歧问题，这属于LLM评估和基准测试范畴，与我的核心关注点（推荐系统、搜索、广告的直接应用或使能技术）相关性较弱。虽然最佳N采样可能在某些场景下用于推荐多样性，但论文焦点是标注分歧而非实际应用，因此得分较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12476v1": {
    "title": "When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection",
    "url": "https://www.alphaxiv.org/abs/2510.12476v1",
    "arxiv_id": "2510.12476v1",
    "authors": "Lang Gao, Xuhui Li, Chenxi Wang, Mingzhe Li, Wei Liu, Zirui Song, Jinghui Zhang, Rui Yan, Preslav Nakov, Xiuying Chen",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 13:10:23",
    "ori_summary": "Large language models (LLMs) have grown more powerful in language generation, producing fluent text and even imitating personal style. Yet, this ability also heightens the risk of identity impersonation. To the best of our knowledge, no prior work has examined personalized machine-generated text (MGT) detection. In this paper, we introduce \\dataset, the first benchmark for evaluating detector robustness in personalized settings, built from literary and blog texts paired with their LLM-generated imitations. Our experimental results demonstrate large performance gaps across detectors in personalized settings: some state-of-the-art models suffer significant drops. We attribute this limitation to the \\textit{feature-inversion trap}, where features that are discriminative in general domains become inverted and misleading when applied to personalized text. Based on this finding, we propose \\method, a simple and reliable way to predict detector performance changes in personalized settings. \\method identifies latent directions corresponding to inverted features and constructs probe datasets that differ primarily along these features to evaluate detector dependence. Our experiments show that \\method can accurately predict both the direction and the magnitude of post-transfer changes, showing 85\\% correlation with the actual performance gaps. We hope that this work will encourage further research on personalized text detection.",
    "summary": "",
    "translation": "当个性化策略欺骗检测器时：机器生成文本检测中的特征反转陷阱",
    "relevance_score": 1,
    "reasoning": "该论文主要关注机器生成文本检测中的个性化策略欺骗问题，这属于检测和对抗性攻击领域。虽然涉及文本生成，但核心焦点是检测方法而非推荐系统、搜索或广告的直接应用。论文内容更偏向安全性和检测可靠性，属于被排除的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12474v1": {
    "title": "SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression",
    "url": "https://www.alphaxiv.org/abs/2510.12474v1",
    "arxiv_id": "2510.12474v1",
    "authors": "Biao Zhang, Lixin Chen, Tong Liu, Bo Zheng",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-14 13:04:22",
    "ori_summary": "Large language models (LLMs) generate high-dimensional embeddings that capture rich semantic and syntactic information. However, high-dimensional embeddings exacerbate computational complexity and storage requirements, thereby hindering practical deployment. To address these challenges, we propose a novel training framework named Sequential Matryoshka Embedding Compression (SMEC). This framework introduces the Sequential Matryoshka Representation Learning(SMRL) method to mitigate gradient variance during training, the Adaptive Dimension Selection (ADS) module to reduce information degradation during dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module to enhance unsupervised learning between high- and low-dimensional embeddings. Experiments on image, text, and multimodal datasets demonstrate that SMEC achieves significant dimensionality reduction while maintaining performance. For instance, on the BEIR dataset, our approach improves the performance of compressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points compared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.",
    "summary": "该论文研究高维嵌入导致的检索系统计算和存储瓶颈问题，核心思想是通过顺序嵌套表示学习、自适应维度选择和可选择性跨批次记忆模块，在保持语义信息的同时实现有效的嵌入压缩。",
    "translation": "SMEC：重新思考套娃表示学习在检索嵌入压缩中的应用",
    "relevance_score": 8,
    "reasoning": "该论文专注于检索系统中的嵌入压缩技术，这直接属于推荐系统和搜索领域的核心进展。通过改进表示学习方法来压缩检索嵌入，可以显著提升大规模推荐和搜索系统的效率，减少存储和计算开销，同时保持检索质量。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接解决检索系统中高维嵌入的压缩问题，提出了新颖的训练框架和维度选择方法，对搜索和推荐系统的实际部署具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12463v1": {
    "title": "Resource-sensitive but language-blind: Community size and not grammatical complexity better predicts the accuracy of Large Language Models in a novel Wug Test",
    "url": "https://www.alphaxiv.org/abs/2510.12463v1",
    "arxiv_id": "2510.12463v1",
    "authors": "Nikoleta Pantelidou, Evelina Leivada, Paolo Morosi",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 12:52:57",
    "ori_summary": "The linguistic abilities of Large Language Models are a matter of ongoing debate. This study contributes to this discussion by investigating model performance in a morphological generalization task that involves novel words. Using a multilingual adaptation of the Wug Test, six models were tested across four partially unrelated languages (Catalan, English, Greek, and Spanish) and compared with human speakers. The aim is to determine whether model accuracy approximates human competence and whether it is shaped primarily by linguistic complexity or by the quantity of available training data. Consistent with previous research, the results show that the models are able to generalize morphological processes to unseen words with human-like accuracy. However, accuracy patterns align more closely with community size and data availability than with structural complexity, refining earlier claims in the literature. In particular, languages with larger speaker communities and stronger digital representation, such as Spanish and English, revealed higher accuracy than less-resourced ones like Catalan and Greek. Overall, our findings suggest that model behavior is mainly driven by the richness of linguistic resources rather than by sensitivity to grammatical complexity, reflecting a form of performance that resembles human linguistic competence only superficially.",
    "summary": "",
    "translation": "资源敏感但语言盲区：社区规模而非语法复杂性更能预测大型语言模型在新型Wug测试中的准确性",
    "relevance_score": 2,
    "reasoning": "该论文研究LLM在语言测试中的表现与社区规模的关系，属于纯语言学和模型评估范畴。虽然涉及LLM性能分析，但缺乏与推荐系统、搜索或广告的直接关联，也没有探讨Transformer架构改进或异构数据建模等核心技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12460v1": {
    "title": "Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12460v1",
    "arxiv_id": "2510.12460v1",
    "authors": "Linfeng Gao, Baolong Bi, Zheng Yuan, Le Wang, Zerui Chen, Zhimin Wei, Shenghua Liu, Qinggang Zhang, Jinsong Su",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 12:48:24",
    "ori_summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to enhance the factuality of Large Language Models (LLMs). However, existing RAG systems often suffer from an unfaithfulness issue, where the model's response contradicts evidence from the retrieved context. Existing approaches to improving contextual faithfulness largely rely on external interventions, such as prompt engineering, decoding constraints, or reward-based fine-tuning. These works treat the LLM as a black box and overlook a crucial question: how does the LLM internally integrate retrieved evidence with its parametric memory, particularly under knowledge conflicts? To address this gap, we conduct a probing-based analysis of hidden-state representations in LLMs and observe three findings: knowledge integration occurs hierarchically, conflicts manifest as latent signals at the sentence level, and irrelevant context is often amplified when aligned with parametric knowledge. Building on these findings, we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a framework that (i) decomposes context into fine-grained sentence-level knowledge, (ii) employs hidden-state probing to localize conflicting knowledge, and (iii) introduces conflict-aware fine-tuning to guide the model to accurately integrate retrieved evidence. Extensive experiments across three benchmarks demonstrate that CLEAR substantially improves both accuracy and contextual faithfulness, consistently outperforming strong baselines under diverse conflict conditions. The related resources are available at https://github.com/LinfengGao/CLEAR.",
    "summary": "",
    "translation": "探究潜在知识冲突以实现忠实的检索增强生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注检索增强生成(RAG)中的知识冲突和忠实性问题，这属于纯粹的NLP评估和幻觉相关主题。虽然检索技术可能与搜索系统相关，但论文的核心焦点是确保生成内容的忠实性，这超出了当前关注的核心推荐系统、搜索排名或广告技术领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12434v1": {
    "title": "PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for Retrieval-Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12434v1",
    "arxiv_id": "2510.12434v1",
    "authors": "Xiangjun Zai, Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Wenjie Zhang",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 12:13:23",
    "ori_summary": "Knowledge Hypergraphs (KHs) have recently emerged as a knowledge representation for retrieval-augmented generation (RAG), offering a paradigm to model multi-entity relations into a structured form. However, existing KH-based RAG methods suffer from three major limitations: static retrieval planning, non-adaptive retrieval execution, and superficial use of KH structure and semantics, which constrain their ability to perform effective multi-hop question answering. To overcome these limitations, we propose PRoH, a dynamic Planning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates three core innovations: (i) a context-aware planning module that sketches the local KH neighborhood to guide structurally grounded reasoning plan generation; (ii) a structured question decomposition process that organizes subquestions as a dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive, multi-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided reasoning path retrieval algorithm that prioritizes semantically coherent hyperedge traversals. Experiments across multiple domains demonstrate that PRoH achieves state-of-the-art performance, surpassing the prior SOTA model HyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation (G-E) score, while maintaining strong robustness in long-range multi-hop reasoning tasks.",
    "summary": "",
    "translation": "PRoH：基于知识超图的动态规划与推理用于检索增强生成",
    "relevance_score": 7,
    "reasoning": "This paper addresses dynamic planning and reasoning over knowledge hypergraphs for RAG systems, which represents enabling LLM technology with clear applications in search and recommendation. The techniques for structured knowledge reasoning and dynamic planning could enhance query understanding, result ranking, and personalized content retrieval in search and recommendation systems.",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.12389v1": {
    "title": "Tokenization Disparities as Infrastructure Bias: How Subword Systems Create Inequities in LLM Access and Efficiency",
    "url": "https://www.alphaxiv.org/abs/2510.12389v1",
    "arxiv_id": "2510.12389v1",
    "authors": "Hailay Kidu Teklehaymanot, Wolfgang Nejdl",
    "categories": "cs.CL, cs.AI, I.2.7; I.2.1; H.3.3; F.2.2",
    "pub_date": "2025-10-14 11:14:38",
    "ori_summary": "Tokenization disparities pose a significant barrier to achieving equitable access to artificial intelligence across linguistically diverse populations. This study conducts a large-scale cross-linguistic evaluation of tokenization efficiency in over 200 languages to systematically quantify computational inequities in large language models (LLMs). Using a standardized experimental framework, we applied consistent preprocessing and normalization protocols, followed by uniform tokenization through the tiktoken library across all language samples. Comprehensive tokenization statistics were collected using established evaluation metrics, including Tokens Per Sentence (TPS) and Relative Tokenization Cost (RTC), benchmarked against English baselines. Our cross-linguistic analysis reveals substantial and systematic disparities: Latin-script languages consistently exhibit higher tokenization efficiency, while non-Latin and morphologically complex languages incur significantly greater token inflation, often 3-5 times higher RTC ratios. These inefficiencies translate into increased computational costs and reduced effective context utilization for underrepresented languages. Overall, the findings highlight structural inequities in current AI systems, where speakers of low-resource and non-Latin languages face disproportionate computational disadvantages. Future research should prioritize the development of linguistically informed tokenization strategies and adaptive vocabulary construction methods that incorporate typological diversity, ensuring more inclusive and computationally equitable multilingual AI systems.",
    "summary": "",
    "translation": "分词差异作为基础设施偏见：子词系统如何导致LLM访问和效率的不平等",
    "relevance_score": 2,
    "reasoning": "该论文主要关注分词系统的公平性和访问不平等问题，这属于伦理和公平性范畴，在无关主题中明确排除。虽然涉及LLM基础设施，但焦点是偏见和公平性影响，而非技术效率或架构改进，与推荐系统、搜索或广告的技术核心进展无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12367v1": {
    "title": "LLM-REVal: Can We Trust LLM Reviewers Yet?",
    "url": "https://www.alphaxiv.org/abs/2510.12367v1",
    "arxiv_id": "2510.12367v1",
    "authors": "Rui Li, Jia-Chen Gu, Po-Nien Kung, Heming Xia, Junfeng liu, Xiangwen Kong, Zhifang Sui, Nanyun Peng",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 10:30:20",
    "ori_summary": "The rapid advancement of large language models (LLMs) has inspired researchers to integrate them extensively into the academic workflow, potentially reshaping how research is practiced and reviewed. While previous studies highlight the potential of LLMs in supporting research and peer review, their dual roles in the academic workflow and the complex interplay between research and review bring new risks that remain largely underexplored. In this study, we focus on how the deep integration of LLMs into both peer-review and research processes may influence scholarly fairness, examining the potential risks of using LLMs as reviewers by simulation. This simulation incorporates a research agent, which generates papers and revises, alongside a review agent, which assesses the submissions. Based on the simulation results, we conduct human annotations and identify pronounced misalignment between LLM-based reviews and human judgments: (1) LLM reviewers systematically inflate scores for LLM-authored papers, assigning them markedly higher scores than human-authored ones; (2) LLM reviewers persistently underrate human-authored papers with critical statements (e.g., risk, fairness), even after multiple revisions. Our analysis reveals that these stem from two primary biases in LLM reviewers: a linguistic feature bias favoring LLM-generated writing styles, and an aversion toward critical statements. These results highlight the risks and equity concerns posed to human authors and academic research if LLMs are deployed in the peer review cycle without adequate caution. On the other hand, revisions guided by LLM reviews yield quality gains in both LLM-based and human evaluations, illustrating the potential of the LLMs-as-reviewers for early-stage researchers and enhancing low-quality papers.",
    "summary": "",
    "translation": "LLM-REVal：我们现在能信任LLM评审员吗？",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于LLM作为学术评审员的可信度评估，这属于纯粹的LLM评估和基准测试范畴。虽然涉及LLM技术，但论文关注的是学术评审这一特定应用场景，与推荐系统、搜索或广告领域的核心进展、技术应用或架构创新没有直接关联，也不涉及异构数据建模或Transformer架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12357v1": {
    "title": "MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture of Big Little Experts",
    "url": "https://www.alphaxiv.org/abs/2510.12357v1",
    "arxiv_id": "2510.12357v1",
    "authors": "Yushu Zhao, Yubin Qin, Yang Wang, Xiaolong Yang, Huiming Han, Shaojun Wei, Yang Hu, Shouyi Yin",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 10:22:44",
    "ori_summary": "Mixture-of-Experts (MoE) models have recently demonstrated exceptional performance across a diverse range of applications. The principle of sparse activation in MoE models facilitates an offloading strategy, wherein active experts are maintained in GPU HBM, while inactive experts are stored in CPU DRAM. The efficacy of this approach, however, is fundamentally constrained by the limited bandwidth of the CPU-GPU interconnect. To mitigate this bottleneck, existing approaches have employed prefetching to accelerate MoE inference. These methods attempt to predict and prefetch the required experts using specially trained modules. Nevertheless, such techniques are often encumbered by significant training overhead and have shown diminished effectiveness on recent MoE models with fine-grained expert segmentation. In this paper, we propose MoBiLE, a plug-and-play offloading-based MoE inference framework with \\textit{mixture of big-little experts}. It reduces the number of experts for unimportant tokens to half for acceleration while maintaining full experts for important tokens to guarantee model quality. Further, a dedicated fallback and prefetching mechanism is designed for switching between little and big experts to improve memory efficiency. We evaluate MoBiLE on four typical modern MoE architectures and challenging generative tasks. Our results show that MoBiLE achieves a speedup of 1.60x to 1.72x compared to the baseline on a consumer GPU system, with negligible degradation in accuracy.",
    "summary": "研究MoE模型在消费级GPU上的推理效率瓶颈问题，核心方法是设计混合大小专家架构，对不重要token使用半数量专家加速，对重要token保持完整专家保证质量，并配合回退和预取机制优化内存效率。",
    "translation": "MoBiLE：在消费级GPU上使用大小专家混合进行高效混合专家推理",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能Transformer技术'范畴，专注于混合专家(MoE)架构的效率优化。MoE技术对于构建大规模推荐和搜索系统至关重要，能够显著降低计算成本同时保持模型容量。在消费级GPU上的高效推理直接适用于部署大规模推荐模型到实际生产环境。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文针对MoE模型在消费级GPU上的推理效率问题，提出了混合大小专家的创新架构，直接属于Transformer架构效率优化的核心领域。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12355v1": {
    "title": "Fine-grained Analysis of Brain-LLM Alignment through Input Attribution",
    "url": "https://www.alphaxiv.org/abs/2510.12355v1",
    "arxiv_id": "2510.12355v1",
    "authors": "Michela Proietti, Roberto Capobianco, Mariya Toneva",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 10:19:01",
    "ori_summary": "Understanding the alignment between large language models (LLMs) and human brain activity can reveal computational principles underlying language processing. We introduce a fine-grained input attribution method to identify the specific words most important for brain-LLM alignment, and leverage it to study a contentious research question about brain-LLM alignment: the relationship between brain alignment (BA) and next-word prediction (NWP). Our findings reveal that BA and NWP rely on largely distinct word subsets: NWP exhibits recency and primacy biases with a focus on syntax, while BA prioritizes semantic and discourse-level information with a more targeted recency effect. This work advances our understanding of how LLMs relate to human language processing and highlights differences in feature reliance between BA and NWP. Beyond this study, our attribution method can be broadly applied to explore the cognitive relevance of model predictions in diverse language processing tasks.",
    "summary": "",
    "translation": "基于输入归因的脑-大语言模型对齐细粒度分析",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于脑科学（Brain）与LLM的对齐分析，这属于生物医学领域的交叉研究。虽然涉及LLM技术，但其应用场景和研究目标与推荐系统、搜索或广告领域完全无关，属于明确排除的医学/生物学应用范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12316v1": {
    "title": "Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12316v1",
    "arxiv_id": "2510.12316v1",
    "authors": "Greta Damo, Elena Cabrio, Serena Villata",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 09:20:01",
    "ori_summary": "Counter-speech generation is at the core of many expert activities, such as fact-checking and hate speech, to counter harmful content. Yet, existing work treats counter-speech generation as pure text generation task, mainly based on Large Language Models or NGO experts. These approaches show severe drawbacks due to the limited reliability and coherence in the generated countering text, and in scalability, respectively. To close this gap, we introduce a novel framework to model counter-speech generation as knowledge-wise text generation process. Our framework integrates advanced Retrieval-Augmented Generation (RAG) pipelines to ensure the generation of trustworthy counter-speech for 8 main target groups identified in the hate speech literature, including women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, and other. We built a knowledge base over the United Nations Digital Library, EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792 texts. We use the MultiTarget-CONAN dataset to empirically assess the quality of the generated counter-speech, both through standard metrics (i.e., JudgeLM) and a human evaluation. Results show that our framework outperforms standard LLM baselines and competitive approach, on both assessments. The resulting framework and the knowledge base pave the way for studying trustworthy and sound counter-speech generation, in hate speech and beyond.",
    "summary": "",
    "translation": "通过事实击败有害刻板印象：基于检索增强生成的反对言论生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于内容生成和有害言论检测，属于纯粹的NLP应用领域。虽然涉及RAG技术，但其应用场景（反对言论生成）与推荐系统、搜索或广告的核心技术需求没有直接关联，且属于被明确排除的AIGC和内容生成范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12306v1": {
    "title": "A large-scale, unsupervised pipeline for automatic corpus annotation using LLMs: variation and change in the English consider construction",
    "url": "https://www.alphaxiv.org/abs/2510.12306v1",
    "arxiv_id": "2510.12306v1",
    "authors": "Cameron Morin, Matti Marttinen Larsson",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 09:06:14",
    "ori_summary": "As natural language corpora expand at an unprecedented rate, manual annotation remains a significant methodological bottleneck in corpus linguistic work. We address this challenge by presenting a scalable, unsupervised pipeline for automating grammatical annotation in voluminous corpora using large language models (LLMs). Unlike previous supervised and iterative approaches, our method employs a four-phase workflow: prompt engineering, pre-hoc evaluation, automated batch processing, and post-hoc validation. We demonstrate the pipeline's accessibility and effectiveness through a diachronic case study of variation in the English consider construction. Using GPT-5 through the OpenAI API, we annotate 143,933 sentences from the Corpus of Historical American English (COHA) in under 60 hours, achieving 98%+ accuracy on two sophisticated annotation procedures. Our results suggest that LLMs can perform a range of data preparation tasks at scale with minimal human intervention, opening new possibilities for corpus-based research, though implementation requires attention to costs, licensing, and other ethical considerations.",
    "summary": "",
    "translation": "基于大语言模型的自动语料标注大规模无监督流程：英语consider结构的变异与演变",
    "relevance_score": 2,
    "reasoning": "该论文主要关注使用LLMs进行语料库标注的语言学应用，属于NLP领域的特定任务。虽然涉及LLMs技术，但其应用场景（语言学语料标注）与推荐系统、搜索或广告领域没有直接关联，且论文焦点是语言结构分析而非推荐或搜索相关的技术进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12287v1": {
    "title": "Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual Projector",
    "url": "https://www.alphaxiv.org/abs/2510.12287v1",
    "arxiv_id": "2510.12287v1",
    "authors": "Sifan Li, Hongkai Chen, Yujun Cai, Qingwen Ye, Liyang Chen, Junsong Yuan, Yiwei Wang",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-14 08:42:58",
    "ori_summary": "Vision Language Models (VLMs) have achieved impressive progress in multimodal reasoning; yet, they remain vulnerable to hallucinations, where outputs are not grounded in visual evidence. In this paper, we investigate a previously overlooked setting: logo hallucination, where models generate brand names or textual content despite logos containing no visible words. Using curated splits of pure symbols, hybrids, and text-bearing logos, as well as the challenging Hard-60 subset, we systematically measure hallucination across leading VLMs. We further probe robustness through nine structured perturbations and show that hallucinations persist even under strong distortions, with occlusion exposing the sharpest weaknesses. Embedding-level analysis with open-weight LLaVA demonstrates that hallucination is tied to a small subset of projector dimensions, and targeted ablation substantially reduces errors while preserving OCR accuracy. Together, these findings reveal that VLMs often rely on symbolic priors rather than genuine glyph perception, particularly for iconic circular logos, and that projector subspaces play a decisive role in this failure mode. Our work contributes both a novel diagnostic lens and actionable mitigation insights, highlighting projector disentanglement and OCR-guided decoding as promising directions for building more trustworthy multimodal systems.",
    "summary": "",
    "translation": "视觉语言模型通过视觉投影器中的语义纠缠将Logo映射到文本",
    "relevance_score": 3,
    "reasoning": "该论文研究视觉语言模型中Logo与文本的映射机制，属于视觉-语言多模态研究。虽然VLM技术对处理异构数据有启发价值，但Logo识别在搜索/推荐/广告中的直接应用有限，且论文更偏重视觉模态的机制分析而非通用异构数据处理框架。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12285v1": {
    "title": "Chinese ModernBERT with Whole-Word Masking",
    "url": "https://www.alphaxiv.org/abs/2510.12285v1",
    "arxiv_id": "2510.12285v1",
    "authors": "Zeyu Zhao, Ningtao Wang, Xing Fu, Yu Cheng",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 08:41:22",
    "ori_summary": "Encoder-only Transformers have advanced along three axes -- architecture, data, and systems -- yielding Pareto gains in accuracy, speed, and memory efficiency. Yet these improvements have not fully transferred to Chinese, where tokenization and morphology differ markedly from English. We introduce Chinese ModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware 32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the embedding budget; (ii) whole-word masking (WWM) with a dynamic masking curriculum (30% -> 15%) to align task difficulty with training progress; (iii) a two-stage pre-training pipeline that extends the native context from 1,024 to 8,192 tokens using RoPE and alternating local/global attention; and (iv) a damped-cosine learning-rate schedule for stable long-horizon optimization. We pre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and Cosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong Chinese encoders under a unified fine-tuning protocol. Under bf16 it achieves high long-sequence throughput while maintaining strong short-sequence speed, reflecting benefits from budget allocation and attention design. To probe retrieval-oriented quality, we add a small amount of open contrastive data: fine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking (~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set. Under this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding on SimCLUE, suggesting a clear scaling path for STS with additional curated pairs. We will release tokenizer and weights to facilitate reproducible research.",
    "summary": "",
    "translation": "采用全词掩码的中文ModernBERT模型",
    "relevance_score": 3,
    "reasoning": "该论文聚焦于中文BERT模型的改进（ModernBERT架构和全词掩码技术），属于基础语言模型技术。虽然语言模型是搜索和推荐系统的核心组件，但该工作主要针对中文NLP优化，未明确涉及推荐、搜索或广告领域的特定应用或架构创新，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12255v1": {
    "title": "Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.12255v1",
    "arxiv_id": "2510.12255v1",
    "authors": "Blazej Manczak, Eric Lin, Francisco Eiras, James O' Neill, Vaikkunth Mugunthan",
    "categories": "cs.CL, cs.AI, I.2.7; I.2.6; J.3",
    "pub_date": "2025-10-14 08:04:18",
    "ori_summary": "Large language models (LLMs) are rapidly transitioning into medical clinical use, yet their reliability under realistic, multi-turn interactions remains poorly understood. Existing evaluation frameworks typically assess single-turn question answering under idealized conditions, overlooking the complexities of medical consultations where conflicting input, misleading context, and authority influence are common. We introduce MedQA-Followup, a framework for systematically evaluating multi-turn robustness in medical question answering. Our approach distinguishes between shallow robustness (resisting misleading initial context) and deep robustness (maintaining accuracy when answers are challenged across turns), while also introducing an indirect-direct axis that separates contextual framing (indirect) from explicit suggestion (direct). Using controlled interventions on the MedQA dataset, we evaluate five state-of-the-art LLMs and find that while models perform reasonably well under shallow perturbations, they exhibit severe vulnerabilities in multi-turn settings, with accuracy dropping from 91.2% to as low as 13.5% for Claude Sonnet 4. Counterintuitively, indirect, context-based interventions are often more harmful than direct suggestions, yielding larger accuracy drops across models and exposing a significant vulnerability for clinical deployment. Further compounding analyses reveal model differences, with some showing additional performance drops under repeated interventions while others partially recovering or even improving. These findings highlight multi-turn robustness as a critical but underexplored dimension for safe and reliable deployment of medical LLMs.",
    "summary": "",
    "translation": "浅层鲁棒性，深层脆弱性：医疗大语言模型的多轮对话评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域的大语言模型评估，属于明确的无关主题范畴。论文标题明确指向医疗应用和多轮对话评估，与搜索、推荐、广告系统的技术进展或LLM在这些领域的应用完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12251v1": {
    "title": "DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.12251v1",
    "arxiv_id": "2510.12251v1",
    "authors": "Jiakai Li, Rongzheng Wang, Yizhuo Ma, Shuang Liang, Guangchun Luo, Ke Qin",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 08:01:59",
    "ori_summary": "While large language models (LLMs) show considerable promise across various fields, they have notable limitations in handling multi-document question answering (Multi-doc QA) tasks. The first challenge is long-range dependency modeling, where LLMs struggle to focus on key information in long texts, which weakens important semantic connections. Second, most LLMs suffer from the ''lost-in-the-middle'' issue, where they have difficulty processing information in the middle of long inputs. Current solutions either truncate global dependencies or demand costly finetuning, ultimately lacking a universal and simple solution for these challenges. To resolve these limitations, we propose Dual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The Contextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by assessing paragraph relevance through layer-wise attention tracking and position-aware weighting. (ii) The Reciprocal Attention Suppression (RAS) module enhances focus on critical paragraphs by suppressing information exchange between key and irrelevant texts, thus mitigating the limitations in long-range dependency modeling. Notably, DSAS functions as a plug-and-play solution requiring no architectural modifications or extra training parameters. Extensive experiments on four benchmarks demonstrate DSAS's efficacy across mainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score improvement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and Qwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of both the CGW and RAS modules. In addition, detailed discussions in the Appendix further validate the robustness and scalability of DSAS.",
    "summary": "",
    "translation": "DSAS：多文档问答中注意力优化的通用即插即用框架",
    "relevance_score": 3,
    "reasoning": "该论文聚焦于注意力优化技术，这属于Transformer架构效率改进范畴，对搜索系统中的多文档检索和问答有潜在应用价值。然而，论文明确限定在多文档问答场景，与推荐系统或广告的直接关联较弱，通用性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12229v1": {
    "title": "Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability",
    "url": "https://www.alphaxiv.org/abs/2510.12229v1",
    "arxiv_id": "2510.12229v1",
    "authors": "Bianca Raimondi, Daniela Dalbagno, Maurizio Gabbrielli",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 07:31:29",
    "ori_summary": "Large language models (LLMs) have been shown to internalize human-like biases during finetuning, yet the mechanisms by which these biases manifest remain unclear. In this work, we investigated whether the well-known Knobe effect, a moral bias in intentionality judgements, emerges in finetuned LLMs and whether it can be traced back to specific components of the model. We conducted a Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the bias is not only learned during finetuning but also localized in a specific set of layers. Surprisingly, we found that patching activations from the corresponding pretrained model into just a few critical layers is sufficient to eliminate the effect. Our findings offer new evidence that social biases in LLMs can be interpreted, localized, and mitigated through targeted interventions, without the need for model retraining.",
    "summary": "",
    "translation": "通过机制可解释性分析微调后大语言模型中的道德偏见",
    "relevance_score": 1,
    "reasoning": "该论文主要关注LLM中的道德偏见分析和机制可解释性，这属于公平性、伦理等非技术性主题，明确列在无关主题中。论文没有展示在推荐系统、搜索或广告中的潜在应用，与当前技术焦点完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12217v1": {
    "title": "HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment",
    "url": "https://www.alphaxiv.org/abs/2510.12217v1",
    "arxiv_id": "2510.12217v1",
    "authors": "Ali Mekky, Omar El Herraoui, Preslav Nakov, Yuxia Wang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 07:13:26",
    "ori_summary": "Large language models (LLMs) are increasingly deployed across high-impact domains, from clinical decision support and legal analysis to hiring and education, making fairness and bias evaluation before deployment critical. However, existing evaluations lack grounding in real-world scenarios and do not account for differences in harm severity, e.g., a biased decision in surgery should not be weighed the same as a stylistic bias in text summarization. To address this gap, we introduce HALF (Harm-Aware LLM Fairness), a deployment-aligned framework that assesses model bias in realistic applications and weighs the outcomes by harm severity. HALF organizes nine application domains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline. Our evaluation results across eight LLMs show that (1) LLMs are not consistently fair across domains, (2) model size or performance do not guarantee fairness, and (3) reasoning models perform better in medical decision support but worse in education. We conclude that HALF exposes a clear gap between previous benchmarking success and deployment readiness.",
    "summary": "",
    "translation": "HALF：与部署对齐的伤害感知大语言模型公平性评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM公平性评估，这明确属于被排除的'公平性、伦理'等非技术性主题。论文标题强调'伤害感知'和'公平性评估'，与我的技术焦点（如推荐系统核心算法、Transformer架构效率、LLM在搜索广告中的直接应用）完全无关，且没有展示任何在推荐、搜索或广告领域的技术应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12210v1": {
    "title": "DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12210v1",
    "arxiv_id": "2510.12210v1",
    "authors": "Yakun Song, Xiaobin Zhuang, Jiawei Chen, Zhikang Niu, Guanrou Yang, Chenpeng Du, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen",
    "categories": "eess.AS, cs.CL, cs.LG",
    "pub_date": "2025-10-14 07:03:29",
    "ori_summary": "Recent attempts to interleave autoregressive (AR) sketchers with diffusion-based refiners over continuous speech representations have shown promise, but they remain brittle under distribution shift and offer limited levers for controllability. We introduce DISTAR, a zero-shot text-to-speech framework that operates entirely in a discrete residual vector quantization (RVQ) code space and tightly couples an AR language model with a masked diffusion model, without forced alignment or a duration predictor. Concretely, DISTAR drafts block-level RVQ tokens with an AR language model and then performs parallel masked-diffusion infilling conditioned on the draft to complete the next block, yielding long-form synthesis with blockwise parallelism while mitigating classic AR exposure bias. The discrete code space affords explicit control at inference: DISTAR produces high-quality audio under both greedy and sample-based decoding using classifier-free guidance, supports trade-offs between robustness and diversity, and enables variable bit-rate and controllable computation via RVQ layer pruning at test time. Extensive experiments and ablations demonstrate that DISTAR surpasses state-of-the-art zero-shot TTS systems in robustness, naturalness, and speaker/style consistency, while maintaining rich output diversity. Audio samples are provided on https://anonymous.4open.science/w/DiSTAR_demo.",
    "summary": "",
    "translation": "DiSTAR：基于可扩展令牌自回归表示的扩散语音生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音生成领域，属于纯粹的语音技术研究。虽然提到了扩散模型和自回归表示等通用技术，但论文明确限定于语音生成应用，与推荐系统、搜索或广告领域没有直接关联。语音生成属于明确的无关主题范畴，无法看出在推荐系统、搜索或广告中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12200v1": {
    "title": "HackWorld: Evaluating Computer-Use Agents on Exploiting Web Application Vulnerabilities",
    "url": "https://www.alphaxiv.org/abs/2510.12200v1",
    "arxiv_id": "2510.12200v1",
    "authors": "Xiaoxue Ren, Penghao Jiang, Kaixin Li, Zhiyong Huang, Xiaoning Du, Jiaojiao Jiang, Zhenchang Xing, Jiamou Sun, Terry Yue Zhuo",
    "categories": "cs.CR, cs.CL",
    "pub_date": "2025-10-14 06:52:15",
    "ori_summary": "Web applications are prime targets for cyberattacks as gateways to critical services and sensitive data. Traditional penetration testing is costly and expertise-intensive, making it difficult to scale with the growing web ecosystem. While language model agents show promise in cybersecurity, modern web applications demand visual understanding, dynamic content handling, and multi-step interactions that only computer-use agents (CUAs) can perform. Yet, their ability to discover and exploit vulnerabilities through graphical interfaces remains largely unexplored. We present HackWorld, the first framework for systematically evaluating CUAs' capabilities to exploit web application vulnerabilities via visual interaction. Unlike sanitized benchmarks, HackWorld includes 36 real-world applications across 11 frameworks and 7 languages, featuring realistic flaws such as injection vulnerabilities, authentication bypasses, and unsafe input handling. Using a Capture-the-Flag (CTF) setup, it tests CUAs' capacity to identify and exploit these weaknesses while navigating complex web interfaces. Evaluation of state-of-the-art CUAs reveals concerning trends: exploitation rates below 12% and low cybersecurity awareness. CUAs often fail at multi-step attack planning and misuse security tools. These results expose the current limitations of CUAs in web security contexts and highlight opportunities for developing more security-aware agents capable of effective vulnerability detection and exploitation.",
    "summary": "",
    "translation": "HackWorld：评估计算机使用代理在利用Web应用程序漏洞方面的能力",
    "relevance_score": 1,
    "reasoning": "该论文专注于网络安全和漏洞利用评估，属于安全领域的研究。这与我的核心关注点（推荐系统、搜索、广告及相关LLM技术）完全无关，且明确属于需要排除的安全相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12195v1": {
    "title": "DPO-Tuned Large Language Models for Segmentation in Simultaneous Speech Translation",
    "url": "https://www.alphaxiv.org/abs/2510.12195v1",
    "arxiv_id": "2510.12195v1",
    "authors": "Zeyu Yang, Satoshi Nakamura",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 06:41:36",
    "ori_summary": "Simultaneous speech translation requires accurate segmentation to balance translation quality and latency. Recent studies such as SHAS have introduced pretrained segmentation models, achieving stronger performance than heuristic rules. However, segmentation models such as SHAS, though pretrained and more robust than heuristic methods, are still constrained by supervised learning objectives and do not incorporate human preference alignment, which is crucial for natural real-time interpretation. In this work, we propose a segmentation framework based on large language models (LLMs) trained with Direct Preference Optimization (DPO). By leveraging preference alignment, our method enables LLMs to predict natural segmentation points that better meet the demands of real-time translation. We evaluate the system on the ACL 60/60 corpus across three language pairs (English-Japanese, Chinese, German), using SeamlessM4T v2 as the translation backbone. Experimental results show that our DPO-tuned LLM achieves higher segmentation accuracy than SHAS and yields consistent improvements in translation quality (BLEU, COMET) as well as latency (Average Lagging). Furthermore, our system benefits from IWSLT baselines for direct comparison. These findings highlight the potential of preference-tuned LLMs to surpass existing pretrained segmentation models and advance adaptive, human-aligned simultaneous interpretation.",
    "summary": "",
    "translation": "用于同步语音翻译中分割任务的DPO调优大语言模型",
    "relevance_score": 2,
    "reasoning": "该论文专注于语音翻译中的分割任务，属于语音处理领域，与推荐系统、搜索或广告的核心关注点没有直接关联。虽然提到了DPO调优和LLMs，但这些技术应用在语音翻译场景中，缺乏明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12185v1": {
    "title": "Not in Sync: Unveiling Temporal Bias in Audio Chat Models",
    "url": "https://www.alphaxiv.org/abs/2510.12185v1",
    "arxiv_id": "2510.12185v1",
    "authors": "Jiayu Yao, Shenghua Liu, Yiwei Wang, Rundong Cheng, Lingrui Mei, Baolong Bi, Zhen Xiong, Xueqi Cheng",
    "categories": "cs.CL, cs.SD",
    "pub_date": "2025-10-14 06:29:40",
    "ori_summary": "Large Audio Language Models (LALMs) are increasingly applied to audio understanding and multimodal reasoning, yet their ability to locate when events occur remains underexplored. We present the first systematic study of temporal bias in LALMs, revealing a key limitation in their timestamp prediction. For example, when asked \"At which second does the lecturer introduce the key formula?\", models often predict timestamps that are consistently earlier or later than the ground truth. Through controlled experiments on timestamped datasets, we find that temporal bias (i) is prevalent across datasets and models, (ii) increases with audio length - even accumulating to tens of seconds in extended recordings, and (iii) varies across event types and positions. We quantify this effect with the Temporal Bias Index (TBI), measuring systematic misalignment in predicted event timings, and complement it with a visualization framework. Our findings highlight a fundamental limitation in current LALMs and call for the development of temporally robust architectures.",
    "summary": "",
    "translation": "不同步：揭示音频聊天模型中的时间偏差",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于音频聊天模型中的时间偏差问题，这属于语音处理领域的技术挑战。虽然提到了模型偏差，但这是针对音频模态的特定问题，与推荐系统、搜索或广告的核心技术发展没有直接关联，也不涉及LLM在推荐/搜索/广告领域的应用或Transformer架构的改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12181v1": {
    "title": "From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing",
    "url": "https://www.alphaxiv.org/abs/2510.12181v1",
    "arxiv_id": "2510.12181v1",
    "authors": "Chengrui Xiang, Tengfei Ma, Xiangzheng Fu, Yiping Liu, Bosheng Song, Xiangxiang Zeng",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 06:15:36",
    "ori_summary": "Drug repurposing plays a critical role in accelerating treatment discovery, especially for complex and rare diseases. Biomedical knowledge graphs (KGs), which encode rich clinical associations, have been widely adopted to support this task. However, existing methods largely overlook common-sense biomedical concept knowledge in real-world labs, such as mechanistic priors indicating that certain drugs are fundamentally incompatible with specific treatments. To address this gap, we propose LLaDR, a Large Language Model-assisted framework for Drug Repurposing, which improves the representation of biomedical concepts within KGs. Specifically, we extract semantically enriched treatment-related textual representations of biomedical entities from large language models (LLMs) and use them to fine-tune knowledge graph embedding (KGE) models. By injecting treatment-relevant knowledge into KGE, LLaDR largely improves the representation of biomedical concepts, enhancing semantic understanding of under-studied or complex indications. Experiments based on benchmarks demonstrate that LLaDR achieves state-of-the-art performance across different scenarios, with case studies on Alzheimer's disease further confirming its robustness and effectiveness. Code is available at https://github.com/xiaomingaaa/LLaDR.",
    "summary": "",
    "translation": "从知识到治疗：大语言模型辅助的生物医学概念表示用于药物重定位",
    "relevance_score": 1,
    "reasoning": "该论文明确聚焦于生物医学领域的药物重定位应用，这属于明确的无关主题范畴（医学/生物学特定领域应用）。虽然涉及LLM技术，但其应用场景与推荐系统、搜索或广告领域没有任何直接或潜在的关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12178v1": {
    "title": "Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey",
    "url": "https://www.alphaxiv.org/abs/2510.12178v1",
    "arxiv_id": "2510.12178v1",
    "authors": "Abdulhady Abas Abdullah, Arkaitz Zubiaga, Seyedali Mirjalili, Amir H. Gandomi, Fatemeh Daneshfar, Mohammadsadra Amini, Alan Salam Mohammed, Hadi Veisi",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-14 06:12:44",
    "ori_summary": "This review surveys the rapid evolution of Meta AI's LLaMA (Large Language Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized parameter-efficient fine-tuning (PEFT) methods developed for these models. We first describe the LLaMA family of foundation models (7B-65B to 288B parameters), their architectures (including native multimodal and Mixtureof-Experts variants), and key performance characteristics. We then describe and discuss the concept of PEFT, which adapts large pre-trained models by updating only a small subset of parameters, and review five PEFT methods that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1 and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's mechanism, parameter savings, and example application to LLaMA (e.g., instruction tuning, multimodal tasks). We provide structured discussion and analysis of model and adapter architectures, parameter counts, and benchmark results (including examples where fine-tuned LLaMA models outperform larger baselines). Finally, we examine real-world use cases where LLaMA-based models and PEFT have been successfully applied (e.g., legal and medical domains), and we discuss ongoing challenges and future research directions (such as scaling to even larger contexts and improving robustness). This survey paper provides a one-stop resource for ML researchers and practitioners interested in LLaMA models and efficient fine-tuning strategies.",
    "summary": "论文系统梳理了Meta LLaMA系列模型从1代到4代的架构演进（包括多模态和MoE变体），并重点分析了LoRA、QLoRA等参数高效微调方法，这些技术通过仅更新少量参数来适配大模型，为实际应用提供了高效的模型定制方案。",
    "translation": "Meta的Llama模型演进与大型语言模型的参数高效微调：综述",
    "relevance_score": 8,
    "reasoning": "该论文直接属于'使能LLM技术'范畴，系统梳理了Llama模型的演进历程和参数高效微调方法。这些核心LLM技术的进展对于推荐系统、搜索和广告领域具有重要应用价值，能够显著提升模型部署效率、降低计算成本，并为个性化服务提供更强大的基础模型支撑。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文系统综述了LLaMA模型演进和参数高效微调技术，直接涉及LLM核心进展和高效适配方法，对推荐搜索广告领域的模型部署优化具有重要参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12167v1": {
    "title": "Towards Inference-time Scaling for Continuous Space Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.12167v1",
    "arxiv_id": "2510.12167v1",
    "authors": "Minghan Wang, Thuy-Trang Vu, Ehsan Shareghi, Gholamreza Haffari",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 05:53:41",
    "ori_summary": "Inference-time scaling through multiple sample generation in combination with Process- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective for text-based reasoning in large language models. This paper investigates whether such established techniques can be successfully adapted to reasoning in the continuous space, using COCONUT (Hao et al. 2024) continuous space reasoning LM as the backbone. We demonstrate the feasibility of generating diverse reasoning paths through dropout-based sampling. Our Pass@N analysis on the generated samples reveals the potential that could enable a significant gain in performance akin to observed gain in the discrete space. However, we highlight unique challenges faced for materializing this gain in the continuous thought space. In particular, working recipes for data generation and training PRM and ORM models in the discrete space unlocks only marginal improvements in the continuous space. Through probing various aspects including geometric properties and trajectory dynamics we identify the underlying reasons that prevent effective discrimination between correct and incorrect reasoning (essential for the functioning of PRM and ORM). Our findings reveal that current limitations stem from the absence of key inductive biases in continuous thought representations. We argue that the training frameworks for continuous reasoning LMs require not only to optimize for accuracy but also to explicitly incorporate inductive biases that could be utilized during inference-time for discrimination of correct and incorrect thoughts.\\footnote{Our code and data will be publicly available.}",
    "summary": "该论文研究如何将离散空间的推理时扩展技术（多样本生成与奖励模型重排序）适配到连续空间推理中。核心发现是连续思维表示缺乏关键归纳偏置，导致正确与错误推理难以区分，需要训练框架显式引入可被推理时利用的归纳偏置。",
    "translation": "面向连续空间推理的推理时扩展方法研究",
    "relevance_score": 8,
    "reasoning": "该论文关注推理时扩展技术，这是LLM效率优化的核心方向，属于'Enabling LLM Tech'范畴。推理时扩展技术可以显著提升推荐系统和搜索中的实时推理效率，降低计算成本，对于大规模在线服务具有直接应用价值。连续空间推理也与推荐系统中的连续特征建模和序列推荐有潜在关联。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文研究推理时扩展技术在连续空间中的应用，直接关联Transformer架构效率和推理优化，对推荐系统中复杂用户行为建模具有重要参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12164v1": {
    "title": "A Survey on Parallel Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.12164v1",
    "arxiv_id": "2510.12164v1",
    "authors": "Ziqi Wang, Boye Niu, Zipeng Gao, Zhi Zheng, Tong Xu, Linghui Meng, Zhongli Li, Jing Liu, Yilong Chen, Chen Zhu, Hua Wu, Haifeng Wang, Enhong Chen",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 05:42:19",
    "ori_summary": "With the increasing capabilities of Large Language Models (LLMs), parallel reasoning has emerged as a new inference paradigm that enhances reasoning robustness by concurrently exploring multiple lines of thought before converging on a final answer. It has become a significant trend to explore parallel reasoning to overcome the fragility of standard sequential methods and improve practical performance. In this paper, we aim to survey and summarize the progress and challenges of parallel reasoning. We first present a formal definition of parallel reasoning and clarify its distinction from related concepts like Chain-of-Thought. Then, we organize and discuss advanced techniques based on a novel taxonomy, including non-interactive reasoning, interactive reasoning, and efficiency-focused decoding strategies. Additionally, we explore various application scenarios, such as solving complex problems and enhancing the reliability of LLM outputs.Finally, we highlight the core challenges of parallel reasoning and suggest potential directions for future research. We hope that our work can provide a useful roadmap for beginners and encourage more research on improving parallel reasoning methods. Related source can be avaliable in https://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.",
    "summary": "",
    "translation": "并行推理技术综述",
    "relevance_score": 2,
    "reasoning": "这篇关于并行推理的综述论文主要关注推理过程的并行化技术，属于通用AI推理优化领域。虽然推理效率对LLM应用有一定影响，但该论文没有明确聚焦于推荐系统、搜索或广告领域的特定应用场景，也没有涉及Transformer架构改进或其他核心领域进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12137v1": {
    "title": "Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.12137v1",
    "arxiv_id": "2510.12137v1",
    "authors": "Shihao Ji, Zihui Song, Jiajie Huang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 04:31:49",
    "ori_summary": "Large Language Models (LLMs) hallucinate, generating factually incorrect yet confident assertions. We argue this stems from the Transformer's Softmax function, which creates \"Artificial Certainty\" by collapsing ambiguous attention scores into a single probability distribution, discarding uncertainty information at each layer. To fix this, we introduce the Credal Transformer, which replaces standard attention with a Credal Attention Mechanism (CAM) based on evidential theory. CAM produces a \"credal set\" (a set of distributions) instead of a single attention vector, with the set's size directly measuring model uncertainty. We implement this by re-conceptualizing attention scores as evidence masses for a Dirichlet distribution: sufficient evidence recovers standard attention, while insufficient evidence yields a diffuse distribution, representing ambiguity. Empirically, the Credal Transformer identifies out-of-distribution inputs, quantifies ambiguity, and significantly reduces confident errors on unanswerable questions by abstaining. Our contribution is a new architecture to mitigate hallucinations and a design paradigm that integrates uncertainty quantification directly into the model, providing a foundation for more reliable AI.",
    "summary": "",
    "translation": "Credal Transformer：一种量化与缓解大语言模型幻觉的原则性方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM的幻觉问题，这属于纯粹的NLP中心话题，被明确列为无关主题。虽然Transformer架构改进可能具有潜在应用价值，但论文专注于幻觉缓解而非架构效率或新注意力机制，与推荐系统、搜索或广告的核心技术需求关联度极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12133v1": {
    "title": "SafeMT: Multi-turn Safety for Multimodal Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.12133v1",
    "arxiv_id": "2510.12133v1",
    "authors": "Han Zhu, Juntao Dai, Jiaming Ji, Haoran Li, Chengkun Cai, Pengcheng Wen, Chi-Min Chan, Boyuan Chen, Yaodong Yang, Sirui Han, Yike Guo",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 04:24:07",
    "ori_summary": "With the widespread use of multi-modal Large Language models (MLLMs), safety issues have become a growing concern. Multi-turn dialogues, which are more common in everyday interactions, pose a greater risk than single prompts; however, existing benchmarks do not adequately consider this situation. To encourage the community to focus on the safety issues of these models in multi-turn dialogues, we introduce SafeMT, a benchmark that features dialogues of varying lengths generated from harmful queries accompanied by images. This benchmark consists of 10,000 samples in total, encompassing 17 different scenarios and four jailbreak methods. Additionally, we propose Safety Index (SI) to evaluate the general safety of MLLMs during conversations. We assess the safety of 17 models using this benchmark and discover that the risk of successful attacks on these models increases as the number of turns in harmful dialogues rises. This observation indicates that the safety mechanisms of these models are inadequate for recognizing the hazard in dialogue interactions. We propose a dialogue safety moderator capable of detecting malicious intent concealed within conversations and providing MLLMs with relevant safety policies. Experimental results from several open-source models indicate that this moderator is more effective in reducing multi-turn ASR compared to existed guard models.",
    "summary": "",
    "translation": "SafeMT：多模态语言模型的多轮对话安全性",
    "relevance_score": 1,
    "reasoning": "该论文专注于多模态语言模型的安全性问题，这属于安全、伦理等非技术性话题，明确列在无关主题中。虽然提到了多模态，但核心关注点是安全性而非技术架构或推荐/搜索/广告应用，因此与当前关注点完全不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12121v1": {
    "title": "Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing",
    "url": "https://www.alphaxiv.org/abs/2510.12121v1",
    "arxiv_id": "2510.12121v1",
    "authors": "Rongzhi Zhang, Liqin Ye, Yuzhao Heng, Xiang Chen, Tong Yu, Lingkai Kong, Sudheer Chava, Chao Zhang",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-14 03:50:22",
    "ori_summary": "Precise attribute intensity control--generating Large Language Model (LLM) outputs with specific, user-defined attribute intensities--is crucial for AI systems adaptable to diverse user expectations. Current LLM alignment methods, however, typically provide only directional or open-ended guidance, failing to reliably achieve exact attribute intensities. We address this limitation with three key designs: (1) reformulating precise attribute intensity control as a target-reaching problem, rather than simple maximization; (2) training a lightweight value function via temporal-difference learning to predict final attribute intensity scores from partial generations, thereby steering LLM outputs; and (3) employing gradient-based interventions on hidden representations to navigate the model precisely towards specific attribute intensity targets. Our method enables fine-grained, continuous control over attribute intensities, moving beyond simple directional alignment. Experiments on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text generation to user-specified attribute intensities with high accuracy. Finally, we demonstrate efficiency enhancements across three downstream tasks: preference data synthesis, Pareto frontier approximation and optimization, and distillation of aligned behaviors for intervention-free inference. Our code is available on https://github.com/Pre-Control/pre-control",
    "summary": "该论文研究LLM输出中精确属性强度控制问题，核心方法是通过训练轻量级值函数预测属性强度，并基于梯度干预隐藏表示来精确导航模型达到特定属性强度目标。",
    "translation": "通过目标表示编辑实现大语言模型中的精确属性强度控制",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能LLM技术'类别，专注于通过表示编辑实现精确的属性控制。这种技术在推荐系统和搜索中具有直接应用潜力，可以用于精确控制内容属性（如情感强度、风格特征）以更好地匹配用户偏好，或通过编辑用户/物品表示来优化个性化推荐质量。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的精确属性强度控制方法通过表示编辑实现细粒度文本生成控制，可直接应用于推荐系统的个性化内容生成和广告文案优化，属于LLM直接应用范畴。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12116v1": {
    "title": "Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.12116v1",
    "arxiv_id": "2510.12116v1",
    "authors": "Bajian Xiang, Shuaijiang Zhao, Tingwei Guo, Wei Zou",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 03:34:38",
    "ori_summary": "End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive conversational generation abilities, yet consistently fall short of traditional pipeline systems on semantic understanding benchmarks. In this work, we reveal through systematic experimentation that although LSLMs lose some text input performance after speech-text alignment training, the performance gap between speech and text inputs is more pronounced, which we refer to as the modality gap. To understand this gap, we analyze both coarse- and fine-grained text and speech representations. At the coarse-grained level, representations of speech and text in deeper layers are found to be increasingly aligned in direction (cosine similarity), while concurrently diverging in magnitude (Euclidean distance). We further find that representation similarity is strongly correlated with the modality gap. At the fine-grained level, a spontaneous token-level alignment pattern between text and speech representations is observed. Based on this, we introduce the Alignment Path Score to quantify token-level alignment quality, which exhibits stronger correlation with the modality gap. Building on these insights, we design targeted interventions on critical tokens through angle projection and length normalization. These strategies demonstrate the potential to improve correctness for speech inputs. Our study provides the first systematic empirical analysis of the modality gap and alignment mechanisms in LSLMs, offering both theoretical and methodological guidance for future optimization.",
    "summary": "",
    "translation": "理解模态鸿沟：大型语音语言模型语音-文本对齐机制的实证研究",
    "relevance_score": 2,
    "reasoning": "该论文主要研究语音-文本模态对齐机制，虽然涉及多模态建模概念，但其核心聚焦于语音模态，这在用户当前关注点中被明确列为不相关主题。语音模态与推荐系统、搜索或广告的核心技术栈关联度极低，缺乏明确的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12115v1": {
    "title": "Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation",
    "url": "https://www.alphaxiv.org/abs/2510.12115v1",
    "arxiv_id": "2510.12115v1",
    "authors": "Xin Zhao, Naoki Yoshinaga, Yuma Tsuta, Akiko Aizawa",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 03:34:17",
    "ori_summary": "Multilingual domain adaptation (ML-DA) is widely used to learn new domain knowledge across languages into large language models (LLMs). Although many methods have been proposed to improve domain adaptation, the mechanisms of multilingual knowledge acquisition, how domain knowledge is learned within a language and transferred across languages, remain underexplored. This gap leads to suboptimal performance, particularly in low-resource settings. This work examines the learning dynamics of LLMs during ML-DA. Because prior ML-DA studies often train and evaluate on datasets with mismatched knowledge coverage, we propose AdaXEval, an adaptive evaluation method that builds multiple-choice QA datasets from the same bilingual domain corpus used for training, thereby directly studying multilingual knowledge acquisition. Through continual training of LLMs with diverse data recipes, we track how LLMs acquire domain facts and pinpoint the mechanism behind the transformation process from domain training data to knowledge. Our experiments on a 13B English-Japanese bilingual LLM reveal that cross-lingual transfer remains challenging despite a high-quality bilingual corpus. The code has been released.",
    "summary": "",
    "translation": "领域适应中多语言知识获取动态的追踪：以英语-日语生物医学适应为例",
    "relevance_score": 2,
    "reasoning": "该论文主要关注生物医学领域的多语言领域适应，这属于特定领域应用（医学/生物学），与我的核心关注点（推荐系统、搜索、广告）无关。虽然涉及领域适应技术，但缺乏明确的RecSys/Search/Ads应用潜力，且生物医学领域被明确列为不相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12110v1": {
    "title": "Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.12110v1",
    "arxiv_id": "2510.12110v1",
    "authors": "Ziliang Qiu, Renfen Hu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 03:26:28",
    "ori_summary": "The evaluation of LLMs' creativity represents a crucial research domain, though challenges such as data contamination and costly human assessments often impede progress. Drawing inspiration from human creativity assessment, we propose PACE, asking LLMs to generate Parallel Association Chains to Evaluate their creativity. PACE minimizes the risk of data contamination and offers a straightforward, highly efficient evaluation, as evidenced by its strong correlation with Chatbot Arena Creative Writing rankings (Spearman's $\\rho = 0.739$, $p < 0.001$) across various proprietary and open-source models. A comparative analysis of associative creativity between LLMs and humans reveals that while high-performing LLMs achieve scores comparable to average human performance, professional humans consistently outperform LLMs. Furthermore, linguistic analysis reveals that both humans and LLMs exhibit a trend of decreasing concreteness in their associations, and humans demonstrating a greater diversity of associative patterns.",
    "summary": "",
    "translation": "深度关联，高度创意：一个简单而有效的大型语言模型评估指标",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM评估指标，这属于纯粹的NLP中心话题，与我的关注点无关。论文标题明确表明其核心是评估方法，而非在推荐系统、搜索或广告领域的应用或技术进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12088v1": {
    "title": "One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration",
    "url": "https://www.alphaxiv.org/abs/2510.12088v1",
    "arxiv_id": "2510.12088v1",
    "authors": "Zaid Khan, Archiki Prasad, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-14 02:49:32",
    "ori_summary": "Symbolic world modeling requires inferring and representing an environment's transitional dynamics as an executable program. Prior work has focused on largely deterministic environments with abundant interaction data, simple mechanics, and human guidance. We address a more realistic and challenging setting, learning in a complex, stochastic environment where the agent has only \"one life\" to explore a hostile environment without human guidance. We introduce OneLife, a framework that models world dynamics through conditionally-activated programmatic laws within a probabilistic programming framework. Each law operates through a precondition-effect structure, activating in relevant world states. This creates a dynamic computation graph that routes inference and optimization only through relevant laws, avoiding scaling challenges when all laws contribute to predictions about a complex, hierarchical state, and enabling the learning of stochastic dynamics even with sparse rule activation. To evaluate our approach under these demanding constraints, we introduce a new evaluation protocol that measures (a) state ranking, the ability to distinguish plausible future states from implausible ones, and (b) state fidelity, the ability to generate future states that closely resemble reality. We develop and evaluate our framework on Crafter-OO, our reimplementation of the Crafter environment that exposes a structured, object-oriented symbolic state and a pure transition function that operates on that state alone. OneLife can successfully learn key environment dynamics from minimal, unguided interaction, outperforming a strong baseline on 16 out of 23 scenarios tested. We also test OneLife's planning ability, with simulated rollouts successfully identifying superior strategies. Our work establishes a foundation for autonomously constructing programmatic world models of unknown, complex environments.",
    "summary": "",
    "translation": "一生一次学习：从无引导探索中推断随机环境的符号世界模型",
    "relevance_score": 2,
    "reasoning": "该论文关注从无监督探索中学习符号世界模型，这属于强化学习和环境建模领域。虽然环境建模在理论上可能与推荐系统中的用户行为建模相关，但论文专注于随机环境中的符号推理，缺乏与推荐、搜索或广告系统的直接联系，也没有涉及LLM、Transformer或多模态建模等核心技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12083v1": {
    "title": "An AI-Based Behavioral Health Safety Filter and Dataset for Identifying Mental Health Crises in Text-Based Conversations",
    "url": "https://www.alphaxiv.org/abs/2510.12083v1",
    "arxiv_id": "2510.12083v1",
    "authors": "Benjamin W. Nelson, Celeste Wong, Matthew T. Silvestrini, Sooyoon Shin, Alanna Robinson, Jessica Lee, Eric Yang, John Torous, Andrew Trister",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 02:47:52",
    "ori_summary": "Large language models often mishandle psychiatric emergencies, offering harmful or inappropriate advice and enabling destructive behaviors. This study evaluated the Verily behavioral health safety filter (VBHSF) on two datasets: the Verily Mental Health Crisis Dataset containing 1,800 simulated messages and the NVIDIA Aegis AI Content Safety Dataset subsetted to 794 mental health-related messages. The two datasets were clinician-labelled and we evaluated performance using the clinician labels. Additionally, we carried out comparative performance analyses against two open source, content moderation guardrails: OpenAI Omni Moderation Latest and NVIDIA NeMo Guardrails. The VBHSF demonstrated, well-balanced performance on the Verily Mental Health Crisis Dataset v1.0, achieving high sensitivity (0.990) and specificity (0.992) in detecting any mental health crises. It achieved an F1-score of 0.939, sensitivity ranged from 0.917-0.992, and specificity was >= 0.978 in identifying specific crisis categories. When evaluated against the NVIDIA Aegis AI Content Safety Dataset 2.0, VBHSF performance remained highly sensitive (0.982) and accuracy (0.921) with reduced specificity (0.859). When compared with the NVIDIA NeMo and OpenAI Omni Moderation Latest guardrails, the VBHSF demonstrated superior performance metrics across both datasets, achieving significantly higher sensitivity in all cases (all p < 0.001) and higher specificity relative to NVIDIA NeMo (p < 0.001), but not to OpenAI Omni Moderation Latest (p = 0.094). NVIDIA NeMo and OpenAI Omni Moderation Latest exhibited inconsistent performance across specific crisis types, with sensitivity for some categories falling below 0.10. Overall, the VBHSF demonstrated robust, generalizable performance that prioritizes sensitivity to minimize missed crises, a crucial feature for healthcare applications.",
    "summary": "",
    "translation": "基于人工智能的行为健康安全过滤器及数据集：用于识别文本对话中的心理健康危机",
    "relevance_score": 1,
    "reasoning": "该论文专注于心理健康危机检测这一特定医疗应用领域，属于明确的无关主题。虽然涉及文本分析技术，但核心应用场景（行为健康安全）与推荐系统、搜索或广告领域没有任何关联，也不涉及LLM、Transformer架构或异构数据建模等关键技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12063v1": {
    "title": "ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.12063v1",
    "arxiv_id": "2510.12063v1",
    "authors": "Sunzhu Li, Zhiyu Lin, Shuling Yang, Jiale Zhao, Wei Chen",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-14 02:02:19",
    "ori_summary": "Large Reasoning Models (LRMs) are powerful, but they still suffer from inefficient and off-target reasoning. Currently, training-free methods are limited to either rigid heuristics or descriptive, non-actionable analyses. In this paper, we introduce ThinkPilot, a training-free framework that automatically optimizes LRMs reasoning. It uses an evolutionary process to generate think-prefixes, which are instructions that evolve driven by a taxonomy of reasoning behaviors to guide models toward superior performance. Extensive experiments demonstrate ThinkPilot's broad effectiveness: it significantly improves the accuracy-length trade-off for efficient reasoning, drastically improves safety (for example, cutting the StrongREJECT score of DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction following. It also synergizes with existing training-based methods. Our analysis reveals that think-prefixes can reliably control LRMs' reasoning behaviors, and that different tasks have strong preferences for specific behavioral distributions. By automatically identifying and eliciting these behaviors, ThinkPilot provides a generalizable framework for aligning LRMs reasoning with task demands. Data and code are available at https://github.com/teqkilla/ThinkPilot",
    "summary": "",
    "translation": "ThinkPilot：通过自动化思维前缀优化引导推理模型",
    "relevance_score": 7,
    "reasoning": "该论文涉及推理模型的引导优化技术，属于LLM核心能力提升范畴。在推荐系统和搜索领域，这种自动化前缀优化技术可以显著提升复杂推理任务的性能，如多步推荐理由生成、复杂查询理解、以及需要多步推理的个性化推荐场景，直接增强LLM在业务应用中的实用性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.12051v1": {
    "title": "APCE: Adaptive Progressive Context Expansion for Long Context Processing",
    "url": "https://www.alphaxiv.org/abs/2510.12051v1",
    "arxiv_id": "2510.12051v1",
    "authors": "Baisub Lee, Sanghyun Byun, Mohanad Odema, Jung Guack, Jacob Song, Woo Seong Chung",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 01:26:36",
    "ori_summary": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing two key challenges: (1) A growing memory footprint due to quadratic self-attention and linear KV-cache scaling in memory as sequence length increases; (2) the ContextRot phenomena where empirical evidence suggests that transformer architecture's performance degrades with increasing context length. Given the shared dependency on the input, a natural question arises: Can we surgically select the most important input chunks for processing to synergistically (a) reduce the memory footprint, and (b) mitigate the ContextRot effects? In this paper, we answer this question in the affirmative for long-context summarization tasks. We propose APCE as a context-aware solution to select the most important input chunks through low-dimensional semantic similarity matching with the current query. By directly operating on the input, APCE decouples from strict dependency on underlying hardware or CUDA environments, promising a compatible solution scalable to different deployment systems. Our empirical evaluations have demonstrated superior or on-par summarization performance for APCE compared to the full dense baseline using a fraction (50%-70%) of the input sequence resulting in KV-cache and self-attention memory efficiency improvements. We hope our findings inspire further research on context-aware efficiency solutions for LCTMs geared towards other relevant long-context tasks.",
    "summary": "研究长上下文Transformer模型的内存效率和处理性能退化问题；核心方法是基于低维语义相似度匹配，自适应选择最重要的输入块进行处理，从而减少计算开销。",
    "translation": "APCE：面向长上下文处理的自适应渐进式上下文扩展",
    "relevance_score": 8,
    "reasoning": "该论文聚焦长上下文处理技术，属于Transformer架构效率优化的核心进展。在推荐系统和搜索领域，处理长用户行为序列、多轮对话历史等长上下文场景至关重要，这种自适应渐进式扩展方法可显著提升长序列建模效率，直接应用于用户行为序列建模和长文档搜索等任务。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文针对长上下文Transformer的内存效率和性能退化问题，提出通过语义相似度选择重要输入块的方法，直接适用于推荐和搜索系统中的长序列处理。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12044v1": {
    "title": "Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer Specialization in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.12044v1",
    "arxiv_id": "2510.12044v1",
    "authors": "Yukun Zhang, Qi Dong",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 00:58:34",
    "ori_summary": "Existing alignment techniques for Large Language Models (LLMs), such as Direct Preference Optimization (DPO), typically treat the model as a monolithic entity, applying uniform optimization pressure across all layers. This approach overlooks the functional specialization within the Transformer architecture, where different layers are known to handle distinct tasks from syntax to abstract reasoning. In this paper, we challenge this one-size-fits-all paradigm by introducing Hierarchical Alignment, a novel method that applies targeted DPO to distinct functional blocks of a model's layers: local (syntax), intermediate (logic), and global (factuality). Through a series of controlled experiments on state-of-the-art models like Llama-3.1-8B and Qwen1.5-7B using LoRA for surgical fine-tuning, our results, evaluated by a powerful LLM-as-Judge, demonstrate significant and predictable improvements. Specifically, aligning the local layers (Local-Align) enhances grammatical fluency. More importantly, aligning the global layers (Global-Align) not only improves factual consistency as hypothesized but also proves to be the most effective strategy for enhancing logical coherence, outperforming all baselines. Critically, all hierarchical strategies successfully avoid the \"alignment tax\" observed in standard DPO, where gains in fluency come at the cost of degraded logical reasoning. These findings establish a more resource-efficient, controllable, and interpretable path for model alignment, highlighting the immense potential of shifting from monolithic optimization to structure-aware surgical fine-tuning to build more advanced and reliable LLMs.",
    "summary": "论文研究LLM对齐过程中忽视模型内部功能分层的问题，核心思想是将DPO优化压力分层施加于处理语法、逻辑和事实性的不同Transformer层，实现手术式精细化微调。",
    "translation": "层次对齐：通过大型语言模型中的功能层专业化进行手术式微调",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM高效微调技术（手术式微调、功能层专业化），属于'Enabling LLM Tech'范畴。这种精细化的微调方法可以应用于推荐系统、搜索和广告领域，通过针对性地调整特定功能层来优化领域特定任务，同时保持模型通用能力，提高部署效率和性能。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出层级对齐方法，针对Transformer不同功能层进行精细化微调，直接属于Transformer架构效率提升和LLM应用优化的核心领域。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.12041v1": {
    "title": "Improving Text-to-Image Generation with Input-Side Inference-Time Scaling",
    "url": "https://www.alphaxiv.org/abs/2510.12041v1",
    "arxiv_id": "2510.12041v1",
    "authors": "Ruibo Chen, Jiacheng Pan, Heng Huang, Zhenheng Yang",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 00:51:39",
    "ori_summary": "Recent advances in text-to-image (T2I) generation have achieved impressive results, yet existing models often struggle with simple or underspecified prompts, leading to suboptimal image-text alignment, aesthetics, and quality. We propose a prompt rewriting framework that leverages large language models (LLMs) to refine user inputs before feeding them into T2I backbones. Our approach introduces a carefully designed reward system and an iterative direct preference optimization (DPO) training pipeline, enabling the rewriter to enhance prompts without requiring supervised fine-tuning data. We evaluate our method across diverse T2I models and benchmarks. Results show that our prompt rewriter consistently improves image-text alignment, visual quality, and aesthetics, outperforming strong baselines. Furthermore, we demonstrate strong transferability by showing that a prompt rewriter trained on one T2I backbone generalizes effectively to others without needing to be retrained. We also systematically study scalability, evaluating how performance gains scale with the capacity of the large LLM used as the rewriter. These findings highlight that prompt rewriting is an effective, scalable, and practical model-agnostic strategy for improving T2I systems. We plan to release the code and trained prompt rewriters soon.",
    "summary": "",
    "translation": "通过输入侧推理时缩放改进文本到图像生成",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于文本到图像生成技术，这属于纯粹的AIGC和内容生成领域，与我的关注点无关。论文标题表明它涉及生成模型的推理时优化，但没有显示出在推荐系统、搜索或广告领域的任何潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12040v1": {
    "title": "Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions",
    "url": "https://www.alphaxiv.org/abs/2510.12040v1",
    "arxiv_id": "2510.12040v1",
    "authors": "Sungmin Kang, Yavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Salman Avestimehr",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 00:49:04",
    "ori_summary": "The rapid advancement of large language models (LLMs) has transformed the landscape of natural language processing, enabling breakthroughs across a wide range of areas including question answering, machine translation, and text summarization. Yet, their deployment in real-world applications has raised concerns over reliability and trustworthiness, as LLMs remain prone to hallucinations that produce plausible but factually incorrect outputs. Uncertainty quantification (UQ) has emerged as a central research direction to address this issue, offering principled measures for assessing the trustworthiness of model generations. We begin by introducing the foundations of UQ, from its formal definition to the traditional distinction between epistemic and aleatoric uncertainty, and then highlight how these concepts have been adapted to the context of LLMs. Building on this, we examine the role of UQ in hallucination detection, where quantifying uncertainty provides a mechanism for identifying unreliable generations and improving reliability. We systematically categorize a wide spectrum of existing methods along multiple dimensions and present empirical results for several representative approaches. Finally, we discuss current limitations and outline promising future research directions, providing a clearer picture of the current landscape of LLM UQ for hallucination detection.",
    "summary": "",
    "translation": "大型语言模型幻觉检测的不确定性量化：基础、方法与未来方向",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM幻觉检测和不确定性量化，属于纯粹的NLP评估基准主题，与您的关注领域无关。虽然涉及LLM技术，但论文重点在于检测和评估幻觉，而非在推荐系统、搜索或广告领域的实际应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12036v1": {
    "title": "On the Interplay between Human Label Variation and Model Fairness",
    "url": "https://www.alphaxiv.org/abs/2510.12036v1",
    "arxiv_id": "2510.12036v1",
    "authors": "Kemal Kurniawan, Meladel Mistica, Timothy Baldwin, Jey Han Lau",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 00:43:48",
    "ori_summary": "The impact of human label variation (HLV) on model fairness is an unexplored topic. This paper examines the interplay by comparing training on majority-vote labels with a range of HLV methods. Our experiments show that without explicit debiasing, HLV training methods have a positive impact on fairness.",
    "summary": "",
    "translation": "关于人类标注差异与模型公平性之间相互作用的研究",
    "relevance_score": 1,
    "reasoning": "该论文明确聚焦于模型公平性，这属于明确排除的非技术性话题范畴。虽然涉及标注差异可能看似与技术相关，但论文的核心关注点是公平性，这超出了当前关注的技术领域范围。该研究没有显示出与推荐系统、搜索或广告的核心技术进展或使能技术相关的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12032v1": {
    "title": "Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.12032v1",
    "arxiv_id": "2510.12032v1",
    "authors": "Jung-Woo Shim, Yeong-Joon Ju, Ji-Hoon Park, Seong-Whan Lee",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-14 00:31:36",
    "ori_summary": "Recent advancements in large language models (LLMs) have shown strong performance in natural language understanding and generation tasks. However, LLMs continue to encounter challenges with hallucinations, where models generate plausible but incorrect information. While several factors contribute to hallucinations, the impact of ill-formed prompts, prompts with ambiguous wording, incorrect grammar, or incomplete information, was relatively under explored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a framework designed to systematically improve these ill-formed prompts across multiple stages. Each stage addresses specific errors such as punctuation, typographical mistakes, and misuse of key terms, using small language models (SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of prompts with additional context and employs a self-reflection mechanism with ranking to prioritize the most relevant input. Experimental results on hallucination benchmarks show that prompts refined by MPR achieve over an 85~\\% win rate compared to their original forms, demonstrating its effectiveness in reducing hallucinations and improving LLM output accuracy. Interestingly, we reveal that MPR can be combined with existing post-hoc hallucination mitigation frameworks, further enhancing its versatility. MPR provides a lightweight and adaptable solution for enhancing LLM reliability across various domains.",
    "summary": "",
    "translation": "用于缓解大语言模型幻觉的多阶段提示优化方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM幻觉缓解这一纯NLP中心主题，属于用户明确排除的无关主题范畴。虽然提示优化技术可能在某些场景下对推荐或搜索系统有间接价值，但论文核心聚焦于幻觉问题而非其在推荐/搜索/广告领域的直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12029v1": {
    "title": "CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement",
    "url": "https://www.alphaxiv.org/abs/2510.12029v1",
    "arxiv_id": "2510.12029v1",
    "authors": "Jung-Woo Shim, Yeong-Joon Ju, Ji-Hoon Park, Seong-Whan Lee",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-14 00:27:46",
    "ori_summary": "Recent advancements in large language models (LLMs) highlight their fluency in generating responses to diverse prompts. However, these models sometimes generate plausible yet incorrect ``hallucinated\" facts, undermining trust. A frequent but often overlooked cause of such errors is the use of poorly structured or vague prompts by users, leading LLMs to base responses on assumed rather than actual intentions. To mitigate hallucinations induced by these ill-formed prompts, we introduce Curative Prompt Refinement (CPR), a plug-and-play framework for curative prompt refinement that 1) cleans ill-formed prompts, and 2) generates additional informative task descriptions to align the intention of the user and the prompt using a fine-tuned small language model. When applied to language models, we discover that CPR significantly increases the quality of generation while also mitigating hallucination. Empirical studies show that prompts with CPR applied achieves over a 90\\% win rate over the original prompts without any external knowledge.",
    "summary": "",
    "translation": "CPR：通过治疗性提示精炼缓解大语言模型幻觉",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM幻觉缓解，这属于纯粹的NLP中心主题，被明确列为不相关主题。虽然提示精炼技术可能间接影响搜索质量，但论文核心焦点是幻觉问题本身，而非在推荐系统、搜索或广告中的具体应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12023v1": {
    "title": "Information Extraction from Conversation Transcripts: Neuro-Symbolic vs. LLM",
    "url": "https://www.alphaxiv.org/abs/2510.12023v1",
    "arxiv_id": "2510.12023v1",
    "authors": "Alice Saebom Kwak, Maria Alexeeva, Gus Hahn-Powell, Keith Alcock, Kevin McLaughlin, Doug McCorkle, Gabe McNunn, Mihai Surdeanu",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 00:10:24",
    "ori_summary": "The current trend in information extraction (IE) is to rely extensively on large language models, effectively discarding decades of experience in building symbolic or statistical IE systems. This paper compares a neuro-symbolic (NS) and an LLM-based IE system in the agricultural domain, evaluating them on nine interviews across pork, dairy, and crop subdomains. The LLM-based system outperforms the NS one (F1 total: 69.4 vs. 52.7; core: 63.0 vs. 47.2), where total includes all extracted information and core focuses on essential details. However, each system has trade-offs: the NS approach offers faster runtime, greater control, and high accuracy in context-free tasks but lacks generalizability, struggles with contextual nuances, and requires significant resources to develop and maintain. The LLM-based system achieves higher performance, faster deployment, and easier maintenance but has slower runtime, limited control, model dependency and hallucination risks. Our findings highlight the \"hidden cost\" of deploying NLP systems in real-world applications, emphasizing the need to balance performance, efficiency, and control.",
    "summary": "",
    "translation": "对话转录文本的信息抽取：神经符号方法 vs 大语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要比较神经符号方法和LLM在对话信息抽取中的表现，属于通用NLP任务范畴。虽然信息抽取技术理论上可以应用于搜索中的查询理解或推荐中的用户意图识别，但论文本身专注于对话转录这一特定场景，与RecSys/Search/Ads的核心领域进展或直接应用关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12798v1": {
    "title": "Detect Anything via Next Point Prediction",
    "url": "https://www.alphaxiv.org/abs/2510.12798v1",
    "arxiv_id": "2510.12798v1",
    "authors": "Qing Jiang, Junan Huo, Xingyu Chen, Yuda Xiong, Zhaoyang Zeng, Yihao Chen, Tianhe Ren, Junzhi Yu, Lei Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:59:54",
    "ori_summary": "Object detection has long been dominated by traditional coordinate regression-based models, such as YOLO, DETR, and Grounding DINO. Although recent efforts have attempted to leverage MLLMs to tackle this task, they face challenges like low recall rate, duplicate predictions, coordinate misalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a 3B-scale MLLM that achieves state-of-the-art object perception performance. On benchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or exceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot setting. This is enabled by three key designs: 1) Task Formulation: we use special tokens to represent quantized coordinates from 0 to 999, reducing the model's learning difficulty and improving token efficiency for coordinate prediction; 2) Data Engines: we construct multiple data engines to generate high-quality grounding, referring, and pointing data, providing semantically rich supervision for training; \\3) Training Pipelines: we employ a two-stage training process, combining supervised fine-tuning on 22 million data with GRPO-based reinforcement post-training. This RL post-training leverages geometry-aware rewards to effectively bridge the discrete-to-continuous coordinate prediction gap, improve box accuracy, and mitigate undesirable behaviors like duplicate predictions that stem from the teacher-guided nature of the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent language understanding enables versatile capabilities such as object referring, pointing, visual prompting, GUI grounding, spatial referring, OCR and key-pointing, all systematically evaluated on dedicated benchmarks. We believe that Rex-Omni paves the way for more versatile and language-aware visual perception systems.",
    "summary": "",
    "translation": "通过下一位置预测检测任意目标",
    "relevance_score": 3,
    "reasoning": "该论文提出了一种通用的检测方法，可能属于计算机视觉领域的基础技术。虽然检测技术在搜索和推荐系统中可用于内容理解（如图像/视频分析），但论文标题未明确表明与推荐系统、搜索或广告的直接关联，且未提及Transformer架构或LLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12796v1": {
    "title": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
    "url": "https://www.alphaxiv.org/abs/2510.12796v1",
    "arxiv_id": "2510.12796v1",
    "authors": "Yingyan Li, Shuyao Shang, Weisong Liu, Bing Zhan, Haochen Wang, Yuqi Wang, Yuntao Chen, Xiaoman Wang, Yasong An, Chufeng Tang, Lu Hou, Lue Fan, Zhaoxiang Zhang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-14 17:59:47",
    "ori_summary": "Scaling Vision-Language-Action (VLA) models on large-scale data offers a promising path to achieving a more generalized driving intelligence. However, VLA models are limited by a ``supervision deficit'': the vast model capacity is supervised by sparse, low-dimensional actions, leaving much of their representational power underutilized. To remedy this, we propose \\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to predict future images. This task generates a dense, self-supervised signal that compels the model to learn the underlying dynamics of the driving environment. We showcase the paradigm's versatility by instantiating it for two dominant VLA archetypes: an autoregressive world model for VLAs that use discrete visual tokens, and a diffusion world model for those operating on continuous visual features. Building on the rich representations learned from world modeling, we introduce a lightweight action expert to address the inference latency for real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a 680x larger in-house dataset demonstrate that DriveVLA-W0 significantly outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling law, showing that performance gains accelerate as the training dataset size increases.",
    "summary": "",
    "translation": "DriveVLA-W0：世界模型在自动驾驶中放大数据缩放定律",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域的世界模型和数据缩放定律，属于特定领域应用。虽然涉及模型架构，但其核心应用场景（自动驾驶）与搜索、推荐、广告系统无关，且没有明确的技术迁移潜力到目标领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12795v1": {
    "title": "CuMPerLay: Learning Cubical Multiparameter Persistence Vectorizations",
    "url": "https://www.alphaxiv.org/abs/2510.12795v1",
    "arxiv_id": "2510.12795v1",
    "authors": "Caner Korkmaz, Brighton Nuwagira, Barış Coşkunuzer, Tolga Birdal",
    "categories": "cs.CV, cs.AI, cs.LG, math.AT, stat.ML",
    "pub_date": "2025-10-14 17:59:01",
    "ori_summary": "We present CuMPerLay, a novel differentiable vectorization layer that enables the integration of Cubical Multiparameter Persistence (CMP) into deep learning pipelines. While CMP presents a natural and powerful way to topologically work with images, its use is hindered by the complexity of multifiltration structures as well as the vectorization of CMP. In face of these challenges, we introduce a new algorithm for vectorizing MP homologies of cubical complexes. Our CuMPerLay decomposes the CMP into a combination of individual, learnable single-parameter persistence, where the bifiltration functions are jointly learned. Thanks to the differentiability, its robust topological feature vectors can be seamlessly used within state-of-the-art architectures such as Swin Transformers. We establish theoretical guarantees for the stability of our vectorization under generalized Wasserstein metrics. Our experiments on benchmark medical imaging and computer vision datasets show the benefit CuMPerLay on classification and segmentation performance, particularly in limited-data scenarios. Overall, CuMPerLay offers a promising direction for integrating global structural information into deep networks for structured image analysis.",
    "summary": "",
    "translation": "CuMPerLay：学习立方多参数持久性向量化",
    "relevance_score": 1,
    "reasoning": "这篇论文涉及拓扑数据分析中的多参数持久性同调，这是一种高度专业化的数学方法，与推荐系统、搜索或广告没有明显关联。该技术专注于代数拓扑中的向量化表示，在当前焦点领域没有已知的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12793v1": {
    "title": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution",
    "url": "https://www.alphaxiv.org/abs/2510.12793v1",
    "arxiv_id": "2510.12793v1",
    "authors": "Long Cui, Weiyun Wang, Jie Shao, Zichen Wen, Gen Luo, Linfeng Zhang, Yanting Zhang, Yu Qiao, Wenhai Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:58:10",
    "ori_summary": "Existing Multimodal Large Language Models (MLLMs) suffer from increased inference costs due to the additional vision tokens introduced by image inputs. In this work, we propose Visual Consistency Learning (ViCO), a novel training algorithm that enables the model to represent images of varying semantic complexities using different numbers of vision tokens. The key idea behind our method is to employ multiple MLP connectors, each with a different image compression ratio, to downsample the vision tokens based on the semantic complexity of the image. During training, we minimize the KL divergence between the responses conditioned on different MLP connectors. At inference time, we introduce an image router, termed Visual Resolution Router (ViR), that automatically selects the appropriate compression rate for each image patch. Compared with existing dynamic high-resolution strategies, which adjust the number of visual tokens based on image resolutions, our method dynamically adapts the number of visual tokens according to semantic complexity. Experimental results demonstrate that our method can reduce the number of vision tokens by up to 50% while maintaining the model's perception, reasoning, and OCR capabilities. We hope this work will contribute to the development of more efficient MLLMs. The code and models will be released to facilitate future research.",
    "summary": "",
    "translation": "ViCO：一种面向语义感知动态高分辨率的训练策略",
    "relevance_score": 2,
    "reasoning": "该论文标题暗示了计算机视觉领域的训练策略，可能涉及分辨率调整或视觉表示学习。虽然标题提到“语义感知”，但缺乏与推荐系统、搜索或广告领域的明确联系。没有证据表明该方法适用于异构数据建模或Transformer架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12789v1": {
    "title": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12789v1",
    "arxiv_id": "2510.12789v1",
    "authors": "Kevin Li, Manuel Brack, Sudeep Katakol, Hareesh Ravi, Ajinkya Kale",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-14 17:57:56",
    "ori_summary": "Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images and text. This separation constrains diffusion models' ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often use the last layer information from VLM, employ multiple visual encoders, or train large unified models jointly for text and image generation, which demands substantial computational resources and large-scale data, limiting its accessibility.We present UniFusion, a diffusion-based generative model conditioned on a frozen large vision-language model (VLM) that serves as a unified multimodal encoder. At the core of UniFusion is the Layerwise Attention Pooling (LAP) mechanism that extracts both high level semantics and low level details from text and visual tokens of a frozen VLM to condition a diffusion generative model. We demonstrate that LAP outperforms other shallow fusion architectures on text-image alignment for generation and faithful transfer of visual information from VLM to the diffusion model which is key for editing. We propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI), which conditions a diffusion transformer (DiT) only on the text tokens generated by the VLM during in-model prompt rewriting. VERIFI combines the alignment of the conditioning distribution with the VLM's reasoning capabilities for increased capabilities and flexibility at inference. In addition, finetuning on editing task not only improves text-image alignment for generation, indicative of cross-modality knowledge transfer, but also exhibits tremendous generalization capabilities. Our model when trained on single image editing, zero-shot generalizes to multiple image references further motivating the unified encoder design of UniFusion.",
    "summary": "",
    "translation": "UniFusion：视觉语言模型作为图像生成中的统一编码器",
    "relevance_score": 2,
    "reasoning": "该论文主要关注图像生成领域的视觉语言模型应用，属于纯粹的视觉内容生成方向。虽然标题提到了统一编码器的概念，但其核心应用场景是图像生成而非推荐系统、搜索或广告中的排名任务，与当前关注点的相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12788v1": {
    "title": "Efficient Real-World Deblurring using Single Images: AIM 2025 Challenge Report",
    "url": "https://www.alphaxiv.org/abs/2510.12788v1",
    "arxiv_id": "2510.12788v1",
    "authors": "Daniel Feijoo, Paula Garrido-Mellado, Marcos V. Conde, Jaesung Rim, Alvaro Garcia, Sunghyun Cho, Radu Timofte",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:57:04",
    "ori_summary": "This paper reviews the AIM 2025 Efficient Real-World Deblurring using Single Images Challenge, which aims to advance in efficient real-blur restoration. The challenge is based on a new test set based on the well known RSBlur dataset. Pairs of blur and degraded images in this dataset are captured using a double-camera system. Participant were tasked with developing solutions to effectively deblur these type of images while fulfilling strict efficiency constraints: fewer than 5 million model parameters and a computational budget under 200 GMACs. A total of 71 participants registered, with 4 teams finally submitting valid solutions. The top-performing approach achieved a PSNR of 31.1298 dB, showcasing the potential of efficient methods in this domain. This paper provides a comprehensive overview of the challenge, compares the proposed solutions, and serves as a valuable reference for researchers in efficient real-world image deblurring.",
    "summary": "",
    "translation": "使用单张图像进行高效真实世界去模糊：AIM 2025挑战赛报告",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的图像去模糊技术，属于纯粹的视觉处理领域。虽然标题提到'真实世界'应用，但该技术主要面向图像质量提升，与推荐系统、搜索或广告中的排序、用户建模、内容理解等核心问题没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12785v1": {
    "title": "MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars",
    "url": "https://www.alphaxiv.org/abs/2510.12785v1",
    "arxiv_id": "2510.12785v1",
    "authors": "Felix Taubner, Ruihang Zhang, Mathieu Tuli, Sherwin Bahmani, David B. Lindell",
    "categories": "cs.CV, cs.AI, cs.GR",
    "pub_date": "2025-10-14 17:56:14",
    "ori_summary": "Digital human avatars aim to simulate the dynamic appearance of humans in virtual environments, enabling immersive experiences across gaming, film, virtual reality, and more. However, the conventional process for creating and animating photorealistic human avatars is expensive and time-consuming, requiring large camera capture rigs and significant manual effort from professional 3D artists. With the advent of capable image and video generation models, recent methods enable automatic rendering of realistic animated avatars from a single casually captured reference image of a target subject. While these techniques significantly lower barriers to avatar creation and offer compelling realism, they lack constraints provided by multi-view information or an explicit 3D representation. So, image quality and realism degrade when rendered from viewpoints that deviate strongly from the reference image. Here, we build a video model that generates animatable multi-view videos of digital humans based on a single reference image and target expressions. Our model, MVP4D, is based on a state-of-the-art pre-trained video diffusion model and generates hundreds of frames simultaneously from viewpoints varying by up to 360 degrees around a target subject. We show how to distill the outputs of this model into a 4D avatar that can be rendered in real-time. Our approach significantly improves the realism, temporal consistency, and 3D consistency of generated avatars compared to previous methods.",
    "summary": "",
    "translation": "MVP4D：基于多视角肖像视频扩散的可动画化4D虚拟人生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于4D虚拟人动画生成技术，属于计算机图形学和视觉内容生成领域。虽然标题提到扩散模型，但核心应用是虚拟人动画而非推荐系统、搜索或广告的排名与建模。该技术缺乏在推荐、搜索或广告中的直接应用潜力，属于纯粹的视觉内容生成范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12777v1": {
    "title": "What If : Understanding Motion Through Sparse Interactions",
    "url": "https://www.alphaxiv.org/abs/2510.12777v1",
    "arxiv_id": "2510.12777v1",
    "authors": "Stefan Andreas Baumann, Nick Stracke, Timy Phan, Björn Ommer",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:52:17",
    "ori_summary": "Understanding the dynamics of a physical scene involves reasoning about the diverse ways it can potentially change, especially as a result of local interactions. We present the Flow Poke Transformer (FPT), a novel framework for directly predicting the distribution of local motion, conditioned on sparse interactions termed \"pokes\". Unlike traditional methods that typically only enable dense sampling of a single realization of scene dynamics, FPT provides an interpretable directly accessible representation of multi-modal scene motion, its dependency on physical interactions and the inherent uncertainties of scene dynamics. We also evaluate our model on several downstream tasks to enable comparisons with prior methods and highlight the flexibility of our approach. On dense face motion generation, our generic pre-trained model surpasses specialized baselines. FPT can be fine-tuned in strongly out-of-distribution tasks such as synthetic datasets to enable significant improvements over in-domain methods in articulated object motion estimation. Additionally, predicting explicit motion distributions directly enables our method to achieve competitive performance on tasks like moving part segmentation from pokes which further demonstrates the versatility of our FPT. Code and models are publicly available at https://compvis.github.io/flow-poke-transformer.",
    "summary": "",
    "translation": "假设：通过稀疏交互理解运动",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于计算机视觉中的运动理解，涉及稀疏交互模式分析。这与推荐系统、搜索或广告的核心技术领域没有直接关联，也不涉及LLM、Transformer架构或异构数据建模等关键技术。即使考虑潜在应用，运动理解在RecSys/Search/Ads领域的应用场景非常有限且不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12768v1": {
    "title": "Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2510.12768v1",
    "arxiv_id": "2510.12768v1",
    "authors": "Fengzhi Guo, Chih-Chuan Hsu, Sihao Ding, Cheng Zhang",
    "categories": "cs.CV, cs.AI, cs.GR",
    "pub_date": "2025-10-14 17:47:11",
    "ori_summary": "Reconstructing dynamic 3D scenes from monocular input is fundamentally under-constrained, with ambiguities arising from occlusion and extreme novel views. While dynamic Gaussian Splatting offers an efficient representation, vanilla models optimize all Gaussian primitives uniformly, ignoring whether they are well or poorly observed. This limitation leads to motion drifts under occlusion and degraded synthesis when extrapolating to unseen views. We argue that uncertainty matters: Gaussians with recurring observations across views and time act as reliable anchors to guide motion, whereas those with limited visibility are treated as less reliable. To this end, we introduce USplat4D, a novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates reliable motion cues to enhance 4D reconstruction. Our key insight is to estimate time-varying per-Gaussian uncertainty and leverages it to construct a spatio-temporal graph for uncertainty-aware optimization. Experiments on diverse real and synthetic datasets show that explicitly modeling uncertainty consistently improves dynamic Gaussian Splatting models, yielding more stable geometry under occlusion and high-quality synthesis at extreme viewpoints.",
    "summary": "",
    "translation": "不确定性在用于单目4D重建的动态高斯溅射中至关重要",
    "relevance_score": 2,
    "reasoning": "这篇论文主要关注计算机视觉中的4D重建技术，使用高斯溅射方法处理动态场景。虽然技术上先进，但与搜索、推荐或广告系统的核心领域进展、LLM技术应用或Transformer架构改进没有直接关联。该方法可能对某些特定场景的视觉理解有潜在价值，但与当前关注点的相关性非常有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12765v1": {
    "title": "Efficient Perceptual Image Super Resolution: AIM 2025 Study and Benchmark",
    "url": "https://www.alphaxiv.org/abs/2510.12765v1",
    "arxiv_id": "2510.12765v1",
    "authors": "Bruno Longarela, Marcos V. Conde, Alvaro Garcia, Radu Timofte",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:45:22",
    "ori_summary": "This paper presents a comprehensive study and benchmark on Efficient Perceptual Super-Resolution (EPSR). While significant progress has been made in efficient PSNR-oriented super resolution, approaches focusing on perceptual quality metrics remain relatively inefficient. Motivated by this gap, we aim to replicate or improve the perceptual results of Real-ESRGAN while meeting strict efficiency constraints: a maximum of 5M parameters and 2000 GFLOPs, calculated for an input size of 960x540 pixels. The proposed solutions were evaluated on a novel dataset consisting of 500 test images of 4K resolution, each degraded using multiple degradation types, without providing the original high-quality counterparts. This design aims to reflect realistic deployment conditions and serves as a diverse and challenging benchmark. The top-performing approach manages to outperform Real-ESRGAN across all benchmark datasets, demonstrating the potential of efficient methods in the perceptual domain. This paper establishes the modern baselines for efficient perceptual super resolution.",
    "summary": "",
    "translation": "高效感知图像超分辨率：AIM 2025研究与基准测试",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的图像超分辨率技术，属于纯粹的视觉处理任务。虽然标题提到'高效'可能涉及计算优化，但论文核心是图像增强而非推荐系统、搜索或广告应用。图像超分辨率在推荐/搜索/广告中的潜在应用非常有限且间接，不符合任何当前关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12764v1": {
    "title": "AnyUp: Universal Feature Upsampling",
    "url": "https://www.alphaxiv.org/abs/2510.12764v1",
    "arxiv_id": "2510.12764v1",
    "authors": "Thomas Wimmer, Prune Truong, Marie-Julie Rakotosaona, Michael Oechsle, Federico Tombari, Bernt Schiele, Jan Eric Lenssen",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-14 17:45:17",
    "ori_summary": "We introduce AnyUp, a method for feature upsampling that can be applied to any vision feature at any resolution, without encoder-specific training. Existing learning-based upsamplers for features like DINO or CLIP need to be re-trained for every feature extractor and thus do not generalize to different feature types at inference time. In this work, we propose an inference-time feature-agnostic upsampling architecture to alleviate this limitation and improve upsampling quality. In our experiments, AnyUp sets a new state of the art for upsampled features, generalizes to different feature types, and preserves feature semantics while being efficient and easy to apply to a wide range of downstream tasks.",
    "summary": "",
    "translation": "AnyUp：通用特征上采样",
    "relevance_score": 6,
    "reasoning": "特征上采样技术是Transformer架构中的核心效率优化技术，在推荐系统和搜索系统中具有直接应用价值。通过高效的特征上采样，可以提升序列建模、特征交互和表示学习的能力，这对于处理用户行为序列和多模态特征融合至关重要。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.12758v1": {
    "title": "PET Head Motion Estimation Using Supervised Deep Learning with Attention",
    "url": "https://www.alphaxiv.org/abs/2510.12758v1",
    "arxiv_id": "2510.12758v1",
    "authors": "Zhuotong Cai, Tianyi Zeng, Jiazhen Zhang, Eléonore V. Lieffrig, Kathryn Fontaine, Chenyu You, Enette Mae Revilla, James S. Duncan, Jingmin Xin, Yihuan Lu, John A. Onofrey",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:37:12",
    "ori_summary": "Head movement poses a significant challenge in brain positron emission tomography (PET) imaging, resulting in image artifacts and tracer uptake quantification inaccuracies. Effective head motion estimation and correction are crucial for precise quantitative image analysis and accurate diagnosis of neurological disorders. Hardware-based motion tracking (HMT) has limited applicability in real-world clinical practice. To overcome this limitation, we propose a deep-learning head motion correction approach with cross-attention (DL-HMC++) to predict rigid head motion from one-second 3D PET raw data. DL-HMC++ is trained in a supervised manner by leveraging existing dynamic PET scans with gold-standard motion measurements from external HMT. We evaluate DL-HMC++ on two PET scanners (HRRT and mCT) and four radiotracers (18F-FDG, 18F-FPEB, 11C-UCB-J, and 11C-LSN3172176) to demonstrate the effectiveness and generalization of the approach in large cohort PET studies. Quantitative and qualitative results demonstrate that DL-HMC++ consistently outperforms state-of-the-art data-driven motion estimation methods, producing motion-free images with clear delineation of brain structures and reduced motion artifacts that are indistinguishable from gold-standard HMT. Brain region of interest standard uptake value analysis exhibits average difference ratios between DL-HMC++ and gold-standard HMT to be 1.2 plus-minus 0.5% for HRRT and 0.5 plus-minus 0.2% for mCT. DL-HMC++ demonstrates the potential for data-driven PET head motion correction to remove the burden of HMT, making motion correction accessible to clinical populations beyond research settings. The code is available at https://github.com/maxxxxxxcai/DL-HMC-TMI.",
    "summary": "",
    "translation": "基于注意力机制的监督式深度学习在PET头部运动估计中的应用",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（PET）中的运动估计问题，属于医学/生物医学领域，与推荐系统、搜索或广告完全无关。论文中提到的注意力机制虽然与Transformer相关，但应用于医学影像处理，没有明显的推荐系统、搜索或广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12753v1": {
    "title": "E-MoFlow: Learning Egomotion and Optical Flow from Event Data via Implicit Regularization",
    "url": "https://www.alphaxiv.org/abs/2510.12753v1",
    "arxiv_id": "2510.12753v1",
    "authors": "Wenpu Li, Bangyan Liao, Yi Zhou, Qi Xu, Pian Wan, Peidong Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:33:44",
    "ori_summary": "The estimation of optical flow and 6-DoF ego-motion, two fundamental tasks in 3D vision, has typically been addressed independently. For neuromorphic vision (e.g., event cameras), however, the lack of robust data association makes solving the two problems separately an ill-posed challenge, especially in the absence of supervision via ground truth. Existing works mitigate this ill-posedness by either enforcing the smoothness of the flow field via an explicit variational regularizer or leveraging explicit structure-and-motion priors in the parametrization to improve event alignment. The former notably introduces bias in results and computational overhead, while the latter, which parametrizes the optical flow in terms of the scene depth and the camera motion, often converges to suboptimal local minima. To address these issues, we propose an unsupervised framework that jointly optimizes egomotion and optical flow via implicit spatial-temporal and geometric regularization. First, by modeling camera's egomotion as a continuous spline and optical flow as an implicit neural representation, our method inherently embeds spatial-temporal coherence through inductive biases. Second, we incorporate structure-and-motion priors through differential geometric constraints, bypassing explicit depth estimation while maintaining rigorous geometric consistency. As a result, our framework (called E-MoFlow) unifies egomotion and optical flow estimation via implicit regularization under a fully unsupervised paradigm. Experiments demonstrate its versatility to general 6-DoF motion scenarios, achieving state-of-the-art performance among unsupervised methods and competitive even with supervised approaches.",
    "summary": "",
    "translation": "E-MoFlow：通过隐式正则化从事件数据中学习自我运动和光流",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉中的事件相机数据处理，涉及自我运动估计和光流计算，属于纯粹的视觉技术范畴。该工作与推荐系统、搜索或广告领域没有明显的直接关联，也不涉及LLM技术、Transformer架构或异构数据建模，因此完全不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12750v1": {
    "title": "VQArt-Bench: A semantically rich VQA Benchmark for Art and Cultural Heritage",
    "url": "https://www.alphaxiv.org/abs/2510.12750v1",
    "arxiv_id": "2510.12750v1",
    "authors": "A. Alfarano, L. Venturoli, D. Negueruela del Castillo",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-14 17:29:52",
    "ori_summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant capabilities in joint visual and linguistic tasks. However, existing Visual Question Answering (VQA) benchmarks often fail to evaluate deep semantic understanding, particularly in complex domains like visual art analysis. Confined to simple syntactic structures and surface-level attributes, these questions fail to capture the diversity and depth of human visual inquiry. This limitation incentivizes models to exploit statistical shortcuts rather than engage in visual reasoning. To address this gap, we introduce VQArt-Bench, a new, large-scale VQA benchmark for the cultural heritage domain. This benchmark is constructed using a novel multi-agent pipeline where specialized agents collaborate to generate nuanced, validated, and linguistically diverse questions. The resulting benchmark is structured along relevant visual understanding dimensions that probe a model's ability to interpret symbolic meaning, narratives, and complex visual relationships. Our evaluation of 14 state-of-the-art MLLMs on this benchmark reveals significant limitations in current models, including a surprising weakness in simple counting tasks and a clear performance gap between proprietary and open-source models.",
    "summary": "",
    "translation": "VQArt-Bench：一个面向艺术与文化遗产的语义丰富视觉问答基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于艺术与文化遗产领域的视觉问答基准测试，属于纯粹的视觉语言模型评估范畴。虽然涉及VLM技术，但应用领域（艺术文化遗产）与推荐系统、搜索或广告无关，且没有展示在相关领域应用的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12749v1": {
    "title": "SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.12749v1",
    "arxiv_id": "2510.12749v1",
    "authors": "Zhiliu Yang, Jinyu Dai, Jianyuan Zhang, Zhu Yang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:28:19",
    "ori_summary": "The scene perception, understanding, and simulation are fundamental techniques for embodied-AI agents, while existing solutions are still prone to segmentation deficiency, dynamic objects' interference, sensor data sparsity, and view-limitation problems. This paper proposes a novel framework, named SPORTS, for holistic scene understanding via tightly integrating Video Panoptic Segmentation (VPS), Visual Odometry (VO), and Scene Rendering (SR) tasks into an iterative and unified perspective. Firstly, VPS designs an adaptive attention-based geometric fusion mechanism to align cross-frame features via enrolling the pose, depth, and optical flow modality, which automatically adjust feature maps for different decoding stages. And a post-matching strategy is integrated to improve identities tracking. In VO, panoptic segmentation results from VPS are combined with the optical flow map to improve the confidence estimation of dynamic objects, which enhances the accuracy of the camera pose estimation and completeness of the depth map generation via the learning-based paradigm. Furthermore, the point-based rendering of SR is beneficial from VO, transforming sparse point clouds into neural fields to synthesize high-fidelity RGB views and twin panoptic views. Extensive experiments on three public datasets demonstrate that our attention-based feature fusion outperforms most existing state-of-the-art methods on the odometry, tracking, segmentation, and novel view synthesis tasks.",
    "summary": "",
    "translation": "SPORTS：面向城市场景理解的同步全景里程计、渲染、跟踪与分割",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的城市场景理解，涉及里程计、渲染、跟踪和分割等纯视觉任务。这些技术主要面向自动驾驶和机器人导航领域，与推荐系统、搜索或广告的核心技术栈没有直接关联。论文内容属于纯粹的视觉技术范畴，没有展示在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12747v1": {
    "title": "FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution",
    "url": "https://www.alphaxiv.org/abs/2510.12747v1",
    "arxiv_id": "2510.12747v1",
    "authors": "Junhao Zhuang, Shi Guo, Xin Cai, Xiaohui Li, Yihao Liu, Chun Yuan, Tianfan Xue",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:25:54",
    "ori_summary": "Diffusion models have recently advanced video restoration, but applying them to real-world video super-resolution (VSR) remains challenging due to high latency, prohibitive computation, and poor generalization to ultra-high resolutions. Our goal in this work is to make diffusion-based VSR practical by achieving efficiency, scalability, and real-time performance. To this end, we propose FlashVSR, the first diffusion-based one-step streaming framework towards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408 videos on a single A100 GPU by combining three complementary innovations: (i) a train-friendly three-stage distillation pipeline that enables streaming super-resolution, (ii) locality-constrained sparse attention that cuts redundant computation while bridging the train-test resolution gap, and (iii) a tiny conditional decoder that accelerates reconstruction without sacrificing quality. To support large-scale training, we also construct VSR-120K, a new dataset with 120k videos and 180k images. Extensive experiments show that FlashVSR scales reliably to ultra-high resolutions and achieves state-of-the-art performance with up to 12x speedup over prior one-step diffusion VSR models. We will release the code, pretrained models, and dataset to foster future research in efficient diffusion-based VSR.",
    "summary": "",
    "translation": "FlashVSR：面向实时基于扩散模型的流式视频超分辨率",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频超分辨率技术，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。扩散模型在该论文中用于视觉质量提升，而非在推荐、搜索或广告场景中的序列建模、特征表示或用户理解等应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12741v1": {
    "title": "Personalized Federated Fine-Tuning of Vision Foundation Models for Healthcare",
    "url": "https://www.alphaxiv.org/abs/2510.12741v1",
    "arxiv_id": "2510.12741v1",
    "authors": "Adam Tupper, Christian Gagné",
    "categories": "cs.CV, cs.DC",
    "pub_date": "2025-10-14 17:18:12",
    "ori_summary": "Foundation models open up new possibilities for the use of AI in healthcare. However, even when pre-trained on health data, they still need to be fine-tuned for specific downstream tasks. Furthermore, although foundation models reduce the amount of training data required to achieve good performance, obtaining sufficient data is still a challenge. This is due, in part, to restrictions on sharing and aggregating data from different sources to protect patients' privacy. One possible solution to this is to fine-tune foundation models via federated learning across multiple participating clients (i.e., hospitals, clinics, etc.). In this work, we propose a new personalized federated fine-tuning method that learns orthogonal LoRA adapters to disentangle general and client-specific knowledge, enabling each client to fully exploit both their own data and the data of others. Our preliminary results on real-world federated medical imaging tasks demonstrate that our approach is competitive against current federated fine-tuning methods.",
    "summary": "",
    "translation": "面向医疗保健的个性化联邦微调视觉基础模型",
    "relevance_score": 1,
    "reasoning": "该论文涉及医疗领域应用和联邦学习，这两个主题均被明确列为无关主题。虽然提到了基础模型微调，但医疗领域的特定应用使其与推荐系统、搜索或广告领域完全不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12712v1": {
    "title": "Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.12712v1",
    "arxiv_id": "2510.12712v1",
    "authors": "Xingang Guo, Utkarsh Tyagi, Advait Gosai, Paula Vergara, Ernesto Gabriel Hernández Montoya, Chen Bo Calvin Zhang, Bin Hu, Yunzhong He, Bing Liu, Rakshith Sharma Srinivasa",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-14 16:50:49",
    "ori_summary": "Multimodal Large Language Models (MLLMs) are increasingly applied in real-world scenarios where user-provided images are often imperfect, requiring active image manipulations such as cropping, editing, or enhancement to uncover salient visual cues. Beyond static visual perception, MLLMs must also think with images: dynamically transforming visual content and integrating it with other tools to solve complex tasks. However, this shift from treating vision as passive context to a manipulable cognitive workspace remains underexplored. Most existing benchmarks still follow a think about images paradigm, where images are regarded as static inputs. To address this gap, we introduce IRIS, an Interactive Reasoning with Images and Systems that evaluates MLLMs' ability to perceive, transform, and reason across complex visual-textual tasks under the think with images paradigm. IRIS comprises 1,204 challenging, open-ended vision tasks (603 single-turn, 601 multi-turn) spanning across five diverse domains, each paired with detailed rubrics to enable systematic evaluation. Our evaluation shows that current MLLMs struggle with tasks requiring effective integration of vision and general-purpose tools. Even the strongest model (GPT-5-think) reaches only 18.68% pass rate. We further observe divergent tool-use behaviors, with OpenAI models benefiting from diverse image manipulations while Gemini-2.5-pro shows no improvement. By introducing the first benchmark centered on think with images, IRIS offers critical insights for advancing visual intelligence in MLLMs.",
    "summary": "",
    "translation": "超越视觉：评估基于工具的多模态大语言模型在图像感知、转换与推理任务上的表现",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态LLM在图像处理工具上的评估，属于纯粹的视觉-语言模型评估范畴。虽然提到了工具增强能力，但核心焦点是图像感知和推理的基准测试，与推荐系统、搜索或广告中的异构数据处理没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12704v1": {
    "title": "Hybrid Explanation-Guided Learning for Transformer-Based Chest X-Ray Diagnosis",
    "url": "https://www.alphaxiv.org/abs/2510.12704v1",
    "arxiv_id": "2510.12704v1",
    "authors": "Shelley Zixin Shu, Haozhe Luo, Alexander Poellinger, Mauricio Reyes",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-14 16:39:02",
    "ori_summary": "Transformer-based deep learning models have demonstrated exceptional performance in medical imaging by leveraging attention mechanisms for feature representation and interpretability. However, these models are prone to learning spurious correlations, leading to biases and limited generalization. While human-AI attention alignment can mitigate these issues, it often depends on costly manual supervision. In this work, we propose a Hybrid Explanation-Guided Learning (H-EGL) framework that combines self-supervised and human-guided constraints to enhance attention alignment and improve generalization. The self-supervised component of H-EGL leverages class-distinctive attention without relying on restrictive priors, promoting robustness and flexibility. We validate our approach on chest X-ray classification using the Vision Transformer (ViT), where H-EGL outperforms two state-of-the-art Explanation-Guided Learning (EGL) methods, demonstrating superior classification accuracy and generalization capability. Additionally, it produces attention maps that are better aligned with human expertise.",
    "summary": "",
    "translation": "基于Transformer的胸部X光诊断的混合解释引导学习",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像诊断（胸部X光），属于明确的医学领域应用，这在无关主题中被明确排除。虽然使用了Transformer架构，但应用场景与搜索、推荐或广告系统完全无关，且没有证据表明该方法可以迁移到这些领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12691v1": {
    "title": "DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization",
    "url": "https://www.alphaxiv.org/abs/2510.12691v1",
    "arxiv_id": "2510.12691v1",
    "authors": "Danial Hosseintabar, Fan Chen, Giannis Daras, Antonio Torralba, Constantinos Daskalakis",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-14 16:25:02",
    "ori_summary": "Diffusion models have emerged as powerful generative priors for high-dimensional inverse problems, yet learning them when only corrupted or noisy observations are available remains challenging. In this work, we propose a new method for training diffusion models with Expectation-Maximization (EM) from corrupted data. Our proposed method, DiffEM, utilizes conditional diffusion models to reconstruct clean data from observations in the E-step, and then uses the reconstructed data to refine the conditional diffusion model in the M-step. Theoretically, we provide monotonic convergence guarantees for the DiffEM iteration, assuming appropriate statistical conditions. We demonstrate the effectiveness of our approach through experiments on various image reconstruction tasks.",
    "summary": "",
    "translation": "DiffEM：通过期望最大化利用扩散模型从损坏数据中学习",
    "relevance_score": 3,
    "reasoning": "该论文主要关注扩散模型在损坏数据上的学习问题，属于生成模型的技术改进。虽然扩散模型是LLM相关技术，但论文聚焦于数据损坏场景下的学习算法，与推荐系统、搜索或广告的核心应用关联较弱。期望最大化算法可能对处理推荐系统中的噪声数据有潜在价值，但论文标题未明确指向这些领域的具体应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12687v1": {
    "title": "EReLiFM: Evidential Reliability-Aware Residual Flow Meta-Learning for Open-Set Domain Generalization under Noisy Labels",
    "url": "https://www.alphaxiv.org/abs/2510.12687v1",
    "arxiv_id": "2510.12687v1",
    "authors": "Kunyu Peng, Di Wen, Kailun Yang, Jia Fu, Yufan Chen, Ruiping Liu, Jiamin Wu, Junwei Zheng, M. Saquib Sarfraz, Luc Van Gool, Danda Pani Paudel, Rainer Stiefelhagen",
    "categories": "cs.CV, cs.LG, cs.RO",
    "pub_date": "2025-10-14 16:23:11",
    "ori_summary": "Open-Set Domain Generalization (OSDG) aims to enable deep learning models to recognize unseen categories in new domains, which is crucial for real-world applications. Label noise hinders open-set domain generalization by corrupting source-domain knowledge, making it harder to recognize known classes and reject unseen ones. While existing methods address OSDG under Noisy Labels (OSDG-NL) using hyperbolic prototype-guided meta-learning, they struggle to bridge domain gaps, especially with limited clean labeled data. In this paper, we propose Evidential Reliability-Aware Residual Flow Meta-Learning (EReLiFM). We first introduce an unsupervised two-stage evidential loss clustering method to promote label reliability awareness. Then, we propose a residual flow matching mechanism that models structured domain- and category-conditioned residuals, enabling diverse and uncertainty-aware transfer paths beyond interpolation-based augmentation. During this meta-learning process, the model is optimized such that the update direction on the clean set maximizes the loss decrease on the noisy set, using pseudo labels derived from the most confident predicted class for supervision. Experimental results show that EReLiFM outperforms existing methods on OSDG-NL, achieving state-of-the-art performance. The source code is available at https://github.com/KPeng9510/ERELIFM.",
    "summary": "",
    "translation": "EReLiFM：面向噪声标签下开放集领域泛化的证据可靠性感知残差流元学习",
    "relevance_score": 2,
    "reasoning": "该论文主要关注噪声标签下的开放集领域泛化问题，虽然涉及元学习技术，但其核心焦点是噪声鲁棒性和领域泛化，与推荐系统、搜索或广告中的核心进展或LLM技术应用关联度较低。元学习技术本身在推荐系统中可能有潜在应用，但论文的具体技术方向（证据可靠性、残差流）与当前关注的核心领域进展或LLM使能技术缺乏直接联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12679v1": {
    "title": "MCOP: Multi-UAV Collaborative Occupancy Prediction",
    "url": "https://www.alphaxiv.org/abs/2510.12679v1",
    "arxiv_id": "2510.12679v1",
    "authors": "Zefu Lin, Wenbo Chen, Xiaojuan Jin, Yuran Yang, Lue Fan, Yixin Zhang, Yufeng Zhang, Zhaoxiang Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 16:17:42",
    "ori_summary": "Unmanned Aerial Vehicle (UAV) swarm systems necessitate efficient collaborative perception mechanisms for diverse operational scenarios. Current Bird's Eye View (BEV)-based approaches exhibit two main limitations: bounding-box representations fail to capture complete semantic and geometric information of the scene, and their performance significantly degrades when encountering undefined or occluded objects. To address these limitations, we propose a novel multi-UAV collaborative occupancy prediction framework. Our framework effectively preserves 3D spatial structures and semantics through integrating a Spatial-Aware Feature Encoder and Cross-Agent Feature Integration. To enhance efficiency, we further introduce Altitude-Aware Feature Reduction to compactly represent scene information, along with a Dual-Mask Perceptual Guidance mechanism to adaptively select features and reduce communication overhead. Due to the absence of suitable benchmark datasets, we extend three datasets for evaluation: two virtual datasets (Air-to-Pred-Occ and UAV3D-Occ) and one real-world dataset (GauUScene-Occ). Experiments results demonstrate that our method achieves state-of-the-art accuracy, significantly outperforming existing collaborative methods while reducing communication overhead to only a fraction of previous approaches.",
    "summary": "",
    "translation": "MCOP：多无人机协同占用预测",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于无人机协同感知和占用预测，属于机器人学和自主系统领域。虽然涉及多智能体协作，但核心技术与推荐系统、搜索或广告没有直接关联，也不涉及LLM、Transformer架构或异构数据建模。该工作主要面向物理世界感知和路径规划应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12670v1": {
    "title": "TerraCodec: Compressing Earth Observations",
    "url": "https://www.alphaxiv.org/abs/2510.12670v1",
    "arxiv_id": "2510.12670v1",
    "authors": "Julen Costa-Watanabe, Isabelle Wittmann, Benedikt Blumenstiel, Konrad Schindler",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 16:05:31",
    "ori_summary": "Earth observation (EO) satellites produce massive streams of multispectral image time series, posing pressing challenges for storage and transmission. Yet, learned EO compression remains fragmented, lacking publicly available pretrained models and misaligned with advances in compression for natural imagery. Image codecs overlook temporal redundancy, while video codecs rely on motion priors that fail to capture the radiometric evolution of largely static scenes. We introduce TerraCodec (TEC), a family of learned codecs tailored to EO. TEC includes efficient image-based variants adapted to multispectral inputs, as well as a Temporal Transformer model (TEC-TT) that leverages dependencies across time. To overcome the fixed-rate setting of today's neural codecs, we present Latent Repacking, a novel method for training flexible-rate transformer models that operate on varying rate-distortion settings. Trained on Sentinel-2 data, TerraCodec outperforms classical codecs, achieving 3-10x stronger compression at equivalent image quality. Beyond compression, TEC-TT enables zero-shot cloud inpainting, surpassing state-of-the-art methods on the AllClear benchmark. Our results establish bespoke, learned compression algorithms as a promising direction for Earth observation. Code and model weights will be released under a permissive license.",
    "summary": "",
    "translation": "TerraCodec：地球观测数据压缩",
    "relevance_score": 1,
    "reasoning": "该论文专注于地球观测数据的压缩技术，属于遥感或地理信息系统领域。与推荐系统、搜索、广告或LLM技术没有明显关联，也不涉及Transformer架构或多模态建模。地球观测数据压缩属于特定领域应用，不在当前关注范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12660v1": {
    "title": "On the Use of Hierarchical Vision Foundation Models for Low-Cost Human Mesh Recovery and Pose Estimation",
    "url": "https://www.alphaxiv.org/abs/2510.12660v1",
    "arxiv_id": "2510.12660v1",
    "authors": "Shuhei Tarashima, Yushan Wang, Norio Tagawa",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 15:57:40",
    "ori_summary": "In this work, we aim to develop simple and efficient models for human mesh recovery (HMR) and its predecessor task, human pose estimation (HPE). State-of-the-art HMR methods, such as HMR2.0 and its successors, rely on large, non-hierarchical vision transformers as encoders, which are inherited from the corresponding HPE models like ViTPose. To establish baselines across varying computational budgets, we first construct three lightweight HMR2.0 variants by adapting the corresponding ViTPose models. In addition, we propose leveraging the early stages of hierarchical vision foundation models (VFMs), including Swin Transformer, GroupMixFormer, and VMamba, as encoders. This design is motivated by the observation that intermediate stages of hierarchical VFMs produce feature maps with resolutions comparable to or higher than those of non-hierarchical counterparts. We conduct a comprehensive evaluation of 27 hierarchical-VFM-based HMR and HPE models, demonstrating that using only the first two or three stages achieves performance on par with full-stage models. Moreover, we show that the resulting truncated models exhibit better trade-offs between accuracy and computational efficiency compared to existing lightweight alternatives.",
    "summary": "",
    "translation": "关于使用分层视觉基础模型进行低成本人体网格恢复与姿态估计的研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的人体姿态估计和网格恢复，属于纯粹的视觉技术研究。虽然标题中提到'基础模型'，但内容聚焦于视觉模态的特定应用，与推荐系统、搜索或广告的核心技术需求没有直接关联，也没有展示出在异构数据处理方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12646v1": {
    "title": "Zero-Shot CFC: Fast Real-World Image Denoising based on Cross-Frequency Consistency",
    "url": "https://www.alphaxiv.org/abs/2510.12646v1",
    "arxiv_id": "2510.12646v1",
    "authors": "Yanlin Jiang, Yuchen Liu, Mingren Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 15:35:59",
    "ori_summary": "Zero-shot denoisers address the dataset dependency of deep-learning-based denoisers, enabling the denoising of unseen single images. Nonetheless, existing zero-shot methods suffer from long training times and rely on the assumption of noise independence and a zero-mean property, limiting their effectiveness in real-world denoising scenarios where noise characteristics are more complicated. This paper proposes an efficient and effective method for real-world denoising, the Zero-Shot denoiser based on Cross-Frequency Consistency (ZSCFC), which enables training and denoising with a single noisy image and does not rely on assumptions about noise distribution. Specifically, image textures exhibit position similarity and content consistency across different frequency bands, while noise does not. Based on this property, we developed cross-frequency consistency loss and an ultralight network to realize image denoising. Experiments on various real-world image datasets demonstrate that our ZSCFC outperforms other state-of-the-art zero-shot methods in terms of computational efficiency and denoising performance.",
    "summary": "",
    "translation": "零样本跨频一致性：基于跨频一致性的快速真实世界图像去噪",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的图像去噪技术，属于纯粹的图像处理研究方向。虽然提到了'零样本'概念，但核心内容是图像去噪算法，与推荐系统、搜索或广告的排名和建模任务没有直接关联。该技术缺乏在RecSys/Search/Ads领域的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12605v1": {
    "title": "WaterFlow: Explicit Physics-Prior Rectified Flow for Underwater Saliency Mask Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12605v1",
    "arxiv_id": "2510.12605v1",
    "authors": "Runting Li, Shijie Lian, Hua Li, Yutong Li, Wenhui Wu, Sam Kwong",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 15:02:24",
    "ori_summary": "Underwater Salient Object Detection (USOD) faces significant challenges, including underwater image quality degradation and domain gaps. Existing methods tend to ignore the physical principles of underwater imaging or simply treat degradation phenomena in underwater images as interference factors that must be eliminated, failing to fully exploit the valuable information they contain. We propose WaterFlow, a rectified flow-based framework for underwater salient object detection that innovatively incorporates underwater physical imaging information as explicit priors directly into the network training process and introduces temporal dimension modeling, significantly enhancing the model's capability for salient object identification. On the USOD10K dataset, WaterFlow achieves a 0.072 gain in S_m, demonstrating the effectiveness and superiority of our method. The code will be published after the acceptance.",
    "summary": "",
    "translation": "WaterFlow：用于水下显著性掩码生成的显式物理先验校正流",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的水下显著性检测，涉及物理先验和图像生成技术。这与我的关注领域（推荐系统、搜索、广告及相关的LLM/Transformer技术）完全无关，因为水下显著性掩码生成在RecSys/Search/Ads中没有任何实际应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12586v1": {
    "title": "Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training",
    "url": "https://www.alphaxiv.org/abs/2510.12586v1",
    "arxiv_id": "2510.12586v1",
    "authors": "Jiachen Lei, Keli Liu, Julius Berner, Haiming Yu, Hongkai Zheng, Jiahong Wu, Xiangxiang Chu",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 14:41:16",
    "ori_summary": "Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion and consistency models. In the first stage, we pre-train encoders to capture meaningful semantics from clean images while aligning them with points along the same deterministic sampling trajectory, which evolves points from the prior to the data distribution. In the second stage, we integrate the encoder with a randomly initialized decoder and fine-tune the complete model end-to-end for both diffusion and consistency models. Our training framework demonstrates strong empirical performance on ImageNet dataset. Specifically, our diffusion model reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75 number of function evaluations (NFE), surpassing prior pixel-space methods by a large margin in both generation quality and efficiency while rivaling leading VAE-based models at comparable training cost. Furthermore, on ImageNet-256, our consistency model achieves an impressive FID of 8.82 in a single sampling step, significantly surpassing its latent-space counterpart. To the best of our knowledge, this marks the first successful training of a consistency model directly on high-resolution images without relying on pre-trained VAEs or diffusion models.",
    "summary": "",
    "translation": "通过自监督预训练推进端到端像素空间生成建模",
    "relevance_score": 2,
    "reasoning": "该论文专注于像素空间生成建模，属于计算机视觉领域的纯视觉生成任务，与推荐系统、搜索或广告的核心技术需求没有直接关联。虽然自监督预训练是通用技术，但论文明确聚焦于像素空间生成，这种视觉内容生成应用超出了当前关注的技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12581v1": {
    "title": "LayerSync: Self-aligning Intermediate Layers",
    "url": "https://www.alphaxiv.org/abs/2510.12581v1",
    "arxiv_id": "2510.12581v1",
    "authors": "Yasaman Haghighi, Bastien van Delft, Mariam Hassan, Alexandre Alahi",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-14 14:39:14",
    "ori_summary": "We propose LayerSync, a domain-agnostic approach for improving the generation quality and the training efficiency of diffusion models. Prior studies have highlighted the connection between the quality of generation and the representations learned by diffusion models, showing that external guidance on model intermediate representations accelerates training. We reconceptualize this paradigm by regularizing diffusion models with their own intermediate representations. Building on the observation that representation quality varies across diffusion model layers, we show that the most semantically rich representations can act as an intrinsic guidance for weaker ones, reducing the need for external supervision. Our approach, LayerSync, is a self-sufficient, plug-and-play regularizer term with no overhead on diffusion model training and generalizes beyond the visual domain to other modalities. LayerSync requires no pretrained models nor additional data. We extensively evaluate the method on image generation and demonstrate its applicability to other domains such as audio, video, and motion generation. We show that it consistently improves the generation quality and the training efficiency. For example, we speed up the training of flow-based transformer by over 8.75x on ImageNet dataset and improved the generation quality by 23.6%. The code is available at https://github.com/vita-epfl/LayerSync.",
    "summary": "",
    "translation": "LayerSync：自对齐中间层",
    "relevance_score": 6,
    "reasoning": "该论文涉及中间层对齐技术，属于Transformer架构效率优化范畴，与'Enabling Transformer Tech'相关。在推荐系统或搜索中，中间层对齐可提升模型训练稳定性、加速收敛，并可能改善多任务学习效果，对大规模部署具有实用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.12579v1": {
    "title": "Unlocking Zero-Shot Plant Segmentation with Pl@ntNet Intelligence",
    "url": "https://www.alphaxiv.org/abs/2510.12579v1",
    "arxiv_id": "2510.12579v1",
    "authors": "Simon Ravé, Jean-Christophe Lombardo, Pejman Rasti, Alexis Joly, David Rousseau",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 14:38:32",
    "ori_summary": "We present a zero-shot segmentation approach for agricultural imagery that leverages Plantnet, a large-scale plant classification model, in conjunction with its DinoV2 backbone and the Segment Anything Model (SAM). Rather than collecting and annotating new datasets, our method exploits Plantnet's specialized plant representations to identify plant regions and produce coarse segmentation masks. These masks are then refined by SAM to yield detailed segmentations. We evaluate on four publicly available datasets of various complexity in terms of contrast including some where the limited size of the training data and complex field conditions often hinder purely supervised methods. Our results show consistent performance gains when using Plantnet-fine-tuned DinoV2 over the base DinoV2 model, as measured by the Jaccard Index (IoU). These findings highlight the potential of combining foundation models with specialized plant-centric models to alleviate the annotation bottleneck and enable effective segmentation in diverse agricultural scenarios.",
    "summary": "",
    "translation": "利用Pl@ntNet智能解锁零样本植物分割",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于植物分割的计算机视觉应用，属于纯粹的视觉领域研究。虽然提到了零样本学习概念，但应用场景（植物识别）与推荐系统、搜索或广告领域没有任何关联，也不涉及LLM技术或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12573v1": {
    "title": "Learning Human Motion with Temporally Conditional Mamba",
    "url": "https://www.alphaxiv.org/abs/2510.12573v1",
    "arxiv_id": "2510.12573v1",
    "authors": "Quang Nguyen, Tri Le, Baoru Huang, Minh Nhat Vu, Ngan Le, Thieu Vo, Anh Nguyen",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 14:29:51",
    "ori_summary": "Learning human motion based on a time-dependent input signal presents a challenging yet impactful task with various applications. The goal of this task is to generate or estimate human movement that consistently reflects the temporal patterns of conditioning inputs. Existing methods typically rely on cross-attention mechanisms to fuse the condition with motion. However, this approach primarily captures global interactions and struggles to maintain step-by-step temporal alignment. To address this limitation, we introduce Temporally Conditional Mamba, a new mamba-based model for human motion generation. Our approach integrates conditional information into the recurrent dynamics of the Mamba block, enabling better temporally aligned motion. To validate the effectiveness of our method, we evaluate it on a variety of human motion tasks. Extensive experiments demonstrate that our model significantly improves temporal alignment, motion realism, and condition consistency over state-of-the-art approaches. Our project page is available at https://zquang2202.github.io/TCM.",
    "summary": "",
    "translation": "基于时间条件Mamba的人类运动学习",
    "relevance_score": 2,
    "reasoning": "该论文主要关注人类运动建模，属于计算机视觉和运动分析领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然Mamba架构作为状态空间模型在序列建模方面有潜力，但该论文的应用场景（人类运动）与RecSys/Search/Ads领域相距甚远，缺乏明确的跨领域应用路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12565v1": {
    "title": "MMOT: The First Challenging Benchmark for Drone-based Multispectral Multi-Object Tracking",
    "url": "https://www.alphaxiv.org/abs/2510.12565v1",
    "arxiv_id": "2510.12565v1",
    "authors": "Tianhao Li, Tingfa Xu, Ying Wang, Haolin Qin, Xu Lin, Jianan Li",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 14:25:17",
    "ori_summary": "Drone-based multi-object tracking is essential yet highly challenging due to small targets, severe occlusions, and cluttered backgrounds. Existing RGB-based tracking algorithms heavily depend on spatial appearance cues such as color and texture, which often degrade in aerial views, compromising reliability. Multispectral imagery, capturing pixel-level spectral reflectance, provides crucial cues that enhance object discriminability under degraded spatial conditions. However, the lack of dedicated multispectral UAV datasets has hindered progress in this domain. To bridge this gap, we introduce MMOT, the first challenging benchmark for drone-based multispectral multi-object tracking. It features three key characteristics: (i) Large Scale - 125 video sequences with over 488.8K annotations across eight categories; (ii) Comprehensive Challenges - covering diverse conditions such as extreme small targets, high-density scenarios, severe occlusions, and complex motion; and (iii) Precise Oriented Annotations - enabling accurate localization and reduced ambiguity under aerial perspectives. To better extract spectral features and leverage oriented annotations, we further present a multispectral and orientation-aware MOT scheme adapting existing methods, featuring: (i) a lightweight Spectral 3D-Stem integrating spectral features while preserving compatibility with RGB pretraining; (ii) an orientation-aware Kalman filter for precise state estimation; and (iii) an end-to-end orientation-adaptive transformer. Extensive experiments across representative trackers consistently show that multispectral input markedly improves tracking performance over RGB baselines, particularly for small and densely packed objects. We believe our work will advance drone-based multispectral multi-object tracking research. Our MMOT, code, and benchmarks are publicly available at https://github.com/Annzstbl/MMOT.",
    "summary": "",
    "translation": "MMOT：首个基于无人机的多光谱多目标跟踪挑战性基准",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于无人机视觉领域的多目标跟踪基准，属于纯粹的计算机视觉应用。虽然多目标跟踪技术在某些边缘场景可能有潜在应用，但该论文明确针对无人机和光谱分析，与推荐系统、搜索或广告的核心技术栈相距甚远，且未涉及任何Transformer架构、LLM技术或异构数据建模的相关内容。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12560v1": {
    "title": "CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving",
    "url": "https://www.alphaxiv.org/abs/2510.12560v1",
    "arxiv_id": "2510.12560v1",
    "authors": "Xiaoji Zheng, Ziyuan Yang, Yanhao Chen, Yuhang Peng, Yuanrong Tang, Gengyuan Liu, Bokui Chen, Jiangtao Gong",
    "categories": "cs.CV, cs.LG, cs.RO",
    "pub_date": "2025-10-14 14:21:52",
    "ori_summary": "End-to-end autonomous driving models trained solely with imitation learning (IL) often suffer from poor generalization. In contrast, reinforcement learning (RL) promotes exploration through reward maximization but faces challenges such as sample inefficiency and unstable convergence. A natural solution is to combine IL and RL. Moving beyond the conventional two-stage paradigm (IL pretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive dual-policy framework that enables IL and RL agents to interact during training. CoIRL-AD introduces a competition-based mechanism that facilitates knowledge exchange while preventing gradient conflicts. Experiments on the nuScenes dataset show an 18% reduction in collision rate compared to baselines, along with stronger generalization and improved performance on long-tail scenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.",
    "summary": "",
    "translation": "CoIRL-AD：用于自动驾驶的潜在世界模型中协作-竞争模仿-强化学习",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域的强化学习和世界模型，这属于明确的无关主题。论文标题中没有任何元素表明与推荐系统、搜索或广告相关，也没有涉及LLM技术、Transformer架构或异构数据建模。该研究纯粹是自动驾驶领域的特定应用，与我的关注焦点完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12537v1": {
    "title": "Unconditional Human Motion and Shape Generation via Balanced Score-Based Diffusion",
    "url": "https://www.alphaxiv.org/abs/2510.12537v1",
    "arxiv_id": "2510.12537v1",
    "authors": "David Björkstrand, Tiesheng Wang, Lars Bretzner, Josephine Sullivan",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-14 14:02:22",
    "ori_summary": "Recent work has explored a range of model families for human motion generation, including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and diffusion-based models. Despite their differences, many methods rely on over-parameterized input features and auxiliary losses to improve empirical results. These strategies should not be strictly necessary for diffusion models to match the human motion distribution. We show that on par with state-of-the-art results in unconditional human motion generation are achievable with a score-based diffusion model using only careful feature-space normalization and analytically derived weightings for the standard L2 score-matching loss, while generating both motion and shape directly, thereby avoiding slow post hoc shape recovery from joints. We build the method step by step, with a clear theoretical motivation for each component, and provide targeted ablations demonstrating the effectiveness of each proposed addition in isolation.",
    "summary": "",
    "translation": "基于平衡分数扩散的无条件人体运动与形状生成",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉领域的人体运动生成，属于纯粹的视觉生成任务。虽然扩散模型是LLM相关的生成技术，但该工作没有展示与推荐系统、搜索或广告的明确联系。人体运动生成可能的应用场景如虚拟试衣或AR广告过于间接，不属于核心关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12524v1": {
    "title": "Voronoi-Assisted Diffusion for Computing Unsigned Distance Fields from Unoriented Points",
    "url": "https://www.alphaxiv.org/abs/2510.12524v1",
    "arxiv_id": "2510.12524v1",
    "authors": "Jiayi Kong, Chen Zong, Junkai Deng, Xuhui Chen, Fei Hou, Shiqing Xin, Junhui Hou, Chen Qian, Ying He",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 13:49:53",
    "ori_summary": "Unsigned Distance Fields (UDFs) provide a flexible representation for 3D shapes with arbitrary topology, including open and closed surfaces, orientable and non-orientable geometries, and non-manifold structures. While recent neural approaches have shown promise in learning UDFs, they often suffer from numerical instability, high computational cost, and limited controllability. We present a lightweight, network-free method, Voronoi-Assisted Diffusion (VAD), for computing UDFs directly from unoriented point clouds. Our approach begins by assigning bi-directional normals to input points, guided by two Voronoi-based geometric criteria encoded in an energy function for optimal alignment. The aligned normals are then diffused to form an approximate UDF gradient field, which is subsequently integrated to recover the final UDF. Experiments demonstrate that VAD robustly handles watertight and open surfaces, as well as complex non-manifold and non-orientable geometries, while remaining computationally efficient and stable.",
    "summary": "",
    "translation": "基于无定向点云的Voronoi辅助扩散计算无符号距离场",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机图形学中的几何处理技术，涉及Voronoi图和距离场计算，属于纯粹的3D视觉和图形学领域。这些技术没有明确的推荐系统、搜索或广告应用潜力，与当前关注的LLM技术、推荐算法或异构数据建模完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12493v1": {
    "title": "BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring",
    "url": "https://www.alphaxiv.org/abs/2510.12493v1",
    "arxiv_id": "2510.12493v1",
    "authors": "An Zhao, Piaopiao Yu, Zhe Zhu, Mingqiang Wei",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 13:26:56",
    "ori_summary": "3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene reconstruction.However, reconstructing high-quality 3D scenes from motion-blurred images caused by camera motion poses a significant challenge.The performance of existing 3DGS-based deblurring methods are limited due to their inherent mechanisms, such as extreme dependence on the accuracy of camera poses and inability to effectively control erroneous Gaussian primitives densification caused by motion blur.To solve these problems, we introduce a novel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D scenes from motion-blurred images.BSGS contains two stages. First, Camera Pose Refinement roughly optimizes camera poses to reduce motion-induced distortions. Second, with fixed rough camera poses, Global RigidTransformation further corrects motion-induced blur distortions.To alleviate multi-subframe gradient conflicts, we propose a subframe gradient aggregation strategy to optimize both stages.Furthermore, a space-time bi-stage optimization strategy is introduced to dynamically adjust primitive densification thresholds and prevent premature noisy Gaussian generation in blurred regions. Comprehensive experiments verify the effectiveness of our proposed deblurring method and show its superiority over the state of the arts.",
    "summary": "",
    "translation": "BSGS：用于相机运动去模糊的双阶段3D高斯泼溅方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D视觉和图像去模糊技术，属于纯粹的计算机视觉领域。虽然标题提到相机运动处理，但这与推荐系统、搜索或广告的核心技术没有直接关联，也不涉及LLM、Transformer架构或异构数据建模等当前关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12483v1": {
    "title": "Fast Visuomotor Policy for Robotic Manipulation",
    "url": "https://www.alphaxiv.org/abs/2510.12483v1",
    "arxiv_id": "2510.12483v1",
    "authors": "Jingkai Jia, Tong Yang, Xueyao Chen, Chenhuan Liu, Wenqiang Zhang",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-14 13:18:45",
    "ori_summary": "We present a fast and effective policy framework for robotic manipulation, named Energy Policy, designed for high-frequency robotic tasks and resource-constrained systems. Unlike existing robotic policies, Energy Policy natively predicts multimodal actions in a single forward pass, enabling high-precision manipulation at high speed. The framework is built upon two core components. First, we adopt the energy score as the learning objective to facilitate multimodal action modeling. Second, we introduce an energy MLP to implement the proposed objective while keeping the architecture simple and efficient. We conduct comprehensive experiments in both simulated environments and real-world robotic tasks to evaluate the effectiveness of Energy Policy. The results show that Energy Policy matches or surpasses the performance of state-of-the-art manipulation methods while significantly reducing computational overhead. Notably, on the MimicGen benchmark, Energy Policy achieves superior performance with at a faster inference compared to existing approaches.",
    "summary": "",
    "translation": "用于机器人操作的快速视觉运动策略",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于机器人操作和视觉运动控制，属于机器人技术领域。虽然提到了策略学习，但这是针对物理机器人操作的特定应用，与推荐系统、搜索或广告的核心技术领域没有直接关联。该研究不涉及Transformer架构、LLM技术或任何可应用于RecSys/Search/Ads的通用方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12482v1": {
    "title": "A Text-Image Fusion Method with Data Augmentation Capabilities for Referring Medical Image Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.12482v1",
    "arxiv_id": "2510.12482v1",
    "authors": "Shurong Chai, Rahul Kumar JAIN, Rui Xu, Shaocong Mo, Ruibo Hou, Shiyu Teng, Jiaqing Liu, Lanfen Lin, Yen-Wei Chen",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-14 13:18:34",
    "ori_summary": "Deep learning relies heavily on data augmentation to mitigate limited data, especially in medical imaging. Recent multimodal learning integrates text and images for segmentation, known as referring or text-guided image segmentation. However, common augmentations like rotation and flipping disrupt spatial alignment between image and text, weakening performance. To address this, we propose an early fusion framework that combines text and visual features before augmentation, preserving spatial consistency. We also design a lightweight generator that projects text embeddings into visual space, bridging semantic gaps. Visualization of generated pseudo-images shows accurate region localization. Our method is evaluated on three medical imaging tasks and four segmentation frameworks, achieving state-of-the-art results. Code is publicly available on GitHub: https://github.com/11yxk/MedSeg_EarlyFusion.",
    "summary": "",
    "translation": "一种具有数据增强能力的文本-图像融合方法用于参考医学图像分割",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分割领域，属于明确的医学应用范畴，与RecSys、搜索或广告领域无关。虽然提到了文本-图像融合方法，但其应用场景严格限定在医疗领域，不涉及推荐系统、搜索或广告中的异构数据处理。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12468v1": {
    "title": "MS-GAGA: Metric-Selective Guided Adversarial Generation Attack",
    "url": "https://www.alphaxiv.org/abs/2510.12468v1",
    "arxiv_id": "2510.12468v1",
    "authors": "Dion J. X. Ho, Gabriel Lee Jun Rong, Niharika Shrivastava, Harshavardhan Abichandani, Pai Chet Ng, Xiaoxiao Miao",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 13:01:40",
    "ori_summary": "We present MS-GAGA (Metric-Selective Guided Adversarial Generation Attack), a two-stage framework for crafting transferable and visually imperceptible adversarial examples against deepfake detectors in black-box settings. In Stage 1, a dual-stream attack module generates adversarial candidates: MNTD-PGD applies enhanced gradient calculations optimized for small perturbation budgets, while SG-PGD focuses perturbations on visually salient regions. This complementary design expands the adversarial search space and improves transferability across unseen models. In Stage 2, a metric-aware selection module evaluates candidates based on both their success against black-box models and their structural similarity (SSIM) to the original image. By jointly optimizing transferability and imperceptibility, MS-GAGA achieves up to 27% higher misclassification rates on unseen detectors compared to state-of-the-art attacks.",
    "summary": "",
    "translation": "MS-GAGA：度量选择性引导的对抗生成攻击",
    "relevance_score": 1,
    "reasoning": "该论文关注对抗性攻击和生成方法，属于安全/隐私领域，与我的关注点（推荐系统、搜索、广告的核心进展及LLM/Transformer技术）无关。对抗性攻击主要涉及模型鲁棒性和安全性，这些被明确列为不相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12451v1": {
    "title": "A Function Centric Perspective On Flat and Sharp Minima",
    "url": "https://www.alphaxiv.org/abs/2510.12451v1",
    "arxiv_id": "2510.12451v1",
    "authors": "Israel Mason-Williams, Gabryel Mason-Williams, Helen Yannakoudakis",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-14 12:33:14",
    "ori_summary": "Flat minima are widely believed to correlate with improved generalisation in deep neural networks. However, this connection has proven more nuanced in recent studies, with both theoretical counterexamples and empirical exceptions emerging in the literature. In this paper, we revisit the role of sharpness in model performance, proposing that sharpness is better understood as a function-dependent property rather than a reliable indicator of poor generalisation. We conduct extensive empirical studies, from single-objective optimisation to modern image classification tasks, showing that sharper minima often emerge when models are regularised (e.g., via SAM, weight decay, or data augmentation), and that these sharp minima can coincide with better generalisation, calibration, robustness, and functional consistency. Across a range of models and datasets, we find that baselines without regularisation tend to converge to flatter minima yet often perform worse across all safety metrics. Our findings demonstrate that function complexity, rather than flatness alone, governs the geometry of solutions, and that sharper minima can reflect more appropriate inductive biases (especially under regularisation), calling for a function-centric reappraisal of loss landscape geometry.",
    "summary": "",
    "translation": "基于函数中心视角的平坦与尖锐极小值研究",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于优化理论中的平坦与尖锐极小值问题，属于通用的机器学习优化理论研究。虽然优化方法对LLM训练有基础性作用，但论文本身没有明确指向Transformer架构、推荐系统或搜索广告的具体应用场景，与当前关注的领域技术进展关联度极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12444v1": {
    "title": "A Review of Longitudinal Radiology Report Generation: Dataset Composition, Methods, and Performance Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.12444v1",
    "arxiv_id": "2510.12444v1",
    "authors": "Shaoyang Zhou, Yingshu Li, Yunyi Liu, Lingqiao Liu, Lei Wang, Luping Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 12:26:23",
    "ori_summary": "Chest Xray imaging is a widely used diagnostic tool in modern medicine, and its high utilization creates substantial workloads for radiologists. To alleviate this burden, vision language models are increasingly applied to automate Chest Xray radiology report generation (CXRRRG), aiming for clinically accurate descriptions while reducing manual effort. Conventional approaches, however, typically rely on single images, failing to capture the longitudinal context necessary for producing clinically faithful comparison statements. Recently, growing attention has been directed toward incorporating longitudinal data into CXR RRG, enabling models to leverage historical studies in ways that mirror radiologists diagnostic workflows. Nevertheless, existing surveys primarily address single image CXRRRG and offer limited guidance for longitudinal settings, leaving researchers without a systematic framework for model design. To address this gap, this survey provides the first comprehensive review of longitudinal radiology report generation (LRRG). Specifically, we examine dataset construction strategies, report generation architectures alongside longitudinally tailored designs, and evaluation protocols encompassing both longitudinal specific measures and widely used benchmarks. We further summarize LRRG methods performance, alongside analyses of different ablation studies, which collectively highlight the critical role of longitudinal information and architectural design choices in improving model performance. Finally, we summarize five major limitations of current research and outline promising directions for future development, aiming to lay a foundation for advancing this emerging field.",
    "summary": "",
    "translation": "纵向放射学报告生成综述：数据集构成、方法及性能评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像领域的放射学报告生成，属于医疗领域的特定应用。虽然涉及生成式技术，但内容完全围绕医学诊断和放射学，与推荐系统、搜索或广告领域没有任何直接或间接关联，也不涉及任何可能应用于这些领域的核心技术进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12425v1": {
    "title": "Tensor Completion via Monotone Inclusion: Generalized Low-Rank Priors Meet Deep Denoisers",
    "url": "https://www.alphaxiv.org/abs/2510.12425v1",
    "arxiv_id": "2510.12425v1",
    "authors": "Peng Chen, Deliang Wei, Jiale Yao, Fang Li",
    "categories": "math.OC, cs.CV, 65K10, 68T07, 94A08",
    "pub_date": "2025-10-14 12:01:32",
    "ori_summary": "Missing entries in multi dimensional data pose significant challenges for downstream analysis across diverse real world applications. These data are naturally modeled as tensors, and recent completion methods integrating global low rank priors with plug and play denoisers have demonstrated strong empirical performance. However, these approaches often rely on empirical convergence alone or unrealistic assumptions, such as deep denoisers acting as proximal operators of implicit regularizers, which generally does not hold. To address these limitations, we propose a novel tensor completion framework grounded in the monotone inclusion paradigm, which unifies generalized low rank priors with deep pseudo contractive denoisers and extends beyond traditional convex optimization. Building on the Davis Yin splitting scheme, we develop the GTCTV DPC algorithm and rigorously establish its global convergence. Extensive experiments demonstrate that GTCTV DPC consistently outperforms existing methods in both quantitative metrics and visual quality, particularly at low sampling rates.",
    "summary": "",
    "translation": "基于单调包含的张量补全：广义低秩先验与深度去噪器的结合",
    "relevance_score": 3,
    "reasoning": "该论文主要关注张量补全的数学优化方法，属于通用的矩阵/张量补全技术，而非专门针对推荐系统、搜索或广告领域的核心进展。虽然张量补全在推荐系统中可用于处理稀疏交互数据，但论文聚焦于单调包含优化和深度去噪器的理论结合，缺乏明确的RecSys/Search/Ads应用场景说明，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12422v1": {
    "title": "VideoLucy: Deep Memory Backtracking for Long Video Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.12422v1",
    "arxiv_id": "2510.12422v1",
    "authors": "Jialong Zuo, Yongtai Deng, Lingdong Kong, Jingkang Yang, Rui Jin, Yiwei Zhang, Nong Sang, Liang Pan, Ziwei Liu, Changxin Gao",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 11:59:19",
    "ori_summary": "Recent studies have shown that agent-based systems leveraging large language models (LLMs) for key information retrieval and integration have emerged as a promising approach for long video understanding. However, these systems face two major challenges. First, they typically perform modeling and reasoning on individual frames, struggling to capture the temporal context of consecutive frames. Second, to reduce the cost of dense frame-level captioning, they adopt sparse frame sampling, which risks discarding crucial information. To overcome these limitations, we propose VideoLucy, a deep memory backtracking framework for long video understanding. Inspired by the human recollection process from coarse to fine, VideoLucy employs a hierarchical memory structure with progressive granularity. This structure explicitly defines the detail level and temporal scope of memory at different hierarchical depths. Through an agent-based iterative backtracking mechanism, VideoLucy systematically mines video-wide, question-relevant deep memories until sufficient information is gathered to provide a confident answer. This design enables effective temporal understanding of consecutive frames while preserving critical details. In addition, we introduce EgoMem, a new benchmark for long video understanding. EgoMem is designed to comprehensively evaluate a model's ability to understand complex events that unfold over time and capture fine-grained details in extremely long videos. Extensive experiments demonstrate the superiority of VideoLucy. Built on open-source models, VideoLucy significantly outperforms state-of-the-art methods on multiple long video understanding benchmarks, achieving performance even surpassing the latest proprietary models such as GPT-4o. Our code and dataset will be made publicly at https://videolucy.github.io",
    "summary": "",
    "translation": "VideoLucy：用于长视频理解的深度记忆回溯",
    "relevance_score": 2,
    "reasoning": "虽然该论文涉及视频理解，但主要关注长视频处理中的记忆回溯技术，这与推荐系统、搜索或广告的核心技术焦点关联较弱。其潜在的时序建模方法可能在用户行为序列分析中有间接应用，但缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12408v1": {
    "title": "Low-Field Magnetic Resonance Image Quality Enhancement using a Conditional Flow Matching Model",
    "url": "https://www.alphaxiv.org/abs/2510.12408v1",
    "arxiv_id": "2510.12408v1",
    "authors": "Huu Tien Nguyen, Ahmed Karam Eldaly",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-14 11:41:27",
    "ori_summary": "This paper introduces a novel framework for image quality transfer based on conditional flow matching (CFM). Unlike conventional generative models that rely on iterative sampling or adversarial objectives, CFM learns a continuous flow between a noise distribution and target data distributions through the direct regression of an optimal velocity field. We evaluate this approach in the context of low-field magnetic resonance imaging (LF-MRI), a rapidly emerging modality that offers affordable and portable scanning but suffers from inherently low signal-to-noise ratio and reduced diagnostic quality. Our framework is designed to reconstruct high-field-like MR images from their corresponding low-field inputs, thereby bridging the quality gap without requiring expensive infrastructure. Experiments demonstrate that CFM not only achieves state-of-the-art performance, but also generalizes robustly to both in-distribution and out-of-distribution data. Importantly, it does so while utilizing significantly fewer parameters than competing deep learning methods. These results underline the potential of CFM as a powerful and scalable tool for MRI reconstruction, particularly in resource-limited clinical environments.",
    "summary": "",
    "translation": "使用条件流匹配模型的低场磁共振图像质量增强",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像领域的磁共振图像处理，属于明确的医学应用范畴，与推荐系统、搜索或广告技术领域完全无关。论文内容涉及图像质量增强技术，但这是针对特定医疗设备的医学图像处理，没有任何潜在的应用于RecSys/Search/Ads的可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12400v1": {
    "title": "Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda",
    "url": "https://www.alphaxiv.org/abs/2510.12400v1",
    "arxiv_id": "2510.12400v1",
    "authors": "André Torneiro, Diogo Monteiro, Paulo Novais, Pedro Rangel Henriques, Nuno F. Rodrigues",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 11:27:46",
    "ori_summary": "Urban monitoring of public infrastructure (such as waste bins, road signs, vegetation, sidewalks, and construction sites) poses significant challenges due to the diversity of objects, environments, and contextual conditions involved. Current state-of-the-art approaches typically rely on a combination of IoT sensors and manual inspections, which are costly, difficult to scale, and often misaligned with citizens' perception formed through direct visual observation. This raises a critical question: Can machines now \"see\" like citizens and infer informed opinions about the condition of urban infrastructure? Vision-Language Models (VLMs), which integrate visual understanding with natural language reasoning, have recently demonstrated impressive capabilities in processing complex visual information, turning them into a promising technology to address this challenge. This systematic review investigates the role of VLMs in urban monitoring, with particular emphasis on zero-shot applications. Following the PRISMA methodology, we analyzed 32 peer-reviewed studies published between 2021 and 2025 to address four core research questions: (1) What urban monitoring tasks have been effectively addressed using VLMs? (2) Which VLM architectures and frameworks are most commonly used and demonstrate superior performance? (3) What datasets and resources support this emerging field? (4) How are VLM-based applications evaluated, and what performance levels have been reported?",
    "summary": "",
    "translation": "迈向基于视觉语言模型的通用城市监控：综述、评估与研究议程",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉和城市监控应用，属于纯粹的视觉领域研究。虽然涉及视觉语言模型技术，但其应用场景（城市监控）与推荐系统、搜索或广告领域没有直接关联，也不涉及处理异构数据或用户行为序列等推荐系统核心问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12387v1": {
    "title": "Scene Coordinate Reconstruction Priors",
    "url": "https://www.alphaxiv.org/abs/2510.12387v1",
    "arxiv_id": "2510.12387v1",
    "authors": "Wenjing Bian, Axel Barroso-Laguna, Tommaso Cavallari, Victor Adrian Prisacariu, Eric Brachmann",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 11:13:31",
    "ori_summary": "Scene coordinate regression (SCR) models have proven to be powerful implicit scene representations for 3D vision, enabling visual relocalization and structure-from-motion. SCR models are trained specifically for one scene. If training images imply insufficient multi-view constraints SCR models degenerate. We present a probabilistic reinterpretation of training SCR models, which allows us to infuse high-level reconstruction priors. We investigate multiple such priors, ranging from simple priors over the distribution of reconstructed depth values to learned priors over plausible scene coordinate configurations. For the latter, we train a 3D point cloud diffusion model on a large corpus of indoor scans. Our priors push predicted 3D scene points towards plausible geometry at each training step to increase their likelihood. On three indoor datasets our priors help learning better scene representations, resulting in more coherent scene point clouds, higher registration rates and better camera poses, with a positive effect on down-stream tasks such as novel view synthesis and camera relocalization.",
    "summary": "",
    "translation": "场景坐标重建先验",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及计算机视觉中的场景坐标重建技术，主要关注3D场景理解或SLAM领域。虽然场景理解在广义上可能与搜索推荐相关，但该标题缺乏明确的连接点，且属于纯粹的视觉技术范畴，与当前关注的推荐系统、搜索广告、LLM技术及Transformer架构等核心方向关联度极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12385v1": {
    "title": "Learning to Recognize Correctly Completed Procedure Steps in Egocentric Assembly Videos through Spatio-Temporal Modeling",
    "url": "https://www.alphaxiv.org/abs/2510.12385v1",
    "arxiv_id": "2510.12385v1",
    "authors": "Tim J. Schoonbeek, Shao-Hsuan Hung, Dan Lehman, Hans Onvlee, Jacek Kustra, Peter H. N. de With, Fons van der Sommen",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 11:03:30",
    "ori_summary": "Procedure step recognition (PSR) aims to identify all correctly completed steps and their sequential order in videos of procedural tasks. The existing state-of-the-art models rely solely on detecting assembly object states in individual video frames. By neglecting temporal features, model robustness and accuracy are limited, especially when objects are partially occluded. To overcome these limitations, we propose Spatio-Temporal Occlusion-Resilient Modeling for Procedure Step Recognition (STORM-PSR), a dual-stream framework for PSR that leverages both spatial and temporal features. The assembly state detection stream operates effectively with unobstructed views of the object, while the spatio-temporal stream captures both spatial and temporal features to recognize step completions even under partial occlusion. This stream includes a spatial encoder, pre-trained using a novel weakly supervised approach to capture meaningful spatial representations, and a transformer-based temporal encoder that learns how these spatial features relate over time. STORM-PSR is evaluated on the MECCANO and IndustReal datasets, reducing the average delay between actual and predicted assembly step completions by 11.2% and 26.1%, respectively, compared to prior methods. We demonstrate that this reduction in delay is driven by the spatio-temporal stream, which does not rely on unobstructed views of the object to infer completed steps. The code for STORM-PSR, along with the newly annotated MECCANO labels, is made publicly available at https://timschoonbeek.github.io/stormpsr .",
    "summary": "",
    "translation": "通过时空建模学习识别自我中心装配视频中正确完成的步骤",
    "relevance_score": 2,
    "reasoning": "该论文专注于自我中心视角的装配视频分析，属于计算机视觉领域。虽然涉及步骤识别和时空建模，但其应用场景（装配视频）与推荐系统、搜索或广告的核心技术栈关联性较弱。论文的技术方法可能对理解用户行为序列有一定启发，但缺乏明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12376v1": {
    "title": "Deep Attention-guided Adaptive Subsampling",
    "url": "https://www.alphaxiv.org/abs/2510.12376v1",
    "arxiv_id": "2510.12376v1",
    "authors": "Sharath M Shankaranarayana, Soumava Kumar Roy, Prasad Sudhakar, Chandan Aladahalli",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-14 10:50:45",
    "ori_summary": "Although deep neural networks have provided impressive gains in performance, these improvements often come at the cost of increased computational complexity and expense. In many cases, such as 3D volume or video classification tasks, not all slices or frames are necessary due to inherent redundancies. To address this issue, we propose a novel learnable subsampling framework that can be integrated into any neural network architecture. Subsampling, being a nondifferentiable operation, poses significant challenges for direct adaptation into deep learning models. While some works, have proposed solutions using the Gumbel-max trick to overcome the problem of non-differentiability, they fall short in a crucial aspect: they are only task-adaptive and not inputadaptive. Once the sampling mechanism is learned, it remains static and does not adjust to different inputs, making it unsuitable for real-world applications. To this end, we propose an attention-guided sampling module that adapts to inputs even during inference. This dynamic adaptation results in performance gains and reduces complexity in deep neural network models. We demonstrate the effectiveness of our method on 3D medical imaging datasets from MedMNIST3D as well as two ultrasound video datasets for classification tasks, one of them being a challenging in-house dataset collected under real-world clinical conditions.",
    "summary": "",
    "translation": "深度注意力引导的自适应子采样",
    "relevance_score": 7,
    "reasoning": "该论文关注注意力机制和自适应采样技术，属于'使能Transformer技术'范畴。在推荐系统和搜索中，自适应子采样可用于高效处理长用户序列或大规模候选集，通过注意力引导选择最相关的子集，提升计算效率同时保持模型性能。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.12362v1": {
    "title": "CurriFlow: Curriculum-Guided Depth Fusion with Optical Flow-Based Temporal Alignment for 3D Semantic Scene Completion",
    "url": "https://www.alphaxiv.org/abs/2510.12362v1",
    "arxiv_id": "2510.12362v1",
    "authors": "Jinzhou Lin, Jie Zhou, Wenhao Xu, Rongtao Xu, Changwei Wang, Shunpeng Chen, Kexue Fu, Yihua Shao, Li Guo, Shibiao Xu",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 10:25:26",
    "ori_summary": "Semantic Scene Completion (SSC) aims to infer complete 3D geometry and semantics from monocular images, serving as a crucial capability for camera-based perception in autonomous driving. However, existing SSC methods relying on temporal stacking or depth projection often lack explicit motion reasoning and struggle with occlusions and noisy depth supervision. We propose CurriFlow, a novel semantic occupancy prediction framework that integrates optical flow-based temporal alignment with curriculum-guided depth fusion. CurriFlow employs a multi-level fusion strategy to align segmentation, visual, and depth features across frames using pre-trained optical flow, thereby improving temporal consistency and dynamic object understanding. To enhance geometric robustness, a curriculum learning mechanism progressively transitions from sparse yet accurate LiDAR depth to dense but noisy stereo depth during training, ensuring stable optimization and seamless adaptation to real-world deployment. Furthermore, semantic priors from the Segment Anything Model (SAM) provide category-agnostic supervision, strengthening voxel-level semantic learning and spatial consistency. Experiments on the SemanticKITTI benchmark demonstrate that CurriFlow achieves state-of-the-art performance with a mean IoU of 16.9, validating the effectiveness of our motion-guided and curriculum-aware design for camera-based 3D semantic scene completion.",
    "summary": "",
    "translation": "CurriFlow：基于课程学习的深度融合与光流时序对齐的3D语义场景补全",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D视觉和场景补全技术，属于纯粹的计算机视觉领域。虽然涉及深度学习和时序建模，但缺乏与推荐系统、搜索或广告领域的直接关联或潜在应用场景。论文的技术方向与用户当前关注的LLM、Transformer架构及推荐系统核心进展没有明显交集。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12308v1": {
    "title": "Hybrid Gaussian Splatting for Novel Urban View Synthesis",
    "url": "https://www.alphaxiv.org/abs/2510.12308v1",
    "arxiv_id": "2510.12308v1",
    "authors": "Mohamed Omran, Farhad Zanjani, Davide Abati, Jens Petersen, Amirhossein Habibian",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 09:09:13",
    "ori_summary": "This paper describes the Qualcomm AI Research solution to the RealADSim-NVS challenge, hosted at the RealADSim Workshop at ICCV 2025. The challenge concerns novel view synthesis in street scenes, and participants are required to generate, starting from car-centric frames captured during some training traversals, renders of the same urban environment as viewed from a different traversal (e.g. different street lane or car direction). Our solution is inspired by hybrid methods in scene generation and generative simulators merging gaussian splatting and diffusion models, and it is composed of two stages: First, we fit a 3D reconstruction of the scene and render novel views as seen from the target cameras. Then, we enhance the resulting frames with a dedicated single-step diffusion model. We discuss specific choices made in the initialization of gaussian primitives as well as the finetuning of the enhancer model and its training data curation. We report the performance of our model design and we ablate its components in terms of novel view quality as measured by PSNR, SSIM and LPIPS. On the public leaderboard reporting test results, our proposal reaches an aggregated score of 0.432, achieving the second place overall.",
    "summary": "",
    "translation": "用于新颖城市视角合成的混合高斯泼溅方法",
    "relevance_score": 2,
    "reasoning": "这篇论文主要关注计算机视觉中的新颖视角合成技术，特别是针对城市环境的3D重建和渲染。虽然高斯泼溅是计算机图形学中的一种高效渲染方法，但该工作主要面向纯粹的视觉应用，与推荐系统、搜索或广告的核心技术栈没有明确的直接关联。在推荐/搜索/广告领域可能的应用场景（如虚拟试穿、商品展示）过于间接且不是该技术的核心目标。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12283v1": {
    "title": "Dual Learning with Dynamic Knowledge Distillation and Soft Alignment for Partially Relevant Video Retrieval",
    "url": "https://www.alphaxiv.org/abs/2510.12283v1",
    "arxiv_id": "2510.12283v1",
    "authors": "Jianfeng Dong, Lei Huang, Daizong Liu, Xianke Chen, Xun Yang, Changting Lin, Xun Wang, Meng Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 08:38:20",
    "ori_summary": "Almost all previous text-to-video retrieval works ideally assume that videos are pre-trimmed with short durations containing solely text-related content. However, in practice, videos are typically untrimmed in long durations with much more complicated background content. Therefore, in this paper, we focus on the more practical yet challenging task of Partially Relevant Video Retrieval (PRVR), which aims to retrieve partially relevant untrimmed videos with the given query. To tackle this task, we propose a novel framework that distills generalization knowledge from a powerful large-scale vision-language pre-trained model and transfers it to a lightweight, task-specific PRVR network. Specifically, we introduce a Dual Learning framework with Dynamic Knowledge Distillation (DL-DKD++), where a large teacher model provides supervision to a compact dual-branch student network. The student model comprises two branches: an inheritance branch that absorbs transferable knowledge from the teacher, and an exploration branch that learns task-specific information from the PRVR dataset to address domain gaps. To further enhance learning, we incorporate a dynamic soft-target construction mechanism. By replacing rigid hard-target supervision with adaptive soft targets that evolve during training, our method enables the model to better capture the fine-grained, partial relevance between videos and queries. Experiment results demonstrate that our proposed model achieves state-of-the-art performance on TVR, ActivityNet, and Charades-STA datasets for PRVR. The code is available at https://github.com/HuiGuanLab/DL-DKD.",
    "summary": "",
    "translation": "基于动态知识蒸馏与软对齐的部分相关视频检索双学习",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视频检索中的部分相关性匹配问题，虽然涉及知识蒸馏和检索技术，但其核心应用场景是视频内容检索而非推荐系统、搜索或广告领域。动态知识蒸馏技术可能对模型效率有一定启发，但缺乏明确的RecSys/Search/Ads应用场景的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12282v1": {
    "title": "PAGS: Priority-Adaptive Gaussian Splatting for Dynamic Driving Scenes",
    "url": "https://www.alphaxiv.org/abs/2510.12282v1",
    "arxiv_id": "2510.12282v1",
    "authors": "Ying A, Wenzhang Sun, Chang Zeng, Chunfeng Wang, Hao Li, Jianxun Cui",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 08:36:09",
    "ori_summary": "Reconstructing dynamic 3D urban scenes is crucial for autonomous driving, yet current methods face a stark trade-off between fidelity and computational cost. This inefficiency stems from their semantically agnostic design, which allocates resources uniformly, treating static backgrounds and safety-critical objects with equal importance. To address this, we introduce Priority-Adaptive Gaussian Splatting (PAGS), a framework that injects task-aware semantic priorities directly into the 3D reconstruction and rendering pipeline. PAGS introduces two core contributions: (1) Semantically-Guided Pruning and Regularization strategy, which employs a hybrid importance metric to aggressively simplify non-critical scene elements while preserving fine-grained details on objects vital for navigation. (2) Priority-Driven Rendering pipeline, which employs a priority-based depth pre-pass to aggressively cull occluded primitives and accelerate the final shading computations. Extensive experiments on the Waymo and KITTI datasets demonstrate that PAGS achieves exceptional reconstruction quality, particularly on safety-critical objects, while significantly reducing training time and boosting rendering speeds to over 350 FPS.",
    "summary": "",
    "translation": "PAGS：面向动态驾驶场景的优先级自适应高斯泼溅",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的3D场景重建技术（高斯泼溅），应用于动态驾驶场景，这属于纯粹的视觉领域研究。虽然标题提及动态场景，但缺乏与推荐系统、搜索或广告领域的直接关联，也没有涉及Transformer架构、LLM技术或异构数据建模等核心关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12267v1": {
    "title": "SpineBench: Benchmarking Multimodal LLMs for Spinal Pathology Analysis",
    "url": "https://www.alphaxiv.org/abs/2510.12267v1",
    "arxiv_id": "2510.12267v1",
    "authors": "Chenghanyu Zhang, Zekun Li, Peipei Li, Xing Cui, Shuhan Xia, Weixiang Yan, Yiqiao Zhang, Qianyu Zhuang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 08:19:22",
    "ori_summary": "With the increasing integration of Multimodal Large Language Models (MLLMs) into the medical field, comprehensive evaluation of their performance in various medical domains becomes critical. However, existing benchmarks primarily assess general medical tasks, inadequately capturing performance in nuanced areas like the spine, which relies heavily on visual input. To address this, we introduce SpineBench, a comprehensive Visual Question Answering (VQA) benchmark designed for fine-grained analysis and evaluation of MLLMs in the spinal domain. SpineBench comprises 64,878 QA pairs from 40,263 spine images, covering 11 spinal diseases through two critical clinical tasks: spinal disease diagnosis and spinal lesion localization, both in multiple-choice format. SpineBench is built by integrating and standardizing image-label pairs from open-source spinal disease datasets, and samples challenging hard negative options for each VQA pair based on visual similarity (similar but not the same disease), simulating real-world challenging scenarios. We evaluate 12 leading MLLMs on SpineBench. The results reveal that these models exhibit poor performance in spinal tasks, highlighting limitations of current MLLM in the spine domain and guiding future improvements in spinal medicine applications. SpineBench is publicly available at https://zhangchenghanyu.github.io/SpineBench.github.io/.",
    "summary": "",
    "translation": "SpineBench：用于脊柱病理分析的多模态大语言模型基准测试",
    "relevance_score": 2,
    "reasoning": "该论文专注于医学领域的脊柱病理分析基准测试，属于明确的医疗应用场景。虽然涉及多模态LLM技术，但其应用领域与推荐系统、搜索或广告完全无关，属于被明确排除的医疗生物学应用范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12260v1": {
    "title": "AngularFuse: A Closer Look at Angle-based Perception for Spatial-Sensitive Multi-Modality Image Fusion",
    "url": "https://www.alphaxiv.org/abs/2510.12260v1",
    "arxiv_id": "2510.12260v1",
    "authors": "Xiaopeng Liu, Yupei Lin, Sen Zhang, Xiao Wang, Yukai Shi, Liang Lin",
    "categories": "cs.CV, cs.LG, eess.IV",
    "pub_date": "2025-10-14 08:13:15",
    "ori_summary": "Visible-infrared image fusion is crucial in key applications such as autonomous driving and nighttime surveillance. Its main goal is to integrate multimodal information to produce enhanced images that are better suited for downstream tasks. Although deep learning based fusion methods have made significant progress, mainstream unsupervised approaches still face serious challenges in practical applications. Existing methods mostly rely on manually designed loss functions to guide the fusion process. However, these loss functions have obvious limitations. On one hand, the reference images constructed by existing methods often lack details and have uneven brightness. On the other hand, the widely used gradient losses focus only on gradient magnitude. To address these challenges, this paper proposes an angle-based perception framework for spatial-sensitive image fusion (AngularFuse). At first, we design a cross-modal complementary mask module to force the network to learn complementary information between modalities. Then, a fine-grained reference image synthesis strategy is introduced. By combining Laplacian edge enhancement with adaptive histogram equalization, reference images with richer details and more balanced brightness are generated. Last but not least, we introduce an angle-aware loss, which for the first time constrains both gradient magnitude and direction simultaneously in the gradient domain. AngularFuse ensures that the fused images preserve both texture intensity and correct edge orientation. Comprehensive experiments on the MSRS, RoadScene, and M3FD public datasets show that AngularFuse outperforms existing mainstream methods with clear margin. Visual comparisons further confirm that our method produces sharper and more detailed results in challenging scenes, demonstrating superior fusion capability.",
    "summary": "",
    "translation": "AngularFuse：基于角度感知的空间敏感多模态图像融合方法深入探究",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态图像融合技术，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术栈关联度较低。虽然标题提到多模态融合，但其具体应用场景（空间敏感图像融合）更偏向视觉处理而非推荐/搜索中的异构数据建模，因此潜在应用价值有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12259v1": {
    "title": "Local Background Features Matter in Out-of-Distribution Detection",
    "url": "https://www.alphaxiv.org/abs/2510.12259v1",
    "arxiv_id": "2510.12259v1",
    "authors": "Jinlun Ye, Zhuohao Sun, Yiqiao Qiu, Qiu Li, Zhijun Tan, Ruixuan Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 08:12:49",
    "ori_summary": "Out-of-distribution (OOD) detection is crucial when deploying deep neural networks in the real world to ensure the reliability and safety of their applications. One main challenge in OOD detection is that neural network models often produce overconfident predictions on OOD data. While some methods using auxiliary OOD datasets or generating fake OOD images have shown promising OOD detection performance, they are limited by the high costs of data collection and training. In this study, we propose a novel and effective OOD detection method that utilizes local background features as fake OOD features for model training. Inspired by the observation that OOD images generally share similar background regions with ID images, the background features are extracted from ID images as simulated OOD visual representations during training based on the local invariance of convolution. Through being optimized to reduce the $L_2$-norm of these background features, the neural networks are able to alleviate the overconfidence issue on OOD data. Extensive experiments on multiple standard OOD detection benchmarks confirm the effectiveness of our method and its wide combinatorial compatibility with existing post-hoc methods, with new state-of-the-art performance achieved from our method.",
    "summary": "",
    "translation": "局部背景特征在分布外检测中至关重要",
    "relevance_score": 2,
    "reasoning": "这篇论文主要关注计算机视觉领域的分布外检测问题，属于纯粹的视觉检测任务。虽然背景特征处理在推荐系统中可能有类比应用，但论文本身没有明确展示与推荐系统、搜索或广告的直接关联，也不涉及LLM技术或Transformer架构的改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12258v1": {
    "title": "Multiplicative Loss for Enhancing Semantic Segmentation in Medical and Cellular Images",
    "url": "https://www.alphaxiv.org/abs/2510.12258v1",
    "arxiv_id": "2510.12258v1",
    "authors": "Yuto Yokoi, Kazuhiro Hotta",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 08:07:02",
    "ori_summary": "We propose two novel loss functions, Multiplicative Loss and Confidence-Adaptive Multiplicative Loss, for semantic segmentation in medical and cellular images. Although Cross Entropy and Dice Loss are widely used, their additive combination is sensitive to hyperparameters and often performs suboptimally, especially with limited data. Medical images suffer from data scarcity due to privacy, ethics, and costly annotations, requiring robust and efficient training objectives. Our Multiplicative Loss combines Cross Entropy and Dice losses multiplicatively, dynamically modulating gradients based on prediction confidence. This reduces penalties for confident correct predictions and amplifies gradients for incorrect overconfident ones, stabilizing optimization. Building on this, Confidence-Adaptive Multiplicative Loss applies a confidence-driven exponential scaling inspired by Focal Loss, integrating predicted probabilities and Dice coefficients to emphasize difficult samples. This enhances learning under extreme data scarcity by strengthening gradients when confidence is low. Experiments on cellular and medical segmentation benchmarks show our framework consistently outperforms tuned additive and existing loss functions, offering a simple, effective, and hyperparameter-free mechanism for robust segmentation under challenging data limitations.",
    "summary": "",
    "translation": "用于增强医学和细胞图像语义分割的乘法损失",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学和细胞图像的语义分割，属于明确的医学领域应用，与我的关注领域完全不相关。论文标题中提到的乘法损失技术是专门针对视觉分割任务的，没有显示出在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12256v1": {
    "title": "Vectorized Video Representation with Easy Editing via Hierarchical Spatio-Temporally Consistent Proxy Embedding",
    "url": "https://www.alphaxiv.org/abs/2510.12256v1",
    "arxiv_id": "2510.12256v1",
    "authors": "Ye Chen, Liming Tan, Yupeng Zhu, Yuanbin Wang, Bingbing Ni",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 08:05:30",
    "ori_summary": "Current video representations heavily rely on unstable and over-grained priors for motion and appearance modelling, \\emph{i.e.}, pixel-level matching and tracking. A tracking error of just a few pixels would lead to the collapse of the visual object representation, not to mention occlusions and large motion frequently occurring in videos. To overcome the above mentioned vulnerability, this work proposes spatio-temporally consistent proxy nodes to represent dynamically changing objects/scenes in the video. On the one hand, the hierarchical proxy nodes have the ability to stably express the multi-scale structure of visual objects, so they are not affected by accumulated tracking error, long-term motion, occlusion, and viewpoint variation. On the other hand, the dynamic representation update mechanism of the proxy nodes adequately leverages spatio-temporal priors of the video to mitigate the impact of inaccurate trackers, thereby effectively handling drastic changes in scenes and objects. Additionally, the decoupled encoding manner of the shape and texture representations across different visual objects in the video facilitates controllable and fine-grained appearance editing capability. Extensive experiments demonstrate that the proposed representation achieves high video reconstruction accuracy with fewer parameters and supports complex video processing tasks, including video in-painting and keyframe-based temporally consistent video editing.",
    "summary": "",
    "translation": "基于分层时空一致代理嵌入的可轻松编辑的向量化视频表示",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频表示和编辑技术，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术关联性较弱。虽然向量化表示技术可能在某些多媒体推荐场景中有间接应用，但论文标题明确聚焦于视频编辑功能，这超出了当前关注的技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12241v1": {
    "title": "Ivan-ISTD: Rethinking Cross-domain Heteroscedastic Noise Perturbations in Infrared Small Target Detection",
    "url": "https://www.alphaxiv.org/abs/2510.12241v1",
    "arxiv_id": "2510.12241v1",
    "authors": "Yuehui Li, Yahao Lu, Haoyuan Wu, Sen Zhang, Liang Lin, Yukai Shi",
    "categories": "cs.CV, eess.IV",
    "pub_date": "2025-10-14 07:48:31",
    "ori_summary": "In the multimedia domain, Infrared Small Target Detection (ISTD) plays a important role in drone-based multi-modality sensing. To address the dual challenges of cross-domain shift and heteroscedastic noise perturbations in ISTD, we propose a doubly wavelet-guided Invariance learning framework(Ivan-ISTD). In the first stage, we generate training samples aligned with the target domain using Wavelet-guided Cross-domain Synthesis. This wavelet-guided alignment machine accurately separates the target background through multi-frequency wavelet filtering. In the second stage, we introduce Real-domain Noise Invariance Learning, which extracts real noise characteristics from the target domain to build a dynamic noise library. The model learns noise invariance through self-supervised loss, thereby overcoming the limitations of distribution bias in traditional artificial noise modeling. Finally, we create the Dynamic-ISTD Benchmark, a cross-domain dynamic degradation dataset that simulates the distribution shifts encountered in real-world applications. Additionally, we validate the versatility of our method using other real-world datasets. Experimental results demonstrate that our approach outperforms existing state-of-the-art methods in terms of many quantitative metrics. In particular, Ivan-ISTD demonstrates excellent robustness in cross-domain scenarios. The code for this work can be found at: https://github.com/nanjin1/Ivan-ISTD.",
    "summary": "",
    "translation": "Ivan-ISTD：重新思考红外小目标检测中的跨域异方差噪声扰动",
    "relevance_score": 1,
    "reasoning": "该论文专注于红外小目标检测这一特定计算机视觉任务，涉及跨域噪声扰动处理。虽然提到了跨域和噪声问题，但这属于纯粹的视觉检测领域，与推荐系统、搜索或广告的核心技术没有直接关联，也没有展示出在异构数据统一建模方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12231v1": {
    "title": "BIGFix: Bidirectional Image Generation with Token Fixing",
    "url": "https://www.alphaxiv.org/abs/2510.12231v1",
    "arxiv_id": "2510.12231v1",
    "authors": "Victor Besnier, David Hurych, Andrei Bursuc, Eduardo Valle",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 07:34:44",
    "ori_summary": "Recent advances in image and video generation have raised significant interest from both academia and industry. A key challenge in this field is improving inference efficiency, as model size and the number of inference steps directly impact the commercial viability of generative models while also posing fundamental scientific challenges. A promising direction involves combining auto-regressive sequential token modeling with multi-token prediction per step, reducing inference time by up to an order of magnitude. However, predicting multiple tokens in parallel can introduce structural inconsistencies due to token incompatibilities, as capturing complex joint dependencies during training remains challenging. Traditionally, once tokens are sampled, there is no mechanism to backtrack and refine erroneous predictions. We propose a method for self-correcting image generation by iteratively refining sampled tokens. We achieve this with a novel training scheme that injects random tokens in the context, improving robustness and enabling token fixing during sampling. Our method preserves the efficiency benefits of parallel token prediction while significantly enhancing generation quality. We evaluate our approach on image generation using the ImageNet-256 and CIFAR-10 datasets, as well as on video generation with UCF-101 and NuScenes, demonstrating substantial improvements across both modalities.",
    "summary": "",
    "translation": "BIGFix：基于令牌修复的双向图像生成",
    "relevance_score": 2,
    "reasoning": "该论文专注于图像生成技术，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术栈关联性较弱。虽然图像生成技术可能间接应用于广告创意生成等场景，但这属于明确的非相关主题范畴，因此整体相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12225v1": {
    "title": "HoneyBee: Data Recipes for Vision-Language Reasoners",
    "url": "https://www.alphaxiv.org/abs/2510.12225v1",
    "arxiv_id": "2510.12225v1",
    "authors": "Hritik Bansal, Devandra Singh Sachan, Kai-Wei Chang, Aditya Grover, Gargi Ghosh, Wen-tau Yih, Ramakanth Pasunuru",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-14 07:23:44",
    "ori_summary": "Recent advances in vision-language models (VLMs) have made them highly effective at reasoning tasks. However, the principles underlying the construction of performant VL reasoning training datasets remain poorly understood. In this work, we introduce several data curation approaches and study their impacts on VL reasoning capabilities by carefully controlling training and evaluation setups. We analyze the effects of context (image and question pair) sources, implement targeted data interventions, and explore scaling up images, questions, and chain-of-thought (CoT) solutions. Our findings reveal that (a) context source strategies significantly affect VLM performance, (b) interventions such as auxiliary signals from image captions and the inclusion of text-only reasoning yield substantial gains, and (c) scaling all data dimensions (e.g., unique questions per image and unique CoTs per image-question pair) consistently improves reasoning capability. Motivated by these insights, we introduce HoneyBee, a large-scale, high-quality CoT reasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs trained with HoneyBee outperform state-of-the-art models across model sizes. For instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA model and the base model by 7.8% and 24.8%, respectively, on MathVerse. Furthermore, we propose a test-time scaling strategy that reduces decoding cost by 73% without sacrificing accuracy. Overall, this work presents improved strategies for VL reasoning dataset curation research.",
    "summary": "",
    "translation": "HoneyBee：面向视觉-语言推理模型的数据配方",
    "relevance_score": 8,
    "reasoning": "该论文提出为视觉-语言模型设计数据配方的技术，这属于'VLM Analogy for Heterogeneous Data'范畴。通过将异构数据（如上下文特征和用户序列）视为不同模态进行统一建模，该方法可应用于推荐系统中处理多模态用户行为数据，实现更精准的用户意图理解。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.12219v1": {
    "title": "DIANet: A Phase-Aware Dual-Stream Network for Micro-Expression Recognition via Dynamic Images",
    "url": "https://www.alphaxiv.org/abs/2510.12219v1",
    "arxiv_id": "2510.12219v1",
    "authors": "Vu Tram Anh Khuong, Luu Tu Nguyen, Thi Bich Phuong Man, Thanh Ha Le, Thi Duyen Ngo",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 07:15:29",
    "ori_summary": "Micro-expressions are brief, involuntary facial movements that typically last less than half a second and often reveal genuine emotions. Accurately recognizing these subtle expressions is critical for applications in psychology, security, and behavioral analysis. However, micro-expression recognition (MER) remains a challenging task due to the subtle and transient nature of facial cues and the limited availability of annotated data. While dynamic image (DI) representations have been introduced to summarize temporal motion into a single frame, conventional DI-based methods often overlook the distinct characteristics of different temporal phases within a micro-expression. To address this issue, this paper proposes a novel dual-stream framework, DIANet, which leverages phase-aware dynamic images - one encoding the onset-to-apex phase and the other capturing the apex-to-offset phase. Each stream is processed by a dedicated convolutional neural network, and a cross-attention fusion module is employed to adaptively integrate features from both streams based on their contextual relevance. Extensive experiments conducted on three benchmark MER datasets (CASME-II, SAMM, and MMEW) demonstrate that the proposed method consistently outperforms conventional single-phase DI-based approaches. The results highlight the importance of modeling temporal phase information explicitly and suggest a promising direction for advancing MER.",
    "summary": "",
    "translation": "DIANet：一种基于动态图像的相位感知双流网络用于微表情识别",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的微表情识别，属于纯粹的视觉分析任务。虽然涉及动态图像处理，但微表情识别与推荐系统、搜索或广告的核心技术没有直接关联，也不涉及LLM技术、Transformer架构或异构数据建模。该研究属于特定视觉应用领域，不在当前关注范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12208v1": {
    "title": "The Impact of Synthetic Data on Object Detection Model Performance: A Comparative Analysis with Real-World Data",
    "url": "https://www.alphaxiv.org/abs/2510.12208v1",
    "arxiv_id": "2510.12208v1",
    "authors": "Muammer Bay, Timo von Marcard, Dren Fazlija",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 06:59:51",
    "ori_summary": "Recent advances in generative AI, particularly in computer vision (CV), offer new opportunities to optimize workflows across industries, including logistics and manufacturing. However, many AI applications are limited by a lack of expertise and resources, which forces a reliance on general-purpose models. Success with these models often requires domain-specific data for fine-tuning, which can be costly and inefficient. Thus, using synthetic data for fine-tuning is a popular, cost-effective alternative to gathering real-world data. This work investigates the impact of synthetic data on the performance of object detection models, compared to models trained on real-world data only, specifically within the domain of warehouse logistics. To this end, we examined the impact of synthetic data generated using the NVIDIA Omniverse Replicator tool on the effectiveness of object detection models in real-world scenarios. It comprises experiments focused on pallet detection in a warehouse setting, utilizing both real and various synthetic dataset generation strategies. Our findings provide valuable insights into the practical applications of synthetic image data in computer vision, suggesting that a balanced integration of synthetic and real data can lead to robust and efficient object detection models.",
    "summary": "",
    "translation": "合成数据对目标检测模型性能的影响：与真实世界数据的对比分析",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的目标检测任务，与推荐系统、搜索或广告的核心技术关联度较低。虽然合成数据生成技术在某些场景下可能应用于数据增强，但论文本身并未明确展示在推荐、搜索或广告领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12190v1": {
    "title": "Hierarchical Reasoning with Vision-Language Models for Incident Reports from Dashcam Videos",
    "url": "https://www.alphaxiv.org/abs/2510.12190v1",
    "arxiv_id": "2510.12190v1",
    "authors": "Shingo Yokoi, Kento Sasaki, Yu Yamaguchi",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 06:36:41",
    "ori_summary": "Recent advances in end-to-end (E2E) autonomous driving have been enabled by training on diverse large-scale driving datasets, yet autonomous driving models still struggle in out-of-distribution (OOD) scenarios. The COOOL benchmark targets this gap by encouraging hazard understanding beyond closed taxonomies, and the 2COOOL challenge extends it to generating human-interpretable incident reports. We present a hierarchical reasoning framework for incident report generation from dashcam videos that integrates frame-level captioning, incident frame detection, and fine-grained reasoning within vision-language models (VLMs). We further improve factual accuracy and readability through model ensembling and a Blind A/B Scoring selection protocol. On the official 2COOOL open leaderboard, our method ranks 2nd among 29 teams and achieves the best CIDEr-D score, producing accurate and coherent incident narratives. These results indicate that hierarchical reasoning with VLMs is a promising direction for accident analysis and for broader understanding of safety-critical traffic events. The implementation and code are available at https://github.com/riron1206/kaggle-2COOOL-2nd-Place-Solution.",
    "summary": "",
    "translation": "基于行车记录仪视频的事故报告分层推理视觉语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型在行车记录仪视频事故报告中的特定应用，属于纯粹的视觉-语言多模态任务。虽然涉及视觉语言模型技术，但其应用场景（事故报告、行车视频）与推荐系统、搜索或广告领域没有直接关联，也不涉及处理推荐系统常见的异构数据模态。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12184v1": {
    "title": "CompoDistill: Attention Distillation for Compositional Reasoning in Multimodal LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.12184v1",
    "arxiv_id": "2510.12184v1",
    "authors": "Jiwan Kim, Kibum Kim, Sangwoo Seo, Chanyoung Park",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-14 06:27:26",
    "ori_summary": "Recently, efficient Multimodal Large Language Models (MLLMs) have gained significant attention as a solution to their high computational complexity, making them more practical for real-world applications. In this regard, the knowledge distillation (KD) approach has emerged as a promising alternative, which transfers the rich visual and linguistic knowledge from a larger model (teacher) to a smaller model (student). However, we observe that existing KD methods struggle to effectively distill the teacher MLLM's rich visual perception abilities to the student, a challenge that has been largely overlooked in previous studies. Through a systematic analysis, we identify visual attention misalignment between student and teacher as the main cause of this issue. Based on this insight, we propose CompoDistill, a novel KD framework that explicitly aligns the student's visual attention with that of the teacher to enhance the student's visual perception abilities. Our extensive experiments show that CompoDistill significantly improves performance on compositional reasoning tasks that require visual perception abilities while maintaining strong performance on visual question answering tasks, as done in existing studies. Furthermore, CompoDistill demonstrates effectiveness with a more advanced backbone, highlighting its generalizability.",
    "summary": "",
    "translation": "CompoDistill：面向多模态大语言模型组合推理的注意力蒸馏",
    "relevance_score": 8,
    "reasoning": "该论文关注多模态LLM中的注意力蒸馏技术，属于'Enabling LLM Tech'范畴，可提升模型推理能力。在推荐和搜索系统中，这种组合推理能力可应用于多模态内容理解、用户意图解析和复杂查询处理，提高系统对异构信息的建模精度。注意力蒸馏技术还可优化模型效率，对大规模推荐系统的部署具有实用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.12182v1": {
    "title": "BEEP3D: Box-Supervised End-to-End Pseudo-Mask Generation for 3D Instance Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.12182v1",
    "arxiv_id": "2510.12182v1",
    "authors": "Youngju Yoo, Seho Kim, Changick Kim",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 06:23:18",
    "ori_summary": "3D instance segmentation is crucial for understanding complex 3D environments, yet fully supervised methods require dense point-level annotations, resulting in substantial annotation costs and labor overhead. To mitigate this, box-level annotations have been explored as a weaker but more scalable form of supervision. However, box annotations inherently introduce ambiguity in overlapping regions, making accurate point-to-instance assignment challenging. Recent methods address this ambiguity by generating pseudo-masks through training a dedicated pseudo-labeler in an additional training stage. However, such two-stage pipelines often increase overall training time and complexity, hinder end-to-end optimization. To overcome these challenges, we propose BEEP3D-Box-supervised End-to-End Pseudo-mask generation for 3D instance segmentation. BEEP3D adopts a student-teacher framework, where the teacher model serves as a pseudo-labeler and is updated by the student model via an Exponential Moving Average. To better guide the teacher model to generate precise pseudo-masks, we introduce an instance center-based query refinement that enhances position query localization and leverages features near instance centers. Additionally, we design two novel losses-query consistency loss and masked feature consistency loss-to align semantic and geometric signals between predictions and pseudo-masks. Extensive experiments on ScanNetV2 and S3DIS datasets demonstrate that BEEP3D achieves competitive or superior performance compared to state-of-the-art weakly supervised methods while remaining computationally efficient.",
    "summary": "",
    "translation": "BEEP3D：用于3D实例分割的框监督端到端伪掩码生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D计算机视觉中的实例分割技术，使用边界框监督生成伪掩码。虽然技术本身具有创新性，但3D实例分割与推荐系统、搜索或广告的核心领域没有直接关联，也不涉及LLM技术、Transformer架构改进或异构数据统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12174v1": {
    "title": "UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering",
    "url": "https://www.alphaxiv.org/abs/2510.12174v1",
    "arxiv_id": "2510.12174v1",
    "authors": "Yusen Xie, Zhenmin Huang, Jianhao Jiao, Dimitrios Kanoulas, Jun Ma",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-14 06:07:57",
    "ori_summary": "In this paper, we propose UniGS, a unified map representation and differentiable framework for high-fidelity multimodal 3D reconstruction based on 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated rasterization pipeline capable of rendering photo-realistic RGB images, geometrically accurate depth maps, consistent surface normals, and semantic logits simultaneously. We redesign the rasterization to render depth via differentiable ray-ellipsoid intersection rather than using Gaussian centers, enabling effective optimization of rotation and scale attribute through analytic depth gradients. Furthermore, we derive the analytic gradient formulation for surface normal rendering, ensuring geometric consistency among reconstructed 3D scenes. To improve computational and storage efficiency, we introduce a learnable attribute that enables differentiable pruning of Gaussians with minimal contribution during training. Quantitative and qualitative experiments demonstrate state-of-the-art reconstruction accuracy across all modalities, validating the efficacy of our geometry-aware paradigm. Source code and multimodal viewer will be available on GitHub.",
    "summary": "",
    "translation": "UniGS：面向多模态渲染的统一几何感知高斯溅射",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机图形学中的多模态渲染技术，属于纯粹的视觉/图形学领域，与推荐系统、搜索或广告的核心技术栈没有直接关联。虽然标题中提到'多模态'，但这指的是视觉渲染中的不同模态（如RGB、深度等），而非推荐系统中常见的用户行为、上下文特征等异构数据模态，因此潜在应用价值有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12160v1": {
    "title": "State Space Prompting via Gathering and Spreading Spatio-Temporal Information for Video Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.12160v1",
    "arxiv_id": "2510.12160v1",
    "authors": "Jiahuan Zhou, Kai Zhu, Zhenyu Cui, Zichen Liu, Xu Zou, Gang Hua",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 05:30:36",
    "ori_summary": "Recently, pre-trained state space models have shown great potential for video classification, which sequentially compresses visual tokens in videos with linear complexity, thereby improving the processing efficiency of video data while maintaining high performance. To apply powerful pre-trained models to downstream tasks, prompt learning is proposed to achieve efficient downstream task adaptation with only a small number of fine-tuned parameters. However, the sequentially compressed visual prompt tokens fail to capture the spatial and temporal contextual information in the video, thus limiting the effective propagation of spatial information within a video frame and temporal information between frames in the state compression model and the extraction of discriminative information. To tackle the above issue, we proposed a State Space Prompting (SSP) method for video understanding, which combines intra-frame and inter-frame prompts to aggregate and propagate key spatiotemporal information in the video. Specifically, an Intra-Frame Gathering (IFG) module is designed to aggregate spatial key information within each frame. Besides, an Inter-Frame Spreading (IFS) module is designed to spread discriminative spatio-temporal information across different frames. By adaptively balancing and compressing key spatio-temporal information within and between frames, our SSP effectively propagates discriminative information in videos in a complementary manner. Extensive experiments on four video benchmark datasets verify that our SSP significantly outperforms existing SOTA methods by 2.76% on average while reducing the overhead of fine-tuning parameters.",
    "summary": "",
    "translation": "通过聚集和传播时空信息进行状态空间提示的视频理解",
    "relevance_score": 2,
    "reasoning": "该论文专注于视频理解，属于纯粹的视觉领域应用，与推荐系统、搜索或广告没有明确关联。虽然状态空间模型和提示技术是LLM相关技术，但论文的应用场景局限于视频理解，没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12159v1": {
    "title": "DPL: Spatial-Conditioned Diffusion Prototype Enhancement for One-Shot Medical Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.12159v1",
    "arxiv_id": "2510.12159v1",
    "authors": "Ziyuan Gao, Philippe Morel",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 05:28:58",
    "ori_summary": "One-shot medical image segmentation faces fundamental challenges in prototype representation due to limited annotated data and significant anatomical variability across patients. Traditional prototype-based methods rely on deterministic averaging of support features, creating brittle representations that fail to capture intra-class diversity essential for robust generalization. This work introduces Diffusion Prototype Learning (DPL), a novel framework that reformulates prototype construction through diffusion-based feature space exploration. DPL models one-shot prototypes as learnable probability distributions, enabling controlled generation of diverse yet semantically coherent prototype variants from minimal labeled data. The framework operates through three core innovations: (1) a diffusion-based prototype enhancement module that transforms single support prototypes into diverse variant sets via forward-reverse diffusion processes, (2) a spatial-aware conditioning mechanism that leverages geometric properties derived from prototype feature statistics, and (3) a conservative fusion strategy that preserves prototype fidelity while maximizing representational diversity. DPL ensures training-inference consistency by using the same diffusion enhancement and fusion pipeline in both phases. This process generates enhanced prototypes that serve as the final representations for similarity calculations, while the diffusion process itself acts as a regularizer. Extensive experiments on abdominal MRI and CT datasets demonstrate significant improvements respectively, establishing new state-of-the-art performance in one-shot medical image segmentation.",
    "summary": "",
    "translation": "DPL：用于一次性医学分割的空间条件扩散原型增强",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分割领域，属于明确的无关主题（医学应用）。虽然提到了扩散模型技术，但其应用场景和核心问题与推荐系统、搜索或广告领域没有任何关联。该技术无法直接应用于RecSys/Search/Ads领域，因为其针对的是医学图像处理的特定需求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12150v1": {
    "title": "Class-aware Domain Knowledge Fusion and Fission for Continual Test-Time Adaptation",
    "url": "https://www.alphaxiv.org/abs/2510.12150v1",
    "arxiv_id": "2510.12150v1",
    "authors": "Jiahuan Zhou, Chao Zhu, Zhenyu Cui, Zichen Liu, Xu Zou, Gang Hua",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 05:09:50",
    "ori_summary": "Continual Test-Time Adaptation (CTTA) aims to quickly fine-tune the model during the test phase so that it can adapt to multiple unknown downstream domain distributions without pre-acquiring downstream domain data. To this end, existing advanced CTTA methods mainly reduce the catastrophic forgetting of historical knowledge caused by irregular switching of downstream domain data by restoring the initial model or reusing historical models. However, these methods are usually accompanied by serious insufficient learning of new knowledge and interference from potentially harmful historical knowledge, resulting in severe performance degradation. To this end, we propose a class-aware domain Knowledge Fusion and Fission method for continual test-time adaptation, called KFF, which adaptively expands and merges class-aware domain knowledge in old and new domains according to the test-time data from different domains, where discriminative historical knowledge can be dynamically accumulated. Specifically, considering the huge domain gap within streaming data, a domain Knowledge FIssion (KFI) module is designed to adaptively separate new domain knowledge from a paired class-aware domain prompt pool, alleviating the impact of negative knowledge brought by old domains that are distinct from the current domain. Besides, to avoid the cumulative computation and storage overheads from continuously fissioning new knowledge, a domain Knowledge FUsion (KFU) module is further designed to merge the fissioned new knowledge into the existing knowledge pool with minimal cost, where a greedy knowledge dynamic merging strategy is designed to improve the compatibility of new and old knowledge while keeping the computational efficiency. Extensive experiments on the ImageNet-C dataset verify the effectiveness of our proposed method against other methods.",
    "summary": "",
    "translation": "面向持续测试时自适应的类感知域知识融合与裂变",
    "relevance_score": 2,
    "reasoning": "该论文主要关注持续测试时自适应和域适应问题，属于迁移学习范畴。虽然域适应技术可能间接应用于推荐系统中的冷启动或分布偏移问题，但论文标题未明确表明与推荐、搜索或广告系统的直接关联，也未涉及LLM或Transformer架构的核心进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12141v1": {
    "title": "MAPS: Masked Attribution-based Probing of Strategies- A computational framework to align human and model explanations",
    "url": "https://www.alphaxiv.org/abs/2510.12141v1",
    "arxiv_id": "2510.12141v1",
    "authors": "Sabine Muzellec, Yousif Kashef Alghetaa, Simon Kornblith, Kohitij Kar",
    "categories": "q-bio.NC, cs.CV",
    "pub_date": "2025-10-14 04:40:23",
    "ori_summary": "Human core object recognition depends on the selective use of visual information, but the strategies guiding these choices are difficult to measure directly. We present MAPS (Masked Attribution-based Probing of Strategies), a behaviorally validated computational tool that tests whether explanations derived from artificial neural networks (ANNs) can also explain human vision. MAPS converts attribution maps into explanation-masked images (EMIs) and compares image-by-image human accuracies on these minimal images with limited pixel budgets with accuracies on the full stimuli. MAPS provides a principled way to evaluate and choose among competing ANN interpretability methods. In silico, EMI-based behavioral similarity between models reliably recovers the ground-truth similarity computed from their attribution maps, establishing which explanation methods best capture the model's strategy. When applied to humans and macaques, MAPS identifies ANN-explanation combinations whose explanations align most closely with biological vision, achieving the behavioral validity of Bubble masks while requiring far fewer behavioral trials. Because it needs only access to model attributions and a modest set of behavioral data on the original images, MAPS avoids exhaustive psychophysics while offering a scalable tool for adjudicating explanations and linking human behavior, neural activity, and model decisions under a common standard.",
    "summary": "",
    "translation": "MAPS：基于掩码归因的策略探测——一种对齐人类与模型解释的计算框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注模型解释性和人类解释的对齐，属于模型可解释性研究范畴。虽然解释性在推荐/搜索系统中可能有辅助价值，但该工作更偏向通用NLP模型的可解释性方法，与当前关注的推荐系统核心进展、LLM技术应用或Transformer架构创新等焦点领域关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12132v1": {
    "title": "FedHUG: Federated Heterogeneous Unsupervised Generalization for Remote Physiological Measurements",
    "url": "https://www.alphaxiv.org/abs/2510.12132v1",
    "arxiv_id": "2510.12132v1",
    "authors": "Xiao Yang, Jiyao Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 04:17:25",
    "ori_summary": "Remote physiological measurement gained wide attention, while it requires collecting users' privacy-sensitive information, and existing contactless measurements still rely on labeled client data. This presents challenges when we want to further update real-world deployed models with numerous user data lacking labels. To resolve these challenges, we instantiate a new protocol called Federated Unsupervised Domain Generalization (FUDG) in this work. Subsequently, the \\textbf{Fed}erated \\textbf{H}eterogeneous \\textbf{U}nsupervised \\textbf{G}eneralization (\\textbf{FedHUG}) framework is proposed and consists of: (1) Minimal Bias Aggregation module dynamically adjusts aggregation weights based on prior-driven bias evaluation to cope with heterogeneous non-IID features from multiple domains. (2) The Global Distribution-aware Learning Controller parameterizes the label distribution and dynamically manipulates client-specific training strategies, thereby mitigating the server-client label distribution skew and long-tail issue. The proposal shows superior performance across state-of-the-art techniques in estimation with either RGB video or mmWave radar. The code will be released.",
    "summary": "",
    "translation": "FedHUG：面向远程生理测量的联邦异构无监督泛化",
    "relevance_score": 1,
    "reasoning": "该论文明确涉及联邦学习（标题中的'Fed'）和医疗领域应用（远程生理测量），这两个主题均在无关主题列表中明确排除。论文标题没有显示出与推荐系统、搜索、广告或相关使能技术（如Transformer架构、LLM应用）的任何潜在关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12126v1": {
    "title": "MetaCaptioner: Towards Generalist Visual Captioning with Open-source Suites",
    "url": "https://www.alphaxiv.org/abs/2510.12126v1",
    "arxiv_id": "2510.12126v1",
    "authors": "Zhenxin Lei, Zhangwei Gao, Changyao Tian, Erfei Cui, Guanzhou Chen, Danni Yang, Yuchen Duan, Zhaokai Wang, Wenhao Li, Weiyun Wang, Xiangyu Zhao, Jiayi Ji, Yu Qiao, Wenhai Wang, Gen Luo",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 04:03:25",
    "ori_summary": "Generalist visual captioning goes beyond a simple appearance description task, but requires integrating a series of visual cues into a caption and handling various visual domains. In this task, current open-source models present a large performance gap with commercial ones, which limits various applications such as data synthesis. To bridge the gap, this paper proposes CapFlow, a novel multi-agent collaboration workflow. CapFlow demonstrates for the first time that, by capitalizing on open-source models, it is possible to achieve caption quality on par with GPT-4.1 in various domains with an 89.5% reduction in costs. By leveraging CapFlow as the data synthesizer, we produce high-quality visual captions from image and video domains at scale, and obtain a generalist visual captioner via fine-tuning, namely MetaCaptioner. Through extensive experiments, we show that MetaCaptioner not only achieves comparable captioning capabilities with commercial models but also reaches top-tier multimodal performance in the open-source community. We hope CapFlow and MetaCaptioner can benefit future multimodal research by providing a strong and cost-effective visual captioning solution.",
    "summary": "",
    "translation": "MetaCaptioner：基于开源套件实现通用视觉描述生成",
    "relevance_score": 2,
    "reasoning": "该论文专注于视觉描述生成（Visual Captioning），属于纯粹的视觉-语言多模态任务。虽然标题提到'通用'和'开源套件'，但核心内容与推荐系统、搜索或广告中的排序任务没有直接关联。视觉描述技术可能间接应用于商品描述生成，但这属于内容生成范畴而非核心排序问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12123v1": {
    "title": "Hardware-aware Coding Function Design for Compressive Single-Photon 3D Cameras",
    "url": "https://www.alphaxiv.org/abs/2510.12123v1",
    "arxiv_id": "2510.12123v1",
    "authors": "David Parra, Felipe Gutierrez-Barragan, Trevor Seets, Andreas Velten",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 03:52:24",
    "ori_summary": "Single-photon cameras are becoming increasingly popular in time-of-flight 3D imaging because they can time-tag individual photons with extreme resolution. However, their performance is susceptible to hardware limitations, such as system bandwidth, maximum laser power, sensor data rates, and in-sensor memory and compute resources. Compressive histograms were recently introduced as a solution to the challenge of data rates through an online in-sensor compression of photon timestamp data. Although compressive histograms work within limited in-sensor memory and computational resources, they underperform when subjected to real-world illumination hardware constraints. To address this, we present a constrained optimization approach for designing practical coding functions for compressive single-photon 3D imaging. Using gradient descent, we jointly optimize an illumination and coding matrix (i.e., the coding functions) that adheres to hardware constraints. We show through extensive simulations that our coding functions consistently outperform traditional coding designs under both bandwidth and peak power constraints. This advantage is particularly pronounced in systems constrained by peak power. Finally, we show that our approach adapts to arbitrary parameterized impulse responses by evaluating it on a real-world system with a non-ideal impulse response function.",
    "summary": "",
    "translation": "面向压缩单光子3D相机的硬件感知编码函数设计",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D视觉硬件和压缩感知技术，属于纯粹的计算机视觉领域。虽然3D相机技术在其他领域有应用，但论文标题明确指向单光子3D相机的硬件优化，与推荐系统、搜索或广告的核心技术栈没有直接关联，也不涉及LLM、Transformer或异构数据处理等焦点领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12119v1": {
    "title": "ImageSentinel: Protecting Visual Datasets from Unauthorized Retrieval-Augmented Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12119v1",
    "arxiv_id": "2510.12119v1",
    "authors": "Ziyuan Luo, Yangyi Zhao, Ka Chun Cheung, Simon See, Renjie Wan",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 03:45:19",
    "ori_summary": "The widespread adoption of Retrieval-Augmented Image Generation (RAIG) has raised significant concerns about the unauthorized use of private image datasets. While these systems have shown remarkable capabilities in enhancing generation quality through reference images, protecting visual datasets from unauthorized use in such systems remains a challenging problem. Traditional digital watermarking approaches face limitations in RAIG systems, as the complex feature extraction and recombination processes fail to preserve watermark signals during generation. To address these challenges, we propose ImageSentinel, a novel framework for protecting visual datasets in RAIG. Our framework synthesizes sentinel images that maintain visual consistency with the original dataset. These sentinels enable protection verification through randomly generated character sequences that serve as retrieval keys. To ensure seamless integration, we leverage vision-language models to generate the sentinel images. Experimental results demonstrate that ImageSentinel effectively detects unauthorized dataset usage while preserving generation quality for authorized applications. Code is available at https://github.com/luo-ziyuan/ImageSentinel.",
    "summary": "",
    "translation": "ImageSentinel：保护视觉数据集免遭未经授权的检索增强图像生成",
    "relevance_score": 1,
    "reasoning": "该论文关注视觉数据集的保护和安全问题，属于隐私和安全范畴，这在Irrelevant Topics中明确排除。虽然标题提到检索增强，但核心焦点是防止未经授权的使用，而非推荐系统、搜索或广告中的技术应用。没有明确的潜在应用场景与我的当前关注点相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12114v1": {
    "title": "Self-Supervised Selective-Guided Diffusion Model for Old-Photo Face Restoration",
    "url": "https://www.alphaxiv.org/abs/2510.12114v1",
    "arxiv_id": "2510.12114v1",
    "authors": "Wenjie Li, Xiangyi Wang, Heng Guo, Guangwei Gao, Zhanyu Ma",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 03:34:15",
    "ori_summary": "Old-photo face restoration poses significant challenges due to compounded degradations such as breakage, fading, and severe blur. Existing pre-trained diffusion-guided methods either rely on explicit degradation priors or global statistical guidance, which struggle with localized artifacts or face color. We propose Self-Supervised Selective-Guided Diffusion (SSDiff), which leverages pseudo-reference faces generated by a pre-trained diffusion model under weak guidance. These pseudo-labels exhibit structurally aligned contours and natural colors, enabling region-specific restoration via staged supervision: structural guidance applied throughout the denoising process and color refinement in later steps, aligned with the coarse-to-fine nature of diffusion. By incorporating face parsing maps and scratch masks, our method selectively restores breakage regions while avoiding identity mismatch. We further construct VintageFace, a 300-image benchmark of real old face photos with varying degradation levels. SSDiff outperforms existing GAN-based and diffusion-based methods in perceptual quality, fidelity, and regional controllability. Code link: https://github.com/PRIS-CV/SSDiff.",
    "summary": "",
    "translation": "基于自监督选择性引导扩散模型的旧照片人脸修复",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉领域的人脸修复任务，属于纯粹的图像处理范畴。虽然提到了扩散模型，但其应用场景（旧照片修复）与推荐系统、搜索或广告没有直接关联，也不涉及Transformer架构改进或异构数据建模。该技术没有明显的潜在应用价值于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12107v1": {
    "title": "DRL: Discriminative Representation Learning with Parallel Adapters for Class Incremental Learning",
    "url": "https://www.alphaxiv.org/abs/2510.12107v1",
    "arxiv_id": "2510.12107v1",
    "authors": "Jiawei Zhan, Jun Liu, Jinlong Peng, Xiaochen Chen, Bin-Bin Gao, Yong Liu, Chengjie Wang",
    "categories": "cs.CV, 68T05, 68T07, I.2.6; I.5.4",
    "pub_date": "2025-10-14 03:19:15",
    "ori_summary": "With the excellent representation capabilities of Pre-Trained Models (PTMs), remarkable progress has been made in non-rehearsal Class-Incremental Learning (CIL) research. However, it remains an extremely challenging task due to three conundrums: increasingly large model complexity, non-smooth representation shift during incremental learning and inconsistency between stage-wise sub-problem optimization and global inference. In this work, we propose the Discriminative Representation Learning (DRL) framework to specifically address these challenges. To conduct incremental learning effectively and yet efficiently, the DRL's network, called Incremental Parallel Adapter (IPA) network, is built upon a PTM and increasingly augments the model by learning a lightweight adapter with a small amount of parameter learning overhead in each incremental stage. The adapter is responsible for adapting the model to new classes, it can inherit and propagate the representation capability from the current model through parallel connection between them by a transfer gate. As a result, this design guarantees a smooth representation shift between different incremental stages. Furthermore, to alleviate inconsistency and enable comparable feature representations across incremental stages, we design the Decoupled Anchor Supervision (DAS). It decouples constraints of positive and negative samples by respectively comparing them with the virtual anchor. This decoupling promotes discriminative representation learning and aligns the feature spaces learned at different stages, thereby narrowing the gap between stage-wise local optimization over a subset of data and global inference across all classes. Extensive experiments on six benchmarks reveal that our DRL consistently outperforms other state-of-the-art methods throughout the entire CIL period while maintaining high efficiency in both training and inference phases.",
    "summary": "",
    "translation": "DRL：用于类增量学习的并行适配器判别式表示学习",
    "relevance_score": 2,
    "reasoning": "该论文主要关注类增量学习中的表示学习，虽然涉及表示学习技术，但其核心应用场景是计算机视觉领域的分类任务，与推荐系统、搜索或广告的关联性较弱。论文中提到的并行适配器架构可能对模型效率有一定启发，但这种增量学习技术在RecSys/Search/Ads中的直接应用潜力有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12101v1": {
    "title": "Gaussian Semantic Field for One-shot LiDAR Global Localization",
    "url": "https://www.alphaxiv.org/abs/2510.12101v1",
    "arxiv_id": "2510.12101v1",
    "authors": "Pengyu Yin, Shenghai Yuan, Haozhi Cao, Xingyu Ji, Ruofei Bai, Siyu Chen, Lihua Xie",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-14 03:08:02",
    "ori_summary": "We present a one-shot LiDAR global localization algorithm featuring semantic disambiguation ability based on a lightweight tri-layered scene graph. While landmark semantic registration-based methods have shown promising performance improvements in global localization compared with geometric-only methods, landmarks can be repetitive and misleading for correspondence establishment. We propose to mitigate this problem by modeling semantic distributions with continuous functions learned from a population of Gaussian processes. Compared with discrete semantic labels, the continuous functions capture finer-grained geo-semantic information and also provide more detailed metric information for correspondence establishment. We insert this continuous function as the middle layer between the object layer and the metric-semantic layer, forming a tri-layered 3D scene graph, serving as a light-weight yet performant backend for one-shot localization. We term our global localization pipeline Outram-GSF (Gaussian semantic field) and conduct a wide range of experiments on publicly available data sets, validating the superior performance against the current state-of-the-art.",
    "summary": "",
    "translation": "用于单次激光雷达全局定位的高斯语义场",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及激光雷达定位技术，属于机器人学和计算机视觉领域。虽然定位技术在某些特定搜索场景中可能有间接应用，但论文本身专注于传感器数据处理和几何定位，与推荐系统、搜索或广告的核心技术栈关联度极低。没有明确的潜在应用路径可以连接到RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12099v1": {
    "title": "G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior",
    "url": "https://www.alphaxiv.org/abs/2510.12099v1",
    "arxiv_id": "2510.12099v1",
    "authors": "Junfeng Ni, Yixin Chen, Zhifei Yang, Yu Liu, Ruijie Lu, Song-Chun Zhu, Siyuan Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 03:06:28",
    "ori_summary": "Despite recent advances in leveraging generative prior from pre-trained diffusion models for 3D scene reconstruction, existing methods still face two critical limitations. First, due to the lack of reliable geometric supervision, they struggle to produce high-quality reconstructions even in observed regions, let alone in unobserved areas. Second, they lack effective mechanisms to mitigate multi-view inconsistencies in the generated images, leading to severe shape-appearance ambiguities and degraded scene geometry. In this paper, we identify accurate geometry as the fundamental prerequisite for effectively exploiting generative models to enhance 3D scene reconstruction. We first propose to leverage the prevalence of planar structures to derive accurate metric-scale depth maps, providing reliable supervision in both observed and unobserved regions. Furthermore, we incorporate this geometry guidance throughout the generative pipeline to improve visibility mask estimation, guide novel view selection, and enhance multi-view consistency when inpainting with video diffusion models, resulting in accurate and consistent scene completion. Extensive experiments on Replica, ScanNet++, and DeepBlending show that our method consistently outperforms existing baselines in both geometry and appearance reconstruction, particularly for unobserved regions. Moreover, our method naturally supports single-view inputs and unposed videos, with strong generalizability in both indoor and outdoor scenarios with practical real-world applicability. The project page is available at https://dali-jack.github.io/g4splat-web/.",
    "summary": "",
    "translation": "G4Splat：基于生成先验的几何引导高斯溅射",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及3D场景重建和生成式建模的计算机视觉技术，属于纯粹的视觉领域研究。虽然提到了生成先验，但其核心是3D高斯溅射和几何引导方法，与推荐系统、搜索或广告的排序和建模需求没有直接关联，也没有明显的潜在应用路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12098v1": {
    "title": "An Adaptive Edge-Guided Dual-Network Framework for Fast QR Code Motion Deblurring",
    "url": "https://www.alphaxiv.org/abs/2510.12098v1",
    "arxiv_id": "2510.12098v1",
    "authors": "Jianping Li, Dongyang Guo, Wenjie Li, Wei Zhao",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 03:03:47",
    "ori_summary": "Unlike general image deblurring that prioritizes perceptual quality, QR code deblurring focuses on ensuring successful decoding. QR codes are characterized by highly structured patterns with sharp edges, a robust prior for restoration. Yet existing deep learning methods rarely exploit these priors explicitly. To address this gap, we propose the Edge-Guided Attention Block (EGAB), which embeds explicit edge priors into a Transformer architecture. Based on EGAB, we develop Edge-Guided Restormer (EG-Restormer), an effective network that significantly boosts the decoding rate of severely blurred QR codes. For mildly blurred inputs, we design the Lightweight and Efficient Network (LENet) for fast deblurring. We further integrate these two networks into an Adaptive Dual-network (ADNet), which dynamically selects the suitable network based on input blur severity, making it ideal for resource-constrained mobile devices. Extensive experiments show that our EG-Restormer and ADNet achieve state-of-the-art performance with a competitive speed. Project page: https://github.com/leejianping/ADNet",
    "summary": "",
    "translation": "一种用于快速QR码运动去模糊的自适应边缘引导双网络框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的QR码图像去模糊问题，属于纯粹的视觉处理任务。虽然标题提到'自适应'和'双网络框架'等技术元素，但核心应用场景（QR码运动去模糊）与推荐系统、搜索或广告领域没有直接关联，也不涉及LLM技术、Transformer架构或异构数据建模等关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12095v1": {
    "title": "IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12095v1",
    "arxiv_id": "2510.12095v1",
    "authors": "Wenxu Zhou, Kaixuan Nie, Hang Du, Dong Yin, Wei Huang, Siqiang Guo, Xiaobo Zhang, Pengbo Hu",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 03:02:33",
    "ori_summary": "In this study, we present IL3D, a large-scale dataset meticulously designed for large language model (LLM)-driven 3D scene generation, addressing the pressing demand for diverse, high-quality training data in indoor layout design. Comprising 27,816 indoor layouts across 18 prevalent room types and a library of 29,215 high-fidelity 3D object assets, IL3D is enriched with instance-level natural language annotations to support robust multimodal learning for vision-language tasks. We establish rigorous benchmarks to evaluate LLM-driven scene generation. Experimental results show that supervised fine-tuning (SFT) of LLMs on IL3D significantly improves generalization and surpasses the performance of SFT on other datasets. IL3D offers flexible multimodal data export capabilities, including point clouds, 3D bounding boxes, multiview images, depth maps, normal maps, and semantic masks, enabling seamless adaptation to various visual tasks. As a versatile and robust resource, IL3D significantly advances research in 3D scene generation and embodied intelligence, by providing high-fidelity scene data to support environment perception tasks of embodied agents.",
    "summary": "",
    "translation": "IL3D：面向LLM驱动的3D场景生成的大规模室内布局数据集",
    "relevance_score": 2,
    "reasoning": "该论文聚焦3D场景生成和室内布局数据集，属于计算机视觉和3D生成领域。虽然提及LLM驱动，但核心是3D视觉内容生成，与推荐系统、搜索或广告的排名和建模需求缺乏直接关联。3D场景生成技术可能间接应用于商品展示或虚拟试穿，但这种应用过于边缘且非核心。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12089v1": {
    "title": "Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback",
    "url": "https://www.alphaxiv.org/abs/2510.12089v1",
    "arxiv_id": "2510.12089v1",
    "authors": "Xingpei Ma, Shenneng Huang, Jiaran Cai, Yuansheng Guan, Shen Zheng, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 02:50:05",
    "ori_summary": "Recent advances in diffusion models have significantly improved audio-driven human video generation, surpassing traditional methods in both quality and controllability. However, existing approaches still face challenges in lip-sync accuracy, temporal coherence for long video generation, and multi-character animation. In this work, we propose a diffusion transformer (DiT)-based framework for generating lifelike talking videos of arbitrary length, and introduce a training-free method for multi-character audio-driven animation. First, we employ a LoRA-based training strategy combined with a position shift inference approach, which enables efficient long video generation while preserving the capabilities of the foundation model. Moreover, we combine partial parameter updates with reward feedback to enhance both lip synchronization and natural body motion. Finally, we propose a training-free approach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character animation, which requires no specialized datasets or model modifications and supports audio-driven animation for three or more characters. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches, achieving high-quality, temporally coherent, and multi-character audio-driven video generation in a simple, efficient, and cost-effective manner.",
    "summary": "",
    "translation": "Playmate2：基于扩散Transformer与奖励反馈的无训练多角色音频驱动动画",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机图形学中的音频驱动动画生成，属于纯粹的视觉/图形领域应用。虽然使用了Transformer架构，但内容涉及角色动画生成，与推荐系统、搜索或广告的核心技术领域没有直接关联，也不符合任何指定的关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12075v1": {
    "title": "A Review on Domain Adaption and Generative Adversarial Networks(GANs)",
    "url": "https://www.alphaxiv.org/abs/2510.12075v1",
    "arxiv_id": "2510.12075v1",
    "authors": "Aashish Dhawan, Divyanshu Mudgal",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-14 02:32:10",
    "ori_summary": "The major challenge in today's computer vision scenario is the availability of good quality labeled data. In a field of study like image classification, where data is of utmost importance, we need to find more reliable methods which can overcome the scarcity of data to produce results comparable to previous benchmark results. In most cases, obtaining labeled data is very difficult because of the high cost of human labor and in some cases impossible. The purpose of this paper is to discuss Domain Adaptation and various methods to implement it. The main idea is to use a model trained on a particular dataset to predict on data from a different domain of the same kind, for example - a model trained on paintings of airplanes predicting on real images of airplanes",
    "summary": "",
    "translation": "领域自适应与生成对抗网络综述",
    "relevance_score": 2,
    "reasoning": "虽然领域自适应技术在某些推荐系统场景中可能有应用价值，但本文主要聚焦于GANs的综述性内容，这属于生成式模型的通用研究范畴。根据指导原则，纯粹的生成式模型研究（如GANs）如果没有明确展示在推荐、搜索或广告中的具体应用，则被视为相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12069v1": {
    "title": "VIDMP3: Video Editing by Representing Motion with Pose and Position Priors",
    "url": "https://www.alphaxiv.org/abs/2510.12069v1",
    "arxiv_id": "2510.12069v1",
    "authors": "Sandeep Mishra, Oindrila Saha, Alan C. Bovik",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 02:20:12",
    "ori_summary": "Motion-preserved video editing is crucial for creators, particularly in scenarios that demand flexibility in both the structure and semantics of swapped objects. Despite its potential, this area remains underexplored. Existing diffusion-based editing methods excel in structure-preserving tasks, using dense guidance signals to ensure content integrity. While some recent methods attempt to address structure-variable editing, they often suffer from issues such as temporal inconsistency, subject identity drift, and the need for human intervention. To address these challenges, we introduce VidMP3, a novel approach that leverages pose and position priors to learn a generalized motion representation from source videos. Our method enables the generation of new videos that maintain the original motion while allowing for structural and semantic flexibility. Both qualitative and quantitative evaluations demonstrate the superiority of our approach over existing methods. The code will be made publicly available at https://github.com/sandeep-sm/VidMP3.",
    "summary": "",
    "translation": "VIDMP3：通过姿态与位置先验表示运动进行视频编辑",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频编辑技术，涉及姿态和位置先验的运动表示，属于纯粹的视觉领域应用。该工作与推荐系统、搜索或广告的核心技术进展、LLM赋能技术、Transformer架构改进或异构数据统一建模均无直接关联，也不符合VLM类比在推荐/搜索/广告中的应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12060v1": {
    "title": "Your VAR Model is Secretly an Efficient and Explainable Generative Classifier",
    "url": "https://www.alphaxiv.org/abs/2510.12060v1",
    "arxiv_id": "2510.12060v1",
    "authors": "Yi-Chung Chen, David I. Inouye, Jing Gao",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-14 01:59:01",
    "ori_summary": "Generative classifiers, which leverage conditional generative models for classification, have recently demonstrated desirable properties such as robustness to distribution shifts. However, recent progress in this area has been largely driven by diffusion-based models, whose substantial computational cost severely limits scalability. This exclusive focus on diffusion-based methods has also constrained our understanding of generative classifiers. In this work, we propose a novel generative classifier built on recent advances in visual autoregressive (VAR) modeling, which offers a new perspective for studying generative classifiers. To further enhance its performance, we introduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which achieves a superior trade-off between accuracy and inference speed, thereby significantly improving practical applicability. Moreover, we show that the VAR-based method exhibits fundamentally different properties from diffusion-based methods. In particular, due to its tractable likelihood, the VAR-based classifier enables visual explainability via token-wise mutual information and demonstrates inherent resistance to catastrophic forgetting in class-incremental learning tasks.",
    "summary": "",
    "translation": "你的VAR模型实际上是一个高效且可解释的生成式分类器",
    "relevance_score": 2,
    "reasoning": "这篇论文讨论的是VAR（向量自回归）模型与生成式分类器的关系，这属于传统时间序列建模和生成模型的理论分析。虽然生成模型在推荐系统中有所应用，但VAR模型主要应用于经济预测和传统时间序列分析，与现代RecSys/Search/Ads中使用的Transformer、LLM等先进技术关联度较低，且论文没有明确展示在推荐、搜索或广告领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12056v1": {
    "title": "APGNet: Adaptive Prior-Guided for Underwater Camouflaged Object Detection",
    "url": "https://www.alphaxiv.org/abs/2510.12056v1",
    "arxiv_id": "2510.12056v1",
    "authors": "Xinxin Huang, Han Sun, Junmin Cai, Ningzhong Liu, Huiyu Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 01:51:44",
    "ori_summary": "Detecting camouflaged objects in underwater environments is crucial for marine ecological research and resource exploration. However, existing methods face two key challenges: underwater image degradation, including low contrast and color distortion, and the natural camouflage of marine organisms. Traditional image enhancement techniques struggle to restore critical features in degraded images, while camouflaged object detection (COD) methods developed for terrestrial scenes often fail to adapt to underwater environments due to the lack of consideration for underwater optical characteristics. To address these issues, we propose APGNet, an Adaptive Prior-Guided Network, which integrates a Siamese architecture with a novel prior-guided mechanism to enhance robustness and detection accuracy. First, we employ the Multi-Scale Retinex with Color Restoration (MSRCR) algorithm for data augmentation, generating illumination-invariant images to mitigate degradation effects. Second, we design an Extended Receptive Field (ERF) module combined with a Multi-Scale Progressive Decoder (MPD) to capture multi-scale contextual information and refine feature representations. Furthermore, we propose an adaptive prior-guided mechanism that hierarchically fuses position and boundary priors by embedding spatial attention in high-level features for coarse localization and using deformable convolution to refine contours in low-level features. Extensive experimental results on two public MAS datasets demonstrate that our proposed method APGNet outperforms 15 state-of-art methods under widely used evaluation metrics.",
    "summary": "",
    "translation": "APGNet：用于水下伪装目标检测的自适应先验引导网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于水下计算机视觉中的伪装目标检测，属于纯粹的视觉领域应用。虽然标题提到'先验引导'和'自适应'等技术概念，但核心应用场景（水下伪装目标检测）与推荐系统、搜索或广告领域没有任何直接关联，也不涉及Transformer架构或LLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13738v1": {
    "title": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based Sequential Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.13738v1",
    "arxiv_id": "2510.13738v1",
    "authors": "Jingyi Zhou, Cheng Chen, Kai Zuo, Manjie Xu, Zhendong Fu, Yibo Chen, Xu Tang, Yao Hu",
    "categories": "cs.IR",
    "pub_date": "2025-10-15 16:45:59",
    "ori_summary": "Large language models (LLMs) have recently demonstrated strong potential for sequential recommendation. However, current LLM-based approaches face critical limitations in modeling users' long-term and diverse interests. First, due to inference latency and feature fetching bandwidth constraints, existing methods typically truncate user behavior sequences to include only the most recent interactions, resulting in the loss of valuable long-range preference signals. Second, most current methods rely on next-item prediction with a single predicted embedding, overlooking the multifaceted nature of user interests and limiting recommendation diversity. To address these challenges, we propose HyMiRec, a hybrid multi-interest sequential recommendation framework, which leverages a lightweight recommender to extracts coarse interest embeddings from long user sequences and an LLM-based recommender to captures refined interest embeddings. To alleviate the overhead of fetching features, we introduce a residual codebook based on cosine similarity, enabling efficient compression and reuse of user history embeddings. To model the diverse preferences of users, we design a disentangled multi-interest learning module, which leverages multiple interest queries to learn disentangles multiple interest signals adaptively, allowing the model to capture different facets of user intent. Extensive experiments are conducted on both benchmark datasets and a collected industrial dataset, demonstrating our effectiveness over existing state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec brings consistent improvements in real-world recommendation systems.",
    "summary": "论文研究LLM在序列推荐中因序列截断导致长程偏好信号丢失和单一预测忽略用户兴趣多样性的问题，核心方法是提出混合多兴趣学习框架，通过轻量推荐器提取粗粒度兴趣嵌入和LLM推荐器捕获细粒度兴趣嵌入，并设计解耦多兴趣学习模块自适应学习多个兴趣信号。",
    "translation": "HyMiRec：一种用于基于大语言模型的序列推荐的混合多兴趣学习框架",
    "relevance_score": 9,
    "reasoning": "该论文直接结合了LLM技术与序列推荐系统，属于'直接LLM应用'范畴。混合多兴趣学习框架可以增强推荐系统对用户动态兴趣的建模能力，在电商、内容推荐等场景中具有直接应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM在序列推荐中的两大核心限制提出解决方案，与LLM应用和Transformer架构优化高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13590v1": {
    "title": "RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for Evolving Knowledge",
    "url": "https://www.alphaxiv.org/abs/2510.13590v1",
    "arxiv_id": "2510.13590v1",
    "authors": "Jiale Han, Austin Cheung, Yubai Wei, Zheng Yu, Xusheng Wang, Bing Zhu, Yi Yang",
    "categories": "cs.IR",
    "pub_date": "2025-10-15 14:21:08",
    "ori_summary": "Knowledge is inherently time-sensitive and continuously evolves over time. Although current Retrieval-Augmented Generation (RAG) systems enrich LLMs with external knowledge, they largely ignore this temporal nature. This raises two challenges for RAG. First, current RAG methods lack effective time-aware representations. Same facts of different time are difficult to distinguish with vector embeddings or conventional knowledge graphs. Second, most RAG evaluations assume a static corpus, leaving a blind spot regarding update costs and retrieval stability as knowledge evolves. To make RAG time-aware, we propose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level temporal graph consisting of a temporal knowledge graph with timestamped relations and a hierarchical time graph. Multi-granularity temporal summaries are generated for each time node to capture both key events and broader trends at that time. The design supports incremental updates by extracting new temporal facts from the incoming corpus and merging them into the existing graph. The temporal graph explicitly represents identical facts at different times as distinct edges to avoid ambiguity, and the time hierarchy graph allows only generating reports for new leaf time nodes and their ancestors, ensuring effective and efficient updates. During inference, TG-RAG dynamically retrieves a subgraph within the temporal and semantic scope of the query, enabling precise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive question-answering dataset featuring both specific and abstract queries, along with a comprehensive evaluation protocol designed to assess incremental update capabilities of RAG systems. Extensive experiments show that TG-RAG significantly outperforms existing baselines, demonstrating the effectiveness of our method in handling temporal knowledge and incremental updates.",
    "summary": "研究RAG系统如何处理时间敏感知识演化的核心问题，核心方法是构建包含时序知识图和层次时间图的双层次时序图结构，通过多粒度时间摘要和增量更新机制来区分不同时间的相同事实。",
    "translation": "RAG与时间图交汇：面向演化知识的时序敏感建模与检索",
    "relevance_score": 8,
    "reasoning": "该论文结合RAG（检索增强生成）与时间图建模，直接适用于搜索和推荐系统中处理动态用户行为、商品生命周期等时序数据。时序敏感的检索机制可显著提升推荐系统对用户兴趣演变的捕捉能力，并为搜索系统提供更准确的时间上下文理解。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对RAG系统的时间感知问题，提出了双层次时序图建模方法，对搜索和推荐系统中的动态知识更新具有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13371v1": {
    "title": "MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.13371v1",
    "arxiv_id": "2510.13371v1",
    "authors": "Jiin Park, Misuk Kim",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-15 10:03:29",
    "ori_summary": "Recent attempts to integrate large language models (LLMs) into recommender systems have gained momentum, but most remain limited to simple text generation or static prompt-based inference, failing to capture the complexity of user preferences and real-world interactions. This study proposes the Multi-Aspect Driven LLM Agent MADRec, an autonomous LLM-based recommender that constructs user and item profiles by unsupervised extraction of multi-aspect information from reviews and performs direct recommendation, sequential recommendation, and explanation generation. MADRec generates structured profiles via aspect-category-based summarization and applies Re-Ranking to construct high-density inputs. When the ground-truth item is missing from the output, the Self-Feedback mechanism dynamically adjusts the inference criteria. Experiments across multiple domains show that MADRec outperforms traditional and LLM-based baselines in both precision and explainability, with human evaluation further confirming the persuasiveness of the generated explanations.",
    "summary": "论文研究如何构建能捕捉用户偏好复杂性的LLM推荐系统，核心方法是提出多维度驱动的LLM代理，通过无监督提取评论中的多维度信息构建用户和物品画像，并采用重排序和自反馈机制实现自适应推荐。",
    "translation": "MADREC：一种面向可解释和自适应推荐的多方面驱动大语言模型智能体",
    "relevance_score": 9,
    "reasoning": "该论文直接应用LLM技术构建推荐系统智能体，属于Direct LLM Applications范畴。标题明确表明这是一个用于推荐系统的LLM智能体，具备多方面驱动、可解释性和自适应能力，这些特性在个性化推荐中具有重要价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用LLM构建多维度用户画像并实现自适应推荐，完美契合直接LLM应用和推荐系统核心进展的研究重点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13359v1": {
    "title": "Improving Visual Recommendation on E-commerce Platforms Using Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13359v1",
    "arxiv_id": "2510.13359v1",
    "authors": "Yuki Yada, Sho Akiyama, Ryo Watanabe, Yuta Ueno, Yusuke Shido, Andre Rusli",
    "categories": "cs.IR, cs.CV, cs.LG",
    "pub_date": "2025-10-15 09:46:27",
    "ori_summary": "On large-scale e-commerce platforms with tens of millions of active monthly users, recommending visually similar products is essential for enabling users to efficiently discover items that align with their preferences. This study presents the application of a vision-language model (VLM) -- which has demonstrated strong performance in image recognition and image-text retrieval tasks -- to product recommendations on Mercari, a major consumer-to-consumer marketplace used by more than 20 million monthly users in Japan. Specifically, we fine-tuned SigLIP, a VLM employing a sigmoid-based contrastive loss, using one million product image-title pairs from Mercari collected over a three-month period, and developed an image encoder for generating item embeddings used in the recommendation system. Our evaluation comprised an offline analysis of historical interaction logs and an online A/B test in a production environment. In offline analysis, the model achieved a 9.1% improvement in nDCG@5 compared with the baseline. In the online A/B test, the click-through rate improved by 50% whereas the conversion rate improved by 14% compared with the existing model. These results demonstrate the effectiveness of VLM-based encoders for e-commerce product recommendations and provide practical insights into the development of visual similarity-based recommendation systems.",
    "summary": "该论文研究电商平台中基于视觉相似性的产品推荐问题，其核心方法是利用视觉语言模型（VLM）对产品图像和标题进行联合建模，通过微调SigLIP模型生成用于推荐系统的商品嵌入表示。",
    "translation": "基于视觉语言模型改进电商平台的视觉推荐",
    "relevance_score": 9,
    "reasoning": "该论文直接应用视觉语言模型技术于电商推荐系统，属于'Direct LLM Applications'范畴。视觉推荐在电商平台中至关重要，VLM能够更好地理解商品图像与文本描述的关联，从而提升推荐质量和用户体验。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用视觉语言模型于电商推荐系统，完美契合VLM类比异构数据和直接LLM应用两个重点领域。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13312v1": {
    "title": "ChatR1: Reinforcement Learning for Conversational Reasoning and Retrieval Augmented Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.13312v1",
    "arxiv_id": "2510.13312v1",
    "authors": "Simon Lupart, Mohammad Aliannejadi, Evangelos Kanoulas",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2025-10-15 09:00:20",
    "ori_summary": "We present ChatR1, a reasoning framework based on reinforcement learning (RL) for conversational question answering (CQA). Reasoning plays an important role in CQA, where user intent evolves across dialogue turns, and utterances are often underspecified, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Unlike static `rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through RL. To address the challenge of sparse and delayed rewards in RL, we propose an intent-aware reward that provides turn-level feedback by aligning retrieval and reasoning with evolving user goals. Our proposed ChatR1 demonstrates strong performance on both 3B and 7B model backbones, outperforming competitive models on five CQA datasets, measured by different metrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA datasets to cover topic shifts, evolving intents, mixed-initiative dialogues, and multi-document grounding, testing ChatR1's performance from various aspects. Ablation studies confirm the effectiveness of the intent-aware reward. Our analyses further reveal diverse reasoning trajectories and effective use of the search tool. ChatR1 also generalizes robustly across domains, demonstrating that RL-based reasoning enables more flexible and context-sensitive behavior than static CQA pipelines.",
    "summary": "",
    "translation": "ChatR1：用于对话推理和检索增强问答的强化学习",
    "relevance_score": 3,
    "reasoning": "虽然论文涉及检索增强和对话推理，这些技术可能应用于搜索系统，但核心焦点是强化学习在问答中的应用。根据指导原则，没有明确展示与推荐系统、搜索或广告相关性的强化学习论文应被视为低相关性。检索增强组件具有潜在搜索应用，但强化学习主导使其相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13229v1": {
    "title": "Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning for Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.13229v1",
    "arxiv_id": "2510.13229v1",
    "authors": "Yi Zhang, Lili Xie, Ruihong Qiu, Jiajun Liu, Sen Wang",
    "categories": "cs.IR",
    "pub_date": "2025-10-15 07:28:29",
    "ori_summary": "Recommender systems (RecSys) have become critical tools for enhancing user engagement by delivering personalized content across diverse digital platforms. Recent advancements in large language models (LLMs) demonstrate significant potential for improving RecSys, primarily due to their exceptional generalization capabilities and sophisticated contextual understanding, which facilitate the generation of flexible and interpretable recommendations. However, the direct deployment of LLMs as primary recommendation policies presents notable challenges, including persistent latency issues stemming from frequent API calls and inherent model limitations such as hallucinations and biases. To address these issues, this paper proposes a novel offline reinforcement learning (RL) framework that leverages imitation learning from LLM-generated trajectories. Specifically, inverse reinforcement learning is employed to extract robust reward models from LLM demonstrations. This approach negates the need for LLM fine-tuning, thereby substantially reducing computational overhead. Simultaneously, the RL policy is guided by the cumulative rewards derived from these demonstrations, effectively transferring the semantic insights captured by the LLM. Comprehensive experiments conducted on two benchmark datasets validate the effectiveness of the proposed method, demonstrating superior performance when compared against state-of-the-art RL-based and in-context learning baselines. The code can be found at https://github.com/ArronDZhang/IL-Rec.",
    "summary": "论文研究如何克服LLM直接作为推荐策略的延迟和幻觉问题。核心思想是利用模仿学习从LLM生成的轨迹中提取奖励模型，通过离线强化学习框架将LLM的语义理解转移到高效策略中，无需微调LLM。",
    "translation": "超越静态大语言模型策略：用于推荐的模仿增强强化学习",
    "relevance_score": 8,
    "reasoning": "该论文直接结合了强化学习与推荐系统，属于核心领域进展。虽然排除了不相关的RL论文，但这里RL明确应用于推荐场景，符合Direct LLM Applications范畴。模仿增强方法可能提升推荐策略的适应性和性能，在动态用户交互环境中具有实际应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM在推荐系统中的应用挑战，提出结合模仿学习和强化学习的新框架，完美契合核心关注点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13217v1": {
    "title": "LLM-guided Hierarchical Retrieval",
    "url": "https://www.alphaxiv.org/abs/2510.13217v1",
    "arxiv_id": "2510.13217v1",
    "authors": "Nilesh Gupta, Wei-Cheng Chang, Ngot Bui, Cho-Jui Hsieh, Inderjit S. Dhillon",
    "categories": "cs.IR, cs.LG",
    "pub_date": "2025-10-15 07:05:17",
    "ori_summary": "Modern IR systems are increasingly tasked with answering complex, multi-faceted queries that require deep reasoning rather than simple keyword or semantic matching. While LLM-based IR has shown great promise, the prevailing retrieve-then-rerank paradigm inherits the limitations of embedding-based retrieval; parametric generative approaches are difficult to update with new information; and long-context methods that place the entire corpus in context are computationally infeasible for large document collections. To address these challenges, we introduce LATTICE, a hierarchical retrieval framework that enables an LLM to reason over and navigate large corpora with logarithmic search complexity by imposing a semantic tree structure on the corpus. Our approach consists of two stages: (1) an offline phase that organizes the corpus into a semantic hierarchy via either a bottom-up agglomerative strategy or a top-down divisive strategy using multi-level summaries and (2) an online traversal phase where a search LLM navigates this tree. A central challenge in such LLM-guided search is that the model's relevance judgments are noisy, context-dependent, and unaware of the hierarchy, making cross-branch and cross-level comparisons difficult. To overcome this, we propose a traversal algorithm that estimates calibrated latent relevance scores from local LLM outputs and aggregates them into a global path relevance metric. Our training-free framework achieves state-of-the-art zero-shot performance on the reasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in Recall@100 and 5% in nDCG@10 over the next best zero-shot baseline. Furthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains comparable results on BRIGHT subsets that use a static corpus for evaluation.",
    "summary": "论文研究复杂查询下的高效文档检索问题，核心思想是通过构建语义层次树结构并设计LLM导航算法，实现对数复杂度的层次化检索。",
    "translation": "LLM引导的分层检索",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及LLM在检索系统中的应用，属于'直接LLM应用'范畴。分层检索是搜索和推荐系统中的核心架构模式，LLM引导可以显著提升检索效率和准确性，在电商搜索、内容推荐等场景中有直接应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对搜索系统中的核心检索问题，提出LLM引导的层次检索框架，属于LLM在搜索领域的直接应用创新，与关注点高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13193v1": {
    "title": "ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient RAG",
    "url": "https://www.alphaxiv.org/abs/2510.13193v1",
    "arxiv_id": "2510.13193v1",
    "authors": "Yikuan Hu, Jifeng Zhu, Lanrui Tang, Chen Huang",
    "categories": "cs.IR",
    "pub_date": "2025-10-15 06:31:29",
    "ori_summary": "Knowledge graphs (KGs), with their structured representation capabilities, offer promising avenue for enhancing Retrieval Augmented Generation (RAG) systems, leading to the development of KG-RAG systems. Nevertheless, existing methods often struggle to achieve effective synergy between system effectiveness and cost efficiency, leading to neither unsatisfying performance nor excessive LLM prompt tokens and inference time. To this end, this paper proposes REMINDRAG, which employs an LLM-guided graph traversal featuring node exploration, node exploitation, and, most notably, memory replay, to improve both system effectiveness and cost efficiency. Specifically, REMINDRAG memorizes traversal experience within KG edge embeddings, mirroring the way LLMs \"memorize\" world knowledge within their parameters, but in a train-free manner. We theoretically and experimentally confirm the effectiveness of REMINDRAG, demonstrating its superiority over existing baselines across various benchmark datasets and LLM backbones. Our code is available at https://github.com/kilgrims/ReMindRAG.",
    "summary": "该论文研究知识图谱增强检索生成(RAG)系统中效率与效果难以兼顾的问题，核心思想是采用LLM引导的图遍历策略，通过节点探索、利用和记忆回放机制，并在图边嵌入中存储遍历经验以实现免训练的记忆功能。",
    "translation": "ReMindRAG：面向高效检索增强生成的低成本大语言模型引导知识图谱遍历",
    "relevance_score": 8,
    "reasoning": "该论文聚焦于RAG系统的效率优化，通过LLM引导的知识图谱遍历技术，这直接适用于搜索和推荐系统中的知识增强检索场景。低成本的LLM指导机制可显著提升推荐和搜索系统中知识图谱查询的效率，为处理大规模用户-物品交互图提供技术支撑。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出LLM引导的知识图谱遍历方法，直接应用于搜索和推荐系统的检索增强生成(RAG)领域，同时关注Transformer架构的效率优化问题。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13095v1": {
    "title": "Retrieval-in-the-Chain: Bootstrapping Large Language Models for Generative Retrieval",
    "url": "https://www.alphaxiv.org/abs/2510.13095v1",
    "arxiv_id": "2510.13095v1",
    "authors": "Yingchen zhang, Ruqing zhang, Jiafeng Guo, Wenjun Peng, Sen Li, Fuyu Lv",
    "categories": "cs.IR",
    "pub_date": "2025-10-15 02:29:10",
    "ori_summary": "Generative retrieval (GR) is an emerging paradigm that leverages large language models (LLMs) to autoregressively generate document identifiers (docids) relevant to a given query. Prior works have focused on leveraging the generative capabilities of LLMs to improve GR, while overlooking that their reasoning capabilities could likewise help. This raises a key question: Can explicit reasoning benefit GR? To investigate, we first conduct a preliminary study where an LLM is prompted to generate free-form chain-of-thought (CoT) reasoning before performing constrained docid decoding. Although this method outperforms standard GR, the generated reasoning tends to be verbose and poorly aligned with the docid space. These limitations motivate the development of a reasoning mechanism better tailored to GR. Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented framework for GR that converts free-form CoT reasoning into a compact, structured format, and iteratively refines the reasoning during the retrieval process. R4R augments an existing GR method by leveraging a reasoning-capable LLM that has been instruction-tuned for GR. At inference time, R4R first uses the LLM to generate an initial structured reasoning; then the same LLM alternates between (i) constrained decoding with the chosen GR method to produce candidate docids and (ii) updating the reasoning based on retrieval results to improve the next round. R4R does not require additional models or training, and instead a single LLM serves as both the reasoning generator and the retriever. Extensive experiments on Natural Questions, MS MARCO, and a real-world item-search benchmark validate the effectiveness of R4R.",
    "summary": "研究如何利用大语言模型的推理能力改进生成式检索；核心方法是提出结构化推理框架R4R，将自由形式推理转换为紧凑结构格式，并在检索过程中迭代优化推理。",
    "translation": "链式检索：自举大语言模型实现生成式检索",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及LLM在检索系统中的应用，属于'直接LLM应用'和'核心领域进展'范畴。生成式检索是搜索和推荐系统中的前沿技术，通过LLM直接生成相关文档或项目，可以显著提升检索效率和准确性，在个性化推荐和语义搜索中有重要应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文将推理能力引入生成式检索，提出结构化推理迭代优化方法，直接提升检索性能，与搜索和推荐系统高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13804v1": {
    "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
    "url": "https://www.alphaxiv.org/abs/2510.13804v1",
    "arxiv_id": "2510.13804v1",
    "authors": "Xinchen Zhang, Xiaoying Zhang, Youbin Wu, Yanbin Cao, Renrui Zhang, Ruihang Chu, Ling Yang, Yujiu Yang",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-10-15 17:59:24",
    "ori_summary": "We introduce Generative Universal Verifier, a novel concept and plugin designed for next-generation multimodal reasoning in vision-language models and unified multimodal models, providing the fundamental capability of reflection and refinement on visual outcomes during the reasoning and generation process. This work makes three main contributions: (1) We build ViVerBench, a comprehensive benchmark spanning 16 categories of critical tasks for evaluating visual outcomes in multimodal reasoning. Results show that existing VLMs consistently underperform across these tasks, underscoring a substantial gap from human-level capability in reliable visual verification. (2) We design two automated pipelines to construct large-scale visual verification data and train OmniVerifier-7B, the first omni-capable generative verifier trained for universal visual verification and achieves notable gains on ViVerBench(+8.3). Through training, we identify three atomic capabilities in visual verification and demonstrate how they generalize and interact synergistically. (3) We propose OmniVerifier-TTS, a sequential test-time scaling paradigm that leverages the universal verifier to bridge image generation and editing within unified models, enhancing the upper bound of generative ability through iterative fine-grained optimization. Beyond generation, we extend universal verifier to broader world-modeling interleaved reasoning scenarios. Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7), and GenEval++(+4.3), outperforming existing parallel test-time scaling methods, such as Best-of-N. By endowing multimodal reasoning with reliable visual verification, OmniVerifier advances both reliable reflection during generation and scalable test-time refinement, marking a step toward more trustworthy and controllable next-generation reasoning systems.",
    "summary": "",
    "translation": "生成式通用验证器作为多模态元推理器",
    "relevance_score": 2,
    "reasoning": "该论文标题暗示了多模态推理和验证能力，这可能与VLM类比处理异构数据的概念有微弱联系。然而，它更侧重于通用的多模态推理和验证，而非专门针对推荐系统、搜索或广告领域的应用，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13799v1": {
    "title": "BRIEF-Pro: Universal Context Compression with Short-to-Long Synthesis for Fast and Accurate Multi-Hop Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.13799v1",
    "arxiv_id": "2510.13799v1",
    "authors": "Jia-Chen Gu, Junyi Zhang, Di Wu, Yuankai Li, Kai-Wei Chang, Nanyun Peng",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 17:57:45",
    "ori_summary": "As retrieval-augmented generation (RAG) tackles complex tasks, increasingly expanded contexts offer richer information, but at the cost of higher latency and increased cognitive load on the model. To mitigate this bottleneck, especially for intricate multi-hop questions, we introduce BRIEF-Pro. It is a universal, lightweight compressor that distills relevant evidence for a given query from retrieved documents into a concise summary for seamless integration into in-context RAG. Using seed data consisting of relatively short contexts (fewer than 1k words), BRIEF-Pro is trained to perform abstractive compression of extended contexts exceeding 10k words across a wide range of scenarios. Furthermore, BRIEF-Pro offers flexible user control over summary length by allowing users to specify the desired number of sentences. Experiments on four open-domain multi-hop question-answering datasets show that BRIEF-Pro generates more concise and relevant summaries, enhancing performance across small, large, and proprietary language models. With the 70B reader model, 32x compression by BRIEF-Pro improves QA performance by 4.67% on average over LongLLMLingua's 9x, while requiring only 23% of its computational overhead.",
    "summary": "论文研究检索增强生成中长上下文导致的延迟和认知负载问题，核心方法是训练通用轻量级压缩器，通过短上下文训练实现长文档的抽象压缩，并支持用户自定义摘要长度。",
    "translation": "BRIEF-Pro：通过短到长合成实现通用上下文压缩，用于快速准确的多跳推理",
    "relevance_score": 7,
    "reasoning": "该论文提出的上下文压缩技术属于核心LLM技术进展，能够显著提升推理效率。在搜索和推荐系统中，这种压缩技术可以加速复杂查询处理和多跳推理任务，同时保持准确性，对于处理长用户历史序列和复杂上下文特征具有直接应用价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的上下文压缩技术直接解决检索增强生成中的延迟和认知负载问题，对搜索和推荐系统中的长文档处理具有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13797v1": {
    "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons",
    "url": "https://www.alphaxiv.org/abs/2510.13797v1",
    "arxiv_id": "2510.13797v1",
    "authors": "Giovanni Monea, Yair Feldman, Shankar Padmanabhan, Kianté Brantley, Yoav Artzi",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 17:57:21",
    "ori_summary": "The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques.",
    "summary": "论文研究Transformer模型长上下文推理中的内存效率问题，核心思想是通过学习特殊压缩标记周期性压缩生成过程中的KV缓存，利用信息价值递减特性实现内存优化。",
    "translation": "面包屑推理：基于压缩信标的内存高效推理",
    "relevance_score": 8,
    "reasoning": "该论文提出内存高效的推理方法，通过压缩技术减少计算需求，这直接属于'Enabling LLM Tech'范畴。在推荐系统和搜索广告中，内存高效的推理可以显著降低大规模部署成本，支持更复杂的模型在资源受限环境下的实时应用，如长序列用户行为建模和复杂上下文理解。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出KV缓存压缩方法直接解决Transformer架构的效率瓶颈，属于Transformer技术进展的核心领域，对推荐系统和搜索中的长序列处理有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13796v1": {
    "title": "The Mechanistic Emergence of Symbol Grounding in Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13796v1",
    "arxiv_id": "2510.13796v1",
    "authors": "Shuyu Wu, Ziqiao Ma, Xiaoxi Luo, Yidong Huang, Josue Torres-Fonseca, Freda Shi, Joyce Chai",
    "categories": "cs.CL, cs.CV",
    "pub_date": "2025-10-15 17:56:15",
    "ori_summary": "Symbol grounding (Harnad, 1990) describes how symbols such as words acquire their meanings by connecting to real-world sensorimotor experiences. Recent work has shown preliminary evidence that grounding may emerge in (vision-)language models trained at scale without using explicit grounding objectives. Yet, the specific loci of this emergence and the mechanisms that drive it remain largely unexplored. To address this problem, we introduce a controlled evaluation framework that systematically traces how symbol grounding arises within the internal computations through mechanistic and causal analysis. Our findings show that grounding concentrates in middle-layer computations and is implemented through the aggregate mechanism, where attention heads aggregate the environmental ground to support the prediction of linguistic forms. This phenomenon replicates in multimodal dialogue and across architectures (Transformers and state-space models), but not in unidirectional LSTMs. Our results provide behavioral and mechanistic evidence that symbol grounding can emerge in language models, with practical implications for predicting and potentially controlling the reliability of generation.",
    "summary": "",
    "translation": "语言模型中符号接地的机制性涌现",
    "relevance_score": 6,
    "reasoning": "该论文探讨语言模型中符号接地的机制性涌现，这属于核心LLM技术的基础进展。符号接地对于推荐和搜索系统至关重要，因为它能增强模型对用户查询和商品特征的理解能力，从而提高语义匹配的准确性。这种基础理解能力的提升可以直接应用于改善搜索相关性排序和推荐系统的语义理解模块。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.13750v1": {
    "title": "Confidence-Based Response Abstinence: Improving LLM Trustworthiness via Activation-Based Uncertainty Estimation",
    "url": "https://www.alphaxiv.org/abs/2510.13750v1",
    "arxiv_id": "2510.13750v1",
    "authors": "Zhiqi Huang, Vivek Datla, Chenyang Zhu, Alfy Samuel, Daben Liu, Anoop Kumar, Ritesh Soni",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 16:55:56",
    "ori_summary": "We propose a method for confidence estimation in retrieval-augmented generation (RAG) systems that aligns closely with the correctness of large language model (LLM) outputs. Confidence estimation is especially critical in high-stakes domains such as finance and healthcare, where the cost of an incorrect answer outweighs that of not answering the question. Our approach extends prior uncertainty quantification methods by leveraging raw feed-forward network (FFN) activations as auto-regressive signals, avoiding the information loss inherent in token logits and probabilities after projection and softmax normalization. We model confidence prediction as a sequence classification task, and regularize training with a Huber loss term to improve robustness against noisy supervision. Applied in a real-world financial industry customer-support setting with complex knowledge bases, our method outperforms strong baselines and maintains high accuracy under strict latency constraints. Experiments on Llama 3.1 8B model show that using activations from only the 16th layer preserves accuracy while reducing response latency. Our results demonstrate that activation-based confidence modeling offers a scalable, architecture-aware path toward trustworthy RAG deployment.",
    "summary": "",
    "translation": "基于置信度的响应弃权：通过基于激活的不确定性估计提升大语言模型可信度",
    "relevance_score": 3,
    "reasoning": "该论文主要关注LLM可信度和不确定性估计，属于LLM评估和可靠性范畴。虽然不确定性估计在技术上可能对推荐系统或搜索中的置信度校准有潜在应用，但论文焦点更偏向NLP-centric的评估和可信度问题，而非直接的RecSys/Search/Ads应用或核心架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13749v1": {
    "title": "Assessing Web Search Credibility and Response Groundedness in Chat Assistants",
    "url": "https://www.alphaxiv.org/abs/2510.13749v1",
    "arxiv_id": "2510.13749v1",
    "authors": "Ivan Vykopal, Matúš Pikuliak, Simon Ostermann, Marián Šimko",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 16:55:47",
    "ori_summary": "Chat assistants increasingly integrate web search functionality, enabling them to retrieve and cite external sources. While this promises more reliable answers, it also raises the risk of amplifying misinformation from low-credibility sources. In this paper, we introduce a novel methodology for evaluating assistants' web search behavior, focusing on source credibility and the groundedness of responses with respect to cited sources. Using 100 claims across five misinformation-prone topics, we assess GPT-4o, GPT-5, Perplexity, and Qwen Chat. Our findings reveal differences between the assistants, with Perplexity achieving the highest source credibility, whereas GPT-4o exhibits elevated citation of non-credibility sources on sensitive topics. This work provides the first systematic comparison of commonly used chat assistants for fact-checking behavior, offering a foundation for evaluating AI systems in high-stakes information environments.",
    "summary": "",
    "translation": "评估聊天助手中的网页搜索可信度与回答真实性",
    "relevance_score": 2,
    "reasoning": "该论文主要关注搜索可信度和回答真实性评估，这属于纯粹的NLP评估基准范畴，与我的核心关注点不相关。虽然涉及搜索领域，但焦点是评估指标而非核心推荐系统、搜索或广告的技术进步或应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13744v1": {
    "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math",
    "url": "https://www.alphaxiv.org/abs/2510.13744v1",
    "arxiv_id": "2510.13744v1",
    "authors": "Shrey Pandit, Austin Xu, Xuan-Phi Nguyen, Yifei Ming, Caiming Xiong, Shafiq Joty",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-15 16:50:54",
    "ori_summary": "Large language model (LLM)-based reasoning systems have recently achieved gold medal-level performance in the IMO 2025 competition, writing mathematical proofs where, to receive full credit, each step must be not only correct but also sufficiently supported. To train LLM-based reasoners in such challenging, open-ended settings, strong verifiers capable of catching step-level mistakes are necessary prerequisites. We introduce Hard2Verify, a human-annotated, step-level verification benchmark produced with over 500 hours of human labor. Hard2Verify is designed to rigorously assess step-level verifiers at the frontier: Verifiers must provide step-level annotations or identify the first error in responses generated by frontier LLMs for very recent, challenging, and open-ended math questions. We evaluate 29 generative critics and process reward models, demonstrating that, beyond a few standouts, open-source verifiers lag closed source models. We subsequently analyze what drives poor performance in step-level verification, the impacts of scaling verifier compute, as well as fundamental questions such as self-verification and verification-generation dynamics.",
    "summary": "",
    "translation": "Hard2Verify：面向开放式前沿数学问题的步骤级验证基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于数学问题验证基准，属于纯粹的评估基准研究。虽然涉及验证技术，但完全限定在数学领域，与推荐系统、搜索或广告没有任何潜在应用关联。这属于纯粹的NLP评估基准范畴，属于明确的无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13734v1": {
    "title": "GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI Clinicians",
    "url": "https://www.alphaxiv.org/abs/2510.13734v1",
    "arxiv_id": "2510.13734v1",
    "authors": "Xiuyuan Chen, Tao Sun, Dexin Su, Ailing Yu, Junwei Liu, Zhe Chen, Gangzeng Jin, Xin Wang, Jingnan Liu, Hansong Xiao, Hualei Zhou, Dongjie Tao, Chunxiao Guo, Minghui Yang, Yuan Xia, Jing Zhao, Qianrui Fan, Yanyun Wang, Shuai Zhen, Kezhong Chen, Jun Wang, Zewen Sun, Heng Zhao, Tian Guan, Shaodong Wang, Geyun Chang, Jiaming Deng, Hongchengcheng Chen, Kexin Feng, Ruzhen Li, Jiayi Geng, Changtai Zhao, Jun Wang, Guihu Lin, Peihao Li, Liqi Liu, Peng Wei, Jian Wang, Jinjie Gu, Ping Wang, Fan Yang",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 16:40:28",
    "ori_summary": "Current benchmarks for AI clinician systems, often based on multiple-choice exams or manual rubrics, fail to capture the depth, robustness, and safety required for real-world clinical practice. To address this, we introduce the GAPS framework, a multidimensional paradigm for evaluating \\textbf{G}rounding (cognitive depth), \\textbf{A}dequacy (answer completeness), \\textbf{P}erturbation (robustness), and \\textbf{S}afety. Critically, we developed a fully automated, guideline-anchored pipeline to construct a GAPS-aligned benchmark end-to-end, overcoming the scalability and subjectivity limitations of prior work. Our pipeline assembles an evidence neighborhood, creates dual graph and tree representations, and automatically generates questions across G-levels. Rubrics are synthesized by a DeepResearch agent that mimics GRADE-consistent, PICO-driven evidence review in a ReAct loop. Scoring is performed by an ensemble of large language model (LLM) judges. Validation confirmed our automated questions are high-quality and align with clinician judgment. Evaluating state-of-the-art models on the benchmark revealed key failure modes: performance degrades sharply with increased reasoning depth (G-axis), models struggle with answer completeness (A-axis), and they are highly vulnerable to adversarial perturbations (P-axis) as well as certain safety issues (S-axis). This automated, clinically-grounded approach provides a reproducible and scalable method for rigorously evaluating AI clinician systems and guiding their development toward safer, more reliable clinical practice.",
    "summary": "",
    "translation": "GAPS：基于临床基础的自动化基准，用于评估AI临床医生",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域AI临床医生的评估基准，属于明确的医学应用范畴。根据筛选规则，医疗、生物学等特定领域应用属于不相关主题，且论文内容与推荐系统、搜索、广告或相关使能技术无任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13721v1": {
    "title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching",
    "url": "https://www.alphaxiv.org/abs/2510.13721v1",
    "arxiv_id": "2510.13721v1",
    "authors": "Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, Tat-Seng Chua",
    "categories": "cs.CL, cs.AI, cs.CV, cs.MM",
    "pub_date": "2025-10-15 16:25:18",
    "ori_summary": "Next-generation multimodal foundation models capable of any-to-any cross-modal generation and multi-turn interaction will serve as core components of artificial general intelligence systems, playing a pivotal role in human-machine interaction. However, most existing multimodal models remain constrained by autoregressive architectures, whose inherent limitations prevent a balanced integration of understanding and generation capabilities. Although hybrid and decoupling strategies have been explored to address these tasks within unified frameworks separately, their redundant, non-integrated designs limit their applicability to broader scenarios, such as cross-modal retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal foundation model that achieves unified modeling through discrete flow paradigms. By leveraging metric-induced probability paths and kinetic optimal velocities, NExT-OMNI natively supports any-to-any understanding and generation with enhanced response efficiency, while enabling broader application scenarios through concise unified representations rather than task-decoupled designs. Trained on large-scale interleaved text, image, video, and audio data, NExT-OMNI delivers competitive performance on multimodal generation and understanding benchmarks, while outperforming prior unified models in multi-turn multimodal interaction and cross-modal retrieval, highlighting its architectural advantages as a next-generation multimodal foundation model. To advance further research, we release training details, data protocols, and open-source both the code and model checkpoints.",
    "summary": "研究下一代多模态基础模型如何实现任意模态间的统一理解与生成；核心方法是采用离散流范式，通过度量诱导概率路径和动力学最优速度实现统一建模，避免传统自回归架构的理解与生成能力不平衡问题。",
    "translation": "NExT-OMNI：基于离散流匹配实现任意模态到任意模态的全模态基础模型",
    "relevance_score": 9,
    "reasoning": "该论文提出的任意模态到任意模态的全模态基础模型技术，与'VLM类比处理异构数据'高度相关，可将推荐系统中的用户行为序列、上下文特征等异构数据视为不同模态进行统一建模。离散流匹配技术作为'使能Transformer技术'的进步，能够提升多模态建模的效率，为推荐系统处理复杂用户数据提供新的架构范式。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的离散流匹配统一建模方法可直接应用于推荐系统的多模态理解与生成，其任意到任意跨模态能力与异构数据处理理念高度契合搜索推荐场景。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13681v1": {
    "title": "How Sampling Affects the Detectability of Machine-written texts: A Comprehensive Study",
    "url": "https://www.alphaxiv.org/abs/2510.13681v1",
    "arxiv_id": "2510.13681v1",
    "authors": "Matthieu Dubois, François Yvon, Pablo Piantanida",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 15:36:45",
    "ori_summary": "As texts generated by Large Language Models (LLMs) are ever more common and often indistinguishable from human-written content, research on automatic text detection has attracted growing attention. Many recent detectors report near-perfect accuracy, often boasting AUROC scores above 99\\%. However, these claims typically assume fixed generation settings, leaving open the question of how robust such systems are to changes in decoding strategies. In this work, we systematically examine how sampling-based decoding impacts detectability, with a focus on how subtle variations in a model's (sub)word-level distribution affect detection performance. We find that even minor adjustments to decoding parameters - such as temperature, top-p, or nucleus sampling - can severely impair detector accuracy, with AUROC dropping from near-perfect levels to 1\\% in some settings. Our findings expose critical blind spots in current detection methods and emphasize the need for more comprehensive evaluation protocols. To facilitate future research, we release a large-scale dataset encompassing 37 decoding configurations, along with our code and evaluation framework https://github.com/BaggerOfWords/Sampling-and-Detection",
    "summary": "",
    "translation": "采样如何影响机器生成文本的可检测性：一项综合性研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器生成文本的检测技术，这属于纯粹的NLP评估和检测领域，与推荐系统、搜索或广告的核心技术无关。论文内容涉及文本检测的基准测试和评估方法，属于明确的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13632v1": {
    "title": "Closing the Gap Between Text and Speech Understanding in LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.13632v1",
    "arxiv_id": "2510.13632v1",
    "authors": "Santiago Cuervo, Skyler Seto, Maureen de Seyssel, Richard He Bai, Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly, Zakaria Aldeneh",
    "categories": "cs.CL, cs.AI, eess.AS",
    "pub_date": "2025-10-15 14:57:16",
    "ori_summary": "Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts--and even cascaded pipelines--on language understanding tasks. We term this shortfall the text-speech understanding gap: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce SALAD--Sample-efficient Alignment with Learning through Active selection and cross-modal Distillation--which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora.",
    "summary": "",
    "translation": "弥合大型语言模型中文本与语音理解之间的差距",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文本与语音模态的融合理解，这属于多模态LLM范畴，但语音理解在推荐系统、搜索或广告中的直接应用场景非常有限。虽然多模态技术可能间接启发异构数据处理，但论文标题明确聚焦于语音这一与当前关注点相关性较低的模态。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13626v1": {
    "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models",
    "url": "https://www.alphaxiv.org/abs/2510.13626v1",
    "arxiv_id": "2510.13626v1",
    "authors": "Senyu Fei, Siyin Wang, Junhao Shi, Zihao Dai, Jikun Cai, Pengfang Qian, Li Ji, Xinzhe He, Shiduo Zhang, Zhaoye Fei, Jinlan Fu, Jingjing Gong, Xipeng Qiu",
    "categories": "cs.RO, cs.CL, cs.CV",
    "pub_date": "2025-10-15 14:51:36",
    "ori_summary": "Visual-Language-Action (VLA) models report impressive success rates on robotic manipulation benchmarks, yet these results may mask fundamental weaknesses in robustness. We perform a systematic vulnerability analysis by introducing controlled perturbations across seven dimensions: objects layout, camera viewpoints, robot initial states, language instructions, light conditions, background textures and sensor noise. We comprehensively analyzed multiple state-of-the-art models and revealed consistent brittleness beneath apparent competence. Our analysis exposes critical weaknesses: models exhibit extreme sensitivity to perturbation factors, including camera viewpoints and robot initial states, with performance dropping from 95% to below 30% under modest perturbations. Surprisingly, models are largely insensitive to language variations, with further experiments revealing that models tend to ignore language instructions completely. Our findings challenge the assumption that high benchmark scores equate to true competency and highlight the need for evaluation practices that assess reliability under realistic variation.",
    "summary": "",
    "translation": "LIBERO-Plus：视觉-语言-动作模型的深度鲁棒性分析",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视觉-语言-动作模型的鲁棒性分析，属于多模态模型的评估范畴。虽然视觉-语言模型（VLM）与异构数据建模有概念关联，但该论文聚焦于动作规划和机器人控制领域的鲁棒性测试，与推荐系统、搜索或广告的核心技术需求关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13624v1": {
    "title": "Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of German Tumor Diagnoses",
    "url": "https://www.alphaxiv.org/abs/2510.13624v1",
    "arxiv_id": "2510.13624v1",
    "authors": "Stefan Lenz, Lakisha Ortiz Rosario, Georg Vollmar, Arsenij Ustjanzew, Fatma Alickovic, Thomas Kindler, Torsten Panholzer",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-15 14:51:28",
    "ori_summary": "Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential for structured cancer documentation in Germany. Smaller open-weight LLMs are appealing for privacy-preserving automation but often struggle with coding accuracy in German-language contexts. This study investigates whether instruction-based fine-tuning on public datasets improves the coding accuracy of open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded diagnoses from the local tumor documentation system as test data. In a systematic data quality assessment, the upper limit for ICD-10 coding performance was estimated at 60-79% for exact and 81-94% for partial (three-character codes only) derivation. As training data, over 500,000 question-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS catalogues. Eight open-weight models from the Qwen, Llama, and Mistral families (7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to 41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3 topography coding also improved but started and remained considerably lower with an exact accuracy of 22-40% and a partial accuracy of 56-67% after fine-tuning. Malformed code outputs dropped to 0% for all models. Tumor-diagnosis recognition reached 99%. Accuracy correlated positively with model size, but gaps between small and large models narrowed after fine-tuning. The reasoning mode in Qwen3 generally yielded a lower performance than fine-tuning and was over 100 times slower. Our findings highlight the potential of leveraging public catalogues to build instruction datasets that improve LLMs in medical documentation tasks. The complete training dataset and the best-performing checkpoints of the fine-tuned models are available from https://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024.",
    "summary": "",
    "translation": "解锁公共目录：为德国肿瘤诊断ICD编码的指令微调大语言模型",
    "relevance_score": 2,
    "reasoning": "该论文专注于医疗领域的ICD编码任务，属于医学信息学应用，与推荐系统、搜索或广告的核心领域无关。虽然涉及LLM指令微调技术，但该技术本身是通用的，论文并未探讨其在RecSys/Search/Ads中的潜在应用，因此相关性很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13614v1": {
    "title": "MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.13614v1",
    "arxiv_id": "2510.13614v1",
    "authors": "Xingyu Tan, Xiaoyang Wang, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 14:43:31",
    "ori_summary": "Large Language Models (LLMs) have achieved impressive reasoning abilities, but struggle with temporal understanding, especially when questions involve multiple entities, compound operators, and evolving event sequences. Temporal Knowledge Graphs (TKGs), which capture vast amounts of temporal facts in a structured format, offer a reliable source for temporal reasoning. However, existing TKG-based LLM reasoning methods still struggle with four major challenges: maintaining temporal faithfulness in multi-hop reasoning, achieving multi-entity temporal synchronization, adapting retrieval to diverse temporal operators, and reusing prior reasoning experience for stability and efficiency. To address these issues, we propose MemoTime, a memory-augmented temporal knowledge graph framework that enhances LLM reasoning through structured grounding, recursive reasoning, and continual experience learning. MemoTime decomposes complex temporal questions into a hierarchical Tree of Time, enabling operator-aware reasoning that enforces monotonic timestamps and co-constrains multiple entities under unified temporal bounds. A dynamic evidence retrieval layer adaptively selects operator-specific retrieval strategies, while a self-evolving experience memory stores verified reasoning traces, toolkit decisions, and sub-question embeddings for cross-type reuse. Comprehensive experiments on multiple temporal QA benchmarks show that MemoTime achieves overall state-of-the-art results, outperforming the strong baseline by up to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo.",
    "summary": "",
    "translation": "MemoTime：记忆增强时序知识图谱增强的大型语言模型推理",
    "relevance_score": 4,
    "reasoning": "该论文主要关注知识图谱增强的LLM推理，属于LLM技术使能范畴。时序知识图谱在推荐系统中具有潜在应用价值，可用于建模用户行为序列和动态兴趣演化，但论文标题未明确指向搜索、推荐或广告领域的直接应用，因此相关性中等。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.13602v1": {
    "title": "NOSA: Native and Offloadable Sparse Attention",
    "url": "https://www.alphaxiv.org/abs/2510.13602v1",
    "arxiv_id": "2510.13602v1",
    "authors": "Yuxiang Huang, Chaojun Xiao, Xu Han, Zhiyuan Liu",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-15 14:33:16",
    "ori_summary": "Trainable sparse attention has emerged as a promising solution to address the decoding efficiency bottleneck of LLMs in long-context processing, significantly saving memory accesses while minimally impacting task performance. However, existing sparse attention methods leave a crucial limitation unresolved: the size of the key-value (KV) cache remains unreduced, which constrains on-GPU batch sizes and throttles decoding throughput, especially in large-scale batched inference. In this paper, we show that trainable sparse attention naturally exhibits strong locality in token selection across adjacent decoding steps, thereby enabling KV cache offloading without altering the underlying attention computation. However, the inherent locality remains insufficient to achieve efficient offloading, as the transfer of selected KV pairs between the CPU and GPU continues to dominate the overall decoding cost. Building on this insight, we present NOSA, a trainable sparse attention framework designed to natively support KV cache offloading. NOSA introduces explicit locality constraints by decomposing token selection into query-aware and query-agnostic components, thereby reducing KV transfers while preserving the same attention computation as used during training. We pretrain a 1B-parameter model with NOSA and conduct extensive benchmarks, showing that it preserves near-lossless performance while achieving up to a 2.3x improvement in decoding throughput compared with the vanilla trainable sparse attention baseline (InfLLM-V2).",
    "summary": "论文研究LLM长上下文处理中KV缓存导致的解码效率瓶颈问题，核心思想是通过分解令牌选择为查询感知和查询无关组件，在保持训练时注意力计算不变的前提下实现高效的KV缓存卸载。",
    "translation": "NOSA：原生与可卸载稀疏注意力",
    "relevance_score": 9,
    "reasoning": "该论文聚焦Transformer架构中的稀疏注意力机制优化，属于'Enabling Transformer Tech'范畴。稀疏注意力技术可显著提升长序列处理效率，在推荐系统和搜索场景中具有直接应用价值，能够高效处理用户长历史行为序列和长文档检索任务。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM推理效率瓶颈，提出了可训练稀疏注意力框架，通过KV缓存卸载技术显著提升解码吞吐量，与Transformer架构效率和LLM应用直接相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13598v1": {
    "title": "FreshTab: Sourcing Fresh Data for Table-to-Text Generation Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.13598v1",
    "arxiv_id": "2510.13598v1",
    "authors": "Kristýna Onderková, Ondřej Plátek, Zdeněk Kasner, Ondřej Dušek",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 14:31:44",
    "ori_summary": "Table-to-text generation (insight generation from tables) is a challenging task that requires precision in analyzing the data. In addition, the evaluation of existing benchmarks is affected by contamination of Large Language Model (LLM) training data as well as domain imbalance. We introduce FreshTab, an on-the-fly table-to-text benchmark generation from Wikipedia, to combat the LLM data contamination problem and enable domain-sensitive evaluation. While non-English table-to-text datasets are limited, FreshTab collects datasets in different languages on demand (we experiment with German, Russian and French in addition to English). We find that insights generated by LLMs from recent tables collected by our method appear clearly worse by automatic metrics, but this does not translate into LLM and human evaluations. Domain effects are visible in all evaluations, showing that a~domain-balanced benchmark is more challenging.",
    "summary": "",
    "translation": "FreshTab：为表格到文本生成评估获取新鲜数据",
    "relevance_score": 1,
    "reasoning": "该论文专注于表格到文本生成的评估数据获取，这属于纯粹的文本生成和评估领域，与推荐系统、搜索或广告的核心技术无关。论文标题表明其关注的是内容生成和评估基准，这些主题已被明确列为不相关内容，没有显示出在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13586v1": {
    "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs",
    "url": "https://www.alphaxiv.org/abs/2510.13586v1",
    "arxiv_id": "2510.13586v1",
    "authors": "Pasin Buakhaw, Kun Kerdthaisong, Phuree Phenhiran, Pitikorn Khlaisamniang, Supasate Vorathammathorn, Piyalitt Ittichaiwong, Nutchanon Yongsatianchot",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 14:17:23",
    "ori_summary": "The emergence of large language models (LLMs) has opened new opportunities for cre- ating dynamic non-player characters (NPCs) in gaming environments, enabling both func- tional task execution and persona-consistent dialogue generation. In this paper, we (Tu_Character_lab) report our participation in the Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which eval- uates agents across three tracks: task-oriented dialogue, context-aware dialogue, and their integration. Our approach combines two complementary strategies: (i) lightweight prompting techniques in the API track, including a Deflanderization prompting method to suppress excessive role-play and improve task fidelity, and (ii) fine-tuned large models in the GPU track, leveraging Qwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our best submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on Task 3 (GPU track).",
    "summary": "",
    "translation": "游戏对话去扁平化：在基于大语言模型的非玩家角色中平衡角色真实性与任务执行",
    "relevance_score": 2,
    "reasoning": "该论文主要关注游戏领域中NPC对话系统的具体应用，属于领域特定的对话生成问题。虽然涉及LLM技术，但其应用场景（游戏NPC）与推荐系统、搜索或广告领域没有直接关联，也不涉及核心推荐算法、检索技术或广告排序等关键技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13580v1": {
    "title": "Sparse Subnetwork Enhancement for Underrepresented Languages in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13580v1",
    "arxiv_id": "2510.13580v1",
    "authors": "Daniil Gurgurov, Josef van Genabith, Simon Ostermann",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 14:14:49",
    "ori_summary": "Large language models exhibit uneven performance across languages, with substantial gaps between high- and low-resource languages. We present a framework for enhancing monolingual capabilities of LLMs in underrepresented languages while preserving their general-purpose performance through targeted fine-tuning of language-specific subnetworks. Our approach identifies language-specific neurons using Language Activation Probability Entropy and fine-tunes only the weights associated with these neurons, a dedicated subnetwork, on target-language data. Experiments on Llama-3.1-8B and Mistral-Nemo-12B across 12 mid- and low-resource languages demonstrate that our method consistently outperforms full fine-tuning, FFN-only fine-tuning, LoRA adaptation, and random subset fine-tuning baselines while efficiently updating only up to 1% of model parameters. Beyond performance improvements, we observe enhanced favorable training dynamics, cross-lingual representational alignment, and systematic weight update changes. To facilitate future research, we release language-specific neuron identifications for over 100 languages as well as our adaptation pipeline, offering a cost-effective pathway for adapting state-of-the-art models to underrepresented languages.",
    "summary": "",
    "translation": "大型语言模型中低资源语言的稀疏子网络增强",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多语言能力增强和低资源语言处理，属于LLM能力扩展的范畴。虽然稀疏子网络技术本身可能对模型效率有改进，但论文聚焦于语言多样性而非推荐/搜索/广告的核心问题，潜在应用场景不明确且间接。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13554v1": {
    "title": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.13554v1",
    "arxiv_id": "2510.13554v1",
    "authors": "Yang Li, Zhichen Dong, Yuhan Sun, Weixun Wang, Shaopan Xiong, Yijia Luo, Jiashun Liu, Han Lu, Jiamang Wang, Wenbo Su, Bo Zheng, Junchi Yan",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-15 13:49:51",
    "ori_summary": "The reasoning pattern of Large language models (LLMs) remains opaque, and Reinforcement learning (RL) typically applies uniform credit across an entire generation, blurring the distinction between pivotal and routine steps. This work positions attention as a privileged substrate that renders the internal logic of LLMs legible, not merely as a byproduct of computation, but as a mechanistic blueprint of reasoning itself. We first distinguish attention heads between locally and globally focused information processing and reveal that locally focused heads produce a sawtooth pattern near the diagonal indicating phrasal chunks, while globally focused heads expose tokens that exert broad downstream influence over future tokens. We formalize these with two metrics: 1) Windowed Average Attention Distance, which measures the extent of backward attention within a clipped window; 2) Future Attention Influence, which quantifies a token's global importance as the average attention it receives from subsequent tokens. Taken together, these signals reveal a recurring preplan-and-anchor mechanism, where the model first performs a long-range contextual reference to generate an introductory token, which is immediately followed by or coincides with a semantic anchor token that organizes subsequent reasoning. Leveraging these insights, we introduce three novel RL strategies that dynamically perform targeted credit assignment to critical nodes (preplan tokens, anchor tokens, and their temporal coupling) and show consistent performance gains across various reasoning tasks. By aligning optimization with the model's intrinsic reasoning rhythm, we aim to transform opaque optimization into an actionable structure-aware process, hoping to offer a potential step toward more transparent and effective optimization of LLM reasoning.",
    "summary": "论文研究LLM推理过程中注意力机制的内在模式识别问题，核心思想是通过分析注意力头的局部与全局聚焦特征，发现预规划-锚点节奏机制，并基于此设计针对关键推理节点的强化学习信用分配策略。",
    "translation": "注意力机制照亮LLM推理：预规划与锚定节奏实现细粒度策略优化",
    "relevance_score": 8,
    "reasoning": "该论文研究注意力机制如何优化LLM推理过程，属于'使能LLM技术'范畴。在推荐系统和搜索中，这种细粒度的推理优化可提升复杂用户意图理解、多轮对话推荐的质量和效率，以及广告文案的精准生成。预规划与锚定机制可增强模型在复杂决策场景中的稳定性和准确性。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文通过注意力机制揭示LLM推理的内在节奏，并提出基于关键节点的细粒度RL优化策略，直接关联Transformer架构分析和LLM在推理任务中的应用。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13537v1": {
    "title": "K-Merge: Online Continual Merging of Adapters for On-device Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13537v1",
    "arxiv_id": "2510.13537v1",
    "authors": "Donald Shenaj, Ondrej Bohdal, Taha Ceritli, Mete Ozay, Pietro Zanuttigh, Umberto Michieli",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-15 13:32:25",
    "ori_summary": "On-device deployment of Large Language Models (LLMs) frequently leverages Low-Rank Adapters (LoRAs) to support diverse downstream tasks under tight resource constraints. To address the limited storage capacity of mobile devices, recent works have explored model merging techniques to fuse multiple LoRAs into a single one. In practice, however, LoRAs are often delivered incrementally, as users request support for new tasks (e.g., novel problem types or languages). This scenario introduces a new challenge: on-device online continual merging, where the objective is to incorporate new LoRAs while preserving the performance on previously supported tasks. In this paper, we propose a data-free and computationally efficient strategy for selecting and merging LoRAs when a new one becomes available, assuming the device can store only a limited number of adapters. Extensive experiments across real-world tasks demonstrate the superiority of our approach compared to alternative strategies while adhering to the storage budget and compute limitations of on-device settings.",
    "summary": "",
    "translation": "K-Merge：面向设备端大语言模型的适配器在线持续合并方法",
    "relevance_score": 7,
    "reasoning": "该论文属于'使能LLM技术'范畴，专注于设备端LLM的高效适配器管理。在推荐系统和搜索场景中，这种在线持续合并技术可以支持个性化模型在移动设备上的高效更新，实现用户偏好的实时适应，同时保持模型效率。这对于移动端推荐和搜索应用的个性化体验优化具有直接应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.13500v1": {
    "title": "MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts",
    "url": "https://www.alphaxiv.org/abs/2510.13500v1",
    "arxiv_id": "2510.13500v1",
    "authors": "Shujun Xia, Haokun Lin, Yichen Wu, Yinan Zhou, Zixuan Li, Zhongwei Wan, Xingrun Xing, Yefeng Zheng, Xiang Li, Caifeng Shan, Zhenan Sun, Quanzheng Li",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 12:50:33",
    "ori_summary": "LLMs hold great promise for healthcare applications, but the rapid evolution of medical knowledge and errors in training data often cause them to generate outdated or inaccurate information, limiting their applicability in high-stakes clinical practice. Model editing has emerged as a potential remedy without full retraining. While parameter-based editing often compromises locality and is thus ill-suited for the medical domain, retrieval-based editing offers a more viable alternative. However, it still faces two critical challenges: (1) representation overlap within the medical knowledge space often causes inaccurate retrieval and reduces editing accuracy; (2) existing methods are restricted to single-sample edits, while batch-editing remains largely unexplored despite its importance for real-world medical applications. To address these challenges, we first construct MedVersa, \\hk{an enhanced benchmark with broader coverage of medical subjects, designed to evaluate both single and batch edits under strict locality constraints}. We then propose MedREK, a retrieval-based editing framework that integrates a shared query-key module for precise matching with an attention-based prompt encoder for informative guidance. Experimental results on various medical benchmarks demonstrate that our MedREK achieves superior performance across different core metrics and provides the first validated solution for batch-editing in medical LLMs. Our code and dataset are available at https://github.com/mylittleriver/MedREK.",
    "summary": "",
    "translation": "MedREK：基于检索的医学大语言模型编辑方法及关键感知提示",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的LLM编辑技术，属于明确的医学领域应用，属于用户指定的不相关主题。论文标题明确提及医学应用，与RecSys、搜索或广告领域无直接关联，检索编辑技术也未显示出在这些领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13499v1": {
    "title": "ConsintBench: Evaluating Language Models on Real-World Consumer Intent Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.13499v1",
    "arxiv_id": "2510.13499v1",
    "authors": "Xiaozhe Li, TianYi Lyu, Siyi Yang, Yuxi Gong, Yizhao Yang, Jinxuan Huang, Ligao Zhang, Zhuoyi Huang, Qingwen Liu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 12:49:45",
    "ori_summary": "Understanding human intent is a complex, high-level task for large language models (LLMs), requiring analytical reasoning, contextual interpretation, dynamic information aggregation, and decision-making under uncertainty. Real-world public discussions, such as consumer product discussions, are rarely linear or involve a single user. Instead, they are characterized by interwoven and often conflicting perspectives, divergent concerns, goals, emotional tendencies, as well as implicit assumptions and background knowledge about usage scenarios. To accurately understand such explicit public intent, an LLM must go beyond parsing individual sentences; it must integrate multi-source signals, reason over inconsistencies, and adapt to evolving discourse, similar to how experts in fields like politics, economics, or finance approach complex, uncertain environments. Despite the importance of this capability, no large-scale benchmark currently exists for evaluating LLMs on real-world human intent understanding, primarily due to the challenges of collecting real-world public discussion data and constructing a robust evaluation pipeline. To bridge this gap, we introduce \\bench, the first dynamic, live evaluation benchmark specifically designed for intent understanding, particularly in the consumer domain. \\bench is the largest and most diverse benchmark of its kind, supporting real-time updates while preventing data contamination through an automated curation pipeline.",
    "summary": "",
    "translation": "ConsintBench：在真实世界消费者意图理解上评估语言模型",
    "relevance_score": 2,
    "reasoning": "虽然消费者意图理解与搜索和推荐系统相关，但该论文主要关注语言模型的评估基准，这属于纯粹的NLP评估范畴。根据指导原则，评估基准、幻觉和纯粹NLP中心主题被视为不相关主题，因此该论文与当前关注点相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13494v1": {
    "title": "LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA",
    "url": "https://www.alphaxiv.org/abs/2510.13494v1",
    "arxiv_id": "2510.13494v1",
    "authors": "Tommaso Bonomo, Luca Gioffré, Roberto Navigli",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 12:43:59",
    "ori_summary": "Question Answering (QA) on narrative text poses a unique challenge to current systems, requiring a deep understanding of long, complex documents. However, the reliability of NarrativeQA, the most widely used benchmark in this domain, is hindered by noisy documents and flawed QA pairs. In this work, we introduce LiteraryQA, a high-quality subset of NarrativeQA focused on literary works. Using a human- and LLM-validated pipeline, we identify and correct low-quality QA samples while removing extraneous text from source documents. We then carry out a meta-evaluation of automatic metrics to clarify how systems should be evaluated on LiteraryQA. This analysis reveals that all n-gram-based metrics have a low system-level correlation to human judgment, while LLM-as-a-Judge evaluations, even with small open-weight models, can strongly agree with the ranking identified by humans. Finally, we benchmark a set of long-context LLMs on LiteraryQA. We release our code and data at https://github.com/SapienzaNLP/LiteraryQA.",
    "summary": "",
    "translation": "LiteraryQA：面向长文档叙事问答的有效评估",
    "relevance_score": 2,
    "reasoning": "该论文专注于长文档问答的评估基准，属于纯粹的NLP评估主题，与推荐系统、搜索或广告的核心技术无关。虽然问答系统与搜索有一定关联，但该论文明确针对文学叙事领域的专业评估，缺乏在推荐、搜索或广告领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13434v1": {
    "title": "Beyond Single-Reward: Multi-Pair, Multi-Perspective Preference Optimization for Machine Translation",
    "url": "https://www.alphaxiv.org/abs/2510.13434v1",
    "arxiv_id": "2510.13434v1",
    "authors": "Hao Wang, Linlong Xu, Heng Liu, Yangyang Liu, Xiaohu Zhao, Bo Zeng, Liangying Shao, Longyue Wang, Weihua Luo, Kaifu Zhang",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 11:30:49",
    "ori_summary": "Direct Preference Optimization (DPO) is a powerful paradigm for aligning Large Language Models (LLMs) to human preferences in Machine Translation (MT), but current methods are hindered by two fundamental challenges: (1) flawed reward signals from Quality Estimation (QE) models that overlook critical errors like translation hallucination, and (2) inefficient data utilization that discards valuable learning signals by selecting only a single win-loss pair. To address these limitations, we introduce M^2PO: Multi-Pair, Multi-Perspective Preference Optimization. Our framework integrates a multi-perspective reward engine that creates a more robust signal by combining two key viewpoints: a new hallucination penalty for factuality, and an innovative dynamic quality score that adaptively fuses external evaluations with the model's own evolving judgment. This is synergistically paired with a multi-pair construction strategy that systematically creates a comprehensive set of preference pairs from the entire pool of translation candidates. This synergistic approach ensures the model learns from a richer spectrum of quality trade-offs, leading to more robust and faithful translations. On challenging WMT21-22 benchmarks, M^2PO substantially outperforms existing preference optimization methods and demonstrates highly competitive performance against leading proprietary LLMs.",
    "summary": "",
    "translation": "超越单一奖励：面向机器翻译的多配对、多视角偏好优化",
    "relevance_score": 2,
    "reasoning": "该论文专注于机器翻译领域的偏好优化方法，虽然涉及多奖励和多视角优化，但核心应用场景是机器翻译这一特定NLP任务。对于推荐系统、搜索或广告领域，该方法可能缺乏直接的适用性，因为翻译任务的优化目标和特征与推荐/搜索的排序优化有本质差异。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13430v1": {
    "title": "Evaluating Arabic Large Language Models: A Survey of Benchmarks, Methods, and Gaps",
    "url": "https://www.alphaxiv.org/abs/2510.13430v1",
    "arxiv_id": "2510.13430v1",
    "authors": "Ahmed Alzubaidi, Shaikha Alsuwaidi, Basma El Amel Boussaha, Leen AlQadi, Omar Alkaabi, Mohammed Alyafeai, Hamza Alobeidli, Hakim Hacid",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 11:25:33",
    "ori_summary": "This survey provides the first systematic review of Arabic LLM benchmarks, analyzing 40+ evaluation benchmarks across NLP tasks, knowledge domains, cultural understanding, and specialized capabilities. We propose a taxonomy organizing benchmarks into four categories: Knowledge, NLP Tasks, Culture and Dialects, and Target-Specific evaluations. Our analysis reveals significant progress in benchmark diversity while identifying critical gaps: limited temporal evaluation, insufficient multi-turn dialogue assessment, and cultural misalignment in translated datasets. We examine three primary approaches: native collection, translation, and synthetic generation discussing their trade-offs regarding authenticity, scale, and cost. This work serves as a comprehensive reference for Arabic NLP researchers, providing insights into benchmark methodologies, reproducibility standards, and evaluation metrics while offering recommendations for future development.",
    "summary": "",
    "translation": "阿拉伯语大语言模型评估：基准、方法与差距综述",
    "relevance_score": 1,
    "reasoning": "该论文专注于阿拉伯语LLM的评估基准和方法，属于纯粹的评估基准和NLP中心主题，与我的关注点无关。论文没有涉及推荐系统、搜索或广告的核心进展，也没有讨论可能应用于这些领域的使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13417v1": {
    "title": "Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse",
    "url": "https://www.alphaxiv.org/abs/2510.13417v1",
    "arxiv_id": "2510.13417v1",
    "authors": "Liesbeth Allein, Nataly Pineda-Castañeda, Andrea Rocci, Marie-Francine Moens",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-15 11:15:00",
    "ori_summary": "How does a cause lead to an effect, and which intermediate causal steps explain their connection? This work scrutinizes the mechanistic causal reasoning capabilities of large language models (LLMs) to answer these questions through the task of implicit causal chain discovery. In a diagnostic evaluation framework, we instruct nine LLMs to generate all possible intermediate causal steps linking given cause-effect pairs in causal chain structures. These pairs are drawn from recent resources in argumentation studies featuring polarized discussion on climate change. Our analysis reveals that LLMs vary in the number and granularity of causal steps they produce. Although they are generally self-consistent and confident about the intermediate causal connections in the generated chains, their judgments are mainly driven by associative pattern matching rather than genuine causal reasoning. Nonetheless, human evaluations confirmed the logical coherence and integrity of the generated chains. Our baseline causal chain discovery approach, insights from our diagnostic evaluation, and benchmark dataset with causal chains lay a solid foundation for advancing future work in implicit, mechanistic causal reasoning in argumentation settings.",
    "summary": "",
    "translation": "通过气候论述中的隐式因果链发现评估大语言模型推理能力",
    "relevance_score": 1,
    "reasoning": "该论文专注于评估LLM在特定领域（气候论述）中的推理能力，这属于纯粹的NLP评估基准研究。虽然涉及LLM推理，但论文关注的是气候领域的因果链发现，没有展示在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13407v1": {
    "title": "Investigating Lexical Change through Cross-Linguistic Colexification Patterns",
    "url": "https://www.alphaxiv.org/abs/2510.13407v1",
    "arxiv_id": "2510.13407v1",
    "authors": "Kim Gfeller, Sabine Stoll, Chundra Cathcart, Paul Widmer",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 11:04:28",
    "ori_summary": "One of the most intriguing features of language is its constant change, with ongoing shifts in how meaning is expressed. Despite decades of research, the factors that determine how and why meanings evolve remain only partly understood. Colexification -- the phenomenon of expressing multiple distinct concepts using the same word form -- serves as a valuable window onto the dynamics of meaning change across languages. Here, we apply phylogenetic comparative models to dictionary data from three language families, Austronesian, Indo-European, and Uralic, in order to shed light on the evolutionary dynamics underlying the colexification of concept pairs. We assess the effects of three predictors: associativity, borrowability, and usage frequency. Our results show that more closely related concept pairs are colexified across a larger portion of the family tree and exhibit slower rates of change. In contrast, concept pairs that are more frequent and more prone to borrowing tend to change more rapidly and are less often colexified. We also find considerable differences between the language families under study, suggesting that areal and cultural factors may play a role.",
    "summary": "",
    "translation": "通过跨语言共词化模式研究词汇演变",
    "relevance_score": 2,
    "reasoning": "该论文主要研究语言学中的词汇演变和共词化模式，属于纯粹的语言学领域。虽然涉及语言模式分析，但缺乏与推荐系统、搜索或广告领域的直接关联，也没有展示出在Transformer架构、LLM技术或异构数据处理方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13395v1": {
    "title": "Doing Things with Words: Rethinking Theory of Mind Simulation in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13395v1",
    "arxiv_id": "2510.13395v1",
    "authors": "Agnese Lombardi, Alessandro Lenci",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 10:48:31",
    "ori_summary": "Language is fundamental to human cooperation, facilitating not only the exchange of information but also the coordination of actions through shared interpretations of situational contexts. This study explores whether the Generative Agent-Based Model (GABM) Concordia can effectively model Theory of Mind (ToM) within simulated real-world environments. Specifically, we assess whether this framework successfully simulates ToM abilities and whether GPT-4 can perform tasks by making genuine inferences from social context, rather than relying on linguistic memorization. Our findings reveal a critical limitation: GPT-4 frequently fails to select actions based on belief attribution, suggesting that apparent ToM-like abilities observed in previous studies may stem from shallow statistical associations rather than true reasoning. Additionally, the model struggles to generate coherent causal effects from agent actions, exposing difficulties in processing complex social interactions. These results challenge current statements about emergent ToM-like capabilities in LLMs and highlight the need for more rigorous, action-based evaluation frameworks.",
    "summary": "",
    "translation": "用语言做事：重新思考大型语言模型中的心智理论模拟",
    "relevance_score": 2,
    "reasoning": "这篇论文主要关注LLM中的心智理论（Theory of Mind）模拟，这是一个纯粹的认知科学和心理学导向的NLP研究主题。虽然心智理论可能对理解用户意图有间接帮助，但该论文没有展示明确的推荐系统、搜索或广告应用潜力，且更偏向理论心理学而非实际系统改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13387v1": {
    "title": "Make an Offer They Can't Refuse: Grounding Bayesian Persuasion in Real-World Dialogues without Pre-Commitment",
    "url": "https://www.alphaxiv.org/abs/2510.13387v1",
    "arxiv_id": "2510.13387v1",
    "authors": "Buwei He, Yang Liu, Zhaowei Zhang, Zixia Jia, Huijia Wu, Zhaofeng He, Zilong Zheng, Yipeng Kang",
    "categories": "cs.CL, cs.GT",
    "pub_date": "2025-10-15 10:26:02",
    "ori_summary": "Persuasion, a fundamental social capability for humans, remains a challenge for AI systems such as large language models (LLMs). Current studies often overlook the strategic use of information asymmetry in message design or rely on strong assumptions regarding pre-commitment. In this work, we explore the application of Bayesian Persuasion (BP) in natural language within single-turn dialogue settings, to enhance the strategic persuasion capabilities of LLMs. Our framework incorporates a commitment-communication mechanism, where the persuader explicitly outlines an information schema by narrating their potential types (e.g., honest or dishonest), thereby guiding the persuadee in performing the intended Bayesian belief update. We evaluate two variants of our approach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language (FNL) BP, benchmarking them against both naive and strong non-BP (NBP) baselines within a comprehensive evaluation framework. This framework covers a diverse set of persuadees -- including LLM instances with varying prompts and fine-tuning and human participants -- across tasks ranging from specially designed persuasion scenarios to general everyday situations. Experimental results on LLM-based agents reveal three main findings: (1) LLMs guided by BP strategies consistently achieve higher persuasion success rates than NBP baselines; (2) SFNL exhibits greater credibility and logical coherence, while FNL shows stronger emotional resonance and robustness in naturalistic conversations; (3) with supervised fine-tuning, smaller models can attain BP performance comparable to that of larger models.",
    "summary": "",
    "translation": "提出无法拒绝的报价：在无需预先承诺的真实世界对话中实现贝叶斯劝说",
    "relevance_score": 2,
    "reasoning": "该论文主要研究贝叶斯劝说理论在对话系统中的应用，属于对话AI和博弈论领域。虽然对话系统可能与搜索推荐有一定关联，但论文聚焦于劝说机制而非核心的推荐排序、检索或广告技术，与当前关注的核心领域进展和LLM技术应用相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13366v1": {
    "title": "Document Intelligence in the Era of Large Language Models: A Survey",
    "url": "https://www.alphaxiv.org/abs/2510.13366v1",
    "arxiv_id": "2510.13366v1",
    "authors": "Weishi Wang, Hengchang Hu, Zhijie Zhang, Zhaochen Li, Hongxin Shao, Daniel Dahlmeier",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 09:57:03",
    "ori_summary": "Document AI (DAI) has emerged as a vital application area, and is significantly transformed by the advent of large language models (LLMs). While earlier approaches relied on encoder-decoder architectures, decoder-only LLMs have revolutionized DAI, bringing remarkable advancements in understanding and generation. This survey provides a comprehensive overview of DAI's evolution, highlighting current research attempts and future prospects of LLMs in this field. We explore key advancements and challenges in multimodal, multilingual, and retrieval-augmented DAI, while also suggesting future research directions, including agent-based approaches and document-specific foundation models. This paper aims to provide a structured analysis of the state-of-the-art in DAI and its implications for both academic and practical applications.",
    "summary": "",
    "translation": "大语言模型时代的文档智能：综述",
    "relevance_score": 6,
    "reasoning": "该综述涵盖文档智能技术，在搜索系统中具有直接应用价值，如文档理解、信息提取和检索增强。虽然不专门针对推荐或广告，但文档智能技术可以增强搜索系统的内容理解和用户查询处理能力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.13363v1": {
    "title": "D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured Memory And Reasoning Tree",
    "url": "https://www.alphaxiv.org/abs/2510.13363v1",
    "arxiv_id": "2510.13363v1",
    "authors": "Xiang Lei, Qin Li, Min Zhang, Min Zhang",
    "categories": "cs.CL, 68T50, 68T30, I.2.7; I.2.4",
    "pub_date": "2025-10-15 09:53:11",
    "ori_summary": "Large Language Models (LLMs) often exhibit factual inconsistencies and logical decay in extended, multi-turn dialogues, a challenge stemming from their reliance on static, pre-trained knowledge and an inability to reason adaptively over the dialogue history. Prevailing mitigation strategies, such as Retrieval-Augmented Generation (RAG) and agentic working memories, improve information recall but still engage with fundamentally static knowledge sources and follow pre-defined single reasoning path. This hinders their ability to preserve factual and logical consistency of their responses in multi-turn dialogues while the context evolves over time. To address this issue, we propose D-SMART, a model-agnostic framework designed to maintain multi-turn dialogue consistency by enabling LLMs to build and reason over a dynamic, structured representation of the conversational context. This is achieved via two synergistic components: (1) a Dynamic Structured Memory (DSM), which incrementally constructs and maintains an authoritative, OWL-compliant knowledge graph of the conversation; and (2) a Reasoning Tree (RT), which executes inferences as an explicit and traceable multi-step search over the graph. As the popular-used quality score (judged by GPT-4) can overlook logical flaws, we introduce new NLI-based metrics to better measure multi-turn dialogue consistency. Comprehensive experiments on the MT-Bench-101 benchmark show that D-SMART significantly outperforms state-of-the-art baselines, elevating the dialogue consistency score by over 48\\% for both proprietary and open-source models, and notably improves the quality score of the latter by up to 10.1\\%.",
    "summary": "",
    "translation": "D-SMART：通过动态结构化记忆与推理树增强大语言模型对话一致性",
    "relevance_score": 3,
    "reasoning": "该论文主要关注LLM对话一致性改进，属于对话系统的技术优化。虽然涉及记忆和推理机制，但核心应用场景是对话系统而非推荐/搜索/广告领域。动态结构化记忆技术理论上可能应用于用户行为序列建模，但论文标题未显示明确的推荐/搜索/广告应用连接。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13357v1": {
    "title": "Personal Attribute Leakage in Federated Speech Models",
    "url": "https://www.alphaxiv.org/abs/2510.13357v1",
    "arxiv_id": "2510.13357v1",
    "authors": "Hamdan Al-Ali, Ali Reza Ghavamipour, Tommaso Caselli, Fatih Turkmen, Zeerak Talat, Hanan Aldarmaki",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 09:43:10",
    "ori_summary": "Federated learning is a common method for privacy-preserving training of machine learning models. In this paper, we analyze the vulnerability of ASR models to attribute inference attacks in the federated setting. We test a non-parametric white-box attack method under a passive threat model on three ASR models: Wav2Vec2, HuBERT, and Whisper. The attack operates solely on weight differentials without access to raw speech from target speakers. We demonstrate attack feasibility on sensitive demographic and clinical attributes: gender, age, accent, emotion, and dysarthria. Our findings indicate that attributes that are underrepresented or absent in the pre-training data are more vulnerable to such inference attacks. In particular, information about accents can be reliably inferred from all models. Our findings expose previously undocumented vulnerabilities in federated ASR models and offer insights towards improved security.",
    "summary": "",
    "translation": "联邦语音模型中的个人属性泄露",
    "relevance_score": 1,
    "reasoning": "该论文涉及联邦学习和隐私泄露问题，这属于明确排除的无关主题。虽然标题提到语音模型，但核心关注点是隐私和安全方面，与推荐系统、搜索或广告的核心技术进展没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13351v1": {
    "title": "Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise LLM Systems",
    "url": "https://www.alphaxiv.org/abs/2510.13351v1",
    "arxiv_id": "2510.13351v1",
    "authors": "Karthik Avinash, Nikhil Pareek, Rishav Hada",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 09:40:24",
    "ori_summary": "The increasing deployment of Large Language Models (LLMs) across enterprise and mission-critical domains has underscored the urgent need for robust guardrailing systems that ensure safety, reliability, and compliance. Existing solutions often struggle with real-time oversight, multi-modal data handling, and explainability -- limitations that hinder their adoption in regulated environments. Existing guardrails largely operate in isolation, focused on text alone making them inadequate for multi-modal, production-scale environments. We introduce Protect, natively multi-modal guardrailing model designed to operate seamlessly across text, image, and audio inputs, designed for enterprise-grade deployment. Protect integrates fine-tuned, category-specific adapters trained via Low-Rank Adaptation (LoRA) on an extensive, multi-modal dataset covering four safety dimensions: toxicity, sexism, data privacy, and prompt injection. Our teacher-assisted annotation pipeline leverages reasoning and explanation traces to generate high-fidelity, context-aware labels across modalities. Experimental results demonstrate state-of-the-art performance across all safety dimensions, surpassing existing open and proprietary models such as WildGuard, LlamaGuard-4, and GPT-4.1. Protect establishes a strong foundation for trustworthy, auditable, and production-ready safety systems capable of operating across text, image, and audio modalities.",
    "summary": "",
    "translation": "Protect：面向可信企业级大语言模型系统的鲁棒护栏技术栈",
    "relevance_score": 2,
    "reasoning": "该论文主要关注企业级LLM系统的护栏技术和可信性保障，这属于安全和可靠性范畴，而非核心推荐系统、搜索或广告的技术进展。虽然提到了LLM系统，但其焦点是防护机制而非LLM在推荐/搜索/广告中的直接应用或架构创新，因此与当前关注点相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13344v1": {
    "title": "UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE",
    "url": "https://www.alphaxiv.org/abs/2510.13344v1",
    "arxiv_id": "2510.13344v1",
    "authors": "Zhenyu Liu, Yunxin Li, Xuanyu Zhang, Qixun Teng, Shenyuan Jiang, Xinyu Chen, Haoyuan Shi, Jinchao Li, Qi Wang, Haolan Chen, Fanbo Meng, Mingjun Zhao, Yu Xu, Yancheng He, Baotian Hu, Min Zhang",
    "categories": "cs.SD, cs.CL",
    "pub_date": "2025-10-15 09:30:25",
    "ori_summary": "Recent advances in unified multimodal models indicate a clear trend towards comprehensive content generation. However, the auditory domain remains a significant challenge, with music and speech often developed in isolation, hindering progress towards universal audio synthesis. This separation stems from inherent task conflicts and severe data imbalances, which impede the development of a truly unified audio generation model. To address this challenge, we propose UniMoE-Audio, a unified speech and music generation model within a novel Dynamic-Capacity Mixture-of-Experts (MoE) framework. Architecturally, UniMoE-Audio introduces a Top-P routing strategy for dynamic expert number allocation, and a hybrid expert design comprising routed experts for domain-specific knowledge, shared experts for domain-agnostic features, and null experts for adaptive computation skipping. To tackle data imbalance, we introduce a three-stage training curriculum: 1) Independent Specialist Training leverages original datasets to instill domain-specific knowledge into each \"proto-expert\" without interference; 2) MoE Integration and Warmup incorporates these specialists into the UniMoE-Audio architecture, warming up the gate module and shared expert using a subset of balanced dataset; and 3) Synergistic Joint Training trains the entire model end-to-end on the fully balanced dataset, fostering enhanced cross-domain synergy. Extensive experiments show that UniMoE-Audio not only achieves state-of-the-art performance on major speech and music generation benchmarks, but also demonstrates superior synergistic learning, mitigating the performance degradation typically seen in naive joint training. Our findings highlight the substantial potential of specialized MoE architecture and curated training strategies in advancing the field of universal audio generation. Homepage: https://mukioxun.github.io/Uni-MoE-site/home.html",
    "summary": "",
    "translation": "UniMoE-Audio：基于动态容量专家混合模型的统一语音与音乐生成",
    "relevance_score": 3,
    "reasoning": "该论文主要关注语音和音乐生成的特定领域应用，属于音频生成而非推荐系统、搜索或广告的核心技术。虽然MoE架构本身是Transformer的效率改进技术，但论文专注于音频生成这一与RecSys/Search/Ads无关的应用场景，缺乏明确的跨领域应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13341v1": {
    "title": "Are Proverbs the New Pythian Oracles? Exploring Sentiment in Greek Sayings",
    "url": "https://www.alphaxiv.org/abs/2510.13341v1",
    "arxiv_id": "2510.13341v1",
    "authors": "Katerina Korre, John Pavlopoulos",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 09:26:52",
    "ori_summary": "Proverbs are among the most fascinating linguistic phenomena that transcend cultural and linguistic boundaries. Yet, much of the global landscape of proverbs remains underexplored, as many cultures preserve their traditional wisdom within their own communities due to the oral tradition of the phenomenon. Taking advantage of the current advances in Natural Language Processing (NLP), we focus on Greek proverbs, analyzing their sentiment. Departing from an annotated dataset of Greek proverbs, we expand it to include local dialects, effectively mapping the annotated sentiment. We present (1) a way to exploit LLMs in order to perform sentiment classification of proverbs, (2) a map of Greece that provides an overview of the distribution of sentiment, (3) a combinatory analysis in terms of the geographic position, dialect, and topic of proverbs. Our findings show that LLMs can provide us with an accurate enough picture of the sentiment of proverbs, especially when approached as a non-conventional sentiment polarity task. Moreover, in most areas of Greece negative sentiment is more prevalent.",
    "summary": "",
    "translation": "谚语是新的皮媞亚神谕吗？探索希腊谚语中的情感",
    "relevance_score": 1,
    "reasoning": "该论文专注于希腊谚语的情感分析，属于语言学和文化研究领域，与推荐系统、搜索或广告的核心技术进展完全无关。论文内容不涉及任何LLM技术、Transformer架构改进或异构数据建模，也没有任何潜在的应用于RecSys/Search/Ads的技术路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13334v1": {
    "title": "Taming the Fragility of KV Cache Eviction in LLM Inference",
    "url": "https://www.alphaxiv.org/abs/2510.13334v1",
    "arxiv_id": "2510.13334v1",
    "authors": "Yuan Feng, Haoyu Guo, JunLin Lv, S. Kevin Zhou, Xike Xie",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 09:18:58",
    "ori_summary": "Large language models have revolutionized natural language processing, yet their deployment remains hampered by the substantial memory and runtime overhead of the transformer's Key-Value cache. To mitigate this, recent methods employ a scoring-aggregation framework to evict unimportant cache entries, based on the stability assumption-that a fixed subset of entries remains consistently important during generation. However, prior work has largely focused on refining importance indicators for scoring, while defaulting to mean aggregation due to a faithful trust in the stability assumption. In this work, we argue that this underlying assumption is inherently fragile, making mean aggregation highly vulnerable in extreme cases. To counter this, we propose a simple yet elegant defensive aggregation strategy: a two-step, linear-time approach that controls worst-case risk, thereby defending against extreme cases with negligible computational overhead. Embodying this strategy, we propose a novel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV, which incorporates layer-wise budget allocation. Across seven task domains (18 datasets), our methods reduce generation quality loss by 2.3x and 4.3x respectively, versus the strongest baseline under a 20% cache size. These results set new performance benchmarks and pioneer a promising direction for optimizing cache eviction against underlying fragility through worst-case risk management. Our code is available at https://github.com/FFY0/DefensiveKV.",
    "summary": "论文研究LLM推理中KV缓存驱逐的脆弱性问题，核心思想是通过两阶段线性时间防御性聚合策略控制最坏情况风险，提升缓存管理鲁棒性。",
    "translation": "驯服大语言模型推理中键值缓存驱逐的脆弱性",
    "relevance_score": 8,
    "reasoning": "该论文关注LLM推理中的KV缓存优化，属于'Enabling LLM Tech'范畴。KV缓存效率直接影响LLM在推荐和搜索系统中的实时推理性能，更高效的缓存管理可以显著降低延迟和计算成本，这对于需要快速响应的工业级推荐和搜索应用至关重要。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接解决LLM推理中的KV缓存效率问题，这是Transformer架构优化的核心挑战，对搜索和推荐系统的实时部署至关重要。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13329v1": {
    "title": "Embedding-Based Context-Aware Reranker",
    "url": "https://www.alphaxiv.org/abs/2510.13329v1",
    "arxiv_id": "2510.13329v1",
    "authors": "Ye Yuan, Mohammad Amin Shabani, Siqi Liu",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 09:14:04",
    "ori_summary": "Retrieval-Augmented Generation (RAG) systems rely on retrieving relevant evidence from a corpus to support downstream generation. The common practice of splitting a long document into multiple shorter passages enables finer-grained and targeted information retrieval. However, it also introduces challenges when a correct retrieval would require inference across passages, such as resolving coreference, disambiguating entities, and aggregating evidence scattered across multiple sources. Many state-of-the-art (SOTA) reranking methods, despite utilizing powerful large pretrained language models with potentially high inference costs, still neglect the aforementioned challenges. Therefore, we propose Embedding-Based Context-Aware Reranker (EBCAR), a lightweight reranking framework operating directly on embeddings of retrieved passages with enhanced cross-passage understandings through the structural information of the passages and a hybrid attention mechanism, which captures both high-level interactions across documents and low-level relationships within each document. We evaluate EBCAR against SOTA rerankers on the ConTEB benchmark, demonstrating its effectiveness for information retrieval requiring cross-passage inference and its advantages in both accuracy and efficiency.",
    "summary": "论文研究检索增强生成系统中跨段落推理的挑战，核心方法是基于嵌入的轻量级重排序框架，通过结构信息和混合注意力机制增强跨段落理解。",
    "translation": "基于嵌入的情境感知重排序器",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及推荐系统和搜索中的核心排序技术，嵌入方法和情境感知都是这些领域的关键组件。基于嵌入的重排序器可以应用于搜索结果的精排阶段或推荐系统的重排序模块，通过考虑用户上下文信息来提升排序质量。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文针对检索系统中跨段落推理的核心挑战，提出轻量级重排序框架，直接应用于搜索和推荐系统的核心环节。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13302v1": {
    "title": "LLM one-shot style transfer for Authorship Attribution and Verification",
    "url": "https://www.alphaxiv.org/abs/2510.13302v1",
    "arxiv_id": "2510.13302v1",
    "authors": "Pablo Miralles-González, Javier Huertas-Tato, Alejandro Martín, David Camacho",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 08:43:24",
    "ori_summary": "Computational stylometry analyzes writing style through quantitative patterns in text, supporting applications from forensic tasks such as identity linking and plagiarism detection to literary attribution in the humanities. Supervised and contrastive approaches rely on data with spurious correlations and often confuse style with topic. Despite their natural use in AI-generated text detection, the CLM pre-training of modern LLMs has been scarcely leveraged for general authorship problems. We propose a novel unsupervised approach based on this extensive pre-training and the in-context learning capabilities of LLMs, employing the log-probabilities of an LLM to measure style transferability from one text to another. Our method significantly outperforms LLM prompting approaches of comparable scale and achieves higher accuracy than contrastively trained baselines when controlling for topical correlations. Moreover, performance scales fairly consistently with the size of the base model and, in the case of authorship verification, with an additional mechanism that increases test-time computation; enabling flexible trade-offs between computational cost and accuracy.",
    "summary": "",
    "translation": "用于作者归属与验证的LLM单样本风格迁移",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM在文本风格迁移和作者识别方面的应用，这属于纯粹的NLP任务。虽然涉及LLM技术，但作者归属和验证与推荐系统、搜索或广告的核心领域没有直接关联，也没有明显的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13293v1": {
    "title": "Mismatch Aware Guidance for Robust Emotion Control in Auto-Regressive TTS Models",
    "url": "https://www.alphaxiv.org/abs/2510.13293v1",
    "arxiv_id": "2510.13293v1",
    "authors": "Yizhou Peng, Yukun Ma, Chong Zhang, Yi-Wen Chao, Chongjia Ni, Bin Ma",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 08:37:16",
    "ori_summary": "While Text-to-Speech (TTS) systems can achieve fine-grained control over emotional expression via natural language prompts, a significant challenge emerges when the desired emotion (style prompt) conflicts with the semantic content of the text. This mismatch often results in unnatural-sounding speech, undermining the goal of achieving fine-grained emotional control. Classifier-Free Guidance (CFG) is a key technique for enhancing prompt alignment; however, its application to auto-regressive (AR) TTS models remains underexplored, which can lead to degraded audio quality. This paper directly addresses the challenge of style-content mismatch in AR TTS models by proposing an adaptive CFG scheme that adjusts to different levels of the detected mismatch, as measured using large language models or natural language inference models. This solution is based on a comprehensive analysis of CFG's impact on emotional expressiveness in state-of-the-art AR TTS models. Our results demonstrate that the proposed adaptive CFG scheme improves the emotional expressiveness of the AR TTS model while maintaining audio quality and intelligibility.",
    "summary": "",
    "translation": "面向自回归TTS模型的鲁棒情感控制的不匹配感知引导",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本转语音（TTS）模型中的情感控制，属于语音生成领域，与推荐系统、搜索或广告的核心技术无直接关联。尽管涉及自回归模型，但其应用场景（TTS情感控制）与RecSys/Search/Ads的排名、检索或用户行为建模需求不匹配，且未提及任何潜在的跨领域应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13291v1": {
    "title": "Higher Satisfaction, Lower Cost: A Technical Report on How LLMs Revolutionize Meituan's Intelligent Interaction Systems",
    "url": "https://www.alphaxiv.org/abs/2510.13291v1",
    "arxiv_id": "2510.13291v1",
    "authors": "Xuxin Cheng, Ke Zeng, Zhiquan Cao, Linyi Dai, Wenxuan Gao, Fei Han, Ai Jian, Feng Hong, Wenxing Hu, Zihe Huang, Dejian Kong, Jia Leng, Zhuoyuan Liao, Pei Liu, Jiaye Lin, Xing Ma, Jingqing Ruan, Jiaxing Song, Xiaoyu Tan, Ruixuan Xiao, Wenhui Yu, Wenyu Zhan, Haoxing Zhang, Chao Zhou, Hao Zhou, Shaodong Zheng, Ruinian Chen, Siyuan Chen, Ziyang Chen, Yiwen Dong, Yaoyou Fan, Yangyi Fang, Yang Gan, Shiguang Guo, Qi He, Chaowen Hu, Binghui Li, Dailin Li, Xiangyu Li, Yan Li, Chengjian Liu, Xiangfeng Liu, Jiahui Lv, Qiao Ma, Jiang Pan, Cong Qin, Chenxing Sun, Wen Sun, Zhonghui Wang, Abudukelimu Wuerkaixi, Xin Yang, Fangyi Yuan, Yawen Zhu, Tianyi Zhai, Jie Zhang, Runlai Zhang, Yao Xu, Yiran Zhao, Yifan Wang, Xunliang Cai, Yangen Hu, Cao Liu, Lu Pan, Xiaoli Wang, Bo Xiao, Wenyuan Yao, Qianlin Zhou, Benchang Zhu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 08:35:51",
    "ori_summary": "Enhancing customer experience is essential for business success, particularly as service demands grow in scale and complexity. Generative artificial intelligence and Large Language Models (LLMs) have empowered intelligent interaction systems to deliver efficient, personalized, and 24/7 support. In practice, intelligent interaction systems encounter several challenges: (1) Constructing high-quality data for cold-start training is difficult, hindering self-evolution and raising labor costs. (2) Multi-turn dialogue performance remains suboptimal due to inadequate intent understanding, rule compliance, and solution extraction. (3) Frequent evolution of business rules affects system operability and transferability, constraining low-cost expansion and adaptability. (4) Reliance on a single LLM is insufficient in complex scenarios, where the absence of multi-agent frameworks and effective collaboration undermines process completeness and service quality. (5) The open-domain nature of multi-turn dialogues, lacking unified golden answers, hampers quantitative evaluation and continuous optimization. To address these challenges, we introduce WOWService, an intelligent interaction system tailored for industrial applications. With the integration of LLMs and multi-agent architectures, WOWService enables autonomous task management and collaborative problem-solving. Specifically, WOWService focuses on core modules including data construction, general capability enhancement, business scenario adaptation, multi-agent coordination, and automated evaluation. Currently, WOWService is deployed on the Meituan App, achieving significant gains in key metrics, e.g., User Satisfaction Metric 1 (USM 1) -27.53% and User Satisfaction Metric 2 (USM 2) +25.51%, demonstrating its effectiveness in capturing user needs and advancing personalized service.",
    "summary": "论文研究如何解决智能交互系统在工业应用中的核心挑战，核心方法是集成LLM和多智能体架构构建WOWService系统，通过数据构建、能力增强、场景适配、智能体协作和自动化评估等模块实现自主任务管理和协同问题解决。",
    "translation": "更高满意度，更低成本：关于大语言模型如何革新美团智能交互系统的技术报告",
    "relevance_score": 9,
    "reasoning": "该论文直接探讨LLM在美团智能交互系统中的应用，属于'直接LLM应用'范畴，涉及推荐系统/搜索领域的实际部署。美团作为大型平台，其智能交互系统必然包含推荐、搜索等核心业务，论文内容应展示LLM如何优化这些系统的用户体验和成本效益。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用LLM技术构建智能交互系统，解决推荐/搜索领域的关键挑战，包括冷启动、多轮对话、业务规则演进和多智能体协作，与关注领域高度契合。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13285v1": {
    "title": "In-Distribution Steering: Balancing Control and Coherence in Language Model Generation",
    "url": "https://www.alphaxiv.org/abs/2510.13285v1",
    "arxiv_id": "2510.13285v1",
    "authors": "Arthur Vogels, Benjamin Wong, Yann Choho, Annabelle Blangero, Milan Bhan",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 08:31:37",
    "ori_summary": "Activation steering methods control large language model (LLM) behavior by modifying internal activations at inference time. However, most existing activation steering methods rely on a fixed steering strength, leading to either insufficient control or unadapted intervention that degrades text plausibility and coherence. We introduce In-Distribution Steering (IDS), a novel method that adapts steering strength based on the input data distribution in representation space. IDS dynamically adjusts interventions according to how far a given input lies within the distribution, enabling adaptive intervention and generation stability during text generation. Experiments demonstrate that IDS achieves strong accuracy on classification tasks while producing coherent text without collapse, making IDS particularly well suited for real-world applications.",
    "summary": "",
    "translation": "分布内引导：在语言模型生成中平衡控制与连贯性",
    "relevance_score": 6,
    "reasoning": "该论文属于'使能LLM技术'范畴，专注于语言模型生成控制技术。在推荐系统、搜索和广告领域，这种控制技术可以应用于生成更符合业务目标的推荐理由、搜索摘要或广告文案，同时保持内容的连贯性和自然性，对提升用户体验和商业效果具有直接应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.13281v1": {
    "title": "Two Heads Are Better Than One: Audio-Visual Speech Error Correction with Dual Hypotheses",
    "url": "https://www.alphaxiv.org/abs/2510.13281v1",
    "arxiv_id": "2510.13281v1",
    "authors": "Sungnyun Kim, Kangwook Jang, Sungwoo Cho, Joon Son Chung, Hoirin Kim, Se-Young Yun",
    "categories": "eess.AS, cs.CL, cs.LG",
    "pub_date": "2025-10-15 08:27:16",
    "ori_summary": "This paper introduces a new paradigm for generative error correction (GER) framework in audio-visual speech recognition (AVSR) that reasons over modality-specific evidences directly in the language space. Our framework, DualHyp, empowers a large language model (LLM) to compose independent N-best hypotheses from separate automatic speech recognition (ASR) and visual speech recognition (VSR) models. To maximize the effectiveness of DualHyp, we further introduce RelPrompt, a noise-aware guidance mechanism that provides modality-grounded prompts to the LLM. RelPrompt offers the temporal reliability of each modality stream, guiding the model to dynamically switch its focus between ASR and VSR hypotheses for an accurate correction. Under various corruption scenarios, our framework attains up to 57.7% error rate gain on the LRS2 benchmark over standard ASR baseline, contrary to single-stream GER approaches that achieve only 10% gain. To facilitate research within our DualHyp framework, we release the code and the dataset comprising ASR and VSR hypotheses at https://github.com/sungnyun/dualhyp.",
    "summary": "",
    "translation": "双头胜于单头：基于双重假设的视听语音错误纠正",
    "relevance_score": 1,
    "reasoning": "该论文专注于音频-视觉语音错误纠正，属于多模态语音处理领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然涉及多模态融合，但其应用场景（语音错误纠正）和核心技术（视听语音处理）与我的关注领域相距甚远，无法看出在RecSys/Search/Ads中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13276v1": {
    "title": "MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13276v1",
    "arxiv_id": "2510.13276v1",
    "authors": "Keyan Zhou, Zecheng Tang, Lingfeng Ming, Guanghao Zhou, Qiguang Chen, Dan Qiao, Zheming Yang, Libo Qin, Minghui Qiu, Juntao Li, Min Zhang",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-15 08:22:03",
    "ori_summary": "The rapid advancement of large vision language models (LVLMs) has led to a significant expansion of their context windows. However, an extended context window does not guarantee the effective utilization of the context, posing a critical challenge for real-world applications. Current evaluations of such long-context faithfulness are predominantly focused on the text-only domain, while multimodal assessments remain limited to short contexts. To bridge this gap, we introduce MMLongCite, a comprehensive benchmark designed to evaluate the fidelity of LVLMs in long-context scenarios. MMLongCite comprises 8 distinct tasks spanning 6 context length intervals and incorporates diverse modalities, including text, images, and videos. Our evaluation of state-of-the-art LVLMs reveals their limited faithfulness in handling long multimodal contexts. Furthermore, we provide an in-depth analysis of how context length and the position of crucial content affect the faithfulness of these models.",
    "summary": "",
    "translation": "MMLongCite：用于评估长上下文视觉语言模型保真度的基准",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型的评估基准，属于纯粹的VLM评估范畴，与推荐系统、搜索或广告的核心技术进展无关。虽然提到了长上下文处理，但缺乏明确的机制说明或在这些领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13272v1": {
    "title": "Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2510.13272v1",
    "arxiv_id": "2510.13272v1",
    "authors": "Zhichao Xu, Zongyu Wu, Yun Zhou, Aosong Feng, Kang Zhou, Sangmin Woo, Kiran Ramnath, Yijun Tian, Xuan Qi, Weikang Qiu, Lin Lee Cheong, Haibo Ding",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 08:17:52",
    "ori_summary": "Inspired by the success of reinforcement learning (RL) in Large Language Model (LLM) training for domains like math and code, recent works have begun exploring how to train LLMs to use search engines more effectively as tools for retrieval-augmented generation. Although these methods achieve performance improvement across QA benchmarks, many prioritize final answer correctness while overlooking the quality of intermediate reasoning steps, which may lead to chain-of-thought unfaithfulness. In this paper, we first introduce a comprehensive evaluation framework for evaluating RL-based search agents, covering three distinct faithfulness metrics: information-think faithfulness, think-answer faithfulness, and think-search faithfulness. Our evaluations reveal that a prototypical RL-based search agent, Search-R1, has significant room for improvement in this regard. To foster faithful reasoning, we introduce VERITAS (Verifying Entailed Reasoning through Intermediate Traceability in Agentic Search), a novel framework that integrates fine-grained faithfulness rewards into the reinforcement learning process. Our experiments show that models trained with VERITAS not only significantly improve reasoning faithfulness, but also achieve comparable task performance across seven QA benchmarks.",
    "summary": "",
    "translation": "超越正确性：在检索增强生成中奖励忠实推理",
    "relevance_score": 3,
    "reasoning": "该论文主要关注检索增强生成(RAG)中的忠实推理评估，这属于纯粹的LLM评估和基准测试范畴。虽然RAG技术在搜索系统中有应用，但该论文聚焦于推理忠实性的评估方法，而非搜索/推荐/广告系统的核心算法改进或架构创新，与当前关注点的相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13271v1": {
    "title": "Do You Get the Hint? Benchmarking LLMs on the Board Game Concept",
    "url": "https://www.alphaxiv.org/abs/2510.13271v1",
    "arxiv_id": "2510.13271v1",
    "authors": "Ine Gevers, Walter Daelemans",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 08:17:25",
    "ori_summary": "Large language models (LLMs) have achieved striking successes on many benchmarks, yet recent studies continue to expose fundamental weaknesses. In particular, tasks that require abstract reasoning remain challenging, often because they use representations such as grids, symbols, or visual patterns that differ from the natural language data LLMs are trained on. In this paper, we introduce Concept, a simple word-guessing board game, as a benchmark for probing abductive reasoning in a representation that is much closer to LLM pre-training data: natural language. Our results show that this game, easily solved by humans (with a success rate of over 90\\%), is still very challenging for state-of-the-art LLMs (no model exceeds 40\\% success rate). Specifically, we observe that LLMs struggle with interpreting other players' strategic intents, and with correcting initial hypotheses given sequential information updates. In addition, we extend the evaluation across multiple languages, and find that the LLM performance drops further in lower-resource languages (Dutch, French, and Spanish) compared to English.",
    "summary": "",
    "translation": "你理解提示了吗？在棋盘游戏概念上对大型语言模型进行基准测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注在棋盘游戏上对LLM进行基准测试，这属于纯粹的LLM评估和基准测试范畴，与我的核心关注点无关。虽然涉及LLM，但没有展示在推荐系统、搜索或广告中的潜在应用，并且基准测试属于被排除的纯粹NLP中心主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13255v1": {
    "title": "Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain",
    "url": "https://www.alphaxiv.org/abs/2510.13255v1",
    "arxiv_id": "2510.13255v1",
    "authors": "Jingmin An, Yilong Song, Ruolin Yang, Nai Ding, Lingxi Lu, Yuxuan Wang, Wei Wang, Chu Zhuang, Qian Wang, Fang Fang",
    "categories": "cs.CL, cs.NE",
    "pub_date": "2025-10-15 08:04:49",
    "ori_summary": "Large Language Models (LLMs) demonstrate human-level or even superior language abilities, effectively modeling syntactic structures, yet the specific computational modules responsible remain unclear. A key question is whether LLM behavioral capabilities stem from mechanisms akin to those in the human brain. To address these questions, we introduce the Hierarchical Frequency Tagging Probe (HFTP), a tool that utilizes frequency-domain analysis to identify neuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP) neurons) and cortical regions (via intracranial recordings) encoding syntactic structures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama 2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human brain relies on distinct cortical regions for different syntactic levels. Representational similarity analysis reveals a stronger alignment between LLM representations and the left hemisphere of the brain (dominant in language processing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows greater brain similarity than Gemma, while Llama 3.1 shows less alignment with the brain compared to Llama 2. These findings offer new insights into the interpretability of LLM behavioral improvements, raising questions about whether these advancements are driven by human-like or non-human-like mechanisms, and establish HFTP as a valuable tool bridging computational linguistics and cognitive neuroscience. This project is available at https://github.com/LilTiger/HFTP.",
    "summary": "",
    "translation": "层次频率标记探针（HFTP）：一种研究大型语言模型和人脑中句法结构表征的统一方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM中句法结构的神经表征研究，这属于纯NLP基础研究范畴。虽然涉及LLM内部表征分析，但聚焦于句法结构这种语言特异性问题，缺乏明确的推荐系统、搜索或广告应用前景。论文更像是探索LLM与人脑认知机制的对比研究，而非开发可应用于实际业务场景的技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13220v1": {
    "title": "EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems",
    "url": "https://www.alphaxiv.org/abs/2510.13220v1",
    "arxiv_id": "2510.13220v1",
    "authors": "Yufei He, Juncheng Liu, Yue Liu, Yibo Li, Tri Cao, Zhiyuan Hu, Xinxing Xu, Bryan Hooi",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-15 07:16:28",
    "ori_summary": "A fundamental limitation of current AI agents is their inability to learn complex skills on the fly at test time, often behaving like \"clever but clueless interns\" in novel environments. This severely limits their practical utility. To systematically measure and drive progress on this challenge, we first introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a new evaluation setup where an agent must play the same game for several consecutive episodes, attempting to improve its performance from one episode to the next. On J-TTL, we find that existing adaptation methods like reflection, memory, or reinforcement learning struggle. To address the challenges posed by our benchmark, we present EvoTest, an evolutionary test-time learning framework that improves an agent without any fine-tuning or gradients-by evolving the entire agentic system after every episode. EvoTest has two roles: the Actor Agent, which plays the game, and the Evolver Agent, which analyzes the episode transcript to propose a revised configuration for the next run. This configuration rewrites the prompt, updates memory by logging effective state-action choices, tunes hyperparameters, and learns the tool-use routines. On our J-TTL benchmark, EvoTest consistently increases performance, outperforming not only reflection and memory-only baselines but also more complex online fine-tuning methods. Notably, our method is the only one capable of winning two games (Detective and Library), while all baselines fail to win any.",
    "summary": "",
    "translation": "EvoTest：用于自改进智能体系统的进化式测试时学习",
    "relevance_score": 2,
    "reasoning": "该论文主要关注智能体系统的测试时学习和自改进能力，这属于通用AI智能体技术，与推荐系统、搜索或广告的核心技术领域关联较弱。虽然测试时学习概念在理论上可能应用于在线学习场景，但论文标题未表明任何具体的RecSys/Search/Ads应用或相关技术组件。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13215v1": {
    "title": "Personalized Learning Path Planning with Goal-Driven Learner State Modeling",
    "url": "https://www.alphaxiv.org/abs/2510.13215v1",
    "arxiv_id": "2510.13215v1",
    "authors": "Joy Jia Yin Lim, Ye He, Jifan Yu, Xin Cong, Daniel Zhang-Li, Zhiyuan Liu, Huiqin Liu, Lei Hou, Juanzi Li, Bin Xu",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-15 06:59:49",
    "ori_summary": "Personalized Learning Path Planning (PLPP) aims to design adaptive learning paths that align with individual goals. While large language models (LLMs) show potential in personalizing learning experiences, existing approaches often lack mechanisms for goal-aligned planning. We introduce Pxplore, a novel framework for PLPP that integrates a reinforcement-based training paradigm and an LLM-driven educational architecture. We design a structured learner state model and an automated reward function that transforms abstract objectives into computable signals. We train the policy combining supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), and deploy it within a real-world learning platform. Extensive experiments validate Pxplore's effectiveness in producing coherent, personalized, and goal-driven learning paths. We release our code and dataset to facilitate future research.",
    "summary": "",
    "translation": "基于目标驱动学习者状态建模的个性化学习路径规划",
    "relevance_score": 2,
    "reasoning": "该论文主要关注教育领域的个性化学习路径规划，虽然涉及个性化推荐概念，但属于教育技术领域而非推荐系统/搜索/广告的核心范畴。论文的技术方法（目标驱动状态建模）可能对推荐系统有启发，但这种应用过于间接且非核心关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13211v1": {
    "title": "A fully automated and scalable Parallel Data Augmentation for Low Resource Languages using Image and Text Analytics",
    "url": "https://www.alphaxiv.org/abs/2510.13211v1",
    "arxiv_id": "2510.13211v1",
    "authors": "Prawaal Sharma, Navneet Goyal, Poonam Goyal, Vishnupriyan R",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 06:57:23",
    "ori_summary": "Linguistic diversity across the world creates a disparity with the availability of good quality digital language resources thereby restricting the technological benefits to majority of human population. The lack or absence of data resources makes it difficult to perform NLP tasks for low-resource languages. This paper presents a novel scalable and fully automated methodology to extract bilingual parallel corpora from newspaper articles using image and text analytics. We validate our approach by building parallel data corpus for two different language combinations and demonstrate the value of this dataset through a downstream task of machine translation and improve over the current baseline by close to 3 BLEU points.",
    "summary": "",
    "translation": "基于图像与文本分析的、全自动可扩展的低资源语言并行数据增强方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注低资源语言的数据增强技术，虽然涉及文本分析，但其核心应用场景是机器翻译等NLP任务，而非推荐系统、搜索或广告领域。论文提到的图像和文本分析可能在某些边缘情况下用于多模态推荐，但缺乏明确的RecSys/Search/Ads应用连接。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13202v1": {
    "title": "LLM-Guided Synthetic Augmentation (LGSA) for Mitigating Bias in AI Systems",
    "url": "https://www.alphaxiv.org/abs/2510.13202v1",
    "arxiv_id": "2510.13202v1",
    "authors": "Sai Suhruth Reddy Karri, Yashwanth Sai Nallapuneni, Laxmi Narasimha Reddy Mallireddy, Gopichand G",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 06:42:35",
    "ori_summary": "Bias in AI systems, especially those relying on natural language data, raises ethical and practical concerns. Underrepresentation of certain groups often leads to uneven performance across demographics. Traditional fairness methods, such as pre-processing, in-processing, and post-processing, depend on protected-attribute labels, involve accuracy-fairness trade-offs, and may not generalize across datasets. To address these challenges, we propose LLM-Guided Synthetic Augmentation (LGSA), which uses large language models to generate counterfactual examples for underrepresented groups while preserving label integrity. We evaluated LGSA on a controlled dataset of short English sentences with gendered pronouns, professions, and binary classification labels. Structured prompts were used to produce gender-swapped paraphrases, followed by quality control including semantic similarity checks, attribute verification, toxicity screening, and human spot checks. The augmented dataset expanded training coverage and was used to train a classifier under consistent conditions. Results show that LGSA reduces performance disparities without compromising accuracy. The baseline model achieved 96.7 percent accuracy with a 7.2 percent gender bias gap. Simple swap augmentation reduced the gap to 0.7 percent but lowered accuracy to 95.6 percent. LGSA achieved 99.1 percent accuracy with a 1.9 percent bias gap, improving performance on female-labeled examples. These findings demonstrate that LGSA is an effective strategy for bias mitigation, enhancing subgroup balance while maintaining high task accuracy and label fidelity.",
    "summary": "",
    "translation": "LLM引导的合成增强（LGSA）用于减轻AI系统中的偏见",
    "relevance_score": 2,
    "reasoning": "虽然论文涉及LLM技术，但其核心关注点是偏见缓解，这属于公平性、伦理等非技术性话题，明确列在无关主题中。该论文没有展示在推荐系统、搜索或广告中的直接应用潜力，主要解决的是AI系统的社会技术问题而非核心算法进步。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13197v1": {
    "title": "Text Anomaly Detection with Simplified Isolation Kernel",
    "url": "https://www.alphaxiv.org/abs/2510.13197v1",
    "arxiv_id": "2510.13197v1",
    "authors": "Yang Cao, Sikun Yang, Yujiu Yang, Lianyong Qi, Ming Liu",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 06:35:54",
    "ori_summary": "Two-step approaches combining pre-trained large language model embeddings and anomaly detectors demonstrate strong performance in text anomaly detection by leveraging rich semantic representations. However, high-dimensional dense embeddings extracted by large language models pose challenges due to substantial memory requirements and high computation time. To address this challenge, we introduce the Simplified Isolation Kernel (SIK), which maps high-dimensional dense embeddings to lower-dimensional sparse representations while preserving crucial anomaly characteristics. SIK has linear time complexity and significantly reduces space complexity through its innovative boundary-focused feature mapping. Experiments across 7 datasets demonstrate that SIK achieves better detection performance than 11 state-of-the-art (SOTA) anomaly detection algorithms while maintaining computational efficiency and low memory cost. All code and demonstrations are available at https://github.com/charles-cao/SIK.",
    "summary": "",
    "translation": "基于简化隔离核的文本异常检测",
    "relevance_score": 2,
    "reasoning": "该论文专注于文本异常检测这一特定NLP任务，属于纯文本处理范畴，与推荐系统、搜索或广告的核心技术关联度较低。虽然异常检测在理论上可能用于识别异常用户行为，但论文标题明确限定于文本数据，且未提及任何与推荐、搜索或广告相关的应用场景，因此相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13194v1": {
    "title": "StressTransfer: Stress-Aware Speech-to-Speech Translation with Emphasis Preservation",
    "url": "https://www.alphaxiv.org/abs/2510.13194v1",
    "arxiv_id": "2510.13194v1",
    "authors": "Xi Chen, Yuchen Song, Satoshi Nakamura",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 06:32:24",
    "ori_summary": "We propose a stress-aware speech-to-speech translation (S2ST) system that preserves word-level emphasis by leveraging LLMs for cross-lingual emphasis conversion. Our method translates source-language stress into target-language tags that guide a controllable TTS model. To overcome data scarcity, we developed a pipeline to automatically generate aligned training data and introduce the \"LLM-as-Judge\" for evaluation. Experiments show our approach substantially outperforms baselines in preserving emphasis while maintaining comparable translation quality, speaker intent, and naturalness. Our work highlights the importance of prosody in translation and provides an effective, data-efficient solution for preserving paralinguistic cues in S2ST.",
    "summary": "",
    "translation": "StressTransfer：具有重音保持功能的压力感知语音到语音翻译",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音到语音翻译中的重音处理，属于纯粹的语音处理领域，与搜索、推荐或广告系统没有明显关联。即使考虑其作为使能技术的潜力，语音重音处理在RecSys/Search/Ads应用场景中缺乏明确的价值主张和实际应用路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13191v1": {
    "title": "Grounding Long-Context Reasoning with Contextual Normalization for Retrieval-Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2510.13191v1",
    "arxiv_id": "2510.13191v1",
    "authors": "Jiamin Chen, Yuchen Li, Xinyu Ma, Xinran Chen, Xiaokun Zhang, Shuaiqiang Wang, Chen Ma, Dawei Yin",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 06:28:25",
    "ori_summary": "Retrieval-Augmented Generation (RAG) has become an essential approach for extending the reasoning and knowledge capacity of large language models (LLMs). While prior research has primarily focused on retrieval quality and prompting strategies, the influence of how the retrieved documents are framed, i.e., context format, remains underexplored. We show that seemingly superficial choices, such as delimiters or structural markers in key-value extraction, can induce substantial shifts in accuracy and stability, even when semantic content is identical. To systematically investigate this effect, we design controlled experiments that vary context density, delimiter styles, and positional placement, revealing the underlying factors that govern performance differences. Building on these insights, we introduce Contextual Normalization, a lightweight strategy that adaptively standardizes context representations before generation. Extensive experiments on both controlled and real-world RAG benchmarks across diverse settings demonstrate that the proposed strategy consistently improves robustness to order variation and strengthens long-context utilization. These findings underscore that reliable RAG depends not only on retrieving the right content, but also on how that content is presented, offering both new empirical evidence and a practical technique for better long-context reasoning.",
    "summary": "论文研究检索增强生成中上下文格式对模型推理性能的影响问题，核心思想是上下文呈现方式（如分隔符、结构标记）会显著影响模型表现，并提出上下文归一化方法来标准化上下文表示以提升长上下文推理的鲁棒性。",
    "translation": "基于上下文归一化的检索增强生成长文本推理",
    "relevance_score": 8,
    "reasoning": "该论文聚焦检索增强生成(RAG)技术，这是搜索和推荐系统的核心使能技术。长文本推理和上下文归一化技术可以显著提升LLM在搜索问答和个性化推荐中的准确性和效率，直接应用于处理用户历史行为和上下文特征的复杂建模。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对检索增强生成中的上下文表示问题，提出了上下文归一化方法，对搜索和推荐系统中的内容呈现方式有重要启示。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13190v1": {
    "title": "SHIELD: Classifier-Guided Prompting for Robust and Safer LVLMs",
    "url": "https://www.alphaxiv.org/abs/2510.13190v1",
    "arxiv_id": "2510.13190v1",
    "authors": "Juan Ren, Mark Dras, Usman Naseem",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 06:27:46",
    "ori_summary": "Large Vision-Language Models (LVLMs) unlock powerful multimodal reasoning but also expand the attack surface, particularly through adversarial inputs that conceal harmful goals in benign prompts. We propose SHIELD, a lightweight, model-agnostic preprocessing framework that couples fine-grained safety classification with category-specific guidance and explicit actions (Block, Reframe, Forward). Unlike binary moderators, SHIELD composes tailored safety prompts that enforce nuanced refusals or safe redirection without retraining. Across five benchmarks and five representative LVLMs, SHIELD consistently lowers jailbreak and non-following rates while preserving utility. Our method is plug-and-play, incurs negligible overhead, and is easily extendable to new attack types -- serving as a practical safety patch for both weakly and strongly aligned LVLMs.",
    "summary": "",
    "translation": "SHIELD：用于鲁棒且更安全大视觉语言模型的分类器引导提示",
    "relevance_score": 2,
    "reasoning": "该论文主要关注大视觉语言模型的安全性和鲁棒性，属于VLM安全增强技术。虽然涉及视觉语言模型，但其核心焦点是安全防护而非异构数据统一建模，与VLM类比异构数据的关注点仅有微弱关联。在推荐/搜索/广告领域，这种安全技术可能用于内容安全过滤，但应用潜力有限且非核心关注方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13183v1": {
    "title": "DSCD: Large Language Model Detoxification with Self-Constrained Decoding",
    "url": "https://www.alphaxiv.org/abs/2510.13183v1",
    "arxiv_id": "2510.13183v1",
    "authors": "Ming Dong, Jinkui Zhang, Bolong Zheng, Xinhui Tu, Po Hu, Tingting He",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 06:10:47",
    "ori_summary": "Detoxification in large language models (LLMs) remains a significant research challenge. Existing decoding detoxification methods are all based on external constraints, which require additional resource overhead and lose generation fluency. This work proposes Detoxification with Self-Constrained Decoding (DSCD), a novel method for LLM detoxification without parameter fine-tuning. DSCD strengthens the inner next-token distribution of the safety layer while weakening that of hallucination and toxic layers during output generation. This effectively diminishes toxicity and enhances output safety. DSCD offers lightweight, high compatibility, and plug-and-play capabilities, readily integrating with existing detoxification methods for further performance improvement. Extensive experiments on representative open-source LLMs and public datasets validate DSCD's effectiveness, demonstrating state-of-the-art (SOTA) performance in both detoxification and generation fluency, with superior efficiency compared to existing methods. These results highlight DSCD's potential as a practical and scalable solution for safer LLM deployments.",
    "summary": "",
    "translation": "DSCD：基于自约束解码的大语言模型去毒",
    "relevance_score": 2,
    "reasoning": "该论文专注于大语言模型的安全性和内容过滤问题（去毒），这属于模型安全范畴而非推荐系统、搜索或广告的核心技术。虽然去毒技术可能间接影响内容推荐的质量，但论文本身不涉及排序算法、用户建模或广告投放等核心领域，也不属于Transformer架构改进或异构数据建模等使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13170v1": {
    "title": "Putting on the Thinking Hats: A Survey on Chain of Thought Fine-tuning from the Perspective of Human Reasoning Mechanism",
    "url": "https://www.alphaxiv.org/abs/2510.13170v1",
    "arxiv_id": "2510.13170v1",
    "authors": "Xiaoshu Chen, Sihang Zhou, Ke Liang, Duanyang Yuan, Haoyuan Chen, Xiaoyu Sun, Linyuan Meng, Xinwang Liu",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 05:54:13",
    "ori_summary": "Chain of thought (CoT) fine-tuning aims to endow large language models (LLMs) with reasoning capabilities by training them on curated reasoning traces. It leverages both supervised and reinforced fine-tuning to cultivate human-like reasoning skills in LLMs, including detailed planning, divergent thinking, intuitive judgment, timely reflection, internal thinking, and fact perception, etc. As CoT fine-tuning has advanced, LLMs have demonstrated substantial improvements in tasks such as mathematical reasoning and code generation. However, existing surveys about CoT fine-tuning primarily focus on technical aspects and overlook a systematic analysis from the perspective of human reasoning mechanisms. Given that the ultimate goal of CoT fine-tuning is to enable LLMs to reason like humans, it is crucial to investigate this technique through the lens of human cognition. To fill this gap, we present the first comprehensive survey of CoT fine-tuning grounded in human reasoning theory. Specifically, inspired by the well-known Six Thinking Hats framework, which systematically characterizes common human thinking modes using six metaphorical hats, we classify and examine CoT fine-tuning methods through this lens. Furthermore, building upon this theory, we outline potential directions for future research in CoT fine-tuning. In addition, we compile a comprehensive overview of existing datasets and model performances, and a real-time GitHub repository \\footnote{https://github.com/AI-Chen/Awesome-CoT-Finetuning} that continuously tracks recent advances in this area is maintained. We hope this survey will serve as a valuable resource to inspire innovation and foster progress in this rapidly evolving field.",
    "summary": "论文研究如何从人类推理机制视角系统分析链式思维微调技术，核心思想是借鉴六顶思考帽框架将CoT微调方法按人类思维模式进行分类，为开发类人推理的LLM提供理论指导。",
    "translation": "戴上思考帽：从人类推理机制视角审视思维链微调的研究综述",
    "relevance_score": 8,
    "reasoning": "该论文聚焦思维链（Chain of Thought）微调技术，这是LLM推理能力的核心进展，属于'Enabling LLM Tech'范畴。思维链技术可显著提升LLM在推荐和搜索中的复杂推理能力，如多步骤用户意图理解、个性化推荐理由生成和复杂查询处理，对RecSys和Search有直接应用价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文从人类推理机制角度系统分析CoT微调，直接关联LLM推理能力提升，对推荐和搜索中的复杂推理任务具有重要启发价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13166v1": {
    "title": "CoT-Evo: Evolutionary Distillation of Chain-of-Thought for Scientific Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.13166v1",
    "arxiv_id": "2510.13166v1",
    "authors": "Kehua Feng, Keyan Ding, Zhihui Zhu, Lei Liang, Qiang Zhang, Huajun Chen",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 05:29:56",
    "ori_summary": "While chain-of-thought (CoT) distillation from advanced large language models (LLMs) has proven effective in general reasoning tasks, it struggles in scientific domains where even advanced models often produce incorrect or superficial reasoning due to high complexity and specialized knowledge requirements. Directly distilling from such flawed outputs results in low-quality training data and limits the performance of smaller student models. To overcome this, we propose CoT-Evo, an evolutionary CoT distillation framework. It begins by constructing a diverse pool of reasoning trajectories from multiple LLM thinkers, enriches them with automatically retrieved domain knowledge, and iteratively refines the trajectories using novelty-driven selection, reflective recombination and mutation. The refinement is guided by a fitness function that evaluates answer correctness, coherence, and effective knowledge utilization. This results in a high-quality CoT dataset tailored for scientific reasoning. We employ this evolved dataset to fine-tune a compact model, which achieves state-of-the-art performance on scientific reasoning benchmarks. Our work establishes a scalable approach to synthesizing high-fidelity scientific reasoning data from diverse and fallible LLMs.",
    "summary": "",
    "translation": "CoT-Evo：用于科学推理的思维链进化蒸馏",
    "relevance_score": 2,
    "reasoning": "该论文聚焦于科学推理领域的思维链蒸馏技术，属于纯粹的NLP推理优化范畴。虽然思维链技术本身是LLM的重要能力，但论文明确限定在科学推理应用场景，没有展示在推荐系统、搜索或广告领域的潜在应用价值，与当前关注的技术方向关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13163v1": {
    "title": "A Matter of Representation: Towards Graph-Based Abstract Code Generation",
    "url": "https://www.alphaxiv.org/abs/2510.13163v1",
    "arxiv_id": "2510.13163v1",
    "authors": "Nyx Iskandar, Hisham Bedri, Andy Tsen",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 05:26:36",
    "ori_summary": "Most large language models (LLMs) today excel at generating raw, sequential code with minimal abstractions and custom structures. However, there has been little work on graph-based abstract code generation, where significant logic is encapsulated in predefined nodes and execution flow is determined by edges. This is relevant for visual programming languages, and in cases where raw source code is inaccessible to users and LLM training sets. In this work, we propose and evaluate JSON representations for graphs to enable high accuracy graph-based abstract code generation. We evaluate these representations on ScratchTest, a mini-benchmark based on our custom Python re-implementation of Scratch, which tests the LLM in code graph space. Our findings demonstrate that LLMs can indeed perform the aforementioned generation task in a single pass without relying on specialized or complex pipelines, given the correct graph representations. We also show that different representations induce significantly different accuracies, highlighting the instrumental role of representations in this generation task. All in all, this work establishes the first steps towards representation learning for graph-based abstract code generation.",
    "summary": "",
    "translation": "表征问题：基于图的抽象代码生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注代码生成任务，属于纯粹的NLP应用领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然图神经网络技术可能在某些推荐系统中使用，但论文的抽象代码生成应用场景与我的关注领域相距甚远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13161v1": {
    "title": "Mirror Speculative Decoding: Breaking the Serial Barrier in LLM Inference",
    "url": "https://www.alphaxiv.org/abs/2510.13161v1",
    "arxiv_id": "2510.13161v1",
    "authors": "Nikhil Bhendawade, Kumari Nishu, Arnav Kundu, Chris Bartels, Minsik Cho, Irina Belousova",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 05:22:57",
    "ori_summary": "Speculative decoding accelerates LLM inference by using a draft model to look ahead, but gains are capped by the cost of autoregressive draft generation: increasing draft size elevates acceptance rates but introduces additional latency overhead exacerbating the speed-accuracy tradeoff. Prior methods (Medusa, Hydra, EAGLE) partially reduce draft cost but either degrade acceptance or introduce overheads that limit scaling. We present Mirror Speculative Decoding (Mirror-SD), an inference algorithm that breaks the latency-acceptance tradeoff. Mirror-SD launches branch-complete rollouts from early-exit signals in parallel with the target model's suffix and explicitly maps computation across heterogeneous accelerators (GPU and NPU) to exploit cross-device parallelism. The draft speculates forward continuations for the target to verify, while the target simultaneously speculates correction paths for the draft, converting speculation into two complementary execution pipelines. To further cut draft latency without weakening acceptance semantics, we add speculative streaming so the draft emits multiple tokens per step. This dual strategy of parallel heterogeneous execution plus multi-token speculative streaming pushes speculative decoding toward its ideal regime of high acceptance with low overhead. On SpecBench with server-scale models from 14B to 66B parameters, Mirror-SD delivers consistent end-to-end gains, achieving 2.8x-5.8x wall-time speedups across diverse tasks and a 30% average relative improvement over the strongest baseline, EAGLE3.",
    "summary": "该论文研究如何突破LLM推理中的串行瓶颈问题。核心思想是通过并行异构执行架构，让草稿模型和目标模型同时进行推测计算，并引入多令牌推测流技术，实现低延迟下的高接受率推理加速。",
    "translation": "镜像推测解码：打破大语言模型推理中的串行障碍",
    "relevance_score": 8,
    "reasoning": "该论文专注于LLM推理加速技术，属于'Enabling LLM Tech'范畴。推测解码技术可以显著降低LLM推理延迟，这对于需要实时响应的推荐系统、搜索和广告应用至关重要，能够提升用户体验和系统吞吐量。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出了一种突破LLM推理串行瓶颈的新方法，通过并行异构执行和多令牌推测流技术，直接提升LLM推理效率，对搜索和推荐系统的实时响应至关重要。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13157v1": {
    "title": "Program of Thoughts for Financial Reasoning: Leveraging Dynamic In-Context Examples and Generative Retrieval",
    "url": "https://www.alphaxiv.org/abs/2510.13157v1",
    "arxiv_id": "2510.13157v1",
    "authors": "Subhendu Khatuya, Shashwat Naidu, Pawan Goyal, Niloy Ganguly",
    "categories": "cs.CE, cs.AI, cs.CL",
    "pub_date": "2025-10-15 05:16:54",
    "ori_summary": "Despite continuous advancements in the capabilities of large language models (LLMs), numerical reasoning remains a challenging area. Techniques like chain-of-thought prompting, tree-of-thought prompting, and program-of-thought prompting guide LLMs through intermediate reasoning steps. Although in-context learning with few-shot prompting has improved performance, LLMs still lag behind state-of-the-art models on financial numerical reasoning datasets such as FinQA and ConvFinQA. In this work, we introduce FINDER, a novel two-step framework, to enhance LLMs' capabilities in financial numerical reasoning. The first step utilizes a generative retriever to extract relevant facts from unstructured data, including both text and tables. This is followed by context-aware Program of Thought prompting with dynamic selection of in-context examples. Our model FINDER achieves a new state-of-the-art performance on both the FinQA and ConvFinQA datasets, surpassing previous benchmarks with execution accuracy improvements of 5.98% and 4.05%, respectively.",
    "summary": "",
    "translation": "用于金融推理的思维程序：利用动态上下文示例和生成式检索",
    "relevance_score": 2,
    "reasoning": "该论文主要关注金融领域的推理任务，属于特定领域应用而非核心推荐系统、搜索或广告技术。虽然涉及思维链和上下文学习等LLM技术，但缺乏与推荐系统、搜索或广告的直接关联或潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13154v1": {
    "title": "I Am Aligned, But With Whom? MENA Values Benchmark for Evaluating Cultural Alignment and Multilingual Bias in LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.13154v1",
    "arxiv_id": "2510.13154v1",
    "authors": "Pardis Sadat Zahraei, Ehsaneddin Asgari",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 05:10:57",
    "ori_summary": "We introduce MENAValues, a novel benchmark designed to evaluate the cultural alignment and multilingual biases of large language models (LLMs) with respect to the beliefs and values of the Middle East and North Africa (MENA) region, an underrepresented area in current AI evaluation efforts. Drawing from large-scale, authoritative human surveys, we curate a structured dataset that captures the sociocultural landscape of MENA with population-level response distributions from 16 countries. To probe LLM behavior, we evaluate diverse models across multiple conditions formed by crossing three perspective framings (neutral, personalized, and third-person/cultural observer) with two language modes (English and localized native languages: Arabic, Persian, Turkish). Our analysis reveals three critical phenomena: \"Cross-Lingual Value Shifts\" where identical questions yield drastically different responses based on language, \"Reasoning-Induced Degradation\" where prompting models to explain their reasoning worsens cultural alignment, and \"Logit Leakage\" where models refuse sensitive questions while internal probabilities reveal strong hidden preferences. We further demonstrate that models collapse into simplistic linguistic categories when operating in native languages, treating diverse nations as monolithic entities. MENAValues offers a scalable framework for diagnosing cultural misalignment, providing both empirical insights and methodological tools for developing more culturally inclusive AI.",
    "summary": "",
    "translation": "我是对齐的，但和谁对齐？面向中东和北非价值观的基准，用于评估大型语言模型的文化对齐与多语言偏见",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM的文化对齐评估和多语言偏见检测，这属于伦理评估和公平性范畴，属于明确的无关主题。虽然提到了多语言能力，但核心焦点是价值观对齐评估而非技术改进，对推荐系统、搜索或广告的技术进步没有直接贡献。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13143v1": {
    "title": "Stable LLM Ensemble: Interaction between Example Representativeness and Diversity",
    "url": "https://www.alphaxiv.org/abs/2510.13143v1",
    "arxiv_id": "2510.13143v1",
    "authors": "Junichiro Niimi",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 04:49:23",
    "ori_summary": "Large language models (LLMs) have achieved remarkable results in wide range of domains. However, the accuracy and robustness of one-shot LLM predictions remain highly sensitive to the examples and the diversity among ensemble members. This study systematically investigates the effects of example representativeness (one-shot strategy) and output diversity (sampling temperature) on LLM ensemble performance. Two one-shot strategies are compared: centroid-based representative examples (proposed) and randomly sampled examples (baseline) and sampling temperature also is varied. The proposed approach with higher temperature setting significantly outperforms random selection by +7.6% (macro-F1) and -10.5% (RMSE). Furthermore, the proposed model exceeds 5-shot prompting by +21.1% (macro-F1) and -24.0% (RMSE). Our findings demonstrate that combining representative example selection with increased temperature provides the appropriate level of diversity to the ensemble. This work highlights the practical importance of both example selection and controlled diversity in designing effective one-shot LLM ensembles.",
    "summary": "",
    "translation": "稳定大语言模型集成：示例代表性与其多样性之间的相互作用",
    "relevance_score": 3,
    "reasoning": "该论文主要关注LLM集成方法中的示例选择策略，属于LLM技术的基础研究。虽然集成方法可能间接提升推荐或搜索系统的稳定性，但论文焦点更偏向通用的LLM优化而非直接面向RecSys/Search/Ads的应用场景，相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13139v1": {
    "title": "Addressing the alignment problem in transportation policy making: an LLM approach",
    "url": "https://www.alphaxiv.org/abs/2510.13139v1",
    "arxiv_id": "2510.13139v1",
    "authors": "Xiaoyu Yan, Tianxing Dai, Yu, Nie",
    "categories": "cs.CY, cs.CE, cs.CL, cs.MA",
    "pub_date": "2025-10-15 04:36:38",
    "ori_summary": "A key challenge in transportation planning is that the collective preferences of heterogeneous travelers often diverge from the policies produced by model-driven decision tools. This misalignment frequently results in implementation delays or failures. Here, we investigate whether large language models (LLMs), noted for their capabilities in reasoning and simulating human decision-making, can help inform and address this alignment problem. We develop a multi-agent simulation in which LLMs, acting as agents representing residents from different communities in a city, participate in a referendum on a set of transit policy proposals. Using chain-of-thought reasoning, LLM agents provide ranked-choice or approval-based preferences, which are aggregated using instant-runoff voting (IRV) to model democratic consensus. We implement this simulation framework with both GPT-4o and Claude-3.5, and apply it for Chicago and Houston. Our findings suggest that LLM agents are capable of approximating plausible collective preferences and responding to local context, while also displaying model-specific behavioral biases and modest divergences from optimization-based benchmarks. These capabilities underscore both the promise and limitations of LLMs as tools for solving the alignment problem in transportation decision-making.",
    "summary": "",
    "translation": "解决交通政策制定中的对齐问题：一种大语言模型方法",
    "relevance_score": 2,
    "reasoning": "该论文虽然涉及LLM应用，但聚焦于交通政策制定这一特定领域，与推荐系统、搜索或广告的核心技术领域相关性较弱。交通政策属于城市规划和公共管理范畴，不属于当前关注的RecSys/Search/Ads技术领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13117v1": {
    "title": "On the Reasoning Abilities of Masked Diffusion Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13117v1",
    "arxiv_id": "2510.13117v1",
    "authors": "Anej Svete, Ashish Sabharwal",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-15 03:29:26",
    "ori_summary": "Masked diffusion models (MDMs) for text offer a compelling alternative to traditional autoregressive language models. Parallel generation makes them efficient, but their computational capabilities and the limitations inherent to their parallelism remain largely unexplored. To this end, we characterize what types of reasoning problems MDMs can provably solve and how efficiently. We do this by connecting MDMs to the well-understood reasoning frameworks of chain of thought (CoT) and padded looped transformers (PLTs) in the finite-precision log-width setting: We show that MDMs and polynomially-padded PLTs are, in fact, equivalent in this setting, and that MDMs can solve all problems that CoT-augmented transformers can. Moreover, we showcase classes of problems (including regular languages) for which MDMs are inherently more efficient than CoT transformers, where parallel generation allows for substantially faster reasoning.",
    "summary": "",
    "translation": "关于掩码扩散语言模型推理能力的研究",
    "relevance_score": 2,
    "reasoning": "该论文主要研究扩散语言模型的推理能力，属于纯粹的LLM能力评估范畴。虽然推理能力在理论上可能对推荐和搜索有帮助，但论文本身聚焦于基础模型能力分析，没有明确展示在推荐系统、搜索或广告领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13115v1": {
    "title": "Multi-Label Clinical Text Eligibility Classification and Summarization System",
    "url": "https://www.alphaxiv.org/abs/2510.13115v1",
    "arxiv_id": "2510.13115v1",
    "authors": "Surya Tejaswi Yerramsetty, Almas Fathimah",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 03:21:43",
    "ori_summary": "Clinical trials are central to medical progress because they help improve understanding of human health and the healthcare system. They play a key role in discovering new ways to detect, prevent, or treat diseases, and it is essential that clinical trials include participants with appropriate and diverse medical backgrounds. In this paper, we propose a system that leverages Natural Language Processing (NLP) and Large Language Models (LLMs) to automate multi-label clinical text eligibility classification and summarization. The system combines feature extraction methods such as word embeddings (Word2Vec) and named entity recognition to identify relevant medical concepts, along with traditional vectorization techniques such as count vectorization and TF-IDF (Term Frequency-Inverse Document Frequency). We further explore weighted TF-IDF word embeddings that integrate both count-based and embedding-based strengths to capture term importance effectively. Multi-label classification using Random Forest and SVM models is applied to categorize documents based on eligibility criteria. Summarization techniques including TextRank, Luhn, and GPT-3 are evaluated to concisely summarize eligibility requirements. Evaluation with ROUGE scores demonstrates the effectiveness of the proposed methods. This system shows potential for automating clinical trial eligibility assessment using data-driven approaches, thereby improving research efficiency.",
    "summary": "",
    "translation": "多标签临床文本资格分类与摘要系统",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域的临床文本处理，属于明确的医疗领域特定应用，与推荐系统、搜索或广告的核心技术无关。虽然涉及文本分类和摘要技术，但这些技术在医疗场景中的应用不符合当前关注的任何技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13106v1": {
    "title": "TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13106v1",
    "arxiv_id": "2510.13106v1",
    "authors": "Ruoyu Sun, Da Song, Jiayang Song, Yuheng Huang, Lei Ma",
    "categories": "cs.SE, cs.AI, cs.CL",
    "pub_date": "2025-10-15 02:59:07",
    "ori_summary": "As Large Language Models (LLMs) continue to revolutionize Natural Language Processing (NLP) applications, critical concerns about their trustworthiness persist, particularly in safety and robustness. To address these challenges, we introduce TRUSTVIS, an automated evaluation framework that provides a comprehensive assessment of LLM trustworthiness. A key feature of our framework is its interactive user interface, designed to offer intuitive visualizations of trustworthiness metrics. By integrating well-known perturbation methods like AutoDAN and employing majority voting across various evaluation methods, TRUSTVIS not only provides reliable results but also makes complex evaluation processes accessible to users. Preliminary case studies on models like Vicuna-7b, Llama2-7b, and GPT-3.5 demonstrate the effectiveness of our framework in identifying safety and robustness vulnerabilities, while the interactive interface allows users to explore results in detail, empowering targeted model improvements. Video Link: https://youtu.be/k1TrBqNVg8g",
    "summary": "",
    "translation": "TRUSTVIS：面向大语言模型的多维度可信度评估框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于大语言模型的可信度评估框架，属于评估基准和可信度研究范畴，这些都被明确列为不相关主题。虽然标题涉及LLMs，但内容方向是评估而非核心技术进步或直接应用，没有展示在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13103v1": {
    "title": "ESI: Epistemic Uncertainty Quantification via Semantic-preserving Intervention for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13103v1",
    "arxiv_id": "2510.13103v1",
    "authors": "Mingda Li, Xinyu Li, Weinan Zhang, Longxuan Ma",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-15 02:46:43",
    "ori_summary": "Uncertainty Quantification (UQ) is a promising approach to improve model reliability, yet quantifying the uncertainty of Large Language Models (LLMs) is non-trivial. In this work, we establish a connection between the uncertainty of LLMs and their invariance under semantic-preserving intervention from a causal perspective. Building on this foundation, we propose a novel grey-box uncertainty quantification method that measures the variation in model outputs before and after the semantic-preserving intervention. Through theoretical justification, we show that our method provides an effective estimate of epistemic uncertainty. Our extensive experiments, conducted across various LLMs and a variety of question-answering (QA) datasets, demonstrate that our method excels not only in terms of effectiveness but also in computational efficiency.",
    "summary": "",
    "translation": "ESI：通过语义保持干预实现大型语言模型的认知不确定性量化",
    "relevance_score": 3,
    "reasoning": "该论文主要关注LLM的不确定性量化方法，属于LLM可信性和可靠性范畴。虽然不确定性估计在推荐系统中可能有应用价值（如置信度评分），但论文标题未明确指向RecSys/Search/Ads领域的特定应用，且更偏向通用LLM可靠性研究而非直接的应用创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13079v1": {
    "title": "GatePro: Parameter-Free Expert Selection Optimization for Mixture-of-Experts Models",
    "url": "https://www.alphaxiv.org/abs/2510.13079v1",
    "arxiv_id": "2510.13079v1",
    "authors": "Chen Zheng, Yuhang Cai, Deyi Liu, Jin Ma, Yiyuan Ma, Yuan Yang, Jing Liu, Yutao Zeng, Xun Zhou, Siyuan Qiao",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-15 01:47:45",
    "ori_summary": "Modern large language models leverage Mixture-of-Experts (MoE) architectures for efficient scaling, but face a critical challenge: functionally similar experts are often selected simultaneously, creating redundant computation and limiting effective model capacity. Existing auxiliary balance loss methods improve token distribution but fail to address the underlying expert diversity problem. We introduce GatePro, a novel parameter-free method that directly promotes expert selection diversity. GatePro identifies the most similar expert pairs and introduces localized competition mechanisms, preventing redundant expert co-activation while maintaining natural expert specialization. Our comprehensive evaluation demonstrates GatePro's effectiveness across model scales and benchmarks. Analysis demonstrates GatePro's ability to achieve enhanced expert diversity, where experts develop more distinct and complementary capabilities, avoiding functional redundancy. This approach can be deployed hot-swappable during any training phase without additional learnable parameters, offering a practical solution for improving MoE effectiveness.",
    "summary": "该论文研究MoE模型中专家功能冗余导致计算浪费和有效容量受限的问题。核心方法是识别最相似专家对并引入局部竞争机制，防止冗余专家同时激活，同时保持专家自然专业化。",
    "translation": "GatePro：面向专家混合模型的免参数专家选择优化",
    "relevance_score": 8,
    "reasoning": "该论文专注于专家混合模型（MoE）中的专家选择优化，这直接属于'赋能Transformer技术'范畴中的MoE架构效率改进。在推荐系统和搜索领域，MoE模型可以通过专家网络处理不同类型的用户行为、物品特征或查询意图，而免参数的专家选择机制能够显著降低模型部署复杂度和计算开销，提升大规模推荐/搜索系统的推理效率。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文针对MoE架构的核心效率问题提出参数免费优化方法，直接提升专家多样性，对Transformer架构效率和LLM规模化具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13809v1": {
    "title": "PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.13809v1",
    "arxiv_id": "2510.13809v1",
    "authors": "Sihui Ji, Xi Chen, Xin Tao, Pengfei Wan, Hengshuang Zhao",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 17:59:59",
    "ori_summary": "Video generation models nowadays are capable of generating visually realistic videos, but often fail to adhere to physical laws, limiting their ability to generate physically plausible videos and serve as ''world models''. To address this issue, we propose PhysMaster, which captures physical knowledge as a representation for guiding video generation models to enhance their physics-awareness. Specifically, PhysMaster is based on the image-to-video task where the model is expected to predict physically plausible dynamics from the input image. Since the input image provides physical priors like relative positions and potential interactions of objects in the scenario, we devise PhysEncoder to encode physical information from it as an extra condition to inject physical knowledge into the video generation process. The lack of proper supervision on the model's physical performance beyond mere appearance motivates PhysEncoder to apply reinforcement learning with human feedback to physical representation learning, which leverages feedback from generation models to optimize physical representations with Direct Preference Optimization (DPO) in an end-to-end manner. PhysMaster provides a feasible solution for improving physics-awareness of PhysEncoder and thus of video generation, proving its ability on a simple proxy task and generalizability to wide-ranging physical scenarios. This implies that our PhysMaster, which unifies solutions for various physical processes via representation learning in the reinforcement learning paradigm, can act as a generic and plug-in solution for physics-aware video generation and broader applications.",
    "summary": "",
    "translation": "PhysMaster：通过强化学习掌握视频生成的物理表征",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频生成中的物理表征学习，属于纯粹的视觉生成领域，与推荐系统、搜索或广告没有直接关联。虽然使用了强化学习，但应用场景是视频生成而非排名或用户建模，完全超出了相关技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13808v1": {
    "title": "VisCoP: Visual Probing for Video Domain Adaptation of Vision Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13808v1",
    "arxiv_id": "2510.13808v1",
    "authors": "Dominick Reilly, Manish Kumar Govind, Le Xue, Srijan Das",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 17:59:52",
    "ori_summary": "Large Vision-Language Models (VLMs) excel at general visual reasoning tasks but exhibit sharp performance degradation when applied to novel domains with substantial distribution shifts from pretraining data. Existing domain adaptation approaches finetune different VLM components, but this often results in limited domain-specific feature learning or catastrophic forgetting of prior capabilities. To address these issues, we introduce Vision Contextualized Probing (VisCoP), which augments the VLM's vision encoder with a compact set of learnable visual probes. These probes enable efficient domain-specific adaptation with minimal modification to pretrained parameters. We evaluate VisCoP across three challenging domain adaptation settings-cross-view (exocentric to egocentric), cross-modal (RGB to depth), and cross-task (human understanding to robot control). Experiments show that VisCoP consistently outperforms existing adaptation strategies, achieving superior performance on target domains while effectively retaining source-domain knowledge.",
    "summary": "",
    "translation": "VisCoP：面向视觉语言模型视频领域自适应的视觉探测方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型在视频领域的适应性问题，属于计算机视觉和视频理解领域。虽然提到了视觉语言模型，但其应用场景（视频领域适应）与推荐系统、搜索或广告的核心技术关联度很低。视频领域适应技术可能对多媒体内容理解有一定帮助，但这种间接联系在当前聚焦范围内相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13802v1": {
    "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
    "url": "https://www.alphaxiv.org/abs/2510.13802v1",
    "arxiv_id": "2510.13802v1",
    "authors": "Xinhang Liu, Yuxi Xiao, Donny Y. Chen, Jiashi Feng, Yu-Wing Tai, Chi-Keung Tang, Bingyi Kang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 17:59:04",
    "ori_summary": "Effective spatio-temporal representation is fundamental to modeling, understanding, and predicting dynamics in videos. The atomic unit of a video, the pixel, traces a continuous 3D trajectory over time, serving as the primitive element of dynamics. Based on this principle, we propose representing any video as a Trajectory Field: a dense mapping that assigns a continuous 3D trajectory function of time to each pixel in every frame. With this representation, we introduce Trace Anything, a neural network that predicts the entire trajectory field in a single feed-forward pass. Specifically, for each pixel in each frame, our model predicts a set of control points that parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at arbitrary query time instants. We trained the Trace Anything model on large-scale 4D data, including data from our new platform, and our experiments demonstrate that: (i) Trace Anything achieves state-of-the-art performance on our new benchmark for trajectory field estimation and performs competitively on established point-tracking benchmarks; (ii) it offers significant efficiency gains thanks to its one-pass paradigm, without requiring iterative optimization or auxiliary estimators; and (iii) it exhibits emergent abilities, including goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion. Project page: https://trace-anything.github.io/.",
    "summary": "",
    "translation": "追踪万物：通过轨迹场实现任意视频的4D表示",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频理解和4D表示学习，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术关联较弱。虽然轨迹场技术理论上可以用于建模用户行为序列，但论文本身没有明确展示在RecSys/Search/Ads领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13800v1": {
    "title": "Reasoning in Space via Grounding in the World",
    "url": "https://www.alphaxiv.org/abs/2510.13800v1",
    "arxiv_id": "2510.13800v1",
    "authors": "Yiming Chen, Zekun Qi, Wenyao Zhang, Xin Jin, Li Zhang, Peidong Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 17:58:08",
    "ori_summary": "In this paper, we claim that 3D visual grounding is the cornerstone of spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to explore the effective spatial representations that bridge the gap between them. Existing 3D LLMs suffer from the absence of a unified 3D representation capable of jointly capturing semantic and geometric information. This deficiency is manifested either in poor performance on grounding or in an excessive reliance on external modules, ultimately hindering the seamless integration of grounding and spatial reasoning. To address this, we propose a simple yet effective dual-path pooling mechanism that tightly aligns geometric features with both semantic and positional cues, constructing a unified image patch-based 3D representation that encapsulates all essential information without increasing the number of input tokens. Leveraging this holistic representation, GS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely without external modules while delivering performance comparable to state-of-the-art models, establishing a unified and self-contained framework for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we introduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is meticulously curated to include both 3D bounding box annotations for objects referenced in reasoning questions and step-by-step reasoning paths that integrate grounding as a core component of the problem-solving process. Extensive experiments demonstrate that GS-Reasoner achieves impressive results on 3D visual grounding, which in turn significantly enhances its spatial reasoning capabilities, leading to state-of-the-art performance.",
    "summary": "",
    "translation": "通过世界中的接地在空间中进行推理",
    "relevance_score": 2,
    "reasoning": "该论文标题暗示了空间推理和物理世界接地，这主要与具身AI、机器人技术或物理世界交互相关。虽然空间推理在某些搜索场景中可能有边缘应用，但缺乏与推荐系统、广告或核心LLM技术的直接联系，且未明确涉及Transformer架构或异构数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13795v1": {
    "title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs",
    "url": "https://www.alphaxiv.org/abs/2510.13795v1",
    "arxiv_id": "2510.13795v1",
    "authors": "Yi Zhang, Bolin Ni, Xin-Sheng Chen, Heng-Rui Zhang, Yongming Rao, Houwen Peng, Qinglin Lu, Han Hu, Meng-Hao Guo, Shi-Min Hu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 17:52:59",
    "ori_summary": "Fully open multimodal large language models (MLLMs) currently lag behind proprietary counterparts, primarily due to a significant gap in data quality for supervised fine-tuning (SFT). Existing open-source datasets are often plagued by widespread noise and a critical deficit in complex reasoning data, such as Chain-of-Thought (CoT), which hinders the development of advanced model capabilities. Addressing these challenges, our work makes three primary contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising approximately 15 million QA pairs, processed through multiple cleaning techniques and enhanced with a novel dual-level (short and long) CoT enrichment strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its underlying framework DataStudio, providing the community with a transparent and adaptable methodology for data curation that moves beyond static dataset releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B model on Honey-Data-15M. Experiments show that Bee-8B establishes a new state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is competitive with, and in some cases surpasses, recent semi-open models such as InternVL3.5-8B. Our work delivers to the community a suite of foundational resources, including: the Honey-Data-15M corpus; the full-stack suite comprising HoneyPipe and DataStudio; training recipes; an evaluation harness; and the model weights. This effort demonstrates that a principled focus on data quality is a key pathway to developing fully open MLLMs that are highly competitive with their semi-open counterparts.",
    "summary": "",
    "translation": "Bee：一个高质量语料库与全栈套件，用于解锁先进的完全开放多模态大语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态大语言模型（MLLMs）的语料库构建和开源工具套件，属于纯粹的LLM技术基础设施。虽然多模态建模与VLM类比有概念关联，但论文没有明确展示在推荐系统、搜索或广告中的具体应用潜力，且更偏向通用多模态AI而非特定领域应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13793v1": {
    "title": "NoisePrints: Distortion-Free Watermarks for Authorship in Private Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2510.13793v1",
    "arxiv_id": "2510.13793v1",
    "authors": "Nir Goren, Oren Katzir, Abhinav Nakarmi, Eyal Ronen, Mahmood Sharif, Or Patashnik",
    "categories": "cs.CV, cs.CR, cs.LG",
    "pub_date": "2025-10-15 17:50:45",
    "ori_summary": "With the rapid adoption of diffusion models for visual content generation, proving authorship and protecting copyright have become critical. This challenge is particularly important when model owners keep their models private and may be unwilling or unable to handle authorship issues, making third-party verification essential. A natural solution is to embed watermarks for later verification. However, existing methods require access to model weights and rely on computationally heavy procedures, rendering them impractical and non-scalable. To address these challenges, we propose , a lightweight watermarking scheme that utilizes the random seed used to initialize the diffusion process as a proof of authorship without modifying the generation process. Our key observation is that the initial noise derived from a seed is highly correlated with the generated visual content. By incorporating a hash function into the noise sampling process, we further ensure that recovering a valid seed from the content is infeasible. We also show that sampling an alternative seed that passes verification is infeasible, and demonstrate the robustness of our method under various manipulations. Finally, we show how to use cryptographic zero-knowledge proofs to prove ownership without revealing the seed. By keeping the seed secret, we increase the difficulty of watermark removal. In our experiments, we validate NoisePrints on multiple state-of-the-art diffusion models for images and videos, demonstrating efficient verification using only the seed and output, without requiring access to model weights.",
    "summary": "",
    "translation": "NoisePrints：用于私有扩散模型中作者身份识别的无失真水印技术",
    "relevance_score": 1,
    "reasoning": "该论文专注于扩散模型中的水印技术，属于内容生成和版权保护领域，与推荐系统、搜索或广告的核心技术无关。虽然提到了私有模型，但这属于隐私保护范畴，属于明确排除的非技术性话题。该技术没有明显的应用场景可以转化为推荐、搜索或广告系统的核心算法改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13787v1": {
    "title": "Adaptive Visual Conditioning for Semantic Consistency in Diffusion-Based Story Continuation",
    "url": "https://www.alphaxiv.org/abs/2510.13787v1",
    "arxiv_id": "2510.13787v1",
    "authors": "Seyed Mohammad Mousavi, Morteza Analoui",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 17:43:22",
    "ori_summary": "Story continuation focuses on generating the next image in a narrative sequence so that it remains coherent with both the ongoing text description and the previously observed images. A central challenge in this setting lies in utilizing prior visual context effectively, while ensuring semantic alignment with the current textual input. In this work, we introduce AVC (Adaptive Visual Conditioning), a framework for diffusion-based story continuation. AVC employs the CLIP model to retrieve the most semantically aligned image from previous frames. Crucially, when no sufficiently relevant image is found, AVC adaptively restricts the influence of prior visuals to only the early stages of the diffusion process. This enables the model to exploit visual context when beneficial, while avoiding the injection of misleading or irrelevant information. Furthermore, we improve data quality by re-captioning a noisy dataset using large language models, thereby strengthening textual supervision and semantic alignment. Quantitative results and human evaluations demonstrate that AVC achieves superior coherence, semantic consistency, and visual fidelity compared to strong baselines, particularly in challenging cases where prior visuals conflict with the current input.",
    "summary": "",
    "translation": "基于扩散模型的故事延续中语义一致性的自适应视觉条件控制",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于扩散模型在故事延续中的视觉生成应用，属于纯粹的视觉内容生成领域。虽然提到了语义一致性，但这是针对故事文本到视觉内容的生成任务，与推荐系统、搜索或广告中的排序、检索或用户建模没有直接关联。该技术缺乏在RecSys/Search/Ads领域的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13778v1": {
    "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy",
    "url": "https://www.alphaxiv.org/abs/2510.13778v1",
    "arxiv_id": "2510.13778v1",
    "authors": "Xinyi Chen, Yilun Chen, Yanwei Fu, Ning Gao, Jiaya Jia, Weiyang Jin, Hao Li, Yao Mu, Jiangmiao Pang, Yu Qiao, Yang Tian, Bin Wang, Bolun Wang, Fangjing Wang, Hanqing Wang, Tai Wang, Ziqin Wang, Xueyuan Wei, Chao Wu, Shuai Yang, Jinhui Ye, Junqiu Yu, Jia Zeng, Jingjing Zhang, Jinyu Zhang, Shi Zhang, Feng Zheng, Bowen Zhou, Yangkun Zhu",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2025-10-15 17:30:05",
    "ori_summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act'' by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act'' by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.",
    "summary": "",
    "translation": "InternVLA-M1：一种用于通用机器人策略的空间引导视觉-语言-动作框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人控制领域的视觉-语言-动作框架，属于机器人学特定应用。虽然涉及多模态建模，但其核心是机器人策略学习，与推荐系统、搜索或广告领域没有直接关联。该技术不具备在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13774v1": {
    "title": "UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of Robust Spatial Representations",
    "url": "https://www.alphaxiv.org/abs/2510.13774v1",
    "arxiv_id": "2510.13774v1",
    "authors": "Dominik J. Mühlematter, Lin Che, Ye Hong, Martin Raubal, Nina Wiedemann",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-15 17:26:24",
    "ori_summary": "Forecasting urban phenomena such as housing prices and public health indicators requires the effective integration of various geospatial data. Current methods primarily utilize task-specific models, while recent foundation models for spatial representations often support only limited modalities and lack multimodal fusion capabilities. To overcome these challenges, we present UrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal Fusion (SMF). The framework employs modality-specific encoders to process different types of inputs, including street view imagery, remote sensing data, cartographic maps, and points of interest (POIs) data. These multimodal inputs are integrated via a Transformer-based fusion module that learns unified representations. An extensive evaluation across 41 tasks in 56 cities worldwide demonstrates UrbanFusion's strong generalization and predictive performance compared to state-of-the-art GeoAI models. Specifically, it 1) outperforms prior foundation models on location-encoding, 2) allows multimodal input during inference, and 3) generalizes well to regions unseen during training. UrbanFusion can flexibly utilize any subset of available modalities for a given location during both pretraining and inference, enabling broad applicability across diverse data availability scenarios. All source code is available at https://github.com/DominikM198/UrbanFusion.",
    "summary": "",
    "translation": "UrbanFusion：用于鲁棒空间表示对比学习的随机多模态融合",
    "relevance_score": 3,
    "reasoning": "该论文提出了一种多模态融合方法用于空间表示学习，这在一定程度上与'VLM类比用于异构数据'相关，因为它处理多模态数据。然而，该方法专注于城市空间表示，没有明确展示在推荐系统、搜索或广告中的应用潜力，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13768v1": {
    "title": "Scaling Vision Transformers for Functional MRI with Flat Maps",
    "url": "https://www.alphaxiv.org/abs/2510.13768v1",
    "arxiv_id": "2510.13768v1",
    "authors": "Connor Lane, Daniel Z. Kaplan, Tanishq Mathew Abraham, Paul S. Scotti",
    "categories": "cs.CV, cs.AI, q-bio.NC",
    "pub_date": "2025-10-15 17:15:00",
    "ori_summary": "A key question for adapting modern deep learning architectures to functional MRI (fMRI) is how to represent the data for model input. To bridge the modality gap between fMRI and natural images, we transform the 4D volumetric fMRI data into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K hours of fMRI flat map videos from the Human Connectome Project using the spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI modeling performance improves with dataset size according to a strict power scaling law. Downstream classification benchmarks show that our model learns rich representations supporting both fine-grained state decoding across subjects, as well as subject-specific trait decoding across changes in brain state. This work is part of an ongoing open science project to build foundation models for fMRI data. Our code and datasets are available at https://github.com/MedARC-AI/fmri-fm.",
    "summary": "",
    "translation": "使用平面映射扩展视觉变换器用于功能性磁共振成像",
    "relevance_score": 2,
    "reasoning": "该论文专注于医学影像（fMRI）领域的视觉变换器应用，属于明确的无关主题范畴。虽然涉及Transformer架构扩展，但其在功能性磁共振成像的特定医学应用与推荐系统、搜索或广告领域没有明显的技术关联或潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13759v1": {
    "title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark",
    "url": "https://www.alphaxiv.org/abs/2510.13759v1",
    "arxiv_id": "2510.13759v1",
    "authors": "Kai Zou, Ziqi Huang, Yuhao Dong, Shulin Tian, Dian Zheng, Hongbo Liu, Jingwen He, Bin Liu, Yu Qiao, Ziwei Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 17:10:35",
    "ori_summary": "Unified multimodal models aim to jointly enable visual understanding and generation, yet current benchmarks rarely examine their true integration. Existing evaluations either treat the two abilities in isolation or overlook tasks that inherently couple them. To address this gap, we present Uni-MMMU, a comprehensive and discipline-aware benchmark that systematically unfolds the bidirectional synergy between generation and understanding across eight reasoning-centric domains, including science, coding, mathematics, and puzzles. Each task is bidirectionally coupled, demanding models to (i) leverage conceptual understanding to guide precise visual synthesis, or (ii) utilize generation as a cognitive scaffold for analytical reasoning. Uni-MMMU incorporates verifiable intermediate reasoning steps, unique ground truths, and a reproducible scoring protocol for both textual and visual outputs. Through extensive evaluation of state-of-the-art unified, generation-only, and understanding-only models, we reveal substantial performance disparities and cross-modal dependencies, offering new insights into when and how these abilities reinforce one another, and establishing a reliable foundation for advancing unified models.",
    "summary": "",
    "translation": "Uni-MMMU：一个大规模多学科多模态统一基准",
    "relevance_score": 1,
    "reasoning": "这是一个多模态基准测试论文，专注于评估模型在各种学科任务上的表现。虽然涉及多模态，但它属于纯粹的评估基准范畴，与推荐系统、搜索或广告的核心技术进展、Transformer架构改进或直接应用无关。该论文没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13756v1": {
    "title": "RECODE: Reasoning Through Code Generation for Visual Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.13756v1",
    "arxiv_id": "2510.13756v1",
    "authors": "Junhong Shen, Mu Cai, Bo Hu, Ameet Talwalkar, David A Ross, Cordelia Schmid, Alireza Fathi",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-15 17:05:37",
    "ori_summary": "Multimodal Large Language Models (MLLMs) struggle with precise reasoning for structured visuals like charts and diagrams, as pixel-based perception lacks a mechanism for verification. To address this, we propose to leverage derendering -- the process of reverse-engineering visuals into executable code -- as a new modality for verifiable visual reasoning. Specifically, we propose RECODE, an agentic framework that first generates multiple candidate programs to reproduce the input image. It then uses a critic to select the most faithful reconstruction and iteratively refines the code. This process not only transforms an ambiguous perceptual task into a verifiable, symbolic problem, but also enables precise calculations and logical inferences later on. On various visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K, RECODE significantly outperforms methods that do not leverage code or only use code for drawing auxiliary lines or cropping. Our work demonstrates that grounding visual perception in executable code provides a new path toward more accurate and verifiable multimodal reasoning.",
    "summary": "",
    "translation": "RECODE：通过代码生成进行推理的视觉问答方法",
    "relevance_score": 2,
    "reasoning": "该论文专注于视觉问答领域的代码生成推理方法，属于纯粹的视觉-语言交叉研究。虽然涉及多模态建模，但其应用场景和核心方法都局限于视觉问答这一特定NLP任务，与推荐系统、搜索或广告领域没有明显的技术迁移路径或应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13747v1": {
    "title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue",
    "url": "https://www.alphaxiv.org/abs/2510.13747v1",
    "arxiv_id": "2510.13747v1",
    "authors": "Wenwen Tong, Hewei Guo, Dongchuan Ran, Jiangnan Chen, Jiefan Lu, Kaibin Wang, Keqiang Li, Xiaoxu Zhu, Jiakui Li, Kehan Li, Xueheng Li, Lumin Li, Chenxu Guo, Jiasheng Zhou, Jiandong Chen, Xianye Wu, Jiahao Wang, Silei Wu, Lei Chen, Hanming Deng, Yuxuan Song, Dinghao Zhou, Guiping Zhong, Ken Zheng, Shiyin Kang, Lewei Lu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 16:52:48",
    "ori_summary": "We introduce InteractiveOmni, a unified and open-source omni-modal large language model for audio-visual multi-turn interaction, ranging from 4B to 8B parameters, designed to lead the field of lightweight models by offering comprehensive omni-modal understanding and speech generation capabilities. To achieve this, we integrate the vision encoder, audio encoder, large language model, and speech decoder into a unified model for understanding and generation tasks. We design a multi-stage training strategy to ensure robust cross-modal capabilities, including pre-training for omni-modal understanding, followed by post-training with speech conversation and audio-visual interaction. To enable human-like long-term conversational ability, we meticulously curate a multi-turn training dataset that enhances the model's ability to handle complex and multi-turn interactions. To effectively evaluate the multi-turn memory and speech interaction capabilities, we construct the multi-modal multi-turn memory benchmark and the multi-turn speech interaction benchmark. Experiments demonstrate that InteractiveOmni significantly outperforms leading open-source models and provides a more intelligent multi-turn audio-visual experience, particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B is comparable to the much larger model like Qwen2.5-Omni-7B on general benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B while utilizing only 50% of the model size. Achieving state-of-the-art results against similarly sized models across image, audio, video understanding, and speech generation tasks, InteractiveOmni is an accessible, open-source foundation for next-generation intelligent interactive systems.",
    "summary": "",
    "translation": "InteractiveOmni：面向音视频多轮对话的统一全模态模型",
    "relevance_score": 2,
    "reasoning": "该论文虽然涉及多模态建模，但其核心聚焦于音视频对话系统，这与推荐系统、搜索或广告的核心技术需求关联度较低。尽管多模态统一建模的思想可能启发异构数据处理，但缺乏明确的RecSys/Search/Ads应用场景，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13745v1": {
    "title": "UniCalli: A Unified Diffusion Framework for Column-Level Generation and Recognition of Chinese Calligraphy",
    "url": "https://www.alphaxiv.org/abs/2510.13745v1",
    "arxiv_id": "2510.13745v1",
    "authors": "Tianshuo Xu, Kai Wang, Zhifei Chen, Leyi Wu, Tianshui Wen, Fei Chao, Ying-Cong Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 16:52:07",
    "ori_summary": "Computational replication of Chinese calligraphy remains challenging. Existing methods falter, either creating high-quality isolated characters while ignoring page-level aesthetics like ligatures and spacing, or attempting page synthesis at the expense of calligraphic correctness. We introduce \\textbf{UniCalli}, a unified diffusion framework for column-level recognition and generation. Training both tasks jointly is deliberate: recognition constrains the generator to preserve character structure, while generation provides style and layout priors. This synergy fosters concept-level abstractions that improve both tasks, especially in limited-data regimes. We curated a dataset of over 8,000 digitized pieces, with ~4,000 densely annotated. UniCalli employs asymmetric noising and a rasterized box map for spatial priors, trained on a mix of synthetic, labeled, and unlabeled data. The model achieves state-of-the-art generative quality with superior ligature continuity and layout fidelity, alongside stronger recognition. The framework successfully extends to other ancient scripts, including Oracle bone inscriptions and Egyptian hieroglyphs. Code and data can be viewed in \\href{https://github.com/EnVision-Research/UniCalli}{this URL}.",
    "summary": "",
    "translation": "UniCalli：一个用于中文书法列级生成与识别的统一扩散框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于中文书法的生成与识别，属于计算机视觉和艺术生成领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然扩散模型是生成技术，但该应用场景过于特定，无法为RecSys/Search/Ads领域提供可转移的技术见解或应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13740v1": {
    "title": "Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs",
    "url": "https://www.alphaxiv.org/abs/2510.13740v1",
    "arxiv_id": "2510.13740v1",
    "authors": "Mustafa Munir, Alex Zhang, Radu Marculescu",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-15 16:47:09",
    "ori_summary": "Vision graph neural networks (ViG) have demonstrated promise in vision tasks as a competitive alternative to conventional convolutional neural nets (CNN) and transformers (ViTs); however, common graph construction methods, such as k-nearest neighbor (KNN), can be expensive on larger images. While methods such as Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed step scale can lead to over-squashing and missing multiple connections to gain the same information that could be gained from a long-range link. Through this observation, we propose a new graph construction method, Logarithmic Scalable Graph Construction (LSGC) to enhance performance by limiting the number of long-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN model that utilizes LSGC. Furthermore, inspired by the successes of multi-scale and high-resolution architectures, we introduce and apply a high-resolution branch and fuse features between our high-resolution and low-resolution branches for a multi-scale high-resolution Vision GNN network. Extensive experiments show that LogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy, GMACs, and parameters on image classification and semantic segmentation tasks. Our smallest model, Ti-LogViG, achieves an average top-1 accuracy on ImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average accuracy than Vision GNN with a 24.3% reduction in parameters and 35.3% reduction in GMACs. Our work shows that leveraging long-range links in graph construction for ViGs through our proposed LSGC can exceed the performance of current state-of-the-art ViGs. Code is available at https://github.com/mmunir127/LogViG-Official.",
    "summary": "",
    "translation": "用于高效视觉图神经网络的多尺度高分辨率对数图绘制模块",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉图神经网络(GNN)的效率优化，属于计算机视觉架构改进。虽然提到了多尺度特征处理，但缺乏与推荐系统、搜索或广告领域的直接关联。论文专注于视觉GNN的特定模块设计，没有展示在异构数据处理或序列建模方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13735v1": {
    "title": "Cyclic Self-Supervised Diffusion for Ultra Low-field to High-field MRI Synthesis",
    "url": "https://www.alphaxiv.org/abs/2510.13735v1",
    "arxiv_id": "2510.13735v1",
    "authors": "Zhenxuan Zhang, Peiyuan Jing, Zi Wang, Ula Briski, Coraline Beitone, Yue Yang, Yinzhe Wu, Fanwen Wang, Liutao Yang, Jiahao Huang, Zhifan Gao, Zhaolin Chen, Kh Tohidul Islam, Guang Yang, Peter J. Lally",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 16:41:54",
    "ori_summary": "Synthesizing high-quality images from low-field MRI holds significant potential. Low-field MRI is cheaper, more accessible, and safer, but suffers from low resolution and poor signal-to-noise ratio. This synthesis process can reduce reliance on costly acquisitions and expand data availability. However, synthesizing high-field MRI still suffers from a clinical fidelity gap. There is a need to preserve anatomical fidelity, enhance fine-grained structural details, and bridge domain gaps in image contrast. To address these issues, we propose a \\emph{cyclic self-supervised diffusion (CSS-Diff)} framework for high-field MRI synthesis from real low-field MRI data. Our core idea is to reformulate diffusion-based synthesis under a cycle-consistent constraint. It enforces anatomical preservation throughout the generative process rather than just relying on paired pixel-level supervision. The CSS-Diff framework further incorporates two novel processes. The slice-wise gap perception network aligns inter-slice inconsistencies via contrastive learning. The local structure correction network enhances local feature restoration through self-reconstruction of masked and perturbed patches. Extensive experiments on cross-field synthesis tasks demonstrate the effectiveness of our method, achieving state-of-the-art performance (e.g., 31.80 $\\pm$ 2.70 dB in PSNR, 0.943 $\\pm$ 0.102 in SSIM, and 0.0864 $\\pm$ 0.0689 in LPIPS). Beyond pixel-wise fidelity, our method also preserves fine-grained anatomical structures compared with the original low-field MRI (e.g., left cerebral white matter error drops from 12.1$\\%$ to 2.1$\\%$, cortex from 4.2$\\%$ to 3.7$\\%$). To conclude, our CSS-Diff can synthesize images that are both quantitatively reliable and anatomically consistent.",
    "summary": "",
    "translation": "用于超低场到高场MRI合成的循环自监督扩散方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学成像领域的MRI合成技术，属于医疗应用范畴，与推荐系统、搜索或广告领域完全无关。论文涉及的自监督扩散方法虽然技术上有创新性，但缺乏在RecSys/Search/Ads领域的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13729v1": {
    "title": "LiFMCR: Dataset and Benchmark for Light Field Multi-Camera Registration",
    "url": "https://www.alphaxiv.org/abs/2510.13729v1",
    "arxiv_id": "2510.13729v1",
    "authors": "Aymeric Fleith, Julian Zirbel, Daniel Cremers, Niclas Zeller",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 16:32:27",
    "ori_summary": "We present LiFMCR, a novel dataset for the registration of multiple micro lens array (MLA)-based light field cameras. While existing light field datasets are limited to single-camera setups and typically lack external ground truth, LiFMCR provides synchronized image sequences from two high-resolution Raytrix R32 plenoptic cameras, together with high-precision 6-degrees of freedom (DoF) poses recorded by a Vicon motion capture system. This unique combination enables rigorous evaluation of multi-camera light field registration methods. As a baseline, we provide two complementary registration approaches: a robust 3D transformation estimation via a RANSAC-based method using cross-view point clouds, and a plenoptic PnP algorithm estimating extrinsic 6-DoF poses from single light field images. Both explicitly integrate the plenoptic camera model, enabling accurate and scalable multi-camera registration. Experiments show strong alignment with the ground truth, supporting reliable multi-view light field processing. Project page: https://lifmcr.github.io/",
    "summary": "",
    "translation": "LiFMCR：光场多相机配准的数据集与基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于光场多相机配准，属于计算机视觉中的特定技术领域，与推荐系统、搜索或广告的核心技术进展没有直接关联。光场相机技术主要应用于3D重建和计算摄影，在当前聚焦的LLM技术、Transformer架构改进或异构数据统一建模方面缺乏明确的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13720v1": {
    "title": "Circle of Willis Centerline Graphs: A Dataset and Baseline Algorithm",
    "url": "https://www.alphaxiv.org/abs/2510.13720v1",
    "arxiv_id": "2510.13720v1",
    "authors": "Fabio Musio, Norman Juchler, Kaiyuan Yang, Suprosanna Shit, Chinmay Prabhakar, Bjoern Menze, Sven Hirsch",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 16:22:51",
    "ori_summary": "The Circle of Willis (CoW) is a critical network of arteries in the brain, often implicated in cerebrovascular pathologies. Voxel-level segmentation is an important first step toward an automated CoW assessment, but a full quantitative analysis requires centerline representations. However, conventional skeletonization techniques often struggle to extract reliable centerlines due to the CoW's complex geometry, and publicly available centerline datasets remain scarce. To address these challenges, we used a thinning-based skeletonization algorithm to extract and curate centerline graphs and morphometric features from the TopCoW dataset, which includes 200 stroke patients, each imaged with MRA and CTA. The curated graphs were used to develop a baseline algorithm for centerline and feature extraction, combining U-Net-based skeletonization with A* graph connection. Performance was evaluated on a held-out test set, focusing on anatomical accuracy and feature robustness. Further, we used the extracted features to predict the frequency of fetal PCA variants, confirm theoretical bifurcation optimality relations, and detect subtle modality differences. The baseline algorithm consistently reconstructed graph topology with high accuracy (F1 = 1), and the average Euclidean node distance between reference and predicted graphs was below one voxel. Features such as segment radius, length, and bifurcation ratios showed strong robustness, with median relative errors below 5% and Pearson correlations above 0.95. Our results demonstrate the utility of learning-based skeletonization combined with graph connection for anatomically plausible centerline extraction. We emphasize the importance of going beyond simple voxel-based measures by evaluating anatomical accuracy and feature robustness. The dataset and baseline algorithm have been released to support further method development and clinical research.",
    "summary": "",
    "translation": "Willis环中心线图：数据集与基线算法",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及医学影像领域（Willis环是大脑中的血管结构），专注于血管中心线提取的数据集和算法。这与我的关注领域（推荐系统、搜索、广告及相关的LLM/Transformer技术）完全无关，属于明确的医学应用范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13714v1": {
    "title": "Dedelayed: Deleting remote inference delay via on-device correction",
    "url": "https://www.alphaxiv.org/abs/2510.13714v1",
    "arxiv_id": "2510.13714v1",
    "authors": "Dan Jacobellis, Mateen Ulhaq, Fabien Racapé, Hyomin Choi, Neeraja J. Yadwadkar",
    "categories": "eess.IV, cs.AI, cs.CV, cs.LG",
    "pub_date": "2025-10-15 16:13:44",
    "ori_summary": "Remote inference allows lightweight devices to leverage powerful cloud models. However, communication network latency makes predictions stale and unsuitable for real-time tasks. To address this, we introduce Dedelayed, a delay-corrective method that mitigates arbitrary remote inference delays, allowing the local device to produce low-latency outputs in real time. Our method employs a lightweight local model that processes the current frame and fuses in features that a heavyweight remote model computes from past frames. On video from the BDD100K driving dataset, Dedelayed improves semantic segmentation accuracy over the stronger of the local-only and remote-only baselines across all realistic communication network delays beyond 33 ms. Without incurring additional delay, it improves accuracy by 6.4 mIoU compared to fully local inference and 9.8 mIoU compared to remote inference, for a round-trip delay of 100 ms. The advantage grows under longer delays and higher-motion scenes, as delay-mitigated split inference sustains accuracy more effectively, providing clear advantages for real-time tasks that must remain aligned with the current world state.",
    "summary": "",
    "translation": "Dedelayed：通过设备端校正消除远程推理延迟",
    "relevance_score": 3,
    "reasoning": "该论文关注于通过设备端校正来减少远程推理延迟，这属于边缘计算和推理优化的范畴。虽然延迟优化在推荐和广告系统中具有潜在价值，但论文标题未明确表明与LLM、Transformer架构或推荐系统核心技术的直接关联。设备端推理优化可能应用于移动推荐场景，但相关性较弱且不够具体。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13702v1": {
    "title": "MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering and Completion",
    "url": "https://www.alphaxiv.org/abs/2510.13702v1",
    "arxiv_id": "2510.13702v1",
    "authors": "Minjung Shin, Hyunin Cho, Sooyeon Go, Jin-Hwa Kim, Youngjung Uh",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 16:00:26",
    "ori_summary": "Multi-view generation with camera pose control and prompt-based customization are both essential elements for achieving controllable generative models. However, existing multi-view generation models do not support customization with geometric consistency, whereas customization models lack explicit viewpoint control, making them challenging to unify. Motivated by these gaps, we introduce a novel task, multi-view customization, which aims to jointly achieve multi-view camera pose control and customization. Due to the scarcity of training data in customization, existing multi-view generation models, which inherently rely on large-scale datasets, struggle to generalize to diverse prompts. To address this, we propose MVCustom, a novel diffusion-based framework explicitly designed to achieve both multi-view consistency and customization fidelity. In the training stage, MVCustom learns the subject's identity and geometry using a feature-field representation, incorporating the text-to-video diffusion backbone enhanced with dense spatio-temporal attention, which leverages temporal coherence for multi-view consistency. In the inference stage, we introduce two novel techniques: depth-aware feature rendering explicitly enforces geometric consistency, and consistent-aware latent completion ensures accurate perspective alignment of the customized subject and surrounding backgrounds. Extensive experiments demonstrate that MVCustom is the only framework that simultaneously achieves faithful multi-view generation and customization.",
    "summary": "",
    "translation": "MVCustom：通过几何潜在渲染与补全实现多视图定制化扩散",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及多视图扩散模型和几何潜在渲染，属于计算机视觉和3D生成领域。虽然扩散模型是LLM相关技术，但该工作的核心应用场景是3D内容生成和视图定制，与推荐系统、搜索或广告的排序任务没有直接关联。其技术路径更偏向纯粹的视觉生成，而非我关注的异构数据建模或推荐应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13698v1": {
    "title": "Risk-adaptive Activation Steering for Safe Multimodal Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13698v1",
    "arxiv_id": "2510.13698v1",
    "authors": "Jonghyun Park, Minhyuk Seo, Jonghyun Choi",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 15:57:17",
    "ori_summary": "One of the key challenges of modern AI models is ensuring that they provide helpful responses to benign queries while refusing malicious ones. But often, the models are vulnerable to multimodal queries with harmful intent embedded in images. One approach for safety alignment is training with extensive safety datasets at the significant costs in both dataset curation and training. Inference-time alignment mitigates these costs, but introduces two drawbacks: excessive refusals from misclassified benign queries and slower inference speed due to iterative output adjustments. To overcome these limitations, we propose to reformulate queries to strengthen cross-modal attention to safety-critical image regions, enabling accurate risk assessment at the query level. Using the assessed risk, it adaptively steers activations to generate responses that are safe and helpful without overhead from iterative output adjustments. We call this Risk-adaptive Activation Steering (RAS). Extensive experiments across multiple benchmarks on multimodal safety and utility demonstrate that the RAS significantly reduces attack success rates, preserves general task performance, and improves inference speed over prior inference-time defenses.",
    "summary": "",
    "translation": "面向安全多模态大语言模型的风险自适应激活导向",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态LLM的安全性问题，属于安全控制机制的研究。虽然涉及多模态和激活导向技术，但其核心焦点是安全风险控制而非推荐/搜索/广告系统的性能提升。安全主题属于明确排除的无关主题范畴，且论文未展示在推荐系统或搜索广告中的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13684v1": {
    "title": "Generating healthy counterfactuals with denoising diffusion bridge models",
    "url": "https://www.alphaxiv.org/abs/2510.13684v1",
    "arxiv_id": "2510.13684v1",
    "authors": "Ana Lawry Aguila, Peirong Liu, Marina Crespo Aguirre, Juan Eugenio Iglesias",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 15:40:57",
    "ori_summary": "Generating healthy counterfactuals from pathological images holds significant promise in medical imaging, e.g., in anomaly detection or for application of analysis tools that are designed for healthy scans. These counterfactuals should represent what a patient's scan would plausibly look like in the absence of pathology, preserving individual anatomical characteristics while modifying only the pathological regions. Denoising diffusion probabilistic models (DDPMs) have become popular methods for generating healthy counterfactuals of pathology data. Typically, this involves training on solely healthy data with the assumption that a partial denoising process will be unable to model disease regions and will instead reconstruct a closely matched healthy counterpart. More recent methods have incorporated synthetic pathological images to better guide the diffusion process. However, it remains challenging to guide the generative process in a way that effectively balances the removal of anomalies with the retention of subject-specific features. To solve this problem, we propose a novel application of denoising diffusion bridge models (DDBMs) - which, unlike DDPMs, condition the diffusion process not only on the initial point (i.e., the healthy image), but also on the final point (i.e., a corresponding synthetically generated pathological image). Treating the pathological image as a structurally informative prior enables us to generate counterfactuals that closely match the patient's anatomy while selectively removing pathology. The results show that our DDBM outperforms previously proposed diffusion models and fully supervised approaches at segmentation and anomaly detection tasks.",
    "summary": "",
    "translation": "使用去噪扩散桥模型生成健康反事实",
    "relevance_score": 2,
    "reasoning": "该论文主要关注健康领域的反事实生成，属于特定领域应用而非核心推荐系统、搜索或广告技术。虽然扩散模型是生成式AI的重要技术，但论文的应用场景（健康领域）和焦点（反事实生成）与当前关注的核心领域进展和直接LLM应用相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13678v1": {
    "title": "FlashWorld: High-quality 3D Scene Generation within Seconds",
    "url": "https://www.alphaxiv.org/abs/2510.13678v1",
    "arxiv_id": "2510.13678v1",
    "authors": "Xinyang Li, Tengfei Wang, Zixiao Gu, Shengchuan Zhang, Chunchao Guo, Liujuan Cao",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 15:35:48",
    "ori_summary": "We propose FlashWorld, a generative model that produces 3D scenes from a single image or text prompt in seconds, 10~100$\\times$ faster than previous works while possessing superior rendering quality. Our approach shifts from the conventional multi-view-oriented (MV-oriented) paradigm, which generates multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach where the model directly produces 3D Gaussian representations during multi-view generation. While ensuring 3D consistency, 3D-oriented method typically suffers poor visual quality. FlashWorld includes a dual-mode pre-training phase followed by a cross-mode post-training phase, effectively integrating the strengths of both paradigms. Specifically, leveraging the prior from a video diffusion model, we first pre-train a dual-mode multi-view diffusion model, which jointly supports MV-oriented and 3D-oriented generation modes. To bridge the quality gap in 3D-oriented generation, we further propose a cross-mode post-training distillation by matching distribution from consistent 3D-oriented mode to high-quality MV-oriented mode. This not only enhances visual quality while maintaining 3D consistency, but also reduces the required denoising steps for inference. Also, we propose a strategy to leverage massive single-view images and text prompts during this process to enhance the model's generalization to out-of-distribution inputs. Extensive experiments demonstrate the superiority and efficiency of our method.",
    "summary": "",
    "translation": "FlashWorld：数秒内生成高质量3D场景",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D场景生成技术，属于计算机图形学领域，与推荐系统、搜索或广告的核心技术栈无直接关联。3D生成技术主要应用于游戏、虚拟现实和数字孪生等场景，无法为推荐排序、用户意图理解或广告投放等核心业务提供技术支撑。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13675v1": {
    "title": "Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning",
    "url": "https://www.alphaxiv.org/abs/2510.13675v1",
    "arxiv_id": "2510.13675v1",
    "authors": "Hongkuan Zhou, Lavdim Halilaj, Sebastian Monka, Stefan Schmid, Yuqicheng Zhu, Jingcheng Wu, Nadeem Nazer, Steffen Staab",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-15 15:33:36",
    "ori_summary": "Open-domain visual entity recognition aims to identify and link entities depicted in images to a vast and evolving set of real-world concepts, such as those found in Wikidata. Unlike conventional classification tasks with fixed label sets, it operates under open-set conditions, where most target entities are unseen during training and exhibit long-tail distributions. This makes the task inherently challenging due to limited supervision, high visual ambiguity, and the need for semantic disambiguation. In this work, we propose a Knowledge-guided Contrastive Learning (KnowCoL) framework that combines both images and text descriptions into a shared semantic space grounded by structured information from Wikidata. By abstracting visual and textual inputs to a conceptual level, the model leverages entity descriptions, type hierarchies, and relational context to support zero-shot entity recognition. We evaluate our approach on the OVEN benchmark, a large-scale open-domain visual recognition dataset with Wikidata IDs as the label space. Our experiments show that using visual, textual, and structured knowledge greatly improves accuracy, especially for rare and unseen entities. Our smallest model improves the accuracy on unseen entities by 10.5% compared to the state-of-the-art, despite being 35 times smaller.",
    "summary": "",
    "translation": "野外视觉与认知：基于对比学习的大规模知识图谱开放域视觉实体识别",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视觉实体识别与知识图谱的结合，属于视觉-语言多模态领域，与VLM类比异构数据的理念有一定关联。然而，其核心应用场景偏向通用视觉识别而非推荐/搜索/广告中的异构数据处理，潜在应用有限，仅能间接启发多模态特征融合方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13670v1": {
    "title": "NTIRE 2025 Challenge on Low Light Image Enhancement: Methods and Results",
    "url": "https://www.alphaxiv.org/abs/2510.13670v1",
    "arxiv_id": "2510.13670v1",
    "authors": "Xiaoning Liu, Zongwei Wu, Florin-Alexandru Vasluianu, Hailong Yan, Bin Ren, Yulun Zhang, Shuhang Gu, Le Zhang, Ce Zhu, Radu Timofte, Kangbiao Shi, Yixu Feng, Tao Hu, Yu Cao, Peng Wu, Yijin Liang, Yanning Zhang, Qingsen Yan, Han Zhou, Wei Dong, Yan Min, Mohab Kishawy, Jun Chen, Pengpeng Yu, Anjin Park, Seung-Soo Lee, Young-Joon Park, Zixiao Hu, Junyv Liu, Huilin Zhang, Jun Zhang, Fei Wan, Bingxin Xu, Hongzhe Liu, Cheng Xu, Weiguo Pan, Songyin Dai, Xunpeng Yi, Qinglong Yan, Yibing Zhang, Jiayi Ma, Changhui Hu, Kerui Hu, Donghang Jing, Tiesheng Chen, Zhi Jin, Hongjun Wu, Biao Huang, Haitao Ling, Jiahao Wu, Dandan Zhan, G Gyaneshwar Rao, Vijayalaxmi Ashok Aralikatti, Nikhil Akalwadi, Ramesh Ashok Tabib, Uma Mudenagudi, Ruirui Lin, Guoxi Huang, Nantheera Anantrasirichai, Qirui Yang, Alexandru Brateanu, Ciprian Orhei, Cosmin Ancuti, Daniel Feijoo, Juan C. Benito, Álvaro García, Marcos V. Conde, Yang Qin, Raul Balmez, Anas M. Ali, Bilel Benjdira, Wadii Boulila, Tianyi Mao, Huan Zheng, Yanyan Wei, Shengeng Tang, Dan Guo, Zhao Zhang, Sabari Nathan, K Uma, A Sasithradevi, B Sathya Bama, S. Mohamed Mansoor Roomi, Ao Li, Xiangtao Zhang, Zhe Liu, Yijie Tang, Jialong Tang, Zhicheng Fu, Gong Chen, Joe Nasti, John Nicholson, Zeyu Xiao, Zhuoyuan Li, Ashutosh Kulkarni, Prashant W. Patil, Santosh Kumar Vipparthi, Subrahmanyam Murala, Duan Liu, Weile Li, Hangyuan Lu, Rixian Liu, Tengfeng Wang, Jinxing Liang, Chenxin Yu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 15:30:16",
    "ori_summary": "This paper presents a comprehensive review of the NTIRE 2025 Low-Light Image Enhancement (LLIE) Challenge, highlighting the proposed solutions and final outcomes. The objective of the challenge is to identify effective networks capable of producing brighter, clearer, and visually compelling images under diverse and challenging conditions. A remarkable total of 762 participants registered for the competition, with 28 teams ultimately submitting valid entries. This paper thoroughly evaluates the state-of-the-art advancements in LLIE, showcasing the significant progress.",
    "summary": "",
    "translation": "NTIRE 2025低光照图像增强挑战赛：方法与结果",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的低光照图像增强技术，属于纯粹的视觉处理任务。虽然图像质量提升在某些广告或内容场景中有间接价值，但该研究本身不涉及推荐系统、搜索或广告的核心技术，也没有与Transformer架构、LLM技术或异构数据建模的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13669v1": {
    "title": "CanvasMAR: Improving Masked Autoregressive Video Generation With Canvas",
    "url": "https://www.alphaxiv.org/abs/2510.13669v1",
    "arxiv_id": "2510.13669v1",
    "authors": "Zian Li, Muhan Zhang",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-15 15:29:09",
    "ori_summary": "Masked autoregressive models (MAR) have recently emerged as a powerful paradigm for image and video generation, combining the flexibility of masked modeling with the potential of continuous tokenizer. However, video MAR models suffer from two major limitations: the slow-start problem, caused by the lack of a structured global prior at early sampling stages, and error accumulation across the autoregression in both spatial and temporal dimensions. In this work, we propose CanvasMAR, a novel video MAR model that mitigates these issues by introducing a canvas mechanism--a blurred, global prediction of the next frame, used as the starting point for masked generation. The canvas provides global structure early in sampling, enabling faster and more coherent frame synthesis. Furthermore, we introduce compositional classifier-free guidance that jointly enlarges spatial (canvas) and temporal conditioning, and employ noise-based canvas augmentation to enhance robustness. Experiments on the BAIR and Kinetics-600 benchmarks demonstrate that CanvasMAR produces high-quality videos with fewer autoregressive steps. Our approach achieves remarkable performance among autoregressive models on Kinetics-600 dataset and rivals diffusion-based methods.",
    "summary": "",
    "translation": "CanvasMAR：通过画布改进掩码自回归视频生成",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于视频生成技术，属于纯粹的视觉生成领域，与推荐系统、搜索或广告的核心技术没有直接关联。尽管涉及自回归模型，但该技术主要面向视频内容生成，没有明确的推荐、搜索或广告应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13660v1": {
    "title": "OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild",
    "url": "https://www.alphaxiv.org/abs/2510.13660v1",
    "arxiv_id": "2510.13660v1",
    "authors": "Hongyu Qu, Jianan Wei, Xiangbo Shu, Yazhou Yao, Wenguan Wang, Jinhui Tang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 15:19:52",
    "ori_summary": "Current 3D gaze estimation methods struggle to generalize across diverse data domains, primarily due to i) the scarcity of annotated datasets, and ii) the insufficient diversity of labeled data. In this work, we present OmniGaze, a semi-supervised framework for 3D gaze estimation, which utilizes large-scale unlabeled data collected from diverse and unconstrained real-world environments to mitigate domain bias and generalize gaze estimation in the wild. First, we build a diverse collection of unlabeled facial images, varying in facial appearances, background environments, illumination conditions, head poses, and eye occlusions. In order to leverage unlabeled data spanning a broader distribution, OmniGaze adopts a standard pseudo-labeling strategy and devises a reward model to assess the reliability of pseudo labels. Beyond pseudo labels as 3D direction vectors, the reward model also incorporates visual embeddings extracted by an off-the-shelf visual encoder and semantic cues from gaze perspective generated by prompting a Multimodal Large Language Model to compute confidence scores. Then, these scores are utilized to select high-quality pseudo labels and weight them for loss computation. Extensive experiments demonstrate that OmniGaze achieves state-of-the-art performance on five datasets under both in-domain and cross-domain settings. Furthermore, we also evaluate the efficacy of OmniGaze as a scalable data engine for gaze estimation, which exhibits robust zero-shot generalization on four unseen datasets.",
    "summary": "",
    "translation": "OmniGaze：基于奖励启发的野外可泛化视线估计",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的视线估计技术，属于纯粹的视觉研究方向。虽然标题提到'奖励启发'，但这与推荐系统、搜索或广告中的奖励机制无关，而是视觉任务中的技术方法。该工作没有展示与异构数据建模、Transformer架构改进或LLM技术应用的明显关联，因此与当前关注领域高度不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13643v1": {
    "title": "Towards Adversarial Robustness and Uncertainty Quantification in DINOv2-based Few-Shot Anomaly Detection",
    "url": "https://www.alphaxiv.org/abs/2510.13643v1",
    "arxiv_id": "2510.13643v1",
    "authors": "Akib Mohammed Khan, Bartosz Krawczyk",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 15:06:45",
    "ori_summary": "Foundation models such as DINOv2 have shown strong performance in few-shot anomaly detection, yet two key questions remain unexamined: (i) how susceptible are these detectors to adversarial perturbations; and (ii) how well do their anomaly scores reflect calibrated uncertainty? Building on AnomalyDINO, a training-free deep nearest-neighbor detector over DINOv2 features, we present one of the first systematic studies of adversarial attacks and uncertainty estimation in this setting. To enable white-box gradient attacks while preserving test-time behavior, we attach a lightweight linear head to frozen DINOv2 features only for crafting perturbations. Using this heuristic, we evaluate the impact of FGSM across the MVTec-AD and VisA datasets and observe consistent drops in F1, AUROC, AP, and G-mean, indicating that imperceptible perturbations can flip nearest-neighbor relations in feature space to induce confident misclassification. Complementing robustness, we probe reliability and find that raw anomaly scores are poorly calibrated, revealing a gap between confidence and correctness that limits safety-critical use. As a simple, strong baseline toward trustworthiness, we apply post-hoc Platt scaling to the anomaly scores for uncertainty estimation. The resulting calibrated posteriors yield significantly higher predictive entropy on adversarially perturbed inputs than on clean ones, enabling a practical flagging mechanism for attack detection while reducing calibration error (ECE). Our findings surface concrete vulnerabilities in DINOv2-based few-shot anomaly detectors and establish an evaluation protocol and baseline for robust, uncertainty-aware anomaly detection. We argue that adversarial robustness and principled uncertainty quantification are not optional add-ons but essential capabilities if anomaly detection systems are to be trustworthy and ready for real-world deployment.",
    "summary": "",
    "translation": "基于DINOv2的小样本异常检测中的对抗鲁棒性与不确定性量化研究",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的异常检测，涉及对抗鲁棒性和不确定性量化等安全相关主题。虽然DINOv2是视觉Transformer模型，但论文焦点在异常检测而非推荐/搜索/广告应用，且对抗鲁棒性属于安全范畴，属于应排除的主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13638v1": {
    "title": "Challenges, Advances, and Evaluation Metrics in Medical Image Enhancement: A Systematic Literature Review",
    "url": "https://www.alphaxiv.org/abs/2510.13638v1",
    "arxiv_id": "2510.13638v1",
    "authors": "Chun Wai Chin, Haniza Yazid, Hoi Leong Lee",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 15:01:58",
    "ori_summary": "Medical image enhancement is crucial for improving the quality and interpretability of diagnostic images, ultimately supporting early detection, accurate diagnosis, and effective treatment planning. Despite advancements in imaging technologies such as X-ray, CT, MRI, and ultrasound, medical images often suffer from challenges like noise, artifacts, and low contrast, which limit their diagnostic potential. Addressing these challenges requires robust preprocessing, denoising algorithms, and advanced enhancement methods, with deep learning techniques playing an increasingly significant role. This systematic literature review, following the PRISMA approach, investigates the key challenges, recent advancements, and evaluation metrics in medical image enhancement. By analyzing findings from 39 peer-reviewed studies, this review provides insights into the effectiveness of various enhancement methods across different imaging modalities and the importance of evaluation metrics in assessing their impact. Key issues like low contrast and noise are identified as the most frequent, with MRI and multi-modal imaging receiving the most attention, while specialized modalities such as histopathology, endoscopy, and bone scintigraphy remain underexplored. Out of the 39 studies, 29 utilize conventional mathematical methods, 9 focus on deep learning techniques, and 1 explores a hybrid approach. In terms of image quality assessment, 18 studies employ both reference-based and non-reference-based metrics, 9 rely solely on reference-based metrics, and 12 use only non-reference-based metrics, with a total of 65 IQA metrics introduced, predominantly non-reference-based. This review highlights current limitations, research gaps, and potential future directions for advancing medical image enhancement.",
    "summary": "",
    "translation": "医学图像增强中的挑战、进展与评估指标：系统性文献综述",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像增强领域，属于明确的医学领域特定应用，与搜索、推荐、广告等核心领域完全无关。论文内容涉及医学图像处理技术，属于被明确排除的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13630v1": {
    "title": "AVAR-Net: A Lightweight Audio-Visual Anomaly Recognition Framework with a Benchmark Dataset",
    "url": "https://www.alphaxiv.org/abs/2510.13630v1",
    "arxiv_id": "2510.13630v1",
    "authors": "Amjid Ali, Zulfiqar Ahmad Khan, Altaf Hussain, Muhammad Munsif, Adnan Hussain, Sung Wook Baik",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 14:56:00",
    "ori_summary": "Anomaly recognition plays a vital role in surveillance, transportation, healthcare, and public safety. However, most existing approaches rely solely on visual data, making them unreliable under challenging conditions such as occlusion, low illumination, and adverse weather. Moreover, the absence of large-scale synchronized audio-visual datasets has hindered progress in multimodal anomaly recognition. To address these limitations, this study presents AVAR-Net, a lightweight and efficient audio-visual anomaly recognition framework designed for real-world environments. AVAR-Net consists of four main modules: an audio feature extractor, a video feature extractor, fusion strategy, and a sequential pattern learning network that models cross-modal relationships for anomaly recognition. Specifically, the Wav2Vec2 model extracts robust temporal features from raw audio, while MobileViT captures both local and global visual representations from video frames. An early fusion mechanism combines these modalities, and a Multi-Stage Temporal Convolutional Network (MTCN) model that learns long-range temporal dependencies within the fused representation, enabling robust spatiotemporal reasoning. A novel Visual-Audio Anomaly Recognition (VAAR) dataset, is also introduced, serving as a medium-scale benchmark containing 3,000 real-world videos with synchronized audio across ten diverse anomaly classes. Experimental evaluations demonstrate that AVAR-Net achieves 89.29% accuracy on VAAR and 88.56% Average Precision on the XD-Violence dataset, improving Average Precision by 2.8% over existing state-of-the-art methods. These results highlight the effectiveness, efficiency, and generalization capability of the proposed framework, as well as the utility of VAAR as a benchmark for advancing multimodal anomaly recognition research.",
    "summary": "",
    "translation": "AVAR-Net：一种轻量级音视频异常识别框架及基准数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于音视频异常识别，属于纯粹的视觉和音频处理领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然提到了轻量级框架，但其应用场景局限于异常检测，无法为RecSys/Search/Ads领域提供可借鉴的技术或方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13620v1": {
    "title": "Fusion Meets Diverse Conditions: A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues",
    "url": "https://www.alphaxiv.org/abs/2510.13620v1",
    "arxiv_id": "2510.13620v1",
    "authors": "Chen Chen, Kangcheng Bin, Ting Hu, Jiahao Qi, Xingyue Liu, Tianpeng Liu, Zhen Liu, Yongxiang Liu, Ping Zhong",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 14:50:37",
    "ori_summary": "Unmanned aerial vehicles (UAV)-based object detection with visible (RGB) and infrared (IR) images facilitates robust around-the-clock detection, driven by advancements in deep learning techniques and the availability of high-quality dataset. However, the existing dataset struggles to fully capture real-world complexity for limited imaging conditions. To this end, we introduce a high-diversity dataset ATR-UMOD covering varying scenarios, spanning altitudes from 80m to 300m, angles from 0{\\deg} to 75{\\deg}, and all-day, all-year time variations in rich weather and illumination conditions. Moreover, each RGB-IR image pair is annotated with 6 condition attributes, offering valuable high-level contextual information. To meet the challenge raised by such diverse conditions, we propose a novel prompt-guided condition-aware dynamic fusion (PCDF) to adaptively reassign multimodal contributions by leveraging annotated condition cues. By encoding imaging conditions as text prompts, PCDF effectively models the relationship between conditions and multimodal contributions through a task-specific soft-gating transformation. A prompt-guided condition-decoupling module further ensures the availability in practice without condition annotations. Experiments on ATR-UMOD dataset reveal the effectiveness of PCDF.",
    "summary": "",
    "translation": "融合遇见多样条件：基于无人机多模态目标检测的高多样性基准与基线方法（含条件线索）",
    "relevance_score": 2,
    "reasoning": "该论文主要关注无人机多模态目标检测，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术关联性较弱。虽然涉及多模态融合技术，但其特定于无人机视觉检测的应用场景难以直接迁移到RecSys/Search/Ads领域，潜在应用价值有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13565v1": {
    "title": "XD-RCDepth: Lightweight Radar-Camera Depth Estimation with Explainability-Aligned and Distribution-Aware Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.13565v1",
    "arxiv_id": "2510.13565v1",
    "authors": "Huawei Sun, Zixu Wang, Xiangyuan Peng, Julius Ott, Georg Stettinger, Lorenzo Servadei, Robert Wille",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 14:05:33",
    "ori_summary": "Depth estimation remains central to autonomous driving, and radar-camera fusion offers robustness in adverse conditions by providing complementary geometric cues. In this paper, we present XD-RCDepth, a lightweight architecture that reduces the parameters by 29.7% relative to the state-of-the-art lightweight baseline while maintaining comparable accuracy. To preserve performance under compression and enhance interpretability, we introduce two knowledge-distillation strategies: an explainability-aligned distillation that transfers the teacher's saliency structure to the student, and a depth-distribution distillation that recasts depth regression as soft classification over discretized bins. Together, these components reduce the MAE compared with direct training with 7.97% and deliver competitive accuracy with real-time efficiency on nuScenes and ZJU-4DRadarCam datasets.",
    "summary": "",
    "translation": "XD-RCDepth：具有可解释性对齐和分布感知蒸馏的轻量级雷达-相机深度估计",
    "relevance_score": 2,
    "reasoning": "虽然论文涉及多模态融合（雷达和相机），但这主要属于计算机视觉领域的深度估计任务，与推荐系统、搜索或广告的核心技术没有直接关联。该技术可能在某些边缘计算场景中有应用，但缺乏明确的RecSys/Search/Ads应用场景，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13562v1": {
    "title": "An efficient approach with theoretical guarantees to simultaneously reconstruct activity and attenuation sinogram for TOF-PET",
    "url": "https://www.alphaxiv.org/abs/2510.13562v1",
    "arxiv_id": "2510.13562v1",
    "authors": "Liyang Hu, Chong Chen",
    "categories": "physics.med-ph, cs.CV, cs.NA, math.NA, 65J15, 65R32, 65J22, 68U10",
    "pub_date": "2025-10-15 14:01:03",
    "ori_summary": "In positron emission tomography (PET), it is indispensable to perform attenuation correction in order to obtain the quantitatively accurate activity map (tracer distribution) in the body. Generally, this is carried out based on the estimated attenuation map obtained from computed tomography or magnetic resonance imaging. However, except for errors in the attenuation correction factors obtained, the additional scan not only brings in new radiation doses and/or increases the scanning time but also leads to severe misalignment induced by various motions during and between the two sequential scans. To address these issues, based on maximum likelihood estimation, we propose a new mathematical model for simultaneously reconstructing the activity and attenuation sinogram from the time-of-flight (TOF)-PET emission data only. Particularly, we make full use of the exclusively exponential form for the attenuation correction factors, and consider the constraint of a total amount of the activity in some mask region in the proposed model. Furthermore, we prove its well-posedness, including the existence, uniqueness and stability of the solution. We propose an alternating update algorithm to solve the model, and also analyze its convergence. Finally, numerical experiments with various TOF-PET emission data demonstrate that the proposed method is of numerical convergence and robust to noise, and outperforms some state-of-the-art methods in terms of accuracy and efficiency, and has the capability of autonomous attenuation correction.",
    "summary": "",
    "translation": "一种具有理论保证的高效方法，用于同时重建飞行时间正电子发射断层扫描的活动和衰减正弦图",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于医学成像领域的正电子发射断层扫描（PET）重建技术，属于医学物理和生物医学工程的范畴。该主题与推荐系统、搜索、广告或LLM技术没有任何关联，完全超出了指定的关注范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13557v1": {
    "title": "Modeling Cultural Bias in Facial Expression Recognition with Adaptive Agents",
    "url": "https://www.alphaxiv.org/abs/2510.13557v1",
    "arxiv_id": "2510.13557v1",
    "authors": "David Freire-Obregón, José Salas-Cáceres, Javier Lorenzo-Navarro, Oliverio J. Santana, Daniel Hernández-Sosa, Modesto Castrillón-Santana",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 13:53:30",
    "ori_summary": "Facial expression recognition (FER) must remain robust under both cultural variation and perceptually degraded visual conditions, yet most existing evaluations assume homogeneous data and high-quality imagery. We introduce an agent-based, streaming benchmark that reveals how cross-cultural composition and progressive blurring interact to shape face recognition robustness. Each agent operates in a frozen CLIP feature space with a lightweight residual adapter trained online at sigma=0 and fixed during testing. Agents move and interact on a 5x5 lattice, while the environment provides inputs with sigma-scheduled Gaussian blur. We examine monocultural populations (Western-only, Asian-only) and mixed environments with balanced (5/5) and imbalanced (8/2, 2/8) compositions, as well as different spatial contact structures. Results show clear asymmetric degradation curves between cultural groups: JAFFE (Asian) populations maintain higher performance at low blur but exhibit sharper drops at intermediate stages, whereas KDEF (Western) populations degrade more uniformly. Mixed populations exhibit intermediate patterns, with balanced mixtures mitigating early degradation, but imbalanced settings amplify majority-group weaknesses under high blur. These findings quantify how cultural composition and interaction structure influence the robustness of FER as perceptual conditions deteriorate.",
    "summary": "",
    "translation": "基于自适应代理的面部表情识别中的文化偏见建模",
    "relevance_score": 2,
    "reasoning": "该论文主要关注面部表情识别中的文化偏见问题，这属于计算机视觉和公平性研究的交叉领域。虽然提到了建模和自适应技术，但其核心焦点是文化偏见这一非技术性话题，与推荐系统、搜索或广告的核心技术进展相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13546v1": {
    "title": "Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU",
    "url": "https://www.alphaxiv.org/abs/2510.13546v1",
    "arxiv_id": "2510.13546v1",
    "authors": "Ruiqi Ye, Mikel Luján",
    "categories": "cs.CV, cs.ET, cs.PF, cs.RO, C.3; C.4; I.4.6",
    "pub_date": "2025-10-15 13:40:55",
    "ori_summary": "Feature detection is a common yet time-consuming module in Simultaneous Localization and Mapping (SLAM) implementations, which are increasingly deployed on power-constrained platforms, such as drones. Graphics Processing Units (GPUs) have been a popular accelerator for computer vision in general, and feature detection and SLAM in particular. On the other hand, System-on-Chips (SoCs) with integrated Field Programmable Gate Array (FPGA) are also widely available. This paper presents the first study of hardware-accelerated feature detectors considering a Visual SLAM (V-SLAM) pipeline. We offer new insights by comparing the best GPU-accelerated FAST, Harris, and SuperPoint implementations against the FPGA-accelerated counterparts on modern SoCs (Nvidia Jetson Orin and AMD Versal). The evaluation shows that when using a non-learning-based feature detector such as FAST and Harris, their GPU implementations, and the GPU-accelerated V-SLAM can achieve better run-time performance and energy efficiency than the FAST and Harris FPGA implementations as well as the FPGA-accelerated V-SLAM. However, when considering a learning-based detector such as SuperPoint, its FPGA implementation can achieve better run-time performance and energy efficiency (up to 3.1$\\times$ and 1.4$\\times$ improvements, respectively) than the GPU implementation. The FPGA-accelerated V-SLAM can also achieve comparable run-time performance compared to the GPU-accelerated V-SLAM, with better FPS in 2 out of 5 dataset sequences. When considering the accuracy, the results show that the GPU-accelerated V-SLAM is more accurate than the FPGA-accelerated V-SLAM in general. Last but not least, the use of hardware acceleration for feature detection could further improve the performance of the V-SLAM pipeline by having the global bundle adjustment module invoked less frequently without sacrificing accuracy.",
    "summary": "",
    "translation": "视觉SLAM加速特征检测器：FPGA与GPU对比研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的SLAM（同时定位与地图构建）技术，主要研究硬件加速器（FPGA vs GPU）在视觉特征检测中的性能比较。这与搜索、推荐或广告系统的核心领域进展、LLM技术或Transformer架构没有直接关联，也不涉及异构数据的统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13540v1": {
    "title": "Learning Neural Parametric 3D Breast Shape Models for Metrical Surface Reconstruction From Monocular RGB Videos",
    "url": "https://www.alphaxiv.org/abs/2510.13540v1",
    "arxiv_id": "2510.13540v1",
    "authors": "Maximilian Weiherer, Antonia von Riedheim, Vanessa Brébant, Bernhard Egger, Christoph Palm",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 13:35:03",
    "ori_summary": "We present a neural parametric 3D breast shape model and, based on this model, introduce a low-cost and accessible 3D surface reconstruction pipeline capable of recovering accurate breast geometry from a monocular RGB video. In contrast to widely used, commercially available yet prohibitively expensive 3D breast scanning solutions and existing low-cost alternatives, our method requires neither specialized hardware nor proprietary software and can be used with any device that is able to record RGB videos. The key building blocks of our pipeline are a state-of-the-art, off-the-shelf Structure-from-motion pipeline, paired with a parametric breast model for robust and metrically correct surface reconstruction. Our model, similarly to the recently proposed implicit Regensburg Breast Shape Model (iRBSM), leverages implicit neural representations to model breast shapes. However, unlike the iRBSM, which employs a single global neural signed distance function (SDF), our approach -- inspired by recent state-of-the-art face models -- decomposes the implicit breast domain into multiple smaller regions, each represented by a local neural SDF anchored at anatomical landmark positions. When incorporated into our surface reconstruction pipeline, the proposed model, dubbed liRBSM (short for localized iRBSM), significantly outperforms the iRBSM in terms of reconstruction quality, yielding more detailed surface reconstruction than its global counterpart. Overall, we find that the introduced pipeline is able to recover high-quality 3D breast geometry within an error margin of less than 2 mm. Our method is fast (requires less than six minutes), fully transparent and open-source, and -- together with the model -- publicly available at https://rbsm.re-mic.de/local-implicit.",
    "summary": "",
    "translation": "学习神经参数化3D乳房形状模型用于从单目RGB视频进行度量表面重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D医学图像重建和计算机视觉，涉及乳房形状建模和度量表面重建。这与我的关注点完全无关，因为它是纯粹的医学/生物医学应用，属于明确排除的领域，与推荐系统、搜索或广告没有任何潜在联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13534v1": {
    "title": "High Semantic Features for the Continual Learning of Complex Emotions: a Lightweight Solution",
    "url": "https://www.alphaxiv.org/abs/2510.13534v1",
    "arxiv_id": "2510.13534v1",
    "authors": "Thibault Geoffroy, gauthier Gerspacher, Lionel Prevost",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 13:27:41",
    "ori_summary": "Incremental learning is a complex process due to potential catastrophic forgetting of old tasks when learning new ones. This is mainly due to transient features that do not fit from task to task. In this paper, we focus on complex emotion recognition. First, we learn basic emotions and then, incrementally, like humans, complex emotions. We show that Action Units, describing facial muscle movements, are non-transient, highly semantical features that outperform those extracted by both shallow and deep convolutional neural networks. Thanks to this ability, our approach achieves interesting results when learning incrementally complex, compound emotions with an accuracy of 0.75 on the CFEE dataset and can be favorably compared to state-of-the-art results. Moreover, it results in a lightweight model with a small memory footprint.",
    "summary": "",
    "translation": "用于复杂情感持续学习的高语义特征：一种轻量级解决方案",
    "relevance_score": 2,
    "reasoning": "该论文主要关注情感计算和持续学习，属于NLP领域的情感分析方向。虽然提及了轻量级解决方案，但论文的核心焦点是复杂情感建模，这与推荐系统、搜索或广告的核心技术需求关联度较低，缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13515v1": {
    "title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
    "url": "https://www.alphaxiv.org/abs/2510.13515v1",
    "arxiv_id": "2510.13515v1",
    "authors": "Tiancheng Gu, Kaicheng Yang, Kaichen Zhang, Xiang An, Ziyong Feng, Yueyi Zhang, Weidong Cai, Jiankang Deng, Lidong Bing",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 13:07:00",
    "ori_summary": "Universal multimodal embedding models are foundational to various tasks. Existing approaches typically employ in-batch negative mining by measuring the similarity of query-candidate pairs. However, these methods often struggle to capture subtle semantic differences among candidates and lack diversity in negative samples. Moreover, the embeddings exhibit limited discriminative ability in distinguishing false and hard negatives. In this paper, we leverage the advanced understanding capabilities of MLLMs to enhance representation learning and present a novel Universal Multimodal Embedding (UniME-V2) model. Our approach first constructs a potential hard negative set through global retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes MLLMs to assess the semantic alignment of query-candidate pairs and generate soft semantic matching scores. These scores serve as a foundation for hard negative mining, mitigating the impact of false negatives and enabling the identification of diverse, high-quality hard negatives. Furthermore, the semantic matching scores are used as soft labels to mitigate the rigid one-to-one mapping constraint. By aligning the similarity matrix with the soft semantic matching score matrix, the model learns semantic distinctions among candidates, significantly enhancing its discriminative capacity. To further improve performance, we propose UniME-V2-Reranker, a reranking model trained on our mined hard negatives through a joint pairwise and listwise optimization approach. We conduct comprehensive experiments on the MMEB benchmark and multiple retrieval tasks, demonstrating that our method achieves state-of-the-art performance on average across all tasks.",
    "summary": "",
    "translation": "UniME-V2：作为评判者的多模态大语言模型用于通用多模态嵌入学习",
    "relevance_score": 3,
    "reasoning": "该论文主要关注多模态大语言模型作为评判者用于嵌入学习，属于多模态领域的技术进展。虽然多模态嵌入技术可能应用于搜索中的跨模态检索，但论文标题未明确表明与推荐系统、搜索或广告的直接关联，且多模态大语言模型作为评判者的应用更偏向通用评估而非特定领域优化，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13493v1": {
    "title": "ExpressNet-MoE: A Hybrid Deep Neural Network for Emotion Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.13493v1",
    "arxiv_id": "2510.13493v1",
    "authors": "Deeptimaan Banerjee, Prateek Gothwal, Ashis Kumer Biswas",
    "categories": "cs.CV, cs.LG, I.2.10; I.5.2; H.4.2",
    "pub_date": "2025-10-15 12:42:49",
    "ori_summary": "In many domains, including online education, healthcare, security, and human-computer interaction, facial emotion recognition (FER) is essential. Real-world FER is still difficult despite its significance because of some factors such as variable head positions, occlusions, illumination shifts, and demographic diversity. Engagement detection, which is essential for applications like virtual learning and customer services, is frequently challenging due to FER limitations by many current models. In this article, we propose ExpressNet-MoE, a novel hybrid deep learning model that blends both Convolution Neural Networks (CNNs) and Mixture of Experts (MoE) framework, to overcome the difficulties. Our model dynamically chooses the most pertinent expert networks, thus it aids in the generalization and providing flexibility to model across a wide variety of datasets. Our model improves on the accuracy of emotion recognition by utilizing multi-scale feature extraction to collect both global and local facial features. ExpressNet-MoE includes numerous CNN-based feature extractors, a MoE module for adaptive feature selection, and finally a residual network backbone for deep feature learning. To demonstrate efficacy of our proposed model we evaluated on several datasets, and compared with current state-of-the-art methods. Our model achieves accuracies of 74.77% on AffectNet (v7), 72.55% on AffectNet (v8), 84.29% on RAF-DB, and 64.66% on FER-2013. The results show how adaptive our model is and how it may be used to develop end-to-end emotion recognition systems in practical settings. Reproducible codes and results are made publicly accessible at https://github.com/DeeptimaanB/ExpressNet-MoE.",
    "summary": "",
    "translation": "ExpressNet-MoE：一种用于情感识别的混合深度神经网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于情感识别这一特定应用领域，与推荐系统、搜索或广告的核心技术无关。虽然采用了混合专家（MoE）架构，但情感识别属于心理学和情感计算领域，没有明显的潜在应用可以关联到RecSys/Search/Ads的核心技术需求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13464v1": {
    "title": "Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.13464v1",
    "arxiv_id": "2510.13464v1",
    "authors": "Emily Miller, Michael Milford, Muhammad Burhan Hafez, SD Ramchurn, Shoaib Ehsan",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-15 12:12:55",
    "ori_summary": "Visual Place Recognition (VPR) enables robots and autonomous vehicles to identify previously visited locations by matching current observations against a database of known places. However, VPR systems face significant challenges when deployed across varying visual environments, lighting conditions, seasonal changes, and viewpoints changes. Failure-critical VPR applications, such as loop closure detection in simultaneous localization and mapping (SLAM) pipelines, require robust estimation of place matching uncertainty. We propose three training-free uncertainty metrics that estimate prediction confidence by analyzing inherent statistical patterns in similarity scores from any existing VPR method. Similarity Distribution (SD) quantifies match distinctiveness by measuring score separation between candidates; Ratio Spread (RS) evaluates competitive ambiguity among top-scoring locations; and Statistical Uncertainty (SU) is a combination of SD and RS that provides a unified metric that generalizes across datasets and VPR methods without requiring validation data to select the optimal metric. All three metrics operate without additional model training, architectural modifications, or computationally expensive geometric verification. Comprehensive evaluation across nine state-of-the-art VPR methods and six benchmark datasets confirms that our metrics excel at discriminating between correct and incorrect VPR matches, and consistently outperform existing approaches while maintaining negligible computational overhead, making it deployable for real-time robotic applications across varied environmental conditions with improved precision-recall performance.",
    "summary": "",
    "translation": "透过怀疑的视角：视觉位置识别的鲁棒高效不确定性估计",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉中的位置识别和不确定性估计，属于纯粹的视觉技术领域。虽然不确定性估计在推荐系统中可能有间接应用，但该论文没有展示与推荐、搜索或广告系统的明确联系，也没有涉及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13454v1": {
    "title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator",
    "url": "https://www.alphaxiv.org/abs/2510.13454v1",
    "arxiv_id": "2510.13454v1",
    "authors": "Hyojun Go, Dominik Narnhofer, Goutam Bhat, Prune Truong, Federico Tombari, Konrad Schindler",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 11:55:08",
    "ori_summary": "The rapid progress of large, pretrained models for both visual content generation and 3D reconstruction opens up new possibilities for text-to-3D generation. Intuitively, one could obtain a formidable 3D scene generator if one were able to combine the power of a modern latent text-to-video model as \"generator\" with the geometric abilities of a recent (feedforward) 3D reconstruction system as \"decoder\". We introduce VIST3A, a general framework that does just that, addressing two main challenges. First, the two components must be joined in a way that preserves the rich knowledge encoded in their weights. We revisit model stitching, i.e., we identify the layer in the 3D decoder that best matches the latent representation produced by the text-to-video generator and stitch the two parts together. That operation requires only a small dataset and no labels. Second, the text-to-video generator must be aligned with the stitched 3D decoder, to ensure that the generated latents are decodable into consistent, perceptually convincing 3D scene geometry. To that end, we adapt direct reward finetuning, a popular technique for human preference alignment. We evaluate the proposed VIST3A approach with different video generators and 3D reconstruction models. All tested pairings markedly improve over prior text-to-3D models that output Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also enables high-quality text-to-pointmap generation.",
    "summary": "",
    "translation": "VIST3A：通过将多视图重建网络与视频生成器拼接实现文本到3D生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本到3D生成和视频生成技术，属于计算机视觉和图形学领域。虽然涉及多模态处理，但其核心是3D内容生成和视频合成，与推荐系统、搜索或广告中的排名、检索、用户建模等核心任务没有直接关联。该技术主要面向AIGC和内容生成应用，属于明确排除的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13452v1": {
    "title": "Near-Infrared Hyperspectral Imaging Applications in Food Analysis -- Improving Algorithms and Methodologies",
    "url": "https://www.alphaxiv.org/abs/2510.13452v1",
    "arxiv_id": "2510.13452v1",
    "authors": "Ole-Christian Galbo Engstrøm",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-15 11:53:01",
    "ori_summary": "This thesis investigates the application of near-infrared hyperspectral imaging (NIR-HSI) for food quality analysis. The investigation is conducted through four studies operating with five research hypotheses. For several analyses, the studies compare models based on convolutional neural networks (CNNs) and partial least squares (PLS). Generally, joint spatio-spectral analysis with CNNs outperforms spatial analysis with CNNs and spectral analysis with PLS when modeling parameters where chemical and physical visual information are relevant. When modeling chemical parameters with a 2-dimensional (2D) CNN, augmenting the CNN with an initial layer dedicated to performing spectral convolution enhances its predictive performance by learning a spectral preprocessing similar to that applied by domain experts. Still, PLS-based spectral modeling performs equally well for analysis of the mean content of chemical parameters in samples and is the recommended approach. Modeling the spatial distribution of chemical parameters with NIR-HSI is limited by the ability to obtain spatially resolved reference values. Therefore, a study used bulk mean references for chemical map generation of fat content in pork bellies. A PLS-based approach gave non-smooth chemical maps and pixel-wise predictions outside the range of 0-100\\%. Conversely, a 2D CNN augmented with a spectral convolution layer mitigated all issues arising with PLS. The final study attempted to model barley's germinative capacity by analyzing NIR spectra, RGB images, and NIR-HSI images. However, the results were inconclusive due to the dataset's low degree of germination. Additionally, this thesis has led to the development of two open-sourced Python packages. The first facilitates fast PLS-based modeling, while the second facilitates very fast cross-validation of PLS and other classical machine learning models with a new algorithm.",
    "summary": "",
    "translation": "近红外高光谱成像在食品分析中的应用——算法与方法的改进",
    "relevance_score": 1,
    "reasoning": "该论文专注于食品分析领域的近红外高光谱成像技术，属于食品科学和化学分析领域，与推荐系统、搜索或广告的核心技术完全无关。论文内容涉及特定领域的传感器技术和分析方法，没有任何与LLM、Transformer架构或推荐系统相关的技术元素。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13441v1": {
    "title": "Steerable Conditional Diffusion for Domain Adaptation in PET Image Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2510.13441v1",
    "arxiv_id": "2510.13441v1",
    "authors": "George Webber, Alexander Hammers, Andrew P. King, Andrew J. Reader",
    "categories": "physics.med-ph, cs.CV, cs.LG",
    "pub_date": "2025-10-15 11:40:03",
    "ori_summary": "Diffusion models have recently enabled state-of-the-art reconstruction of positron emission tomography (PET) images while requiring only image training data. However, domain shift remains a key concern for clinical adoption: priors trained on images from one anatomy, acquisition protocol or pathology may produce artefacts on out-of-distribution data. We propose integrating steerable conditional diffusion (SCD) with our previously-introduced likelihood-scheduled diffusion (PET-LiSch) framework to improve the alignment of the diffusion model's prior to the target subject. At reconstruction time, for each diffusion step, we use low-rank adaptation (LoRA) to align the diffusion model prior with the target domain on the fly. Experiments on realistic synthetic 2D brain phantoms demonstrate that our approach suppresses hallucinated artefacts under domain shift, i.e. when our diffusion model is trained on perturbed images and tested on normal anatomy, our approach suppresses the hallucinated structure, outperforming both OSEM and diffusion model baselines qualitatively and quantitatively. These results provide a proof-of-concept that steerable priors can mitigate domain shift in diffusion-based PET reconstruction and motivate future evaluation on real data.",
    "summary": "",
    "translation": "用于PET图像重建领域自适应的可操控条件扩散模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（PET图像重建）领域，属于明确的无关主题范畴。虽然涉及扩散模型技术，但其应用场景与搜索、推荐、广告系统完全无关，且没有展示任何在推荐系统领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13433v1": {
    "title": "Beyond Pixels: A Differentiable Pipeline for Probing Neuronal Selectivity in 3D",
    "url": "https://www.alphaxiv.org/abs/2510.13433v1",
    "arxiv_id": "2510.13433v1",
    "authors": "Pavithra Elumalai, Mohammad Bashiri, Goirik Chakrabarty, Suhas Shrinivasan, Fabian H. Sinz",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 11:29:21",
    "ori_summary": "Visual perception relies on inference of 3D scene properties such as shape, pose, and lighting. To understand how visual sensory neurons enable robust perception, it is crucial to characterize their selectivity to such physically interpretable factors. However, current approaches mainly operate on 2D pixels, making it difficult to isolate selectivity for physical scene properties. To address this limitation, we introduce a differentiable rendering pipeline that optimizes deformable meshes to obtain MEIs directly in 3D. The method parameterizes mesh deformations with radial basis functions and learns offsets and scales that maximize neuronal responses while enforcing geometric regularity. Applied to models of monkey area V4, our approach enables probing neuronal selectivity to interpretable 3D factors such as pose and lighting. This approach bridges inverse graphics with systems neuroscience, offering a way to probe neural selectivity with physically grounded, 3D stimuli beyond conventional pixel-based methods.",
    "summary": "",
    "translation": "超越像素：一种用于探测3D神经元选择性的可微分流程",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D神经科学和神经元选择性分析，属于生物医学领域的神经科学研究。标题中提到的3D、神经元选择性等概念与推荐系统、搜索或广告的技术焦点完全无关，也没有任何潜在的Transformer架构或LLM应用前景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13432v1": {
    "title": "CoDS: Enhancing Collaborative Perception in Heterogeneous Scenarios via Domain Separation",
    "url": "https://www.alphaxiv.org/abs/2510.13432v1",
    "arxiv_id": "2510.13432v1",
    "authors": "Yushan Han, Hui Zhang, Honglei Zhang, Chuntao Ding, Yuanzhouhan Cao, Yidong Li",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 11:29:14",
    "ori_summary": "Collaborative perception has been proven to improve individual perception in autonomous driving through multi-agent interaction. Nevertheless, most methods often assume identical encoders for all agents, which does not hold true when these models are deployed in real-world applications. To realize collaborative perception in actual heterogeneous scenarios, existing methods usually align neighbor features to those of the ego vehicle, which is vulnerable to noise from domain gaps and thus fails to address feature discrepancies effectively. Moreover, they adopt transformer-based modules for domain adaptation, which causes the model inference inefficiency on mobile devices. To tackle these issues, we propose CoDS, a Collaborative perception method that leverages Domain Separation to address feature discrepancies in heterogeneous scenarios. The CoDS employs two feature alignment modules, i.e., Lightweight Spatial-Channel Resizer (LSCR) and Distribution Alignment via Domain Separation (DADS). Besides, it utilizes the Domain Alignment Mutual Information (DAMI) loss to ensure effective feature alignment. Specifically, the LSCR aligns the neighbor feature across spatial and channel dimensions using a lightweight convolutional layer. Subsequently, the DADS mitigates feature distribution discrepancy with encoder-specific and encoder-agnostic domain separation modules. The former removes domain-dependent information and the latter captures task-related information. During training, the DAMI loss maximizes the mutual information between aligned heterogeneous features to enhance the domain separation process. The CoDS employs a fully convolutional architecture, which ensures high inference efficiency. Extensive experiments demonstrate that the CoDS effectively mitigates feature discrepancies in heterogeneous scenarios and achieves a trade-off between detection accuracy and inference efficiency.",
    "summary": "",
    "translation": "CoDS：通过领域分离增强异构场景中的协同感知",
    "relevance_score": 3,
    "reasoning": "该论文主要关注计算机视觉领域的协同感知，虽然涉及异构场景处理，但其核心应用场景是自动驾驶等视觉感知任务，而非推荐系统、搜索或广告。虽然领域分离技术可能对处理异构数据有启发，但缺乏明确的RecSys/Search/Ads应用连接。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13419v1": {
    "title": "Ultra High-Resolution Image Inpainting with Patch-Based Content Consistency Adapter",
    "url": "https://www.alphaxiv.org/abs/2510.13419v1",
    "arxiv_id": "2510.13419v1",
    "authors": "Jianhui Zhang, Sheng Cheng, Qirui Sun, Jia Liu, Wang Luyang, Chaoyu Feng, Chen Fang, Lei Lei, Jue Wang, Shuaicheng Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 11:18:24",
    "ori_summary": "In this work, we present Patch-Adapter, an effective framework for high-resolution text-guided image inpainting. Unlike existing methods limited to lower resolutions, our approach achieves 4K+ resolution while maintaining precise content consistency and prompt alignment, two critical challenges in image inpainting that intensify with increasing resolution and texture complexity. Patch-Adapter leverages a two-stage adapter architecture to scale the diffusion model's resolution from 1K to 4K+ without requiring structural overhauls: (1) Dual Context Adapter learns coherence between masked and unmasked regions at reduced resolutions to establish global structural consistency; and (2) Reference Patch Adapter implements a patch-level attention mechanism for full-resolution inpainting, preserving local detail fidelity through adaptive feature fusion. This dual-stage architecture uniquely addresses the scalability gap in high-resolution inpainting by decoupling global semantics from localized refinement. Experiments demonstrate that Patch-Adapter not only resolves artifacts common in large-scale inpainting but also achieves state-of-the-art performance on the OpenImages and Photo-Concept-Bucket datasets, outperforming existing methods in both perceptual quality and text-prompt adherence.",
    "summary": "",
    "translation": "基于补丁内容一致性适配器的超高分辨率图像修复",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的图像修复技术，与推荐系统、搜索或广告的核心技术无关。虽然图像修复在某些边缘场景可能有应用（如广告素材处理），但这属于明确的无关主题范畴，且论文未提及任何与LLM、Transformer架构或推荐系统相关的技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13418v1": {
    "title": "Reinforcement Learning Meets Masked Generative Models: Mask-GRPO for Text-to-Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.13418v1",
    "arxiv_id": "2510.13418v1",
    "authors": "Yifu Luo, Xinhao Hu, Keyu Fan, Haoyuan Sun, Zeyu Chen, Bo Xia, Tiantian Zhang, Yongzhe Chang, Xueqian Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 11:18:12",
    "ori_summary": "Reinforcement learning (RL) has garnered increasing attention in text-to-image (T2I) generation. However, most existing RL approaches are tailored to either diffusion models or autoregressive models, overlooking an important alternative: masked generative models. In this work, we propose Mask-GRPO, the first method to incorporate Group Relative Policy Optimization (GRPO)-based RL into this overlooked paradigm. Our core insight is to redefine the transition probability, which is different from current approaches, and formulate the unmasking process as a multi-step decision-making problem. To further enhance our method, we explore several useful strategies, including removing the KL constraint, applying the reduction strategy, and filtering out low-quality samples. Using Mask-GRPO, we improve a base model, Show-o, with substantial improvements on standard T2I benchmarks and preference alignment, outperforming existing state-of-the-art approaches. The code is available on https://github.com/xingzhejun/Mask-GRPO",
    "summary": "",
    "translation": "强化学习遇见掩码生成模型：用于文本到图像生成的Mask-GRPO",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本到图像生成，这属于纯粹的AIGC和内容生成领域，与我的关注点无关。虽然涉及强化学习，但应用于图像生成任务，与推荐系统、搜索或广告没有明确关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13394v1": {
    "title": "Spatial-DISE: A Unified Benchmark for Evaluating Spatial Reasoning in Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13394v1",
    "arxiv_id": "2510.13394v1",
    "authors": "Xinmiao Huang, Qisong He, Zhenglin Huang, Boxuan Wang, Zhuoyun Li, Guangliang Cheng, Yi Dong, Xiaowei Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 10:44:01",
    "ori_summary": "Spatial reasoning ability is crucial for Vision Language Models (VLMs) to support real-world applications in diverse domains including robotics, augmented reality, and autonomous navigation. Unfortunately, existing benchmarks are inadequate in assessing spatial reasoning ability, especially the \\emph{intrinsic-dynamic} spatial reasoning which is a fundamental aspect of human spatial cognition. In this paper, we propose a unified benchmark, \\textbf{Spatial-DISE}, based on a cognitively grounded taxonomy that categorizes tasks into four fundamental quadrants: \\textbf{I}ntrinsic-\\textbf{S}tatic, Intrinsic-\\textbf{D}ynamic, \\textbf{E}xtrinsic-Static, and Extrinsic-Dynamic spatial reasoning. Moreover, to address the issue of data scarcity, we develop a scalable and automated pipeline to generate diverse and verifiable spatial reasoning questions, resulting in a new \\textbf{Spatial-DISE} dataset that includes Spatial-DISE Bench (559 evaluation VQA pairs) and Spatial-DISE-12K (12K+ training VQA pairs). Our comprehensive evaluation across 28 state-of-the-art VLMs reveals that, current VLMs have a large and consistent gap to human competence, especially on multi-step multi-view spatial reasoning. Spatial-DISE offers a robust framework, valuable dataset, and clear direction for future research toward human-like spatial intelligence. Benchmark, dataset, and code will be publicly released.",
    "summary": "",
    "translation": "Spatial-DISE：评估视觉语言模型中空间推理能力的统一基准",
    "relevance_score": 2,
    "reasoning": "该论文专注于视觉语言模型的评估基准，属于纯粹的VLM评估范畴，与推荐系统、搜索或广告的核心技术进展无关。虽然提到了空间推理，但缺乏将这些能力转化为推荐/搜索/广告应用的明确路径或潜力说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13390v1": {
    "title": "Generalizing WiFi Gesture Recognition via Large-Model-Aware Semantic Distillation and Alignment",
    "url": "https://www.alphaxiv.org/abs/2510.13390v1",
    "arxiv_id": "2510.13390v1",
    "authors": "Feng-Qi Cui, Yu-Tong Guo, Tianyue Zheng, Jinyang Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 10:28:50",
    "ori_summary": "WiFi-based gesture recognition has emerged as a promising RF sensing paradigm for enabling non-contact and privacy-preserving human-computer interaction in AIoT environments. However, existing methods often suffer from limited generalization and semantic expressiveness due to the domain-sensitive nature of Channel State Information and the lack of high-level gesture abstraction. To address these challenges, we propose a novel generalization framework, termed Large-Model-Aware Semantic Distillation and Alignment (GLSDA), which leverages the semantic prior of pre-trained large foundation models to enhance gesture representation learning in both in-domain and cross-domain scenarios. Specifically, we first design a dual-path CSI encoding pipeline that captures geometric and dynamic gesture patterns via CSI-Ratio phase sequences and Doppler spectrograms. These representations are then fed into a Multiscale Semantic Encoder, which learns robust temporal embeddings and aligns them with gesture semantics through cross-modal attention mechanisms. To further enhance category discrimination, we introduce a Semantic-Aware Soft Supervision scheme that encodes inter-class correlations and reduces label ambiguity, especially for semantically similar gestures. Finally, we develop a Robust Dual-Distillation strategy to compress the aligned model into a lightweight student network, jointly distilling intermediate features and semantic-informed soft labels from the teacher model. Extensive experiments on the Widar3.0 benchmark show that GLSDA consistently outperforms state-of-the-art methods in both in-domain and cross-domain gesture recognition tasks, while significantly reducing model size and inference latency. Our method offers a scalable and deployable solution for generalized RF-based gesture interfaces in real-world AIoT applications.",
    "summary": "",
    "translation": "通过大模型感知语义蒸馏与对齐实现WiFi手势识别的泛化",
    "relevance_score": 2,
    "reasoning": "该论文主要关注WiFi手势识别这一特定感知任务，属于计算机视觉和信号处理领域。虽然提到了大模型和语义蒸馏技术，但其应用场景（手势识别）与推荐系统、搜索或广告的核心技术栈关联性较弱，缺乏明确的跨领域应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13381v1": {
    "title": "Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering",
    "url": "https://www.alphaxiv.org/abs/2510.13381v1",
    "arxiv_id": "2510.13381v1",
    "authors": "Siddharth Tourani, Jayaram Reddy, Akash Kumbar, Satyajit Tourani, Nishant Goyal, Madhava Krishna, N. Dinesh Reddy, Muhammad Haris Khan",
    "categories": "cs.CV, cs.GR",
    "pub_date": "2025-10-15 10:21:36",
    "ori_summary": "Dynamic scene rendering and reconstruction play a crucial role in computer vision and augmented reality. Recent methods based on 3D Gaussian Splatting (3DGS), have enabled accurate modeling of dynamic urban scenes, but for urban scenes they require both camera and LiDAR data, ground-truth 3D segmentations and motion data in the form of tracklets or pre-defined object templates such as SMPL. In this work, we explore whether a combination of 2D object agnostic priors in the form of depth and point tracking coupled with a signed distance function (SDF) representation for dynamic objects can be used to relax some of these requirements. We present a novel approach that integrates Signed Distance Functions (SDFs) with 3D Gaussian Splatting (3DGS) to create a more robust object representation by harnessing the strengths of both methods. Our unified optimization framework enhances the geometric accuracy of 3D Gaussian splatting and improves deformation modeling within the SDF, resulting in a more adaptable and precise representation. We demonstrate that our method achieves state-of-the-art performance in rendering metrics even without LiDAR data on urban scenes. When incorporating LiDAR, our approach improved further in reconstructing and generating novel views across diverse object categories, without ground-truth 3D motion annotation. Additionally, our method enables various scene editing tasks, including scene decomposition, and scene composition.",
    "summary": "",
    "translation": "利用2D先验和SDF引导进行动态城市场景渲染",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉中的动态场景渲染技术，涉及2D先验和符号距离函数(SDF)引导。虽然标题提到城市场景，但这属于纯粹的视觉渲染领域，与推荐系统、搜索或广告的核心技术没有直接关联。该研究缺乏在RecSys/Search/Ads领域的潜在应用前景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13375v1": {
    "title": "DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.13375v1",
    "arxiv_id": "2510.13375v1",
    "authors": "Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Zhuoguang Chen, Tao Jiang, Hang Zhao",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 10:09:00",
    "ori_summary": "Vision-Language-Action (VLA) models have recently shown impressive generalization and language-guided manipulation capabilities. However, their performance degrades on tasks requiring precise spatial reasoning due to limited spatial reasoning inherited from Vision-Language Models (VLMs). Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3D space, which reduces training efficiency and is still insufficient for accurate spatial understanding. In this work, we present DepthVLA, a simple yet effective VLA architecture that explicitly incorporates spatial awareness through a pretrained depth prediction module. DepthVLA adopts a mixture-of-transformers design that unifies a VLM, a depth transformer, and an action expert with fully shared attentions, forming an end-to-end model with enhanced spatial reasoning. Extensive evaluations in both real-world and simulated environments show that DepthVLA outperforms state-of-the-art approaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs. 93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator. Our code will be made publicly available.",
    "summary": "",
    "translation": "DepthVLA：通过深度感知空间推理增强视觉-语言-动作模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉-语言-动作模型中的深度感知和空间推理，属于机器人学和具身AI领域。虽然提到了多模态建模概念，但其核心应用方向（机器人控制、物理交互）与推荐系统、搜索或广告的典型应用场景关联度很低，难以找到直接的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13364v1": {
    "title": "Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity",
    "url": "https://www.alphaxiv.org/abs/2510.13364v1",
    "arxiv_id": "2510.13364v1",
    "authors": "MingZe Tang, Jubal Chandy Jacob",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 09:53:46",
    "ori_summary": "Recent Vision-Language Models (VLMs) enable zero-shot classification by aligning images and text in a shared space, a promising approach for data-scarce conditions. However, the influence of prompt design on recognizing visually similar categories, such as human postures, is not well understood. This study investigates how prompt specificity affects the zero-shot classification of sitting, standing, and walking/running on a small, 285-image COCO-derived dataset. A suite of modern VLMs, including OpenCLIP, MetaCLIP 2, and SigLip, were evaluated using a three-tiered prompt design that systematically increases linguistic detail. Our findings reveal a compelling, counter-intuitive trend: for the highest-performing models (MetaCLIP 2 and OpenCLIP), the simplest, most basic prompts consistently achieve the best results. Adding descriptive detail significantly degrades performance for instance, MetaCLIP 2's multi-class accuracy drops from 68.8\\% to 55.1\\% a phenomenon we term \"prompt overfitting\". Conversely, the lower-performing SigLip model shows improved classification on ambiguous classes when given more descriptive, body-cue-based prompts.",
    "summary": "",
    "translation": "语言作为标签：数据稀缺下日常姿态的零样本多模态分类",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态分类和姿态识别，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术关联性较弱。虽然提到了多模态和零样本学习，但这些技术在当前论文中的应用场景（日常姿态分类）与RecSys/Search/Ads领域缺乏直接联系，潜在应用价值有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13349v1": {
    "title": "No-Reference Rendered Video Quality Assessment: Dataset and Metrics",
    "url": "https://www.alphaxiv.org/abs/2510.13349v1",
    "arxiv_id": "2510.13349v1",
    "authors": "Sipeng Yang, Jiayu Ji, Qingchuan Zhu, Zhiyao Yang, Xiaogang Jin",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 09:36:52",
    "ori_summary": "Quality assessment of videos is crucial for many computer graphics applications, including video games, virtual reality, and augmented reality, where visual performance has a significant impact on user experience. When test videos cannot be perfectly aligned with references or when references are unavailable, the significance of no-reference video quality assessment (NR-VQA) methods is undeniable. However, existing NR-VQA datasets and metrics are primarily focused on camera-captured videos; applying them directly to rendered videos would result in biased predictions, as rendered videos are more prone to temporal artifacts. To address this, we present a large rendering-oriented video dataset with subjective quality annotations, as well as a designed NR-VQA metric specific to rendered videos. The proposed dataset includes a wide range of 3D scenes and rendering settings, with quality scores annotated for various display types to better reflect real-world application scenarios. Building on this dataset, we calibrate our NR-VQA metric to assess rendered video quality by looking at both image quality and temporal stability. We compare our metric to existing NR-VQA metrics, demonstrating its superior performance on rendered videos. Finally, we demonstrate that our metric can be used to benchmark supersampling methods and assess frame generation strategies in real-time rendering.",
    "summary": "",
    "translation": "无参考渲染视频质量评估：数据集与指标",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频质量评估的特定计算机视觉任务，与推荐系统、搜索或广告的核心领域进展无关。虽然视频内容可能出现在某些推荐场景中，但该研究本身不涉及LLM技术、Transformer架构改进，也没有展示在推荐/搜索/广告中的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13331v1": {
    "title": "Group-Wise Optimization for Self-Extensible Codebooks in Vector Quantized Models",
    "url": "https://www.alphaxiv.org/abs/2510.13331v1",
    "arxiv_id": "2510.13331v1",
    "authors": "Hong-Kai Zheng, Piji Li",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 09:14:22",
    "ori_summary": "Vector Quantized Variational Autoencoders (VQ-VAEs) leverage self-supervised learning through reconstruction tasks to represent continuous vectors using the closest vectors in a codebook. However, issues such as codebook collapse persist in the VQ model. To address these issues, existing approaches employ implicit static codebooks or jointly optimize the entire codebook, but these methods constrain the codebook's learning capability, leading to reduced reconstruction quality. In this paper, we propose Group-VQ, which performs group-wise optimization on the codebook. Each group is optimized independently, with joint optimization performed within groups. This approach improves the trade-off between codebook utilization and reconstruction performance. Additionally, we introduce a training-free codebook resampling method, allowing post-training adjustment of the codebook size. In image reconstruction experiments under various settings, Group-VQ demonstrates improved performance on reconstruction metrics. And the post-training codebook sampling method achieves the desired flexibility in adjusting the codebook size.",
    "summary": "",
    "translation": "向量量化模型中自扩展码书的组优化方法",
    "relevance_score": 6,
    "reasoning": "该论文涉及向量量化模型的码书优化技术，属于Transformer架构效率改进的范畴。在推荐系统中，向量量化可用于高效的嵌入表示和检索，自扩展码书技术可提升模型对动态用户兴趣和物品特征的适应性，从而改善个性化推荐效果。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.13326v1": {
    "title": "DEF-YOLO: Leveraging YOLO for Concealed Weapon Detection in Thermal Imagin",
    "url": "https://www.alphaxiv.org/abs/2510.13326v1",
    "arxiv_id": "2510.13326v1",
    "authors": "Divya Bhardwaj, Arnav Ramamoorthy, Poonam Goyal",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 09:13:35",
    "ori_summary": "Concealed weapon detection aims at detecting weapons hidden beneath a person's clothing or luggage. Various imaging modalities like Millimeter Wave, Microwave, Terahertz, Infrared, etc., are exploited for the concealed weapon detection task. These imaging modalities have their own limitations, such as poor resolution in microwave imaging, privacy concerns in millimeter wave imaging, etc. To provide a real-time, 24 x 7 surveillance, low-cost, and privacy-preserved solution, we opted for thermal imaging in spite of the lack of availability of a benchmark dataset. We propose a novel approach and a dataset for concealed weapon detection in thermal imagery. Our YOLO-based architecture, DEF-YOLO, is built with key enhancements in YOLOv8 tailored to the unique challenges of concealed weapon detection in thermal vision. We adopt deformable convolutions at the SPPF layer to exploit multi-scale features; backbone and neck layers to extract low, mid, and high-level features, enabling DEF-YOLO to adaptively focus on localization around the objects in thermal homogeneous regions, without sacrificing much of the speed and throughput. In addition to these simple yet effective key architectural changes, we introduce a new, large-scale Thermal Imaging Concealed Weapon dataset, TICW, featuring a diverse set of concealed weapons and capturing a wide range of scenarios. To the best of our knowledge, this is the first large-scale contributed dataset for this task. We also incorporate focal loss to address the significant class imbalance inherent in the concealed weapon detection task. The efficacy of the proposed work establishes a new benchmark through extensive experimentation for concealed weapon detection in thermal imagery.",
    "summary": "",
    "translation": "DEF-YOLO：利用YOLO进行热成像中的隐蔽武器检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于热成像中的武器检测，属于纯粹的计算机视觉应用，与推荐系统、搜索或广告领域没有直接关联。虽然YOLO是目标检测模型，但该应用场景（隐蔽武器检测）属于安防领域，无法应用于RecSys/Search/Ads中的任何核心问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13317v1": {
    "title": "Removing Cost Volumes from Optical Flow Estimators",
    "url": "https://www.alphaxiv.org/abs/2510.13317v1",
    "arxiv_id": "2510.13317v1",
    "authors": "Simon Kiefhaber, Stefan Roth, Simone Schaub-Meyer",
    "categories": "cs.CV, I.4.8",
    "pub_date": "2025-10-15 09:07:09",
    "ori_summary": "Cost volumes are used in every modern optical flow estimator, but due to their computational and space complexity, they are often a limiting factor regarding both processing speed and the resolution of input frames. Motivated by our empirical observation that cost volumes lose their importance once all other network parts of, e.g., a RAFT-based pipeline have been sufficiently trained, we introduce a training strategy that allows removing the cost volume from optical flow estimators throughout training. This leads to significantly improved inference speed and reduced memory requirements. Using our training strategy, we create three different models covering different compute budgets. Our most accurate model reaches state-of-the-art accuracy while being $1.2\\times$ faster and having a $6\\times$ lower memory footprint than comparable models; our fastest model is capable of processing Full HD frames at $20\\,\\mathrm{FPS}$ using only $500\\,\\mathrm{MB}$ of GPU memory.",
    "summary": "",
    "translation": "从光流估计器中移除代价体",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉中的光流估计技术，属于纯粹的视觉处理领域。虽然光流在视频理解中有应用，但该论文的核心创新（移除代价体）是视觉算法优化，与推荐系统、搜索或广告中的排序、检索、用户建模等核心问题没有直接关联，也没有明显的跨模态应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13316v1": {
    "title": "Visual Interestingness Decoded: How GPT-4o Mirrors Human Interests",
    "url": "https://www.alphaxiv.org/abs/2510.13316v1",
    "arxiv_id": "2510.13316v1",
    "authors": "Fitim Abdullahu, Helmut Grabner",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 09:04:48",
    "ori_summary": "Our daily life is highly influenced by what we consume and see. Attracting and holding one's attention -- the definition of (visual) interestingness -- is essential. The rise of Large Multimodal Models (LMMs) trained on large-scale visual and textual data has demonstrated impressive capabilities. We explore these models' potential to understand to what extent the concepts of visual interestingness are captured and examine the alignment between human assessments and GPT-4o's, a leading LMM, predictions through comparative analysis. Our studies reveal partial alignment between humans and GPT-4o. It already captures the concept as best compared to state-of-the-art methods. Hence, this allows for the effective labeling of image pairs according to their (commonly) interestingness, which are used as training data to distill the knowledge into a learning-to-rank model. The insights pave the way for a deeper understanding of human interest.",
    "summary": "",
    "translation": "视觉趣味性解码：GPT-4o如何反映人类兴趣",
    "relevance_score": 3,
    "reasoning": "该论文研究GPT-4o对人类视觉兴趣的反映，主要涉及视觉理解和人类兴趣建模，但缺乏明确的推荐系统、搜索或广告应用场景。虽然视觉语言模型技术可能间接支持多模态推荐，但论文焦点更偏向纯粹的视觉理解和心理学研究，与当前关注的核心领域相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13315v1": {
    "title": "Self-Augmented Visual Contrastive Decoding",
    "url": "https://www.alphaxiv.org/abs/2510.13315v1",
    "arxiv_id": "2510.13315v1",
    "authors": "Eun Woo Im, Muhammad Kashif Ali, Vivek Gupta",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 09:03:34",
    "ori_summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal capabilities, but they inherit the tendency to hallucinate from their underlying language models. While visual contrastive decoding has been proposed to mitigate this issue, existing methods often apply generic visual augmentations that disregard the specific context provided by the text query, limiting their effectiveness. This study introduces a novel training-free decoding strategy that addresses these limitations, featuring two key contributions. First, a self-augmentation prompting strategy that leverages the intrinsic knowledge of the model to dynamically align semantics between the query and the visual augmentation. Second, an adaptive thresholding algorithm that adaptively adjusts next token candidate size based on the output sparsity, utilizing full information from the logit distribution. Extensive experiments across four LVLMs and seven benchmarks demonstrate that the proposed decoding significantly enhances factual consistency compared to state-of-the-art decoding methods. This work highlights the importance of integrating query-dependent augmentation and entropy-aware decoding for improving effective generation of LVLMs.",
    "summary": "",
    "translation": "自增强视觉对比解码",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉领域的解码技术，属于纯粹的视觉研究方向。虽然标题提到'对比解码'可能涉及一些通用技术，但没有明确证据表明该技术有在推荐系统、搜索或广告领域的潜在应用。该工作更偏向于计算机视觉而非多模态或异质数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13310v1": {
    "title": "InstantSfM: Fully Sparse and Parallel Structure-from-Motion",
    "url": "https://www.alphaxiv.org/abs/2510.13310v1",
    "arxiv_id": "2510.13310v1",
    "authors": "Jiankun Zhong, Zitong Zhan, Quankai Gao, Ziyu Chen, Haozhe Lou, Jiageng Mao, Ulrich Neumann, Yue Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 08:58:05",
    "ori_summary": "Structure-from-Motion (SfM), a method that recovers camera poses and scene geometry from uncalibrated images, is a central component in robotic reconstruction and simulation. Despite the state-of-the-art performance of traditional SfM methods such as COLMAP and its follow-up work, GLOMAP, naive CPU-specialized implementations of bundle adjustment (BA) or global positioning (GP) introduce significant computational overhead when handling large-scale scenarios, leading to a trade-off between accuracy and speed in SfM. Moreover, the blessing of efficient C++-based implementations in COLMAP and GLOMAP comes with the curse of limited flexibility, as they lack support for various external optimization options. On the other hand, while deep learning based SfM pipelines like VGGSfM and VGGT enable feed-forward 3D reconstruction, they are unable to scale to thousands of input views at once as GPU memory consumption increases sharply as the number of input views grows. In this paper, we unleash the full potential of GPU parallel computation to accelerate each critical stage of the standard SfM pipeline. Building upon recent advances in sparse-aware bundle adjustment optimization, our design extends these techniques to accelerate both BA and GP within a unified global SfM framework. Through extensive experiments on datasets of varying scales (e.g. 5000 images where VGGSfM and VGGT run out of memory), our method demonstrates up to about 40 times speedup over COLMAP while achieving consistently comparable or even improved reconstruction accuracy. Our project page can be found at https://cre185.github.io/InstantSfM/.",
    "summary": "",
    "translation": "InstantSfM：完全稀疏且并行的运动恢复结构方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的运动恢复结构（SfM）技术，这是一个纯粹的3D视觉和重建问题。虽然提到了稀疏性和并行化等效率改进，但这些优化特定于视觉重建流程，与推荐系统、搜索或广告中的排序、检索或用户建模没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13307v1": {
    "title": "Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.13307v1",
    "arxiv_id": "2510.13307v1",
    "authors": "Yang Li, Aming Wu, Zihao Zhang, Yahong Han",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 08:54:41",
    "ori_summary": "In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation (3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classes using only the supervision from labeled (base) 3D classes. The key to this task is to setup the exact correlations between the point representations and their base class labels, as well as the representation correlations between the points from base and novel classes. A coarse or statistical correlation learning may lead to the confusion in novel class inference. lf we impose a causal relationship as a strong correlated constraint upon the learning process, the essential point cloud representations that accurately correspond to the classes should be uncovered. To this end, we introduce a structural causal model (SCM) to re-formalize the 3D-NCD problem and propose a new method, i.e., Joint Learning of Causal Representation and Reasoning. Specifically, we first analyze hidden confounders in the base class representations and the causal relationships between the base and novel classes through SCM. We devise a causal representation prototype that eliminates confounders to capture the causal representations of base classes. A graph structure is then used to model the causal relationships between the base classes' causal representation prototypes and the novel class prototypes, enabling causal reasoning from base to novel classes. Extensive experiments and visualization results on 3D and 2D NCD semantic segmentation demonstrate the superiorities of our method.",
    "summary": "",
    "translation": "基于因果表示与推理联合学习的点云分割新类发现",
    "relevance_score": 1,
    "reasoning": "该论文专注于点云分割和计算机视觉领域的新类发现问题，与推荐系统、搜索或广告的核心技术无直接关联。论文涉及的因果表示学习虽然具有理论价值，但在当前标题描述中缺乏明确的推荐/搜索/广告应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13303v1": {
    "title": "Automated document processing system for government agencies using DBNET++ and BART models",
    "url": "https://www.alphaxiv.org/abs/2510.13303v1",
    "arxiv_id": "2510.13303v1",
    "authors": "Aya Kaysan Bahjat",
    "categories": "cs.CV, cs.GR",
    "pub_date": "2025-10-15 08:48:02",
    "ori_summary": "An automatic document classification system is presented that detects textual content in images and classifies documents into four predefined categories (Invoice, Report, Letter, and Form). The system supports both offline images (e.g., files on flash drives, HDDs, microSD) and real-time capture via connected cameras, and is designed to mitigate practical challenges such as variable illumination, arbitrary orientation, curved or partially occluded text, low resolution, and distant text. The pipeline comprises four stages: image capture and preprocessing, text detection [1] using a DBNet++ (Differentiable Binarization Network Plus) detector, and text classification [2] using a BART (Bidirectional and Auto-Regressive Transformers) classifier, all integrated within a user interface implemented in Python with PyQt5. The achieved results by the system for text detection in images were good at about 92.88% through 10 hours on Total-Text dataset that involve high resolution images simulate a various and very difficult challenges. The results indicate the proposed approach is effective for practical, mixed-source document categorization in unconstrained imaging scenarios.",
    "summary": "",
    "translation": "基于DBNET++和BART模型的政府机构自动化文档处理系统",
    "relevance_score": 2,
    "reasoning": "该论文主要关注政府文档处理的特定应用场景，虽然使用了BART模型，但其应用领域与搜索、推荐或广告系统无关。文档处理系统属于通用NLP应用范畴，没有展示在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13282v1": {
    "title": "Universal Image Restoration Pre-training via Masked Degradation Classification",
    "url": "https://www.alphaxiv.org/abs/2510.13282v1",
    "arxiv_id": "2510.13282v1",
    "authors": "JiaKui Hu, Zhengjian Yao, Lujia Jin, Yinghao Chen, Yanye Lu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 08:30:15",
    "ori_summary": "This study introduces a Masked Degradation Classification Pre-Training method (MaskDCPT), designed to facilitate the classification of degradation types in input images, leading to comprehensive image restoration pre-training. Unlike conventional pre-training methods, MaskDCPT uses the degradation type of the image as an extremely weak supervision, while simultaneously leveraging the image reconstruction to enhance performance and robustness. MaskDCPT includes an encoder and two decoders: the encoder extracts features from the masked low-quality input image. The classification decoder uses these features to identify the degradation type, whereas the reconstruction decoder aims to reconstruct a corresponding high-quality image. This design allows the pre-training to benefit from both masked image modeling and contrastive learning, resulting in a generalized representation suited for restoration tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained encoder can be used to address universal image restoration and achieve outstanding performance. Implementing MaskDCPT significantly improves performance for both convolution neural networks (CNNs) and Transformers, with a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and a 34.8% reduction in PIQE compared to baseline in real-world degradation scenarios. It also emergences strong generalization to previously unseen degradation types and levels. In addition, we curate and release the UIR-2.5M dataset, which includes 2.5 million paired restoration samples across 19 degradation types and over 200 degradation levels, incorporating both synthetic and real-world data. The dataset, source code, and models are available at https://github.com/MILab-PKU/MaskDCPT.",
    "summary": "",
    "translation": "基于掩码退化分类的通用图像修复预训练",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉领域的图像修复技术，虽然涉及预训练方法，但其核心应用场景是图像处理而非推荐系统、搜索或广告。掩码退化分类方法可能对视觉内容理解有一定启发，但缺乏明确的RecSys/Search/Ads应用连接，与当前关注的核心领域相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13253v1": {
    "title": "End-to-End Multi-Modal Diffusion Mamba",
    "url": "https://www.alphaxiv.org/abs/2510.13253v1",
    "arxiv_id": "2510.13253v1",
    "authors": "Chunhao Lu, Qiang Lu, Meichen Dong, Jake Luo",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 08:03:50",
    "ori_summary": "Current end-to-end multi-modal models utilize different encoders and decoders to process input and output information. This separation hinders the joint representation learning of various modalities. To unify multi-modal processing, we propose a novel architecture called MDM (Multi-modal Diffusion Mamba). MDM utilizes a Mamba-based multi-step selection diffusion model to progressively generate and refine modality-specific information through a unified variational autoencoder for both encoding and decoding. This innovative approach allows MDM to achieve superior performance when processing high-dimensional data, particularly in generating high-resolution images and extended text sequences simultaneously. Our evaluations in areas such as image generation, image captioning, visual question answering, text comprehension, and reasoning tasks demonstrate that MDM significantly outperforms existing end-to-end models (MonoFormer, LlamaGen, and Chameleon etc.) and competes effectively with SOTA models like GPT-4V, Gemini Pro, and Mistral. Our results validate MDM's effectiveness in unifying multi-modal processes while maintaining computational efficiency, establishing a new direction for end-to-end multi-modal architectures.",
    "summary": "",
    "translation": "端到端多模态扩散Mamba",
    "relevance_score": 3,
    "reasoning": "该论文结合了扩散模型（多模态生成）和Mamba架构（状态空间模型），属于架构创新。虽然Mamba在序列建模效率方面有潜力，但该论文主要关注多模态扩散生成，与推荐/搜索/广告的核心排序任务关联较弱。其潜在应用可能在于多模态内容理解，但距离实际RecSys/Search/Ads应用较远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13251v1": {
    "title": "Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs",
    "url": "https://www.alphaxiv.org/abs/2510.13251v1",
    "arxiv_id": "2510.13251v1",
    "authors": "Minji Kim, Taekyung Kim, Bohyung Han",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 07:59:06",
    "ori_summary": "Video Large Language Models (VideoLLMs) extend the capabilities of vision-language models to spatiotemporal inputs, enabling tasks such as video question answering (VideoQA). Despite recent advances in VideoLLMs, their internal mechanisms on where and how they extract and propagate video and textual information remain less explored. In this study, we investigate the internal information flow of VideoLLMs using mechanistic interpretability techniques. Our analysis reveals consistent patterns across diverse VideoQA tasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame interactions in early-to-middle layers, (2) followed by progressive video-language integration in middle layers. This is facilitated by alignment between video representations and linguistic embeddings containing temporal concepts. (3) Upon completion of this integration, the model is ready to generate correct answers in middle-to-late layers. (4) Based on our analysis, we show that VideoLLMs can retain their VideoQA performance by selecting these effective information pathways while suppressing a substantial amount of attention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a blueprint on how VideoLLMs perform temporal reasoning and offer practical insights for improving model interpretability and downstream generalization. Our project page with the source code is available at https://map-the-flow.github.io",
    "summary": "",
    "translation": "映射信息流：揭示视频大语言模型中隐藏的信息传递路径",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视频大语言模型中的信息流分析，属于视觉语言模型范畴。虽然提到了VLM，但焦点在于模型内部机制分析而非异构数据统一建模，与'VLM Analogy for Heterogeneous Data'的核心理念关联较弱。在推荐/搜索/广告领域的潜在应用有限，主要是模型诊断工具而非直接应用技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13250v1": {
    "title": "Real-Time Crowd Counting for Embedded Systems with Lightweight Architecture",
    "url": "https://www.alphaxiv.org/abs/2510.13250v1",
    "arxiv_id": "2510.13250v1",
    "authors": "Zhiyuan Zhao, Yubin Wen, Siyu Yang, Lichen Ning, Yuandong Liu, Junyu Gao",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 07:58:46",
    "ori_summary": "Crowd counting is a task of estimating the number of the crowd through images, which is extremely valuable in the fields of intelligent security, urban planning, public safety management, and so on. However, the existing counting methods have some problems in practical application on embedded systems for these fields, such as excessive model parameters, abundant complex calculations, etc. The practical application of embedded systems requires the model to be real-time, which means that the model is fast enough. Considering the aforementioned problems, we design a super real-time model with a stem-encoder-decoder structure for crowd counting tasks, which achieves the fastest inference compared with state-of-the-arts. Firstly, large convolution kernels in the stem network are used to enlarge the receptive field, which effectively extracts detailed head information. Then, in the encoder part, we use conditional channel weighting and multi-branch local fusion block to merge multi-scale features with low computational consumption. This part is crucial to the super real-time performance of the model. Finally, the feature pyramid networks are added to the top of the encoder to alleviate its incomplete fusion problems. Experiments on three benchmarks show that our network is suitable for super real-time crowd counting on embedded systems, ensuring competitive accuracy. At the same time, the proposed network reasoning speed is the fastest. Specifically, the proposed network achieves 381.7 FPS on NVIDIA GTX 1080Ti and 71.9 FPS on NVIDIA Jetson TX1.",
    "summary": "",
    "translation": "面向嵌入式系统的轻量级架构实时人群计数",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的人群计数技术，属于纯粹的视觉应用，与推荐系统、搜索或广告的核心技术栈没有直接关联。虽然人群计数在某些场景下可能有辅助作用（如实体店客流分析），但这不属于当前关注的核心领域进展或使能技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13245v1": {
    "title": "CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation",
    "url": "https://www.alphaxiv.org/abs/2510.13245v1",
    "arxiv_id": "2510.13245v1",
    "authors": "Li Liang, Bo Miao, Xinyu Wang, Naveed Akhtar, Jordan Vice, Ajmal Mian",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 07:47:00",
    "ori_summary": "Outdoor 3D semantic scene generation produces realistic and semantically rich environments for applications such as urban simulation and autonomous driving. However, advances in this direction are constrained by the absence of publicly available, well-annotated datasets. We introduce SketchSem3D, the first large-scale benchmark for generating 3D outdoor semantic scenes from abstract freehand sketches and pseudo-labeled annotations of satellite images. SketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-based KITTI-360 (containing LiDAR voxels along with their corresponding sketches and annotated satellite images), to enable standardized, rigorous, and diverse evaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) that significantly enhances spatial coherence in outdoor 3D scene generation. CymbaDiff imposes structured spatial ordering, explicitly captures cylindrical continuity and vertical hierarchy, and preserves both physical neighborhood relationships and global context within the generated scenes. Extensive experiments on SketchSem3D demonstrate that CymbaDiff achieves superior semantic consistency, spatial realism, and cross-dataset generalization. The code and dataset will be available at https://github.com/Lillian-research-hub/CymbaDiff",
    "summary": "",
    "translation": "CymbaDiff：基于草图的3D语义城市场景生成的结构化空间扩散方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于基于草图的3D城市场景生成，属于计算机图形学和3D视觉领域。虽然涉及扩散模型技术，但其应用场景（3D城市场景生成）与推荐系统、搜索或广告的核心技术栈没有直接关联，也不涉及处理用户行为序列、上下文特征或排序优化等RecSys/Search/Ads的关键问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13243v1": {
    "title": "FlyAwareV2: A Multimodal Cross-Domain UAV Dataset for Urban Scene Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.13243v1",
    "arxiv_id": "2510.13243v1",
    "authors": "Francesco Barbato, Matteo Caligiuri, Pietro Zanuttigh",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 07:44:31",
    "ori_summary": "The development of computer vision algorithms for Unmanned Aerial Vehicle (UAV) applications in urban environments heavily relies on the availability of large-scale datasets with accurate annotations. However, collecting and annotating real-world UAV data is extremely challenging and costly. To address this limitation, we present FlyAwareV2, a novel multimodal dataset encompassing both real and synthetic UAV imagery tailored for urban scene understanding tasks. Building upon the recently introduced SynDrone and FlyAware datasets, FlyAwareV2 introduces several new key contributions: 1) Multimodal data (RGB, depth, semantic labels) across diverse environmental conditions including varying weather and daytime; 2) Depth maps for real samples computed via state-of-the-art monocular depth estimation; 3) Benchmarks for RGB and multimodal semantic segmentation on standard architectures; 4) Studies on synthetic-to-real domain adaptation to assess the generalization capabilities of models trained on the synthetic data. With its rich set of annotations and environmental diversity, FlyAwareV2 provides a valuable resource for research on UAV-based 3D urban scene understanding.",
    "summary": "",
    "translation": "FlyAwareV2：面向城市场景理解的多模态跨域无人机数据集",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于无人机数据集和城市场景理解，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然涉及多模态数据，但主要应用于无人机和城市环境感知，无法为RecSys/Search/Ads领域提供可迁移的技术或应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13237v1": {
    "title": "Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models",
    "url": "https://www.alphaxiv.org/abs/2510.13237v1",
    "arxiv_id": "2510.13237v1",
    "authors": "Haochuan Xu, Yun Sing Koh, Shuhuai Huang, Zirun Zhou, Di Wang, Jun Sakuma, Jingfeng Zhang",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-15 07:42:44",
    "ori_summary": "Vision-Language-Action (VLA) models have achieved revolutionary progress in robot learning, enabling robots to execute complex physical robot tasks from natural language instructions. Despite this progress, their adversarial robustness remains underexplored. In this work, we propose both adversarial patch attack and corresponding defense strategies for VLA models. We first introduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic adversarial attack that generates patches directly placeable within the camera's view. In comparison to prior methods, EDPA can be readily applied to different VLA models without requiring prior knowledge of the model architecture, or the controlled robotic manipulator. EDPA constructs these patches by (i) disrupting the semantic alignment between visual and textual latent representations, and (ii) maximizing the discrepancy of latent representations between adversarial and corresponding clean visual inputs. Through the optimization of these objectives, EDPA distorts the VLA's interpretation of visual information, causing the model to repeatedly generate incorrect actions and ultimately result in failure to complete the given robotic task. To counter this, we propose an adversarial fine-tuning scheme for the visual encoder, in which the encoder is optimized to produce similar latent representations for both clean and adversarially perturbed visual inputs. Extensive evaluations on the widely recognized LIBERO robotic simulation benchmark demonstrate that EDPA substantially increases the task failure rate of cutting-edge VLA models, while our proposed defense effectively mitigates this degradation. The codebase is accessible via the homepage at https://edpa-attack.github.io/.",
    "summary": "",
    "translation": "面向视觉-语言-动作模型的模型无关对抗攻击与防御",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉-语言-动作模型的对抗攻击与防御，属于机器人控制和具身智能领域，与推荐系统、搜索或广告的核心技术关联性较弱。虽然标题提及'视觉-语言'，但其核心是动作控制和安全防御，缺乏在RecSys/Search/Ads领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13235v1": {
    "title": "EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking",
    "url": "https://www.alphaxiv.org/abs/2510.13235v1",
    "arxiv_id": "2510.13235v1",
    "authors": "Yukuan Zhang, Jiarui Zhao, Shangqing Nie, Jin Kuang, Shengsheng Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 07:39:30",
    "ori_summary": "Multimodal semantic cues, such as textual descriptions, have shown strong potential in enhancing target perception for tracking. However, existing methods rely on static textual descriptions from large language models, which lack adaptability to real-time target state changes and prone to hallucinations. To address these challenges, we propose a unified multimodal vision-language tracking framework, named EPIPTrack, which leverages explicit and implicit prompts for dynamic target modeling and semantic alignment. Specifically, explicit prompts transform spatial motion information into natural language descriptions to provide spatiotemporal guidance. Implicit prompts combine pseudo-words with learnable descriptors to construct individualized knowledge representations capturing appearance attributes. Both prompts undergo dynamic adjustment via the CLIP text encoder to respond to changes in target state. Furthermore, we design a Discriminative Feature Augmentor to enhance visual and cross-modal representations. Extensive experiments on MOT17, MOT20, and DanceTrack demonstrate that EPIPTrack outperforms existing trackers in diverse scenarios, exhibiting robust adaptability and superior performance.",
    "summary": "",
    "translation": "EPIPTrack：基于显式和隐式提示重新思考多目标跟踪中的提示建模",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的多目标跟踪任务，属于纯粹的视觉跟踪技术。虽然提到了提示建模概念，但这是针对视觉跟踪的具体应用，与推荐系统、搜索或广告的核心技术领域没有直接关联。论文内容不涉及任何推荐、搜索或广告相关的应用场景或技术迁移可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13234v1": {
    "title": "UniVector: Unified Vector Extraction via Instance-Geometry Interaction",
    "url": "https://www.alphaxiv.org/abs/2510.13234v1",
    "arxiv_id": "2510.13234v1",
    "authors": "Yinglong Yan, Jun Yue, Shaobo Xia, Hanmeng Sun, Tianxu Ying, Chengcheng Wu, Sifan Lan, Min He, Pedram Ghamisi, Leyuan Fang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 07:39:25",
    "ori_summary": "Vector extraction retrieves structured vector geometry from raster images, offering high-fidelity representation and broad applicability. Existing methods, however, are usually tailored to a single vector type (e.g., polygons, polylines, line segments), requiring separate models for different structures. This stems from treating instance attributes (category, structure) and geometric attributes (point coordinates, connections) independently, limiting the ability to capture complex structures. Inspired by the human brain's simultaneous use of semantic and spatial interactions in visual perception, we propose UniVector, a unified VE framework that leverages instance-geometry interaction to extract multiple vector types within a single model. UniVector encodes vectors as structured queries containing both instance- and geometry-level information, and iteratively updates them through an interaction module for cross-level context exchange. A dynamic shape constraint further refines global structures and key points. To benchmark multi-structure scenarios, we introduce the Multi-Vector dataset with diverse polygons, polylines, and line segments. Experiments show UniVector sets a new state of the art on both single- and multi-structure VE tasks. Code and dataset will be released at https://github.com/yyyyll0ss/UniVector.",
    "summary": "",
    "translation": "UniVector：通过实例-几何交互的统一向量提取",
    "relevance_score": 3,
    "reasoning": "该论文标题暗示了向量提取的统一方法，可能涉及特征表示学习，这与推荐系统中的嵌入学习有一定相关性。然而，标题中强调的“实例-几何交互”更偏向计算机视觉中的目标检测或实例分割，缺乏明确的推荐、搜索或广告应用场景的直接联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13232v1": {
    "title": "What \"Not\" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging",
    "url": "https://www.alphaxiv.org/abs/2510.13232v1",
    "arxiv_id": "2510.13232v1",
    "authors": "Inha Kang, Youngsun Lim, Seonho Lee, Jiho Choi, Junsuk Choe, Hyunjung Shim",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 07:36:38",
    "ori_summary": "State-of-the-art vision-language models (VLMs) suffer from a critical failure in understanding negation, often referred to as affirmative bias. This limitation is particularly severe in described object detection (DOD) tasks. To address this, we propose two primary contributions: (1) a new dataset pipeline and (2) a novel, lightweight adaptation recipe. First, we introduce CoVAND, a dataset constructed with a systematic chain-of-thought (CoT) and VQA-based pipeline to generate high-quality, instance-grounded negation data. Second, we propose NegToMe, a novel text token merging module that directly tackles the architectural cause of affirmative bias. NegToMe fundamentally addresses the structural loss of negation cues in tokenization, grouping them with attributes into coherent semantic phrases. It maintains correct polarity at the input level, enabling robust negation understanding even with limited data. For instance, to prevent a model from treating the fragmented tokens \"not\" and \"girl\" as simply \"girl\", NegToMe binds them into a single token whose meaning is correctly distinguished from that of \"girl\" alone. This module is integrated with a parameter-efficient and strategic LoRA fine-tuning approach. Our method significantly improves performance on challenging negation benchmarks with a lowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval and demonstrating generalization to SoTA VLMs. This work marks a crucial step forward in addressing negation understanding for real-world detection applications.",
    "summary": "",
    "translation": "通过结构化推理与令牌合并实现否定感知的视觉语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型(VLMs)中的否定检测问题，属于纯粹的视觉-语言交叉领域研究。虽然提到了结构化推理和令牌合并技术，但这些技术改进主要针对视觉理解任务，在推荐系统、搜索或广告领域的直接应用潜力非常有限。论文核心关注的是视觉语言理解中的特定语义问题，而非能够直接应用于异构数据处理或推荐系统架构的技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13226v1": {
    "title": "Sample-Centric Multi-Task Learning for Detection and Segmentation of Industrial Surface Defects",
    "url": "https://www.alphaxiv.org/abs/2510.13226v1",
    "arxiv_id": "2510.13226v1",
    "authors": "Hang-Cheng Dong, Yibo Jiao, Fupeng Wei, Guodong Liu, Dong Ye, Bingguo Liu",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-15 07:24:26",
    "ori_summary": "Industrial surface defect inspection for sample-wise quality control (QC) must simultaneously decide whether a given sample contains defects and localize those defects spatially. In real production lines, extreme foreground-background imbalance, defect sparsity with a long-tailed scale distribution, and low contrast are common. As a result, pixel-centric training and evaluation are easily dominated by large homogeneous regions, making it difficult to drive models to attend to small or low-contrast defects-one of the main bottlenecks for deployment. Empirically, existing models achieve strong pixel-overlap metrics (e.g., mIoU) but exhibit insufficient stability at the sample level, especially for sparse or slender defects. The root cause is a mismatch between the optimization objective and the granularity of QC decisions. To address this, we propose a sample-centric multi-task learning framework and evaluation suite. Built on a shared-encoder architecture, the method jointly learns sample-level defect classification and pixel-level mask localization. Sample-level supervision modulates the feature distribution and, at the gradient level, continually boosts recall for small and low-contrast defects, while the segmentation branch preserves boundary and shape details to enhance per-sample decision stability and reduce misses. For evaluation, we propose decision-linked metrics, Seg_mIoU and Seg_Recall, which remove the bias of classical mIoU caused by empty or true-negative samples and tightly couple localization quality with sample-level decisions. Experiments on two benchmark datasets demonstrate that our approach substantially improves the reliability of sample-level decisions and the completeness of defect localization.",
    "summary": "",
    "translation": "以样本为中心的多任务学习用于工业表面缺陷检测与分割",
    "relevance_score": 1,
    "reasoning": "该论文专注于工业表面缺陷检测的计算机视觉应用，属于纯粹的视觉检测任务。虽然涉及多任务学习技术，但其应用场景（工业缺陷检测）与推荐系统、搜索或广告领域没有直接关联，也不涉及LLM技术或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13219v1": {
    "title": "Prompt-based Adaptation in Large-scale Vision Models: A Survey",
    "url": "https://www.alphaxiv.org/abs/2510.13219v1",
    "arxiv_id": "2510.13219v1",
    "authors": "Xi Xiao, Yunbei Zhang, Lin Zhao, Yiyang Liu, Xiaoying Liao, Zheda Mai, Xingjian Li, Xiao Wang, Hao Xu, Jihun Hamm, Xue Lin, Min Xu, Qifan Wang, Tianyang Wang, Cheng Han",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 07:14:50",
    "ori_summary": "In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) have recently emerged as lightweight and effective alternatives to full fine-tuning for adapting large-scale vision models within the ``pretrain-then-finetune'' paradigm. However, despite rapid progress, their conceptual boundaries remain blurred, as VP and VPT are frequently used interchangeably in current research, reflecting a lack of systematic distinction between these techniques and their respective applications. In this survey, we revisit the designs of VP and VPT from first principles, and conceptualize them within a unified framework termed Prompt-based Adaptation (PA). We provide a taxonomy that categorizes existing methods into learnable, generative, and non-learnable prompts, and further organizes them by injection granularity -- pixel-level and token-level. Beyond the core methodologies, we examine PA's integrations across diverse domains, including medical imaging, 3D point clouds, and vision-language tasks, as well as its role in test-time adaptation and trustworthy AI. We also summarize current benchmarks and identify key challenges and future directions. To the best of our knowledge, we are the first comprehensive survey dedicated to PA's methodologies and applications in light of their distinct characteristics. Our survey aims to provide a clear roadmap for researchers and practitioners in all area to understand and explore the evolving landscape of PA-related research.",
    "summary": "",
    "translation": "基于提示的大规模视觉模型适应方法综述",
    "relevance_score": 3,
    "reasoning": "这篇论文主要关注视觉模型的提示适应方法，属于纯粹的视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然提示工程在LLM中有广泛应用，但本文聚焦于视觉模型而非语言模型，因此对当前关注领域的潜在应用价值有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13208v1": {
    "title": "MimicParts: Part-aware Style Injection for Speech-Driven 3D Motion Generation",
    "url": "https://www.alphaxiv.org/abs/2510.13208v1",
    "arxiv_id": "2510.13208v1",
    "authors": "Lianlian Liu, YongKang He, Zhaojie Chu, Xiaofen Xing, Xiangmin Xu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 06:53:15",
    "ori_summary": "Generating stylized 3D human motion from speech signals presents substantial challenges, primarily due to the intricate and fine-grained relationships among speech signals, individual styles, and the corresponding body movements. Current style encoding approaches either oversimplify stylistic diversity or ignore regional motion style differences (e.g., upper vs. lower body), limiting motion realism. Additionally, motion style should dynamically adapt to changes in speech rhythm and emotion, but existing methods often overlook this. To address these issues, we propose MimicParts, a novel framework designed to enhance stylized motion generation based on part-aware style injection and part-aware denoising network. It divides the body into different regions to encode localized motion styles, enabling the model to capture fine-grained regional differences. Furthermore, our part-aware attention block allows rhythm and emotion cues to guide each body region precisely, ensuring that the generated motion aligns with variations in speech rhythm and emotional state. Experimental results show that our method outperforming existing methods showcasing naturalness and expressive 3D human motion sequences.",
    "summary": "",
    "translation": "MimicParts：面向语音驱动3D运动生成的部分感知风格注入",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音驱动的3D运动生成，属于计算机图形学和语音处理交叉领域。虽然涉及风格注入技术，但主要应用于3D动画生成，与推荐系统、搜索或广告的核心技术栈无直接关联。论文内容不符合当前关注的任何技术方向，包括核心推荐系统进展、LLM使能技术或Transformer架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13201v1": {
    "title": "Paper Copilot: Tracking the Evolution of Peer Review in AI Conferences",
    "url": "https://www.alphaxiv.org/abs/2510.13201v1",
    "arxiv_id": "2510.13201v1",
    "authors": "Jing Yang, Qiyao Wei, Jiaxin Pei",
    "categories": "cs.CV, cs.AI, cs.DL, cs.LG",
    "pub_date": "2025-10-15 06:41:06",
    "ori_summary": "The rapid growth of AI conferences is straining an already fragile peer-review system, leading to heavy reviewer workloads, expertise mismatches, inconsistent evaluation standards, superficial or templated reviews, and limited accountability under compressed timelines. In response, conference organizers have introduced new policies and interventions to preserve review standards. Yet these ad-hoc changes often create further concerns and confusion about the review process, leaving how papers are ultimately accepted - and how practices evolve across years - largely opaque. We present Paper Copilot, a system that creates durable digital archives of peer reviews across a wide range of computer-science venues, an open dataset that enables researchers to study peer review at scale, and a large-scale empirical analysis of ICLR reviews spanning multiple years. By releasing both the infrastructure and the dataset, Paper Copilot supports reproducible research on the evolution of peer review. We hope these resources help the community track changes, diagnose failure modes, and inform evidence-based improvements toward a more robust, transparent, and reliable peer-review system.",
    "summary": "",
    "translation": "论文副驾驶：追踪AI会议中同行评审的演变",
    "relevance_score": 1,
    "reasoning": "该论文关注AI会议同行评审系统的演变追踪，这属于学术流程和评估机制的研究。论文内容与推荐系统、搜索、广告的核心技术进展、LLM使能技术或Transformer架构改进完全无关，也不涉及异构数据的统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13198v1": {
    "title": "Complementary Information Guided Occupancy Prediction via Multi-Level Representation Fusion",
    "url": "https://www.alphaxiv.org/abs/2510.13198v1",
    "arxiv_id": "2510.13198v1",
    "authors": "Rongtao Xu, Jinzhou Lin, Jialei Zhou, Jiahua Dong, Changwei Wang, Ruisheng Wang, Li Guo, Shibiao Xu, Xiaodan Liang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 06:37:33",
    "ori_summary": "Camera-based occupancy prediction is a mainstream approach for 3D perception in autonomous driving, aiming to infer complete 3D scene geometry and semantics from 2D images. Almost existing methods focus on improving performance through structural modifications, such as lightweight backbones and complex cascaded frameworks, with good yet limited performance. Few studies explore from the perspective of representation fusion, leaving the rich diversity of features in 2D images underutilized. Motivated by this, we propose \\textbf{CIGOcc, a two-stage occupancy prediction framework based on multi-level representation fusion. \\textbf{CIGOcc extracts segmentation, graphics, and depth features from an input image and introduces a deformable multi-level fusion mechanism to fuse these three multi-level features. Additionally, CIGOcc incorporates knowledge distilled from SAM to further enhance prediction accuracy. Without increasing training costs, CIGOcc achieves state-of-the-art performance on the SemanticKITTI benchmark. The code is provided in the supplementary material and will be released https://github.com/VitaLemonTea1/CIGOcc",
    "summary": "",
    "translation": "基于多层级表征融合的互补信息引导占用预测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的占用预测任务，涉及多层级表征融合技术。虽然多模态融合技术对推荐系统中的异构数据处理有一定启发，但论文标题明确指向视觉占用预测这一特定应用领域，与搜索、推荐、广告系统的核心需求关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13186v1": {
    "title": "STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client Selection and Power Control",
    "url": "https://www.alphaxiv.org/abs/2510.13186v1",
    "arxiv_id": "2510.13186v1",
    "authors": "Zhen Li, Xibin Jin, Guoliang Li, Shuai Wang, Miaowen Wen, Huseyin Arslan, Derrick Wing Kwan Ng, Chengzhong Xu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 06:20:47",
    "ori_summary": "Edge Gaussian splatting (EGS), which aggregates data from distributed clients and trains a global GS model at the edge server, is an emerging paradigm for scene reconstruction. Unlike traditional edge resource management methods that emphasize communication throughput or general-purpose learning performance, EGS explicitly aims to maximize the GS qualities, rendering existing approaches inapplicable. To address this problem, this paper formulates a novel GS-oriented objective function that distinguishes the heterogeneous view contributions of different clients. However, evaluating this function in turn requires clients' images, leading to a causality dilemma. To this end, this paper further proposes a sample-then-transmit EGS (or STT-GS for short) strategy, which first samples a subset of images as pilot data from each client for loss prediction. Based on the first-stage evaluation, communication resources are then prioritized towards more valuable clients. To achieve efficient sampling, a feature-domain clustering (FDC) scheme is proposed to select the most representative data and pilot transmission time minimization (PTTM) is adopted to reduce the pilot overhead.Subsequently, we develop a joint client selection and power control (JCSPC) framework to maximize the GS-oriented function under communication resource constraints. Despite the nonconvexity of the problem, we propose a low-complexity efficient solution based on the penalty alternating majorization minimization (PAMM) algorithm. Experiments unveil that the proposed scheme significantly outperforms existing benchmarks on real-world datasets. It is found that the GS-oriented objective can be accurately predicted with low sampling ratios (e.g.,10%), and our method achieves an excellent tradeoff between view contributions and communication costs.",
    "summary": "",
    "translation": "STT-GS：具有联合客户端选择与功率控制的采样后传输边缘高斯泼溅",
    "relevance_score": 1,
    "reasoning": "该论文关注边缘计算中的通信优化（客户端选择和功率控制）与计算机视觉渲染技术（高斯泼溅），属于分布式系统优化和计算机图形学领域。这些技术与推荐系统、搜索或广告的核心进展没有直接关联，也不涉及LLM、Transformer架构或异构数据建模等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13160v1": {
    "title": "DP-TTA: Test-time Adaptation for Transient Electromagnetic Signal Denoising via Dictionary-driven Prior Regularization",
    "url": "https://www.alphaxiv.org/abs/2510.13160v1",
    "arxiv_id": "2510.13160v1",
    "authors": "Meng Yang, Kecheng Chen, Wei Luo, Xianjie Chen, Yong Jia, Mingyue Wang, Fanqiang Lin",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 05:22:03",
    "ori_summary": "Transient Electromagnetic (TEM) method is widely used in various geophysical applications, providing valuable insights into subsurface properties. However, time-domain TEM signals are often submerged in various types of noise. While recent deep learning-based denoising models have shown strong performance, these models are mostly trained on simulated or single real-world scenario data, overlooking the significant differences in noise characteristics from different geographical regions. Intuitively, models trained in one environment often struggle to perform well in new settings due to differences in geological conditions, equipment, and external interference, leading to reduced denoising performance. To this end, we propose the Dictionary-driven Prior Regularization Test-time Adaptation (DP-TTA). Our key insight is that TEM signals possess intrinsic physical characteristics, such as exponential decay and smoothness, which remain consistent across different regions regardless of external conditions. These intrinsic characteristics serve as ideal prior knowledge for guiding the TTA strategy, which helps the pre-trained model dynamically adjust parameters by utilizing self-supervised losses, improving denoising performance in new scenarios. To implement this, we customized a network, named DTEMDNet. Specifically, we first use dictionary learning to encode these intrinsic characteristics as a dictionary-driven prior, which is integrated into the model during training. At the testing stage, this prior guides the model to adapt dynamically to new environments by minimizing self-supervised losses derived from the dictionary-driven consistency and the signal one-order variation. Extensive experimental results demonstrate that the proposed method achieves much better performance than existing TEM denoising methods and TTA methods.",
    "summary": "",
    "translation": "DP-TTA：基于字典驱动先验正则化的瞬态电磁信号去噪测试时自适应方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于瞬态电磁信号去噪这一特定信号处理任务，属于物理和工程领域。虽然涉及测试时自适应技术，但其应用场景（电磁信号处理）与推荐系统、搜索或广告领域没有任何直接关联，也不涉及LLM、Transformer架构或异构数据建模等核心技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13151v1": {
    "title": "Foveation Improves Payload Capacity in Steganography",
    "url": "https://www.alphaxiv.org/abs/2510.13151v1",
    "arxiv_id": "2510.13151v1",
    "authors": "Lifeng Qiu Lin, Henry Kam, Qi Sun, Kaan Akşit",
    "categories": "cs.CV, cs.GR, I.2.10; I.4",
    "pub_date": "2025-10-15 05:00:59",
    "ori_summary": "Steganography finds its use in visual medium such as providing metadata and watermarking. With support of efficient latent representations and foveated rendering, we trained models that improve existing capacity limits from 100 to 500 bits, while achieving better accuracy of up to 1 failure bit out of 2000, at 200K test bits. Finally, we achieve a comparable visual quality of 31.47 dB PSNR and 0.13 LPIPS, showing the effectiveness of novel perceptual design in creating multi-modal latent representations in steganography.",
    "summary": "",
    "translation": "中央凹注视提升隐写术的有效载荷容量",
    "relevance_score": 1,
    "reasoning": "该论文专注于隐写术（信息隐藏技术），属于安全/隐私领域，与推荐系统、搜索或广告的核心技术完全无关。隐写术主要用于秘密通信和数据保护，不涉及任何推荐算法、搜索技术、广告排名或LLM相关应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13137v1": {
    "title": "Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN",
    "url": "https://www.alphaxiv.org/abs/2510.13137v1",
    "arxiv_id": "2510.13137v1",
    "authors": "Madhumati Pol, Anvay Anturkar, Anushka Khot, Ayush Andure, Aniruddha Ghosh, Anvit Magadum, Anvay Bahadur",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 04:26:33",
    "ori_summary": "This study investigates the performance of 3D Convolutional Neural Networks (3D CNNs) and Long Short-Term Memory (LSTM) networks for real-time American Sign Language (ASL) recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences, LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1,200 ASL signs across 50 classes, comparing their accuracy, computational efficiency, and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2% more processing time per frame compared to LSTMs, which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNNLSTM model shows decent performance, which suggests that context-dependent architecture selection is crucial for practical implementation.This project provides professional benchmarks for developing assistive technologies, highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments.",
    "summary": "",
    "translation": "基于深度学习的实时手语到文本翻译：LSTM与3D CNN的对比研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于手语翻译这一特定领域应用，属于计算机视觉和时序建模的交叉领域。虽然使用了深度学习技术，但与搜索、推荐、广告系统的核心进展或使能技术没有直接关联，也不涉及我关注的异构数据统一建模或Transformer架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13131v1": {
    "title": "OS-HGAdapter: Open Semantic Hypergraph Adapter for Large Language Models Assisted Entropy-Enhanced Image-Text Alignment",
    "url": "https://www.alphaxiv.org/abs/2510.13131v1",
    "arxiv_id": "2510.13131v1",
    "authors": "Rongjun Chen, Chengsi Yao, Jinchang Ren, Xianxian Zeng, Peixian Wang, Jun Yuan, Jiawen Li, Huimin Zhao, Xu Lu",
    "categories": "cs.CV, cs.MM",
    "pub_date": "2025-10-15 04:09:00",
    "ori_summary": "Text-image alignment constitutes a foundational challenge in multimedia content understanding, where effective modeling of cross-modal semantic correspondences critically enhances retrieval system performance through joint embedding space optimization. Given the inherent difference in information entropy between texts and images, conventional approaches often show an imbalance in the mutual retrieval of these two modalities. To address this particular challenge, we propose to use the open semantic knowledge of Large Language Model (LLM) to fill for the entropy gap and reproduce the alignment ability of humans in these tasks. Our entropy-enhancing alignment is achieved through a two-step process: 1) a new prompt template that does not rely on explicit knowledge in the task domain is designed to use LLM to enhance the polysemy description of the text modality. By analogy, the information entropy of the text modality relative to the visual modality is increased; 2) A hypergraph adapter is used to construct multilateral connections between the text and image modalities, which can correct the positive and negative matching errors for synonymous semantics in the same fixed embedding space, whilst reducing the noise caused by open semantic entropy by mapping the reduced dimensions back to the original dimensions. Comprehensive evaluations on the Flickr30K and MS-COCO benchmarks validate the superiority of our Open Semantic Hypergraph Adapter (OS-HGAdapter), showcasing 16.8\\% (text-to-image) and 40.1\\% (image-to-text) cross-modal retrieval gains over existing methods while establishing new state-of-the-art performance in semantic alignment tasks.",
    "summary": "",
    "translation": "OS-HGAdapter：面向大语言模型辅助熵增强图文对齐的开放语义超图适配器",
    "relevance_score": 6,
    "reasoning": "该论文涉及多模态对齐（图文对齐）和LLM适配器技术，这与VLM类比异质数据建模相关，可能应用于搜索中的跨模态检索或推荐中的多模态内容理解。然而，其核心焦点是视觉-语言对齐而非纯粹的推荐/搜索/广告排名问题，因此相关性中等。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.13109v1": {
    "title": "VPREG: An Optimal Control Formulation for Diffeomorphic Image Registration Based on the Variational Principle Grid Generation Method",
    "url": "https://www.alphaxiv.org/abs/2510.13109v1",
    "arxiv_id": "2510.13109v1",
    "authors": "Zicong Zhou, Baihan Zhao, Andreas Mang, Guojun Liao",
    "categories": "cs.CV, math.OC, 49J20, 49K20, 49N45",
    "pub_date": "2025-10-15 03:02:39",
    "ori_summary": "This paper introduces VPreg, a novel diffeomorphic image registration method. This work provides several improvements to our past work on mesh generation and diffeomorphic image registration. VPreg aims to achieve excellent registration accuracy while controlling the quality of the registration transformations. It ensures a positive Jacobian determinant of the spatial transformation and provides an accurate approximation of the inverse of the registration, a crucial property for many neuroimaging workflows. Unlike conventional methods, VPreg generates this inverse transformation within the group of diffeomorphisms rather than operating on the image space. The core of VPreg is a grid generation approach, referred to as \\emph{Variational Principle} (VP), which constructs non-folding grids with prescribed Jacobian determinant and curl. These VP-generated grids guarantee diffeomorphic spatial transformations essential for computational anatomy and morphometry, and provide a more accurate inverse than existing methods. To assess the potential of the proposed approach, we conduct a performance analysis for 150 registrations of brain scans from the OASIS-1 dataset. Performance evaluation based on Dice scores for 35 regions of interest, along with an empirical analysis of the properties of the computed spatial transformations, demonstrates that VPreg outperforms state-of-the-art methods in terms of Dice scores, regularity properties of the computed transformation, and accuracy and consistency of the provided inverse map. We compare our results to ANTs-SyN, Freesurfer-Easyreg, and FSL-Fnirt.",
    "summary": "",
    "translation": "VPREG：基于变分原理网格生成方法的微分同胚图像配准最优控制公式",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像配准领域的最优控制方法，属于计算机视觉中的特定应用方向。虽然涉及变分方法和网格生成技术，但这些技术与推荐系统、搜索或广告的核心技术栈没有直接关联。论文内容纯粹针对医学图像处理，属于明确的无关领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13108v1": {
    "title": "DriveCritic: Towards Context-Aware, Human-Aligned Evaluation for Autonomous Driving with Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13108v1",
    "arxiv_id": "2510.13108v1",
    "authors": "Jingyu Song, Zhenxin Li, Shiyi Lan, Xinglong Sun, Nadine Chang, Maying Shen, Joshua Chen, Katherine A. Skinner, Jose M. Alvarez",
    "categories": "cs.CV, cs.AI, cs.RO",
    "pub_date": "2025-10-15 03:00:38",
    "ori_summary": "Benchmarking autonomous driving planners to align with human judgment remains a critical challenge, as state-of-the-art metrics like the Extended Predictive Driver Model Score (EPDMS) lack context awareness in nuanced scenarios. To address this, we introduce DriveCritic, a novel framework featuring two key contributions: the DriveCritic dataset, a curated collection of challenging scenarios where context is critical for correct judgment and annotated with pairwise human preferences, and the DriveCritic model, a Vision-Language Model (VLM) based evaluator. Fine-tuned using a two-stage supervised and reinforcement learning pipeline, the DriveCritic model learns to adjudicate between trajectory pairs by integrating visual and symbolic context. Experiments show DriveCritic significantly outperforms existing metrics and baselines in matching human preferences and demonstrates strong context awareness. Overall, our work provides a more reliable, human-aligned foundation to evaluating autonomous driving systems.",
    "summary": "",
    "translation": "DriveCritic：基于视觉语言模型实现自动驾驶的上下文感知、人类对齐评估",
    "relevance_score": 2,
    "reasoning": "该论文专注于自动驾驶领域的视觉语言模型应用，属于特定领域应用而非通用推荐系统、搜索或广告技术。虽然提到了上下文感知和人类对齐的概念，但这些概念在自动驾驶中的实现方式与推荐/搜索/广告领域的异构数据处理有本质区别，缺乏直接的技术迁移潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13105v1": {
    "title": "EgoSocial: Benchmarking Proactive Intervention Ability of Omnimodal LLMs via Egocentric Social Interaction Perception",
    "url": "https://www.alphaxiv.org/abs/2510.13105v1",
    "arxiv_id": "2510.13105v1",
    "authors": "Xijun Wang, Tanay Sharma, Achin Kulshrestha, Abhimitra Meka, Aveek Purohit, Dinesh Manocha",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 02:52:19",
    "ori_summary": "As AR/VR technologies become integral to daily life, there's a growing need for AI that understands human social dynamics from an egocentric perspective. However, current LLMs often lack the social awareness to discern when to intervene as AI assistant. This leads to constant, socially unaware responses that may disrupt natural conversation and negatively impact user focus. To address these limitations, we introduce EgoSocial, a large-scale egocentric dataset with 13,500 social video-question pairs, specifically designed to benchmark intervention in social interaction perception. We also present an in-depth analysis of current omnimodal LLMs (OLLMs) to assess their effectiveness in detecting diverse social contextual cues. Experiments show that OLLMs still struggle to detect the intervention timing (14.4% for Gemini 2.5 Pro). We also propose EgoSoD (EgoSocial Detection), an end-to-end method for robustly discerning social dynamics. Informed by our OLLM analysis, EgoSoD integrates multimodal contextual cues (e.g., audio and visual cues) into a social thinking graph, dynamically modeling participants and interactions. Our method proactively detects intervention timing and social interactions, precisely determining when to intervene. Our EgoSoD improves Phi-4 by 45.6% and Gemini 2.5 Pro by 9.9% on Intervention Timing performance, and improves Phi-4 by 20.4% and Gemini 2.5 Pro by 6.9% on overall Social Interaction performance. We will release the dataset and code soon.",
    "summary": "",
    "translation": "EgoSocial：通过以自我为中心的社会交互感知基准测试全模态大语言模型的主动干预能力",
    "relevance_score": 3,
    "reasoning": "该论文主要关注全模态LLM在社交交互感知和主动干预方面的基准测试，这属于纯粹的LLM评估范畴，与我的核心关注点无关。虽然提到了'全模态'概念，但焦点是社交交互基准测试，而非推荐系统、搜索或广告领域的实际应用或架构创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13084v1": {
    "title": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar Propagation",
    "url": "https://www.alphaxiv.org/abs/2510.13084v1",
    "arxiv_id": "2510.13084v1",
    "authors": "Yi Zuo, Zitao Wang, Lingling Li, Xu Liu, Fang Liu, Licheng Jiao",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 01:55:32",
    "ori_summary": "Text-to-image (T2I) diffusion models have recently demonstrated significant progress in video editing. However, existing video editing methods are severely limited by their high computational overhead and memory consumption. Furthermore, these approaches often sacrifice visual fidelity, leading to undesirable temporal inconsistencies and artifacts such as blurring and pronounced mosaic-like patterns. We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video editing method. Edit-Your-Interest introduces a spatio-temporal feature memory to cache features from previous frames, significantly reducing computational overhead compared to full-sequence spatio-temporal modeling approaches. Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM), which is designed to efficiently cache and retain the crucial image tokens processed by spatial attention. Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP propagates the most relevant tokens from previous frames to subsequent ones, preserving temporal consistency. Finally, we introduce an SFM update algorithm that continuously refreshes the cached features, ensuring their long-term relevance and effectiveness throughout the video sequence. Furthermore, we leverage cross-attention maps to automatically extract masks for the instances of interest. These masks are seamlessly integrated into the diffusion denoising process, enabling fine-grained control over target objects and allowing Edit-Your-Interest to perform highly accurate edits while robustly preserving the background integrity. Extensive experiments decisively demonstrate that the proposed Edit-Your-Interest outperforms state-of-the-art methods in both efficiency and visual fidelity, validating its superior effectiveness and practicality.",
    "summary": "",
    "translation": "编辑您的兴趣：通过特征最相似传播实现高效视频编辑",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频编辑技术，涉及特征传播方法，与推荐系统、搜索或广告的核心领域进展、LLM技术或Transformer架构没有直接关联。视频编辑属于计算机视觉应用领域，不在当前关注的技术范畴内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13080v1": {
    "title": "Counting Hallucinations in Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2510.13080v1",
    "arxiv_id": "2510.13080v1",
    "authors": "Shuai Fu, Jian Zhou, Qi Chen, Huang Jing, Huy Anh Nguyen, Xiaohan Liu, Zhixiong Zeng, Lin Ma, Quanshi Zhang, Qi Wu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 01:48:04",
    "ori_summary": "Diffusion probabilistic models (DPMs) have demonstrated remarkable progress in generative tasks, such as image and video synthesis. However, they still often produce hallucinated samples (hallucinations) that conflict with real-world knowledge, such as generating an implausible duplicate cup floating beside another cup. Despite their prevalence, the lack of feasible methodologies for systematically quantifying such hallucinations hinders progress in addressing this challenge and obscures potential pathways for designing next-generation generative models under factual constraints. In this work, we bridge this gap by focusing on a specific form of hallucination, which we term counting hallucination, referring to the generation of an incorrect number of instances or structured objects, such as a hand image with six fingers, despite such patterns being absent from the training data. To this end, we construct a dataset suite CountHalluSet, with well-defined counting criteria, comprising ToyShape, SimObject, and RealHand. Using these datasets, we develop a standardized evaluation protocol for quantifying counting hallucinations, and systematically examine how different sampling conditions in DPMs, including solver type, ODE solver order, sampling steps, and initial noise, affect counting hallucination levels. Furthermore, we analyze their correlation with common evaluation metrics such as FID, revealing that this widely used image quality metric fails to capture counting hallucinations consistently. This work aims to take the first step toward systematically quantifying hallucinations in diffusion models and offer new insights into the investigation of hallucination phenomena in image generation.",
    "summary": "",
    "translation": "扩散模型中的幻觉计数",
    "relevance_score": 1,
    "reasoning": "该论文专注于扩散模型的幻觉评估，这属于纯粹的生成模型质量评估范畴，与推荐系统、搜索或广告的核心技术进展无关。论文标题明确指向幻觉计数这一特定评估任务，没有显示出在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13075v1": {
    "title": "Unsupervised Domain Adaptation via Content Alignment for Hippocampus Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.13075v1",
    "arxiv_id": "2510.13075v1",
    "authors": "Hoda Kalabizadeh, Ludovica Griffanti, Pak-Hei Yeung, Ana I. L. Namburete, Nicola K. Dinsdale, Konstantinos Kamnitsas",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 01:34:41",
    "ori_summary": "Deep learning models for medical image segmentation often struggle when deployed across different datasets due to domain shifts - variations in both image appearance, known as style, and population-dependent anatomical characteristics, referred to as content. This paper presents a novel unsupervised domain adaptation framework that directly addresses domain shifts encountered in cross-domain hippocampus segmentation from MRI, with specific emphasis on content variations. Our approach combines efficient style harmonisation through z-normalisation with a bidirectional deformable image registration (DIR) strategy. The DIR network is jointly trained with segmentation and discriminator networks to guide the registration with respect to a region of interest and generate anatomically plausible transformations that align source images to the target domain. We validate our approach through comprehensive evaluations on both a synthetic dataset using Morpho-MNIST (for controlled validation of core principles) and three MRI hippocampus datasets representing populations with varying degrees of atrophy. Across all experiments, our method outperforms existing baselines. For hippocampus segmentation, when transferring from young, healthy populations to clinical dementia patients, our framework achieves up to 15% relative improvement in Dice score compared to standard augmentation methods, with the largest gains observed in scenarios with substantial content shift. These results highlight the efficacy of our approach for accurate hippocampus segmentation across diverse populations.",
    "summary": "",
    "translation": "通过内容对齐实现海马体分割的无监督域自适应",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像中的海马体分割和域自适应，属于医学/生物领域的特定应用。虽然提到了域自适应技术，但该技术应用于医学影像分割，与推荐系统、搜索或广告没有直接关联，也不涉及LLM、Transformer架构或异构数据处理。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13067v1": {
    "title": "Direction-aware multi-scale gradient loss for infrared and visible image fusion",
    "url": "https://www.alphaxiv.org/abs/2510.13067v1",
    "arxiv_id": "2510.13067v1",
    "authors": "Kaixuan Yang, Wei Xiang, Zhenshuai Chen, Tong Jin, Yunpeng Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 01:26:39",
    "ori_summary": "Infrared and visible image fusion aims to integrate complementary information from co-registered source images to produce a single, informative result. Most learning-based approaches train with a combination of structural similarity loss, intensity reconstruction loss, and a gradient-magnitude term. However, collapsing gradients to their magnitude removes directional information, yielding ambiguous supervision and suboptimal edge fidelity. We introduce a direction-aware, multi-scale gradient loss that supervises horizontal and vertical components separately and preserves their sign across scales. This axis-wise, sign-preserving objective provides clear directional guidance at both fine and coarse resolutions, promoting sharper, better-aligned edges and richer texture preservation without changing model architectures or training protocols. Experiments on open-source model and multiple public benchmarks demonstrate effectiveness of our approach.",
    "summary": "",
    "translation": "面向红外与可见光图像融合的方向感知多尺度梯度损失",
    "relevance_score": 1,
    "reasoning": "该论文专注于红外与可见光图像融合的计算机视觉任务，属于纯粹的视觉处理领域。论文内容涉及图像融合的损失函数设计，与推荐系统、搜索或广告的核心技术领域没有直接关联，也不涉及LLM、Transformer架构或异构数据建模等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13063v1": {
    "title": "True Self-Supervised Novel View Synthesis is Transferable",
    "url": "https://www.alphaxiv.org/abs/2510.13063v1",
    "arxiv_id": "2510.13063v1",
    "authors": "Thomas W. Mitchel, Hyunwoo Ryu, Vincent Sitzmann",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-15 01:09:56",
    "ori_summary": "In this paper, we identify that the key criterion for determining whether a model is truly capable of novel view synthesis (NVS) is transferability: Whether any pose representation extracted from one video sequence can be used to re-render the same camera trajectory in another. We analyze prior work on self-supervised NVS and find that their predicted poses do not transfer: The same set of poses lead to different camera trajectories in different 3D scenes. Here, we present XFactor, the first geometry-free self-supervised model capable of true NVS. XFactor combines pair-wise pose estimation with a simple augmentation scheme of the inputs and outputs that jointly enables disentangling camera pose from scene content and facilitates geometric reasoning. Remarkably, we show that XFactor achieves transferability with unconstrained latent pose variables, without any 3D inductive biases or concepts from multi-view geometry -- such as an explicit parameterization of poses as elements of SE(3). We introduce a new metric to quantify transferability, and through large-scale experiments, we demonstrate that XFactor significantly outperforms prior pose-free NVS transformers, and show that latent poses are highly correlated with real-world poses through probing experiments.",
    "summary": "",
    "translation": "真正的自监督新视角合成具有可迁移性",
    "relevance_score": 2,
    "reasoning": "这篇论文主要关注计算机视觉中的新视角合成任务，属于纯粹的视觉生成领域。虽然自监督学习技术本身具有通用性，但该论文的特定应用（新视角合成）与推荐系统、搜索或广告的关联性非常有限，没有明确的跨模态应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14880v1": {
    "title": "Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report",
    "url": "https://www.alphaxiv.org/abs/2510.14880v1",
    "arxiv_id": "2510.14880v1",
    "authors": "Rikiya Takehi, Benjamin Clavié, Sean Lee, Aamir Shakir",
    "categories": "cs.IR",
    "pub_date": "2025-10-16 17:00:35",
    "ori_summary": "In this work, we introduce mxbai-edge-colbert-v0 models, at two different parameter counts: 17M and 32M. As part of our research, we conduct numerous experiments to improve retrieval and late-interaction models, which we intend to distill into smaller models as proof-of-concepts. Our ultimate aim is to support retrieval at all scales, from large-scale retrieval which lives in the cloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a model that we hope will serve as a solid foundation backbone for all future experiments, representing the first version of a long series of small proof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we conducted multiple ablation studies, of which we report the results. In terms of downstream performance, mxbai-edge-colbert-v0 is a particularly capable small model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and representing a large step forward in long-context tasks, with unprecedented efficiency.",
    "summary": "该论文研究如何训练高效的小型检索模型，核心方法是开发参数极少的ColBERT变体模型，通过知识蒸馏等技术实现边缘设备上的高质量检索能力。",
    "translation": "神奇（小型）检索器及其训练方法：mxbai-edge-colbert-v0 技术报告",
    "relevance_score": 8,
    "reasoning": "该论文聚焦于小型高效检索器的训练，这直接属于搜索领域的核心进展。小型检索器技术对于边缘计算和移动端搜索推荐系统具有重要应用价值，能够显著提升检索效率并降低计算成本。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文专注于小型检索模型训练，直接涉及检索系统核心领域进展，并探索模型压缩技术，对搜索和推荐系统的边缘部署具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14857v1": {
    "title": "A Simulation Framework for Studying Systemic Effects of Feedback Loops in Recommender Systems",
    "url": "https://www.alphaxiv.org/abs/2510.14857v1",
    "arxiv_id": "2510.14857v1",
    "authors": "Gabriele Barlacchi, Margherita Lalli, Emanuele Ferragina, Fosca Giannotti, Luca Pappalardo",
    "categories": "cs.IR, cs.CY",
    "pub_date": "2025-10-16 16:31:01",
    "ori_summary": "Recommender systems continuously interact with users, creating feedback loops that shape both individual behavior and collective market dynamics. This paper introduces a simulation framework to model these loops in online retail environments, where recommenders are periodically retrained on evolving user-item interactions. Using the Amazon e-Commerce dataset, we analyze how different recommendation algorithms influence diversity, purchase concentration, and user homogenization over time. Results reveal a systematic trade-off: while the feedback loop increases individual diversity, it simultaneously reduces collective diversity and concentrates demand on a few popular items. Moreover, for some recommender systems, the feedback loop increases user homogenization over time, making user purchase profiles increasingly similar. These findings underscore the need for recommender designs that balance personalization with long-term diversity.",
    "summary": "该论文研究推荐系统中反馈循环对用户行为和市场动态的系统性影响，核心方法是构建仿真框架来建模推荐算法与用户交互的周期性循环过程，分析算法如何随时间影响多样性和用户同质化。",
    "translation": "推荐系统中反馈循环系统性效应的仿真研究框架",
    "relevance_score": 9,
    "reasoning": "该论文直接聚焦于推荐系统的核心机制——反馈循环，这是RecSys领域的核心前沿问题。仿真框架能够研究系统性效应，对于理解推荐算法如何影响用户行为和数据分布具有重要价值，直接应用于推荐系统的优化和稳定性分析。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接研究推荐系统中的反馈循环效应，这是推荐系统领域的核心问题，与用户行为建模和系统长期影响高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14824v1": {
    "title": "Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking",
    "url": "https://www.alphaxiv.org/abs/2510.14824v1",
    "arxiv_id": "2510.14824v1",
    "authors": "Ziqi Dai, Xin Zhang, Mingxin Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, Min Zhang",
    "categories": "cs.CL, cs.CV, cs.IR",
    "pub_date": "2025-10-16 16:02:27",
    "ori_summary": "In information retrieval, training reranking models mainly focuses on two types of objectives: metric learning (e.g. contrastive loss to increase the predicted scores on relevant query-document pairs) and classification (binary label prediction of relevance vs. irrelevance). For BERT-style encoders, various studies have shown that contrastive learning (CL) can be more effective than discriminative (classification) learning. However, for large language models (LLMs), classification via supervised fine-tuning (SFT), which predicts ''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears more promising as it aligns well with the generative nature of LLMs. This divergence raises a central question: which objective is intrinsically better suited to LLM-based reranking, and what mechanism underlies the difference? In this work, we conduct a comprehensive comparison and analysis between CL and SFT for reranking, taking the universal multimodal retrieval (UMR) as the experimental playground. We first decompose the objectives into two components: weight, which controls the magnitude of those updates, and direction, which guides the model updates, then present a unified framework for understanding their interactions. Through probing experiments, we find that SFT provides a substantially stronger weighting scheme than CL, whereas the preferred scoring direction shows no clear winner. Taken together, these results point to a consistent advantage of SFT over CL for LLM reranking. To further validate our findings, we conduct large-scale training with SFT and present new state-of-the-art rerankers on the MRB benchmark. We also provide ablations on SFT settings and expect our findings to benefit future research and applications in this area.",
    "summary": "研究LLM重排序中监督微调与对比学习两种训练目标的本质差异；核心思想是通过分解权重和方向两个组件构建统一分析框架，发现SFT提供更强的权重机制而方向偏好无明确优劣。",
    "translation": "监督微调还是对比学习？迈向更优的多模态大语言模型重排序",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及多模态LLM在重排序中的应用，这属于'直接LLM应用'和'VLM类比'范畴。多模态重排序技术可直接应用于搜索和推荐系统中，通过结合文本、图像等异构数据提升排序质量，对比学习与监督微调的比较对实际部署具有重要指导意义。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接比较监督微调与对比学习在LLM重排序中的效果，属于LLM在搜索领域的直接应用，并深入分析Transformer架构的训练机制。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14788v1": {
    "title": "Cross-Scenario Unified Modeling of User Interests at Billion Scale",
    "url": "https://www.alphaxiv.org/abs/2510.14788v1",
    "arxiv_id": "2510.14788v1",
    "authors": "Manjie Xu, Cheng Chen, Xin Jia, Jingyi Zhou, Yongji Wu, Zejian Wang, Chi Zhang, Kai Zuo, Yibo Chen, Xu Tang, Yao Hu, Yixin Zhu",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-16 15:20:49",
    "ori_summary": "User interests on content platforms are inherently diverse, manifesting through complex behavioral patterns across heterogeneous scenarios such as search, feed browsing, and content discovery. Traditional recommendation systems typically prioritize business metric optimization within isolated specific scenarios, neglecting cross-scenario behavioral signals and struggling to integrate advanced techniques like LLMs at billion-scale deployments, which finally limits their ability to capture holistic user interests across platform touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender Engine for Diversified scenarios, tailored for industry-level content recommendation systems. RED-Rec unifies user interest representations across multiple behavioral contexts by aggregating and synthesizing actions from varied scenarios, resulting in comprehensive item and user modeling. At its core, a two-tower LLM-powered framework enables nuanced, multifaceted representations with deployment efficiency, and a scenario-aware dense mixing and querying policy effectively fuses diverse behavioral signals to capture cross-scenario user intent patterns and express fine-grained, context-specific intents during serving. We validate RED-Rec through online A/B testing on hundreds of millions of users in RedNote through online A/B testing, showing substantial performance gains in both content recommendation and advertisement targeting tasks. We further introduce a million-scale sequential recommendation dataset, RED-MMU, for comprehensive offline training and evaluation. Our work advances unified user modeling, unlocking deeper personalization and fostering more meaningful user engagement in large-scale UGC platforms.",
    "summary": "论文研究大规模内容平台中用户兴趣的跨场景统一建模问题，核心方法是提出LLM增强的层次化推荐引擎，通过两塔框架聚合多场景行为信号来构建综合的用户兴趣表示。",
    "translation": "十亿规模用户兴趣的跨场景统一建模",
    "relevance_score": 9,
    "reasoning": "该论文直接聚焦于推荐系统的核心领域进展，涉及大规模用户兴趣建模和跨场景统一表示，这是推荐系统的关键技术挑战。这种统一建模方法可以显著提升个性化推荐效果，并为广告定向和搜索个性化提供强大的用户理解能力。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对大规模推荐系统中的跨场景统一建模问题，核心采用LLM增强的两塔框架处理异构行为数据，与关注领域高度契合。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14704v1": {
    "title": "Dataset Pruning in RecSys and ML: Best Practice or Mal-Practice?",
    "url": "https://www.alphaxiv.org/abs/2510.14704v1",
    "arxiv_id": "2510.14704v1",
    "authors": "Leonie Winter",
    "categories": "cs.IR",
    "pub_date": "2025-10-16 14:08:30",
    "ori_summary": "Offline evaluations in recommender system research depend heavily on datasets, many of which are pruned, such as the widely used MovieLens collections. This thesis examines the impact of data pruning - specifically, removing users with fewer than a specified number of interactions - on both dataset characteristics and algorithm performance. Five benchmark datasets were analysed in both their unpruned form and at five successive pruning levels (5, 10, 20, 50, 100). For each coreset, we examined structural and distributional characteristics and trained and tested eleven representative algorithms. To further assess if pruned datasets lead to artificially inflated performance results, we also evaluated models trained on the pruned train sets but tested on unpruned data. Results show that commonly applied core pruning can be highly selective, leaving as little as 2% of the original users in some datasets. Traditional algorithms achieved higher nDCG@10 scores when both training and testing on pruned data; however, this advantage largely disappeared when evaluated on unpruned test sets. Across all algorithms, performance declined with increasing pruning levels when tested on unpruned data, highlighting the impact of dataset reduction on the performance of recommender algorithms.",
    "summary": "该论文研究推荐系统中数据剪枝（移除交互次数少的用户）对算法评估的影响。核心发现是数据剪枝会显著改变数据集结构特征，并在剪枝数据集上训练测试时产生性能虚高，但在未剪枝测试集上这种优势消失。",
    "translation": "推荐系统与机器学习中的数据集剪枝：最佳实践还是不当实践？",
    "relevance_score": 8,
    "reasoning": "该论文直接探讨推荐系统（RecSys）领域的数据集剪枝技术，属于核心领域进展范畴。数据集剪枝技术对于提升推荐模型训练效率、处理大规模用户行为数据具有重要价值，能够直接影响推荐系统的性能和部署成本。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接研究推荐系统数据集处理的核心实践问题，揭示了数据剪枝对算法性能评估的潜在误导，对RecSys领域的方法论有重要影响。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14670v1": {
    "title": "TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence",
    "url": "https://www.alphaxiv.org/abs/2510.14670v1",
    "arxiv_id": "2510.14670v1",
    "authors": "Marco Simoni, Aleksandar Fontana, Andrea Saracino, Paolo Mori",
    "categories": "cs.AI, cs.CL, cs.CR, cs.IR",
    "pub_date": "2025-10-16 13:27:05",
    "ori_summary": "TITAN (Threat Intelligence Through Automated Navigation) is a framework that connects natural-language cyber threat queries with executable reasoning over a structured knowledge graph. It integrates a path planner model, which predicts logical relation chains from text, and a graph executor that traverses the TITAN Ontology to retrieve factual answers and supporting evidence. Unlike traditional retrieval systems, TITAN operates on a typed, bidirectional graph derived from MITRE, allowing reasoning to move clearly and reversibly between threats, behaviors, and defenses. To support training and evaluation, we introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test: 13951) pairing natural language questions with executable reasoning paths and step by step Chain of Thought explanations. Empirical evaluations show that TITAN enables models to generate syntactically valid and semantically coherent reasoning paths that can be deterministically executed on the underlying graph.",
    "summary": "",
    "translation": "TITAN：面向网络威胁情报的图可执行推理",
    "relevance_score": 1,
    "reasoning": "该论文专注于网络安全领域的威胁情报分析，这属于安全领域的特定应用，与推荐系统、搜索或广告的核心技术无关。论文标题中提到的图推理技术虽然可能涉及图神经网络，但其应用场景明确限定在网络安全领域，没有显示出在推荐、搜索或广告系统中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14660v1": {
    "title": "An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.14660v1",
    "arxiv_id": "2510.14660v1",
    "authors": "Linyue Ma, Yilong Xu, Xiang Long, Zhi Zheng",
    "categories": "cs.CL, cs.AI, cs.IR",
    "pub_date": "2025-10-16 13:15:40",
    "ori_summary": "Search augmentation empowers Large Language Models with retrieval capabilities to overcome the limitations imposed by static parameters. Recently, Reinforcement Learning leverages tailored reward signals as a viable technique to enhance LLMs performing tasks involving search. However, existing reward modeling for search-augmented LLMs faces several limitations. Rule-based rewards, such as Exact Match, are verifiable but fragile to variations in expression and cannot be applied to long-form workloads. In contrast, generative rewards improve robustness, but designing verifiable and stable rewards for long-form workloads in dynamic corpora remains challenging and also incurs high computational costs. In this paper, we propose a unified and verifiable paradigm, \"nugget-as-rubric\", which treats atomic information points as structured evaluation criteria for different search-augmentation workloads. Short-form tasks correspond to a single rubric, whereas long-form tasks expand to multiple rubrics aligned with the question's information needs. To support long-form settings, we design an automatic rubric construction pipeline based on query rewriting, which can automatically retrieve passages relevant to each question and extract rubrics from them, both from static corpora and from dynamic online web content. Furthermore, we introduce \\textbf{Search-Gen-V}, a 4B-parameter efficient generative verifier under our proposed verifiable paradigm, which is trained via the idea of distillation and a two-stage strategy. Experimental results show that Search-Gen-V achieves strong verification accuracy across different workloads, making it a scalable, robust, and efficient verifiable reward constructor for search-augmented LLMs.",
    "summary": "",
    "translation": "一种基于评分标准的高效生成式验证器，用于搜索增强型大语言模型",
    "relevance_score": 5,
    "reasoning": "该论文涉及搜索增强型LLMs的验证机制，与搜索领域直接相关，但主要关注验证而非核心排名或推荐算法。评分标准验证方法可能应用于搜索结果的可靠性评估，但论文焦点更偏向LLM验证而非搜索系统本身的核心改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14641v1": {
    "title": "Causality Enhancement for Cross-Domain Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.14641v1",
    "arxiv_id": "2510.14641v1",
    "authors": "Zhibo Wu, Yunfan Wu, Lin Jiang, Ping Yang, Yao Hu",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-16 12:54:46",
    "ori_summary": "Cross-domain recommendation forms a crucial component in recommendation systems. It leverages auxiliary information through source domain tasks or features to enhance target domain recommendations. However, incorporating inconsistent source domain tasks may result in insufficient cross-domain modeling or negative transfer. While incorporating source domain features without considering the underlying causal relationships may limit their contribution to final predictions. Thus, a natural idea is to directly train a cross-domain representation on a causality-labeled dataset from the source to target domain. Yet this direction has been rarely explored, as identifying unbiased real causal labels is highly challenging in real-world scenarios. In this work, we attempt to take a first step in this direction by proposing a causality-enhanced framework, named CE-CDR. Specifically, we first reformulate the cross-domain recommendation as a causal graph for principled guidance. We then construct a causality-aware dataset heuristically. Subsequently, we derive a theoretically unbiased Partial Label Causal Loss to generalize beyond the biased causality-aware dataset to unseen cross-domain patterns, yielding an enriched cross-domain representation, which is then fed into the target model to enhance target-domain recommendations. Theoretical and empirical analyses, as well as extensive experiments, demonstrate the rationality and effectiveness of CE-CDR and its general applicability as a model-agnostic plugin. Moreover, it has been deployed in production since April 2025, showing its practical value in real-world applications.",
    "summary": "论文研究跨域推荐中因忽略因果关系导致的负迁移问题，核心思想是通过构建因果图和因果感知数据集，设计无偏的因果损失函数来学习增强的跨域表示，从而提升目标域推荐效果。",
    "translation": "跨域推荐的因果性增强",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及推荐系统的核心领域进展，专注于跨域推荐这一重要挑战。因果性增强技术可以显著提升推荐系统的泛化能力和可解释性，这对于处理稀疏数据和冷启动问题具有重要价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统的核心领域——跨域推荐，提出因果增强框架，通过构建因果图和因果感知数据集来解决跨域建模中的负迁移问题，与我的核心领域高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14640v1": {
    "title": "Intent Clustering with Shared Pseudo-Labels",
    "url": "https://www.alphaxiv.org/abs/2510.14640v1",
    "arxiv_id": "2510.14640v1",
    "authors": "I-Fan Lin, Faegheh Hasibi, Suzan Verberne",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2025-10-16 12:54:40",
    "ori_summary": "In this paper, we propose an intuitive, training-free and label-free method for intent clustering that makes minimal assumptions using lightweight and open-source LLMs. Many current approaches rely on commercial LLMs, which are costly, and offer limited transparency. Additionally, their methods often explicitly depend on knowing the number of clusters in advance, which is often not the case in realistic settings. To address these challenges, instead of asking the LLM to match similar text directly, we first ask it to generate pseudo-labels for each text, and then perform multi-label classification in this pseudo-label set for each text. This approach is based on the hypothesis that texts belonging to the same cluster will share more labels, and will therefore be closer when encoded into embeddings. These pseudo-labels are more human-readable than direct similarity matches. Our evaluation on four benchmark sets shows that our approach achieves results comparable to and better than recent baselines, while remaining simple and computationally efficient. Our findings indicate that our method can be applied in low-resource scenarios and is stable across multiple models and datasets.",
    "summary": "该论文研究无监督意图聚类问题，核心思想是通过LLM生成伪标签，然后基于文本共享伪标签的相似性进行聚类，避免了传统方法需要预先指定聚类数量的限制。",
    "translation": "基于共享伪标签的意图聚类",
    "relevance_score": 8,
    "reasoning": "该论文涉及意图聚类，这是搜索和推荐系统中的核心任务，用于理解用户查询意图并改进个性化服务。共享伪标签方法作为一种先进的聚类技术，可直接应用于用户行为分析、查询理解和兴趣建模，提升推荐和搜索的准确性。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出了一种基于共享伪标签的无监督聚类方法，直接适用于推荐系统中的用户意图识别和内容分类，属于核心推荐系统领域的创新方法。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14629v1": {
    "title": "MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation Assistant with LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.14629v1",
    "arxiv_id": "2510.14629v1",
    "authors": "Jiani Huang, Xingchen Zou, Lianghao Xia, Qing Li",
    "categories": "cs.IR",
    "pub_date": "2025-10-16 12:40:48",
    "ori_summary": "The application of Large Language Models (LLMs) in recommender systems faces key challenges in delivering deep personalization and intelligent reasoning, especially for interactive scenarios. Current methods are often constrained by limited context windows and single-turn reasoning, hindering their ability to capture dynamic user preferences and proactively reason over recommendation contexts. To address these limitations, we propose MR.Rec, a novel framework that synergizes memory and reasoning for LLM-based recommendations. To achieve personalization, we develop a comprehensive Retrieval-Augmented Generation (RAG) system that efficiently indexes and retrieves relevant external memory to enhance LLM personalization capabilities. Furthermore, to enable the synergy between memory and reasoning, our RAG system goes beyond conventional query-based retrieval by integrating reasoning enhanced memory retrieval. Finally, we design a reinforcement learning framework that trains the LLM to autonomously learn effective strategies for both memory utilization and reasoning refinement. By combining dynamic memory retrieval with adaptive reasoning, this approach ensures more accurate, context-aware, and highly personalized recommendations. Extensive experiments demonstrate that MR.Rec significantly outperforms state-of-the-art baselines across multiple metrics, validating its efficacy in delivering intelligent and personalized recommendations. We will release code and data upon paper notification.",
    "summary": "论文研究LLM在推荐系统中实现深度个性化和智能推理的挑战，核心思想是通过检索增强生成系统结合强化学习，实现记忆检索与推理过程的协同优化，从而生成更准确、上下文感知的个性化推荐。",
    "translation": "MR.Rec：基于大语言模型的个性化推荐助手，融合记忆与推理能力",
    "relevance_score": 9,
    "reasoning": "该论文直接应用LLM技术构建个性化推荐助手，属于'Direct LLM Applications'范畴。论文标题明确提及'Personalized Recommendation Assistant with LLMs'，展示了LLM在推荐系统中的具体应用，通过结合记忆和推理能力来增强推荐效果，具有明确的实用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM在推荐系统中的核心挑战，提出了结合记忆与推理的创新框架，完全符合直接LLM应用和核心推荐系统发展的关注点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14626v1": {
    "title": "GemiRec: Interest Quantization and Generation for Multi-Interest Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.14626v1",
    "arxiv_id": "2510.14626v1",
    "authors": "Zhibo Wu, Yunfan Wu, Quan Liu, Lin Jiang, Ping Yang, Yao Hu",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-16 12:37:15",
    "ori_summary": "Multi-interest recommendation has gained attention, especially in industrial retrieval stage. Unlike classical dual-tower methods, it generates multiple user representations instead of a single one to model comprehensive user interests. However, prior studies have identified two underlying limitations: the first is interest collapse, where multiple representations homogenize. The second is insufficient modeling of interest evolution, as they struggle to capture latent interests absent from a user's historical behavior. We begin with a thorough review of existing works in tackling these limitations. Then, we attempt to tackle these limitations from a new perspective. Specifically, we propose a framework-level refinement for multi-interest recommendation, named GemiRec. The proposed framework leverages interest quantization to enforce a structural interest separation and interest generation to learn the evolving dynamics of user interests explicitly. It comprises three modules: (a) Interest Dictionary Maintenance Module (IDMM) maintains a shared quantized interest dictionary. (b) Multi-Interest Posterior Distribution Module (MIPDM) employs a generative model to capture the distribution of user future interests. (c) Multi-Interest Retrieval Module (MIRM) retrieves items using multiple user-interest representations. Both theoretical and empirical analyses, as well as extensive experiments, demonstrate its advantages and effectiveness. Moreover, it has been deployed in production since March 2025, showing its practical value in industrial applications.",
    "summary": "论文研究多兴趣推荐中兴趣坍缩和演化建模不足的核心问题，核心方法是通过兴趣量化和兴趣生成框架，强制分离兴趣结构并显式学习用户兴趣的动态演化。",
    "translation": "GemiRec：多兴趣推荐中的兴趣量化与生成",
    "relevance_score": 9,
    "reasoning": "该论文直接针对推荐系统的核心领域进展，专注于多兴趣建模这一关键挑战。兴趣量化和生成技术对于处理用户多样化的偏好模式具有重要价值，能够显著提升个性化推荐的准确性和多样性，属于推荐系统领域的核心技术方向。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统核心问题——多兴趣建模，提出量化与生成框架解决兴趣坍缩和演化不足问题，与工业检索阶段高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14592v1": {
    "title": "Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval",
    "url": "https://www.alphaxiv.org/abs/2510.14592v1",
    "arxiv_id": "2510.14592v1",
    "authors": "Rashmi R, Vidyadhar Upadhya",
    "categories": "cs.LG, cs.IR",
    "pub_date": "2025-10-16 11:55:24",
    "ori_summary": "Current Retrieval-Augmented Generation (RAG) systems primarily operate on unimodal textual data, limiting their effectiveness on unstructured multimodal documents. Such documents often combine text, images, tables, equations, and graphs, each contributing unique information. In this work, we present a Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for multimodal question answering with reasoning through a modality-aware knowledge graph. MAHA integrates dense vector retrieval with structured graph traversal, where the knowledge graph encodes cross-modal semantics and relationships. This design enables both semantically rich and context-aware retrieval across diverse modalities. Evaluations on multiple benchmark datasets demonstrate that MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of 0.486, providing complete modality coverage. These results highlight MAHA's ability to combine embeddings with explicit document structure, enabling effective multimodal retrieval. Our work establishes a scalable and interpretable retrieval framework that advances RAG systems by enabling modality-aware reasoning over unstructured multimodal data.",
    "summary": "论文研究多模态非结构化文档的检索增强生成问题，核心方法是构建模态感知知识图谱并融合稠密向量检索与结构化图遍历，实现跨模态语义关系的统一建模。",
    "translation": "面向非结构化数据的多模态检索增强生成：利用模态感知知识图谱与混合检索",
    "relevance_score": 8,
    "reasoning": "该论文涉及多模态RAG技术，与'VLM类比用于异构数据'焦点高度相关，可将用户行为序列、上下文特征等异构数据视为不同模态进行统一建模。混合检索和知识图谱技术可直接应用于搜索和推荐系统中的多源信息融合与增强检索，提升个性化推荐和搜索相关性。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的多模态知识图谱和混合检索架构直接对应VLM异构数据处理理念，为推荐系统中的多模态内容理解提供了核心方法论。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14545v1": {
    "title": "Agentic Entropy-Balanced Policy Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.14545v1",
    "arxiv_id": "2510.14545v1",
    "authors": "Guanting Dong, Licheng Bao, Zhongyuan Wang, Kangzhi Zhao, Xiaoxi Li, Jiajie Jin, Jinghan Yang, Hangyu Mao, Fuzheng Zhang, Kun Gai, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, Zhicheng Dou",
    "categories": "cs.LG, cs.AI, cs.CL, cs.IR",
    "pub_date": "2025-10-16 10:40:52",
    "ori_summary": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn, long-horizon tool-use capabilities of web agents. While mainstream agentic RL algorithms autonomously explore high-uncertainty tool-call steps under the guidance of entropy, excessive reliance on entropy signals can impose further constraints, leading to the training collapse. In this paper, we delve into the challenges caused by entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an agentic RL algorithm designed to balance entropy in both the rollout and policy update phases. AEPO comprises two core components: (1) a dynamic entropy-balanced rollout mechanism that adaptively allocate global and branch sampling budget through entropy pre-monitoring, while imposing a branch penalty on consecutive high-entropy tool-call steps to prevent over-branching issues; and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient operation into the high-entropy clipping term to preserve and properly rescale gradients on high-entropy tokens, while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens. Results across 14 challenging datasets show that AEPO consistently outperforms 7 mainstream RL algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout sampling diversity while maintaining stable policy entropy, facilitating scalable web agent training.",
    "summary": "",
    "translation": "智能体熵平衡策略优化",
    "relevance_score": 2,
    "reasoning": "该论文标题暗示了强化学习中的策略优化方法，涉及熵平衡概念。虽然强化学习在推荐系统中可能有应用，但标题没有明确表明与RecSys/Search/Ads的直接关联，也没有提到LLM、Transformer或异构数据建模等核心技术。需要更多上下文来确定其实际应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14535v1": {
    "title": "Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval",
    "url": "https://www.alphaxiv.org/abs/2510.14535v1",
    "arxiv_id": "2510.14535v1",
    "authors": "Keima Abe, Hayato Muraki, Shuhei Tomoshige, Kenichi Oishi, Hitoshi Iyatomi",
    "categories": "cs.CV, cs.IR",
    "pub_date": "2025-10-16 10:27:21",
    "ori_summary": "Medical images like MR scans often show domain shifts across imaging sites due to scanner and protocol differences, which degrade machine learning performance in tasks such as disease classification. Domain harmonization is thus a critical research focus. Recent approaches encode brain images $\\boldsymbol{x}$ into a low-dimensional latent space $\\boldsymbol{z}$, then disentangle it into $\\boldsymbol{z_u}$ (domain-invariant) and $\\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these methods often lack interpretability$-$an essential requirement in medical applications$-$leaving practical issues unresolved. We propose Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a general framework for domain harmonization and interpretable representation learning that preserves disease-relevant information in brain MR images. PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract $\\boldsymbol{z_u}$ and $\\boldsymbol{z_d}$, a decoder to reconstruct the image $f_D$, and a domain predictor $g_D$. Beyond adversarial training between the encoder and domain predictor, the model learns to reconstruct the input image $\\boldsymbol{x}$ by summing reconstructions from $\\boldsymbol{z_u}$ and $\\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared to prior methods, PL-SE-ADA achieves equal or better performance in image reconstruction, disease classification, and domain recognition. It also enables visualization of both domain-independent brain features and domain-specific components, offering high interpretability across the entire framework.",
    "summary": "",
    "translation": "基于内容的脑部MR图像检索中可解释领域信息的获取与图像协调",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像处理领域，具体涉及脑部MR图像的协调和基于内容的图像检索，这与推荐系统、搜索或广告的核心领域完全无关。虽然提到了图像检索，但这是医学影像领域的特定应用，没有任何与推荐系统、搜索或广告相关的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14400v1": {
    "title": "MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.14400v1",
    "arxiv_id": "2510.14400v1",
    "authors": "Yingpeng Ning, Yuanyuan Sun, Ling Luo, Yanhua Wang, Yuchen Pan, Hongfei Lin",
    "categories": "cs.CL, cs.AI, cs.IR",
    "pub_date": "2025-10-16 07:59:11",
    "ori_summary": "Biomedical question answering (QA) requires accurate interpretation of complex medical knowledge. Large language models (LLMs) have shown promising capabilities in this domain, with retrieval-augmented generation (RAG) systems enhancing performance by incorporating external medical literature. However, RAG-based approaches in biomedical QA suffer from hallucinations due to post-retrieval noise and insufficient verification of retrieved evidence, undermining response reliability. We propose MedTrust-Guided Iterative RAG, a framework designed to enhance factual consistency and mitigate hallucinations in medical QA. Our method introduces three key innovations. First, it enforces citation-aware reasoning by requiring all generated content to be explicitly grounded in retrieved medical documents, with structured Negative Knowledge Assertions used when evidence is insufficient. Second, it employs an iterative retrieval-verification process, where a verification agent assesses evidence adequacy and refines queries through Medical Gap Analysis until reliable information is obtained. Third, it integrates the MedTrust-Align Module (MTAM) that combines verified positive examples with hallucination-aware negative samples, leveraging Direct Preference Optimization to reinforce citation-grounded reasoning while penalizing hallucination-prone response patterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our approach consistently outperforms competitive baselines across multiple model architectures, achieving the best average accuracy with gains of 2.7% for LLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.",
    "summary": "",
    "translation": "MedTrust-RAG：面向生物医学问答的证据验证与信任对齐",
    "relevance_score": 1,
    "reasoning": "该论文专注于生物医学领域的问答系统，属于明确的医疗领域应用，属于用户指定的无关主题范畴。虽然涉及RAG技术，但其特定领域应用使其与推荐系统、搜索或广告的核心技术进展无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14377v1": {
    "title": "PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora",
    "url": "https://www.alphaxiv.org/abs/2510.14377v1",
    "arxiv_id": "2510.14377v1",
    "authors": "Mykolas Sveistrys, Richard Kunert",
    "categories": "cs.CL, cs.IR, cs.LG",
    "pub_date": "2025-10-16 07:22:58",
    "ori_summary": "Recent advances in large language models (LLMs) and retrieval-augmented generation (RAG) have enabled progress on question answering (QA) when relevant evidence is in one (single-hop) or multiple (multi-hop) passages. Yet many realistic questions about recurring report data - medical records, compliance filings, maintenance logs - require aggregation across all documents, with no clear stopping point for retrieval and high sensitivity to even one missed passage. We term these pluri-hop questions and formalize them by three criteria: recall sensitivity, exhaustiveness, and exactness. To study this setting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48 pluri-hop questions built from 191 real-world wind industry reports in German and English. We show that PluriHopWIND is 8-40% more repetitive than other common datasets and thus has higher density of distractor documents, better reflecting practical challenges of recurring report corpora. We test a traditional RAG pipeline as well as graph-based and multimodal variants, and find that none of the tested approaches exceed 40% in statement-wise F1 score. Motivated by this, we propose PluriHopRAG, a RAG architecture that follows a \"check all documents individually, filter cheaply\" approach: it (i) decomposes queries into document-level subquestions and (ii) uses a cross-encoder filter to discard irrelevant documents before costly LLM reasoning. We find that PluriHopRAG achieves relative F1 score improvements of 18-52% depending on base LLM. Despite its modest size, PluriHopWIND exposes the limitations of current QA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance highlights the value of exhaustive retrieval and early filtering as a powerful alternative to top-k methods.",
    "summary": "",
    "translation": "PluriHop：在干扰项丰富的语料库上进行详尽、召回敏感的问答",
    "relevance_score": 2,
    "reasoning": "该论文主要关注问答系统在干扰项丰富环境下的召回性能，这属于通用NLP问答领域，与推荐系统、搜索或广告的核心技术关联较弱。虽然召回概念在搜索中有一定相关性，但论文的焦点是问答而非信息检索或排序，对当前关注领域的直接应用潜力有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14330v1": {
    "title": "Ensembling Multiple Hallucination Detectors Trained on VLLM Internal Representations",
    "url": "https://www.alphaxiv.org/abs/2510.14330v1",
    "arxiv_id": "2510.14330v1",
    "authors": "Yuto Nakamizo, Ryuhei Miyazato, Hikaru Tanabe, Ryuta Yamakura, Kiori Hatanaka",
    "categories": "cs.IR",
    "pub_date": "2025-10-16 06:09:26",
    "ori_summary": "This paper presents the 5th place solution by our team, y3h2, for the Meta CRAG-MM Challenge at KDD Cup 2025. The CRAG-MM benchmark is a visual question answering (VQA) dataset focused on factual questions about images, including egocentric images. The competition was contested based on VQA accuracy, as judged by an LLM-based automatic evaluator. Since incorrect answers result in negative scores, our strategy focused on reducing hallucinations from the internal representations of the VLM. Specifically, we trained logistic regression-based hallucination detection models using both the hidden_state and the outputs of specific attention heads. We then employed an ensemble of these models. As a result, while our method sacrificed some correct answers, it significantly reduced hallucinations and allowed us to place among the top entries on the final leaderboard. For implementation details and code, please refer to https://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit.",
    "summary": "",
    "translation": "基于视觉语言大模型内部表征训练的多重幻觉检测器集成",
    "relevance_score": 1,
    "reasoning": "该论文专注于幻觉检测这一纯NLP中心主题，属于明确排除的无关主题范畴。虽然涉及VLLM技术，但核心关注点是检测模型输出中的幻觉问题，而非在推荐系统、搜索或广告领域的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14321v1": {
    "title": "Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm",
    "url": "https://www.alphaxiv.org/abs/2510.14321v1",
    "arxiv_id": "2510.14321v1",
    "authors": "Jianting Tang, Dongshuai Li, Tao Wen, Fuyu Lv, Dan Ou, Linli Xu",
    "categories": "cs.IR",
    "pub_date": "2025-10-16 05:37:39",
    "ori_summary": "In modern e-commerce search systems, dense retrieval has become an indispensable component. By computing similarities between query and item (product) embeddings, it efficiently selects candidate products from large-scale repositories. With the breakthroughs in large language models (LLMs), mainstream embedding models have gradually shifted from BERT to LLMs for more accurate text modeling. However, these models still adopt direct-embedding methods, and the semantic accuracy of embeddings remains inadequate. Therefore, contrastive learning is heavily employed to achieve tight semantic alignment between positive pairs. Consequently, such models tend to capture statistical co-occurrence patterns in the training data, biasing them toward shallow lexical and semantic matches. For difficult queries exhibiting notable lexical disparity from target items, the performance degrades significantly. In this work, we propose the Large Reasoning Embedding Model (LREM), which novelly integrates reasoning processes into representation learning. For difficult queries, LREM first conducts reasoning to achieve a deep understanding of the original query, and then produces a reasoning-augmented query embedding for retrieval. This reasoning process effectively bridges the semantic gap between original queries and target items, significantly improving retrieval accuracy. Specifically, we adopt a two-stage training process: the first stage optimizes the LLM on carefully curated Query-CoT-Item triplets with SFT and InfoNCE losses to establish preliminary reasoning and embedding capabilities, and the second stage further refines the reasoning trajectories via reinforcement learning (RL). Extensive offline and online experiments validate the effectiveness of LREM, leading to its deployment on China's largest e-commerce platform since August 2025.",
    "summary": "论文研究电商搜索中语义差距导致的稠密检索性能下降问题，核心思想是通过在表示学习中引入推理过程，让模型对困难查询进行深度理解后生成推理增强的查询嵌入。",
    "translation": "大型推理嵌入模型：迈向下一代稠密检索范式",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及稠密检索范式的核心进展，这是搜索和推荐系统的关键技术。作为使能技术，大型推理嵌入模型可以通过改进语义理解和多模态数据处理，显著提升搜索相关性排序和个性化推荐质量。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对搜索系统中的稠密检索问题，提出将推理过程融入表示学习的新范式，与核心领域进展和直接LLM应用高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14296v1": {
    "title": "Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval Approach for Text-to-SQL",
    "url": "https://www.alphaxiv.org/abs/2510.14296v1",
    "arxiv_id": "2510.14296v1",
    "authors": "Md Mahadi Hasan Nahid, Davood Rafiei, Weiwei Zhang, Yong Zhang",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2025-10-16 04:46:22",
    "ori_summary": "Schema linking -- the process of aligning natural language questions with database schema elements -- is a critical yet underexplored component of Text-to-SQL systems. While recent methods have focused primarily on improving SQL generation, they often neglect the retrieval of relevant schema elements, which can lead to hallucinations and execution failures. In this work, we propose a context-aware bidirectional schema retrieval framework that treats schema linking as a standalone problem. Our approach combines two complementary strategies: table-first retrieval followed by column selection, and column-first retrieval followed by table selection. It is further augmented with techniques such as question decomposition, keyword extraction, and keyphrase extraction. Through comprehensive evaluations on challenging benchmarks such as BIRD and Spider, we demonstrate that our method significantly improves schema recall while reducing false positives. Moreover, SQL generation using our retrieved schema consistently outperforms full-schema baselines and closely approaches oracle performance, all without requiring query refinement. Notably, our method narrows the performance gap between full and perfect schema settings by 50\\%. Our findings highlight schema linking as a powerful lever for enhancing Text-to-SQL accuracy and efficiency.",
    "summary": "",
    "translation": "重新思考模式链接：一种面向文本到SQL的上下文感知双向检索方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文本到SQL转换中的模式链接问题，属于数据库查询优化的特定领域。虽然涉及检索技术，但其应用场景局限于数据库查询而非推荐系统、搜索或广告领域。论文的技术方法（上下文感知双向检索）在通用检索系统中的潜在应用价值有限，与当前关注的核心领域匹配度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14278v1": {
    "title": "PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.14278v1",
    "arxiv_id": "2510.14278v1",
    "authors": "Md Mahadi Hasan Nahid, Davood Rafiei",
    "categories": "cs.CL, cs.AI, cs.IR",
    "pub_date": "2025-10-16 04:02:29",
    "ori_summary": "Retrieval plays a central role in multi-hop question answering (QA), where answering complex questions requires gathering multiple pieces of evidence. We introduce an Agentic Retrieval System that leverages large language models (LLMs) in a structured loop to retrieve relevant evidence with high precision and recall. Our framework consists of three specialized agents: a Question Analyzer that decomposes a multi-hop question into sub-questions, a Selector that identifies the most relevant context for each sub-question (focusing on precision), and an Adder that brings in any missing evidence (focusing on recall). The iterative interaction between Selector and Adder yields a compact yet comprehensive set of supporting passages. In particular, it achieves higher retrieval accuracy while filtering out distracting content, enabling downstream QA models to surpass full-context answer accuracy while relying on significantly less irrelevant information. Experiments on four multi-hop QA benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG -- demonstrates that our approach consistently outperforms strong baselines.",
    "summary": "",
    "translation": "PRISM：基于大语言模型的智能检索用于多跳问答",
    "relevance_score": 6,
    "reasoning": "该论文涉及LLM驱动的检索技术，这直接适用于搜索领域的查询理解和文档检索。多跳推理能力可以增强复杂搜索查询的处理，提升搜索系统的理解深度和准确性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14257v1": {
    "title": "Synergistic Integration and Discrepancy Resolution of Contextualized Knowledge for Personalized Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.14257v1",
    "arxiv_id": "2510.14257v1",
    "authors": "Lingyu Mu, Hao Deng, Haibo Xing, Kaican Lin, Zhitong Zhu, Yu Zhang, Xiaoyi Zeng, Zhengxiao Liu, Zheng Lin, Jinxin Hu",
    "categories": "cs.IR",
    "pub_date": "2025-10-16 03:16:21",
    "ori_summary": "The integration of large language models (LLMs) into recommendation systems has revealed promising potential through their capacity to extract world knowledge for enhanced reasoning capabilities. However, current methodologies that adopt static schema-based prompting mechanisms encounter significant limitations: (1) they employ universal template structures that neglect the multi-faceted nature of user preference diversity; (2) they implement superficial alignment between semantic knowledge representations and behavioral feature spaces without achieving comprehensive latent space integration. To address these challenges, we introduce CoCo, an end-to-end framework that dynamically constructs user-specific contextual knowledge embeddings through a dual-mechanism approach. Our method realizes profound integration of semantic and behavioral latent dimensions via adaptive knowledge fusion and contradiction resolution modules. Experimental evaluations across diverse benchmark datasets and an enterprise-level e-commerce platform demonstrate CoCo's superiority, achieving a maximum 8.58% improvement over seven cutting-edge methods in recommendation accuracy. The framework's deployment on a production advertising system resulted in a 1.91% sales growth, validating its practical effectiveness. With its modular design and model-agnostic architecture, CoCo provides a versatile solution for next-generation recommendation systems requiring both knowledge-enhanced reasoning and personalized adaptation.",
    "summary": "该论文研究如何解决LLM在推荐系统中静态提示机制无法处理用户偏好多样性和语义行为空间浅层对齐的问题。核心方法是构建动态用户特定上下文知识嵌入，通过自适应知识融合和矛盾解决模块实现语义与行为潜在维度的深度整合。",
    "translation": "面向个性化推荐的上下文知识协同集成与差异消解",
    "relevance_score": 9,
    "reasoning": "该论文直接聚焦于个性化推荐系统的核心技术，涉及上下文知识的集成与差异处理，这属于核心推荐系统领域的核心进展。论文标题暗示了处理异构上下文特征的方法，这与VLM类比中处理多模态数据的思路高度相关，可直接应用于提升推荐系统的上下文建模能力。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "论文直接针对LLM在推荐系统中的核心挑战，提出动态知识融合框架，完美契合个性化推荐和LLM应用的研究重点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14223v1": {
    "title": "Large Scale Retrieval for the LinkedIn Feed using Causal Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.14223v1",
    "arxiv_id": "2510.14223v1",
    "authors": "Sudarshan Srinivasa Ramanujam, Antonio Alonso, Saurabh Kataria, Siddharth Dangi, Akhilesh Gupta, Birjodh Singh Tiwana, Manas Somaiya, Luke Simon, David Byrne, Sojeong Ha, Sen Zhou, Andrei Akterskii, Zhanglong Liu, Samira Sriram, Crescent Xiong, Zhoutao Pei, Angela Shao, Alex Li, Annie Xiao, Caitlin Kolb, Thomas Kistler, Zach Moore, Hamed Firooz",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-16 02:01:33",
    "ori_summary": "In large scale recommendation systems like the LinkedIn Feed, the retrieval stage is critical for narrowing hundreds of millions of potential candidates to a manageable subset for ranking. LinkedIn's Feed serves suggested content from outside of the member's network (based on the member's topical interests), where 2000 candidates are retrieved from a pool of hundreds of millions candidate with a latency budget of a few milliseconds and inbound QPS of several thousand per second. This paper presents a novel retrieval approach that fine-tunes a large causal language model (Meta's LLaMA 3) as a dual encoder to generate high quality embeddings for both users (members) and content (items), using only textual input. We describe the end to end pipeline, including prompt design for embedding generation, techniques for fine-tuning at LinkedIn's scale, and infrastructure for low latency, cost effective online serving. We share our findings on how quantizing numerical features in the prompt enables the information to get properly encoded in the embedding, facilitating greater alignment between the retrieval and ranking layer. The system was evaluated using offline metrics and an online A/B test, which showed substantial improvements in member engagement. We observed significant gains among newer members, who often lack strong network connections, indicating that high-quality suggested content aids retention. This work demonstrates how generative language models can be effectively adapted for real time, high throughput retrieval in industrial applications.",
    "summary": "研究大规模推荐系统中从海量候选池快速检索内容的核心问题；核心方法是微调因果语言模型作为双编码器，仅使用文本输入生成用户和内容的高质量嵌入，实现检索层与排序层的更好对齐。",
    "translation": "基于因果语言模型的大规模LinkedIn信息流检索",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及大规模检索系统在社交推荐场景中的应用，属于核心推荐系统领域。因果语言模型作为LLM技术，在推荐系统中具有直接应用价值，可用于理解用户行为序列的因果关系，提升内容检索的准确性和相关性。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用因果语言模型于大规模推荐系统的检索阶段，实现了用户和内容的文本化统一建模，完全契合LLM在推荐系统的直接应用和异构数据统一建模的研究重点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14980v1": {
    "title": "Agentic Design of Compositional Machines",
    "url": "https://www.alphaxiv.org/abs/2510.14980v1",
    "arxiv_id": "2510.14980v1",
    "authors": "Wenqian Zhang, Weiyang Liu, Zhen Liu",
    "categories": "cs.AI, cs.CL, cs.CV, cs.GR, cs.LG",
    "pub_date": "2025-10-16 17:59:58",
    "ori_summary": "The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning.",
    "summary": "",
    "translation": "组合式机器的智能体化设计",
    "relevance_score": 2,
    "reasoning": "该论文标题暗示了关于组合式系统或机器设计的智能体方法，但未明确涉及推荐系统、搜索或广告领域。虽然智能体系统可能在某些场景下应用于推荐决策，但标题过于宽泛且缺乏与LLM、Transformer架构或异构数据建模的直接关联，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14973v1": {
    "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.14973v1",
    "arxiv_id": "2510.14973v1",
    "authors": "Quan Nguyen-Tri, Mukul Ranjan, Zhiqiang Shen",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-16 17:59:48",
    "ori_summary": "This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose ${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that jointly decides ${when}$ to refresh (via an attention-aware drift test on the most-attended token) and ${where}$ to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences, and $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput ($6.8\\times$ on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs.",
    "summary": "",
    "translation": "注意力机制是扩散大语言模型中KV缓存的全部所需",
    "relevance_score": 6,
    "reasoning": "该论文聚焦于Transformer架构中的KV缓存效率优化，这属于'Enabling Transformer Tech'范畴。KV缓存优化技术可以直接应用于推荐系统和搜索中的大规模Transformer模型，显著降低推理延迟和计算成本，对于实时推荐和搜索服务具有重要价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14972v1": {
    "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar",
    "url": "https://www.alphaxiv.org/abs/2510.14972v1",
    "arxiv_id": "2510.14972v1",
    "authors": "Yinxi Li, Yuntian Deng, Pengyu Nie",
    "categories": "cs.CL, cs.AI, cs.LG, cs.PL, cs.SE",
    "pub_date": "2025-10-16 17:59:45",
    "ori_summary": "Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural language text and programming language code but driven by statistics rather than grammar. As a result, semantically identical code snippets can be tokenized differently depending on superficial factors such as whitespace or identifier naming. To measure the impact of this misalignment, we introduce TokDrift, a framework that applies semantic-preserving rewrite rules to create code variants differing only in tokenization. Across nine code LLMs, including large ones with over 30B parameters, even minor formatting changes can cause substantial shifts in model behavior. Layer-wise analysis shows that the issue originates in early embeddings, where subword segmentation fails to capture grammar token boundaries. Our findings identify misaligned tokenization as a hidden obstacle to reliable code understanding and generation, highlighting the need for grammar-aware tokenization for future code LLMs.",
    "summary": "",
    "translation": "TokDrift：当大语言模型以子词说话而代码以语法说话",
    "relevance_score": 3,
    "reasoning": "该论文关注LLM分词与代码语法之间的不匹配问题，属于核心LLM技术进展。虽然这可能在代码搜索或代码推荐中有潜在应用，但论文焦点似乎更偏向通用代码理解而非专门的推荐/搜索/广告领域，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14969v1": {
    "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training",
    "url": "https://www.alphaxiv.org/abs/2510.14969v1",
    "arxiv_id": "2510.14969v1",
    "authors": "Yiming Wang, Da Yin, Yuedong Cui, Ruichen Zheng, Zhiqian Li, Zongyu Lin, Di Wu, Xueqing Wu, Chenchen Ye, Yu Zhou, Kai-Wei Chang",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-16 17:59:38",
    "ori_summary": "Digital agents require diverse, large-scale UI trajectories to generalize across real-world tasks, yet collecting such data is prohibitively expensive in both human annotation, infra and engineering perspectives. To this end, we introduce $\\textbf{UI-Simulator}$, a scalable paradigm that generates structured UI states and transitions to synthesize training trajectories at scale. Our paradigm integrates a digital world simulator for diverse UI states, a guided rollout process for coherent exploration, and a trajectory wrapper that produces high-quality and diverse trajectories for agent training. We further propose $\\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that enables more rapid and data-efficient scaling by prioritizing high-impact tasks and synthesizes informative trajectory variants. Experiments on WebArena and AndroidWorld show that UI-Simulator rivals or surpasses open-source agents trained on real UIs with significantly better robustness, despite using weaker teacher models. Moreover, UI-Simulator-Grow matches the performance of Llama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model, highlighting the potential of targeted synthesis scaling paradigm to continuously and efficiently enhance the digital agents.",
    "summary": "该论文研究数字代理训练中UI轨迹数据收集成本高昂的问题，核心方法是构建基于LLM的可扩展UI模拟器范式，通过数字世界模拟器、引导探索过程和轨迹包装器来合成大规模训练轨迹。",
    "translation": "LLM作为可扩展的通用模拟器用于演进式数字智能体训练",
    "relevance_score": 8,
    "reasoning": "该论文属于'直接LLM应用'范畴，将LLM作为模拟器训练数字智能体，可直接应用于推荐系统和搜索中的用户行为模拟与智能体训练。这种模拟器方法可以用于训练更鲁棒的推荐智能体、模拟用户交互序列，以及优化搜索和广告中的决策策略。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出使用LLM作为数字代理训练的可扩展模拟器，直接应用于数字代理训练场景，与直接LLM应用和推荐系统/搜索领域高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14967v1": {
    "title": "Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents",
    "url": "https://www.alphaxiv.org/abs/2510.14967v1",
    "arxiv_id": "2510.14967v1",
    "authors": "Guoqing Wang, Sunhao Dai, Guangze Ye, Zeyu Gan, Wei Yao, Yong Deng, Xiaofeng Wu, Zhenzhe Ying",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-16 17:59:32",
    "ori_summary": "Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.",
    "summary": "",
    "translation": "基于信息增益的策略优化：一种简单有效的多轮LLM智能体方法",
    "relevance_score": 7,
    "reasoning": "该论文提出了一种基于信息增益的策略优化方法，属于Enabling LLM Tech范畴，在多轮LLM智能体优化方面具有直接应用潜力。这种方法可以应用于搜索和推荐系统中的多轮对话优化，通过信息增益指导策略学习，提升智能体在复杂交互场景中的性能表现。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14961v1": {
    "title": "Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.14961v1",
    "arxiv_id": "2510.14961v1",
    "authors": "Jonas Geiping, Xinyu Yang, Guinan Su",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-16 17:59:07",
    "ori_summary": "Language models with recurrent depth, also referred to as universal or looped when considering transformers, are defined by the capacity to increase their computation through the repetition of layers. Recent efforts in pretraining have demonstrated that these architectures can scale to modern language modeling tasks while exhibiting advantages in reasoning tasks. In this work, we examine the relationship between recurrent-depth models and diffusion language models. Building on their similarities, we develop a new diffusion forcing sampler for these models to accelerate generation. The sampler advances by decoding new tokens at every forward pass of the model, while the latent states of these tokens can be further refined in parallel through recurrence. Theoretically, generation with our sampler is strictly more expressive than the baseline autoregressive generation using the same time budget on modern hardware. Moreover, this sampler, based on principles from diffusion literature, can be directly applied to existing 3.5B recurrent-depth transformers without any tuning, leading to up to a 5x speedup. Consequently, our findings not only provide an efficient mechanism for parallelizing the extra computation in recurrent-depth models at inference, but also suggest that such models can be naturally viewed as strong continuous, though causal, diffusion language models.",
    "summary": "",
    "translation": "用于循环深度模型的高效并行采样器及其与扩散语言模型的联系",
    "relevance_score": 3,
    "reasoning": "该论文主要关注循环深度模型的采样效率和扩散模型的连接，属于模型架构和采样方法的底层技术。虽然这些技术可能间接应用于推荐系统或搜索中的序列建模，但论文标题没有明确指向RecSys/Search/Ads的具体应用场景，相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14958v1": {
    "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.14958v1",
    "arxiv_id": "2510.14958v1",
    "authors": "Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, Linjiang Huang, Si Liu, Rui Liu, Hongsheng Li",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-16 17:58:58",
    "ori_summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: https://mathcanvas.github.io/",
    "summary": "",
    "translation": "MathCanvas：用于多模态数学推理的固有视觉思维链",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态数学推理和视觉思维链技术，属于特定领域应用。虽然涉及多模态建模，但其核心是数学问题解决，与推荐系统、搜索或广告的关联性较弱。在推荐/搜索场景中，数学推理的直接应用有限，且不属于核心领域进展或使能技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14949v1": {
    "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14949v1",
    "arxiv_id": "2510.14949v1",
    "authors": "Yu Zhou, Sohyun An, Haikang Deng, Da Yin, Clark Peng, Cho-Jui Hsieh, Kai-Wei Chang, Nanyun Peng",
    "categories": "cs.CL, cs.CV, cs.LG",
    "pub_date": "2025-10-16 17:56:55",
    "ori_summary": "Contact languages like English exhibit rich regional variations in the form of dialects, which are often used by dialect speakers interacting with generative models. However, can multimodal generative models effectively produce content given dialectal textual input? In this work, we study this question by constructing a new large-scale benchmark spanning six common English dialects. We work with dialect speakers to collect and verify over 4200 unique prompts and evaluate on 17 image and video generative models. Our automatic and human evaluation results show that current state-of-the-art multimodal generative models exhibit 32.26% to 48.17% performance degradation when a single dialect word is used in the prompt. Common mitigation methods such as fine-tuning and prompt rewriting can only improve dialect performance by small margins (< 7%), while potentially incurring significant performance degradation in Standard American English (SAE). To this end, we design a general encoder-based mitigation strategy for multimodal generative models. Our method teaches the model to recognize new dialect features while preserving SAE performance. Experiments on models such as Stable Diffusion 1.5 show that our method is able to simultaneously raise performance on five dialects to be on par with SAE (+34.4%), while incurring near zero cost to SAE performance.",
    "summary": "",
    "translation": "DialectGen：多模态生成中的方言鲁棒性基准测试与改进",
    "relevance_score": 1,
    "reasoning": "该论文专注于多模态生成中的方言鲁棒性基准测试，属于纯粹的NLP和多模态生成领域，与推荐系统、搜索或广告的核心技术进展无关。论文内容涉及方言处理和多模态生成，这些主题在无关主题列表中明确排除，没有明显的推荐/搜索/广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14944v1": {
    "title": "MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics",
    "url": "https://www.alphaxiv.org/abs/2510.14944v1",
    "arxiv_id": "2510.14944v1",
    "authors": "Yuxing Lu, Xukai Zhao, J. Ben Tamo, Micky C. Nnamdi, Rui Peng, Shuang Zeng, Xingyu Hu, Jinzhuo Wang, May D. Wang",
    "categories": "cs.CL, cs.AI, cs.CE",
    "pub_date": "2025-10-16 17:55:14",
    "ori_summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities on general text; however, their proficiency in specialized scientific domains that require deep, interconnected knowledge remains largely uncharacterized. Metabolomics presents unique challenges with its complex biochemical pathways, heterogeneous identifier systems, and fragmented databases. To systematically evaluate LLM capabilities in this domain, we introduce MetaBench, the first benchmark for metabolomics assessment. Curated from authoritative public resources, MetaBench evaluates five capabilities essential for metabolomics research: knowledge, understanding, grounding, reasoning, and research. Our evaluation of 25 open- and closed-source LLMs reveals distinct performance patterns across metabolomics tasks: while models perform well on text generation tasks, cross-database identifier grounding remains challenging even with retrieval augmentation. Model performance also decreases on long-tail metabolites with sparse annotations. With MetaBench, we provide essential infrastructure for developing and evaluating metabolomics AI systems, enabling systematic progress toward reliable computational tools for metabolomics research.",
    "summary": "",
    "translation": "MetaBench：用于评估代谢组学中大型语言模型的多任务基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于代谢组学领域的LLM评估基准，这是一个生物学/医学特定领域应用，属于明确的无关主题。论文标题表明其关注点纯粹是生物医学领域的模型评估，与推荐系统、搜索或广告没有任何技术关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14943v1": {
    "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
    "url": "https://www.alphaxiv.org/abs/2510.14943v1",
    "arxiv_id": "2510.14943v1",
    "authors": "Wenkai Yang, Weijie Liu, Ruobing Xie, Yiju Guo, Lulu Wu, Saiyong Yang, Yankai Lin",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-16 17:55:11",
    "ori_summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.",
    "summary": "",
    "translation": "LaSeR：基于最后令牌自奖励的强化学习",
    "relevance_score": 2,
    "reasoning": "该论文主要关注强化学习中的奖励机制设计，属于纯粹的RL方法研究。虽然强化学习在推荐系统中有所应用，但该论文没有明确展示与RecSys/Search/Ads的直接关联，也没有提到LLM、Transformer架构或异构数据处理等核心技术。其技术路径与当前关注的LLM赋能技术、Transformer架构进展等焦点领域距离较远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14937v1": {
    "title": "AI-Powered Early Diagnosis of Mental Health Disorders from Real-World Clinical Conversations",
    "url": "https://www.alphaxiv.org/abs/2510.14937v1",
    "arxiv_id": "2510.14937v1",
    "authors": "Jianfeng Zhu, Julina Maharjan, Xinyu Li, Karin G. Coifman, Ruoming Jin",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 17:50:04",
    "ori_summary": "Mental health disorders remain among the leading cause of disability worldwide, yet conditions such as depression, anxiety, and Post-Traumatic Stress Disorder (PTSD) are frequently underdiagnosed or misdiagnosed due to subjective assessments, limited clinical resources, and stigma and low awareness. In primary care settings, studies show that providers misidentify depression or anxiety in over 60% of cases, highlighting the urgent need for scalable, accessible, and context-aware diagnostic tools that can support early detection and intervention. In this study, we evaluate the effectiveness of machine learning models for mental health screening using a unique dataset of 553 real-world, semistructured interviews, each paried with ground-truth diagnoses for major depressive episodes (MDE), anxiety disorders, and PTSD. We benchmark multiple model classes, including zero-shot prompting with GPT-4.1 Mini and MetaLLaMA, as well as fine-tuned RoBERTa models using LowRank Adaptation (LoRA). Our models achieve over 80% accuracy across diagnostic categories, with especially strongperformance on PTSD (up to 89% accuracy and 98% recall). We also find that using shorter context, focused context segments improves recall, suggesting that focused narrative cues enhance detection sensitivity. LoRA fine-tuning proves both efficient and effective, with lower-rank configurations (e.g., rank 8 and 16) maintaining competitive performance across evaluation metrics. Our results demonstrate that LLM-based models can offer substantial improvements over traditional self-report screening tools, providing a path toward low-barrier, AI-powerd early diagnosis. This work lays the groundwork for integrating machine learning into real-world clinical workflows, particularly in low-resource or high-stigma environments where access to timely mental health care is most limited.",
    "summary": "",
    "translation": "基于人工智能从真实世界临床对话中早期诊断心理健康障碍",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域的心理健康诊断应用，属于明确的医学领域特定应用，与推荐系统、搜索或广告的核心技术无关。论文内容涉及临床对话分析和医疗诊断，完全落在排除的医疗/生物学应用范畴内，对当前关注的技术方向没有相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14936v1": {
    "title": "Circuit Insights: Towards Interpretability Beyond Activations",
    "url": "https://www.alphaxiv.org/abs/2510.14936v1",
    "arxiv_id": "2510.14936v1",
    "authors": "Elena Golimblevskaia, Aakriti Jain, Bruno Puri, Ammar Ibrahim, Wojciech Samek, Sebastian Lapuschkin",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-16 17:49:41",
    "ori_summary": "The fields of explainable AI and mechanistic interpretability aim to uncover the internal structure of neural networks, with circuit discovery as a central tool for understanding model computations. Existing approaches, however, rely on manual inspection and remain limited to toy tasks. Automated interpretability offers scalability by analyzing isolated features and their activations, but it often misses interactions between features and depends strongly on external LLMs and dataset quality. Transcoders have recently made it possible to separate feature attributions into input-dependent and input-invariant components, providing a foundation for more systematic circuit analysis. Building on this, we propose WeightLens and CircuitLens, two complementary methods that go beyond activation-based analysis. WeightLens interprets features directly from their learned weights, removing the need for explainer models or datasets while matching or exceeding the performance of existing methods on context-independent features. CircuitLens captures how feature activations arise from interactions between components, revealing circuit-level dynamics that activation-only approaches cannot identify. Together, these methods increase interpretability robustness and enhance scalable mechanistic analysis of circuits while maintaining efficiency and quality.",
    "summary": "",
    "translation": "电路洞察：超越激活的模型可解释性研究",
    "relevance_score": 2,
    "reasoning": "该论文主要关注模型可解释性技术，这属于通用的机器学习研究领域。虽然可解释性在推荐和搜索系统中具有一定价值，但论文标题明确聚焦于'超越激活'的电路级解释方法，这更多是模型诊断工具而非直接应用于推荐/搜索/广告的核心技术。没有明确证据表明该工作针对推荐系统或具有特定的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14925v1": {
    "title": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.14925v1",
    "arxiv_id": "2510.14925v1",
    "authors": "Akira Okutomi",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-16 17:40:28",
    "ori_summary": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, viewing reason as a regulator that keeps inference within the bounds of possible experience. We formalize this intuition via a composite instability index (H-Risk) combining spectral margin, conditioning, temporal sensitivity, and innovation amplification. In linear-Gaussian simulations, higher H-Risk predicts overconfident errors even under formal stability, revealing a gap between nominal and epistemic stability. Extending to large language models (LLMs), we find that fragile internal dynamics correlate with miscalibration and hallucination, while critique-style prompts show mixed effects on calibration and hallucination. These results suggest a structural bridge between Kantian self-limitation and feedback control, offering a principled lens for diagnosing -- and selectively reducing -- overconfidence in reasoning systems. This is a preliminary version; supplementary experiments and broader replication will be reported in a future revision.",
    "summary": "",
    "translation": "稳定但校准不足：从滤波器到大型语言模型的过度自信之康德视角",
    "relevance_score": 2,
    "reasoning": "该论文主要探讨LLM的校准问题和过度自信现象，这属于纯粹的LLM中心话题，与评估基准和幻觉问题高度相关。虽然提到了LLM，但论文焦点是哲学视角下的模型校准问题，没有展示在推荐系统、搜索或广告中的具体应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14922v1": {
    "title": "TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG",
    "url": "https://www.alphaxiv.org/abs/2510.14922v1",
    "arxiv_id": "2510.14922v1",
    "authors": "Annisaa Fitri Nurfidausi, Eleonora Mancini, Paolo Torroni",
    "categories": "cs.AI, cs.CL, cs.LG, eess.AS, eess.SP",
    "pub_date": "2025-10-16 17:39:59",
    "ori_summary": "Depression is a widespread mental health disorder, yet its automatic detection remains challenging. Prior work has explored unimodal and multimodal approaches, with multimodal systems showing promise by leveraging complementary signals. However, existing studies are limited in scope, lack systematic comparisons of features, and suffer from inconsistent evaluation protocols. We address these gaps by systematically exploring feature representations and modelling strategies across EEG, together with speech and text. We evaluate handcrafted features versus pre-trained embeddings, assess the effectiveness of different neural encoders, compare unimodal, bimodal, and trimodal configurations, and analyse fusion strategies with attention to the role of EEG. Consistent subject-independent splits are applied to ensure robust, reproducible benchmarking. Our results show that (i) the combination of EEG, speech and text modalities enhances multimodal detection, (ii) pretrained embeddings outperform handcrafted features, and (iii) carefully designed trimodal models achieve state-of-the-art performance. Our work lays the groundwork for future research in multimodal depression detection.",
    "summary": "",
    "translation": "TRI-DEP：基于语音、文本和脑电图的抑郁症检测三模态对比研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域的抑郁症检测，使用语音、文本和脑电图等多模态数据。这属于明确的医学应用领域，与搜索、推荐或广告系统没有任何技术关联，完全超出了相关技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14919v1": {
    "title": "Predicting Task Performance with Context-aware Scaling Laws",
    "url": "https://www.alphaxiv.org/abs/2510.14919v1",
    "arxiv_id": "2510.14919v1",
    "authors": "Kyle Montgomery, David Park, Jianhong Tu, Michael Bendersky, Beliz Gunel, Dawn Song, Chenguang Wang",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-16 17:35:18",
    "ori_summary": "Scaling laws have transformed our understanding of large language models by linking upstream metrics like cross-entropy loss to design factors such as model size, training data, and compute. However, these conventional laws fail to capture downstream task performance, where context plays a critical role. In this work, we propose a straightforward, interpretable framework that jointly models downstream performance as a function of the training compute and the provided context. We empirically validate our framework by fitting it on the observed downstream performance of extended-context variants of Llama-2-7B and Llama-2-13B across 65,500 unique instances spanning three tasks: arithmetic reasoning, common sense reasoning, and machine translation. Our results demonstrate that our framework accurately models in-distribution downstream performance, generalizes across three orders of magnitude in training compute, and reliably extrapolates performance as the amount of context increases. These findings offer valuable insights into the interplay between training compute and context utilization, providing guidance for designing more efficient long-context LLMs for diverse downstream tasks. Our code is available at https://github.com/wang-research-lab/context-scaling.",
    "summary": "该论文研究传统扩展定律无法预测下游任务性能的问题，核心思想是提出一个联合建模训练计算和上下文因素的框架来预测LLM在下游任务中的表现。",
    "translation": "基于上下文感知的缩放定律预测任务性能",
    "relevance_score": 8,
    "reasoning": "该论文涉及缩放定律研究，这是LLM领域的核心进展，对推荐系统、搜索和广告有直接应用价值。上下文感知的缩放定律可以帮助优化模型规模与性能的权衡，指导在特定业务场景下选择最合适的模型架构和参数规模，从而在计算成本约束下实现最佳推荐/搜索效果。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接研究LLM在下游任务中的性能预测，提出了结合训练计算和上下文因素的扩展定律框架，对搜索和推荐系统中上下文建模具有重要参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14915v1": {
    "title": "Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14915v1",
    "arxiv_id": "2510.14915v1",
    "authors": "Xujun Peng, Anoop Kumar, Jingyu Wu, Parker Glenn, Daben Liu",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 17:30:28",
    "ori_summary": "Retrieval-Augmented Generation (RAG) systems leverage Large Language Models (LLMs) to generate accurate and reliable responses that are grounded in retrieved context. However, LLMs often generate inconsistent outputs for semantically equivalent inputs, a problem compounded by the scarcity of consistency-focused training data and the limitations of current fine-tuning techniques in enhancing output consistency. We propose a new approach combining systematic synthetic data generation, triplet loss for better embeddings, and a novel layer-wise model merging approach. Using consistency-aware weights derived from intermediate layer activations, our method effectively integrates knowledge from specialized models. Experimental results how that our merged model significantly enhances output consistency, achieving a ~47.5\\% improvement in response similarity over the baseline, thus offering a practical solution for increasing the reliability of an industrial RAG system.",
    "summary": "",
    "translation": "协调多样化模型：一种用于一致生成的逐层合并策略",
    "relevance_score": 3,
    "reasoning": "该论文关注模型合并策略，属于模型效率优化的范畴，可能对Enabling LLM Tech有一定贡献。然而，论文标题明确聚焦于生成任务（Consistent Generation），这属于AIGC和内容生成领域，根据Irrelevant Topics列表，这些主题应被排除。模型合并技术本身可能有间接应用，但论文的直接应用领域不在关注范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14913v1": {
    "title": "Budget-aware Test-time Scaling via Discriminative Verification",
    "url": "https://www.alphaxiv.org/abs/2510.14913v1",
    "arxiv_id": "2510.14913v1",
    "authors": "Kyle Montgomery, Sijun Tan, Yuqi Chen, Siyuan Zhuang, Tianjun Zhang, Raluca Ada Popa, Chenguang Wang",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-16 17:30:02",
    "ori_summary": "Test-time scaling is a powerful strategy for boosting the performance of large language models on complex reasoning tasks. While state-of-the-art approaches often employ generative verifiers to select the best solution from a pool of candidates, this method incurs prohibitive computational costs, limiting its practicality. In this work, we shift the focus to a more budget-aware paradigm: discriminative verification. We conduct a thorough empirical analysis and demonstrate that while discriminative verifiers may underperform in isolation, combining them with self-consistency in a hybrid approach creates a powerful and efficient test-time scaling mechanism. Notably, under a fixed compute budget, this hybrid approach surpasses state-of-the-art generative verification by a significant margin: achieving up to 15.3\\% higher accuracy on AIME2025. Our findings establish that for practical, real-world applications, budget-aware scaling with discriminative verifiers is not only a \"free\" upgrade over self-consistency, but also a more effective and efficient alternative to costly generative techniques. Code is available at https://github.com/wang-research-lab/verification.",
    "summary": "",
    "translation": "基于判别验证的预算感知测试时缩放",
    "relevance_score": 2,
    "reasoning": "该论文主要关注测试时缩放和预算约束下的模型验证，属于模型效率优化领域。虽然测试时缩放技术可能间接应用于推荐或搜索系统的资源优化，但论文标题未明确表明与RecSys/Search/Ads的直接关联，且缺乏明确的Transformer或LLM技术连接。预算感知方法可能对在线服务系统有一般性参考价值，但应用潜力不够明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14901v1": {
    "title": "Reasoning with Sampling: Your Base Model is Smarter Than You Think",
    "url": "https://www.alphaxiv.org/abs/2510.14901v1",
    "arxiv_id": "2510.14901v1",
    "authors": "Aayush Karan, Yilun Du",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-16 17:18:11",
    "ori_summary": "Frontier reasoning models have exhibited incredible capabilities across a wide array of disciplines, driven by posttraining large language models (LLMs) with reinforcement learning (RL). However, despite the widespread success of this paradigm, much of the literature has been devoted to disentangling truly novel behaviors that emerge during RL but are not present in the base models. In our work, we approach this question from a different angle, instead asking whether comparable reasoning capabilites can be elicited from base models at inference time by pure sampling, without any additional training. Inspired by Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened distributions, we propose a simple iterative sampling algorithm leveraging the base models' own likelihoods. Over different base models, we show that our algorithm offers substantial boosts in reasoning that nearly match and even outperform those from RL on a wide variety of single-shot tasks, including MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in diversity over multiple samples that is characteristic of RL-posttraining. Crucially, our method does not require training, curated datasets, or a verifier, suggesting broad applicability beyond easily verifiable domains.",
    "summary": "该论文研究如何从基础语言模型中激发更强的推理能力，核心思想是设计基于MCMC的迭代采样算法，利用模型自身似然度在推理时提升性能。",
    "translation": "基于采样的推理：您的基础模型比您想象的更智能",
    "relevance_score": 8,
    "reasoning": "该论文探讨推理与采样方法，属于核心LLM技术进步范畴。在推荐系统和搜索中，改进的推理采样技术可以显著提升模型在复杂用户意图理解、多轮对话推荐和多样化结果生成方面的性能，这对于构建更智能的推荐和搜索系统至关重要。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出无需额外训练的推理增强方法，直接应用于基础LLM，对搜索和推荐系统的推理能力提升具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14889v1": {
    "title": "Detecting Early and Implicit Suicidal Ideation via Longitudinal and Information Environment Signals on Social Media",
    "url": "https://www.alphaxiv.org/abs/2510.14889v1",
    "arxiv_id": "2510.14889v1",
    "authors": "Soorya Ram Shimgekar, Ruining Zhao, Agam Goyal, Violeta J. Rodriguez, Paul A. Bloom, Hari Sundaram, Koustuv Saha",
    "categories": "cs.SI, cs.AI, cs.CL, cs.CY, cs.HC",
    "pub_date": "2025-10-16 17:09:14",
    "ori_summary": "On social media, many individuals experiencing suicidal ideation (SI) do not disclose their distress explicitly. Instead, signs may surface indirectly through everyday posts or peer interactions. Detecting such implicit signals early is critical but remains challenging. We frame early and implicit SI as a forward-looking prediction task and develop a computational framework that models a user's information environment, consisting of both their longitudinal posting histories as well as the discourse of their socially proximal peers. We adopted a composite network centrality measure to identify top neighbors of a user, and temporally aligned the user's and neighbors' interactions -- integrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a Reddit study of 1,000 (500 Case and 500 Control) users, our approach improves early and implicit SI detection by 15% over individual-only baselines. These findings highlight that peer interactions offer valuable predictive signals and carry broader implications for designing early detection systems that capture indirect as well as masked expressions of risk in online environments.",
    "summary": "",
    "translation": "基于社交媒体纵向动态与信息环境信号检测早期及隐性自杀意念",
    "relevance_score": 1,
    "reasoning": "该论文专注于心理健康领域的自杀意念检测，属于医疗/心理学应用范畴。虽然涉及社交媒体数据分析，但其核心目标（自杀预防）和技术方法（心理健康风险评估）与推荐系统、搜索或广告领域的技术发展没有直接关联，也不涉及LLM或Transformer架构的进步。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14885v1": {
    "title": "You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction",
    "url": "https://www.alphaxiv.org/abs/2510.14885v1",
    "arxiv_id": "2510.14885v1",
    "authors": "Logan Lawrence, Oindrila Saha, Megan Wei, Chen Sun, Subhransu Maji, Grant Van Horn",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-16 17:04:25",
    "ori_summary": "Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or don't consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language.",
    "summary": "",
    "translation": "畅所欲言：通过答案提取提升多模态大语言模型的细粒度视觉识别能力",
    "relevance_score": 2,
    "reasoning": "该论文专注于提升多模态大语言模型的视觉识别能力，属于纯粹的视觉-语言交叉领域研究。虽然涉及多模态建模，但其核心是视觉细粒度识别，与推荐系统、搜索或广告中的异构数据处理没有直接关联，且未展示明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14871v1": {
    "title": "From Loop Nests to Silicon: Mapping AI Workloads onto AMD NPUs with MLIR-AIR",
    "url": "https://www.alphaxiv.org/abs/2510.14871v1",
    "arxiv_id": "2510.14871v1",
    "authors": "Erwei Wang, Samuel Bayliss, Andra Bisca, Zachary Blair, Sangeeta Chowdhary, Kristof Denolf, Jeff Fifield, Brandon Freiberger, Erika Hunhoff, Phil James-Roxby, Jack Lo, Joseph Melber, Stephen Neuendorffer, Eddie Richter, Andre Rosti, Javier Setoain, Gagandeep Singh, Endri Taka, Pranathi Vasireddy, Zhewen Yu, Niansong Zhang, Jinming Zhuang",
    "categories": "cs.CL, cs.AR, cs.LG",
    "pub_date": "2025-10-16 16:49:05",
    "ori_summary": "General-purpose compilers abstract away parallelism, locality, and synchronization, limiting their effectiveness on modern spatial architectures. As modern computing architectures increasingly rely on fine-grained control over data movement, execution order, and compute placement for performance, compiler infrastructure must provide explicit mechanisms for orchestrating compute and data to fully exploit such architectures. We introduce MLIR-AIR, a novel, open-source compiler stack built on MLIR that bridges the semantic gap between high-level workloads and fine-grained spatial architectures such as AMD's NPUs. MLIR-AIR defines the AIR dialect, which provides structured representations for asynchronous and hierarchical operations across compute and memory resources. AIR primitives allow the compiler to orchestrate spatial scheduling, distribute computation across hardware regions, and overlap communication with computation without relying on ad hoc runtime coordination or manual scheduling. We demonstrate MLIR-AIR's capabilities through two case studies: matrix multiplication and the multi-head attention block from the LLaMA 2 model. For matrix multiplication, MLIR-AIR achieves up to 78.7% compute efficiency and generates implementations with performance almost identical to state-of-the-art, hand-optimized matrix multiplication written using the lower-level, close-to-metal MLIR-AIE framework. For multi-head attention, we demonstrate that the AIR interface supports fused implementations using approximately 150 lines of code, enabling tractable expression of complex workloads with efficient mapping to spatial hardware. MLIR-AIR transforms high-level structured control flow into spatial programs that efficiently utilize the compute fabric and memory hierarchy of an NPU, leveraging asynchronous execution, tiling, and communication overlap through compiler-managed scheduling.",
    "summary": "",
    "translation": "从循环嵌套到硅芯片：使用MLIR-AIR将AI工作负载映射至AMD NPU",
    "relevance_score": 2,
    "reasoning": "该论文主要关注AI工作负载在特定硬件（AMD NPU）上的映射和优化，属于底层系统优化领域。虽然MLIR编译器技术可能间接提升LLM推理效率，但论文焦点是硬件映射而非核心算法创新，与推荐系统、搜索或广告的直接关联性较弱，仅作为潜在的底层效率优化技术存在间接价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14866v1": {
    "title": "Benchmarking Multimodal Large Language Models for Face Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.14866v1",
    "arxiv_id": "2510.14866v1",
    "authors": "Hatef Otroshi Shahreza, Sébastien Marcel",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-10-16 16:42:27",
    "ori_summary": "Multimodal large language models (MLLMs) have achieved remarkable performance across diverse vision-and-language tasks. However, their potential in face recognition remains underexplored. In particular, the performance of open-source MLLMs needs to be evaluated and compared with existing face recognition models on standard benchmarks with similar protocol. In this work, we present a systematic benchmark of state-of-the-art MLLMs for face recognition on several face recognition datasets, including LFW, CALFW, CPLFW, CFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich semantic cues useful for face-related tasks, they lag behind specialized models in high-precision recognition scenarios in zero-shot applications. This benchmark provides a foundation for advancing MLLM-based face recognition, offering insights for the design of next-generation models with higher accuracy and generalization. The source code of our benchmark is publicly available in the project page.",
    "summary": "",
    "translation": "用于人脸识别的多模态大语言模型基准测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态LLM在人脸识别任务上的基准测试，这属于纯粹的视觉应用领域。虽然涉及多模态技术，但人脸识别本身与推荐系统、搜索或广告的核心排名任务没有直接关联，且论文焦点是基准测试而非架构创新或应用开发。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14865v1": {
    "title": "Midtraining Bridges Pretraining and Posttraining Distributions",
    "url": "https://www.alphaxiv.org/abs/2510.14865v1",
    "arxiv_id": "2510.14865v1",
    "authors": "Emmy Liu, Graham Neubig, Chenyan Xiong",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 16:39:52",
    "ori_summary": "Recently, many language models have been pretrained with a \"midtraining\" phase, in which higher quality, often instruction-formatted data, is mixed in at the end of pretraining. Despite the popularity of this practice, there is little scientific understanding of this phase of model training or why it is effective. In this work, we conduct the first systematic investigation of midtraining through controlled experiments with language models pretrained from scratch and fine-tuned on supervised finetuning datasets in different domains. We find that when compared after supervised fine-tuning, the effectiveness of midtraining is highest in the math and code domains, where midtraining can best reduce the syntactic gap between pretraining and posttraining data. In these cases, midtraining consistently outperforms continued pretraining in both in-domain validation loss as well as pretraining data forgetting after posttraining. We conduct ablations on the starting time of the midtraining phase and mixture weights of the midtraining data, using code midtraining as a case study, and find that timing has a greater impact than mixture weights, with earlier introduction of specialized data, yielding greater benefits in-domain as well as preserving general language modeling better. These findings establish midtraining as a domain adaptation technique that compared to continued pretraining yields better performance through reduced forgetting.",
    "summary": "论文研究midtraining阶段在语言模型训练中的作用机制，核心发现是midtraining通过减少预训练与后训练数据之间的语法差距，作为领域适应技术优于持续预训练。",
    "translation": "中期训练桥接预训练与后训练分布",
    "relevance_score": 8,
    "reasoning": "该论文涉及训练分布桥接技术，属于核心LLM技术进展。在推荐系统、搜索和广告中，这种方法可以解决预训练通用模型与特定领域微调之间的分布差异，提升模型在目标任务的适应性和性能，减少领域迁移中的性能下降。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文系统研究midtraining训练范式，揭示其作为领域适应技术的机制，对LLM训练效率优化具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14853v1": {
    "title": "Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models",
    "url": "https://www.alphaxiv.org/abs/2510.14853v1",
    "arxiv_id": "2510.14853v1",
    "authors": "Guinan Su, Yanwu Yang, Li Shen, Lu Yin, Shiwei Liu, Jonas Geiping",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 16:24:36",
    "ori_summary": "Mixture-of-Experts (MoE) models achieve efficient scaling through sparse expert activation, but often suffer from suboptimal routing decisions due to distribution shifts in deployment. While existing test-time adaptation methods could potentially address these issues, they primarily focus on dense models and require access to external data, limiting their practical applicability to MoE architectures. However, we find that, instead of relying on reference data, we can optimize MoE expert selection on-the-fly based only on input context. As such, we propose \\textit{a data-free, online test-time framework} that continuously adapts MoE routing decisions during text generation without external supervision or data. Our method cycles between two phases: During the prefill stage, and later in regular intervals, we optimize the routing decisions of the model using self-supervision based on the already generated sequence. Then, we generate text as normal, maintaining the modified router until the next adaption. We implement this through lightweight additive vectors that only update router logits in selected layers, maintaining computational efficiency while preventing over-adaptation. The experimental results show consistent performance gains on challenging reasoning tasks while maintaining robustness to context shifts. For example, our method achieves a 5.5\\% improvement on HumanEval with OLMoE. Furthermore, owing to its plug-and-play property, our method naturally complements existing test-time scaling techniques, e.g., achieving 6\\% average gains when incorporated with self-consistency on DeepSeek-V2-Lite.",
    "summary": "该论文研究MoE模型在部署中因分布偏移导致的路由决策次优问题，核心方法是提出无需外部数据的在线测试时框架，通过自监督优化路由决策，使用轻量级向量动态调整路由器参数。",
    "translation": "动态重连专家：为混合专家模型实现更好的在线自适应而进行的连续重路由",
    "relevance_score": 9,
    "reasoning": "该论文专注于混合专家模型的在线自适应优化，这直接属于'使能Transformer技术'范畴。在推荐系统和搜索中，MoE模型需要实时适应用户行为变化，动态重路由专家可以显著提升模型在动态环境下的性能表现和效率。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文针对MoE模型的路由优化问题，提出无需外部数据的在线自适应方法，直接涉及Transformer架构效率和MoE技术改进，对推荐系统等领域的模型部署具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14846v1": {
    "title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents",
    "url": "https://www.alphaxiv.org/abs/2510.14846v1",
    "arxiv_id": "2510.14846v1",
    "authors": "Zhuo-Yang Song",
    "categories": "cs.AI, cs.CL, cs.LO",
    "pub_date": "2025-10-16 16:18:37",
    "ori_summary": "The generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science. However, the effectiveness of search depends on where to search, namely, how to encode the domain prior into an operationally structured hypothesis space. To this end, this paper proposes a compact formal theory that describes and measures LLM-assisted iterative search guided by domain priors. We represent an agent as a fuzzy relation operator on inputs and outputs to capture feasible transitions; the agent is thereby constrained by a fixed safety envelope. To describe multi-step reasoning/search, we weight all reachable paths by a single continuation parameter and sum them to obtain a coverage generating function; this induces a measure of reachability difficulty; and it provides a geometric interpretation of search on the graph induced by the safety envelope. We further provide the simplest testable inferences and validate them via a majority-vote instantiation. This theory offers a workable language and operational tools to measure agents and their search spaces, proposing a systematic formal description of iterative search constructed by LLMs.",
    "summary": "",
    "translation": "何处搜索：衡量LLM智能体的先验结构化搜索空间",
    "relevance_score": 4,
    "reasoning": "该论文关注LLM智能体的搜索空间分析，这属于'Enabling LLM Tech'范畴，可能应用于搜索系统中的查询理解和结果排序优化。然而，标题未明确说明其在推荐或广告系统中的具体应用潜力，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14773v1": {
    "title": "Finding Answers in Thought Matters: Revisiting Evaluation on Large Language Models with Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.14773v1",
    "arxiv_id": "2510.14773v1",
    "authors": "Hwiyeol Jo, Joosung Lee, Jaehone Lee, Sang-Woo Lee, Joonsuk Park, Kang Min Yoo",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 15:09:22",
    "ori_summary": "Evaluating generative models, such as large language models (LLMs), commonly involves question-answering tasks where the final answer is selected based on probability of answer choices. On the other hand, for models requiring reasoning, the method of answer extraction plays a critical role. Our research reveals that the performance of reasoning models and their final answer distributions are highly sensitive to the answer extraction algorithm employed. In order to mitigate this, we propose a basic framework: Answer Regeneration. The method uses an additional model inference, providing the prior input and output prefaced by the prompt \"Answer:\". The final answer is then selected or extracted from the regenerated output. We show that this extraction-rule-agnostic approach exhibits improved performance and enhanced robustness. Furthermore, we have applied this framework to general math problems and open-ended question answering tasks. Our analysis and this framework could offer a more reliable results for model evaluation.",
    "summary": "",
    "translation": "在思维过程中寻找答案：基于推理重新审视大型语言模型的评估",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM的评估方法和推理能力，属于纯粹的NLP评估基准范畴。虽然推理能力可能间接影响推荐或搜索系统的性能，但论文本身没有明确指向RecSys/Search/Ads领域的应用，且评估基准属于明确排除的无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14763v1": {
    "title": "COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes",
    "url": "https://www.alphaxiv.org/abs/2510.14763v1",
    "arxiv_id": "2510.14763v1",
    "authors": "Yunwen Li, Shuangshuang Ying, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Tianyu Zheng, Xeron Du, Qiguang Chen, Jiajun Shi, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Libo Qin, Stephen Huang, Wanxiang Che, Chenghua Lin, Eli Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 15:01:19",
    "ori_summary": "Large language models exhibit systematic deficiencies in creative writing, particularly in non-English contexts where training data is scarce and lacks process-level supervision. We present COIG-Writer, a novel Chinese creative writing dataset that captures both diverse outputs and their underlying thought processes through systematic reverse-engineering of high-quality texts. Unlike existing datasets that provide only input-output pairs, COIG-Writer comprises 1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a reverse-engineered prompt, (2) detailed creative reasoning documenting decision-making processes, and (3) the final text. Through comprehensive experiments, we identify a two-component model of creative writing: narrative logic (provided by process supervision) and linguistic expression (maintained by general-purpose data). Our findings reveal three critical insights: (1) Process supervision is highly effective but requires stabilization with general data. A ratio of at least one creative sample to twelve general samples is needed to achieve optimal performance; below this threshold, the win rate progressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities are culturally-bound with no cross-lingual transfer (89.26pp gap between Chinese and English performance), and (3) lexical diversity inversely correlates with creative quality (TTR paradox), suggesting high diversity signals compensatory behavior for logical deficiencies. These findings establish that creative excellence emerges from the interaction between logical scaffolding and linguistic grounding, analogous to how mathematical reasoning enhances but cannot replace linguistic competence in foundation models.",
    "summary": "",
    "translation": "COIG-Writer：一个包含思维过程的高质量中文创意写作数据集",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于中文创意写作数据集构建，属于内容生成领域，与推荐系统、搜索或广告的核心技术无关。虽然涉及思维过程建模，但主要应用于创意写作而非推荐排序、搜索相关性或广告定向等核心业务场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14756v1": {
    "title": "Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware Code",
    "url": "https://www.alphaxiv.org/abs/2510.14756v1",
    "arxiv_id": "2510.14756v1",
    "authors": "Manar Abdelatty, Maryam Nouh, Jacob K. Rosenstein, Sherief Reda",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 14:57:01",
    "ori_summary": "Large Language Models (LLMs) are increasingly used to automate hardware design tasks, including the generation of Verilog code. While early benchmarks focus primarily on functional correctness, efficient hardware design demands additional optimization for synthesis metrics such as area, delay, and power. Existing benchmarks fall short in evaluating these aspects comprehensively: they often lack optimized baselines or testbenches for verification. To address these gaps, we present Pluto, a benchmark and evaluation framework designed to assess the efficiency of LLM-generated Verilog designs. Pluto presents a comprehensive evaluation set of 114 problems with self-checking testbenches and multiple Pareto-optimal reference implementations. Experimental results show that state-of-the-art LLMs can achieve high functional correctness, reaching 78.3\\% at pass@1, but their synthesis efficiency still lags behind expert-crafted implementations, with area efficiency of 63.8\\%, delay efficiency of 65.9\\%, and power efficiency of 64.0\\% at eff@1. This highlights the need for efficiency-aware evaluation frameworks such as Pluto to drive progress in hardware-focused LLM research.",
    "summary": "",
    "translation": "Pluto：用于评估LLM生成硬件代码效率的基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM生成的硬件代码效率评估基准，属于硬件优化和代码生成领域。这与我的关注点（推荐系统、搜索、广告中的核心进展、LLM技术应用或Transformer架构改进）没有直接关联，也没有明显的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14738v1": {
    "title": "AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.14738v1",
    "arxiv_id": "2510.14738v1",
    "authors": "Mengzhao Jia, Zhihan Zhang, Ignacio Cases, Zheyuan Liu, Meng Jiang, Peng Qi",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 14:40:02",
    "ori_summary": "Multimodal large language models (MLLMs) have rapidly advanced from perception tasks to complex multi-step reasoning, yet reinforcement learning with verifiable rewards (RLVR) often leads to spurious reasoning since only the final-answer correctness is rewarded. To address this limitation, we propose AutoRubric-R1V, a framework that integrates RLVR with process-level supervision through automatically collected rubric-based generative rewards. Our key innovation lies in a scalable self-aggregation method that distills consistent reasoning checkpoints from successful trajectories, enabling problem-specific rubric construction without human annotation or stronger teacher models. By jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves state-of-the-art performance on six multimodal reasoning benchmarks and substantially improves reasoning faithfulness in dedicated evaluations.",
    "summary": "",
    "translation": "AutoRubric-R1V：基于评分标准的生成式奖励机制用于忠实多模态推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态推理的忠实性评估和奖励机制，属于生成式AI的评估范畴。虽然涉及多模态技术，但其核心聚焦于推理忠实性和奖励设计，与推荐系统、搜索或广告的核心技术（如排序、召回、用户建模）缺乏直接关联，且更偏向NLP评估而非实际应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14718v1": {
    "title": "Speculative Model Risk in Healthcare AI: Using Storytelling to Surface Unintended Harms",
    "url": "https://www.alphaxiv.org/abs/2510.14718v1",
    "arxiv_id": "2510.14718v1",
    "authors": "Xingmeng Zhao, Dan Schumacher, Veronica Rammouz, Anthony Rios",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 14:18:31",
    "ori_summary": "Artificial intelligence (AI) is rapidly transforming healthcare, enabling fast development of tools like stress monitors, wellness trackers, and mental health chatbots. However, rapid and low-barrier development can introduce risks of bias, privacy violations, and unequal access, especially when systems ignore real-world contexts and diverse user needs. Many recent methods use AI to detect risks automatically, but this can reduce human engagement in understanding how harms arise and who they affect. We present a human-centered framework that generates user stories and supports multi-agent discussions to help people think creatively about potential benefits and harms before deployment. In a user study, participants who read stories recognized a broader range of harms, distributing their responses more evenly across all 13 harm types. In contrast, those who did not read stories focused primarily on privacy and well-being (58.3%). Our findings show that storytelling helped participants speculate about a broader range of harms and benefits and think more creatively about AI's impact on users.",
    "summary": "",
    "translation": "医疗AI中的推测模型风险：利用叙事方法揭示意外危害",
    "relevance_score": 1,
    "reasoning": "该论文聚焦医疗AI领域的模型风险、意外危害和伦理问题，这些都属于明确的无关主题。论文内容涉及医疗领域特定应用和伦理考量，与推荐系统、搜索、广告等核心领域的技术进展完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14662v1": {
    "title": "Semantic Prosody in Machine Translation: the English-Chinese Case of Passive Structures",
    "url": "https://www.alphaxiv.org/abs/2510.14662v1",
    "arxiv_id": "2510.14662v1",
    "authors": "Xinyue Ma, Pol Pastells, Mireia Farrús, Mariona Taulé",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 13:16:59",
    "ori_summary": "Semantic prosody is a collocational meaning formed through the co-occurrence of a linguistic unit and a consistent series of collocates, which should be treated separately from semantic meaning. Since words that are literal translations of each other may have different semantic prosody, more attention should be paid to this linguistic property to generate accurate translations. However, current machine translation models cannot handle this problem. To bridge the gap, we propose an approach to teach machine translation models about semantic prosody of a specific structure. We focus on Chinese BEI passives and create a dataset of English-Chinese sentence pairs with the purpose of demonstrating the negative semantic prosody of BEI passives. Then we fine-tune OPUS-MT, NLLB-600M and mBART50 models with our dataset for the English-Chinese translation task. Our results show that fine-tuned MT models perform better on using BEI passives for translating unfavourable content and avoid using it for neutral and favourable content. Also, in NLLB-600M, which is a multilingual model, this knowledge of semantic prosody can be transferred from English-Chinese translation to other language pairs, such as Spanish-Chinese.",
    "summary": "",
    "translation": "机器翻译中的语义韵律：英语-中文被动结构的案例研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器翻译中特定语言结构（被动结构）的语义韵律问题，属于纯NLP领域的技术细节研究。论文内容不涉及推荐系统、搜索或广告领域的任何技术应用，也没有展示在Transformer架构、LLM技术或异构数据建模方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14628v1": {
    "title": "RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF",
    "url": "https://www.alphaxiv.org/abs/2510.14628v1",
    "arxiv_id": "2510.14628v1",
    "authors": "Qing Yang, Zhenghao Liu, Junxin Wang, Yangfan Du, Pengcheng Huang, Tong Xiao",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 12:40:37",
    "ori_summary": "Text-To-Speech synthesis has achieved near-human quality in neutral speech, but emotional expressiveness remains a challenge. Existing methods often rely on costly emotion annotations or optimize indirect objectives that fail to capture the emotional expressiveness and perceptual naturalness of speech, leading to generated speech that is accurate but emotionally flat. To address these challenges, we propose the RLAIF-SPA framework, incorporating a Reinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic Speech Recognition (ASR) and Large Language Model (LLM) techniques to respectively judge semantic accuracy and prosodic-emotional label alignment as a direct reward for emotional expressiveness and intelligibility optimization. Specifically, it leverages Prosodic Label Alignment to enhance expressive quality by jointly considering semantic accuracy and prosodic-emotional alignment along four fine-grained dimensions: Structure, Emotion, Speed, and Tone. In addition, it incorporates Semantic Accuracy Feedback to ensure the generation of clear and accurate speech. Experiments on the Libri Speech dataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in WER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation.",
    "summary": "",
    "translation": "RLAIF-SPA：通过强化学习从AI反馈优化基于大语言模型的情感语音合成",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音合成技术，属于语音处理领域，与推荐系统、搜索或广告的核心技术无关。虽然提到了LLM和强化学习，但应用场景仅限于语音生成，没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14621v1": {
    "title": "ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks",
    "url": "https://www.alphaxiv.org/abs/2510.14621v1",
    "arxiv_id": "2510.14621v1",
    "authors": "Yuanyi Song, Heyuan Huang, Qiqiang Lin, Yin Zhao, Xiangmou Qu, Jun Wang, Xingyu Lou, Weiwen Liu, Zhuosheng Zhang, Jun Wang, Yong Yu, Weinan Zhang, Zhaoxiang Wang",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-16 12:30:05",
    "ori_summary": "The rapid advancement of multimodal large language models has enabled agents to operate mobile devices by directly interacting with graphical user interfaces, opening new possibilities for mobile automation. However, real-world mobile tasks are often complex and allow for multiple valid solutions. This contradicts current mobile agent evaluation standards: offline static benchmarks can only validate a single predefined \"golden path\", while online dynamic testing is constrained by the complexity and non-reproducibility of real devices, making both approaches inadequate for comprehensively assessing agent capabilities. To bridge the gap between offline and online evaluation and enhance testing stability, this paper introduces a novel graph-structured benchmarking framework. By modeling the finite states observed during real-device interactions, it achieves static simulation of dynamic behaviors. Building on this, we develop ColorBench, a benchmark focused on complex long-horizon tasks. It supports evaluation of multiple valid solutions, subtask completion rate statistics, and atomic-level capability analysis. ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average length of over 13 steps. Each task includes at least two correct paths and several typical error paths, enabling quasi-dynamic interaction. By evaluating ColorBench across various baselines, we discover limitations of existing models and propose improvement directions and feasible technical pathways to enhance agents' performance on complex, long-horizon problems based on experimental results. Code and data are available at: https://github.com/MadeAgents/ColorBench.",
    "summary": "",
    "translation": "ColorBench：基于图结构框架的移动智能体复杂长周期任务基准测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注移动智能体的基准测试和长周期任务评估，这属于机器人学和通用AI智能体领域。虽然提到了图结构框架，但核心焦点是移动智能体的性能评估，与推荐系统、搜索或广告的核心技术进展没有直接关联。图结构框架在推荐系统中虽有应用，但该论文的移动智能体背景使其相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14620v1": {
    "title": "Code-driven Number Sequence Calculation: Enhancing the inductive Reasoning Abilities of Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.14620v1",
    "arxiv_id": "2510.14620v1",
    "authors": "Kedi Chen, Zhikai Lei, Xu Guo, Xuecheng Wu, Siyuan Zeng, Jianghao Yin, Yinqi Zhang, Qin Chen, Jie Zhou, Liang He, Qipeng Guo, Kai Chen, Wei Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 12:29:40",
    "ori_summary": "Large language models (LLMs) make remarkable progress in reasoning tasks. Among different reasoning modes, inductive reasoning, due to its better alignment with human learning, attracts increasing interest. However, research on inductive reasoning faces certain challenges. First, existing inductive data mostly focuses on superficial regularities while lacking more complex internal patterns. Second, current works merely prompt LLMs or finetune on simple prompt-response pairs, but do not provide precise thinking processes nor implement difficulty control. Unlike previous work, we address these challenges by introducing \\textit{CodeSeq}, a synthetic post-training dataset built from number sequences. We package number sequences into algorithmic problems to discover their general terms, defining a general term generation (GTG) task correspondingly. Our pipeline generates supervised finetuning data by reflecting on failed test cases and incorporating iterative corrections, thereby teaching LLMs to learn autonomous case generation and self-checking. Additionally, it leverages reinforcement learning with a novel Case-Synergy Solvability Scaling Reward based on both solvability, estimated from the problem pass rate, and the success rate of self-directed case generation, enabling models to learn more effectively from both successes and failures. Experimental results show that the models trained with \\textit{CodeSeq} improve on various reasoning tasks and can preserve the models' OOD performance.",
    "summary": "",
    "translation": "代码驱动的数字序列计算：增强大语言模型的归纳推理能力",
    "relevance_score": 3,
    "reasoning": "该论文主要关注增强LLM的归纳推理能力，属于'Enabling LLM Tech'范畴，但应用场景仅限于数字序列计算。这种推理能力改进在推荐系统中可能有潜在应用，例如用户行为序列的模式识别和预测，但论文标题未明确表明与RecSys/Search/Ads的直接关联，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14616v1": {
    "title": "Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures",
    "url": "https://www.alphaxiv.org/abs/2510.14616v1",
    "arxiv_id": "2510.14616v1",
    "authors": "Shuangshuang Ying, Yunwen Li, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Xeron Du, Tianyu Zheng, Yichi Zhang, Letian Ni, Yuyang Cheng, Qiguang Chen, Jingzhe Ding, Shengda Long, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Libo Qin, Ge Zhang, Wenhao Huang, Wanxiang Che, Chenghua Lin",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 12:23:13",
    "ori_summary": "Current preference learning methods achieve high accuracy on standard benchmarks but exhibit significant performance degradation when objective quality signals are removed. We introduce WritingPreferenceBench, a dataset of 1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8 creative writing genres, where responses are matched for objective correctness, factual accuracy, and length. On this benchmark, sequence-based reward models--the standard architecture for RLHF--achieve only 52.7% mean accuracy, while zero-shot language model judges perform at 53.9%. In contrast, generative reward models that produce explicit reasoning chains achieve 81.8% accuracy. We observe high within-model variance across genres: individual models range from 18.2% to 81.8% accuracy across different writing categories, with standard deviations averaging 10.1%. This variance persists regardless of model scale, with 27B parameter models showing no consistent improvement over 8B variants. Our results suggest that current RLHF methods primarily learn to detect objective errors rather than capture subjective quality preferences (e.g., creativity, stylistic flair, and emotional resonance), and that successful preference modeling may require intermediate reasoning representations rather than direct classification.",
    "summary": "",
    "translation": "超越正确性：跨文化评估主观写作偏好",
    "relevance_score": 1,
    "reasoning": "该论文专注于跨文化主观写作偏好的评估，这属于纯粹的NLP评估基准范畴，与推荐系统、搜索或广告的核心技术进展无关。论文内容涉及文化差异和写作风格的主观评价，没有展示在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14591v1": {
    "title": "Just-In-Time Objectives: A General Approach for Specialized AI Interactions",
    "url": "https://www.alphaxiv.org/abs/2510.14591v1",
    "arxiv_id": "2510.14591v1",
    "authors": "Michelle S. Lam, Omar Shaikh, Hallie Xu, Alice Guo, Diyi Yang, Jeffrey Heer, James A. Landay, Michael S. Bernstein",
    "categories": "cs.HC, cs.AI, cs.CL",
    "pub_date": "2025-10-16 11:53:17",
    "ori_summary": "Large language models promise a broad set of functions, but when not given a specific objective, they default to milquetoast results such as drafting emails littered with cliches. We demonstrate that inferring the user's in-the-moment objective, then rapidly optimizing for that singular objective, enables LLMs to produce tools, interfaces, and responses that are more responsive and desired. We contribute an architecture for automatically inducing just-in-time objectives by passively observing user behavior, then steering downstream AI systems through generation and evaluation against this objective. Inducing just-in-time objectives (e.g., \"Clarify the abstract's research contribution\") enables automatic generation of tools, e.g., those that critique a draft based on relevant HCI methodologies, anticipate related researchers' reactions, or surface ambiguous terminology. In a series of experiments (N=14, N=205) on participants' own tasks, JIT objectives enable LLM outputs that achieve 66-86% win rates over typical LLMs, and in-person use sessions (N=17) confirm that JIT objectives produce specialized tools unique to each participant.",
    "summary": "",
    "translation": "即时目标：一种用于专用AI交互的通用方法",
    "relevance_score": 2,
    "reasoning": "该论文标题暗示了一种通用的AI交互方法，但未明确涉及推荐系统、搜索或广告的具体技术。标题中的“专用AI交互”可能过于宽泛，无法确定与核心领域进展、LLM技术或Transformer架构的直接关联。缺乏具体的技术细节使得难以评估其在RecSys/Search/Ads中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14583v1": {
    "title": "Talking Points: Describing and Localizing Pixels",
    "url": "https://www.alphaxiv.org/abs/2510.14583v1",
    "arxiv_id": "2510.14583v1",
    "authors": "Matan Rusanovsky, Shimon Malnick, Shai Avidan",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-16 11:42:03",
    "ori_summary": "Vision-language models have achieved remarkable success in cross-modal understanding. Yet, these models remain limited to object-level or region-level grounding, lacking the capability for pixel-precise keypoint comprehension through natural language. We introduce a novel framework for pixel level grounding. The framework consists of two complementary components: a Point Descriptor that generates rich, contextual descriptions of individual keypoints, and a Point Localizer that regresses precise pixel coordinates from these descriptions. Unlike prior work that relies on templated prompts or keypoint names, our approach produces free-form, coarse-to-fine descriptions that situate keypoints within their visual context. Since there is no available dataset to train such a system, we introduce LlamaPointInPart, a carefully curated dataset of 20K+ image-keypoint-description triplets synthesized from multiple vision-language models, capturing multi-scale information from scene-level context to visual features around the keypoint. For cross-category generalization, we optimize the Point Descriptor on AP-10K via GRPO, using the frozen Point Localizer as a reward model to produce descriptions that maximize localization accuracy. To evaluate our results we establish a new evaluation protocol. Instead of comparing the text description produced by our method to the ground truth, we use the localizer to determine how close is the predicted point generated to the ground truth point. Experiments demonstrate superior performance compared to baseline models on LlamaPointInPart.The bidirectional nature of our framework should enable future applications in both keypoint-guided image understanding and language-guided precise localization. Our code and dataset are publicly available at https://github.com/matanr/Talking_Points.",
    "summary": "",
    "translation": "对话点：描述与定位像素",
    "relevance_score": 2,
    "reasoning": "该论文标题表明其专注于视觉内容的像素级描述与定位，属于计算机视觉领域。虽然视觉内容理解在推荐和搜索中有应用，但该论文未明确涉及推荐系统、搜索或广告的核心技术，也未提及LLM或Transformer架构的进展，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14565v1": {
    "title": "Assessing Socio-Cultural Alignment and Technical Safety of Sovereign LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.14565v1",
    "arxiv_id": "2510.14565v1",
    "authors": "Kyubyung Chae, Gihoon Kim, Gyuseong Lee, Taesup Kim, Jaejin Lee, Heejin Kim",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 11:17:44",
    "ori_summary": "Recent trends in LLMs development clearly show growing interest in the use and application of sovereign LLMs. The global debate over sovereign LLMs highlights the need for governments to develop their LLMs, tailored to their unique socio-cultural and historical contexts. However, there remains a shortage of frameworks and datasets to verify two critical questions: (1) how well these models align with users' socio-cultural backgrounds, and (2) whether they maintain safety and technical robustness without exposing users to potential harms and risks. To address this gap, we construct a new dataset and introduce an analytic framework for extracting and evaluating the socio-cultural elements of sovereign LLMs, alongside assessments of their technical robustness. Our experimental results demonstrate that while sovereign LLMs play a meaningful role in supporting low-resource languages, they do not always meet the popular claim that these models serve their target users well. We also show that pursuing this untested claim may lead to underestimating critical quality attributes such as safety. Our study suggests that advancing sovereign LLMs requires a more extensive evaluation that incorporates a broader range of well-grounded and practical criteria.",
    "summary": "",
    "translation": "评估主权大语言模型的社会文化对齐与技术安全性",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM的社会文化对齐和安全性评估，属于伦理、公平性和安全性的范畴，这些是明确列出的不相关主题。论文标题没有表明任何与推荐系统、搜索或广告相关的技术应用，也没有涉及Transformer架构改进或异构数据建模等核心技术领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14509v1": {
    "title": "E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task",
    "url": "https://www.alphaxiv.org/abs/2510.14509v1",
    "arxiv_id": "2510.14509v1",
    "authors": "Jingyao Liu, Chen Huang, Zhizhao Guan, Wenqiang Lei, Yang Deng",
    "categories": "cs.SE, cs.AI, cs.CL",
    "pub_date": "2025-10-16 09:54:26",
    "ori_summary": "E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple BDD test scenarios with corresponding Python step implementations for each requirement}, and (iii) a fully automated testing pipeline built on the Behave framework. To ensure its quality while reducing the annotation effort, E2EDev leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework (HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with E2EDev}, our analysis reveals a persistent struggle to effectively solve these tasks, underscoring the critical need for more effective and cost-efficient E2ESD solutions. Our codebase and benchmark are publicly available at https://github.com/SCUNLP/E2EDev.",
    "summary": "",
    "translation": "E2Edev：在端到端软件开发任务中评估大型语言模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM在软件开发任务中的基准测试，属于纯粹的LLM应用评估范畴。虽然涉及LLM技术，但软件开发与推荐系统、搜索或广告领域没有直接关联，且论文重点在于评估而非核心技术进步或架构创新，无法看出在目标领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14504v1": {
    "title": "Efficient Seq2seq Coreference Resolution Using Entity Representations",
    "url": "https://www.alphaxiv.org/abs/2510.14504v1",
    "arxiv_id": "2510.14504v1",
    "authors": "Matt Grenander, Shay B. Cohen, Mark Steedman",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 09:50:03",
    "ori_summary": "Seq2seq coreference models have introduced a new paradigm for coreference resolution by learning to generate text corresponding to coreference labels, without requiring task-specific parameters. While these models achieve new state-of-the-art performance, they do so at the cost of flexibility and efficiency. In particular, they do not efficiently handle incremental settings such as dialogue, where text must processed sequentially. We propose a compressed representation in order to improve the efficiency of these methods in incremental settings. Our method works by extracting and re-organizing entity-level tokens, and discarding the majority of other input tokens. On OntoNotes, our best model achieves just 0.6 CoNLL F1 points below a full-prefix, incremental baseline while achieving a compression ratio of 1.8. On LitBank, where singleton mentions are annotated, it passes state-of-the-art performance. Our results indicate that discarding a wide portion of tokens in seq2seq resolvers is a feasible strategy for incremental coreference resolution.",
    "summary": "",
    "translation": "基于实体表示的高效序列到序列共指消解",
    "relevance_score": 2,
    "reasoning": "该论文专注于序列到序列模型在共指消解任务中的应用，这是NLP领域的核心任务，与推荐系统、搜索或广告的直接相关性较弱。虽然实体表示技术可能对用户建模有一定启发，但论文主要解决的是文本理解中的指代消解问题，缺乏明确的推荐/搜索/广告应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14466v1": {
    "title": "LiRA: Linguistic Robust Anchoring for Cross-lingual Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.14466v1",
    "arxiv_id": "2510.14466v1",
    "authors": "Haolin Li, Haipeng Zhang, Mang Li, Yaohua Wang, Lijie Wen, Yu Zhang, Biqing Huang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 09:08:24",
    "ori_summary": "As large language models (LLMs) rapidly advance, performance on high-resource languages (e.g., English, Chinese) is nearing saturation, yet remains substantially lower for low-resource languages (e.g., Urdu, Thai) due to limited training data, machine-translation noise, and unstable cross-lingual alignment. We introduce LiRA (Linguistic Robust Anchoring for Large Language Models), a training framework that robustly improves cross-lingual representations under low-resource conditions while jointly strengthening retrieval and reasoning. LiRA comprises two modules: (i) Arca (Anchored Representation Composition Architecture), which anchors low-resource languages to an English semantic space via anchor-based alignment and multi-agent collaborative encoding, preserving geometric stability in a shared embedding space; and (ii) LaSR (Language-coupled Semantic Reasoner), which adds a language-aware lightweight reasoning head with consistency regularization on top of Arca's multilingual representations, unifying the training objective to enhance cross-lingual understanding, retrieval, and reasoning robustness. We further construct and release a multilingual product retrieval dataset covering five Southeast Asian and two South Asian languages. Experiments across low-resource benchmarks (cross-lingual retrieval, semantic similarity, and reasoning) show consistent gains and robustness under few-shot and noise-amplified settings; ablations validate the contribution of both Arca and LaSR. Code will be released on GitHub and the dataset on Hugging Face.",
    "summary": "",
    "translation": "LiRA：面向跨语言大语言模型的语言鲁棒锚定",
    "relevance_score": 2,
    "reasoning": "该论文专注于跨语言大语言模型的语言鲁棒性技术，属于核心LLM技术的进步。虽然跨语言能力在理论上可能应用于多语言搜索或推荐场景，但论文标题未明确指向搜索、推荐或广告领域的特定应用，且跨语言建模与当前关注的异构数据统一建模或直接应用关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14453v1": {
    "title": "Natural Language Tools: A Natural Language Approach to Tool Calling In Large Language Agents",
    "url": "https://www.alphaxiv.org/abs/2510.14453v1",
    "arxiv_id": "2510.14453v1",
    "authors": "Reid T. Johnson, Michelle D. Pain, Jordan D. West",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 08:52:52",
    "ori_summary": "We present Natural Language Tools (NLT), a framework that replaces programmatic JSON tool calling in large language models (LLMs) with natural language outputs. By decoupling tool selection from response generation, NLT eliminates task interference and format constraints that degrade tool call performance. When evaluated across 10 models and 6,400 trials spanning customer service and mental health domains, NLT improves tool calling accuracy by 18.4 percentage points while reducing output variance by 70%. Open-weight models see the largest gains, surpassing flagship closed-weight alternatives, with implications for model training in both reinforcement learning and supervised fine-tuning stages. These improvements persist under prompt perturbations and extend tool-calling capabilities to models lacking native support.",
    "summary": "",
    "translation": "自然语言工具：面向大型语言智能体中工具调用的自然语言方法",
    "relevance_score": 6,
    "reasoning": "该论文涉及LLM工具调用能力，这在推荐系统和搜索中具有应用潜力，例如通过自然语言接口调用外部API获取实时信息或执行复杂推理。然而，论文焦点更偏向通用工具调用框架，而非专门针对RecSys/Search/Ads的优化，因此相关性中等。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14438v1": {
    "title": "Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents",
    "url": "https://www.alphaxiv.org/abs/2510.14438v1",
    "arxiv_id": "2510.14438v1",
    "authors": "Rui Wang, Ce Zhang, Jun-Yu Ma, Jianshu Zhang, Hongru Wang, Yi Chen, Boyang Xue, Tianqing Fang, Zhisong Zhang, Hongming Zhang, Haitao Mi, Dong Yu, Kam-Fai Wong",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 08:37:42",
    "ori_summary": "Deep research web agents not only retrieve information from diverse sources such as web environments, files, and multimodal inputs, but more importantly, they need to rigorously analyze and aggregate knowledge for insightful research. However, existing open-source deep research agents predominantly focus on enhancing information-seeking capabilities of web agents to locate specific information, while overlooking the essential need for information aggregation, which would limit their ability to support in-depth research. We propose an Explore to Evolve paradigm to scalably construct verifiable training data for web agents. Begins with proactive online exploration, an agent sources grounded information by exploring the real web. Using the collected evidence, the agent then self-evolves an aggregation program by selecting, composing, and refining operations from 12 high-level logical types to synthesize a verifiable QA pair. This evolution from high-level guidance to concrete operations allowed us to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K websites and 11 domains. Based on an open-source agent framework, SmolAgents, we collect supervised fine-tuning trajectories to develop a series of foundation models, WebAggregator. WebAggregator-8B matches the performance of GPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text and closely approaches Claude-3.7-sonnet. Moreover, given the limited availability of benchmarks that evaluate web agents' information aggregation abilities, we construct a human-annotated evaluation split of WebAggregatorQA as a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves 28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all references, they still struggle on WebAggregatorQA, highlighting the need to strengthen the information aggregation capabilities of web agent foundations.",
    "summary": "",
    "translation": "探索以进化：通过主动在线探索扩展进化聚合逻辑用于深度研究智能体",
    "relevance_score": 2,
    "reasoning": "该论文主要关注深度研究智能体的进化聚合逻辑和在线探索机制，这属于通用AI智能体领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然提到了在线探索，但缺乏明确的连接点说明这些技术如何应用于RecSys/Search/Ads领域的具体场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14420v1": {
    "title": "Instructions are all you need: Self-supervised Reinforcement Learning for Instruction Following",
    "url": "https://www.alphaxiv.org/abs/2510.14420v1",
    "arxiv_id": "2510.14420v1",
    "authors": "Qingyu Ren, Qianyu He, Bowei Zhang, Jie Zeng, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 08:24:44",
    "ori_summary": "Language models often struggle to follow multi-constraint instructions that are crucial for real-world applications. Existing reinforcement learning (RL) approaches suffer from dependency on external supervision and sparse reward signals from multi-constraint tasks. We propose a label-free self-supervised RL framework that eliminates dependency on external supervision by deriving reward signals directly from instructions and generating pseudo-labels for reward model training. Our approach introduces constraint decomposition strategies and efficient constraint-wise binary classification to address sparse reward challenges while maintaining computational efficiency. Experiments show that our approach generalizes well, achieving strong improvements across 3 in-domain and 5 out-of-domain datasets, including challenging agentic and multi-turn instruction following. The data and code are publicly available at https://github.com/Rainier-rq/verl-if",
    "summary": "",
    "translation": "指令即所需：用于指令跟随的自监督强化学习",
    "relevance_score": 3,
    "reasoning": "该论文主要关注自监督强化学习用于指令跟随，这属于强化学习的通用方法研究。虽然强化学习在推荐系统中可能有应用，但论文标题没有明确指向RecSys/Search/Ads领域的具体应用场景，也没有展示与Transformer架构或LLM技术的直接关联，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14406v1": {
    "title": "IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning",
    "url": "https://www.alphaxiv.org/abs/2510.14406v1",
    "arxiv_id": "2510.14406v1",
    "authors": "Xikai Zhang, Bo Wang, Likang Xiao, Yongzhi Li, Quan Chen, Wenju Wu, Liu Liu",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-16 08:06:35",
    "ori_summary": "Although large language models (LLMs) have made significant strides across various tasks, they still face significant challenges in complex reasoning and planning. For example, even with carefully designed prompts and prior information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on the TravelPlanner dataset in the sole-planning mode. Similarly, even in the thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent Systems (MAS) can offer improved collective reasoning, they often suffer from high reasoning costs due to multi-round internal interactions, long per-response latency, and difficulties in end-to-end training. To address these challenges, we propose a general and scalable framework called IMAGINE, short for Integrating Multi-Agent System into One Model. This framework not only integrates the reasoning and planning capabilities of MAS into a single, compact model, but also significantly surpass the capabilities of the MAS through a simple end-to-end training. Through this pipeline, a single small-scale model is not only able to acquire the structured reasoning and planning capabilities of a well-organized MAS but can also significantly outperform it. Experimental results demonstrate that, when using Qwen3-8B-Instruct as the base model and training it with our method, the model achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.",
    "summary": "",
    "translation": "IMAGINE：将多智能体系统集成到单一模型中用于复杂推理与规划",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多智能体系统的复杂推理与规划，属于通用AI推理领域。虽然推理能力对搜索和推荐有一定价值，但论文标题未明确涉及推荐系统、搜索或广告的具体应用场景，也未提及Transformer架构改进或异质数据建模等核心技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14398v1": {
    "title": "Your Next Token Prediction: A Multilingual Benchmark for Personalized Response Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14398v1",
    "arxiv_id": "2510.14398v1",
    "authors": "Shiyao Ding, Takayuki Ito",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 07:54:02",
    "ori_summary": "Large language models (LLMs) excel at general next-token prediction but still struggle to generate responses that reflect how individuals truly communicate, such as replying to emails or social messages in their own style. However, real SNS or email histories are difficult to collect due to privacy concerns. To address this, we propose the task of \"Your Next Token Prediction (YNTP)\", which models a user's precise word choices through controlled human-agent conversations. We build a multilingual benchmark of 100 dialogue sessions across English, Japanese, and Chinese, where users interact for five days with psychologically grounded NPCs based on MBTI dimensions. This setup captures natural, daily-life communication patterns and enables analysis of users' internal models. We evaluate prompt-based and fine-tuning-based personalization methods, establishing the first benchmark for YNTP and a foundation for user-aligned language modeling. The dataset is available at: https://github.com/AnonymousHub4Submissions/your-next-token-prediction-dataset-100",
    "summary": "论文研究如何让语言模型生成反映个体真实沟通风格的个性化响应，核心方法是构建基于MBTI心理维度的多语言对话数据集，通过用户与NPC的日常互动来建模用户的精确词汇选择和内部语言模型。",
    "translation": "您的下一个令牌预测：面向个性化响应生成的多语言基准",
    "relevance_score": 8,
    "reasoning": "该论文涉及个性化响应生成和多语言基准，直接适用于推荐系统和搜索领域的个性化内容生成。作为LLM应用，它在个性化响应生成方面具有明确的RecSys/Search应用潜力，特别是在多语言环境下为用户提供定制化内容推荐和搜索响应。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对个性化响应生成这一推荐系统核心问题，通过构建多语言基准数据集来研究用户个性化语言建模，与个性化推荐高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14395v1": {
    "title": "Suicidal Comment Tree Dataset: Enhancing Risk Assessment and Prediction Through Contextual Analysis",
    "url": "https://www.alphaxiv.org/abs/2510.14395v1",
    "arxiv_id": "2510.14395v1",
    "authors": "Jun Li, Qun Zhao",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 07:47:03",
    "ori_summary": "Suicide remains a critical global public health issue. While previous studies have provided valuable insights into detecting suicidal expressions in individual social media posts, limited attention has been paid to the analysis of longitudinal, sequential comment trees for predicting a user's evolving suicidal risk. Users, however, often reveal their intentions through historical posts and interactive comments over time. This study addresses this gap by investigating how the information in comment trees affects both the discrimination and prediction of users' suicidal risk levels. We constructed a high-quality annotated dataset, sourced from Reddit, which incorporates users' posting history and comments, using a refined four-label annotation framework based on the Columbia Suicide Severity Rating Scale (C-SSRS). Statistical analysis of the dataset, along with experimental results from Large Language Models (LLMs) experiments, demonstrates that incorporating comment trees data significantly enhances the discrimination and prediction of user suicidal risk levels. This research offers a novel insight to enhancing the detection accuracy of at-risk individuals, thereby providing a valuable foundation for early suicide intervention strategies.",
    "summary": "",
    "translation": "自杀评论树数据集：通过上下文分析增强风险评估与预测",
    "relevance_score": 1,
    "reasoning": "该论文专注于心理健康风险评估和自杀预防，这属于医疗/心理学领域，与推荐系统、搜索或广告的核心技术无关。论文内容涉及风险评估和预测，但这些应用场景完全在医疗健康领域，没有明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14381v1": {
    "title": "Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers",
    "url": "https://www.alphaxiv.org/abs/2510.14381v1",
    "arxiv_id": "2510.14381v1",
    "authors": "Andrew Zhao, Reshmi Ghosh, Vitor Carvalho, Emily Lawton, Keegan Hines, Gao Huang, Jack W. Stokes",
    "categories": "cs.LG, cs.AI, cs.CL, cs.CR",
    "pub_date": "2025-10-16 07:28:54",
    "ori_summary": "Large language model (LLM) systems now underpin everyday AI applications such as chatbots, computer-use assistants, and autonomous robots, where performance often depends on carefully designed prompts. LLM-based prompt optimizers reduce that effort by iteratively refining prompts from scored feedback, yet the security of this optimization stage remains underexamined. We present the first systematic analysis of poisoning risks in LLM-based prompt optimization. Using HarmBench, we find systems are substantially more vulnerable to manipulated feedback than to injected queries: feedback-based attacks raise attack success rate (ASR) by up to $\\Delta$ASR = 0.48. We introduce a simple fake-reward attack that requires no access to the reward model and significantly increases vulnerability, and we propose a lightweight highlighting defense that reduces the fake-reward $\\Delta$ASR from 0.23 to 0.07 without degrading utility. These results establish prompt optimization pipelines as a first-class attack surface and motivate stronger safeguards for feedback channels and optimization frameworks.",
    "summary": "",
    "translation": "我的优化提示词是否已遭泄露？探索基于大语言模型的优化器的安全漏洞",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM优化器的安全漏洞和提示词泄露问题，这属于安全/隐私领域，在无关主题列表中明确排除。虽然涉及LLM优化器技术，但论文焦点是安全漏洞而非核心推荐/搜索/广告应用或使能技术进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14369v1": {
    "title": "From Binary to Bilingual: How the National Weather Service is Using Artificial Intelligence to Develop a Comprehensive Translation Program",
    "url": "https://www.alphaxiv.org/abs/2510.14369v1",
    "arxiv_id": "2510.14369v1",
    "authors": "Joseph E. Trujillo-Falcon, Monica L. Bozeman, Liam E. Llewellyn, Samuel T. Halvorson, Meryl Mizell, Stuti Deshpande, Bob Manning, Todd Fagin",
    "categories": "cs.CL, cs.AI, cs.CY, cs.HC",
    "pub_date": "2025-10-16 07:06:05",
    "ori_summary": "To advance a Weather-Ready Nation, the National Weather Service (NWS) is developing a systematic translation program to better serve the 68.8 million people in the U.S. who do not speak English at home. This article outlines the foundation of an automated translation tool for NWS products, powered by artificial intelligence. The NWS has partnered with LILT, whose patented training process enables large language models (LLMs) to adapt neural machine translation (NMT) tools for weather terminology and messaging. Designed for scalability across Weather Forecast Offices (WFOs) and National Centers, the system is currently being developed in Spanish, Simplified Chinese, Vietnamese, and other widely spoken non-English languages. Rooted in best practices for multilingual risk communication, the system provides accurate, timely, and culturally relevant translations, significantly reducing manual translation time and easing operational workloads across the NWS. To guide the distribution of these products, GIS mapping was used to identify language needs across different NWS regions, helping prioritize resources for the communities that need them most. We also integrated ethical AI practices throughout the program's design, ensuring that transparency, fairness, and human oversight guide how automated translations are created, evaluated, and shared with the public. This work has culminated into a website featuring experimental multilingual NWS products, including translated warnings, 7-day forecasts, and educational campaigns, bringing the country one step closer to a national warning system that reaches all Americans.",
    "summary": "",
    "translation": "从二元到双语：国家气象局如何利用人工智能开发综合性翻译项目",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于气象领域的翻译应用，属于特定领域（气象）的AI应用，与推荐系统、搜索或广告的核心技术进展无关。论文内容涉及翻译程序开发，属于纯粹的NLP应用，没有展示在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14365v1": {
    "title": "On the Ability of LLMs to Handle Character-Level Perturbations: How Well and How?",
    "url": "https://www.alphaxiv.org/abs/2510.14365v1",
    "arxiv_id": "2510.14365v1",
    "authors": "Anyun Zhuo, Xuefei Ning, Ningyuan Li, Yu Wang, Pinyan Lu",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 06:59:58",
    "ori_summary": "This work investigates the resilience of contemporary LLMs against frequent and structured character-level perturbations, specifically through the insertion of noisy characters after each input character. We introduce \\nameshort{}, a practical method that inserts invisible Unicode control characters into text to discourage LLM misuse in scenarios such as online exam systems. Surprisingly, despite strong obfuscation that fragments tokenization and reduces the signal-to-noise ratio significantly, many LLMs still maintain notable performance. Through comprehensive evaluation across model-, problem-, and noise-related configurations, we examine the extent and mechanisms of this robustness, exploring both the handling of character-level tokenization and \\textit{implicit} versus \\textit{explicit} denoising mechanism hypotheses of character-level noises. We hope our findings on the low-level robustness of LLMs will shed light on the risks of their misuse and on the reliability of deploying LLMs across diverse applications.",
    "summary": "",
    "translation": "论大语言模型处理字符级扰动的能力：表现如何及机制探究",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLMs对字符级扰动的鲁棒性，这属于纯NLP评估范畴，与推荐系统、搜索或广告的核心技术进展无关。虽然LLM鲁棒性在理论上可能间接影响文本处理任务，但论文本身没有展示在RecSys/Search/Ads领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14359v1": {
    "title": "AI for Service: Proactive Assistance with AI Glasses",
    "url": "https://www.alphaxiv.org/abs/2510.14359v1",
    "arxiv_id": "2510.14359v1",
    "authors": "Zichen Wen, Yiyu Wang, Chenfei Liao, Boxue Yang, Junxian Li, Weifeng Liu, Haocong He, Bolong Feng, Xuyang Liu, Yuanhuiyi Lyu, Xu Zheng, Xuming Hu, Linfeng Zhang",
    "categories": "cs.AI, cs.CL, cs.CV",
    "pub_date": "2025-10-16 06:55:28",
    "ori_summary": "In an era where AI is evolving from a passive tool into an active and adaptive companion, we introduce AI for Service (AI4Service), a new paradigm that enables proactive and real-time assistance in daily life. Existing AI services remain largely reactive, responding only to explicit user commands. We argue that a truly intelligent and helpful assistant should be capable of anticipating user needs and taking actions proactively when appropriate. To realize this vision, we propose Alpha-Service, a unified framework that addresses two fundamental challenges: Know When to intervene by detecting service opportunities from egocentric video streams, and Know How to provide both generalized and personalized services. Inspired by the von Neumann computer architecture and based on AI glasses, Alpha-Service consists of five key components: an Input Unit for perception, a Central Processing Unit for task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit for long-term personalization, and an Output Unit for natural human interaction. As an initial exploration, we implement Alpha-Service through a multi-agent system deployed on AI glasses. Case studies, including a real-time Blackjack advisor, a museum tour guide, and a shopping fit assistant, demonstrate its ability to seamlessly perceive the environment, infer user intent, and provide timely and useful assistance without explicit prompts.",
    "summary": "",
    "translation": "面向服务的AI：通过AI眼镜实现主动辅助",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于可穿戴AI设备和主动服务辅助，属于应用层的人机交互范畴。虽然涉及AI技术，但未提及推荐系统、搜索或广告的核心技术，也没有涉及LLM、Transformer架构或异构数据建模等关键技术方向。该研究更偏向于终端设备应用而非核心算法进步。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14353v1": {
    "title": "CURE: Confidence-driven Unified Reasoning Ensemble Framework for Medical Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.14353v1",
    "arxiv_id": "2510.14353v1",
    "authors": "Ziad Elshaer, Essam A. Rashed",
    "categories": "cs.CL, cs.AI, physics.med-ph",
    "pub_date": "2025-10-16 06:46:11",
    "ori_summary": "High-performing medical Large Language Models (LLMs) typically require extensive fine-tuning with substantial computational resources, limiting accessibility for resource-constrained healthcare institutions. This study introduces a confidence-driven multi-model framework that leverages model diversity to enhance medical question answering without fine-tuning. Our framework employs a two-stage architecture: a confidence detection module assesses the primary model's certainty, and an adaptive routing mechanism directs low-confidence queries to Helper models with complementary knowledge for collaborative reasoning. We evaluate our approach using Qwen3-30B-A3B-Instruct, Phi-4 14B, and Gemma 2 12B across three medical benchmarks; MedQA, MedMCQA, and PubMedQA. Result demonstrate that our framework achieves competitive performance, with particularly strong results in PubMedQA (95.0\\%) and MedMCQA (78.0\\%). Ablation studies confirm that confidence-aware routing combined with multi-model collaboration substantially outperforms single-model approaches and uniform reasoning strategies. This work establishes that strategic model collaboration offers a practical, computationally efficient pathway to improve medical AI systems, with significant implications for democratizing access to advanced medical AI in resource-limited settings.",
    "summary": "",
    "translation": "CURE：基于置信度驱动的统一推理集成框架用于医疗问答",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域的问答应用，属于明确的无关主题范畴。虽然提到了推理集成框架，但其医疗领域的特定应用使其与推荐系统、搜索或广告领域完全不相关，没有任何潜在的应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14351v1": {
    "title": "Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts",
    "url": "https://www.alphaxiv.org/abs/2510.14351v1",
    "arxiv_id": "2510.14351v1",
    "authors": "Perapard Ngokpol, Kun Kerdthaisong, Pasin Buakhaw, Pitikorn Khlaisamniang, Supasate Vorathammathorn, Piyalitt Ittichaiwong, Nutchanon Yongsatianchot",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 06:39:27",
    "ori_summary": "Large language models (LLMs) are increasingly used as role-playing agents, yet their capacity to faithfully and consistently portray version-specific characters -- for example, superheroes across comic and cinematic universes -- remains underexplored. Superhero canons such as Marvel and DC provide a rich testbed: decades of storytelling yield multiple incarnations of the same character with distinct histories, values, and moral codes. To study this problem, we introduce Beyond One World, a benchmark for character-grounded roleplay spanning 30 iconic heroes and 90 canon-specific versions. The benchmark comprises two tasks: (i) Canon Events, which probes factual recall of pivotal life stages, and (ii) Moral Dilemmas, which confronts models with ethically charged scenarios. We score responses for canonical accuracy and reasoning fidelity under a framework that separates internal deliberation (\"thinking\") from outward decisions (\"acting\"). We further propose Think-Act Matching, a metric that quantifies alignment between reasons and actions and serves as a proxy for model trustworthiness. Experiments across reasoning- and non-reasoning-oriented models yield three findings: (1) chain-of-thought prompting improves narrative coherence in weaker models but can reduce canonical accuracy in stronger ones; (2) cross-version generalization within a character remains a major obstacle; and (3) models often excel at either thinking or acting, but rarely both. Beyond One World exposes critical gaps in multiversal consistency and reasoning alignment, offering a challenging evaluation for role-playing LLMs.",
    "summary": "",
    "translation": "超越单一世界：跨多元宇宙语境下角色扮演中超级英雄的基准测试",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于角色扮演游戏中的超级英雄基准测试，属于游戏AI和角色扮演领域，与推荐系统、搜索或广告的核心技术进展无关。标题中提到的多元宇宙语境和角色扮演概念在推荐系统、搜索或广告领域没有直接的技术关联或应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14332v1": {
    "title": "A Robust Classification Method using Hybrid Word Embedding for Early Diagnosis of Alzheimer's Disease",
    "url": "https://www.alphaxiv.org/abs/2510.14332v1",
    "arxiv_id": "2510.14332v1",
    "authors": "Yangyang Li",
    "categories": "cs.CL, cs.AI, cs.LG, eess.AS, I.2.7; I.2.6",
    "pub_date": "2025-10-16 06:10:31",
    "ori_summary": "Early detection of Alzheimer's Disease (AD) is greatly beneficial to AD patients, leading to early treatments that lessen symptoms and alleviating financial burden of health care. As one of the leading signs of AD, language capability changes can be used for early diagnosis of AD. In this paper, I develop a robust classification method using hybrid word embedding and fine-tuned hyperparameters to achieve state-of-the-art accuracy in the early detection of AD. Specifically, we create a hybrid word embedding based on word vectors from Doc2Vec and ELMo to obtain perplexity scores of the sentences. The scores identify whether a sentence is fluent or not and capture semantic context of the sentences. I enrich the word embedding by adding linguistic features to analyze syntax and semantics. Further, we input an embedded feature vector into logistic regression and fine tune hyperparameters throughout the pipeline. By tuning hyperparameters of the machine learning pipeline (e.g., model regularization parameter, learning rate and vector size of Doc2Vec, and vector size of ELMo), I achieve 91% classification accuracy and an Area Under the Curve (AUC) of 97% in distinguishing early AD from healthy subjects. Based on my knowledge, my model with 91% accuracy and 97% AUC outperforms the best existing NLP model for AD diagnosis with an accuracy of 88% [32]. I study the model stability through repeated experiments and find that the model is stable even though the training data is split randomly (standard deviation of accuracy = 0.0403; standard deviation of AUC = 0.0174). This affirms our proposed method is accurate and stable. This model can be used as a large-scale screening method for AD, as well as a complementary examination for doctors to detect AD.",
    "summary": "",
    "translation": "一种基于混合词嵌入的鲁棒分类方法用于阿尔茨海默病的早期诊断",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的阿尔茨海默病诊断应用，这属于明确的无关主题范畴。虽然提到了词嵌入技术，但应用场景与推荐系统、搜索或广告完全无关，且没有显示出任何将这些技术迁移到相关领域的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14318v1": {
    "title": "Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL",
    "url": "https://www.alphaxiv.org/abs/2510.14318v1",
    "arxiv_id": "2510.14318v1",
    "authors": "Marwa Abdulhai, Ryan Cheng, Aryansh Shrivastava, Natasha Jaques, Yarin Gal, Sergey Levine",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-16 05:29:36",
    "ori_summary": "Large Language Models (LLMs) interact with millions of people worldwide in applications such as customer support, education and healthcare. However, their ability to produce deceptive outputs, whether intentionally or inadvertently, poses significant safety concerns. The unpredictable nature of LLM behavior, combined with insufficient safeguards against hallucination, misinformation, and user manipulation, makes their misuse a serious, real-world risk. In this paper, we investigate the extent to which LLMs engage in deception within dialogue, and propose the belief misalignment metric to quantify deception. We evaluate deception across four distinct dialogue scenarios, using five established deception detection metrics and our proposed metric. Our findings reveal this novel deception measure correlates more closely with human judgments than any existing metrics we test. Additionally, our benchmarking of eight state-of-the-art models indicates that LLMs naturally exhibit deceptive behavior in approximately 26% of dialogue turns, even when prompted with seemingly benign objectives. When prompted to deceive, LLMs are capable of increasing deceptiveness by as much as 31% relative to baselines. Unexpectedly, models trained with RLHF, the predominant approach for ensuring the safety of widely-deployed LLMs, still exhibit deception at a rate of 43% on average. Given that deception in dialogue is a behavior that develops over an interaction history, its effective evaluation and mitigation necessitates moving beyond single-utterance analyses. We introduce a multi-turn reinforcement learning methodology to fine-tune LLMs to reduce deceptive behaviors, leading to a 77.6% reduction compared to other instruction-tuned models.",
    "summary": "",
    "translation": "使用多轮强化学习评估和减少语言模型中的欺骗性对话",
    "relevance_score": 1,
    "reasoning": "该论文主要关注语言模型的欺骗性对话评估和减少，这属于纯粹的NLP评估和安全性问题。虽然涉及强化学习，但核心关注点是模型行为的可信性和安全性，与推荐系统、搜索或广告中的核心排名、匹配或建模问题没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14312v1": {
    "title": "Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies",
    "url": "https://www.alphaxiv.org/abs/2510.14312v1",
    "arxiv_id": "2510.14312v1",
    "authors": "Mason Nakamura, Abhinav Kumar, Saaduddin Mahmud, Sahar Abdelnabi, Shlomo Zilberstein, Eugene Bagdasarian",
    "categories": "cs.AI, cs.CL, cs.CR, I.2.7; I.2.11",
    "pub_date": "2025-10-16 05:19:13",
    "ori_summary": "A multi-agent system (MAS) powered by large language models (LLMs) can automate tedious user tasks such as meeting scheduling that requires inter-agent collaboration. LLMs enable nuanced protocols that account for unstructured private data, user constraints, and preferences. However, this design introduces new risks, including misalignment and attacks by malicious parties that compromise agents or steal user data. In this paper, we propose the Terrarium framework for fine-grained study on safety, privacy, and security in LLM-based MAS. We repurpose the blackboard design, an early approach in multi-agent systems, to create a modular, configurable testbed for multi-agent collaboration. We identify key attack vectors such as misalignment, malicious agents, compromised communication, and data poisoning. We implement three collaborative MAS scenarios with four representative attacks to demonstrate the framework's flexibility. By providing tools to rapidly prototype, evaluate, and iterate on defenses and designs, Terrarium aims to accelerate progress toward trustworthy multi-agent systems.",
    "summary": "",
    "translation": "Terrarium：重新审视多智能体安全、隐私与安全研究的黑板架构",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及安全、隐私和安全研究，这些都属于明确的无关主题范畴。虽然提到了多智能体系统，但核心焦点是安全隐私而非推荐系统、搜索或广告的技术应用。没有任何迹象表明该研究在效率、架构或应用方面与我的关注领域相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14307v1": {
    "title": "MERLIN: A Testbed for Multilingual Multimodal Entity Recognition and Linking",
    "url": "https://www.alphaxiv.org/abs/2510.14307v1",
    "arxiv_id": "2510.14307v1",
    "authors": "Sathyanarayanan Ramamoorthy, Vishwa Shah, Simran Khanuja, Zaid Sheikh, Shan Jie, Ann Chia, Shearman Chua, Graham Neubig",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 05:06:54",
    "ori_summary": "This paper introduces MERLIN, a novel testbed system for the task of Multilingual Multimodal Entity Linking. The created dataset includes BBC news article titles, paired with corresponding images, in five languages: Hindi, Japanese, Indonesian, Vietnamese, and Tamil, featuring over 7,000 named entity mentions linked to 2,500 unique Wikidata entities. We also include several benchmarks using multilingual and multimodal entity linking methods exploring different language models like LLaMa-2 and Aya-23. Our findings indicate that incorporating visual data improves the accuracy of entity linking, especially for entities where the textual context is ambiguous or insufficient, and particularly for models that do not have strong multilingual abilities. For the work, the dataset, methods are available here at https://github.com/rsathya4802/merlin",
    "summary": "",
    "translation": "MERLIN：多语言多模态实体识别与链接的测试平台",
    "relevance_score": 3,
    "reasoning": "该论文专注于多语言多模态实体识别与链接，虽然实体识别在搜索系统中具有潜在应用价值，但论文标题明确强调这是一个测试平台而非核心算法创新。对于推荐系统或广告领域，这种测试平台性质的论文缺乏直接的架构改进或应用创新，相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14305v1": {
    "title": "MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical Problem Solving and Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.14305v1",
    "arxiv_id": "2510.14305v1",
    "authors": "Mahbub E Sobhani, Md. Faiyaz Abdullah Sayeedi, Tasnim Mohiuddin, Md Mofijul Islam, Swakkhar Shatabda",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 04:59:52",
    "ori_summary": "Mathematical reasoning remains one of the most challenging domains for large language models (LLMs), requiring not only linguistic understanding but also structured logical deduction and numerical precision. While recent LLMs demonstrate strong general-purpose reasoning abilities, their mathematical competence across diverse languages remains underexplored. Existing benchmarks primarily focus on English or a narrow subset of high-resource languages, leaving significant gaps in assessing multilingual and cross-lingual mathematical reasoning. To address this, we introduce MathMist, a parallel multilingual benchmark for mathematical problem solving and reasoning. MathMist encompasses over 21K aligned question-answer pairs across seven languages, representing a balanced coverage of high-, medium-, and low-resource linguistic settings. The dataset captures linguistic variety, multiple types of problem settings, and solution synthesizing capabilities. We systematically evaluate a diverse suite of models, including open-source small and medium LLMs, proprietary systems, and multilingual-reasoning-focused models, under zero-shot, chain-of-thought (CoT), and code-switched reasoning paradigms. Our results reveal persistent deficiencies in LLMs' ability to perform consistent and interpretable mathematical reasoning across languages, with pronounced degradation in low-resource settings. All the codes and data are available at GitHub: https://github.com/mahbubhimel/MathMist",
    "summary": "",
    "translation": "MathMist：一个用于数学问题求解与推理的并行多语言基准数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于数学问题求解的基准数据集创建，属于纯粹的NLP评估基准范畴。虽然数学推理能力对通用LLM很重要，但该论文本身不涉及推荐系统、搜索或广告领域的任何技术进展，也没有展示在异构数据建模、Transformer架构改进或直接应用方面的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14303v1": {
    "title": "Constraint-Driven Small Language Models Based on Agent and OpenAlex Knowledge Graph: Mining Conceptual Pathways and Discovering Innovation Points in Academic Papers",
    "url": "https://www.alphaxiv.org/abs/2510.14303v1",
    "arxiv_id": "2510.14303v1",
    "authors": "Ziye Xia, Sergei S. Ospichev",
    "categories": "cs.CL, cs.LG, I.2.7",
    "pub_date": "2025-10-16 04:58:28",
    "ori_summary": "In recent years, the rapid increase in academic publications across various fields has posed severe challenges for academic paper analysis: scientists struggle to timely and comprehensively track the latest research findings and methodologies. Key concept extraction has proven to be an effective analytical paradigm, and its automation has been achieved with the widespread application of language models in industrial and scientific domains. However, existing paper databases are mostly limited to similarity matching and basic classification of key concepts, failing to deeply explore the relational networks between concepts. This paper is based on the OpenAlex opensource knowledge graph. By analyzing nearly 8,000 open-source paper data from Novosibirsk State University, we discovered a strong correlation between the distribution patterns of paper key concept paths and both innovation points and rare paths. We propose a prompt engineering-based key concept path analysis method. This method leverages small language models to achieve precise key concept extraction and innovation point identification, and constructs an agent based on a knowledge graph constraint mechanism to enhance analysis accuracy. Through fine-tuning of the Qwen and DeepSeek models, we achieved significant improvements in accuracy, with the models publicly available on the Hugging Face platform.",
    "summary": "",
    "translation": "基于智能体与OpenAlex知识图谱的约束驱动式小型语言模型：学术论文中的概念路径挖掘与创新点发现",
    "relevance_score": 2,
    "reasoning": "该论文主要关注学术文献挖掘和知识图谱应用，属于特定领域的信息检索。虽然涉及小型语言模型和知识图谱技术，但其应用场景局限于学术论文分析，与推荐系统、搜索或广告的核心技术发展缺乏直接关联。论文中的约束驱动方法和概念路径挖掘技术可能对某些信息检索任务有启发，但整体相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14276v1": {
    "title": "Qwen3Guard Technical Report",
    "url": "https://www.alphaxiv.org/abs/2510.14276v1",
    "arxiv_id": "2510.14276v1",
    "authors": "Haiquan Zhao, Chenhan Yuan, Fei Huang, Xiaomeng Hu, Yichang Zhang, An Yang, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin, Baosong Yang, Chen Cheng, Jialong Tang, Jiandong Jiang, Jianwei Zhang, Jijie Xu, Ming Yan, Minmin Sun, Pei Zhang, Pengjun Xie, Qiaoyu Tang, Qin Zhu, Rong Zhang, Shibin Wu, Shuo Zhang, Tao He, Tianyi Tang, Tingyu Xia, Wei Liao, Weizhou Shen, Wenbiao Yin, Wenmeng Zhou, Wenyuan Yu, Xiaobin Wang, Xiaodong Deng, Xiaodong Xu, Xinyu Zhang, Yang Liu, Yeqiu Li, Yi Zhang, Yong Jiang, Yu Wan, Yuxin Zhou",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 04:00:18",
    "ori_summary": "As large language models (LLMs) become more capable and widely used, ensuring the safety of their outputs is increasingly critical. Existing guardrail models, though useful in static evaluation settings, face two major limitations in real-world applications: (1) they typically output only binary \"safe/unsafe\" labels, which can be interpreted inconsistently across diverse safety policies, rendering them incapable of accommodating varying safety tolerances across domains; and (2) they require complete model outputs before performing safety checks, making them fundamentally incompatible with streaming LLM inference, thereby preventing timely intervention during generation and increasing exposure to harmful partial outputs. To address these challenges, we present Qwen3Guard, a series of multilingual safety guardrail models with two specialized variants: Generative Qwen3Guard, which casts safety classification as an instruction-following task to enable fine-grained tri-class judgments (safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a token-level classification head for real-time safety monitoring during incremental text generation. Both variants are available in three sizes (0.6B, 4B, and 8B parameters) and support up to 119 languages and dialects, providing comprehensive, scalable, and low-latency safety moderation for global LLM deployments. Evaluated across English, Chinese, and multilingual benchmarks, Qwen3Guard achieves state-of-the-art performance in both prompt and response safety classification. All models are released under the Apache 2.0 license for public use.",
    "summary": "",
    "translation": "Qwen3Guard技术报告",
    "relevance_score": 1,
    "reasoning": "标题仅表明这是一个技术报告，没有提供任何关于内容的具体信息。无法判断是否涉及推荐系统、搜索、广告或相关使能技术。缺乏具体的技术细节使得无法评估其与关注领域的潜在相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14274v1": {
    "title": "Retrofitting Small Multilingual Models for Retrieval: Matching 7B Performance with 300M Parameters",
    "url": "https://www.alphaxiv.org/abs/2510.14274v1",
    "arxiv_id": "2510.14274v1",
    "authors": "Lifu Tu, Yingbo Zhou, Semih Yavuz",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 03:48:59",
    "ori_summary": "Training effective multilingual embedding models presents unique challenges due to the diversity of languages and task objectives. Although small multilingual models (<1 B parameters) perform well on multilingual tasks generally, they consistently lag behind larger models (>1 B) in the most prevalent use case: retrieval. This raises a critical question: Can smaller models be retrofitted specifically for retrieval tasks to enhance their performance? In this work, we investigate key factors that influence the effectiveness of multilingual embeddings, focusing on training data scale, negative sampling strategies, and data diversity. We find that while increasing the scale of training data yields initial performance gains, these improvements quickly plateau - indicating diminishing returns. Incorporating hard negatives proves essential for consistently improving retrieval accuracy. Furthermore, our analysis reveals that task diversity in the training data contributes more significantly to performance than language diversity alone. As a result, we develop a compact (approximately 300M) multilingual model that achieves retrieval performance comparable to or even surpassing current strong 7B models.",
    "summary": "",
    "translation": "为检索任务改造小型多语言模型：用3亿参数匹配70亿参数模型的性能",
    "relevance_score": 8,
    "reasoning": "该论文涉及模型效率优化和检索任务，直接关联到'Enabling LLM Tech'中的模型效率提升。在搜索和推荐系统中，高效的检索模型对于处理大规模用户查询和物品匹配至关重要，这种参数效率优化技术可以显著降低部署成本并提高推理速度。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14271v1": {
    "title": "Less is More: Denoising Knowledge Graphs For Retrieval Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14271v1",
    "arxiv_id": "2510.14271v1",
    "authors": "Yilun Zheng, Dan Yang, Jie Li, Lin Shang, Lihui Chen, Jiahao Xu, Sitao Luan",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-16 03:41:44",
    "ori_summary": "Retrieval-Augmented Generation (RAG) systems enable large language models (LLMs) instant access to relevant information for the generative process, demonstrating their superior performance in addressing common LLM challenges such as hallucination, factual inaccuracy, and the knowledge cutoff. Graph-based RAG further extends this paradigm by incorporating knowledge graphs (KGs) to leverage rich, structured connections for more precise and inferential responses. A critical challenge, however, is that most Graph-based RAG systems rely on LLMs for automated KG construction, often yielding noisy KGs with redundant entities and unreliable relationships. This noise degrades retrieval and generation performance while also increasing computational cost. Crucially, current research does not comprehensively address the denoising problem for LLM-generated KGs. In this paper, we introduce DEnoised knowledge Graphs for Retrieval Augmented Generation (DEG-RAG), a framework that addresses these challenges through: (1) entity resolution, which eliminates redundant entities, and (2) triple reflection, which removes erroneous relations. Together, these techniques yield more compact, higher-quality KGs that significantly outperform their unprocessed counterparts. Beyond the methods, we conduct a systematic evaluation of entity resolution for LLM-generated KGs, examining different blocking strategies, embedding choices, similarity metrics, and entity merging techniques. To the best of our knowledge, this is the first comprehensive exploration of entity resolution in LLM-generated KGs. Our experiments demonstrate that this straightforward approach not only drastically reduces graph size but also consistently improves question answering performance across diverse popular Graph-based RAG variants.",
    "summary": "",
    "translation": "少即是多：用于检索增强生成的去噪知识图谱",
    "relevance_score": 8,
    "reasoning": "该论文涉及检索增强生成（RAG）中的知识图谱去噪，这直接属于核心领域进展中的搜索系统优化。知识图谱去噪技术可以显著提升检索系统的准确性和效率，在搜索和推荐系统中具有直接应用价值，能够改善用户查询理解和结果相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14262v1": {
    "title": "CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions",
    "url": "https://www.alphaxiv.org/abs/2510.14262v1",
    "arxiv_id": "2510.14262v1",
    "authors": "Zihao Fu, Ming Liao, Chris Russell, Zhenguang G. Cai",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-16 03:27:15",
    "ori_summary": "Large language models have achieved remarkable success but remain largely black boxes with poorly understood internal mechanisms. To address this limitation, many researchers have proposed various interpretability methods including mechanistic analysis, probing classifiers, and activation visualization, each providing valuable insights from different perspectives. Building upon this rich landscape of complementary approaches, we introduce CAST (Compositional Analysis via Spectral Tracking), a probe-free framework that contributes a novel perspective by analyzing transformer layer functions through direct transformation matrix estimation and comprehensive spectral analysis. CAST offers complementary insights to existing methods by estimating the realized transformation matrices for each layer using Moore-Penrose pseudoinverse and applying spectral analysis with six interpretable metrics characterizing layer behavior. Our analysis reveals distinct behaviors between encoder-only and decoder-only models, with decoder models exhibiting compression-expansion cycles while encoder models maintain consistent high-rank processing. Kernel analysis further demonstrates functional relationship patterns between layers, with CKA similarity matrices clearly partitioning layers into three phases: feature extraction, compression, and specialization.",
    "summary": "研究Transformer模型内部工作机制的可解释性问题，核心方法是通过直接估计变换矩阵和谱分析来理解各层的功能，揭示编码器和解码器模型在处理模式上的差异。",
    "translation": "CAST：通过谱跟踪进行组合分析以理解Transformer层功能",
    "relevance_score": 9,
    "reasoning": "该论文直接属于'使能Transformer技术'范畴，专注于Transformer架构分析和理解层功能。通过谱跟踪的组合分析方法可以揭示Transformer内部工作机制，这对于优化推荐系统和搜索中的Transformer模型效率、设计更好的注意力机制以及改进模型可解释性具有直接应用价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文通过谱分析直接研究Transformer层功能，为核心LLM技术提供了新的可解释性方法，对理解推荐系统中使用的Transformer模型内部机制具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14261v1": {
    "title": "Rewriting History: A Recipe for Interventional Analyses to Study Data Effects on Model Behavior",
    "url": "https://www.alphaxiv.org/abs/2510.14261v1",
    "arxiv_id": "2510.14261v1",
    "authors": "Rahul Nadkarni, Yanai Elazar, Hila Gonen, Noah A. Smith",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 03:22:48",
    "ori_summary": "We present an experimental recipe for studying the relationship between training data and language model (LM) behavior. We outline steps for intervening on data batches -- i.e., ``rewriting history'' -- and then retraining model checkpoints over that data to test hypotheses relating data to behavior. Our recipe breaks down such an intervention into stages that include selecting evaluation items from a benchmark that measures model behavior, matching relevant documents to those items, and modifying those documents before retraining and measuring the effects. We demonstrate the utility of our recipe through case studies on factual knowledge acquisition in LMs, using both cooccurrence statistics and information retrieval methods to identify documents that might contribute to knowledge learning. Our results supplement past observational analyses that link cooccurrence to model behavior, while demonstrating that extant methods for identifying relevant training documents do not fully explain an LM's ability to correctly answer knowledge questions. Overall, we outline a recipe that researchers can follow to test further hypotheses about how training data affects model behavior. Our code is made publicly available to promote future work.",
    "summary": "",
    "translation": "重写历史：研究数据对模型行为影响的干预分析方案",
    "relevance_score": 2,
    "reasoning": "该论文主要关注数据干预分析和模型行为研究，属于模型可解释性和因果推断范畴。虽然数据对模型的影响在推荐系统和搜索中有一定相关性，但该工作更偏向于通用的机器学习方法论，缺乏明确的RecSys/Search/Ads具体应用场景，且不属于核心领域进展或直接LLM应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14252v1": {
    "title": "MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems",
    "url": "https://www.alphaxiv.org/abs/2510.14252v1",
    "arxiv_id": "2510.14252v1",
    "authors": "Jihao Zhao, Zhiyuan Ji, Simin Niu, Hanyu Wang, Feiyu Xiong, Zhiyu Li",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 03:09:51",
    "ori_summary": "The traditional RAG paradigm, which typically engages in the comprehension of relevant text chunks in response to received queries, inherently restricts both the depth of knowledge internalization and reasoning capabilities. To address this limitation, our research transforms the text processing in RAG from passive chunking to proactive understanding, defining this process as document memory extraction with the objective of simulating human cognitive processes during reading. Building upon this, we propose the Mixtures of scenario-aware document Memories (MoM) framework, engineered to efficiently handle documents from multiple domains and train small language models (SLMs) to acquire the ability to proactively explore and construct document memories. The MoM initially instructs large language models (LLMs) to simulate domain experts in generating document logical outlines, thereby directing structured chunking and core content extraction. It employs a multi-path sampling and multi-perspective evaluation mechanism, specifically designing comprehensive metrics that represent chunk clarity and extraction completeness to select the optimal document memories. Additionally, to infuse deeper human-like reading abilities during the training of SLMs, we incorporate a reverse reasoning strategy, which deduces refined expert thinking paths from high-quality outcomes. Finally, leveraging diverse forms of content generated by MoM, we develop a three-layer document memory retrieval mechanism, which is grounded in our theoretical proof from the perspective of probabilistic modeling. Extensive experimental results across three distinct domains demonstrate that the MoM framework not only resolves text chunking challenges in existing RAG systems, providing LLMs with semantically complete document memories, but also paves the way for SLMs to achieve human-centric intelligent text processing.",
    "summary": "",
    "translation": "MoM：面向检索增强生成系统的场景感知文档记忆混合模型",
    "relevance_score": 8,
    "reasoning": "该论文提出了一种场景感知的文档记忆混合模型，直接属于检索增强生成(RAG)系统的核心改进。RAG技术是搜索和推荐系统中的关键使能技术，通过改进文档检索和记忆机制，可以显著提升搜索相关性排序和个性化推荐的准确性。这种场景感知的记忆混合方法特别适用于处理搜索和推荐中的多场景、多上下文需求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14242v1": {
    "title": "Flip-Flop Consistency: Unsupervised Training for Robustness to Prompt Perturbations in LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.14242v1",
    "arxiv_id": "2510.14242v1",
    "authors": "Parsa Hejabi, Elnaz Rahmati, Alireza S. Ziabari, Morteza Dehghani",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-16 02:54:01",
    "ori_summary": "Large Language Models (LLMs) often produce inconsistent answers when faced with different phrasings of the same prompt. In this paper, we propose Flip-Flop Consistency ($F^2C$), an unsupervised training method that improves robustness to such perturbations. $F^2C$ is composed of two key components. The first, Consensus Cross-Entropy (CCE), uses a majority vote across prompt variations to create a hard pseudo-label. The second is a representation alignment loss that pulls lower-confidence and non-majority predictors toward the consensus established by high-confidence, majority-voting variations. We evaluate our method on 11 datasets spanning four NLP tasks, with 4-15 prompt variations per dataset. On average, $F^2C$ raises observed agreement by 11.62%, improves mean $F_1$ by 8.94%, and reduces performance variance across formats by 3.29%. In out-of-domain evaluations, $F^2C$ generalizes effectively, increasing $\\overline{F_1}$ and agreement while decreasing variance across most source-target pairs. Finally, when trained on only a subset of prompt perturbations and evaluated on held-out formats, $F^2C$ consistently improves both performance and agreement while reducing variance. These findings highlight $F^2C$ as an effective unsupervised method for enhancing LLM consistency, performance, and generalization under prompt perturbations. Code is available at https://github.com/ParsaHejabi/Flip-Flop-Consistency-Unsupervised-Training-for-Robustness-to-Prompt-Perturbations-in-LLMs.",
    "summary": "",
    "translation": "翻转一致性：针对大语言模型中提示扰动鲁棒性的无监督训练",
    "relevance_score": 3,
    "reasoning": "该论文关注LLM对提示扰动的鲁棒性训练，属于Enabling LLM Tech范畴。虽然鲁棒性技术可能间接提升RecSys/Search系统中LLM组件的稳定性，但论文主要解决的是提示工程中的稳定性问题，与核心推荐、搜索或广告算法的直接关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14232v1": {
    "title": "Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models",
    "url": "https://www.alphaxiv.org/abs/2510.14232v1",
    "arxiv_id": "2510.14232v1",
    "authors": "Mehrzad Samadi, Aleksander Ficek, Sean Narenthiran, Siddhartha Jain, Wasi Uddin Ahmad, Somshubra Majumdar, Vahid Noroozi, Boris Ginsburg",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-16 02:19:25",
    "ori_summary": "Competitive programming has become a rigorous benchmark for evaluating the reasoning and problem-solving capabilities of large language models (LLMs). The International Olympiad in Informatics (IOI) stands out as one of the most prestigious annual competitions in competitive programming and has become a key benchmark for comparing human and AI-level programming ability. While several proprietary models have been claimed to achieve gold medal-level performance at the IOI, often with undisclosed methods, achieving comparable results with open-weight models remains a significant challenge. In this paper, we present \\gencluster, a scalable and reproducible test-time compute framework that attains IOI gold-level performance using open-weight models. It combines large-scale generation, behavioral clustering, ranking, and a round-robin submission strategy to efficiently explore diverse solution spaces under limited validation budgets. Our experiments show that the performance of our proposed approach scales consistently with available compute, narrowing the gap between open and closed systems. Notably, we will show that GenCluster can achieve a gold medal at IOI 2025 for the first time with an open-weight model gpt-oss-120b, setting a new benchmark for transparent and reproducible evaluation of reasoning in LLMs.",
    "summary": "",
    "translation": "通过扩展测试时计算资源，使用开源权重模型实现IOI金牌级性能",
    "relevance_score": 2,
    "reasoning": "该论文主要关注测试时计算扩展以实现特定竞赛性能，这属于模型推理优化范畴。虽然计算效率对推荐/搜索系统有间接价值，但论文焦点过于特定于竞赛性能而非通用的推荐/搜索应用场景，相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14211v1": {
    "title": "LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.14211v1",
    "arxiv_id": "2510.14211v1",
    "authors": "Beomseok Kang, Jiwon Song, Jae-Joon Kim",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 01:37:39",
    "ori_summary": "Multi-stage reasoning has emerged as an effective strategy for enhancing the reasoning capability of small language models by decomposing complex problems into sequential sub-stages. However, this comes at the cost of increased latency. We observe that existing adaptive acceleration techniques, such as layer skipping, struggle to balance efficiency and accuracy in this setting due to two key challenges: (1) stage-wise variation in skip sensitivity, and (2) the generation of redundant output tokens. To address these, we propose LiteStage, a latency-aware layer skipping framework for multi-stage reasoning. LiteStage combines a stage-wise offline search that allocates optimal layer budgets with an online confidence-based generation early exit to suppress unnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and StrategyQA, show that LiteStage achieves up to 1.70x speedup with less than 4.0% accuracy loss, outperforming prior training-free layer skipping methods.",
    "summary": "",
    "translation": "LiteStage：面向多阶段推理的延迟感知层跳过",
    "relevance_score": 7,
    "reasoning": "该论文提出的延迟感知层跳过技术属于Transformer架构效率优化范畴，可应用于推荐系统或搜索中的多阶段推理场景。通过动态跳过某些层来减少推理延迟，能够显著提升在线服务的响应速度，这对于需要实时交互的推荐和搜索应用具有直接价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14205v1": {
    "title": "DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans",
    "url": "https://www.alphaxiv.org/abs/2510.14205v1",
    "arxiv_id": "2510.14205v1",
    "authors": "Bingsheng Yao, Bo Sun, Yuanzhe Dong, Yuxuan Lu, Dakuo Wang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 01:26:38",
    "ori_summary": "The emerging large language model role-playing agents (LLM RPAs) aim to simulate individual human behaviors, but the persona fidelity is often undermined by manually-created profiles (e.g., cherry-picked information and personality characteristics) without validating the alignment with the target individuals. To address this limitation, our work introduces the Dynamic Persona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLM RPAs' behaviors with those of target individuals by iteratively identifying the cognitive divergence, either through free-form or theory-grounded, structured analysis, between generated behaviors and human ground truth, and refining the persona profile to mitigate these divergences.We evaluate DPRF with five LLMs on four diverse behavior-prediction scenarios: formal debates, social media posts with mental health issues, public interviews, and movie reviews.DPRF can consistently improve behavioral alignment considerably over baseline personas and generalizes across models and scenarios.Our work provides a robust methodology for creating high-fidelity persona profiles and enhancing the validity of downstream applications, such as user simulation, social studies, and personalized AI.",
    "summary": "",
    "translation": "DPRF：一种可泛化的动态角色精炼框架，用于优化个性化大语言模型角色扮演代理与人类之间的行为对齐",
    "relevance_score": 3,
    "reasoning": "该论文主要关注LLM角色扮演代理的行为对齐，属于个性化交互领域，但未明确涉及推荐系统、搜索或广告的核心应用。虽然个性化概念与推荐系统相关，但框架专注于角色扮演而非用户建模或内容推荐，在推荐/搜索/广告领域的直接应用潜力有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14203v1": {
    "title": "Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.14203v1",
    "arxiv_id": "2510.14203v1",
    "authors": "Ryo Masumura, Shota Orihashi, Mana Ihori, Tomohiro Tanaka, Naoki Makishima, Taiga Yamane, Naotaka Kawata, Satoshi Suzuki, Taichi Katayama",
    "categories": "cs.CV, cs.CL, cs.MM",
    "pub_date": "2025-10-16 01:21:57",
    "ori_summary": "This paper proposes a joint modeling method of the Big Five, which has long been studied, and HEXACO, which has recently attracted attention in psychology, for automatically recognizing apparent personality traits from multimodal human behavior. Most previous studies have used the Big Five for multimodal apparent personality-trait recognition. However, no study has focused on apparent HEXACO which can evaluate an Honesty-Humility trait related to displaced aggression and vengefulness, social-dominance orientation, etc. In addition, the relationships between the Big Five and HEXACO when modeled by machine learning have not been clarified. We expect awareness of multimodal human behavior to improve by considering these relationships. The key advance of our proposed method is to optimize jointly recognizing the Big Five and HEXACO. Experiments using a self-introduction video dataset demonstrate that the proposed method can effectively recognize the Big Five and HEXACO.",
    "summary": "",
    "translation": "面向多模态显性人格特质识别的五因素模型与HEXACO模型的联合建模",
    "relevance_score": 2,
    "reasoning": "该论文专注于心理学人格特质识别，虽然涉及多模态数据，但核心应用场景偏向心理学评估而非推荐/搜索/广告系统。人格建模在个性化推荐中有潜在应用价值，但论文标题未体现与推荐系统的直接关联，且人格识别本身属于心理学领域而非核心推荐技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14200v1": {
    "title": "RLSR: Reinforcement Learning with Supervised Reward Outperforms SFT in Instruction Following",
    "url": "https://www.alphaxiv.org/abs/2510.14200v1",
    "arxiv_id": "2510.14200v1",
    "authors": "Zhichao Wang, Andy Wong, Ruslan Belkin",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 01:13:14",
    "ori_summary": "After the pretraining stage of LLMs, techniques such as SFT, RLHF, RLVR, and RFT are applied to enhance instruction-following ability, mitigate undesired responses, improve reasoning capability and enable efficient domain adaptation with minimal data. SFT relies on the next-token prediction objective to strengthen instruction following in a base model using a large corpus of human-labeled responses. In contrast, RFT employs a RL-based approach to adapt fine-tuned reasoning models to specific domains with limited supervision. Inspired by RFT, we propose replacing SFT with RLSR to leverage the extensive SFT dataset in an RL framework, thereby improving the base model's instruction-following ability. In RLSR, the base model generates multiple responses for each prompt, and reward scores are computed as the cosine similarity in the semantic embedding space between the generated and human-labeled responses. RLSR can be utilized in multiple ways. It can directly replace SFT, achieving superior performance on instruction-following benchmarks-for example, RLSR (SB) on Qwen-7B (INFINITY) achieved an AlpacaEval win rate of 26.34%, surpassing SFT's 21.01%. Furthermore, combining SFT and RLSR further enhances downstream task performance; Qwen-7B (INFINITY) achieved a win rate of 30.73% when trained with SFT + RLSR.",
    "summary": "",
    "translation": "RLSR：在指令跟随任务中，基于监督奖励的强化学习优于监督微调",
    "relevance_score": 2,
    "reasoning": "虽然该论文涉及强化学习，但它专注于纯粹的指令跟随任务，这是通用NLP应用的核心问题。论文没有展示与推荐系统、搜索或广告的具体相关性，也没有说明其方法如何应用于这些领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14184v1": {
    "title": "MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation",
    "url": "https://www.alphaxiv.org/abs/2510.14184v1",
    "arxiv_id": "2510.14184v1",
    "authors": "Mahmood Hegazy, Aaron Rodrigues, Azzam Naeem",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-16 00:30:08",
    "ori_summary": "We present MAFA (Multi-Agent Framework for Annotation), a production-deployed system that transforms enterprise-scale annotation workflows through configurable multi-agent collaboration. Addressing the critical challenge of annotation backlogs in financial services, where millions of customer utterances require accurate categorization, MAFA combines specialized agents with structured reasoning and a judge-based consensus mechanism. Our framework uniquely supports dynamic task adaptation, allowing organizations to define custom annotation types (FAQs, intents, entities, or domain-specific categories) through configuration rather than code changes. Deployed at JP Morgan Chase, MAFA has eliminated a 1 million utterance backlog while achieving, on average, 86% agreement with human annotators, annually saving over 5,000 hours of manual annotation work. The system processes utterances with annotation confidence classifications, which are typically 85% high, 10% medium, and 5% low across all datasets we tested. This enables human annotators to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's effectiveness across multiple datasets and languages, showing consistent improvements over traditional and single-agent annotation baselines: 13.8% higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1 in our internal intent classification dataset and similar gains on public benchmarks. This work bridges the gap between theoretical multi-agent systems and practical enterprise deployment, providing a blueprint for organizations facing similar annotation challenges.",
    "summary": "",
    "translation": "MAFA：一种用于企业级标注的多智能体框架，具有可配置任务自适应能力",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多智能体框架用于企业级数据标注，属于数据标注基础设施而非核心推荐/搜索/广告算法。虽然高质量标注数据对模型训练很重要，但这属于数据工程范畴，与我的核心关注点（模型架构、LLM应用、Transformer技术等）只有间接关系。该技术可能通过提升数据质量间接影响推荐系统，但缺乏直接的技术关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14981v1": {
    "title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing",
    "url": "https://www.alphaxiv.org/abs/2510.14981v1",
    "arxiv_id": "2510.14981v1",
    "authors": "Hadi Alzayer, Yunzhi Zhang, Chen Geng, Jia-Bin Huang, Jiajun Wu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 17:59:59",
    "ori_summary": "We present an inference-time diffusion sampling method to perform multi-view consistent image editing using pre-trained 2D image editing models. These models can independently produce high-quality edits for each image in a set of multi-view images of a 3D scene or object, but they do not maintain consistency across views. Existing approaches typically address this by optimizing over explicit 3D representations, but they suffer from a lengthy optimization process and instability under sparse view settings. We propose an implicit 3D regularization approach by constraining the generated 2D image sequences to adhere to a pre-trained multi-view image distribution. This is achieved through coupled diffusion sampling, a simple diffusion sampling technique that concurrently samples two trajectories from both a multi-view image distribution and a 2D edited image distribution, using a coupling term to enforce the multi-view consistency among the generated images. We validate the effectiveness and generality of this framework on three distinct multi-view image editing tasks, demonstrating its applicability across various model architectures and highlighting its potential as a general solution for multi-view consistent editing.",
    "summary": "",
    "translation": "用于免训练多视图图像编辑的耦合扩散采样",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于多视图图像编辑和扩散模型，属于计算机视觉和图像生成领域。虽然扩散模型是重要的生成技术，但该论文的具体应用（多视图图像编辑）与搜索、推荐或广告系统的核心排名和建模任务没有直接关联，也不涉及处理异构数据或改进Transformer架构。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14979v1": {
    "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at Scale",
    "url": "https://www.alphaxiv.org/abs/2510.14979v1",
    "arxiv_id": "2510.14979v1",
    "authors": "Haiwen Diao, Mingxuan Li, Silei Wu, Linjun Dai, Xiaohua Wang, Hanming Deng, Lewei Lu, Dahua Lin, Ziwei Liu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 17:59:58",
    "ori_summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
    "summary": "",
    "translation": "从像素到词语——迈向大规模原生视觉语言原语",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及视觉语言模型(VLM)技术，这与'VLM类比用于异构数据'焦点高度相关。VLM处理视觉和语言模态的方法可以类比应用于推荐/搜索中的异构数据(如用户序列、上下文特征)，实现统一建模，在商品图像理解、多模态搜索等场景有直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14978v1": {
    "title": "Learning an Image Editing Model without Image Editing Pairs",
    "url": "https://www.alphaxiv.org/abs/2510.14978v1",
    "arxiv_id": "2510.14978v1",
    "authors": "Nupur Kumari, Sheng-Yu Wang, Nanxuan Zhao, Yotam Nitzan, Yuheng Li, Krishna Kumar Singh, Richard Zhang, Eli Shechtman, Jun-Yan Zhu, Xun Huang",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-16 17:59:57",
    "ori_summary": "Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is a critical bottleneck, as such naturally occurring pairs are hard to curate at scale. Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present a new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes a few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
    "summary": "",
    "translation": "无需图像编辑对学习图像编辑模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注图像编辑任务的训练方法，属于计算机视觉领域的特定应用。虽然图像编辑技术可能间接应用于广告创意生成，但这属于明确排除的'Ads creative generation'范畴。论文没有展示与推荐系统、搜索或广告排名相关的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14976v1": {
    "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation",
    "url": "https://www.alphaxiv.org/abs/2510.14976v1",
    "arxiv_id": "2510.14976v1",
    "authors": "Shaowei Liu, Chuan Guo, Bing Zhou, Jian Wang",
    "categories": "cs.CV, cs.GR, cs.RO",
    "pub_date": "2025-10-16 17:59:56",
    "ori_summary": "Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework.",
    "summary": "",
    "translation": "Ponimator：展开交互式姿态以实现多功能人-人交互动画",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的人体姿态动画生成，属于纯粹的视觉应用领域。虽然涉及交互建模，但与人机交互动画、图形渲染直接相关，与推荐系统、搜索或广告的核心技术栈没有明显关联，也不涉及LLM或Transformer架构的进步。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14977v1": {
    "title": "Terra: Explorable Native 3D World Model with Point Latents",
    "url": "https://www.alphaxiv.org/abs/2510.14977v1",
    "arxiv_id": "2510.14977v1",
    "authors": "Yuanhui Huang, Weiliang Chen, Wenzhao Zheng, Xin Tao, Pengfei Wan, Jie Zhou, Jiwen Lu",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-16 17:59:56",
    "ori_summary": "World models have garnered increasing attention for comprehensive modeling of the real world. However, most existing methods still rely on pixel-aligned representations as the basis for world evolution, neglecting the inherent 3D nature of the physical world. This could undermine the 3D consistency and diminish the modeling efficiency of world models. In this paper, we present Terra, a native 3D world model that represents and generates explorable environments in an intrinsic 3D latent space. Specifically, we propose a novel point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into a latent point representation, which is subsequently decoded as 3D Gaussian primitives to jointly model geometry and appearance. We then introduce a sparse point flow matching network (SPFlow) for generating the latent point representation, which simultaneously denoises the positions and features of the point latents. Our Terra enables exact multi-view consistency with native 3D representation and architecture, and supports flexible rendering from any viewpoint with only a single generation process. Furthermore, Terra achieves explorable world modeling through progressive generation in the point latent space. We conduct extensive experiments on the challenging indoor scenes from ScanNet v2. Terra achieves state-of-the-art performance in both reconstruction and generation with high 3D consistency.",
    "summary": "",
    "translation": "Terra：基于点潜在表示的可探索原生3D世界模型",
    "relevance_score": 2,
    "reasoning": "该论文专注于3D世界建模和点云表示，属于纯粹的3D视觉领域。虽然3D建模在某些特定场景下可能有潜在应用，但该论文标题没有显示出与推荐系统、搜索或广告的直接关联，也没有涉及Transformer架构改进或LLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14975v1": {
    "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14975v1",
    "arxiv_id": "2510.14975v1",
    "authors": "Hengyuan Xu, Wei Cheng, Peng Xing, Yixiao Fang, Shuhan Wu, Rui Wang, Xianfang Zeng, Daxin Jiang, Gang Yu, Xingjun Ma, Yu-Gang Jiang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 17:59:54",
    "ori_summary": "Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.",
    "summary": "",
    "translation": "WithAnyone：面向可控且身份一致性的图像生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于可控图像生成和身份一致性，这属于AIGC和内容生成领域，与我的关注点无关。论文标题表明其核心是图像生成技术，没有显示出在推荐系统、搜索或广告方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14974v1": {
    "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.14974v1",
    "arxiv_id": "2510.14974v1",
    "authors": "Hansheng Chen, Kai Zhang, Hao Tan, Leonidas Guibas, Gordon Wetzstein, Sai Bi",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-16 17:59:51",
    "ori_summary": "Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves substantially better diversity than state-of-the-art few-step methods, while maintaining teacher-level quality.",
    "summary": "",
    "translation": "pi-Flow：基于策略的模仿蒸馏少步生成方法",
    "relevance_score": 3,
    "reasoning": "该论文涉及策略学习和模仿蒸馏技术，属于强化学习范畴，但标题未明确展示与推荐系统、搜索或广告的直接相关性。虽然少步生成技术可能在某些序列生成场景中有潜在应用，但缺乏明确的RecSys/Search/Ads应用场景说明，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14968v1": {
    "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks",
    "url": "https://www.alphaxiv.org/abs/2510.14968v1",
    "arxiv_id": "2510.14968v1",
    "authors": "Mingxuan Yan, Yuping Wang, Zechun Liu, Jiachen Li",
    "categories": "cs.RO, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY",
    "pub_date": "2025-10-16 17:59:37",
    "ori_summary": "To tackle long-horizon tasks, recent hierarchical vision-language-action (VLAs) frameworks employ vision-language model (VLM)-based planners to decompose complex manipulation tasks into simpler sub-tasks that low-level visuomotor policies can easily handle. Typically, the VLM planner is finetuned to learn to decompose a target task. This finetuning requires target task demonstrations segmented into sub-tasks by either human annotation or heuristic rules. However, the heuristic subtasks can deviate significantly from the training data of the visuomotor policy, which degrades task performance. To address these issues, we propose a Retrieval-based Demonstration Decomposer (RDD) that automatically decomposes demonstrations into sub-tasks by aligning the visual features of the decomposed sub-task intervals with those from the training data of the low-level visuomotor policies. Our method outperforms the state-of-the-art sub-task decomposer on both simulation and real-world tasks, demonstrating robustness across diverse settings. Code and more results are available at rdd-neurips.github.io.",
    "summary": "",
    "translation": "RDD：基于检索的演示分解器，用于长周期任务中的规划器对齐",
    "relevance_score": 3,
    "reasoning": "该论文主要关注长周期任务的规划器对齐，属于通用AI规划领域。虽然基于检索的演示分解可能对推荐系统中的序列规划有间接启发，但论文没有明确涉及推荐、搜索或广告的具体应用场景。其技术可能为复杂的用户行为序列建模提供参考，但直接相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14965v1": {
    "title": "ChangingGrounding: 3D Visual Grounding in Changing Scenes",
    "url": "https://www.alphaxiv.org/abs/2510.14965v1",
    "arxiv_id": "2510.14965v1",
    "authors": "Miao Hu, Zhiwei Huang, Tai Wang, Jiangmiao Pang, Dahua Lin, Nanning Zheng, Runsen Xu",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 17:59:16",
    "ori_summary": "Real-world robots localize objects from natural-language instructions while scenes around them keep changing. Yet most of the existing 3D visual grounding (3DVG) method still assumes a reconstructed and up-to-date point cloud, an assumption that forces costly re-scans and hinders deployment. We argue that 3DVG should be formulated as an active, memory-driven problem, and we introduce ChangingGrounding, the first benchmark that explicitly measures how well an agent can exploit past observations, explore only where needed, and still deliver precise 3D boxes in changing scenes. To set a strong reference point, we also propose Mem-ChangingGrounder, a zero-shot method for this task that marries cross-modal retrieval with lightweight multi-view fusion: it identifies the object type implied by the query, retrieves relevant memories to guide actions, then explores the target efficiently in the scene, falls back when previous operations are invalid, performs multi-view scanning of the target, and projects the fused evidence from multi-view scans to get accurate object bounding boxes. We evaluate different baselines on ChangingGrounding, and our Mem-ChangingGrounder achieves the highest localization accuracy while greatly reducing exploration cost. We hope this benchmark and method catalyze a shift toward practical, memory-centric 3DVG research for real-world applications. Project page: https://hm123450.github.io/CGB/ .",
    "summary": "",
    "translation": "ChangingGrounding：变化场景中的3D视觉定位",
    "relevance_score": 2,
    "reasoning": "该论文专注于3D视觉定位在动态场景中的应用，这属于纯粹的计算机视觉领域。虽然视觉定位技术在某些特定场景下可能对推荐或搜索有间接价值，但论文标题明确聚焦于3D视觉和场景变化，与推荐系统、搜索或广告的核心技术栈缺乏直接关联，也不涉及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14962v1": {
    "title": "RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention Diffusion",
    "url": "https://www.alphaxiv.org/abs/2510.14962v1",
    "arxiv_id": "2510.14962v1",
    "authors": "Thao Nguyen, Jiaqi Ma, Fahad Shahbaz Khan, Souhaib Ben Taieb, Salman Khan",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 17:59:13",
    "ori_summary": "Precipitation nowcasting, predicting future radar echo sequences from current observations, is a critical yet challenging task due to the inherently chaotic and tightly coupled spatio-temporal dynamics of the atmosphere. While recent advances in diffusion-based models attempt to capture both large-scale motion and fine-grained stochastic variability, they often suffer from scalability issues: latent-space approaches require a separately trained autoencoder, adding complexity and limiting generalization, while pixel-space approaches are computationally intensive and often omit attention mechanisms, reducing their ability to model long-range spatio-temporal dependencies. To address these limitations, we propose a Token-wise Attention integrated into not only the U-Net diffusion model but also the spatio-temporal encoder that dynamically captures multi-scale spatial interactions and temporal evolution. Unlike prior approaches, our method natively integrates attention into the architecture without incurring the high resource cost typical of pixel-space diffusion, thereby eliminating the need for separate latent modules. Our extensive experiments and visual evaluations across diverse datasets demonstrate that the proposed method significantly outperforms state-of-the-art approaches, yielding superior local fidelity, generalization, and robustness in complex precipitation forecasting scenarios.",
    "summary": "",
    "translation": "RainDiff：通过词元级注意力扩散实现端到端降水临近预报",
    "relevance_score": 1,
    "reasoning": "该论文专注于气象领域的降水预测问题，属于特定领域应用而非推荐系统、搜索或广告的核心技术。虽然涉及注意力机制和扩散模型，但这些技术在此文中纯粹用于气象预测，没有展示在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14960v1": {
    "title": "C4D: 4D Made from 3D through Dual Correspondences",
    "url": "https://www.alphaxiv.org/abs/2510.14960v1",
    "arxiv_id": "2510.14960v1",
    "authors": "Shizun Wang, Zhenxiang Jiang, Xingyi Yang, Xinchao Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 17:59:06",
    "ori_summary": "Recovering 4D from monocular video, which jointly estimates dynamic geometry and camera poses, is an inevitably challenging problem. While recent pointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great progress in reconstructing static scenes, directly applying them to dynamic scenes leads to inaccurate results. This discrepancy arises because moving objects violate multi-view geometric constraints, disrupting the reconstruction. To address this, we introduce C4D, a framework that leverages temporal Correspondences to extend existing 3D reconstruction formulation to 4D. Specifically, apart from predicting pointmaps, C4D captures two types of correspondences: short-term optical flow and long-term point tracking. We train a dynamic-aware point tracker that provides additional mobility information, facilitating the estimation of motion masks to separate moving elements from the static background, thus offering more reliable guidance for dynamic scenes. Furthermore, we introduce a set of dynamic scene optimization objectives to recover per-frame 3D geometry and camera parameters. Simultaneously, the correspondences lift 2D trajectories into smooth 3D trajectories, enabling fully integrated 4D reconstruction. Experiments show that our framework achieves complete 4D recovery and demonstrates strong performance across multiple downstream tasks, including depth estimation, camera pose estimation, and point tracking. Project Page: https://littlepure2333.github.io/C4D",
    "summary": "",
    "translation": "C4D：通过双重对应关系从3D生成4D",
    "relevance_score": 1,
    "reasoning": "这篇论文标题表明它专注于4D生成和3D到4D的转换技术，这属于计算机视觉和图形学领域。该主题与我的关注点（推荐系统、搜索、广告、Transformer架构或LLM应用）没有直接关联，也没有明显的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14955v1": {
    "title": "RealDPO: Real or Not Real, that is the Preference",
    "url": "https://www.alphaxiv.org/abs/2510.14955v1",
    "arxiv_id": "2510.14955v1",
    "authors": "Guo Cheng, Danni Yang, Ziqi Huang, Jianlou Si, Chenyang Si, Ziwei Liu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 17:58:25",
    "ori_summary": "Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.",
    "summary": "",
    "translation": "RealDPO：真实与否，即为偏好",
    "relevance_score": 8,
    "reasoning": "该论文标题暗示了一种新的偏好优化方法（DPO变体），这属于LLM核心技术的进步。在推荐系统和搜索领域，偏好学习对于个性化排序至关重要，RealDPO方法可能直接应用于优化用户偏好建模，提升推荐和搜索结果的准确性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14954v1": {
    "title": "OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression",
    "url": "https://www.alphaxiv.org/abs/2510.14954v1",
    "arxiv_id": "2510.14954v1",
    "authors": "Zhe Li, Weihao Yuan, Weichao Shen, Siyu Zhu, Zilong Dong, Chang Xu",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 17:57:53",
    "ori_summary": "Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework. Unlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion. Within this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities. To further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets. To fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals. Experimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance. The code of our method will be made public.",
    "summary": "",
    "translation": "OmniMotion：基于连续掩码自回归的多模态运动生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态运动生成，属于计算机视觉和动作生成领域，与推荐系统、搜索或广告的核心技术栈关联度较低。虽然连续掩码自回归技术可能在某些序列建模场景中有启发意义，但缺乏明确的在RecSys/Search/Ads领域的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14952v1": {
    "title": "From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance",
    "url": "https://www.alphaxiv.org/abs/2510.14952v1",
    "arxiv_id": "2510.14952v1",
    "authors": "Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Yibo Peng, Tao Huang, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang, Chang Xu",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-16 17:57:47",
    "ori_summary": "Natural language offers a natural interface for humanoid robots, but existing language-guided humanoid locomotion pipelines remain cumbersome and unreliable. They typically decode human motion, retarget it to robot morphology, and then track it with a physics-based controller. However, this multi-stage process is prone to cumulative errors, introduces high latency, and yields weak coupling between semantics and control. These limitations call for a more direct pathway from language to action, one that eliminates fragile intermediate stages. Therefore, we present RoboGhost, a retargeting-free framework that directly conditions humanoid policies on language-grounded motion latents. By bypassing explicit motion decoding and retargeting, RoboGhost enables a diffusion-based policy to denoise executable actions directly from noise, preserving semantic intent and supporting fast, reactive control. A hybrid causal transformer-diffusion motion generator further ensures long-horizon consistency while maintaining stability and diversity, yielding rich latent representations for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost substantially reduces deployment latency, improves success rates and tracking accuracy, and produces smooth, semantically aligned locomotion on real humanoids. Beyond text, the framework naturally extends to other modalities such as images, audio, and music, providing a general foundation for vision-language-action humanoid systems.",
    "summary": "",
    "translation": "从语言到运动：基于运动潜在引导的无重定向人形机器人控制",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人控制和人形运动生成，属于计算机视觉和机器人学领域。虽然涉及语言到动作的转换，但其核心是物理机器人控制技术，与推荐系统、搜索或广告的排名和建模问题没有直接关联，也不涉及Transformer架构改进或异构数据统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14945v1": {
    "title": "3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14945v1",
    "arxiv_id": "2510.14945v1",
    "authors": "JoungBin Lee, Jaewoo Jung, Jisang Han, Takuya Narihira, Kazumi Fukuda, Junyoung Seo, Sunghwan Hong, Yuki Mitsufuji, Seungryong Kim",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 17:55:25",
    "ori_summary": "We present 3DScenePrompt, a framework that generates the next video chunk from arbitrary-length input while enabling precise camera control and preserving scene consistency. Unlike methods conditioned on a single image or a short clip, we employ dual spatio-temporal conditioning that reformulates context-view referencing across the input video. Our approach conditions on both temporally adjacent frames for motion continuity and spatially adjacent content for scene consistency. However, when generating beyond temporal boundaries, directly using spatially adjacent frames would incorrectly preserve dynamic elements from the past. We address this by introducing a 3D scene memory that represents exclusively the static geometry extracted from the entire input video. To construct this memory, we leverage dynamic SLAM with our newly introduced dynamic masking strategy that explicitly separates static scene geometry from moving elements. The static scene representation can then be projected to any target viewpoint, providing geometrically consistent warped views that serve as strong 3D spatial prompts while allowing dynamic regions to evolve naturally from temporal context. This enables our model to maintain long-range spatial coherence and precise camera control without sacrificing computational efficiency or motion realism. Extensive experiments demonstrate that our framework significantly outperforms existing methods in scene consistency, camera controllability, and generation quality. Project page : https://cvlab-kaist.github.io/3DScenePrompt/",
    "summary": "",
    "translation": "面向场景一致性相机可控视频生成的三维场景提示",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D场景和视频生成技术，属于纯粹的视觉生成领域。虽然提到了场景一致性，但这与推荐系统、搜索或广告中的异构数据统一建模没有直接关联，也没有明显的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14904v1": {
    "title": "MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos",
    "url": "https://www.alphaxiv.org/abs/2510.14904v1",
    "arxiv_id": "2510.14904v1",
    "authors": "Gabriel Fiastre, Antoine Yang, Cordelia Schmid",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-16 17:20:22",
    "ori_summary": "Dense Video Object Captioning (DVOC) is the task of jointly detecting, tracking, and captioning object trajectories in a video, requiring the ability to understand spatio-temporal details and describe them in natural language. Due to the complexity of the task and the high cost associated with manual annotation, previous approaches resort to disjoint training strategies, potentially leading to suboptimal performance. To circumvent this issue, we propose to generate captions about spatio-temporally localized entities leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an end-to-end model capable of jointly detecting, segmenting, tracking and captioning object trajectories. Moreover, with pretraining on LVISCap and LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are available at https://www.gabriel.fiastre.fr/maskcaptioner/.",
    "summary": "",
    "translation": "MaskCaptioner：学习在视频中联合分割和描述对象轨迹",
    "relevance_score": 2,
    "reasoning": "该论文专注于视频中的对象轨迹分割和描述，这属于计算机视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然视频理解技术可能在某些边缘场景中用于内容分析，但缺乏明确的推荐、搜索或广告应用潜力，且不涉及LLM、Transformer架构或异构数据建模等关键技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14896v1": {
    "title": "Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection",
    "url": "https://www.alphaxiv.org/abs/2510.14896v1",
    "arxiv_id": "2510.14896v1",
    "authors": "Furkan Mumcu, Michael J. Jones, Anoop Cherian, Yasin Yilmaz",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 17:13:33",
    "ori_summary": "Existing semi-supervised video anomaly detection (VAD) methods often struggle with detecting complex anomalies involving object interactions and generally lack explainability. To overcome these limitations, we propose a novel VAD framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous MLLM-based approaches that make direct anomaly judgments at the frame level, our method focuses on extracting and interpreting object activity and interactions over time. By querying an MLLM with visual inputs of object pairs at different moments, we generate textual descriptions of the activity and interactions from nominal videos. These textual descriptions serve as a high-level representation of the activity and interactions of objects in a video. They are used to detect anomalies during test time by comparing them to textual descriptions found in nominal training videos. Our approach inherently provides explainability and can be combined with many traditional VAD methods to further enhance their interpretability. Extensive experiments on benchmark datasets demonstrate that our method not only detects complex interaction-based anomalies effectively but also achieves state-of-the-art performance on datasets without interaction anomalies.",
    "summary": "",
    "translation": "利用多模态大语言模型对活动描述进行可解释的半监督视频异常检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频异常检测，这是一个计算机视觉应用领域，与推荐系统、搜索或广告的核心技术关联性较弱。虽然提到了多模态LLM，但其应用场景（视频分析）和核心问题（异常检测）偏离了当前关注的推荐、搜索和广告领域的技术进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14882v1": {
    "title": "ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention",
    "url": "https://www.alphaxiv.org/abs/2510.14882v1",
    "arxiv_id": "2510.14882v1",
    "authors": "Keli Liu, Zhendong Wang, Wengang Zhou, Shaodong Xu, Ruixiao Dong, Houqiang Li",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 17:00:59",
    "ori_summary": "Text-to-image generation with visual autoregressive~(VAR) models has recently achieved impressive advances in generation fidelity and inference efficiency. While control mechanisms have been explored for diffusion models, enabling precise and flexible control within VAR paradigm remains underexplored. To bridge this critical gap, in this paper, we introduce ScaleWeaver, a novel framework designed to achieve high-fidelity, controllable generation upon advanced VAR models through parameter-efficient fine-tuning. The core module in ScaleWeaver is the improved MMDiT block with the proposed Reference Attention module, which efficiently and effectively incorporates conditional information. Different from MM Attention, the proposed Reference Attention module discards the unnecessary attention from image$\\rightarrow$condition, reducing computational cost while stabilizing control injection. Besides, it strategically emphasizes parameter reuse, leveraging the capability of the VAR backbone itself with a few introduced parameters to process control information, and equipping a zero-initialized linear projection to ensure that control signals are incorporated effectively without disrupting the generative capability of the base model. Extensive experiments show that ScaleWeaver delivers high-quality generation and precise control while attaining superior efficiency over diffusion-based methods, making ScaleWeaver a practical and effective solution for controllable text-to-image generation within the visual autoregressive paradigm. Code and models will be released.",
    "summary": "",
    "translation": "ScaleWeaver：通过多尺度参考注意力编织高效可控的文本到图像生成",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于文本到图像生成的可控性技术，属于纯粹的AIGC和内容生成领域。虽然提到了注意力机制，但其应用场景仅限于图像生成，与推荐系统、搜索或广告的排名任务没有直接关联，也不具备在这些领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14876v1": {
    "title": "BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data",
    "url": "https://www.alphaxiv.org/abs/2510.14876v1",
    "arxiv_id": "2510.14876v1",
    "authors": "Roni Goldshmidt, Hamish Scott, Lorenzo Niccolini, Shizhan Zhu, Daniel Moura, Orly Zvitia",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 16:55:30",
    "ori_summary": "Existing collision prediction methods often fail to distinguish between ego-vehicle threats and random accidents not involving the ego vehicle, leading to excessive false alerts in real-world deployment. We present BADAS, a family of collision prediction models trained on Nexar's real-world dashcam collision dataset -- the first benchmark designed explicitly for ego-centric evaluation. We re-annotate major benchmarks to identify ego involvement, add consensus alert-time labels, and synthesize negatives where needed, enabling fair AP/AUC and temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and comes in two variants: BADAS-Open (trained on our 1.5k public videos) and BADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and Nexar, BADAS achieves state-of-the-art AP/AUC and outperforms a forward-collision ADAS baseline while producing more realistic time-to-accident estimates. We release our BADAS-Open model weights and code, along with re-annotations of all evaluation datasets to promote ego-centric collision prediction research.",
    "summary": "",
    "translation": "BADAS：基于真实世界行车记录仪数据的上下文感知碰撞预测",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉领域的碰撞预测应用，使用行车记录仪数据进行交通安全分析。虽然涉及上下文感知，但这是纯粹的视觉应用场景，与推荐系统、搜索或广告的核心技术领域没有任何直接关联，也不涉及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14874v1": {
    "title": "TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions",
    "url": "https://www.alphaxiv.org/abs/2510.14874v1",
    "arxiv_id": "2510.14874v1",
    "authors": "Guangyi Han, Wei Zhai, Yuhang Yang, Yang Cao, Zheng-Jun Zha",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 16:52:58",
    "ori_summary": "Hand-object interaction (HOI) is fundamental for humans to express intent. Existing HOI generation research is predominantly confined to fixed grasping patterns, where control is tied to physical priors such as force closure or generic intent instructions, even when expressed through elaborate language. Such an overly general conditioning imposes a strong inductive bias for stable grasps, thus failing to capture the diversity of daily HOI. To address these limitations, we introduce Free-Form HOI Generation, which aims to generate controllable, diverse, and physically plausible HOI conditioned on fine-grained intent, extending HOI from grasping to free-form interactions, like pushing, poking, and rotating. To support this task, we construct WildO2, an in-the-wild diverse 3D HOI dataset, which includes diverse HOI derived from internet videos. Specifically, it contains 4.4k unique interactions across 92 intents and 610 object categories, each with detailed semantic annotations. Building on this dataset, we propose TOUCH, a three-stage framework centered on a multi-level diffusion model that facilitates fine-grained semantic control to generate versatile hand poses beyond grasping priors. This process leverages explicit contact modeling for conditioning and is subsequently refined with contact consistency and physical constraints to ensure realism. Comprehensive experiments demonstrate our method's ability to generate controllable, diverse, and physically plausible hand interactions representative of daily activities. The project page is $\\href{https://guangyid.github.io/hoi123touch}{here}$.",
    "summary": "",
    "translation": "TOUCH：文本引导的自由形式手-物体交互可控生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于手-物体交互的3D生成，属于计算机视觉和图形学领域，与推荐系统、搜索或广告的核心技术无关。虽然涉及可控生成技术，但应用场景局限于物理交互模拟，没有明显的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14862v1": {
    "title": "Multi-modal video data-pipelines for machine learning with minimal human supervision",
    "url": "https://www.alphaxiv.org/abs/2510.14862v1",
    "arxiv_id": "2510.14862v1",
    "authors": "Mihai-Cristian Pîrvu, Marius Leordeanu",
    "categories": "cs.CV, cs.DC",
    "pub_date": "2025-10-16 16:36:29",
    "ori_summary": "The real-world is inherently multi-modal at its core. Our tools observe and take snapshots of it, in digital form, such as videos or sounds, however much of it is lost. Similarly for actions and information passing between humans, languages are used as a written form of communication. Traditionally, Machine Learning models have been unimodal (i.e. rgb -> semantic or text -> sentiment_class). Recent trends go towards bi-modality, where images and text are learned together, however, in order to truly understand the world, we need to integrate all these independent modalities. In this work we try to combine as many visual modalities as we can using little to no human supervision. In order to do this, we use pre-trained experts and procedural combinations between them on top of raw videos using a fully autonomous data-pipeline, which we also open-source. We then make use of PHG-MAE, a model specifically designed to leverage multi-modal data. We show that this model which was efficiently distilled into a low-parameter (<1M) can have competitive results compared to models of ~300M parameters. We deploy this model and analyze the use-case of real-time semantic segmentation from handheld devices or webcams on commodity hardware. Finally, we deploy other off-the-shelf models using the same framework, such as DPT for near real-time depth estimation.",
    "summary": "",
    "translation": "用于机器学习的多模态视频数据管道，具有最少的人工监督",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频数据的多模态处理管道和减少人工监督，这与计算机视觉和视频理解领域更相关。虽然多模态数据处理在推荐系统中可能有间接应用，但论文标题没有明确表明与推荐系统、搜索或广告的直接关联，也没有提到LLM或Transformer技术的应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14855v1": {
    "title": "A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution Simulation",
    "url": "https://www.alphaxiv.org/abs/2510.14855v1",
    "arxiv_id": "2510.14855v1",
    "authors": "Harsha Kotla, Arun Kumar Rajasekaran, Hannah Rana",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-16 16:28:21",
    "ori_summary": "Early detection of melanoma has grown to be essential because it significantly improves survival rates, but automated analysis of skin lesions still remains challenging. ABCDE, which stands for Asymmetry, Border irregularity, Color variation, Diameter, and Evolving, is a well-known classification method for skin lesions, but most deep learning mechanisms treat it as a black box, as most of the human interpretable features are not explained. In this work, we propose a deep learning framework that both classifies skin lesions into categories and also quantifies scores for each ABCD feature. It simulates the evolution of these features over time in order to represent the E aspect, opening more windows for future exploration. The A, B, C, and D values are quantified particularly within this work. Moreover, this framework also visualizes ABCD feature trajectories in latent space as skin lesions evolve from benign nevuses to malignant melanoma. The experiments are conducted using the HAM10000 dataset that contains around ten thousand images of skin lesions of varying stages. In summary, the classification worked with an accuracy of around 89 percent, with melanoma AUC being 0.96, while the feature evaluation performed well in predicting asymmetry, color variation, and diameter, though border irregularity remains more difficult to model. Overall, this work provides a deep learning framework that will allow doctors to link ML diagnoses to clinically relevant criteria, thus improving our understanding of skin cancer progression.",
    "summary": "",
    "translation": "用于皮肤病变分类、ABCDE特征量化和演变模拟的多任务深度学习框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的皮肤病变分析，属于明确的医学应用范畴。虽然采用了多任务深度学习框架，但其应用场景（皮肤病变分类、特征量化、演变模拟）与推荐系统、搜索或广告领域没有任何关联，完全属于被排除的医学领域特定应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14847v1": {
    "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
    "url": "https://www.alphaxiv.org/abs/2510.14847v1",
    "arxiv_id": "2510.14847v1",
    "authors": "Meiqi Wu, Jiashu Zhu, Xiaokun Feng, Chubin Chen, Chen Zhu, Bingze Song, Fangyuan Mao, Jiahong Wu, Xiangxiang Chu, Kaiqi Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 16:19:13",
    "ori_summary": "Video generation models have achieved remarkable progress, particularly excelling in realistic scenarios; however, their performance degrades notably in imaginative scenarios. These prompts often involve rarely co-occurring concepts with long-distance semantic relationships, falling outside training distributions. Existing methods typically apply test-time scaling for improving video quality, but their fixed search spaces and static reward designs limit adaptability to imaginative scenarios. To fill this gap, we propose ImagerySearch, a prompt-guided adaptive test-time search strategy that dynamically adjusts both the inference search space and reward function according to semantic relationships in the prompt. This enables more coherent and visually plausible videos in challenging imaginative settings. To evaluate progress in this direction, we introduce LDT-Bench, the first dedicated benchmark for long-distance semantic prompts, consisting of 2,839 diverse concept pairs and an automated protocol for assessing creative generation capabilities. Extensive experiments show that ImagerySearch consistently outperforms strong video generation baselines and existing test-time scaling approaches on LDT-Bench, and achieves competitive improvements on VBench, demonstrating its effectiveness across diverse prompt types. We will release LDT-Bench and code to facilitate future research on imaginative video generation.",
    "summary": "",
    "translation": "ImagerySearch：超越语义依赖约束的自适应测试时搜索视频生成",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视频生成中的搜索技术，虽然搜索是相关领域，但论文重点在视频内容生成而非推荐系统、搜索或广告中的信息检索。作为使能技术，其自适应搜索方法可能启发推荐系统中动态内容发现，但应用潜力有限且主要针对生成任务。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14845v1": {
    "title": "Backdoor Unlearning by Linear Task Decomposition",
    "url": "https://www.alphaxiv.org/abs/2510.14845v1",
    "arxiv_id": "2510.14845v1",
    "authors": "Amel Abdelraheem, Alessandro Favero, Gerome Bovet, Pascal Frossard",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-16 16:18:07",
    "ori_summary": "Foundation models have revolutionized computer vision by enabling broad generalization across diverse tasks. Yet, they remain highly susceptible to adversarial perturbations and targeted backdoor attacks. Mitigating such vulnerabilities remains an open challenge, especially given that the large-scale nature of the models prohibits retraining to ensure safety. Existing backdoor removal approaches rely on costly fine-tuning to override the harmful behavior, and can often degrade performance on other unrelated tasks. This raises the question of whether backdoors can be removed without compromising the general capabilities of the models. In this work, we address this question and study how backdoors are encoded in the model weight space, finding that they are disentangled from other benign tasks. Specifically, this separation enables the isolation and erasure of the backdoor's influence on the model with minimal impact on clean performance. Building on this insight, we introduce a simple unlearning method that leverages such disentanglement. Through extensive experiments with CLIP-based models and common adversarial triggers, we show that, given the knowledge of the attack, our method achieves approximately perfect unlearning, while retaining, on average, 96% of clean accuracy. Additionally, we demonstrate that even when the attack and its presence are unknown, our method successfully unlearns backdoors by proper estimation using reverse-engineered triggers. Overall, our method consistently yields better unlearning and clean accuracy tradeoffs when compared to present state-of-the-art defenses.",
    "summary": "",
    "translation": "通过线性任务分解实现后门遗忘",
    "relevance_score": 1,
    "reasoning": "该论文涉及后门攻击和遗忘学习，属于安全隐私领域，与我的核心关注点（推荐系统、搜索广告、LLM技术、Transformer架构等）完全无关。后门遗忘技术主要解决模型安全问题，这在当前关注中被明确列为不相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14836v1": {
    "title": "QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models",
    "url": "https://www.alphaxiv.org/abs/2510.14836v1",
    "arxiv_id": "2510.14836v1",
    "authors": "Yixuan Li, Yuhui Chen, Mingcai Zhou, Haoran Li",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-16 16:11:18",
    "ori_summary": "Spatial perception and reasoning are crucial for Vision-Language-Action (VLA) models to accomplish fine-grained manipulation tasks. However, existing approaches often lack the ability to understand and reason over the essential 3D structures necessary for precise control. To address this limitation, we propose QDepth-VLA, a general framework that augments VLA models with an auxiliary depth prediction task. A dedicated depth expert is designed to predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder, enabling the model to learn depth-aware representations that capture critical geometric cues. Experimental results on the simulation benchmarks and real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning and competitive performance on manipulation tasks.",
    "summary": "",
    "translation": "QDepth-VLA：量化深度预测作为视觉-语言-动作模型的辅助监督",
    "relevance_score": 1,
    "reasoning": "该论文专注于视觉-语言-动作模型和深度预测，属于机器人学和计算机视觉领域，与推荐系统、搜索或广告的核心关注点无关。量化技术虽然具有通用性，但该工作没有展示在RecSys/Search/Ads领域的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14831v1": {
    "title": "Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data",
    "url": "https://www.alphaxiv.org/abs/2510.14831v1",
    "arxiv_id": "2510.14831v1",
    "authors": "Qi Chen, Xinze Zhou, Chen Liu, Hao Chen, Wenxuan Li, Zekun Jiang, Ziyan Huang, Yuxuan Zhao, Dexin Yu, Junjun He, Yefeng Zheng, Ling Shao, Alan Yuille, Zongwei Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 16:08:09",
    "ori_summary": "AI for tumor segmentation is limited by the lack of large, voxel-wise annotated datasets, which are hard to create and require medical experts. In our proprietary JHH dataset of 3,000 annotated pancreatic tumor scans, we found that AI performance stopped improving after 1,500 scans. With synthetic data, we reached the same performance using only 500 real scans. This finding suggests that synthetic data can steepen data scaling laws, enabling more efficient model training than real data alone. Motivated by these lessons, we created AbdomenAtlas 2.0--a dataset of 10,135 CT scans with a total of 15,130 tumor instances per-voxel manually annotated in six organs (pancreas, liver, kidney, colon, esophagus, and uterus) and 5,893 control scans. Annotated by 23 expert radiologists, it is several orders of magnitude larger than existing public tumor datasets. While we continue expanding the dataset, the current version of AbdomenAtlas 2.0 already provides a strong foundation--based on lessons from the JHH dataset--for training AI to segment tumors in six organs. It achieves notable improvements over public datasets, with a +7% DSC gain on in-distribution tests and +16% on out-of-distribution tests.",
    "summary": "",
    "translation": "肿瘤分割的规模化：来自真实与合成数据的最佳经验",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像中的肿瘤分割，属于医疗领域的特定应用，与推荐系统、搜索或广告领域完全无关。论文内容涉及医学图像处理和医疗数据分析，这在用户指定的无关话题中明确排除。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14823v1": {
    "title": "FraQAT: Quantization Aware Training with Fractional bits",
    "url": "https://www.alphaxiv.org/abs/2510.14823v1",
    "arxiv_id": "2510.14823v1",
    "authors": "Luca Morreale, Alberto Gil C. P. Ramos, Malcolm Chadwick, Mehid Noroozi, Ruchika Chavhan, Abhinav Mehrotra, Sourav Bhattacharya",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 16:01:08",
    "ori_summary": "State-of-the-art (SOTA) generative models have demonstrated impressive capabilities in image synthesis or text generation, often with a large capacity model. However, these large models cannot be deployed on smartphones due to the limited availability of on-board memory and computations. Quantization methods lower the precision of the model parameters, allowing for efficient computations, \\eg, in \\INT{8}. Although aggressive quantization addresses efficiency and memory constraints, preserving the quality of the model remains a challenge. To retain quality in previous aggressive quantization, we propose a new fractional bits quantization (\\short) approach. The novelty is a simple yet effective idea: we progressively reduce the model's precision from 32 to 4 bits per parameter, and exploit the fractional bits during optimization to maintain high generation quality. We show that the \\short{} yields improved quality on a variety of diffusion models, including SD3.5-Medium, Sana, \\pixart, and FLUX.1-schnell, while achieving $4-7\\%$ lower FiD than standard QAT. Finally, we deploy and run Sana on a Samsung S25U, which runs on the Qualcomm SM8750-AB Snapdragon 8 Elite Hexagon Tensor Processor (HTP).",
    "summary": "",
    "translation": "FraQAT：使用分数位数的量化感知训练",
    "relevance_score": 8,
    "reasoning": "该论文涉及Transformer架构效率的核心进展，通过分数位数量化技术优化模型计算和存储效率。这种量化方法可直接应用于大规模推荐和搜索系统中的LLM部署，显著降低推理延迟和内存占用，同时保持模型性能。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14819v1": {
    "title": "Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning",
    "url": "https://www.alphaxiv.org/abs/2510.14819v1",
    "arxiv_id": "2510.14819v1",
    "authors": "Ji Cao, Yu Wang, Tongya Zheng, Zujie Ren, Canghong Jin, Gang Chen, Mingli Song",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-16 15:55:28",
    "ori_summary": "Trajectory Representation Learning (TRL) aims to encode raw trajectories into low-dimensional vectors, which can then be leveraged in various downstream tasks, including travel time estimation, location prediction, and trajectory similarity analysis. However, existing TRL methods suffer from a key oversight: treating trajectories as isolated spatio-temporal sequences, without considering the external environment and internal route choice behavior that govern their formation. To bridge this gap, we propose a novel framework that unifies comprehensive environment \\textbf{P}erception and explicit \\textbf{R}oute choice modeling for effective \\textbf{Traj}ectory representation learning, dubbed \\textbf{PRTraj}. Specifically, PRTraj first introduces an Environment Perception Module to enhance the road network by capturing multi-granularity environmental semantics from surrounding POI distributions. Building on this environment-aware backbone, a Route Choice Encoder then captures the route choice behavior inherent in each trajectory by modeling its constituent road segment transitions as a sequence of decisions. These route-choice-aware representations are finally aggregated to form the global trajectory embedding. Extensive experiments on 3 real-world datasets across 5 downstream tasks validate the effectiveness and generalizability of PRTraj. Moreover, PRTraj demonstrates strong data efficiency, maintaining robust performance under few-shot scenarios. Our code is available at: https://anonymous.4open.science/r/PRTraj.",
    "summary": "",
    "translation": "环境感知与路径选择建模的统一化用于轨迹表示学习",
    "relevance_score": 2,
    "reasoning": "该论文主要关注轨迹表示学习，属于移动行为建模领域，与推荐系统、搜索或广告的核心焦点关联度较低。虽然轨迹数据可能在某些边缘场景下用于位置推荐，但论文标题表明其重点在于环境感知和路径选择的统一建模，缺乏明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14803v1": {
    "title": "Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks",
    "url": "https://www.alphaxiv.org/abs/2510.14803v1",
    "arxiv_id": "2510.14803v1",
    "authors": "Pedro R. A. S. Bassi, Xinze Zhou, Wenxuan Li, Szymon Płotka, Jieneng Chen, Qi Chen, Zheren Zhu, Jakub Prządo, Ibrahim E. Hamacı, Sezgin Er, Yuhan Wang, Ashwin Kumar, Bjoern Menze, Jarosław B. Ćwikła, Yuyin Zhou, Akshay S. Chaudhari, Curtis P. Langlotz, Sergio Decherchi, Andrea Cavalli, Kang Wang, Yang Yang, Alan L. Yuille, Zongwei Zhou",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 15:35:44",
    "ori_summary": "Early tumor detection save lives. Each year, more than 300 million computed tomography (CT) scans are performed worldwide, offering a vast opportunity for effective cancer screening. However, detecting small or early-stage tumors on these CT scans remains challenging, even for experts. Artificial intelligence (AI) models can assist by highlighting suspicious regions, but training such models typically requires extensive tumor masks--detailed, voxel-wise outlines of tumors manually drawn by radiologists. Drawing these masks is costly, requiring years of effort and millions of dollars. In contrast, nearly every CT scan in clinical practice is already accompanied by medical reports describing the tumor's size, number, appearance, and sometimes, pathology results--information that is rich, abundant, and often underutilized for AI training. We introduce R-Super, which trains AI to segment tumors that match their descriptions in medical reports. This approach scales AI training with large collections of readily available medical reports, substantially reducing the need for manually drawn tumor masks. When trained on 101,654 reports, AI models achieved performance comparable to those trained on 723 masks. Combining reports and masks further improved sensitivity by +13% and specificity by +8%, surpassing radiologists in detecting five of the seven tumor types. Notably, R-Super enabled segmentation of tumors in the spleen, gallbladder, prostate, bladder, uterus, and esophagus, for which no public masks or AI models previously existed. This study challenges the long-held belief that large-scale, labor-intensive tumor mask creation is indispensable, establishing a scalable and accessible path toward early detection across diverse tumor types. We plan to release our trained models, code, and dataset at https://github.com/MrGiovanni/R-Super",
    "summary": "",
    "translation": "利用更多报告、更少掩码扩展人工智能用于多肿瘤早期检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的多肿瘤早期检测应用，属于明确的医疗生物领域，与推荐系统、搜索或广告完全无关。标题中提到的'报告'和'掩码'在医学影像分析背景下，与推荐系统、搜索或广告中的用户行为序列、上下文特征等异构数据处理没有关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14800v1": {
    "title": "Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&E Whole Slide Images",
    "url": "https://www.alphaxiv.org/abs/2510.14800v1",
    "arxiv_id": "2510.14800v1",
    "authors": "Usama Sajjad, Abdul Rehman Akbar, Ziyu Su, Deborah Knight, Wendy L. Frankel, Metin N. Gurcan, Wei Chen, Muhammad Khalid Khan Niazi",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 15:32:05",
    "ori_summary": "Colorectal cancer (CRC) remains the third most prevalent malignancy globally, with approximately 154,000 new cases and 54,000 projected deaths anticipated for 2025. The recent advancement of foundation models in computational pathology has been largely propelled by task agnostic methodologies that can overlook organ-specific crucial morphological patterns that represent distinct biological processes that can fundamentally influence tumor behavior, therapeutic response, and patient outcomes. The aim of this study is to develop a novel, interpretable AI model, PRISM (Prognostic Representation of Integrated Spatial Morphology), that incorporates a continuous variability spectrum within each distinct morphology to characterize phenotypic diversity and reflecting the principle that malignant transformation occurs through incremental evolutionary processes rather than abrupt phenotypic shifts. PRISM is trained on 8.74 million histological images extracted from surgical resection specimens of 424 patients with stage III CRC. PRISM achieved superior prognostic performance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%; HR = 3.34, 95% CI = 2.28-4.90; p < 0.0001), outperforming existing CRC-specific methods by 15% and AI foundation models by ~23% accuracy. It showed sex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stable performance across clinicopathological subgroups, with minimal accuracy fluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens, replicating the Alliance cohort finding of no survival difference between treatments.",
    "summary": "",
    "translation": "基于H&E全切片图像的结直肠癌五年生存预测形态学感知预后模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的结直肠癌生存预测，属于明确的医学/生物学应用范畴。虽然涉及图像分析技术，但其应用场景和问题定义完全限定在医疗诊断领域，与推荐系统、搜索或广告没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14792v1": {
    "title": "CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection",
    "url": "https://www.alphaxiv.org/abs/2510.14792v1",
    "arxiv_id": "2510.14792v1",
    "authors": "Hojun Choi, Youngsun Lim, Jaeyo Shin, Hyunjung Shim",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 15:27:10",
    "ori_summary": "Open-vocabulary object detection (OVD) seeks to recognize and localize object categories beyond those seen during training. Recent approaches typically leverage vision-language models (VLMs) to generate pseudo-labels using image-text alignment, allowing detectors to generalize to unseen classes without explicit supervision. However, these methods depend heavily on direct image-text matching, neglecting the intermediate reasoning steps essential for interpreting semantically complex scenes. This results in limited robustness when confronted with crowded or occluded visual contexts. In this paper, we introduce CoT-PL, a new framework that employs structured visual chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL decomposes object understanding into three interpretable steps: (1) region perception even for unseen objects, (2) category recognition via zero-shot reasoning, and (3) background grounding to separate semantically complex objects. Crucially, the third step naturally motivates our contrastive background learning (CBL) that uses the pre-computed background cues as negatives to promote feature disentanglement between objects and background. In this way, CoT reasoning and CBL form an integrated pipeline tailored to robust pseudo-labeling in crowded or occluded scenes. Notably, in these two settings, our novel-class pseudo-label quality achieves relative improvements of 103.4% and 168.4% over the best prior, respectively. Our extensive experiments demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9 mask AP on LVIS for novel classes, setting a new state of the art.",
    "summary": "",
    "translation": "CoT-PL：视觉思维链推理与伪标签技术相结合用于开放词汇目标检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的开放词汇目标检测，结合视觉思维链和伪标签技术。虽然涉及多模态学习概念，但其核心应用场景是纯粹的视觉检测任务，与推荐系统、搜索或广告的排名和建模需求没有直接关联。视觉思维链技术主要针对视觉推理任务，难以迁移到RecSys/Search/Ads中的用户行为序列建模或异构特征融合场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14770v1": {
    "title": "MoCom: Motion-based Inter-MAV Visual Communication Using Event Vision and Spiking Neural Networks",
    "url": "https://www.alphaxiv.org/abs/2510.14770v1",
    "arxiv_id": "2510.14770v1",
    "authors": "Zhang Nengbo, Hann Woei Ho, Ye Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 15:06:51",
    "ori_summary": "Reliable communication in Micro Air Vehicle (MAV) swarms is challenging in environments, where conventional radio-based methods suffer from spectrum congestion, jamming, and high power consumption. Inspired by the waggle dance of honeybees, which efficiently communicate the location of food sources without sound or contact, we propose a novel visual communication framework for MAV swarms using motion-based signaling. In this framework, MAVs convey information, such as heading and distance, through deliberate flight patterns, which are passively captured by event cameras and interpreted using a predefined visual codebook of four motion primitives: vertical (up/down), horizontal (left/right), left-to-up-to-right, and left-to-down-to-right, representing control symbols (``start'', ``end'', ``1'', ``0''). To decode these signals, we design an event frame-based segmentation model and a lightweight Spiking Neural Network (SNN) for action recognition. An integrated decoding algorithm then combines segmentation and classification to robustly interpret MAV motion sequences. Experimental results validate the framework's effectiveness, which demonstrates accurate decoding and low power consumption, and highlights its potential as an energy-efficient alternative for MAV communication in constrained environments.",
    "summary": "",
    "translation": "MoCom：基于运动的多MAV视觉通信，使用事件视觉与脉冲神经网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于无人机之间的视觉通信技术，涉及事件视觉和脉冲神经网络，属于计算机视觉和机器人领域。这些技术与推荐系统、搜索或广告的核心焦点没有直接关联，也没有明显的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14765v1": {
    "title": "Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality",
    "url": "https://www.alphaxiv.org/abs/2510.14765v1",
    "arxiv_id": "2510.14765v1",
    "authors": "Giuseppe Lorenzo Catalano, Agata Marta Soccini",
    "categories": "cs.CV, cs.AI, cs.GR",
    "pub_date": "2025-10-16 15:02:05",
    "ori_summary": "Space exploration increasingly relies on Virtual Reality for several tasks, such as mission planning, multidisciplinary scientific analysis, and astronaut training. A key factor for the reliability of the simulations is having accurate 3D representations of planetary terrains. Extraterrestrial heightmaps derived from satellite imagery often contain missing values due to acquisition and transmission constraints. Mars is among the most studied planets beyond Earth, and its extensive terrain datasets make the Martian surface reconstruction a valuable task, although many areas remain unmapped. Deep learning algorithms can support void-filling tasks; however, whereas Earth's comprehensive datasets enables the use of conditional methods, such approaches cannot be applied to Mars. Current approaches rely on simpler interpolation techniques which, however, often fail to preserve geometric coherence. In this work, we propose a method for reconstructing the surface of Mars based on an unconditional diffusion model. Training was conducted on an augmented dataset of 12000 Martian heightmaps derived from NASA's HiRISE survey. A non-homogeneous rescaling strategy captures terrain features across multiple scales before resizing to a fixed 128x128 model resolution. We compared our method against established void-filling and inpainting techniques, including Inverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an evaluation set of 1000 samples. Results show that our approach consistently outperforms these methods in terms of reconstruction accuracy (4-15% on RMSE) and perceptual similarity (29-81% on LPIPS) with the original data.",
    "summary": "",
    "translation": "修复红色星球：用于虚拟现实中火星环境重建的扩散模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于使用扩散模型进行火星环境重建和虚拟现实应用，这属于计算机视觉和图形学领域。虽然扩散模型是LLM相关技术，但该应用与推荐系统、搜索或广告没有任何直接或间接的关联，完全超出了关注范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14753v1": {
    "title": "LightQANet: Quantized and Adaptive Feature Learning for Low-Light Image Enhancement",
    "url": "https://www.alphaxiv.org/abs/2510.14753v1",
    "arxiv_id": "2510.14753v1",
    "authors": "Xu Wu, Zhihui Lai, Xianxu Hou, Jie Zhou, Ya-nan Zhang, Linlin Shen",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 14:54:42",
    "ori_summary": "Low-light image enhancement (LLIE) aims to improve illumination while preserving high-quality color and texture. However, existing methods often fail to extract reliable feature representations due to severely degraded pixel-level information under low-light conditions, resulting in poor texture restoration, color inconsistency, and artifact. To address these challenges, we propose LightQANet, a novel framework that introduces quantized and adaptive feature learning for low-light enhancement, aiming to achieve consistent and robust image quality across diverse lighting conditions. From the static modeling perspective, we design a Light Quantization Module (LQM) to explicitly extract and quantify illumination-related factors from image features. By enforcing structured light factor learning, LQM enhances the extraction of light-invariant representations and mitigates feature inconsistency across varying illumination levels. From the dynamic adaptation perspective, we introduce a Light-Aware Prompt Module (LAPM), which encodes illumination priors into learnable prompts to dynamically guide the feature learning process. LAPM enables the model to flexibly adapt to complex and continuously changing lighting conditions, further improving image enhancement. Extensive experiments on multiple low-light datasets demonstrate that our method achieves state-of-the-art performance, delivering superior qualitative and quantitative results across various challenging lighting scenarios.",
    "summary": "",
    "translation": "LightQANet：用于低光照图像增强的量化与自适应特征学习",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的低光照图像增强技术，涉及量化方法和自适应特征学习，但缺乏与推荐系统、搜索或广告的明确关联。虽然图像增强在某些视觉应用中有用，但该论文没有展示在异构数据处理、Transformer架构改进或LLM技术方面的潜在应用，这些是当前关注的核心领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14741v1": {
    "title": "DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models",
    "url": "https://www.alphaxiv.org/abs/2510.14741v1",
    "arxiv_id": "2510.14741v1",
    "authors": "Simone Carnemolla, Matteo Pennisi, Sarinda Samarasinghe, Giovanni Bellitto, Simone Palazzo, Daniela Giordano, Mubarak Shah, Concetto Spampinato",
    "categories": "cs.CV, cs.AI, I.2.m",
    "pub_date": "2025-10-16 14:43:25",
    "ori_summary": "Understanding and explaining the behavior of machine learning models is essential for building transparent and trustworthy AI systems. We introduce DEXTER, a data-free framework that employs diffusion models and large language models to generate global, textual explanations of visual classifiers. DEXTER operates by optimizing text prompts to synthesize class-conditional images that strongly activate a target classifier. These synthetic samples are then used to elicit detailed natural language reports that describe class-specific decision patterns and biases. Unlike prior work, DEXTER enables natural language explanation about a classifier's decision process without access to training data or ground-truth labels. We demonstrate DEXTER's flexibility across three tasks-activation maximization, slice discovery and debiasing, and bias explanation-each illustrating its ability to uncover the internal mechanisms of visual classifiers. Quantitative and qualitative evaluations, including a user study, show that DEXTER produces accurate, interpretable outputs. Experiments on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms existing approaches in global model explanation and class-level bias reporting. Code is available at https://github.com/perceivelab/dexter.",
    "summary": "",
    "translation": "DEXTER：面向视觉模型的扩散引导式解释与文本推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉模型的解释性方法，结合扩散模型和文本推理技术。虽然解释性技术在某些推荐场景中可能有辅助作用，但该工作本质上是计算机视觉领域的可解释性研究，与推荐系统、搜索或广告的核心技术方向关联度较低，且未明确展示在推荐/搜索/广告领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14737v1": {
    "title": "Free-Grained Hierarchical Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.14737v1",
    "arxiv_id": "2510.14737v1",
    "authors": "Seulki Park, Zilin Wang, Stella X. Yu",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 14:35:18",
    "ori_summary": "Hierarchical image classification predicts labels across a semantic taxonomy, but existing methods typically assume complete, fine-grained annotations, an assumption rarely met in practice. Real-world supervision varies in granularity, influenced by image quality, annotator expertise, and task demands; a distant bird may be labeled Bird, while a close-up reveals Bald eagle. We introduce ImageNet-F, a large-scale benchmark curated from ImageNet and structured into cognitively inspired basic, subordinate, and fine-grained levels. Using CLIP as a proxy for semantic ambiguity, we simulate realistic, mixed-granularity labels reflecting human annotation behavior. We propose free-grain learning, with heterogeneous supervision across instances. We develop methods that enhance semantic guidance via pseudo-attributes from vision-language models and visual guidance via semi-supervised learning. These, along with strong baselines, substantially improve performance under mixed supervision. Together, our benchmark and methods advance hierarchical classification under real-world constraints.",
    "summary": "",
    "translation": "细粒度层次化识别",
    "relevance_score": 2,
    "reasoning": "该标题涉及细粒度和层次化识别概念，可能在计算机视觉领域有应用，但与推荐系统、搜索或广告的核心技术进展缺乏直接关联。虽然细粒度识别在商品识别或内容理解中有潜在用途，但标题本身过于宽泛，无法确定其与Transformer架构、LLM技术或推荐系统具体应用的明确联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14726v1": {
    "title": "Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection",
    "url": "https://www.alphaxiv.org/abs/2510.14726v1",
    "arxiv_id": "2510.14726v1",
    "authors": "Dingzhou Xie, Rushi Lan, Cheng Pang, Enhao Ning, Jiahao Zeng, Wei Zheng",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 14:25:21",
    "ori_summary": "Recent object detection methods have made remarkable progress by leveraging attention mechanisms to improve feature discriminability. However, most existing approaches are confined to refining single-layer or fusing dual-layer features, overlooking the rich inter-layer dependencies across multi-scale representations. This limits their ability to capture comprehensive contextual information essential for detecting objects with large scale variations. In this paper, we propose a novel Cross-Layer Feature Self-Attention Module (CFSAM), which holistically models both local and global dependencies within multi-scale feature maps. CFSAM consists of three key components: a convolutional local feature extractor, a Transformer-based global modeling unit that efficiently captures cross-layer interactions, and a feature fusion mechanism to restore and enhance the original representations. When integrated into the SSD300 framework, CFSAM significantly boosts detection performance, achieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO (vs. 43.1% baseline), outperforming existing attention modules. Moreover, the module accelerates convergence during training without introducing substantial computational overhead. Our work highlights the importance of explicit cross-layer attention modeling in advancing multi-scale object detection.",
    "summary": "",
    "translation": "用于多尺度目标检测的跨层特征自注意力模块",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的目标检测任务，虽然涉及注意力机制，但其应用场景和核心问题与推荐系统、搜索或广告领域相距甚远。论文中的跨层特征自注意力模块可能对多模态建模有一定启发，但这种关联性非常间接且微弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14713v1": {
    "title": "Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models",
    "url": "https://www.alphaxiv.org/abs/2510.14713v1",
    "arxiv_id": "2510.14713v1",
    "authors": "Tingyu Lin, Armin Dadras, Florian Kleber, Robert Sablatnig",
    "categories": "cs.CV, cs.AI, eess.IV",
    "pub_date": "2025-10-16 14:11:52",
    "ori_summary": "Camera movement conveys spatial and narrative information essential for understanding video content. While recent camera movement classification (CMC) methods perform well on modern datasets, their generalization to historical footage remains unexplored. This paper presents the first systematic evaluation of deep video CMC models on archival film material. We summarize representative methods and datasets, highlighting differences in model design and label definitions. Five standard video classification models are assessed on the HISTORIAN dataset, which includes expert-annotated World War II footage. The best-performing model, Video Swin Transformer, achieves 80.25% accuracy, showing strong convergence despite limited training data. Our findings highlight the challenges and potential of adapting existing models to low-quality video and motivate future work combining diverse input modalities and temporal architectures.",
    "summary": "",
    "translation": "历史影像中摄像机运动分类：深度视频模型的比较研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于历史影像中的摄像机运动分类，属于纯粹的计算机视觉应用领域。论文内容涉及视频分析和深度视频模型，与推荐系统、搜索或广告的核心技术发展没有直接关联，也不涉及LLM技术、Transformer架构进展或异构数据统一建模等关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14709v1": {
    "title": "Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery",
    "url": "https://www.alphaxiv.org/abs/2510.14709v1",
    "arxiv_id": "2510.14709v1",
    "authors": "Caleb Robinson, Kimberly T. Goetz, Christin B. Khan, Meredith Sackett, Kathleen Leonard, Rahul Dodhia, Juan M. Lavista Ferres",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 14:10:51",
    "ori_summary": "Effective monitoring of whale populations is critical for conservation, but traditional survey methods are expensive and difficult to scale. While prior work has shown that whales can be identified in very high-resolution (VHR) satellite imagery, large-scale automated detection remains challenging due to a lack of annotated imagery, variability in image quality and environmental conditions, and the cost of building robust machine learning pipelines over massive remote sensing archives. We present a semi-automated approach for surfacing possible whale detections in VHR imagery using a statistical anomaly detection method that flags spatial outliers, i.e. \"interesting points\". We pair this detector with a web-based labeling interface designed to enable experts to quickly annotate the interesting points. We evaluate our system on three benchmark scenes with known whale annotations and achieve recalls of 90.3% to 96.4%, while reducing the area requiring expert inspection by up to 99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method does not rely on labeled training data and offers a scalable first step toward future machine-assisted marine mammal monitoring from space. We have open sourced this pipeline at https://github.com/microsoft/whales.",
    "summary": "",
    "translation": "鲸在何处：一种人机协同检测方法用于高分辨率卫星图像中的鲸鱼识别",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的目标检测任务，应用于卫星图像分析领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然检测技术本身是基础技术，但论文的应用场景（鲸鱼检测）和领域（遥感图像分析）与我的关注焦点相距甚远，没有明显的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14705v1": {
    "title": "Leveraging Learned Image Prior for 3D Gaussian Compression",
    "url": "https://www.alphaxiv.org/abs/2510.14705v1",
    "arxiv_id": "2510.14705v1",
    "authors": "Seungjoo Shin, Jaesik Park, Sunghyun Cho",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 14:10:02",
    "ori_summary": "Compression techniques for 3D Gaussian Splatting (3DGS) have recently achieved considerable success in minimizing storage overhead for 3D Gaussians while preserving high rendering quality. Despite the impressive storage reduction, the lack of learned priors restricts further advances in the rate-distortion trade-off for 3DGS compression tasks. To address this, we introduce a novel 3DGS compression framework that leverages the powerful representational capacity of learned image priors to recover compression-induced quality degradation. Built upon initially compressed Gaussians, our restoration network effectively models the compression artifacts in the image space between degraded and original Gaussians. To enhance the rate-distortion performance, we provide coarse rendering residuals into the restoration network as side information. By leveraging the supervision of restored images, the compressed Gaussians are refined, resulting in a highly compact representation with enhanced rendering performance. Our framework is designed to be compatible with existing Gaussian compression methods, making it broadly applicable across different baselines. Extensive experiments validate the effectiveness of our framework, demonstrating superior rate-distortion performance and outperforming the rendering quality of state-of-the-art 3DGS compression methods while requiring substantially less storage.",
    "summary": "",
    "translation": "利用学习到的图像先验进行3D高斯压缩",
    "relevance_score": 2,
    "reasoning": "该论文主要关注3D高斯压缩和图像先验学习，属于计算机视觉和图形学领域。虽然压缩技术可能有间接应用，但论文没有明确涉及推荐系统、搜索或广告的核心技术。其视觉处理焦点与当前关注的LLM技术、推荐系统架构或异构数据建模缺乏直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14672v1": {
    "title": "VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.14672v1",
    "arxiv_id": "2510.14672v1",
    "authors": "Jinglei Zhang, Yuanfan Guo, Rolandos Alexandros Potamias, Jiankang Deng, Hang Xu, Chao Ma",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 13:29:02",
    "ori_summary": "In recent years, video question answering based on multimodal large language models (MLLM) has garnered considerable attention, due to the benefits from the substantial advancements in LLMs. However, these models have a notable deficiency in the domains of video temporal grounding and reasoning, posing challenges to the development of effective real-world video understanding systems. Inspired by how humans use video players to interact with the progress bar for video comprehension, we introduce VTimeCoT, a simple yet effective training-free framework, designed for high-performance video grounding and reasoning. The proposed framework incorporates two novel visual tools of the progress bar: a plug-and-play progress bar integration tool and a high-efficiency highlighting tool. In addition, to address the limitations of conventional text-based chain-of-thought (CoT) approaches, we introduce a visuotemporal CoT process that integrates cross-modality reasoning across both video and text. Our approach demonstrates significant performance improvements on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and reasoning-based question answering. Finally, we showcase that the proposed framework achieves a compositional and interpretable reasoning process. Project page: https://vtimecot.github.io",
    "summary": "",
    "translation": "VTimeCoT：通过绘图思考实现视频时序定位与推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频时序定位和推理任务，属于计算机视觉领域。虽然提到了思维链（CoT）推理方法，但其核心应用场景是视频理解，与推荐系统、搜索或广告的关联性较弱。视频理解技术可能间接应用于内容推荐，但缺乏明确的直接应用路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14668v1": {
    "title": "WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging",
    "url": "https://www.alphaxiv.org/abs/2510.14668v1",
    "arxiv_id": "2510.14668v1",
    "authors": "Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Sami Azam, Asif Karim, Jemima Beissbarth, Amanda Leach",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 13:22:51",
    "ori_summary": "Knowledge distillation (KD) has traditionally relied on a static teacher-student framework, where a large, well-trained teacher transfers knowledge to a single student model. However, these approaches often suffer from knowledge degradation, inefficient supervision, and reliance on either a very strong teacher model or large labeled datasets, which limits their effectiveness in real-world, limited-data scenarios. To address these, we present the first-ever Weakly-supervised Chain-based KD network (WeCKD) that redefines knowledge transfer through a structured sequence of interconnected models. Unlike conventional KD, it forms a progressive distillation chain, where each model not only learns from its predecessor but also refines the knowledge before passing it forward. This structured knowledge transfer further enhances feature learning, reduces data dependency, and mitigates the limitations of one-step KD. Each model in the distillation chain is trained on only a fraction of the dataset and demonstrates that effective learning can be achieved with minimal supervision. Extensive evaluations across four otoscopic imaging datasets demonstrate that it not only matches but in many cases surpasses the performance of existing supervised methods. Experimental results on two other datasets further underscore its generalization across diverse medical imaging modalities, including microscopic and magnetic resonance imaging. Furthermore, our evaluations resulted in cumulative accuracy gains of up to +23% over a single backbone trained on the same limited data, which highlights its potential for real-world adoption.",
    "summary": "",
    "translation": "WeCKD：用于高效多模态医学成像的弱监督链式蒸馏网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于多模态医学成像领域，这属于明确的无关主题范畴。虽然提到了多模态和蒸馏技术，但其医学应用背景使其与推荐系统、搜索或广告领域完全无关，没有任何潜在的应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14661v1": {
    "title": "EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)",
    "url": "https://www.alphaxiv.org/abs/2510.14661v1",
    "arxiv_id": "2510.14661v1",
    "authors": "Weikang Yu, Vincent Nwazelibe, Xianping Ma, Xiaokang Zhang, Richard Gloaguen, Xiao Xiang Zhu, Pedram Ghamisi",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 13:15:53",
    "ori_summary": "Mining activities are essential for industrial and economic development, but remain a leading source of environmental degradation, contributing to deforestation, soil erosion, and water contamination. Sustainable resource management and environmental governance require consistent, long-term monitoring of mining-induced land surface changes, yet existing datasets are often limited in temporal depth or geographic scope. To address this gap, we present EuroMineNet, the first comprehensive multitemporal benchmark for mining footprint mapping and monitoring based on Sentinel-2 multispectral imagery. Spanning 133 mining sites across the European Union, EuroMineNet provides annual observations and expert-verified annotations from 2015 to 2024, enabling GeoAI-based models to analyze environmental dynamics at a continental scale. It supports two sustainability-driven tasks: (1) multitemporal mining footprint mapping for consistent annual land-use delineation, evaluated with a novel Change-Aware Temporal IoU (CA-TIoU) metric, and (2) cross-temporal change detection to capture both gradual and abrupt surface transformations. Benchmarking 20 state-of-the-art deep learning models reveals that while GeoAI methods effectively identify long-term environmental changes, challenges remain in detecting short-term dynamics critical for timely mitigation. By advancing temporally consistent and explainable mining monitoring, EuroMineNet contributes to sustainable land-use management, environmental resilience, and the broader goal of applying GeoAI for social and environmental good. We release the codes and datasets by aligning with FAIR and the open science paradigm at https://github.com/EricYu97/EuroMineNet.",
    "summary": "",
    "translation": "EuroMineNet：一个用于欧盟时空采矿足迹分析的多时相哨兵2号基准数据集（2015-2024）",
    "relevance_score": 1,
    "reasoning": "该论文专注于遥感图像分析和地理空间数据挖掘，属于纯粹的计算机视觉应用领域。虽然涉及时空数据分析，但其核心关注点是环境监测和采矿足迹检测，与推荐系统、搜索或广告领域没有任何直接或间接的技术关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14657v1": {
    "title": "Decorrelation Speeds Up Vision Transformers",
    "url": "https://www.alphaxiv.org/abs/2510.14657v1",
    "arxiv_id": "2510.14657v1",
    "authors": "Kieran Carrigg, Rob van Gastel, Melda Yeghaian, Sander Dalm, Faysal Boughorbel, Marcel van Gerven",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-16 13:13:12",
    "ori_summary": "Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields strong performance in low-label regimes but comes with substantial computational costs, making it impractical in time- and resource-constrained industrial settings. We address this by integrating Decorrelated Backpropagation (DBP) into MAE pre-training, an optimization method that iteratively reduces input correlations at each layer to accelerate convergence. Applied selectively to the encoder, DBP achieves faster pre-training without loss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE reduces wall-clock time to baseline performance by 21.1%, lowers carbon emissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe similar gains when pre-training and fine-tuning on proprietary industrial data, confirming the method's applicability in real-world scenarios. These results demonstrate that DBP can reduce training time and energy use while improving downstream performance for large-scale ViT pre-training.",
    "summary": "",
    "translation": "去相关加速视觉Transformer",
    "relevance_score": 6,
    "reasoning": "该论文属于Transformer架构效率提升的范畴，这是'Enabling Transformer Tech'的重点关注领域。虽然论文聚焦于视觉Transformer，但去相关技术可以应用于推荐系统和搜索中的Transformer模型，通过减少特征冗余来加速推理，这对于大规模在线服务至关重要。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14648v1": {
    "title": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing",
    "url": "https://www.alphaxiv.org/abs/2510.14648v1",
    "arxiv_id": "2510.14648v1",
    "authors": "Xinyao Liao, Xianfang Zeng, Ziye Song, Zhoujie Fu, Gang Yu, Guosheng Lin",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 13:02:11",
    "ori_summary": "Despite the rapid progress of instruction-based image editing, its extension to video remains underexplored, primarily due to the prohibitive cost and complexity of constructing large-scale paired video editing datasets. To address this challenge, we introduce a low-cost pretraining strategy for instruction-based video editing that leverages in-context learning from unpaired video clips. We show that pretraining a foundation video generation model with this strategy endows it with general editing capabilities, such as adding, replacing, or deleting operations, according to input editing instructions. The pretrained model can then be efficiently refined with a small amount of high-quality paired editing data. Built upon HunyuanVideoT2V, our framework first pretrains on approximately 1M real video clips to learn basic editing concepts, and subsequently fine-tunes on fewer than 150k curated editing pairs to extend more editing tasks and improve the editing quality. Comparative experiments show that our method surpasses existing instruction-based video editing approaches in both instruction alignment and visual fidelity, achieving a 12\\% improvement in editing instruction following and a 15\\% improvement in editing quality.",
    "summary": "",
    "translation": "基于指令的视频编辑中未配对片段的情境学习",
    "relevance_score": 2,
    "reasoning": "该论文专注于视频编辑中的情境学习，这属于计算机视觉和内容生成领域。虽然情境学习是LLM的重要能力，但该论文的应用场景（视频编辑）与推荐系统、搜索或广告的核心排名和检索任务没有直接关联。该技术可能对内容生成有帮助，但这超出了当前关注的直接应用范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14634v1": {
    "title": "SteeringTTA: Guiding Diffusion Trajectories for Robust Test-Time-Adaptation",
    "url": "https://www.alphaxiv.org/abs/2510.14634v1",
    "arxiv_id": "2510.14634v1",
    "authors": "Jihyun Yu, Yoojin Oh, Wonho Bae, Mingyu Kim, Junhyug Noh",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 12:46:53",
    "ori_summary": "Test-time adaptation (TTA) aims to correct performance degradation of deep models under distribution shifts by updating models or inputs using unlabeled test data. Input-only diffusion-based TTA methods improve robustness for classification to corruptions but rely on gradient guidance, limiting exploration and generalization across distortion types. We propose SteeringTTA, an inference-only framework that adapts Feynman-Kac steering to guide diffusion-based input adaptation for classification with rewards driven by pseudo-label. SteeringTTA maintains multiple particle trajectories, steered by a combination of cumulative top-K probabilities and an entropy schedule, to balance exploration and confidence. On ImageNet-C, SteeringTTA consistently outperforms the baseline without any model updates or source data.",
    "summary": "",
    "translation": "SteeringTTA：引导扩散轨迹以实现鲁棒的测试时自适应",
    "relevance_score": 2,
    "reasoning": "该论文主要关注扩散模型的测试时自适应技术，属于计算机视觉领域的特定优化方法。虽然扩散模型在某些推荐系统中可能用于内容生成，但论文标题明确聚焦于测试时自适应，这与推荐系统、搜索或广告的核心排序和建模问题关联度较低，且未明确涉及Transformer架构或LLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14630v1": {
    "title": "Adapting Self-Supervised Representations as a Latent Space for Efficient Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14630v1",
    "arxiv_id": "2510.14630v1",
    "authors": "Ming Gui, Johannes Schusterbauer, Timy Phan, Felix Krause, Josh Susskind, Miguel Angel Bautista, Björn Ommer",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 12:43:03",
    "ori_summary": "We introduce Representation Tokenizer (RepTok), a generative modeling framework that represents an image using a single continuous latent token obtained from self-supervised vision transformers. Building on a pre-trained SSL encoder, we fine-tune only the semantic token embedding and pair it with a generative decoder trained jointly using a standard flow matching objective. This adaptation enriches the token with low-level, reconstruction-relevant details, enabling faithful image reconstruction. To preserve the favorable geometry of the original SSL space, we add a cosine-similarity loss that regularizes the adapted token, ensuring the latent space remains smooth and suitable for generation. Our single-token formulation resolves spatial redundancies of 2D latent spaces and significantly reduces training costs. Despite its simplicity and efficiency, RepTok achieves competitive results on class-conditional ImageNet generation and naturally extends to text-to-image synthesis, reaching competitive zero-shot performance on MS-COCO under extremely limited training budgets. Our findings highlight the potential of fine-tuned SSL representations as compact and effective latent spaces for efficient generative modeling.",
    "summary": "",
    "translation": "将自监督表示作为高效生成的潜在空间进行适配",
    "relevance_score": 6,
    "reasoning": "该论文涉及自监督表示学习，这是LLM和推荐系统的核心使能技术。将自监督表示作为潜在空间进行高效生成，可直接应用于推荐系统的用户表示学习、内容特征提取以及高效的个性化生成任务，在搜索和广告的表示学习优化中也有重要价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14627v1": {
    "title": "GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement",
    "url": "https://www.alphaxiv.org/abs/2510.14627v1",
    "arxiv_id": "2510.14627v1",
    "authors": "Yao Zhong, Hanzhi Chen, Simon Schaefer, Anran Zhang, Stefan Leutenegger",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-16 12:38:14",
    "ori_summary": "Robots are expected to serve as intelligent assistants, helping humans with everyday household organization. A central challenge in this setting is the task of object placement, which requires reasoning about both semantic preferences (e.g., common-sense object relations) and geometric feasibility (e.g., collision avoidance). We present GOPLA, a hierarchical framework that learns generalizable object placement from augmented human demonstrations. A multi-modal large language model translates human instructions and visual inputs into structured plans that specify pairwise object relationships. These plans are then converted into 3D affordance maps with geometric common sense by a spatial mapper, while a diffusion-based planner generates placement poses guided by test-time costs, considering multi-plan distributions and collision avoidance. To overcome data scarcity, we introduce a scalable pipeline that expands human placement demonstrations into diverse synthetic training data. Extensive experiments show that our approach improves placement success rates by 30.04 percentage points over the runner-up, evaluated on positioning accuracy and physical plausibility, demonstrating strong generalization across a wide range of real-world robotic placement scenarios.",
    "summary": "",
    "translation": "GOPLA：通过人类排列的合成增强实现可泛化的物体放置学习",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的物体放置问题，属于视觉合成和场景生成领域。虽然标题提到'人类排列'，但这更偏向于视觉内容生成和布局优化，与推荐系统、搜索或广告中的核心排名、用户建模或Transformer架构改进没有直接关联。即使考虑VLM类比，该工作主要处理视觉模态内的物体布局，而非处理推荐/搜索中典型的异构数据模态统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14624v1": {
    "title": "Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference",
    "url": "https://www.alphaxiv.org/abs/2510.14624v1",
    "arxiv_id": "2510.14624v1",
    "authors": "Natan Bagrov, Eugene Khvedchenia, Borys Tymchenko, Shay Aharon, Lior Kadoch, Tomer Keren, Ofri Masad, Yonatan Geifman, Ran Zilberstein, Tuomas Rintamaki, Matthieu Le, Andrew Tao",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 12:34:38",
    "ori_summary": "Vision-language models (VLMs) have recently expanded from static image understanding to video reasoning, but their scalability is fundamentally limited by the quadratic cost of processing dense frame sequences. Long videos often exceed the token budget of modern language models, leading to severe context limitations and latency issues. We introduce Efficient Video Sampling (EVS), a simple, plug-and-play method for reducing token redundancy in videos by identifying and pruning temporally static patches -- spatial regions that remain unchanged across consecutive frames. EVS preserves positional identity, requires no architectural changes or retraining. We show that EVS substantially reduces token count while maintaining semantic fidelity, enabling faster inference and longer input sequences. Applied at inference time, EVS reduces large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal accuracy loss. When combined with an uptraining phase using stochastic pruning rates, EVS yields models that are robust to varying compression levels and retain full performance under aggressive pruning. Extensive experiments demonstrate that EVS consistently improves efficiency-accuracy trade-offs, unlocking scalable video-language understanding without sacrificing quality.",
    "summary": "",
    "translation": "高效视频采样：通过剪枝时间冗余令牌加速视觉语言模型推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型中的视频处理效率优化，属于纯粹的视觉效率技术。虽然提到了推理加速，但其应用场景主要针对视频内容理解，与推荐系统、搜索或广告中的核心排名和建模问题缺乏直接关联。该技术可能对视频推荐有一定间接价值，但应用潜力有限且非核心关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14617v1": {
    "title": "Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.14617v1",
    "arxiv_id": "2510.14617v1",
    "authors": "Ning Ding, Keisuke Fujii, Toru Tamaki",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 12:24:51",
    "ori_summary": "Tactical understanding in badminton involves interpreting not only individual actions but also how tactics are dynamically executed over time. In this paper, we propose \\textbf{Shot2Tactic-Caption}, a novel framework for semantic and temporal multi-scale video captioning in badminton, capable of generating shot-level captions that describe individual actions and tactic-level captions that capture how these actions unfold over time within a tactical execution. We also introduce the Shot2Tactic-Caption Dataset, the first badminton captioning dataset containing 5,494 shot captions and 544 tactic captions. Shot2Tactic-Caption adopts a dual-branch design, with both branches including a visual encoder, a spatio-temporal Transformer encoder, and a Transformer-based decoder to generate shot and tactic captions. To support tactic captioning, we additionally introduce a Tactic Unit Detector that identifies valid tactic units, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic captioning, we further incorporate a shot-wise prompt-guided mechanism, where the predicted tactic type and state are embedded as prompts and injected into the decoder via cross-attention. The shot-wise prompt-guided mechanism enables our system not only to describe successfully executed tactics but also to capture tactical executions that are temporarily interrupted and later resumed. Experimental results demonstrate the effectiveness of our framework in generating both shot and tactic captions. Ablation studies show that the ResNet50-based spatio-temporal encoder outperforms other variants, and that shot-wise prompt structuring leads to more coherent and accurate tactic captioning.",
    "summary": "",
    "translation": "Shot2Tactic-Caption：面向战术理解的多尺度羽毛球视频描述生成",
    "relevance_score": 2,
    "reasoning": "该论文专注于体育视频分析和战术理解，属于计算机视觉领域的特定应用。虽然涉及多模态理解和序列建模，但其应用场景（羽毛球视频）与推荐系统、搜索或广告领域缺乏直接关联。论文中的多尺度分析方法可能对处理用户行为序列有启发，但这种关联性较弱且间接。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14605v1": {
    "title": "Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering",
    "url": "https://www.alphaxiv.org/abs/2510.14605v1",
    "arxiv_id": "2510.14605v1",
    "authors": "Yuyang Hong, Jiaqi Gu, Qi Yang, Lubin Fan, Yue Wu, Ying Wang, Kun Ding, Shiming Xiang, Jieping Ye",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 12:10:00",
    "ori_summary": "Knowledge-based visual question answering (KB-VQA) requires visual language models (VLMs) to integrate visual understanding with external knowledge retrieval. Although retrieval-augmented generation (RAG) achieves significant advances in this task by combining knowledge-base querying, it still struggles with the quality of multimodal queries and the relevance of retrieved results. To overcome these challenges, we propose a novel three-stage method, termed Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing stage dynamically invokes visual tools to extract precise multimodal information for retrieval. The retrieval stage integrates visual and text features to achieve multimodal knowledge retrieval. The filtering stage performs relevance filtering and concentration on retrieval results. To this end, we introduce a visual language model trained with answer accuracy and format consistency as reward signals via a reinforcement learning manner. This enhances the model's reasoning, tool invocation for accurate queries, and filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality, achieving state-of-the-art performance. Code is available at https://github.com/cqu-student/Wiki-PRF",
    "summary": "",
    "translation": "基于知识的视觉问答与多模态处理、检索和过滤",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉问答任务，属于纯粹的视觉-语言多模态研究，与推荐系统、搜索或广告的核心技术领域没有直接关联。虽然提到了检索和过滤技术，但这些是服务于视觉问答的特定应用，缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14596v1": {
    "title": "Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering",
    "url": "https://www.alphaxiv.org/abs/2510.14596v1",
    "arxiv_id": "2510.14596v1",
    "authors": "Hugo Markoff, Jevgenijs Galaktionovs",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 11:59:18",
    "ori_summary": "Camera traps generate millions of wildlife images, yet many datasets contain species that are absent from existing classifiers. This work evaluates zero-shot approaches for organizing unlabeled wildlife imagery using self-supervised vision transformers, developed and tested within the Animal Detect platform for camera trap analysis. We compare unsupervised clustering methods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor) combined with dimensionality reduction techniques (PCA, UMAP), and we demonstrate continuous 1D similarity ordering via t-SNE projection. On a 5-species test set with ground truth labels used only for evaluation, DINOv2 with UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D sorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent for fish across 1,500 images. Based on these findings, we deployed continuous similarity ordering in production, enabling rapid exploratory analysis and accelerating manual annotation workflows for biodiversity monitoring.",
    "summary": "",
    "translation": "使用视觉Transformer进行零样本野生动物分类：评估聚类与连续相似性排序",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉在野生动物分类领域的应用，使用Vision Transformer进行零样本学习和相似性排序。虽然涉及Transformer架构，但其应用场景（野生动物分类）与推荐系统、搜索或广告领域没有直接关联，且没有明显的技术迁移潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14594v1": {
    "title": "Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers",
    "url": "https://www.alphaxiv.org/abs/2510.14594v1",
    "arxiv_id": "2510.14594v1",
    "authors": "Hugo Markoff, Jevgenijs Galaktionovs",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 11:57:07",
    "ori_summary": "State-of-the-art animal classification models like SpeciesNet provide predictions across thousands of species but use conservative rollup strategies, resulting in many animals labeled at high taxonomic levels rather than species. We present a hierarchical re-classification system for the Animal Detect platform that combines SpeciesNet EfficientNetV2-M predictions with CLIP embeddings and metric learning to refine high-level taxonomic labels toward species-level identification. Our five-stage pipeline (high-confidence acceptance, bird override, centroid building, triplet-loss metric learning, and adaptive cosine-distance scoring) is evaluated on a segment of the LILA BC Desert Lion Conservation dataset (4,018 images, 15,031 detections). After recovering 761 bird detections from \"blank\" and \"animal\" labels, we re-classify 456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving species-level identification for 64.9 percent",
    "summary": "",
    "translation": "分层重分类：将动物分类模型与视觉变换器相结合",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的动物分类任务，使用视觉变换器进行图像识别。虽然涉及变换器架构，但这是纯粹的视觉应用，没有展示与推荐系统、搜索或广告领域的潜在关联。该工作属于纯粹的视觉研究范畴，与我的关注焦点无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14588v1": {
    "title": "STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding",
    "url": "https://www.alphaxiv.org/abs/2510.14588v1",
    "arxiv_id": "2510.14588v1",
    "authors": "Zhifei Chen, Tianshuo Xu, Leyi Wu, Luozhou Wang, Dongyu Yan, Zihan You, Wenting Luo, Guo Zhang, Yingcong Chen",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 11:50:38",
    "ori_summary": "Video generation has recently made striking visual progress, but maintaining coherent object motion and interactions remains difficult. We trace two practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps) often collapse to too few effective tokens after encoding, weakening guidance; and (ii) optimizing for appearance and motion in a single head can favor texture over temporal consistency. We present STANCE, an image-to-video framework that addresses both issues with two simple components. First, we introduce Instance Cues -- a pixel-aligned control signal that turns sparse, user-editable hints into a dense 2.5D (camera-relative) motion field by averaging per-instance flow and augmenting with monocular depth over the instance mask. This reduces depth ambiguity compared to 2D arrow inputs while remaining easy to use. Second, we preserve the salience of these cues in token space with Dense RoPE, which tags a small set of motion tokens (anchored on the first frame) with spatial-addressable rotary embeddings. Paired with joint RGB \\(+\\) auxiliary-map prediction (segmentation or depth), our model anchors structure while RGB handles appearance, stabilizing optimization and improving temporal coherence without requiring per-frame trajectory scripts.",
    "summary": "",
    "translation": "STANCE：通过稀疏到稠密锚定编码实现运动一致性的视频生成",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于视频生成技术，属于计算机视觉领域的特定应用，与推荐系统、搜索或广告的核心技术没有直接关联。虽然提到了编码机制，但这是针对视频运动一致性的专门技术，在推荐、搜索或广告领域没有明显的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14576v1": {
    "title": "CALM-Net: Curvature-Aware LiDAR Point Cloud-based Multi-Branch Neural Network for Vehicle Re-Identification",
    "url": "https://www.alphaxiv.org/abs/2510.14576v1",
    "arxiv_id": "2510.14576v1",
    "authors": "Dongwook Lee, Sol Han, Jinwhan Kim",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 11:36:54",
    "ori_summary": "This paper presents CALM-Net, a curvature-aware LiDAR point cloud-based multi-branch neural network for vehicle re-identification. The proposed model addresses the challenge of learning discriminative and complementary features from three-dimensional point clouds to distinguish between vehicles. CALM-Net employs a multi-branch architecture that integrates edge convolution, point attention, and a curvature embedding that characterizes local surface variation in point clouds. By combining these mechanisms, the model learns richer geometric and contextual features that are well suited for the re-identification task. Experimental evaluation on the large-scale nuScenes dataset demonstrates that CALM-Net achieves a mean re-identification accuracy improvement of approximately 1.97\\% points compared with the strongest baseline in our study. The results confirms the effectiveness of incorporating curvature information into deep learning architectures and highlight the benefit of multi-branch feature learning for LiDAR point cloud-based vehicle re-identification.",
    "summary": "",
    "translation": "CALM-Net：基于曲率感知LiDAR点云的多分支神经网络用于车辆重识别",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的车辆重识别任务，使用LiDAR点云数据，与推荐系统、搜索或广告的核心领域无关。虽然涉及神经网络架构，但属于纯粹的视觉应用领域，没有展示出在RecSys/Search/Ads中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14564v1": {
    "title": "BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU",
    "url": "https://www.alphaxiv.org/abs/2510.14564v1",
    "arxiv_id": "2510.14564v1",
    "authors": "Junyi Wu, Jiaming Xu, Jinhao Li, Yongkang Zhou, Jiayi Pan, Xingyang Li, Guohao Dai",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 11:16:58",
    "ori_summary": "3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction technique. The traditional 3DGS training pipeline follows three sequential steps: Gaussian densification, Gaussian projection, and color splatting. Despite its promising reconstruction quality, this conventional approach suffers from three critical inefficiencies: (1) Skewed density allocation during Gaussian densification, (2) Imbalanced computation workload during Gaussian projection and (3) Fragmented memory access during color splatting. To tackle the above challenges, we introduce BalanceGS, the algorithm-system co-design for efficient training in 3DGS. (1) At the algorithm level, we propose heuristic workload-sensitive Gaussian density control to automatically balance point distributions - removing 80% redundant Gaussians in dense regions while filling gaps in sparse areas. (2) At the system level, we propose Similarity-based Gaussian sampling and merging, which replaces the static one-to-one thread-pixel mapping with adaptive workload distribution - threads now dynamically process variable numbers of Gaussians based on local cluster density. (3) At the mapping level, we propose reordering-based memory access mapping strategy that restructures RGB storage and enables batch loading in shared memory. Extensive experiments demonstrate that compared with 3DGS, our approach achieves a 1.44$\\times$ training speedup on a NVIDIA A100 GPU with negligible quality degradation.",
    "summary": "",
    "translation": "BalanceGS：面向GPU高效3D高斯泼溅训练的算法-系统协同设计",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于3D计算机图形学中的高斯泼溅训练优化，属于纯粹的3D视觉技术。虽然提到了GPU效率优化，但该技术主要应用于3D场景重建和渲染，与推荐系统、搜索或广告的核心领域没有直接关联。论文内容不涉及任何推荐、搜索排序、广告排名或相关的多模态建模应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14560v1": {
    "title": "Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video",
    "url": "https://www.alphaxiv.org/abs/2510.14560v1",
    "arxiv_id": "2510.14560v1",
    "authors": "Yulin Zhang, Cheng Shi, Yang Wang, Sibei Yang",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 11:11:13",
    "ori_summary": "Envision an AI capable of functioning in human-like settings, moving beyond mere observation to actively understand, anticipate, and proactively respond to unfolding events. Towards this vision, we focus on the innovative task where, given ego-streaming video input, an assistant proactively answers diverse, evolving questions at the opportune moment, while maintaining synchronized perception and reasoning. This task embodies three key properties: (1) Proactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized Efficiency. To evaluate and address these properties, we first introduce ESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a novel framework designed for their rigorous assessment. Secondly, we propose a comprehensive technical pipeline to enable models to tackle this challenging task. This pipeline comprises: (1) a data engine, (2) a multi-stage training strategy, and (3) a proactive dynamic compression technique. Our proposed model effectively addresses these critical properties while outperforming multiple baselines across diverse online and offline benchmarks. Project Page:https://zhangyl4.github.io/publications/eyes-wide-open/",
    "summary": "",
    "translation": "睁大眼睛：面向流媒体视频的自我主动视频大语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频流媒体场景下的主动式视频LLM，属于视频语言模型领域。虽然提到了LLM技术，但其核心应用场景是视频流媒体理解，与推荐系统、搜索或广告的关联性较弱。论文没有明确展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14553v1": {
    "title": "Consistent text-to-image generation via scene de-contextualization",
    "url": "https://www.alphaxiv.org/abs/2510.14553v1",
    "arxiv_id": "2510.14553v1",
    "authors": "Song Tang, Peihao Gong, Kunyu Li, Kai Guo, Boyu Wang, Mao Ye, Jianwei Zhang, Xiatian Zhu",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 10:54:49",
    "ori_summary": "Consistent text-to-image (T2I) generation seeks to produce identity-preserving images of the same subject across diverse scenes, yet it often fails due to a phenomenon called identity (ID) shift. Previous methods have tackled this issue, but typically rely on the unrealistic assumption of knowing all target scenes in advance. This paper reveals that a key source of ID shift is the native correlation between subject and scene context, called scene contextualization, which arises naturally as T2I models fit the training distribution of vast natural images. We formally prove the near-universality of this scene-ID correlation and derive theoretical bounds on its strength. On this basis, we propose a novel, efficient, training-free prompt embedding editing approach, called Scene De-Contextualization (SDeC), that imposes an inversion process of T2I's built-in scene contextualization. Specifically, it identifies and suppresses the latent scene-ID correlation within the ID prompt's embedding by quantifying the SVD directional stability to adaptively re-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene use (one scene per prompt) without requiring prior access to all target scenes. This makes it a highly flexible and general solution well-suited to real-world applications where such prior knowledge is often unavailable or varies over time. Experiments demonstrate that SDeC significantly enhances identity preservation while maintaining scene diversity.",
    "summary": "",
    "translation": "通过场景去上下文化实现一致的文本到图像生成",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于文本到图像生成的AIGC领域，属于纯粹的生成式AI应用。虽然提到了'一致性'和'上下文'概念，但核心是视觉内容生成，与推荐系统、搜索或广告的排名和匹配任务没有直接关联。该技术没有明显的应用潜力于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14543v1": {
    "title": "Exploring Cross-Modal Flows for Few-Shot Learning",
    "url": "https://www.alphaxiv.org/abs/2510.14543v1",
    "arxiv_id": "2510.14543v1",
    "authors": "Ziqi Jiang, Yanghao Wang, Long Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 10:32:48",
    "ori_summary": "Aligning features from different modalities, is one of the most fundamental challenges for cross-modal tasks. Although pre-trained vision-language models can achieve a general alignment between image and text, they often require parameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT methods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively fine-tune a subset of parameters, which can slightly adjust either visual or textual features, and avoid overfitting. In this paper, we are the first to highlight that all existing PEFT methods perform one-step adjustment. It is insufficient for complex (or difficult) datasets, where features of different modalities are highly entangled. To this end, we propose the first model-agnostic multi-step adjustment approach by learning a cross-modal velocity field: Flow Matching Alignment (FMA). Specifically, to ensure the correspondence between categories during training, we first utilize a fixed coupling strategy. Then, we propose a noise augmentation strategy to alleviate the data scarcity issue. Finally, we design an early-stopping solver, which terminates the transformation process earlier, improving both efficiency and accuracy. Compared with one-step PEFT methods, FMA has the multi-step rectification ability to achieve more precise and robust alignment. Extensive results have demonstrated that FMA can consistently yield significant performance gains across various benchmarks and backbones, particularly on challenging datasets.",
    "summary": "",
    "translation": "探索跨模态流用于少样本学习",
    "relevance_score": 7,
    "reasoning": "该论文涉及跨模态学习，与'VLM类比用于异构数据'焦点直接相关，其中不同模态可类比于推荐/搜索中的异构特征（如用户序列、上下文特征）。跨模态流的少样本学习技术可应用于冷启动用户/物品推荐、新查询处理等场景，通过跨模态信息迁移缓解数据稀疏问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14536v1": {
    "title": "Exploring Image Representation with Decoupled Classical Visual Descriptors",
    "url": "https://www.alphaxiv.org/abs/2510.14536v1",
    "arxiv_id": "2510.14536v1",
    "authors": "Chenyuan Qu, Hao Chen, Jianbo Jiao",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 10:27:55",
    "ori_summary": "Exploring and understanding efficient image representations is a long-standing challenge in computer vision. While deep learning has achieved remarkable progress across image understanding tasks, its internal representations are often opaque, making it difficult to interpret how visual information is processed. In contrast, classical visual descriptors (e.g. edge, colour, and intensity distribution) have long been fundamental to image analysis and remain intuitively understandable to humans. Motivated by this gap, we ask a central question: Can modern learning benefit from these classical cues? In this paper, we answer it with VisualSplit, a framework that explicitly decomposes images into decoupled classical descriptors, treating each as an independent but complementary component of visual knowledge. Through a reconstruction-driven pre-training scheme, VisualSplit learns to capture the essence of each visual descriptor while preserving their interpretability. By explicitly decomposing visual attributes, our method inherently facilitates effective attribute control in various advanced visual tasks, including image generation and editing, extending beyond conventional classification and segmentation, suggesting the effectiveness of this new learning approach for visual understanding. Project page: https://chenyuanqu.com/VisualSplit/.",
    "summary": "",
    "translation": "基于解耦经典视觉描述符的图像表示探索",
    "relevance_score": 2,
    "reasoning": "该论文专注于纯粹的计算机视觉技术，研究传统视觉描述符的解耦表示方法。虽然视觉特征在推荐和搜索系统中可能作为输入特征使用，但论文本身不涉及推荐系统、搜索广告的核心算法，也没有明确的LLM或Transformer技术应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14532v1": {
    "title": "Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology",
    "url": "https://www.alphaxiv.org/abs/2510.14532v1",
    "arxiv_id": "2510.14532v1",
    "authors": "Xinrui Huang, Fan Xiao, Dongming He, Anqi Gao, Dandan Li, Xiaofan Zhang, Shaoting Zhang, Xudong Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 10:24:23",
    "ori_summary": "Oral and maxillofacial radiology plays a vital role in dental healthcare, but radiographic image interpretation is limited by a shortage of trained professionals. While AI approaches have shown promise, existing dental AI systems are restricted by their single-modality focus, task-specific design, and reliance on costly labeled data, hindering their generalization across diverse clinical scenarios. To address these challenges, we introduce DentVFM, the first family of vision foundation models (VFMs) designed for dentistry. DentVFM generates task-agnostic visual representations for a wide range of dental applications and uses self-supervised learning on DentVista, a large curated dental imaging dataset with approximately 1.6 million multi-modal radiographic images from various medical centers. DentVFM includes 2D and 3D variants based on the Vision Transformer (ViT) architecture. To address gaps in dental intelligence assessment and benchmarks, we introduce DentBench, a comprehensive benchmark covering eight dental subspecialties, more diseases, imaging modalities, and a wide geographical distribution. DentVFM shows impressive generalist intelligence, demonstrating robust generalization to diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker identification, and anatomical landmark detection and segmentation. Experimental results indicate DentVFM significantly outperforms supervised, self-supervised, and weakly supervised baselines, offering superior generalization, label efficiency, and scalability. Additionally, DentVFM enables cross-modality diagnostics, providing more reliable results than experienced dentists in situations where conventional imaging is unavailable. DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and label-efficient model to improve intelligent dental healthcare and address critical gaps in global oral healthcare.",
    "summary": "",
    "translation": "迈向牙科通用智能：面向口腔颌面放射学的视觉基础模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于牙科医学领域的视觉基础模型应用，属于医疗领域的特定应用。虽然涉及基础模型技术，但其应用场景（口腔颌面放射学）与推荐系统、搜索或广告领域完全无关，属于明确的医疗领域应用，因此不符合任何相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14528v1": {
    "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model",
    "url": "https://www.alphaxiv.org/abs/2510.14528v1",
    "arxiv_id": "2510.14528v1",
    "authors": "Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Handong Zheng, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, Yanjun Ma",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 10:18:48",
    "ori_summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios.",
    "summary": "",
    "translation": "PaddleOCR-VL：通过0.9B超紧凑视觉语言模型增强多语言文档解析",
    "relevance_score": 2,
    "reasoning": "虽然该论文涉及视觉语言模型（VLM），但其核心应用是文档解析和OCR，这属于计算机视觉的特定领域，与推荐系统、搜索或广告的排名和建模需求没有直接关联。超紧凑模型设计可能对效率有启发，但论文未表明在异构数据统一建模或推荐/搜索应用方面的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14526v1": {
    "title": "Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2510.14526v1",
    "arxiv_id": "2510.14526v1",
    "authors": "Yunze Tong, Didi Zhu, Zijing Hu, Jinluan Yang, Ziyu Zhao",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-16 10:14:34",
    "ori_summary": "In text-to-image generation, different initial noises induce distinct denoising paths with a pretrained Stable Diffusion (SD) model. While this pattern could output diverse images, some of them may fail to align well with the prompt. Existing methods alleviate this issue either by altering the denoising dynamics or by drawing multiple noises and conducting post-selection. In this paper, we attribute the misalignment to a training-inference mismatch: during training, prompt-conditioned noises lie in a prompt-specific subset of the latent space, whereas at inference the noise is drawn from a prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector that applies text-conditioned refinement to the initial noise before denoising. Conditioned on the prompt embedding, it maps the noise to a prompt-aware counterpart that better matches the distribution observed during SD training, without modifying the SD model. Our framework consists of these steps: we first sample some noises and obtain token-level feedback for their corresponding images from a vision-language model (VLM), then distill these signals into a reward model, and finally optimize the noise projector via a quasi-direct preference optimization. Our design has two benefits: (i) it requires no reference images or handcrafted priors, and (ii) it incurs small inference cost, replacing multi-sample selection with a single forward pass. Extensive experiments further show that our prompt-aware noise projection improves text-image alignment across diverse prompts.",
    "summary": "",
    "translation": "噪声投影：弥合扩散模型中文本到图像错位的提示无关差距",
    "relevance_score": 2,
    "reasoning": "该论文专注于扩散模型中的文本到图像对齐问题，这属于AIGC和内容生成领域。虽然扩散模型是生成式AI的重要技术，但论文的核心关注点是图像生成质量而非推荐/搜索/广告中的排名或匹配问题。没有明确的潜在应用可以连接到推荐系统、搜索或广告的核心任务。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14525v1": {
    "title": "Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing",
    "url": "https://www.alphaxiv.org/abs/2510.14525v1",
    "arxiv_id": "2510.14525v1",
    "authors": "Qurrat Ul Ain, Atif Aftab Ahmed Jilani, Zunaira Shafqat, Nigar Azhar Butt",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 10:14:32",
    "ori_summary": "Defective surgical instruments pose serious risks to sterility, mechanical integrity, and patient safety, increasing the likelihood of surgical complications. However, quality control in surgical instrument manufacturing often relies on manual inspection, which is prone to human error and inconsistency. This study introduces SurgScan, an AI-powered defect detection framework for surgical instruments. Using YOLOv8, SurgScan classifies defects in real-time, ensuring high accuracy and industrial scalability. The model is trained on a high-resolution dataset of 102,876 images, covering 11 instrument types and five major defect categories. Extensive evaluation against state-of-the-art CNN architectures confirms that SurgScan achieves the highest accuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image, making it suitable for industrial deployment. Statistical analysis demonstrates that contrast-enhanced preprocessing significantly improves defect detection, addressing key limitations in visual inspection. SurgScan provides a scalable, cost-effective AI solution for automated quality control, reducing reliance on manual inspection while ensuring compliance with ISO 13485 and FDA standards, paving the way for enhanced defect detection in medical manufacturing.",
    "summary": "",
    "translation": "基于无损检测的实时手术器械缺陷检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域的手术器械缺陷检测，属于医学应用范畴，与推荐系统、搜索或广告领域完全无关。论文标题明确指向医疗设备检测，没有任何技术要素可以应用于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14516v1": {
    "title": "Vision Mamba for Permeability Prediction of Porous Media",
    "url": "https://www.alphaxiv.org/abs/2510.14516v1",
    "arxiv_id": "2510.14516v1",
    "authors": "Ali Kashefi, Tapan Mukerji",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 10:02:33",
    "ori_summary": "Vision Mamba has recently received attention as an alternative to Vision Transformers (ViTs) for image classification. The network size of Vision Mamba scales linearly with input image resolution, whereas ViTs scale quadratically, a feature that improves computational and memory efficiency. Moreover, Vision Mamba requires a significantly smaller number of trainable parameters than traditional convolutional neural networks (CNNs), and thus, they can be more memory efficient. Because of these features, we introduce, for the first time, a neural network that uses Vision Mamba as its backbone for predicting the permeability of three-dimensional porous media. We compare the performance of Vision Mamba with ViT and CNN models across multiple aspects of permeability prediction and perform an ablation study to assess the effects of its components on accuracy. We demonstrate in practice the aforementioned advantages of Vision Mamba over ViTs and CNNs in the permeability prediction of three-dimensional porous media. We make the source code publicly available to facilitate reproducibility and to enable other researchers to build on and extend this work. We believe the proposed framework has the potential to be integrated into large vision models in which Vision Mamba is used instead of ViTs.",
    "summary": "",
    "translation": "用于多孔介质渗透率预测的视觉Mamba模型",
    "relevance_score": 2,
    "reasoning": "该论文虽然涉及Mamba架构（一种序列建模技术），但其应用领域是多孔介质渗透率预测，这属于物理学和材料科学领域。该应用与推荐系统、搜索或广告没有直接关联，也不涉及处理用户行为序列或上下文特征等异构数据。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14493v1": {
    "title": "Grazing Detection using Deep Learning and Sentinel-2 Time Series Data",
    "url": "https://www.alphaxiv.org/abs/2510.14493v1",
    "arxiv_id": "2510.14493v1",
    "authors": "Aleksis Pirinen, Delia Fano Yela, Smita Chakraborty, Erik Källman",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 09:37:43",
    "ori_summary": "Grazing shapes both agricultural production and biodiversity, yet scalable monitoring of where grazing occurs remains limited. We study seasonal grazing detection from Sentinel-2 L2A time series: for each polygon-defined field boundary, April-October imagery is used for binary prediction (grazed / not grazed). We train an ensemble of CNN-LSTM models on multi-temporal reflectance features, and achieve an average F1 score of 77 percent across five validation splits, with 90 percent recall on grazed pastures. Operationally, if inspectors can visit at most 4 percent of sites annually, prioritising fields predicted by our model as non-grazed yields 17.2 times more confirmed non-grazing sites than random inspection. These results indicate that coarse-resolution, freely available satellite data can reliably steer inspection resources for conservation-aligned land-use compliance. Code and models have been made publicly available.",
    "summary": "",
    "translation": "基于深度学习和哨兵-2时序数据的放牧检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于遥感图像分析中的放牧检测应用，属于农业和环境监测领域。其使用的深度学习和时序数据处理技术虽然具有技术价值，但与推荐系统、搜索、广告等核心领域没有直接关联，也不涉及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14463v1": {
    "title": "Pruning Overparameterized Multi-Task Networks for Degraded Web Image Restoration",
    "url": "https://www.alphaxiv.org/abs/2510.14463v1",
    "arxiv_id": "2510.14463v1",
    "authors": "Thomas Katraouras, Dimitrios Rafailidis",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 09:04:05",
    "ori_summary": "Image quality is a critical factor in delivering visually appealing content on web platforms. However, images often suffer from degradation due to lossy operations applied by online social networks (OSNs), negatively affecting user experience. Image restoration is the process of recovering a clean high-quality image from a given degraded input. Recently, multi-task (all-in-one) image restoration models have gained significant attention, due to their ability to simultaneously handle different types of image degradations. However, these models often come with an excessively high number of trainable parameters, making them computationally inefficient. In this paper, we propose a strategy for compressing multi-task image restoration models. We aim to discover highly sparse subnetworks within overparameterized deep models that can match or even surpass the performance of their dense counterparts. The proposed model, namely MIR-L, utilizes an iterative pruning strategy that removes low-magnitude weights across multiple rounds, while resetting the remaining weights to their original initialization. This iterative process is important for the multi-task image restoration model's optimization, effectively uncovering \"winning tickets\" that maintain or exceed state-of-the-art performance at high sparsity levels. Experimental evaluation on benchmark datasets for the deraining, dehazing, and denoising tasks shows that MIR-L retains only 10% of the trainable parameters while maintaining high image restoration performance. Our code, datasets and pre-trained models are made publicly available at https://github.com/Thomkat/MIR-L.",
    "summary": "",
    "translation": "面向退化网络图像恢复的过参数化多任务网络剪枝",
    "relevance_score": 2,
    "reasoning": "该论文主要关注图像恢复任务中的网络剪枝技术，属于计算机视觉领域的特定应用。虽然多任务学习和网络剪枝是通用技术，但论文明确针对'退化网络图像恢复'这一具体视觉任务，与推荐系统、搜索或广告的核心技术关联性较弱。这些技术可能间接应用于模型效率优化，但缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14462v1": {
    "title": "Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review",
    "url": "https://www.alphaxiv.org/abs/2510.14462v1",
    "arxiv_id": "2510.14462v1",
    "authors": "Youwan Mahé, Elise Bannier, Stéphanie Leplaideur, Elisa Fromont, Francesca Galassi",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 09:02:52",
    "ori_summary": "Unsupervised deep generative models are emerging as a promising alternative to supervised methods for detecting and segmenting anomalies in brain imaging. Unlike fully supervised approaches, which require large voxel-level annotated datasets and are limited to well-characterised pathologies, these models can be trained exclusively on healthy data and identify anomalies as deviations from learned normative brain structures. This PRISMA-guided scoping review synthesises recent work on unsupervised deep generative models for anomaly detection in neuroimaging, including autoencoders, variational autoencoders, generative adversarial networks, and denoising diffusion models. A total of 49 studies published between 2018 - 2025 were identified, covering applications to brain MRI and, less frequently, CT across diverse pathologies such as tumours, stroke, multiple sclerosis, and small vessel disease. Reported performance metrics are compared alongside architectural design choices. Across the included studies, generative models achieved encouraging performance for large focal lesions and demonstrated progress in addressing more subtle abnormalities. A key strength of generative models is their ability to produce interpretable pseudo-healthy (also referred to as counterfactual) reconstructions, which is particularly valuable when annotated data are scarce, as in rare or heterogeneous diseases. Looking ahead, these models offer a compelling direction for anomaly detection, enabling semi-supervised learning, supporting the discovery of novel imaging biomarkers, and facilitating within- and cross-disease deviation mapping in unified end-to-end frameworks. To realise clinical impact, future work should prioritise anatomy-aware modelling, development of foundation models, task-appropriate evaluation metrics, and rigorous clinical validation.",
    "summary": "",
    "translation": "神经影像异常检测的无监督深度生成模型：系统性范围综述",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学神经影像领域的异常检测应用，属于明确的医疗领域特定应用。论文讨论的深度生成模型虽然具有技术价值，但其应用场景和问题定义完全限定在医疗影像分析，与推荐系统、搜索或广告领域没有任何直接或潜在的关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14460v1": {
    "title": "Structured Universal Adversarial Attacks on Object Detection for Video Sequences",
    "url": "https://www.alphaxiv.org/abs/2510.14460v1",
    "arxiv_id": "2510.14460v1",
    "authors": "Sven Jacob, Weijia Shao, Gjergji Kasneci",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 09:00:41",
    "ori_summary": "Video-based object detection plays a vital role in safety-critical applications. While deep learning-based object detectors have achieved impressive performance, they remain vulnerable to adversarial attacks, particularly those involving universal perturbations. In this work, we propose a minimally distorted universal adversarial attack tailored for video object detection, which leverages nuclear norm regularization to promote structured perturbations concentrated in the background. To optimize this formulation efficiently, we employ an adaptive, optimistic exponentiated gradient method that enhances both scalability and convergence. Our results demonstrate that the proposed attack outperforms both low-rank projected gradient descent and Frank-Wolfe based attacks in effectiveness while maintaining high stealthiness. All code and data are publicly available at https://github.com/jsve96/AO-Exp-Attack.",
    "summary": "",
    "translation": "针对视频序列目标检测的结构化通用对抗攻击",
    "relevance_score": 2,
    "reasoning": "该论文主要研究计算机视觉领域的目标检测对抗攻击，属于安全/对抗性攻击范畴，这在无关主题中被明确排除。虽然目标检测在视觉搜索中有潜在应用，但论文焦点是攻击而非核心推荐系统、搜索或广告的进展，也没有涉及LLM、Transformer或异构数据建模等当前关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14431v1": {
    "title": "Real-Time Neural Video Compression with Unified Intra and Inter Coding",
    "url": "https://www.alphaxiv.org/abs/2510.14431v1",
    "arxiv_id": "2510.14431v1",
    "authors": "Hui Xiang, Yifan Bian, Li Li, Jingran Wu, Xianguo Zhang, Dong Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 08:31:44",
    "ori_summary": "Neural video compression (NVC) technologies have advanced rapidly in recent years, yielding state-of-the-art schemes such as DCVC-RT that offer superior compression efficiency to H.266/VVC and real-time encoding/decoding capabilities. Nonetheless, existing NVC schemes have several limitations, including inefficiency in dealing with disocclusion and new content, interframe error propagation and accumulation, among others. To eliminate these limitations, we borrow the idea from classic video coding schemes, which allow intra coding within inter-coded frames. With the intra coding tool enabled, disocclusion and new content are properly handled, and interframe error propagation is naturally intercepted without the need for manual refresh mechanisms. We present an NVC framework with unified intra and inter coding, where every frame is processed by a single model that is trained to perform intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame compression design to exploit interframe redundancy not only forwardly but also backwardly. Experimental results show that our scheme outperforms DCVC-RT by an average of 10.7\\% BD-rate reduction, delivers more stable bitrate and quality per frame, and retains real-time encoding/decoding performances. Code and models will be released.",
    "summary": "",
    "translation": "基于统一帧内与帧间编码的实时神经视频压缩",
    "relevance_score": 2,
    "reasoning": "该论文专注于视频压缩技术，属于计算机视觉领域，与推荐系统、搜索或广告的核心关注点没有直接关联。虽然高效的视频处理可能间接支持多媒体内容推荐，但这种连接过于间接，无法满足当前关注点的直接应用要求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14427v1": {
    "title": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14427v1",
    "arxiv_id": "2510.14427v1",
    "authors": "Ho Yin Au, Jie Chen, Junkun Jiang, Jingyu Xiang",
    "categories": "cs.MM, cs.CV",
    "pub_date": "2025-10-16 08:28:46",
    "ori_summary": "Recent research on motion generation has shown significant progress in generating semantically aligned motion with singular semantics. However, when employing these models to create composite sequences containing multiple semantically generated motion clips, they often struggle to preserve the continuity of motion dynamics at the transition boundaries between clips, resulting in awkward transitions and abrupt artifacts. To address these challenges, we present Compositional Phase Diffusion, which leverages the Semantic Phase Diffusion Module (SPDM) and Transitional Phase Diffusion Module (TPDM) to progressively incorporate semantic guidance and phase details from adjacent motion clips into the diffusion process. Specifically, SPDM and TPDM operate within the latent motion frequency domain established by the pre-trained Action-Centric Motion Phase Autoencoder (ACT-PAE). This allows them to learn semantically important and transition-aware phase information from variable-length motion clips during training. Experimental results demonstrate the competitive performance of our proposed framework in generating compositional motion sequences that align semantically with the input conditions, while preserving phase transitional continuity between preceding and succeeding motion clips. Additionally, motion inbetweening task is made possible by keeping the phase parameter of the input motion sequences fixed throughout the diffusion process, showcasing the potential for extending the proposed framework to accommodate various application scenarios. Codes are available at https://github.com/asdryau/TransPhase.",
    "summary": "",
    "translation": "深度组合相位扩散用于长运动序列生成",
    "relevance_score": 2,
    "reasoning": "该论文专注于运动序列生成，属于计算机图形学或动画领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然扩散模型是生成式AI的重要技术，但该工作针对的是运动数据模态，没有明确展示在推荐、搜索或广告场景中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14403v1": {
    "title": "DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis",
    "url": "https://www.alphaxiv.org/abs/2510.14403v1",
    "arxiv_id": "2510.14403v1",
    "authors": "Chao Tu, Kun Huang, Jie Zhang, Qianjin Feng, Yu Zhang, Zhenyuan Ning",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 08:03:54",
    "ori_summary": "The burgeoning discipline of computational pathology shows promise in harnessing whole slide images (WSIs) to quantify morphological heterogeneity and develop objective prognostic modes for human cancers. However, progress is impeded by the computational bottleneck of gigapixel-size inputs and the scarcity of dense manual annotations. Current methods often overlook fine-grained information across multi-magnification WSIs and variations in tumor microenvironments. Here, we propose an easy-to-hard progressive representation learning model, termed dual-curriculum contrastive multi-instance learning (DCMIL), to efficiently process WSIs for cancer prognosis. The model does not rely on dense annotations and enables the direct transformation of gigapixel-size WSIs into outcome predictions. Extensive experiments on twelve cancer types (5,954 patients, 12.54 million tiles) demonstrate that DCMIL outperforms standard WSI-based prognostic models. Additionally, DCMIL identifies fine-grained prognosis-salient regions, provides robust instance uncertainty estimation, and captures morphological differences between normal and tumor tissues, with the potential to generate new biological insights. All codes have been made publicly accessible at https://github.com/tuuuc/DCMIL.",
    "summary": "",
    "translation": "DCMIL：一种用于癌症预后分析的全切片图像渐进式表征学习模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分析领域，特别是癌症预后分析，这属于明确的无关主题（医学/生物学应用）。论文内容涉及全切片图像处理和医疗诊断，与推荐系统、搜索或广告的核心技术领域没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14389v1": {
    "title": "BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble",
    "url": "https://www.alphaxiv.org/abs/2510.14389v1",
    "arxiv_id": "2510.14389v1",
    "authors": "Brandon Hill, Kma Solaiman",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-16 07:38:31",
    "ori_summary": "Motherboard defect detection is critical for ensuring reliability in high-volume electronics manufacturing. While prior research in PCB inspection has largely targeted bare-board or trace-level defects, assembly-level inspection of full motherboards inspection remains underexplored. In this work, we present BoardVision, a reproducible framework for detecting assembly-level defects such as missing screws, loose fan wiring, and surface scratches. We benchmark two representative detectors - YOLOv7 and Faster R-CNN, under controlled conditions on the MiracleFactory motherboard dataset, providing the first systematic comparison in this domain. To mitigate the limitations of single models, where YOLO excels in precision but underperforms in recall and Faster R-CNN shows the reverse, we propose a lightweight ensemble, Confidence-Temporal Voting (CTV Voter), that balances precision and recall through interpretable rules. We further evaluate robustness under realistic perturbations including sharpness, brightness, and orientation changes, highlighting stability challenges often overlooked in motherboard defect detection. Finally, we release a deployable GUI-driven inspection tool that bridges research evaluation with operator usability. Together, these contributions demonstrate how computer vision techniques can transition from benchmark results to practical quality assurance for assembly-level motherboard manufacturing.",
    "summary": "",
    "translation": "BoardVision：基于YOLO+Faster-RCNN集成方法的可部署且鲁棒的母板缺陷检测系统",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的硬件缺陷检测应用，属于纯粹的视觉检测任务。虽然采用了先进的检测架构（YOLO+Faster-RCNN），但其应用场景（主板缺陷检测）与推荐系统、搜索或广告领域没有任何关联，也不涉及任何可能应用于这些领域的核心技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14383v1": {
    "title": "DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights",
    "url": "https://www.alphaxiv.org/abs/2510.14383v1",
    "arxiv_id": "2510.14383v1",
    "authors": "Danish Ali, Ajmal Mian, Naveed Akhtar, Ghulam Mubashar Hassan",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 07:31:21",
    "ori_summary": "Accurate brain tumor segmentation is significant for clinical diagnosis and treatment. It is challenging due to the heterogeneity of tumor subregions. Mamba-based State Space Models have demonstrated promising performance. However, they incur significant computational overhead due to sequential feature computation across multiple spatial axes. Moreover, their robustness across diverse BraTS data partitions remains largely unexplored, leaving a critical gap in reliable evaluation. To address these limitations, we propose dual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation model that captures multi-scale long-range dependencies with minimal computational overhead. We leverage a space-filling curve to preserve spatial locality during 3D-to-1D feature mapping, thereby reducing reliance on computationally expensive multi-axial feature scans. To enrich feature representation, we propose a gated fusion module that adaptively integrates forward and reverse contexts, along with a quantization block that discretizes features to improve robustness. In addition, we propose five systematic folds on BraTS2023 for rigorous evaluation of segmentation techniques under diverse conditions and present detailed analysis of common failure scenarios. On the 20\\% test set used by recent methods, our model achieves Dice improvements of 0.10\\% for whole tumor, 1.75\\% for tumor core, and 0.93\\% for enhancing tumor. Evaluations on the proposed systematic five folds demonstrate that our model maintains competitive whole tumor accuracy while achieving clear average Dice gains of 0.86\\% for tumor core and 1.45\\% for enhancing tumor over existing state-of-the-art. Furthermore, our model attains 15 times improvement in efficiency while maintaining high segmentation accuracy, highlighting its robustness and computational advantage over existing approaches.",
    "summary": "",
    "translation": "用于稳健高效脑肿瘤分割及分析洞察的DRBD-Mamba方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像领域的脑肿瘤分割，属于明确的医学应用范畴，与推荐系统、搜索或广告领域完全无关。论文标题中提到的Mamba架构虽然是一种序列建模技术，但应用于特定医疗场景，没有任何潜在的应用于RecSys/Search/Ads的关联性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14376v1": {
    "title": "DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14376v1",
    "arxiv_id": "2510.14376v1",
    "authors": "Dongnam Byun, Jungwon Park, Jumgmin Ko, Changin Choi, Wonjong Rhee",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 07:17:23",
    "ori_summary": "Recent progress in text-to-image (T2I) generative models has led to significant improvements in generating high-quality images aligned with text prompts. However, these models still struggle with prompts involving multiple objects, often resulting in object neglect or object mixing. Through extensive studies, we identify four problematic scenarios, Similar Shapes, Similar Textures, Dissimilar Background Biases, and Many Objects, where inter-object relationships frequently lead to such failures. Motivated by two key observations about CLIP embeddings, we propose DOS (Directional Object Separation), a method that modifies three types of CLIP text embeddings before passing them into text-to-image models. Experimental results show that DOS consistently improves the success rate of multi-object image generation and reduces object mixing. In human evaluations, DOS significantly outperforms four competing methods, receiving 26.24%-43.04% more votes across four benchmarks. These results highlight DOS as a practical and effective solution for improving multi-object image generation.",
    "summary": "",
    "translation": "DOS：文本嵌入中的方向性对象分离用于多对象图像生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于多对象图像生成技术，属于纯粹的视觉生成领域。虽然涉及文本嵌入处理，但其核心应用是图像生成而非推荐、搜索或广告系统。该技术没有明显的潜在应用可以转移到推荐系统、搜索或广告领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14374v1": {
    "title": "Spatial Preference Rewarding for MLLMs Spatial Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.14374v1",
    "arxiv_id": "2510.14374v1",
    "authors": "Han Qiu, Peng Gao, Lewei Lu, Xiaoqin Zhang, Ling Shao, Shijian Lu",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 07:16:18",
    "ori_summary": "Multimodal large language models~(MLLMs) have demonstrated promising spatial understanding capabilities, such as referencing and grounding object descriptions. Despite their successes, MLLMs still fall short in fine-grained spatial perception abilities, such as generating detailed region descriptions or accurately localizing objects. Additionally, they often fail to respond to the user's requirements for desired fine-grained spatial understanding. This issue might arise because existing approaches primarily focus on tuning MLLMs to model pre-annotated instruction data to inject spatial knowledge, without direct supervision of MLLMs' actual responses. We address this issue by SPR, a Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial capabilities by rewarding MLLMs' detailed responses with precise object localization over vague or inaccurate responses. With randomly selected image regions and region descriptions from MLLMs, SPR introduces semantic and localization scores to comprehensively evaluate the text quality and localization quality in MLLM-generated descriptions. We also refine the MLLM descriptions with better localization accuracy and pair the best-scored refinement with the initial descriptions of the lowest score for direct preference optimization, thereby enhancing fine-grained alignment with visual input. Extensive experiments over standard referring and grounding benchmarks show that SPR improves MLLM spatial understanding capabilities effectively with minimal overhead in training. Data and code will be released at https://github.com/hanqiu-hq/SPR",
    "summary": "",
    "translation": "多模态大语言模型空间理解的空间偏好奖励",
    "relevance_score": 3,
    "reasoning": "该论文专注于多模态大语言模型的空间理解能力，属于纯粹的视觉-语言模态研究。虽然提到了偏好奖励机制，但主要针对空间理解这一特定视觉任务，没有明确展示在推荐系统、搜索或广告中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14354v1": {
    "title": "Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration",
    "url": "https://www.alphaxiv.org/abs/2510.14354v1",
    "arxiv_id": "2510.14354v1",
    "authors": "Siddharth Tourani, Jayaram Reddy, Sarvesh Thakur, K Madhava Krishna, Muhammad Haris Khan, N Dinesh Reddy",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-16 06:47:10",
    "ori_summary": "With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has become available. This prompts the question of how to utilize this data for geometric reasoning of scenes. While many RGB-D registration meth- ods rely on geometric and feature-based similarity, we take a different approach. We use cycle-consistent keypoints as salient points to enforce spatial coherence constraints during matching, improving correspondence accuracy. Additionally, we introduce a novel pose block that combines a GRU recurrent unit with transformation synchronization, blending historical and multi-view data. Our approach surpasses previous self- supervised registration methods on ScanNet and 3DMatch, even outperforming some older supervised methods. We also integrate our components into existing methods, showing their effectiveness.",
    "summary": "",
    "translation": "利用循环一致性锚点进行自监督RGB-D配准",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉中的RGB-D配准技术，属于纯粹的视觉处理领域。虽然自监督学习是通用技术，但论文标题明确指向视觉数据配准问题，与推荐系统、搜索或广告的核心技术栈没有直接关联，也没有明显的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14349v1": {
    "title": "Vision-Centric Activation and Coordination for Multimodal Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.14349v1",
    "arxiv_id": "2510.14349v1",
    "authors": "Yunnan Wang, Fan Lu, Kecheng Zheng, Ziyuan Huang, Ziqiang Li, Wenjun Zeng, Xin Jin",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 06:38:39",
    "ori_summary": "Multimodal large language models (MLLMs) integrate image features from visual encoders with LLMs, demonstrating advanced comprehension capabilities. However, mainstream MLLMs are solely supervised by the next-token prediction of textual tokens, neglecting critical vision-centric information essential for analytical abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM representations through Vision-Centric activation and Coordination from multiple vision foundation models (VFMs). VaCo introduces visual discriminative alignment to integrate task-aware perceptual features extracted from VFMs, thereby unifying the optimization of both textual and visual outputs in MLLMs. Specifically, we incorporate the learnable Modular Task Queries (MTQs) and Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals under the supervision of diverse VFMs. To coordinate representation conflicts across VFMs, the crafted Token Gateway Mask (TGM) restricts the information flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo significantly improves the performance of different MLLMs on various benchmarks, showcasing its superior capabilities in visual comprehension.",
    "summary": "",
    "translation": "面向多模态大语言模型的视觉中心激活与协调机制",
    "relevance_score": 4,
    "reasoning": "该论文聚焦多模态大语言模型中的视觉模态激活与协调，属于VLM技术范畴。虽然VLM本身与异构数据处理有类比价值，但论文标题未明确指向推荐系统、搜索或广告领域的应用，更多是通用多模态技术研究，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14340v1": {
    "title": "A Density-Informed Multimodal Artificial Intelligence Framework for Improving Breast Cancer Detection Across All Breast Densities",
    "url": "https://www.alphaxiv.org/abs/2510.14340v1",
    "arxiv_id": "2510.14340v1",
    "authors": "Siva Teja Kakileti, Bharath Govindaraju, Sudhakar Sampangi, Geetha Manjunath",
    "categories": "eess.IV, cs.AI, cs.CV, cs.LG",
    "pub_date": "2025-10-16 06:20:14",
    "ori_summary": "Mammography, the current standard for breast cancer screening, has reduced sensitivity in women with dense breast tissue, contributing to missed or delayed diagnoses. Thermalytix, an AI-based thermal imaging modality, captures functional vascular and metabolic cues that may complement mammographic structural data. This study investigates whether a breast density-informed multi-modal AI framework can improve cancer detection by dynamically selecting the appropriate imaging modality based on breast tissue composition. A total of 324 women underwent both mammography and thermal imaging. Mammography images were analyzed using a multi-view deep learning model, while Thermalytix assessed thermal images through vascular and thermal radiomics. The proposed framework utilized Mammography AI for fatty breasts and Thermalytix AI for dense breasts, optimizing predictions based on tissue type. This multi-modal AI framework achieved a sensitivity of 94.55% (95% CI: 88.54-100) and specificity of 79.93% (95% CI: 75.14-84.71), outperforming standalone mammography AI (sensitivity 81.82%, specificity 86.25%) and Thermalytix AI (sensitivity 92.73%, specificity 75.46%). Importantly, the sensitivity of Mammography dropped significantly in dense breasts (67.86%) versus fatty breasts (96.30%), whereas Thermalytix AI maintained high and consistent sensitivity in both (92.59% and 92.86%, respectively). This demonstrates that a density-informed multi-modal AI framework can overcome key limitations of unimodal screening and deliver high performance across diverse breast compositions. The proposed framework is interpretable, low-cost, and easily deployable, offering a practical path to improving breast cancer screening outcomes in both high-resource and resource-limited settings.",
    "summary": "",
    "translation": "基于密度信息的多模态人工智能框架，用于改善所有乳腺密度下的乳腺癌检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的乳腺癌检测应用，属于明确的医疗领域特定应用。虽然提到了多模态AI框架，但其应用场景和问题领域与推荐系统、搜索或广告完全无关，属于明确的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14314v1": {
    "title": "A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection",
    "url": "https://www.alphaxiv.org/abs/2510.14314v1",
    "arxiv_id": "2510.14314v1",
    "authors": "Shivangi Yadav, Arun Ross",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 05:21:30",
    "ori_summary": "An iris biometric system can be compromised by presentation attacks (PAs) where artifacts such as artificial eyes, printed eye images, or cosmetic contact lenses are presented to the system. To counteract this, several presentation attack detection (PAD) methods have been developed. However, there is a scarcity of datasets for training and evaluating iris PAD techniques due to the implicit difficulties in constructing and imaging PAs. To address this, we introduce the Multi-domain Image Translative Diffusion StyleGAN (MID-StyleGAN), a new framework for generating synthetic ocular images that captures the PA and bonafide characteristics in multiple domains such as bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the strengths of diffusion models and generative adversarial networks (GANs) to produce realistic and diverse synthetic data. Our approach utilizes a multi-domain architecture that enables the translation between bonafide ocular images and different PA domains. The model employs an adaptive loss function tailored for ocular data to maintain domain consistency. Extensive experiments demonstrate that MID-StyleGAN outperforms existing methods in generating high-quality synthetic ocular images. The generated data was used to significantly enhance the performance of PAD systems, providing a scalable solution to the data scarcity problem in iris and ocular biometrics. For example, on the LivDet2020 dataset, the true detect rate at 1% false detect rate improved from 93.41% to 98.72%, showcasing the impact of the proposed method.",
    "summary": "",
    "translation": "用于虹膜呈现攻击检测的多域图像转换扩散风格生成对抗网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的虹膜识别和攻击检测，属于生物识别安全应用。虽然涉及生成模型（StyleGAN和扩散模型），但其应用场景（虹膜安全检测）与推荐系统、搜索或广告的核心技术领域完全无关，且属于被明确排除的领域特定应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14304v1": {
    "title": "Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding",
    "url": "https://www.alphaxiv.org/abs/2510.14304v1",
    "arxiv_id": "2510.14304v1",
    "authors": "Kyungryul Back, Seongbeom Park, Milim Kim, Mincheol Kwon, SangHyeok Lee, Hyunyoung Lee, Junhee Cho, Seunghyun Park, Jinkyu Kim",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 04:58:45",
    "ori_summary": "Large Vision-Language Models (LVLMs) have recently shown promising results on various multimodal tasks, even achieving human-comparable performance in certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often rely heavily on a single modality or memorize training data without properly grounding their outputs. To address this, we propose a training-free, tri-layer contrastive decoding with watermarking, which proceeds in three steps: (1) select a mature layer and an amateur layer among the decoding layers, (2) identify a pivot layer using a watermark-related question to assess whether the layer is visually well-grounded, and (3) apply tri-layer contrastive decoding to generate the final output. Experiments on public benchmarks such as POPE, MME and AMBER demonstrate that our method achieves state-of-the-art performance in reducing hallucinations in LVLMs and generates more visually grounded responses.",
    "summary": "",
    "translation": "面向事实性的水印技术：通过三层对比解码引导视觉语言模型趋向真实",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型的真实性水印和事实性控制，这属于幻觉控制和评估的范畴，属于明确排除的无关主题。虽然提到了视觉语言模型，但核心焦点是事实性保证而非异构数据建模，因此与VLM类比主题的相关性也很有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14293v1": {
    "title": "Learning Human-Humanoid Coordination for Collaborative Object Carrying",
    "url": "https://www.alphaxiv.org/abs/2510.14293v1",
    "arxiv_id": "2510.14293v1",
    "authors": "Yushi Du, Yixuan Li, Baoxiong Jia, Yutang Lin, Pei Zhou, Wei Liang, Yanchao Yang, Siyuan Huang",
    "categories": "cs.RO, cs.AI, cs.CV, cs.LG",
    "pub_date": "2025-10-16 04:36:25",
    "ori_summary": "Human-humanoid collaboration shows significant promise for applications in healthcare, domestic assistance, and manufacturing. While compliant robot-human collaboration has been extensively developed for robotic arms, enabling compliant human-humanoid collaboration remains largely unexplored due to humanoids' complex whole-body dynamics. In this paper, we propose a proprioception-only reinforcement learning approach, COLA, that combines leader and follower behaviors within a single policy. The model is trained in a closed-loop environment with dynamic object interactions to predict object motion patterns and human intentions implicitly, enabling compliant collaboration to maintain load balance through coordinated trajectory planning. We evaluate our approach through comprehensive simulator and real-world experiments on collaborative carrying tasks, demonstrating the effectiveness, generalization, and robustness of our model across various terrains and objects. Simulation experiments demonstrate that our model reduces human effort by 24.7%. compared to baseline approaches while maintaining object stability. Real-world experiments validate robust collaborative carrying across different object types (boxes, desks, stretchers, etc.) and movement patterns (straight-line, turning, slope climbing). Human user studies with 23 participants confirm an average improvement of 27.4% compared to baseline models. Our method enables compliant human-humanoid collaborative carrying without requiring external sensors or complex interaction models, offering a practical solution for real-world deployment.",
    "summary": "",
    "translation": "学习人形机器人与人协作搬运物体的协调能力",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于机器人学和人机协作领域，涉及物理协调和控制问题。该主题与推荐系统、搜索或广告的核心领域没有任何关联，也不涉及LLM技术、Transformer架构或异构数据建模。这是一个纯粹的机器人应用研究，属于无关领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14273v1": {
    "title": "CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts",
    "url": "https://www.alphaxiv.org/abs/2510.14273v1",
    "arxiv_id": "2510.14273v1",
    "authors": "Kieu-Anh Truong Thi, Huy-Hieu Pham, Duc-Trong Le",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 03:45:31",
    "ori_summary": "Domain shift in histopathology, often caused by differences in acquisition processes or data sources, poses a major challenge to the generalization ability of deep learning models. Existing methods primarily rely on modeling statistical correlations by aligning feature distributions or introducing statistical variation, yet they often overlook causal relationships. In this work, we propose a novel causal-inference-based framework that leverages semantic features while mitigating the impact of confounders. Our method implements the front-door principle by designing transformation strategies that explicitly incorporate mediators and observed tissue slides. We validate our method on the CAMELYON17 dataset and a private histopathology dataset, demonstrating consistent performance gains across unseen domains. As a result, our approach achieved up to a 7% improvement in both the CAMELYON17 dataset and the private histopathology dataset, outperforming existing baselines. These results highlight the potential of causal inference as a powerful tool for addressing domain shift in histopathology image analysis.",
    "summary": "",
    "translation": "CLEAR：面向分布外偏移下稳健组织病理学肿瘤检测的因果学习框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分析领域的组织病理学肿瘤检测，属于明确的医疗领域应用。虽然提到了因果学习和分布外鲁棒性等概念，但这些技术是在特定医学背景下应用的，与推荐系统、搜索或广告的核心技术栈没有直接关联。论文内容完全属于被排除的医疗领域范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14270v1": {
    "title": "GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering",
    "url": "https://www.alphaxiv.org/abs/2510.14270v1",
    "arxiv_id": "2510.14270v1",
    "authors": "Alexander Valverde, Brian Xu, Yuyin Zhou, Meng Xu, Hongyun Wang",
    "categories": "cs.CV, cs.GR",
    "pub_date": "2025-10-16 03:38:26",
    "ori_summary": "Scene reconstruction has emerged as a central challenge in computer vision, with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting achieving remarkable progress. While Gaussian Splatting demonstrates strong performance on large-scale datasets, it often struggles to capture fine details or maintain realism in regions with sparse coverage, largely due to the inherent limitations of sparse 3D training data. In this work, we propose GauSSmart, a hybrid method that effectively bridges 2D foundational models and 3D Gaussian Splatting reconstruction. Our approach integrates established 2D computer vision techniques, including convex filtering and semantic feature supervision from foundational models such as DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D segmentation priors and high-dimensional feature embeddings, our method guides the densification and refinement of Gaussian splats, improving coverage in underrepresented areas and preserving intricate structural details. We validate our approach across three datasets, where GauSSmart consistently outperforms existing Gaussian Splatting in the majority of evaluated scenes. Our results demonstrate the significant potential of hybrid 2D-3D approaches, highlighting how the thoughtful combination of 2D foundational models with 3D reconstruction pipelines can overcome the limitations inherent in either approach alone.",
    "summary": "",
    "translation": "GauSSmart：通过2D基础模型和几何滤波增强3D重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D重建技术，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术栈没有直接关联。虽然提到了基础模型，但应用场景局限于3D视觉重建，无法为RecSys/Search/Ads领域提供可行的技术路径或应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14266v1": {
    "title": "Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment",
    "url": "https://www.alphaxiv.org/abs/2510.14266v1",
    "arxiv_id": "2510.14266v1",
    "authors": "Miu Sumino, Mayu Ishii, Shun Kaizu, Daisuke Hisano, Yu Nakayama",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 03:36:08",
    "ori_summary": "We propose a robust demodulation scheme for optical camera communication systems using an event-based vision sensor, combining OOK with toggle demodulation and a digital phase-locked loop. This is the first report to achieve a $\\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor experiments.",
    "summary": "",
    "translation": "长距离室外环境中基于事件的光学相机通信实验验证",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于光学通信系统的实验验证，属于通信工程领域，与推荐系统、搜索或广告的核心技术无关。虽然通信技术可能作为底层基础设施，但论文本身不涉及任何Transformer架构、LLM技术或推荐算法，也没有展示在RecSys/Search/Ads领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14260v1": {
    "title": "MatchAttention: Matching the Relative Positions for High-Resolution Cross-View Matching",
    "url": "https://www.alphaxiv.org/abs/2510.14260v1",
    "arxiv_id": "2510.14260v1",
    "authors": "Tingman Yan, Tao Liu, Xilian Yang, Qunfei Zhao, Zeyang Xia",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 03:21:28",
    "ori_summary": "Cross-view matching is fundamentally achieved through cross-attention mechanisms. However, matching of high-resolution images remains challenging due to the quadratic complexity and lack of explicit matching constraints in the existing cross-attention. This paper proposes an attention mechanism, MatchAttention, that dynamically matches relative positions. The relative position determines the attention sampling center of the key-value pairs given a query. Continuous and differentiable sliding-window attention sampling is achieved by the proposed BilinearSoftmax. The relative positions are iteratively updated through residual connections across layers by embedding them into the feature channels. Since the relative position is exactly the learning target for cross-view matching, an efficient hierarchical cross-view decoder, MatchDecoder, is designed with MatchAttention as its core component. To handle cross-view occlusions, gated cross-MatchAttention and a consistency-constrained loss are proposed. These two components collectively mitigate the impact of occlusions in both forward and backward passes, allowing the model to focus more on learning matching relationships. When applied to stereo matching, MatchStereo-B ranked 1st in average error on the public Middlebury benchmark and requires only 29ms for KITTI-resolution inference. MatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU memory. The proposed models also achieve state-of-the-art performance on KITTI 2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high accuracy and low computational complexity makes real-time, high-resolution, and high-accuracy cross-view matching possible. Code is available at https://github.com/TingmanYan/MatchAttention.",
    "summary": "",
    "translation": "MatchAttention：通过匹配相对位置实现高分辨率跨视图匹配",
    "relevance_score": 4,
    "reasoning": "该论文提出了一种新的注意力机制用于跨视图匹配，属于Transformer架构的进步。虽然核心关注计算机视觉中的跨视图匹配，但类似的相对位置匹配机制可以应用于推荐系统中处理异构用户行为序列和上下文特征的交互建模，类似于VLM处理多模态数据的方式。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14256v1": {
    "title": "Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.14256v1",
    "arxiv_id": "2510.14256v1",
    "authors": "Xiangyu Meng, Zixian Zhang, Zhenghao Zhang, Junchao Liao, Long Qin, Weizhi Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 03:13:56",
    "ori_summary": "While advanced methods like VACE and Phantom have advanced video generation for specific subjects in diverse scenarios, they struggle with multi-human identity preservation in dynamic interactions, where consistent identities across multiple characters are critical. To address this, we propose Identity-GRPO, a human feedback-driven optimization pipeline for refining multi-human identity-preserving video generation. First, we construct a video reward model trained on a large-scale preference dataset containing human-annotated and synthetic distortion data, with pairwise annotations focused on maintaining human consistency throughout the video. We then employ a GRPO variant tailored for multi-human consistency, which greatly enhances both VACE and Phantom. Through extensive ablation studies, we evaluate the impact of annotation quality and design choices on policy optimization. Experiments show that Identity-GRPO achieves up to 18.9% improvement in human consistency metrics over baseline methods, offering actionable insights for aligning reinforcement learning with personalized video generation.",
    "summary": "",
    "translation": "Identity-GRPO：通过强化学习优化多人身份保持的视频生成",
    "relevance_score": 2,
    "reasoning": "该论文专注于视频生成领域，涉及强化学习优化，但核心应用是身份保持的视频生成，这属于AIGC和内容生成的范畴。虽然使用了强化学习技术，但没有明确的推荐系统、搜索或广告应用场景，主要关注视觉内容的生成而非排名或个性化推荐。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14255v1": {
    "title": "Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.14255v1",
    "arxiv_id": "2510.14255v1",
    "authors": "Liao Shen, Wentao Jiang, Yiran Zhu, Tiezheng Ge, Zhiguo Cao, Bo Zheng",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 03:13:47",
    "ori_summary": "Recent advances in image-to-video (I2V) generation have achieved remarkable progress in synthesizing high-quality, temporally coherent videos from static images. Among all the applications of I2V, human-centric video generation includes a large portion. However, existing I2V models encounter difficulties in maintaining identity consistency between the input human image and the generated video, especially when the person in the video exhibits significant expression changes and movements. This issue becomes critical when the human face occupies merely a small fraction of the image. Since humans are highly sensitive to identity variations, this poses a critical yet under-explored challenge in I2V generation. In this paper, we propose Identity-Preserving Reward-guided Optimization (IPRO), a novel video diffusion framework based on reinforcement learning to enhance identity preservation. Instead of introducing auxiliary modules or altering model architectures, our approach introduces a direct and effective tuning algorithm that optimizes diffusion models using a face identity scorer. To improve performance and accelerate convergence, our method backpropagates the reward signal through the last steps of the sampling chain, enabling richer gradient feedback. We also propose a novel facial scoring mechanism that treats faces in ground-truth videos as facial feature pools, providing multi-angle facial information to enhance generalization. A KL-divergence regularization is further incorporated to stabilize training and prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V model and our in-house I2V model demonstrate the effectiveness of our method. Our project and code are available at \\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.",
    "summary": "",
    "translation": "通过奖励引导优化实现身份保持的图像到视频生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频生成中的身份保持技术，属于计算机视觉领域的AIGC/内容生成范畴。虽然提到了奖励引导优化，但这与推荐系统、搜索或广告的核心技术没有直接关联，也不涉及Transformer架构改进或异构数据统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14251v1": {
    "title": "MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale Scene Localization and Rendering",
    "url": "https://www.alphaxiv.org/abs/2510.14251v1",
    "arxiv_id": "2510.14251v1",
    "authors": "Mingkai Liu, Dikai Fan, Haohua Que, Haojia Gao, Xiao Liu, Shuxue Peng, Meixia Lin, Shengyu Gu, Ruicong Ye, Wanli Qiu, Handong Yao, Ruopeng Zhang, Xianliang Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 03:08:19",
    "ori_summary": "Efficient localization and high-quality rendering in large-scale scenes remain a significant challenge due to the computational cost involved. While Scene Coordinate Regression (SCR) methods perform well in small-scale localization, they are limited by the capacity of a single network when extended to large-scale scenes. To address these challenges, we propose the Mixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables efficient localization and high-quality rendering in large-scale scenes. Inspired by the remarkable capabilities of MOE in large model domains, we introduce a gating network to implicitly classify and select sub-networks, ensuring that only a single sub-network is activated during each inference. Furtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to enhance the localization accuracy on large-scale scene. Our framework provides a significant reduction in costs while maintaining higher precision, offering an efficient solution for large-scale scene applications. Additional experiments on the Cambridge test set demonstrate that our method achieves high-quality rendering results with merely 10 minutes of training.",
    "summary": "",
    "translation": "MACE：用于大规模场景定位与渲染的专家混合加速坐标编码",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的场景定位和渲染任务，属于纯粹的视觉技术领域。虽然提到了Mixture-of-Experts架构，但其应用场景（场景定位和渲染）与推荐系统、搜索或广告的核心技术需求没有直接关联，无法看出在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14245v1": {
    "title": "Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication",
    "url": "https://www.alphaxiv.org/abs/2510.14245v1",
    "arxiv_id": "2510.14245v1",
    "authors": "Miu Sumino, Mayu Ishii, Shun Kaizu, Daisuke Hisano, Yu Nakayama",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 02:56:29",
    "ori_summary": "Optical camera communication (OCC) represents a promising visible light communication technology. Nonetheless, typical OCC systems utilizing frame-based cameras are encumbered by limitations, including low bit rate and high processing load. To address these issues, OCC system utilizing an event-based vision sensor (EVS) as receivers have been proposed. The EVS enables high-speed, low-latency, and robust communication due to its asynchronous operation and high dynamic range. In existing event-based OCC systems, conventional modulation schemes such as on-off keying (OOK) and pulse position modulation have been applied, however, to the best of our knowledge, no modulation method has been proposed that fully exploits the unique characteristics of the EVS. This paper proposes a novel modulation scheme, called the event interval modulation (EIM) scheme, specifically designed for event-based OCC. EIM enables improvement in transmission speed by modulating information using the intervals between events. This paper proposes a theoretical model of EIM and conducts a proof-of-concept experiment. First, the parameters of the EVS are tuned and customized to optimize the frequency response specifically for EIM. Then, the maximum modulation order usable in EIM is determined experimentally. We conduct transmission experiments based on the obtained parameters. Finally, we report successful transmission at 28 kbps over 10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new benchmark for bit rate in event-based OCC systems.",
    "summary": "",
    "translation": "事件间隔调制：一种基于事件的光学相机通信新方案",
    "relevance_score": 1,
    "reasoning": "该论文专注于光学相机通信领域的事件间隔调制技术，属于通信工程和信号处理范畴。与我的关注领域（推荐系统、搜索、广告中的LLM技术、Transformer架构进展或异构数据统一建模）没有任何直接或间接关联，也没有在这些领域应用的潜在可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14244v1": {
    "title": "Reinforcement Learning for Unsupervised Domain Adaptation in Spatio-Temporal Echocardiography Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.14244v1",
    "arxiv_id": "2510.14244v1",
    "authors": "Arnaud Judge, Nicolas Duchateau, Thierry Judge, Roman A. Sandler, Joseph Z. Sokol, Christian Desrosiers, Olivier Bernard, Pierre-Marc Jodoin",
    "categories": "eess.IV, cs.AI, cs.CV",
    "pub_date": "2025-10-16 02:55:04",
    "ori_summary": "Domain adaptation methods aim to bridge the gap between datasets by enabling knowledge transfer across domains, reducing the need for additional expert annotations. However, many approaches struggle with reliability in the target domain, an issue particularly critical in medical image segmentation, where accuracy and anatomical validity are essential. This challenge is further exacerbated in spatio-temporal data, where the lack of temporal consistency can significantly degrade segmentation quality, and particularly in echocardiography, where the presence of artifacts and noise can further hinder segmentation performance. To address these issues, we present RL4Seg3D, an unsupervised domain adaptation framework for 2D + time echocardiography segmentation. RL4Seg3D integrates novel reward functions and a fusion scheme to enhance key landmark precision in its segmentations while processing full-sized input videos. By leveraging reinforcement learning for image segmentation, our approach improves accuracy, anatomical validity, and temporal consistency while also providing, as a beneficial side effect, a robust uncertainty estimator, which can be used at test time to further enhance segmentation performance. We demonstrate the effectiveness of our framework on over 30,000 echocardiographic videos, showing that it outperforms standard domain adaptation techniques without the need for any labels on the target domain. Code is available at https://github.com/arnaudjudge/RL4Seg3D.",
    "summary": "",
    "translation": "基于强化学习的时空超声心动图分割无监督域自适应",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像分割中的强化学习应用，属于明确的医学领域特定应用。虽然涉及强化学习技术，但针对的是超声心动图分割这一与推荐系统、搜索或广告无关的医疗场景，完全落在被排除的医学应用范畴内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  }
}