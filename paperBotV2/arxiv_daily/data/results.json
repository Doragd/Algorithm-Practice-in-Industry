{
  "2510.06999v1": {
    "title": "Towards Reliable Retrieval in RAG Systems for Large Legal Datasets",
    "url": "https://www.alphaxiv.org/abs/2510.06999v1",
    "arxiv_id": "2510.06999v1",
    "authors": "Markus Reuter, Tobias Lingenberg, Rūta Liepiņa, Francesca Lagioia, Marco Lippi, Giovanni Sartor, Andrea Passerini, Burcu Sayin",
    "categories": "cs.CL, cs.IR, I.2.7; H.3.3; K.5.0",
    "pub_date": "2025-10-08 13:22:20",
    "ori_summary": "Retrieval-Augmented Generation (RAG) is a promising approach to mitigate hallucinations in Large Language Models (LLMs) for legal applications, but its reliability is critically dependent on the accuracy of the retrieval step. This is particularly challenging in the legal domain, where large databases of structurally similar documents often cause retrieval systems to fail. In this paper, we address this challenge by first identifying and quantifying a critical failure mode we term Document-Level Retrieval Mismatch (DRM), where the retriever selects information from entirely incorrect source documents. To mitigate DRM, we investigate a simple and computationally efficient technique which we refer to as Summary-Augmented Chunking (SAC). This method enhances each text chunk with a document-level synthetic summary, thereby injecting crucial global context that would otherwise be lost during a standard chunking process. Our experiments on a diverse set of legal information retrieval tasks show that SAC greatly reduces DRM and, consequently, also improves text-level retrieval precision and recall. Interestingly, we find that a generic summarization strategy outperforms an approach that incorporates legal expert domain knowledge to target specific legal elements. Our work provides evidence that this practical, scalable, and easily integrable technique enhances the reliability of RAG systems when applied to large-scale legal document datasets.",
    "summary": "该论文研究法律领域RAG系统中检索不可靠的核心问题，核心创新是提出摘要增强分块方法，通过为文本块添加文档级合成摘要来注入全局上下文信息，从而解决文档级检索不匹配问题。",
    "translation": "面向大型法律数据集的RAG系统可靠检索研究",
    "relevance_score": 8,
    "reasoning": "该论文直接针对检索增强生成(RAG)系统的可靠性问题，这是搜索和推荐系统中的核心技术挑战。RAG系统在搜索和推荐领域有广泛应用，可靠的检索机制对于提升内容相关性、减少错误信息至关重要。论文聚焦大型数据集上的检索可靠性，这一技术进展可直接应用于搜索系统的文档检索和推荐系统的候选生成环节。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对RAG系统的检索可靠性问题，提出增强检索准确性的创新方法，对搜索和推荐系统中的信息检索具有重要参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06987v1": {
    "title": "Spiral Model Technique For Data Science & Machine Learning Lifecycle",
    "url": "https://www.alphaxiv.org/abs/2510.06987v1",
    "arxiv_id": "2510.06987v1",
    "authors": "Rohith Mahadevan",
    "categories": "cs.LG, cs.IR, cs.SE",
    "pub_date": "2025-10-08 13:11:58",
    "ori_summary": "Analytics play an important role in modern business. Companies adapt data science lifecycles to their culture to seek productivity and improve their competitiveness among others. Data science lifecycles are fairly an important contributing factor to start and end a project that are data dependent. Data science and Machine learning life cycles comprises of series of steps that are involved in a project. A typical life cycle states that it is a linear or cyclical model that revolves around. It is mostly depicted that it is possible in a traditional data science life cycle to start the process again after reaching the end of cycle. This paper suggests a new technique to incorporate data science life cycle to business problems that have a clear end goal. A new technique called spiral technique is introduced to emphasize versatility, agility and iterative approach to business processes.",
    "summary": "",
    "translation": "数据科学与机器学习生命周期的螺旋模型技术",
    "relevance_score": 1,
    "reasoning": "该论文标题讨论的是数据科学和机器学习的生命周期管理方法（螺旋模型），这属于通用的MLOps或开发流程主题。它不涉及推荐系统、搜索或广告领域的核心进展，也不涉及LLM技术、Transformer架构改进，或异构数据的统一建模。该主题与我的技术焦点完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06924v1": {
    "title": "Ethical AI prompt recommendations in large language models using collaborative filtering",
    "url": "https://www.alphaxiv.org/abs/2510.06924v1",
    "arxiv_id": "2510.06924v1",
    "authors": "Jordan Nelson, Almas Baimagambetov, Konstantinos Avgerinakis, Nikolaos Polatidis",
    "categories": "cs.IR",
    "pub_date": "2025-10-08 12:03:21",
    "ori_summary": "As large language models (LLMs) shape AI development, ensuring ethical prompt recommendations is crucial. LLMs offer innovation but risk bias, fairness issues, and accountability concerns. Traditional oversight methods struggle with scalability, necessitating dynamic solutions. This paper proposes using collaborative filtering, a technique from recommendation systems, to enhance ethical prompt selection. By leveraging user interactions, it promotes ethical guidelines while reducing bias. Contributions include a synthetic dataset for prompt recommendations and the application of collaborative filtering. The work also tackles challenges in ethical AI, such as bias mitigation, transparency, and preventing unethical prompt engineering.",
    "summary": "",
    "translation": "基于协同过滤的大型语言模型伦理AI提示推荐",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及伦理AI主题，这属于被明确排除的无关主题范畴。虽然提到了协同过滤和LLM技术，但核心焦点是伦理方面的提示推荐，与我的技术导向研究重点无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06888v1": {
    "title": "M3Retrieve: Benchmarking Multimodal Retrieval for Medicine",
    "url": "https://www.alphaxiv.org/abs/2510.06888v1",
    "arxiv_id": "2510.06888v1",
    "authors": "Arkadeep Acharya, Akash Ghosh, Pradeepika Verma, Kitsuchart Pasupa, Sriparna Saha, Priti Singh",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-08 11:08:47",
    "ori_summary": "With the increasing use of RetrievalAugmented Generation (RAG), strong retrieval models have become more important than ever. In healthcare, multimodal retrieval models that combine information from both text and images offer major advantages for many downstream tasks such as question answering, cross-modal retrieval, and multimodal summarization, since medical data often includes both formats. However, there is currently no standard benchmark to evaluate how well these models perform in medical settings. To address this gap, we introduce M3Retrieve, a Multimodal Medical Retrieval Benchmark. M3Retrieve, spans 5 domains,16 medical fields, and 4 distinct tasks, with over 1.2 Million text documents and 164K multimodal queries, all collected under approved licenses. We evaluate leading multimodal retrieval models on this benchmark to explore the challenges specific to different medical specialities and to understand their impact on retrieval performance. By releasing M3Retrieve, we aim to enable systematic evaluation, foster model innovation, and accelerate research toward building more capable and reliable multimodal retrieval systems for medical applications. The dataset and the baselines code are available in this github page https://github.com/AkashGhosh/M3Retrieve.",
    "summary": "",
    "translation": "M3Retrieve：面向医学领域的多模态检索基准",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于医学领域的多模态检索基准，这属于明确的医学领域特定应用，属于无关主题。虽然多模态检索技术本身可能有通用性，但论文明确限定在医学领域，与搜索、推荐、广告等核心关注领域没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06838v1": {
    "title": "Crossing Domains without Labels: Distant Supervision for Term Extraction",
    "url": "https://www.alphaxiv.org/abs/2510.06838v1",
    "arxiv_id": "2510.06838v1",
    "authors": "Elena Senger, Yuri Campbell, Rob van der Goot, Barbara Plank",
    "categories": "cs.IR, cs.CL",
    "pub_date": "2025-10-08 10:02:40",
    "ori_summary": "Automatic Term Extraction (ATE) is a critical component in downstream NLP tasks such as document tagging, ontology construction and patent analysis. Current state-of-the-art methods require expensive human annotation and struggle with domain transfer, limiting their practical deployment. This highlights the need for more robust, scalable solutions and realistic evaluation settings. To address this, we introduce a comprehensive benchmark spanning seven diverse domains, enabling performance evaluation at both the document- and corpus-levels. Furthermore, we propose a robust LLM-based model that outperforms both supervised cross-domain encoder models and few-shot learning baselines and performs competitively with its GPT-4o teacher on this benchmark. The first step of our approach is generating psuedo-labels with this black-box LLM on general and scientific domains to ensure generalizability. Building on this data, we fine-tune the first LLMs for ATE. To further enhance document-level consistency, oftentimes needed for downstream tasks, we introduce lightweight post-hoc heuristics. Our approach exceeds previous approaches on 5/7 domains with an average improvement of 10 percentage points. We release our dataset and fine-tuned models to support future research in this area.",
    "summary": "",
    "translation": "跨领域无标签：术语抽取的远程监督方法",
    "relevance_score": 2,
    "reasoning": "该论文专注于术语抽取的远程监督方法，这属于信息抽取领域，与推荐系统、搜索或广告的核心技术关联较弱。虽然术语抽取在搜索中有潜在应用（如查询理解），但论文主要关注跨领域迁移和无标签学习，缺乏明确的RecSys/Search/Ads应用场景，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06823v1": {
    "title": "Exposing Citation Vulnerabilities in Generative Engines",
    "url": "https://www.alphaxiv.org/abs/2510.06823v1",
    "arxiv_id": "2510.06823v1",
    "authors": "Riku Mochizuki, Shusuke Komatsu, Souta Noguchi, Kazuto Ataka",
    "categories": "cs.CR, cs.CL, cs.IR",
    "pub_date": "2025-10-08 09:47:48",
    "ori_summary": "We analyze answers generated by generative engines (GEs) from the perspectives of citation publishers and the content-injection barrier, defined as the difficulty for attackers to manipulate answers to user prompts by placing malicious content on the web. GEs integrate two functions: web search and answer generation that cites web pages using large language models. Because anyone can publish information on the web, GEs are vulnerable to poisoning attacks. Existing studies of citation evaluation focus on how faithfully answer content reflects cited sources, leaving unexamined which web sources should be selected as citations to defend against poisoning attacks. To fill this gap, we introduce evaluation criteria that assess poisoning threats using the citation information contained in answers. Our criteria classify the publisher attributes of citations to estimate the content-injection barrier thereby revealing the threat of poisoning attacks in current GEs. We conduct experiments in political domains in Japan and the United States (U.S.) using our criteria and show that citations from official party websites (primary sources) are approximately \\(25\\%\\)--\\(45\\%\\) in the U.S. and \\(60\\%\\)--\\(65\\%\\) in Japan, indicating that U.S. political answers are at higher risk of poisoning attacks. We also find that sources with low content-injection barriers are frequently cited yet are poorly reflected in answer content. To mitigate this threat, we discuss how publishers of primary sources can increase exposure of their web content in answers and show that well-known techniques are limited by language differences.",
    "summary": "",
    "translation": "揭示生成式引擎中的引用漏洞",
    "relevance_score": 2,
    "reasoning": "该论文主要关注生成式引擎中的引用漏洞，这属于LLM评估和可信度问题，属于被排除的纯粹NLP中心话题。虽然涉及生成式技术，但焦点是漏洞和安全问题，而非在推荐系统、搜索或广告中的实际应用。没有明确的机制表明这些发现可以转化为推荐、搜索或广告系统的改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06805v1": {
    "title": "Overview of the Plagiarism Detection Task at PAN 2025",
    "url": "https://www.alphaxiv.org/abs/2510.06805v1",
    "arxiv_id": "2510.06805v1",
    "authors": "André Greiner-Petter, Maik Fröbe, Jan Philip Wahle, Terry Ruas, Bela Gipp, Akiko Aizawa, Martin Potthast",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2025-10-08 09:33:26",
    "ori_summary": "The generative plagiarism detection task at PAN 2025 aims at identifying automatically generated textual plagiarism in scientific articles and aligning them with their respective sources. We created a novel large-scale dataset of automatically generated plagiarism using three large language models: Llama, DeepSeek-R1, and Mistral. In this task overview paper, we outline the creation of this dataset, summarize and compare the results of all participants and four baselines, and evaluate the results on the last plagiarism detection task from PAN 2015 in order to interpret the robustness of the proposed approaches. We found that the current iteration does not invite a large variety of approaches as naive semantic similarity approaches based on embedding vectors provide promising results of up to 0.8 recall and 0.5 precision. In contrast, most of these approaches underperform significantly on the 2015 dataset, indicating a lack in generalizability.",
    "summary": "",
    "translation": "PAN 2025抄袭检测任务概览",
    "relevance_score": 1,
    "reasoning": "该论文专注于抄袭检测任务，这属于内容安全与诚信验证领域，与推荐系统、搜索或广告的核心技术进展无关。抄袭检测主要涉及文本相似性分析和内容验证，不涉及用户行为建模、个性化推荐、搜索排序或广告投放等关键技术方向，因此与当前关注点完全不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06732v1": {
    "title": "Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.06732v1",
    "arxiv_id": "2510.06732v1",
    "authors": "Tiancheng Xing, Jerry Li, Yixuan Du, Xiyang Hu",
    "categories": "cs.CL, cs.AI, cs.IR",
    "pub_date": "2025-10-08 07:40:40",
    "ori_summary": "Large language models (LLMs) are increasingly used as rerankers in information retrieval, yet their ranking behavior can be steered by small, natural-sounding prompts. To expose this vulnerability, we present Rank Anything First (RAF), a two-stage token optimization method that crafts concise textual perturbations to consistently promote a target item in LLM-generated rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate Gradient to shortlist candidate tokens at the current position by combining the gradient of the rank-target with a readability score; Stage 2 evaluates those candidates under exact ranking and readability losses using an entropy-based dynamic weighting scheme, and selects a token via temperature-controlled sampling. RAF generates ranking-promoting prompts token-by-token, guided by dual objectives: maximizing ranking effectiveness and preserving linguistic naturalness. Experiments across multiple LLMs show that RAF significantly boosts the rank of target items using naturalistic language, with greater robustness than existing methods in both promoting target items and maintaining naturalness. These findings underscore a critical security implication: LLM-based reranking is inherently susceptible to adversarial manipulation, raising new challenges for the trustworthiness and robustness of modern retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.",
    "summary": "研究LLM作为排序器的可靠性问题，核心方法是通过两阶段令牌优化技术生成自然语言扰动来操纵LLM的排序结果，暴露LLM排序系统的内在脆弱性。",
    "translation": "大型语言模型是可靠的排序器吗？通过两阶段令牌优化实现排序操纵",
    "relevance_score": 8,
    "reasoning": "该论文直接研究LLM在排序任务中的可靠性，这是搜索和推荐系统的核心问题。两阶段令牌优化技术作为LLM排序的增强方法，可以应用于提升搜索相关性排序和推荐列表质量，属于直接LLM应用领域。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接研究LLM在推荐/搜索排序中的安全漏洞，揭示了LLM排序器易受对抗性攻击的关键问题，对构建可信赖的推荐系统具有重要安全意义。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06728v1": {
    "title": "Reproducing and Extending Causal Insights Into Term Frequency Computation in Neural Rankers",
    "url": "https://www.alphaxiv.org/abs/2510.06728v1",
    "arxiv_id": "2510.06728v1",
    "authors": "Cile van Marken, Roxana Petcu",
    "categories": "cs.IR",
    "pub_date": "2025-10-08 07:29:31",
    "ori_summary": "Neural ranking models have shown outstanding performance across a variety of tasks, such as document retrieval, re-ranking, question answering and conversational retrieval. However, the inner decision process of these models remains largely unclear, especially as models increase in size. Most interpretability approaches, such as probing, focus on correlational insights rather than establishing causal relationships. The paper 'Axiomatic Causal Interventions for Reverse Engineering Relevance Computation in Neural Retrieval Models' by Chen et al. addresses this gap by introducing a framework for activation patching - a causal interpretability method - in the information retrieval domain, offering insights into how neural retrieval models compute document relevance. The study demonstrates that neural ranking models not only capture term-frequency information, but also that these representations can be localized to specific components of the model, such as individual attention heads or layers. This paper aims to reproduce the findings by Chen et al. and to further explore the presence of pre-defined retrieval axioms in neural IR models. We validate the main claims made by Chen et al., and extend the framework to include an additional term-frequency axiom, which states that the impact of increasing query term frequency on document ranking diminishes as the frequency becomes higher. We successfully identify a group of attention heads that encode this axiom and analyze their behavior to give insight into the inner decision-making process of neural ranking models.",
    "summary": "",
    "translation": "复现与扩展神经网络排序器中词频计算因果洞察",
    "relevance_score": 7,
    "reasoning": "该论文直接研究神经网络排序器中的词频计算机制，这属于搜索领域的核心算法改进。词频计算是搜索排序的基础组件，对神经排序器中因果关系的深入理解可直接应用于提升搜索相关性排序性能，属于Core Domain Advances范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06658v1": {
    "title": "Can We Hide Machines in the Crowd? Quantifying Equivalence in LLM-in-the-loop Annotation Tasks",
    "url": "https://www.alphaxiv.org/abs/2510.06658v1",
    "arxiv_id": "2510.06658v1",
    "authors": "Jiaman He, Zikang Leng, Dana McKay, Damiano Spina, Johanne R. Trippas",
    "categories": "cs.IR",
    "pub_date": "2025-10-08 05:17:33",
    "ori_summary": "Many evaluations of large language models (LLMs) in text annotation focus primarily on the correctness of the output, typically comparing model-generated labels to human-annotated ``ground truth'' using standard performance metrics. In contrast, our study moves beyond effectiveness alone. We aim to explore how labeling decisions -- by both humans and LLMs -- can be statistically evaluated across individuals. Rather than treating LLMs purely as annotation systems, we approach LLMs as an alternative annotation mechanism that may be capable of mimicking the subjective judgments made by humans. To assess this, we develop a statistical evaluation method based on Krippendorff's $\\alpha$, paired bootstrapping, and the Two One-Sided t-Tests (TOST) equivalence test procedure. This evaluation method tests whether an LLM can blend into a group of human annotators without being distinguishable. We apply this approach to two datasets -- MovieLens 100K and PolitiFact -- and find that the LLM is statistically indistinguishable from a human annotator in the former ($p = 0.004$), but not in the latter ($p = 0.155$), highlighting task-dependent differences. It also enables early evaluation on a small sample of human data to inform whether LLMs are suitable for large-scale annotation in a given application.",
    "summary": "",
    "translation": "我们能否将机器隐藏在人群中？量化LLM在环标注任务中的等价性",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM在数据标注任务中的等价性评估，这属于LLM应用的质量评估范畴。虽然涉及LLM技术，但焦点是标注任务的等价性量化，而非在推荐系统、搜索或广告中的直接应用或架构改进，与当前关注的核心领域相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06657v1": {
    "title": "LLM-Powered Nuanced Video Attribute Annotation for Enhanced Recommendations",
    "url": "https://www.alphaxiv.org/abs/2510.06657v1",
    "arxiv_id": "2510.06657v1",
    "authors": "Boyuan Long, Yueqi Wang, Hiloni Mehta, Mick Zomnir, Omkar Pathak, Changping Meng, Ruolin Jia, Yajun Peng, Dapeng Hong, Xia Wu, Mingyan Gao, Onkar Dalal, Ningren Han",
    "categories": "cs.IR",
    "pub_date": "2025-10-08 05:17:17",
    "ori_summary": "This paper presents a case study on deploying Large Language Models (LLMs) as an advanced \"annotation\" mechanism to achieve nuanced content understanding (e.g., discerning content \"vibe\") at scale within a large-scale industrial short-form video recommendation system. Traditional machine learning classifiers for content understanding face protracted development cycles and a lack of deep, nuanced comprehension. The \"LLM-as-annotators\" approach addresses these by significantly shortening development times and enabling the annotation of subtle attributes. This work details an end-to-end workflow encompassing: (1) iterative definition and robust evaluation of target attributes, refined by offline metrics and online A/B testing; (2) scalable offline bulk annotation of video corpora using LLMs with multimodal features, optimized inference, and knowledge distillation for broad application; and (3) integration of these rich annotations into the online recommendation serving system, for example, through personalized restrict retrieval. Experimental results demonstrate the efficacy of this approach, with LLMs outperforming human raters in offline annotation quality for nuanced attributes and yielding significant improvements of user participation and satisfied consumption in online A/B tests. The study provides insights into designing and scaling production-level LLM pipelines for rich content evaluation, highlighting the adaptability and benefits of LLM-generated nuanced understanding for enhancing content discovery, user satisfaction, and the overall effectiveness of modern recommendation systems.",
    "summary": "论文研究如何解决推荐系统中传统内容理解方法开发周期长、缺乏深度理解的问题。核心方法是采用LLM作为标注器，构建端到端工作流程实现大规模细粒度视频属性标注，并将这些丰富标注集成到在线推荐系统中。",
    "translation": "基于大语言模型的细粒度视频属性标注用于增强推荐系统",
    "relevance_score": 8,
    "reasoning": "该论文直接应用LLM技术进行视频属性标注，属于'Direct LLM Applications'范畴，通过改进内容理解来增强推荐系统。细粒度属性标注可以显著提升视频推荐的相关性和个性化程度，这是搜索和推荐系统的核心改进方向。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用LLM技术解决推荐系统中的内容理解难题，通过LLM作为标注器实现细粒度内容属性标注，完全符合直接LLM应用和核心领域进展的关注点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07318v1": {
    "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
    "url": "https://www.alphaxiv.org/abs/2510.07318v1",
    "arxiv_id": "2510.07318v1",
    "authors": "Yunhao Fang, Weihao Yu, Shu Zhong, Qinghao Ye, Xuehan Xiong, Lai Wei",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-08 17:59:55",
    "ori_summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.",
    "summary": "论文研究长序列建模中效率与精度的根本权衡问题，核心思想是结合Transformer的KV缓存作为无损短期记忆与可学习的人工海马体网络压缩长期记忆的混合记忆框架。",
    "translation": "人工海马体网络用于高效长上下文建模",
    "relevance_score": 8,
    "reasoning": "该论文涉及高效长上下文建模，这直接属于'使能Transformer技术'范畴，关注Transformer架构的效率改进。在推荐系统、搜索和广告中，高效处理长用户序列、历史行为和上下文特征至关重要，这种技术可以显著提升长序列建模的效率和效果。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对Transformer架构的效率改进，提出混合记忆框架解决长序列建模的核心瓶颈，与Enabling Transformer Tech高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07315v1": {
    "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
    "url": "https://www.alphaxiv.org/abs/2510.07315v1",
    "arxiv_id": "2510.07315v1",
    "authors": "Ming Zhong, Xiang Zhou, Ting-Yun Chang, Qingze Wang, Nan Xu, Xiance Si, Dan Garrette, Shyam Upadhyay, Jeremiah Liu, Jiawei Han, Benoit Schillings, Jiao Sun",
    "categories": "cs.CL, cs.AI, cs.LG, cs.SE",
    "pub_date": "2025-10-08 17:59:19",
    "ori_summary": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. In this paper, we hypothesize that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify models' code instruction following capabilities with measurable signals, we present VeriCode, a taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. We use the taxonomy to augment established evaluation suites, resulting in Vibe Checker, a testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, we show that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, a composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. Our work identifies core factors of the vibe check, providing a concrete path for benchmarking and developing models that better align with user preferences in coding.",
    "summary": "",
    "translation": "Vibe Checker：将代码评估与人类偏好对齐",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于代码评估和人类偏好对齐，这属于编程辅助和代码质量评估领域，与推荐系统、搜索或广告的核心技术完全无关。虽然涉及偏好对齐概念，但应用场景仅限于代码开发，没有任何与RecSys/Search/Ads相关的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07309v1": {
    "title": "Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain",
    "url": "https://www.alphaxiv.org/abs/2510.07309v1",
    "arxiv_id": "2510.07309v1",
    "authors": "Yue Li, Ran Tao, Derek Hommel, Yusuf Denizay Dönder, Sungyong Chang, David Mimno, Unso Eun Seo Jo",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 17:57:35",
    "ori_summary": "In the business domain, where data-driven decision making is crucial, text-to-SQL is fundamental for easy natural language access to structured data. While recent LLMs have achieved strong performance in code generation, existing text-to-SQL benchmarks remain focused on factual retrieval of past records. We introduce CORGI, a new benchmark specifically designed for real-world business contexts. CORGI is composed of synthetic databases inspired by enterprises such as Doordash, Airbnb, and Lululemon. It provides questions across four increasingly complex categories of business queries: descriptive, explanatory, predictive, and recommendational. This challenge calls for causal reasoning, temporal forecasting, and strategic recommendation, reflecting multi-level and multi-step agentic intelligence. We find that LLM performance drops on high-level questions, struggling to make accurate predictions and offer actionable plans. Based on execution success rate, the CORGI benchmark is about 21\\% more difficult than the BIRD benchmark. This highlights the gap between popular LLMs and the need for real-world business intelligence. We release a public dataset and evaluation framework, and a website for public submissions.",
    "summary": "",
    "translation": "Agent Bain vs. Agent McKinsey：面向商业领域的新型文本转SQL基准测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文本到SQL转换的基准测试，属于数据库查询领域的特定应用。虽然SQL查询与搜索系统有一定关联，但该基准专注于商业咨询领域的特定场景，与推荐系统、搜索排名或广告的核心技术关联度较低。文本到SQL技术可能间接应用于某些搜索场景，但论文焦点是基准测试而非直接的技术创新应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07300v1": {
    "title": "Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.07300v1",
    "arxiv_id": "2510.07300v1",
    "authors": "Xue Zhang, Yunlong Liang, Fandong Meng, Songming Zhang, Kaiyu Huang, Yufeng Chen, Jinan Xu, Jie Zhou",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 17:55:02",
    "ori_summary": "Large Reasoning Models (LRMs) have achieved remarkable performance on complex reasoning tasks by adopting the \"think-then-answer\" paradigm, which enhances both accuracy and interpretability. However, current LRMs exhibit two critical limitations when processing non-English languages: (1) They often struggle to maintain input-output language consistency; (2) They generally perform poorly with wrong reasoning paths and lower answer accuracy compared to English. These limitations significantly degrade the user experience for non-English speakers and hinder the global deployment of LRMs. To address these limitations, we propose M-Thinker, which is trained by the GRPO algorithm that involves a Language Consistency (LC) reward and a novel Cross-lingual Thinking Alignment (CTA) reward. Specifically, the LC reward defines a strict constraint on the language consistency between the input, thought, and answer. Besides, the CTA reward compares the model's non-English reasoning paths with its English reasoning path to transfer its own reasoning capability from English to non-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B models not only achieve nearly 100% language consistency and superior performance on two multilingual benchmarks (MMATH and PolyMath), but also exhibit excellent generalization on out-of-domain languages.",
    "summary": "",
    "translation": "原生思考：通过一致性增强强化学习解锁多语言推理能力",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多语言推理和强化学习的一致性增强技术，这属于纯粹的NLP领域研究。虽然强化学习技术本身可能具有通用性，但论文标题没有显示出与推荐系统、搜索或广告领域的直接关联或潜在应用，且多语言推理主要针对语言理解而非工业级推荐/搜索系统。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07293v1": {
    "title": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.07293v1",
    "arxiv_id": "2510.07293v1",
    "authors": "Peize He, Zichen Wen, Yubo Wang, Yuxuan Wang, Xiaoqian Liu, Jiajie Huang, Zehui Lei, Zhuangcheng Gu, Xiangqi Jin, Jiabing Yang, Kai Li, Zhifei Liu, Weijia Li, Cunxiang Wang, Conghui He, Linfeng Zhang",
    "categories": "cs.SD, cs.AI, cs.CL, eess.AS",
    "pub_date": "2025-10-08 17:50:16",
    "ori_summary": "Processing long-form audio is a major challenge for Large Audio Language models (LALMs). These models struggle with the quadratic cost of attention ($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio benchmarks are built mostly from short clips and do not evaluate models in realistic long context settings. To address this gap, we introduce AudioMarathon, a benchmark designed to evaluate both understanding and inference efficiency on long-form audio. AudioMarathon provides a diverse set of tasks built upon three pillars: long-context audio inputs with durations ranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of 2,250 to 7,500 audio tokens, respectively, full domain coverage across speech, sound, and music, and complex reasoning that requires multi-hop inference. We evaluate state-of-the-art LALMs and observe clear performance drops as audio length grows. We also study acceleration techniques and analyze the trade-offs of token pruning and KV cache eviction. The results show large gaps across current LALMs and highlight the need for better temporal reasoning and memory-efficient architectures. We believe AudioMarathon will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.",
    "summary": "",
    "translation": "AudioMarathon：面向音频大语言模型的长上下文音频理解与效率的综合基准",
    "relevance_score": 2,
    "reasoning": "该论文主要关注音频领域的基准测试和长上下文理解，属于特定模态（音频）的评估工作。虽然涉及LLM效率，但其核心应用场景是音频理解而非推荐系统、搜索或广告领域。论文没有展示与异构数据建模或推荐/搜索应用的明确联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07290v1": {
    "title": "On the Convergence of Moral Self-Correction in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07290v1",
    "arxiv_id": "2510.07290v1",
    "authors": "Guangliang Liu, Haitao Mao, Bochuan Cao, Zhiyu Xue, Xitong Zhang, Rongrong Wang, Kristen Marie Johnson",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-08 17:46:27",
    "ori_summary": "Large Language Models (LLMs) are able to improve their responses when instructed to do so, a capability known as self-correction. When instructions provide only a general and abstract goal without specific details about potential issues in the response, LLMs must rely on their internal knowledge to improve response quality, a process referred to as intrinsic self-correction. The empirical success of intrinsic self-correction is evident in various applications, but how and why it is effective remains unknown. Focusing on moral self-correction in LLMs, we reveal a key characteristic of intrinsic self-correction: performance convergence through multi-round interactions; and provide a mechanistic analysis of this convergence behavior. Based on our experimental results and analysis, we uncover the underlying mechanism of convergence: consistently injected self-correction instructions activate moral concepts that reduce model uncertainty, leading to converged performance as the activated moral concepts stabilize over successive rounds. This paper demonstrates the strong potential of moral self-correction by showing that it exhibits a desirable property of converged performance.",
    "summary": "",
    "translation": "论大型语言模型中道德自我修正的收敛性",
    "relevance_score": 1,
    "reasoning": "该论文关注LLM的道德自我修正和收敛性，这属于伦理、对齐和安全范畴，属于明确的无关主题。论文内容与推荐系统、搜索或广告的核心技术进展、LLM使能技术或直接应用完全无关，没有任何潜在的技术应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07284v1": {
    "title": "Online Rubrics Elicitation from Pairwise Comparisons",
    "url": "https://www.alphaxiv.org/abs/2510.07284v1",
    "arxiv_id": "2510.07284v1",
    "authors": "MohammadHossein Rezaei, Robert Vacareanu, Zihao Wang, Clinton Wang, Yunzhong He, Afra Feyza Akyürek",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-08 17:44:59",
    "ori_summary": "Rubrics provide a flexible way to train LLMs on open-ended long-form answers where verifiable rewards are not applicable and human preferences provide coarse signals. Prior work shows that reinforcement learning with rubric-based rewards leads to consistent gains in LLM post-training. Most existing approaches rely on rubrics that remain static over the course of training. Such static rubrics, however, are vulnerable to reward-hacking type behaviors and fail to capture emergent desiderata that arise during training. We introduce Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates evaluation criteria in an online manner through pairwise comparisons of responses from current and reference policies. This online process enables continuous identification and mitigation of errors as training proceeds. Empirically, this approach yields consistent improvements of up to 8% over training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as well as the validation sets of expert questions and rubrics. We qualitatively analyze the elicited criteria and identify prominent themes such as transparency, practicality, organization, and reasoning.",
    "summary": "",
    "translation": "基于成对比较的在线评分标准获取",
    "relevance_score": 2,
    "reasoning": "该论文主要关注从成对比较中获取评分标准，这属于偏好学习领域，与推荐系统中的用户偏好建模有一定关联。然而，论文标题未明确表明与LLM、Transformer架构或搜索/广告系统的直接联系，且在线评分标准获取本身更偏向通用机器学习方法而非特定于推荐/搜索/广告领域的核心技术突破。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07248v1": {
    "title": "Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models",
    "url": "https://www.alphaxiv.org/abs/2510.07248v1",
    "arxiv_id": "2510.07248v1",
    "authors": "Jonggeun Lee, Woojung Song, Jongwook Han, Haesung Pyun, Yohan Jo",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 17:16:07",
    "ori_summary": "Small language models (SLMs) offer significant computational advantages for tool-augmented AI systems, yet they struggle with tool-use tasks, particularly in selecting appropriate tools and identifying correct parameters. A common failure mode is schema misalignment: models hallucinate plausible but non-existent tool names that reflect naming conventions internalized during pretraining but absent from the provided tool schema. Rather than forcing models to adapt to arbitrary schemas, we propose adapting schemas to align with models' pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool Schema Generation), a training-free method that leverages peakedness-a signal from contamination detection indicating pretraining familiarity-to automatically rename tool components. By generating multiple candidates and selecting those with highest output concentration across samples, PA-Tool identifies pretrain-aligned naming patterns. Experiments on MetaTool and RoTBench show improvements of up to 17% points, with schema misalignment errors reduced by 80%. PA-Tool enables small models to approach state-of-the-art performance while maintaining computational efficiency for adaptation to new tools without retraining. Our work demonstrates that schema-level interventions can unlock the tool-use potential of resource-efficient models by adapting schemas to models rather than models to schemas.",
    "summary": "论文研究小语言模型在工具使用中的模式对齐问题，核心思想是通过分析预训练熟悉度自动重命名工具组件，使工具模式适配模型知识而非强制模型适应任意模式。",
    "translation": "不要为工具适配小型语言模型；将工具模式适配到模型",
    "relevance_score": 8,
    "reasoning": "这篇论文涉及工具使用和模型适配，这直接适用于搜索和推荐系统中的LLM应用，其中模型需要与外部工具和API交互。将工具模式适配到较小模型的方法可以显著提高搜索和推荐系统中工具集成LLM的效率和可扩展性。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出通过适配工具模式而非模型来解决SLM工具使用问题，这种模式对齐方法对推荐系统和搜索中的工具集成具有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07243v1": {
    "title": "LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.07243v1",
    "arxiv_id": "2510.07243v1",
    "authors": "Joseph Enguehard, Morgane Van Ermengem, Kate Atkinson, Sujeong Cha, Arijit Ghosh Chowdhury, Prashanth Kallur Ramaswamy, Jeremy Roghair, Hannah R Marlowe, Carina Suzana Negreanu, Kitty Boxall, Diana Mincu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 17:10:47",
    "ori_summary": "Evaluating large language model (LLM) outputs in the legal domain presents unique challenges due to the complex and nuanced nature of legal analysis. Current evaluation approaches either depend on reference data, which is costly to produce, or use standardized assessment methods, both of which have significant limitations for legal applications. Although LLM-as-a-Judge has emerged as a promising evaluation technique, its reliability and effectiveness in legal contexts depend heavily on evaluation processes unique to the legal industry and how trustworthy the evaluation appears to the human legal expert. This is where existing evaluation methods currently fail and exhibit considerable variability. This paper aims to close the gap: a) we break down lengthy responses into 'Legal Data Points' (LDPs), self-contained units of information, and introduce a novel, reference-free evaluation methodology that reflects how lawyers evaluate legal answers; b) we demonstrate that our method outperforms a variety of baselines on both our proprietary dataset and an open-source dataset (LegalBench); c) we show how our method correlates more closely with human expert evaluations and helps improve inter-annotator agreement; and finally d) we open source our Legal Data Points for a subset of LegalBench used in our experiments, allowing the research community to replicate our results and advance research in this vital area of LLM evaluation on legal question-answering.",
    "summary": "",
    "translation": "LeMAJ（法律大语言模型作为法官）：连接法律推理与LLM评估",
    "relevance_score": 2,
    "reasoning": "该论文专注于法律领域的LLM应用和评估，属于特定领域应用，与推荐系统、搜索或广告的核心技术进展无关。虽然涉及LLM评估，但这是法律推理场景下的特定评估，而非通用的RecSys/Search/Ads相关技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07242v1": {
    "title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense",
    "url": "https://www.alphaxiv.org/abs/2510.07242v1",
    "arxiv_id": "2510.07242v1",
    "authors": "Leitian Tao, Ilia Kulikov, Swarnadeep Saha, Tianlu Wang, Jing Xu, Yixuan Li, Jason E Weston, Ping Yu",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-08 17:09:41",
    "ori_summary": "Post-training for reasoning of large language models (LLMs) increasingly relies on verifiable rewards: deterministic checkers that provide 0-1 correctness signals. While reliable, such binary feedback is brittle--many tasks admit partially correct or alternative answers that verifiers under-credit, and the resulting all-or-nothing supervision limits learning. Reward models offer richer, continuous feedback, which can serve as a complementary supervisory signal to verifiers. We introduce HERO (Hybrid Ensemble Reward Optimization), a reinforcement learning framework that integrates verifier signals with reward-model scores in a structured way. HERO employs stratified normalization to bound reward-model scores within verifier-defined groups, preserving correctness while refining quality distinctions, and variance-aware weighting to emphasize challenging prompts where dense signals matter most. Across diverse mathematical reasoning benchmarks, HERO consistently outperforms RM-only and verifier-only baselines, with strong gains on both verifiable and hard-to-verify tasks. Our results show that hybrid reward design retains the stability of verifiers while leveraging the nuance of reward models to advance reasoning.",
    "summary": "",
    "translation": "混合强化学习：当奖励稀疏时，密集化策略更优",
    "relevance_score": 2,
    "reasoning": "该论文主要关注强化学习中的奖励稀疏性问题，属于纯粹的强化学习技术研究。虽然强化学习在推荐系统和广告中有应用，但论文标题没有明确指向这些领域的具体应用场景，也没有涉及LLM、Transformer架构或异构数据建模等当前关注的核心技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07239v1": {
    "title": "Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts",
    "url": "https://www.alphaxiv.org/abs/2510.07239v1",
    "arxiv_id": "2510.07239v1",
    "authors": "Christos Ziakas, Nicholas Loo, Nishita Jain, Alessandra Russo",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 17:06:20",
    "ori_summary": "Automated red-teaming has emerged as a scalable approach for auditing Large Language Models (LLMs) prior to deployment, yet existing approaches lack mechanisms to efficiently adapt to model-specific vulnerabilities at inference. We introduce Red-Bandit, a red-teaming framework that adapts online to identify and exploit model failure modes under distinct attack styles (e.g., manipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA experts, each specialized for a particular attack style, using reinforcement learning that rewards the generation of unsafe prompts via a rule-based safety model. At inference, a multi-armed bandit policy dynamically selects among these attack-style experts based on the target model's response safety, balancing exploration and exploitation. Red-Bandit achieves state-of-the-art results on AdvBench under sufficient exploration (ASR@10), while producing more human-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy serves as a diagnostic tool for uncovering model-specific vulnerabilities by indicating which attack styles most effectively elicit unsafe behaviors.",
    "summary": "",
    "translation": "红队-强盗：通过强盗引导的LoRA专家进行LLM红队测试时自适应",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM红队测试（安全测试）和测试时自适应，这属于安全评估领域，与我的核心关注点（推荐系统、搜索、广告）无关。虽然涉及LoRA技术，但其应用方向是安全测试而非推荐/搜索/广告系统的改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07238v1": {
    "title": "When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.07238v1",
    "arxiv_id": "2510.07238v1",
    "authors": "Xunyi Jiang, Dingyi Chang, Julian McAuley, Xin Xu",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 17:06:07",
    "ori_summary": "The rapid evolution of large language models (LLMs) and the real world has outpaced the static nature of widely used evaluation benchmarks, raising concerns about their reliability for evaluating LLM factuality. While substantial works continue to rely on the popular but old benchmarks, their temporal misalignment with real-world facts and modern LLMs, and their effects on LLM factuality evaluation remain underexplored. Therefore, in this work, we present a systematic investigation of this issue by examining five popular factuality benchmarks and eight LLMs released across different years. An up-to-date fact retrieval pipeline and three metrics are tailored to quantify benchmark aging and its impact on LLM factuality evaluation. Experimental results and analysis illustrate that a considerable portion of samples in the widely used factuality benchmarks are outdated, leading to unreliable assessments of LLM factuality. We hope our work can provide a testbed to assess the reliability of a benchmark for LLM factuality evaluation and inspire more research on the benchmark aging issue. Codes are available in https://github.com/JiangXunyi/BenchAge.",
    "summary": "",
    "translation": "当基准过时：通过大语言模型事实性评估揭示的时间错位",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM事实性评估和基准时效性问题，这属于纯粹的NLP评估基准范畴。论文内容涉及幻觉、评估基准等明确被列为不相关的主题，与推荐系统、搜索或广告的核心技术进展没有任何直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07233v1": {
    "title": "LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.07233v1",
    "arxiv_id": "2510.07233v1",
    "authors": "Zhivar Sourati, Zheng Wang, Marianne Menglin Liu, Yazhe Hu, Mengqing Guo, Sujeeth Bharadwaj, Kyu Han, Tao Sheng, Sujith Ravi, Morteza Dehghani, Dan Roth",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 17:02:04",
    "ori_summary": "Question answering over visually rich documents (VRDs) requires reasoning not only over isolated content but also over documents' structural organization and cross-page dependencies. However, conventional retrieval-augmented generation (RAG) methods encode content in isolated chunks during ingestion, losing structural and cross-page dependencies, and retrieve a fixed number of pages at inference, regardless of the specific demands of the question or context. This often results in incomplete evidence retrieval and degraded answer quality for multi-page reasoning tasks. To address these limitations, we propose LAD-RAG, a novel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs a symbolic document graph that captures layout structure and cross-page dependencies, adding it alongside standard neural embeddings to yield a more holistic representation of the document. During inference, an LLM agent dynamically interacts with the neural and symbolic indices to adaptively retrieve the necessary evidence based on the query. Experiments on MMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG improves retrieval, achieving over 90% perfect recall on average without any top-k tuning, and outperforming baseline retrievers by up to 20% in recall at comparable noise levels, yielding higher QA accuracy with minimal latency.",
    "summary": "",
    "translation": "LAD-RAG：面向视觉丰富文档理解的布局感知动态检索增强生成",
    "relevance_score": 3,
    "reasoning": "该论文虽然涉及检索增强生成（RAG）技术，但其核心关注点是视觉丰富文档理解，这主要属于文档分析和视觉语言处理领域。对于搜索系统，布局感知的文档理解可能有潜在应用价值，比如改进文档搜索中的内容提取和相关性匹配，但该技术更偏向文档分析而非核心的推荐/搜索/广告排序问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07231v1": {
    "title": "Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships",
    "url": "https://www.alphaxiv.org/abs/2510.07231v1",
    "arxiv_id": "2510.07231v1",
    "authors": "Donggyu Lee, Sungwon Park, Yerin Hwang, Hyunwoo Oh, Hyoshin Kim, Jungwon Kim, Meeyoung Cha, Sangyoon Park, Jihee Kim",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 17:00:49",
    "ori_summary": "Causal reasoning is fundamental for Large Language Models (LLMs) to understand genuine cause-and-effect relationships beyond pattern matching. Existing benchmarks suffer from critical limitations such as reliance on synthetic data and narrow domain coverage. We introduce a novel benchmark constructed from casually identified relationships extracted from top-tier economics and finance journals, drawing on rigorous methodologies including instrumental variables, difference-in-differences, and regression discontinuity designs. Our benchmark comprises 40,379 evaluation items covering five task types across domains such as health, environment, technology, law, and culture. Experimental results on eight state-of-the-art LLMs reveal substantial limitations, with the best model achieving only 57.6\\% accuracy. Moreover, model scale does not consistently translate to superior performance, and even advanced reasoning models struggle with fundamental causal relationship identification. These findings underscore a critical gap between current LLM capabilities and demands of reliable causal reasoning in high-stakes applications.",
    "summary": "",
    "translation": "基于科学验证关系对大型语言模型因果推理能力进行基准测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM因果推理能力的基准测试和评估，这属于纯粹的NLP评估基准范畴，与我的核心关注点无关。虽然因果推理在推荐和搜索中有潜在应用，但论文的重点是基准测试而非实际应用或架构改进，因此相关性很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07230v1": {
    "title": "Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping",
    "url": "https://www.alphaxiv.org/abs/2510.07230v1",
    "arxiv_id": "2510.07230v1",
    "authors": "Ziyi Wang, Yuxuan Lu, Yimeng Zhang, Jing Huang, Dakuo Wang",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 17:00:25",
    "ori_summary": "Simulating step-wise human behavior with Large Language Models (LLMs) has become an emerging research direction, enabling applications in various practical domains. While prior methods, including prompting, supervised fine-tuning (SFT), and reinforcement learning (RL), have shown promise in modeling step-wise behavior, they primarily learn a population-level policy without conditioning on a user's persona, yielding generic rather than personalized simulations. In this work, we pose a critical question: how can LLM agents better simulate personalized user behavior? We introduce Customer-R1, an RL-based method for personalized, step-wise user behavior simulation in online shopping environments. Our policy is conditioned on an explicit persona, and we optimize next-step rationale and action generation via action correctness reward signals. Experiments on the OPeRA dataset emonstrate that Customer-R1 not only significantly outperforms prompting and SFT-based baselines in next-action prediction tasks, but also better matches users' action distribution, indicating higher fidelity in personalized behavior simulation.",
    "summary": "论文研究如何让LLM代理更好地模拟个性化用户行为。核心方法是提出Customer-R1，通过基于明确用户画像的强化学习策略，优化下一步推理和动作生成，实现个性化逐步行为模拟。",
    "translation": "Customer-R1：基于强化学习的LLM智能体在在线购物中实现个性化人类行为模拟",
    "relevance_score": 9,
    "reasoning": "该论文直接应用LLM技术构建个性化智能体来模拟在线购物行为，属于Direct LLM Applications范畴。RL-based LLM Agent方法可以应用于推荐系统和搜索场景中的用户行为建模与仿真，为系统优化和A/B测试提供高效工具。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对个性化用户行为模拟这一推荐系统核心问题，提出基于强化学习的LLM代理方法，完全符合个性化推荐和LLM应用的研究方向。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07227v1": {
    "title": "Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.07227v1",
    "arxiv_id": "2510.07227v1",
    "authors": "Arjun Krishnakumar, Rhea Sanjay Sukthanker, Hannan Javed Mahadik, Gabriela Kadlecová, Vladyslav Moroshan, Timur Carstensen, Frank Hutter, Aaron Klein",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 16:57:46",
    "ori_summary": "Small Language models (SLMs) offer an efficient and accessible alternative to Large Language Models (LLMs), delivering strong performance while using far fewer resources. We introduce a simple and effective framework for pretraining SLMs that brings together three complementary ideas. First, we identify structurally sparse sub-network initializations that consistently outperform randomly initialized models of similar size under the same compute budget. Second, we use evolutionary search to automatically discover high-quality sub-network initializations, providing better starting points for pretraining. Third, we apply knowledge distillation from larger teacher models to speed up training and improve generalization. Together, these components make SLM pretraining substantially more efficient: our best model, discovered using evolutionary search and initialized with LLM weights, matches the validation perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining tokens. We release all code and models at https://github.com/whittle-org/whittle/, offering a practical and reproducible path toward cost-efficient small language model development at scale.",
    "summary": "该论文研究如何高效预训练小语言模型。核心方法是结合结构稀疏子网络初始化、进化搜索发现优质初始化点以及知识蒸馏技术，构建更高效的预训练框架。",
    "translation": "从何处开始：通过子网络选择与蒸馏实现高效预训练",
    "relevance_score": 8,
    "reasoning": "该论文聚焦于高效预训练方法，这属于'使能LLM技术'范畴，通过子网络选择和蒸馏技术可以显著降低LLM预训练的计算成本和资源需求。在推荐系统、搜索和广告领域，这种高效预训练技术能够使企业以更低的成本训练更大规模的模型，或实现更频繁的模型更新迭代，从而提升系统性能。",
    "rerank_relevance_score": 6,
    "rerank_reasoning": "该论文聚焦小语言模型高效预训练方法，通过子网络选择和蒸馏技术提升训练效率，虽然不直接针对推荐系统，但其模型压缩和高效训练技术对推荐系统模型优化有潜在应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07226v1": {
    "title": "Machines in the Crowd? Measuring the Footprint of Machine-Generated Text on Reddit",
    "url": "https://www.alphaxiv.org/abs/2510.07226v1",
    "arxiv_id": "2510.07226v1",
    "authors": "Lucio La Cava, Luca Maria Aiello, Andrea Tagarelli",
    "categories": "cs.SI, cs.CL, cs.CY, physics.soc-ph",
    "pub_date": "2025-10-08 16:57:45",
    "ori_summary": "Generative Artificial Intelligence is reshaping online communication by enabling large-scale production of Machine-Generated Text (MGT) at low cost. While its presence is rapidly growing across the Web, little is known about how MGT integrates into social media environments. In this paper, we present the first large-scale characterization of MGT on Reddit. Using a state-of-the-art statistical method for detection of MGT, we analyze over two years of activity (2022-2024) across 51 subreddits representative of Reddit's main community types such as information seeking, social support, and discussion. We study the concentration of MGT across communities and over time, and compared MGT to human-authored text in terms of social signals it expresses and engagement it receives. Our very conservative estimate of MGT prevalence indicates that synthetic text is marginally present on Reddit, but it can reach peaks of up to 9% in some communities in some months. MGT is unevenly distributed across communities, more prevalent in subreddits focused on technical knowledge and social support, and often concentrated in the activity of a small fraction of users. MGT also conveys distinct social signals of warmth and status giving typical of language of AI assistants. Despite these stylistic differences, MGT achieves engagement levels comparable than human-authored content and in a few cases even higher, suggesting that AI-generated text is becoming an organic component of online social discourse. This work offers the first perspective on the MGT footprint on Reddit, paving the way for new investigations involving platform governance, detection strategies, and community dynamics.",
    "summary": "",
    "translation": "人群中的机器？测量机器生成文本在Reddit上的足迹",
    "relevance_score": 2,
    "reasoning": "该论文主要关注机器生成文本的检测和测量，这属于内容生成和真实性验证领域。虽然涉及社交媒体平台，但核心焦点是检测生成内容而非推荐、搜索或广告系统的改进，与我的技术焦点重叠有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07221v1": {
    "title": "How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu",
    "url": "https://www.alphaxiv.org/abs/2510.07221v1",
    "arxiv_id": "2510.07221v1",
    "authors": "Benjamin Akera, Evelyn Nafula, Patrick Walukagga, Gilbert Yiga, John Quinn, Ernest Mwebaze",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 16:55:28",
    "ori_summary": "The development of Automatic Speech Recognition (ASR) systems for low-resource African languages remains challenging due to limited transcribed speech data. While recent advances in large multilingual models like OpenAI's Whisper offer promising pathways for low-resource ASR development, critical questions persist regarding practical deployment requirements. This paper addresses two fundamental concerns for practitioners: determining the minimum data volumes needed for viable performance and characterizing the primary failure modes that emerge in production systems. We evaluate Whisper's performance through comprehensive experiments on two Bantu languages: systematic data scaling analysis on Kinyarwanda using training sets from 1 to 1,400 hours, and detailed error characterization on Kikuyu using 270 hours of training data. Our scaling experiments demonstrate that practical ASR performance (WER < 13\\%) becomes achievable with as little as 50 hours of training data, with substantial improvements continuing through 200 hours (WER < 10\\%). Complementing these volume-focused findings, our error analysis reveals that data quality issues, particularly noisy ground truth transcriptions, account for 38.6\\% of high-error cases, indicating that careful data curation is as critical as data volume for robust system performance. These results provide actionable benchmarks and deployment guidance for teams developing ASR systems across similar low-resource language contexts. We release accompanying and models see https://github.com/SunbirdAI/kinyarwanda-whisper-eval",
    "summary": "",
    "translation": "非洲语言自动语音识别需要多少语音数据？基尼亚卢旺达语和基库尤语中数据规模扩展的评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动语音识别（ASR）在非洲语言中的研究，属于语音处理领域。虽然涉及数据扩展评估，但核心关注点是语音识别而非推荐系统、搜索或广告应用，与当前关注的LLM技术、推荐系统架构或Transformer改进等焦点领域无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07213v1": {
    "title": "Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07213v1",
    "arxiv_id": "2510.07213v1",
    "authors": "Chengzhi Zhong, Fei Cheng, Qianying Liu, Yugo Murawaki, Chenhui Chu, Sadao Kurohashi",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 16:46:57",
    "ori_summary": "Large language models exhibit strong multilingual capabilities despite limited exposure to non-English data. Prior studies show that English-centric large language models map multilingual content into English-aligned representations at intermediate layers and then project them back into target-language token spaces in the final layer. From this observation, we hypothesize that this cross-lingual transition is governed by a small and sparse set of dimensions, which occur at consistent indices across the intermediate to final layers. Building on this insight, we introduce a simple, training-free method to identify and manipulate these dimensions, requiring only as few as 50 sentences of either parallel or monolingual data. Experiments on a multilingual generation control task reveal the interpretability of these dimensions, demonstrating that the interventions in these dimensions can switch the output language while preserving semantic content, and that it surpasses the performance of prior neuron-based approaches at a substantially lower cost.",
    "summary": "",
    "translation": "语言存在于稀疏维度：面向大语言模型的可解释与高效多语言控制",
    "relevance_score": 6,
    "reasoning": "该论文聚焦于LLM的多语言控制与稀疏维度表征，属于'Enabling LLM Tech'范畴。稀疏控制机制可应用于多语言搜索和推荐系统的精准用户意图理解，通过可解释的稀疏激活实现跨语言内容匹配和个性化推荐。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07203v1": {
    "title": "Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07203v1",
    "arxiv_id": "2510.07203v1",
    "authors": "Benjamin Akera, Evelyn Nafula Ouma, Gilbert Yiga, Patrick Walukagga, Phionah Natukunda, Trevor Saaka, Solomon Nsumba, Lilian Teddy Nabukeera, Joel Muhanguzi, Imran Sekalala, Nimpamya Janat Namara, Engineer Bainomugisha, Ernest Mwebaze, John Quinn",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 16:35:53",
    "ori_summary": "There are more than 2000 living languages in Africa, most of which have been bypassed by advances in language technology. Current leading LLMs exhibit strong performance on a number of the most common languages (e.g. Swahili or Yoruba), but prioritise support for the languages with the most speakers first, resulting in piecemeal ability across disparate languages. We contend that a regionally focussed approach is more efficient, and present a case study for Uganda, a country with high linguistic diversity. We describe the development of Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the art comprehension in the majority of all Ugandan languages. These models are open source and can be used to reduce language barriers in a number of important practical applications.",
    "summary": "",
    "translation": "向日葵：一种扩大大型语言模型中非洲语言覆盖范围的新方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于非洲语言的多语言扩展，这属于纯粹的NLP语言覆盖问题，与推荐系统、搜索或广告的核心技术无关。即使作为使能技术，非洲语言的扩展在主流RecSys/Search/Ads应用中的潜在价值非常有限，因为这些领域主要关注主流语言和用户行为建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07178v1": {
    "title": "Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible",
    "url": "https://www.alphaxiv.org/abs/2510.07178v1",
    "arxiv_id": "2510.07178v1",
    "authors": "Imry Ziv, Nur Lan, Emmanuel Chemla, Roni Katzir",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 16:17:13",
    "ori_summary": "Are large language models (LLMs) sensitive to the distinction between humanly possible languages and humanly impossible languages? This question is taken by many to bear on whether LLMs and humans share the same innate learning biases. Previous work has attempted to answer it in the positive by comparing LLM learning curves on existing language datasets and on \"impossible\" datasets derived from them via various perturbation functions. Using the same methodology, we examine this claim on a wider set of languages and impossible perturbations. We find that in most cases, GPT-2 learns each language and its impossible counterpart equally easily, in contrast to previous claims. We also apply a more lenient condition by testing whether GPT-2 provides any kind of separation between the whole set of natural languages and the whole set of impossible languages. By considering cross-linguistic variance in various metrics computed on the perplexity curves, we show that GPT-2 provides no systematic separation between the possible and the impossible. Taken together, these perspectives show that LLMs do not share the human innate biases that shape linguistic typology.",
    "summary": "",
    "translation": "无偏见语言模型学习不自然：LLMs如何无法区分可能与不可能",
    "relevance_score": 2,
    "reasoning": "该论文主要关注语言模型的偏见问题和区分可能/不可能的能力，这属于纯粹的NLP评估和基准测试范畴。虽然LLM技术本身是相关领域，但论文焦点是语言模型的内在缺陷评估，而非在推荐系统、搜索或广告中的实际应用或架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07177v1": {
    "title": "CARPAS: Towards Content-Aware Refinement of Provided Aspects for Summarization in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07177v1",
    "arxiv_id": "2510.07177v1",
    "authors": "Yong-En Tian, Yu-Chien Tang, An-Zi Yen, Wen-Chih Peng",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 16:16:46",
    "ori_summary": "Aspect-based summarization has attracted significant attention for its ability to generate more fine-grained and user-aligned summaries. While most existing approaches assume a set of predefined aspects as input, real-world scenarios often present challenges where these given aspects may be incomplete, irrelevant, or entirely missing from the document. Users frequently expect systems to adaptively refine or filter the provided aspects based on the actual content. In this paper, we initiate this novel task setting, termed Content-Aware Refinement of Provided Aspects for Summarization (CARPAS), with the aim of dynamically adjusting the provided aspects based on the document context before summarizing. We construct three new datasets to facilitate our pilot experiments, and by using LLMs with four representative prompting strategies in this task, we find that LLMs tend to predict an overly comprehensive set of aspects, which often results in excessively long and misaligned summaries. Building on this observation, we propose a preliminary subtask to predict the number of relevant aspects, and demonstrate that the predicted number can serve as effective guidance for the LLMs, reducing the inference difficulty, and enabling them to focus on the most pertinent aspects. Our extensive experiments show that the proposed approach significantly improves performance across all datasets. Moreover, our deeper analyses uncover LLMs' compliance when the requested number of aspects differs from their own estimations, establishing a crucial insight for the deployment of LLMs in similar real-world applications.",
    "summary": "",
    "translation": "CARPAS：面向大型语言模型摘要任务的内容感知提供方面精炼",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM在文本摘要任务中的内容感知方面精炼，这属于纯粹的NLP应用领域。虽然涉及LLM技术，但论文聚焦于摘要生成这一与推荐系统、搜索或广告无关的特定任务，没有展示在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07175v1": {
    "title": "Quantifying Data Contamination in Psychometric Evaluations of LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.07175v1",
    "arxiv_id": "2510.07175v1",
    "authors": "Jongwook Han, Woojung Song, Jonggeun Lee, Yohan Jo",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-08 16:16:20",
    "ori_summary": "Recent studies apply psychometric questionnaires to Large Language Models (LLMs) to assess high-level psychological constructs such as values, personality, moral foundations, and dark traits. Although prior work has raised concerns about possible data contamination from psychometric inventories, which may threaten the reliability of such evaluations, there has been no systematic attempt to quantify the extent of this contamination. To address this gap, we propose a framework to systematically measure data contamination in psychometric evaluations of LLMs, evaluating three aspects: (1) item memorization, (2) evaluation memorization, and (3) target score matching. Applying this framework to 21 models from major families and four widely used psychometric inventories, we provide evidence that popular inventories such as the Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40) exhibit strong contamination, where models not only memorize items but can also adjust their responses to achieve specific target scores.",
    "summary": "",
    "translation": "量化大语言模型心理测量评估中的数据污染",
    "relevance_score": 1,
    "reasoning": "该论文关注LLM评估中的数据污染问题，属于纯粹的评估基准研究，与我的核心关注点（推荐系统、搜索、广告的算法进步和LLM应用）无关。论文内容涉及评估方法论而非技术应用，属于明确排除的'评估基准'类别。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07173v1": {
    "title": "NurseLLM: The First Specialized Language Model for Nursing",
    "url": "https://www.alphaxiv.org/abs/2510.07173v1",
    "arxiv_id": "2510.07173v1",
    "authors": "Md Tawkat Islam Khondaker, Julia Harrington, Shady Shehata",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-08 16:15:06",
    "ori_summary": "Recent advancements in large language models (LLMs) have significantly transformed medical systems. However, their potential within specialized domains such as nursing remains largely underexplored. In this work, we introduce NurseLLM, the first nursing-specialized LLM tailored for multiple choice question-answering (MCQ) tasks. We develop a multi-stage data generation pipeline to build the first large scale nursing MCQ dataset to train LLMs on a broad spectrum of nursing topics. We further introduce multiple nursing benchmarks to enable rigorous evaluation. Our extensive experiments demonstrate that NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of comparable size on different benchmarks, underscoring the importance of a specialized LLM for the nursing domain. Finally, we explore the role of reasoning and multi-agent collaboration systems in nursing, highlighting their promise for future research and applications.",
    "summary": "",
    "translation": "NurseLLM：首个面向护理领域的专用语言模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗护理领域的专用语言模型开发，属于明确的领域特定应用，与推荐系统、搜索或广告领域完全无关。论文标题明确指向医疗护理这一被排除的领域，没有任何技术内容表明其在推荐系统、搜索或广告方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07169v1": {
    "title": "More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.07169v1",
    "arxiv_id": "2510.07169v1",
    "authors": "Yike Zhao, Simin Guo, Ziqing Yang, Shifan Han, Dahua Lin, Fei Tan",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 16:07:26",
    "ori_summary": "The reasoning capabilities of Large Language Models (LLMs) play a critical role in many downstream tasks, yet depend strongly on the quality of training data. Despite various proposed data construction methods, their practical utility in real-world pipelines remains underexplored. In this work, we conduct a comprehensive analysis of open-source datasets and data synthesis techniques for mathematical reasoning, evaluating them under a unified pipeline designed to mirror training and deployment scenarios. We further distill effective data selection strategies and identify practical methods suitable for industrial applications. Our findings highlight that structuring data in more interpretable formats, or distilling from stronger models often outweighs simply scaling up data volume. This study provides actionable guidance for integrating training data to enhance LLM capabilities, supporting both cost-effective data curation and scalable model enhancement. We hope this work will inspire further research on how to balance \"more data\" versus \"better data\" for real-world reasoning tasks.",
    "summary": "",
    "translation": "更多数据还是更好数据？数学推理中数据选择与合成的关键分析",
    "relevance_score": 2,
    "reasoning": "该论文主要关注数学推理领域的数据选择与合成方法，属于特定领域的数据处理技术。虽然数据质量是推荐系统和搜索系统的重要考虑因素，但该论文缺乏与推荐、搜索或广告领域的直接联系，也没有涉及LLM技术、Transformer架构或异构数据建模等核心关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07167v1": {
    "title": "Reasoning for Hierarchical Text Classification: The Case of Patents",
    "url": "https://www.alphaxiv.org/abs/2510.07167v1",
    "arxiv_id": "2510.07167v1",
    "authors": "Lekang Jiang, Wenjun Sun, Stephan Goetz",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 16:06:04",
    "ori_summary": "Hierarchical text classification (HTC) assigns documents to multiple levels of a pre-defined taxonomy. Automated patent subject classification represents one of the hardest HTC scenarios because of domain knowledge difficulty and a huge number of labels. Prior approaches only output a flat label set, which offers little insight into the reason behind predictions. Therefore, we propose Reasoning for Hierarchical Classification (RHC), a novel framework that reformulates HTC as a step-by-step reasoning task to sequentially deduce hierarchical labels. RHC trains large language models (LLMs) in two stages: a cold-start stage that aligns outputs with chain-of-thought (CoT) reasoning format and a reinforcement learning (RL) stage to enhance multi-step reasoning ability. RHC demonstrates four advantages in our experiments. (1) Effectiveness: RHC surpasses previous baselines and outperforms the supervised fine-tuning counterparts by approximately 3% in accuracy and macro F1. (2) Explainability: RHC produces natural-language justifications before prediction to facilitate human inspection. (3) Scalability: RHC scales favorably with model size with larger gains compared to standard fine-tuning. (4) Applicability: Beyond patents, we further demonstrate that RHC achieves state-of-the-art performance on other widely used HTC benchmarks, which highlights its broad applicability.",
    "summary": "",
    "translation": "层次化文本分类的推理机制：以专利文本为例",
    "relevance_score": 2,
    "reasoning": "该论文专注于层次化文本分类这一特定NLP任务，主要解决专利文档的分类问题。虽然分类技术在某些推荐系统中可能有间接应用，但该工作缺乏与推荐系统、搜索或广告的直接关联，也不涉及LLM技术、Transformer架构进展或异构数据统一建模等核心关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07147v1": {
    "title": "A Multi-Agent Framework for Stateful Inference-Time Search",
    "url": "https://www.alphaxiv.org/abs/2510.07147v1",
    "arxiv_id": "2510.07147v1",
    "authors": "Arshika Lalan, Rajat Ghosh, Aditya Kolsur, Debojyoti Dutta",
    "categories": "cs.LG, cs.AI, cs.CL, cs.MA, cs.SE",
    "pub_date": "2025-10-08 15:48:41",
    "ori_summary": "Recent work explores agentic inference-time techniques to perform structured, multi-step reasoning. However, stateless inference often struggles on multi-step tasks due to the absence of persistent state. Moreover, task-specific fine-tuning or instruction-tuning often achieve surface-level code generation but remain brittle on tasks requiring deeper reasoning and long-horizon dependencies. To address these limitations, we propose stateful multi-agent evolutionary search, a training-free framework that departs from prior stateless approaches by combining (i) persistent inference-time state, (ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate its effectiveness in automated unit test generation through the generation of edge cases. We generate robust edge cases using an evolutionary search process, where specialized agents sequentially propose, mutate, and score candidates. A controller maintains persistent state across generations, while evolutionary preservation ensures diversity and exploration across all possible cases. This yields a generalist agent capable of discovering robust, high-coverage edge cases across unseen codebases. Experiments show our stateful multi-agent inference framework achieves substantial gains in coverage over stateless single-step baselines, evaluated on prevalent unit-testing benchmarks such as HumanEval and TestGenEvalMini and using three diverse LLM families - Llama, Gemma, and GPT. These results indicate that combining persistent inference-time state with evolutionary search materially improves unit-test generation.",
    "summary": "论文研究多步推理任务中无状态推理的局限性问题，核心思想是结合持久推理状态、对抗性变异和进化保留，构建状态保持的多智能体进化搜索框架来增强深度推理能力。",
    "translation": "用于有状态推理时搜索的多智能体框架",
    "relevance_score": 8,
    "reasoning": "该论文提出多智能体框架用于推理时搜索，这属于核心搜索领域的进展。有状态推理时搜索技术可直接应用于搜索系统的查询理解和结果优化，通过多智能体协作提升搜索质量和效率，与搜索领域的核心算法改进高度相关。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出的状态保持多智能体进化搜索框架在推理架构上有创新，虽然应用于单元测试生成，但其核心思想对推荐系统的复杂推理和搜索优化有直接借鉴意义。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07141v1": {
    "title": "Comparing human and language models sentence processing difficulties on complex structures",
    "url": "https://www.alphaxiv.org/abs/2510.07141v1",
    "arxiv_id": "2510.07141v1",
    "authors": "Samuel Joseph Amouyal, Aya Meltzer-Asscher, Jonathan Berant",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 15:42:49",
    "ori_summary": "Large language models (LLMs) that fluently converse with humans are a reality - but do LLMs experience human-like processing difficulties? We systematically compare human and LLM sentence comprehension across seven challenging linguistic structures. We collect sentence comprehension data from humans and five families of state-of-the-art LLMs, varying in size and training procedure in a unified experimental framework. Our results show LLMs overall struggle on the target structures, but especially on garden path (GP) sentences. Indeed, while the strongest models achieve near perfect accuracy on non-GP structures (93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5). Additionally, when ranking structures based on average performance, rank correlation between humans and models increases with parameter count. For each target structure, we also collect data for their matched baseline without the difficult structure. Comparing performance on the target vs. baseline sentences, the performance gap observed in humans holds for LLMs, with two exceptions: for models that are too weak performance is uniformly low across both sentence types, and for models that are too strong the performance is uniformly high. Together, these reveal convergence and divergence in human and LLM sentence comprehension, offering new insights into the similarity of humans and LLMs.",
    "summary": "",
    "translation": "比较人类和语言模型在复杂结构上的句子处理困难",
    "relevance_score": 3,
    "reasoning": "该论文主要关注语言模型与人类在复杂句子处理上的比较，属于语言模型评估范畴。虽然涉及语言模型，但论文焦点是认知科学角度的处理困难分析，而非直接应用于推荐系统、搜索或广告的技术进展。对于'使能技术'领域，这种基础认知分析可能间接帮助理解模型在复杂查询处理中的局限性，但缺乏明确的直接应用路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07118v1": {
    "title": "TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning",
    "url": "https://www.alphaxiv.org/abs/2510.07118v1",
    "arxiv_id": "2510.07118v1",
    "authors": "Manish Nagaraj, Sakshi Choudhary, Utkarsh Saxena, Deepak Ravikumar, Kaushik Roy",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-08 15:11:04",
    "ori_summary": "Instruction tuning is essential for aligning large language models (LLMs) to downstream tasks and commonly relies on large, diverse corpora. However, small, high-quality subsets, known as coresets, can deliver comparable or superior results, though curating them remains challenging. Existing methods often rely on coarse, sample-level signals like gradients, an approach that is computationally expensive and overlooks fine-grained features. To address this, we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a forward-only, token-centric framework. Instead of using gradients, TRIM operates by matching underlying representational patterns identified via attention-based \"fingerprints\" from a handful of target samples. Such an approach makes TRIM highly efficient and uniquely sensitive to the structural features that define a task. Coresets selected by our method consistently outperform state-of-the-art baselines by up to 9% on downstream tasks and even surpass the performance of full-data fine-tuning in some settings. By avoiding expensive backward passes, TRIM achieves this at a fraction of the computational cost. These findings establish TRIM as a scalable and efficient alternative for building high-quality instruction-tuning datasets.",
    "summary": "论文研究如何从大规模指令调优数据中高效选择高质量子集；核心思想是利用前向传播中的注意力模式作为token级指纹，匹配目标样本的表征结构来识别关键训练数据。",
    "translation": "TRIM：基于逐词注意力推导显著性的数据高效指令微调",
    "relevance_score": 8,
    "reasoning": "该论文提出基于注意力机制的数据高效指令微调方法，属于核心LLM技术进步。注意力驱动的显著性和数据效率技术可直接应用于推荐系统和搜索中的用户意图理解与个性化建模，通过更高效的指令微调提升模型在特定领域的性能表现。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出了基于注意力指纹的高效指令调优数据选择方法，直接针对LLM训练效率这一核心瓶颈，对大规模推荐系统的模型优化具有重要参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07105v1": {
    "title": "Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning",
    "url": "https://www.alphaxiv.org/abs/2510.07105v1",
    "arxiv_id": "2510.07105v1",
    "authors": "Taylor Sorensen, Yejin Choi",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 14:59:24",
    "ori_summary": "Many natural language processing (NLP) tasks involve subjectivity, ambiguity, or legitimate disagreement between annotators. In this paper, we outline our system for modeling human variation. Our system leverages language models' (LLMs) in-context learning abilities, along with a two-step meta-learning training procedure for 1) post-training on many datasets requiring in-context learning and 2) specializing the model via in-context meta-learning to the particular data distribution of interest. We also evaluate the performance of our system submission to the Learning With Disagreements (LeWiDi) competition, where it was the overall winner on both tasks. Additionally, we perform an ablation study to measure the importance of each system component. We find that including rater examples in-context is crucial for our system's performance, dataset-specific fine-tuning is helpful on the larger datasets, post-training on other in-context datasets is helpful on one of the competition datasets, and that performance improves with model scale.",
    "summary": "",
    "translation": "Opt-ICL在LeWiDi-2025：通过元学习最大化来自评分者示例的上下文信号",
    "relevance_score": 3,
    "reasoning": "该论文关注于通过元学习优化上下文学习（ICL），这属于LLM核心技术范畴，可能应用于搜索或推荐系统中更有效地利用少量示例进行个性化调整。然而，论文标题明确指向LeWiDi-2025（可能是一个特定竞赛或数据集），且焦点是'评分者示例'，这表明其应用可能更偏向评估基准或特定NLP任务，而非直接面向推荐/搜索/广告系统的核心问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07098v1": {
    "title": "TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription",
    "url": "https://www.alphaxiv.org/abs/2510.07098v1",
    "arxiv_id": "2510.07098v1",
    "authors": "Guo Yutong, Wanying Wang, Yue Wu, Zichen Miao, Haoyu Wang",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 14:56:42",
    "ori_summary": "Table Visual Question Answering (Table VQA) is typically addressed by large vision-language models (VLMs). While such models can answer directly from images, they often miss fine-grained details unless scaled to very large sizes, which are computationally prohibitive, especially for mobile deployment. A lighter alternative is to have a small VLM perform OCR and then use a large language model (LLM) to reason over structured outputs such as Markdown tables. However, these representations are not naturally optimized for LLMs and still introduce substantial errors. We propose TALENT (Table VQA via Augmented Language-Enhanced Natural-text Transcription), a lightweight framework that leverages dual representations of tables. TALENT prompts a small VLM to produce both OCR text and natural language narration, then combines them with the question for reasoning by an LLM. This reframes Table VQA as an LLM-centric multimodal reasoning task, where the VLM serves as a perception-narration module rather than a monolithic solver. Additionally, we construct ReTabVQA, a more challenging Table VQA dataset requiring multi-step quantitative reasoning over table images. Experiments show that TALENT enables a small VLM-LLM combination to match or surpass a single large VLM at significantly lower computational cost on both public datasets and ReTabVQA.",
    "summary": "",
    "translation": "TALENT：通过增强语言的自然文本转录实现表格视觉问答",
    "relevance_score": 2,
    "reasoning": "虽然该论文涉及表格理解和问答，但其核心是视觉问答(VQA)任务，主要关注表格图像到文本的转换和问答能力。这与推荐系统、搜索或广告的核心技术栈关联较弱，表格数据在这些领域通常以结构化格式直接处理，而非通过视觉问答方式。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07096v1": {
    "title": "Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis",
    "url": "https://www.alphaxiv.org/abs/2510.07096v1",
    "arxiv_id": "2510.07096v1",
    "authors": "Zhu Li, Yuqing Zhang, Xiyuan Gao, Shekhar Nayak, Matt Coler",
    "categories": "cs.CL, cs.SD, eess.AS",
    "pub_date": "2025-10-08 14:53:48",
    "ori_summary": "Sarcasm is a subtle form of non-literal language that poses significant challenges for speech synthesis due to its reliance on nuanced semantic, contextual, and prosodic cues. While existing speech synthesis research has focused primarily on broad emotional categories, sarcasm remains largely unexplored. In this paper, we propose a Large Language Model (LLM)-enhanced Retrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach combines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture pragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic exemplars retrieved via a Retrieval Augmented Generation (RAG) module, which provide expressive reference patterns of sarcastic delivery. Integrated within a VITS backbone, this dual conditioning enables more natural and contextually appropriate sarcastic speech. Experiments demonstrate that our method outperforms baselines in both objective measures and subjective evaluations, yielding improvements in speech naturalness, sarcastic expressivity, and downstream sarcasm detection.",
    "summary": "",
    "translation": "让机器听起来讽刺：LLM增强与检索引导的讽刺语音合成",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音合成和讽刺检测，属于纯语音领域，与推荐系统、搜索或广告的排名任务没有直接关联。即使涉及LLM技术，其应用方向（语音合成）在指定的关注领域中缺乏明确的实用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07091v1": {
    "title": "The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas",
    "url": "https://www.alphaxiv.org/abs/2510.07091v1",
    "arxiv_id": "2510.07091v1",
    "authors": "Baixuan Xu, Tianshi Zheng, Zhaowei Wang, Hong Ting Tsang, Weiqi Wang, Tianqing Fang, Yangqiu Song",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-08 14:47:40",
    "ori_summary": "Enabling LLMs to effectively operate long-horizon task which requires long-term planning and multiple interactions is essential for open-world autonomy. Conventional methods adopt planning with actions where a executable action list would be provided as reference. However, this action representation choice would be impractical when the environment action space is combinatorial exploded (e.g., open-ended real world). This naturally leads to a question: As environmental action space scales, what is the optimal action representation for long-horizon agents? In this paper, we systematically study the effectiveness of two different action representations. The first one is conventional planning with actions (PwA) which is predominantly adopted for its effectiveness on existing benchmarks. The other one is planning with schemas (PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ] to [OBJ]\" -> \"move apple to desk\") to ensure concise action space and reliable scalability. This alternative is motivated by its alignment with human cognition and its compliance with environment-imposed action format restriction. We propose cognitive bandwidth perspective as a conceptual framework to qualitatively understand the differences between these two action representations and empirically observe a representation-choice inflection point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve as evidence of the need for scalable representations. We further conduct controlled experiments to study how the location of this inflection point interacts with different model capacities: stronger planning proficiency shifts the inflection rightward, whereas better schema instantiation shifts it leftward. Finally, noting the suboptimal performance of PwS agents, we provide an actionable guide for building more capable PwS agents for better scalable autonomy.",
    "summary": "",
    "translation": "认知带宽瓶颈：将长视野智能体从动作规划转向模式规划",
    "relevance_score": 3,
    "reasoning": "该论文聚焦于智能体规划和认知架构，属于AI代理领域而非推荐系统、搜索或广告的核心技术。虽然规划效率提升可能间接影响需要长期规划的推荐场景，但缺乏与Transformer架构、LLM技术或推荐系统直接相关的明确连接点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07083v1": {
    "title": "All Claims Are Equal, but Some Claims Are More Equal Than Others: Importance-Sensitive Factuality Evaluation of LLM Generations",
    "url": "https://www.alphaxiv.org/abs/2510.07083v1",
    "arxiv_id": "2510.07083v1",
    "authors": "Miriam Wanner, Leif Azzopardi, Paul Thomas, Soham Dan, Benjamin Van Durme, Nick Craswell",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 14:40:33",
    "ori_summary": "Existing methods for evaluating the factuality of large language model (LLM) responses treat all claims as equally important. This results in misleading evaluations when vital information is missing or incorrect as it receives the same weight as peripheral details, raising the question: how can we reliably detect such differences when there are errors in key information? Current approaches that measure factuality tend to be insensitive to omitted or false key information. To investigate this lack of sensitivity, we construct VITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses designed to omit or falsify key information. Using this dataset, we demonstrate the insensitivities of existing evaluation metrics to key information errors. To address this gap, we introduce VITAL, a set of metrics that provide greater sensitivity in measuring the factuality of responses by incorporating the relevance and importance of claims with respect to the query. Our analysis demonstrates that VITAL metrics more reliably detect errors in key information than previous methods. Our dataset, metrics, and analysis provide a foundation for more accurate and robust assessment of LLM factuality.",
    "summary": "",
    "translation": "所有声明皆平等，但某些声明比其他声明更平等：LLM生成内容的重要性敏感事实性评估",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM生成内容的事实性评估和幻觉检测，这属于纯粹的NLP评估基准范畴。虽然事实性在搜索系统中可能有一定相关性，但论文的核心焦点是通用的LLM评估方法，而非针对推荐系统、搜索或广告的具体应用或架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07081v1": {
    "title": "Accelerating Diffusion LLM Inference via Local Determinism Propagation",
    "url": "https://www.alphaxiv.org/abs/2510.07081v1",
    "arxiv_id": "2510.07081v1",
    "authors": "Fanheng Kong, Jingyuan Zhang, Yahui Liu, Zirui Wu, Yu Tian, Victoria W., Guorui Zhou",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 14:39:34",
    "ori_summary": "Diffusion large language models (dLLMs) represent a significant advancement in text generation, offering parallel token decoding capabilities. However, existing open-source implementations suffer from quality-speed trade-offs that impede their practical deployment. Conservative sampling strategies typically decode only the most confident token per step to ensure quality (i.e., greedy decoding), at the cost of inference efficiency due to repeated redundant refinement iterations--a phenomenon we term delayed decoding. Through systematic analysis of dLLM decoding dynamics, we characterize this delayed decoding behavior and propose a training-free adaptive parallel decoding strategy, named LocalLeap, to address these inefficiencies. LocalLeap is built on two fundamental empirical principles: local determinism propagation centered on high-confidence anchors and progressive spatial consistency decay. By applying these principles, LocalLeap identifies anchors and performs localized relaxed parallel decoding within bounded neighborhoods, achieving substantial inference step reduction through early commitment of already-determined tokens without compromising output quality. Comprehensive evaluation on various benchmarks demonstrates that LocalLeap achieves 6.94$\\times$ throughput improvements and reduces decoding steps to just 14.2\\% of the original requirement, achieving these gains with negligible performance impact. The source codes are available at: https://github.com/friedrichor/LocalLeap.",
    "summary": "",
    "translation": "通过局部确定性传播加速扩散大语言模型推理",
    "relevance_score": 3,
    "reasoning": "该论文主要关注扩散模型（Diffusion Models）的推理加速技术，属于生成式AI的效率优化范畴。虽然扩散模型在AIGC领域有广泛应用，但论文标题未明确显示与推荐系统、搜索或广告的直接关联。局部确定性传播技术可能对序列建模有一定启发，但应用潜力在标题层面不够明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07074v1": {
    "title": "LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish",
    "url": "https://www.alphaxiv.org/abs/2510.07074v1",
    "arxiv_id": "2510.07074v1",
    "authors": "Fred Philippy, Laura Bernardy, Siwen Guo, Jacques Klein, Tegawendé F. Bissyandé",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 14:35:59",
    "ori_summary": "Instruction tuning has become a key technique for enhancing the performance of large language models, enabling them to better follow human prompts. However, low-resource languages such as Luxembourgish face severe limitations due to the lack of high-quality instruction datasets. Traditional reliance on machine translation often introduces semantic misalignment and cultural inaccuracies. In this work, we address these challenges by creating a cross-lingual instruction tuning dataset for Luxembourgish, without resorting to machine-generated translations into it. Instead, by leveraging aligned data from English, French, and German, we build a high-quality dataset that preserves linguistic and cultural nuances. We provide evidence that cross-lingual instruction tuning not only improves representational alignment across languages but also the model's generative capabilities in Luxembourgish. This highlights how cross-lingual data curation can avoid the common pitfalls of machine-translated data and directly benefit low-resource language development.",
    "summary": "",
    "translation": "LuxInstruct：卢森堡语跨语言指令微调数据集",
    "relevance_score": 2,
    "reasoning": "该论文专注于卢森堡语这一小众语言的指令微调数据集构建，属于特定语言NLP任务。虽然跨语言技术可能对多语言搜索系统有间接价值，但论文本身聚焦于特定语言的数据集创建，与推荐系统、搜索或广告的核心技术进展关联度极低，且未涉及Transformer架构改进或异构数据建模等关键技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07061v1": {
    "title": "Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages",
    "url": "https://www.alphaxiv.org/abs/2510.07061v1",
    "arxiv_id": "2510.07061v1",
    "authors": "Amir Hossein Yari, Kalmit Kulkarni, Ahmad Raza Khan, Fajri Koto",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 14:27:02",
    "ori_summary": "While automatic metrics drive progress in Machine Translation (MT) and Text Summarization (TS), existing metrics have been developed and validated almost exclusively for English and other high-resource languages. This narrow focus leaves Indian languages, spoken by over 1.5 billion people, largely overlooked, casting doubt on the universality of current evaluation practices. To address this gap, we introduce ITEM, a large-scale benchmark that systematically evaluates the alignment of 26 automatic metrics with human judgments across six major Indian languages, enriched with fine-grained annotations. Our extensive evaluation, covering agreement with human judgments, sensitivity to outliers, language-specific reliability, inter-metric correlations, and resilience to controlled perturbations, reveals four central findings: (1) LLM-based evaluators show the strongest alignment with human judgments at both segment and system levels; (2) outliers exert a significant impact on metric-human agreement; (3) in TS, metrics are more effective at capturing content fidelity, whereas in MT, they better reflect fluency; and (4) metrics differ in their robustness and sensitivity when subjected to diverse perturbations. Collectively, these findings offer critical guidance for advancing metric design and evaluation in Indian languages.",
    "summary": "",
    "translation": "重新审视印度语言机器翻译与摘要细粒度评估中的度量可靠性",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器翻译和摘要的评估基准与度量可靠性，属于纯粹的NLP评估主题。虽然提到了细粒度评估，但完全围绕语言生成任务在特定语言上的评估方法，没有任何与推荐系统、搜索或广告相关的技术内容或潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07060v1": {
    "title": "Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations",
    "url": "https://www.alphaxiv.org/abs/2510.07060v1",
    "arxiv_id": "2510.07060v1",
    "authors": "Miriam Wanner, Sophia Hager, Anjalie Field",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 14:27:00",
    "ori_summary": "Local news stations are often considered to be reliable sources of non-politicized information, particularly local concerns that residents care about. Because these stations are trusted news sources, viewers are particularly susceptible to the information they report. The Sinclair Broadcast group is a broadcasting company that has acquired many local news stations in the last decade. We investigate the effects of local news stations being acquired by Sinclair: how does coverage change? We use computational methods to investigate changes in internet content put out by local news stations before and after being acquired by Sinclair and in comparison to national news outlets. We find that there is clear evidence that local news stations report more frequently on national news at the expense of local topics, and that their coverage of polarizing national topics increases.",
    "summary": "",
    "translation": "地方新闻是否保持本地化？：辛克莱收购电视台后的在线内容转变",
    "relevance_score": 1,
    "reasoning": "该论文研究媒体所有权变化对地方新闻内容的影响，属于媒体研究领域。这与推荐系统、搜索或广告的核心技术进步、LLM技术、Transformer架构或异构数据建模均无直接关联，也不涉及任何相关技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07048v1": {
    "title": "Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07048v1",
    "arxiv_id": "2510.07048v1",
    "authors": "Yuntao Gui, James Cheng",
    "categories": "cs.CL, cs.AI, I.2.7",
    "pub_date": "2025-10-08 14:16:20",
    "ori_summary": "Despite their remarkable natural language understanding capabilities, Large Language Models (LLMs) have been underutilized for retrieval tasks. We present Search-R3, a novel framework that addresses this limitation by adapting LLMs to generate search embeddings as a direct output of their reasoning process. Our approach exploits LLMs' chain-of-thought capabilities, allowing them to produce more effective embeddings by reasoning step-by-step through complex semantic analyses. We implement this through three complementary mechanisms. (1) a supervised learning stage enables the model's ability to produce quality embeddings, (2) a reinforcement learning (RL) methodology that optimizes embedding generation alongside reasoning, and (3) a specialized RL environment that efficiently handles evolving embedding representations without requiring complete corpus re-encoding at each training iteration. Our extensive evaluations on diverse benchmarks demonstrate that Search-R3 significantly outperforms prior methods by unifying the reasoning and embedding generation processes. This integrated post-training approach represents a substantial advancement in handling complex knowledge-intensive tasks that require both sophisticated reasoning and effective information retrieval. Project page: https://github.com/ytgui/Search-R3",
    "summary": "论文研究如何解决LLM在检索任务中能力未充分利用的问题，核心思想是通过链式推理过程直接生成搜索嵌入，将推理与嵌入生成统一在LLM中。",
    "translation": "Search-R3：在大型语言模型中统一推理与嵌入生成",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及LLM核心技术的进步，将推理与嵌入生成统一起来，这对搜索和推荐系统具有重要应用价值。统一的嵌入生成可以显著提升语义匹配和内容理解能力，同时增强的推理能力能够改善复杂查询处理和个性化推荐的质量。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接统一LLM的推理与嵌入生成，核心解决了检索任务中LLM能力未充分利用的问题，与搜索和LLM应用领域高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07037v1": {
    "title": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07037v1",
    "arxiv_id": "2510.07037v1",
    "authors": "Rajvee Sheth, Samridhi Raj Sinha, Mahavir Patil, Himanshu Beniwal, Mayank Singh",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 14:04:14",
    "ori_summary": "Code-switching (CSW), the alternation of languages and scripts within a single utterance, remains a fundamental challenge for multiling ual NLP, even amidst the rapid advances of large language models (LLMs). Most LLMs still struggle with mixed-language inputs, limited CSW datasets, and evaluation biases, hindering deployment in multilingual societies. This survey provides the first comprehensive analysis of CSW-aware LLM research, reviewing \\total{unique_references} studies spanning five research areas, 12 NLP tasks, 30+ datasets, and 80+ languages. We classify recent advances by architecture, training strategy, and evaluation methodology, outlining how LLMs have reshaped CSW modeling and what challenges persist. The paper concludes with a roadmap emphasizing the need for inclusive datasets, fair evaluation, and linguistically grounded models to achieve truly multilingual intelligence. A curated collection of all resources is maintained at https://github.com/lingo-iitgn/awesome-code-mixing/.",
    "summary": "",
    "translation": "超越单语假设：大语言模型时代下的语码转换自然语言处理综述",
    "relevance_score": 2,
    "reasoning": "该论文主要关注语码转换（code-switching）这一特定NLP任务，属于多语言处理的专门领域。虽然涉及大语言模型，但其核心应用场景是跨语言文本处理，与推荐系统、搜索或广告的核心技术需求（如用户行为建模、内容理解、排序算法）关联度较低，缺乏明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07024v1": {
    "title": "Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge",
    "url": "https://www.alphaxiv.org/abs/2510.07024v1",
    "arxiv_id": "2510.07024v1",
    "authors": "Shrestha Ghosh, Luca Giordano, Yujia Hu, Tuan-Phong Nguyen, Simon Razniewski",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 13:48:38",
    "ori_summary": "LLMs are remarkable artifacts that have revolutionized a range of NLP and AI tasks. A significant contributor is their factual knowledge, which, to date, remains poorly understood, and is usually analyzed from biased samples. In this paper, we take a deep tour into the factual knowledge (or beliefs) of a frontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited set of 100 million beliefs of one of the strongest currently available frontier LLMs, GPT-4.1. We find that the models' factual knowledge differs quite significantly from established knowledge bases, and that its accuracy is significantly lower than indicated by previous benchmarks. We also find that inconsistency, ambiguity and hallucinations are major issues, shedding light on future research opportunities concerning factual LLM knowledge.",
    "summary": "",
    "translation": "挖掘心智：从1亿条信念中揭示前沿大语言模型的知识边界",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM知识边界的探索和信念分析，属于纯粹的LLM评估研究范畴。虽然涉及前沿LLM技术，但缺乏明确的推荐系统、搜索或广告应用场景，更侧重于模型内部知识表征的学术分析而非实际应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07019v1": {
    "title": "Native Hybrid Attention for Efficient Sequence Modeling",
    "url": "https://www.alphaxiv.org/abs/2510.07019v1",
    "arxiv_id": "2510.07019v1",
    "authors": "Jusen Du, Jiaxi Hu, Tao Zhang, Weigao Sun, Yu Cheng",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-08 13:44:57",
    "ori_summary": "Transformers excel at sequence modeling but face quadratic complexity, while linear attention offers improved efficiency but often compromises recall accuracy over long contexts. In this work, we introduce Native Hybrid Attention (NHA), a novel hybrid architecture of linear and full attention that integrates both intra \\& inter-layer hybridization into a unified layer design. NHA maintains long-term context in key-value slots updated by a linear RNN, and augments them with short-term tokens from a sliding window. A single \\texttt{softmax attention} operation is then applied over all keys and values, enabling per-token and per-head context-dependent weighting without requiring additional fusion parameters. The inter-layer behavior is controlled through a single hyperparameter, the sliding window size, which allows smooth adjustment between purely linear and full attention while keeping all layers structurally uniform. Experimental results show that NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains. Code is available at https://github.com/JusenD/NHA.",
    "summary": "研究Transformer序列建模中的二次复杂度问题，核心思想是通过将线性RNN与滑动窗口注意力混合的单层统一设计，实现长短上下文的自适应建模，无需额外融合参数。",
    "translation": "原生混合注意力机制用于高效序列建模",
    "relevance_score": 8,
    "reasoning": "该论文提出了一种新的注意力机制，属于'使能Transformer技术'范畴，直接改进Transformer架构的效率。高效的序列建模对于处理推荐系统中的用户行为序列和搜索中的查询-文档序列至关重要，能够显著提升大规模推荐和搜索系统的性能与效率。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对Transformer架构的效率瓶颈提出混合注意力机制，属于Transformer架构进步的核心领域，对推荐系统和搜索中的长序列建模有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07000v1": {
    "title": "Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages",
    "url": "https://www.alphaxiv.org/abs/2510.07000v1",
    "arxiv_id": "2510.07000v1",
    "authors": "Neel Prabhanjan Rachamalla, Aravind Konakalla, Gautam Rajeev, Ashish Kulkarni, Chandra Khatri, Shubham Agarwal",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 13:23:45",
    "ori_summary": "The effectiveness of Large Language Models (LLMs) depends heavily on the availability of high-quality post-training data, particularly instruction-tuning and preference-based examples. Existing open-source datasets, however, often lack multilingual coverage, cultural grounding, and suffer from task diversity gaps that are especially pronounced for Indian languages. We introduce a human-in-the-loop pipeline that combines translations with synthetic expansion to produce reliable and diverse Indic post-training data. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and Pragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56 sub-categories, leveraging 57 diverse datasets. Our dataset protocol incorporates several often-overlooked dimensions and emphasize task diversity, multi-turn dialogue, instruction fidelity, safety alignment, and preservation of cultural nuance, providing a foundation for more inclusive and effective multilingual LLMs.",
    "summary": "",
    "translation": "Pragyaan：为印度语言设计和策划高质量文化后训练数据集",
    "relevance_score": 2,
    "reasoning": "该论文专注于为特定语言（印度语言）创建文化数据集，这属于数据工程领域，与推荐系统、搜索或广告的核心技术进展无关。虽然多语言能力在理论上可能对全球化推荐系统有帮助，但论文重点在于文化数据集创建而非核心模型架构或推荐算法，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06994v1": {
    "title": "RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning",
    "url": "https://www.alphaxiv.org/abs/2510.06994v1",
    "arxiv_id": "2510.06994v1",
    "authors": "Artur Horal, Daniel Pina, Henrique Paz, Iago Paulo, João Soares, Rafael Ferreira, Diogo Tavares, Diogo Glória-Silva, João Magalhães, David Semedo",
    "categories": "cs.CR, cs.CL",
    "pub_date": "2025-10-08 13:18:42",
    "ori_summary": "This paper presents the vision, scientific contributions, and technical details of RedTWIZ: an adaptive and diverse multi-turn red teaming framework, to audit the robustness of Large Language Models (LLMs) in AI-assisted software development. Our work is driven by three major research streams: (1) robust and systematic assessment of LLM conversational jailbreaks; (2) a diverse generative multi-turn attack suite, supporting compositional, realistic and goal-oriented jailbreak conversational strategies; and (3) a hierarchical attack planner, which adaptively plans, serializes, and triggers attacks tailored to specific LLM's vulnerabilities. Together, these contributions form a unified framework -- combining assessment, attack generation, and strategic planning -- to comprehensively evaluate and expose weaknesses in LLMs' robustness. Extensive evaluation is conducted to systematically assess and analyze the performance of the overall system and each component. Experimental results demonstrate that our multi-turn adversarial attack strategies can successfully lead state-of-the-art LLMs to produce unsafe generations, highlighting the pressing need for more research into enhancing LLM's robustness.",
    "summary": "",
    "translation": "RedTWIZ：通过自适应攻击规划的多样化大语言模型红队测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注大语言模型的安全性和对抗性测试（红队测试），这属于模型安全评估范畴。虽然涉及LLM技术，但其核心关注点是安全测试而非推荐系统、搜索或广告领域的应用。该研究缺乏与RecSys/Search/Ads领域的直接关联或潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06975v1": {
    "title": "VelLMes: A high-interaction AI-based deception framework",
    "url": "https://www.alphaxiv.org/abs/2510.06975v1",
    "arxiv_id": "2510.06975v1",
    "authors": "Muris Sladić, Veronica Valeros, Carlos Catania, Sebastian Garcia",
    "categories": "cs.CR, cs.AI, cs.CL",
    "pub_date": "2025-10-08 13:00:23",
    "ori_summary": "There are very few SotA deception systems based on Large Language Models. The existing ones are limited only to simulating one type of service, mainly SSH shells. These systems - but also the deception technologies not based on LLMs - lack an extensive evaluation that includes human attackers. Generative AI has recently become a valuable asset for cybersecurity researchers and practitioners, and the field of cyber-deception is no exception. Researchers have demonstrated how LLMs can be leveraged to create realistic-looking honeytokens, fake users, and even simulated systems that can be used as honeypots. This paper presents an AI-based deception framework called VelLMes, which can simulate multiple protocols and services such as SSH Linux shell, MySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus VelLMes offers a variety of choices for deception design based on the users' needs. VelLMes is designed to be attacked by humans, so interactivity and realism are key for its performance. We evaluate the generative capabilities and the deception capabilities. Generative capabilities were evaluated using unit tests for LLMs. The results of the unit tests show that, with careful prompting, LLMs can produce realistic-looking responses, with some LLMs having a 100% passing rate. In the case of the SSH Linux shell, we evaluated deception capabilities with 89 human attackers. The results showed that about 30% of the attackers thought that they were interacting with a real system when they were assigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH Linux shell honeypot on the Internet to capture real-life attacks. Analysis of these attacks showed us that LLM honeypots simulating Linux shells can perform well against unstructured and unexpected attacks on the Internet, responding correctly to most of the issued commands.",
    "summary": "",
    "translation": "VelLMes：一种基于人工智能的高交互性欺骗框架",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及欺骗框架和AI安全领域，这属于明确的无关主题范畴，特别是安全相关技术。该研究没有任何与推荐系统、搜索、广告或相关使能技术相关的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06974v1": {
    "title": "Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups",
    "url": "https://www.alphaxiv.org/abs/2510.06974v1",
    "arxiv_id": "2510.06974v1",
    "authors": "Geng Liu, Feng Li, Junjie Mu, Mengxiao Zhu, Francesco Pierri",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 13:00:12",
    "ori_summary": "Large language models (LLMs) are increasingly deployed in user-facing applications, raising concerns about their potential to reflect and amplify social biases. We investigate social identity framing in Chinese LLMs using Mandarin-specific prompts across ten representative Chinese LLMs, evaluating responses to ingroup (\"We\") and outgroup (\"They\") framings, and extending the setting to 240 social groups salient in the Chinese context. To complement controlled experiments, we further analyze Chinese-language conversations from a corpus of real interactions between users and chatbots. Across models, we observe systematic ingroup-positive and outgroup-negative tendencies, which are not confined to synthetic prompts but also appear in naturalistic dialogue, indicating that bias dynamics might strengthen in real interactions. Our study provides a language-aware evaluation framework for Chinese LLMs, demonstrating that social identity biases documented in English generalize cross-linguistically and intensify in user-facing contexts.",
    "summary": "",
    "translation": "使用性别代词和社会群体探测中文大语言模型中的社会身份偏见",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM偏见检测和评估，属于公平性、伦理等非技术性话题，明确列在无关主题中。虽然涉及中文LLM，但核心关注点是社会身份偏见探测，与推荐系统、搜索或广告的核心技术进展、架构改进或直接应用无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06965v1": {
    "title": "EDUMATH: Generating Standards-aligned Educational Math Word Problems",
    "url": "https://www.alphaxiv.org/abs/2510.06965v1",
    "arxiv_id": "2510.06965v1",
    "authors": "Bryan R. Christ, Penelope Molitz, Jonathan Kropko, Thomas Hartvigsen",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 12:53:06",
    "ori_summary": "Math word problems (MWPs) are critical K-12 educational tools, and customizing them to students' interests and ability levels can increase learning outcomes. However, teachers struggle to find time to customize MWPs for each student given large class sizes and increasing burnout. We propose that LLMs can support math education by generating MWPs customized to student interests and math education standards. To this end, we use a joint human expert-LLM judge approach to evaluate over 11,000 MWPs generated by open and closed LLMs and develop the first teacher-annotated dataset for standards-aligned educational MWP generation. We show the value of our data by using it to train a 12B open model that matches the performance of larger and more capable open models. We also use our teacher-annotated data to train a text classifier that enables a 30B open LLM to outperform existing closed baselines without any training. Next, we show our models' MWPs are more similar to human-written MWPs than those from existing models. We conclude by conducting the first study of customized LLM-generated MWPs with grade school students, finding they perform similarly on our models' MWPs relative to human-written MWPs but consistently prefer our customized MWPs.",
    "summary": "",
    "translation": "EDUMATH：生成符合标准的数学教育应用题",
    "relevance_score": 1,
    "reasoning": "该论文专注于教育领域的数学应用题生成，属于特定领域的内容生成应用。这与我的关注点（推荐系统、搜索、广告中的核心进展、使能技术或直接应用）没有直接关联，并且明确排除了AIGC、内容生成等纯LLM中心化主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06961v1": {
    "title": "Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.06961v1",
    "arxiv_id": "2510.06961v1",
    "authors": "Vaibhav Srivastav, Steven Zheng, Eric Bezzam, Eustache Le Bihan, Nithin Koluguri, Piotr Żelasko, Somshubra Majumdar, Adel Moumen, Sanchit Gandhi",
    "categories": "cs.CL, cs.AI, cs.SD, eess.AS",
    "pub_date": "2025-10-08 12:44:51",
    "ori_summary": "Despite rapid progress, ASR evaluation remains saturated with short-form English, and efficiency is rarely reported. We present the Open ASR Leaderboard, a fully reproducible benchmark and interactive leaderboard comparing 60+ open-source and proprietary systems across 11 datasets, including dedicated multilingual and long-form tracks. We standardize text normalization and report both word error rate (WER) and inverse real-time factor (RTFx), enabling fair accuracy-efficiency comparisons. For English transcription, Conformer encoders paired with LLM decoders achieve the best average WER but are slower, while CTC and TDT decoders deliver much better RTFx, making them attractive for long-form and offline use. Whisper-derived encoders fine-tuned for English improve accuracy but often trade off multilingual coverage. All code and dataset loaders are open-sourced to support transparent, extensible evaluation.",
    "summary": "",
    "translation": "开放ASR排行榜：迈向可复现和透明的多语言及长格式语音识别评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音识别（ASR）的评估基准和排行榜，属于纯粹的语音处理领域。虽然语音识别在某些边缘场景可能与搜索相关，但论文本身不涉及推荐系统、搜索排名或广告的核心技术，也没有与LLM、Transformer架构或异构数据建模的直接关联，因此与当前关注点基本无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06953v1": {
    "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces",
    "url": "https://www.alphaxiv.org/abs/2510.06953v1",
    "arxiv_id": "2510.06953v1",
    "authors": "Minju Gwak, Guijin Son, Jaehyung Kim",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-08 12:37:04",
    "ori_summary": "The Uniform Information Density (UID) hypothesis suggests that effective communication maintains a stable flow of information. In this work, we revisit this principle in the context of large language model (LLM) reasoning traces, asking whether step-level uniformity reflects reasoning quality. To this end, we propose an entropy-based stepwise information density metric and introduce two complementary measures of uniformity, local and global uniformity scores. Across the experiments on six different reasoning benchmarks, we find that step-level uniformity not only provides a strong theoretical lens but also yields practical performance benefits; for example, selecting reasoning traces with more uniform information density at the step-level improves accuracy by 10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals that correct reasoning traces tend to avoid sharp information density spikes, while incorrect traces exhibit irregular information bursts. These results demonstrate that UID-inspired information density measures outperform alternative internal signals as predictors of reasoning quality. Results highlight the uniformity of the information density as a robust diagnostic and selection criterion for building more reliable and accurate reasoning systems.",
    "summary": "",
    "translation": "重新审视LLM推理轨迹中的均匀信息密度假设",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLM推理过程中的信息密度分布假设，属于纯粹的NLP理论分析范畴。虽然涉及LLM内部工作机制，但缺乏明确的推荐系统、搜索或广告应用场景，且更侧重于语言模型的理论特性而非实际应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06917v1": {
    "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.06917v1",
    "arxiv_id": "2510.06917v1",
    "authors": "Cheng-Han Chiang, Xiaofei Wang, Linjie Li, Chung-Ching Lin, Kevin Lin, Shujie Liu, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, Lijuan Wang",
    "categories": "cs.CL, eess.AS",
    "pub_date": "2025-10-08 11:48:59",
    "ori_summary": "Current large language models (LLMs) and spoken language models (SLMs) begin thinking and taking actions only after the user has finished their turn. This prevents the model from interacting during the user's turn and can lead to high response latency while it waits to think. Consequently, thinking after receiving the full input is not suitable for speech-to-speech interaction, where real-time, low-latency exchange is important. We address this by noting that humans naturally \"think while listening.\" In this paper, we propose SHANKS, a general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning while listening to the user input. SHANKS streams the input speech in fixed-duration chunks and, as soon as a chunk is received, generates unspoken reasoning based on all previous speech and reasoning, while the user continues speaking. SHANKS uses this unspoken reasoning to decide whether to interrupt the user and to make tool calls to complete the task. We demonstrate that SHANKS enhances real-time user-SLM interaction in two scenarios: (1) when the user is presenting a step-by-step solution to a math problem, SHANKS can listen, reason, and interrupt when the user makes a mistake, achieving 37.1% higher interruption accuracy than a baseline that interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can complete 56.9% of the tool calls before the user finishes their turn. Overall, SHANKS moves toward models that keep thinking throughout the conversation, not only after a turn ends. Animated illustrations of Shanks can be found at https://d223302.github.io/SHANKS/",
    "summary": "",
    "translation": "SHANKS：面向口语语言模型的同步听觉与思考机制",
    "relevance_score": 2,
    "reasoning": "该论文主要关注口语语言模型中的听觉处理与推理同步机制，属于语音处理与语言模型的交叉领域。虽然涉及语言模型技术，但其核心关注点在于口语交互和听觉处理，与推荐系统、搜索或广告领域的直接关联性较弱。该技术可能在某些语音交互场景中有潜在应用，但并非当前关注的核心领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06915v1": {
    "title": "LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling",
    "url": "https://www.alphaxiv.org/abs/2510.06915v1",
    "arxiv_id": "2510.06915v1",
    "authors": "Zecheng Tang, Baibei Ji, Quantong Qiu, Haitian Wang, Xiaobo Liang, Juntao Li, Min Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 11:48:16",
    "ori_summary": "Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasingly involve long history trajectories, e.g., LLM agent, it becomes indispensable to evaluate whether a model's responses are not only high-quality but also grounded in and consistent with the provided context. Yet, current RMs remain confined to short-context settings and primarily focus on response-level attributes (e.g., safety or helpfulness), while largely neglecting the critical dimension of long context-response consistency. In this work, we introduce Long-RewardBench, a benchmark specifically designed for long-context RM evaluation, featuring both Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that even state-of-the-art generative RMs exhibit significant fragility in long-context scenarios, failing to maintain context-aware preference judgments. Motivated by the analysis of failure patterns observed in model outputs, we propose a general multi-stage training strategy that effectively scales arbitrary models into robust Long-context RMs (LongRMs). Experiments show that our approach not only substantially improves performance on long-context evaluation but also preserves strong short-context capability. Notably, our 8B LongRM outperforms much larger 70B-scale baselines and matches the performance of the proprietary Gemini 2.5 Pro model.",
    "summary": "",
    "translation": "LongRM：揭示并突破奖励建模的上下文边界",
    "relevance_score": 6,
    "reasoning": "该论文涉及奖励建模的上下文边界扩展，这属于LLM核心技术的进展。在推荐系统和搜索领域，更长的上下文处理能力可以显著提升用户行为序列建模、长文档理解以及复杂上下文特征整合的效果，直接支持更精准的个性化推荐和搜索结果优化。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06889v1": {
    "title": "MeXtract: Light-Weight Metadata Extraction from Scientific Papers",
    "url": "https://www.alphaxiv.org/abs/2510.06889v1",
    "arxiv_id": "2510.06889v1",
    "authors": "Zaid Alyafeai, Maged S. Al-Shaibani, Bernard Ghanem",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 11:12:28",
    "ori_summary": "Metadata plays a critical role in indexing, documenting, and analyzing scientific literature, yet extracting it accurately and efficiently remains a challenging task. Traditional approaches often rely on rule-based or task-specific models, which struggle to generalize across domains and schema variations. In this paper, we present MeXtract, a family of lightweight language models designed for metadata extraction from scientific papers. The models, ranging from 0.5B to 3B parameters, are built by fine-tuning Qwen 2.5 counterparts. In their size family, MeXtract achieves state-of-the-art performance on metadata extraction on the MOLE benchmark. To further support evaluation, we extend the MOLE benchmark to incorporate model-specific metadata, providing an out-of-domain challenging subset. Our experiments show that fine-tuning on a given schema not only yields high accuracy but also transfers effectively to unseen schemas, demonstrating the robustness and adaptability of our approach. We release all the code, datasets, and models openly for the research community.",
    "summary": "",
    "translation": "MeXtract：从科学论文中轻量级提取元数据",
    "relevance_score": 2,
    "reasoning": "该论文专注于科学论文领域的元数据提取，属于特定领域的文档处理技术。虽然元数据提取在信息检索中有一般性应用，但该工作明确限定于科学论文领域，且没有明确展示与推荐系统、搜索或广告中用户行为建模、内容理解等核心问题的直接关联。其轻量级特性可能在效率方面有一般性启发，但缺乏针对RecSys/Search/Ads领域的特定应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06870v1": {
    "title": "$λ$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences",
    "url": "https://www.alphaxiv.org/abs/2510.06870v1",
    "arxiv_id": "2510.06870v1",
    "authors": "Yining Wang, Jinman Zhao, Chuangxin Zhao, Shuhao Guan, Gerald Penn, Shinan Liu",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 10:39:07",
    "ori_summary": "Reinforcement Learning with Human Feedback (RLHF) has been the dominant approach for improving the reasoning capabilities of Large Language Models (LLMs). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has simplified this paradigm by replacing the reward and value models with rule-based verifiers. A prominent example is Group Relative Policy Optimization (GRPO). However, GRPO inherently suffers from a length bias, since the same advantage is uniformly assigned to all tokens of a response. As a result, longer responses distribute the reward over more tokens and thus contribute disproportionately to gradient updates. Several variants, such as DAPO and Dr. GRPO, modify the token-level aggregation of the loss, yet these methods remain heuristic and offer limited interpretability regarding their implicit token preferences. In this work, we explore the possibility of allowing the model to learn its own token preference during optimization. We unify existing frameworks under a single formulation and introduce a learnable parameter $\\lambda$ that adaptively controls token-level weighting. We use $\\lambda$-GRPO to denote our method, and we find that $\\lambda$-GRPO achieves consistent improvements over vanilla GRPO and DAPO on multiple mathematical reasoning benchmarks. On Qwen2.5 models with 1.5B, 3B, and 7B parameters, $\\lambda$-GRPO improves average accuracy by $+1.9\\%$, $+1.0\\%$, and $+1.7\\%$ compared to GRPO, respectively. Importantly, these gains come without any modifications to the training data or additional computational cost, highlighting the effectiveness and practicality of learning token preferences.",
    "summary": "该论文研究GRPO框架中的长度偏差问题，核心思想是通过引入可学习参数λ来自适应控制token级权重，统一现有优化方法并让模型在训练中学习token偏好。",
    "translation": "λ-GRPO：通过可学习令牌偏好统一GRPO框架",
    "relevance_score": 8,
    "reasoning": "该论文涉及强化学习优化框架的改进，GRPO通常指Group Policy Optimization，在推荐系统和广告排名中有直接应用。通过引入可学习令牌偏好，该方法可以优化多目标推荐中的用户偏好建模，提升个性化推荐效果和广告投放效率。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出可学习token偏好的RL优化方法，直接改进LLM训练框架，对推荐和搜索系统的模型优化有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06866v1": {
    "title": "Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding",
    "url": "https://www.alphaxiv.org/abs/2510.06866v1",
    "arxiv_id": "2510.06866v1",
    "authors": "Wafaa Mohammed, Vlad Niculae, Chrysoula Zerva",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 10:37:17",
    "ori_summary": "Large language models (LLMs) have emerged as strong contenders in machine translation.Yet, they still struggle to adequately handle discourse phenomena, such as pronoun resolution and lexical cohesion at the document level. In this study, we thoroughly investigate the discourse phenomena performance of LLMs in context-aware translation. We demonstrate that discourse knowledge is encoded within LLMs and propose the use of quality-aware decoding (QAD) to effectively extract this knowledge, showcasing its superiority over other decoding approaches through comprehensive analysis. Furthermore, we illustrate that QAD enhances the semantic richness of translations and aligns them more closely with human preferences.",
    "summary": "",
    "translation": "通过质量感知解码解锁大语言模型中的潜在话语翻译",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM中的翻译质量改进，属于纯NLP应用领域。虽然质量感知解码技术本身可能有通用性，但论文标题明确限定在话语翻译场景，与推荐系统、搜索或广告的核心技术需求关联度极低，难以识别出在RecSys/Search/Ads中的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06847v1": {
    "title": "OpenJAI-v1.0: An Open Thai Large Language Model",
    "url": "https://www.alphaxiv.org/abs/2510.06847v1",
    "arxiv_id": "2510.06847v1",
    "authors": "Pontakorn Trakuekul, Attapol T. Rutherford, Jullajak Karnjanaekarin, Narongkorn Panitsrisit, Sumana Sumanakul",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 10:12:56",
    "ori_summary": "We introduce OpenJAI-v1.0, an open-source large language model for Thai and English, developed from the Qwen3-14B model. Our work focuses on boosting performance on practical tasks through carefully curated data across three key use cases: instruction following, long-context understanding, and tool use. Evaluation results show that OpenJAI-v1.0 improves on the capabilities of its base model and outperforms other leading open-source Thai models on a diverse suite of benchmarks, while avoiding catastrophic forgetting. OpenJAI-v1.0 is publicly released as another alternative NLP resource for the Thai AI community.",
    "summary": "",
    "translation": "OpenJAI-v1.0：一个开源的泰语大语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要介绍一个特定语言（泰语）的大语言模型，属于基础模型开发而非核心推荐系统、搜索或广告领域的进展。虽然大语言模型本身是使能技术，但该论文专注于特定语言能力，没有明确展示在推荐系统、搜索或广告中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06843v1": {
    "title": "SID: Multi-LLM Debate Driven by Self Signals",
    "url": "https://www.alphaxiv.org/abs/2510.06843v1",
    "arxiv_id": "2510.06843v1",
    "authors": "Xuhang Chen, Zhifan Song, Deyi Ji, Shuo Gao, Lanyun Zhu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 10:10:11",
    "ori_summary": "Large Language Models (LLMs) have exhibited impressive capabilities across diverse application domains. Recent work has explored Multi-LLM Agent Debate (MAD) as a way to enhance performance by enabling multiple LLMs to discuss and refine responses iteratively. Nevertheless, existing MAD methods predominantly focus on utilizing external structures, such as debate graphs, using LLM-as-a-Judge, while neglecting the application of self signals, such as token logits and attention, that arise during generation. This omission leads to redundant computation and potential performance degradation. In this paper, we shift the focus to the self signals of multi-LLM debate and introduce a Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of self-signals: model-level confidence and token-level semantic focus, to adaptively guide the debate process. Our approach enables high-confidence agents to exit early at the model level and compress the redundant debate contents based on the attention mechanism. We evaluate our method on various LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental results demonstrate that our method not only outperforms existing MAD techniques in accuracy but also reduces token consumption, highlighting the effectiveness of utilizing self signals in enhancing both the performance and efficiency of multi-agent debate systems. Our code will be available at~\\href{https://github.com/xuhang2019/SID}{\\texttt{https://github.com/xuhang2019/SID}}.",
    "summary": "",
    "translation": "SID：基于自我信号驱动的多LLM辩论",
    "relevance_score": 3,
    "reasoning": "该论文涉及多LLM辩论机制，属于LLM推理技术范畴。虽然多模型协作可能应用于推荐系统的决策融合或搜索结果的多样性优化，但论文标题未明确指向推荐、搜索或广告的具体应用场景，潜在关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06841v1": {
    "title": "GAMBIT+: A Challenge Set for Evaluating Gender Bias in Machine Translation Quality Estimation Metrics",
    "url": "https://www.alphaxiv.org/abs/2510.06841v1",
    "arxiv_id": "2510.06841v1",
    "authors": "Giorgos Filandrianos, Orfeas Menis Mastromichalakis, Wafaa Mohammed, Giuseppe Attanasio, Chrysoula Zerva",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 10:09:03",
    "ori_summary": "Gender bias in machine translation (MT) systems has been extensively documented, but bias in automatic quality estimation (QE) metrics remains comparatively underexplored. Existing studies suggest that QE metrics can also exhibit gender bias, yet most analyses are limited by small datasets, narrow occupational coverage, and restricted language variety. To address this gap, we introduce a large-scale challenge set specifically designed to probe the behavior of QE metrics when evaluating translations containing gender-ambiguous occupational terms. Building on the GAMBIT corpus of English texts with gender-ambiguous occupations, we extend coverage to three source languages that are genderless or natural-gendered, and eleven target languages with grammatical gender, resulting in 33 source-target language pairs. Each source text is paired with two target versions differing only in the grammatical gender of the occupational term(s) (masculine vs. feminine), with all dependent grammatical elements adjusted accordingly. An unbiased QE metric should assign equal or near-equal scores to both versions. The dataset's scale, breadth, and fully parallel design, where the same set of texts is aligned across all languages, enables fine-grained bias analysis by occupation and systematic comparisons across languages.",
    "summary": "",
    "translation": "GAMBIT+：用于评估机器翻译质量评估指标中性别偏见的挑战集",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器翻译质量评估中的性别偏见评估，属于公平性和偏见检测领域。这完全属于被排除的无关主题（公平性、伦理），与推荐系统、搜索或广告的核心技术进展、LLM技术或Transformer架构改进没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06826v1": {
    "title": "Mid-Training of Large Language Models: A Survey",
    "url": "https://www.alphaxiv.org/abs/2510.06826v1",
    "arxiv_id": "2510.06826v1",
    "authors": "Kaixiang Mo, Yuxin Shi, Weiwei Weng, Zhiqiang Zhou, Shuman Liu, Haibo Zhang, Anxiang Zeng",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 09:49:37",
    "ori_summary": "Large language models (LLMs) are typically developed through large-scale pre-training followed by task-specific fine-tuning. Recent advances highlight the importance of an intermediate mid-training stage, where models undergo multiple annealing-style phases that refine data quality, adapt optimization schedules, and extend context length. This stage mitigates diminishing returns from noisy tokens, stabilizes convergence, and expands model capability in late training. Its effectiveness can be explained through gradient noise scale, the information bottleneck, and curriculum learning, which together promote generalization and abstraction. Despite widespread use in state-of-the-art systems, there has been no prior survey of mid-training as a unified paradigm. We introduce the first taxonomy of LLM mid-training spanning data distribution, learning-rate scheduling, and long-context extension. We distill practical insights, compile evaluation benchmarks, and report gains to enable structured comparisons across models. We also identify open challenges and propose avenues for future research and practice.",
    "summary": "论文研究LLM训练过程中预训练与微调之间的中间阶段优化问题，核心思想是通过多阶段退火式训练策略，在数据质量优化、学习率调度和上下文扩展等方面进行系统化改进，以提升模型泛化能力和抽象能力。",
    "translation": "大型语言模型中期训练：综述",
    "relevance_score": 8,
    "reasoning": "这篇综述关注LLM训练过程中的中期训练技术，这属于'使能LLM技术'范畴。更高效的训练方法可以直接应用于构建更强大的推荐和搜索系统，通过改进模型训练效率和质量来增强下游应用性能。中期训练优化对于大规模工业级推荐和搜索系统的部署具有重要实践意义。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文系统化研究LLM训练中的中间阶段优化技术，直接关联核心LLM技术进步，对推荐系统和搜索中的模型训练具有重要指导意义。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06825v1": {
    "title": "Adaptive Tool Generation with Models as Tools and Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.06825v1",
    "arxiv_id": "2510.06825v1",
    "authors": "Chenpeng Wang, Xiaojie Cheng, Chunye Wang, Linfeng Yang, Lei Zhang",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 09:48:50",
    "ori_summary": "Tool-augmented language models have demonstrated strong capabilities, but their reliance on live API access creates scalability and reliability challenges during training and deployment. We propose MTR, a simulation-first training framework for tool-augmented reasoning. Instead of relying on live APIs, MTR learns from complete ReAct traces with schema-validated, simulated observations. Our approach operates through a multi-agent architecture where a ToolMaker generates task-specific, OpenAI-compatible tool interfaces, an AutoAgent produces structured think-act-observe sequences, and a ToolActor simulates realistic responses. Training proceeds in two stages: Stage-1 Supervised Fine-Tuning (SFT) teaches 'trace grammar' from complete reasoning sequences; Stage-2 Group Relative Policy Optimization (GRPO) optimizes strategy with a composite trace reward that balances answer correctness and internal consistency. Across four multi-hop QA benchmarks (HotpotQA, MuSiQue, 2WikiMultiHopQA, Bamboogle), MTR attains competitive Exact Match (EM) scores to live-API systems and excels on reasoning-intensive tasks, suggesting that effective tool reasoning can be learned from structured traces without live interactions.",
    "summary": "该论文研究工具增强语言模型依赖实时API带来的可扩展性和可靠性问题，核心方法是提出MTR框架，通过多智能体架构生成任务特定工具接口和模拟观察，从结构化轨迹中学习有效工具推理而无需实时交互。",
    "translation": "基于模型即工具与强化学习的自适应工具生成",
    "relevance_score": 8,
    "reasoning": "该论文涉及使用强化学习进行工具生成，这在推荐系统和搜索领域具有直接应用潜力，例如动态生成个性化推荐策略或搜索查询重写工具。模型即工具的概念可以应用于构建自适应推荐系统，根据用户上下文自动生成和优化推荐工具。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出模拟优先训练框架和多智能体架构，直接解决工具增强推理的可扩展性和可靠性问题，与LLM在推荐搜索领域的应用高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06811v1": {
    "title": "BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods",
    "url": "https://www.alphaxiv.org/abs/2510.06811v1",
    "arxiv_id": "2510.06811v1",
    "authors": "Philipp Mondorf, Mingyang Wang, Sebastian Gerstner, Ahmad Dawar Hakimi, Yihong Liu, Leonor Veloso, Shijia Zhou, Hinrich Schütze, Barbara Plank",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-08 09:39:40",
    "ori_summary": "The Circuit Localization track of the Mechanistic Interpretability Benchmark (MIB) evaluates methods for localizing circuits within large language models (LLMs), i.e., subnetworks responsible for specific task behaviors. In this work, we investigate whether ensembling two or more circuit localization methods can improve performance. We explore two variants: parallel and sequential ensembling. In parallel ensembling, we combine attribution scores assigned to each edge by different methods-e.g., by averaging or taking the minimum or maximum value. In the sequential ensemble, we use edge attribution scores obtained via EAP-IG as a warm start for a more expensive but more precise circuit identification method, namely edge pruning. We observe that both approaches yield notable gains on the benchmark metrics, leading to a more precise circuit identification approach. Finally, we find that taking a parallel ensemble over various methods, including the sequential ensemble, achieves the best results. We evaluate our approach in the BlackboxNLP 2025 MIB Shared Task, comparing ensemble scores to official baselines across multiple model-task combinations.",
    "summary": "",
    "translation": "BlackboxNLP-2025 MIB共享任务：探索电路定位方法的集成策略",
    "relevance_score": 2,
    "reasoning": "该论文专注于电路定位方法的集成策略，这属于模型可解释性/机制解释领域，与推荐系统、搜索或广告的核心技术进展没有直接关联。虽然理解Transformer内部机制可能间接有助于模型优化，但论文本身没有展示在推荐/搜索/广告中的具体应用潜力，且电路定位更偏向理论研究而非实际系统改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06800v1": {
    "title": "FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline",
    "url": "https://www.alphaxiv.org/abs/2510.06800v1",
    "arxiv_id": "2510.06800v1",
    "authors": "Haotian Wu, Shufan Jiang, Chios Chen, Yiyang Feng, Hehai Lin, Heqing Zou, Yao Shu, Yanran Li, Chengwei Qin",
    "categories": "cs.CL, cs.AI, cs.HC, cs.MA",
    "pub_date": "2025-10-08 09:30:36",
    "ori_summary": "As large language models (LLMs) advance in role-playing (RP) tasks, existing benchmarks quickly become obsolete due to their narrow scope, outdated interaction paradigms, and limited adaptability across diverse application scenarios. To address this gap, we introduce FURINA-Builder, a novel multi-agent collaboration pipeline that automatically constructs fully customizable RP benchmarks at any scale. It enables evaluation of arbitrary characters across diverse scenarios and prompt formats, as the first benchmark builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues between a test character and other characters drawn from a well-constructed character-scene pool, while an LLM judge selects fine-grained evaluation dimensions and adjusts the test character's responses into final test utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive role-playing benchmark featuring both established and synthesized test characters, each assessed with dimension-specific evaluation criteria. Human evaluation and preliminary separability analysis justify our pipeline and benchmark design. We conduct extensive evaluations of cutting-edge LLMs and find that o3 and DeepSeek-R1 achieve the best performance on English and Chinese RP tasks, respectively. Across all models, established characters consistently outperform synthesized ones, with reasoning capabilities further amplifying this disparity. Interestingly, we observe that model scale does not monotonically reduce hallucinations. More critically, for reasoning LLMs, we uncover a novel trade-off: reasoning improves RP performance but simultaneously increases RP hallucinations. This trade-off extends to a broader Pareto frontier between RP performance and reliability for all LLMs. These findings demonstrate the effectiveness of FURINA-Builder and the challenge posed by FURINA-Bench.",
    "summary": "",
    "translation": "FURINA：通过可扩展多智能体协作流程构建的完全可定制角色扮演基准",
    "relevance_score": 1,
    "reasoning": "该论文标题表明这是一个关于多智能体协作和角色扮演的基准测试系统，主要关注游戏、模拟或对话评估场景。这与我的核心关注领域（推荐系统、搜索、广告）以及相关的LLM/Transformer技术应用没有直接关联。多智能体协作基准测试在推荐、搜索或广告领域缺乏明确的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06782v1": {
    "title": "GPT-5 Model Corrected GPT-4V's Chart Reading Errors, Not Prompting",
    "url": "https://www.alphaxiv.org/abs/2510.06782v1",
    "arxiv_id": "2510.06782v1",
    "authors": "Kaichun Yang, Jian Chen",
    "categories": "cs.HC, cs.CL, cs.CV",
    "pub_date": "2025-10-08 09:09:29",
    "ori_summary": "We present a quantitative evaluation to understand the effect of zero-shot large-language model (LLMs) and prompting uses on chart reading tasks. We asked LLMs to answer 107 visualization questions to compare inference accuracies between the agentic GPT-5 and multimodal GPT-4V, for difficult image instances, where GPT-4V failed to produce correct answers. Our results show that model architecture dominates the inference accuracy: GPT5 largely improved accuracy, while prompt variants yielded only small effects. Pre-registration of this work is available here: https://osf.io/u78td/?view_only=6b075584311f48e991c39335c840ded3; the Google Drive materials are here:https://drive.google.com/file/d/1ll8WWZDf7cCNcfNWrLViWt8GwDNSvVrp/view.",
    "summary": "",
    "translation": "GPT-5模型纠正了GPT-4V的图表读取错误，而非通过提示工程",
    "relevance_score": 3,
    "reasoning": "该论文主要关注多模态模型的错误纠正能力，属于VLM范畴，但与推荐系统、搜索或广告的直接应用关联较弱。虽然涉及模型迭代改进，但缺乏明确的RecSys/Search/Ads应用场景，仅作为通用能力提升，潜在应用不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06780v1": {
    "title": "Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness",
    "url": "https://www.alphaxiv.org/abs/2510.06780v1",
    "arxiv_id": "2510.06780v1",
    "authors": "Luca Giordano, Simon Razniewski",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-08 09:03:58",
    "ori_summary": "Large Language Models (LLMs) encode substantial factual knowledge, yet measuring and systematizing this knowledge remains challenging. Converting it into structured format, for example through recursive extraction approaches such as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key open questions include whether such extraction can terminate, whether its outputs are reproducible, and how robust they are to variations. We systematically study LLM knowledge materialization using miniGPTKBs (domain-specific, tractable subcrawls), analyzing termination, reproducibility, and robustness across three categories of metrics: yield, lexical similarity, and semantic similarity. We experiment with four variations (seed, language, randomness, model) and three illustrative domains (from history, entertainment, and finance). Our findings show (i) high termination rates, though model-dependent; (ii) mixed reproducibility; and (iii) robustness that varies by perturbation type: high for seeds and temperature, lower for languages and models. These results suggest that LLM knowledge materialization can reliably surface core knowledge, while also revealing important limitations.",
    "summary": "",
    "translation": "大语言模型知识物化基础：终止性、可复现性与鲁棒性",
    "relevance_score": 6,
    "reasoning": "该论文聚焦LLM知识物化的基础理论问题，属于'使能LLM技术'范畴。虽然不直接涉及推荐系统或搜索应用，但知识物化的终止性、可复现性和鲁棒性对于构建可靠的LLM增强推荐/搜索系统至关重要，可确保模型在工业场景下的稳定性和一致性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06774v1": {
    "title": "Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition",
    "url": "https://www.alphaxiv.org/abs/2510.06774v1",
    "arxiv_id": "2510.06774v1",
    "authors": "Lei Xu, Pierre Beckmann, Marco Valentino, André Freitas",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 08:57:16",
    "ori_summary": "Neuro-symbolic NLP methods aim to leverage the complementary strengths of large language models and formal logical solvers. However, current approaches are mostly static in nature, i.e., the integration of a target solver is predetermined at design time, hindering the ability to employ diverse formal inference strategies. To address this, we introduce an adaptive, multi-paradigm, neuro-symbolic inference framework that: (1) automatically identifies formal reasoning strategies from problems expressed in natural language; and (2) dynamically selects and applies specialized formal logical solvers via autoformalization interfaces. Extensive experiments on individual and multi-paradigm reasoning tasks support the following conclusions: LLMs are effective at predicting the necessary formal reasoning strategies with an accuracy above 90 percent. This enables flexible integration with formal logical solvers, resulting in our framework outperforming competing baselines by 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively. Moreover, adaptive reasoning can even positively impact pure LLM methods, yielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT settings with GPT-4o. Finally, although smaller models struggle with adaptive neuro-symbolic reasoning, post-training offers a viable path to improvement. Overall, this work establishes the foundations for adaptive LLM-symbolic reasoning, offering a path forward for unifying material and formal inferences on heterogeneous reasoning challenges.",
    "summary": "论文研究如何动态整合大语言模型与形式逻辑求解器，核心思想是通过自动识别自然语言问题中的推理策略，动态选择和组合专门的形式逻辑求解器来实现自适应神经符号推理。",
    "translation": "基于动态逻辑求解器组合的自适应大语言模型符号推理",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能LLM技术'范畴，专注于增强LLM的符号推理能力。动态逻辑求解器组合技术可应用于搜索和推荐系统中的复杂查询理解、多约束条件推理以及用户意图的精确解析，这对于提升搜索相关性和推荐准确性具有重要价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的动态逻辑求解器组合框架直接属于LLM符号推理的核心技术，对推荐系统中复杂推理任务具有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06761v1": {
    "title": "Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration",
    "url": "https://www.alphaxiv.org/abs/2510.06761v1",
    "arxiv_id": "2510.06761v1",
    "authors": "Zhi Zhang, Yan Liu, Zhejing Hu, Gong Chen, Sheng-hua Zhong, Jiannong Cao",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-08 08:40:58",
    "ori_summary": "Automating the end-to-end scientific research process poses a fundamental challenge: it requires both evolving high-level plans that are novel and sound, and executing these plans correctly amidst dynamic and uncertain conditions. To address this bilevel challenge, we propose a novel Double-Loop Multi-Agent (DLMA) framework to solve the given research problem automatically. The leader loop, composed of professor agents, is responsible for evolving research plans. It employs an evolutionary algorithm through involvement, improvement, and integration meetings to iteratively generate and refine a pool of research proposals, exploring the solution space effectively. The follower loop, composed of doctoral student agents, is responsible for executing the best-evolved plan. It dynamically adjusts the plan during implementation via pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is well-supported by contextual and external observations. Extensive experiments on benchmarks like ACLAward and Laboratory show that DLMA generates research papers that achieve state-of-the-art scores in automated evaluation, significantly outperforming strong baselines. Ablation studies confirm the critical roles of both loops, with evolution driving novelty and execution ensuring soundness.",
    "summary": "",
    "translation": "通过双循环多智能体协作演进与执行研究计划",
    "relevance_score": 3,
    "reasoning": "该论文主要关注多智能体协作的研究计划管理，属于通用AI系统架构范畴。虽然多智能体系统在推荐和搜索中有潜在应用（如多智能体协同决策），但论文标题未明确指向推荐系统、搜索或广告领域的特定技术需求，也未涉及LLM、Transformer架构或异构数据建模等核心关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06750v1": {
    "title": "Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.06750v1",
    "arxiv_id": "2510.06750v1",
    "authors": "Jaeseong Lee, Dayoung Kwon, seung-won hwang",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 08:17:57",
    "ori_summary": "Large Reasoning Models (LRMs) excel in structured tasks by emulating deliberate human reasoning but often suffer from overthinking, degrading performance and wasting resources. One possible baseline is to deploy both LLM and LRM, then route input by predicting whether it requires reasoning and may cause overthinking. However, deploying multiple models can be costly or impractical. We propose a superposed deployment strategy with a lightweight, training-free regulation to optimize inference by switching one model on and off. Instead of routing, we selectively unlearn from LRM at inference, scaling down computation while preserving reasoning. By analyzing the cumulative energy of singular values, we identify optimal low-rank projections to adjust reasoning just right.",
    "summary": "该论文研究大型推理模型因过度思考导致的性能下降和资源浪费问题，核心方法是基于奇异值累积能量分析，通过低秩投影在推理时选择性遗忘，实现慢速与快速思维模型的叠加部署。",
    "translation": "Gold-Switch：免训练叠加慢思考与快思考大语言模型",
    "relevance_score": 8,
    "reasoning": "该论文提出的免训练叠加慢思考与快思考LLM方法属于使能LLM技术范畴，通过组合不同推理速度的模型来提高效率。这种方法在推荐系统和搜索中有直接应用潜力，可以动态平衡精度与延迟，例如在用户查询时快速返回初步结果，同时后台进行更深入的推理分析。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出无需训练的动态推理调控方法，通过奇异值分析实现模型计算效率优化，直接适用于推荐和搜索系统的大规模部署场景。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06749v1": {
    "title": "A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction",
    "url": "https://www.alphaxiv.org/abs/2510.06749v1",
    "arxiv_id": "2510.06749v1",
    "authors": "Eitan Klinger, Zihao Huang, Tran Minh Nguyen, Emma Jayeon Park, Yige Chen, Yang Gu, Qingyu Gao, Siliang Liu, Mengyang Qiu, Jungyeul Park",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 08:15:44",
    "ori_summary": "Evaluating grammatical error correction requires metrics that reflect the diversity of valid human corrections rather than privileging a single reference. Existing frameworks, largely edit-based and English-centric, rely on rigid alignments between system and reference edits, limiting their applicability in multilingual and generative settings. This paper introduces a formal framework for \\textit{fluency-based multi-reference evaluation}, framing $n$-gram similarity as an aggregation problem over multiple legitimate corrections. Within this formulation, we instantiate GLEU through four aggregation strategies--\\textsc{select-best}, \\textsc{simple-average}, \\textsc{weighted-average}, and \\textsc{merged-counts}--and analyze their properties of boundedness, monotonicity, and sensitivity to reference variation. Empirical results on Czech, Estonian, Ukrainian, and Chinese corpora show that these strategies capture complementary aspects of fluency and coverage. The framework unifies multi-reference evaluation into a principled, fluency-oriented approach that incorporates linguistic diversity without penalizing legitimate variation.",
    "summary": "",
    "translation": "基于流畅度的多参考评估在语法错误纠正中的形式化框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于语法错误纠正的评估方法，属于纯粹的NLP评估基准主题。虽然LLM技术可能用于语法纠正，但论文本身关注的是评估框架而非核心的推荐系统、搜索或广告应用，也没有涉及Transformer架构改进或异构数据建模等使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06747v1": {
    "title": "TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.06747v1",
    "arxiv_id": "2510.06747v1",
    "authors": "I-Fan Lin, Faegheh Hasibi, Suzan Verberne",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 08:05:39",
    "ori_summary": "In this paper, we propose a training-free and label-free method for short text clustering that can be used on top of any existing embedder. In the context of customer-facing chatbots, companies are dealing with large amounts of user utterances that need to be clustered according to their intent. In these commercial settings, no labeled data is typically available, and the number of clusters is not known. Our method is based on iterative vector updating: it constructs sparse vectors based on representative texts, and then iteratively refines them through LLM guidance. Our method achieves comparable or superior results to state-of-the-art methods that use contrastive learning, but without assuming prior knowledge of clusters or labels. Experiments on diverse datasets and smaller LLMs show that our method is model agnostic and can be applied to any embedder, with relatively small LLMs, and different clustering methods. We also show that our method scales to large datasets, reducing the computational cost of the LLM. These low-resource, adaptable settings and the scalability of our method make it more aligned with real-world scenarios than existing clustering methods.",
    "summary": "",
    "translation": "TWIST：基于大语言模型通过迭代向量更新的免训练免标签短文本聚类方法",
    "relevance_score": 6,
    "reasoning": "该论文提出了一种免训练免标签的短文本聚类方法，属于直接应用LLM技术进行文本处理。在搜索和推荐系统中，短文本聚类可用于用户查询理解、内容分类和用户兴趣挖掘等任务，具有直接的应用价值。虽然该方法不涉及推荐系统的核心排序或匹配算法，但作为文本预处理和特征提取技术，能够增强系统的语义理解能力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06743v1": {
    "title": "Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities",
    "url": "https://www.alphaxiv.org/abs/2510.06743v1",
    "arxiv_id": "2510.06743v1",
    "authors": "Maria Levchenko",
    "categories": "cs.CV, cs.AI, cs.CL, 68T50",
    "pub_date": "2025-10-08 08:01:40",
    "ori_summary": "Digital humanities scholars increasingly use Large Language Models for historical document digitization, yet lack appropriate evaluation frameworks for LLM-based OCR. Traditional metrics fail to capture temporal biases and period-specific errors crucial for historical corpus creation. We present an evaluation methodology for LLM-based historical OCR, addressing contamination risks and systematic biases in diplomatic transcription. Using 18th-century Russian Civil font texts, we introduce novel metrics including Historical Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside protocols for contamination control and stability testing. We evaluate 12 multimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR while exhibiting over-historicization: inserting archaic characters from incorrect historical periods. Post-OCR correction degrades rather than improves performance. Our methodology provides digital humanities practitioners with guidelines for model selection and quality assessment in historical corpus digitization.",
    "summary": "",
    "translation": "评估大语言模型在历史文档光学字符识别中的应用：数字人文领域的方法论框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于历史文档OCR在数字人文领域的应用，属于特定领域应用而非核心推荐系统、搜索或广告技术。虽然涉及LLM评估，但应用场景（历史文档、数字人文）与我的关注领域完全无关，且不涉及任何推荐、搜索或广告相关的技术或潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06738v1": {
    "title": "AWM: Accurate Weight-Matrix Fingerprint for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.06738v1",
    "arxiv_id": "2510.06738v1",
    "authors": "Boyi Zeng, Lin Chen, Ziwei He, Xinbing Wang, Zhouhan Lin",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 07:51:11",
    "ori_summary": "Protecting the intellectual property of large language models (LLMs) is crucial, given the substantial resources required for their training. Consequently, there is an urgent need for both model owners and third parties to determine whether a suspect LLM is trained from scratch or derived from an existing base model. However, the intensive post-training processes that models typically undergo-such as supervised fine-tuning, extensive continued pretraining, reinforcement learning, multi-modal extension, pruning, and upcycling-pose significant challenges to reliable identification. In this work, we propose a training-free fingerprinting method based on weight matrices. We leverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel Alignment (CKA) similarity to neutralize the effects of parameter manipulations, yielding a highly robust and high-fidelity similarity metric. On a comprehensive testbed of 60 positive and 90 negative model pairs, our method demonstrates exceptional robustness against all six aforementioned post-training categories while exhibiting a near-zero risk of false positives. By achieving perfect scores on all classification metrics, our approach establishes a strong basis for reliable model lineage verification. Moreover, the entire computation completes within 30s on an NVIDIA 3090 GPU. The code is available at https://github.com/LUMIA-Group/AWM.",
    "summary": "",
    "translation": "AWM：面向大语言模型的精确权重矩阵指纹方法",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及'指纹'技术，这属于明确排除的'Fingerprint'相关主题。权重矩阵指纹主要用于模型识别、安全验证或版权保护，与推荐系统、搜索或广告的核心技术进展没有直接关联，也不属于LLM架构效率、Transformer改进或异质数据建模等关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06730v1": {
    "title": "PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.06730v1",
    "arxiv_id": "2510.06730v1",
    "authors": "Manuel Frank, Haithem Afli",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 07:37:19",
    "ori_summary": "Current evaluations of sentence embedding models typically rely on static test beds such as the Massive Text Embedding Benchmark (MTEB). While invaluable, repeated tuning on a fixed suite can inflate reported performance and obscure real-world robustness. We introduce the Paraphrasing Text Embedding Benchmark (PTEB), a dynamic protocol that stochastically generates meaning-preserving paraphrases at evaluation time and aggregates results across multiple runs. Using a cost-efficient LLM-based method grounded in semantic textual similarity gold ratings, we show that LLMs generate token-diverse but semantically preserving, paraphrases. Across 7 MTEB tasks, we validate our hypothesis that the performance of sentence encoders is sensitive to changes in token space even when semantics remain fixed. We also observe that smaller models are not disproportionately affected relative to larger ones. Our results are statistically robust over multiple runs and we extended our experiments to 3 multilingual datasets covering 10 languages. More generally, we aim to propose a new evaluation paradigm in NLP that relies less on static, pre-defined benchmarks but shifts towards dynamic, stochastic evaluation leveraging eval-time compute.",
    "summary": "",
    "translation": "PTEB：通过LLM在评估时进行随机释义实现鲁棒文本嵌入评估",
    "relevance_score": 2,
    "reasoning": "该论文专注于文本嵌入评估方法，属于纯NLP评估基准范畴，与推荐系统、搜索或广告的核心技术进展无关。虽然提到了LLMs，但仅作为生成释义的工具，没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06727v1": {
    "title": "Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management",
    "url": "https://www.alphaxiv.org/abs/2510.06727v1",
    "arxiv_id": "2510.06727v1",
    "authors": "Miao Lu, Weiwei Sun, Weihua Du, Zhan Ling, Xuesong Yao, Kang Liu, Jiecao Chen",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-08 07:29:22",
    "ori_summary": "We study reinforcement learning (RL) fine-tuning of large language model (LLM) agents for long-horizon multi-turn tool use, where context length quickly becomes a fundamental bottleneck. Existing RL pipelines can suffer from degraded instruction following, excessive rollout costs, and most importantly, strict context limits. To address these challenges, we introduce summarization-based context management to training. In specific, it periodically compresses the tool using history by LLM-generated summaries that retain task-relevant information to keep a compact context while enabling the agent to scale beyond the fixed context window. Building on this formulation, we derive a policy gradient representation that seamlessly enables standard LLM RL infrastructures to optimize both tool-use behaviors as well as summarization strategies in an end-to-end fashion. We instantiate this framework with \\underline{SU}mmarization augmented \\underline{P}olicy \\underline{O}ptimization (\\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond a fixed context limit. Experiments on interactive function calling and searching tasks demonstrate that \\texttt{SUPO} significantly improves the success rate while maintaining the same or even lower working context length compared to baselines. We also demonstrate that for complex searching tasks, \\texttt{SUPO} can further improve the evaluation performance when scaling test-time maximum round of summarization beyond that of training time. Our results establish summarization-based context management as a principled and scalable approach for training RL agents beyond a fixed context length limit.",
    "summary": "论文研究LLM在多轮工具使用中的上下文长度瓶颈问题，核心方法是引入摘要式上下文管理，通过LLM生成的任务相关摘要压缩历史信息，实现端到端联合优化工具使用行为和摘要策略。",
    "translation": "基于端到端摘要化上下文管理的可扩展LLM多轮强化学习",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM多轮强化学习的高效扩展技术，这属于'Enabling LLM Tech'范畴，通过端到端摘要化上下文管理提升RL效率。在推荐系统和搜索场景中，这种技术可显著改善多轮对话推荐、会话搜索的上下文管理和长期用户交互优化，实现更高效的多轮决策过程。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM在推荐/搜索系统多轮交互中的核心瓶颈——上下文长度限制，提出端到端的摘要式上下文管理框架，与多轮推荐和搜索场景高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06719v1": {
    "title": "Differentially Private Synthetic Text Generation for Retrieval-Augmented Generation (RAG)",
    "url": "https://www.alphaxiv.org/abs/2510.06719v1",
    "arxiv_id": "2510.06719v1",
    "authors": "Junki Mori, Kazuya Kakizaki, Taiki Miyagawa, Jun Sakuma",
    "categories": "cs.CR, cs.CL, cs.LG",
    "pub_date": "2025-10-08 07:15:50",
    "ori_summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by grounding them in external knowledge. However, its application in sensitive domains is limited by privacy risks. Existing private RAG methods typically rely on query-time differential privacy (DP), which requires repeated noise injection and leads to accumulated privacy loss. To address this issue, we propose DP-SynRAG, a framework that uses LLMs to generate differentially private synthetic RAG databases. Unlike prior methods, the synthetic text can be reused once created, thereby avoiding repeated noise injection and additional privacy costs. To preserve essential information for downstream RAG tasks, DP-SynRAG extends private prediction, which instructs LLMs to generate text that mimics subsampled database records in a DP manner. Experiments show that DP-SynRAG achieves superior performanec to the state-of-the-art private RAG systems while maintaining a fixed privacy budget, offering a scalable solution for privacy-preserving RAG.",
    "summary": "",
    "translation": "面向检索增强生成（RAG）的差分隐私合成文本生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注差分隐私技术，这属于隐私保护范畴，被明确列为不相关主题。虽然RAG技术在搜索系统中可能有应用，但论文的核心焦点是隐私保护而非核心推荐/搜索系统进展或LLM技术本身。差分隐私技术本身没有直接的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06706v1": {
    "title": "XLSR-Kanformer: A KAN-Intergrated model for Synthetic Speech Detection",
    "url": "https://www.alphaxiv.org/abs/2510.06706v1",
    "arxiv_id": "2510.06706v1",
    "authors": "Phuong Tuan Dat, Tran Huy Dat",
    "categories": "cs.SD, cs.CL, eess.AS",
    "pub_date": "2025-10-08 06:58:58",
    "ori_summary": "Recent advancements in speech synthesis technologies have led to increasingly sophisticated spoofing attacks, posing significant challenges for automatic speaker verification systems. While systems based on self-supervised learning (SSL) models, particularly the XLSR-Conformer architecture, have demonstrated remarkable performance in synthetic speech detection, there remains room for architectural improvements. In this paper, we propose a novel approach that replaces the traditional Multi-Layer Perceptron (MLP) in the XLSR-Conformer model with a Kolmogorov-Arnold Network (KAN), a powerful universal approximator based on the Kolmogorov-Arnold representation theorem. Our experimental results on ASVspoof2021 demonstrate that the integration of KAN to XLSR-Conformer model can improve the performance by 60.55% relatively in Equal Error Rate (EER) LA and DF sets, further achieving 0.70% EER on the 21LA set. Besides, the proposed replacement is also robust to various SSL architectures. These findings suggest that incorporating KAN into SSL-based models is a promising direction for advances in synthetic speech detection.",
    "summary": "",
    "translation": "XLSR-Kanformer：一种集成KAN的合成语音检测模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于合成语音检测，属于语音处理领域，与推荐系统、搜索或广告没有直接关联。KAN架构虽然是一种新的神经网络方法，但论文的应用场景（语音检测）在指定的无关主题范围内，没有展示在RecSys/Search/Ads中的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06700v1": {
    "title": "How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects",
    "url": "https://www.alphaxiv.org/abs/2510.06700v1",
    "arxiv_id": "2510.06700v1",
    "authors": "Leonardo Bertolazzi, Sandro Pezzelle, Raffaelle Bernardi",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 06:48:08",
    "ori_summary": "Both humans and large language models (LLMs) exhibit content effects: biases in which the plausibility of the semantic content of a reasoning problem influences judgments regarding its logical validity. While this phenomenon in humans is best explained by the dual-process theory of reasoning, the mechanisms behind content effects in LLMs remain unclear. In this work, we address this issue by investigating how LLMs encode the concepts of validity and plausibility within their internal representations. We show that both concepts are linearly represented and strongly aligned in representational geometry, leading models to conflate plausibility with validity. Using steering vectors, we demonstrate that plausibility vectors can causally bias validity judgements, and vice versa, and that the degree of alignment between these two concepts predicts the magnitude of behavioral content effects across models. Finally, we construct debiasing vectors that disentangle these concepts, reducing content effects and improving reasoning accuracy. Our findings advance understanding of how abstract logical concepts are represented in LLMs and highlight representational interventions as a path toward more logical systems.",
    "summary": "",
    "translation": "语言模型如何混淆逻辑有效性与合理性：内容效应的表征分析",
    "relevance_score": 2,
    "reasoning": "该论文主要分析语言模型在逻辑推理中的混淆问题，属于LLM评估和认知偏差研究范畴。虽然涉及LLM表征分析，但焦点是逻辑推理而非推荐/搜索/广告应用，且内容效应分析更偏向理论认知研究，与当前关注的推荐系统、搜索广告技术进展关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06695v1": {
    "title": "Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks",
    "url": "https://www.alphaxiv.org/abs/2510.06695v1",
    "arxiv_id": "2510.06695v1",
    "authors": "Qinhao Zhou, Xiang Xiang, Kun He, John E. Hopcroft",
    "categories": "cs.CL, cs.AI, cs.LG, eess.AS",
    "pub_date": "2025-10-08 06:40:06",
    "ori_summary": "In recent years, the growing interest in Large Language Models (LLMs) has significantly advanced prompt engineering, transitioning from manual design to model-based optimization. Prompts for LLMs generally comprise two components: the \\textit{instruction}, which defines the task or objective, and the \\textit{input}, which is tailored to the instruction type. In natural language generation (NLG) tasks such as machine translation, the \\textit{input} component is particularly critical, while the \\textit{instruction} component tends to be concise. Existing prompt engineering methods primarily focus on optimizing the \\textit{instruction} component for general tasks, often requiring large-parameter LLMs as auxiliary tools. However, these approaches exhibit limited applicability for tasks like machine translation, where the \\textit{input} component plays a more pivotal role. To address this limitation, this paper introduces a novel prompt optimization method specifically designed for machine translation tasks. The proposed approach employs a small-parameter model trained using a back-translation-based strategy, significantly reducing training overhead for single-task optimization while delivering highly effective performance. With certain adaptations, this method can also be extended to other downstream tasks.",
    "summary": "论文研究机器翻译等下游任务中LLM提示优化问题，核心思想是使用基于反向翻译策略训练的小参数模型专门优化提示中的输入组件，而非传统的大模型指令优化方法。",
    "translation": "学习重写提示以引导LLM在下游任务上进行自举学习",
    "relevance_score": 8,
    "reasoning": "该论文涉及提示工程和LLM自举技术，这属于'直接LLM应用'范畴，对搜索和推荐系统有直接应用价值。通过优化提示重写，可以显著提升LLM在推荐、搜索排序等下游任务中的性能表现，减少对大量标注数据的依赖。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出的小参数模型提示重写方法可直接应用于搜索和推荐系统的查询优化，其输入组件优化思路与推荐系统的用户输入处理高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06677v1": {
    "title": "Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback",
    "url": "https://www.alphaxiv.org/abs/2510.06677v1",
    "arxiv_id": "2510.06677v1",
    "authors": "Yisha Wu, Cen, Zhao, Yuanpei Cao, Xiaoqing Su, Yashar Mehdad, Mindy Ji, Claire Na Cheng",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-08 06:05:58",
    "ori_summary": "We introduce an incremental summarization system for customer support agents that intelligently determines when to generate concise bullet notes during conversations, reducing agents' context-switching effort and redundant review. Our approach combines a fine-tuned Mixtral-8x7B model for continuous note generation with a DeBERTa-based classifier to filter trivial content. Agent edits refine the online notes generation and regularly inform offline model retraining, closing the agent edits feedback loop. Deployed in production, our system achieved a 3% reduction in case handling time compared to bulk summarization (with reductions of up to 9% in highly complex cases), alongside high agent satisfaction ratings from surveys. These results demonstrate that incremental summarization with continuous feedback effectively enhances summary quality and agent productivity at scale.",
    "summary": "",
    "translation": "基于渐进式笔记记录与客服反馈的客户支持增量式摘要生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注客户支持场景下的增量式摘要生成，属于纯粹的文本摘要应用，与推荐系统、搜索或广告的核心技术进展无关。虽然涉及用户交互（客服反馈），但这属于特定领域的对话系统应用，而非RecSys/Search/Ads领域的核心技术或使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06670v1": {
    "title": "PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch",
    "url": "https://www.alphaxiv.org/abs/2510.06670v1",
    "arxiv_id": "2510.06670v1",
    "authors": "Shangjian Yin, Shining Liang, Wenbiao Ding, Yuli Qian, Zhouxing Shi, Hongzhi Li, Yutao Xie",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 05:47:37",
    "ori_summary": "Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone for aligning large language models (LLMs). However, its effectiveness depends on high-quality instruction data. Most existing alignment datasets are either private or require costly human annotation, which limits reproducibility and scalability. Even with Reinforcement Learning from AI Feedback (RLAIF), concerns about data quality remain. Moreover, it is unclear how much data is actually required to fine-tune a base model into a strong instruction-following model. Current approaches often rely on over 300k examples even at the supervised fine-tuning (SFT) stage, yet they still underperform compared to proprietary models, creating barriers for academic and resource-limited communities. To address this gap, we introduce PiKa, a data-efficient family of expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only 30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets, we show that PiKa-SFT outperforms models trained on much larger data. On AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses the official Llama-3-8B-Instruct model trained on over 10 million proprietary examples. We further extend our study by training the Qwen2.5 series (0.5B to 7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that high-quality alignment can be achieved with significantly less data, offering a scalable path for open-source LLM alignment. Code and data: https://github.com/SJY8460/PiKa.",
    "summary": "论文研究如何解决LLM对齐过程中对大规模高质量指令数据的依赖问题，核心思想是通过构建专家级合成数据集，用远少于现有方法的数据量实现高质量模型对齐。",
    "translation": "PIKA：从零开始用于后训练对齐的专家级合成数据集",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能LLM技术'范畴，专注于通过合成数据实现后训练对齐，这是LLM发展的核心进展。在推荐系统、搜索和广告领域，合成数据集可用于对齐特定领域的LLM模型，提高其在商业场景中的安全性和性能表现，避免有害输出并优化用户体验。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文专注于LLM对齐的高质量数据集生成，虽然不直接应用于推荐系统，但其数据高效方法对需要高质量训练数据的推荐和搜索模型有重要参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06664v1": {
    "title": "ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory",
    "url": "https://www.alphaxiv.org/abs/2510.06664v1",
    "arxiv_id": "2510.06664v1",
    "authors": "Yunzhong Xiao, Yangmin Li, Hewei Wang, Yunlong Tang, Zora Zhiruo Wang",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 05:32:31",
    "ori_summary": "Agents utilizing tools powered by large language models (LLMs) or vision-language models (VLMs) have demonstrated remarkable progress in diverse tasks across text and visual modalities. Unlike traditional tools such as calculators, which give deterministic outputs, neural tools perform uncertainly across task scenarios. While different tools for a task may excel in varied scenarios, existing agents typically rely on fixed tools, thus limiting the flexibility in selecting the most suitable tool for specific tasks. In contrast, humans snowball their understanding of the capabilities of different tools by interacting with them, and apply this knowledge to select the optimal tool when solving a future task. To build agents that similarly benefit from this process, we propose ToolMem that enables agents to develop memories of tool capabilities from previous interactions, by summarizing their strengths and weaknesses and storing them in memory; at inference, the agent can retrieve relevant entries from ToolMem, and select the best tool to solve individual tasks more accurately. We evaluate ToolMem on learning varied text generation and text-to-image generation neural tools. Compared to no-memory, generic agents, we find ToolMem-augmented agents predict tool performance 14.8% and 28.7% more accurately across text and multimodal generation scenarios. Moreover, ToolMem facilitates optimal tool selection among multiple choices by 21% and 24% absolute increases in respective scenarios.",
    "summary": "",
    "translation": "ToolMem：通过可学习的工具能力记忆增强多模态智能体",
    "relevance_score": 6,
    "reasoning": "该论文涉及多模态智能体和工具学习，属于使能LLM技术范畴。在推荐系统或搜索中，这种工具能力记忆机制可以用于构建更智能的对话推荐系统，让智能体记住并有效利用各种推荐工具（如用户画像分析、物品匹配、上下文理解等），提升推荐交互的质量和效率。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06652v1": {
    "title": "Aligning Large Language Models via Fully Self-Synthetic Data",
    "url": "https://www.alphaxiv.org/abs/2510.06652v1",
    "arxiv_id": "2510.06652v1",
    "authors": "Shangjian Yin, Zhepei Wei, Xinyu Zhu, Wei-Lin Chen, Yu Meng",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 05:07:45",
    "ori_summary": "Traditional reinforcement learning from human feedback (RLHF) for large language models (LLMs) relies on expensive human-annotated datasets, while Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs, requiring the collection of diverse prompts and corresponding responses, often necessitating external reward models or proprietary models like GPT-4 to annotate preference pairs. In this work, we introduce Self-Alignment Optimization (SAO), a fully self-synthetic framework for LLM alignment, where all training data, including prompts (i.e., user queries), responses, and preferences, are generated by the model itself. Specifically, SAO first instructs the LLM to engage in persona role-play and generate diverse prompts and responses, which are then self-evaluated for preference optimization. Extensive experiments demonstrate that SAO effectively enhances the model's chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining strong performance on downstream objective tasks (e.g., question-answering, math reasoning). Our work provides a practical solution for self-improvement in aligning LLMs, and the code for reproducing our results is available at: https://github.com/SJY8460/SAO.",
    "summary": "",
    "translation": "通过完全自合成数据对齐大型语言模型",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能LLM技术'范畴，专注于LLM对齐这一核心挑战。在推荐系统、搜索和广告中，对齐技术可以显著提升LLM在理解用户意图、生成相关内容和遵循商业约束方面的能力，从而改善用户体验和商业效果。完全自合成数据的方法为大规模部署提供了成本效益高的解决方案。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06640v1": {
    "title": "A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures",
    "url": "https://www.alphaxiv.org/abs/2510.06640v1",
    "arxiv_id": "2510.06640v1",
    "authors": "Nhat M. Hoang, Do Xuan Long, Cong-Duy Nguyen, Min-Yen Kan, Luu Anh Tuan",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-08 04:46:11",
    "ori_summary": "State Space Models (SSMs) have recently emerged as efficient alternatives to Transformer-Based Models (TBMs) for long-sequence processing, offering linear scaling and lower memory use. Yet, how contextual information flows across layers and tokens in these architectures remains understudied. We present the first unified, token- and layer-level analysis of representation propagation in SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing, we characterize how representations evolve within and across layers. We find a key divergence: TBMs rapidly homogenize token representations, with diversity reemerging only in later layers, while SSMs preserve token uniqueness early but converge to homogenization deeper. Theoretical analysis and parameter randomization further reveal that oversmoothing in TBMs stems from architectural design, whereas in SSMs it arises mainly from training dynamics. These insights clarify the inductive biases of both architectures and inform future model and training designs for long-context reasoning.",
    "summary": "论文研究状态空间模型和Transformer架构中上下文表示传播的核心差异问题，核心发现是Transformer通过架构设计导致早期表示同质化而后期重新分化，状态空间模型则呈现相反的传播模式。",
    "translation": "状态空间与Transformer架构中上下文表示流的对比分析",
    "relevance_score": 8,
    "reasoning": "该论文直接比较状态空间模型（如Mamba）与Transformer架构的表示流特性，属于Transformer架构效率研究的核心范畴。这种对比分析对于开发更高效的序列建模架构具有重要价值，可应用于推荐系统中的长序列用户行为建模和搜索中的长文本理解，提升系统效率同时保持性能。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文深入分析Transformer架构的表示传播机制，直接关联Transformer技术进展，对理解模型内部工作方式具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.06605v1": {
    "title": "Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation",
    "url": "https://www.alphaxiv.org/abs/2510.06605v1",
    "arxiv_id": "2510.06605v1",
    "authors": "Shuo Shao, Yiming Li, Hongwei Yao, Yifei Chen, Yuchen Yang, Zhan Qin",
    "categories": "cs.CR, cs.AI, cs.CL",
    "pub_date": "2025-10-08 03:27:38",
    "ori_summary": "The substantial investment required to develop Large Language Models (LLMs) makes them valuable intellectual property, raising significant concerns about copyright protection. LLM fingerprinting has emerged as a key technique to address this, which aims to verify a model's origin by extracting an intrinsic, unique signature (a \"fingerprint\") and comparing it to that of a source model to identify illicit copies. However, existing black-box fingerprinting methods often fail to generate distinctive LLM fingerprints. This ineffectiveness arises because black-box methods typically rely on model outputs, which lose critical information about the model's unique parameters due to the usage of non-linear functions. To address this, we first leverage Fisher Information Theory to formally demonstrate that the gradient of the model's input is a more informative feature for fingerprinting than the output. Based on this insight, we propose ZeroPrint, a novel method that approximates these information-rich gradients in a black-box setting using zeroth-order estimation. ZeroPrint overcomes the challenge of applying this to discrete text by simulating input perturbations via semantic-preserving word substitutions. This operation allows ZeroPrint to estimate the model's Jacobian matrix as a unique fingerprint. Experiments on the standard benchmark show ZeroPrint achieves a state-of-the-art effectiveness and robustness, significantly outperforming existing black-box methods.",
    "summary": "",
    "translation": "字里行间：基于零阶梯度估计实现可靠的黑盒大语言模型指纹识别",
    "relevance_score": 1,
    "reasoning": "该论文涉及指纹识别技术，这属于明确列出的无关主题范畴。虽然提到了LLM技术，但核心焦点是模型识别和指纹方法，与推荐系统、搜索或广告的核心技术进展没有直接关联。零阶梯度估计技术本身可能在其他领域有应用，但在此上下文中主要用于指纹识别目的。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06594v1": {
    "title": "Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?",
    "url": "https://www.alphaxiv.org/abs/2510.06594v1",
    "arxiv_id": "2510.06594v1",
    "authors": "Sri Durga Sai Sowmya Kadali, Evangelos E. Papalexakis",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 02:55:31",
    "ori_summary": "Jailbreaking large language models (LLMs) has emerged as a pressing concern with the increasing prevalence and accessibility of conversational LLMs. Adversarial users often exploit these models through carefully engineered prompts to elicit restricted or sensitive outputs, a strategy widely referred to as jailbreaking. While numerous defense mechanisms have been proposed, attackers continuously develop novel prompting techniques, and no existing model can be considered fully resistant. In this study, we investigate the jailbreak phenomenon by examining the internal representations of LLMs, with a focus on how hidden layers respond to jailbreak versus benign prompts. Specifically, we analyze the open-source LLM GPT-J and the state-space model Mamba2, presenting preliminary findings that highlight distinct layer-wise behaviors. Our results suggest promising directions for further research on leveraging internal model dynamics for robust jailbreak detection and defense.",
    "summary": "",
    "translation": "LLM内部层是否揭示越狱检测的模式？",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM安全性和越狱检测，这属于安全、伦理相关主题，已被明确列为不相关主题。虽然涉及LLM内部表征分析，但核心应用是安全检测而非推荐/搜索/广告系统的改进，因此相关性很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06579v1": {
    "title": "TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents",
    "url": "https://www.alphaxiv.org/abs/2510.06579v1",
    "arxiv_id": "2510.06579v1",
    "authors": "Haofei Yu, Keyang Xuan, Fenghai Li, Kunlun Zhu, Zijie Lei, Jiaxun Zhang, Ziheng Qi, Kyle Richardson, Jiaxuan You",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 02:18:57",
    "ori_summary": "Automatic research with Large Language Models (LLMs) is rapidly gaining importance, driving the development of increasingly complex workflows involving multi-agent systems, planning, tool usage, code execution, and human-agent interaction to accelerate research processes. However, as more researchers and developers begin to use and build upon these tools and platforms, the complexity and difficulty of extending and maintaining such agentic workflows have become a significant challenge, particularly as algorithms and architectures continue to advance. To address this growing complexity, TinyScientist identifies the essential components of the automatic research workflow and proposes an interactive, extensible, and controllable framework that easily adapts to new tools and supports iterative growth. We provide an open-source codebase, an interactive web demonstration, and a PyPI Python package to make state-of-the-art auto-research pipelines broadly accessible to every researcher and developer.",
    "summary": "",
    "translation": "TinyScientist：一个用于构建研究型智能体的交互式、可扩展且可控框架",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于构建研究型智能体的通用框架，属于AIGC和智能体开发领域，与推荐系统、搜索或广告的核心技术无直接关联。框架的可扩展性和可控性特征未体现与异构数据建模、Transformer架构改进或LLM在RecSys/Search/Ads中的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06559v1": {
    "title": "The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law",
    "url": "https://www.alphaxiv.org/abs/2510.06559v1",
    "arxiv_id": "2510.06559v1",
    "authors": "Cheonkam Jeong, Sungdo Kim, Jewoo Park",
    "categories": "cs.CL, cs.AI, cs.LO",
    "pub_date": "2025-10-08 01:22:26",
    "ori_summary": "Contemporary language models are fluent yet routinely mis-handle the types of meaning their outputs entail. We argue that hallucination, brittle moderation, and opaque compliance outcomes are symptoms of missing type-theoretic semantics rather than data or scale limitations. Building on Montague's view of language as typed, compositional algebra, we recast alignment as a parsing problem: natural-language inputs must be compiled into structures that make explicit their descriptive, normative, and legal dimensions under context. We present Savassan, a neuro-symbolic architecture that compiles utterances into Montague-style logical forms and maps them to typed ontologies extended with deontic operators and jurisdictional contexts. Neural components extract candidate structures from unstructured inputs; symbolic components perform type checking, constraint reasoning, and cross-jurisdiction mapping to produce compliance-aware guidance rather than binary censorship. In cross-border scenarios, the system \"parses once\" (e.g., defect claim(product x, company y)) and projects the result into multiple legal ontologies (e.g., defamation risk in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into a single, explainable decision. This paper contributes: (i) a diagnosis of hallucination as a type error; (ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii) a production-oriented design that embeds typed interfaces across the pipeline. We outline an evaluation plan using legal reasoning benchmarks and synthetic multi-jurisdiction suites. Our position is that trustworthy autonomy requires compositional typing of meaning, enabling systems to reason about what is described, what is prescribed, and what incurs liability within a unified algebra of meaning.",
    "summary": "",
    "translation": "意义代数：为何机器更需要蒙塔古而非摩尔定律",
    "relevance_score": 2,
    "reasoning": "该论文标题暗示其关注语义理解和形式语义学（蒙塔古语义学），这属于语言学理论范畴，而非推荐系统、搜索或广告领域的技术进展。虽然语义理解是LLM的基础能力之一，但该标题未表明任何具体的架构创新、效率改进或直接应用场景，与当前关注的四大方向均无明显关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06557v1": {
    "title": "The Markovian Thinker",
    "url": "https://www.alphaxiv.org/abs/2510.06557v1",
    "arxiv_id": "2510.06557v1",
    "authors": "Milad Aghajohari, Kamran Chitsaz, Amirhossein Kazemnejad, Sarath Chandar, Alessandro Sordoni, Aaron Courville, Siva Reddy",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-08 01:18:13",
    "ori_summary": "Reinforcement learning (RL) has recently become a strong recipe for training reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard RL \"thinking environment\", where the state is the prompt plus all prior reasoning tokens, makes the state unbounded and forces attention-based policies to pay quadratic compute as thoughts lengthen. We revisit the environment itself. We propose Markovian Thinking, a paradigm in which the policy advances reasoning while conditioning on a constant-size state, decoupling thinking length from context size. As an immediate consequence this yields linear compute with constant memory. We instantiate this idea with Delethink, an RL environment that structures reasoning into fixed-size chunks. Within each chunk, the model thinks as usual; at the boundary, the environment resets the context and reinitializes the prompt with a short carryover. Through RL, the policy learns to write a textual state near the end of each chunk sufficient for seamless continuation of reasoning after reset. Trained in this environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget. With test-time scaling, Delethink continues to improve where LongCoT plateaus. The effect of linear compute is substantial: we empirically estimate at 96K average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink. Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B) often sample Markovian traces zero-shot across diverse benchmarks, providing positive samples that make RL effective at scale. Our results show that redesigning the thinking environment is a powerful lever: it enables very long reasoning without quadratic overhead and opens a path toward efficient, scalable reasoning LLMs.",
    "summary": "",
    "translation": "马尔可夫思考者",
    "relevance_score": 1,
    "reasoning": "该标题暗示了与马尔可夫过程或决策理论相关的内容，但没有明确表明与推荐系统、搜索或广告领域的直接关联。标题过于宽泛，无法识别出任何具体的LLM技术、Transformer架构进展或异构数据建模的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06552v1": {
    "title": "Flipping the Dialogue: Training and Evaluating User Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.06552v1",
    "arxiv_id": "2510.06552v1",
    "authors": "Tarek Naous, Philippe Laban, Wei Xu, Jennifer Neville",
    "categories": "cs.CL",
    "pub_date": "2025-10-08 01:04:36",
    "ori_summary": "Conversations with LMs involve two participants: a human user leading the conversation, and an LM assistant responding to the user's request. To satisfy this specific role, LMs are post-trained to be helpful assistants -- optimized to produce exhaustive and well-structured responses, free of ambiguity and grammar errors. User utterances, on the other hand, are rarely perfected, with each user phrasing requests in unique ways, sometimes putting in partial effort at each turn and refining on the fly. To evaluate LM performance in realistic settings, prior work simulated users in multi-turn conversations, often prompting an LLM originally trained to be a helpful assistant to act as a user. However, we show that assistant LMs make for poor user simulators, with the surprising finding that better assistants yield worse simulators. Instead, we introduce purpose-built User Language Models (User LMs) - models post-trained to simulate human users in multi-turn conversations. Through various evaluations, we show how User LMs align better with human behavior and achieve better simulation robustness than existing simulation methods. When leveraging User LMs to simulate coding and math conversations, the performance of a strong assistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic simulation environments lead to assistant struggles as they fail to cope with the nuances of users in multi-turn setups.",
    "summary": "",
    "translation": "翻转对话：训练与评估用户语言模型",
    "relevance_score": 8,
    "reasoning": "该论文聚焦于用户语言模型的训练与评估，直接属于'直接LLM应用'范畴，在推荐系统和搜索领域具有明确应用价值。用户语言模型可用于个性化对话推荐、用户意图理解、以及基于对话历史的搜索优化，能够显著提升用户体验和系统交互质量。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06548v1": {
    "title": "From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining",
    "url": "https://www.alphaxiv.org/abs/2510.06548v1",
    "arxiv_id": "2510.06548v1",
    "authors": "Seng Pei Liew, Takuya Kato",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-08 00:59:33",
    "ori_summary": "Bootstrapped pretraining, i.e., the reuse of a pretrained base model for further pretraining, such as continual pretraining or model growth, is promising at reducing the cost of training language models from scratch. However, its effectiveness remains unclear, especially when applied to overtrained base models. In this work, we empirically study the scaling behavior of bootstrapped pretraining and find that its scaling efficiency diminishes in a predictable manner: The scaling exponent with respect to second-stage pretraining tokens decreases logarithmically with the number of tokens used to pretrain the base model. The joint dependence on first- and second-stage tokens is accurately modeled by a simple scaling law. Such saturation effect reveals a fundamental trade-off in multi-stage pretraining strategies: the more extensively a model is pretrained, the less additional benefit bootstrapping provides. Our findings provide practical insights for efficient language model training and raise important considerations for the reuse of overtrained models.",
    "summary": "",
    "translation": "从加速到饱和：自举语言模型预训练的缩放行为研究",
    "relevance_score": 8,
    "reasoning": "该论文研究语言模型预训练的缩放行为，属于'Enabling LLM Tech'范畴，探讨LLM训练过程中的基础进展。理解预训练缩放规律对于在搜索、推荐和广告系统中高效部署和优化LLM模型具有直接应用价值，可以帮助确定模型规模与性能的最佳平衡点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07319v1": {
    "title": "Temporal Prompting Matters: Rethinking Referring Video Object Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.07319v1",
    "arxiv_id": "2510.07319v1",
    "authors": "Ci-Siang Lin, Min-Hung Chen, I-Jieh Liu, Chien-Yi Wang, Sifei Liu, Yu-Chiang Frank Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 17:59:57",
    "ori_summary": "Referring Video Object Segmentation (RVOS) aims to segment the object referred to by the query sentence in the video. Most existing methods require end-to-end training with dense mask annotations, which could be computation-consuming and less scalable. In this work, we rethink the RVOS problem and aim to investigate the key to this task. Based on existing foundation segmentation models, we decompose the RVOS task into referring, video, and segmentation factors, and propose a Temporal Prompt Generation and Selection (Tenet) framework to address the referring and video factors while leaving the segmentation problem to foundation models. To efficiently adapt image-based foundation segmentation models to referring video object segmentation, we leverage off-the-shelf object detectors and trackers to produce temporal prompts associated with the referring sentence. While high-quality temporal prompts could be produced, they can not be easily identified from confidence scores. To tackle this issue, we propose Prompt Preference Learning to evaluate the quality of the produced temporal prompts. By taking such prompts to instruct image-based foundation segmentation models, we would be able to produce high-quality masks for the referred object, enabling efficient model adaptation to referring video object segmentation. Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenet framework.",
    "summary": "",
    "translation": "时序提示至关重要：重新思考指代视频目标分割",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频目标分割的计算机视觉任务，虽然涉及时序建模，但核心是纯粹的视觉理解问题。没有明确的机制或方法可以应用于推荐系统、搜索或广告领域，与我的关注点缺乏直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07317v1": {
    "title": "Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms",
    "url": "https://www.alphaxiv.org/abs/2510.07317v1",
    "arxiv_id": "2510.07317v1",
    "authors": "Natacha Kuete Meli, Shuteng Wang, Marcel Seelbach Benkner, Michele Sasdelli, Tat-Jun Chin, Tolga Birdal, Michael Moeller, Vladislav Golyanik",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 17:59:51",
    "ori_summary": "Quantum-enhanced Computer Vision (QeCV) is a new research field at the intersection of computer vision, optimisation theory, machine learning and quantum computing. It has high potential to transform how visual signals are processed and interpreted with the help of quantum computing that leverages quantum-mechanical effects in computations inaccessible to classical (i.e. non-quantum) computers. In scenarios where existing non-quantum methods cannot find a solution in a reasonable time or compute only approximate solutions, quantum computers can provide, among others, advantages in terms of better time scalability for multiple problem classes. Parametrised quantum circuits can also become, in the long term, a considerable alternative to classical neural networks in computer vision. However, specialised and fundamentally new algorithms must be developed to enable compatibility with quantum hardware and unveil the potential of quantum computational paradigms in computer vision. This survey contributes to the existing literature on QeCV with a holistic review of this research field. It is designed as a quantum computing reference for the computer vision community, targeting computer vision students, scientists and readers with related backgrounds who want to familiarise themselves with QeCV. We provide a comprehensive introduction to QeCV, its specifics, and methodologies for formulations compatible with quantum hardware and QeCV methods, leveraging two main quantum computational paradigms, i.e. gate-based quantum computing and quantum annealing. We elaborate on the operational principles of quantum computers and the available tools to access, program and simulate them in the context of QeCV. Finally, we review existing quantum computing tools and learning materials and discuss aspects related to publishing and reviewing QeCV papers, open challenges and potential social implications.",
    "summary": "",
    "translation": "量子增强计算机视觉：超越经典算法",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于量子计算在计算机视觉领域的应用，属于纯粹的视觉研究方向。虽然标题提到'超越经典算法'，但内容明确限定在计算机视觉领域，与推荐系统、搜索或广告的核心技术栈没有直接关联。量子计算在视觉领域的进展对于RecSys/Search/Ads的潜在应用过于间接和推测性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07316v1": {
    "title": "Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers",
    "url": "https://www.alphaxiv.org/abs/2510.07316v1",
    "arxiv_id": "2510.07316v1",
    "authors": "Gangwei Xu, Haotong Lin, Hongcheng Luo, Xianqi Wang, Jingfeng Yao, Lianghui Zhu, Yuechuan Pu, Cheng Chi, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Sida Peng, Xin Yang",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 17:59:33",
    "ori_summary": "This paper presents Pixel-Perfect Depth, a monocular depth estimation model based on pixel-space diffusion generation that produces high-quality, flying-pixel-free point clouds from estimated depth maps. Current generative depth estimation models fine-tune Stable Diffusion and achieve impressive performance. However, they require a VAE to compress depth maps into latent space, which inevitably introduces \\textit{flying pixels} at edges and details. Our model addresses this challenge by directly performing diffusion generation in the pixel space, avoiding VAE-induced artifacts. To overcome the high complexity associated with pixel-space generation, we introduce two novel designs: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which incorporate semantic representations from vision foundation models into DiT to prompt the diffusion process, thereby preserving global semantic consistency while enhancing fine-grained visual details; and 2) Cascade DiT Design that progressively increases the number of tokens to further enhance efficiency and accuracy. Our model achieves the best performance among all published generative models across five benchmarks, and significantly outperforms all other models in edge-aware point cloud evaluation.",
    "summary": "",
    "translation": "基于语义提示扩散变换器的像素级完美深度估计",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的深度估计任务，使用扩散变换器技术实现像素级精度。虽然涉及变换器架构，但其核心应用场景是视觉感知而非推荐系统、搜索或广告领域，与当前关注点的直接关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07313v1": {
    "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation",
    "url": "https://www.alphaxiv.org/abs/2510.07313v1",
    "arxiv_id": "2510.07313v1",
    "authors": "Zezhong Qian, Xiaowei Chi, Yuming Li, Shizun Wang, Zhiyuan Qin, Xiaozhu Ju, Sirui Han, Shanghang Zhang",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-08 17:59:08",
    "ori_summary": "Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhance manipulation performance. Yet large-scale datasets rarely include such recordings, resulting in a substantial gap between abundant anchor views and scarce wrist views. Existing world models cannot bridge this gap, as they require a wrist-view first frame and thus fail to generate wrist-view videos from anchor views alone. Amid this gap, recent visual geometry models such as VGGT emerge with geometric and cross-view priors that make it possible to address extreme viewpoint shifts. Inspired by these insights, we propose WristWorld, the first 4D world model that generates wrist-view videos solely from anchor views. WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and incorporates our Spatial Projection Consistency (SPC) Loss to estimate geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation, which employs our video generation model to synthesize temporally coherent wrist-view videos from the reconstructed perspective. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation with superior spatial consistency, while also improving VLA performance, raising the average task completion length on Calvin by 3.81% and closing 42.4% of the anchor-wrist view gap.",
    "summary": "",
    "translation": "WristWorld：通过4D世界模型生成腕部视图用于机器人操作",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人操作和计算机视觉领域，涉及4D世界模型和腕部视图生成。这与推荐系统、搜索或广告的核心关注点完全无关，也不涉及任何可能应用于这些领域的LLM技术或Transformer架构进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07310v1": {
    "title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation",
    "url": "https://www.alphaxiv.org/abs/2510.07310v1",
    "arxiv_id": "2510.07310v1",
    "authors": "Siyoon Jin, Seongchan Kim, Dahyun Chung, Jaeho Lee, Hyunwook Choi, Jisu Nam, Jiyoung Kim, Seungryong Kim",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 17:57:38",
    "ori_summary": "Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-object interactions. This raises a key question: How do these models internally represent interactions? To answer this, we curate MATRIX-11K, a video dataset with interaction-aware captions and multi-instance mask tracks. Using this dataset, we conduct a systematic analysis that formalizes two perspectives of video DiTs: semantic grounding, via video-to-text attention, which evaluates whether noun and verb tokens capture instances and their relations; and semantic propagation, via video-to-video attention, which assesses whether instance bindings persist across frames. We find both effects concentrate in a small subset of interaction-dominant layers. Motivated by this, we introduce MATRIX, a simple and effective regularization that aligns attention in specific layers of video DiTs with multi-instance mask tracks from the MATRIX-11K dataset, enhancing both grounding and propagation. We further propose InterGenEval, an evaluation protocol for interaction-aware video generation. In experiments, MATRIX improves both interaction fidelity and semantic alignment while reducing drift and hallucination. Extensive ablations validate our design choices. Codes and weights will be released.",
    "summary": "",
    "translation": "MATRIX：用于交互感知视频生成的掩码轨迹对齐",
    "relevance_score": 2,
    "reasoning": "该论文专注于视频生成技术，特别是掩码轨迹对齐方法用于交互感知的视频生成。虽然视频生成在某些广告场景中可能有潜在应用，但这属于AIGC和内容生成领域，与我的核心关注点（推荐系统、搜索、广告排名以及相关的LLM/Transformer技术）相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07302v1": {
    "title": "SpecGuard: Spectral Projection-based Advanced Invisible Watermarking",
    "url": "https://www.alphaxiv.org/abs/2510.07302v1",
    "arxiv_id": "2510.07302v1",
    "authors": "Inzamamul Alam, Md Tanvir Islam, Khan Muhammad, Simon S. Woo",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 17:56:21",
    "ori_summary": "Watermarking embeds imperceptible patterns into images for authenticity verification. However, existing methods often lack robustness against various transformations primarily including distortions, image regeneration, and adversarial perturbation, creating real-world challenges. In this work, we introduce SpecGuard, a novel watermarking approach for robust and invisible image watermarking. Unlike prior approaches, we embed the message inside hidden convolution layers by converting from the spatial domain to the frequency domain using spectral projection of a higher frequency band that is decomposed by wavelet projection. Spectral projection employs Fast Fourier Transform approximation to transform spatial data into the frequency domain efficiently. In the encoding phase, a strength factor enhances resilience against diverse attacks, including adversarial, geometric, and regeneration-based distortions, ensuring the preservation of copyrighted information. Meanwhile, the decoder leverages Parseval's theorem to effectively learn and extract the watermark pattern, enabling accurate retrieval under challenging transformations. We evaluate the proposed SpecGuard based on the embedded watermark's invisibility, capacity, and robustness. Comprehensive experiments demonstrate the proposed SpecGuard outperforms the state-of-the-art models. To ensure reproducibility, the full code is released on \\href{https://github.com/inzamamulDU/SpecGuard_ICCV_2025}{\\textcolor{blue}{\\textbf{GitHub}}}.",
    "summary": "",
    "translation": "SpecGuard：基于频谱投影的先进隐形水印技术",
    "relevance_score": 1,
    "reasoning": "该论文涉及数字水印技术，属于信息安全领域，与我的核心关注点（推荐系统、搜索、广告、LLM技术及其应用）完全无关。水印技术主要用于内容保护和版权管理，不涉及任何推荐、搜索、广告或LLM相关的技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07277v1": {
    "title": "Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema Detection",
    "url": "https://www.alphaxiv.org/abs/2510.07277v1",
    "arxiv_id": "2510.07277v1",
    "authors": "Franco Javier Arellano, José Ignacio Orlando",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 17:41:02",
    "ori_summary": "Diabetic Macular Edema (DME) is a leading cause of vision loss among patients with Diabetic Retinopathy (DR). While deep learning has shown promising results for automatically detecting this condition from fundus images, its application remains challenging due the limited availability of annotated data. Foundation Models (FM) have emerged as an alternative solution. However, it is unclear if they can cope with DME detection in particular. In this paper, we systematically compare different FM and standard transfer learning approaches for this task. Specifically, we compare the two most popular FM for retinal images--RETFound and FLAIR--and an EfficientNet-B0 backbone, across different training regimes and evaluation settings in IDRiD, MESSIDOR-2 and OCT-and-Eye-Fundus-Images (OEFI). Results show that despite their scale, FM do not consistently outperform fine-tuned CNNs in this task. In particular, an EfficientNet-B0 ranked first or second in terms of area under the ROC and precision/recall curves in most evaluation settings, with RETFound only showing promising results in OEFI. FLAIR, on the other hand, demonstrated competitive zero-shot performance, achieving notable AUC-PR scores when prompted appropriately. These findings reveal that FM might not be a good tool for fine-grained ophthalmic tasks such as DME detection even after fine-tuning, suggesting that lightweight CNNs remain strong baselines in data-scarce environments.",
    "summary": "",
    "translation": "评估用于糖尿病性黄斑水肿检测的眼底特异性基础模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的眼底图像分析和糖尿病性黄斑水肿检测，属于明确的医学应用范畴。这与我的关注领域（推荐系统、搜索、广告及其相关技术）完全无关，且医学应用被明确列为不相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07249v1": {
    "title": "TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation",
    "url": "https://www.alphaxiv.org/abs/2510.07249v1",
    "arxiv_id": "2510.07249v1",
    "authors": "Jiaben Chen, Zixin Wang, Ailing Zeng, Yang Fu, Xueyang Yu, Siyuan Cen, Julian Tanke, Yihang Chen, Koichi Saito, Yuki Mitsufuji, Chuang Gan",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 17:16:09",
    "ori_summary": "In this work, we present TalkCuts, a large-scale dataset designed to facilitate the study of multi-shot human speech video generation. Unlike existing datasets that focus on single-shot, static viewpoints, TalkCuts offers 164k clips totaling over 500 hours of high-quality human speech videos with diverse camera shots, including close-up, half-body, and full-body views. The dataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X motion annotations, covering over 10k identities, enabling multimodal learning and evaluation. As a first attempt to showcase the value of the dataset, we present Orator, an LLM-guided multi-modal generation framework as a simple baseline, where the language model functions as a multi-faceted director, orchestrating detailed specifications for camera transitions, speaker gesticulations, and vocal modulation. This architecture enables the synthesis of coherent long-form videos through our integrated multi-modal video generation module. Extensive experiments in both pose-guided and audio-driven settings show that training on TalkCuts significantly enhances the cinematographic coherence and visual appeal of generated multi-shot speech videos. We believe TalkCuts provides a strong foundation for future work in controllable, multi-shot speech video generation and broader multimodal learning.",
    "summary": "",
    "translation": "TalkCuts：用于多镜头人类语音视频生成的大规模数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音视频生成，属于纯粹的视觉和语音生成领域，与推荐系统、搜索或广告的核心技术无关。虽然涉及大规模数据集，但应用场景仅限于视频生成，没有显示出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07217v1": {
    "title": "GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.07217v1",
    "arxiv_id": "2510.07217v1",
    "authors": "Wen Ye, Zhaocheng Liu, Yuwei Gui, Tingyu Yuan, Yunyue Su, Bowen Fang, Chaoyang Zhao, Qiang Liu, Liang Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 16:51:52",
    "ori_summary": "Text-to-image synthesis has made remarkable progress, yet accurately interpreting complex and lengthy prompts remains challenging, often resulting in semantic inconsistencies and missing details. Existing solutions, such as fine-tuning, are model-specific and require training, while prior automatic prompt optimization (APO) approaches typically lack systematic error analysis and refinement strategies, resulting in limited reliability and effectiveness. Meanwhile, test-time scaling methods operate on fixed prompts and on noise or sample numbers, limiting their interpretability and adaptability. To solve these, we introduce a flexible and efficient test-time prompt optimization strategy that operates directly on the input text. We propose a plug-and-play multi-agent system called GenPilot, integrating error analysis, clustering-based adaptive exploration, fine-grained verification, and a memory module for iterative optimization. Our approach is model-agnostic, interpretable, and well-suited for handling long and complex prompts. Simultaneously, we summarize the common patterns of errors and the refinement strategy, offering more experience and encouraging further exploration. Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7% demonstrate the strong capability of our methods in enhancing the text and image consistency and structural coherence of generated images, revealing the effectiveness of our test-time prompt optimization strategy. The code is available at https://github.com/27yw/GenPilot.",
    "summary": "",
    "translation": "GenPilot：一种用于图像生成中测试时提示优化的多智能体系统",
    "relevance_score": 1,
    "reasoning": "该论文专注于图像生成的提示优化，属于纯粹的AIGC和内容生成领域。虽然涉及多智能体系统，但其核心应用是图像生成而非推荐系统、搜索或广告中的排名任务，与当前关注的领域没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07206v1": {
    "title": "EigenScore: OOD Detection using Covariance in Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2510.07206v1",
    "arxiv_id": "2510.07206v1",
    "authors": "Shirin Shoushtari, Yi Wang, Xiao Shi, M. Salman Asif, Ulugbek S. Kamilov",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 16:42:20",
    "ori_summary": "Out-of-distribution (OOD) detection is critical for the safe deployment of machine learning systems in safety-sensitive domains. Diffusion models have recently emerged as powerful generative models, capable of capturing complex data distributions through iterative denoising. Building on this progress, recent work has explored their potential for OOD detection. We propose EigenScore, a new OOD detection method that leverages the eigenvalue spectrum of the posterior covariance induced by a diffusion model. We argue that posterior covariance provides a consistent signal of distribution shift, leading to larger trace and leading eigenvalues on OOD inputs, yielding a clear spectral signature. We further provide analysis explicitly linking posterior covariance to distribution mismatch, establishing it as a reliable signal for OOD detection. To ensure tractability, we adopt a Jacobian-free subspace iteration method to estimate the leading eigenvalues using only forward evaluations of the denoiser. Empirically, EigenScore achieves SOTA performance, with up to 5% AUROC improvement over the best baseline. Notably, it remains robust in near-OOD settings such as CIFAR-10 vs CIFAR-100, where existing diffusion-based methods often fail.",
    "summary": "",
    "translation": "EigenScore：基于扩散模型中协方差的分布外检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于扩散模型中的分布外检测方法，属于计算机视觉领域的特定技术。虽然扩散模型是生成模型的一种，但该工作主要解决视觉数据的异常检测问题，与推荐系统、搜索或广告的核心技术栈没有直接关联。论文没有展示在异构数据处理、序列建模或Transformer架构方面的创新，无法看出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07191v1": {
    "title": "Resolution scaling governs DINOv3 transfer performance in chest radiograph classification",
    "url": "https://www.alphaxiv.org/abs/2510.07191v1",
    "arxiv_id": "2510.07191v1",
    "authors": "Soroosh Tayebi Arasteh, Mina Shaigan, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-08 16:25:04",
    "ori_summary": "Self-supervised learning (SSL) has advanced visual representation learning, but its value in chest radiography, a high-volume imaging modality with fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL models through Gram-anchored self-distillation. Whether these design choices improve transfer learning for chest radiography has not been systematically tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across seven datasets (n>814,000). Two representative backbones were evaluated: ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and 1024x1024 pixels. We additionally assessed frozen features from a 7B model. The primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2 achieved comparable performance on adult datasets. Increasing resolution to 512x512 yielded consistent improvements for DINOv3 over both DINOv2 and ImageNet. In contrast, results in pediatric cohort showed no differences across initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models using frozen DINOv3-7B features underperformed relative to fully finetuned 86-89M-parameter backbones, highlighting the importance of domain adaptation. Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains were most evident for boundary-dependent and small focal abnormalities. In chest radiography, higher input resolution is critical for leveraging the benefits of modern self-supervised models. 512x512 pixels represent a practical upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest performance, while larger inputs offer minimal return on cost. Clinically, these findings support use of finetuned, mid-sized backbones at 512x512 for chest radiograph interpretation, with the greatest gains expected in detecting subtle or boundary-centered lesions relevant to emergency and critical care settings.",
    "summary": "",
    "translation": "分辨率缩放决定DINOv3在胸部X光片分类中的迁移性能",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（胸部X光片）分类，这属于明确的无关主题领域。虽然DINOv3是视觉模型，但论文的应用场景纯粹是医学诊断，与推荐系统、搜索或广告没有任何潜在关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07190v1": {
    "title": "MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis",
    "url": "https://www.alphaxiv.org/abs/2510.07190v1",
    "arxiv_id": "2510.07190v1",
    "authors": "Yihao Zhi, Chenghong Li, Hongjie Liao, Xihe Yang, Zhengwentai Sun, Jiahao Chang, Xiaodong Cun, Wensen Feng, Xiaoguang Han",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 16:24:22",
    "ori_summary": "Recent breakthroughs in video generation, powered by large-scale datasets and diffusion techniques, have shown that video diffusion models can function as implicit 4D novel view synthesizers. Nevertheless, current methods primarily concentrate on redirecting camera trajectory within the front view while struggling to generate 360-degree viewpoint changes. In this paper, we focus on human-centric subdomain and present MV-Performer, an innovative framework for creating synchronized novel view videos from monocular full-body captures. To achieve a 360-degree synthesis, we extensively leverage the MVHumanNet dataset and incorporate an informative condition signal. Specifically, we use the camera-dependent normal maps rendered from oriented partial point clouds, which effectively alleviate the ambiguity between seen and unseen observations. To maintain synchronization in the generated videos, we propose a multi-view human-centric video diffusion model that fuses information from the reference video, partial rendering, and different viewpoints. Additionally, we provide a robust inference procedure for in-the-wild video cases, which greatly mitigates the artifacts induced by imperfect monocular depth estimation. Extensive experiments on three datasets demonstrate our MV-Performer's state-of-the-art effectiveness and robustness, setting a strong model for human-centric 4D novel view synthesis.",
    "summary": "",
    "translation": "MV-Performer：驯服视频扩散模型以实现忠实且同步的多视角表演者合成",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频生成和多视角合成技术，属于计算机视觉和内容生成领域。虽然涉及扩散模型，但其应用场景（表演者合成）与推荐系统、搜索或广告的核心技术需求没有直接关联，也不符合VLM类比中处理异构数据的模式。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07181v1": {
    "title": "TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics",
    "url": "https://www.alphaxiv.org/abs/2510.07181v1",
    "arxiv_id": "2510.07181v1",
    "authors": "Yi Han, Cheng Chi, Enshen Zhou, Shanyu Rong, Jingkun An, Pengwei Wang, Zhongyuan Wang, Lu Sheng, Shanghang Zhang",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2025-10-08 16:20:23",
    "ori_summary": "Vision-Language Models (VLMs) have shown remarkable capabilities in spatial reasoning, yet they remain fundamentally limited to qualitative precision and lack the computational precision required for real-world robotics. Current approaches fail to leverage metric cues from depth sensors and camera calibration, instead reducing geometric problems to pattern recognition tasks that cannot deliver the centimeter-level accuracy essential for robotic manipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel framework that transforms VLMs from perceptual estimators to geometric computers by enabling them to generate and execute precise geometric computations through external tools. Rather than attempting to internalize complex geometric operations within neural networks, TIGeR empowers models to recognize geometric reasoning requirements, synthesize appropriate computational code, and invoke specialized libraries for exact calculations. To support this paradigm, we introduce TIGeR-300K, a comprehensive tool-invocation-oriented dataset covering point transformations, pose estimation, trajectory generation, and spatial compatibility verification, complete with tool invocation sequences and intermediate computations. Through a two-stage training pipeline combining supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT) with our proposed hierarchical reward design, TIGeR achieves SOTA performance on geometric reasoning benchmarks while demonstrating centimeter-level precision in real-world robotic manipulation tasks.",
    "summary": "",
    "translation": "TIGeR：机器人视觉语言模型中的工具集成几何推理",
    "relevance_score": 2,
    "reasoning": "该论文专注于机器人领域的视觉语言模型应用，虽然涉及多模态建模，但其核心应用场景（机器人）与搜索、推荐或广告系统没有直接关联。视觉语言模型的异构数据处理思想在理论上与推荐系统中的多模态建模有相似之处，但这种关联过于间接且应用领域完全不同。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07143v1": {
    "title": "Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods",
    "url": "https://www.alphaxiv.org/abs/2510.07143v1",
    "arxiv_id": "2510.07143v1",
    "authors": "Chenfei Liao, Wensong Wang, Zichen Wen, Xu Zheng, Yiyu Wang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Xin Zou, Yuqian Fu, Bin Ren, Linfeng Zhang, Xuming Hu",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 15:44:28",
    "ori_summary": "Recent endeavors to accelerate inference in Multimodal Large Language Models (MLLMs) have primarily focused on visual token compression. The effectiveness of these methods is typically assessed by measuring the accuracy drop on established benchmarks, comparing model performance before and after compression. However, these benchmarks are originally designed to assess the perception and reasoning capabilities of MLLMs, rather than to evaluate compression techniques. As a result, directly applying them to visual token compression introduces a task mismatch. Strikingly, our investigation reveals that simple image downsampling consistently outperforms many advanced compression methods across multiple widely used benchmarks. Through extensive experiments, we make the following observations: (i) Current benchmarks are noisy for the visual token compression task. (ii) Down-sampling is able to serve as a data filter to evaluate the difficulty of samples in the visual token compression task. Motivated by these findings, we introduce VTC-Bench, an evaluation framework that incorporates a data filtering mechanism to denoise existing benchmarks, thereby enabling fairer and more accurate assessment of visual token compression methods. All data and code are available at https://github.com/Chenfei-Liao/VTC-Bench.",
    "summary": "",
    "translation": "我们是否使用了正确的基准：视觉令牌压缩方法的评估框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉令牌压缩方法的评估框架，属于纯粹的视觉领域研究。虽然令牌压缩技术理论上可以应用于多模态推荐系统中的图像处理，但论文标题明确聚焦于视觉令牌和评估基准，缺乏与推荐系统、搜索或广告的直接关联。这种视觉中心的评估研究距离实际应用场景较远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07135v1": {
    "title": "Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07135v1",
    "arxiv_id": "2510.07135v1",
    "authors": "Karim El Khoury, Maxime Zanella, Christophe De Vleeschouwer, Benoit Macq",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 15:29:48",
    "ori_summary": "Remote Sensing Vision-Language Models (RSVLMs) have shown remarkable potential thanks to large-scale pretraining, achieving strong zero-shot performance on various tasks. However, their ability to generalize in low-data regimes, such as few-shot learning, remains insufficiently explored. In this work, we present the first structured benchmark for evaluating few-shot adaptation methods on RSVLMs. We conduct comprehensive experiments across ten remote sensing scene classification datasets, applying five widely used few-shot adaptation strategies to three state-of-the-art RSVLMs with varying backbones. Our findings reveal that models with similar zero-shot performance can exhibit markedly different behavior under few-shot adaptation, with some RSVLMs being inherently more amenable to such adaptation than others. The variability of performance and the absence of a clear winner among existing methods highlight the need for the development of more robust methods for few-shot adaptation tailored to RS. To facilitate future research, we provide a reproducible benchmarking framework and open-source code to systematically evaluate RSVLMs under few-shot conditions. The source code is publicly available on Github: https://github.com/elkhouryk/fewshot_RSVLMs",
    "summary": "",
    "translation": "遥感视觉语言模型的少样本适应基准",
    "relevance_score": 2,
    "reasoning": "该论文虽然涉及视觉语言模型（VLM），但专注于遥感这一特定领域应用，与推荐系统、搜索或广告没有直接关联。遥感数据与商业推荐/搜索场景中的异构数据模态存在本质差异，且论文主要关注基准测试而非核心架构创新，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07134v1": {
    "title": "TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking",
    "url": "https://www.alphaxiv.org/abs/2510.07134v1",
    "arxiv_id": "2510.07134v1",
    "authors": "Jiahang Liu, Yunpeng Qi, Jiazhao Zhang, Minghan Li, Shaoan Wang, Kui Wu, Hanjing Ye, Hong Zhang, Zhibo Chen, Fangwei Zhong, Zhizheng Zhang, He Wang",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2025-10-08 15:29:17",
    "ori_summary": "Embodied Visual Tracking (EVT) is a fundamental ability that underpins practical applications, such as companion robots, guidance robots and service assistants, where continuously following moving targets is essential. Recent advances have enabled language-guided tracking in complex and unstructured scenes. However, existing approaches lack explicit spatial reasoning and effective temporal memory, causing failures under severe occlusions or in the presence of similar-looking distractors. To address these challenges, we present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances embodied visual tracking with two key modules, a spatial reasoning mechanism and a Target Identification Memory (TIM). The reasoning module introduces a Chain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative position and encodes it as a compact polar-coordinate token for action prediction. Guided by these spatial priors, the TIM employs a gated update strategy to preserve long-horizon target memory, ensuring spatiotemporal consistency and mitigating target loss during extended occlusions. Extensive experiments show that TrackVLA++ achieves state-of-the-art performance on public benchmarks across both egocentric and multi-camera settings. On the challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong zero-shot generalization, enabling robust real-world tracking in dynamic and occluded scenarios.",
    "summary": "",
    "translation": "TrackVLA++：在具身视觉跟踪中释放视觉语言模型的推理与记忆能力",
    "relevance_score": 2,
    "reasoning": "该论文主要关注具身视觉跟踪中的视觉语言模型能力提升，属于机器人学和具身AI领域。虽然涉及视觉语言模型技术，但其应用场景（具身视觉跟踪）与推荐系统、搜索或广告领域没有直接关联，且论文焦点是机器人环境中的视觉跟踪任务，而非文本或序列数据的异质模态建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07129v1": {
    "title": "Graph Conditioned Diffusion for Controllable Histopathology Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.07129v1",
    "arxiv_id": "2510.07129v1",
    "authors": "Sarah Cechnicka, Matthew Baugh, Weitong Zhang, Mischa Dombrowski, Zhe Li, Johannes C. Paetzold, Candice Roufosse, Bernhard Kainz",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 15:26:08",
    "ori_summary": "Recent advances in Diffusion Probabilistic Models (DPMs) have set new standards in high-quality image synthesis. Yet, controlled generation remains challenging, particularly in sensitive areas such as medical imaging. Medical images feature inherent structure such as consistent spatial arrangement, shape or texture, all of which are critical for diagnosis. However, existing DPMs operate in noisy latent spaces that lack semantic structure and strong priors, making it difficult to ensure meaningful control over generated content. To address this, we propose graph-based object-level representations for Graph-Conditioned-Diffusion. Our approach generates graph nodes corresponding to each major structure in the image, encapsulating their individual features and relationships. These graph representations are processed by a transformer module and integrated into a diffusion model via the text-conditioning mechanism, enabling fine-grained control over generation. We evaluate this approach using a real-world histopathology use case, demonstrating that our generated data can reliably substitute for annotated patient data in downstream segmentation tasks. The code is available here.",
    "summary": "",
    "translation": "基于图条件扩散的可控病理图像生成",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于病理图像生成，属于医学影像领域的特定应用，与推荐系统、搜索或广告的核心技术无关。虽然提到了扩散模型和条件生成技术，但这些技术在该论文中的应用仅限于医疗领域，没有明确的潜力应用于推荐系统、搜索或广告场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07126v1": {
    "title": "Validation of Various Normalization Methods for Brain Tumor Segmentation: Can Federated Learning Overcome This Heterogeneity?",
    "url": "https://www.alphaxiv.org/abs/2510.07126v1",
    "arxiv_id": "2510.07126v1",
    "authors": "Jan Fiszer, Dominika Ciupek, Maciej Malawski",
    "categories": "cs.CV, cs.DC",
    "pub_date": "2025-10-08 15:21:53",
    "ori_summary": "Deep learning (DL) has been increasingly applied in medical imaging, however, it requires large amounts of data, which raises many challenges related to data privacy, storage, and transfer. Federated learning (FL) is a training paradigm that overcomes these issues, though its effectiveness may be reduced when dealing with non-independent and identically distributed (non-IID) data. This study simulates non-IID conditions by applying different MRI intensity normalization techniques to separate data subsets, reflecting a common cause of heterogeneity. These subsets are then used for training and testing models for brain tumor segmentation. The findings provide insights into the influence of the MRI intensity normalization methods on segmentation models, both training and inference. Notably, the FL methods demonstrated resilience to inconsistently normalized data across clients, achieving the 3D Dice score of 92%, which is comparable to a centralized model (trained using all data). These results indicate that FL is a solution to effectively train high-performing models without violating data privacy, a crucial concern in medical applications. The code is available at: https://github.com/SanoScience/fl-varying-normalization.",
    "summary": "",
    "translation": "脑肿瘤分割中多种归一化方法的验证：联邦学习能否克服这种异质性？",
    "relevance_score": 1,
    "reasoning": "该论文主要关注医学领域的脑肿瘤分割和联邦学习技术，属于明确的医学应用范畴。虽然提到了联邦学习，但这属于隐私保护技术，属于指定的不相关主题，与推荐系统、搜索或广告领域的核心进展没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07119v1": {
    "title": "MoRe: Monocular Geometry Refinement via Graph Optimization for Cross-View Consistency",
    "url": "https://www.alphaxiv.org/abs/2510.07119v1",
    "arxiv_id": "2510.07119v1",
    "authors": "Dongki Jung, Jaehoon Choi, Yonghan Lee, Sungmin Eum, Heesung Kwon, Dinesh Manocha",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 15:11:32",
    "ori_summary": "Monocular 3D foundation models offer an extensible solution for perception tasks, making them attractive for broader 3D vision applications. In this paper, we propose MoRe, a training-free Monocular Geometry Refinement method designed to improve cross-view consistency and achieve scale alignment. To induce inter-frame relationships, our method employs feature matching between frames to establish correspondences. Rather than applying simple least squares optimization on these matched points, we formulate a graph-based optimization framework that performs local planar approximation using the estimated 3D points and surface normals estimated by monocular foundation models. This formulation addresses the scale ambiguity inherent in monocular geometric priors while preserving the underlying 3D structure. We further demonstrate that MoRe not only enhances 3D reconstruction but also improves novel view synthesis, particularly in sparse view rendering scenarios.",
    "summary": "",
    "translation": "MoRe：通过图优化实现跨视图一致性的单目几何细化",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的单目几何优化和跨视图一致性，属于纯粹的视觉几何处理领域。虽然提到了图优化技术，但该技术在此处仅用于视觉几何对齐，与推荐系统、搜索或广告中的用户行为建模、序列处理或异构数据融合没有明显关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07115v1": {
    "title": "Enhancing Concept Localization in CLIP-based Concept Bottleneck Models",
    "url": "https://www.alphaxiv.org/abs/2510.07115v1",
    "arxiv_id": "2510.07115v1",
    "authors": "Rémi Kazmierczak, Steve Azzolin, Eloïse Berthier, Goran Frehse, Gianni Franchi",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 15:07:16",
    "ori_summary": "This paper addresses explainable AI (XAI) through the lens of Concept Bottleneck Models (CBMs) that do not require explicit concept annotations, relying instead on concepts extracted using CLIP in a zero-shot manner. We show that CLIP, which is central in these techniques, is prone to concept hallucination, incorrectly predicting the presence or absence of concepts within an image in scenarios used in numerous CBMs, hence undermining the faithfulness of explanations. To mitigate this issue, we introduce Concept Hallucination Inhibition via Localized Interpretability (CHILI), a technique that disentangles image embeddings and localizes pixels corresponding to target concepts. Furthermore, our approach supports the generation of saliency-based explanations that are more interpretable.",
    "summary": "",
    "translation": "增强基于CLIP的概念瓶颈模型中的概念定位能力",
    "relevance_score": 3,
    "reasoning": "该论文主要关注CLIP模型中的概念定位改进，属于计算机视觉领域的技术优化。虽然CLIP本身是多模态模型，但论文焦点是概念瓶颈模型的视觉概念定位，与推荐系统、搜索或广告的核心技术关联较弱。其潜在应用可能仅限于需要视觉概念理解的特定场景，对主流RecSys/Search/Ads的直接价值有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07089v1": {
    "title": "DADO: A Depth-Attention framework for Object Discovery",
    "url": "https://www.alphaxiv.org/abs/2510.07089v1",
    "arxiv_id": "2510.07089v1",
    "authors": "Federico Gonzalez, Estefania Talavera, Petia Radeva",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 14:46:34",
    "ori_summary": "Unsupervised object discovery, the task of identifying and localizing objects in images without human-annotated labels, remains a significant challenge and a growing focus in computer vision. In this work, we introduce a novel model, DADO (Depth-Attention self-supervised technique for Discovering unseen Objects), which combines an attention mechanism and a depth model to identify potential objects in images. To address challenges such as noisy attention maps or complex scenes with varying depth planes, DADO employs dynamic weighting to adaptively emphasize attention or depth features based on the global characteristics of each image. We evaluated DADO on standard benchmarks, where it outperforms state-of-the-art methods in object discovery accuracy and robustness without the need for fine-tuning.",
    "summary": "",
    "translation": "DADO：一种用于对象发现的深度注意力框架",
    "relevance_score": 2,
    "reasoning": "该论文提出了一种结合深度信息的注意力框架用于对象发现，属于计算机视觉领域。虽然注意力机制是Transformer的核心组件，但该工作专注于纯粹的视觉对象检测任务，没有明确展示在推荐系统、搜索或广告中的潜在应用。其技术路径更偏向基础视觉研究，与当前关注的LLM技术、推荐系统核心进展或异构数据统一建模缺乏直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07077v1": {
    "title": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
    "url": "https://www.alphaxiv.org/abs/2510.07077v1",
    "arxiv_id": "2510.07077v1",
    "authors": "Kento Kawaharazuka, Jihoon Oh, Jun Yamada, Ingmar Posner, Yuke Zhu",
    "categories": "cs.RO, cs.AI, cs.CV, cs.LG",
    "pub_date": "2025-10-08 14:38:25",
    "ori_summary": "Amid growing efforts to leverage advances in large language models (LLMs) and vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models have recently gained significant attention. By unifying vision, language, and action data at scale, which have traditionally been studied separately, VLA models aim to learn policies that generalise across diverse tasks, objects, embodiments, and environments. This generalisation capability is expected to enable robots to solve novel downstream tasks with minimal or no additional task-specific data, facilitating more flexible and scalable real-world deployment. Unlike previous surveys that focus narrowly on action representations or high-level model architectures, this work offers a comprehensive, full-stack review, integrating both software and hardware components of VLA systems. In particular, this paper provides a systematic review of VLAs, covering their strategy and architectural transition, architectures and building blocks, modality-specific processing techniques, and learning paradigms. In addition, to support the deployment of VLAs in real-world robotic applications, we also review commonly used robot platforms, data collection strategies, publicly available datasets, data augmentation methods, and evaluation benchmarks. Throughout this comprehensive survey, this paper aims to offer practical guidance for the robotics community in applying VLAs to real-world robotic systems. All references categorized by training approach, evaluation method, modality, and dataset are available in the table on our project website: https://vla-survey.github.io .",
    "summary": "",
    "translation": "面向机器人技术的视觉-语言-动作模型：迈向实际应用的研究综述",
    "relevance_score": 3,
    "reasoning": "虽然该论文涉及多模态建模（视觉-语言），但其核心应用领域是机器人技术而非推荐系统、搜索或广告。视觉-语言模型的概念可能为异构数据处理提供启发，但缺乏明确的RecSys/Search/Ads应用路径，且机器人动作控制与当前关注领域关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07058v1": {
    "title": "Concept Retrieval -- What and How?",
    "url": "https://www.alphaxiv.org/abs/2510.07058v1",
    "arxiv_id": "2510.07058v1",
    "authors": "Ori nizan, Oren Shrout, Ayellet Tal",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 14:26:18",
    "ori_summary": "A concept may reflect either a concrete or abstract idea. Given an input image, this paper seeks to retrieve other images that share its central concepts, capturing aspects of the underlying narrative. This goes beyond conventional retrieval or clustering methods, which emphasize visual or semantic similarity. We formally define the problem, outline key requirements, and introduce appropriate evaluation metrics. We propose a novel approach grounded in two key observations: (1) While each neighbor in the embedding space typically shares at least one concept with the query, not all neighbors necessarily share the same concept with one another. (2) Modeling this neighborhood with a bimodal Gaussian distribution uncovers meaningful structure that facilitates concept identification. Qualitative, quantitative, and human evaluations confirm the effectiveness of our approach. See the package on PyPI: https://pypi.org/project/coret/",
    "summary": "",
    "translation": "概念检索——是什么以及如何实现？",
    "relevance_score": 8,
    "reasoning": "概念检索直接涉及搜索和推荐系统的核心功能，专注于理解用户查询背后的语义概念而非字面匹配。这种技术可以显著提升搜索相关性，并为推荐系统提供更精准的用户意图理解，是RecSys和Search领域的重要研究方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07053v1": {
    "title": "Introspection in Learned Semantic Scene Graph Localisation",
    "url": "https://www.alphaxiv.org/abs/2510.07053v1",
    "arxiv_id": "2510.07053v1",
    "authors": "Manshika Charvi Bissessur, Efimia Panagiotaki, Daniele De Martini",
    "categories": "cs.LG, cs.AI, cs.CV, cs.RO, I.2.10; I.2.9; I.4.8; I.5.2; I.5.1",
    "pub_date": "2025-10-08 14:21:45",
    "ori_summary": "This work investigates how semantics influence localisation performance and robustness in a learned self-supervised, contrastive semantic localisation framework. After training a localisation network on both original and perturbed maps, we conduct a thorough post-hoc introspection analysis to probe whether the model filters environmental noise and prioritises distinctive landmarks over routine clutter. We validate various interpretability methods and present a comparative reliability analysis. Integrated gradients and Attention Weights consistently emerge as the most reliable probes of learned behaviour. A semantic class ablation further reveals an implicit weighting in which frequent objects are often down-weighted. Overall, the results indicate that the model learns noise-robust, semantically salient relations about place definition, thereby enabling explainable registration under challenging visual and structural variations.",
    "summary": "",
    "translation": "学习型语义场景图定位中的自省机制",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的场景图定位和自省机制，属于纯粹的视觉技术研究。虽然场景图理解在广义上可能与搜索中的图像检索相关，但论文标题明确聚焦于视觉场景分析，没有显示出与推荐系统、搜索排序或广告的直接关联，也不涉及LLM技术或Transformer架构的进步。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07041v1": {
    "title": "U-Bench: A Comprehensive Understanding of U-Net through 100-Variant Benchmarking",
    "url": "https://www.alphaxiv.org/abs/2510.07041v1",
    "arxiv_id": "2510.07041v1",
    "authors": "Fenghe Tang, Chengqi Dong, Wenxin Ma, Zikang Xu, Heqin Zhu, Zihang Jiang, Rongsheng Wang, Yuhao Wang, Chenxu Wu, Shaohua Kevin Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 14:06:17",
    "ori_summary": "Over the past decade, U-Net has been the dominant architecture in medical image segmentation, leading to the development of thousands of U-shaped variants. Despite its widespread adoption, there is still no comprehensive benchmark to systematically evaluate their performance and utility, largely because of insufficient statistical validation and limited consideration of efficiency and generalization across diverse datasets. To bridge this gap, we present U-Bench, the first large-scale, statistically rigorous benchmark that evaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our contributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates models along three key dimensions: statistical robustness, zero-shot generalization, and computational efficiency. We introduce a novel metric, U-Score, which jointly captures the performance-efficiency trade-off, offering a deployment-oriented perspective on model progress. (2) Systematic Analysis and Model Selection Guidance: We summarize key findings from the large-scale evaluation and systematically analyze the impact of dataset characteristics and architectural paradigms on model performance. Based on these insights, we propose a model advisor agent to guide researchers in selecting the most suitable models for specific datasets and tasks. (3) Public Availability: We provide all code, models, protocols, and weights, enabling the community to reproduce our results and extend the benchmark with future methods. In summary, U-Bench not only exposes gaps in previous evaluations but also establishes a foundation for fair, reproducible, and practically relevant benchmarking in the next decade of U-Net-based segmentation models. The project can be accessed at: https://fenghetan9.github.io/ubench. Code is available at: https://github.com/FengheTan9/U-Bench.",
    "summary": "",
    "translation": "U-Bench：通过100种变体基准测试全面理解U-Net",
    "relevance_score": 1,
    "reasoning": "该论文专注于U-Net架构的基准测试，U-Net主要应用于计算机视觉领域（如图像分割），与推荐系统、搜索或广告的核心技术无直接关联。论文内容属于纯粹的计算机视觉架构分析，没有展示在异构数据处理、Transformer改进或LLM应用方面的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07018v1": {
    "title": "Sharpness-Aware Data Generation for Zero-shot Quantization",
    "url": "https://www.alphaxiv.org/abs/2510.07018v1",
    "arxiv_id": "2510.07018v1",
    "authors": "Dung Hoang-Anh, Cuong Pham Trung Le, Jianfei Cai, Thanh-Toan Do",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-08 13:43:39",
    "ori_summary": "Zero-shot quantization aims to learn a quantized model from a pre-trained full-precision model with no access to original real training data. The common idea in zero-shot quantization approaches is to generate synthetic data for quantizing the full-precision model. While it is well-known that deep neural networks with low sharpness have better generalization ability, none of the previous zero-shot quantization works considers the sharpness of the quantized model as a criterion for generating training data. This paper introduces a novel methodology that takes into account quantized model sharpness in synthetic data generation to enhance generalization. Specifically, we first demonstrate that sharpness minimization can be attained by maximizing gradient matching between the reconstruction loss gradients computed on synthetic and real validation data, under certain assumptions. We then circumvent the problem of the gradient matching without real validation set by approximating it with the gradient matching between each generated sample and its neighbors. Experimental evaluations on CIFAR-100 and ImageNet datasets demonstrate the superiority of the proposed method over the state-of-the-art techniques in low-bit quantization settings.",
    "summary": "",
    "translation": "面向零样本量化的锐度感知数据生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注模型量化中的数据生成技术，属于模型压缩和效率优化领域。虽然量化技术可以应用于推荐或搜索系统中的模型部署效率提升，但论文标题明确聚焦于零样本量化场景，缺乏与推荐系统、搜索或广告领域的直接关联，且未涉及LLM或Transformer架构的核心进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07008v1": {
    "title": "Bayesian Modelling of Multi-Year Crop Type Classification Using Deep Neural Networks and Hidden Markov Models",
    "url": "https://www.alphaxiv.org/abs/2510.07008v1",
    "arxiv_id": "2510.07008v1",
    "authors": "Gianmarco Perantoni, Giulio Weikmann, Lorenzo Bruzzone",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 13:33:32",
    "ori_summary": "The temporal consistency of yearly land-cover maps is of great importance to model the evolution and change of the land cover over the years. In this paper, we focus the attention on a novel approach to classification of yearly satellite image time series (SITS) that combines deep learning with Bayesian modelling, using Hidden Markov Models (HMMs) integrated with Transformer Encoder (TE) based DNNs. The proposed approach aims to capture both i) intricate temporal correlations in yearly SITS and ii) specific patterns in multiyear crop type sequences. It leverages the cascade classification of an HMM layer built on top of the TE, discerning consistent yearly crop-type sequences. Validation on a multiyear crop type classification dataset spanning 47 crop types and six years of Sentinel-2 acquisitions demonstrates the importance of modelling temporal consistency in the predicted labels. HMMs enhance the overall performance and F1 scores, emphasising the effectiveness of the proposed approach.",
    "summary": "",
    "translation": "基于深度神经网络和隐马尔可夫模型的多年作物类型分类贝叶斯建模",
    "relevance_score": 1,
    "reasoning": "该论文专注于农业领域的作物分类问题，使用计算机视觉和时序模型进行多年作物类型识别。这与推荐系统、搜索或广告的核心技术领域完全无关，也不涉及LLM、Transformer架构或异构数据统一建模等关键技术。论文的应用场景和核心技术都超出了指定关注范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06988v1": {
    "title": "No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts",
    "url": "https://www.alphaxiv.org/abs/2510.06988v1",
    "arxiv_id": "2510.06988v1",
    "authors": "Girolamo Macaluso, Lorenzo Mandelli, Mirko Bicchierai, Stefano Berretti, Andrew D. Bagdanov",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 13:12:10",
    "ori_summary": "Diffusion models have recently advanced human motion generation, producing realistic and diverse animations from textual prompts. However, adapting these models to unseen actions or styles typically requires additional motion capture data and full retraining, which is costly and difficult to scale. We propose a post-training framework based on Reinforcement Learning that fine-tunes pretrained motion diffusion models using only textual prompts, without requiring any motion ground truth. Our approach employs a pretrained text-motion retrieval network as a reward signal and optimizes the diffusion policy with Denoising Diffusion Policy Optimization, effectively shifting the model's generative distribution toward the target domain without relying on paired motion data. We evaluate our method on cross-dataset adaptation and leave-one-out motion experiments using the HumanML3D and KIT-ML datasets across both latent- and joint-space diffusion architectures. Results from quantitative metrics and user studies show that our approach consistently improves the quality and diversity of generated motions, while preserving performance on the original distribution. Our approach is a flexible, data-efficient, and privacy-preserving solution for motion adaptation.",
    "summary": "",
    "translation": "无需动作捕捉：仅使用文本提示通过强化学习对后训练运动扩散模型进行优化",
    "relevance_score": 2,
    "reasoning": "该论文主要关注运动生成和扩散模型，属于计算机视觉和图形学领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然提到了强化学习，但应用场景是运动生成而非推荐/搜索/广告的排序或建模问题，因此相关性很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06982v1": {
    "title": "Revisiting Mixout: An Overlooked Path to Robust Finetuning",
    "url": "https://www.alphaxiv.org/abs/2510.06982v1",
    "arxiv_id": "2510.06982v1",
    "authors": "Masih Aminbeidokhti, Heitor Rapela Medeiros, Eric Granger, Marco Pedersoli",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-08 13:07:50",
    "ori_summary": "Finetuning vision foundation models often improves in-domain accuracy but comes at the cost of robustness under distribution shift. We revisit Mixout, a stochastic regularizer that intermittently replaces finetuned weights with their pretrained reference, through the lens of a single-run, weight-sharing implicit ensemble. This perspective reveals three key levers that govern robustness: the \\emph{masking anchor}, \\emph{resampling frequency}, and \\emph{mask sparsity}. Guided by this analysis, we introduce GMixout, which (i) replaces the fixed anchor with an exponential moving-average snapshot that adapts during training, and (ii) regulates masking period via an explicit resampling-frequency hyperparameter. Our sparse-kernel implementation updates only a small fraction of parameters with no inference-time overhead, enabling training on consumer-grade GPUs. Experiments on benchmarks covering covariate shift, corruption, and class imbalance, ImageNet / ImageNet-LT, DomainNet, iWildCam, and CIFAR100-C, GMixout consistently improves in-domain accuracy beyond zero-shot performance while surpassing both Model Soups and strong parameter-efficient finetuning baselines under distribution shift.",
    "summary": "",
    "translation": "重新审视Mixout：一条被忽视的通往鲁棒微调的路径",
    "relevance_score": 6,
    "reasoning": "Mixout是一种正则化技术，通过随机混合预训练和微调参数来防止过拟合，这属于Enabling LLM Tech范畴。在推荐系统和搜索领域，这种鲁棒微调方法可应用于LLM的领域适配，提高模型在用户行为数据上的泛化能力，减少对稀疏用户交互的过拟合。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06973v1": {
    "title": "Addressing the ID-Matching Challenge in Long Video Captioning",
    "url": "https://www.alphaxiv.org/abs/2510.06973v1",
    "arxiv_id": "2510.06973v1",
    "authors": "Zhantao Yang, Huangji Wang, Ruili Feng, Han Zhang, Yuting Hu, Shangwen Zhu, Junyan Li, Yu Liu, Fan Cheng",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 12:59:21",
    "ori_summary": "Generating captions for long and complex videos is both critical and challenging, with significant implications for the growing fields of text-to-video generation and multi-modal understanding. One key challenge in long video captioning is accurately recognizing the same individuals who appear in different frames, which we refer to as the ID-Matching problem. Few prior works have focused on this important issue. Those that have, usually suffer from limited generalization and depend on point-wise matching, which limits their overall effectiveness. In this paper, unlike previous approaches, we build upon LVLMs to leverage their powerful priors. We aim to unlock the inherent ID-Matching capabilities within LVLMs themselves to enhance the ID-Matching performance of captions. Specifically, we first introduce a new benchmark for assessing the ID-Matching capabilities of video captions. Using this benchmark, we investigate LVLMs containing GPT-4o, revealing key insights that the performance of ID-Matching can be improved through two methods: 1) enhancing the usage of image information and 2) increasing the quantity of information of individual descriptions. Based on these insights, we propose a novel video captioning method called Recognizing Identities for Captioning Effectively (RICE). Extensive experiments including assessments of caption quality and ID-Matching performance, demonstrate the superiority of our approach. Notably, when implemented on GPT-4o, our RICE improves the precision of ID-Matching from 50% to 90% and improves the recall of ID-Matching from 15% to 80% compared to baseline. RICE makes it possible to continuously track different individuals in the captions of long videos.",
    "summary": "",
    "translation": "解决长视频字幕生成中的ID匹配挑战",
    "relevance_score": 2,
    "reasoning": "该论文专注于视频字幕生成中的ID匹配问题，这属于纯粹的视觉-语言多模态任务。虽然标题提到“长视频”可能涉及序列建模，但核心关注点是视频字幕生成而非推荐、搜索或广告系统。该技术缺乏明确的跨模态推荐或搜索应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06969v1": {
    "title": "Learning Global Representation from Queries for Vectorized HD Map Construction",
    "url": "https://www.alphaxiv.org/abs/2510.06969v1",
    "arxiv_id": "2510.06969v1",
    "authors": "Shoumeng Qiu, Xinrun Li, Yang Long, Xiangyang Xue, Varun Ojha, Jian Pu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 12:56:08",
    "ori_summary": "The online construction of vectorized high-definition (HD) maps is a cornerstone of modern autonomous driving systems. State-of-the-art approaches, particularly those based on the DETR framework, formulate this as an instance detection problem. However, their reliance on independent, learnable object queries results in a predominantly local query perspective, neglecting the inherent global representation within HD maps. In this work, we propose \\textbf{MapGR} (\\textbf{G}lobal \\textbf{R}epresentation learning for HD \\textbf{Map} construction), an architecture designed to learn and utilize a global representations from queries. Our method introduces two synergistic modules: a Global Representation Learning (GRL) module, which encourages the distribution of all queries to better align with the global map through a carefully designed holistic segmentation task, and a Global Representation Guidance (GRG) module, which endows each individual query with explicit, global-level contextual information to facilitate its optimization. Evaluations on the nuScenes and Argoverse2 datasets validate the efficacy of our approach, demonstrating substantial improvements in mean Average Precision (mAP) compared to leading baselines.",
    "summary": "",
    "translation": "基于查询学习全局表示用于矢量化高精地图构建",
    "relevance_score": 2,
    "reasoning": "该论文主要关注高精地图构建的计算机视觉任务，虽然涉及查询学习和全局表示学习，但其应用场景（HD地图）与推荐系统、搜索或广告领域没有直接关联。论文中的技术方法可能对多模态学习有一定启发，但缺乏明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06967v1": {
    "title": "Generating Surface for Text-to-3D using 2D Gaussian Splatting",
    "url": "https://www.alphaxiv.org/abs/2510.06967v1",
    "arxiv_id": "2510.06967v1",
    "authors": "Huanning Dong, Fan Li, Ping Kuang, Jianwen Min",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 12:54:57",
    "ori_summary": "Recent advancements in Text-to-3D modeling have shown significant potential for the creation of 3D content. However, due to the complex geometric shapes of objects in the natural world, generating 3D content remains a challenging task. Current methods either leverage 2D diffusion priors to recover 3D geometry, or train the model directly based on specific 3D representations. In this paper, we propose a novel method named DirectGaussian, which focuses on generating the surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize conditional text generation models and the surface of a 3D object is rendered by 2D Gaussian splatting with multi-view normal and texture priors. For multi-view geometric consistency problems, DirectGaussian incorporates curvature constraints on the generated surface during optimization process. Through extensive experiments, we demonstrate that our framework is capable of achieving diverse and high-fidelity 3D content creation.",
    "summary": "",
    "translation": "使用2D高斯泼溅生成文本到3D的表面",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于3D内容生成和计算机图形学，与推荐系统、搜索或广告的核心技术没有直接关联。虽然文本到3D生成在概念上涉及多模态处理，但该技术主要面向3D视觉和图形应用，没有明确的推荐、搜索或广告应用前景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06955v1": {
    "title": "High-Rate Mixout: Revisiting Mixout for Robust Domain Generalization",
    "url": "https://www.alphaxiv.org/abs/2510.06955v1",
    "arxiv_id": "2510.06955v1",
    "authors": "Masih Aminbeidokhti, Heitor Rapela Medeiros, Eric Granger, Marco Pedersoli",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-08 12:37:56",
    "ori_summary": "Ensembling fine-tuned models initialized from powerful pre-trained weights is a common strategy to improve robustness under distribution shifts, but it comes with substantial computational costs due to the need to train and store multiple models. Dropout offers a lightweight alternative by simulating ensembles through random neuron deactivation; however, when applied to pre-trained models, it tends to over-regularize and disrupt critical representations necessary for generalization. In this work, we investigate Mixout, a stochastic regularization technique that provides an alternative to Dropout for domain generalization. Rather than deactivating neurons, Mixout mitigates overfitting by probabilistically swapping a subset of fine-tuned weights with their pre-trained counterparts during training, thereby maintaining a balance between adaptation and retention of prior knowledge. Our study reveals that achieving strong performance with Mixout on domain generalization benchmarks requires a notably high masking probability of 0.9 for ViTs and 0.8 for ResNets. While this may seem like a simple adjustment, it yields two key advantages for domain generalization: (1) higher masking rates more strongly penalize deviations from the pre-trained parameters, promoting better generalization to unseen domains; and (2) high-rate masking substantially reduces computational overhead, cutting gradient computation by up to 45% and gradient memory usage by up to 90%. Experiments across five domain generalization benchmarks, PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet, using ResNet and ViT architectures, show that our approach, High-rate Mixout, achieves out-of-domain accuracy comparable to ensemble-based methods while significantly reducing training costs.",
    "summary": "",
    "translation": "高比率混合丢弃：重新审视混合丢弃以实现鲁棒的领域泛化",
    "relevance_score": 2,
    "reasoning": "该论文主要关注领域泛化和模型鲁棒性，属于通用机器学习范畴。虽然Mixout是一种正则化技术，但它与Transformer架构效率、注意力机制或LLM核心进展没有直接关联，在推荐系统、搜索或广告中的潜在应用不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06952v1": {
    "title": "OBJVanish: Physically Realizable Text-to-3D Adv. Generation of LiDAR-Invisible Objects",
    "url": "https://www.alphaxiv.org/abs/2510.06952v1",
    "arxiv_id": "2510.06952v1",
    "authors": "Bing Li, Wuqi Wang, Yanan Zhang, Jingzheng Li, Haigen Min, Wei Feng, Xingyu Zhao, Jie Zhang, Qing Guo",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 12:35:35",
    "ori_summary": "LiDAR-based 3D object detectors are fundamental to autonomous driving, where failing to detect objects poses severe safety risks. Developing effective 3D adversarial attacks is essential for thoroughly testing these detection systems and exposing their vulnerabilities before real-world deployment. However, existing adversarial attacks that add optimized perturbations to 3D points have two critical limitations: they rarely cause complete object disappearance and prove difficult to implement in physical environments. We introduce the text-to-3D adversarial generation method, a novel approach enabling physically realizable attacks that can generate 3D models of objects truly invisible to LiDAR detectors and be easily realized in the real world. Specifically, we present the first empirical study that systematically investigates the factors influencing detection vulnerability by manipulating the topology, connectivity, and intensity of individual pedestrian 3D models and combining pedestrians with multiple objects within the CARLA simulation environment. Building on the insights, we propose the physically-informed text-to-3D adversarial generation (Phy3DAdvGen) that systematically optimizes text prompts by iteratively refining verbs, objects, and poses to produce LiDAR-invisible pedestrians. To ensure physical realizability, we construct a comprehensive object pool containing 13 3D models of real objects and constrain Phy3DAdvGen to generate 3D objects based on combinations of objects in this set. Extensive experiments demonstrate that our approach can generate 3D pedestrians that evade six state-of-the-art (SOTA) LiDAR 3D detectors in both CARLA simulation and physical environments, thereby highlighting vulnerabilities in safety-critical applications.",
    "summary": "",
    "translation": "OBJVanish：物理可实现的文本到3D对抗生成激光雷达不可见物体",
    "relevance_score": 1,
    "reasoning": "该论文涉及文本到3D生成和激光雷达不可见物体的对抗生成，属于计算机视觉和3D生成领域。虽然标题提到文本到3D生成，但其核心关注点是物理世界中的对抗生成和激光雷达技术，与推荐系统、搜索或广告的核心技术栈没有直接关联。该研究主要面向自动驾驶和安全领域，而非推荐、搜索或广告的应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06928v1": {
    "title": "IAR2: Improving Autoregressive Visual Generation with Semantic-Detail Associated Token Prediction",
    "url": "https://www.alphaxiv.org/abs/2510.06928v1",
    "arxiv_id": "2510.06928v1",
    "authors": "Ran Yi, Teng Hu, Zihan Su, Lizhuang Ma",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 12:08:21",
    "ori_summary": "Autoregressive models have emerged as a powerful paradigm for visual content creation, but often overlook the intrinsic structural properties of visual data. Our prior work, IAR, initiated a direction to address this by reorganizing the visual codebook based on embedding similarity, thereby improving generation robustness. However, it is constrained by the rigidity of pre-trained codebooks and the inaccuracies of hard, uniform clustering. To overcome these limitations, we propose IAR2, an advanced autoregressive framework that enables a hierarchical semantic-detail synthesis process. At the core of IAR2 is a novel Semantic-Detail Associated Dual Codebook, which decouples image representations into a semantic codebook for global semantic information and a detail codebook for fine-grained refinements. It expands the quantization capacity from a linear to a polynomial scale, significantly enhancing expressiveness. To accommodate this dual representation, we propose a Semantic-Detail Autoregressive Prediction scheme coupled with a Local-Context Enhanced Autoregressive Head, which performs hierarchical prediction-first the semantic token, then the detail token-while leveraging a local context window to enhance spatial coherence. Furthermore, for conditional generation, we introduce a Progressive Attention-Guided Adaptive CFG mechanism that dynamically modulates the guidance scale for each token based on its relevance to the condition and its temporal position in the generation sequence, improving conditional alignment without sacrificing realism. Extensive experiments demonstrate that IAR2 sets a new state-of-the-art for autoregressive image generation, achieving a FID of 1.50 on ImageNet. Our model not only surpasses previous methods in performance but also demonstrates superior computational efficiency, highlighting the effectiveness of our structured, coarse-to-fine generation strategy.",
    "summary": "",
    "translation": "IAR2：通过语义-细节关联的令牌预测改进自回归视觉生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于视觉生成领域的自回归模型改进，属于纯粹的视觉生成技术。虽然涉及自回归架构，但主要针对视觉内容生成而非推荐、搜索或广告场景，与当前关注的RecSys、Search、Ads核心领域以及LLM在其中的应用没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06926v1": {
    "title": "Label-frugal satellite image change detection with generative virtual exemplar learning",
    "url": "https://www.alphaxiv.org/abs/2510.06926v1",
    "arxiv_id": "2510.06926v1",
    "authors": "Hichem Sahbi",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 12:07:35",
    "ori_summary": "Change detection is a major task in remote sensing which consists in finding all the occurrences of changes in multi-temporal satellite or aerial images. The success of existing methods, and particularly deep learning ones, is tributary to the availability of hand-labeled training data that capture the acquisition conditions and the subjectivity of the user (oracle). In this paper, we devise a novel change detection algorithm, based on active learning. The main contribution of our work resides in a new model that measures how important is each unlabeled sample, and provides an oracle with only the most critical samples (also referred to as virtual exemplars) for further labeling. These exemplars are generated, using an invertible graph convnet, as the optimum of an adversarial loss that (i) measures representativity, diversity and ambiguity of the data, and thereby (ii) challenges (the most) the current change detection criteria, leading to a better re-estimate of these criteria in the subsequent iterations of active learning. Extensive experiments show the positive impact of our label-efficient learning model against comparative methods.",
    "summary": "",
    "translation": "基于生成式虚拟范例学习的标签节俭卫星图像变化检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注卫星图像变化检测，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然生成式学习方法在技术上可能有一定先进性，但缺乏明确的在RecSys/Search/Ads领域的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06907v1": {
    "title": "Angular Constraint Embedding via SpherePair Loss for Constrained Clustering",
    "url": "https://www.alphaxiv.org/abs/2510.06907v1",
    "arxiv_id": "2510.06907v1",
    "authors": "Shaojie Zhang, Ke Chen",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-08 11:43:20",
    "ori_summary": "Constrained clustering integrates domain knowledge through pairwise constraints. However, existing deep constrained clustering (DCC) methods are either limited by anchors inherent in end-to-end modeling or struggle with learning discriminative Euclidean embedding, restricting their scalability and real-world applicability. To avoid their respective pitfalls, we propose a novel angular constraint embedding approach for DCC, termed SpherePair. Using the SpherePair loss with a geometric formulation, our method faithfully encodes pairwise constraints and leads to embeddings that are clustering-friendly in angular space, effectively separating representation learning from clustering. SpherePair preserves pairwise relations without conflict, removes the need to specify the exact number of clusters, generalizes to unseen data, enables rapid inference of the number of clusters, and is supported by rigorous theoretical guarantees. Comparative evaluations with state-of-the-art DCC methods on diverse benchmarks, along with empirical validation of theoretical insights, confirm its superior performance, scalability, and overall real-world effectiveness. Code is available at \\href{https://github.com/spherepaircc/SpherePairCC/tree/main}{our repository}.",
    "summary": "",
    "translation": "基于SpherePair损失的角约束嵌入用于约束聚类",
    "relevance_score": 2,
    "reasoning": "该论文主要关注约束聚类中的嵌入方法，属于通用的机器学习技术，与推荐系统、搜索或广告的核心领域进展没有直接关联。虽然嵌入技术可能间接应用于用户表示学习，但论文没有明确展示在RecSys/Search/Ads领域的潜在应用价值，且不属于当前关注的LLM技术、Transformer架构或异构数据建模范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06887v1": {
    "title": "Lung Infection Severity Prediction Using Transformers with Conditional TransMix Augmentation and Cross-Attention",
    "url": "https://www.alphaxiv.org/abs/2510.06887v1",
    "arxiv_id": "2510.06887v1",
    "authors": "Bouthaina Slika, Fadi Dornaika, Fares Bougourzi, Karim Hammoudi",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 11:08:34",
    "ori_summary": "Lung infections, particularly pneumonia, pose serious health risks that can escalate rapidly, especially during pandemics. Accurate AI-based severity prediction from medical imaging is essential to support timely clinical decisions and optimize patient outcomes. In this work, we present a novel method applicable to both CT scans and chest X-rays for assessing lung infection severity. Our contributions are twofold: (i) QCross-Att-PVT, a Transformer-based architecture that integrates parallel encoders, a cross-gated attention mechanism, and a feature aggregator to capture rich multi-scale features; and (ii) Conditional Online TransMix, a custom data augmentation strategy designed to address dataset imbalance by generating mixed-label image patches during training. Evaluated on two benchmark datasets, RALO CXR and Per-COVID-19 CT, our method consistently outperforms several state-of-the-art deep learning models. The results emphasize the critical role of data augmentation and gated attention in improving both robustness and predictive accuracy. This approach offers a reliable, adaptable tool to support clinical diagnosis, disease monitoring, and personalized treatment planning. The source code of this work is available at https://github.com/bouthainas/QCross-Att-PVT.",
    "summary": "",
    "translation": "使用具有条件TransMix增强和交叉注意力的Transformer进行肺部感染严重程度预测",
    "relevance_score": 1,
    "reasoning": "该论文属于医学领域的特定应用，专注于肺部感染严重程度预测，这属于明确的无关主题。虽然使用了Transformer架构，但应用场景与推荐系统、搜索或广告完全无关，且没有证据表明其技术具有跨领域应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06876v1": {
    "title": "HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.06876v1",
    "arxiv_id": "2510.06876v1",
    "authors": "Samir Abou Haidar, Alexandre Chariot, Mehdi Darouich, Cyril Joly, Jean-Emmanuel Deschaud",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-08 10:46:07",
    "ori_summary": "LiDAR semantic segmentation is crucial for autonomous vehicles and mobile robots, requiring high accuracy and real-time processing, especially on resource-constrained embedded systems. Previous state-of-the-art methods often face a trade-off between accuracy and speed. Point-based and sparse convolution-based methods are accurate but slow due to the complexity of neighbor searching and 3D convolutions. Projection-based methods are faster but lose critical geometric information during the 2D projection. Additionally, many recent methods rely on test-time augmentation (TTA) to improve performance, which further slows the inference. Moreover, the pre-processing phase across all methods increases execution time and is demanding on embedded platforms. Therefore, we introduce HARP-NeXt, a high-speed and accurate LiDAR semantic segmentation network. We first propose a novel pre-processing methodology that significantly reduces computational overhead. Then, we design the Conv-SE-NeXt feature extraction block to efficiently capture representations without deep layer stacking per network stage. We also employ a multi-scale range-point fusion backbone that leverages information at multiple abstraction levels to preserve essential geometric details, thereby enhancing accuracy. Experiments on the nuScenes and SemanticKITTI benchmarks show that HARP-NeXt achieves a superior speed-accuracy trade-off compared to all state-of-the-art methods, and, without relying on ensemble models or TTA, is comparable to the top-ranked PTv3, while running 24$\\times$ faster. The code is available at https://github.com/SamirAbouHaidar/HARP-NeXt",
    "summary": "",
    "translation": "HARP-NeXt：用于3D LiDAR语义分割的高速精确范围-点融合网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D LiDAR语义分割，属于纯粹的3D视觉领域，与推荐系统、搜索或广告没有直接关联。论文内容涉及传感器数据处理和3D场景理解，没有显示出在异构数据建模、Transformer架构或LLM应用方面的潜在价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06871v1": {
    "title": "SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models",
    "url": "https://www.alphaxiv.org/abs/2510.06871v1",
    "arxiv_id": "2510.06871v1",
    "authors": "Huahui Yi, Kun Wang, Qiankun Li, Miao Yu, Liang Lin, Gongli Xi, Hao Wu, Xuming Hu, Kang Li, Yang Liu",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-08 10:39:12",
    "ori_summary": "Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal reasoning but often amplify safety risks under adversarial or unsafe prompts, a phenomenon we call the \\textit{Reasoning Tax}. Existing defenses mainly act at the output level and do not constrain the reasoning process, leaving models exposed to implicit risks. In this paper, we propose SaFeR-VLM, a safety-aligned reinforcement learning framework that embeds safety directly into multimodal reasoning. The framework integrates four components: (I) QI-Safe-10K, a curated dataset emphasizing safety-critical and reasoning-sensitive cases; (II) safety-aware rollout, where unsafe generations undergo reflection and correction instead of being discarded; (III) structured reward modeling with multi-dimensional weighted criteria and explicit penalties for hallucinations and contradictions; and (IV) GRPO optimization, which reinforces both safe and corrected trajectories. This unified design shifts safety from a passive safeguard to an active driver of reasoning, enabling scalable and generalizable safety-aware reasoning. SaFeR-VLM further demonstrates robustness against both explicit and implicit risks, supporting dynamic and interpretable safety decisions beyond surface-level filtering. SaFeR-VLM-3B achieves average performance $70.13$ and $78.97$ on safety and helpfulness across six benchmarks, surpassing both same-scale and $>10\\times$ larger models such as Skywork-R1V3-38B, Qwen2.5VL-72B, and GLM4.5V-106B. Remarkably, SaFeR-VLM-7B benefits from its increased scale to surpass GPT-5-mini and Gemini-2.5-Flash by \\num{6.47} and \\num{16.76} points respectively on safety metrics, achieving this improvement without any degradation in helpfulness performance. Our codes are available at https://github.com/HarveyYi/SaFeR-VLM.",
    "summary": "",
    "translation": "SaFeR-VLM：面向多模态模型的安全感知细粒度推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态模型的安全性和细粒度推理能力，属于VLM（视觉语言模型）的安全研究领域。虽然涉及多模态模型，但其核心焦点是安全性（Safety-aware），这属于被明确排除的非技术性话题（安全、伦理等），与推荐系统、搜索或广告的核心技术进展没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06858v1": {
    "title": "Explaining raw data complexity to improve satellite onboard processing",
    "url": "https://www.alphaxiv.org/abs/2510.06858v1",
    "arxiv_id": "2510.06858v1",
    "authors": "Adrien Dorise, Marjorie Bellizzi, Adrien Girard, Benjamin Francesconi, Stéphane May",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 10:26:02",
    "ori_summary": "With increasing processing power, deploying AI models for remote sensing directly onboard satellites is becoming feasible. However, new constraints arise, mainly when using raw, unprocessed sensor data instead of preprocessed ground-based products. While current solutions primarily rely on preprocessed sensor images, few approaches directly leverage raw data. This study investigates the effects of utilising raw data on deep learning models for object detection and classification tasks. We introduce a simulation workflow to generate raw-like products from high-resolution L1 imagery, enabling systemic evaluation. Two object detection models (YOLOv11s and YOLOX-S) are trained on both raw and L1 datasets, and their performance is compared using standard detection metrics and explainability tools. Results indicate that while both models perform similarly at low to medium confidence thresholds, the model trained on raw data struggles with object boundary identification at high confidence levels. It suggests that adapting AI architectures with improved contouring methods can enhance object detection on raw images, improving onboard AI for remote sensing.",
    "summary": "",
    "translation": "解释原始数据复杂性以改进卫星星上处理",
    "relevance_score": 1,
    "reasoning": "该论文专注于卫星数据处理和星上处理优化，属于遥感或航天工程领域。这与推荐系统、搜索或广告的核心关注点完全无关，也不涉及LLM技术、Transformer架构或异构数据建模。该主题属于被排除的物理/领域特定应用类别。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06855v1": {
    "title": "Online Generic Event Boundary Detection",
    "url": "https://www.alphaxiv.org/abs/2510.06855v1",
    "arxiv_id": "2510.06855v1",
    "authors": "Hyungrok Jung, Daneul Kim, Seunggyun Lim, Jeany Son, Jonghyun Choi",
    "categories": "cs.CV, eess.IV",
    "pub_date": "2025-10-08 10:23:45",
    "ori_summary": "Generic Event Boundary Detection (GEBD) aims to interpret long-form videos through the lens of human perception. However, current GEBD methods require processing complete video frames to make predictions, unlike humans processing data online and in real-time. To bridge this gap, we introduce a new task, Online Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries of generic events immediately in streaming videos. This task faces unique challenges of identifying subtle, taxonomy-free event changes in real-time, without the access to future frames. To tackle these challenges, we propose a novel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST) which explains how humans segment ongoing activity into events by leveraging the discrepancies between predicted and actual information. Our framework consists of two key components: the Consistent Event Anticipator (CEA), and the Online Boundary Discriminator (OBD). Specifically, the CEA generates a prediction of the future frame reflecting current event dynamics based solely on prior frames. Then, the OBD measures the prediction error and adaptively adjusts the threshold using statistical tests on past errors to capture diverse, subtle event transitions. Experimental results demonstrate that Estimator outperforms all baselines adapted from recent online video understanding models and achieves performance comparable to prior offline-GEBD methods on the Kinetics-GEBD and TAPOS datasets.",
    "summary": "",
    "translation": "在线通用事件边界检测",
    "relevance_score": 2,
    "reasoning": "该论文关注通用事件边界检测，属于计算机视觉中的时序分析任务，与推荐系统、搜索或广告的核心领域进展没有直接关联。虽然事件检测在视频内容理解中有应用，但论文标题未表明其与用户行为序列建模、多模态推荐或搜索排序等具体应用场景的明确联系，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06842v1": {
    "title": "Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph Regularization",
    "url": "https://www.alphaxiv.org/abs/2510.06842v1",
    "arxiv_id": "2510.06842v1",
    "authors": "Kanglei Zhou, Qingyi Pan, Xingxing Zhang, Hubert P. H. Shum, Frederick W. B. Li, Xiaohui Liang, Liyuan Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 10:09:47",
    "ori_summary": "Action Quality Assessment (AQA) quantifies human actions in videos, supporting applications in sports scoring, rehabilitation, and skill evaluation. A major challenge lies in the non-stationary nature of quality distributions in real-world scenarios, which limits the generalization ability of conventional methods. We introduce Continual AQA (CAQA), which equips AQA with Continual Learning (CL) capabilities to handle evolving distributions while mitigating catastrophic forgetting. Although parameter-efficient fine-tuning of pretrained models has shown promise in CL for image classification, we find it insufficient for CAQA. Our empirical and theoretical analyses reveal two insights: (i) Full-Parameter Fine-Tuning (FPFT) is necessary for effective representation learning; yet (ii) uncontrolled FPFT induces overfitting and feature manifold shift, thereby aggravating forgetting. To address this, we propose Adaptive Manifold-Aligned Graph Regularization (MAGR++), which couples backbone fine-tuning that stabilizes shallow layers while adapting deeper ones with a two-step feature rectification pipeline: a manifold projector to translate deviated historical features into the current representation space, and a graph regularizer to align local and global distributions. We construct four CAQA benchmarks from three datasets with tailored evaluation protocols and strong baselines, enabling systematic cross-dataset comparison. Extensive experiments show that MAGR++ achieves state-of-the-art performance, with average correlation gains of 3.6% offline and 12.2% online over the strongest baseline, confirming its robustness and effectiveness. Our code is available at https://github.com/ZhouKanglei/MAGRPP.",
    "summary": "",
    "translation": "基于自适应流形对齐图正则化的持续动作质量评估",
    "relevance_score": 2,
    "reasoning": "该论文主要关注动作质量评估，属于计算机视觉和运动分析领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然图正则化技术在某些推荐系统中有所应用，但论文专注于动作质量评估这一特定任务，缺乏明确的跨领域应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06829v1": {
    "title": "Lattice-allocated Real-time Line Segment Feature Detection and Tracking Using Only an Event-based Camera",
    "url": "https://www.alphaxiv.org/abs/2510.06829v1",
    "arxiv_id": "2510.06829v1",
    "authors": "Mikihiro Ikura, Arren Glover, Masayoshi Mizuno, Chiara Bartolozzi",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 09:52:35",
    "ori_summary": "Line segment extraction is effective for capturing geometric features of human-made environments. Event-based cameras, which asynchronously respond to contrast changes along edges, enable efficient extraction by reducing redundant data. However, recent methods often rely on additional frame cameras or struggle with high event rates. This research addresses real-time line segment detection and tracking using only a modern, high-resolution (i.e., high event rate) event-based camera. Our lattice-allocated pipeline consists of (i) velocity-invariant event representation, (ii) line segment detection based on a fitting score, (iii) and line segment tracking by perturbating endpoints. Evaluation using ad-hoc recorded dataset and public datasets demonstrates real-time performance and higher accuracy compared to state-of-the-art event-only and event-frame hybrid baselines, enabling fully stand-alone event camera operation in real-world settings.",
    "summary": "",
    "translation": "仅使用事件相机的网格分配实时线段特征检测与跟踪",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于事件相机的计算机视觉技术，涉及线段特征检测和跟踪，属于纯粹的视觉处理范畴。该技术没有展示出在推荐系统、搜索或广告领域的潜在应用价值，与当前关注的LLM技术、推荐系统核心进展或Transformer架构改进完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06827v1": {
    "title": "StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance",
    "url": "https://www.alphaxiv.org/abs/2510.06827v1",
    "arxiv_id": "2510.06827v1",
    "authors": "Jaeseok Jeong, Junho Kim, Gayoung Lee, Yunjey Choi, Youngjung Uh",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 09:50:34",
    "ori_summary": "In the domain of text-to-image generation, diffusion models have emerged as powerful tools. Recently, studies on visual prompting, where images are used as prompts, have enabled more precise control over style and content. However, existing methods often suffer from content leakage, where undesired elements of the visual style prompt are transferred along with the intended style. To address this issue, we 1) extend classifier-free guidance (CFG) to utilize swapping self-attention and propose 2) negative visual query guidance (NVQG) to reduce the transfer of unwanted contents. NVQG employs negative score by intentionally simulating content leakage scenarios that swap queries instead of key and values of self-attention layers from visual style prompts. This simple yet effective method significantly reduces content leakage. Furthermore, we provide careful solutions for using a real image as visual style prompts. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, reflecting the style of the references, and ensuring that resulting images match the text prompts. Our code is available \\href{https://github.com/naver-ai/StyleKeeper}{here}.",
    "summary": "",
    "translation": "StyleKeeper：使用负向视觉查询引导防止内容泄露",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉内容保护和防止泄露，这属于安全/隐私领域，与我的核心关注点（推荐系统、搜索、广告中的技术进展）无关。虽然标题提到“视觉查询”，但防止内容泄露的应用场景主要涉及版权保护、数据安全等非技术性话题，而非推荐或搜索系统的改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06820v1": {
    "title": "Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking",
    "url": "https://www.alphaxiv.org/abs/2510.06820v1",
    "arxiv_id": "2510.06820v1",
    "authors": "Mitchell Keren Taraday, Shahaf Wagner, Chaim Baskin",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-08 09:46:09",
    "ori_summary": "Multimodal retrieval still leans on embedding-based models like CLIP for fast vector search over pre-computed image embeddings. Yet, unlike text retrieval, where joint-encoder rerankers are standard, comparable vision--language rerankers are largely absent. We find that seminal joint encoders such as BLIP are severely bottlenecked by an expensive visual feature-extraction stage, preventing practical deployment at scale. Motivated by this bottleneck, we introduce EDJE, an Efficient Discriminative Joint Encoder that precomputes vision tokens offline and compresses them via a lightweight attention-based adapter, so online inference runs only a compact joint encoder over a small set of visual tokens plus the text. EDJE preserves strong retrieval performance while drastically reducing storage and online compute, enabling high-throughput inference. Specifically, EDJE processes 50k image--text pairs/second while requiring 49kB of disk storage per image, matching prior art on Flickr (zero-shot) and COCO (fine-tuned) retrieval. The implementation and checkpoints will be made publicly available shortly.",
    "summary": "",
    "translation": "面向大规模视觉语言重排序的高效判别式联合编码器",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及VLM类比思想，将视觉和语言作为不同模态进行联合建模，这与处理异构数据（如上下文特征和用户序列）的理念高度相关。高效的联合编码器架构可以应用于搜索和推荐系统中的多模态内容重排序，特别是处理商品图像与文本描述、用户行为序列与上下文特征等异构数据场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.06809v1": {
    "title": "VA-Adapter: Adapting Ultrasound Foundation Model to Echocardiography Probe Guidance",
    "url": "https://www.alphaxiv.org/abs/2510.06809v1",
    "arxiv_id": "2510.06809v1",
    "authors": "Teng Wang, Haojun Jiang, Yuxuan Wang, Zhenguo Sun, Shiji Song, Gao Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 09:38:30",
    "ori_summary": "Echocardiography is a critical tool for detecting heart diseases. Recently, ultrasound foundation models have demonstrated remarkable capabilities in cardiac ultrasound image analysis. However, obtaining high-quality ultrasound images is a prerequisite for accurate diagnosis. Due to the exceptionally high operational difficulty of cardiac ultrasound, there is a shortage of highly skilled personnel, which hinders patients from receiving timely examination services. In this paper, we aim to adapt the medical knowledge learned by foundation models from vast datasets to the probe guidance task, which is designed to provide real-time operational recommendations for junior sonographers to acquire high-quality ultrasound images. Moreover, inspired by the practice where experts optimize action decisions based on past explorations, we meticulously design a parameter-efficient Vision-Action Adapter (VA-Adapter) to enable foundation model's image encoder to encode vision-action sequences, thereby enhancing guidance performance. With built-in sequential reasoning capabilities in a compact design, the VA-Adapter enables a pre-trained ultrasound foundation model to learn precise probe adjustment strategies by fine-tuning only a small subset of parameters. Extensive experiments demonstrate that the VA-Adapter can surpass strong probe guidance models. Our code will be released after acceptance.",
    "summary": "",
    "translation": "VA-Adapter：将超声基础模型适配到超声心动图探头引导任务",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学超声成像领域的特定应用，属于医疗影像处理范畴。虽然涉及基础模型适配技术，但其应用场景（超声心动图探头引导）与推荐系统、搜索或广告领域完全无关，属于明确的医疗领域特定应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06802v1": {
    "title": "Capture and Interact: Rapid 3D Object Acquisition and Rendering with Gaussian Splatting in Unity",
    "url": "https://www.alphaxiv.org/abs/2510.06802v1",
    "arxiv_id": "2510.06802v1",
    "authors": "Islomjon Shukhratov, Sergey Gorinsky",
    "categories": "cs.GR, cs.CV",
    "pub_date": "2025-10-08 09:31:29",
    "ori_summary": "Capturing and rendering three-dimensional (3D) objects in real time remain a significant challenge, yet hold substantial potential for applications in augmented reality, digital twin systems, remote collaboration and prototyping. We present an end-to-end pipeline that leverages 3D Gaussian Splatting (3D GS) to enable rapid acquisition and interactive rendering of real-world objects using a mobile device, cloud processing and a local computer. Users scan an object with a smartphone video, upload it for automated 3D reconstruction, and visualize it interactively in Unity at an average of 150 frames per second (fps) on a laptop. The system integrates mobile capture, cloud-based 3D GS and Unity rendering to support real-time telepresence. Our experiments show that the pipeline processes scans in approximately 10 minutes on a graphics processing unit (GPU) achieving real-time rendering on the laptop.",
    "summary": "",
    "translation": "捕获与交互：基于高斯泼溅技术在Unity中实现快速3D物体采集与渲染",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D计算机视觉中的物体采集和渲染技术，使用高斯泼溅方法在Unity引擎中实现。虽然涉及3D数据处理，但缺乏与推荐系统、搜索或广告领域的直接关联，且未展示在异构数据建模或Transformer架构方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06791v1": {
    "title": "Extreme Amodal Face Detection",
    "url": "https://www.alphaxiv.org/abs/2510.06791v1",
    "arxiv_id": "2510.06791v1",
    "authors": "Changlin Song, Yunzhong Hou, Michael Randall Barnes, Rahul Shome, Dylan Campbell",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 09:22:03",
    "ori_summary": "Extreme amodal detection is the task of inferring the 2D location of objects that are not fully visible in the input image but are visible within an expanded field-of-view. This differs from amodal detection, where the object is partially visible within the input image, but is occluded. In this paper, we consider the sub-problem of face detection, since this class provides motivating applications involving safety and privacy, but do not tailor our method specifically to this class. Existing approaches rely on image sequences so that missing detections may be interpolated from surrounding frames or make use of generative models to sample possible completions. In contrast, we consider the single-image task and propose a more efficient, sample-free approach that makes use of the contextual cues from the image to infer the presence of unseen faces. We design a heatmap-based extreme amodal object detector that addresses the problem of efficiently predicting a lot (the out-of-frame region) from a little (the image) with a selective coarse-to-fine decoder. Our method establishes strong results for this new task, even outperforming less efficient generative approaches.",
    "summary": "",
    "translation": "极端非模态人脸检测",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉中的人脸检测任务，特别是处理非模态（amodal）场景，这属于纯粹的视觉领域研究。标题表明该工作与推荐系统、搜索或广告的核心技术没有直接关联，也不涉及LLM技术、Transformer架构进展或异构数据统一建模等当前关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06784v1": {
    "title": "Bionetta: Efficient Client-Side Zero-Knowledge Machine Learning Proving",
    "url": "https://www.alphaxiv.org/abs/2510.06784v1",
    "arxiv_id": "2510.06784v1",
    "authors": "Dmytro Zakharov, Oleksandr Kurbatov, Artem Sdobnov, Lev Soukhanov, Yevhenii Sekhin, Vitalii Volovyk, Mykhailo Velykodnyi, Mark Cherepovskyi, Kyrylo Baibula, Lasha Antadze, Pavlo Kravchenko, Volodymyr Dubinin, Yaroslav Panasenko",
    "categories": "cs.CR, cs.CV",
    "pub_date": "2025-10-08 09:10:32",
    "ori_summary": "In this report, we compare the performance of our UltraGroth-based zero-knowledge machine learning framework Bionetta to other tools of similar purpose such as EZKL, Lagrange's deep-prove, or zkml. The results show a significant boost in the proving time for custom-crafted neural networks: they can be proven even on mobile devices, enabling numerous client-side proving applications. While our scheme increases the cost of one-time preprocessing steps, such as circuit compilation and generating trusted setup, our approach is, to the best of our knowledge, the only one that is deployable on the native EVM smart contracts without overwhelming proof size and verification overheads.",
    "summary": "",
    "translation": "Bionetta：高效的客户端零知识机器学习证明",
    "relevance_score": 1,
    "reasoning": "该论文涉及零知识证明和机器学习，但明确属于被排除的隐私和安全范畴。虽然提到了机器学习，但核心焦点是隐私保护技术，与推荐系统、搜索或广告的核心技术进展无关。该技术没有明显的应用潜力来增强推荐、搜索或广告系统的核心排名或建模能力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06783v1": {
    "title": "TTRV: Test-Time Reinforcement Learning for Vision Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.06783v1",
    "arxiv_id": "2510.06783v1",
    "authors": "Akshit Singh, Shyam Marjit, Wei Lin, Paul Gavrikov, Serena Yeung-Levy, Hilde Kuehne, Rogerio Feris, Sivan Doveh, James Glass, M. Jehanzeb Mirza",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 09:10:31",
    "ori_summary": "Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and dedicated training splits, a setup that contrasts with how humans learn directly from their environment. In this work, we propose TTRV to enhance vision language understanding by adapting the model on the fly at inference time, without the need for any labeled data. Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework by designing rewards based on the frequency of the base model's output, while inferring on each test sample multiple times. Further, we also propose to control the diversity of the model's output by simultaneously rewarding the model for obtaining low entropy of the output empirical distribution. Our approach delivers consistent gains across both object recognition and visual question answering (VQA), with improvements of up to 52.4% and 29.8%, respectively, and average boosts of 24.6% and 10.0% across 16 datasets.Remarkably, on image recognition, TTRV applied to InternVL 8B surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining highly competitive on VQA, demonstrating that test-time reinforcement learning can match or exceed the strongest proprietary models. Finally, we find many interesting properties of test-time RL for VLMs: for example, even in extremely data-constrained scenarios, where adaptation is performed on a single randomly chosen unlabeled test example, TTRV still yields non-trivial improvements of up to 5.5% in recognition tasks.",
    "summary": "",
    "translation": "TTRV：视觉语言模型的测试时强化学习",
    "relevance_score": 2,
    "reasoning": "虽然论文涉及视觉语言模型（VLM）和强化学习，但主要关注测试时优化和纯粹的VLM技术，与推荐系统、搜索或广告的核心领域缺乏直接关联。强化学习部分没有明确展示在推荐/搜索/广告场景中的应用潜力，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06769v1": {
    "title": "A deep multiple instance learning approach based on coarse labels for high-resolution land-cover mapping",
    "url": "https://www.alphaxiv.org/abs/2510.06769v1",
    "arxiv_id": "2510.06769v1",
    "authors": "Gianmarco Perantoni, Lorenzo Bruzzone",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 08:50:39",
    "ori_summary": "The quantity and the quality of the training labels are central problems in high-resolution land-cover mapping with machine-learning-based solutions. In this context, weak labels can be gathered in large quantities by leveraging on existing low-resolution or obsolete products. In this paper, we address the problem of training land-cover classifiers using high-resolution imagery (e.g., Sentinel-2) and weak low-resolution reference data (e.g., MODIS -derived land-cover maps). Inspired by recent works in Deep Multiple Instance Learning (DMIL), we propose a method that trains pixel-level multi-class classifiers and predicts low-resolution labels (i.e., patch-level classification), where the actual high-resolution labels are learned implicitly without direct supervision. This is achieved with flexible pooling layers that are able to link the semantics of the pixels in the high-resolution imagery to the low-resolution reference labels. Then, the Multiple Instance Learning (MIL) problem is re-framed in a multi-class and in a multi-label setting. In the former, the low-resolution annotation represents the majority of the pixels in the patch. In the latter, the annotation only provides us information on the presence of one of the land-cover classes in the patch and thus multiple labels can be considered valid for a patch at a time, whereas the low-resolution labels provide us only one label. Therefore, the classifier is trained with a Positive-Unlabeled Learning (PUL) strategy. Experimental results on the 2020 IEEE GRSS Data Fusion Contest dataset show the effectiveness of the proposed framework compared to standard training strategies.",
    "summary": "",
    "translation": "基于粗粒度标签的高分辨率土地覆盖制图深度多示例学习方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于遥感图像的土地覆盖制图，属于计算机视觉在特定领域（地理信息）的应用。论文的核心技术（多示例学习）和问题领域（土地覆盖映射）与推荐系统、搜索或广告没有直接关联，也不涉及LLM、Transformer架构或异构数据统一建模等关键技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06757v1": {
    "title": "Transforming Noise Distributions with Histogram Matching: Towards a Single Denoiser for All",
    "url": "https://www.alphaxiv.org/abs/2510.06757v1",
    "arxiv_id": "2510.06757v1",
    "authors": "Sheng Fu, Junchao Zhang, Kailun Yang",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 08:34:50",
    "ori_summary": "Supervised Gaussian denoisers exhibit limited generalization when confronted with out-of-distribution noise, due to the diverse distributional characteristics of different noise types. To bridge this gap, we propose a histogram matching approach that transforms arbitrary noise towards a target Gaussian distribution with known intensity. Moreover, a mutually reinforcing cycle is established between noise transformation and subsequent denoising. This cycle progressively refines the noise to be converted, making it approximate the real noise, thereby enhancing the noise transformation effect and further improving the denoising performance. We tackle specific noise complexities: local histogram matching handles signal-dependent noise, intrapatch permutation processes channel-related noise, and frequency-domain histogram matching coupled with pixel-shuffle down-sampling breaks spatial correlation. By applying these transformations, a single Gaussian denoiser gains remarkable capability to handle various out-of-distribution noises, including synthetic noises such as Poisson, salt-and-pepper and repeating pattern noises, as well as complex real-world noises. Extensive experiments demonstrate the superior generalization and effectiveness of our method.",
    "summary": "",
    "translation": "通过直方图匹配转换噪声分布：迈向单一去噪器适用于所有场景",
    "relevance_score": 2,
    "reasoning": "该论文关注图像去噪中的直方图匹配技术，属于计算机视觉领域。虽然去噪技术在某些数据预处理场景中可能有间接应用，但该工作主要针对图像噪声分布转换，与推荐系统、搜索或广告的核心技术栈没有直接关联，且未涉及LLM、Transformer架构或异构数据建模等关键焦点领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06754v1": {
    "title": "UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene",
    "url": "https://www.alphaxiv.org/abs/2510.06754v1",
    "arxiv_id": "2510.06754v1",
    "authors": "Christian Maurer, Snehal Jauhri, Sophie Lueth, Georgia Chalvatzaki",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-08 08:30:26",
    "ori_summary": "Comprehensive visual, geometric, and semantic understanding of a 3D scene is crucial for successful execution of robotic tasks, especially in unstructured and complex environments. Additionally, to make robust decisions, it is necessary for the robot to evaluate the reliability of perceived information. While recent advances in 3D neural feature fields have enabled robots to leverage features from pretrained foundation models for tasks such as language-guided manipulation and navigation, existing methods suffer from two critical limitations: (i) they are typically scene-specific, and (ii) they lack the ability to model uncertainty in their predictions. We present UniFField, a unified uncertainty-aware neural feature field that combines visual, semantic, and geometric features in a single generalizable representation while also predicting uncertainty in each modality. Our approach, which can be applied zero shot to any new environment, incrementally integrates RGB-D images into our voxel-based feature representation as the robot explores the scene, simultaneously updating uncertainty estimation. We evaluate our uncertainty estimations to accurately describe the model prediction errors in scene reconstruction and semantic feature prediction. Furthermore, we successfully leverage our feature predictions and their respective uncertainty for an active object search task using a mobile manipulator robot, demonstrating the capability for robust decision-making.",
    "summary": "",
    "translation": "UniFField：一种可泛化的统一神经特征场，用于处理任意场景中的视觉、语义和空间不确定性",
    "relevance_score": 3,
    "reasoning": "该论文提出了一个统一的神经特征场来处理多模态不确定性，这与VLM类比处理异构数据的思路有一定相关性。然而，该方法主要针对视觉场景理解，没有明确展示在推荐系统、搜索或广告中的直接应用潜力，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06751v1": {
    "title": "OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot",
    "url": "https://www.alphaxiv.org/abs/2510.06751v1",
    "arxiv_id": "2510.06751v1",
    "authors": "Junhan Zhu, Hesong Wang, Mingluo Su, Zefang Wang, Huan Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 08:19:15",
    "ori_summary": "Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computational cost. Existing one-shot network pruning methods can hardly be directly applied to them due to the iterative denoising nature of diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel one-shot pruning framework that enables accurate and training-free compression of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex architectures of modern diffusion models and supporting diverse pruning granularity, including unstructured, N:M semi-structured, and structured (MHA heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the iterative dynamics of the diffusion process, by examining the problem from an error-accumulation perspective, we propose a novel timestep-aware Hessian construction that incorporates a logarithmic-decrease weighting scheme, assigning greater importance to earlier timesteps to mitigate potential error accumulation; (iii) Furthermore, a computationally efficient group-wise sequential pruning strategy is proposed to amortize the expensive calibration process. Extensive experiments show that OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering inference acceleration with minimal degradation in visual quality.",
    "summary": "",
    "translation": "OBS-Diff：一次性精确剪枝扩散模型",
    "relevance_score": 2,
    "reasoning": "该论文专注于扩散模型的模型压缩技术，属于通用的模型优化方法，与推荐系统、搜索或广告的核心技术领域没有直接关联。虽然高效的模型架构可能间接影响部署，但论文没有明确展示在推荐/搜索/广告领域的应用潜力，且不属于核心的Transformer架构或LLM技术进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06746v1": {
    "title": "DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement for Image Deraining",
    "url": "https://www.alphaxiv.org/abs/2510.06746v1",
    "arxiv_id": "2510.06746v1",
    "authors": "Zhiliang Zhu, Tao Zeng, Tao Yang, Guoliang Luo, Jiyong Zeng",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 08:05:11",
    "ori_summary": "Image deraining is crucial for improving visual quality and supporting reliable downstream vision tasks. Although Mamba-based models provide efficient sequence modeling, their limited ability to capture fine-grained details and lack of frequency-domain awareness restrict further improvements. To address these issues, we propose DeRainMamba, which integrates a Frequency-Aware State-Space Module (FASSM) and Multi-Directional Perception Convolution (MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from high-frequency image details, balancing rain removal and detail preservation. MDPConv further restores local structures by capturing anisotropic gradient features and efficiently fusing multiple convolution branches. Extensive experiments on four public benchmarks demonstrate that DeRainMamba consistently outperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer parameters and lower computational costs. These results validate the effectiveness of combining frequency-domain modeling and spatial detail enhancement within a state-space framework for single image deraining.",
    "summary": "",
    "translation": "DeRainMamba：一种用于图像去雨的频率感知状态空间模型与细节增强方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的图像去雨任务，使用状态空间模型处理视觉退化问题。这与推荐系统、搜索或广告的核心技术领域没有直接关联，也不涉及LLM技术、Transformer架构改进或异构数据统一建模等关注方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06694v1": {
    "title": "SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis",
    "url": "https://www.alphaxiv.org/abs/2510.06694v1",
    "arxiv_id": "2510.06694v1",
    "authors": "Jipeng Lyu, Jiahua Dong, Yu-Xiong Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 06:39:33",
    "ori_summary": "Persistent dynamic scene modeling for tracking and novel-view synthesis remains challenging due to the difficulty of capturing accurate deformations while maintaining computational efficiency. We propose SCas4D, a cascaded optimization framework that leverages structural patterns in 3D Gaussian Splatting for dynamic scenes. The key idea is that real-world deformations often exhibit hierarchical patterns, where groups of Gaussians share similar transformations. By progressively refining deformations from coarse part-level to fine point-level, SCas4D achieves convergence within 100 iterations per time frame and produces results comparable to existing methods with only one-twentieth of the training iterations. The approach also demonstrates effectiveness in self-supervised articulated object segmentation, novel view synthesis, and dense point tracking tasks.",
    "summary": "",
    "translation": "SCas4D：用于提升持久性4D新视角合成的结构级联优化",
    "relevance_score": 1,
    "reasoning": "该论文专注于4D新视角合成和结构优化，属于计算机视觉和图形学领域。虽然标题提到'优化'，但这与推荐系统、搜索或广告的核心技术无关，也不涉及LLM、Transformer架构或异构数据建模。该工作主要面向视觉内容生成和3D场景重建，属于明确的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06687v1": {
    "title": "Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion",
    "url": "https://www.alphaxiv.org/abs/2510.06687v1",
    "arxiv_id": "2510.06687v1",
    "authors": "Jie Luo, Yuxuan Jiang, Xin Jin, Mingyu Liu, Yihui Fan",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 06:15:06",
    "ori_summary": "Semantic segmentation serves as a cornerstone of scene understanding in autonomous driving but continues to face significant challenges under complex conditions such as occlusion. Light field and LiDAR modalities provide complementary visual and spatial cues that are beneficial for robust perception; however, their effective integration is hindered by limited viewpoint diversity and inherent modality discrepancies. To address these challenges, the first multimodal semantic segmentation dataset integrating light field data and point cloud data is proposed. Based on this dataset, we proposed a multi-modal light field point-cloud fusion segmentation network(Mlpfseg), incorporating feature completion and depth perception to segment both camera images and LiDAR point clouds simultaneously. The feature completion module addresses the density mismatch between point clouds and image pixels by performing differential reconstruction of point-cloud feature maps, enhancing the fusion of these modalities. The depth perception module improves the segmentation of occluded objects by reinforcing attention scores for better occlusion awareness. Our method outperforms image-only segmentation by 1.71 Mean Intersection over Union(mIoU) and point cloud-only segmentation by 2.38 mIoU, demonstrating its effectiveness.",
    "summary": "",
    "translation": "基于光场与激光雷达融合的语义分割算法",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的语义分割技术，结合光场和激光雷达两种传感器数据。虽然语义分割在自动驾驶等领域有应用，但论文标题未显示与推荐系统、搜索或广告的直接关联，也未提及任何Transformer架构、LLM技术或异构数据建模等核心关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06679v1": {
    "title": "DreamOmni2: Multimodal Instruction-based Editing and Generation",
    "url": "https://www.alphaxiv.org/abs/2510.06679v1",
    "arxiv_id": "2510.06679v1",
    "authors": "Bin Xia, Bohao Peng, Yuechen Zhang, Junjia Huang, Jiyang Liu, Jingyao Li, Haoru Tan, Sitong Wu, Chengyao Wang, Yitong Wang, Xinglong Wu, Bei Yu, Jiaya Jia",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 06:07:14",
    "ori_summary": "Recent advancements in instruction-based image editing and subject-driven generation have garnered significant attention, yet both tasks still face limitations in meeting practical user needs. Instruction-based editing relies solely on language instructions, which often fail to capture specific editing details, making reference images necessary. Meanwhile, subject-driven generation is limited to combining concrete objects or people, overlooking broader, abstract concepts. To address these challenges, we propose two novel tasks: multimodal instruction-based editing and generation. These tasks support both text and image instructions and extend the scope to include both concrete and abstract concepts, greatly enhancing their practical applications. We introduce DreamOmni2, tackling two primary challenges: data creation and model framework design. Our data synthesis pipeline consists of three steps: (1) using a feature mixing method to create extraction data for both abstract and concrete concepts, (2) generating multimodal instruction-based editing training data using the editing and extraction models, and (3) further applying the extraction model to create training data for multimodal instruction-based editing. For the framework, to handle multi-image input, we propose an index encoding and position encoding shift scheme, which helps the model distinguish images and avoid pixel confusion. Additionally, we introduce joint training with the VLM and our generation/editing model to better process complex instructions. In addition, we have proposed comprehensive benchmarks for these two new tasks to drive their development. Experiments show that DreamOmni2 has achieved impressive results. Models and codes will be released.",
    "summary": "",
    "translation": "DreamOmni2：基于多模态指令的编辑与生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态内容生成和编辑，属于纯粹的AIGC领域，与我的核心关注点（推荐系统、搜索、广告中的排名和建模技术）没有直接关联。虽然多模态技术可能在某些边缘场景中有潜在应用，但该论文标题明确指向内容生成任务，这属于明确排除的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06673v1": {
    "title": "Heptapod: Language Modeling on Visual Signals",
    "url": "https://www.alphaxiv.org/abs/2510.06673v1",
    "arxiv_id": "2510.06673v1",
    "authors": "Yongxin Zhu, Jiawei Chen, Yuanzhe Chen, Zhuo Chen, Dongya Jia, Jian Cong, Xiaobin Zhuang, Yuping Wang, Yuxuan Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 05:54:46",
    "ori_summary": "We introduce Heptapod, an image autoregressive model that adheres to the foundational principles of language modeling. Heptapod employs \\textbf{causal attention}, \\textbf{eliminates reliance on CFG}, and \\textbf{eschews the trend of semantic tokenizers}. Our key innovation is \\textit{next 2D distribution prediction}: a causal Transformer with reconstruction-focused visual tokenizer, learns to predict the distribution over the entire 2D spatial grid of images at each timestep. This learning objective unifies the sequential modeling of autoregressive framework with the holistic self-supervised learning of masked autoencoding, enabling the model to capture comprehensive image semantics via generative training. On the ImageNet generation benchmark, Heptapod achieves an FID of $2.70$, significantly outperforming previous causal autoregressive approaches. We hope our work inspires a principled rethinking of language modeling on visual signals and beyond.",
    "summary": "",
    "translation": "Heptapod：基于视觉信号的语言建模",
    "relevance_score": 3,
    "reasoning": "该论文涉及视觉信号的语言建模，与视觉语言模型（VLM）相关，这属于'VLM类比处理异构数据'的范畴。然而，标题没有明确说明该方法如何应用于推荐系统、搜索或广告中的异构数据建模，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06669v1": {
    "title": "Automated Neural Architecture Design for Industrial Defect Detection",
    "url": "https://www.alphaxiv.org/abs/2510.06669v1",
    "arxiv_id": "2510.06669v1",
    "authors": "Yuxi Liu, Yunfeng Ma, Yi Tang, Min Liu, Shuai Jiang, Yaonan Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 05:37:59",
    "ori_summary": "Industrial surface defect detection (SDD) is critical for ensuring product quality and manufacturing reliability. Due to the diverse shapes and sizes of surface defects, SDD faces two main challenges: intraclass difference and interclass similarity. Existing methods primarily utilize manually designed models, which require extensive trial and error and often struggle to address both challenges effectively. To overcome this, we propose AutoNAD, an automated neural architecture design framework for SDD that jointly searches over convolutions, transformers, and multi-layer perceptrons. This hybrid design enables the model to capture both fine-grained local variations and long-range semantic context, addressing the two key challenges while reducing the cost of manual network design. To support efficient training of such a diverse search space, AutoNAD introduces a cross weight sharing strategy, which accelerates supernet convergence and improves subnet performance. Additionally, a searchable multi-level feature aggregation module (MFAM) is integrated to enhance multi-scale feature learning. Beyond detection accuracy, runtime efficiency is essential for industrial deployment. To this end, AutoNAD incorporates a latency-aware prior to guide the selection of efficient architectures. The effectiveness of AutoNAD is validated on three industrial defect datasets and further applied within a defect imaging and detection platform. Code will be available at https://github.com/Yuxi104/AutoNAD.",
    "summary": "",
    "translation": "面向工业缺陷检测的自动化神经架构设计",
    "relevance_score": 1,
    "reasoning": "该论文专注于工业缺陷检测这一特定视觉应用领域，与推荐系统、搜索或广告的核心关注点无关。虽然涉及神经架构设计，但属于计算机视觉中的特定应用场景，且明确属于被排除的'Purely Vision'类别，没有任何与RecSys/Search/Ads相关的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06646v1": {
    "title": "The False Promise of Zero-Shot Super-Resolution in Machine-Learned Operators",
    "url": "https://www.alphaxiv.org/abs/2510.06646v1",
    "arxiv_id": "2510.06646v1",
    "authors": "Mansi Sakarvadia, Kareem Hegazy, Amin Totounferoush, Kyle Chard, Yaoqing Yang, Ian Foster, Michael W. Mahoney",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-08 04:59:56",
    "ori_summary": "A core challenge in scientific machine learning, and scientific computing more generally, is modeling continuous phenomena which (in practice) are represented discretely. Machine-learned operators (MLOs) have been introduced as a means to achieve this modeling goal, as this class of architecture can perform inference at arbitrary resolution. In this work, we evaluate whether this architectural innovation is sufficient to perform \"zero-shot super-resolution,\" namely to enable a model to serve inference on higher-resolution data than that on which it was originally trained. We comprehensively evaluate both zero-shot sub-resolution and super-resolution (i.e., multi-resolution) inference in MLOs. We decouple multi-resolution inference into two key behaviors: 1) extrapolation to varying frequency information; and 2) interpolating across varying resolutions. We empirically demonstrate that MLOs fail to do both of these tasks in a zero-shot manner. Consequently, we find MLOs are not able to perform accurate inference at resolutions different from those on which they were trained, and instead they are brittle and susceptible to aliasing. To address these failure modes, we propose a simple, computationally-efficient, and data-driven multi-resolution training protocol that overcomes aliasing and that provides robust multi-resolution generalization.",
    "summary": "",
    "translation": "机器学习算子中零样本超分辨率的虚假承诺",
    "relevance_score": 2,
    "reasoning": "该论文主要讨论机器学习算子中的零样本超分辨率问题，这属于计算机视觉领域的特定技术应用。虽然超分辨率技术理论上可能用于广告或推荐系统中的图像质量提升，但论文标题明确指向技术局限性和虚假承诺，缺乏与推荐系统、搜索或广告排名的直接关联，且未涉及Transformer架构、LLM技术或异构数据建模等核心关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06638v1": {
    "title": "StaR-KVQA: Structured Reasoning Traces for Implicit-Knowledge Visual Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.06638v1",
    "arxiv_id": "2510.06638v1",
    "authors": "Zhihao Wen, Wenkang Wei, Yuan Fang, Xingtong Yu, Hui Zhang, Weicheng Zhu, Xin Zhang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 04:37:53",
    "ori_summary": "Knowledge-based Visual Question Answering (KVQA) requires models to ground entities in images and reason over factual knowledge. We study its implicit-knowledge variant, IK-KVQA, where a multimodal large language model (MLLM) is the sole knowledge source, without external retrieval. Yet, MLLMs lack explicit reasoning supervision and produce inconsistent justifications, and generalize poorly after standard supervised fine-tuning (SFT). We present StaR-KVQA (Structured Reasoning Traces for IK-KVQA), which supervises structured traces - dual symbolic relation paths plus path-grounded natural-language explanations - so that reasoning becomes transparent and verifiable. With one open-source MLLM, StaR-KVQA constructs and selects path-grounded reasoning traces to form a trace-enriched dataset, then fine-tunes via structured self-distillation to align generation with supervision; no external retrievers, verifiers, or curated knowledge bases (KBs) are used, traces are built offline, and inference is a single autoregressive pass. Across benchmarks, StaR-KVQA improves both accuracy and interpretability, achieving up to +11.3% higher answer accuracy on OK-VQA over the strongest baseline while exhibiting robust cross-domain generalization.",
    "summary": "",
    "translation": "StaR-KVQA：面向隐式知识视觉问答的结构化推理轨迹",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉问答中的结构化推理轨迹，属于视觉语言模型领域。虽然VLM技术可能对处理异构数据有启发，但该工作聚焦于纯粹的视觉问答任务，与推荐系统、搜索或广告的直接关联性较弱，且未明确展示在这些领域的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06637v1": {
    "title": "Control-Augmented Autoregressive Diffusion for Data Assimilation",
    "url": "https://www.alphaxiv.org/abs/2510.06637v1",
    "arxiv_id": "2510.06637v1",
    "authors": "Prakhar Srivastava, Farrin Marouf Sofian, Francesco Immorlano, Kushagra Pandey, Stephan Mandt",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-08 04:37:32",
    "ori_summary": "Despite recent advances in test-time scaling and finetuning of diffusion models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains underexplored. We introduce an amortized framework that augments pretrained ARDMs with a lightweight controller network, trained offline by previewing future ARDM rollouts and learning stepwise controls that anticipate upcoming observations under a terminal cost objective. We evaluate this framework in the context of data assimilation (DA) for chaotic spatiotemporal partial differential equations (PDEs), a setting where existing methods are often computationally prohibitive and prone to forecast drift under sparse observations. Our approach reduces DA inference to a single forward rollout with on-the-fly corrections, avoiding expensive adjoint computations and/or optimizations during inference. We demonstrate that our method consistently outperforms four state-of-the-art baselines in stability, accuracy, and physical fidelity across two canonical PDEs and six observation regimes. We will release code and checkpoints publicly.",
    "summary": "",
    "translation": "用于数据同化的控制增强自回归扩散模型",
    "relevance_score": 3,
    "reasoning": "该论文提出了控制增强自回归扩散方法用于数据同化，这属于生成模型的技术进步。虽然扩散模型在推荐系统中可用于生成用户行为序列或内容表示，但数据同化主要应用于物理科学和天气预报领域，与推荐/搜索/广告的直接关联较弱。该方法可能通过改进序列生成质量间接应用于用户行为建模，但应用路径不够明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06635v1": {
    "title": "StruSR: Structure-Aware Symbolic Regression with Physics-Informed Taylor Guidance",
    "url": "https://www.alphaxiv.org/abs/2510.06635v1",
    "arxiv_id": "2510.06635v1",
    "authors": "Yunpeng Gong, Sihan Lan, Can Yang, Kunpeng Xu, Min Jiang",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-08 04:37:04",
    "ori_summary": "Symbolic regression aims to find interpretable analytical expressions by searching over mathematical formula spaces to capture underlying system behavior, particularly in scientific modeling governed by physical laws. However, traditional methods lack mechanisms for extracting structured physical priors from time series observations, making it difficult to capture symbolic expressions that reflect the system's global behavior. In this work, we propose a structure-aware symbolic regression framework, called StruSR, that leverages trained Physics-Informed Neural Networks (PINNs) to extract locally structured physical priors from time series data. By performing local Taylor expansions on the outputs of the trained PINN, we obtain derivative-based structural information to guide symbolic expression evolution. To assess the importance of expression components, we introduce a masking-based attribution mechanism that quantifies each subtree's contribution to structural alignment and physical residual reduction. These sensitivity scores steer mutation and crossover operations within genetic programming, preserving substructures with high physical or structural significance while selectively modifying less informative components. A hybrid fitness function jointly minimizes physics residuals and Taylor coefficient mismatch, ensuring consistency with both the governing equations and the local analytical behavior encoded by the PINN. Experiments on benchmark PDE systems demonstrate that StruSR improves convergence speed, structural fidelity, and expression interpretability compared to conventional baselines, offering a principled paradigm for physics-grounded symbolic discovery.",
    "summary": "",
    "translation": "StruSR：具有物理信息泰勒引导的结构感知符号回归",
    "relevance_score": 2,
    "reasoning": "该论文主要关注符号回归和物理信息引导，属于通用机器学习方法而非特定于推荐系统、搜索或广告领域。虽然结构感知建模在某些场景下可能有间接应用，但论文标题明确指向物理系统建模，与我的核心关注点（RecSys/Search/Ads的进展、LLM技术、Transformer架构或异构数据统一建模）缺乏直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06629v1": {
    "title": "Unsupervised Backdoor Detection and Mitigation for Spiking Neural Networks",
    "url": "https://www.alphaxiv.org/abs/2510.06629v1",
    "arxiv_id": "2510.06629v1",
    "authors": "Jiachen Li, Bang Wu, Xiaoyu Xia, Xiaoning Liu, Xun Yi, Xiuzhen Zhang",
    "categories": "cs.CR, cs.CV, cs.LG",
    "pub_date": "2025-10-08 04:25:35",
    "ori_summary": "Spiking Neural Networks (SNNs) have gained increasing attention for their superior energy efficiency compared to Artificial Neural Networks (ANNs). However, their security aspects, particularly under backdoor attacks, have received limited attention. Existing defense methods developed for ANNs perform poorly or can be easily bypassed in SNNs due to their event-driven and temporal dependencies. This paper identifies the key blockers that hinder traditional backdoor defenses in SNNs and proposes an unsupervised post-training detection framework, Temporal Membrane Potential Backdoor Detection (TMPBD), to overcome these challenges. TMPBD leverages the maximum margin statistics of temporal membrane potential (TMP) in the final spiking layer to detect target labels without any attack knowledge or data access. We further introduce a robust mitigation mechanism, Neural Dendrites Suppression Backdoor Mitigation (NDSBM), which clamps dendritic connections between early convolutional layers to suppress malicious neurons while preserving benign behaviors, guided by TMP extracted from a small, clean, unlabeled dataset. Extensive experiments on multiple neuromorphic benchmarks and state-of-the-art input-aware dynamic trigger attacks demonstrate that TMPBD achieves 100% detection accuracy, while NDSBM reduces the attack success rate from 100% to 8.44%, and to 2.81% when combined with detection, without degrading clean accuracy.",
    "summary": "",
    "translation": "脉冲神经网络的非监督后门检测与缓解",
    "relevance_score": 1,
    "reasoning": "该论文专注于脉冲神经网络（SNN）的安全问题，属于网络安全和模型安全领域，与推荐系统、搜索或广告的核心技术进展无关。虽然涉及模型检测，但其针对的是脉冲神经网络这种特定架构的安全漏洞，在推荐、搜索或广告系统中没有明显的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06621v1": {
    "title": "FEAorta: A Fully Automated Framework for Finite Element Analysis of the Aorta From 3D CT Images",
    "url": "https://www.alphaxiv.org/abs/2510.06621v1",
    "arxiv_id": "2510.06621v1",
    "authors": "Jiasong Chen, Linchen Qian, Ruonan Gong, Christina Sun, Tongran Qin, Thuy Pham, Caitlin Martin, Mohammad Zafar, John Elefteriades, Wei Sun, Liang Liang",
    "categories": "eess.IV, cs.CE, cs.CV, cs.LG",
    "pub_date": "2025-10-08 04:00:46",
    "ori_summary": "Aortic aneurysm disease ranks consistently in the top 20 causes of death in the U.S. population. Thoracic aortic aneurysm is manifested as an abnormal bulging of thoracic aortic wall and it is a leading cause of death in adults. From the perspective of biomechanics, rupture occurs when the stress acting on the aortic wall exceeds the wall strength. Wall stress distribution can be obtained by computational biomechanical analyses, especially structural Finite Element Analysis. For risk assessment, probabilistic rupture risk of TAA can be calculated by comparing stress with material strength using a material failure model. Although these engineering tools are currently available for TAA rupture risk assessment on patient specific level, clinical adoption has been limited due to two major barriers: labor intensive 3D reconstruction current patient specific anatomical modeling still relies on manual segmentation, making it time consuming and difficult to scale to a large patient population, and computational burden traditional FEA simulations are resource intensive and incompatible with time sensitive clinical workflows. The second barrier was successfully overcome by our team through the development of the PyTorch FEA library and the FEA DNN integration framework. By incorporating the FEA functionalities within PyTorch FEA and applying the principle of static determinacy, we reduced the FEA based stress computation time to approximately three minutes per case. Moreover, by integrating DNN and FEA through the PyTorch FEA library, our approach further decreases the computation time to only a few seconds per case. This work focuses on overcoming the first barrier through the development of an end to end deep neural network capable of generating patient specific finite element meshes of the aorta directly from 3D CT images.",
    "summary": "",
    "translation": "FEAorta：基于3D CT图像的主动脉有限元分析全自动框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像处理和生物力学分析，属于医疗领域特定应用。标题中提到的3D CT图像、主动脉有限元分析等均与推荐系统、搜索、广告或LLM技术完全无关，不涉及任何异构数据建模、Transformer架构改进或推荐系统核心进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06619v1": {
    "title": "MSITrack: A Challenging Benchmark for Multispectral Single Object Tracking",
    "url": "https://www.alphaxiv.org/abs/2510.06619v1",
    "arxiv_id": "2510.06619v1",
    "authors": "Tao Feng, Tingfa Xu, Haolin Qin, Tianhao Li, Shuaihao Han, Xuyang Zou, Zhan Lv, Jianan Li",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 03:56:36",
    "ori_summary": "Visual object tracking in real-world scenarios presents numerous challenges including occlusion, interference from similar objects and complex backgrounds-all of which limit the effectiveness of RGB-based trackers. Multispectral imagery, which captures pixel-level spectral reflectance, enhances target discriminability. However, the availability of multispectral tracking datasets remains limited. To bridge this gap, we introduce MSITrack, the largest and most diverse multispectral single object tracking dataset to date. MSITrack offers the following key features: (i) More Challenging Attributes-including interference from similar objects and similarity in color and texture between targets and backgrounds in natural scenarios, along with a wide range of real-world tracking challenges; (ii) Richer and More Natural Scenes-spanning 55 object categories and 300 distinct natural scenes, MSITrack far exceeds the scope of existing benchmarks. Many of these scenes and categories are introduced to the multispectral tracking domain for the first time; (iii) Larger Scale-300 videos comprising over 129k frames of multispectral imagery. To ensure annotation precision, each frame has undergone meticulous processing, manual labeling and multi-stage verification. Extensive evaluations using representative trackers demonstrate that the multispectral data in MSITrack significantly improves performance over RGB-only baselines, highlighting its potential to drive future advancements in the field. The MSITrack dataset is publicly available at: https://github.com/Fengtao191/MSITrack.",
    "summary": "",
    "translation": "MSITrack：一个用于多光谱单目标跟踪的挑战性基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的多光谱目标跟踪基准，属于纯粹的视觉研究方向。虽然多模态学习在推荐系统中有所应用，但该论文明确聚焦于单目标跟踪这一特定视觉任务，与推荐系统、搜索或广告的核心技术没有直接关联，也不涉及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06612v1": {
    "title": "A Bridge from Audio to Video: Phoneme-Viseme Alignment Allows Every Face to Speak Multiple Languages",
    "url": "https://www.alphaxiv.org/abs/2510.06612v1",
    "arxiv_id": "2510.06612v1",
    "authors": "Zibo Su, Kun Wei, Jiahua Li, Xu Yang, Cheng Deng",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 03:46:39",
    "ori_summary": "Speech-driven talking face synthesis (TFS) focuses on generating lifelike facial animations from audio input. Current TFS models perform well in English but unsatisfactorily in non-English languages, producing wrong mouth shapes and rigid facial expressions. The terrible performance is caused by the English-dominated training datasets and the lack of cross-language generalization abilities. Thus, we propose Multilingual Experts (MuEx), a novel framework featuring a Phoneme-Guided Mixture-of-Experts (PG-MoE) architecture that employs phonemes and visemes as universal intermediaries to bridge audio and video modalities, achieving lifelike multilingual TFS. To alleviate the influence of linguistic differences and dataset bias, we extract audio and video features as phonemes and visemes respectively, which are the basic units of speech sounds and mouth movements. To address audiovisual synchronization issues, we introduce the Phoneme-Viseme Alignment Mechanism (PV-Align), which establishes robust cross-modal correspondences between phonemes and visemes. In addition, we build a Multilingual Talking Face Benchmark (MTFB) comprising 12 diverse languages with 95.04 hours of high-quality videos for training and evaluating multilingual TFS performance. Extensive experiments demonstrate that MuEx achieves superior performance across all languages in MTFB and exhibits effective zero-shot generalization to unseen languages without additional training.",
    "summary": "",
    "translation": "从音频到视频的桥梁：音素-视位对齐使每张脸都能说多种语言",
    "relevance_score": 2,
    "reasoning": "该论文主要关注音视频跨模态对齐和语音驱动面部动画，属于计算机视觉和多媒体处理领域。虽然涉及多模态建模，但其应用场景（语音到视频的面部动画）与推荐系统、搜索或广告的核心技术需求没有直接关联，潜在应用价值有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06611v1": {
    "title": "Self-supervised Physics-guided Model with Implicit Representation Regularization for Fast MRI Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2510.06611v1",
    "arxiv_id": "2510.06611v1",
    "authors": "Jingran Xu, Yuanyuan Liu, Yanjie Zhu",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 03:40:40",
    "ori_summary": "Magnetic Resonance Imaging (MRI) is a vital clinical diagnostic tool, yet its widespread application is limited by prolonged scan times. Fast MRI reconstruction techniques effectively reduce acquisition duration by reconstructing high-fidelity MR images from undersampled k-space data. In recent years, deep learning-based methods have demonstrated remarkable progress in this field, with self-supervised and unsupervised learning approaches proving particularly valuable in scenarios where fully sampled data are difficult to obtain. This paper proposes a novel zero-shot self-supervised reconstruction framework named UnrollINR, which enables scan-specific MRI reconstruction without relying on external training data. The method adopts a physics-guided unrolled iterative reconstruction architecture and introduces Implicit Neural Representation (INR) as a regularization prior to effectively constrain the solution space. By combining a deep unrolled structure with the powerful implicit representation capability of INR, the model's interpretability and reconstruction performance are enhanced. Experimental results demonstrate that even at a high acceleration rate of 10, UnrollINR achieves superior reconstruction performance compared to the supervised learning method, validating the superiority of the proposed method.",
    "summary": "",
    "translation": "基于隐式表示正则化的自监督物理引导模型用于快速MRI重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（MRI）重建领域，属于医疗应用范畴，与推荐系统、搜索或广告的核心技术无关。虽然提到了自监督学习和正则化技术，但这些方法在医学影像中的特定应用无法直接迁移到推荐系统、搜索或广告领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06601v1": {
    "title": "AIM 2025 Challenge on Real-World RAW Image Denoising",
    "url": "https://www.alphaxiv.org/abs/2510.06601v1",
    "arxiv_id": "2510.06601v1",
    "authors": "Feiran Li, Jiacheng Li, Marcos V. Conde, Beril Besbinar, Vlad Hosu, Daisuke Iso, Radu Timofte",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 03:22:42",
    "ori_summary": "We introduce the AIM 2025 Real-World RAW Image Denoising Challenge, aiming to advance efficient and effective denoising techniques grounded in data synthesis. The competition is built upon a newly established evaluation benchmark featuring challenging low-light noisy images captured in the wild using five different DSLR cameras. Participants are tasked with developing novel noise synthesis pipelines, network architectures, and training methodologies to achieve high performance across different camera models. Winners are determined based on a combination of performance metrics, including full-reference measures (PSNR, SSIM, LPIPS), and non-reference ones (ARNIQA, TOPIQ). By pushing the boundaries of camera-agnostic low-light RAW image denoising trained on synthetic data, the competition promotes the development of robust and practical models aligned with the rapid progress in digital photography. We expect the competition outcomes to influence multiple domains, from image restoration to night-time autonomous driving.",
    "summary": "",
    "translation": "AIM 2025 真实世界RAW图像去噪挑战赛",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的图像去噪技术，特别是针对RAW格式图像的处理。虽然图像质量在某些推荐和广告场景中可能间接相关，但该工作本身是纯粹的视觉处理任务，没有涉及推荐系统、搜索或广告的核心技术，也没有LLM或Transformer架构的应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06596v1": {
    "title": "SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.06596v1",
    "arxiv_id": "2510.06596v1",
    "authors": "Ayush Zenith, Arnold Zumbrun, Neel Raut, Jing Lin",
    "categories": "cs.CV, cs.AI, cs.IT, cs.LG, math.IT",
    "pub_date": "2025-10-08 03:01:26",
    "ori_summary": "The performance of machine learning models depends heavily on training data. The scarcity of large-scale, well-annotated datasets poses significant challenges in creating robust models. To address this, synthetic data generated through simulations and generative models has emerged as a promising solution, enhancing dataset diversity and improving the performance, reliability, and resilience of models. However, evaluating the quality of this generated data requires an effective metric. This paper introduces the Synthetic Dataset Quality Metric (SDQM) to assess data quality for object detection tasks without requiring model training to converge. This metric enables more efficient generation and selection of synthetic datasets, addressing a key challenge in resource-constrained object detection tasks. In our experiments, SDQM demonstrated a strong correlation with the mean Average Precision (mAP) scores of YOLOv11, a leading object detection model, while previous metrics only exhibited moderate or weak correlations. Additionally, it provides actionable insights for improving dataset quality, minimizing the need for costly iterative training. This scalable and efficient metric sets a new standard for evaluating synthetic data. The code for SDQM is available at https://github.com/ayushzenith/SDQM",
    "summary": "",
    "translation": "SDQM：用于目标检测数据集评估的合成数据质量度量",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的目标检测数据集评估和合成数据质量度量，与推荐系统、搜索或广告的核心技术领域没有直接关联。合成数据生成和评估属于计算机视觉和数据集构建的范畴，不具备在推荐、搜索或广告系统中应用的明显潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06592v1": {
    "title": "Adaptive Stain Normalization for Cross-Domain Medical Histology",
    "url": "https://www.alphaxiv.org/abs/2510.06592v1",
    "arxiv_id": "2510.06592v1",
    "authors": "Tianyue Xu, Yanlin Wu, Abhai K. Tripathi, Matthew M. Ippolito, Benjamin D. Haeffele",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 02:53:28",
    "ori_summary": "Deep learning advances have revolutionized automated digital pathology analysis. However, differences in staining protocols and imaging conditions can introduce significant color variability. In deep learning, such color inconsistency often reduces performance when deploying models on data acquired under different conditions from the training data, a challenge known as domain shift. Many existing methods attempt to address this problem via color normalization but suffer from several notable drawbacks such as introducing artifacts or requiring careful choice of a template image for stain mapping. To address these limitations, we propose a trainable color normalization model that can be integrated with any backbone network for downstream tasks such as object detection and classification. Based on the physics of the imaging process per the Beer-Lambert law, our model architecture is derived via algorithmic unrolling of a nonnegative matrix factorization (NMF) model to extract stain-invariant structural information from the original pathology images, which serves as input for further processing. Experimentally, we evaluate the method on publicly available pathology datasets and an internally curated collection of malaria blood smears for cross-domain object detection and classification, where our method outperforms many state-of-the-art stain normalization methods. Our code is available at https://github.com/xutianyue/BeerLaNet.",
    "summary": "",
    "translation": "用于跨域医学组织学的自适应染色归一化",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像处理中的染色归一化技术，属于医学领域的特定应用。虽然涉及跨域适应，但其核心关注医学组织学图像处理，与推荐系统、搜索或广告的技术焦点完全无关。该研究没有显示出在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06590v1": {
    "title": "Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer",
    "url": "https://www.alphaxiv.org/abs/2510.06590v1",
    "arxiv_id": "2510.06590v1",
    "authors": "Ziyuan Huang, DanDan Zheng, Cheng Zou, Rui Liu, Xiaolong Wang, Kaixiang Ji, Weilong Chai, Jianxin Sun, Libin Wang, Yongjie Lv, Taozhi Huang, Jiajia Liu, Qingpei Guo, Ming Yang, Jingdong Chen, Jun Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 02:50:14",
    "ori_summary": "Visual tokenization remains a core challenge in unifying visual understanding and generation within the autoregressive paradigm. Existing methods typically employ tokenizers in discrete latent spaces to align with the tokens from large language models, where the quantization errors can limit semantic expressiveness and degrade the capability of vision-language understanding. To address this, we introduce MingTok, a new family of visual tokenizers with a continuous latent space, for unified autoregressive generation and understanding. While understanding tasks favor discriminative high-dimensional features, generation tasks prefer compact low-level codes. Thus, to reconcile these competing demands, MingTok adopts a three-stage sequential architecture involving low-level encoding, semantic expansion, and visual reconstruction. Built on top of it, Ming-UniVision eliminates the need for task-specific visual representations, and unifies diverse vision-language tasks under a single autoregrsssive prediction paradigm. By formulating both understanding and generation as next-token prediction in a shared continuous space, it seamlessly supports multi-round, in-context tasks such as iterative understanding, generation and editing. Empirically, we find that using a unified continuous visual representation reconciles the competing requirements on the tokenizers by the understanding and generation tasks, thereby leading to state-of-the-art level performance across both domains. We hope our findings will facilitate unified visual tokenization in the continuous domain. Inference code and model weights are released to benefit community.",
    "summary": "",
    "translation": "Ming-UniVision：基于统一连续分词器的联合图像理解与生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉领域的统一建模（图像理解与生成），属于纯粹的视觉或多模态研究方向。虽然标题提到'统一分词器'技术，但缺乏明确的与推荐系统、搜索或广告领域的应用关联。这种视觉-语言统一建模技术可能对处理图像内容有启发，但未直接涉及用户行为序列、上下文特征或排名优化等核心RecSys/Search/Ads问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06584v1": {
    "title": "Improving Artifact Robustness for CT Deep Learning Models Without Labeled Artifact Images via Domain Adaptation",
    "url": "https://www.alphaxiv.org/abs/2510.06584v1",
    "arxiv_id": "2510.06584v1",
    "authors": "Justin Cheung, Samuel Savine, Calvin Nguyen, Lin Lu, Alhassan S. Yasin",
    "categories": "cs.CV, q-bio.TO",
    "pub_date": "2025-10-08 02:27:09",
    "ori_summary": "Deep learning models which perform well on images from their training distribution can degrade substantially when applied to new distributions. If a CT scanner introduces a new artifact not present in the training labels, the model may misclassify the images. Although modern CT scanners include design features which mitigate these artifacts, unanticipated or difficult-to-mitigate artifacts can still appear in practice. The direct solution of labeling images from this new distribution can be costly. As a more accessible alternative, this study evaluates domain adaptation as an approach for training models that maintain classification performance despite new artifacts, even without corresponding labels. We simulate ring artifacts from detector gain error in sinogram space and evaluate domain adversarial neural networks (DANN) against baseline and augmentation-based approaches on the OrganAMNIST abdominal CT dataset. Our results demonstrate that baseline models trained only on clean images fail to generalize to images with ring artifacts, and traditional augmentation with other distortion types provides no improvement on unseen artifact domains. In contrast, the DANN approach successfully maintains high classification accuracy on ring artifact images using only unlabeled artifact data during training, demonstrating the viability of domain adaptation for artifact robustness. The domain-adapted model achieved classification performance on ring artifact test data comparable to models explicitly trained with labeled artifact images, while also showing unexpected generalization to uniform noise. These findings provide empirical evidence that domain adaptation can effectively address distribution shift in medical imaging without requiring expensive expert labeling of new artifact distributions, suggesting promise for deployment in clinical settings where novel artifacts may emerge.",
    "summary": "",
    "translation": "通过领域自适应改进CT深度学习模型的伪影鲁棒性，无需带标签的伪影图像",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（CT扫描）中的深度学习模型鲁棒性改进，属于医疗领域的特定应用。虽然涉及领域自适应技术，但该技术应用于医学图像伪影处理，与推荐系统、搜索或广告领域没有直接关联，也不符合任何当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06582v1": {
    "title": "Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.06582v1",
    "arxiv_id": "2510.06582v1",
    "authors": "Fei Zhang, Rob Chancia, Josie Clapp, Amirhossein Hassanzadeh, Dimah Dera, Richard MacKenzie, Jan van Aardt",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-08 02:25:59",
    "ori_summary": "Accurate semantic segmentation of terrestrial laser scanning (TLS) point clouds is limited by costly manual annotation. We propose a semi-automated, uncertainty-aware pipeline that integrates spherical projection, feature enrichment, ensemble learning, and targeted annotation to reduce labeling effort, while sustaining high accuracy. Our approach projects 3D points to a 2D spherical grid, enriches pixels with multi-source features, and trains an ensemble of segmentation networks to produce pseudo-labels and uncertainty maps, the latter guiding annotation of ambiguous regions. The 2D outputs are back-projected to 3D, yielding densely annotated point clouds supported by a three-tier visualization suite (2D feature maps, 3D colorized point clouds, and compact virtual spheres) for rapid triage and reviewer guidance. Using this pipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove forests. We further evaluate data efficiency and feature importance to address two key questions: (1) how much annotated data are needed and (2) which features matter most. Results show that performance saturates after ~12 annotated scans, geometric features contribute the most, and compact nine-channel stacks capture nearly all discriminative power, with the mean Intersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm the generalization of our feature-enrichment strategy through cross-dataset tests on ForestSemantic and Semantic3D. Our contributions include: (i) a robust, uncertainty-aware TLS annotation pipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii) empirical guidance on data efficiency and feature importance, thus enabling scalable, high-quality segmentation of TLS point clouds for ecological monitoring and beyond. The dataset and processing scripts are publicly available at https://fz-rit.github.io/through-the-lidars-eye/.",
    "summary": "",
    "translation": "基于LiDAR视角：面向地面点云分割的特征增强与不确定性感知标注流程",
    "relevance_score": 1,
    "reasoning": "该论文专注于LiDAR点云分割和标注流程，属于纯粹的计算机视觉和3D感知领域。虽然涉及特征增强技术，但其应用场景（地面点云分割）与推荐系统、搜索或广告领域没有任何直接或间接关联，完全超出了关注范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06564v1": {
    "title": "HSNet: Heterogeneous Subgraph Network for Single Image Super-resolution",
    "url": "https://www.alphaxiv.org/abs/2510.06564v1",
    "arxiv_id": "2510.06564v1",
    "authors": "Qiongyang Hu, Wenyang Liu, Wenbin Zou, Yuejiao Su, Lap-Pui Chau, Yi Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-08 01:32:52",
    "ori_summary": "Existing deep learning approaches for image super-resolution, particularly those based on CNNs and attention mechanisms, often suffer from structural inflexibility. Although graph-based methods offer greater representational adaptability, they are frequently impeded by excessive computational complexity. To overcome these limitations, this paper proposes the Heterogeneous Subgraph Network (HSNet), a novel framework that efficiently leverages graph modeling while maintaining computational feasibility. The core idea of HSNet is to decompose the global graph into manageable sub-components. First, we introduce the Constructive Subgraph Set Block (CSSB), which generates a diverse set of complementary subgraphs. Rather than relying on a single monolithic graph, CSSB captures heterogeneous characteristics of the image by modeling different relational patterns and feature interactions, producing a rich ensemble of both local and global graph structures. Subsequently, the Subgraph Aggregation Block (SAB) integrates the representations embedded across these subgraphs. Through adaptive weighting and fusion of multi-graph features, SAB constructs a comprehensive and discriminative representation that captures intricate interdependencies. Furthermore, a Node Sampling Strategy (NSS) is designed to selectively retain the most salient features, thereby enhancing accuracy while reducing computational overhead. Extensive experiments demonstrate that HSNet achieves state-of-the-art performance, effectively balancing reconstruction quality with computational efficiency. The code will be made publicly available.",
    "summary": "",
    "translation": "HSNet：用于单图像超分辨率的异质子图网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的单图像超分辨率任务，属于纯粹的视觉处理领域。虽然涉及异质图网络技术，但该技术应用于图像像素处理，与推荐系统、搜索或广告中的异构数据处理没有直接关联，也没有展示在推荐/搜索/广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06541v1": {
    "title": "Cluster Paths: Navigating Interpretability in Neural Networks",
    "url": "https://www.alphaxiv.org/abs/2510.06541v1",
    "arxiv_id": "2510.06541v1",
    "authors": "Nicholas M. Kroeger, Vincent Bindschaedler",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-08 00:41:09",
    "ori_summary": "While modern deep neural networks achieve impressive performance in vision tasks, they remain opaque in their decision processes, risking unwarranted trust, undetected biases and unexpected failures. We propose cluster paths, a post-hoc interpretability method that clusters activations at selected layers and represents each input as its sequence of cluster IDs. To assess these cluster paths, we introduce four metrics: path complexity (cognitive load), weighted-path purity (class alignment), decision-alignment faithfulness (predictive fidelity), and path agreement (stability under perturbations). In a spurious-cue CIFAR-10 experiment, cluster paths identify color-based shortcuts and collapse when the cue is removed. On a five-class CelebA hair-color task, they achieve 90% faithfulness and maintain 96% agreement under Gaussian noise without sacrificing accuracy. Scaling to a Vision Transformer pretrained on ImageNet, we extend cluster paths to concept paths derived from prompting a large language model on minimal path divergences. Finally, we show that cluster paths can serve as an effective out-of-distribution (OOD) detector, reliably flagging anomalous samples before the model generates over-confident predictions. Cluster paths uncover visual concepts, such as color palettes, textures, or object contexts, at multiple network depths, demonstrating that cluster paths scale to large vision models while generating concise and human-readable explanations.",
    "summary": "",
    "translation": "簇路径：神经网络可解释性导航",
    "relevance_score": 2,
    "reasoning": "该论文主要关注神经网络的可解释性方法，属于通用AI研究领域。虽然可解释性在推荐系统和搜索中有一定价值，但论文标题没有表明与Transformer架构、LLM技术或推荐/搜索/广告领域的直接关联，也没有提到处理异构数据或多模态建模等核心关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.06529v1": {
    "title": "VUGEN: Visual Understanding priors for GENeration",
    "url": "https://www.alphaxiv.org/abs/2510.06529v1",
    "arxiv_id": "2510.06529v1",
    "authors": "Xiangyi Chen, Théophane Vallaeys, Maha Elbayad, John Nguyen, Jakob Verbeek",
    "categories": "cs.CV",
    "pub_date": "2025-10-08 00:04:47",
    "ori_summary": "Recent advances in Vision-Language Models (VLMs) have enabled unified understanding across text and images, yet equipping these models with robust image generation capabilities remains challenging. Existing approaches often rely on reconstruction-oriented autoencoders or complex bridging mechanisms, leading to misalignment between understanding and generation representations, or architectural complexity. In this work, we propose VUGEN, a novel framework that explicitly leverages VLM's pretrained visual understanding priors for efficient and high-quality image generation. Our approach first transforms the high-dimensional latent space of the VLM's native vision encoder into a lower-dimensional, tractable distribution that maximally preserves visual information. The VLM is then trained to sample within this reduced latent space, ensuring alignment with its visual understanding capabilities. Finally, a dedicated pixel decoder maps these generated latents back to the image space. We find that a VAE-free pixel diffusion decoder to be on par or better than commonly used complex latent diffusion decoders that internally rely on VAE latents. Extensive experiments demonstrate that VUGEN achieves superior image generation performance, improving DPG Bench from 71.17 to 74.32 and FID from 11.86 to 9.06 on COCO, while fully preserving the VLM's original understanding capabilities.",
    "summary": "",
    "translation": "VUGEN：用于生成的视觉理解先验",
    "relevance_score": 2,
    "reasoning": "该论文标题表明其聚焦于视觉理解和生成任务，属于纯粹的视觉-语言模型或视觉生成领域。虽然标题提及'理解先验'可能暗示多模态建模，但缺乏与推荐系统、搜索或广告中异构数据处理的具体关联，且生成导向与当前关注的排序和检索核心问题不符。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08558v1": {
    "title": "Agent Learning via Early Experience",
    "url": "https://www.alphaxiv.org/abs/2510.08558v1",
    "arxiv_id": "2510.08558v1",
    "authors": "Kai Zhang, Xiangchao Chen, Bo Liu, Tianci Xue, Zeyi Liao, Zhihan Liu, Xiyao Wang, Yuting Ning, Zhaorun Chen, Xiaohan Fu, Jian Xie, Yuxuan Sun, Boyu Gou, Qi Qi, Zihang Meng, Jianwei Yang, Ning Zhang, Xian Li, Ashish Shah, Dat Huynh, Hengduo Li, Zi Yang, Sara Cao, Lawrence Jang, Shuyan Zhou, Jiacheng Zhu, Huan Sun, Jason Weston, Yu Su, Yifan Wu",
    "categories": "cs.AI, cs.CL, cs.IR, cs.LG",
    "pub_date": "2025-10-09 17:59:17",
    "ori_summary": "A long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks. However, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As a result, most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only a narrow range of scenarios and expose the agent to limited environment diversity. We address this limitation with a middle-ground paradigm we call early experience: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals. Within this paradigm we study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making. We evaluate across eight diverse environments and multiple model families. Our approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover, in environments with verifiable rewards, our results provide promising signals that early experience offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge between imitation learning and fully experience-driven agents.",
    "summary": "",
    "translation": "智能体通过早期经验学习",
    "relevance_score": 2,
    "reasoning": "该论文标题聚焦于智能体学习机制，主要涉及强化学习或智能体训练方法，属于通用AI技术范畴。虽然智能体技术可能间接应用于推荐系统或搜索的交互优化，但标题未明确显示与推荐系统、搜索、广告或相关使能技术（如Transformer、LLM应用）的直接关联，且未提及多模态数据处理等具体应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08385v1": {
    "title": "Detecting Legend Items on Historical Maps Using GPT-4o with In-Context Learning",
    "url": "https://www.alphaxiv.org/abs/2510.08385v1",
    "arxiv_id": "2510.08385v1",
    "authors": "Sofia Kirsanova, Yao-Yi Chiang, Weiwei Duan",
    "categories": "cs.CV, cs.AI, cs.DB, cs.IR, H.2.8; H.3.3; I.2.10; I.4.8",
    "pub_date": "2025-10-09 16:08:48",
    "ori_summary": "Historical map legends are critical for interpreting cartographic symbols. However, their inconsistent layouts and unstructured formats make automatic extraction challenging. Prior work focuses primarily on segmentation or general optical character recognition (OCR), with few methods effectively matching legend symbols to their corresponding descriptions in a structured manner. We present a method that combines LayoutLMv3 for layout detection with GPT-4o using in-context learning to detect and link legend items and their descriptions via bounding box predictions. Our experiments show that GPT-4 with structured JSON prompts outperforms the baseline, achieving 88% F-1 and 85% IoU, and reveal how prompt design, example counts, and layout alignment affect performance. This approach supports scalable, layout-aware legend parsing and improves the indexing and searchability of historical maps across various visual styles.",
    "summary": "",
    "translation": "使用GPT-4o和上下文学习检测历史地图上的图例项",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及计算机视觉任务（历史地图分析）和LLM的特定应用，与推荐系统、搜索或广告的核心领域进展无关。虽然使用了GPT-4o，但应用场景（历史地图图例检测）在RecSys/Search/Ads领域没有明显的实际应用潜力，属于纯粹的视觉应用范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08281v1": {
    "title": "Mobile Gamer Lifetime Value Prediction via Objective Decomposition and Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2510.08281v1",
    "arxiv_id": "2510.08281v1",
    "authors": "Tianwei Li, Yu Zhao, Yunze Li, Sheng Li",
    "categories": "cs.IR",
    "pub_date": "2025-10-09 14:33:12",
    "ori_summary": "For Internet platforms operating real-time bidding (RTB) advertising service, a comprehensive understanding of user lifetime value (LTV) plays a pivotal role in optimizing advertisement allocation efficiency and maximizing the return on investment (ROI) for advertisement sponsors, thereby facilitating growth of commercialization revenue for the platform. However, the inherent complexity of user LTV distributions induces significant challenges in accurate LTV prediction. Existing state-of-the-art works, which primarily focus on directly learning the LTV distributions through well-designed loss functions, achieve limited success due to their vulnerability to outliers. In this paper, we proposed a novel LTV prediction method to address distribution challenges through an objective decomposition and reconstruction framework. Briefly speaking, based on the in-app purchase characteristics of mobile gamers, our model was designed to first predict the number of transactions at specific prices and then calculate the total payment amount from these intermediate predictions. Our proposed model was evaluated through experiments on real-world industrial dataset, and deployed on the TapTap RTB advertising system for online A/B testing along with the state-of-the-art ZILN model.",
    "summary": "论文研究移动游戏广告中用户终身价值预测的分布复杂性挑战，核心思想是通过将LTV预测分解为特定价格交易次数预测，然后重构总支付金额来应对分布异常值问题。",
    "translation": "通过目标分解与重构的移动游戏玩家终身价值预测",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及推荐系统中的核心预测任务——用户价值预测，这是广告和推荐系统的关键组成部分。目标分解与重构方法可以应用于更广泛的用户行为建模场景，包括电商推荐和广告投放中的用户价值评估。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对广告系统中的用户终身价值预测问题，提出了分解重构的建模方法，与广告领域优化和推荐系统核心挑战高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08252v1": {
    "title": "ReasonEmbed: Enhanced Text Embeddings for Reasoning-Intensive Document Retrieval",
    "url": "https://www.alphaxiv.org/abs/2510.08252v1",
    "arxiv_id": "2510.08252v1",
    "authors": "Jianlyu Chen, Junwei Lan, Chaofan Li, Defu Lian, Zheng Liu",
    "categories": "cs.IR, cs.CL",
    "pub_date": "2025-10-09 14:10:26",
    "ori_summary": "In this paper, we introduce ReasonEmbed, a novel text embedding model developed for reasoning-intensive document retrieval. Our work includes three key technical contributions. First, we propose ReMixer, a new data synthesis method that overcomes the triviality problem prevalent in previous synthetic datasets, enabling large-scale production of 82K high-quality training samples. Second, we design Redapter, a self-adaptive learning algorithm that dynamically adjusts training each sample's weight based on its reasoning intensity. This allows the model to effectively capture the complex semantic relationships between queries and documents. Third, we implement ReasonEmbed across multiple backbones of varying sizes, all of which achieve superior performance on reasoning-intensive retrieval tasks. Notably, our ReasonEmbed-Qwen3-8B model offers a record-high nDCG@10 score of 38.1 on the BRIGHT benchmark, which significantly outperforms existing text embedding models. We will fully open-source our created resources in ReasonEmbed to push forward the research advancement in this field.",
    "summary": "该论文研究推理密集型文档检索中的文本嵌入问题，核心方法是提出ReMixer数据合成技术解决合成数据简单化问题，并设计Redapter自适应学习算法动态调整训练权重以捕捉复杂语义关系。",
    "translation": "ReasonEmbed：用于推理密集型文档检索的增强文本嵌入",
    "relevance_score": 8,
    "reasoning": "该论文专注于增强文本嵌入技术，这属于核心LLM技术的进步，对搜索系统具有直接应用价值。改进的文本嵌入可以显著提升文档检索的准确性和推理能力，在搜索和推荐系统中能够更好地理解用户查询意图和文档语义，从而提高检索质量。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文专注于推理密集型文档检索的文本嵌入技术，其数据合成和自适应学习算法在搜索和推荐系统中具有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08109v1": {
    "title": "VersionRAG: Version-Aware Retrieval-Augmented Generation for Evolving Documents",
    "url": "https://www.alphaxiv.org/abs/2510.08109v1",
    "arxiv_id": "2510.08109v1",
    "authors": "Daniel Huwiler, Kurt Stockinger, Jonathan Fürst",
    "categories": "cs.IR, cs.AI, cs.CL",
    "pub_date": "2025-10-09 11:48:58",
    "ori_summary": "Retrieval-Augmented Generation (RAG) systems fail when documents evolve through versioning-a ubiquitous characteristic of technical documentation. Existing approaches achieve only 58-64% accuracy on version-sensitive questions, retrieving semantically similar content without temporal validity checks. We present VersionRAG, a version-aware RAG framework that explicitly models document evolution through a hierarchical graph structure capturing version sequences, content boundaries, and changes between document states. During retrieval, VersionRAG routes queries through specialized paths based on intent classification, enabling precise version-aware filtering and change tracking. On our VersionQA benchmark-100 manually curated questions across 34 versioned technical documents-VersionRAG achieves 90% accuracy, outperforming naive RAG (58%) and GraphRAG (64%). VersionRAG reaches 60% accuracy on implicit change detection where baselines fail (0-10%), demonstrating its ability to track undocumented modifications. Additionally, VersionRAG requires 97% fewer tokens during indexing than GraphRAG, making it practical for large-scale deployment. Our work establishes versioned document QA as a distinct task and provides both a solution and benchmark for future research.",
    "summary": "",
    "translation": "VersionRAG：面向演进文档的版本感知检索增强生成",
    "relevance_score": 7,
    "reasoning": "该论文提出的版本感知RAG技术属于LLM核心技术的进展，在搜索和推荐系统中具有直接应用潜力。当文档内容随时间演进时（如产品描述、新闻文章、政策法规），版本感知检索能够确保系统返回最新且相关的信息，这对于搜索结果的时效性和推荐系统的准确性至关重要。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08048v1": {
    "title": "TaoSR-AGRL: Adaptive Guided Reinforcement Learning Framework for E-commerce Search Relevance",
    "url": "https://www.alphaxiv.org/abs/2510.08048v1",
    "arxiv_id": "2510.08048v1",
    "authors": "Jianhui Yang, Yiming Jin, Pengkun Jiao, Chenhe Dong, Zerui Huang, Shaowei Yao, Xiaojiang Zhou, Dan Ou, Haihong Tang",
    "categories": "cs.IR, cs.AI, cs.CL",
    "pub_date": "2025-10-09 10:34:39",
    "ori_summary": "Query-product relevance prediction is fundamental to e-commerce search and has become even more critical in the era of AI-powered shopping, where semantic understanding and complex reasoning directly shape the user experience and business conversion. Large Language Models (LLMs) enable generative, reasoning-based approaches, typically aligned via supervised fine-tuning (SFT) or preference optimization methods like Direct Preference Optimization (DPO). However, the increasing complexity of business rules and user queries exposes the inability of existing methods to endow models with robust reasoning capacity for long-tail and challenging cases. Efforts to address this via reinforcement learning strategies like Group Relative Policy Optimization (GRPO) often suffer from sparse terminal rewards, offering insufficient guidance for multi-step reasoning and slowing convergence. To address these challenges, we propose TaoSR-AGRL, an Adaptive Guided Reinforcement Learning framework for LLM-based relevance prediction in Taobao Search Relevance. TaoSR-AGRL introduces two key innovations: (1) Rule-aware Reward Shaping, which decomposes the final relevance judgment into dense, structured rewards aligned with domain-specific relevance criteria; and (2) Adaptive Guided Replay, which identifies low-accuracy rollouts during training and injects targeted ground-truth guidance to steer the policy away from stagnant, rule-violating reasoning patterns toward compliant trajectories. TaoSR-AGRL was evaluated on large-scale real-world datasets and through online side-by-side human evaluations on Taobao Search. It consistently outperforms DPO and standard GRPO baselines in offline experiments, improving relevance accuracy, rule adherence, and training stability. The model trained with TaoSR-AGRL has been successfully deployed in the main search scenario on Taobao, serving hundreds of millions of users.",
    "summary": "论文研究电商搜索中LLM相关性预测的推理能力不足问题，核心思想是通过规则感知奖励塑造和自适应引导回放机制，为多步推理提供密集的结构化奖励和针对性指导。",
    "translation": "TaoSR-AGRL：面向电商搜索相关性的自适应引导强化学习框架",
    "relevance_score": 8,
    "reasoning": "该论文直接聚焦于电商搜索相关性优化，属于核心推荐系统领域的进展。虽然涉及强化学习，但明确应用于搜索排名场景，具有直接的实践意义。自适应引导机制可能提升搜索结果的个性化相关性，对电商平台的用户体验和转化率有重要影响。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对电商搜索相关性预测这一核心问题，提出了结合强化学习和LLM的创新框架，完美契合搜索领域的技术前沿需求。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07885v1": {
    "title": "Generation and annotation of item usage scenarios in e-commerce using large language models",
    "url": "https://www.alphaxiv.org/abs/2510.07885v1",
    "arxiv_id": "2510.07885v1",
    "authors": "Madoka Hagiri, Kazushi Okamoto, Koki Karube, Kei Harada, Atsushi Shibata",
    "categories": "cs.IR",
    "pub_date": "2025-10-09 07:37:54",
    "ori_summary": "Complementary recommendations suggest combinations of useful items that play important roles in e-commerce. However, complementary relationships are often subjective and vary among individuals, making them difficult to infer from historical data. Unlike conventional history-based methods that rely on statistical co-occurrence, we focus on the underlying usage context that motivates item combinations. We hypothesized that people select complementary items by imagining specific usage scenarios and identifying the needs in such situations. Based on this idea, we explored the use of large language models (LLMs) to generate item usage scenarios as a starting point for constructing complementary recommendation systems. First, we evaluated the plausibility of LLM-generated scenarios through manual annotation. The results demonstrated that approximately 85% of the generated scenarios were determined to be plausible, suggesting that LLMs can effectively generate realistic item usage scenarios.",
    "summary": "研究电商中互补推荐的主观性和个性化难题，核心思想是利用LLM生成具体的物品使用场景，通过情境想象来理解用户组合物品的动机。",
    "translation": "使用大型语言模型生成和标注电子商务中的物品使用场景",
    "relevance_score": 8,
    "reasoning": "该论文直接应用LLM技术于电子商务领域，属于'Direct LLM Applications'范畴。通过生成和标注物品使用场景，可以显著增强推荐系统的上下文理解和个性化推荐能力，为搜索和推荐系统提供更丰富的语义信息。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用LLM技术生成电商场景下的物品使用情境，为核心推荐系统提供新的数据构建方法，与用户关注点高度契合。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07796v1": {
    "title": "HySim-LLM: Embedding-Weighted Fine-Tuning Bounds and Manifold Denoising for Domain-Adapted LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.07796v1",
    "arxiv_id": "2510.07796v1",
    "authors": "Majid Jaberi-Douraki, Hossein Sholehrasa, Xuan Xu, Remya Ampadi Ramachandran",
    "categories": "cs.LG, cs.IR",
    "pub_date": "2025-10-09 05:16:46",
    "ori_summary": "The extraction and standardization of pharmacokinetic (PK) information from scientific literature remain significant challenges in computational pharmacology, which limits the reliability of data-driven models in drug development. Large language models (LLMs) have achieved remarkable progress in text understanding and reasoning, yet their adaptation to structured biomedical data, such as PK tables, remains constrained by heterogeneity, noise, and domain shift. To address these limitations, we propose HySim-LLM, a unified mathematical and computational framework that integrates embedding-weighted fine-tuning and manifold-aware denoising to enhance the robustness and interpretability of LLMs. We establish two theoretical results: (1) a similarity-weighted generalization bound that quantifies adaptation performance under embedding divergence, and (2) a manifold-based denoising guarantee that bounds loss contributions from noisy or off-manifold samples. These theorems provide a principled foundation for fine-tuning LLMs in structured biomedical settings. The framework offers a mathematically grounded pathway toward reliable and interpretable LLM adaptation for biomedical and data-intensive scientific domains.",
    "summary": "该论文研究领域适应中LLM处理异构结构化数据的鲁棒性问题，核心思想是通过嵌入加权微调边界和流形去噪理论来提升模型在领域迁移中的泛化能力和可解释性。",
    "translation": "HySim-LLM：面向领域自适应大语言模型的嵌入加权微调边界与流形去噪",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM领域自适应和微调技术，属于'赋能LLM技术'范畴。嵌入加权微调和流形去噪方法可显著提升LLM在特定领域（如电商、搜索）的适应能力，直接应用于构建更精准的推荐系统和搜索引擎中的查询理解模块。",
    "rerank_relevance_score": 6,
    "rerank_reasoning": "该论文提出了嵌入加权微调和流形去噪的理论框架，虽然应用于生物医学领域，但其核心方法对推荐系统中处理异构数据和领域适应具有直接参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07784v1": {
    "title": "PLUM: Adapting Pre-trained Language Models for Industrial-scale Generative Recommendations",
    "url": "https://www.alphaxiv.org/abs/2510.07784v1",
    "arxiv_id": "2510.07784v1",
    "authors": "Ruining He, Lukasz Heldt, Lichan Hong, Raghunandan Keshavan, Shifan Mao, Nikhil Mehta, Zhengyang Su, Alicia Tsai, Yueqi Wang, Shao-Chuan Wang, Xinyang Yi, Lexi Baugher, Baykal Cakici, Ed Chi, Cristos Goodrow, Ningren Han, He Ma, Romer Rosales, Abby Van Soest, Devansh Tandon, Su-Lin Wu, Weilong Yang, Yilin Zheng",
    "categories": "cs.IR, cs.LG",
    "pub_date": "2025-10-09 05:01:05",
    "ori_summary": "Large Language Models (LLMs) pose a new paradigm of modeling and computation for information tasks. Recommendation systems are a critical application domain poised to benefit significantly from the sequence modeling capabilities and world knowledge inherent in these large models. In this paper, we introduce PLUM, a framework designed to adapt pre-trained LLMs for industry-scale recommendation tasks. PLUM consists of item tokenization using Semantic IDs, continued pre-training (CPT) on domain-specific data, and task-specific fine-tuning for recommendation objectives. For fine-tuning, we focus particularly on generative retrieval, where the model is directly trained to generate Semantic IDs of recommended items based on user context. We conduct comprehensive experiments on large-scale internal video recommendation datasets. Our results demonstrate that PLUM achieves substantial improvements for retrieval compared to a heavily-optimized production model built with large embedding tables. We also present a scaling study for the model's retrieval performance, our learnings about CPT, a few enhancements to Semantic IDs, along with an overview of the training and inference methods that enable launching this framework to billions of users in YouTube.",
    "summary": "研究如何将预训练语言模型适配到工业级推荐任务；核心方法是构建包含项目语义ID化、领域持续预训练和生成式检索微调的完整框架，使模型能直接生成推荐项目的语义ID。",
    "translation": "PLUM：为工业级生成式推荐系统适配预训练语言模型",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及LLM在推荐系统中的应用，属于'Direct LLM Applications'范畴。将预训练语言模型适配于工业级生成式推荐，展示了LLM技术在推荐领域的实际应用潜力，包括生成式推荐和个性化内容生成。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM在推荐系统中的应用，提出了完整的工业级生成式推荐框架，完全符合核心关注领域。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07728v1": {
    "title": "Who Stole Your Data? A Method for Detecting Unauthorized RAG Theft",
    "url": "https://www.alphaxiv.org/abs/2510.07728v1",
    "arxiv_id": "2510.07728v1",
    "authors": "Peiyang Liu, Ziqiang Cui, Di Liang, Wei Ye",
    "categories": "cs.IR, cs.CL",
    "pub_date": "2025-10-09 03:09:18",
    "ori_summary": "Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs) by mitigating hallucinations and outdated information issues, yet simultaneously facilitates unauthorized data appropriation at scale. This paper addresses this challenge through two key contributions. First, we introduce RPD, a novel dataset specifically designed for RAG plagiarism detection that encompasses diverse professional domains and writing styles, overcoming limitations in existing resources. Second, we develop a dual-layered watermarking system that embeds protection at both semantic and lexical levels, complemented by an interrogator-detective framework that employs statistical hypothesis testing on accumulated evidence. Extensive experimentation demonstrates our approach's effectiveness across varying query volumes, defense prompts, and retrieval parameters, while maintaining resilience against adversarial evasion techniques. This work establishes a foundational framework for intellectual property protection in retrieval-augmented AI systems.",
    "summary": "",
    "translation": "谁窃取了您的数据？一种检测未经授权的RAG窃取的方法",
    "relevance_score": 1,
    "reasoning": "该论文关注数据安全和未经授权访问的检测，这属于隐私和安全领域，被明确列为不相关主题。虽然提到了RAG（检索增强生成），但核心焦点是安全检测而非RAG在推荐系统或搜索中的应用。该研究没有提供在推荐、搜索或广告领域的潜在技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07720v1": {
    "title": "Queries Are Not Alone: Clustering Text Embeddings for Video Search",
    "url": "https://www.alphaxiv.org/abs/2510.07720v1",
    "arxiv_id": "2510.07720v1",
    "authors": "Peyang Liu, Xi Wang, Ziqiang Cui, Wei Ye",
    "categories": "cs.IR",
    "pub_date": "2025-10-09 02:56:18",
    "ori_summary": "The rapid proliferation of video content across various platforms has highlighted the urgent need for advanced video retrieval systems. Traditional methods, which primarily depend on directly matching textual queries with video metadata, often fail to bridge the semantic gap between text descriptions and the multifaceted nature of video content. This paper introduces a novel framework, the Video-Text Cluster (VTC), which enhances video retrieval by clustering text queries to capture a broader semantic scope. We propose a unique clustering mechanism that groups related queries, enabling our system to consider multiple interpretations and nuances of each query. This clustering is further refined by our innovative Sweeper module, which identifies and mitigates noise within these clusters. Additionally, we introduce the Video-Text Cluster-Attention (VTC-Att) mechanism, which dynamically adjusts focus within the clusters based on the video content, ensuring that the retrieval process emphasizes the most relevant textual features. Further experiments have demonstrated that our proposed model surpasses existing state-of-the-art models on five public datasets.",
    "summary": "",
    "translation": "查询并非孤立：基于文本嵌入聚类的视频搜索",
    "relevance_score": 7,
    "reasoning": "该论文聚焦于搜索领域，通过文本嵌入聚类技术改进视频搜索效果，这直接属于核心搜索领域的进展。虽然论文具体针对视频搜索，但其基于文本嵌入和聚类的方法可以推广到通用的搜索和推荐系统中，用于处理用户查询的多样性和语义理解。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07644v1": {
    "title": "ISMIE: A Framework to Characterize Information Seeking in Modern Information Environments",
    "url": "https://www.alphaxiv.org/abs/2510.07644v1",
    "arxiv_id": "2510.07644v1",
    "authors": "Shuoqi Sun, Danula Hettiachchi, Damiano Spina",
    "categories": "cs.IR",
    "pub_date": "2025-10-09 00:32:07",
    "ori_summary": "The modern information environment (MIE) is increasingly complex, shaped by a wide range of techniques designed to satisfy users' information needs. Information seeking (IS) models are effective mechanisms for characterizing user-system interactions. However, conceptualizing a model that fully captures the MIE landscape poses a challenge. We argue: Does such a model exist? To address this, we propose the Information Seeking in Modern Information Environments (ISMIE) framework as a fundamental step. ISMIE conceptualizes the information seeking process (ISP) via three key concepts: Components (e.g., Information Seeker), Intervening Variables (e.g., Interactive Variables), and Activities (e.g., Acquiring). Using ISMIE's concepts and employing a case study based on a common scenario - misinformation dissemination - we analyze six existing IS and information retrieval (IR) models to illustrate their limitations and the necessity of ISMIE. We then show how ISMIE serves as an actionable framework for both characterization and experimental design. We characterize three pressing issues and then outline two research blueprints: a user-centric, industry-driven experimental design for the authenticity and trust crisis to AI-generated content and a system-oriented, academic-driven design for tackling dopamine-driven content consumption. Our framework offers a foundation for developing IS and IR models to advance knowledge on understanding human interactions and system design in MIEs.",
    "summary": "",
    "translation": "ISMIE：一个用于表征现代信息环境中信息寻求行为的框架",
    "relevance_score": 3,
    "reasoning": "该论文标题聚焦于信息寻求行为表征框架，虽然与搜索系统有一定关联，但更偏向信息行为学和人机交互领域，而非核心推荐系统、搜索或广告的技术进展。该框架可能为理解用户搜索意图提供基础，但缺乏明确的LLM、Transformer或推荐系统技术应用的具体指向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08569v1": {
    "title": "ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.08569v1",
    "arxiv_id": "2510.08569v1",
    "authors": "Qin Liu, Jacob Dineen, Yuxi Huang, Sheng Zhang, Hoifung Poon, Ben Zhou, Muhao Chen",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-09 17:59:55",
    "ori_summary": "Benchmarks are central to measuring the capabilities of large language models and guiding model development, yet widespread data leakage from pretraining corpora undermines their validity. Models can match memorized content rather than demonstrate true generalization, which inflates scores, distorts cross-model comparisons, and misrepresents progress. We introduce ArenaBencher, a model-agnostic framework for automatic benchmark evolution that updates test cases while preserving comparability. Given an existing benchmark and a diverse pool of models to be evaluated, ArenaBencher infers the core ability of each test case, generates candidate question-answer pairs that preserve the original objective, verifies correctness and intent with an LLM as a judge, and aggregates feedback from multiple models to select candidates that expose shared weaknesses. The process runs iteratively with in-context demonstrations that steer generation toward more challenging and diagnostic cases. We apply ArenaBencher to math problem solving, commonsense reasoning, and safety domains and show that it produces verified, diverse, and fair updates that uncover new failure modes, increase difficulty while preserving test objective alignment, and improve model separability. The framework provides a scalable path to continuously evolve benchmarks in step with the rapid progress of foundation models.",
    "summary": "",
    "translation": "ArenaBencher：通过多模型竞争性评估实现自动基准演化",
    "relevance_score": 1,
    "reasoning": "该论文关注自动基准评估和演化，属于纯粹的评估基准研究，与我的关注点无关。论文标题表明其核心是基准测试方法学，没有涉及推荐系统、搜索或广告的核心进展，也没有LLM技术或Transformer架构的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08567v1": {
    "title": "MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.08567v1",
    "arxiv_id": "2510.08567v1",
    "authors": "Tajamul Ashraf, Umair Nawaz, Abdelrahman M. Shaker, Rao Anwer, Philip Torr, Fahad Shahbaz Khan, Salman Khan",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-10-09 17:59:54",
    "ori_summary": "Vision language models (VLMs) are increasingly deployed as controllers with access to external tools for complex reasoning and decision-making, yet their effectiveness remains limited by the scarcity of high-quality multimodal trajectories and the cost of manual annotation. We address this challenge with a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories, generates step-wise preference pairs, and trains a VLM controller for robust tool-use reasoning. Our pipeline first constructs M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified trajectories, enabling imitation-based trajectory tuning. Building on this, we develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool reasoning. To achieve finer alignment, we further introduce Pref-X, a set of 11K automatically generated preference pairs, and optimize MATRIX on it via step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA, MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating scalable and effective multimodal tool use. Our data and code is avaliable at https://github.com/mbzuai-oryx/MATRIX.",
    "summary": "",
    "translation": "MATRIX：面向鲁棒工具使用推理的多模态智能体调优",
    "relevance_score": 4,
    "reasoning": "该论文涉及多模态智能体和工具使用推理，属于LLM应用范畴。虽然多模态智能体在搜索和推荐系统中具有潜在应用价值（如处理异构数据和外部工具集成），但论文标题未明确表明与推荐系统、搜索或广告的直接关联，且工具使用推理可能更偏向通用AI助手应用而非核心排序任务。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08543v1": {
    "title": "VideoNorms: Benchmarking Cultural Awareness of Video Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08543v1",
    "arxiv_id": "2510.08543v1",
    "authors": "Nikhil Reddy Varimalla, Yunfei Xu, Arkadiy Saakyan, Meng Fan Wang, Smaranda Muresan",
    "categories": "cs.CV, cs.AI, cs.CL, cs.CY",
    "pub_date": "2025-10-09 17:54:55",
    "ori_summary": "As Video Large Language Models (VideoLLMs) are deployed globally, they require understanding of and grounding in the relevant cultural background. To properly assess these models' cultural awareness, adequate benchmarks are needed. We introduce VideoNorms, a benchmark of over 1000 (video clip, norm) pairs from US and Chinese cultures annotated with socio-cultural norms grounded in speech act theory, norm adherence and violations labels, and verbal and non-verbal evidence. To build VideoNorms, we use a human-AI collaboration framework, where a teacher model using theoretically-grounded prompting provides candidate annotations and a set of trained human experts validate and correct the annotations. We benchmark a variety of open-weight VideoLLMs on the new dataset which highlight several common trends: 1) models performs worse on norm violation than adherence; 2) models perform worse w.r.t Chinese culture compared to the US culture; 3) models have more difficulty in providing non-verbal evidence compared to verbal for the norm adhere/violation label and struggle to identify the exact norm corresponding to a speech-act; and 4) unlike humans, models perform worse in formal, non-humorous contexts. Our findings emphasize the need for culturally-grounded video language model training - a gap our benchmark and framework begin to address.",
    "summary": "",
    "translation": "VideoNorms：视频语言模型文化意识基准测试",
    "relevance_score": 2,
    "reasoning": "该论文聚焦于视频语言模型的文化意识基准测试，属于纯粹的评估基准研究范畴，与我的核心关注点无关。虽然涉及多模态模型，但主要关注文化评估这一特定NLP任务，缺乏在推荐系统、搜索或广告领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08531v1": {
    "title": "SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08531v1",
    "arxiv_id": "2510.08531v1",
    "authors": "Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-10-09 17:50:54",
    "ori_summary": "Spatial reasoning remains a fundamental challenge for Vision-Language Models (VLMs), with current approaches struggling to achieve robust performance despite recent advances. We identify that this limitation stems from a critical gap: existing methods attempt to learn spatial reasoning directly without establishing the hierarchical foundations of perception and understanding. To address this challenge, we present a comprehensive methodology for building spatial intelligence progressively. We introduce SpatialLadder-26k, a multimodal dataset containing 26,610 samples spanning object localization, single image, multi-view, and video spatial reasoning tasks, constructed through a standardized pipeline that ensures systematic coverage across modalities. Building on this dataset, we design a three-stage progressive training framework that (1) establishes spatial perception through object localization, (2) develops spatial understanding through multi-dimensional spatial tasks, and (3) strengthens complex reasoning via reinforcement learning with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter model that achieves state-of-the-art performance on spatial reasoning benchmarks, with 23.4% average improvement over the base model, surpassing GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains strong generalization with 7.2% improvement on out-of-domain benchmarks, demonstrating that progressive training from perception to reasoning is essential for robust spatial intelligence.",
    "summary": "",
    "translation": "SpatialLadder：视觉语言模型中空间推理的渐进式训练",
    "relevance_score": 2,
    "reasoning": "该论文专注于视觉语言模型中的空间推理能力提升，属于纯粹的视觉-语言多模态研究。虽然标题提及了渐进式训练方法，但核心应用场景是空间理解而非推荐系统、搜索或广告领域。这种空间推理技术缺乏明确的路径应用于异构数据处理或推荐/搜索场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08529v1": {
    "title": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards",
    "url": "https://www.alphaxiv.org/abs/2510.08529v1",
    "arxiv_id": "2510.08529v1",
    "authors": "Xiangyuan Xue, Yifan Zhou, Guibin Zhang, Zaibin Zhang, Yijiang Li, Chen Zhang, Zhenfei Yin, Philip Torr, Wanli Ouyang, Lei Bai",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 17:50:26",
    "ori_summary": "Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agent's policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as a novel and effective paradigm for self-evolution in LLM-based agents.",
    "summary": "",
    "translation": "CoMAS：通过交互奖励共同演化的多智能体系统",
    "relevance_score": 2,
    "reasoning": "该论文关注多智能体系统的共同演化机制，属于强化学习领域。虽然多智能体系统在理论上可能应用于推荐系统中的多智能体协同，但论文标题未明确显示与推荐、搜索或广告系统的直接关联，也未涉及LLM、Transformer架构或异构数据建模等核心技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08525v1": {
    "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression",
    "url": "https://www.alphaxiv.org/abs/2510.08525v1",
    "arxiv_id": "2510.08525v1",
    "authors": "Wenjie Du, Li Jiang, Keda Tao, Xue Liu, Huan Wang",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 17:50:00",
    "ori_summary": "Reasoning large language models exhibit complex reasoning behaviors through the extended chain-of-thought generation, creating unprecedented Key-Value (KV) cache overhead during the decoding phase. Existing KV cache compression methods underperform on reasoning models: token-dropping methods break reasoning integrity by discarding critical information, while head-reallocating methods mistakenly compress reasoning-critical heads since they are designed for retrieval tasks, resulting in significant performance degradation as compression rates increase. We hypothesize that KV heads exhibit functional heterogeneity in reasoning models-some heads are critical for chain-of-thought consistency while others are compressible. To validate and exploit this insight, we propose RLKV, a novel reasoning-critical head identification framework, which uses reinforcement learning to directly optimize the relationship between each head's cache usage and reasoning quality. As RLKV produces rewards from actual generated samples during training, it naturally identifies heads relevant to reasoning behaviors. We then allocate full KV cache to these heads while applying compressed constant KV cache to others for efficient inference. Our experiments reveal that only a small fraction of attention heads is essential for reasoning, enabling our KV compression approach to outperform baseline methods while achieving 20-50% cache reduction with near lossless performance compared to uncompressed results.",
    "summary": "研究推理大语言模型中KV缓存压缩的核心问题，提出基于强化学习识别推理关键注意力头的方法，仅对关键头保留完整缓存而对其他头进行压缩，实现高效推理。",
    "translation": "哪些注意力头对推理至关重要？基于强化学习的KV缓存压缩",
    "relevance_score": 8,
    "reasoning": "该论文关注Transformer架构中的KV缓存压缩，这属于'使能Transformer技术'范畴，直接涉及注意力机制效率优化。KV缓存压缩技术可显著降低推理延迟和内存占用，在推荐系统和搜索场景中对于处理长序列用户历史、大规模候选集排序具有重要应用价值，能够提升在线服务的实时性能。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM推理效率问题，提出基于强化学习的注意力头重要性识别方法，对推荐系统和搜索中的高效推理具有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08524v1": {
    "title": "Efficient Prompt Optimisation for Legal Text Classification with Proxy Prompt Evaluator",
    "url": "https://www.alphaxiv.org/abs/2510.08524v1",
    "arxiv_id": "2510.08524v1",
    "authors": "Hyunji Lee, Kevin Chenhao Li, Matthias Grabmair, Shanshan Xu",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 17:49:53",
    "ori_summary": "Prompt optimization aims to systematically refine prompts to enhance a language model's performance on specific tasks. Fairness detection in Terms of Service (ToS) clauses is a challenging legal NLP task that demands carefully crafted prompts to ensure reliable results. However, existing prompt optimization methods are often computationally expensive due to inefficient search strategies and costly prompt candidate scoring. In this paper, we propose a framework that combines Monte Carlo Tree Search (MCTS) with a proxy prompt evaluator to more effectively explore the prompt space while reducing evaluation costs. Experiments demonstrate that our approach achieves higher classification accuracy and efficiency than baseline methods under a constrained computation budget.",
    "summary": "",
    "translation": "基于代理提示评估器的高效法律文本分类提示优化",
    "relevance_score": 2,
    "reasoning": "该论文主要关注法律领域的文本分类提示优化，属于特定领域应用而非核心推荐系统、搜索或广告技术。虽然涉及提示优化技术，但缺乏与异构数据建模、Transformer架构改进或直接应用于推荐/搜索/广告系统的明确联系。代理评估器方法可能对效率优化有启发，但法律领域的特定性限制了其在目标领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08517v1": {
    "title": "CaRT: Teaching LLM Agents to Know When They Know Enough",
    "url": "https://www.alphaxiv.org/abs/2510.08517v1",
    "arxiv_id": "2510.08517v1",
    "authors": "Grace Liu, Yuxiao Qu, Jeff Schneider, Aarti Singh, Aviral Kumar",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-09 17:46:39",
    "ori_summary": "Many tasks require learned models to strategically gather relevant information over multiple rounds of interaction before actually acting on a task. Strategic information gathering requires models to know not only how to effectively acquire information, but also when to stop gathering information and make a decision, in order to avoid overthinking or getting derailed when acting. In this paper, we formalize this problem and introduce Counterfactuals and Reasoning for Termination (CaRT), an approach for teaching LLMs when to stop seeking information. To appropriately learn when to terminate, CaRT fine-tunes LLMs using counterfactual pairs of trajectories, one where termination is appropriate and a minimally modified version of the same trajectory where it is not. It trains the LLM to explain the rationale for the termination decision in either case via verbal reasoning, and imbues this capability into the base LLM via fine-tuning. We instantiate CaRT in two domains: interactive medical diagnosis and math problem solving. In both domains, we find that CaRT improves the efficiency of information gathering and task success rate compared to other fine-tuning methods.",
    "summary": "该论文研究LLM代理在多轮交互任务中如何确定停止信息收集的最佳时机，核心方法是使用反事实轨迹对和言语推理来训练LLM学习终止决策的理性判断能力。",
    "translation": "CaRT：教导LLM智能体知晓何时已掌握足够信息",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM智能体的知识边界判断能力，这属于核心LLM技术进步。在搜索和推荐系统中，这种能力可以显著提升用户体验 - 智能体能够判断何时已收集足够信息来提供准确推荐或搜索结果，避免不必要的交互循环，从而提高系统效率和用户满意度。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的CaRT方法直接解决LLM代理在信息收集过程中的终止决策问题，这与推荐和搜索系统中用户交互的多轮决策优化高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08513v1": {
    "title": "SliceFine: The Universal Winning-Slice Hypothesis for Pretrained Networks",
    "url": "https://www.alphaxiv.org/abs/2510.08513v1",
    "arxiv_id": "2510.08513v1",
    "authors": "Md Kowsher, Ali O. Polat, Ehsan Mohammady Ardehaly, Mehrdad Salehi, Zia Ghiasi, Prasanth Murali, Chen Chen",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-09 17:45:28",
    "ori_summary": "This paper presents a theoretical framework explaining why fine tuning small, randomly selected subnetworks (slices) within pre trained models can be sufficient for downstream adaptation. We prove that pretrained networks exhibit a universal winning slice property arising from two phenomena: (1) spectral balance the eigenspectra of different weight matrix slices are remarkably similar; and (2) high task energy their backbone representations retain rich, task relevant features. This leads to the Universal Winning Slice Hypothesis, which provides a theoretical foundation for parameter efficient fine tuning (PEFT) in large scale models. Inspired by this, we propose SliceFine, a PEFT method that exploits this inherent redundancy by updating only selected slices of the original weights introducing zero new parameters, unlike adapter-based approaches. Empirically, SliceFine matches the performance of state of the art PEFT methods across language and vision tasks, while significantly improving training speed, memory efficiency, and model compactness. Our work bridges theory and practice, offering a theoretically grounded alternative to existing PEFT techniques.",
    "summary": "",
    "translation": "SliceFine：预训练网络的通用获胜切片假设",
    "relevance_score": 3,
    "reasoning": "该论文似乎涉及预训练网络的切片分析或优化，可能属于Transformer架构效率或模型分析领域。如果该技术能够识别和优化推荐/搜索系统中最重要的数据切片或用户群体，可能对个性化推荐或广告定向有潜在应用价值，但仅从标题难以确定具体技术细节和应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08511v1": {
    "title": "AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents",
    "url": "https://www.alphaxiv.org/abs/2510.08511v1",
    "arxiv_id": "2510.08511v1",
    "authors": "Shangheng Du, Xiangchao Yan, Dengyang Jiang, Jiakang Yuan, Yusong Hu, Xin Li, Liang He, Bo Zhang, Lei Bai",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-09 17:45:05",
    "ori_summary": "Large language models (LLMs) have shown impressive performance in general programming tasks. However, in Machine Learning Engineering (MLE) scenarios such as AutoML and Kaggle competitions, achieving high performance depends heavily on expert intervention and repeated adjustments rather than simply generating correct code. When applied directly to these tasks, LLMs often lack fine-grained domain priors, and existing MLE approaches that use linear or tree-structured searches limit knowledge transfer to adjacent hierarchical links. As a result, they cannot leverage past full trajectories or share information across branches, limiting self-evolving ability and search space diversity. To address these limitations, we introduce AutoMLGen, an LLM-based coding agent that integrates a domain knowledge base for high-quality prior guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS retains the tree-guided exploration of MCTS while embedding a graph structure into the expansion stage to enable dynamic path reorganization, historical trajectory reuse, and multi-solution fusion to support both self-evolution and collaborative learning. Combined with fine-grained operator sets, this design improves stability and accelerates convergence. Evaluation on the MLE-Bench shows that AutoMLGen achieves state-of-the-art performance in numerous dimensions, such as the average medal rate and the valid submission rate, under a 12-hour budget (half the standard runtime). The code is available at https://github.com/Alpha-Innovator/InternAgent.",
    "summary": "",
    "translation": "AutoMLGen：为编码智能体导航细粒度优化",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及AutoML（自动机器学习）和编码智能体，这属于明确的无关主题。AutoML被明确列为无关主题，而编码智能体主要涉及代码生成和编程任务，与推荐系统、搜索或广告的核心技术没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08510v1": {
    "title": "To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08510v1",
    "arxiv_id": "2510.08510v1",
    "authors": "Jiayun Luo, Wan-Cyuan Fan, Lyuyang Wang, Xiangteng He, Tanzila Rahman, Purang Abolmaesumi, Leonid Sigal",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-10-09 17:44:42",
    "ori_summary": "Large Vision Language Models (LVLMs) have recently emerged as powerful architectures capable of understanding and reasoning over both visual and textual information. These models typically rely on two key components: a Vision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual content into a sequence of image tokens and serves as the perceptual front-end -- the eyes of the model. In contrast, the LLM interprets these tokens to perform high-level reasoning, generates responses, and functions as the cognitive core -- the brain of the model. However, it remains unclear which visual tokens contribute most significantly to understanding and reasoning, and how effectively these signals are propagated from ViT to the LLM. While most existing works have focused on identifying attention sinks, low-semantic tokens receiving disproportionately high attention, within the LLM, we shift the focus to the vision encoder by identifying a class of high-norm visual tokens from ViT, referred to as ViT attention sinks -- a problem that has been rarely studied but is indeed very important for LVLMs. Our findings show that these ViT sinks encapsulate high-level semantic concepts from images, allowing the LLM to perform more effective understanding and reasoning. Despite their importance, these sink tokens are often overlooked in existing LVLM architectures. To explore their contribution, we present both qualitative and quantitative analyses of the information embedded in these sink tokens. We also propose both training-free and training-based approaches to better leverage how this information is interpreted by the LLM, and to what extent. By explicitly utilizing these tokens, we demonstrate substantial improvements across a range of LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT attention sinks in enhancing visual reasoning.",
    "summary": "",
    "translation": "下沉与否：大型视觉语言模型中的视觉信息通路",
    "relevance_score": 3,
    "reasoning": "该论文研究视觉语言模型中的视觉信息处理机制，属于VLM技术范畴。虽然VLM架构对处理异构数据有启发意义，但论文标题聚焦于纯粹的视觉信息通路分析，缺乏明确的推荐系统、搜索或广告应用连接点，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08506v1": {
    "title": "Neologism Learning for Controllability and Self-Verbalization",
    "url": "https://www.alphaxiv.org/abs/2510.08506v1",
    "arxiv_id": "2510.08506v1",
    "authors": "John Hewitt, Oyvind Tafjord, Robert Geirhos, Been Kim",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 17:41:57",
    "ori_summary": "Humans invent new words when there is a rising demand for a new useful concept (e.g., doomscrolling). We explore and validate a similar idea in our communication with LLMs: introducing new words to better understand and control the models, expanding on the recently introduced neologism learning. This method introduces a new word by adding a new word embedding and training with examples that exhibit the concept with no other changes in model parameters. We show that adding a new word allows for control of concepts such as flattery, incorrect answers, text length, as well as more complex concepts in AxBench. We discover that neologisms can also further our understanding of the model via self-verbalization: models can describe what each new word means to them in natural language, like explaining that a word that represents a concept of incorrect answers means ``a lack of complete, coherent, or meaningful answers...'' To validate self-verbalizations, we introduce plug-in evaluation: we insert the verbalization into the context of a model and measure whether it controls the target concept. In some self-verbalizations, we find machine-only synonyms: words that seem unrelated to humans but cause similar behavior in machines. Finally, we show how neologism learning can jointly learn multiple concepts in multiple words.",
    "summary": "",
    "translation": "新词学习用于可控性和自我言语化",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM的新词学习和自我言语化能力，属于纯粹的NLP中心主题，与推荐系统、搜索或广告的核心技术进展没有直接关联。虽然可控性可能对内容生成有一定意义，但论文焦点不在推荐/搜索/广告领域的实际应用，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08483v1": {
    "title": "DeepPrune: Parallel Scaling without Inter-trace Redundancy",
    "url": "https://www.alphaxiv.org/abs/2510.08483v1",
    "arxiv_id": "2510.08483v1",
    "authors": "Shangqing Tu, Yaxuan Li, Yushi Bai, Lei Hou, Juanzi Li",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 17:24:54",
    "ori_summary": "Parallel scaling has emerged as a powerful paradigm to enhance reasoning capabilities in large language models (LLMs) by generating multiple Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces significant computational inefficiency due to inter-trace redundancy -- our analysis reveals that over 80% of parallel reasoning traces yield identical final answers, representing substantial wasted computation. To address this critical efficiency bottleneck, we propose DeepPrune, a novel framework that enables efficient parallel scaling through dynamic pruning. Our method features a specialized judge model trained with focal loss and oversampling techniques to accurately predict answer equivalence from partial reasoning traces which realizes 0.87 AUROC on equivalence prediction, combined with an online greedy clustering algorithm that dynamically prunes redundant paths while preserving answer diversity. Comprehensive evaluations across three challenging benchmarks (AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that DeepPrune achieves remarkable token reduction by over 80% compared to conventional consensus sampling on most cases, while maintaining competitive accuracy within 3 percentage points. Our work establishes a new standard for efficient parallel reasoning, making high-performance reasoning more efficient. Our code and data are here: https://deepprune.github.io/",
    "summary": "",
    "translation": "DeepPrune：消除轨迹间冗余的并行扩展方法",
    "relevance_score": 3,
    "reasoning": "该论文标题暗示了模型效率优化技术，可能涉及并行计算或模型压缩，这属于Transformer架构效率改进的范畴。然而，标题信息有限，无法明确其具体技术细节或与推荐系统、搜索、广告的直接应用潜力，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08482v1": {
    "title": "The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping",
    "url": "https://www.alphaxiv.org/abs/2510.08482v1",
    "arxiv_id": "2510.08482v1",
    "authors": "Onur Keleş, Aslı Özyürek, Gerardo Ortega, Kadir Gökgö, Esam Ghaleb",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-09 17:21:59",
    "ori_summary": "Iconicity, the resemblance between linguistic form and meaning, is pervasive in signed languages, offering a natural testbed for visual grounding. For vision-language models (VLMs), the challenge is to recover such essential mappings from dynamic human motion rather than static context. We introduce the \\textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts psycholinguistic measures to evaluate VLMs on three tasks: (i) phonological sign-form prediction (e.g., handshape, location), (ii) transparency (inferring meaning from visual form), and (iii) graded iconicity ratings. We assess $13$ state-of-the-art VLMs in zero- and few-shot settings on Sign Language of the Netherlands and compare them to human baselines. On \\textit{phonological form prediction}, VLMs recover some handshape and location detail but remain below human performance; on \\textit{transparency}, they are far from human baselines; and only top models correlate moderately with human \\textit{iconicity ratings}. Interestingly, \\textit{models with stronger phonological form prediction correlate better with human iconicity judgment}, indicating shared sensitivity to visually grounded structure. Our findings validate these diagnostic tasks and motivate human-centric signals and embodied learning methods for modelling iconicity and improving visual grounding in multimodal models.",
    "summary": "",
    "translation": "视觉象似性挑战：评估视觉语言模型在手语形式-意义映射上的表现",
    "relevance_score": 1,
    "reasoning": "该论文专注于手语视觉语言模型的评估，属于特定领域的视觉语言应用。虽然涉及视觉语言模型，但手语处理与推荐系统、搜索或广告的核心技术需求相距甚远，没有明显的技术迁移潜力。论文关注的是语言学的形式-意义映射问题，而非能应用于异构数据建模的通用VLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08470v1": {
    "title": "Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling",
    "url": "https://www.alphaxiv.org/abs/2510.08470v1",
    "arxiv_id": "2510.08470v1",
    "authors": "Bianca-Mihaela Ganescu, Suchir Salhan, Andrew Caines, Paula Buttery",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-09 17:10:36",
    "ori_summary": "Training vision-language models on cognitively-plausible amounts of data requires rethinking how models integrate multimodal information. Within the constraints of the Vision track for the BabyLM Challenge 2025, we propose a lightweight decoder-based architecture with (1) token-wise dynamic gating for adaptive fusion of linguistic and visual cues, (2) feature modulation and channel attention to maximise the utility of limited visual information and (3) auxiliary contrastive objectives for visual grounding. Evaluation on five benchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows competitive or superior performance to multimodal baselines. More notably, our dynamic gate discovers interpretable patterns without explicit supervision, favouring visual cues for content words and linguistic cues for function words. While we identify limitations in the Challenge constraints, such as the information bottleneck created by global image embeddings and training instability from the dataset split, our findings establish dynamic gating as a powerful tool for efficient multimodal learning, offering both interpretability and performance even under severe constraints.",
    "summary": "",
    "translation": "观察学习：面向低资源视觉语言建模的令牌级动态门控机制",
    "relevance_score": 7,
    "reasoning": "该论文提出的令牌级动态门控机制属于Transformer架构效率优化范畴，是Enabling Transformer Tech的典型代表。这种动态计算机制可应用于推荐系统中处理用户行为序列和上下文特征，通过智能分配计算资源来提升长序列建模效率，同时其低资源特性对大规模工业推荐系统具有重要价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08460v1": {
    "title": "LeWiDi-2025 at NLPerspectives: The Third Edition of the Learning with Disagreements Shared Task",
    "url": "https://www.alphaxiv.org/abs/2510.08460v1",
    "arxiv_id": "2510.08460v1",
    "authors": "Elisa Leonardelli, Silvia Casola, Siyao Peng, Giulia Rizzi, Valerio Basile, Elisabetta Fersini, Diego Frassinelli, Hyewon Jang, Maja Pavlovic, Barbara Plank, Massimo Poesio",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 17:04:28",
    "ori_summary": "Many researchers have reached the conclusion that AI models should be trained to be aware of the possibility of variation and disagreement in human judgments, and evaluated as per their ability to recognize such variation. The LEWIDI series of shared tasks on Learning With Disagreements was established to promote this approach to training and evaluating AI models, by making suitable datasets more accessible and by developing evaluation methods. The third edition of the task builds on this goal by extending the LEWIDI benchmark to four datasets spanning paraphrase identification, irony detection, sarcasm detection, and natural language inference, with labeling schemes that include not only categorical judgments as in previous editions, but ordinal judgments as well. Another novelty is that we adopt two complementary paradigms to evaluate disagreement-aware systems: the soft-label approach, in which models predict population-level distributions of judgments, and the perspectivist approach, in which models predict the interpretations of individual annotators. Crucially, we moved beyond standard metrics such as cross-entropy, and tested new evaluation metrics for the two paradigms. The task attracted diverse participation, and the results provide insights into the strengths and limitations of methods to modeling variation. Together, these contributions strengthen LEWIDI as a framework and provide new resources, benchmarks, and findings to support the development of disagreement-aware technologies.",
    "summary": "",
    "translation": "LeWiDi-2025 在 NLPerspectives：第三届“学习与分歧”共享任务的第三版",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向NLP领域的共享任务，专注于处理分歧的学习任务，这属于纯粹的NLP评估基准范畴。根据筛选标准，评估基准和纯粹NLP中心主题属于不相关领域，与推荐系统、搜索或广告的核心技术进展没有任何直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08457v1": {
    "title": "ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping",
    "url": "https://www.alphaxiv.org/abs/2510.08457v1",
    "arxiv_id": "2510.08457v1",
    "authors": "Shuang Chen, Yue Guo, Yimeng Ye, Shijue Huang, Wenbo Hu, Haoxi Li, Manyuan Zhang, Jiayu Chen, Song Guo, Nanyun Peng",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 17:03:28",
    "ori_summary": "Recent advances in multimodal large reasoning models (MLRMs) have substantially improved their ability to solve complex textual and visual tasks. However, these models tend to overthink on simple problems, producing unnecessarily lengthy reasoning traces, while under-exploring on challenging ones, leading to missed solutions. To address this imbalance, we propose ARES, a unified open-source framework for adaptive reasoning that dynamically allocates exploration effort based on task difficulty. Our approach is motivated by two key empirical findings: (i) while single-token entropy is noisy, high window-entropy (HWE) tokens (token-level entropies averaged under a sliding window) can reliably capture reasoning-critical moments; and (ii) reducing HWE usage benefits easy problems, while increasing it is essential for solving hard ones. Building on these insights, ARES introduces a two-stage training pipeline. In the Adaptive Cold-Start stage, we curate multimodal and textual data paired with reasoning traces of length proportional to problem difficulty, equipping the model with initial difficulty awareness. In the second stage, we develop Adaptive Entropy Policy Optimization (AEPO), which uses HWE tokens as exploration triggers to decide when to explore, and a hierarchical entropy reward with dynamic KL control to decide how much to explore. Extensive experiments demonstrate that ARES achieves superior performance and reasoning efficiency across diverse mathematical, logical, and multimodal benchmarks, while closing the gap to leading commercial systems under significantly lower inference costs.",
    "summary": "",
    "translation": "ARES：基于难度感知的令牌级熵整形的多模态自适应推理",
    "relevance_score": 7,
    "reasoning": "该论文提出难度感知的令牌级熵整形技术，属于Transformer架构效率优化领域，与'使能Transformer技术'焦点相关。这种自适应推理机制可应用于推荐系统和搜索中的多模态内容理解，通过动态调整计算复杂度来提高大规模部署的效率。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08439v1": {
    "title": "xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.08439v1",
    "arxiv_id": "2510.08439v1",
    "authors": "Cheng Qian, Zuxin Liu, Shirley Kokane, Akshara Prabhakar, Jielin Qiu, Haolin Chen, Zhiwei Liu, Heng Ji, Weiran Yao, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-09 16:52:01",
    "ori_summary": "Modern LLM deployments confront a widening cost-performance spectrum: premium models deliver strong reasoning but are expensive, while lightweight models are economical yet brittle on complex tasks. Static escalation rules and keyword heuristics under-utilize this spectrum and fail to adapt across task types. We present xRouter, a tool-calling-based routing system in which a learned router can either answer directly or invoke one or more external models. The router is trained end-to-end with reinforcement learning using an explicit, cost-aware reward that encodes cost-performance trade-offs, eliminating the need for hand-engineered routing rules. Our implementation encompasses the full reinforcement learning framework, including reward and cost accounting, as well as the deployment and evaluation pipelines. Across diverse benchmarks, xRouter achieves strong cost-performance trade-offs (e.g., substantial cost reductions at comparable task completion rates), and provides empirical insights into what reliably helps learned routing and what does not, ranging from model trainability to the difficulty of eliciting sophisticated orchestration behaviors in small open models. We hope these findings and our open implementation will serve as a practical substrate for advancing learned, cost-aware LLM orchestration.",
    "summary": "该论文研究如何优化LLM部署中的成本-性能权衡问题，核心方法是使用强化学习训练一个能够智能调用不同成本模型的动态路由系统，通过成本感知的奖励函数实现端到端的编排决策。",
    "translation": "xRouter：基于强化学习的训练成本感知大语言模型编排系统",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM编排系统的优化，属于'直接LLM应用'范畴，可用于提升推荐/搜索系统中多LLM部署的效率。强化学习用于成本感知的编排，在广告/推荐系统中可优化模型选择与资源分配，直接降低运营成本并提升系统性能。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM编排系统的成本效率问题，使用强化学习实现动态路由，完全符合直接LLM应用和核心推荐系统优化的研究方向。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08404v1": {
    "title": "Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT",
    "url": "https://www.alphaxiv.org/abs/2510.08404v1",
    "arxiv_id": "2510.08404v1",
    "authors": "Noor Ul Zain, Mohsin Raza, Ahsan Adeel",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-09 16:22:30",
    "ori_summary": "We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two heads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$ is the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2 (124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two epochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude greater training efficiency on 10M tokens, demonstrating highly sample efficient pretraining. Using the BabyLM challenge evaluation pipeline across complex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning performance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out of 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out of 7 metrics in both cases. These results suggest the need to rethink prevailing deep learning paradigms and associated scaling laws.",
    "summary": "论文研究如何通过更高效的Transformer架构解决传统模型计算复杂度高的问题，核心思想是提出单层Co⁴架构，仅需8M参数和线性计算复杂度就能实现与传统深层模型相当的性能。",
    "translation": "单层微型Co⁴模型性能超越GPT-2和GPT-BERT",
    "relevance_score": 8,
    "reasoning": "该论文涉及高效Transformer架构（Co⁴），属于'使能Transformer技术'范畴。这种高效架构可直接应用于推荐系统和搜索中的大规模部署，通过减少计算开销实现更快的推理速度，同时保持模型性能。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出了一种高效的Transformer架构Co⁴，具有线性复杂度O(N)，直接对应Transformer架构效率提升的核心研究方向，对推荐系统和搜索中的大规模序列处理具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08396v1": {
    "title": "FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts",
    "url": "https://www.alphaxiv.org/abs/2510.08396v1",
    "arxiv_id": "2510.08396v1",
    "authors": "Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-09 16:17:13",
    "ori_summary": "Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains -- general knowledge understanding, scientific question answering, mathematical reasoning, and code generation -- demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.",
    "summary": "研究LoRA在多任务微调中的参数干扰问题，核心思想是设计基于飞蛾嗅觉回路的隐式MoE架构，通过秩级专家激活和随机矩阵投影统一专家路由与降维投影，消除显式路由器并利用随机矩阵正交性减轻任务间干扰。",
    "translation": "FlyLoRA：通过隐式秩级专家混合提升任务解耦与参数效率",
    "relevance_score": 8,
    "reasoning": "该论文专注于LoRA（低秩适应）的改进，属于Transformer架构效率提升的核心技术。FlyLoRA通过专家混合机制增强任务解耦能力和参数效率，可直接应用于推荐/搜索系统中的多任务学习和模型适配，为大规模部署提供更高效的微调方案。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文在Transformer架构效率方面提出创新方法，通过隐式MoE和随机矩阵正交性解决LoRA参数干扰问题，对推荐系统多任务学习具有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08388v1": {
    "title": "If Probable, Then Acceptable? Understanding Conditional Acceptability Judgments in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08388v1",
    "arxiv_id": "2510.08388v1",
    "authors": "Jasmin Orth, Philipp Mondorf, Barbara Plank",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 16:12:10",
    "ori_summary": "Conditional acceptability refers to how plausible a conditional statement is perceived to be. It plays an important role in communication and reasoning, as it influences how individuals interpret implications, assess arguments, and make decisions based on hypothetical scenarios. When humans evaluate how acceptable a conditional \"If A, then B\" is, their judgments are influenced by two main factors: the $\\textit{conditional probability}$ of $B$ given $A$, and the $\\textit{semantic relevance}$ of the antecedent $A$ given the consequent $B$ (i.e., whether $A$ meaningfully supports $B$). While prior work has examined how large language models (LLMs) draw inferences about conditional statements, it remains unclear how these models judge the $\\textit{acceptability}$ of such statements. To address this gap, we present a comprehensive study of LLMs' conditional acceptability judgments across different model families, sizes, and prompting strategies. Using linear mixed-effects models and ANOVA tests, we find that models are sensitive to both conditional probability and semantic relevance-though to varying degrees depending on architecture and prompting style. A comparison with human data reveals that while LLMs incorporate probabilistic and semantic cues, they do so less consistently than humans. Notably, larger models do not necessarily align more closely with human judgments.",
    "summary": "",
    "translation": "若可能，则可接受？理解大型语言模型中的条件可接受性判断",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLM的条件可接受性判断，这属于模型行为分析和评估范畴，与您的核心关注点（推荐系统、搜索广告领域的核心进展、使能技术及应用）相关性较弱。虽然涉及LLM内部机制，但更偏向NLP评估和模型行为理解，而非您关注的使能技术或直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08372v1": {
    "title": "On the Relationship Between the Choice of Representation and In-Context Learning",
    "url": "https://www.alphaxiv.org/abs/2510.08372v1",
    "arxiv_id": "2510.08372v1",
    "authors": "Ioana Marinescu, Kyunghyun Cho, Eric Karl Oermann",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-09 15:55:28",
    "ori_summary": "In-context learning (ICL) is the ability of a large language model (LLM) to learn a new task from a few demonstrations presented as part of the context. Past studies have attributed a large portion of the success of ICL to the way these in-context demonstrations are represented, particularly to how labels are represented in classification tasks. On the other hand, observations of the learning capacity of ICL (i.e., the extent to which more in-context demonstrations can lead to higher performance) have been mixed, and ICL is often thought to occur only under specific conditions. The interaction between these two aspects in ICL, representation and learning, has not been studied in depth until now. We hypothesize that they are largely independent of one another, such that the representation of demonstrations determines the baseline accuracy of ICL, while learning from additional demonstrations improves only on top of this baseline. We validate this hypothesis by developing an optimization algorithm that can enumerate a spectrum of possible label sets (representations) varying in semantic relevance. We then perform ICL with varying numbers of in-context demonstrations for each of these label sets. We observed that learning happens regardless of the quality of the label set itself, although its efficiency, measured by the slope of improvement over in-context demonstrations, is conditioned on both the label set quality and the parameter count of the underlying language model. Despite the emergence of learning, the relative quality (accuracy) of the choice of a label set (representation) is largely maintained throughout learning, confirming our hypothesis and implying their orthogonality. Our work reveals a previously underexplored aspect of ICL: the independent effects of learning from demonstrations and their representations on ICL performance.",
    "summary": "该论文研究ICL中演示表示与学习能力之间的关系问题，核心发现是表示质量决定ICL基线性能而学习能力独立存在，两者具有正交性。",
    "translation": "关于表示选择与上下文学习之间关系的研究",
    "relevance_score": 8,
    "reasoning": "该论文探讨表示选择与上下文学习的关系，这属于核心LLM技术进展（Enabling LLM Tech）。上下文学习能力直接影响LLM在推荐和搜索中的少样本适应性和个性化性能，对构建更高效的推荐和搜索系统具有重要应用价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文深入研究了ICL中表示与学习能力的关系，这对理解LLM在推荐搜索中的上下文学习机制至关重要。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08365v1": {
    "title": "Two-Stage Voting for Robust and Efficient Suicide Risk Detection on Social Media",
    "url": "https://www.alphaxiv.org/abs/2510.08365v1",
    "arxiv_id": "2510.08365v1",
    "authors": "Yukai Song, Pengfei Zhou, César Escobar-Viera, Candice Biernesser, Wei Huang, Jingtong Hu",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 15:51:05",
    "ori_summary": "Suicide rates have risen worldwide in recent years, underscoring the urgent need for proactive prevention strategies. Social media provides valuable signals, as many at-risk individuals - who often avoid formal help due to stigma - choose instead to share their distress online. Yet detecting implicit suicidal ideation, conveyed indirectly through metaphor, sarcasm, or subtle emotional cues, remains highly challenging. Lightweight models like BERT handle explicit signals but fail on subtle implicit ones, while large language models (LLMs) capture nuance at prohibitive computational cost. To address this gap, we propose a two-stage voting architecture that balances efficiency and robustness. In Stage 1, a lightweight BERT classifier rapidly resolves high-confidence explicit cases. In Stage 2, ambiguous inputs are escalated to either (i) a multi-perspective LLM voting framework to maximize recall on implicit ideation, or (ii) a feature-based ML ensemble guided by psychologically grounded indicators extracted via prompt-engineered LLMs for efficiency and interpretability. To the best of our knowledge, this is among the first works to operationalize LLM-extracted psychological features as structured vectors for suicide risk detection. On two complementary datasets - explicit-dominant Reddit and implicit-only DeepSuiMind - our framework outperforms single-model baselines, achieving 98.0% F1 on explicit cases, 99.7% on implicit ones, and reducing the cross-domain gap below 2%, while significantly lowering LLM cost.",
    "summary": "",
    "translation": "基于社交媒体的稳健高效自杀风险检测的两阶段投票方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于社交媒体上的自杀风险检测，这属于心理健康应用领域，与推荐系统、搜索或广告的核心技术无关。虽然可能涉及文本分析，但论文的焦点是医疗健康检测而非商业应用场景，完全超出了指定的关注范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08329v1": {
    "title": "AutoRed: A Free-form Adversarial Prompt Generation Framework for Automated Red Teaming",
    "url": "https://www.alphaxiv.org/abs/2510.08329v1",
    "arxiv_id": "2510.08329v1",
    "authors": "Muxi Diao, Yutao Mou, Keqing He, Hanbo Song, Lulu Zhao, Shikun Zhang, Wei Ye, Kongming Liang, Zhanyu Ma",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 15:17:28",
    "ori_summary": "The safety of Large Language Models (LLMs) is crucial for the development of trustworthy AI applications. Existing red teaming methods often rely on seed instructions, which limits the semantic diversity of the synthesized adversarial prompts. We propose AutoRed, a free-form adversarial prompt generation framework that removes the need for seed instructions. AutoRed operates in two stages: (1) persona-guided adversarial instruction generation, and (2) a reflection loop to iteratively refine low-quality prompts. To improve efficiency, we introduce a verifier to assess prompt harmfulness without querying the target models. Using AutoRed, we build two red teaming datasets -- AutoRed-Medium and AutoRed-Hard -- and evaluate eight state-of-the-art LLMs. AutoRed achieves higher attack success rates and better generalization than existing baselines. Our results highlight the limitations of seed-based approaches and demonstrate the potential of free-form red teaming for LLM safety evaluation. We will open source our datasets in the near future.",
    "summary": "",
    "translation": "AutoRed：一种用于自动化红队测试的自由形式对抗性提示生成框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注对抗性提示生成和红队测试，这属于LLM安全评估领域，与推荐系统、搜索或广告的核心技术进展关联较弱。虽然红队测试可能间接应用于评估推荐系统的鲁棒性，但论文焦点更偏向安全测试而非核心推荐/搜索算法改进，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08325v1": {
    "title": "Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries",
    "url": "https://www.alphaxiv.org/abs/2510.08325v1",
    "arxiv_id": "2510.08325v1",
    "authors": "Marius Dragoi, Ioana Pintilie, Florin Gogianu, Florin Brad",
    "categories": "cs.AI, cs.CL, cs.LG, I.2.6; I.2.7",
    "pub_date": "2025-10-09 15:14:58",
    "ori_summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm to improve Large Language Models on reasoning tasks such as coding, math or logic. To assess the reasoning boundary (the fraction of problems a model can solve) researchers often report Pass@k at large sampling budgets. Recent results reveal a crossover phenomenon: while RLVR models outperform the base model at small k values, the base model usually outperforms them when sampling a very large number of completions. This has been interpreted as evidence that base models have a larger reasoning boundary. We argue that on tasks with discrete answer spaces, such as math with numeric outputs, Pass@k at large k reflects the increasingly higher chance of success in the limit of the number of trials rather than genuine reasoning, and can therefore be misleading. We propose Cover@tau, which measures the fraction of problems that a model can solve for which at least a tau proportion of completions are correct. Unlike Pass@k, Cover@tau captures reasoning under an explicit reliability threshold: models that rely on random guessing degrade rapidly as tau increases. We evaluate several RLVR models using Cover@tau-based metrics and illustrate how the relative rankings of popular algorithms change compared to Pass@1, offering a different perspective on reasoning boundaries.",
    "summary": "",
    "translation": "超越Pass@k：用于推理边界的广度-深度度量",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM推理能力的评估指标，属于纯粹的NLP评估基准研究。虽然LLM推理能力可能间接影响推荐或搜索系统的性能，但论文本身没有明确涉及推荐系统、搜索或广告的直接应用，也不属于核心架构或使能技术进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08284v1": {
    "title": "Neuron-Level Analysis of Cultural Understanding in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08284v1",
    "arxiv_id": "2510.08284v1",
    "authors": "Taisei Yamamoto, Ryoma Kumon, Danushka Bollegala, Hitomi Yanaka",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 14:35:00",
    "ori_summary": "As large language models (LLMs) are increasingly deployed worldwide, ensuring their fair and comprehensive cultural understanding is important. However, LLMs exhibit cultural bias and limited awareness of underrepresented cultures, while the mechanisms underlying their cultural understanding remain underexplored. To fill this gap, we conduct a neuron-level analysis to identify neurons that drive cultural behavior, introducing a gradient-based scoring method with additional filtering for precise refinement. We identify both culture-general neurons contributing to cultural understanding regardless of cultures, and culture-specific neurons tied to an individual culture. These neurons account for less than 1% of all neurons and are concentrated in shallow to middle MLP layers. We validate their role by showing that suppressing them substantially degrades performance on cultural benchmarks (by up to 30%), while performance on general natural language understanding (NLU) benchmarks remains largely unaffected. Moreover, we show that culture-specific neurons support knowledge of not only the target culture, but also related cultures. Finally, we demonstrate that training on NLU benchmarks can diminish models' cultural understanding when we update modules containing many culture-general neurons. These findings provide insights into the internal mechanisms of LLMs and offer practical guidance for model training and engineering. Our code is available at https://github.com/ynklab/CULNIG",
    "summary": "",
    "translation": "大语言模型中文化理解的神经元级分析",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM内部机制分析和文化理解这一特定NLP能力，属于纯粹的LLM内部机制研究。虽然涉及LLM技术，但缺乏明确的推荐系统、搜索或广告应用场景，更偏向于模型可解释性和NLP能力分析，而非能够直接应用于业务场景的使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08276v1": {
    "title": "Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window",
    "url": "https://www.alphaxiv.org/abs/2510.08276v1",
    "arxiv_id": "2510.08276v1",
    "authors": "Qiaoyu Tang, Hao Xiang, Le Yu, Bowen Yu, Yaojie Lu, Xianpei Han, Le Sun, WenJuan Zhang, Pengbo Wang, Shixuan Liu, Zhenru Zhang, Jianhong Tu, Hongyu Lin, Junyang Lin",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 14:31:39",
    "ori_summary": "While recent advances in reasoning models have demonstrated cognitive behaviors through reinforcement learning, existing approaches struggle to invoke deep reasoning capabilities in multi-turn agents with long-horizon interactions. We propose DeepMiner, a novel framework that elicits such abilities by introducing high-difficulty training tasks and dynamic context window. DeepMiner presents a reverse construction method to generate complex but verifiable question-answer pairs from authentic web sources, which ensures the challenge and reliability of training data while injecting cognitive capabilities into multi-turn reasoning scenarios. We further design an elegant yet effective dynamic context management strategy for both training and inference, utilizing sliding window mechanisms while eliminating the dependency on external summarization models, thereby efficiently empowering the model to handle continuously expanding long-horizon contexts. Through reinforcement learning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial performance improvements across multiple search agent benchmarks. DeepMiner attains 33.5% accuracy on BrowseComp-en, surpassing the previous best open-source agent by almost 20 percentage points, and demonstrates consistent improvements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our dynamic context management enables sustained interactions of nearly 100 turns within standard 32k context length, effectively addressing the context limitations that constrain existing multi-turn interaction systems.",
    "summary": "论文研究多轮交互智能体中长序列推理能力受限的问题，核心方法是引入动态上下文管理策略，通过滑动窗口机制处理持续扩展的长序列上下文，无需依赖外部摘要模型。",
    "translation": "超越轮次限制：使用动态上下文窗口训练深度搜索智能体",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及搜索领域的核心进展，专注于训练深度搜索智能体，这属于Core Domain Advances范畴。动态上下文窗口技术作为Enabling Transformer Tech，能够显著提升搜索系统处理长序列用户行为和历史交互的能力，对于构建更智能的搜索推荐系统具有直接应用价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出动态上下文窗口管理方法解决多轮交互中的长序列处理问题，直接关联推荐系统和搜索领域的序列建模与上下文管理需求。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08256v1": {
    "title": "Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.08256v1",
    "arxiv_id": "2510.08256v1",
    "authors": "Jason Bohne, Pawel Polak, David Rosenberg, Brian Bloniarz, Gary Kazantsev",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-09 14:15:14",
    "ori_summary": "Direct Preference Optimization (DPO) has recently emerged as a simple and effective alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with user preferences. However, existing DPO formulations rely on a single monolithic model, which limits their expressivity in multi-task settings and their adaptability to heterogeneous or diverse preference distributions. In this work, we propose Mix- and MoE-DPO, a framework that extends DPO with both soft mixture models and mixture-of-experts (MoE) architectures, using a stochastic variational inference approach. Our method introduces a latent-variable model over expert assignments and optimizes a variational evidence lower bound (ELBO), enabling stable and efficient learning of specialized expert policies from preference data. Mix- and MoE-DPO provides three key advantages over standard DPO: (i) generalization via universal function approximation through mixtures; (ii) reward and policy specialization through expert components tailored to distinct preference modes; and (iii) contextual alignment through input-dependent soft gating that enables user-specific mixture policies. Our framework supports both shared base architectures with expert-specific policy heads and fully independent expert models, allowing flexible trade-offs between parameter efficiency and specialization. We validate our approach on a variety of model sizes and multi-preference datasets, demonstrating that Mix- and MoE-DPO offers a powerful and scalable method for preference-based LLM alignment.",
    "summary": "论文研究标准DPO方法在异构偏好分布和多任务场景下的表达能力限制问题，核心思想是通过变分推理框架将软混合模型和MoE架构引入DPO，实现专家策略的专业化和上下文相关的混合策略。",
    "translation": "混合与专家直接偏好优化：一种基于变分推断的直接偏好优化方法",
    "relevance_score": 8,
    "reasoning": "该论文涉及MoE（专家混合）架构和DPO（直接偏好优化），属于Transformer架构效率和新注意力机制方面的进展。MoE技术可以显著提升推荐系统和搜索中的模型容量和效率，而偏好优化方法可以直接应用于个性化推荐和搜索结果排序的优化。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文将MoE架构与DPO对齐方法结合，直接针对Transformer效率和多任务学习优化，在LLM对齐和专家混合技术方面都有核心贡献。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08255v1": {
    "title": "Opponent Shaping in LLM Agents",
    "url": "https://www.alphaxiv.org/abs/2510.08255v1",
    "arxiv_id": "2510.08255v1",
    "authors": "Marta Emili Garcia Segura, Stephen Hailes, Mirco Musolesi",
    "categories": "cs.LG, cs.AI, cs.CL, cs.MA",
    "pub_date": "2025-10-09 14:13:24",
    "ori_summary": "Large Language Models (LLMs) are increasingly being deployed as autonomous agents in real-world environments. As these deployments scale, multi-agent interactions become inevitable, making it essential to understand strategic behavior in such systems. A central open question is whether LLM agents, like reinforcement learning agents, can shape the learning dynamics and influence the behavior of others through interaction alone. In this paper, we present the first investigation of opponent shaping (OS) with LLM-based agents. Existing OS algorithms cannot be directly applied to LLMs, as they require higher-order derivatives, face scalability constraints, or depend on architectural components that are absent in transformers. To address this gap, we introduce ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based agents. Using ShapeLLM, we examine whether LLM agents can influence co-players' learning dynamics across diverse game-theoretic environments. We demonstrate that LLM agents can successfully guide opponents toward exploitable equilibria in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and Chicken) and promote coordination and improve collective welfare in cooperative games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma). Our findings show that LLM agents can both shape and be shaped through interaction, establishing opponent shaping as a key dimension of multi-agent LLM research.",
    "summary": "",
    "translation": "LLM智能体中的对手塑造",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM智能体在多智能体环境中的对手建模和策略塑造，属于多智能体强化学习范畴。虽然涉及LLM技术，但缺乏与推荐系统、搜索或广告领域的直接关联，且对手塑造概念在这些商业应用场景中并不常见。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08245v1": {
    "title": "Contrastive Decoding for Synthetic Data Generation in Low-Resource Language Modeling",
    "url": "https://www.alphaxiv.org/abs/2510.08245v1",
    "arxiv_id": "2510.08245v1",
    "authors": "Jannek Ulm, Kevin Du, Vésteinn Snæbjarnarson",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-09 14:04:52",
    "ori_summary": "Large language models (LLMs) are trained on huge amounts of textual data, and concerns have been raised that the limits of such data may soon be reached. A potential solution is to train on synthetic data sampled from LLMs. In this work, we build on this idea and investigate the benefits of contrastive decoding for generating synthetic corpora. In a controlled setting, we experiment with sampling corpora using the relative difference between a good and bad model trained on the same original corpus of 100 million words. By amplifying the signal from a model that has better performance, we create a synthetic corpus and mix it with the original training data. Our findings show that training on a mixture of synthesized and real data improves performance on the language modeling objective and a range of downstream tasks. In particular, we see that training with a mix of synthetic data from contrastive decoding benefits tasks that require more reasoning skills, while synthetic data from traditional sampling helps more on tasks dependent on surface level linguistic capabilities.",
    "summary": "",
    "translation": "低资源语言建模中用于合成数据生成的对比解码方法",
    "relevance_score": 4,
    "reasoning": "该论文涉及合成数据生成和对比解码技术，这些属于LLM核心技术进展，可能应用于推荐系统或搜索中的冷启动问题或数据增强。然而，论文明确聚焦于低资源语言建模这一特定领域，与主流RecSys/Search/Ads应用场景的直接关联性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08240v1": {
    "title": "The Alignment Waltz: Jointly Training Agents to Collaborate for Safety",
    "url": "https://www.alphaxiv.org/abs/2510.08240v1",
    "arxiv_id": "2510.08240v1",
    "authors": "Jingyu Zhang, Haozhu Wang, Eric Michael Smith, Sid Wang, Amr Sharaf, Mahesh Pasupuleti, Benjamin Van Durme, Daniel Khashabi, Jason Weston, Hongyuan Zhan",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 14:03:05",
    "ori_summary": "Harnessing the power of LLMs requires a delicate dance between being helpful and harmless. This creates a fundamental tension between two competing challenges: vulnerability to adversarial attacks that elicit unsafe content, and a tendency for overrefusal on benign but sensitive prompts. Current approaches often navigate this dance with safeguard models that completely reject any content that contains unsafe portions. This approach cuts the music entirely-it may exacerbate overrefusals and fails to provide nuanced guidance for queries it refuses. To teach models a more coordinated choreography, we propose WaltzRL, a novel multi-agent reinforcement learning framework that formulates safety alignment as a collaborative, positive-sum game. WaltzRL jointly trains a conversation agent and a feedback agent, where the latter is incentivized to provide useful suggestions that improve the safety and helpfulness of the conversation agent's responses. At the core of WaltzRL is a Dynamic Improvement Reward (DIR) that evolves over time based on how well the conversation agent incorporates the feedback. At inference time, unsafe or overrefusing responses from the conversation agent are improved rather than discarded. The feedback agent is deployed together with the conversation agent and only engages adaptively when needed, preserving helpfulness and low latency on safe queries. Our experiments, conducted across five diverse datasets, demonstrate that WaltzRL significantly reduces both unsafe responses (e.g., from 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on OR-Bench) compared to various baselines. By enabling the conversation and feedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances LLM safety without degrading general capabilities, thereby advancing the Pareto front between helpfulness and harmlessness.",
    "summary": "",
    "translation": "对齐华尔兹：联合训练智能体以实现安全协作",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于多智能体协作中的安全对齐问题，属于强化学习安全领域。虽然涉及多智能体训练，但论文主题明确围绕安全协作，与搜索、推荐、广告等核心领域没有直接关联，也不涉及LLM技术、Transformer架构或异构数据建模等关注方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08224v1": {
    "title": "Investigating Counterclaims in Causality Extraction from Text",
    "url": "https://www.alphaxiv.org/abs/2510.08224v1",
    "arxiv_id": "2510.08224v1",
    "authors": "Tim Hagen, Niklas Deckers, Felix Wolter, Harrisen Scells, Martin Potthast",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-09 13:45:54",
    "ori_summary": "Research on causality extraction from text has so far almost entirely neglected counterclaims. Existing causality extraction datasets focus solely on \"procausal\" claims, i.e., statements that support a relationship. \"Concausal\" claims, i.e., statements that refute a relationship, are entirely ignored or even accidentally annotated as procausal. We address this shortcoming by developing a new dataset that integrates concausality. Based on an extensive literature review, we first show that concausality is an integral part of causal reasoning on incomplete knowledge. We operationalize this theory in the form of a rigorous guideline for annotation and then augment the Causal News Corpus with concausal statements, obtaining a substantial inter-annotator agreement of Cohen's $\\kappa=0.74$. To demonstrate the importance of integrating concausal statements, we show that models trained without concausal relationships tend to misclassify these as procausal instead. Based on our new dataset, this mistake can be mitigated, enabling transformers to effectively distinguish pro- and concausality.",
    "summary": "",
    "translation": "探究文本因果提取中的反主张研究",
    "relevance_score": 2,
    "reasoning": "该论文专注于文本中的因果提取和反主张分析，这属于通用NLP信息抽取任务，与推荐系统、搜索或广告的核心技术没有直接关联。虽然因果推理在理论上可能对某些推荐场景有启发，但论文本身没有展示明确的RecSys/Search/Ads应用潜力，且更偏向纯粹的NLP研究范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08214v1": {
    "title": "SenWave: A Fine-Grained Multi-Language Sentiment Analysis Dataset Sourced from COVID-19 Tweets",
    "url": "https://www.alphaxiv.org/abs/2510.08214v1",
    "arxiv_id": "2510.08214v1",
    "authors": "Qiang Yang, Xiuying Chen, Changsheng Ma, Rui Yin, Xin Gao, Xiangliang Zhang",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 13:38:05",
    "ori_summary": "The global impact of the COVID-19 pandemic has highlighted the need for a comprehensive understanding of public sentiment and reactions. Despite the availability of numerous public datasets on COVID-19, some reaching volumes of up to 100 billion data points, challenges persist regarding the availability of labeled data and the presence of coarse-grained or inappropriate sentiment labels. In this paper, we introduce SenWave, a novel fine-grained multi-language sentiment analysis dataset specifically designed for analyzing COVID-19 tweets, featuring ten sentiment categories across five languages. The dataset comprises 10,000 annotated tweets each in English and Arabic, along with 30,000 translated tweets in Spanish, French, and Italian, derived from English tweets. Additionally, it includes over 105 million unlabeled tweets collected during various COVID-19 waves. To enable accurate fine-grained sentiment classification, we fine-tuned pre-trained transformer-based language models using the labeled tweets. Our study provides an in-depth analysis of the evolving emotional landscape across languages, countries, and topics, revealing significant insights over time. Furthermore, we assess the compatibility of our dataset with ChatGPT, demonstrating its robustness and versatility in various applications. Our dataset and accompanying code are publicly accessible on the repository\\footnote{https://github.com/gitdevqiang/SenWave}. We anticipate that this work will foster further exploration into fine-grained sentiment analysis for complex events within the NLP community, promoting more nuanced understanding and research innovations.",
    "summary": "",
    "translation": "SenWave：一个基于COVID-19推文的细粒度多语言情感分析数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于特定领域（COVID-19）的情感分析数据集构建，属于纯粹的NLP数据资源工作。虽然情感分析在理论上可能用于推荐系统或广告的用户反馈分析，但该论文的领域特定性和数据集性质使其与当前关注的推荐系统、搜索广告核心算法进展、Transformer架构改进或LLM直接应用等焦点领域几乎没有关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08211v1": {
    "title": "LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions",
    "url": "https://www.alphaxiv.org/abs/2510.08211v1",
    "arxiv_id": "2510.08211v1",
    "authors": "XuHao Hu, Peng Wang, Xiaoya Lu, Dongrui Liu, Xuanjing Huang, Jing Shao",
    "categories": "cs.CL, cs.AI, cs.CR",
    "pub_date": "2025-10-09 13:35:19",
    "ori_summary": "Previous research has shown that LLMs finetuned on malicious or incorrect completions within narrow domains (e.g., insecure code or incorrect medical advice) can become broadly misaligned to exhibit harmful behaviors, which is called emergent misalignment. In this work, we investigate whether this phenomenon can extend beyond safety behaviors to a broader spectrum of dishonesty and deception under high-stakes scenarios (e.g., lying under pressure and deceptive behavior). To explore this, we finetune open-sourced LLMs on misaligned completions across diverse domains. Experimental results demonstrate that LLMs show broadly misaligned behavior in dishonesty. Additionally, we further explore this phenomenon in a downstream combined finetuning setting, and find that introducing as little as 1% of misalignment data into a standard downstream task is sufficient to decrease honest behavior over 20%. Furthermore, we consider a more practical human-AI interaction environment where we simulate both benign and biased users to interact with the assistant LLM. Notably, we find that the assistant can be misaligned unintentionally to exacerbate its dishonesty with only 10% biased user population. In summary, we extend the study of emergent misalignment to the domain of dishonesty and deception under high-stakes scenarios, and demonstrate that this risk arises not only through direct finetuning, but also in downstream mixture tasks and practical human-AI interactions.",
    "summary": "",
    "translation": "大语言模型无意中学会欺骗：从错位样本到有偏见的人机交互中出现的诚实度错位",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLMs的欺骗行为和错位问题，这属于模型安全性和对齐范畴，而非推荐系统、搜索或广告的核心技术进展。虽然涉及人机交互，但焦点是伦理和安全性问题，这些被明确列为不相关主题。该研究没有展示在推荐、搜索或广告中的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08203v1": {
    "title": "Memory Retrieval and Consolidation in Large Language Models through Function Tokens",
    "url": "https://www.alphaxiv.org/abs/2510.08203v1",
    "arxiv_id": "2510.08203v1",
    "authors": "Shaohua Zhang, Yuan Lin, Hang Li",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 13:31:20",
    "ori_summary": "The remarkable success of large language models (LLMs) stems from their ability to consolidate vast amounts of knowledge into the memory during pre-training and to retrieve it from the memory during inference, enabling advanced capabilities such as knowledge memorization, instruction-following and reasoning. However, the mechanisms of memory retrieval and consolidation in LLMs remain poorly understood. In this paper, we propose the function token hypothesis to explain the workings of LLMs: During inference, function tokens activate the most predictive features from context and govern next token prediction (memory retrieval). During pre-training, predicting the next tokens (usually content tokens) that follow function tokens increases the number of learned features of LLMs and updates the model parameters (memory consolidation). Function tokens here roughly correspond to function words in linguistics, including punctuation marks, articles, prepositions, and conjunctions, in contrast to content tokens. We provide extensive experimental evidence supporting this hypothesis. Using bipartite graph analysis, we show that a small number of function tokens activate the majority of features. Case studies further reveal how function tokens activate the most predictive features from context to direct next token prediction. We also find that during pre-training, the training loss is dominated by predicting the next content tokens following function tokens, which forces the function tokens to select the most predictive features from context.",
    "summary": "该论文研究LLM中内存检索与巩固的机制问题，核心思想是提出函数令牌假说：函数令牌在推理时激活上下文中最具预测性的特征进行内存检索，在预训练时通过预测后续内容令牌来驱动内存巩固。",
    "translation": "通过函数令牌实现大语言模型中的记忆检索与巩固",
    "relevance_score": 8,
    "reasoning": "该论文研究LLM的记忆机制优化，属于'使能LLM技术'范畴。改进的记忆检索与巩固技术可直接应用于推荐系统中用户长期偏好的建模和搜索中的上下文理解，通过更有效的记忆管理提升个性化推荐和搜索相关性。函数令牌机制可能为处理用户行为序列和上下文特征提供新的技术路径。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出函数令牌假说，深入揭示LLM内存检索与巩固机制，这对理解LLM内部工作原理及提升推荐系统、搜索等领域的模型透明度与可控性具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08202v1": {
    "title": "Sentiment Matters: An Analysis of 200 Human-SAV Interactions",
    "url": "https://www.alphaxiv.org/abs/2510.08202v1",
    "arxiv_id": "2510.08202v1",
    "authors": "Lirui Guo, Michael G. Burke, Wynita M. Griggs",
    "categories": "cs.HC, cs.AI, cs.CL, cs.ET",
    "pub_date": "2025-10-09 13:30:23",
    "ori_summary": "Shared Autonomous Vehicles (SAVs) are likely to become an important part of the transportation system, making effective human-SAV interactions an important area of research. This paper introduces a dataset of 200 human-SAV interactions to further this area of study. We present an open-source human-SAV conversational dataset, comprising both textual data (e.g., 2,136 human-SAV exchanges) and empirical data (e.g., post-interaction survey results on a range of psychological factors). The dataset's utility is demonstrated through two benchmark case studies: First, using random forest modeling and chord diagrams, we identify key predictors of SAV acceptance and perceived service quality, highlighting the critical influence of response sentiment polarity (i.e., perceived positivity). Second, we benchmark the performance of an LLM-based sentiment analysis tool against the traditional lexicon-based TextBlob method. Results indicate that even simple zero-shot LLM prompts more closely align with user-reported sentiment, though limitations remain. This study provides novel insights for designing conversational SAV interfaces and establishes a foundation for further exploration into advanced sentiment modeling, adaptive user interactions, and multimodal conversational systems.",
    "summary": "",
    "translation": "情感至关重要：200次人机交互分析",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于人机交互中的情感分析，属于纯粹的HCI/心理学研究范畴，与推荐系统、搜索或广告的核心技术进展、LLM赋能技术或Transformer架构改进均无直接关联。即使考虑情感分析在内容理解中的应用，该研究明显侧重交互分析而非技术方法创新，缺乏对RecSys/Search/Ads领域的直接技术贡献。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08191v1": {
    "title": "Training-Free Group Relative Policy Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.08191v1",
    "arxiv_id": "2510.08191v1",
    "authors": "Yuzheng Cai, Siqi Cai, Yuchen Shi, Zihan Xu, Lichao Chen, Yulei Qin, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Yong Mao, Ke Li, Xing Sun",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 13:18:17",
    "ori_summary": "Recent advances in Large Language Model (LLM) agents have demonstrated their promising general capabilities. However, their performance in specialized real-world domains often degrades due to challenges in effectively integrating external tools and specific prompting strategies. While methods like agentic reinforcement learning have been proposed to address this, they typically rely on costly parameter updates, for example, through a process that uses Supervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase with Group Relative Policy Optimization (GRPO) to alter the output distribution. However, we argue that LLMs can achieve a similar effect on the output distribution by learning experiential knowledge as a token prior, which is a far more lightweight approach that not only addresses practical data scarcity but also avoids the common issue of overfitting. To this end, we propose Training-Free Group Relative Policy Optimization (Training-Free GRPO), a cost-effective solution that enhances LLM agent performance without any parameter updates. Our method leverages the group relative semantic advantage instead of numerical ones within each group of rollouts, iteratively distilling high-quality experiential knowledge during multi-epoch learning on a minimal ground-truth data. Such knowledge serves as the learned token prior, which is seamlessly integrated during LLM API calls to guide model behavior. Experiments on mathematical reasoning and web searching tasks demonstrate that Training-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly improves out-of-domain performance. With just a few dozen training samples, Training-Free GRPO outperforms fine-tuned small LLMs with marginal training data and cost.",
    "summary": "",
    "translation": "免训练组相对策略优化",
    "relevance_score": 1,
    "reasoning": "该论文标题暗示了强化学习中的策略优化方法，但未提及与推荐系统、搜索或广告的任何关联。训练免训练方法可能涉及RL技术，但根据用户明确排除标准，没有明确相关性的强化学习论文应被视为不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08189v1": {
    "title": "R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?",
    "url": "https://www.alphaxiv.org/abs/2510.08189v1",
    "arxiv_id": "2510.08189v1",
    "authors": "Yi Lu, Jianing Wang, Linsen Guo, Wei He, Hongyin Tang, Tao Gui, Xuanjing Huang, Xuezhi Cao, Wei Wang, Xunliang Cai",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-09 13:16:22",
    "ori_summary": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought (CoT). However, existing benchmarks mainly focus on immediate, single-horizon tasks, failing to adequately evaluate models' ability to understand and respond to complex, long-horizon scenarios. To address this incomplete evaluation of Large Reasoning Models (LRMs), we propose R-HORIZON, a method designed to stimulate long-horizon reasoning behaviors in LRMs through query composition. Based on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising complex multi-step reasoning tasks with interdependent problems that span long reasoning horizons. Through comprehensive evaluation of LRMs using the R-HORIZON benchmark, we find that even the most advanced LRMs suffer significant performance degradation. Our analysis reveals that LRMs exhibit limited effective reasoning length and struggle to allocate thinking budget across multiple problems appropriately. Recognizing these limitations, we use R-HORIZON to construct long-horizon reasoning data for reinforcement learning with verified rewards (RLVR). Compared to training with single-horizon data, RLVR with R-HORIZON not only substantially improves performance on the multi-horizon reasoning tasks, but also promotes accuracy on standard reasoning tasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as a scalable, controllable, and low-cost paradigm for enhancing and evaluating the long-horizon reasoning capabilities of LRMs.",
    "summary": "该论文研究如何评估大型推理模型在复杂长序列任务中的真实能力。核心方法是提出R-HORIZON评估框架，通过查询组合构建长视野推理基准来激发模型的深度推理行为。",
    "translation": "R-Horizon：您的大型推理模型在广度和深度上究竟能走多远？",
    "relevance_score": 8,
    "reasoning": "该论文聚焦大型推理模型的能力边界评估，属于核心LLM技术进步范畴。推理能力的提升对于搜索中的复杂查询理解、推荐系统中的多轮交互决策以及广告中的用户意图深度分析都具有直接应用价值，能够显著增强这些领域系统的智能水平。",
    "rerank_relevance_score": 4,
    "rerank_reasoning": "该论文主要关注推理模型的评估方法改进，与推荐系统、搜索广告的直接应用关联较弱，但在长序列建模和复杂推理方面有一定启发价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08188v1": {
    "title": "METRICALARGS: A Taxonomy for Studying Metrical Poetry with LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.08188v1",
    "arxiv_id": "2510.08188v1",
    "authors": "Chalamalasetti Kranti, Sowmya Vajjala",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 13:14:38",
    "ori_summary": "Prior NLP work studying poetry has focused primarily on automatic poem generation and summarization. Many languages have well-studied traditions of poetic meter which enforce constraints on a poem in terms of syllable and phoneme patterns. Such advanced literary forms offer opportunities for probing deeper reasoning and language understanding in Large Language Models (LLMs) and their ability to follow strict pre-requisites and rules. In this paper, we introduce MetricalARGS, the first taxonomy of poetry-related NLP tasks designed to evaluate LLMs on metrical poetry across four dimensions: Analysis, Retrieval, Generation, and Support. We discuss how these tasks relate to existing NLP tasks, addressing questions around datasets and evaluation metrics. Taking Telugu as our example language, we illustrate how the taxonomy can be used in practice. MetricalARGS highlights the broader possibilities for understanding the capabilities and limitations of today's LLMs through the lens of metrical poetry.",
    "summary": "",
    "translation": "METRICALARGS：一种使用大语言模型研究格律诗歌的分类法",
    "relevance_score": 1,
    "reasoning": "该论文专注于诗歌格律分析这一纯文学领域，与推荐系统、搜索或广告的核心技术进展完全无关。虽然提到了LLMs，但应用场景是诗歌研究这种非商业领域，不属于任何相关技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08173v1": {
    "title": "NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions",
    "url": "https://www.alphaxiv.org/abs/2510.08173v1",
    "arxiv_id": "2510.08173v1",
    "authors": "Haolin Yang, Yuxing Long, Zhuoyuan Yu, Zihan Yang, Minghan Wang, Jiapeng Xu, Yihan Wang, Ziyan Yu, Wenzhe Cai, Lei Kang, Hao Dong",
    "categories": "cs.RO, cs.AI, cs.CL, cs.CV",
    "pub_date": "2025-10-09 12:59:19",
    "ori_summary": "Instruction-following navigation is a key step toward embodied intelligence. Prior benchmarks mainly focus on semantic understanding but overlook systematically evaluating navigation agents' spatial perception and reasoning capabilities. In this work, we introduce the NavSpace benchmark, which contains six task categories and 1,228 trajectory-instruction pairs designed to probe the spatial intelligence of navigation agents. On this benchmark, we comprehensively evaluate 22 navigation agents, including state-of-the-art navigation models and multimodal large language models. The evaluation results lift the veil on spatial intelligence in embodied navigation. Furthermore, we propose SNav, a new spatially intelligent navigation model. SNav outperforms existing navigation agents on NavSpace and real robot tests, establishing a strong baseline for future work.",
    "summary": "",
    "translation": "NavSpace：导航代理如何遵循空间智能指令",
    "relevance_score": 2,
    "reasoning": "该论文主要研究导航代理和空间智能指令，属于机器人导航领域，与推荐系统、搜索或广告的核心技术关联性较弱。虽然涉及智能代理技术，但缺乏明确的RecSys/Search/Ads应用场景，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08163v1": {
    "title": "ARM2: Adaptive Reasoning Model with Vision Understanding and Executable Code",
    "url": "https://www.alphaxiv.org/abs/2510.08163v1",
    "arxiv_id": "2510.08163v1",
    "authors": "Jian Xie, Zhendong Chu, Aoxiao Zhong, Kai Zhang, Mingzhe Han, Xin Fang, Jialie Shen, Qingsong Wen",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 12:49:34",
    "ori_summary": "Large Reasoning Models (LRMs) often suffer from the ``over-thinking'' problem, generating unnecessarily long reasoning on simple tasks. Some strategies have been proposed to mitigate this issue, such as length penalties or routing mechanisms, but they are typically heuristic and task-specific, lacking a general framework for adaptive reasoning. In this paper, we present ARM2, a unified model that adaptively balances reasoning performance and efficiency across multiple formats through a reinforcement learning framework augmented with length-aware optimization. Beyond conventional natural language inference, ARM2 integrates vision understanding, extending its applicability to multimodal. Moreover, ARM2 integrates executable code into reasoning, enabling substantial reductions in token cost while preserving task performance compared to long CoT. Experiments demonstrate that ARM2 achieves performance on par with traditional reasoning models trained with GRPO, while reducing token usage by over 70% on average. We further conduct extensive analyses to validate the effectiveness of ARM2 and the soundness of its design.",
    "summary": "",
    "translation": "ARM2：具有视觉理解与可执行代码的自适应推理模型",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视觉理解和代码执行能力，属于多模态推理范畴。虽然自适应推理机制在概念上可能对推荐系统的复杂决策有启发，但论文的核心焦点是视觉模态和代码执行，与推荐系统、搜索或广告的核心技术栈关联较弱，缺乏明确的直接应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08158v1": {
    "title": "Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.08158v1",
    "arxiv_id": "2510.08158v1",
    "authors": "Shuzhou Yuan, Ercong Nie, Yinuo Sun, Chenxuan Zhao, William LaCroix, Michael Färber",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 12:38:16",
    "ori_summary": "Large language models (LLMs) frequently produce false refusals, declining benign requests that contain terms resembling unsafe queries. We address this challenge by introducing two comprehensive benchmarks: the Exaggerated Safety Benchmark (XSB) for single-turn prompts, annotated with \"Focus\" keywords that identify refusal-inducing triggers, and the Multi-turn Scenario-based Exaggerated Safety Benchmark (MS-XSB), which systematically evaluates refusal calibration in realistic, context-rich dialog settings. Our benchmarks reveal that exaggerated refusals persist across diverse recent LLMs and are especially pronounced in complex, multi-turn scenarios. To mitigate these failures, we leverage post-hoc explanation methods to identify refusal triggers and deploy three lightweight, model-agnostic approaches, ignore-word instructions, prompt rephrasing, and attention steering, at inference time, all without retraining or parameter access. Experiments on four instruction-tuned Llama models demonstrate that these strategies substantially improve compliance on safe prompts while maintaining robust safety protections. Our findings establish a reproducible framework for diagnosing and mitigating exaggerated refusals, highlighting practical pathways to safer and more helpful LLM deployments.",
    "summary": "",
    "translation": "超越过度拒绝：基于场景的诊断与事后缓解方法应对大语言模型中的夸大拒绝行为",
    "relevance_score": 1,
    "reasoning": "该论文主要关注LLM的拒绝行为诊断和缓解，属于纯粹的NLP评估和安全性话题。虽然涉及LLM技术，但核心关注的是模型拒绝行为的评估和修正，与推荐系统、搜索或广告中的排序、匹配、个性化等核心任务没有直接关联，也不涉及Transformer架构改进或异构数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08152v1": {
    "title": "DACIP-RC: Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension on Business Conversations",
    "url": "https://www.alphaxiv.org/abs/2510.08152v1",
    "arxiv_id": "2510.08152v1",
    "authors": "Elena Khasanova, Harsh Saini, Md Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen, Shashi Bhushan TN",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 12:35:20",
    "ori_summary": "The rapid advancements in Large Language Models (LLMs) have enabled their adoption in real-world industrial scenarios for various natural language processing tasks. However, the high inference cost of large-scale LLMs makes their deployment impractical, necessitating the use of smaller models. Despite their efficiency, smaller LLMs lack robust zero-shot instruction-following capabilities across diverse domains, limiting their adaptability to dynamic user requirements. Traditional fine-tuning approaches exacerbate this issue by inducing catastrophic forgetting, reducing the model's generalization ability for unseen tasks. In this paper, we propose Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension (DACIP-RC), a continual pre-training technique that enhances smaller LLMs' domain adaptability for business conversational tasks. Unlike conventional pre-training approaches that rely on next-token prediction, DACIP-RC generates diverse task instructions and responses via reading comprehension on conversation transcripts, enabling better instruction generalization. Our empirical evaluations demonstrate that DACIP-RC significantly improves zero-shot generalization across a wide range of business conversational tasks, including meeting summarization, action item generation, and call purpose identification. To the best of our knowledge, this is the first work to apply instruction pre-training on business conversational data, providing insights into how industries can leverage proprietary datasets for domain adaptation.",
    "summary": "论文研究如何提升小型LLM在商业对话任务中的领域适应能力，核心方法是采用基于阅读理解的持续指令预训练技术，通过对话转录生成多样化任务指令和响应来增强指令泛化能力。",
    "translation": "DACIP-RC：基于商业对话阅读理解任务的领域自适应持续指令预训练",
    "relevance_score": 8,
    "reasoning": "该论文涉及领域自适应和持续指令预训练，属于'Enabling LLM Tech'范畴，专注于提升LLM在特定业务领域的适应能力。这种技术可直接应用于搜索和推荐系统中的对话式交互场景，通过阅读理解商业对话来增强模型对用户意图和业务上下文的理解能力，从而改善个性化推荐和搜索相关性。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出通过阅读理解实现领域自适应持续指令预训练，直接针对LLM在商业对话任务中的领域适应问题，与直接LLM应用和领域适应高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08149v1": {
    "title": "AI Knowledge Assist: An Automated Approach for the Creation of Knowledge Bases for Conversational AI Agents",
    "url": "https://www.alphaxiv.org/abs/2510.08149v1",
    "arxiv_id": "2510.08149v1",
    "authors": "Md Tahmid Rahman Laskar, Julien Bouvier Tremblay, Xue-Yong Fu, Cheng Chen, Shashi Bhushan TN",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-09 12:34:31",
    "ori_summary": "The utilization of conversational AI systems by leveraging Retrieval Augmented Generation (RAG) techniques to solve customer problems has been on the rise with the rapid progress of Large Language Models (LLMs). However, the absence of a company-specific dedicated knowledge base is a major barrier to the integration of conversational AI systems in contact centers. To this end, we introduce AI Knowledge Assist, a system that extracts knowledge in the form of question-answer (QA) pairs from historical customer-agent conversations to automatically build a knowledge base. Fine-tuning a lightweight LLM on internal data demonstrates state-of-the-art performance, outperforming larger closed-source LLMs. More specifically, empirical evaluation on 20 companies demonstrates that the proposed AI Knowledge Assist system that leverages the LLaMA-3.1-8B model eliminates the cold-start gap in contact centers by achieving above 90% accuracy in answering information-seeking questions. This enables immediate deployment of RAG-powered chatbots.",
    "summary": "",
    "translation": "AI知识助手：一种为对话式AI代理创建知识库的自动化方法",
    "relevance_score": 3,
    "reasoning": "该论文主要关注对话式AI代理的知识库创建，属于AIGC和内容生成范畴，与推荐系统、搜索或广告的核心技术关联较弱。虽然知识库构建可能间接支持某些推荐场景，但论文焦点不在排名、检索或个性化等核心领域，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08145v1": {
    "title": "Mitigating Judgment Preference Bias in Large Language Models through Group-Based Polling",
    "url": "https://www.alphaxiv.org/abs/2510.08145v1",
    "arxiv_id": "2510.08145v1",
    "authors": "Shuliang Liu, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Minghe Yu, Yu Gu, Chong Chen, Huiyuan Xie, Ge Yu",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 12:32:31",
    "ori_summary": "Large Language Models (LLMs) as automatic evaluators, commonly referred to as LLM-as-a-Judge, have also attracted growing attention. This approach plays a vital role in aligning LLMs with human judgments, providing accurate and reliable assessments. However, LLM-based judgment models often exhibit judgment preference bias during the evaluation phase, tending to favor responses generated by themselves, undermining the reliability of their judgments. This paper introduces the Group-Based Polling Optimization (Genii), an unsupervised multi-agent collaborative optimization framework that mitigates the inherent judgment preference bias of judgment models. Specifically, Genii integrates various LLM-based judgment models into a multi-agent system and simulates the interactive client-server polling mechanism to optimize each client agent unsupervisedly. Our experiments demonstrate that Genii outperforms supervised models trained on annotated judgment data, while requiring no human-labeled annotations. Genii consistently improves performance across different client agents during the polling, even when weaker models act as server agents. Further analysis reveals that Genii effectively mitigates judgment preference bias of LLM-based judgment models, demonstrating its effectiveness. All codes are available at https://github.com/NEUIR/Genii.",
    "summary": "",
    "translation": "通过基于群体的投票缓解大型语言模型中的判断偏好偏差",
    "relevance_score": 3,
    "reasoning": "该论文主要关注LLM的偏好偏差缓解，属于LLM评估和偏差校正范畴，与我的核心关注点（推荐系统、搜索、广告的直接应用或使能技术）相关性较弱。虽然偏差缓解可能间接影响推荐/搜索系统的公平性，但论文焦点更偏向通用LLM评估而非特定领域应用，且未明确涉及异构数据建模或Transformer架构创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08120v1": {
    "title": "Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations",
    "url": "https://www.alphaxiv.org/abs/2510.08120v1",
    "arxiv_id": "2510.08120v1",
    "authors": "Jasmina Gajcin, Erik Miehling, Rahul Nair, Elizabeth Daly, Radu Marinescu, Seshu Tirupathi",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 12:05:37",
    "ori_summary": "Using LLMs to evaluate text, that is, LLM-as-a-judge, is increasingly being used at scale to augment or even replace human annotations. As such, it is imperative that we understand the potential biases and risks of doing so. In this work, we propose an approach for extracting high-level concept-based global policies from LLM-as-a-Judge. Our approach consists of two algorithms: 1) CLoVE (Contrastive Local Verifiable Explanations), which generates verifiable, concept-based, contrastive local explanations and 2) GloVE (Global Verifiable Explanations), which uses iterative clustering, summarization and verification to condense local rules into a global policy. We evaluate GloVE on seven standard benchmarking datasets for content harm detection. We find that the extracted global policies are highly faithful to decisions of the LLM-as-a-Judge. Additionally, we evaluated the robustness of global policies to text perturbations and adversarial attacks. Finally, we conducted a user study to evaluate user understanding and satisfaction with global policies.",
    "summary": "",
    "translation": "通过可验证的全局解释解读LLM作为评判者的策略",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM作为评判者的策略解释和可验证性，这属于纯粹的LLM评估和解释性研究，与幻觉、评估基准等NLP中心主题密切相关。虽然LLM评估在广义上可能影响推荐系统，但论文标题没有表明任何直接的RecSys/Search/Ads应用，而是专注于纯粹的LLM评判机制解释。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08114v1": {
    "title": "Can Risk-taking AI-Assistants suitably represent entities",
    "url": "https://www.alphaxiv.org/abs/2510.08114v1",
    "arxiv_id": "2510.08114v1",
    "authors": "Ali Mazyaki, Mohammad Naghizadeh, Samaneh Ranjkhah Zonouzaghi, Amirhossein Farshi Sotoudeh",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-09 11:55:31",
    "ori_summary": "Responsible AI demands systems whose behavioral tendencies can be effectively measured, audited, and adjusted to prevent inadvertently nudging users toward risky decisions or embedding hidden biases in risk aversion. As language models (LMs) are increasingly incorporated into AI-driven decision support systems, understanding their risk behaviors is crucial for their responsible deployment. This study investigates the manipulability of risk aversion (MoRA) in LMs, examining their ability to replicate human risk preferences across diverse economic scenarios, with a focus on gender-specific attitudes, uncertainty, role-based decision-making, and the manipulability of risk aversion. The results indicate that while LMs such as DeepSeek Reasoner and Gemini-2.0-flash-lite exhibit some alignment with human behaviors, notable discrepancies highlight the need to refine bio-centric measures of manipulability. These findings suggest directions for refining AI design to better align human and AI risk preferences and enhance ethical decision-making. The study calls for further advancements in model design to ensure that AI systems more accurately replicate human risk preferences, thereby improving their effectiveness in risk management contexts. This approach could enhance the applicability of AI assistants in managing risk.",
    "summary": "",
    "translation": "敢于承担风险的AI助手能否恰当地代表实体",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于AI助手风险承担能力和实体代表性问题，这属于AI伦理、安全性和代理行为的范畴，与推荐系统、搜索或广告的核心技术进展完全无关。标题中没有任何技术元素表明与Transformer架构、LLM效率、推荐算法或多模态建模相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08111v1": {
    "title": "Evaluating LLM-Generated Legal Explanations for Regulatory Compliance in Social Media Influencer Marketing",
    "url": "https://www.alphaxiv.org/abs/2510.08111v1",
    "arxiv_id": "2510.08111v1",
    "authors": "Haoyang Gui, Thales Bertaglia, Taylor Annabell, Catalina Goanta, Tjomme Dooper, Gerasimos Spanakis",
    "categories": "cs.CL, cs.CY",
    "pub_date": "2025-10-09 11:50:37",
    "ori_summary": "The rise of influencer marketing has blurred boundaries between organic content and sponsored content, making the enforcement of legal rules relating to transparency challenging. Effective regulation requires applying legal knowledge with a clear purpose and reason, yet current detection methods of undisclosed sponsored content generally lack legal grounding or operate as opaque \"black boxes\". Using 1,143 Instagram posts, we compare gpt-5-nano and gemini-2.5-flash-lite under three prompting strategies with controlled levels of legal knowledge provided. Both models perform strongly in classifying content as sponsored or not (F1 up to 0.93), though performance drops by over 10 points on ambiguous cases. We further develop a taxonomy of reasoning errors, showing frequent citation omissions (28.57%), unclear references (20.71%), and hidden ads exhibiting the highest miscue rate (28.57%). While adding regulatory text to the prompt improves explanation quality, it does not consistently improve detection accuracy. The contribution of this paper is threefold. First, it makes a novel addition to regulatory compliance technology by providing a taxonomy of common errors in LLM-generated legal reasoning to evaluate whether automated moderation is not only accurate but also legally robust, thereby advancing the transparent detection of influencer marketing content. Second, it features an original dataset of LLM explanations annotated by two students who were trained in influencer marketing law. Third, it combines quantitative and qualitative evaluation strategies for LLM explanations and critically reflects on how these findings can support advertising regulatory bodies in automating moderation processes on a solid legal foundation.",
    "summary": "",
    "translation": "评估大语言模型生成的法律解释在社交媒体网红营销监管合规性中的应用",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM在法律解释和监管合规性评估方面的应用，这属于特定领域应用而非核心推荐系统、搜索或广告技术。虽然涉及社交媒体环境，但焦点是法律合规性而非排名、检索或用户建模等核心领域，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08102v1": {
    "title": "Lossless Vocabulary Reduction for Auto-Regressive Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08102v1",
    "arxiv_id": "2510.08102v1",
    "authors": "Daiki Chijiwa, Taku Hasegawa, Kyosuke Nishida, Shin'ya Yamaguchi, Tomoya Ohba, Tamao Sakao, Susumu Takeuchi",
    "categories": "cs.CL, cs.AI, cs.LG, stat.ML",
    "pub_date": "2025-10-09 11:38:48",
    "ori_summary": "Tokenization -- the process of decomposing a given text into a sequence of subwords called tokens -- is one of the key components in the development of language models. Particularly, auto-regressive language models generate texts token by token, i.e., by predicting the next-token distribution given the previous ones, and thus tokenization directly affects their efficiency in text generation. Since each language model has their own vocabulary as a set of possible tokens, they struggle to cooperate with each other at the level of next-token distributions such as model ensemble. In this paper, we establish a theoretical framework of lossless vocabulary reduction, which efficiently converts a given auto-regressive language model into the one with an arbitrarily small vocabulary without any loss in accuracy. As an application, we demonstrate that language models with different tokenization can cooperate with each other efficiently through their maximal common vocabulary.",
    "summary": "研究自回归语言模型因不同词汇表导致的模型协作困难问题，核心思想是通过理论框架将模型无损转换为任意小词汇表，实现不同分词模型的高效协同。",
    "translation": "自回归语言模型的无损词汇表缩减",
    "relevance_score": 8,
    "reasoning": "该论文属于'Enabling LLM Tech'范畴，专注于语言模型效率优化技术。无损词汇表缩减技术可以显著降低LLM的推理成本和内存占用，这对于需要部署大规模LLM的推荐系统和搜索应用具有直接价值，能够提升在线服务的响应速度和资源利用率。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出无损词汇缩减框架，直接提升自回归语言模型的效率与互操作性，对LLM核心技术进步有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.08098v1": {
    "title": "The Price of Thought: A Multilingual Analysis of Reasoning, Performance, and Cost of Negotiation in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08098v1",
    "arxiv_id": "2510.08098v1",
    "authors": "Sherzod Hakimov, Roland Bernard, Tim Leiber, Karl Osswald, Kristina Richert, Ruilin Yang, Raffaella Bernardi, David Schlangen",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 11:36:38",
    "ori_summary": "Negotiation is a fundamental challenge for AI agents, as it requires an ability to reason strategically, model opponents, and balance cooperation with competition. We conduct the first comprehensive study systematically evaluating the effect of (LLM-)reasoning on the negotiation abilities of both commercial and open-weight LLMs, and do this across three languages. Using a self-play setup across three diverse dialogue games, we analyse trade-offs between performance and cost, the language consistency of reasoning processes, and the nature of strategic adaptation exhibited by models. Our findings show that enabling reasoning-that is, scaling test time compute-significantly improves negotiation outcomes by enhancing collaboration and helping models overcome task complexities, but comes at a substantial computational cost: reasoning improves GPT-5's performance by 31.4 % while increasing its cost by nearly 400 %. Most critically, we uncover a significant multilingual reasoning distinction: open-weight models consistently switch to English for their internal reasoning steps, even when negotiating in German or Italian (and thus possibly impacting potential explainability gains through the disclosure of reasoning traces), while leading commercial models maintain language consistency between their reasoning and final output.",
    "summary": "",
    "translation": "思维的成本：大型语言模型中推理、性能与协商成本的多语言分析",
    "relevance_score": 2,
    "reasoning": "该论文主要分析LLM的推理能力、性能表现和成本协商，属于纯粹的LLM能力评估研究。虽然涉及多语言分析，但核心焦点是模型内在能力评估而非RecSys/Search/Ads应用。论文缺乏对推荐系统、搜索或广告场景的具体应用潜力说明，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08091v1": {
    "title": "Everything is Plausible: Investigating the Impact of LLM Rationales on Human Notions of Plausibility",
    "url": "https://www.alphaxiv.org/abs/2510.08091v1",
    "arxiv_id": "2510.08091v1",
    "authors": "Shramay Palta, Peter Rankel, Sarah Wiegreffe, Rachel Rudinger",
    "categories": "cs.CL, cs.AI, cs.HC",
    "pub_date": "2025-10-09 11:22:29",
    "ori_summary": "We investigate the degree to which human plausibility judgments of multiple-choice commonsense benchmark answers are subject to influence by (im)plausibility arguments for or against an answer, in particular, using rationales generated by LLMs. We collect 3,000 plausibility judgments from humans and another 13,600 judgments from LLMs. Overall, we observe increases and decreases in mean human plausibility ratings in the presence of LLM-generated PRO and CON rationales, respectively, suggesting that, on the whole, human judges find these rationales convincing. Experiments with LLMs reveal similar patterns of influence. Our findings demonstrate a novel use of LLMs for studying aspects of human cognition, while also raising practical concerns that, even in domains where humans are ``experts'' (i.e., common sense), LLMs have the potential to exert considerable influence on people's beliefs.",
    "summary": "",
    "translation": "万物皆合理：探究大语言模型推理对人类合理性认知的影响",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLM推理对人类认知的影响，属于心理学和人类认知交互领域。虽然涉及LLM，但其研究重点在于人类对合理性的感知判断，而非LLM在推荐系统、搜索或广告中的技术应用或架构改进。该研究缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08081v1": {
    "title": "AutoQual: An LLM Agent for Automated Discovery of Interpretable Features for Review Quality Assessment",
    "url": "https://www.alphaxiv.org/abs/2510.08081v1",
    "arxiv_id": "2510.08081v1",
    "authors": "Xiaochong Lan, Jie Feng, Yinxing Liu, Xinlei Shi, Yong Li",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-09 11:11:02",
    "ori_summary": "Ranking online reviews by their intrinsic quality is a critical task for e-commerce platforms and information services, impacting user experience and business outcomes. However, quality is a domain-dependent and dynamic concept, making its assessment a formidable challenge. Traditional methods relying on hand-crafted features are unscalable across domains and fail to adapt to evolving content patterns, while modern deep learning approaches often produce black-box models that lack interpretability and may prioritize semantics over quality. To address these challenges, we propose AutoQual, an LLM-based agent framework that automates the discovery of interpretable features. While demonstrated on review quality assessment, AutoQual is designed as a general framework for transforming tacit knowledge embedded in data into explicit, computable features. It mimics a human research process, iteratively generating feature hypotheses through reflection, operationalizing them via autonomous tool implementation, and accumulating experience in a persistent memory. We deploy our method on a large-scale online platform with a billion-level user base. Large-scale A/B testing confirms its effectiveness, increasing average reviews viewed per user by 0.79% and the conversion rate of review readers by 0.27%.",
    "summary": "",
    "translation": "AutoQual：一种用于自动发现可解释特征以进行评论质量评估的LLM智能体",
    "relevance_score": 3,
    "reasoning": "该论文涉及LLM在内容质量评估中的应用，这与搜索和推荐系统中的内容理解有一定关联。然而，评论质量评估更偏向内容质量分析而非核心的推荐、搜索或广告排名任务，应用范围相对有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08058v1": {
    "title": "FedDTRE: Federated Dialogue Generation Models Powered by Trustworthiness Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.08058v1",
    "arxiv_id": "2510.08058v1",
    "authors": "Shule Lu, Lingxiang Wang, Sijia Wen, Ziwei Wang, Hainan Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 10:43:14",
    "ori_summary": "With the rapid development of artificial intelligence, dialogue systems have become a prominent form of human-computer interaction. However, traditional centralized or fully local training approaches face challenges in balancing privacy preservation and personalization due to data privacy concerns and heterogeneous device capabilities. Federated learning, as a representative distributed paradigm, offers a promising solution. However, existing methods often suffer from overfitting under limited client data and tend to forget global information after multiple training rounds, leading to poor generalization. To address these issues, we propose FedDTRE, a Federated adaptive aggregation strategy for Dialogue generation based on Trustworthiness Evaluation. Instead of directly replacing local models with the global model, FedDTRE leverages trustworthiness scores of both global and local models on a fairness-oriented evaluation dataset to dynamically regulate the global model's contribution during local updates. Experimental results demonstrate that FedDTRE can improve dialogue model performance and enhance the quality of dialogue generation.",
    "summary": "",
    "translation": "FedDTRE：基于可信度评估驱动的联邦对话生成模型",
    "relevance_score": 1,
    "reasoning": "该论文涉及联邦学习和对话生成，这两个主题均被明确列为不相关主题。联邦学习属于隐私/安全范畴，而对话生成属于纯粹的LLM中心化应用，与推荐系统、搜索或广告的核心技术进展没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08049v1": {
    "title": "A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08049v1",
    "arxiv_id": "2510.08049v1",
    "authors": "Congming Zheng, Jiachen Zhu, Zhuoying Ou, Yuxiang Chen, Kangning Zhang, Rong Shan, Zeyu Zheng, Mengyue Yang, Jianghao Lin, Yong Yu, Weinan Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 10:35:31",
    "ori_summary": "Although Large Language Models (LLMs) exhibit advanced reasoning ability, conventional alignment remains largely dominated by outcome reward models (ORMs) that judge only final answers. Process Reward Models(PRMs) address this gap by evaluating and guiding reasoning at the step or trajectory level. This survey provides a systematic overview of PRMs through the full loop: how to generate process data, build PRMs, and use PRMs for test-time scaling and reinforcement learning. We summarize applications across math, code, text, multimodal reasoning, robotics, and agents, and review emerging benchmarks. Our goal is to clarify design spaces, reveal open challenges, and guide future research toward fine-grained, robust reasoning alignment.",
    "summary": "",
    "translation": "过程奖励模型综述：从结果信号到大型语言模型的过程监督",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM的训练方法和奖励建模，属于纯粹的LLM技术范畴。虽然奖励模型在RLHF中有应用，但论文聚焦于过程监督而非具体应用场景，缺乏与推荐系统、搜索或广告领域的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08047v1": {
    "title": "Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic Speech Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.08047v1",
    "arxiv_id": "2510.08047v1",
    "authors": "Yi-Cheng Lin, Yu-Hsuan Li Liang, Hsuan Su, Tzu-Quan Lin, Shang-Tse Chen, Yun-Nung Chen, Hung-yi Lee",
    "categories": "eess.AS, cs.CL",
    "pub_date": "2025-10-09 10:31:47",
    "ori_summary": "Robust ASR under domain shift is crucial because real-world systems encounter unseen accents and domains with limited labeled data. Although pseudo-labeling offers a practical workaround, it often introduces systematic, accent-specific errors that filtering fails to fix. We ask: How can we correct these recurring biases without target ground truth? We propose a simple parameter-space correction: in a source domain containing both real and pseudo-labeled data, two ASR models are fine-tuned from the same initialization, one on ground-truth labels and the other on pseudo-labels, and their weight difference forms a correction vector that captures pseudo-label biases. When applied to a pseudo-labeled target model, this vector enhances recognition, achieving up to a 35% relative Word Error Rate (WER) reduction on AfriSpeech-200 across ten African accents with the Whisper tiny model.",
    "summary": "",
    "translation": "Pseudo2Real：自动语音识别中伪标签校正的任务算术",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动语音识别中的伪标签校正技术，属于纯粹的语音处理领域。虽然提到了任务算术的概念，但核心应用场景是语音识别，与推荐系统、搜索或广告领域没有直接关联，也不涉及LLM在RecSys/Search/Ads中的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08043v1": {
    "title": "Climate Knowledge in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.08043v1",
    "arxiv_id": "2510.08043v1",
    "authors": "Ivan Kuznetsov, Jacopo Grassi, Dmitrii Pantiukhin, Boris Shapkin, Thomas Jung, Nikolay Koldunov",
    "categories": "cs.CL, cs.LG, physics.ao-ph",
    "pub_date": "2025-10-09 10:25:36",
    "ori_summary": "Large language models (LLMs) are increasingly deployed for climate-related applications, where understanding internal climatological knowledge is crucial for reliability and misinformation risk assessment. Despite growing adoption, the capacity of LLMs to recall climate normals from parametric knowledge remains largely uncharacterized. We investigate the capacity of contemporary LLMs to recall climate normals without external retrieval, focusing on a prototypical query: mean July 2-m air temperature 1991-2020 at specified locations. We construct a global grid of queries at 1{\\deg} resolution land points, providing coordinates and location descriptors, and validate responses against ERA5 reanalysis. Results show that LLMs encode non-trivial climate structure, capturing latitudinal and topographic patterns, with root-mean-square errors of 3-6 {\\deg}C and biases of $\\pm$1 {\\deg}C. However, spatially coherent errors remain, particularly in mountains and high latitudes. Performance degrades sharply above 1500 m, where RMSE reaches 5-13 {\\deg}C compared to 2-4 {\\deg}C at lower elevations. We find that including geographic context (country, city, region) reduces errors by 27% on average, with larger models being most sensitive to location descriptors. While models capture the global mean magnitude of observed warming between 1950-1974 and 2000-2024, they fail to reproduce spatial patterns of temperature change, which directly relate to assessing climate change. This limitation highlights that while LLMs may capture present-day climate distributions, they struggle to represent the regional and local expression of long-term shifts in temperature essential for understanding climate dynamics. Our evaluation framework provides a reproducible benchmark for quantifying parametric climate knowledge in LLMs and complements existing climate communication assessments.",
    "summary": "",
    "translation": "大型语言模型中的气候知识",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于LLMs中的特定领域知识（气候），这属于纯粹的LLM知识评估范畴，与推荐系统、搜索或广告的核心技术进展无关。没有证据表明该研究涉及推荐系统架构、搜索算法改进、广告排名或任何与我的关注领域相关的技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08042v1": {
    "title": "ChatGPT as a Translation Engine: A Case Study on Japanese-English",
    "url": "https://www.alphaxiv.org/abs/2510.08042v1",
    "arxiv_id": "2510.08042v1",
    "authors": "Vincent Michael Sutanto, Giovanni Gatti De Giacomo, Toshiaki Nakazawa, Masaru Yamada",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 10:25:10",
    "ori_summary": "This study investigates ChatGPT for Japanese-English translation, exploring simple and enhanced prompts and comparing against commercially available translation engines. Performing both automatic and MQM-based human evaluations, we found that document-level translation outperforms sentence-level translation for ChatGPT. On the other hand, we were not able to determine if enhanced prompts performed better than simple prompts in our experiments. We also discovered that ChatGPT-3.5 was preferred by automatic evaluation, but a tradeoff exists between accuracy (ChatGPT-3.5) and fluency (ChatGPT-4). Lastly, ChatGPT yields competitive results against two widely-known translation systems.",
    "summary": "",
    "translation": "ChatGPT作为翻译引擎：日语-英语案例研究",
    "relevance_score": 1,
    "reasoning": "该论文纯粹研究ChatGPT在机器翻译领域的应用，属于NLP特定任务研究，与推荐系统、搜索或广告的核心技术发展没有直接关联。论文标题明确聚焦于翻译引擎功能，没有涉及任何推荐、搜索排名、广告投放或Transformer架构改进等关键技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08002v1": {
    "title": "Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks",
    "url": "https://www.alphaxiv.org/abs/2510.08002v1",
    "arxiv_id": "2510.08002v1",
    "authors": "Cheng Yang, Xuemeng Yang, Licheng Wen, Daocheng Fu, Jianbiao Mei, Rong Wu, Pinlong Cai, Yufan Shen, Nianchen Deng, Botian Shi, Yu Qiao, Haifeng Li",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 09:40:34",
    "ori_summary": "Large Language Models have demonstrated remarkable capabilities across diverse domains, yet significant challenges persist when deploying them as AI agents for real-world long-horizon tasks. Existing LLM agents suffer from a critical limitation: they are test-time static and cannot learn from experience, lacking the ability to accumulate knowledge and continuously improve on the job. To address this challenge, we propose MUSE, a novel agent framework that introduces an experience-driven, self-evolving system centered around a hierarchical Memory Module. MUSE organizes diverse levels of experience and leverages them to plan and execute long-horizon tasks across multiple applications. After each sub-task execution, the agent autonomously reflects on its trajectory, converting the raw trajectory into structured experience and integrating it back into the Memory Module. This mechanism enables the agent to evolve beyond its static pretrained parameters, fostering continuous learning and self-evolution. We evaluate MUSE on the long-horizon productivity benchmark TAC. It achieves new SOTA performance by a significant margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments demonstrate that as the agent autonomously accumulates experience, it exhibits increasingly superior task completion capabilities, as well as robust continuous learning and self-evolution capabilities. Moreover, the accumulated experience from MUSE exhibits strong generalization properties, enabling zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI agents capable of real-world productivity task automation.",
    "summary": "",
    "translation": "在职学习：面向长周期任务的经验驱动自进化智能体",
    "relevance_score": 3,
    "reasoning": "该论文主要关注智能体的持续学习和自我进化能力，属于通用AI智能体领域。虽然经验驱动的学习机制在推荐系统中可能有潜在应用（如用户行为建模的持续优化），但论文聚焦于长周期任务而非具体的推荐、搜索或广告场景，与核心关注点关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07993v1": {
    "title": "Leveraging Author-Specific Context for Scientific Figure Caption Generation: 3rd SciCap Challenge",
    "url": "https://www.alphaxiv.org/abs/2510.07993v1",
    "arxiv_id": "2510.07993v1",
    "authors": "Watcharapong Timklaypachara, Monrada Chiewhawan, Nopporn Lekuthai, Titipat Achakulvisut",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 09:30:28",
    "ori_summary": "Scientific figure captions require both accuracy and stylistic consistency to convey visual information. Here, we present a domain-specific caption generation system for the 3rd SciCap Challenge that integrates figure-related textual context with author-specific writing styles using the LaMP-Cap dataset. Our approach uses a two-stage pipeline: Stage 1 combines context filtering, category-specific prompt optimization via DSPy's MIPROv2 and SIMBA, and caption candidate selection; Stage 2 applies few-shot prompting with profile figures for stylistic refinement. Our experiments demonstrate that category-specific prompts outperform both zero-shot and general optimized approaches, improving ROUGE-1 recall by +8.3\\% while limiting precision loss to -2.8\\% and BLEU-4 reduction to -10.9\\%. Profile-informed stylistic refinement yields 40--48\\% gains in BLEU scores and 25--27\\% in ROUGE. Overall, our system demonstrates that combining contextual understanding with author-specific stylistic adaptation can generate captions that are both scientifically accurate and stylistically faithful to the source paper.",
    "summary": "",
    "translation": "利用作者特定上下文进行科学图表标题生成：第三届SciCap挑战赛",
    "relevance_score": 2,
    "reasoning": "该论文专注于科学图表标题生成这一特定领域的文本生成任务，属于纯粹的LLM内容生成应用。虽然标题生成在技术上与搜索中的文档理解有一定关联，但该工作明确限定在科学图表这一狭窄领域，且没有展示出在推荐系统、搜索或广告中的直接应用潜力，因此与您关注的领域相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07978v1": {
    "title": "VoiceAgentBench: Are Voice Assistants ready for agentic tasks?",
    "url": "https://www.alphaxiv.org/abs/2510.07978v1",
    "arxiv_id": "2510.07978v1",
    "authors": "Dhruv Jain, Harshit Shukla, Gautam Rajeev, Ashish Kulkarni, Chandra Khatri, Shubham Agarwal",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-09 09:11:38",
    "ori_summary": "Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants capable of understanding natural spoken queries and performing complex tasks. However, existing speech benchmarks primarily focus on isolated capabilities such as transcription, or question-answering, and do not systematically evaluate agentic scenarios encompassing multilingual and cultural understanding, as well as adversarial robustness. To address this, we introduce VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in realistic spoken agentic settings. It comprises over 5,500 synthetic spoken queries, including dialogues grounded in Indian context, covering single-tool invocations, multi-tool workflows, multi-turn interactions, and safety evaluations. The benchmark supports English, Hindi, and 5 other Indian languages, reflecting real-world linguistic and cultural diversity. We simulate speaker variability using a novel sampling algorithm that selects audios for TTS voice conversion based on its speaker embeddings, maximizing acoustic and speaker diversity. Our evaluation measures tool selection accuracy, structural consistency, and the correctness of tool invocations, including adversarial robustness. Our experiments reveal significant gaps in contextual tool orchestration tasks, Indic generalization, and adversarial robustness, exposing critical limitations of current SpeechLMs.",
    "summary": "",
    "translation": "VoiceAgentBench：语音助手是否已准备好执行智能体任务？",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于语音助手的智能体能力评估，属于语音交互和智能体评估领域，与推荐系统、搜索或广告的核心技术无直接关联。语音助手虽然涉及用户交互，但论文关注的是智能体任务准备度评估，而非推荐、搜索或广告中的实际应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07974v1": {
    "title": "Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.07974v1",
    "arxiv_id": "2510.07974v1",
    "authors": "Jialu Du, Guiyang Hou, Yihui Fu, Chen Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 09:07:31",
    "ori_summary": "While large language models (LLMs) excel in mathematical and code reasoning, we observe they struggle with social reasoning tasks, exhibiting cognitive confusion, logical inconsistencies, and conflation between objective world states and subjective belief states. Through deteiled analysis of DeepSeek-R1's reasoning trajectories, we find that LLMs frequently encounter reasoning impasses and tend to output contradictory terms like \"tricky\" and \"confused\" when processing scenarios with multiple participants and timelines, leading to erroneous reasoning or infinite loops. The core issue is their inability to disentangle objective reality from agents' subjective beliefs. To address this, we propose an adaptive world model-enhanced reasoning mechanism that constructs a dynamic textual world model to track entity states and temporal sequences. It dynamically monitors reasoning trajectories for confusion indicators and promptly intervenes by providing clear world state descriptions, helping models navigate through cognitive dilemmas. The mechanism mimics how humans use implicit world models to distinguish between external events and internal beliefs. Evaluations on three social benchmarks demonstrate significant improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational costs (up to 33.8% token reduction), offering a simple yet effective solution for deploying LLMs in social contexts.",
    "summary": "",
    "translation": "大型语言模型中的主动混淆表达：利用世界模型实现更好的社会推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM的社会推理能力和世界模型，这属于纯粹的NLP中心话题，与推荐系统、搜索或广告的核心技术进展无关。虽然提到了世界模型，但没有明确说明其在RecSys/Search/Ads中的潜在应用价值，因此相关性很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07962v1": {
    "title": "LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?",
    "url": "https://www.alphaxiv.org/abs/2510.07962v1",
    "arxiv_id": "2510.07962v1",
    "authors": "Jingyuan Wang, Yankai Chen, Zhonghang Li, Chao Huang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 08:55:12",
    "ori_summary": "Large language models (LLMs) have demonstrated remarkable progress in reasoning, often through supervised fine-tuning (SFT). However, SFT is resource-intensive, relying on large curated datasets, rejection-sampled demonstrations, and uniform optimization across all tokens, even though only a fraction carry meaningful learning value. In this work, we explore a counterintuitive idea: can smaller language models (SLMs) teach larger language models (LLMs) by revealing high-value reasoning moments that reflect the latter's unique strength? We propose LightReasoner, a novel framework that leverages the behavioral divergence between a stronger expert model (LLM) and a weaker amateur model (SLM). LightReasoner operates in two stages: (1) a sampling stage that pinpoints critical reasoning moments and constructs supervision examples capturing the expert's advantage through expert-amateur contrast, and (2) a fine-tuning stage that aligns the expert model with these distilled examples, amplifying its reasoning strengths. Across seven mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while reducing time consumption by 90%, sampled problems by 80%, and tuned token usage by 99%, all without relying on ground-truth labels. By turning weaker SLMs into effective teaching signals, LightReasoner offers a scalable and resource-efficient approach for advancing LLM reasoning. Code is available at: https://github.com/HKUDS/LightReasoner",
    "summary": "",
    "translation": "LightReasoner：小型语言模型能否教会大型语言模型推理？",
    "relevance_score": 4,
    "reasoning": "该论文探讨小型语言模型如何提升大型语言模型的推理能力，这属于核心LLM技术进展。虽然推理能力本身是通用能力，但在推荐和搜索系统中，增强的推理能力可以用于更复杂的用户意图理解、多跳推理查询处理和上下文感知的推荐逻辑。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07958v1": {
    "title": "A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.07958v1",
    "arxiv_id": "2510.07958v1",
    "authors": "Fengji Zhang, Xinyao Niu, Chengyang Ying, Guancheng Lin, Zhongkai Hao, Zhou Fan, Chengen Huang, Jacky Keung, Bei Chen, Junyang Lin",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 08:53:31",
    "ori_summary": "Recent advances in Large Language Models (LLMs) and Reinforcement Learning (RL) have led to strong performance in open-domain question answering (QA). However, existing models still struggle with questions that admit multiple valid answers. Standard QA benchmarks, which typically assume a single gold answer, overlook this reality and thus produce inappropriate training signals. Existing attempts to handle ambiguity often rely on costly manual annotation, which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue. In this paper, we present A$^2$Search, an annotation-free, end-to-end training framework to recognize and handle ambiguity. At its core is an automated pipeline that detects ambiguous questions and gathers alternative answers via trajectory sampling and evidence verification. The model is then optimized with RL using a carefully designed $\\mathrm{AnsF1}$ reward, which naturally accommodates multiple answers. Experiments on eight open-domain QA benchmarks demonstrate that A$^2$Search achieves new state-of-the-art performance. With only a single rollout, A$^2$Search-7B yields an average $\\mathrm{AnsF1}@1$ score of $48.4\\%$ across four multi-hop benchmarks, outperforming all strong baselines, including the substantially larger ReSearch-32B ($46.2\\%$). Extensive analyses further show that A$^2$Search resolves ambiguity and generalizes across benchmarks, highlighting that embracing ambiguity is essential for building more reliable QA systems. Our code, data, and model weights can be found at https://github.com/zfj1998/A2Search",
    "summary": "",
    "translation": "A²Search：基于强化学习的歧义感知问答",
    "relevance_score": 2,
    "reasoning": "该论文虽然涉及搜索领域，但主要关注问答任务中的歧义处理和强化学习应用，这属于纯粹的NLP问答范畴。强化学习部分没有明确展示与推荐系统、搜索排序或广告相关的具体应用场景，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07940v1": {
    "title": "TTOM: Test-Time Optimization and Memorization for Compositional Video Generation",
    "url": "https://www.alphaxiv.org/abs/2510.07940v1",
    "arxiv_id": "2510.07940v1",
    "authors": "Leigang Qu, Ziyang Wang, Na Zheng, Wenjie Wang, Liqiang Nie, Tat-Seng Chua",
    "categories": "cs.CV, cs.AI, cs.CL, cs.LG, cs.MM",
    "pub_date": "2025-10-09 08:37:00",
    "ori_summary": "Video Foundation Models (VFMs) exhibit remarkable visual generation performance, but struggle in compositional scenarios (e.g., motion, numeracy, and spatial relation). In this work, we introduce Test-Time Optimization and Memorization (TTOM), a training-free framework that aligns VFM outputs with spatiotemporal layouts during inference for better text-image alignment. Rather than direct intervention to latents or attention per-sample in existing work, we integrate and optimize new parameters guided by a general layout-attention objective. Furthermore, we formulate video generation within a streaming setting, and maintain historical optimization contexts with a parametric memory mechanism that supports flexible operations, such as insert, read, update, and delete. Notably, we found that TTOM disentangles compositional world knowledge, showing powerful transferability and generalization. Experimental results on the T2V-CompBench and Vbench benchmarks establish TTOM as an effective, practical, scalable, and efficient framework to achieve cross-modal alignment for compositional video generation on the fly.",
    "summary": "",
    "translation": "TTOM：组合式视频生成的测试时优化与记忆机制",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频生成领域，属于纯粹的视觉内容生成任务，与推荐系统、搜索或广告的核心技术无关。论文标题明确指向视频生成中的组合优化问题，没有显示出任何在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07931v1": {
    "title": "Vision-Enabled LLMs in Historical Lexicography: Digitising and Enriching Estonian-German Dictionaries from the 17th and 18th Centuries",
    "url": "https://www.alphaxiv.org/abs/2510.07931v1",
    "arxiv_id": "2510.07931v1",
    "authors": "Madis Jürviste, Joonatan Jakobson",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 08:29:22",
    "ori_summary": "This article presents research conducted at the Institute of the Estonian Language between 2022 and 2025 on the application of large language models (LLMs) to the study of 17th and 18th century Estonian dictionaries. The authors address three main areas: enriching historical dictionaries with modern word forms and meanings; using vision-enabled LLMs to perform text recognition on sources printed in Gothic script (Fraktur); and preparing for the creation of a unified, cross-source dataset. Initial experiments with J. Gutslaff's 1648 dictionary indicate that LLMs have significant potential for semi-automatic enrichment of dictionary information. When provided with sufficient context, Claude 3.7 Sonnet accurately provided meanings and modern equivalents for 81% of headword entries. In a text recognition experiment with A. T. Helle's 1732 dictionary, a zero-shot method successfully identified and structured 41% of headword entries into error-free JSON-formatted output. For digitising the Estonian-German dictionary section of A. W. Hupel's 1780 grammar, overlapping tiling of scanned image files is employed, with one LLM being used for text recognition and a second for merging the structured output. These findings demonstrate that even for minor languages LLMs have a significant potential for saving time and financial resources.",
    "summary": "",
    "translation": "基于视觉能力的LLM在历史词典学中的应用：数字化和丰富17-18世纪爱沙尼亚-德语词典",
    "relevance_score": 1,
    "reasoning": "该论文主要涉及历史文档的数字保存和语言学研究，属于特定领域应用。虽然提到了视觉能力的LLM，但其应用场景（历史词典学）与推荐系统、搜索或广告的核心技术发展没有直接关联，也不涉及Transformer架构改进或异构数据处理等关键技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07926v1": {
    "title": "Comprehensiveness Metrics for Automatic Evaluation of Factual Recall in Text Generation",
    "url": "https://www.alphaxiv.org/abs/2510.07926v1",
    "arxiv_id": "2510.07926v1",
    "authors": "Adam Dejl, James Barry, Alessandra Pascale, Javier Carnerero Cano",
    "categories": "cs.CL, I.2.7",
    "pub_date": "2025-10-09 08:22:24",
    "ori_summary": "Despite demonstrating remarkable performance across a wide range of tasks, large language models (LLMs) have also been found to frequently produce outputs that are incomplete or selectively omit key information. In sensitive domains, such omissions can result in significant harm comparable to that posed by factual inaccuracies, including hallucinations. In this study, we address the challenge of evaluating the comprehensiveness of LLM-generated texts, focusing on the detection of missing information or underrepresented viewpoints. We investigate three automated evaluation strategies: (1) an NLI-based method that decomposes texts into atomic statements and uses natural language inference (NLI) to identify missing links, (2) a Q&A-based approach that extracts question-answer pairs and compares responses across sources, and (3) an end-to-end method that directly identifies missing content using LLMs. Our experiments demonstrate the surprising effectiveness of the simple end-to-end approach compared to more complex methods, though at the cost of reduced robustness, interpretability and result granularity. We further assess the comprehensiveness of responses from several popular open-weight LLMs when answering user queries based on multiple sources.",
    "summary": "",
    "translation": "文本生成中事实召回自动评估的全面性度量",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文本生成中事实召回的评估指标，属于纯粹的NLP评估基准范畴。虽然评估指标在理论上可能对搜索系统中的事实准确性有间接参考价值，但该研究缺乏与推荐系统、广告或搜索排名的直接关联，且不属于核心领域进展或使能技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07923v1": {
    "title": "STEPER: Step-wise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07923v1",
    "arxiv_id": "2510.07923v1",
    "authors": "Kyumin Lee, Minjin Jeon, Sanghwan Jang, Hwanjo Yu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 08:20:27",
    "ori_summary": "Answering complex real-world questions requires step-by-step retrieval and integration of relevant information to generate well-grounded responses. However, existing knowledge distillation methods overlook the need for different reasoning abilities at different steps, hindering transfer in multi-step retrieval-augmented frameworks. To address this, we propose Stepwise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models (StepER). StepER employs step-wise supervision to align with evolving information and reasoning demands across stages. Additionally, it incorporates difficulty-aware training to progressively optimize learning by prioritizing suitable steps. Our method is adaptable to various multi-step retrieval-augmented language models, including those that use retrieval queries for reasoning paths or decomposed questions. Extensive experiments show that StepER outperforms prior methods on multi-hop QA benchmarks, with an 8B model achieving performance comparable to a 70B teacher model.",
    "summary": "该论文研究多步检索增强语言模型中推理能力的提升问题，核心思想是通过分步监督和难度感知训练，针对不同推理阶段的信息需求和能力差异进行知识蒸馏。",
    "translation": "STEPER：通过分步知识蒸馏增强多步检索增强语言模型推理能力",
    "relevance_score": 8,
    "reasoning": "该论文涉及检索增强语言模型(RAG)的推理能力增强，直接关联搜索系统中的多步检索和推理任务。知识蒸馏技术作为使能技术，可以提升检索增强模型在复杂搜索场景下的性能表现，对于构建更智能的搜索系统具有直接应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文针对多步检索增强语言模型提出分步知识蒸馏方法，直接关联LLM在复杂推理任务中的应用，与推荐和搜索系统中的多步推理需求高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07912v1": {
    "title": "Towards Human-Like Grading: A Unified LLM-Enhanced Framework for Subjective Question Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.07912v1",
    "arxiv_id": "2510.07912v1",
    "authors": "Fanwei Zhua, Jiaxuan He, Xiaoxiao Chen, Zulong Chen, Quan Lu, Chenrui Mei",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 08:05:39",
    "ori_summary": "Automatic grading of subjective questions remains a significant challenge in examination assessment due to the diversity in question formats and the open-ended nature of student responses. Existing works primarily focus on a specific type of subjective question and lack the generality to support comprehensive exams that contain diverse question types. In this paper, we propose a unified Large Language Model (LLM)-enhanced auto-grading framework that provides human-like evaluation for all types of subjective questions across various domains. Our framework integrates four complementary modules to holistically evaluate student answers. In addition to a basic text matching module that provides a foundational assessment of content similarity, we leverage the powerful reasoning and generative capabilities of LLMs to: (1) compare key knowledge points extracted from both student and reference answers, (2) generate a pseudo-question from the student answer to assess its relevance to the original question, and (3) simulate human evaluation by identifying content-related and non-content strengths and weaknesses. Extensive experiments on both general-purpose and domain-specific datasets show that our framework consistently outperforms traditional and LLM-based baselines across multiple grading metrics. Moreover, the proposed system has been successfully deployed in real-world training and certification exams at a major e-commerce enterprise.",
    "summary": "",
    "translation": "迈向类人评分：用于主观问题评估的统一LLM增强框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注教育领域的自动评分系统，属于LLM在特定垂直领域的应用。虽然涉及LLM技术，但其核心应用场景（教育评分）与推荐系统、搜索或广告领域没有直接关联，也不涉及这些领域的特定挑战或数据模态。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07896v1": {
    "title": "ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall",
    "url": "https://www.alphaxiv.org/abs/2510.07896v1",
    "arxiv_id": "2510.07896v1",
    "authors": "Jiayu Yang, Yuxuan Fan, Songning Lai, Shengen Wu, Jiaqi Tang, Chun Kang, Zhijiang Guo, Yutao Yue",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 07:46:08",
    "ori_summary": "Large Language Models (LLMs) require efficient knowledge editing (KE) to update factual information, yet existing methods exhibit significant performance decay in multi-hop factual recall. This failure is particularly acute when edits involve intermediate implicit subjects within reasoning chains. Through causal analysis, we reveal that this limitation stems from an oversight of how chained knowledge is dynamically represented and utilized at the neuron level. We discover that during multi hop reasoning, implicit subjects function as query neurons, which sequentially activate corresponding value neurons across transformer layers to accumulate information toward the final answer, a dynamic prior KE work has overlooked. Guided by this insight, we propose ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall, a framework that leverages neuron-level attribution to identify and edit these critical query-value (Q-V) pathways. ACE provides a mechanistically grounded solution for multi-hop KE, empirically outperforming state-of-the-art methods by 9.44% on GPT-J and 37.46% on Qwen3-8B. Our analysis further reveals more fine-grained activation patterns in Qwen3 and demonstrates that the semantic interpretability of value neurons is orchestrated by query-driven accumulation. These findings establish a new pathway for advancing KE capabilities based on the principled understanding of internal reasoning mechanisms.",
    "summary": "",
    "translation": "ACE：面向多跳事实回忆的属性控制知识编辑",
    "relevance_score": 2,
    "reasoning": "该论文主要关注知识编辑和事实回忆，属于LLM内部知识管理范畴，与推荐系统、搜索或广告的核心技术关联较弱。虽然知识编辑技术理论上可能用于更新推荐系统中的实体知识，但论文标题明确聚焦于多跳事实回忆这一特定NLP任务，缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07892v1": {
    "title": "Metric Calculating Benchmark: Code-Verifiable Complicate Instruction Following Benchmark for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07892v1",
    "arxiv_id": "2510.07892v1",
    "authors": "Hyeonseok Moon, Seongtae Hong, Jaehyung Seo, Heuiseok Lim",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 07:43:15",
    "ori_summary": "Recent frontier-level LLMs have saturated many previously difficult benchmarks, leaving little room for further differentiation. This progress highlights the need for challenging benchmarks that provide objective verification. In this paper, we introduce MCBench, a benchmark designed to evaluate whether LLMs can execute string-matching NLP metrics by strictly following step-by-step instructions. Unlike prior benchmarks that depend on subjective judgments or general reasoning, MCBench offers an objective, deterministic and codeverifiable evaluation. This setup allows us to systematically test whether LLMs can maintain accurate step-by-step execution, including instruction adherence, numerical computation, and long-range consistency in handling intermediate results. To ensure objective evaluation of these abilities, we provide a parallel reference code that can evaluate the accuracy of LLM output. We provide three evaluative metrics and three benchmark variants designed to measure the detailed instruction understanding capability of LLMs. Our analyses show that MCBench serves as an effective and objective tool for evaluating the capabilities of cutting-edge LLMs.",
    "summary": "",
    "translation": "指标计算基准：面向大型语言模型的代码可验证复杂指令遵循基准",
    "relevance_score": 2,
    "reasoning": "该论文专注于LLM的基准测试和评估方法，属于纯粹的评估基准研究，与我的核心关注点（推荐系统、搜索广告领域的核心进展、使能技术及应用）无关。虽然涉及复杂指令遵循，但主要关注代码验证和基准构建，没有明确的推荐/搜索/广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07890v1": {
    "title": "Standard-to-Dialect Transfer Trends Differ across Text and Speech: A Case Study on Intent and Topic Classification in German Dialects",
    "url": "https://www.alphaxiv.org/abs/2510.07890v1",
    "arxiv_id": "2510.07890v1",
    "authors": "Verena Blaschke, Miriam Winkler, Barbara Plank",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 07:43:08",
    "ori_summary": "Research on cross-dialectal transfer from a standard to a non-standard dialect variety has typically focused on text data. However, dialects are primarily spoken, and non-standard spellings are known to cause issues in text processing. We compare standard-to-dialect transfer in three settings: text models, speech models, and cascaded systems where speech first gets automatically transcribed and then further processed by a text model. In our experiments, we focus on German and multiple German dialects in the context of written and spoken intent and topic classification. To that end, we release the first dialectal audio intent classification dataset. We find that the speech-only setup provides the best results on the dialect data while the text-only setup works best on the standard data. While the cascaded systems lag behind the text-only models for German, they perform relatively well on the dialectal data if the transcription system generates normalized, standard-like output.",
    "summary": "",
    "translation": "标准语到方言的迁移趋势在文本和语音中存在差异：德语方言意图与主题分类案例研究",
    "relevance_score": 2,
    "reasoning": "该论文主要研究方言处理中的迁移学习差异，属于特定语言处理领域。虽然意图分类在搜索和推荐中有应用，但论文聚焦于德语方言这一狭窄领域，且未涉及LLM、Transformer架构或推荐系统的核心进展，与当前关注点的直接关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07884v1": {
    "title": "Contrastive Weak-to-strong Generalization",
    "url": "https://www.alphaxiv.org/abs/2510.07884v1",
    "arxiv_id": "2510.07884v1",
    "authors": "Houcheng Jiang, Junfeng Fang, Jiaxin Wu, Tianyu Zhang, Chen Gao, Yong Li, Xiang Wang, Xiangnan He, Yang Deng",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 07:37:23",
    "ori_summary": "Weak-to-strong generalization provides a promising paradigm for scaling large language models (LLMs) by training stronger models on samples from aligned weaker ones, without requiring human feedback or explicit reward modeling. However, its robustness and generalization are hindered by the noise and biases in weak-model outputs, which limit its applicability in practice. To address this challenge, we leverage implicit rewards, which approximate explicit rewards through log-likelihood ratios, and reveal their structural equivalence with Contrastive Decoding (CD), a decoding strategy shown to reduce noise in LLM generation. Building on this connection, we propose Contrastive Weak-to-Strong Generalization (ConG), a framework that employs contrastive decoding between pre- and post-alignment weak models to generate higher-quality samples. This approach enables more reliable capability transfer, denoising, and improved robustness, substantially mitigating the limitations of traditional weak-to-strong methods. Empirical results across different model families confirm consistent improvements, demonstrating the generality and effectiveness of ConG. Taken together, our findings highlight the potential of ConG to advance weak-to-strong generalization and provide a promising pathway toward AGI.",
    "summary": "",
    "translation": "对比式弱到强泛化",
    "relevance_score": 6,
    "reasoning": "该论文关注对比学习和弱到强泛化，这是LLM训练和知识蒸馏中的核心技术进步。在推荐系统和搜索领域，这种技术可以用于从大型教师模型向更高效的学生模型进行知识迁移，或者处理用户反馈中的弱监督信号，从而提高模型泛化能力和部署效率。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07881v1": {
    "title": "CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for Mandarin-English Code-Switching",
    "url": "https://www.alphaxiv.org/abs/2510.07881v1",
    "arxiv_id": "2510.07881v1",
    "authors": "Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 07:34:23",
    "ori_summary": "The advancement of multimodal large language models has accelerated the development of speech-to-speech interaction systems. While natural monolingual interaction has been achieved, we find existing models exhibit deficiencies in language alignment. In our proposed Code-Switching Speech-to-Speech Benchmark (CS3-Bench), experiments on 7 mainstream models demonstrate a relative performance drop of up to 66% in knowledge-intensive question answering and varying degrees of misunderstanding in open-ended conversations. Starting from a model with severe performance deterioration, we propose both data constructions and training approaches to improve the language alignment capabilities, specifically employing Chain of Recognition (CoR) to enhance understanding and Keyword Highlighting (KH) to guide generation. Our approach improves the knowledge accuracy from 25.14% to 46.13%, with open-ended understanding rate from 64.5% to 86.5%, and significantly reduces pronunciation errors in the secondary language. CS3-Bench is available at https://huggingface.co/datasets/VocalNet/CS3-Bench.",
    "summary": "",
    "translation": "CS3-Bench：评估和增强普通话-英语语码转换的语音到语音大语言模型",
    "relevance_score": 2,
    "reasoning": "该论文专注于语音到语音的语码转换技术，属于语音处理领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然涉及LLM技术，但其应用场景（语音处理、语码转换）在RecSys/Search/Ads中缺乏明确的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07880v1": {
    "title": "Do LLMs Really Need 10+ Thoughts for \"Find the Time 1000 Days Later\"? Towards Structural Understanding of LLM Overthinking",
    "url": "https://www.alphaxiv.org/abs/2510.07880v1",
    "arxiv_id": "2510.07880v1",
    "authors": "Xinliang Frederick Zhang, Anhad Mohananey, Alexandra Chronopoulou, Pinelopi Papalampidi, Somit Gupta, Tsendsuren Munkhdalai, Lu Wang, Shyam Upadhyay",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 07:33:25",
    "ori_summary": "Models employing long chain-of-thought (CoT) reasoning have shown superior performance on complex reasoning tasks. Yet, this capability introduces a critical and often overlooked inefficiency -- overthinking -- models often engage in unnecessarily extensive reasoning even for simple queries, incurring significant computations without accuracy improvements. While prior work has explored solutions to mitigate overthinking, a fundamental gap remains in our understanding of its underlying causes. Most existing analyses are limited to superficial, profiling-based observations, failing to delve into LLMs' inner workings. This study introduces a systematic, fine-grained analyzer of LLMs' thought process to bridge the gap, TRACE. We first benchmark the overthinking issue, confirming that long-thinking models are five to twenty times slower on simple tasks with no substantial gains. We then use TRACE to first decompose the thought process into minimally complete sub-thoughts. Next, by inferring discourse relationships among sub-thoughts, we construct granular thought progression graphs and subsequently identify common thinking patterns for topically similar queries. Our analysis reveals two major patterns for open-weight thinking models -- Explorer and Late Landing. This finding provides evidence that over-verification and over-exploration are the primary drivers of overthinking in LLMs. Grounded in thought structures, we propose a utility-based definition of overthinking, which moves beyond length-based metrics. This revised definition offers a more insightful understanding of LLMs' thought progression, as well as practical guidelines for principled overthinking management.",
    "summary": "",
    "translation": "LLM真的需要10+次思考来“找出1000天后的时间”吗？——迈向对LLM过度思考的结构化理解",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLM的推理过程和过度思考现象，这属于LLM内部工作机制分析。虽然涉及LLM技术，但焦点是推理效率而非在推荐系统、搜索或广告中的具体应用。论文没有展示明确的RecSys/Search/Ads应用潜力，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07877v1": {
    "title": "Ready to Translate, Not to Represent? Bias and Performance Gaps in Multilingual LLMs Across Language Families and Domains",
    "url": "https://www.alphaxiv.org/abs/2510.07877v1",
    "arxiv_id": "2510.07877v1",
    "authors": "Md. Faiyaz Abdullah Sayeedi, Md. Mahbub Alam, Subhey Sadi Rahman, Md. Adnanul Islam, Jannatul Ferdous Deepti, Tasnim Mohiuddin, Md Mofijul Islam, Swakkhar Shatabda",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 07:28:30",
    "ori_summary": "The rise of Large Language Models (LLMs) has redefined Machine Translation (MT), enabling context-aware and fluent translations across hundreds of languages and textual domains. Despite their remarkable capabilities, LLMs often exhibit uneven performance across language families and specialized domains. Moreover, recent evidence reveals that these models can encode and amplify different biases present in their training data, posing serious concerns for fairness, especially in low-resource languages. To address these gaps, we introduce Translation Tangles, a unified framework and dataset for evaluating the translation quality and fairness of open-source LLMs. Our approach benchmarks 24 bidirectional language pairs across multiple domains using different metrics. We further propose a hybrid bias detection pipeline that integrates rule-based heuristics, semantic similarity filtering, and LLM-based validation. We also introduce a high-quality, bias-annotated dataset based on human evaluations of 1,439 translation-reference pairs. The code and dataset are accessible on GitHub: https://github.com/faiyazabdullah/TranslationTangles",
    "summary": "",
    "translation": "准备翻译而非表征？多语言大语言模型在语言家族和领域间的偏见与性能差距",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多语言LLM的偏见和性能差距问题，这属于纯粹的NLP评估和基准测试范畴。虽然多语言能力在理论上可能对国际化推荐/搜索系统有影响，但论文焦点是语言家族间的性能差异和翻译偏差，而非直接应用于推荐系统、搜索或广告的技术进步。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07842v1": {
    "title": "AdaSwitch: Adaptive Switching Generation for Knowledge Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.07842v1",
    "arxiv_id": "2510.07842v1",
    "authors": "Jingyu Peng, Maolin Wang, Hengyi Cai, Yuchen Li, Kai Zhang, Shuaiqiang Wang, Dawei Yin, Xiangyu Zhao",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 06:38:37",
    "ori_summary": "Small language models (SLMs) are crucial for applications with strict latency and computational constraints, yet achieving high performance remains challenging. Knowledge distillation (KD) can transfer capabilities from large teacher models, but existing methods involve trade-offs: off-policy distillation provides high-quality supervision but introduces a training-inference mismatch, while on-policy approaches maintain consistency but rely on low-quality student outputs. To address these issues, we propose AdaSwitch, a novel approach that dynamically combines on-policy and off-policy generation at the token level. AdaSwitch allows the student to first explore its own predictions and then selectively integrate teacher guidance based on real-time quality assessment. This approach simultaneously preserves consistency and maintains supervision quality. Experiments on three datasets with two teacher-student LLM pairs demonstrate that AdaSwitch consistently improves accuracy, offering a practical and effective method for distilling SLMs with acceptable additional overhead.",
    "summary": "",
    "translation": "AdaSwitch：用于知识蒸馏的自适应切换生成",
    "relevance_score": 4,
    "reasoning": "该论文关注知识蒸馏中的自适应切换生成技术，属于模型压缩和效率优化范畴。虽然知识蒸馏本身是LLM领域的重要技术，可以应用于推荐系统或搜索中的模型部署效率提升，但论文标题未明确指向RecSys/Search/Ads的具体应用场景，因此相关性中等。这种自适应切换机制有潜力应用于推荐系统中的多专家模型或搜索中的查询自适应处理。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07841v1": {
    "title": "Self-Improving LLM Agents at Test-Time",
    "url": "https://www.alphaxiv.org/abs/2510.07841v1",
    "arxiv_id": "2510.07841v1",
    "authors": "Emre Can Acikgoz, Cheng Qian, Heng Ji, Dilek Hakkani-Tür, Gokhan Tur",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-09 06:37:35",
    "ori_summary": "One paradigm of language model (LM) fine-tuning relies on creating large training datasets, under the assumption that high quantity and diversity will enable models to generalize to novel tasks after post-training. In practice, gathering large sets of data is inefficient, and training on them is prohibitively expensive; worse, there is no guarantee that the resulting model will handle complex scenarios or generalize better. Moreover, existing techniques rarely assess whether a training sample provides novel information or is redundant with the knowledge already acquired by the model, resulting in unnecessary costs. In this work, we explore a new test-time self-improvement method to create more effective and generalizable agentic LMs on-the-fly. The proposed algorithm can be summarized in three steps: (i) first it identifies the samples that model struggles with (self-awareness), (ii) then generates similar examples from detected uncertain samples (self-data augmentation), and (iii) uses these newly generated samples at test-time fine-tuning (self-improvement). We study two variants of this approach: Test-Time Self-Improvement (TT-SI), where the same model generates additional training examples from its own uncertain cases and then learns from them, and contrast this approach with Test-Time Distillation (TT-D), where a stronger model generates similar examples for uncertain cases, enabling student to adapt using distilled supervision. Empirical evaluations across different agent benchmarks demonstrate that TT-SI improves the performance with +5.48% absolute accuracy gain on average across all benchmarks and surpasses other standard learning methods, yet using 68x less training samples. Our findings highlight the promise of TT-SI, demonstrating the potential of self-improvement algorithms at test-time as a new paradigm for building more capable agents toward self-evolution.",
    "summary": "该论文研究如何解决传统大模型微调依赖海量训练数据且效率低下的问题，核心思想是让模型在测试时通过识别自身困难样本、生成类似示例并进行在线微调来实现自我改进。",
    "translation": "测试时自改进的大语言模型智能体",
    "relevance_score": 8,
    "reasoning": "该论文属于'直接LLM应用'类别，探索LLM智能体在测试时的自改进能力，这对于推荐系统、搜索和广告中的在线学习和实时优化具有直接应用价值。这种能力可以用于动态调整推荐策略、优化搜索排序或改进广告投放效果，通过实时反馈循环持续提升系统性能。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的测试时自改进方法直接应用于LLM智能体，属于LLM直接应用和使能技术范畴，对推荐和搜索系统的在线学习有重要参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.07835v1": {
    "title": "MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation",
    "url": "https://www.alphaxiv.org/abs/2510.07835v1",
    "arxiv_id": "2510.07835v1",
    "authors": "Weisen Jiang, Sinno Jialin Pan",
    "categories": "cs.LG, cs.AI, cs.CL, cs.CR",
    "pub_date": "2025-10-09 06:27:34",
    "ori_summary": "This paper introduces MetaDefense, a novel framework for defending against finetuning-based jailbreak attacks in large language models (LLMs). We observe that existing defense mechanisms fail to generalize to harmful queries disguised by unseen attack templates, despite LLMs being capable of distinguishing disguised harmful queries in the embedding space. Based on these insights, we propose a two-stage defense approach: (i) pre-generation defense that detects harmful queries before response generation begins, and (ii) mid-generation defense that monitors partial responses during generation to prevent outputting more harmful content. Our MetaDefense trains the LLM to predict the harmfulness of both queries and partial responses using specialized prompts, enabling early termination of potentially harmful interactions. Extensive experiments across multiple LLM architectures (LLaMA-2-7B, Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense significantly outperforms existing defense mechanisms, achieving robust defense against harmful queries with seen and unseen attack templates while maintaining competitive performance on benign tasks. Code is available at https://github.com/ws-jiang/MetaDefense.",
    "summary": "",
    "translation": "MetaDefense：在生成前后防御基于微调的越狱攻击",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM安全防御技术，特别是对抗越狱攻击的方法，这属于安全/隐私范畴，被明确列为无关主题。虽然涉及LLM技术，但其核心关注点是防御机制而非推荐系统、搜索或广告的应用潜力，与当前关注的领域进展、使能技术或直接应用无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07821v1": {
    "title": "From Keywords to Clusters: AI-Driven Analysis of YouTube Comments to Reveal Election Issue Salience in 2024",
    "url": "https://www.alphaxiv.org/abs/2510.07821v1",
    "arxiv_id": "2510.07821v1",
    "authors": "Raisa M. Simoes, Timoteo Kelly, Eduardo J. Simoes, Praveen Rao",
    "categories": "cs.SI, cs.CL",
    "pub_date": "2025-10-09 06:02:10",
    "ori_summary": "This paper aims to explore two competing data science methodologies to attempt answering the question, \"Which issues contributed most to voters' choice in the 2024 presidential election?\" The methodologies involve novel empirical evidence driven by artificial intelligence (AI) techniques. By using two distinct methods based on natural language processing and clustering analysis to mine over eight thousand user comments on election-related YouTube videos from one right leaning journal, Wall Street Journal, and one left leaning journal, New York Times, during pre-election week, we quantify the frequency of selected issue areas among user comments to infer which issues were most salient to potential voters in the seven days preceding the November 5th election. Empirically, we primarily demonstrate that immigration and democracy were the most frequently and consistently invoked issues in user comments on the analyzed YouTube videos, followed by the issue of identity politics, while inflation was significantly less frequently referenced. These results corroborate certain findings of post-election surveys but also refute the supposed importance of inflation as an election issue. This indicates that variations on opinion mining, with their analysis of raw user data online, can be more revealing than polling and surveys for analyzing election outcomes.",
    "summary": "",
    "translation": "从关键词到聚类：基于人工智能的YouTube评论分析揭示2024年选举议题显著性",
    "relevance_score": 3,
    "reasoning": "虽然论文涉及AI驱动的文本分析，但其焦点是政治选举议题分析而非推荐系统、搜索或广告的核心技术。该方法可能间接应用于内容理解，但缺乏对RecSys/Search/Ads架构、Transformer改进或LLM应用的直接贡献。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07812v1": {
    "title": "Multilingual Generative Retrieval via Cross-lingual Semantic Compression",
    "url": "https://www.alphaxiv.org/abs/2510.07812v1",
    "arxiv_id": "2510.07812v1",
    "authors": "Yuxin Huang, Simeng Wu, Ran Song, Yan Xiang, Yantuan Xian, Shengxiang Gao, Zhengtao Yu",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 05:42:57",
    "ori_summary": "Generative Information Retrieval is an emerging retrieval paradigm that exhibits remarkable performance in monolingual scenarios.However, applying these methods to multilingual retrieval still encounters two primary challenges, cross-lingual identifier misalignment and identifier inflation. To address these limitations, we propose Multilingual Generative Retrieval via Cross-lingual Semantic Compression (MGR-CSC), a novel framework that unifies semantically equivalent multilingual keywords into shared atoms to align semantics and compresses the identifier space, and we propose a dynamic multi-step constrained decoding strategy during retrieval. MGR-CSC improves cross-lingual alignment by assigning consistent identifiers and enhances decoding efficiency by reducing redundancy. Experiments demonstrate that MGR-CSC achieves outstanding retrieval accuracy, improving by 6.83% on mMarco100k and 4.77% on mNQ320k, while reducing document identifiers length by 74.51% and 78.2%, respectively.",
    "summary": "",
    "translation": "基于跨语言语义压缩的多语言生成式检索",
    "relevance_score": 8,
    "reasoning": "该论文涉及生成式检索技术，这是搜索系统中的核心进展。跨语言语义压缩技术可以显著提升多语言搜索系统的效率和效果，直接应用于多语言搜索场景。这种压缩方法也有潜力应用于推荐系统中的多语言内容理解和用户兴趣建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07799v1": {
    "title": "Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2510.07799v1",
    "arxiv_id": "2510.07799v1",
    "authors": "Eric Hanchen Jiang, Guancheng Wan, Sophia Yin, Mengting Li, Yuchen Wu, Xiao Liang, Xinfeng Li, Yizhou Sun, Wei Wang, Kai-Wei Chang, Ying Nian Wu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 05:28:28",
    "ori_summary": "The efficiency of multi-agent systems driven by large language models (LLMs) largely hinges on their communication topology. However, designing an optimal topology is a non-trivial challenge, as it requires balancing competing objectives such as task performance, communication cost, and robustness. Existing frameworks often rely on static or hand-crafted topologies, which inherently fail to adapt to diverse task requirements, leading to either excessive token consumption for simple problems or performance bottlenecks for complex ones. To address this challenge, we introduce a novel generative framework called \\textit{Guided Topology Diffusion (GTD)}. Inspired by conditional discrete graph diffusion models, GTD formulates topology synthesis as an iterative construction process. At each step, the generation is steered by a lightweight proxy model that predicts multi-objective rewards (e.g., accuracy, utility, cost), enabling real-time, gradient-free optimization towards task-adaptive topologies. This iterative, guided synthesis process distinguishes GTD from single-step generative frameworks, enabling it to better navigate complex design trade-offs. We validated GTD across multiple benchmarks, and experiments show that this framework can generate highly task-adaptive, sparse, and efficient communication topologies, significantly outperforming existing methods in LLM agent collaboration.",
    "summary": "",
    "translation": "基于图扩散模型动态生成多LLM智能体通信拓扑",
    "relevance_score": 8,
    "reasoning": "该论文涉及多LLM智能体系统，这属于'直接LLM应用'范畴，在推荐系统和搜索中可用于构建协同推理框架。图扩散模型用于动态优化通信拓扑，这种技术可应用于多智能体推荐系统中协调不同LLM专家的交互模式，提升整体系统性能。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07794v1": {
    "title": "HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2510.07794v1",
    "arxiv_id": "2510.07794v1",
    "authors": "Peilin Wu, Mian Zhang, Kun Wan, Wentian Zhao, Kaiyu He, Xinya Du, Zhiyu Chen",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-09 05:13:10",
    "ori_summary": "Agentic RAG is a powerful technique for incorporating external information that LLMs lack, enabling better problem solving and question answering. However, suboptimal search behaviors exist widely, such as over-search (retrieving information already known) and under-search (failing to search when necessary), which leads to unnecessary overhead and unreliable outputs. Current training methods, which typically rely on outcome-based rewards in a RL framework, lack the fine-grained control needed to address these inefficiencies. To overcome this, we introduce Hierarchical Process Rewards for Efficient agentic RAG (HiPRAG), a training methodology that incorporates a fine-grained, knowledge-grounded process reward into the RL training. Our approach evaluates the necessity of each search decision on-the-fly by decomposing the agent's reasoning trajectory into discrete, parsable steps. We then apply a hierarchical reward function that provides an additional bonus based on the proportion of optimal search and non-search steps, on top of commonly used outcome and format rewards. Experiments on the Qwen2.5 and Llama-3.2 models across seven diverse QA benchmarks show that our method achieves average accuracies of 65.4% (3B) and 67.2% (7B). This is accomplished while improving search efficiency, reducing the over-search rate to just 2.3% and concurrently lowering the under-search rate. These results demonstrate the efficacy of optimizing the reasoning process itself, not just the final outcome. Further experiments and analysis demonstrate that HiPRAG shows good generalizability across a wide range of RL algorithms, model families, sizes, and types. This work demonstrates the importance and potential of fine-grained control through RL, for improving the efficiency and optimality of reasoning for search agents.",
    "summary": "",
    "translation": "HiPRAG：用于高效代理检索增强生成的分层过程奖励",
    "relevance_score": 8,
    "reasoning": "该论文涉及检索增强生成(RAG)的代理方法，这直接适用于搜索系统，其中代理可以主动检索和整合信息以响应用户查询。分层过程奖励机制通过优化检索和生成过程，可以显著提升搜索和推荐系统中的内容质量和效率，减少不相关信息的检索。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07793v1": {
    "title": "LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology",
    "url": "https://www.alphaxiv.org/abs/2510.07793v1",
    "arxiv_id": "2510.07793v1",
    "authors": "Sajib Acharjee Dip, Adrika Zafor, Bikash Kumar Paul, Uddip Acharjee Shuvo, Muhit Islam Emon, Xuan Wang, Liqing Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 05:12:09",
    "ori_summary": "Large language models (LLMs) and emerging agentic frameworks are beginning to transform single-cell biology by enabling natural-language reasoning, generative annotation, and multimodal data integration. However, progress remains fragmented across data modalities, architectures, and evaluation standards. LLM4Cell presents the first unified survey of 58 foundation and agentic models developed for single-cell research, spanning RNA, ATAC, multi-omic, and spatial modalities. We categorize these methods into five families-foundation, text-bridge, spatial, multimodal, epigenomic, and agentic-and map them to eight key analytical tasks including annotation, trajectory and perturbation modeling, and drug-response prediction. Drawing on over 40 public datasets, we analyze benchmark suitability, data diversity, and ethical or scalability constraints, and evaluate models across 10 domain dimensions covering biological grounding, multi-omics alignment, fairness, privacy, and explainability. By linking datasets, models, and evaluation domains, LLM4Cell provides the first integrated view of language-driven single-cell intelligence and outlines open challenges in interpretability, standardization, and trustworthy model development.",
    "summary": "",
    "translation": "LLM4Cell：面向单细胞生物学的大语言与智能体模型综述",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于单细胞生物学这一生物医学领域，属于明确的无关主题范畴。标题中虽提及大语言模型，但其应用场景完全限定在生物学领域，与推荐系统、搜索或广告没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07782v1": {
    "title": "RCPU: Rotation-Constrained Error Compensation for Structured Pruning of a Large Language Model",
    "url": "https://www.alphaxiv.org/abs/2510.07782v1",
    "arxiv_id": "2510.07782v1",
    "authors": "Shuichiro Haruta, Kazunori Matsumoto, Zhi Li, Yanan Wang, Mori Kurokawa",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 04:54:09",
    "ori_summary": "In this paper, we propose a rotation-constrained compensation method to address the errors introduced by structured pruning of large language models (LLMs). LLMs are trained on massive datasets and accumulate rich semantic knowledge in their representation space. In contrast, pruning is typically carried out with only a small amount of calibration data, which makes output mismatches unavoidable. Although direct least-squares fitting can reduce such errors, it tends to overfit to the limited calibration set, destructively modifying pretrained weights. To overcome this difficulty, we update the pruned parameters under a rotation constraint. This constrained update preserves the geometry of output representations (i.e., norms and inner products) and simultaneously re-aligns the pruned subspace with the original outputs. Furthermore, in rotation-constrained compensation, removing components that strongly contribute to the principal directions of the output makes error recovery difficult. Since input dimensions with large variance strongly affect these principal directions, we design a variance-aware importance score that ensures such dimensions are preferentially kept in the pruned model. By combining this scoring rule with rotation-constrained updates, the proposed method effectively compensates errors while retaining the components likely to be more important in a geometry-preserving manner. In the experiments, we apply the proposed method to LLaMA-7B and evaluate it on WikiText-2 and multiple language understanding benchmarks. The results demonstrate consistently better perplexity and task accuracy compared with existing baselines.",
    "summary": "",
    "translation": "RCPU：大语言模型结构化剪枝的旋转约束误差补偿",
    "relevance_score": 7,
    "reasoning": "该论文涉及大语言模型的高效压缩技术，属于'使能LLM技术'范畴。结构化剪枝技术可以显著减少LLM的计算和内存需求，这对于在推荐系统、搜索和广告中部署大型模型至关重要，能够实现更快的推理速度和更低的部署成本。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07777v1": {
    "title": "Drift No More? Context Equilibria in Multi-Turn LLM Interactions",
    "url": "https://www.alphaxiv.org/abs/2510.07777v1",
    "arxiv_id": "2510.07777v1",
    "authors": "Vardhan Dongre, Ryan A. Rossi, Viet Dac Lai, David Seunghyun Yoon, Dilek Hakkani-Tür, Trung Bui",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 04:48:49",
    "ori_summary": "Large Language Models (LLMs) excel at single-turn tasks such as instruction following and summarization, yet real-world deployments require sustained multi-turn interactions where user goals and conversational context persist and evolve. A recurring challenge in this setting is context drift: the gradual divergence of a model's outputs from goal-consistent behavior across turns. Unlike single-turn errors, drift unfolds temporally and is poorly captured by static evaluation metrics. In this work, we present a study of context drift in multi-turn interactions and propose a simple dynamical framework to interpret its behavior. We formalize drift as the turn-wise KL divergence between the token-level predictive distributions of the test model and a goal-consistent reference model, and propose a recurrence model that interprets its evolution as a bounded stochastic process with restoring forces and controllable interventions. We instantiate this framework in both synthetic long-horizon rewriting tasks and realistic user-agent simulations such as in $\\tau$-Bench, measuring drift for several open-weight LLMs that are used as user simulators. Our experiments consistently reveal stable, noise-limited equilibria rather than runaway degradation, and demonstrate that simple reminder interventions reliably reduce divergence in line with theoretical predictions. Together, these results suggest that multi-turn drift can be understood as a controllable equilibrium phenomenon rather than as inevitable decay, providing a foundation for studying and mitigating context drift in extended interactions.",
    "summary": "",
    "translation": "不再漂移？多轮大语言模型交互中的上下文均衡",
    "relevance_score": 7,
    "reasoning": "该论文研究多轮LLM交互中的上下文均衡问题，直接关联到LLM在推荐系统和搜索中的实际应用场景。在多轮对话推荐和搜索交互中，上下文漂移是影响用户体验的关键技术挑战，该研究可为构建更稳定的多轮推荐/搜索系统提供技术基础。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07776v1": {
    "title": "Instance Relation Learning Network with Label Knowledge Propagation for Few-shot Multi-label Intent Detection",
    "url": "https://www.alphaxiv.org/abs/2510.07776v1",
    "arxiv_id": "2510.07776v1",
    "authors": "Shiman Zhao, Shangyuan Li, Wei Chen, Tengjiao Wang, Jiahui Yao, Jiabin Zheng, Kam Fai Wong",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-09 04:47:06",
    "ori_summary": "Few-shot Multi-label Intent Detection (MID) is crucial for dialogue systems, aiming to detect multiple intents of utterances in low-resource dialogue domains. Previous studies focus on a two-stage pipeline. They first learn representations of utterances with multiple labels and then use a threshold-based strategy to identify multi-label results. However, these methods rely on representation classification and ignore instance relations, leading to error propagation. To solve the above issues, we propose a multi-label joint learning method for few-shot MID in an end-to-end manner, which constructs an instance relation learning network with label knowledge propagation to eliminate error propagation. Concretely, we learn the interaction relations between instances with class information to propagate label knowledge between a few labeled (support set) and unlabeled (query set) instances. With label knowledge propagation, the relation strength between instances directly indicates whether two utterances belong to the same intent for multi-label prediction. Besides, a dual relation-enhanced loss is developed to optimize support- and query-level relation strength to improve performance. Experiments show that we outperform strong baselines by an average of 9.54% AUC and 11.19% Macro-F1 in 1-shot scenarios.",
    "summary": "",
    "translation": "基于标签知识传播的实例关系学习网络用于少样本多标签意图检测",
    "relevance_score": 3,
    "reasoning": "该论文主要关注少样本多标签意图检测，这属于对话系统和NLP领域的特定任务，与推荐系统、搜索或广告的核心领域进展没有直接关联。虽然意图检测在搜索查询理解中有潜在应用，但论文重点在于少样本学习和多标签分类技术，而非直接应用于推荐、搜索或广告系统的核心排名或建模问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07775v1": {
    "title": "The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.07775v1",
    "arxiv_id": "2510.07775v1",
    "authors": "Omar Mahmoud, Ali Khalil, Buddhika Laknath Semage, Thommen George Karimpanal, Santu Rana",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 04:30:58",
    "ori_summary": "Hallucination in large language models (LLMs) has been widely studied in recent years, with progress in both detection and mitigation aimed at improving truthfulness. Yet, a critical side effect remains largely overlooked: enhancing truthfulness can negatively impact safety alignment. In this paper, we investigate this trade-off and show that increasing factual accuracy often comes at the cost of weakened refusal behavior. Our analysis reveals that this arises from overlapping components in the model that simultaneously encode hallucination and refusal information, leading alignment methods to suppress factual knowledge unintentionally. We further examine how fine-tuning on benign datasets, even when curated for safety, can degrade alignment for the same reason. To address this, we propose a method that disentangles refusal-related features from hallucination features using sparse autoencoders, and preserves refusal behavior during fine-tuning through subspace orthogonalization. This approach prevents hallucinations from increasing while maintaining safety alignment.We evaluate our method on commonsense reasoning tasks and harmful benchmarks (AdvBench and StrongReject). Results demonstrate that our approach preserves refusal behavior and task utility, mitigating the trade-off between truthfulness and safety.",
    "summary": "",
    "translation": "AI对齐的意外权衡：在大型语言模型中平衡幻觉缓解与安全性",
    "relevance_score": 1,
    "reasoning": "该论文主要关注LLM的对齐、幻觉缓解和安全性问题，这些都属于纯粹的NLP中心话题，与推荐系统、搜索或广告的核心技术进展无关。虽然提到了幻觉缓解，但这属于被明确排除的无关主题范畴，没有展示出在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07774v1": {
    "title": "Curing Miracle Steps in LLM Mathematical Reasoning with Rubric Rewards",
    "url": "https://www.alphaxiv.org/abs/2510.07774v1",
    "arxiv_id": "2510.07774v1",
    "authors": "Youliang Yuan, Qiuyang Mang, Jingbang Chen, Hong Wan, Xiaoyuan Liu, Junjielong Xu, Jen-tse Huang, Wenxuan Wang, Wenxiang Jiao, Pinjia He",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 04:30:45",
    "ori_summary": "Large language models for mathematical reasoning are typically trained with outcome-based rewards, which credit only the final answer. In our experiments, we observe that this paradigm is highly susceptible to reward hacking, leading to a substantial overestimation of a model's reasoning ability. This is evidenced by a high incidence of false positives - solutions that reach the correct final answer through an unsound reasoning process. Through a systematic analysis with human verification, we establish a taxonomy of these failure modes, identifying patterns like Miracle Steps - abrupt jumps to a correct output without a valid preceding derivation. Probing experiments suggest a strong association between these Miracle Steps and memorization, where the model appears to recall the answer directly rather than deriving it. To mitigate this systemic issue, we introduce the Rubric Reward Model (RRM), a process-oriented reward function that evaluates the entire reasoning trajectory against problem-specific rubrics. The generative RRM provides fine-grained, calibrated rewards (0-1) that explicitly penalize logical flaws and encourage rigorous deduction. When integrated into a reinforcement learning pipeline, RRM-based training consistently outperforms outcome-only supervision across four math benchmarks. Notably, it boosts Verified Pass@1024 on AIME2024 from 26.7% to 62.6% and reduces the incidence of Miracle Steps by 71%. Our work demonstrates that rewarding the solution process is crucial for building models that are not only more accurate but also more reliable.",
    "summary": "",
    "translation": "使用评分标准奖励治愈LLM数学推理中的奇迹步骤",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM在数学推理任务中的特定问题（奇迹步骤）和训练方法（评分标准奖励），这属于纯粹的NLP推理改进范畴。虽然涉及LLM训练技术，但缺乏明确的与推荐系统、搜索或广告相关的潜在应用场景，更像是通用NLP能力的优化。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07768v1": {
    "title": "ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.07768v1",
    "arxiv_id": "2510.07768v1",
    "authors": "Murong Yue, Zhiwei Liu, Liangwei Yang, Jianguo Zhang, Zuxin Liu, Haolin Chen, Ziyu Yao, Silvio Savarese, Caiming Xiong, Shelby Heinecke, Huan Wang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-09 04:11:16",
    "ori_summary": "Large Language Models (LLMs) equipped with external tools have demonstrated enhanced performance on complex reasoning tasks. The widespread adoption of this tool-augmented reasoning is hindered by the scarcity of domain-specific tools. For instance, in domains such as physics question answering, suitable and specialized tools are often missing. Recent work has explored automating tool creation by extracting reusable functions from Chain-of-Thought (CoT) reasoning traces; however, these approaches face a critical scalability bottleneck. As the number of generated tools grows, storing them in an unstructured collection leads to significant retrieval challenges, including an expanding search space and ambiguity between function-related tools. To address this, we propose a systematic approach to automatically refactor an unstructured collection of tools into a structured tool library. Our system first generates discrete, task-specific tools and clusters them into semantically coherent topics. Within each cluster, we introduce a multi-agent framework to consolidate scattered functionalities: a code agent refactors code to extract shared logic and creates versatile, aggregated tools, while a reviewing agent ensures that these aggregated tools maintain the complete functional capabilities of the original set. This process transforms numerous question-specific tools into a smaller set of powerful, aggregated tools without loss of functionality. Experimental results demonstrate that our approach significantly improves tool retrieval accuracy and overall reasoning performance across multiple reasoning tasks. Furthermore, our method shows enhanced scalability compared with baselines as the number of question-specific increases.",
    "summary": "",
    "translation": "ToolLibGen：面向大语言模型推理的可扩展自动工具创建与聚合",
    "relevance_score": 8,
    "reasoning": "该论文属于Enabling LLM Tech范畴，专注于提升LLM的工具使用和推理能力。在搜索和推荐系统中，这种自动工具创建技术可以用于构建更智能的查询理解、多模态信息检索和复杂用户意图推理模块，显著提升系统处理复杂任务的能力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07761v1": {
    "title": "Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers",
    "url": "https://www.alphaxiv.org/abs/2510.07761v1",
    "arxiv_id": "2510.07761v1",
    "authors": "Nishant Balepur, Atrey Desai, Rachel Rudinger",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 04:00:09",
    "ori_summary": "Large language models (LLMs) now give reasoning before answering, excelling in tasks like multiple-choice question answering (MCQA). Yet, a concern is that LLMs do not solve MCQs as intended, as work finds LLMs sans reasoning succeed in MCQA without using the question, i.e., choices-only. Such partial-input success is often deemed problematic, but reasoning traces could reveal if these strategies are truly shallow in choices-only settings. To study these strategies, reasoning LLMs solve MCQs in full and choices-only inputs; test-time reasoning often boosts accuracy on full and in choices-only half the time. While possibly due to shallow shortcuts, choices-only success is barely affected by the length of reasoning traces, and after finding traces pass faithfulness tests, we show they use less problematic strategies like inferring missing questions. In all, we challenge claims that partial-input success is always a flaw, so we discuss how reasoning traces could separate problematic data from less problematic reasoning.",
    "summary": "",
    "translation": "测试时推理器是策略性多项选择题答题者",
    "relevance_score": 3,
    "reasoning": "该论文关注LLM在测试时的推理策略和多项选择题处理能力，这属于核心LLM技术的进步。虽然测试时推理优化可能间接提升推荐或搜索系统中LLM的决策质量，但论文焦点是通用的测试策略而非具体的RecSys/Search/Ads应用，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07745v1": {
    "title": "Parallel Test-Time Scaling for Latent Reasoning Models",
    "url": "https://www.alphaxiv.org/abs/2510.07745v1",
    "arxiv_id": "2510.07745v1",
    "authors": "Runyang You, Yongqi Li, Meng Liu, Wenjie Wang, Liqiang Nie, Wenjie Li",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-09 03:33:00",
    "ori_summary": "Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (LLMs), typically by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer a more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. \\ This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open a new direction for scalable inference in continuous spaces. Code released at https://github.com/YRYangang/LatentTTS.",
    "summary": "",
    "translation": "潜在推理模型的并行测试时缩放",
    "relevance_score": 7,
    "reasoning": "该论文涉及测试时缩放技术，这属于'使能LLM技术'范畴，通过优化推理阶段的模型扩展来提高效率。在推荐系统和搜索领域，这种技术可以显著降低大规模模型部署的延迟和计算成本，特别是在处理复杂用户序列和上下文特征时实现更高效的实时推理。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.07743v1": {
    "title": "OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment",
    "url": "https://www.alphaxiv.org/abs/2510.07743v1",
    "arxiv_id": "2510.07743v1",
    "authors": "Tianci Liu, Ran Xu, Tony Yu, Ilgee Hong, Carl Yang, Tuo Zhao, Haoyu Wang",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 03:31:26",
    "ori_summary": "Reward modeling lies at the core of reinforcement learning from human feedback (RLHF), yet most existing reward models rely on scalar or pairwise judgments that fail to capture the multifaceted nature of human preferences. Recent studies have explored rubrics-as-rewards (RaR) that uses structured natural language criteria that capture multiple dimensions of response quality. However, producing rubrics that are both reliable and scalable remains a key challenge. In this work, we introduce OpenRubrics, a diverse, large-scale collection of (prompt, rubric) pairs for training rubric-generation and rubric-based reward models. To elicit discriminative and comprehensive evaluation signals, we introduce Contrastive Rubric Generation (CRG), which derives both hard rules (explicit constraints) and principles (implicit qualities) by contrasting preferred and rejected responses. We further improve reliability by enforcing preference-label consistency via rejection sampling to remove noisy rubrics. Across multiple reward-modeling benchmarks, our rubric-based reward model, Rubric-RM, surpasses strong size-matched baselines by 6.8%. These gains transfer to policy models on instruction-following and biomedical benchmarks. Our results show that rubrics provide scalable alignment signals that narrow the gap between costly human evaluation and automated reward modeling, enabling a new principle-driven paradigm for LLM alignment.",
    "summary": "",
    "translation": "OpenRubrics：面向奖励建模和大语言模型对齐的可扩展合成评分标准生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM对齐和奖励建模中的评分标准生成，属于纯粹的LLM对齐技术范畴。虽然奖励建模在理论上可以应用于推荐系统的偏好学习，但论文标题明确聚焦于评分标准生成这一特定对齐任务，与推荐/搜索/广告系统的核心排序和匹配问题关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07737v1": {
    "title": "ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.07737v1",
    "arxiv_id": "2510.07737v1",
    "authors": "Fu Chen, Peng Wang, Xiyin Li, Wen Li, Shichi Lei, Dongdong Xiang",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-09 03:20:13",
    "ori_summary": "Training Large Language Models (LLMs) with Group Relative Policy Optimization (GRPO) encounters a significant challenge: models often fail to produce accurate responses, particularly in small-scale architectures. This limitation not only diminishes performance improvements and undermines the potential of GRPO but also frequently leads to mid-training collapse, adversely affecting stability and final efficacy. To address these issues, we propose ToolExpander, a novel framework that advances tool-oriented reinforcement learning for resource-constrained LLMs through two key innovations:(1) Dynamic Multi-Round Hard Sampling, which dynamically substitutes challenging samples(those without correct outputs over 10 rollouts) with high-quality few-shot demonstrations during training, coupled with an exponential learning rate decay strategy to mitigate oscillations;(2) Self-Exemplifying Thinking, an enhanced GRPO framework that eliminates KL divergence and incorporates adjusted clipping coefficients, encouraging models to autonomously generate and analyze few-shot examples via a minimal additional reward (0.01).Experimental results demonstrate that ToolExpander significantly enhances tool-using capabilities in LLMs, especially in weaker small-scale models, improving both training stability and overall performance.",
    "summary": "",
    "translation": "ToolExpander：将工具使用强化学习的前沿扩展到弱大型语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注工具使用强化学习与弱LLMs的结合，这属于强化学习的特定应用领域。虽然提到了LLMs，但核心是RL方法而非LLM技术本身，且没有明确说明在推荐系统、搜索或广告中的具体应用潜力。该工作更偏向于通用的RL技术改进，而非直接相关的领域应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07736v1": {
    "title": "Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing",
    "url": "https://www.alphaxiv.org/abs/2510.07736v1",
    "arxiv_id": "2510.07736v1",
    "authors": "Cunli Mao, Xiaofei Gao, Ran Song, Shizhu He, Shengxiang Gao, Kang Liu, Zhengtao Yu",
    "categories": "cs.CL",
    "pub_date": "2025-10-09 03:19:21",
    "ori_summary": "Large language models (LLMs) based Multilingual Knowledge Graph Completion (MKGC) aim to predict missing facts by leveraging LLMs' multilingual understanding capabilities, improving the completeness of multilingual knowledge graphs (KGs). However, existing MKGC research underutilizes the multilingual capabilities of LLMs and ignores the shareability of cross-lingual knowledge. In this paper, we propose a novel MKGC framework that leverages multilingual shared knowledge to significantly enhance performance through two components: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative Entity Reranking (IER). KL-GMoE efficiently models shared knowledge, while IER significantly enhances its utilization. To evaluate our framework, we constructed a mKG dataset containing 5 languages and conducted comprehensive comparative experiments with existing state-of-the-art (SOTA) MKGC method. The experimental results demonstrate that our framework achieves improvements of 5.47%, 3.27%, and 1.01% in the Hits@1, Hits@3, and Hits@10 metrics, respectively, compared with SOTA MKGC method. Further experimental analysis revealed the properties of knowledge sharing in settings of unseen and unbalanced languages. We have released the dataset and code for our work on https://github.com/gaoxiaofei07/KL-GMoE.",
    "summary": "",
    "translation": "基于高效多语言知识共享的多语言知识图谱补全",
    "relevance_score": 3,
    "reasoning": "该论文主要关注多语言知识图谱补全，属于通用知识图谱领域，与推荐系统、搜索或广告的核心进展没有直接关联。虽然知识图谱技术可以间接支持这些领域的知识增强，但论文标题未表明其专注于推荐、搜索或广告应用，也未涉及LLM、Transformer架构或异构数据统一建模等关键技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07731v1": {
    "title": "oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation and Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.07731v1",
    "arxiv_id": "2510.07731v1",
    "authors": "Ruiling Xu, Yifan Zhang, Qingyun Wang, Carl Edwards, Heng Ji",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-09 03:13:31",
    "ori_summary": "Organic reaction mechanisms are the stepwise elementary reactions by which reactants form intermediates and products, and are fundamental to understanding chemical reactivity and designing new molecules and reactions. Although large language models (LLMs) have shown promise in understanding chemical tasks such as synthesis design, it is unclear to what extent this reflects genuine chemical reasoning capabilities, i.e., the ability to generate valid intermediates, maintain chemical consistency, and follow logically coherent multi-step pathways. We address this by introducing oMeBench, the first large-scale, expert-curated benchmark for organic mechanism reasoning in organic chemistry. It comprises over 10,000 annotated mechanistic steps with intermediates, type labels, and difficulty ratings. Furthermore, to evaluate LLM capability more precisely and enable fine-grained scoring, we propose oMeS, a dynamic evaluation framework that combines step-level logic and chemical similarity. We analyze the performance of state-of-the-art LLMs, and our results show that although current models display promising chemical intuition, they struggle with correct and consistent multi-step reasoning. Notably, we find that using prompting strategy and fine-tuning a specialist model on our proposed dataset increases performance by 50% over the leading closed-source model. We hope that oMeBench will serve as a rigorous foundation for advancing AI systems toward genuine chemical reasoning.",
    "summary": "",
    "translation": "oMeBench：面向有机机理阐明与推理中大型语言模型鲁棒性基准测试",
    "relevance_score": 1,
    "reasoning": "该论文专注于有机化学机理领域的基准测试，属于化学领域特定应用，与推荐系统、搜索或广告完全无关。论文主题涉及有机化学机理推理，属于明确排除的医学/生物/化学等特定领域应用范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08575v1": {
    "title": "ReSplat: Learning Recurrent Gaussian Splats",
    "url": "https://www.alphaxiv.org/abs/2510.08575v1",
    "arxiv_id": "2510.08575v1",
    "authors": "Haofei Xu, Daniel Barath, Andreas Geiger, Marc Pollefeys",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:59:59",
    "ori_summary": "While feed-forward Gaussian splatting models provide computational efficiency and effectively handle sparse input settings, their performance is fundamentally limited by the reliance on a single forward pass during inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting model that iteratively refines 3D Gaussians without explicitly computing gradients. Our key insight is that the Gaussian splatting rendering error serves as a rich feedback signal, guiding the recurrent network to learn effective Gaussian updates. This feedback signal naturally adapts to unseen data distributions at test time, enabling robust generalization. To initialize the recurrent process, we introduce a compact reconstruction model that operates in a $16 \\times$ subsampled space, producing $16 \\times$ fewer Gaussians than previous per-pixel Gaussian models. This substantially reduces computational overhead and allows for efficient Gaussian updates. Extensive experiments across varying of input views (2, 8, 16), resolutions ($256 \\times 256$ to $540 \\times 960$), and datasets (DL3DV and RealEstate10K) demonstrate that our method achieves state-of-the-art performance while significantly reducing the number of Gaussians and improving the rendering speed. Our project page is at https://haofeixu.github.io/resplat/.",
    "summary": "",
    "translation": "ReSplat：学习循环高斯泼溅",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及计算机视觉中的3D场景表示技术（高斯泼溅），属于纯粹的视觉研究方向。虽然循环学习机制可能具有时序建模能力，但论文标题没有显示与推荐系统、搜索或广告的直接关联，也没有明确表明其技术可以应用于这些领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08571v1": {
    "title": "Scalable Offline Metrics for Autonomous Driving",
    "url": "https://www.alphaxiv.org/abs/2510.08571v1",
    "arxiv_id": "2510.08571v1",
    "authors": "Animikh Aich, Adwait Kulkarni, Eshed Ohn-Bar",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-09 17:59:57",
    "ori_summary": "Real-World evaluation of perception-based planning models for robotic systems, such as autonomous vehicles, can be safely and inexpensively conducted offline, i.e., by computing model prediction error over a pre-collected validation dataset with ground-truth annotations. However, extrapolating from offline model performance to online settings remains a challenge. In these settings, seemingly minor errors can compound and result in test-time infractions or collisions. This relationship is understudied, particularly across diverse closed-loop metrics and complex urban maneuvers. In this work, we revisit this undervalued question in policy evaluation through an extensive set of experiments across diverse conditions and metrics. Based on analysis in simulation, we find an even worse correlation between offline and online settings than reported by prior studies, casting doubts on the validity of current evaluation practices and metrics for driving policies. Next, we bridge the gap between offline and online evaluation. We investigate an offline metric based on epistemic uncertainty, which aims to capture events that are likely to cause errors in closed-loop settings. The resulting metric achieves over 13% improvement in correlation compared to previous offline metrics. We further validate the generalization of our findings beyond the simulation environment in real-world settings, where even greater gains are observed.",
    "summary": "",
    "translation": "自动驾驶的可扩展离线评估指标",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域的离线评估指标，属于特定领域应用（自动驾驶），与推荐系统、搜索或广告的核心技术进展、LLM技术或Transformer架构改进均无直接关联。自动驾驶的评估指标无法直接应用于RecSys/Search/Ads领域，也不涉及任何相关的技术迁移潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08568v1": {
    "title": "NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos",
    "url": "https://www.alphaxiv.org/abs/2510.08568v1",
    "arxiv_id": "2510.08568v1",
    "authors": "Hongyu Li, Lingfeng Sun, Yafei Hu, Duy Ta, Jennifer Barry, George Konidaris, Jiahui Fu",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2025-10-09 17:59:55",
    "ori_summary": "Enabling robots to execute novel manipulation tasks zero-shot is a central goal in robotics. Most existing methods assume in-distribution tasks or rely on fine-tuning with embodiment-matched data, limiting transfer across platforms. We present NovaFlow, an autonomous manipulation framework that converts a task description into an actionable plan for a target robot without any demonstrations. Given a task description, NovaFlow synthesizes a video using a video generation model and distills it into 3D actionable object flow using off-the-shelf perception modules. From the object flow, it computes relative poses for rigid objects and realizes them as robot actions via grasp proposals and trajectory optimization. For deformable objects, this flow serves as a tracking objective for model-based planning with a particle-based dynamics model. By decoupling task understanding from low-level control, NovaFlow naturally transfers across embodiments. We validate on rigid, articulated, and deformable object manipulation tasks using a table-top Franka arm and a Spot quadrupedal mobile robot, and achieve effective zero-shot execution without demonstrations or embodiment-specific training. Project website: https://novaflow.lhy.xyz/.",
    "summary": "",
    "translation": "NovaFlow：通过生成视频中的可操作流实现零样本操控",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频生成和动作操控技术，属于计算机视觉和生成式AI领域。虽然标题提到'零样本'概念，但核心内容涉及视频生成和动作流操控，与推荐系统、搜索或广告的核心技术栈没有直接关联，也没有明确的Transformer架构改进或LLM应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08566v1": {
    "title": "D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2510.08566v1",
    "arxiv_id": "2510.08566v1",
    "authors": "Meixi Song, Xin Lin, Dizhe Zhang, Haodong Li, Xiangtai Li, Bo Du, Lu Qi",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:59:49",
    "ori_summary": "Recent advances in 3D Gaussian Splatting (3DGS) enable real-time, high-fidelity novel view synthesis (NVS) with explicit 3D representations. However, performance degradation and instability remain significant under sparse-view conditions. In this work, we identify two key failure modes under sparse-view conditions: overfitting in regions with excessive Gaussian density near the camera, and underfitting in distant areas with insufficient Gaussian coverage. To address these challenges, we propose a unified framework D$^2$GS, comprising two key components: a Depth-and-Density Guided Dropout strategy that suppresses overfitting by adaptively masking redundant Gaussians based on density and depth, and a Distance-Aware Fidelity Enhancement module that improves reconstruction quality in under-fitted far-field areas through targeted supervision. Moreover, we introduce a new evaluation metric to quantify the stability of learned Gaussian distributions, providing insights into the robustness of the sparse-view 3DGS. Extensive experiments on multiple datasets demonstrate that our method significantly improves both visual quality and robustness under sparse view conditions. The project page can be found at: https://insta360-research-team.github.io/DDGS-website/.",
    "summary": "",
    "translation": "D²GS：基于深度和密度引导的高斯泼溅实现稳定且准确的稀疏视图重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的3D重建技术，特别是高斯泼溅和稀疏视图重建。虽然标题提到深度和密度引导，但这属于纯粹的3D视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。该技术主要应用于场景重建和图形学，没有明显的推荐、搜索或广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08564v1": {
    "title": "How to Teach Large Multimodal Models New Skills",
    "url": "https://www.alphaxiv.org/abs/2510.08564v1",
    "arxiv_id": "2510.08564v1",
    "authors": "Zhen Zhu, Yiming Gong, Yao Xiao, Yaoyao Liu, Derek Hoiem",
    "categories": "cs.AI, cs.CV, cs.LG",
    "pub_date": "2025-10-09 17:59:37",
    "ori_summary": "How can we teach large multimodal models (LMMs) new skills without erasing prior abilities? We study sequential fine-tuning on five target skills while monitoring general ability on eight held-out benchmarks across three model families. We observe that apparent \"forgetting\" on held-out tasks after narrow fine-tuning can partly recover at later stages. We trace this behavior to a measurable shift in the output token distribution, manifested through a simple counting-bias probe that co-varies with forgetting. Guided by this picture, we identify two simple, robust tuning recipes that learn strongly while limiting drift: (i) updating only the self-attention projection layers, and (ii) updating only the MLP Gate&Up while freezing the Down projection. Across models and tasks, these choices deliver strong target gains while largely preserving held-out performance. Code is available at https://github.com/jessemelpolio/LMM_CL",
    "summary": "",
    "translation": "如何教授大型多模态模型新技能",
    "relevance_score": 8,
    "reasoning": "该论文涉及大型多模态模型的新技能学习，属于'Enabling LLM Tech'范畴，对推荐系统和搜索有直接应用价值。在推荐系统中，可以用于快速适应新的用户行为模式或商品特征；在搜索中，能够帮助模型快速学习新的查询意图或文档类型，提升系统适应性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08565v1": {
    "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints",
    "url": "https://www.alphaxiv.org/abs/2510.08565v1",
    "arxiv_id": "2510.08565v1",
    "authors": "Changyao Tian, Hao Li, Gen Luo, Xizhou Zhu, Weijie Su, Hanming Deng, Jinguo Zhu, Jie Shao, Ziran Zhu, Yunpeng Liu, Lewei Lu, Wenhai Wang, Hongsheng Li, Jifeng Dai",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:59:37",
    "ori_summary": "Compositional training has been the de-facto paradigm in existing Multimodal Large Language Models (MLLMs), where pre-trained vision encoders are connected with pre-trained LLMs through continuous multimodal pre-training. However, the multimodal scaling property of this paradigm remains difficult to explore due to the separated training. In this paper, we focus on the native training of MLLMs in an end-to-end manner and systematically study its design space and scaling property under a practical setting, i.e., data constraint. Through careful study of various choices in MLLM, we obtain the optimal meta-architecture that best balances performance and training cost. After that, we further explore the scaling properties of the native MLLM and indicate the positively correlated scaling relationship between visual encoders and LLMs. Based on these findings, we propose a native MLLM called NaViL, combined with a simple and cost-effective recipe. Experimental results on 14 multimodal benchmarks confirm the competitive performance of NaViL against existing MLLMs. Besides that, our findings and results provide in-depth insights for the future study of native MLLMs.",
    "summary": "",
    "translation": "NaViL：在数据约束下重新思考原生多模态大语言模型的缩放特性",
    "relevance_score": 8,
    "reasoning": "该论文研究多模态LLM在数据受限情况下的缩放特性，直接属于'Enabling LLM Tech'范畴。多模态LLM的缩放特性和数据效率研究对于RecSys/Ads至关重要，因为推荐和广告系统经常面临多模态数据（文本、图像、视频）且数据分布不均衡的问题，这些发现可以帮助构建更高效的多模态推荐模型。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08562v1": {
    "title": "ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving",
    "url": "https://www.alphaxiv.org/abs/2510.08562v1",
    "arxiv_id": "2510.08562v1",
    "authors": "Zhiyu Zheng, Shaoyu Chen, Haoran Yin, Xinbang Zhang, Jialv Zou, Xinggang Wang, Qian Zhang, Lefei Zhang",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-09 17:59:36",
    "ori_summary": "End-to-end autonomous driving (E2EAD) systems, which learn to predict future trajectories directly from sensor data, are fundamentally challenged by the inherent spatio-temporal imbalance of trajectory data. This imbalance creates a significant optimization burden, causing models to learn spurious correlations instead of causal inference, while also prioritizing uncertain, distant predictions, thereby compromising immediate safety. To address these issues, we propose ResAD, a novel Normalized Residual Trajectory Modeling framework. Instead of predicting the future trajectory directly, our approach reframes the learning task to predict the residual deviation from a deterministic inertial reference. The inertial reference serves as a counterfactual, forcing the model to move beyond simple pattern recognition and instead identify the underlying causal factors (e.g., traffic rules, obstacles) that necessitate deviations from a default, inertially-guided path. To deal with the optimization imbalance caused by uncertain, long-term horizons, ResAD further incorporates Point-wise Normalization of the predicted residual. It re-weights the optimization objective, preventing large-magnitude errors associated with distant, uncertain waypoints from dominating the learning signal. Extensive experiments validate the effectiveness of our framework. On the NAVSIM benchmark, ResAD achieves a state-of-the-art PDMS of 88.6 using a vanilla diffusion policy with only two denoising steps, demonstrating that our approach significantly simplifies the learning task and improves model performance. The code will be released to facilitate further research.",
    "summary": "",
    "translation": "ResAD：用于端到端自动驾驶的归一化残差轨迹建模",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域，与搜索、推荐或广告系统没有直接关联。自动驾驶属于计算机视觉和机器人技术领域，不在当前关注的RecSys/Search/Ads核心领域或相关使能技术范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08561v1": {
    "title": "MultiCOIN: Multi-Modal COntrollable Video INbetweening",
    "url": "https://www.alphaxiv.org/abs/2510.08561v1",
    "arxiv_id": "2510.08561v1",
    "authors": "Maham Tanveer, Yang Zhou, Simon Niklaus, Ali Mahdavi Amiri, Hao Zhang, Krishna Kumar Singh, Nanxuan Zhao",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:59:27",
    "ori_summary": "Video inbetweening creates smooth and natural transitions between two image frames, making it an indispensable tool for video editing and long-form video synthesis. Existing works in this domain are unable to generate large, complex, or intricate motions. In particular, they cannot accommodate the versatility of user intents and generally lack fine control over the details of intermediate frames, leading to misalignment with the creative mind. To fill these gaps, we introduce \\modelname{}, a video inbetweening framework that allows multi-modal controls, including depth transition and layering, motion trajectories, text prompts, and target regions for movement localization, while achieving a balance between flexibility, ease of use, and precision for fine-grained video interpolation. To achieve this, we adopt the Diffusion Transformer (DiT) architecture as our video generative model, due to its proven capability to generate high-quality long videos. To ensure compatibility between DiT and our multi-modal controls, we map all motion controls into a common sparse and user-friendly point-based representation as the video/noise input. Further, to respect the variety of controls which operate at varying levels of granularity and influence, we separate content controls and motion controls into two branches to encode the required features before guiding the denoising process, resulting in two generators, one for motion and the other for content. Finally, we propose a stage-wise training strategy to ensure that our model learns the multi-modal controls smoothly. Extensive qualitative and quantitative experiments demonstrate that multi-modal controls enable a more dynamic, customizable, and contextually accurate visual narrative.",
    "summary": "",
    "translation": "MultiCOIN：多模态可控视频中间帧生成",
    "relevance_score": 2,
    "reasoning": "该论文专注于视频生成领域的中间帧生成技术，属于计算机视觉范畴。虽然涉及多模态控制，但其核心应用场景是视频内容生成和编辑，与推荐系统、搜索或广告的排序和匹配任务没有直接关联。在推荐/搜索/广告领域缺乏明确的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08559v1": {
    "title": "SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models",
    "url": "https://www.alphaxiv.org/abs/2510.08559v1",
    "arxiv_id": "2510.08559v1",
    "authors": "Andong Deng, Taojiannan Yang, Shoubin Yu, Lincoln Spencer, Mohit Bansal, Chen Chen, Serena Yeung-Levy, Xiaohan Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-09 17:59:23",
    "ori_summary": "Large Multimodal Models (LMMs) have achieved remarkable progress across various capabilities; however, complex video reasoning in the scientific domain remains a significant and challenging frontier. Current video benchmarks predominantly target general scenarios where perception/recognition is heavily relied on, while with relatively simple reasoning tasks, leading to saturation and thus failing to effectively evaluate advanced multimodal cognitive skills. To address this critical gap, we introduce SciVideoBench, a rigorous benchmark specifically designed to assess advanced video reasoning in scientific contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice questions derived from cutting-edge scientific experimental videos spanning over 25 specialized academic subjects and verified by a semi-automatic system. Each question demands sophisticated domain-specific knowledge, precise spatiotemporal perception, and intricate logical reasoning, effectively challenging models' higher-order cognitive abilities. Our evaluation highlights significant performance deficits in state-of-the-art proprietary and open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating substantial room for advancement in video reasoning capabilities. Detailed analyses of critical factors such as reasoning complexity and visual grounding provide valuable insights and clear direction for future developments in LMMs, driving the evolution of truly capable multimodal AI co-scientists. We hope SciVideoBench could fit the interests of the community and help to push the boundary of cutting-edge AI for border science.",
    "summary": "",
    "translation": "SciVideoBench：大型多模态模型中的科学视频推理基准测试",
    "relevance_score": 2,
    "reasoning": "该论文专注于科学视频推理的基准测试，属于纯粹的评估基准范畴，这在无关主题中明确排除。虽然涉及多模态模型，但科学视频领域与推荐系统、搜索或广告没有直接关联，且基准测试本身是评估性质而非技术进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08556v1": {
    "title": "DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model",
    "url": "https://www.alphaxiv.org/abs/2510.08556v1",
    "arxiv_id": "2510.08556v1",
    "authors": "Xueyi Liu, He Wang, Li Yi",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-09 17:59:11",
    "ori_summary": "Achieving generalized in-hand object rotation remains a significant challenge in robotics, largely due to the difficulty of transferring policies from simulation to the real world. The complex, contact-rich dynamics of dexterous manipulation create a \"reality gap\" that has limited prior work to constrained scenarios involving simple geometries, limited object sizes and aspect ratios, constrained wrist poses, or customized hands. We address this sim-to-real challenge with a novel framework that enables a single policy, trained in simulation, to generalize to a wide variety of objects and conditions in the real world. The core of our method is a joint-wise dynamics model that learns to bridge the reality gap by effectively fitting limited amount of real-world collected data and then adapting the sim policy's actions accordingly. The model is highly data-efficient and generalizable across different whole-hand interaction distributions by factorizing dynamics across joints, compressing system-wide influences into low-dimensional variables, and learning each joint's evolution from its own dynamic profile, implicitly capturing these net effects. We pair this with a fully autonomous data collection strategy that gathers diverse, real-world interaction data with minimal human intervention. Our complete pipeline demonstrates unprecedented generality: a single policy successfully rotates challenging objects with complex shapes (e.g., animals), high aspect ratios (up to 5.33), and small sizes, all while handling diverse wrist orientations and rotation axes. Comprehensive real-world evaluations and a teleoperation application for complex tasks validate the effectiveness and robustness of our approach. Website: https://meowuu7.github.io/DexNDM/",
    "summary": "",
    "translation": "DexNDM：通过关节级神经动力学模型缩小灵巧手内旋转的现实差距",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人领域的灵巧手操作和物理模拟，属于机器人控制与动力学建模范畴。虽然涉及神经网络模型，但其应用场景（手内物体旋转、现实差距）与推荐系统、搜索或广告领域没有任何直接或潜在的关联，完全超出了关注范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08555v1": {
    "title": "VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning",
    "url": "https://www.alphaxiv.org/abs/2510.08555v1",
    "arxiv_id": "2510.08555v1",
    "authors": "Minghong Cai, Qiulin Wang, Zongli Ye, Wenze Liu, Quande Liu, Weicai Ye, Xintao Wang, Pengfei Wan, Kun Gai, Xiangyu Yue",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:58:59",
    "ori_summary": "We introduce the task of arbitrary spatio-temporal video completion, where a video is generated from arbitrary, user-specified patches placed at any spatial location and timestamp, akin to painting on a video canvas. This flexible formulation naturally unifies many existing controllable video generation tasks--including first-frame image-to-video, inpainting, extension, and interpolation--under a single, cohesive paradigm. Realizing this vision, however, faces a fundamental obstacle in modern latent video diffusion models: the temporal ambiguity introduced by causal VAEs, where multiple pixel frames are compressed into a single latent representation, making precise frame-level conditioning structurally difficult. We address this challenge with VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC) paradigm to this fine-grained control task with zero new parameters. We propose a hybrid conditioning strategy that decouples spatial and temporal control: spatial placement is handled via zero-padding, while temporal alignment is achieved through Temporal RoPE Interpolation, which assigns each condition a continuous fractional position within the latent sequence. This resolves the VAE's temporal ambiguity and enables pixel-frame-aware control on a frozen backbone. To evaluate this new capability, we develop VideoCanvasBench, the first benchmark for arbitrary spatio-temporal video completion, covering both intra-scene fidelity and inter-scene creativity. Experiments demonstrate that VideoCanvas significantly outperforms existing conditioning paradigms, establishing a new state of the art in flexible and unified video generation.",
    "summary": "",
    "translation": "VideoCanvas：通过上下文条件化从任意时空补丁实现统一视频补全",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频补全技术，属于计算机视觉领域的特定应用。虽然提到了上下文条件化，但其核心是视频内容生成和补全，与推荐系统、搜索或广告的排名和匹配任务关联性较弱。没有明确的机制或应用表明该技术能直接应用于异构数据处理或推荐系统场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08553v1": {
    "title": "Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation",
    "url": "https://www.alphaxiv.org/abs/2510.08553v1",
    "arxiv_id": "2510.08553v1",
    "authors": "Yunzhe Xu, Yiyuan Pan, Zhe Liu",
    "categories": "cs.CV, cs.AI, cs.RO",
    "pub_date": "2025-10-09 17:58:01",
    "ori_summary": "Vision-and-Language Navigation (VLN) requires agents to follow natural language instructions through environments, with memory-persistent variants demanding progressive improvement through accumulated experience. Existing approaches for memory-persistent VLN face critical limitations: they lack effective memory access mechanisms, instead relying on entire memory incorporation or fixed-horizon lookup, and predominantly store only environmental observations while neglecting navigation behavioral patterns that encode valuable decision-making strategies. We present Memoir, which employs imagination as a retrieval mechanism grounded by explicit memory: a world model imagines future navigation states as queries to selectively retrieve relevant environmental observations and behavioral histories. The approach comprises: 1) a language-conditioned world model that imagines future states serving dual purposes: encoding experiences for storage and generating retrieval queries; 2) Hybrid Viewpoint-Level Memory that anchors both observations and behavioral patterns to viewpoints, enabling hybrid retrieval; and 3) an experience-augmented navigation model that integrates retrieved knowledge through specialized encoders. Extensive evaluation across diverse memory-persistent VLN benchmarks with 10 distinctive testing scenarios demonstrates Memoir's effectiveness: significant improvements across all scenarios, with 5.4% SPL gains on IR2R over the best memory-persistent baseline, accompanied by 8.3x training speedup and 74% inference memory reduction. The results validate that predictive retrieval of both environmental and behavioral memories enables more effective navigation, with analysis indicating substantial headroom (73.3% vs 93.4% upper bound) for this imagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.",
    "summary": "",
    "translation": "梦想回忆：基于想象引导的经验检索用于记忆持久性视觉语言导航",
    "relevance_score": 1,
    "reasoning": "该论文专注于视觉语言导航(VLN)领域，这是一个纯粹的机器人导航任务，与推荐系统、搜索或广告无关。虽然提到了检索机制，但这是针对导航环境中的经验记忆，而非用户行为或内容检索。该工作属于纯粹的视觉语言多模态研究，没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08551v1": {
    "title": "ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation",
    "url": "https://www.alphaxiv.org/abs/2510.08551v1",
    "arxiv_id": "2510.08551v1",
    "authors": "Guanghao Li, Kerui Ren, Linning Xu, Zhewen Zheng, Changjian Jiang, Xin Gao, Bo Dai, Jian Pu, Mulin Yu, Jiangmiao Pang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:57:38",
    "ori_summary": "On-the-fly 3D reconstruction from monocular image sequences is a long-standing challenge in computer vision, critical for applications such as real-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff: per-scene optimization yields high fidelity but is computationally expensive, whereas feed-forward foundation models enable real-time inference but struggle with accuracy and robustness. In this work, we propose ARTDECO, a unified framework that combines the efficiency of feed-forward models with the reliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose estimation and point prediction, coupled with a Gaussian decoder that transforms multi-scale features into structured 3D Gaussians. To sustain both fidelity and efficiency at scale, we design a hierarchical Gaussian representation with a LoD-aware rendering strategy, which improves rendering fidelity while reducing redundancy. Experiments on eight diverse indoor and outdoor benchmarks show that ARTDECO delivers interactive performance comparable to SLAM, robustness similar to feed-forward systems, and reconstruction quality close to per-scene optimization, providing a practical path toward on-the-fly digitization of real-world environments with both accurate geometry and high visual fidelity. Explore more demos on our project page: https://city-super.github.io/artdeco/.",
    "summary": "",
    "translation": "ARTDECO：基于结构化场景表示的实时高效高保真3D重建方法",
    "relevance_score": 2,
    "reasoning": "该论文专注于3D重建技术，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术栈关联度极低。虽然结构化场景表示在概念上可能与多模态建模有微弱联系，但论文明确聚焦于3D重建应用，缺乏在推荐/搜索/广告领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08547v1": {
    "title": "R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation",
    "url": "https://www.alphaxiv.org/abs/2510.08547v1",
    "arxiv_id": "2510.08547v1",
    "authors": "Xiuwei Xu, Angyuan Ma, Hankun Li, Bingyao Yu, Zheng Zhu, Jie Zhou, Jiwen Lu",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-09 17:55:44",
    "ori_summary": "Towards the aim of generalized robotic manipulation, spatial generalization is the most fundamental capability that requires the policy to work robustly under different spatial distribution of objects, environment and agent itself. To achieve this, substantial human demonstrations need to be collected to cover different spatial configurations for training a generalized visuomotor policy via imitation learning. Prior works explore a promising direction that leverages data generation to acquire abundant spatially diverse data from minimal source demonstrations. However, most approaches face significant sim-to-real gap and are often limited to constrained settings, such as fixed-base scenarios and predefined camera viewpoints. In this paper, we propose a real-to-real 3D data generation framework (R2RGen) that directly augments the pointcloud observation-action pairs to generate real-world data. R2RGen is simulator- and rendering-free, thus being efficient and plug-and-play. Specifically, given a single source demonstration, we introduce an annotation mechanism for fine-grained parsing of scene and trajectory. A group-wise augmentation strategy is proposed to handle complex multi-object compositions and diverse task constraints. We further present camera-aware processing to align the distribution of generated data with real-world 3D sensor. Empirically, R2RGen substantially enhances data efficiency on extensive experiments and demonstrates strong potential for scaling and application on mobile manipulation.",
    "summary": "",
    "translation": "R2RGEN：面向空间泛化操作的实到实三维数据生成",
    "relevance_score": 2,
    "reasoning": "该论文专注于3D数据生成和空间操作，属于计算机视觉和图形学领域，与推荐系统、搜索或广告的核心技术关联度极低。虽然3D数据在某些特定场景（如AR/VR广告）可能有潜在应用，但论文标题未表明与异构数据建模、Transformer架构或LLM技术有任何直接联系，因此相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08540v1": {
    "title": "MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.08540v1",
    "arxiv_id": "2510.08540v1",
    "authors": "Xiangyu Zhao, Junming Lin, Tianhao Liang, Yifan Zhou, Wenhao Chai, Yuzhe Gu, Weiyun Wang, Kai Chen, Gen Luo, Wenwei Zhang, Junchi Yan, Hua Yang, Haodong Duan, Xue Yang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:53:58",
    "ori_summary": "While current Multimodal Large Language Models (MLLMs) have demonstrated proficiency in reasoning tasks such as mathematics and logic, their capacity for long-chain reflective reasoning, a prerequisite for solving complex real-world problems, remains largely underexplored. In this work, we first conduct an extensive empirical investigation to evaluate this capability. Leveraging a carefully designed data synthesis engine, we construct MM-HELIX, a multimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks that require iterative thinking and backtracking. Empirical results on this benchmark reveal that existing MLLMs exhibit significant performance deficits in long-chain reflective reasoning. To address this limitation, we generate post-training data and further explore learning paradigms for exploiting such data. We first develop the Step-Elicited Response Generation pipeline to create MM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning traces for instruction-tuning stage. Given that standard Reinforcement Learning fails on complex tasks due to sparse reward signals and catastrophic forgetting after Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization (AHPO), a novel training strategy that dynamically unifies offline supervision and online optimization into a single stage. This strategy enables the model to learn from expert data when rewards are sparse and conduct independent exploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our method achieves a +18.6\\% accuracy improvement on MM-HELIX benchmark and demonstrates strong generalization with a +5.7\\% average performance gain on general mathematic and logic tasks. Our work demonstrate that reflective reasoning in MLLMs can be effectively learned and generalized, paving the way for developing more capable MLLMs.",
    "summary": "",
    "translation": "MM-HELIX：通过整体平台与自适应混合策略优化增强多模态长链反思推理",
    "relevance_score": 8,
    "reasoning": "该论文涉及多模态长链推理技术，属于核心LLM进展，具有在搜索和推荐系统中应用的明确潜力。增强的反思推理能力可显著提升复杂用户查询理解、多轮对话推荐以及跨模态内容理解，直接服务于搜索和推荐系统的核心需求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08532v1": {
    "title": "Kontinuous Kontext: Continuous Strength Control for Instruction-based Image Editing",
    "url": "https://www.alphaxiv.org/abs/2510.08532v1",
    "arxiv_id": "2510.08532v1",
    "authors": "Rishubh Parihar, Or Patashnik, Daniil Ostashev, R. Venkatesh Babu, Daniel Cohen-Or, Kuan-Chieh Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-09 17:51:03",
    "ori_summary": "Instruction-based image editing offers a powerful and intuitive way to manipulate images through natural language. Yet, relying solely on text instructions limits fine-grained control over the extent of edits. We introduce Kontinuous Kontext, an instruction-driven editing model that provides a new dimension of control over edit strength, enabling users to adjust edits gradually from no change to a fully realized result in a smooth and continuous manner. Kontinuous Kontext extends a state-of-the-art image editing model to accept an additional input, a scalar edit strength which is then paired with the edit instruction, enabling explicit control over the extent of the edit. To inject this scalar information, we train a lightweight projector network that maps the input scalar and the edit instruction to coefficients in the model's modulation space. For training our model, we synthesize a diverse dataset of image-edit-instruction-strength quadruplets using existing generative models, followed by a filtering stage to ensure quality and consistency. Kontinuous Kontext provides a unified approach for fine-grained control over edit strength for instruction driven editing from subtle to strong across diverse operations such as stylization, attribute, material, background, and shape changes, without requiring attribute-specific training.",
    "summary": "",
    "translation": "连续上下文：基于指令的图像编辑的连续强度控制",
    "relevance_score": 2,
    "reasoning": "该论文专注于图像编辑的指令控制技术，属于计算机视觉和AIGC领域。虽然涉及指令控制机制，但其核心应用是图像内容生成和编辑，与推荐系统、搜索或广告的排序和建模任务没有直接关联。该技术可能间接启发多模态交互，但缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08530v1": {
    "title": "X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering",
    "url": "https://www.alphaxiv.org/abs/2510.08530v1",
    "arxiv_id": "2510.08530v1",
    "authors": "Zhitong Huang, Mohan Zhang, Renhan Wang, Rui Tang, Hao Zhu, Jing Liao",
    "categories": "cs.GR, cs.CV, 68U05, I.3.3; I.3.6",
    "pub_date": "2025-10-09 17:50:31",
    "ori_summary": "We present X2Video, the first diffusion model for rendering photorealistic videos guided by intrinsic channels including albedo, normal, roughness, metallicity, and irradiance, while supporting intuitive multi-modal controls with reference images and text prompts for both global and local regions. The intrinsic guidance allows accurate manipulation of color, material, geometry, and lighting, while reference images and text prompts provide intuitive adjustments in the absence of intrinsic information. To enable these functionalities, we extend the intrinsic-guided image generation model XRGB to video generation by employing a novel and efficient Hybrid Self-Attention, which ensures temporal consistency across video frames and also enhances fidelity to reference images. We further develop a Masked Cross-Attention to disentangle global and local text prompts, applying them effectively onto respective local and global regions. For generating long videos, our novel Recursive Sampling method incorporates progressive frame sampling, combining keyframe prediction and frame interpolation to maintain long-range temporal consistency while preventing error accumulation. To support the training of X2Video, we assembled a video dataset named InteriorVideo, featuring 1,154 rooms from 295 interior scenes, complete with reliable ground-truth intrinsic channel sequences and smooth camera trajectories. Both qualitative and quantitative evaluations demonstrate that X2Video can produce long, temporally consistent, and photorealistic videos guided by intrinsic conditions. Additionally, X2Video effectively accommodates multi-modal controls with reference images, global and local text prompts, and simultaneously supports editing on color, material, geometry, and lighting through parametric tuning. Project page: https://luckyhzt.github.io/x2video",
    "summary": "",
    "translation": "X2Video：适配扩散模型用于多模态可控神经视频渲染",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频生成和渲染技术，属于计算机视觉领域。虽然提到了多模态控制，但缺乏与推荐系统、搜索或广告的直接关联。扩散模型在内容生成方面的应用与排名、检索等核心业务场景关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08527v1": {
    "title": "FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control",
    "url": "https://www.alphaxiv.org/abs/2510.08527v1",
    "arxiv_id": "2510.08527v1",
    "authors": "Zhiyuan Zhang, Can Wang, Dongdong Chen, Jing Liao",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:50:22",
    "ori_summary": "We present FlexTraj, a framework for image-to-video generation with flexible point trajectory control. FlexTraj introduces a unified point-based motion representation that encodes each point with a segmentation ID, a temporally consistent trajectory ID, and an optional color channel for appearance cues, enabling both dense and sparse trajectory control. Instead of injecting trajectory conditions into the video generator through token concatenation or ControlNet, FlexTraj employs an efficient sequence-concatenation scheme that achieves faster convergence, stronger controllability, and more efficient inference, while maintaining robustness under unaligned conditions. To train such a unified point trajectory-controlled video generator, FlexTraj adopts an annealing training strategy that gradually reduces reliance on complete supervision and aligned condition. Experimental results demonstrate that FlexTraj enables multi-granularity, alignment-agnostic trajectory control for video generation, supporting various applications such as motion cloning, drag-based image-to-video, motion interpolation, camera redirection, flexible action control and mesh animations.",
    "summary": "",
    "translation": "FlexTraj：基于灵活点轨迹控制的图像到视频生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于图像到视频生成技术，属于纯粹的视觉内容生成领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然视频生成技术可能有潜在的广告创意应用，但这属于被明确排除的'非排序广告主题'和'纯视觉论文'范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08512v1": {
    "title": "Have We Scene It All? Scene Graph-Aware Deep Point Cloud Compression",
    "url": "https://www.alphaxiv.org/abs/2510.08512v1",
    "arxiv_id": "2510.08512v1",
    "authors": "Nikolaos Stathoulopoulos, Christoforos Kanellakis, George Nikolakopoulos",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-09 17:45:09",
    "ori_summary": "Efficient transmission of 3D point cloud data is critical for advanced perception in centralized and decentralized multi-agent robotic systems, especially nowadays with the growing reliance on edge and cloud-based processing. However, the large and complex nature of point clouds creates challenges under bandwidth constraints and intermittent connectivity, often degrading system performance. We propose a deep compression framework based on semantic scene graphs. The method decomposes point clouds into semantically coherent patches and encodes them into compact latent representations with semantic-aware encoders conditioned by Feature-wise Linear Modulation (FiLM). A folding-based decoder, guided by latent features and graph node attributes, enables structurally accurate reconstruction. Experiments on the SemanticKITTI and nuScenes datasets show that the framework achieves state-of-the-art compression rates, reducing data size by up to 98% while preserving both structural and semantic fidelity. In addition, it supports downstream applications such as multi-robot pose graph optimization and map merging, achieving trajectory accuracy and map alignment comparable to those obtained with raw LiDAR scans.",
    "summary": "",
    "translation": "我们是否已看遍所有场景？基于场景图感知的深度点云压缩",
    "relevance_score": 2,
    "reasoning": "该论文主要关注点云数据的压缩技术，属于计算机视觉和3D视觉领域。虽然提到了场景图概念，但这与推荐系统、搜索或广告中的异构数据处理没有直接关联。点云压缩技术在当前焦点领域缺乏明确的应用场景，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08508v1": {
    "title": "MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration",
    "url": "https://www.alphaxiv.org/abs/2510.08508v1",
    "arxiv_id": "2510.08508v1",
    "authors": "Lu Liu, Chunlei Cai, Shaocheng Shen, Jianfeng Liang, Weimin Ouyang, Tianxiao Ye, Jian Mao, Huiyu Duan, Jiangchao Yao, Xiaoyun Zhang, Qiang Hu, Guangtao Zhai",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:42:51",
    "ori_summary": "Real-world videos often suffer from complex degradations, such as noise, compression artifacts, and low-light distortions, due to diverse acquisition and transmission conditions. Existing restoration methods typically require professional manual selection of specialized models or rely on monolithic architectures that fail to generalize across varying degradations. Inspired by expert experience, we propose MoA-VR, the first \\underline{M}ixture-\\underline{o}f-\\underline{A}gents \\underline{V}ideo \\underline{R}estoration system that mimics the reasoning and processing procedures of human professionals through three coordinated agents: Degradation Identification, Routing and Restoration, and Restoration Quality Assessment. Specifically, we construct a large-scale and high-resolution video degradation recognition benchmark and build a vision-language model (VLM) driven degradation identifier. We further introduce a self-adaptive router powered by large language models (LLMs), which autonomously learns effective restoration strategies by observing tool usage patterns. To assess intermediate and final processed video quality, we construct the \\underline{Res}tored \\underline{V}ideo \\underline{Q}uality (Res-VQ) dataset and design a dedicated VLM-based video quality assessment (VQA) model tailored for restoration tasks. Extensive experiments demonstrate that MoA-VR effectively handles diverse and compound degradations, consistently outperforming existing baselines in terms of both objective metrics and perceptual quality. These results highlight the potential of integrating multimodal intelligence and modular reasoning in general-purpose video restoration systems.",
    "summary": "",
    "translation": "MoA-VR：面向一体化视频修复的智能体混合系统",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频修复的计算机视觉任务，属于纯粹的视觉处理领域。虽然Mixture-of-Agents架构在概念上相关，但该工作没有展示与推荐系统、搜索或广告的潜在应用连接，完全属于被排除的视觉技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08498v1": {
    "title": "AI-Driven Radiology Report Generation for Traumatic Brain Injuries",
    "url": "https://www.alphaxiv.org/abs/2510.08498v1",
    "arxiv_id": "2510.08498v1",
    "authors": "Riadh Bouslimi, Houda Trabelsi, Wahiba Ben Abdssalem Karaa, Hana Hedhli",
    "categories": "eess.IV, cs.AI, cs.CV, cs.LG, 68T07, 68U10, I.2.10; I.2.7; I.4.5",
    "pub_date": "2025-10-09 17:39:04",
    "ori_summary": "Traumatic brain injuries present significant diagnostic challenges in emergency medicine, where the timely interpretation of medical images is crucial for patient outcomes. In this paper, we propose a novel AI-based approach for automatic radiology report generation tailored to cranial trauma cases. Our model integrates an AC-BiFPN with a Transformer architecture to capture and process complex medical imaging data such as CT and MRI scans. The AC-BiFPN extracts multi-scale features, enabling the detection of intricate anomalies like intracranial hemorrhages, while the Transformer generates coherent, contextually relevant diagnostic reports by modeling long-range dependencies. We evaluate the performance of our model on the RSNA Intracranial Hemorrhage Detection dataset, where it outperforms traditional CNN-based models in both diagnostic accuracy and report generation. This solution not only supports radiologists in high-pressure environments but also provides a powerful educational tool for trainee physicians, offering real-time feedback and enhancing their learning experience. Our findings demonstrate the potential of combining advanced feature extraction with transformer-based text generation to improve clinical decision-making in the diagnosis of traumatic brain injuries.",
    "summary": "",
    "translation": "基于人工智能的创伤性脑损伤放射学报告生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的放射学报告生成，属于医疗AI应用范畴，与推荐系统、搜索或广告领域完全无关。论文内容涉及创伤性脑损伤的医学诊断报告生成，属于明确的医疗领域特定应用，完全超出您关注的技术领域范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08492v1": {
    "title": "Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models",
    "url": "https://www.alphaxiv.org/abs/2510.08492v1",
    "arxiv_id": "2510.08492v1",
    "authors": "Sharut Gupta, Shobhita Sundaram, Chenyu Wang, Stefanie Jegelka, Phillip Isola",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-09 17:32:23",
    "ori_summary": "Traditional multimodal learners find unified representations for tasks like visual question answering, but rely heavily on paired datasets. However, an overlooked yet potentially powerful question is: can one leverage auxiliary unpaired multimodal data to directly enhance representation learning in a target modality? We introduce UML: Unpaired Multimodal Learner, a modality-agnostic training paradigm in which a single model alternately processes inputs from different modalities while sharing parameters across them. This design exploits the assumption that different modalities are projections of a shared underlying reality, allowing the model to benefit from cross-modal structure without requiring explicit pairs. Theoretically, under linear data-generating assumptions, we show that unpaired auxiliary data can yield representations strictly more informative about the data-generating process than unimodal training. Empirically, we show that using unpaired data from auxiliary modalities -- such as text, audio, or images -- consistently improves downstream performance across diverse unimodal targets such as image and audio. Our project page: https://unpaired-multimodal.github.io/",
    "summary": "",
    "translation": "协同增效：利用未配对多模态数据构建更强大的单模态模型",
    "relevance_score": 8,
    "reasoning": "该论文涉及多模态数据利用和模型增强，直接关联'VLM Analogy for Heterogeneous Data'焦点，将不同数据模态视为独立输入进行统一建模。在推荐系统和搜索场景中，可应用于处理用户行为序列、上下文特征等异构数据，通过多模态协同提升单模态模型的性能表现。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08491v1": {
    "title": "Splat the Net: Radiance Fields with Splattable Neural Primitives",
    "url": "https://www.alphaxiv.org/abs/2510.08491v1",
    "arxiv_id": "2510.08491v1",
    "authors": "Xilong Zhou, Bao-Huy Nguyen, Loïc Magne, Vladislav Golyanik, Thomas Leimkühler, Christian Theobalt",
    "categories": "cs.GR, cs.CV",
    "pub_date": "2025-10-09 17:31:11",
    "ori_summary": "Radiance fields have emerged as a predominant representation for modeling 3D scene appearance. Neural formulations such as Neural Radiance Fields provide high expressivity but require costly ray marching for rendering, whereas primitive-based methods such as 3D Gaussian Splatting offer real-time efficiency through splatting, yet at the expense of representational power. Inspired by advances in both these directions, we introduce splattable neural primitives, a new volumetric representation that reconciles the expressivity of neural models with the efficiency of primitive-based splatting. Each primitive encodes a bounded neural density field parameterized by a shallow neural network. Our formulation admits an exact analytical solution for line integrals, enabling efficient computation of perspectively accurate splatting kernels. As a result, our representation supports integration along view rays without the need for costly ray marching. The primitives flexibly adapt to scene geometry and, being larger than prior analytic primitives, reduce the number required per scene. On novel-view synthesis benchmarks, our approach matches the quality and speed of 3D Gaussian Splatting while using $10\\times$ fewer primitives and $6\\times$ fewer parameters. These advantages arise directly from the representation itself, without reliance on complex control or adaptation frameworks. The project page is https://vcai.mpi-inf.mpg.de/projects/SplatNet/.",
    "summary": "",
    "translation": "Splat the Net：基于可溅射神经基元的辐射场",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及计算机视觉中的辐射场和神经基元技术，属于3D视觉和图形学领域。这些技术与推荐系统、搜索或广告的核心技术栈没有直接关联，也不具备在相关领域应用的明显潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08485v1": {
    "title": "InstructX: Towards Unified Visual Editing with MLLM Guidance",
    "url": "https://www.alphaxiv.org/abs/2510.08485v1",
    "arxiv_id": "2510.08485v1",
    "authors": "Chong Mou, Qichao Sun, Yanze Wu, Pengze Zhang, Xinghui Li, Fulong Ye, Songtao Zhao, Qian He",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:26:09",
    "ori_summary": "With recent advances in Multimodal Large Language Models (MLLMs) showing strong visual understanding and reasoning, interest is growing in using them to improve the editing performance of diffusion models. Despite rapid progress, most studies lack an in-depth analysis of MLLM design choices. Moreover, the integration of MLLMs and diffusion models remains an open challenge in some difficult tasks, such as video editing. In this paper, we present InstructX, a unified framework for image and video editing. Specifically, we conduct a comprehensive study on integrating MLLMs and diffusion models for instruction-driven editing across diverse tasks. Building on this study, we analyze the cooperation and distinction between images and videos in unified modeling. (1) We show that training on image data can lead to emergent video editing capabilities without explicit supervision, thereby alleviating the constraints imposed by scarce video training data. (2) By incorporating modality-specific MLLM features, our approach effectively unifies image and video editing tasks within a single model. Extensive experiments demonstrate that our method can handle a broad range of image and video editing tasks and achieves state-of-the-art performance.",
    "summary": "",
    "translation": "InstructX：基于多模态大语言模型引导的统一视觉编辑方法",
    "relevance_score": 2,
    "reasoning": "该论文专注于多模态大语言模型在视觉编辑领域的应用，属于纯粹的视觉内容生成范畴。虽然涉及多模态技术，但其核心应用是视觉编辑而非推荐系统、搜索或广告中的排序任务，与当前关注的技术方向关联度极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08480v1": {
    "title": "Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools",
    "url": "https://www.alphaxiv.org/abs/2510.08480v1",
    "arxiv_id": "2510.08480v1",
    "authors": "Zhenlong Yuan, Xiangyan Qu, Chengxuan Qian, Rui Chen, Jing Tang, Lei Sun, Xiangxiang Chu, Dapeng Zhang, Yiwei Wang, Yujun Cai, Shuo Li",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 17:20:44",
    "ori_summary": "Multimodal large language models (MLLMs) have demonstrated remarkable potential in bridging visual and textual reasoning, yet their reliance on text-centric priors often limits their ability to disentangle semantically similar actions in open-vocabulary scenarios. To address this, we propose Video-STAR, a framework that harmonizes contextual sub-motion decomposition with tool-augmented reinforcement learning for open-vocabulary action recognition (OVAR). Unlike prior methods that treat actions as monolithic entities, our approach innovatively decomposes actions into discriminative sub-motions for fine-grained matching while dynamically invoking domain-specific tools for cross-modal interleaving, thereby enabling category-specific reasoning capacity and reducing cross-modal hallucination. Moreover, by designing a hierarchical reward that balances tool-usage efficiency, sub-motion relevance, and structural coherence in reasoning, our method autonomously leverages external tools to prioritize sub-motion patterns without explicit supervision, transmitting from text-centric reasoning to visually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2, Kinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art performance, outperforming existing methods in distinguishing fine-grained actions and handling cross-modal hallucination, validating our excellent robustness and generalization.",
    "summary": "",
    "translation": "Video-STAR：通过工具增强开放词汇动作识别",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的开放词汇动作识别，虽然涉及多模态学习概念，但与推荐系统、搜索或广告的核心技术关联较弱。其工具增强方法可能对处理视频内容有一定启发，但在当前焦点领域的直接应用潜力有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08475v1": {
    "title": "DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos",
    "url": "https://www.alphaxiv.org/abs/2510.08475v1",
    "arxiv_id": "2510.08475v1",
    "authors": "Jhen Hsieh, Kuan-Hsun Tu, Kuo-Han Hung, Tsung-Wei Ke",
    "categories": "cs.RO, cs.CV, cs.LG",
    "pub_date": "2025-10-09 17:17:05",
    "ori_summary": "We present DexMan, an automated framework that converts human visual demonstrations into bimanual dexterous manipulation skills for humanoid robots in simulation. Operating directly on third-person videos of humans manipulating rigid objects, DexMan eliminates the need for camera calibration, depth sensors, scanned 3D object assets, or ground-truth hand and object motion annotations. Unlike prior approaches that consider only simplified floating hands, it directly controls a humanoid robot and leverages novel contact-based rewards to improve policy learning from noisy hand-object poses estimated from in-the-wild videos. DexMan achieves state-of-the-art performance in object pose estimation on the TACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD. Meanwhile, its reinforcement learning policy surpasses previous methods by 19% in success rate on OakInk-v2. Furthermore, DexMan can generate skills from both real and synthetic videos, without the need for manual data collection and costly motion capture, and enabling the creation of large-scale, diverse datasets for training generalist dexterous manipulation.",
    "summary": "",
    "translation": "DexMan：从人类和生成视频中学习双手灵巧操作",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人双手灵巧操作学习，属于纯粹的机器人控制领域。虽然涉及从视频中学习，但这是针对物理机器人动作模仿，与推荐系统、搜索或广告中的序列建模、多模态理解等核心技术没有直接关联。该技术缺乏在RecSys/Search/Ads领域的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08449v1": {
    "title": "Hierarchical Spatial Algorithms for High-Resolution Image Quantization and Feature Extraction",
    "url": "https://www.alphaxiv.org/abs/2510.08449v1",
    "arxiv_id": "2510.08449v1",
    "authors": "Noor Islam S. Mohammad",
    "categories": "cs.CV, 68T45, 68U10, I.4.8; I.2.10",
    "pub_date": "2025-10-09 16:56:24",
    "ori_summary": "This study introduces a modular framework for spatial image processing, integrating grayscale quantization, color and brightness enhancement, image sharpening, bidirectional transformation pipelines, and geometric feature extraction. A stepwise intensity transformation quantizes grayscale images into eight discrete levels, producing a posterization effect that simplifies representation while preserving structural detail. Color enhancement is achieved via histogram equalization in both RGB and YCrCb color spaces, with the latter improving contrast while maintaining chrominance fidelity. Brightness adjustment is implemented through HSV value-channel manipulation, and image sharpening is performed using a 3 * 3 convolution kernel to enhance high-frequency details. A bidirectional transformation pipeline that integrates unsharp masking, gamma correction, and noise amplification achieved accuracy levels of 76.10% and 74.80% for the forward and reverse processes, respectively. Geometric feature extraction employed Canny edge detection, Hough-based line estimation (e.g., 51.50{\\deg} for billiard cue alignment), Harris corner detection, and morphological window localization. Cue isolation further yielded 81.87\\% similarity against ground truth images. Experimental evaluation across diverse datasets demonstrates robust and deterministic performance, highlighting its potential for real-time image analysis and computer vision.",
    "summary": "",
    "translation": "用于高分辨率图像量化和特征提取的层次化空间算法",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉领域的图像量化和特征提取技术，虽然特征提取在广义上与推荐系统相关，但论文明确聚焦于高分辨率图像处理，这属于纯粹的视觉技术范畴。根据筛选标准，纯粹的视觉论文如果没有明确的推荐/搜索/广告应用相关性，应被视为低相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08442v1": {
    "title": "Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning",
    "url": "https://www.alphaxiv.org/abs/2510.08442v1",
    "arxiv_id": "2510.08442v1",
    "authors": "Andrew Lee, Ian Chuang, Dechen Gao, Kai Fukazawa, Iman Soltani",
    "categories": "cs.CV, cs.AI, cs.RO",
    "pub_date": "2025-10-09 16:54:11",
    "ori_summary": "Visual Reinforcement Learning (RL) agents must learn to act based on high-dimensional image data where only a small fraction of the pixels is task-relevant. This forces agents to waste exploration and computational resources on irrelevant features, leading to sample-inefficient and unstable learning. To address this, inspired by human visual foveation, we introduce Gaze on the Prize. This framework augments visual RL with a learnable foveal attention mechanism (Gaze), guided by a self-supervised signal derived from the agent's experience pursuing higher returns (the Prize). Our key insight is that return differences reveal what matters most: If two similar representations produce different outcomes, their distinguishing features are likely task-relevant, and the gaze should focus on them accordingly. This is realized through return-guided contrastive learning that trains the attention to distinguish between the features relevant to success and failure. We group similar visual representations into positives and negatives based on their return differences and use the resulting labels to construct contrastive triplets. These triplets provide the training signal that teaches the attention mechanism to produce distinguishable representations for states associated with different outcomes. Our method achieves up to 2.4x improvement in sample efficiency and can solve tasks that the baseline fails to learn, demonstrated across a suite of manipulation tasks from the ManiSkill3 benchmark, all without modifying the underlying algorithm or hyperparameters.",
    "summary": "",
    "translation": "凝视目标：利用回报引导的对比学习塑造视觉注意力",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉注意力机制和对比学习，虽然对比学习是LLM相关技术，但论文明确聚焦于视觉领域（'Visual Attention', 'Gaze'），缺乏与推荐系统、搜索或广告的明确关联。视觉注意力机制在推荐/搜索中主要用于处理图像内容，但论文标题未表明这种跨领域应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08431v1": {
    "title": "Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency",
    "url": "https://www.alphaxiv.org/abs/2510.08431v1",
    "arxiv_id": "2510.08431v1",
    "authors": "Kaiwen Zheng, Yuji Wang, Qianli Ma, Huayu Chen, Jintao Zhang, Yogesh Balaji, Jianfei Chen, Ming-Yu Liu, Jun Zhu, Qinsheng Zhang",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-09 16:45:30",
    "ori_summary": "This work represents the first effort to scale up continuous-time consistency distillation to general application-level image and video diffusion models. Although continuous-time consistency model (sCM) is theoretically principled and empirically powerful for accelerating academic-scale diffusion, its applicability to large-scale text-to-image and video tasks remains unclear due to infrastructure challenges in Jacobian-vector product (JVP) computation and the limitations of standard evaluation benchmarks. We first develop a parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on models with over 10 billion parameters and high-dimensional video tasks. Our investigation reveals fundamental quality limitations of sCM in fine-detail generation, which we attribute to error accumulation and the \"mode-covering\" nature of its forward-divergence objective. To remedy this, we propose the score-regularized continuous-time consistency model (rCM), which incorporates score distillation as a long-skip regularizer. This integration complements sCM with the \"mode-seeking\" reverse divergence, effectively improving visual quality while maintaining high generation diversity. Validated on large-scale models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM matches or surpasses the state-of-the-art distillation method DMD2 on quality metrics while offering notable advantages in diversity, all without GAN tuning or extensive hyperparameter searches. The distilled models generate high-fidelity samples in only $1\\sim4$ steps, accelerating diffusion sampling by $15\\times\\sim50\\times$. These results position rCM as a practical and theoretically grounded framework for advancing large-scale diffusion distillation.",
    "summary": "",
    "translation": "基于分数正则化连续时间一致性的大规模扩散蒸馏",
    "relevance_score": 3,
    "reasoning": "该论文主要关注扩散模型的蒸馏技术，属于生成模型效率优化领域。虽然扩散模型在内容生成方面有应用，但该技术本身是通用的模型压缩方法，与推荐系统、搜索或广告的核心排序和匹配问题关联较弱。在推荐/搜索场景中，潜在的间接应用可能包括生成式推荐中的模型加速，但这不是直接相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08425v1": {
    "title": "Reinforcing Diffusion Models by Direct Group Preference Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.08425v1",
    "arxiv_id": "2510.08425v1",
    "authors": "Yihong Luo, Tianyang Hu, Jing Tang",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-09 16:40:43",
    "ori_summary": "While reinforcement learning methods such as Group Relative Preference Optimization (GRPO) have significantly enhanced Large Language Models, adapting them to diffusion models remains challenging. In particular, GRPO demands a stochastic policy, yet the most cost-effective diffusion samplers are based on deterministic ODEs. Recent work addresses this issue by using inefficient SDE-based samplers to induce stochasticity, but this reliance on model-agnostic Gaussian noise leads to slow convergence. To resolve this conflict, we propose Direct Group Preference Optimization (DGPO), a new online RL algorithm that dispenses with the policy-gradient framework entirely. DGPO learns directly from group-level preferences, which utilize relative information of samples within groups. This design eliminates the need for inefficient stochastic policies, unlocking the use of efficient deterministic ODE samplers and faster training. Extensive results show that DGPO trains around 20 times faster than existing state-of-the-art methods and achieves superior performance on both in-domain and out-of-domain reward metrics. Code is available at https://github.com/Luo-Yihong/DGPO.",
    "summary": "",
    "translation": "通过直接群体偏好优化强化扩散模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注扩散模型的偏好优化，属于生成模型领域，与我的核心关注点（推荐系统、搜索、广告的直接应用或使能技术）相关性较低。虽然偏好优化技术可能间接影响内容生成质量，但论文没有明确展示在推荐、搜索或广告中的直接应用潜力，且更偏向AIGC和内容生成领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08407v1": {
    "title": "Biology-driven assessment of deep learning super-resolution imaging of the porosity network in dentin",
    "url": "https://www.alphaxiv.org/abs/2510.08407v1",
    "arxiv_id": "2510.08407v1",
    "authors": "Lauren Anderson, Lucas Chatelain, Nicolas Tremblay, Kathryn Grandfield, David Rousseau, Aurélien Gourrier",
    "categories": "cs.LG, cs.CV, q-bio.TO",
    "pub_date": "2025-10-09 16:26:38",
    "ori_summary": "The mechanosensory system of teeth is currently believed to partly rely on Odontoblast cells stimulation by fluid flow through a porosity network extending through dentin. Visualizing the smallest sub-microscopic porosity vessels therefore requires the highest achievable resolution from confocal fluorescence microscopy, the current gold standard. This considerably limits the extent of the field of view to very small sample regions. To overcome this limitation, we tested different deep learning (DL) super-resolution (SR) models to allow faster experimental acquisitions of lower resolution images and restore optimal image quality by post-processing. Three supervised 2D SR models (RCAN, pix2pix, FSRCNN) and one unsupervised (CycleGAN) were applied to a unique set of experimentally paired high- and low-resolution confocal images acquired with different sampling schemes, resulting in a pixel size increase of x2, x4, x8. Model performance was quantified using a broad set of similarity and distribution-based image quality assessment (IQA) metrics, which yielded inconsistent results that mostly contradicted our visual perception. This raises the question of the relevance of such generic metrics to efficiently target the specific structure of dental porosity. To resolve this conflicting information, the generated SR images were segmented taking into account the specific scales and morphology of the porosity network and analysed by comparing connected components. Additionally, the capacity of the SR models to preserve 3D porosity connectivity throughout the confocal image stacks was evaluated using graph analysis. This biology-driven assessment allowed a far better mechanistic interpretation of SR performance, highlighting differences in model sensitivity to weak intensity features and the impact of non-linearity in image generation, which explains the failure of standard IQA metrics.",
    "summary": "",
    "translation": "基于生物学驱动的深度学习牙本质孔隙网络超分辨率成像评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于牙本质孔隙网络的医学成像应用，属于明确的生物学/医学领域，与推荐系统、搜索或广告技术完全无关。深度学习在这里仅作为医学图像处理的工具，没有任何潜在的应用于RecSys/Search/Ads的可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08398v1": {
    "title": "VideoVerse: How Far is Your T2V Generator from a World Model?",
    "url": "https://www.alphaxiv.org/abs/2510.08398v1",
    "arxiv_id": "2510.08398v1",
    "authors": "Zeqing Wang, Xinyu Wei, Bairui Li, Zhen Guo, Jinrui Zhang, Hongyang Wei, Keze Wang, Lei Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 16:18:20",
    "ori_summary": "The recent rapid advancement of Text-to-Video (T2V) generation technologies, which are critical to build ``world models'', makes the existing benchmarks increasingly insufficient to evaluate state-of-the-art T2V models. First, current evaluation dimensions, such as per-frame aesthetic quality and temporal consistency, are no longer able to differentiate state-of-the-art T2V models. Second, event-level temporal causality, which not only distinguishes video from other modalities but also constitutes a crucial component of world models, is severely underexplored in existing benchmarks. Third, existing benchmarks lack a systematic assessment of world knowledge, which are essential capabilities for building world models. To address these issues, we introduce VideoVerse, a comprehensive benchmark that focuses on evaluating whether a T2V model could understand complex temporal causality and world knowledge in the real world. We collect representative videos across diverse domains (e.g., natural landscapes, sports, indoor scenes, science fiction, chemical and physical experiments) and extract their event-level descriptions with inherent temporal causality, which are then rewritten into text-to-video prompts by independent annotators. For each prompt, we design a suite of binary evaluation questions from the perspective of dynamic and static properties, with a total of ten carefully defined evaluation dimensions. In total, our VideoVerse comprises 300 carefully curated prompts, involving 815 events and 793 binary evaluation questions. Consequently, a human preference aligned QA-based evaluation pipeline is developed by using modern vision-language models. Finally, we perform a systematic evaluation of state-of-the-art open-source and closed-source T2V models on VideoVerse, providing in-depth analysis on how far the current T2V generators are from world models.",
    "summary": "",
    "translation": "VideoVerse：您的文本到视频生成器距离世界模型还有多远？",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文本到视频生成技术及其与世界模型的比较，这属于纯粹的视觉内容生成领域。虽然世界模型概念在强化学习中很重要，但论文标题明确聚焦于视频生成评估，与推荐系统、搜索或广告中的排名、用户建模等核心任务没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08394v1": {
    "title": "Spectral Prefiltering of Neural Fields",
    "url": "https://www.alphaxiv.org/abs/2510.08394v1",
    "arxiv_id": "2510.08394v1",
    "authors": "Mustafa B. Yaldiz, Ishit Mehta, Nithin Raghavan, Andreas Meuleman, Tzu-Mao Li, Ravi Ramamoorthi",
    "categories": "cs.GR, cs.CV",
    "pub_date": "2025-10-09 16:15:46",
    "ori_summary": "Neural fields excel at representing continuous visual signals but typically operate at a single, fixed resolution. We present a simple yet powerful method to optimize neural fields that can be prefiltered in a single forward pass. Key innovations and features include: (1) We perform convolutional filtering in the input domain by analytically scaling Fourier feature embeddings with the filter's frequency response. (2) This closed-form modulation generalizes beyond Gaussian filtering and supports other parametric filters (Box and Lanczos) that are unseen at training time. (3) We train the neural field using single-sample Monte Carlo estimates of the filtered signal. Our method is fast during both training and inference, and imposes no additional constraints on the network architecture. We show quantitative and qualitative improvements over existing methods for neural-field filtering.",
    "summary": "",
    "translation": "神经场的光谱预滤波",
    "relevance_score": 2,
    "reasoning": "该论文主要关注神经场（Neural Fields）的光谱预滤波技术，这属于计算机视觉和图形学领域的底层技术。虽然神经场在3D重建和表示方面有应用，但该技术本身与推荐系统、搜索或广告的核心进展缺乏直接关联，也没有明确的Transformer架构改进或LLM应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08393v1": {
    "title": "Robust Source-Free Domain Adaptation for Medical Image Segmentation based on Curriculum Learning",
    "url": "https://www.alphaxiv.org/abs/2510.08393v1",
    "arxiv_id": "2510.08393v1",
    "authors": "Ziqi Zhang, Yuexiang Li, Yawen Huang, Nanjun He, Tao Xu, Liwei Lin, Yefeng Zheng, Shaoxin Li, Feiyue Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 16:15:10",
    "ori_summary": "Recent studies have uncovered a new research line, namely source-free domain adaptation, which adapts a model to target domains without using the source data. Such a setting can address the concerns on data privacy and security issues of medical images. However, current source-free domain adaptation frameworks mainly focus on the pseudo label refinement for target data without the consideration of learning procedure. Indeed, a progressive learning process from source to target domain will benefit the knowledge transfer during model adaptation. To this end, we propose a curriculum-based framework, namely learning from curriculum (LFC), for source-free domain adaptation, which consists of easy-to-hard and source-to-target curricula. Concretely, the former curriculum enables the framework to start learning with `easy' samples and gradually tune the optimization direction of model adaption by increasing the sample difficulty. While, the latter can stablize the adaptation process, which ensures smooth transfer of the model from the source domain to the target. We evaluate the proposed source-free domain adaptation approach on the public cross-domain datasets for fundus segmentation and polyp segmentation. The extensive experimental results show that our framework surpasses the existing approaches and achieves a new state-of-the-art.",
    "summary": "",
    "translation": "基于课程学习的医学图像分割鲁棒无源域自适应",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分割的领域自适应问题，这属于医学/生物领域的特定应用，与推荐系统、搜索或广告的核心技术无关。论文内容涉及课程学习和无源域自适应，但这些技术在该文中被应用于医学图像处理，没有显示出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08377v1": {
    "title": "UniVideo: Unified Understanding, Generation, and Editing for Videos",
    "url": "https://www.alphaxiv.org/abs/2510.08377v1",
    "arxiv_id": "2510.08377v1",
    "authors": "Cong Wei, Quande Liu, Zixuan Ye, Qiulin Wang, Xintao Wang, Pengfei Wan, Kun Gai, Wenhu Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 16:01:30",
    "ori_summary": "Unified multimodal models have shown promising results in multimodal content generation and editing but remain largely limited to the image domain. In this work, we present UniVideo, a versatile framework that extends unified modeling to the video domain. UniVideo adopts a dual-stream design, combining a Multimodal Large Language Model (MLLM) for instruction understanding with a Multimodal DiT (MMDiT) for video generation. This design enables accurate interpretation of complex multimodal instructions while preserving visual consistency. Built on this architecture, UniVideo unifies diverse video generation and editing tasks under a single multimodal instruction paradigm and is jointly trained across them. Extensive experiments demonstrate that UniVideo matches or surpasses state-of-the-art task-specific baselines in text/image-to-video generation, in-context video generation and in-context video editing. Notably, the unified design of UniVideo enables two forms of generalization. First, UniVideo supports task composition, such as combining editing with style transfer, by integrating multiple capabilities within a single instruction. Second, even without explicit training on free-form video editing, UniVideo transfers its editing capability from large-scale image editing data to this setting, handling unseen instructions such as green-screening characters or changing materials within a video. Beyond these core capabilities, UniVideo also supports visual-prompt-based video generation, where the MLLM interprets visual prompts and guides the MMDiT during synthesis. To foster future research, we will release our model and code.",
    "summary": "",
    "translation": "UniVideo：面向视频的统一理解、生成与编辑",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频领域的统一建模，虽然涉及多模态技术，但其核心应用场景是视频内容的理解、生成和编辑，与推荐系统、搜索或广告的关联性较弱。视频生成和编辑技术可能间接应用于广告创意生成，但这属于明确排除的非相关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08363v1": {
    "title": "Hyperspectral data augmentation with transformer-based diffusion models",
    "url": "https://www.alphaxiv.org/abs/2510.08363v1",
    "arxiv_id": "2510.08363v1",
    "authors": "Mattia Ferrari, Lorenzo Bruzzone",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 15:50:29",
    "ori_summary": "The introduction of new generation hyperspectral satellite sensors, combined with advancements in deep learning methodologies, has significantly enhanced the ability to discriminate detailed land-cover classes at medium-large scales. However, a significant challenge in deep learning methods is the risk of overfitting when training networks with small labeled datasets. In this work, we propose a data augmentation technique that leverages a guided diffusion model. To effectively train the model with a limited number of labeled samples and to capture complex patterns in the data, we implement a lightweight transformer network. Additionally, we introduce a modified weighted loss function and an optimized cosine variance scheduler, which facilitate fast and effective training on small datasets. We evaluate the effectiveness of the proposed method on a forest classification task with 10 different forest types using hyperspectral images acquired by the PRISMA satellite. The results demonstrate that the proposed method outperforms other data augmentation techniques in both average and weighted average accuracy. The effectiveness of the method is further highlighted by the stable training behavior of the model, which addresses a common limitation in the practical application of deep generative models for data augmentation.",
    "summary": "",
    "translation": "基于Transformer的扩散模型进行高光谱数据增强",
    "relevance_score": 2,
    "reasoning": "该论文主要关注高光谱数据增强，这属于计算机视觉领域的特定应用，与推荐系统、搜索或广告的核心技术关联度极低。虽然使用了Transformer架构，但应用场景（高光谱数据）在推荐/搜索/广告领域几乎没有实际应用场景，因此整体相关性非常有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08358v1": {
    "title": "SPICE: Simple and Practical Image Clarification and Enhancement",
    "url": "https://www.alphaxiv.org/abs/2510.08358v1",
    "arxiv_id": "2510.08358v1",
    "authors": "Alexander Belyaev, Pierre-Alain Fayolle, Michael Cohen",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 15:43:07",
    "ori_summary": "We introduce a simple and efficient method to enhance and clarify images. More specifically, we deal with low light image enhancement and clarification of hazy imagery (hazy/foggy images, images containing sand dust, and underwater images). Our method involves constructing an image filter to simulate low-light or hazy conditions and deriving approximate reverse filters to minimize distortions in the enhanced images. Experimental results show that our approach is highly competitive and often surpasses state-of-the-art techniques in handling extremely dark images and in enhancing hazy images. A key advantage of our approach lies in its simplicity: Our method is implementable with just a few lines of MATLAB code.",
    "summary": "",
    "translation": "SPICE：简单实用的图像清晰化与增强",
    "relevance_score": 1,
    "reasoning": "该论文专注于图像处理技术，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术栈没有直接关联。虽然图像增强技术可能在某些特定场景下作为预处理步骤，但论文本身并未表明与异构数据建模、Transformer架构或LLM应用有任何联系，因此相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08352v1": {
    "title": "Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception",
    "url": "https://www.alphaxiv.org/abs/2510.08352v1",
    "arxiv_id": "2510.08352v1",
    "authors": "Nikos Theodoridis, Tim Brophy, Reenu Mohandas, Ganesh Sistu, Fiachra Collins, Anthony Scanlan, Ciaran Eising",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-09 15:38:41",
    "ori_summary": "Vision-Language Models (VLMs) are becoming increasingly powerful, demonstrating strong performance on a variety of tasks that require both visual and textual understanding. Their strong generalisation abilities make them a promising component for automated driving systems, which must handle unexpected corner cases. However, to be trusted in such safety-critical applications, a model must first possess a reliable perception system. Moreover, since critical objects and agents in traffic scenes are often at a distance, we require systems that are not \"shortsighted\", i.e., systems with strong perception capabilities at both close (up to 20 meters) and long (30+ meters) range. With this in mind, we introduce Distance-Annotated Traffic Perception Question Answering (DTPQA), the first Visual Question Answering (VQA) benchmark focused solely on perception-based questions in traffic scenes, enriched with distance annotations. By excluding questions that require reasoning, we ensure that model performance reflects perception capabilities alone. Since automated driving hardware has limited processing power and cannot support large VLMs, our study centers on smaller VLMs. More specifically, we evaluate several state-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the simplicity of the questions, these models significantly underperform compared to humans (~60% average accuracy for the best-performing small VLM versus ~85% human performance). However, it is important to note that the human sample size was relatively small, which imposes statistical limitations. We also identify specific perception tasks, such as distinguishing left from right, that remain particularly challenging for these models.",
    "summary": "",
    "translation": "评估小型视觉语言模型在距离相关交通感知任务上的性能",
    "relevance_score": 2,
    "reasoning": "虽然论文涉及视觉语言模型(VLM)，但其焦点是交通感知这一特定领域应用，与推荐系统、搜索或广告的核心技术缺乏直接关联。该研究主要针对计算机视觉任务，没有展示在异构数据处理或推荐系统应用方面的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08318v1": {
    "title": "LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation",
    "url": "https://www.alphaxiv.org/abs/2510.08318v1",
    "arxiv_id": "2510.08318v1",
    "authors": "Yushi Huang, Xingtong Ge, Ruihao Gong, Chengtao Lv, Jun Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 15:03:39",
    "ori_summary": "Video diffusion models (DMs) have enabled high-quality video synthesis. However, their computation costs scale quadratically with sequence length because self-attention has quadratic complexity. While linear attention lowers the cost, fully replacing quadratic attention requires expensive pretraining due to the limited expressiveness of linear attention and the complexity of spatiotemporal modeling in video generation. In this paper, we present LinVideo, an efficient data-free post-training framework that replaces a target number of self-attention modules with linear attention while preserving the original model's performance. First, we observe a significant disparity in the replaceability of different layers. Instead of manual or heuristic choices, we frame layer selection as a binary classification problem and propose selective transfer, which automatically and progressively converts layers to linear attention with minimal performance impact. Additionally, to overcome the ineffectiveness and inefficiency of existing objectives for this transfer process, we introduce an anytime distribution matching (ADM) objective that aligns the distributions of samples across any timestep along the sampling trajectory. This objective is efficient and recovers model performance. Extensive experiments show that our method achieves a 1.25-2.00x speedup while preserving generation quality, and our 4-step distilled model further delivers a 15.92x latency reduction with minimal visual quality drop.",
    "summary": "",
    "translation": "LinVideo：一种实现高效视频生成中O(n)注意力的训练后框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频生成领域的注意力效率优化，属于纯粹的视觉内容生成范畴。虽然涉及注意力机制效率改进，但其应用场景仅限于视频生成，与推荐系统、搜索或广告的排名和建模需求没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08316v1": {
    "title": "Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge",
    "url": "https://www.alphaxiv.org/abs/2510.08316v1",
    "arxiv_id": "2510.08316v1",
    "authors": "Yu Huang, Zelin Peng, Changsong Wen, Xiaokang Yang, Wei Shen",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 15:01:26",
    "ori_summary": "Affordance segmentation aims to parse 3D objects into functionally distinct parts, bridging recognition and interaction for applications in robotic manipulation, embodied AI, and AR. While recent studies leverage visual or textual prompts to guide this process, they often rely on point cloud encoders as generic feature extractors, overlooking the intrinsic challenges of 3D data such as sparsity, noise, and geometric ambiguity. As a result, 3D features learned in isolation frequently lack clear and semantically consistent functional boundaries. To address this bottleneck, we propose a semantic-grounded learning paradigm that transfers rich semantic knowledge from large-scale 2D Vision Foundation Models (VFMs) into the 3D domain. Specifically, We introduce Cross-Modal Affinity Transfer (CMAT), a pre-training strategy that aligns a 3D encoder with lifted 2D semantics and jointly optimizes reconstruction, affinity, and diversity to yield semantically organized representations. Building on this backbone, we further design the Cross-modal Affordance Segmentation Transformer (CAST), which integrates multi-modal prompts with CMAT-pretrained features to generate precise, prompt-aware segmentation maps. Extensive experiments on standard benchmarks demonstrate that our framework establishes new state-of-the-art results for 3D affordance segmentation.",
    "summary": "",
    "translation": "利用2D语义知识解锁3D功能可供性分割",
    "relevance_score": 2,
    "reasoning": "该论文专注于3D视觉中的功能可供性分割，属于纯粹的计算机视觉研究领域。虽然提到了语义知识，但主要应用于3D场景理解，与推荐系统、搜索或广告的核心技术栈没有明确的直接关联或潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08305v1": {
    "title": "LTCA: Long-range Temporal Context Attention for Referring Video Object Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.08305v1",
    "arxiv_id": "2510.08305v1",
    "authors": "Cilin Yan, Jingyun Wang, Guoliang Kang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 14:55:52",
    "ori_summary": "Referring Video Segmentation (RVOS) aims to segment objects in videos given linguistic expressions. The key to solving RVOS is to extract long-range temporal context information from the interactions of expressions and videos to depict the dynamic attributes of each object. Previous works either adopt attention across all the frames or stack dense local attention to achieve a global view of temporal context. However, they fail to strike a good balance between locality and globality, and the computation complexity significantly increases with the increase of video length. In this paper, we propose an effective long-range temporal context attention (LTCA) mechanism to aggregate global context information into object features. Specifically, we aggregate the global context information from two aspects. Firstly, we stack sparse local attentions to balance the locality and globality. We design a dilated window attention across frames to aggregate local context information and perform such attention in a stack of layers to enable a global view. Further, we enable each query to attend to a small group of keys randomly selected from a global pool to enhance the globality. Secondly, we design a global query to interact with all the other queries to directly encode the global context information. Experiments show our method achieves new state-of-the-art on four referring video segmentation benchmarks. Notably, our method shows an improvement of 11.3% and 8.3% on the MeViS valu and val datasets respectively.",
    "summary": "",
    "translation": "LTCA：用于参考视频对象分割的长程时序上下文注意力机制",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉领域的视频对象分割任务，虽然涉及注意力机制，但其核心应用场景（视频对象分割）与推荐系统、搜索或广告没有直接关联。长程时序建模技术理论上可能对处理用户行为序列有启发，但这种跨领域应用过于间接且不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08279v1": {
    "title": "Learning Neural Exposure Fields for View Synthesis",
    "url": "https://www.alphaxiv.org/abs/2510.08279v1",
    "arxiv_id": "2510.08279v1",
    "authors": "Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Christina Tsalicoglou, Keisuke Tateno, Jonathan T. Barron, Federico Tombari",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-09 14:32:41",
    "ori_summary": "Recent advances in neural scene representations have led to unprecedented quality in 3D reconstruction and view synthesis. Despite achieving high-quality results for common benchmarks with curated data, outputs often degrade for data that contain per image variations such as strong exposure changes, present, e.g., in most scenes with indoor and outdoor areas or rooms with windows. In this paper, we introduce Neural Exposure Fields (NExF), a novel technique for robustly reconstructing 3D scenes with high quality and 3D-consistent appearance from challenging real-world captures. In the core, we propose to learn a neural field predicting an optimal exposure value per 3D point, enabling us to optimize exposure along with the neural scene representation. While capture devices such as cameras select optimal exposure per image/pixel, we generalize this concept and perform optimization in 3D instead. This enables accurate view synthesis in high dynamic range scenarios, bypassing the need of post-processing steps or multi-exposure captures. Our contributions include a novel neural representation for exposure prediction, a system for joint optimization of the scene representation and the exposure field via a novel neural conditioning mechanism, and demonstrated superior performance on challenging real-world data. We find that our approach trains faster than prior works and produces state-of-the-art results on several benchmarks improving by over 55% over best-performing baselines.",
    "summary": "",
    "translation": "学习神经曝光场用于视图合成",
    "relevance_score": 2,
    "reasoning": "这篇论文主要关注计算机视觉中的视图合成和神经渲染技术，属于纯粹的视觉领域研究。虽然神经场技术在3D表示方面有创新，但没有明确展示在推荐系统、搜索或广告领域的潜在应用，与当前关注的核心领域距离较远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08278v1": {
    "title": "A Multimodal Depth-Aware Method For Embodied Reference Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.08278v1",
    "arxiv_id": "2510.08278v1",
    "authors": "Fevziye Irem Eyiokur, Dogucan Yaman, Hazım Kemal Ekenel, Alexander Waibel",
    "categories": "cs.CV, cs.HC, cs.RO",
    "pub_date": "2025-10-09 14:32:21",
    "ori_summary": "Embodied Reference Understanding requires identifying a target object in a visual scene based on both language instructions and pointing cues. While prior works have shown progress in open-vocabulary object detection, they often fail in ambiguous scenarios where multiple candidate objects exist in the scene. To address these challenges, we propose a novel ERU framework that jointly leverages LLM-based data augmentation, depth-map modality, and a depth-aware decision module. This design enables robust integration of linguistic and embodied cues, improving disambiguation in complex or cluttered environments. Experimental results on two datasets demonstrate that our approach significantly outperforms existing baselines, achieving more accurate and reliable referent detection.",
    "summary": "",
    "translation": "一种用于具身参考理解的多模态深度感知方法",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及具身AI和深度感知，属于计算机视觉和机器人领域。虽然提到了多模态，但核心焦点是具身参考理解（机器人导航和交互），与推荐系统、搜索或广告的直接相关性较弱。其技术可能间接应用于需要空间理解的场景，但应用潜力有限且不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08273v1": {
    "title": "One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion Models for Text-Guided Image Inpainting",
    "url": "https://www.alphaxiv.org/abs/2510.08273v1",
    "arxiv_id": "2510.08273v1",
    "authors": "Haipeng Liu, Yang Wang, Meng Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 14:30:34",
    "ori_summary": "Text-guided image inpainting aims at reconstructing the masked regions as per text prompts, where the longstanding challenges lie in the preservation for unmasked regions, while achieving the semantics consistency between unmasked and inpainted masked regions. Previous arts failed to address both of them, always with either of them to be remedied. Such facts, as we observed, stem from the entanglement of the hybrid (e.g., mid-and-low) frequency bands that encode varied image properties, which exhibit different robustness to text prompts during the denoising process. In this paper, we propose a null-text-null frequency-aware diffusion models, dubbed \\textbf{NTN-Diff}, for text-guided image inpainting, by decomposing the semantics consistency across masked and unmasked regions into the consistencies as per each frequency band, while preserving the unmasked regions, to circumvent two challenges in a row. Based on the diffusion process, we further divide the denoising process into early (high-level noise) and late (low-level noise) stages, where the mid-and-low frequency bands are disentangled during the denoising process. As observed, the stable mid-frequency band is progressively denoised to be semantically aligned during text-guided denoising process, which, meanwhile, serves as the guidance to the null-text denoising process to denoise low-frequency band for the masked regions, followed by a subsequent text-guided denoising process at late stage, to achieve the semantics consistency for mid-and-low frequency bands across masked and unmasked regions, while preserve the unmasked regions. Extensive experiments validate the superiority of NTN-Diff over the state-of-the-art diffusion models to text-guided diffusion models. Our code can be accessed from https://github.com/htyjers/NTN-Diff.",
    "summary": "",
    "translation": "一石二鸟：面向文本引导图像修复的空文本-空频率感知扩散模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本引导的图像修复和扩散模型，这属于纯粹的视觉内容生成领域。虽然扩散模型是LLM相关技术，但该工作专注于图像修复的具体应用，与推荐系统、搜索或广告中的排序、检索或用户建模没有直接关联。该技术没有明显的潜力应用于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08271v1": {
    "title": "SViM3D: Stable Video Material Diffusion for Single Image 3D Generation",
    "url": "https://www.alphaxiv.org/abs/2510.08271v1",
    "arxiv_id": "2510.08271v1",
    "authors": "Andreas Engelhardt, Mark Boss, Vikram Voletti, Chun-Han Yao, Hendrik P. A. Lensch, Varun Jampani",
    "categories": "cs.GR, cs.CV",
    "pub_date": "2025-10-09 14:29:47",
    "ori_summary": "We present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically based rendering (PBR) materials, given a single image. Recently, video diffusion models have been successfully used to reconstruct 3D objects from a single image efficiently. However, reflectance is still represented by simple material models or needs to be estimated in additional steps to enable relighting and controlled appearance edits. We extend a latent video diffusion model to output spatially varying PBR parameters and surface normals jointly with each generated view based on explicit camera control. This unique setup allows for relighting and generating a 3D asset using our model as neural prior. We introduce various mechanisms to this pipeline that improve quality in this ill-posed setting. We show state-of-the-art relighting and novel view synthesis performance on multiple object-centric datasets. Our method generalizes to diverse inputs, enabling the generation of relightable 3D assets useful in AR/VR, movies, games and other visual media.",
    "summary": "",
    "translation": "SViM3D：基于稳定视频材料扩散的单图像3D生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D视觉生成领域，涉及单图像到3D的转换和视频材料扩散技术，属于纯粹的计算机视觉研究方向。虽然标题提到扩散模型，但核心应用场景是3D内容生成，与推荐系统、搜索或广告的排名、匹配、用户建模等核心任务没有直接关联。该技术缺乏在RecSys/Search/Ads领域的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08269v1": {
    "title": "Adaptive Gradient Calibration for Single-Positive Multi-Label Learning in Remote Sensing Image Scene Classification",
    "url": "https://www.alphaxiv.org/abs/2510.08269v1",
    "arxiv_id": "2510.08269v1",
    "authors": "Chenying Liu, Gianmarco Perantoni, Lorenzo Bruzzone, Xiao Xiang Zhu",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 14:26:09",
    "ori_summary": "Multi-label classification (MLC) offers a more comprehensive semantic understanding of Remote Sensing (RS) imagery compared to traditional single-label classification (SLC). However, obtaining complete annotations for MLC is particularly challenging due to the complexity and high cost of the labeling process. As a practical alternative, single-positive multi-label learning (SPML) has emerged, where each image is annotated with only one relevant label, and the model is expected to recover the full set of labels. While scalable, SPML introduces significant supervision ambiguity, demanding specialized solutions for model training. Although various SPML methods have been proposed in the computer vision domain, research in the RS context remains limited. To bridge this gap, we propose Adaptive Gradient Calibration (AdaGC), a novel and generalizable SPML framework tailored to RS imagery. AdaGC adopts a gradient calibration (GC) mechanism combined with Mixup and a dual exponential moving average (EMA) module for robust pseudo-label generation. To maximize AdaGC's effectiveness, we introduce a simple yet theoretically grounded indicator to adaptively trigger GC after an initial warm-up stage based on training dynamics, thereby guaranteeing the effectiveness of GC in mitigating overfitting to label noise. Extensive experiments on two benchmark RS datasets under two distinct label noise types demonstrate that AdaGC achieves state-of-the-art (SOTA) performance while maintaining strong robustness across diverse settings.",
    "summary": "",
    "translation": "遥感图像场景分类中单正例多标签学习的自适应梯度校准",
    "relevance_score": 1,
    "reasoning": "该论文专注于遥感图像场景分类，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告没有明显关联。论文讨论的单正例多标签学习和梯度校准技术是特定于图像分类任务的，无法看出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08260v1": {
    "title": "Fine-grained text-driven dual-human motion generation via dynamic hierarchical interaction",
    "url": "https://www.alphaxiv.org/abs/2510.08260v1",
    "arxiv_id": "2510.08260v1",
    "authors": "Mu Li, Yin Wang, Zhiying Leng, Jiapeng Liu, Frederick W. B. Li, Xiaohui Liang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 14:18:53",
    "ori_summary": "Human interaction is inherently dynamic and hierarchical, where the dynamic refers to the motion changes with distance, and the hierarchy is from individual to inter-individual and ultimately to overall motion. Exploiting these properties is vital for dual-human motion generation, while existing methods almost model human interaction temporally invariantly, ignoring distance and hierarchy. To address it, we propose a fine-grained dual-human motion generation method, namely FineDual, a tri-stage method to model the dynamic hierarchical interaction from individual to inter-individual. The first stage, Self-Learning Stage, divides the dual-human overall text into individual texts through a Large Language Model, aligning text features and motion features at the individual level. The second stage, Adaptive Adjustment Stage, predicts interaction distance by an interaction distance predictor, modeling human interactions dynamically at the inter-individual level by an interaction-aware graph network. The last stage, Teacher-Guided Refinement Stage, utilizes overall text features as guidance to refine motion features at the overall level, generating fine-grained and high-quality dual-human motion. Extensive quantitative and qualitative evaluations on dual-human motion datasets demonstrate that our proposed FineDual outperforms existing approaches, effectively modeling dynamic hierarchical human interaction.",
    "summary": "",
    "translation": "基于细粒度文本驱动的双人动作生成：通过动态分层交互实现",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本驱动的双人动作生成，属于计算机图形学或动画领域，与推荐系统、搜索或广告的核心技术无关。虽然涉及文本到动作的生成，但这本质上是内容生成任务，属于被明确排除的AIGC和纯视觉应用范畴，没有明显的推荐、搜索或广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08181v1": {
    "title": "InstructUDrag: Joint Text Instructions and Object Dragging for Interactive Image Editing",
    "url": "https://www.alphaxiv.org/abs/2510.08181v1",
    "arxiv_id": "2510.08181v1",
    "authors": "Haoran Yu, Yi Shi",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 13:06:49",
    "ori_summary": "Text-to-image diffusion models have shown great potential for image editing, with techniques such as text-based and object-dragging methods emerging as key approaches. However, each of these methods has inherent limitations: text-based methods struggle with precise object positioning, while object dragging methods are confined to static relocation. To address these issues, we propose InstructUDrag, a diffusion-based framework that combines text instructions with object dragging, enabling simultaneous object dragging and text-based image editing. Our framework treats object dragging as an image reconstruction process, divided into two synergistic branches. The moving-reconstruction branch utilizes energy-based gradient guidance to move objects accurately, refining cross-attention maps to enhance relocation precision. The text-driven editing branch shares gradient signals with the reconstruction branch, ensuring consistent transformations and allowing fine-grained control over object attributes. We also employ DDPM inversion and inject prior information into noise maps to preserve the structure of moved objects. Extensive experiments demonstrate that InstructUDrag facilitates flexible, high-fidelity image editing, offering both precision in object relocation and semantic control over image content.",
    "summary": "",
    "translation": "InstructUDrag：基于联合文本指令和对象拖拽的交互式图像编辑",
    "relevance_score": 1,
    "reasoning": "该论文专注于交互式图像编辑技术，涉及文本指令和对象拖拽操作，属于纯粹的计算机视觉和图像生成领域。虽然标题提到'文本指令'，但核心是图像编辑而非语言模型在推荐/搜索/广告中的应用，与当前关注的推荐系统、搜索广告、Transformer架构进展或异构数据统一建模等焦点完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08179v1": {
    "title": "Dual-granularity Sinkhorn Distillation for Enhanced Learning from Long-tailed Noisy Data",
    "url": "https://www.alphaxiv.org/abs/2510.08179v1",
    "arxiv_id": "2510.08179v1",
    "authors": "Feng Hong, Yu Huang, Zihua Zhao, Zhihan Zhou, Jiangchao Yao, Dongsheng Li, Ya Zhang, Yanfeng Wang",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-09 13:05:27",
    "ori_summary": "Real-world datasets for deep learning frequently suffer from the co-occurring challenges of class imbalance and label noise, hindering model performance. While methods exist for each issue, effectively combining them is non-trivial, as distinguishing genuine tail samples from noisy data proves difficult, often leading to conflicting optimization strategies. This paper presents a novel perspective: instead of primarily developing new complex techniques from scratch, we explore synergistically leveraging well-established, individually 'weak' auxiliary models - specialized for tackling either class imbalance or label noise but not both. This view is motivated by the insight that class imbalance (a distributional-level concern) and label noise (a sample-level concern) operate at different granularities, suggesting that robustness mechanisms for each can in principle offer complementary strengths without conflict. We propose Dual-granularity Sinkhorn Distillation (D-SINK), a novel framework that enhances dual robustness by distilling and integrating complementary insights from such 'weak', single-purpose auxiliary models. Specifically, D-SINK uses an optimal transport-optimized surrogate label allocation to align the target model's sample-level predictions with a noise-robust auxiliary and its class distributions with an imbalance-robust one. Extensive experiments on benchmark datasets demonstrate that D-SINK significantly improves robustness and achieves strong empirical performance in learning from long-tailed noisy data.",
    "summary": "",
    "translation": "双粒度Sinkhorn蒸馏用于增强长尾噪声数据学习",
    "relevance_score": 4,
    "reasoning": "该论文提出了一种处理长尾噪声数据的方法，这在推荐系统中处理真实世界用户行为数据时具有潜在应用价值。然而，该方法主要关注通用数据分布问题，没有明确针对推荐、搜索或广告领域的特定挑战，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.08178v1": {
    "title": "Robust Canonicalization through Bootstrapped Data Re-Alignment",
    "url": "https://www.alphaxiv.org/abs/2510.08178v1",
    "arxiv_id": "2510.08178v1",
    "authors": "Johann Schmidt, Sebastian Stober",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-09 13:05:20",
    "ori_summary": "Fine-grained visual classification (FGVC) tasks, such as insect and bird identification, demand sensitivity to subtle visual cues while remaining robust to spatial transformations. A key challenge is handling geometric biases and noise, such as different orientations and scales of objects. Existing remedies rely on heavy data augmentation, which demands powerful models, or on equivariant architectures, which constrain expressivity and add cost. Canonicalization offers an alternative by shielding such biases from the downstream model. In practice, such functions are often obtained using canonicalization priors, which assume aligned training data. Unfortunately, real-world datasets never fulfill this assumption, causing the obtained canonicalizer to be brittle. We propose a bootstrapping algorithm that iteratively re-aligns training samples by progressively reducing variance and recovering the alignment assumption. We establish convergence guarantees under mild conditions for arbitrary compact groups, and show on four FGVC benchmarks that our method consistently outperforms equivariant, and canonicalization baselines while performing on par with augmentation.",
    "summary": "",
    "translation": "通过自举数据重对齐实现鲁棒规范化",
    "relevance_score": 3,
    "reasoning": "该论文标题暗示了数据对齐和规范化技术，这在推荐系统和搜索中可能用于处理异构数据源或特征工程。然而，标题过于宽泛，没有明确说明与Transformer架构、LLM技术或推荐/搜索/广告系统的直接关联，潜在应用不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08157v1": {
    "title": "Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence Reasoning for Image Editing",
    "url": "https://www.alphaxiv.org/abs/2510.08157v1",
    "arxiv_id": "2510.08157v1",
    "authors": "Zhentao Zou, Zhengrong Yue, Kunpeng Du, Binlei Bao, Hanting Li, Haizhen Xie, Guozheng Xu, Yue Zhou, Yali Wang, Jie Hu, Xue Jiang, Xinghao Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 12:36:51",
    "ori_summary": "Image editing with natural language has gained significant popularity, yet existing methods struggle with intricate object intersections and fine-grained spatial relationships due to the lack of an explicit reasoning process. While Chain-of-Thought (CoT) has been explored to enhance reasoning, purely textual CoT or CoT augmented with coordinate information is fundamentally limited in its ability to represent intricate visual layouts and lacks the necessary visual cues to guide the generation of fine-grained, pixel-level details. To address these challenges, we propose Multimodal Reasoning Edit (MURE), a novel framework that shifts the visual editing process from purely text-based reasoning to a series of interleaved textual and visual rationales. Our framework performs image editing using a natively multimodal, interleaved text-image CoT. This approach generates a step-by-step chain of reasoning where a textual description is followed by a corresponding visual cue, such as a positional mask that defined intended edited regions or a representation of new content. Furthermore, to mitigate the hallucination phenomenon of large language models, we introduce Multimodal Deep Confidence (MMDC) reasoning paradigm. This paradigm explores a tree of visual reasoning paths at each step. By pruning low-quality branches using a deep confidence score from a reward model, it ensures the model consistently follows a high-quality trajectory towards the final edited result. The proposed method decomposes complex editing tasks into interdependent sub-tasks, achieving greater precision at each stage and yielding high-fidelity edited results. We define the formulation for interleaved text-image chains and release the first CoT-Edit-14K dataset, comprising 14K high-quality editing examples. Extensive experiments show that our method yields significant improvements across three image editing benchmarks.",
    "summary": "",
    "translation": "超越文本思维链：基于深度置信推理的交错文本-图像链用于图像编辑",
    "relevance_score": 1,
    "reasoning": "该论文专注于图像编辑任务，涉及视觉模态和内容生成，属于纯粹的视觉应用领域。虽然提到了置信推理，但核心关注点是图像编辑而非推荐系统、搜索或广告中的排序或理解任务。该工作没有明显的潜在应用可以转移到RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08143v1": {
    "title": "UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution",
    "url": "https://www.alphaxiv.org/abs/2510.08143v1",
    "arxiv_id": "2510.08143v1",
    "authors": "Shian Du, Menghan Xia, Chang Liu, Quande Liu, Xintao Wang, Pengfei Wan, Xiangyang Ji",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 12:25:16",
    "ori_summary": "Cascaded video super-resolution has emerged as a promising technique for decoupling the computational burden associated with generating high-resolution videos using large foundation models. Existing studies, however, are largely confined to text-to-video tasks and fail to leverage additional generative conditions beyond text, which are crucial for ensuring fidelity in multi-modal video generation. We address this limitation by presenting UniMMVSR, the first unified generative video super-resolution framework to incorporate hybrid-modal conditions, including text, images, and videos. We conduct a comprehensive exploration of condition injection strategies, training schemes, and data mixture techniques within a latent video diffusion model. A key challenge was designing distinct data construction and condition utilization methods to enable the model to precisely utilize all condition types, given their varied correlations with the target video. Our experiments demonstrate that UniMMVSR significantly outperforms existing methods, producing videos with superior detail and a higher degree of conformity to multi-modal conditions. We also validate the feasibility of combining UniMMVSR with a base model to achieve multi-modal guided generation of 4K video, a feat previously unattainable with existing techniques.",
    "summary": "",
    "translation": "UniMMVSR：一种用于级联视频超分辨率的统一多模态框架",
    "relevance_score": 2,
    "reasoning": "该论文专注于视频超分辨率这一计算机视觉任务，虽然涉及多模态框架，但其核心应用场景是视频质量增强而非推荐、搜索或广告系统。视频超分辨率技术对RecSys/Search/Ads的潜在应用非常有限，可能仅在某些需要高质量视频预览的电商场景中有微弱关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08138v1": {
    "title": "Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement",
    "url": "https://www.alphaxiv.org/abs/2510.08138v1",
    "arxiv_id": "2510.08138v1",
    "authors": "Chengzhi Li, Heyan Huang, Ping Jian, Zhen Yang, Yaning Tian",
    "categories": "cs.CV, cs.AI, cs.MM",
    "pub_date": "2025-10-09 12:22:06",
    "ori_summary": "Large language models (LLMs) often generate self-contradictory outputs, which severely impacts their reliability and hinders their adoption in practical applications. In video-language models (Video-LLMs), this phenomenon recently draws the attention of researchers. Specifically, these models fail to provide logically consistent responses to rephrased questions based on their grounding outputs. However, the underlying causes of this phenomenon remain underexplored. In this work, we adopt an interpretability-driven approach to analyze, statistically summarize, and intervention the potential factors of the phenomenon. We find that one of the primary reasons for the inconsistency in responses lies in the inability of cross-modal attention heads to effectively distinguish video tokens across different timestamps. To address this, we propose an attention enhancement method called Temporally Conditioned Attention Sharpening (TCAS), which constructs an enhancement objective based on attention distinctions to enhance the model's temporal resolution capability, thereby improving its temporal understanding logic consistency. Experimental results demonstrate that our method significantly enhances the temporal logic consistency of Video-LLMs. Further interpretability analyses reveal that our method indeed improves the temporal discriminability of attention heads, validating our conclusions. Additionally, our method achieves performance improvements in general video temporal grounding tasks, highlighting that temporal logic consistency is a bottleneck in temporal understanding. By enhancing consistency, our method drives significant progress in video temporal understanding.",
    "summary": "",
    "translation": "通过注意力增强改进视频语言模型中的时序理解逻辑一致性",
    "relevance_score": 2,
    "reasoning": "虽然这篇论文涉及多模态建模和注意力机制，但其核心焦点是视频-语言模型中的时序理解，这与推荐系统、搜索或广告的直接应用相关性较弱。注意力增强技术可能有潜在的效率改进，但论文主要针对视频时序逻辑而非推荐/搜索场景中的异构数据处理。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08131v1": {
    "title": "Real-Time Motion-Controllable Autoregressive Video Diffusion",
    "url": "https://www.alphaxiv.org/abs/2510.08131v1",
    "arxiv_id": "2510.08131v1",
    "authors": "Kesen Zhao, Jiaxin Shi, Beier Zhu, Junbao Zhou, Xiaolong Shen, Yuan Zhou, Qianru Sun, Hanwang Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 12:17:11",
    "ori_summary": "Real-time motion-controllable video generation remains challenging due to the inherent latency of bidirectional diffusion models and the lack of effective autoregressive (AR) approaches. Existing AR video diffusion models are limited to simple control signals or text-to-video generation, and often suffer from quality degradation and motion artifacts in few-step generation. To address these challenges, we propose AR-Drag, the first RL-enhanced few-step AR video diffusion model for real-time image-to-video generation with diverse motion control. We first fine-tune a base I2V model to support basic motion control, then further improve it via reinforcement learning with a trajectory-based reward model. Our design preserves the Markov property through a Self-Rollout mechanism and accelerates training by selectively introducing stochasticity in denoising steps. Extensive experiments demonstrate that AR-Drag achieves high visual fidelity and precise motion alignment, significantly reducing latency compared with state-of-the-art motion-controllable VDMs, while using only 1.3B parameters. Additional visualizations can be found on our project page: https://kesenzhao.github.io/AR-Drag.github.io/.",
    "summary": "",
    "translation": "实时运动可控自回归视频扩散模型",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于视频生成和运动控制，属于纯粹的视觉内容生成领域。虽然提到了自回归建模，但核心关注点是视频生成技术，与推荐系统、搜索或广告的排名和建模需求没有直接关联。该技术缺乏在RecSys/Search/Ads领域的明确应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08116v1": {
    "title": "Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.08116v1",
    "arxiv_id": "2510.08116v1",
    "authors": "Eirik A. Østmo, Kristoffer K. Wickstrøm, Keyur Radiya, Michael C. Kampffmeyer, Karl Øyvind Mikalsen, Robert Jenssen",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-09 11:57:04",
    "ori_summary": "Contrast-enhanced Computed Tomography (CT) is important for diagnosis and treatment planning for various medical conditions. Deep learning (DL) based segmentation models may enable automated medical image analysis for detecting and delineating tumors in CT images, thereby reducing clinicians' workload. Achieving generalization capabilities in limited data domains, such as radiology, requires modern DL models to be trained with image augmentation. However, naively applying augmentation methods developed for natural images to CT scans often disregards the nature of the CT modality, where the intensities measure Hounsfield Units (HU) and have important physical meaning. This paper challenges the use of such intensity augmentations for CT imaging and shows that they may lead to artifacts and poor generalization. To mitigate this, we propose a CT-specific augmentation technique, called Random windowing, that exploits the available HU distribution of intensities in CT images. Random windowing encourages robustness to contrast-enhancement and significantly increases model performance on challenging images with poor contrast or timing. We perform ablations and analysis of our method on multiple datasets, and compare to, and outperform, state-of-the-art alternatives, while focusing on the challenge of liver tumor segmentation.",
    "summary": "",
    "translation": "用于CT和肝脏肿瘤分割中深度学习鲁棒性的随机窗口增强",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像分割领域，特别是CT扫描和肝脏肿瘤分割，这属于医学/生物学应用范畴。虽然提到了数据增强技术，但该技术完全应用于医疗领域，与推荐系统、搜索或广告没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08096v1": {
    "title": "Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting",
    "url": "https://www.alphaxiv.org/abs/2510.08096v1",
    "arxiv_id": "2510.08096v1",
    "authors": "Ankit Gahlawat, Anirban Mukherjee, Dinesh Babu Jayagopi",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 11:34:55",
    "ori_summary": "Accurate face parsing under extreme viewing angles remains a significant challenge due to limited labeled data in such poses. Manual annotation is costly and often impractical at scale. We propose a novel label refinement pipeline that leverages 3D Gaussian Splatting (3DGS) to generate accurate segmentation masks from noisy multiview predictions. By jointly fitting two 3DGS models, one to RGB images and one to their initial segmentation maps, our method enforces multiview consistency through shared geometry, enabling the synthesis of pose-diverse training data with only minimal post-processing. Fine-tuning a face parsing model on this refined dataset significantly improves accuracy on challenging head poses, while maintaining strong performance on standard views. Extensive experiments, including human evaluations, demonstrate that our approach achieves superior results compared to state-of-the-art methods, despite requiring no ground-truth 3D annotations and using only a small set of initial images. Our method offers a scalable and effective solution for improving face parsing robustness in real-world settings.",
    "summary": "",
    "translation": "基于3D高斯溅射的极端姿态下人脸解析高效标签精炼",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的人脸解析和3D高斯溅射技术，属于纯粹的视觉处理范畴。论文内容涉及人脸姿态处理和标签精炼，与推荐系统、搜索或广告的核心技术栈没有直接关联，也无法识别出在推荐/搜索/广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08094v1": {
    "title": "DarkHash: A Data-Free Backdoor Attack Against Deep Hashing",
    "url": "https://www.alphaxiv.org/abs/2510.08094v1",
    "arxiv_id": "2510.08094v1",
    "authors": "Ziqi Zhou, Menghao Deng, Yufei Song, Hangtao Zhang, Wei Wan, Shengshan Hu, Minghui Li, Leo Yu Zhang, Dezhong Yao",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 11:28:23",
    "ori_summary": "Benefiting from its superior feature learning capabilities and efficiency, deep hashing has achieved remarkable success in large-scale image retrieval. Recent studies have demonstrated the vulnerability of deep hashing models to backdoor attacks. Although these studies have shown promising attack results, they rely on access to the training dataset to implant the backdoor. In the real world, obtaining such data (e.g., identity information) is often prohibited due to privacy protection and intellectual property concerns. Embedding backdoors into deep hashing models without access to the training data, while maintaining retrieval accuracy for the original task, presents a novel and challenging problem. In this paper, we propose DarkHash, the first data-free backdoor attack against deep hashing. Specifically, we design a novel shadow backdoor attack framework with dual-semantic guidance. It embeds backdoor functionality and maintains original retrieval accuracy by fine-tuning only specific layers of the victim model using a surrogate dataset. We consider leveraging the relationship between individual samples and their neighbors to enhance backdoor attacks during training. By designing a topological alignment loss, we optimize both individual and neighboring poisoned samples toward the target sample, further enhancing the attack capability. Experimental results on four image datasets, five model architectures, and two hashing methods demonstrate the high effectiveness of DarkHash, outperforming existing state-of-the-art backdoor attack methods. Defense experiments show that DarkHash can withstand existing mainstream backdoor defense methods.",
    "summary": "",
    "translation": "DarkHash：一种针对深度哈希的无数据后门攻击",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于深度哈希系统的后门攻击和安全性问题，这属于网络安全和模型安全领域。虽然深度哈希技术可能在某些推荐系统中用于相似性搜索，但论文的核心焦点是攻击方法和安全漏洞，这明确属于被排除的\"安全、隐私\"等非技术性话题范畴。该研究没有涉及推荐系统、搜索或广告的核心算法改进或LLM技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08073v1": {
    "title": "Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection",
    "url": "https://www.alphaxiv.org/abs/2510.08073v1",
    "arxiv_id": "2510.08073v1",
    "authors": "Shuhai Zhang, ZiHao Lian, Jiahao Yang, Daiyuan Li, Guoxuan Pang, Feng Liu, Bo Han, Shutao Li, Mingkui Tan",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-09 11:00:35",
    "ori_summary": "AI-generated videos have achieved near-perfect visual realism (e.g., Sora), urgently necessitating reliable detection mechanisms. However, detecting such videos faces significant challenges in modeling high-dimensional spatiotemporal dynamics and identifying subtle anomalies that violate physical laws. In this paper, we propose a physics-driven AI-generated video detection paradigm based on probability flow conservation principles. Specifically, we propose a statistic called Normalized Spatiotemporal Gradient (NSG), which quantifies the ratio of spatial probability gradients to temporal density changes, explicitly capturing deviations from natural video dynamics. Leveraging pre-trained diffusion models, we develop an NSG estimator through spatial gradients approximation and motion-aware temporal modeling without complex motion decomposition while preserving physical constraints. Building on this, we propose an NSG-based video detection method (NSG-VD) that computes the Maximum Mean Discrepancy (MMD) between NSG features of the test and real videos as a detection metric. Last, we derive an upper bound of NSG feature distances between real and generated videos, proving that generated videos exhibit amplified discrepancies due to distributional shifts. Extensive experiments confirm that NSG-VD outperforms state-of-the-art baselines by 16.00% in Recall and 10.75% in F1-Score, validating the superior performance of NSG-VD. The source code is available at https://github.com/ZSHsh98/NSG-VD.",
    "summary": "",
    "translation": "物理驱动的时空建模用于AI生成视频检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于AI生成视频检测这一特定计算机视觉任务，属于内容检测和鉴伪领域。虽然涉及AI生成内容，但这是纯粹的视觉应用，与推荐系统、搜索或广告中的排序、匹配、用户建模等核心问题没有直接关联。论文的物理驱动建模方法也没有显示出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08067v1": {
    "title": "Towards Real-World Deepfake Detection: A Diverse In-the-wild Dataset of Forgery Faces",
    "url": "https://www.alphaxiv.org/abs/2510.08067v1",
    "arxiv_id": "2510.08067v1",
    "authors": "Junyu Shi, Minghui Li, Junguo Zuo, Zhifei Yu, Yipeng Lin, Shengshan Hu, Ziqi Zhou, Yechao Zhang, Wei Wan, Yinzhe Xu, Leo Yu Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 10:54:38",
    "ori_summary": "Deepfakes, leveraging advanced AIGC (Artificial Intelligence-Generated Content) techniques, create hyper-realistic synthetic images and videos of human faces, posing a significant threat to the authenticity of social media. While this real-world threat is increasingly prevalent, existing academic evaluations and benchmarks for detecting deepfake forgery often fall short to achieve effective application for their lack of specificity, limited deepfake diversity, restricted manipulation techniques.To address these limitations, we introduce RedFace (Real-world-oriented Deepfake Face), a specialized facial deepfake dataset, comprising over 60,000 forged images and 1,000 manipulated videos derived from authentic facial features, to bridge the gap between academic evaluations and real-world necessity. Unlike prior benchmarks, which typically rely on academic methods to generate deepfakes, RedFace utilizes 9 commercial online platforms to integrate the latest deepfake technologies found \"in the wild\", effectively simulating real-world black-box scenarios.Moreover, RedFace's deepfakes are synthesized using bespoke algorithms, allowing it to capture diverse and evolving methods used by real-world deepfake creators. Extensive experimental results on RedFace (including cross-domain, intra-domain, and real-world social network dissemination simulations) verify the limited practicality of existing deepfake detection schemes against real-world applications. We further perform a detailed analysis of the RedFace dataset, elucidating the reason of its impact on detection performance compared to conventional datasets. Our dataset is available at: https://github.com/kikyou-220/RedFace.",
    "summary": "",
    "translation": "迈向真实世界深度伪造检测：一个多样化的野外伪造人脸数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于深度伪造检测和计算机视觉安全领域，与推荐系统、搜索或广告的核心技术无关。虽然深度伪造检测在内容审核中有应用，但这属于安全/信任范畴，属于明确排除的非技术性话题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08060v1": {
    "title": "A class-driven hierarchical ResNet for classification of multispectral remote sensing images",
    "url": "https://www.alphaxiv.org/abs/2510.08060v1",
    "arxiv_id": "2510.08060v1",
    "authors": "Giulio Weikmann, Gianmarco Perantoni, Lorenzo Bruzzone",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 10:47:52",
    "ori_summary": "This work presents a multitemporal class-driven hierarchical Residual Neural Network (ResNet) designed for modelling the classification of Time Series (TS) of multispectral images at different semantical class levels. The architecture consists of a modification of the ResNet where we introduce additional branches to perform the classification at the different hierarchy levels and leverage on hierarchy-penalty maps to discourage incoherent hierarchical transitions within the classification. In this way, we improve the discrimination capabilities of classes at different levels of semantic details and train a modular architecture that can be used as a backbone network for introducing new specific classes and additional tasks considering limited training samples available. We exploit the class-hierarchy labels to train efficiently the different layers of the architecture, allowing the first layers to train faster on the first levels of the hierarchy modeling general classes (i.e., the macro-classes) and the intermediate classes, while using the last ones to discriminate more specific classes (i.e., the micro-classes). In this way, the targets are constrained in following the hierarchy defined, improving the classification of classes at the most detailed level. The proposed modular network has intrinsic adaptation capability that can be obtained through fine tuning. The experimental results, obtained on two tiles of the Amazonian Forest on 12 monthly composites of Sentinel 2 images acquired during 2019, demonstrate the effectiveness of the hierarchical approach in both generalizing over different hierarchical levels and learning discriminant features for an accurate classification at the micro-class level on a new target area, with a better representation of the minoritarian classes.",
    "summary": "",
    "translation": "一种用于多光谱遥感图像分类的类驱动分层残差网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于多光谱遥感图像分类，属于纯粹的计算机视觉应用领域，与推荐系统、搜索或广告没有任何直接关联。论文提出的类驱动分层残差网络架构是特定于遥感图像处理的解决方案，没有显示出在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08054v1": {
    "title": "RetouchLLM: Training-free White-box Image Retouching",
    "url": "https://www.alphaxiv.org/abs/2510.08054v1",
    "arxiv_id": "2510.08054v1",
    "authors": "Moon Ye-Bin, Roy Miles, Tae-Hyun Oh, Ismail Elezi, Jiankang Deng",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 10:40:49",
    "ori_summary": "Image retouching not only enhances visual quality but also serves as a means of expressing personal preferences and emotions. However, existing learning-based approaches require large-scale paired data and operate as black boxes, making the retouching process opaque and limiting their adaptability to handle diverse, user- or image-specific adjustments. In this work, we propose RetouchLLM, a training-free white-box image retouching system, which requires no training data and performs interpretable, code-based retouching directly on high-resolution images. Our framework progressively enhances the image in a manner similar to how humans perform multi-step retouching, allowing exploration of diverse adjustment paths. It comprises of two main modules: a visual critic that identifies differences between the input and reference images, and a code generator that produces executable codes. Experiments demonstrate that our approach generalizes well across diverse retouching styles, while natural language-based user interaction enables interpretable and controllable adjustments tailored to user intent.",
    "summary": "",
    "translation": "RetouchLLM：无需训练的白盒图像润色",
    "relevance_score": 1,
    "reasoning": "该论文专注于图像处理领域的白盒图像润色技术，属于纯粹的计算机视觉应用。虽然涉及LLM，但核心是图像编辑任务，与推荐系统、搜索或广告的排名和建模需求没有直接关联。该技术不具备在RecSys/Search/Ads领域的明显应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08052v1": {
    "title": "RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings for Weakly Supervised Anomaly Detection in Brain MRI Scans",
    "url": "https://www.alphaxiv.org/abs/2510.08052v1",
    "arxiv_id": "2510.08052v1",
    "authors": "Bheeshm Sharma, Karthikeyan Jaganathan, Balamurugan Palaniappan",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 10:37:47",
    "ori_summary": "Weakly Supervised Anomaly detection (WSAD) in brain MRI scans is an important challenge useful to obtain quick and accurate detection of brain anomalies when precise pixel-level anomaly annotations are unavailable and only weak labels (e.g., slice-level) are available. In this work, we propose RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings, a novel two-stage WSAD framework. In the first stage, we introduce a Discriminative Dual Prompt Tuning (DDPT) mechanism that generates high-quality pseudo weak masks based on slice-level labels, serving as coarse localization cues. In the second stage, we propose a segmentation network with a region-aware spatial attention mechanism that relies on fixed location-based random embeddings. This design enables the model to effectively focus on anomalous regions. Our approach achieves state-of-the-art anomaly detection performance, significantly outperforming existing WSAD methods while utilizing less than 8 million parameters. Extensive evaluations on the BraTS20, BraTS21, BraTS23, and MSD datasets demonstrate a substantial performance improvement coupled with a significant reduction in computational complexity. Code is available at: https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/.",
    "summary": "",
    "translation": "RASALoRE：基于区域感知空间注意力与位置随机嵌入的脑部MRI扫描弱监督异常检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（脑部MRI）的异常检测，属于医疗领域的特定应用。虽然提到了注意力机制，但其核心应用场景（脑部MRI）与推荐系统、搜索或广告领域完全无关。该技术没有明显的潜力应用于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08017v1": {
    "title": "RayFusion: Ray Fusion Enhanced Collaborative Visual Perception",
    "url": "https://www.alphaxiv.org/abs/2510.08017v1",
    "arxiv_id": "2510.08017v1",
    "authors": "Shaohong Wang, Bin Lu, Xinyu Xiao, Hanzhi Zhong, Bowen Pang, Tong Wang, Zhiyu Xiang, Hangguan Shan, Eryun Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 09:54:08",
    "ori_summary": "Collaborative visual perception methods have gained widespread attention in the autonomous driving community in recent years due to their ability to address sensor limitation problems. However, the absence of explicit depth information often makes it difficult for camera-based perception systems, e.g., 3D object detection, to generate accurate predictions. To alleviate the ambiguity in depth estimation, we propose RayFusion, a ray-based fusion method for collaborative visual perception. Using ray occupancy information from collaborators, RayFusion reduces redundancy and false positive predictions along camera rays, enhancing the detection performance of purely camera-based collaborative perception systems. Comprehensive experiments show that our method consistently outperforms existing state-of-the-art models, substantially advancing the performance of collaborative visual perception. The code is available at https://github.com/wangsh0111/RayFusion.",
    "summary": "",
    "translation": "RayFusion：射线融合增强的协同视觉感知",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉领域的协同感知技术，虽然涉及多模态融合概念，但主要应用于自动驾驶或机器人视觉等场景。在推荐系统、搜索或广告领域，这种射线融合的视觉感知技术缺乏明确的直接应用场景，与异构数据统一建模的关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.08003v1": {
    "title": "CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.08003v1",
    "arxiv_id": "2510.08003v1",
    "authors": "Weihuang Lin, Yiwei Ma, Jiayi Ji, Xiaoshuai Sun, Rongrong Ji",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 09:41:45",
    "ori_summary": "Composed Image Retrieval (CIR), which aims to find a target image from a reference image and a modification text, presents the core challenge of performing unified reasoning across visual and semantic modalities. While current approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more recent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown progress, they predominantly function as ``black boxes.\" This inherent opacity not only prevents users from understanding the retrieval rationale but also restricts the models' ability to follow complex, fine-grained instructions. To overcome these limitations, we introduce CIR-CoT, the first end-to-end retrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT) reasoning. By compelling the model to first generate an interpretable reasoning chain, CIR-CoT enhances its ability to capture crucial cross-modal interactions, leading to more accurate retrieval while making its decision process transparent. Since existing datasets like FashionIQ and CIRR lack the necessary reasoning data, a key contribution of our work is the creation of structured CoT annotations using a three-stage process involving a caption, reasoning, and conclusion. Our model is then fine-tuned to produce this structured output before encoding its final retrieval intent into a dedicated embedding. Comprehensive experiments show that CIR-CoT achieves highly competitive performance on in-domain datasets (FashionIQ, CIRR) and demonstrates remarkable generalization on the out-of-domain CIRCO dataset, establishing a new path toward more effective and trustworthy retrieval systems.",
    "summary": "",
    "translation": "CIR-CoT：通过端到端思维链推理实现可解释的组合图像检索",
    "relevance_score": 3,
    "reasoning": "虽然该论文涉及检索系统，但主要关注组合图像检索这一视觉领域特定任务，与推荐系统、搜索或广告的核心进展关联有限。思维链推理技术可能对理解用户意图有潜在价值，但论文的直接应用场景过于偏向视觉检索而非文本或异构数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07990v1": {
    "title": "GraphEnet: Event-driven Human Pose Estimation with a Graph Neural Network",
    "url": "https://www.alphaxiv.org/abs/2510.07990v1",
    "arxiv_id": "2510.07990v1",
    "authors": "Gaurvi Goyal, Pham Cong Thuong, Arren Glover, Masayoshi Mizuno, Chiara Bartolozzi",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 09:24:48",
    "ori_summary": "Human Pose Estimation is a crucial module in human-machine interaction applications and, especially since the rise in deep learning technology, robust methods are available to consumers using RGB cameras and commercial GPUs. On the other hand, event-based cameras have gained popularity in the vision research community for their low latency and low energy advantages that make them ideal for applications where those resources are constrained like portable electronics and mobile robots. In this work we propose a Graph Neural Network, GraphEnet, that leverages the sparse nature of event camera output, with an intermediate line based event representation, to estimate 2D Human Pose of a single person at a high frequency. The architecture incorporates a novel offset vector learning paradigm with confidence based pooling to estimate the human pose. This is the first work that applies Graph Neural Networks to event data for Human Pose Estimation. The code is open-source at https://github.com/event-driven-robotics/GraphEnet-NeVi-ICCV2025.",
    "summary": "",
    "translation": "GraphEnet：基于图神经网络的事件驱动人体姿态估计",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的人体姿态估计，使用图神经网络处理事件驱动数据。虽然涉及图神经网络技术，但该工作纯粹针对视觉任务，没有展示在推荐系统、搜索或广告领域的潜在应用。人体姿态估计与文本/序列建模、用户行为分析或内容排名等核心关注领域相距甚远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07984v1": {
    "title": "Is Architectural Complexity Always the Answer? A Case Study on SwinIR vs. an Efficient CNN",
    "url": "https://www.alphaxiv.org/abs/2510.07984v1",
    "arxiv_id": "2510.07984v1",
    "authors": "Chandresh Sutariya, Nitin Singh",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-09 09:16:05",
    "ori_summary": "The simultaneous restoration of high-frequency details and suppression of severe noise in low-light imagery presents a significant and persistent challenge in computer vision. While large-scale Transformer models like SwinIR have set the state of the art in performance, their high computational cost can be a barrier for practical applications. This paper investigates the critical trade-off between performance and efficiency by comparing the state-of-the-art SwinIR model against a standard, lightweight Convolutional Neural Network (CNN) on this challenging task. Our experimental results reveal a nuanced but important finding. While the Transformer-based SwinIR model achieves a higher peak performance, with a Peak Signal-to-Noise Ratio (PSNR) of 39.03 dB, the lightweight CNN delivers a surprisingly competitive PSNR of 37.4 dB. Crucially, the CNN reached this performance after converging in only 10 epochs of training, whereas the more complex SwinIR model required 132 epochs. This efficiency is further underscored by the model's size; the CNN is over 55 times smaller than SwinIR. This work demonstrates that a standard CNN can provide a near state-of-the-art result with significantly lower computational overhead, presenting a compelling case for its use in real-world scenarios where resource constraints are a primary concern.",
    "summary": "",
    "translation": "架构复杂性总是答案吗？SwinIR与高效CNN的案例研究",
    "relevance_score": 2,
    "reasoning": "该论文主要比较SwinIR（基于Transformer的架构）与高效CNN在图像恢复任务中的表现，属于计算机视觉领域的架构效率研究。虽然涉及Transformer架构，但论文聚焦于图像恢复这一特定视觉任务，没有明确展示在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07976v1": {
    "title": "The impact of abstract and object tags on image privacy classification",
    "url": "https://www.alphaxiv.org/abs/2510.07976v1",
    "arxiv_id": "2510.07976v1",
    "authors": "Darya Baranouskaya, Andrea Cavallaro",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 09:09:02",
    "ori_summary": "Object tags denote concrete entities and are central to many computer vision tasks, whereas abstract tags capture higher-level information, which is relevant for tasks that require a contextual, potentially subjective scene understanding. Object and abstract tags extracted from images also facilitate interpretability. In this paper, we explore which type of tags is more suitable for the context-dependent and inherently subjective task of image privacy. While object tags are generally used for privacy classification, we show that abstract tags are more effective when the tag budget is limited. Conversely, when a larger number of tags per image is available, object-related information is as useful. We believe that these findings will guide future research in developing more accurate image privacy classifiers, informed by the role of tag types and quantity.",
    "summary": "",
    "translation": "摘要和对象标签对图像隐私分类的影响",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于图像隐私分类，属于计算机视觉和隐私保护领域，与推荐系统、搜索或广告的核心技术无关。虽然涉及标签分类，但主要关注隐私而非推荐或搜索场景中的内容理解与匹配，因此不符合当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07961v1": {
    "title": "Latent Harmony: Synergistic Unified UHD Image Restoration via Latent Space Regularization and Controllable Refinement",
    "url": "https://www.alphaxiv.org/abs/2510.07961v1",
    "arxiv_id": "2510.07961v1",
    "authors": "Yidi Liu, Xueyang Fu, Jie Huang, Jie Xiao, Dong Li, Wenlong Zhang, Lei Bai, Zheng-Jun Zha",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 08:54:26",
    "ori_summary": "Ultra-High Definition (UHD) image restoration faces a trade-off between computational efficiency and high-frequency detail retention. While Variational Autoencoders (VAEs) improve efficiency via latent-space processing, their Gaussian constraint often discards degradation-specific high-frequency information, hurting reconstruction fidelity. To overcome this, we propose Latent Harmony, a two-stage framework that redefines VAEs for UHD restoration by jointly regularizing the latent space and enforcing high-frequency-aware reconstruction.In Stage One, we introduce LH-VAE, which enhances semantic robustness through visual semantic constraints and progressive degradation perturbations, while latent equivariance strengthens high-frequency reconstruction.Stage Two jointly trains this refined VAE with a restoration model using High-Frequency Low-Rank Adaptation (HF-LoRA): an encoder LoRA guided by a fidelity-oriented high-frequency alignment loss to recover authentic details, and a decoder LoRA driven by a perception-oriented loss to synthesize realistic textures. Both LoRA modules are trained via alternating optimization with selective gradient propagation to preserve the pretrained latent structure.At inference, a tunable parameter {\\alpha} enables flexible fidelity-perception trade-offs.Experiments show Latent Harmony achieves state-of-the-art performance across UHD and standard-resolution tasks, effectively balancing efficiency, perceptual quality, and reconstruction accuracy.",
    "summary": "",
    "translation": "潜在和谐：通过潜在空间正则化与可控精细化实现协同统一超高清图像修复",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的超高清图像修复技术，涉及潜在空间正则化和可控精细化方法。虽然技术上有一定先进性，但论文内容纯粹针对图像处理任务，与推荐系统、搜索或广告的核心技术领域没有任何直接或间接的应用关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07953v1": {
    "title": "SimCast: Enhancing Precipitation Nowcasting with Short-to-Long Term Knowledge Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.07953v1",
    "arxiv_id": "2510.07953v1",
    "authors": "Yifang Yin, Shengkai Chen, Yiyao Li, Lu Wang, Ruibing Jin, Wei Cui, Shili Xiang",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-09 08:49:16",
    "ori_summary": "Precipitation nowcasting predicts future radar sequences based on current observations, which is a highly challenging task driven by the inherent complexity of the Earth system. Accurate nowcasting is of utmost importance for addressing various societal needs, including disaster management, agriculture, transportation, and energy optimization. As a complementary to existing non-autoregressive nowcasting approaches, we investigate the impact of prediction horizons on nowcasting models and propose SimCast, a novel training pipeline featuring a short-to-long term knowledge distillation technique coupled with a weighted MSE loss to prioritize heavy rainfall regions. Improved nowcasting predictions can be obtained without introducing additional overhead during inference. As SimCast generates deterministic predictions, we further integrate it into a diffusion-based framework named CasCast, leveraging the strengths from probabilistic models to overcome limitations such as blurriness and distribution shift in deterministic outputs. Extensive experimental results on three benchmark datasets validate the effectiveness of the proposed framework, achieving mean CSI scores of 0.452 on SEVIR, 0.474 on HKO-7, and 0.361 on MeteoNet, which outperforms existing approaches by a significant margin.",
    "summary": "",
    "translation": "SimCast：通过短期到长期知识蒸馏增强降水临近预报",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于气象领域的降水预报，属于纯粹的领域特定应用，与推荐系统、搜索或广告没有任何关联。知识蒸馏技术虽然本身是通用方法，但论文的应用场景和核心问题与我的关注领域完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07951v1": {
    "title": "A Large-scale Dataset for Robust Complex Anime Scene Text Detection",
    "url": "https://www.alphaxiv.org/abs/2510.07951v1",
    "arxiv_id": "2510.07951v1",
    "authors": "Ziyi Dong, Yurui Zhang, Changmao Li, Naomi Rue Golding, Qing Long",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-09 08:47:52",
    "ori_summary": "Current text detection datasets primarily target natural or document scenes, where text typically appear in regular font and shapes, monotonous colors, and orderly layouts. The text usually arranged along straight or curved lines. However, these characteristics differ significantly from anime scenes, where text is often diverse in style, irregularly arranged, and easily confused with complex visual elements such as symbols and decorative patterns. Text in anime scene also includes a large number of handwritten and stylized fonts. Motivated by this gap, we introduce AnimeText, a large-scale dataset containing 735K images and 4.2M annotated text blocks. It features hierarchical annotations and hard negative samples tailored for anime scenarios. %Cross-dataset evaluations using state-of-the-art methods demonstrate that models trained on AnimeText achieve superior performance in anime text detection tasks compared to existing datasets. To evaluate the robustness of AnimeText in complex anime scenes, we conducted cross-dataset benchmarking using state-of-the-art text detection methods. Experimental results demonstrate that models trained on AnimeText outperform those trained on existing datasets in anime scene text detection tasks. AnimeText on HuggingFace: https://huggingface.co/datasets/deepghs/AnimeText",
    "summary": "",
    "translation": "用于鲁棒复杂动漫场景文本检测的大规模数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于动漫场景中的文本检测，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然文本检测在广义上可能与内容理解相关，但该研究的动漫特定性和视觉焦点使其与所列技术领域相距甚远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07944v1": {
    "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving",
    "url": "https://www.alphaxiv.org/abs/2510.07944v1",
    "arxiv_id": "2510.07944v1",
    "authors": "Tianrui Zhang, Yichen Liu, Zilin Guo, Yuxin Guo, Jingcheng Ni, Chenjing Ding, Dan Xu, Lewei Lu, Zehuan Wu",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 08:41:58",
    "ori_summary": "Generative models have been widely applied to world modeling for environment simulation and future state prediction. With advancements in autonomous driving, there is a growing demand not only for high-fidelity video generation under various controls, but also for producing diverse and meaningful information such as depth estimation. To address this, we propose CVD-STORM, a cross-view video diffusion model utilizing a spatial-temporal reconstruction Variational Autoencoder (VAE) that generates long-term, multi-view videos with 4D reconstruction capabilities under various control inputs. Our approach first fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its ability to encode 3D structures and temporal dynamics. Subsequently, we integrate this VAE into the video diffusion process to significantly improve generation quality. Experimental results demonstrate that our model achieves substantial improvements in both FID and FVD metrics. Additionally, the jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic scenes, providing valuable geometric information for comprehensive scene understanding.",
    "summary": "",
    "translation": "CVD-STORM：面向自动驾驶的具有时空重建模型的跨视角视频扩散",
    "relevance_score": 2,
    "reasoning": "该论文主要关注自动驾驶领域的视频生成和时空建模，属于计算机视觉应用范畴。虽然视频扩散模型在技术上有一定先进性，但论文标题明确指向自动驾驶这一特定领域应用，与推荐系统、搜索或广告的核心技术焦点缺乏直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07927v1": {
    "title": "ASBench: Image Anomalies Synthesis Benchmark for Anomaly Detection",
    "url": "https://www.alphaxiv.org/abs/2510.07927v1",
    "arxiv_id": "2510.07927v1",
    "authors": "Qunyi Zhang, Songan Zhang, Jinbao Wang, Xiaoning Lei, Guoyang Xie, Guannan Jiang, Zhichao Lu",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 08:23:29",
    "ori_summary": "Anomaly detection plays a pivotal role in manufacturing quality control, yet its application is constrained by limited abnormal samples and high manual annotation costs. While anomaly synthesis offers a promising solution, existing studies predominantly treat anomaly synthesis as an auxiliary component within anomaly detection frameworks, lacking systematic evaluation of anomaly synthesis algorithms. Current research also overlook crucial factors specific to anomaly synthesis, such as decoupling its impact from detection, quantitative analysis of synthetic data and adaptability across different scenarios. To address these limitations, we propose ASBench, the first comprehensive benchmarking framework dedicated to evaluating anomaly synthesis methods. Our framework introduces four critical evaluation dimensions: (i) the generalization performance across different datasets and pipelines (ii) the ratio of synthetic to real data (iii) the correlation between intrinsic metrics of synthesis images and anomaly detection performance metrics , and (iv) strategies for hybrid anomaly synthesis methods. Through extensive experiments, ASBench not only reveals limitations in current anomaly synthesis methods but also provides actionable insights for future research directions in anomaly synthesis",
    "summary": "",
    "translation": "ASBench：用于异常检测的图像异常合成基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的图像异常检测基准测试，与推荐系统、搜索或广告的核心技术领域没有直接关联。图像异常检测主要应用于工业质检、医疗影像等视觉领域，无法为RecSys/Search/Ads提供有价值的技术启示或应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07915v1": {
    "title": "MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.07915v1",
    "arxiv_id": "2510.07915v1",
    "authors": "Peiran Wu, Zhuorui Yu, Yunze Liu, Chi-Hao Wu, Enmin Zhou, Junxiao Shen",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 08:07:19",
    "ori_summary": "The rapid progress of large language models (LLMs) has laid the foundation for multimodal models. However, visual language models (VLMs) still face heavy computational costs when extended from images to videos due to high frame rates and long durations. Token compression is a promising solution, yet most existing training-free methods cause information loss and performance degradation. To overcome this, we propose \\textbf{Memory-Augmented Reinforcement Learning-based Token Compression (MARC)}, which integrates structured retrieval and RL-based distillation. MARC adopts a \\textit{retrieve-then-compress} strategy using a \\textbf{Visual Memory Retriever (VMR)} to select key clips and a \\textbf{Compression Group Relative Policy Optimization (C-GRPO)} framework to distil reasoning ability from a teacher to a student model. Experiments on six video benchmarks show that MARC achieves near-baseline accuracy using only one frame's tokens -- reducing visual tokens by \\textbf{95\\%}, GPU memory by \\textbf{72\\%}, and latency by \\textbf{23.9\\%}. This demonstrates its potential for efficient, real-time video understanding in resource-constrained settings such as video QA, surveillance, and autonomous driving.",
    "summary": "",
    "translation": "MARC：用于高效视频理解的内存增强强化学习令牌压缩",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频理解的效率优化，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术关联性较弱。虽然压缩技术理论上可能应用于多模态推荐中的视频内容处理，但论文的强化学习和视频理解焦点使其与当前关注点的直接相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07910v1": {
    "title": "MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.07910v1",
    "arxiv_id": "2510.07910v1",
    "authors": "Chongmyung Kwon, Yujin Kim, Seoeun Park, Yunji Lee, Charmgil Hong",
    "categories": "cs.LG, cs.AI, cs.CV, I.2.6; I.5.1",
    "pub_date": "2025-10-09 08:03:14",
    "ori_summary": "Drug recommendation is an essential task in machine learning-based clinical decision support systems. However, the risk of drug-drug interactions (DDI) between co-prescribed medications remains a significant challenge. Previous studies have used graph neural networks (GNNs) to represent drug structures. Regardless, their simplified discrete forms cannot fully capture the molecular binding affinity and reactivity. Therefore, we propose Multimodal DDI Prediction with Molecular Electron Localization Function (ELF) Maps (MMM), a novel framework that integrates three-dimensional (3D) quantum-chemical information into drug representation learning. It generates 3D electron density maps using the ELF. To capture both therapeutic relevance and interaction risks, MMM combines ELF-derived features that encode global electronic properties with a bipartite graph encoder that models local substructure interactions. This design enables learning complementary characteristics of drug molecules. We evaluate MMM in the MIMIC-III dataset (250 drugs, 442 substructures), comparing it with several baseline models. In particular, a comparison with the GNN-based SafeDrug model demonstrates statistically significant improvements in the F1-score (p = 0.0387), Jaccard (p = 0.0112), and the DDI rate (p = 0.0386). These results demonstrate the potential of ELF-based 3D representations to enhance prediction accuracy and support safer combinatorial drug prescribing in clinical practice.",
    "summary": "",
    "translation": "MMM：基于量子化学分子表征学习的组合药物推荐",
    "relevance_score": 1,
    "reasoning": "该论文专注于药物推荐这一特定医疗领域应用，属于明确的无关主题范畴。虽然标题包含'推荐'一词，但内容涉及量子化学分子表征和组合药物推荐，与搜索、推荐系统或广告的核心技术进展无关，且没有明显的通用推荐系统技术迁移潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07905v1": {
    "title": "SatFusion: A Unified Framework for Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion",
    "url": "https://www.alphaxiv.org/abs/2510.07905v1",
    "arxiv_id": "2510.07905v1",
    "authors": "Yufei Tong, Guanjie Cheng, Peihan Wu, Yicheng Zhu, Kexu Lu, Feiyi Chen, Meng Xi, Junqin Huang, Shuiguang Deng",
    "categories": "eess.IV, cs.CV, cs.MM",
    "pub_date": "2025-10-09 07:59:37",
    "ori_summary": "With the rapid advancement of the digital society, the proliferation of satellites in the Satellite Internet of Things (Sat-IoT) has led to the continuous accumulation of large-scale multi-temporal and multi-source images across diverse application scenarios. However, existing methods fail to fully exploit the complementary information embedded in both temporal and source dimensions. For example, Multi-Image Super-Resolution (MISR) enhances reconstruction quality by leveraging temporal complementarity across multiple observations, yet the limited fine-grained texture details in input images constrain its performance. Conversely, pansharpening integrates multi-source images by injecting high-frequency spatial information from panchromatic data, but typically relies on pre-interpolated low-resolution inputs and assumes noise-free alignment, making it highly sensitive to noise and misregistration. To address these issues, we propose SatFusion: A Unified Framework for Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion. Specifically, SatFusion first employs a Multi-Temporal Image Fusion (MTIF) module to achieve deep feature alignment with the panchromatic image. Then, a Multi-Source Image Fusion (MSIF) module injects fine-grained texture information from the panchromatic data. Finally, a Fusion Composition module adaptively integrates the complementary advantages of both modalities while dynamically refining spectral consistency, supervised by a weighted combination of multiple loss functions. Extensive experiments on the WorldStrat, WV3, QB, and GF2 datasets demonstrate that SatFusion significantly improves fusion quality, robustness under challenging conditions, and generalizability to real-world Sat-IoT scenarios. The code is available at: https://github.com/dllgyufei/SatFusion.git.",
    "summary": "",
    "translation": "SatFusion：一种通过多时序和多源数据融合增强卫星物联网图像的统一框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于卫星图像处理和物联网数据融合，属于遥感技术领域。虽然涉及多源数据融合概念，但与推荐系统、搜索或广告的核心技术没有直接关联，也不涉及LLM、Transformer架构或异构数据建模在推荐领域的应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07878v1": {
    "title": "FlowLensing: Simulating Gravitational Lensing with Flow Matching",
    "url": "https://www.alphaxiv.org/abs/2510.07878v1",
    "arxiv_id": "2510.07878v1",
    "authors": "Hamees Sayed, Pranath Reddy, Michael W. Toomey, Sergei Gleyzer",
    "categories": "astro-ph.IM, cs.CV",
    "pub_date": "2025-10-09 07:31:47",
    "ori_summary": "Gravitational lensing is one of the most powerful probes of dark matter, yet creating high-fidelity lensed images at scale remains a bottleneck. Existing tools rely on ray-tracing or forward-modeling pipelines that, while precise, are prohibitively slow. We introduce FlowLensing, a Diffusion Transformer-based compact and efficient flow-matching model for strong gravitational lensing simulation. FlowLensing operates in both discrete and continuous regimes, handling classes such as different dark matter models as well as continuous model parameters ensuring physical consistency. By enabling scalable simulations, our model can advance dark matter studies, specifically for probing dark matter substructure in cosmological surveys. We find that our model achieves a speedup of over 200$\\times$ compared to classical simulators for intensive dark matter models, with high fidelity and low inference latency. FlowLensing enables rapid, scalable, and physically consistent image synthesis, offering a practical alternative to traditional forward-modeling pipelines.",
    "summary": "",
    "translation": "FlowLensing：使用流匹配模拟引力透镜效应",
    "relevance_score": 1,
    "reasoning": "这篇论文涉及物理学中的引力透镜模拟，使用流匹配技术解决物理模拟问题。该主题与推荐系统、搜索或广告领域没有任何直接或间接的联系，完全超出了关注范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07871v1": {
    "title": "Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception - Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track",
    "url": "https://www.alphaxiv.org/abs/2510.07871v1",
    "arxiv_id": "2510.07871v1",
    "authors": "Erjia Xiao, Lingfeng Zhang, Yingbo Tang, Hao Cheng, Renjing Xu, Wenbo Ding, Lei Zhou, Long Chen, Hangjun Ye, Xiaoshuai Hao",
    "categories": "cs.RO, cs.AI, cs.CV, cs.LG",
    "pub_date": "2025-10-09 07:22:12",
    "ori_summary": "In this report, we describe the technical details of our submission to the IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on developing RGBD-based perception and navigation systems that enable autonomous agents to navigate safely, efficiently, and socially compliantly in dynamic human-populated indoor environments. The challenge requires agents to operate from an egocentric perspective using only onboard sensors including RGB-D observations and odometry, without access to global maps or privileged information, while maintaining social norm compliance such as safe distances and collision avoidance. Building upon the Falcon model, we introduce a Proactive Risk Perception Module to enhance social navigation performance. Our approach augments Falcon with collision risk understanding that learns to predict distance-based collision risk scores for surrounding humans, which enables the agent to develop more robust spatial awareness and proactive collision avoidance behaviors. The evaluation on the Social-HM3D benchmark demonstrates that our method improves the agent's ability to maintain personal space compliance while navigating toward goals in crowded indoor scenes with dynamic human agents, achieving 2nd place among 16 participating teams in the challenge.",
    "summary": "",
    "translation": "小米EV-AD VLA团队：通过主动风险感知学习社会性导航——IROS 2025 RoboSense挑战赛社会导航赛道技术报告",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人社会导航和主动风险感知，属于自动驾驶领域。虽然标题包含'VLA'（可能指视觉语言模型），但核心内容围绕机器人导航挑战赛，与推荐系统、搜索或广告没有任何直接或间接关联。该技术无法应用于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07856v1": {
    "title": "XYZCylinder: Feedforward Reconstruction for Driving Scenes Based on A Unified Cylinder Lifting Method",
    "url": "https://www.alphaxiv.org/abs/2510.07856v1",
    "arxiv_id": "2510.07856v1",
    "authors": "Haochen Yu, Qiankun Liu, Hongyuan Liu, Jianfei Jiang, Juntao Lyu, Jiansheng Chen, Huimin Ma",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 06:58:03",
    "ori_summary": "Recently, more attention has been paid to feedforward reconstruction paradigms, which mainly learn a fixed view transformation implicitly and reconstruct the scene with a single representation. However, their generalization capability and reconstruction accuracy are still limited while reconstructing driving scenes, which results from two aspects: (1) The fixed view transformation fails when the camera configuration changes, limiting the generalization capability across different driving scenes equipped with different camera configurations. (2) The small overlapping regions between sparse views of the $360^\\circ$ panorama and the complexity of driving scenes increase the learning difficulty, reducing the reconstruction accuracy. To handle these difficulties, we propose \\textbf{XYZCylinder}, a feedforward model based on a unified cylinder lifting method which involves camera modeling and feature lifting. Specifically, to improve the generalization capability, we design a Unified Cylinder Camera Modeling (UCCM) strategy, which avoids the learning of viewpoint-dependent spatial correspondence and unifies different camera configurations with adjustable parameters. To improve the reconstruction accuracy, we propose a hybrid representation with several dedicated modules based on newly designed Cylinder Plane Feature Group (CPFG) to lift 2D image features to 3D space. Experimental results show that XYZCylinder achieves state-of-the-art performance under different evaluation settings, and can be generalized to other driving scenes in a zero-shot manner. Project page: \\href{https://yuyuyu223.github.io/XYZCYlinder-projectpage/}{here}.",
    "summary": "",
    "translation": "XYZCylinder：基于统一圆柱提升方法的驾驶场景前馈重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的驾驶场景重建，属于纯粹的视觉技术领域。虽然标题提到\"重建\"和\"驾驶场景\"，但这与推荐系统、搜索或广告的核心技术没有直接关联，也没有涉及LLM技术或Transformer架构的进展，无法看出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07853v1": {
    "title": "Self-Supervised Learning Strategies for a Platform to Test the Toxicity of New Chemicals and Materials",
    "url": "https://www.alphaxiv.org/abs/2510.07853v1",
    "arxiv_id": "2510.07853v1",
    "authors": "Thomas Lautenschlager, Nils Friederich, Angelo Jovin Yamachui Sitcheu, Katja Nau, Gaëlle Hayot, Thomas Dickmeis, Ralf Mikut",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-09 06:51:12",
    "ori_summary": "High-throughput toxicity testing offers a fast and cost-effective way to test large amounts of compounds. A key component for such systems is the automated evaluation via machine learning models. In this paper, we address critical challenges in this domain and demonstrate how representations learned via self-supervised learning can effectively identify toxicant-induced changes. We provide a proof-of-concept that utilizes the publicly available EmbryoNet dataset, which contains ten zebrafish embryo phenotypes elicited by various chemical compounds targeting different processes in early embryonic development. Our analysis shows that the learned representations using self-supervised learning are suitable for effectively distinguishing between the modes-of-action of different compounds. Finally, we discuss the integration of machine learning models in a physical toxicity testing device in the context of the TOXBOX project.",
    "summary": "",
    "translation": "用于测试新化学品和材料毒性的平台的自监督学习策略",
    "relevance_score": 1,
    "reasoning": "该论文专注于化学和材料毒性测试领域，这属于明确的无关主题范畴（医学、生物学、化学或其他领域特定应用）。尽管提到了自监督学习，但核心应用场景与推荐系统、搜索或广告完全无关，且没有显示出任何在这些领域应用的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07839v1": {
    "title": "AlignGS: Aligning Geometry and Semantics for Robust Indoor Reconstruction from Sparse Views",
    "url": "https://www.alphaxiv.org/abs/2510.07839v1",
    "arxiv_id": "2510.07839v1",
    "authors": "Yijie Gao, Houqiang Zhong, Tianchi Zhu, Zhengxue Cheng, Qiang Hu, Li Song",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 06:30:20",
    "ori_summary": "The demand for semantically rich 3D models of indoor scenes is rapidly growing, driven by applications in augmented reality, virtual reality, and robotics. However, creating them from sparse views remains a challenge due to geometric ambiguity. Existing methods often treat semantics as a passive feature painted on an already-formed, and potentially flawed, geometry. We posit that for robust sparse-view reconstruction, semantic understanding instead be an active, guiding force. This paper introduces AlignGS, a novel framework that actualizes this vision by pioneering a synergistic, end-to-end optimization of geometry and semantics. Our method distills rich priors from 2D foundation models and uses them to directly regularize the 3D representation through a set of novel semantic-to-geometry guidance mechanisms, including depth consistency and multi-faceted normal regularization. Extensive evaluations on standard benchmarks demonstrate that our approach achieves state-of-the-art results in novel view synthesis and produces reconstructions with superior geometric accuracy. The results validate that leveraging semantic priors as a geometric regularizer leads to more coherent and complete 3D models from limited input views. Our code is avaliable at https://github.com/MediaX-SJTU/AlignGS .",
    "summary": "",
    "translation": "AlignGS：对齐几何与语义以实现稀疏视图下的鲁棒室内重建",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的3D室内重建问题，属于纯粹的视觉领域研究。虽然提到了语义对齐，但其核心是几何重建技术，与推荐系统、搜索或广告的排名和建模需求没有直接关联。该技术缺乏在RecSys/Search/Ads领域的明显应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07837v1": {
    "title": "IsoSignVid2Aud: Sign Language Video to Audio Conversion without Text Intermediaries",
    "url": "https://www.alphaxiv.org/abs/2510.07837v1",
    "arxiv_id": "2510.07837v1",
    "authors": "Harsh Kavediya, Vighnesh Nayak, Bheeshm Sharma, Balamurugan Palaniappan",
    "categories": "cs.CV, cs.MM, cs.SD",
    "pub_date": "2025-10-09 06:29:59",
    "ori_summary": "Sign language to spoken language audio translation is important to connect the hearing- and speech-challenged humans with others. We consider sign language videos with isolated sign sequences rather than continuous grammatical signing. Such videos are useful in educational applications and sign prompt interfaces. Towards this, we propose IsoSignVid2Aud, a novel end-to-end framework that translates sign language videos with a sequence of possibly non-grammatic continuous signs to speech without requiring intermediate text representation, providing immediate communication benefits while avoiding the latency and cascading errors inherent in multi-stage translation systems. Our approach combines an I3D-based feature extraction module with a specialized feature transformation network and an audio generation pipeline, utilizing a novel Non-Maximal Suppression (NMS) algorithm for the temporal detection of signs in non-grammatic continuous sequences. Experimental results demonstrate competitive performance on ASL-Citizen-1500 and WLASL-100 datasets with Top-1 accuracies of 72.01\\% and 78.67\\%, respectively, and audio quality metrics (PESQ: 2.67, STOI: 0.73) indicating intelligible speech output. Code is available at: https://github.com/BheeshmSharma/IsoSignVid2Aud_AIMLsystems-2025.",
    "summary": "",
    "translation": "IsoSignVid2Aud：无需文本中介的手语视频到音频转换",
    "relevance_score": 1,
    "reasoning": "该论文专注于手语视频到音频的模态转换技术，属于计算机视觉和语音处理的交叉领域。虽然涉及多模态处理，但与推荐系统、搜索或广告的核心技术栈没有直接关联，也不涉及Transformer架构改进或LLM技术在推荐领域的应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07830v1": {
    "title": "PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale 3D Gaussian Splatting",
    "url": "https://www.alphaxiv.org/abs/2510.07830v1",
    "arxiv_id": "2510.07830v1",
    "authors": "Houqiang Zhong, Zhenglong Wu, Sihua Fu, Zihan Zheng, Xin Jin, Xiaoyun Zhang, Li Song, Qiang Hu",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 06:21:45",
    "ori_summary": "3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic rendering in compact scenes, but scaling to large urban environments introduces severe aliasing artifacts and optimization instability, especially under high-resolution (e.g., 4K) rendering. These artifacts, manifesting as flickering textures and jagged edges, arise from the mismatch between Gaussian primitives and the multi-scale nature of urban geometry. While existing ``divide-and-conquer'' pipelines address scalability, they fail to resolve this fidelity gap. In this paper, we propose PrismGS, a physically-grounded regularization framework that improves the intrinsic rendering behavior of 3D Gaussians. PrismGS integrates two synergistic regularizers. The first is pyramidal multi-scale supervision, which enforces consistency by supervising the rendering against a pre-filtered image pyramid. This compels the model to learn an inherently anti-aliased representation that remains coherent across different viewing scales, directly mitigating flickering textures. This is complemented by an explicit size regularization that imposes a physically-grounded lower bound on the dimensions of the 3D Gaussians. This prevents the formation of degenerate, view-dependent primitives, leading to more stable and plausible geometric surfaces and reducing jagged edges. Our method is plug-and-play and compatible with existing pipelines. Extensive experiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS achieves state-of-the-art performance, yielding significant PSNR gains around 1.5 dB against CityGaussian, while maintaining its superior quality and robustness under demanding 4K rendering.",
    "summary": "",
    "translation": "PrismGS：面向高保真大规模3D高斯泼溅的物理基础抗锯齿技术",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D计算机图形学中的渲染技术（3D高斯泼溅和抗锯齿），属于纯粹的视觉/图形学领域。虽然标题提到'大规模'，但内容与推荐系统、搜索或广告的核心技术栈（排序、召回、用户建模等）无直接关联，且未提及任何潜在的跨模态应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07828v1": {
    "title": "MMHOI: Modeling Complex 3D Multi-Human Multi-Object Interactions",
    "url": "https://www.alphaxiv.org/abs/2510.07828v1",
    "arxiv_id": "2510.07828v1",
    "authors": "Kaen Kogashi, Anoop Cherian, Meng-Yu Jennifer Kuo",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 06:18:12",
    "ori_summary": "Real-world scenes often feature multiple humans interacting with multiple objects in ways that are causal, goal-oriented, or cooperative. Yet existing 3D human-object interaction (HOI) benchmarks consider only a fraction of these complex interactions. To close this gap, we present MMHOI -- a large-scale, Multi-human Multi-object Interaction dataset consisting of images from 12 everyday scenarios. MMHOI offers complete 3D shape and pose annotations for every person and object, along with labels for 78 action categories and 14 interaction-specific body parts, providing a comprehensive testbed for next-generation HOI research. Building on MMHOI, we present MMHOI-Net, an end-to-end transformer-based neural network for jointly estimating human-object 3D geometries, their interactions, and associated actions. A key innovation in our framework is a structured dual-patch representation for modeling objects and their interactions, combined with action recognition to enhance the interaction prediction. Experiments on MMHOI and the recently proposed CORE4D datasets demonstrate that our approach achieves state-of-the-art performance in multi-HOI modeling, excelling in both accuracy and reconstruction quality.",
    "summary": "",
    "translation": "MMHOI：建模复杂的三维多人与多物体交互",
    "relevance_score": 2,
    "reasoning": "该论文专注于3D视觉中复杂交互的建模，属于纯粹的计算机视觉领域。虽然建模复杂交互的技术在概念上可能与推荐系统中的用户-物品交互有类比，但论文标题明确限定在3D视觉场景，没有显示出与推荐系统、搜索或广告的直接关联或潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07823v1": {
    "title": "Enhancing Visual Prompting through Expanded Transformation Space and Overfitting Mitigation",
    "url": "https://www.alphaxiv.org/abs/2510.07823v1",
    "arxiv_id": "2510.07823v1",
    "authors": "Shohei Enomoto",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 06:08:15",
    "ori_summary": "Visual prompting (VP) has emerged as a promising parameter-efficient fine-tuning approach for adapting pre-trained vision models to downstream tasks without modifying model parameters. Despite offering advantages like negligible computational overhead and compatibility with black-box models, conventional VP methods typically achieve lower accuracy than other adaptation approaches. Our analysis reveals two critical limitations: the restricted expressivity of simple additive transformation and a tendency toward overfitting when the parameter count increases. To address these challenges, we propose ACAVP (Affine, Color, and Additive Visual Prompting), which enhances VP's expressive power by introducing complementary transformation operations: affine transformation for creating task-specific prompt regions while preserving original image information, and color transformation for emphasizing task-relevant visual features. Additionally, we identify that overfitting is a critical issue in VP training and introduce TrivialAugment as an effective data augmentation, which not only benefits our approach but also significantly improves existing VP methods, with performance gains of up to 12 percentage points on certain datasets. This demonstrates that appropriate data augmentation is universally beneficial for VP training. Extensive experiments across twelve diverse image classification datasets with two different model architectures demonstrate that ACAVP achieves state-of-the-art accuracy among VP methods, surpasses linear probing in average accuracy, and exhibits superior robustness to distribution shifts, all while maintaining minimal computational overhead during inference.",
    "summary": "",
    "translation": "通过扩展变换空间与缓解过拟合增强视觉提示",
    "relevance_score": 1,
    "reasoning": "该论文专注于视觉提示技术，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术无关。虽然视觉语言模型被列为关注领域，但该论文仅涉及视觉提示的改进，并未涉及多模态建模或异构数据处理，因此对当前关注点没有实际应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07817v1": {
    "title": "An End-to-End Room Geometry Constrained Depth Estimation Framework for Indoor Panorama Images",
    "url": "https://www.alphaxiv.org/abs/2510.07817v1",
    "arxiv_id": "2510.07817v1",
    "authors": "Kanglin Ning, Ruzhao Chen, Penghong Wang, Xingtao Wang, Ruiqin Xiong, Xiaopeng Fan",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 05:52:48",
    "ori_summary": "Predicting spherical pixel depth from monocular $360^{\\circ}$ indoor panoramas is critical for many vision applications. However, existing methods focus on pixel-level accuracy, causing oversmoothed room corners and noise sensitivity. In this paper, we propose a depth estimation framework based on room geometry constraints, which extracts room geometry information through layout prediction and integrates those information into the depth estimation process through background segmentation mechanism. At the model level, our framework comprises a shared feature encoder followed by task-specific decoders for layout estimation, depth estimation, and background segmentation. The shared encoder extracts multi-scale features, which are subsequently processed by individual decoders to generate initial predictions: a depth map, a room layout map, and a background segmentation map. Furthermore, our framework incorporates two strategies: a room geometry-based background depth resolving strategy and a background-segmentation-guided fusion mechanism. The proposed room-geometry-based background depth resolving strategy leverages the room layout and the depth decoder's output to generate the corresponding background depth map. Then, a background-segmentation-guided fusion strategy derives fusion weights for the background and coarse depth maps from the segmentation decoder's predictions. Extensive experimental results on the Stanford2D3D, Matterport3D and Structured3D datasets show that our proposed methods can achieve significantly superior performance than current open-source methods. Our code is available at https://github.com/emiyaning/RGCNet.",
    "summary": "",
    "translation": "面向室内全景图像的端到端房间几何约束深度估计框架",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉中的深度估计任务，特别是针对室内全景图像，这属于纯粹的视觉处理范畴。虽然深度估计在增强现实和机器人导航中有应用，但与推荐系统、搜索或广告的核心技术缺乏直接关联，也没有涉及LLM、Transformer架构或异构数据建模等焦点领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07810v1": {
    "title": "FMANet: A Novel Dual-Phase Optical Flow Approach with Fusion Motion Attention Network for Robust Micro-expression Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.07810v1",
    "arxiv_id": "2510.07810v1",
    "authors": "Luu Tu Nguyen, Vu Tram Anh Khuong, Thi Bich Phuong Man, Thi Duyen Ngo, Thanh Ha Le",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 05:36:40",
    "ori_summary": "Facial micro-expressions, characterized by their subtle and brief nature, are valuable indicators of genuine emotions. Despite their significance in psychology, security, and behavioral analysis, micro-expression recognition remains challenging due to the difficulty of capturing subtle facial movements. Optical flow has been widely employed as an input modality for this task due to its effectiveness. However, most existing methods compute optical flow only between the onset and apex frames, thereby overlooking essential motion information in the apex-to-offset phase. To address this limitation, we first introduce a comprehensive motion representation, termed Magnitude-Modulated Combined Optical Flow (MM-COF), which integrates motion dynamics from both micro-expression phases into a unified descriptor suitable for direct use in recognition networks. Building upon this principle, we then propose FMANet, a novel end-to-end neural network architecture that internalizes the dual-phase analysis and magnitude modulation into learnable modules. This allows the network to adaptively fuse motion cues and focus on salient facial regions for classification. Experimental evaluations on the MMEW, SMIC, CASME-II, and SAMM datasets, widely recognized as standard benchmarks, demonstrate that our proposed MM-COF representation and FMANet outperforms existing methods, underscoring the potential of a learnable, dual-phase framework in advancing micro-expression recognition.",
    "summary": "",
    "translation": "FMANet：一种用于鲁棒微表情识别的融合运动注意力网络的新型双阶段光流方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于微表情识别这一计算机视觉特定领域，使用光流和注意力机制进行面部微表情分析。这与搜索、推荐或广告系统的核心需求没有直接关联，也不涉及LLM技术、Transformer架构进展或异构数据统一建模等关键焦点领域。微表情识别主要应用于心理学、人机交互等场景，与RecSys/Search/Ads的技术栈和应用场景相距甚远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07791v1": {
    "title": "GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.07791v1",
    "arxiv_id": "2510.07791v1",
    "authors": "Qinghongbing Xie, Zhaoyuan Xia, Feng Zhu, Lijun Gong, Ziyue Li, Rui Zhao, Long Zeng",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 05:09:27",
    "ori_summary": "Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has attracted much attention due to its importance for Autonomous Driving, Embodied AI and General Artificial Intelligence. Existing spatial-temporal benchmarks mainly focus on egocentric perspective reasoning with images/video context, or geographic perspective reasoning with graphics context (eg. a map), thus fail to assess VLMs' geographic spatial-temporal intelligence with both images/video and graphics context, which is important for areas like traffic management and emergency response. To address the gaps, we introduce Geo-Temporal Reasoning benchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of moving targets in a large-scale camera network. GTR-Bench is more challenging as it requires multiple perspective switches between maps and videos, joint reasoning across multiple videos with non-overlapping fields of view, and inference over spatial-temporal regions that are unobserved by any video context. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that even the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags behind human performance (78.61%) on geo-temporal reasoning. Moreover, our comprehensive analysis on GTR-Bench reveals three primary deficiencies of current models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by an imbalanced utilization of spatial-temporal context. (2) VLMs are weak in temporal forecasting, which leads to worse performance on temporal-emphasized tasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to comprehend or align the map data with multi-view video inputs. We believe GTR-Bench offers valuable insights and opens up new opportunities for research and applications in spatial-temporal intelligence. Benchmark and code will be released at https://github.com/X-Luffy/GTR-Bench.",
    "summary": "",
    "translation": "GTR-Bench：评估视觉语言模型中的地理时空推理能力",
    "relevance_score": 2,
    "reasoning": "虽然该论文涉及视觉语言模型评估，但其焦点是地理时空推理这一特定能力，与推荐系统、搜索或广告的核心技术关联较弱。地理时空推理在本地化推荐中可能有潜在应用，但论文主要关注评估基准而非直接的技术应用或架构创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07785v1": {
    "title": "Demystifying Deep Learning-based Brain Tumor Segmentation with 3D UNets and Explainable AI (XAI): A Comparative Analysis",
    "url": "https://www.alphaxiv.org/abs/2510.07785v1",
    "arxiv_id": "2510.07785v1",
    "authors": "Ming Jie Ong, Sze Yinn Ung, Sim Kuan Goh, Jimmy Y. Zhong",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 05:03:31",
    "ori_summary": "The current study investigated the use of Explainable Artificial Intelligence (XAI) to improve the accuracy of brain tumor segmentation in MRI images, with the goal of assisting physicians in clinical decision-making. The study focused on applying UNet models for brain tumor segmentation and using the XAI techniques of Gradient-weighted Class Activation Mapping (Grad-CAM) and attention-based visualization to enhance the understanding of these models. Three deep learning models - UNet, Residual UNet (ResUNet), and Attention UNet (AttUNet) - were evaluated to identify the best-performing model. XAI was employed with the aims of clarifying model decisions and increasing physicians' trust in these models. We compared the performance of two UNet variants (ResUNet and AttUNet) with the conventional UNet in segmenting brain tumors from the BraTS2020 public dataset and analyzed model predictions with Grad-CAM and attention-based visualization. Using the latest computer hardware, we trained and validated each model using the Adam optimizer and assessed their performance with respect to: (i) training, validation, and inference times, (ii) segmentation similarity coefficients and loss functions, and (iii) classification performance. Notably, during the final testing phase, ResUNet outperformed the other models with respect to Dice and Jaccard similarity scores, as well as accuracy, recall, and F1 scores. Grad-CAM provided visuospatial insights into the tumor subregions each UNet model focused on while attention-based visualization provided valuable insights into the working mechanisms of AttUNet's attention modules. These results demonstrated ResUNet as the best-performing model and we conclude by recommending its use for automated brain tumor segmentation in future clinical assessments. Our source code and checkpoint are available at https://github.com/ethanong98/MultiModel-XAI-Brats2020",
    "summary": "",
    "translation": "基于3D UNet和可解释人工智能（XAI）的深度学习脑肿瘤分割技术解析：一项对比分析",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像领域的脑肿瘤分割，使用3D UNet和可解释AI技术，属于明确的医学/生物学应用范畴。根据用户要求，医学、生物学等特定领域应用属于不相关主题，且该技术没有明显的推荐系统、搜索或广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07778v1": {
    "title": "IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction",
    "url": "https://www.alphaxiv.org/abs/2510.07778v1",
    "arxiv_id": "2510.07778v1",
    "authors": "Yandu Chen, Kefan Gu, Yuqing Wen, Yucheng Zhao, Tiancai Wang, Liqiang Nie",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2025-10-09 04:49:46",
    "ori_summary": "Vision-Language-Action (VLA) models leverage pretrained vision-language models (VLMs) to couple perception with robotic control, offering a promising path toward general-purpose embodied intelligence. However, current SOTA VLAs are primarily pretrained on multimodal tasks with limited relevance to embodied scenarios, and then finetuned to map explicit instructions to actions. Consequently, due to the lack of reasoning-intensive pretraining and reasoning-guided manipulation, these models are unable to perform implicit human intention reasoning required for complex, real-world interactions. To overcome these limitations, we propose \\textbf{IntentionVLA}, a VLA framework with a curriculum training paradigm and an efficient inference mechanism. Our proposed method first leverages carefully designed reasoning data that combine intention inference, spatial grounding, and compact embodied reasoning, endowing the model with both reasoning and perception capabilities. In the following finetuning stage, IntentionVLA employs the compact reasoning outputs as contextual guidance for action generation, enabling fast inference under indirect instructions. Experimental results show that IntentionVLA substantially outperforms $\\pi_0$, achieving 18\\% higher success rates with direct instructions and 28\\% higher than ECoT under intention instructions. On out-of-distribution intention tasks, IntentionVLA achieves over twice the success rate of all baselines, and further enables zero-shot human-robot interaction with 40\\% success rate. These results highlight IntentionVLA as a promising paradigm for next-generation human-robot interaction (HRI) systems.",
    "summary": "",
    "translation": "IntentionVLA：面向人机交互的可泛化高效具身意图推理",
    "relevance_score": 2,
    "reasoning": "该论文专注于机器人领域的具身意图推理和人机交互，属于特定领域应用。虽然涉及多模态理解和意图建模，但其核心应用场景（机器人交互）与推荐系统、搜索或广告领域缺乏直接关联，且未明确展示在推荐/搜索/广告中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07752v1": {
    "title": "DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream",
    "url": "https://www.alphaxiv.org/abs/2510.07752v1",
    "arxiv_id": "2510.07752v1",
    "authors": "Junhao He, Jiaxu Wang, Jia Li, Mingyuan Sun, Qiang Zhang, Jiahang Cao, Ziyi Zhang, Yi Gu, Jingkai Sun, Renjing Xu",
    "categories": "cs.CV",
    "pub_date": "2025-10-09 03:43:27",
    "ori_summary": "Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB videos is challenging. This is because large inter-frame motions will increase the uncertainty of the solution space. For example, one pixel in the first frame might have more choices to reach the corresponding pixel in the second frame. Event cameras can asynchronously capture rapid visual changes and are robust to motion blur, but they do not provide color information. Intuitively, the event stream can provide deterministic constraints for the inter-frame large motion by the event trajectories. Hence, combining low-temporal-resolution images with high-framerate event streams can address this challenge. However, it is challenging to jointly optimize Dynamic 3DGS using both RGB and event modalities due to the significant discrepancy between these two data modalities. This paper introduces a novel framework that jointly optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event motion priors to guide the optimization of the deformation fields. First, we extract the motion priors encoded in event streams by using the proposed LoCM unsupervised fine-tuning framework to adapt an event flow estimator to a certain unseen scene. Then, we present the geometry-aware data association method to build the event-Gaussian motion correspondence, which is the primary foundation of the pipeline, accompanied by two useful strategies, namely motion decomposition and inter-frame pseudo-label. Extensive experiments show that our method outperforms existing image and event-based approaches across synthetic and real scenes and prove that our method can effectively optimize dynamic 3DGS with the help of event data.",
    "summary": "",
    "translation": "DEGS：基于可变形事件的三维高斯泼溅，从RGB和事件流中重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于事件相机和3D重建的计算机视觉技术，属于纯粹的视觉领域研究。标题中提到的3D高斯泼溅、事件流和RGB输入都是视觉感知和重建的特定技术，没有显示出与推荐系统、搜索或广告领域的任何潜在关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.07741v1": {
    "title": "UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes",
    "url": "https://www.alphaxiv.org/abs/2510.07741v1",
    "arxiv_id": "2510.07741v1",
    "authors": "Yuang Meng, Xin Jin, Lina Lei, Chun-Le Guo, Chongyi Li",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-09 03:29:39",
    "ori_summary": "Ultra-high dynamic range (UHDR) scenes exhibit significant exposure disparities between bright and dark regions. Such conditions are commonly encountered in nighttime scenes with light sources. Even with standard exposure settings, a bimodal intensity distribution with boundary peaks often emerges, making it difficult to preserve both highlight and shadow details simultaneously. RGB-based bracketing methods can capture details at both ends using short-long exposure pairs, but are susceptible to misalignment and ghosting artifacts. We found that a short-exposure image already retains sufficient highlight detail. The main challenge of UHDR reconstruction lies in denoising and recovering information in dark regions. In comparison to the RGB images, RAW images, thanks to their higher bit depth and more predictable noise characteristics, offer greater potential for addressing this challenge. This raises a key question: can we learn to see everything in UHDR scenes using only a single short-exposure RAW image? In this study, we rely solely on a single short-exposure frame, which inherently avoids ghosting and motion blur, making it particularly robust in dynamic scenes. To achieve that, we introduce UltraLED, a two-stage framework that performs exposure correction via a ratio map to balance dynamic range, followed by a brightness-aware RAW denoiser to enhance detail recovery in dark regions. To support this setting, we design a 9-stop bracketing pipeline to synthesize realistic UHDR images and contribute a corresponding dataset based on diverse scenes, using only the shortest exposure as input for reconstruction. Extensive experiments show that UltraLED significantly outperforms existing single-frame approaches. Our code and dataset are made publicly available at https://srameo.github.io/projects/ultraled.",
    "summary": "",
    "translation": "UltraLED：学习在超高动态范围场景中看到一切",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉中的超高动态范围成像技术，属于纯粹的视觉处理领域。虽然视觉技术在搜索和推荐中有潜在应用（如图像搜索），但论文标题没有表明与推荐系统、搜索或广告的直接关联，也没有涉及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11654v1": {
    "title": "FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection",
    "url": "https://www.alphaxiv.org/abs/2510.11654v1",
    "arxiv_id": "2510.11654v1",
    "authors": "Daniel Berhane Araya, Duoduo Liao",
    "categories": "cs.IR, cs.AI, cs.CL",
    "pub_date": "2025-10-13 17:31:49",
    "ori_summary": "Financial markets face growing threats from misinformation that can trigger billions in losses in minutes. Most existing approaches lack transparency in their decision-making and provide limited attribution to credible sources. We introduce FinVet, a novel multi-agent framework that integrates two Retrieval-Augmented Generation (RAG) pipelines with external fact-checking through a confidence-weighted voting mechanism. FinVet employs adaptive three-tier processing that dynamically adjusts verification strategies based on retrieval confidence, from direct metadata extraction to hybrid reasoning to full model-based analysis. Unlike existing methods, FinVet provides evidence-backed verdicts, source attribution, confidence scores, and explicit uncertainty flags when evidence is insufficient. Experimental evaluation on the FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a 10.4% improvement over the best individual pipeline (fact-check pipeline) and 37% improvement over standalone RAG approaches.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11639v1": {
    "title": "OneRec-Think: In-Text Reasoning for Generative Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.11639v1",
    "arxiv_id": "2510.11639v1",
    "authors": "Zhanyu Liu, Shiyao Wang, Xingmei Wang, Rongzhou Zhang, Jiaxin Deng, Honghui Bao, Jinghao Zhang, Wuchao Li, Pengfei Zheng, Xiangyu Wu, Yifei Hu, Qigen Hu, Xinchen Luo, Lejian Ren, Zixing Zhang, Qianqian Wang, Kuo Cai, Yunfan Wu, Hongtao Cheng, Zexuan Cheng, Lu Ren, Huanjie Wang, Yi Su, Ruiming Tang, Kun Gai, Guorui Zhou",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 17:20:13",
    "ori_summary": "The powerful generative capacity of Large Language Models (LLMs) has instigated a paradigm shift in recommendation. However, existing generative models (e.g., OneRec) operate as implicit predictors, critically lacking the capacity for explicit and controllable reasoning-a key advantage of LLMs. To bridge this gap, we propose OneRec-Think, a unified framework that seamlessly integrates dialogue, reasoning, and personalized recommendation. OneRec-Think incorporates: (1) Itemic Alignment: cross-modal Item-Textual Alignment for semantic grounding; (2) Reasoning Activation: Reasoning Scaffolding to activate LLM reasoning within the recommendation context; and (3) Reasoning Enhancement, where we design a recommendation-specific reward function that accounts for the multi-validity nature of user preferences. Experiments across public benchmarks show state-of-the-art performance. Moreover, our proposed \"Think-Ahead\" architecture enables effective industrial deployment on Kuaishou, achieving a 0.159\\% gain in APP Stay Time and validating the practical efficacy of the model's explicit reasoning capability.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11599v1": {
    "title": "SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific and Interpretable Scientific Domain Mapping",
    "url": "https://www.alphaxiv.org/abs/2510.11599v1",
    "arxiv_id": "2510.11599v1",
    "authors": "Marc Brinner, Sina Zarrieß",
    "categories": "cs.CL, cs.AI, cs.IR, cs.LG",
    "pub_date": "2025-10-13 16:38:20",
    "ori_summary": "We propose SemCSE-Multi, a novel unsupervised framework for generating multifaceted embeddings of scientific abstracts, evaluated in the domains of invasion biology and medicine. These embeddings capture distinct, individually specifiable aspects in isolation, thus enabling fine-grained and controllable similarity assessments as well as adaptive, user-driven visualizations of scientific domains. Our approach relies on an unsupervised procedure that produces aspect-specific summarizing sentences and trains embedding models to map semantically related summaries to nearby positions in the embedding space. We then distill these aspect-specific embedding capabilities into a unified embedding model that directly predicts multiple aspect embeddings from a scientific abstract in a single, efficient forward pass. In addition, we introduce an embedding decoding pipeline that decodes embeddings back into natural language descriptions of their associated aspects. Notably, we show that this decoding remains effective even for unoccupied regions in low-dimensional visualizations, thus offering vastly improved interpretability in user-centric settings.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11592v1": {
    "title": "REGENT: Relevance-Guided Attention for Entity-Aware Multi-Vector Neural Re-Ranking",
    "url": "https://www.alphaxiv.org/abs/2510.11592v1",
    "arxiv_id": "2510.11592v1",
    "authors": "Shubham Chatterjee",
    "categories": "cs.IR, cs.CL",
    "pub_date": "2025-10-13 16:31:42",
    "ori_summary": "Current neural re-rankers often struggle with complex information needs and long, content-rich documents. The fundamental issue is not computational--it is intelligent content selection: identifying what matters in lengthy, multi-faceted texts. While humans naturally anchor their understanding around key entities and concepts, neural models process text within rigid token windows, treating all interactions as equally important and missing critical semantic signals. We introduce REGENT, a neural re-ranking model that mimics human-like understanding by using entities as a \"semantic skeleton\" to guide attention. REGENT integrates relevance guidance directly into the attention mechanism, combining fine-grained lexical matching with high-level semantic reasoning. This relevance-guided attention enables the model to focus on conceptually important content while maintaining sensitivity to precise term matches. REGENT achieves new state-of-the-art performance in three challenging datasets, providing up to 108% improvement over BM25 and consistently outperforming strong baselines including ColBERT and RankVicuna. To our knowledge, this is the first work to successfully integrate entity semantics directly into neural attention, establishing a new paradigm for entity-aware information retrieval.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11589v1": {
    "title": "QDER: Query-Specific Document and Entity Representations for Multi-Vector Document Re-Ranking",
    "url": "https://www.alphaxiv.org/abs/2510.11589v1",
    "arxiv_id": "2510.11589v1",
    "authors": "Shubham Chatterjee, Jeff Dalton",
    "categories": "cs.IR, cs.CL",
    "pub_date": "2025-10-13 16:31:06",
    "ori_summary": "Neural IR has advanced through two distinct paths: entity-oriented approaches leveraging knowledge graphs and multi-vector models capturing fine-grained semantics. We introduce QDER, a neural re-ranking model that unifies these approaches by integrating knowledge graph semantics into a multi-vector model. QDER's key innovation lies in its modeling of query-document relationships: rather than computing similarity scores on aggregated embeddings, we maintain individual token and entity representations throughout the ranking process, performing aggregation only at the final scoring stage - an approach we call \"late aggregation.\" We first transform these fine-grained representations through learned attention patterns, then apply carefully chosen mathematical operations for precise matches. Experiments across five standard benchmarks show that QDER achieves significant performance gains, with improvements of 36% in nDCG@20 over the strongest baseline on TREC Robust 2004 and similar improvements on other datasets. QDER particularly excels on difficult queries, achieving an nDCG@20 of 0.70 where traditional approaches fail completely (nDCG@20 = 0.0), setting a foundation for future work in entity-aware retrieval.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11560v1": {
    "title": "Characterizing Web Search in The Age of Generative AI",
    "url": "https://www.alphaxiv.org/abs/2510.11560v1",
    "arxiv_id": "2510.11560v1",
    "authors": "Elisabeth Kirsten, Jost Grosse Perdekamp, Mihir Upadhyay, Krishna P. Gummadi, Muhammad Bilal Zafar",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-13 16:04:03",
    "ori_summary": "The advent of LLMs has given rise to a new type of web search: Generative search, where LLMs retrieve web pages related to a query and generate a single, coherent text as a response. This output modality stands in stark contrast to traditional web search, where results are returned as a ranked list of independent web pages. In this paper, we ask: Along what dimensions do generative search outputs differ from traditional web search? We compare Google, a traditional web search engine, with four generative search engines from two providers (Google and OpenAI) across queries from four domains. Our analysis reveals intriguing differences. Most generative search engines cover a wider range of sources compared to web search. Generative search engines vary in the degree to which they rely on internal knowledge contained within the model parameters v.s. external knowledge retrieved from the web. Generative search engines surface varying sets of concepts, creating new opportunities for enhancing search diversity and serendipity. Our results also highlight the need for revisiting evaluation criteria for web search in the age of Generative AI.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11483v1": {
    "title": "Uncertainty Quantification for Retrieval-Augmented Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.11483v1",
    "arxiv_id": "2510.11483v1",
    "authors": "Heydar Soudani, Hamed Zamani, Faegheh Hasibi",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 14:55:28",
    "ori_summary": "Retrieval-augmented reasoning (RAR) is a recent evolution of retrieval-augmented generation (RAG) that employs multiple reasoning steps for retrieval and generation. While effective for some complex queries, RAR remains vulnerable to errors and misleading outputs. Uncertainty quantification (UQ) offers methods to estimate the confidence of systems' outputs. These methods, however, often handle simple queries with no retrieval or single-step retrieval, without properly handling RAR setup. Accurate estimation of UQ for RAR requires accounting for all sources of uncertainty, including those arising from retrieval and generation. In this paper, we account for all these sources and introduce Retrieval-Augmented Reasoning Consistency (R2C)--a novel UQ method for RAR. The core idea of R2C is to perturb the multi-step reasoning process by applying various actions to reasoning steps. These perturbations alter the retriever's input, which shifts its output and consequently modifies the generator's input at the next step. Through this iterative feedback loop, the retriever and generator continuously reshape one another's inputs, enabling us to capture uncertainty arising from both components. Experiments on five popular RAR systems across diverse QA datasets show that R2C improves AUROC by over 5% on average compared to the state-of-the-art UQ baselines. Extrinsic evaluations using R2C as an external signal further confirm its effectiveness for two downstream tasks: in Abstention, it achieves ~5% gains in both F1Abstain and AccAbstain; in Model Selection, it improves the exact match by ~7% over single models and ~3% over selection methods.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11438v1": {
    "title": "What Generative Search Engines Like and How to Optimize Web Content Cooperatively",
    "url": "https://www.alphaxiv.org/abs/2510.11438v1",
    "arxiv_id": "2510.11438v1",
    "authors": "Yujiang Wu, Shanshan Zhong, Yubin Kim, Chenyan Xiong",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 14:10:26",
    "ori_summary": "By employing large language models (LLMs) to retrieve documents and generate natural language responses, Generative Engines, such as Google AI overview and ChatGPT, provide significantly enhanced user experiences and have rapidly become the new form of search. Their rapid adoption also drives the needs of Generative Engine Optimization (GEO), as content providers are eager to gain more traction from them. In this paper, we introduce AutoGEO, a framework to automatically learn generative engine preferences when using retrieved contents for response generation, and rewrite web contents for more such traction. AutoGEO first prompts frontier LLMs to explain generative engine preferences and extract meaningful preference rules from these explanations. Then it uses preference rules as context engineering for AutoGEO$_\\text{API}$, a prompt-based GEO system, and as rule-based rewards to train AutoGEO$_\\text{Mini}$, a cost-effective GEO model. Experiments on the standard GEO-Bench and two newly constructed benchmarks using real user queries demonstrate the effectiveness of AutoGEO in enhancing content traction while preserving search utility. Analyses confirm the learned rules' robustness and abilities to capture unique preferences in variant domains, and AutoGEO systems' ability to embed them in content optimization. The code is released at https://github.com/cxcscmu/AutoGEO.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11402v1": {
    "title": "On Inherited Popularity Bias in Cold-Start Item Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.11402v1",
    "arxiv_id": "2510.11402v1",
    "authors": "Gregor Meehan, Johan Pauwels",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 13:44:13",
    "ori_summary": "Collaborative filtering (CF) recommender systems struggle with making predictions on unseen, or 'cold', items. Systems designed to address this challenge are often trained with supervision from warm CF models in order to leverage collaborative and content information from the available interaction data. However, since they learn to replicate the behavior of CF methods, cold-start models may therefore also learn to imitate their predictive biases. In this paper, we show that cold-start systems can inherit popularity bias, a common cause of recommender system unfairness arising when CF models overfit to more popular items, thereby maximizing user-oriented accuracy but neglecting rarer items. We demonstrate that cold-start recommenders not only mirror the popularity biases of warm models, but are in fact affected more severely: because they cannot infer popularity from interaction data, they instead attempt to estimate it based solely on content features. This leads to significant over-prediction of certain cold items with similar content to popular warm items, even if their ground truth popularity is very low. Through experiments on three multimedia datasets, we analyze the impact of this behavior on three generative cold-start methods. We then describe a simple post-processing bias mitigation method that, by using embedding magnitude as a proxy for predicted popularity, can produce more balanced recommendations with limited harm to user-oriented cold-start accuracy.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11394v1": {
    "title": "VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation via Rigorous Verification",
    "url": "https://www.alphaxiv.org/abs/2510.11394v1",
    "arxiv_id": "2510.11394v1",
    "authors": "Haosheng Qian, Yixing Fan, Jiafeng Guo, Ruqing Zhang, Qi Chen, Dawei Yin, Xueqi Cheng",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 13:38:54",
    "ori_summary": "Retrieval-Augmented Generation (RAG) has emerged as a crucial approach for enhancing the responses of large language models (LLMs) with external knowledge sources. Despite the impressive performance in complex question-answering tasks, RAG still struggles with hallucinations. Attributing RAG-generated content through in-line citations has demonstrated potential in reducing hallucinations and facilitating human verification. Existing citation generation methods primarily rely on either fine-tuning the generator or employing post-processing approaches for citation matching. However, the former approach demands substantial annotated data and computational resources, while the latter often encounters difficulties in managing multiple citations and frequently produces suboptimal results. In this paper, we introduce a novel framework, called VeriCite, designed to rigorously validate supporting evidence and enhance answer attribution. Specifically, VeriCite breaks down into a three-stage generation: 1) The initial answer generation first generates a response based on all available contexts and has its claims verified through the NLI model; 2) the supporting evidence selection assesses the utility of each document and extracts useful supporting evidences; 3) the final answer refinement integrates the initial response and collected evidences to produce the final, refined answer.We conduct experiments across five open-source LLMs and four datasets, demonstrating that VeriCite can significantly improve citation quality while maintaining the correctness of the answers.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11358v1": {
    "title": "LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2510.11358v1",
    "arxiv_id": "2510.11358v1",
    "authors": "Hengran Zhang, Keping Bi, Jiafeng Guo, Jiaming Zhang, Shuaiqiang Wang, Dawei Yin, Xueqi Cheng",
    "categories": "cs.CL, cs.AI, cs.IR",
    "pub_date": "2025-10-13 12:57:45",
    "ori_summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge. While traditional retrieval focuses on relevance, RAG's effectiveness depends on the utility of retrieved passages, i.e., the usefulness in facilitating the generation of an accurate and comprehensive answer. Existing studies often treat utility as a generic attribute, ignoring the fact that different LLMs may benefit differently from the same passage due to variations in internal knowledge and comprehension ability. In this work, we introduce and systematically investigate the notion of LLM-specific utility. Through large-scale experiments across multiple datasets and LLMs, we demonstrate that human-annotated passages are not optimal for LLMs and that ground-truth utilitarian passages are not transferable across different LLMs. These findings highlight the necessity of adopting the LLM-specific utility in RAG research. Our findings indicate that some human-annotated passages are not ground-truth utilitarian passages for specific LLMs, partially due to the varying readability of queries and passages for LLMs, a tendency for which perplexity is a key metric. Based on these findings, we propose a benchmarking procedure for LLM-specific utility judgments. We evaluate existing utility judgment methods on six datasets and find that while verbalized methods using pseudo-answers perform robustly, LLMs struggle to assess utility effectively-failing to reject all passages for known queries and to select truly useful ones for unknown queries.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11323v1": {
    "title": "Dynamic Network-Based Two-Stage Time Series Forecasting for Affiliate Marketing",
    "url": "https://www.alphaxiv.org/abs/2510.11323v1",
    "arxiv_id": "2510.11323v1",
    "authors": "Zhe Wang, Yaming Yang, Ziyu Guan, Bin Tong, Rui Wang, Wei Zhao, Hongbo Deng",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 12:21:29",
    "ori_summary": "In recent years, affiliate marketing has emerged as a revenue-sharing strategy where merchants collaborate with promoters to promote their products. It not only increases product exposure but also allows promoters to earn a commission. This paper addresses the pivotal yet under-explored challenge in affiliate marketing: accurately assessing and predicting the contributions of promoters in product promotion. We design a novel metric for evaluating the indirect contributions of the promoter, called propagation scale. Unfortunately, existing time series forecasting techniques fail to deliver accurate predictions due to the propagation scale being influenced by multiple factors and the inherent complexities arising from dynamic scenarios. To address this issue, we decouple the network structure from the node signals and propose a two-stage solution: initially, the basic self-sales and network structure prediction are conducted separately, followed by the synthesis of the propagation scale. Specifically, we design a graph convolution encoding scheme based on descendant neighbors and incorporate hypergraph convolution to efficiently capture complex promotional dynamics. Additionally, three auxiliary tasks are employed: self-sales prediction for base estimations, descendant prediction to synthesize propagation scale, and promoter activation prediction to mitigate high volatility issues. Extensive offline experiments on large-scale industrial datasets validate the superiority of our method. We further deploy our model on Alimama platform with over $100,000$ promoters, achieving a $9.29\\%$ improvement in GMV and a $5.89\\%$ increase in sales volume.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11317v1": {
    "title": "Next Interest Flow: A Generative Pre-training Paradigm for Recommender Systems by Modeling All-domain Movelines",
    "url": "https://www.alphaxiv.org/abs/2510.11317v1",
    "arxiv_id": "2510.11317v1",
    "authors": "Chen Gao, Zixin Zhao, Lv Shao, Tong Liu",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 12:13:17",
    "ori_summary": "Click-Through Rate (CTR) prediction, a cornerstone of modern recommender systems, has been dominated by discriminative models that react to past user behavior rather than proactively modeling user intent. Existing generative paradigms attempt to address this but suffer from critical limitations: Large Language Model (LLM) based methods create a semantic mismatch by forcing e-commerce signals into a linguistic space, while ID-based generation is constrained by item memorization and cold-start issues. To overcome these limitations, we propose a novel generative pre-training paradigm. Our model learns to predict the Next Interest Flow, a dense vector sequence representing a user's future intent, while simultaneously modeling its internal Interest Diversity and Interest Evolution Velocity to ensure the representation is both rich and coherent. However, this two-stage approach introduces a critical objective mismatch between the generative and discriminative stages. We resolve this via a bidirectional alignment strategy, which harmonizes the two stages through cross-stage weight initialization and a dynamic Semantic Alignment Module for fine-tuning. Additionally, we enhance the underlying discriminative model with a Temporal Sequential Pairwise (TSP) mechanism to better capture temporal causality. We present the All-domain Moveline Evolution Network (AMEN), a unified framework implementing our entire pipeline. Extensive offline experiments validate AMEN's superiority over strong baselines, and a large-scale online A/B test demonstrates its significant real-world impact, delivering substantial improvements in key business metrics.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11168v1": {
    "title": "ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces",
    "url": "https://www.alphaxiv.org/abs/2510.11168v1",
    "arxiv_id": "2510.11168v1",
    "authors": "Jinbin Zhang, Nasib Ullah, Erik Schultheis, Rohit Babbar",
    "categories": "cs.LG, cs.CL, cs.IR",
    "pub_date": "2025-10-13 08:59:13",
    "ori_summary": "Large output spaces, also referred to as Extreme multilabel classification (XMC), is a setting that arises, e.g., in large-scale tagging and product-to-product recommendation, and is characterized by the number of labels ranging from hundreds of thousands to millions. This means that the linear classification head, usually only a tiny fraction of the overall model, turns into the main driver for compute and memory demand. Current state-of-the-art XMC methods predominantly rely on FP16-FP32 mixed-precision training, which we show can be unstable, and inefficient in terms of memory usage and computational overhead. Meanwhile, existing low-precision methods typically retain higher precision for the classification layer. In this work, we propose ELMO, a pure low-precision training framework for XMC models using BFloat16 and Float8 data types. By leveraging Kahan summation and stochastic rounding, we demonstrate that XMC models can be effectively trained entirely in Float8, without relying on single-precision master weights or tensor scaling. Low-precision training, combined with our proposed memory optimizations -- gradient fusion and chunking -- enables significant reductions in GPU memory usage. For example, we train a 3-million-label XMC model with only 6.6 GiB of GPU memory, compared to the 39.7 GiB required by the optimized SOTA method, Renee without compromising accuracy.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11122v1": {
    "title": "DyKnow-RAG: Dynamic Knowledge Utilization Reinforcement Framework for Noisy Retrieval-Augmented Generation in E-commerce Search Relevance",
    "url": "https://www.alphaxiv.org/abs/2510.11122v1",
    "arxiv_id": "2510.11122v1",
    "authors": "Tingqiao Xu, Shaowei Yao, Chenhe Dong, Yiming Jin, Zerui Huang, Dan Ou, Haihong Tang",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 08:08:59",
    "ori_summary": "Accurately modeling query-item relevance drives e-commerce ranking, yet long-tail, knowledge-heavy, and fast-evolving queries exceed parametric LLM coverage. External context (reviews, attribute encyclopedias, UGC) can help but is noisy, and single-pass latency and cost forbid any clean-then-summarize step. The model must, per query, judge relevance and decide whether to use, partially use, or ignore the context. DyKnow-RAG is a dynamic noisy-RAG framework built on Group Relative Policy Optimization. It trains two rollout groups (no external context vs a single retrieved chunk) and applies posterior-driven inter-group advantage scaling that adaptively reweights their contributions by the per-query correctness gap. This teaches when to trust retrieval versus fall back to parametric knowledge, without process labels, value networks, or extra inference passes, preserving single-pass, single-chunk deployment under production latency. Training combines: (1) supervised initialization with a structured rationale that explicitly records the context-usage decision; (2) an RL pool prioritized by SFT uncertainty to focus where context choice is most consequential; and (3) an optional lightweight DPO warm start to stabilize with-context calibration. Under a unified retrieval/index and fixed latency budget, DyKnow-RAG outperforms SFT, DPO, and vanilla GRPO in offline tests, and delivers consistent lifts on GSB, Query Goodrate, and Item Goodrate in Taobao A/B testing. It is deployed in Taobao's production relevance system, serving live traffic. To our knowledge, it is among the first single-pass RAG solutions for e-commerce relevance, turning noisy external signals into reliable gains without added online complexity.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11100v1": {
    "title": "HoMer: Addressing Heterogeneities by Modeling Sequential and Set-wise Contexts for CTR Prediction",
    "url": "https://www.alphaxiv.org/abs/2510.11100v1",
    "arxiv_id": "2510.11100v1",
    "authors": "Shuwei Chen, Jiajun Cui, Zhengqi Xu, Fan Zhang, Jiangke Fan, Teng Zhang, Xingxing Wang",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-13 07:47:03",
    "ori_summary": "Click-through rate (CTR) prediction, which models behavior sequence and non-sequential features (e.g., user/item profiles or cross features) to infer user interest, underpins industrial recommender systems. However, most methods face three forms of heterogeneity that degrade predictive performance: (i) Feature Heterogeneity persists when limited sequence side features provide less granular interest representation compared to extensive non-sequential features, thereby impairing sequence modeling performance; (ii) Context Heterogeneity arises because a user's interest in an item will be influenced by other items, yet point-wise prediction neglects cross-item interaction context from the entire item set; (iii) Architecture Heterogeneity stems from the fragmented integration of specialized network modules, which compounds the model's effectiveness, efficiency and scalability in industrial deployments. To tackle the above limitations, we propose HoMer, a Homogeneous-Oriented TransforMer for modeling sequential and set-wise contexts. First, we align sequence side features with non-sequential features for accurate sequence modeling and fine-grained interest representation. Second, we shift the prediction paradigm from point-wise to set-wise, facilitating cross-item interaction in a highly parallel manner. Third, HoMer's unified encoder-decoder architecture achieves dual optimization through structural simplification and shared computation, ensuring computational efficiency while maintaining scalability with model size. Without arduous modification to the prediction pipeline, HoMer successfully scales up and outperforms our industrial baseline by 0.0099 in the AUC metric, and enhances online business metrics like CTR/RPM by 1.99%/2.46%. Additionally, HoMer saves 27% of GPU resources via preliminary engineering optimization, further validating its superiority and practicality.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11066v1": {
    "title": "Decoupled Multimodal Fusion for User Interest Modeling in Click-Through Rate Prediction",
    "url": "https://www.alphaxiv.org/abs/2510.11066v1",
    "arxiv_id": "2510.11066v1",
    "authors": "Alin Fan, Hanqing Li, Sihan Lu, Jingsong Yuan, Jiandong Zhang",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 07:06:26",
    "ori_summary": "Modern industrial recommendation systems improve recommendation performance by integrating multimodal representations from pre-trained models into ID-based Click-Through Rate (CTR) prediction frameworks. However, existing approaches typically adopt modality-centric modeling strategies that process ID-based and multimodal embeddings independently, failing to capture fine-grained interactions between content semantics and behavioral signals. In this paper, we propose Decoupled Multimodal Fusion (DMF), which introduces a modality-enriched modeling strategy to enable fine-grained interactions between ID-based collaborative representations and multimodal representations for user interest modeling. Specifically, we construct target-aware features to bridge the semantic gap across different embedding spaces and leverage them as side information to enhance the effectiveness of user interest modeling. Furthermore, we design an inference-optimized attention mechanism that decouples the computation of target-aware features and ID-based embeddings before the attention layer, thereby alleviating the computational bottleneck introduced by incorporating target-aware features. To achieve comprehensive multimodal integration, DMF combines user interest representations learned under the modality-centric and modality-enriched modeling strategies. Offline experiments on public and industrial datasets demonstrate the effectiveness of DMF. Moreover, DMF has been deployed on the product recommendation system of the international e-commerce platform Lazada, achieving relative improvements of 5.30% in CTCVR and 7.43% in GMV with negligible computational overhead.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11056v1": {
    "title": "From Reasoning LLMs to BERT: A Two-Stage Distillation Framework for Search Relevance",
    "url": "https://www.alphaxiv.org/abs/2510.11056v1",
    "arxiv_id": "2510.11056v1",
    "authors": "Runze Xia, Yupeng Ji, Yuxi Zhou, Haodong Liu, Teng Zhang, Piji Li",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-13 06:46:43",
    "ori_summary": "Query-service relevance prediction in e-commerce search systems faces strict latency requirements that prevent the direct application of Large Language Models (LLMs). To bridge this gap, we propose a two-stage reasoning distillation framework to transfer reasoning capabilities from a powerful teacher LLM to a lightweight, deployment-friendly student model. In the first stage, we address the limitations of general-purpose LLMs by constructing a domain-adapted teacher model. This is achieved through a three-step process: domain-adaptive pre-training to inject platform knowledge, supervised fine-tuning to elicit reasoning skills, and preference optimization with a multi-dimensional reward model to ensure the generation of reliable and preference-aligned reasoning paths. This teacher can then automatically annotate massive query-service pairs from search logs with both relevance labels and reasoning chains. In the second stage, to address the challenges of architectural heterogeneity in standard distillation, we introduce Contrastive Reasoning Self-Distillation (CRSD). By modeling the behavior of the same student model under \"standard\" and \"reasoning-augmented\" inputs as a teacher-student relationship, CRSD enables the lightweight model to internalize the teacher's complex decision-making mechanisms without needing the explicit reasoning path at inference. Offline evaluations and online A/B testing in the Meituan search advertising system demonstrate that our framework achieves significant improvements across multiple metrics, validating its effectiveness and practical value.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11003v1": {
    "title": "FBS Model-based Maintenance Record Accumulation for Failure-Cause Inference in Manufacturing Systems",
    "url": "https://www.alphaxiv.org/abs/2510.11003v1",
    "arxiv_id": "2510.11003v1",
    "authors": "Takuma Fujiu, Sho Okazaki, Kohei Kaminishi, Yuji Nakata, Shota Hamamoto, Kenshin Yokose, Tatsunori Hara, Yasushi Umeda, Jun Ota",
    "categories": "cs.AI, cs.IR",
    "pub_date": "2025-10-13 04:37:40",
    "ori_summary": "In manufacturing systems, identifying the causes of failures is crucial for maintaining and improving production efficiency. In knowledge-based failure-cause inference, it is important that the knowledge base (1) explicitly structures knowledge about the target system and about failures, and (2) contains sufficiently long causal chains of failures. In this study, we constructed Diagnostic Knowledge Ontology and proposed a Function-Behavior-Structure (FBS) model-based maintenance-record accumulation method based on it. Failure-cause inference using the maintenance records accumulated by the proposed method showed better agreement with the set of candidate causes enumerated by experts, especially in difficult cases where the number of related cases is small and the vocabulary used differs. In the future, it will be necessary to develop inference methods tailored to these maintenance records, build a user interface, and carry out validation on larger and more diverse systems. Additionally, this approach leverages the understanding and knowledge of the target in the design phase to support knowledge accumulation and problem solving during the maintenance phase, and it is expected to become a foundation for knowledge sharing across the entire engineering chain in the future.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10978v1": {
    "title": "Does LLM Focus on the Right Words? Diagnosing Language Bias in LLM-based Recommenders",
    "url": "https://www.alphaxiv.org/abs/2510.10978v1",
    "arxiv_id": "2510.10978v1",
    "authors": "Bohao Wang, Jiawei Chen, Feng Liu, Changwang Zhang, Jun Wang, Canghong Jin, Chun Chen, Can Wang",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 03:35:26",
    "ori_summary": "Large language models (LLMs), owing to their extensive open-domain knowledge and semantic reasoning capabilities, have been increasingly integrated into recommender systems (RS). However, a substantial gap remains between the pre-training objectives of LLMs and the specific requirements of recommendation tasks. To address this gap, supervised fine-tuning (SFT) is commonly performed on specially curated recommendation datasets to further enhance their predictive ability. Despite its success, SFT exhibits a critical limitation: it induces Language Bias, whereby the model over-relies on auxiliary tokens-such as task descriptions and prefix-generated tokens-while underutilizing core user interaction tokens that encode user-specific preferences. This bias not only undermines recommendation accuracy but also raises unfairness concerns. To address this issue, we propose Group Distributionally Robust Optimization-based Tuning (GDRT), a novel fine-tuning paradigm that enforces consistent model performance across token groups with varying degrees of relevance to auxiliary tokens. By adaptively upweighting underperforming groups, typically those weakly correlated with auxiliary tokens, GDRT shifts the model's attention from superficial auxiliary cues to informative user interaction tokens, thereby mitigating language bias. Extensive experiments conducted on three public datasets demonstrate that GDRT effectively mitigates language bias, yielding substantial improvements in recommendation accuracy (with an average NDCG@10 gain of 24.29%) and significantly enhancing recommendation fairness.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10955v1": {
    "title": "HatLLM: Hierarchical Attention Masking for Enhanced Collaborative Modeling in LLM-based Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.10955v1",
    "arxiv_id": "2510.10955v1",
    "authors": "Yu Cui, Feng Liu, Jiawei Chen, Canghong Jin, Xingyu Lou, Changwang Zhang, Jun Wang, Yuegang Sun, Can Wang",
    "categories": "cs.IR",
    "pub_date": "2025-10-13 03:05:03",
    "ori_summary": "Recent years have witnessed a surge of research on leveraging large language models (LLMs) for sequential recommendation. LLMs have demonstrated remarkable potential in inferring users' nuanced preferences through fine-grained semantic reasoning. However, they also exhibit a notable limitation in effectively modeling collaborative signals, i.e., behavioral correlations inherent in users' historical interactions. Our empirical analysis further reveals that the attention mechanisms in LLMs tend to disproportionately focus on tokens within the same item, thereby impeding the capture of cross-item correlations. To address this limitation, we propose a novel hierarchical attention masking strategy for LLM-based recommendation, termed HatLLM. Specifically, in shallow layers, HatLLM masks attention between tokens from different items, facilitating intra-item semantic understanding; in contrast, in deep layers, HatLLM masks attention within items, thereby compelling the model to capture cross-item correlations. This progressive, layer-wise approach enables LLMs to jointly model both token-level and item-level dependencies. Extensive experiments on three real-world datasets demonstrate that HatLLM achieves significant performance gains (9.13% on average) over existing LLM-based methods.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10920v1": {
    "title": "Comparative Explanations via Counterfactual Reasoning in Recommendations",
    "url": "https://www.alphaxiv.org/abs/2510.10920v1",
    "arxiv_id": "2510.10920v1",
    "authors": "Yi Yu, Zhenxing Hu",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-13 02:31:03",
    "ori_summary": "Explainable recommendation through counterfactual reasoning seeks to identify the influential aspects of items in recommendations, which can then be used as explanations. However, state-of-the-art approaches, which aim to minimize changes in product aspects while reversing their recommended decisions according to an aggregated decision boundary score, often lead to factual inaccuracies in explanations. To solve this problem, in this work we propose a novel method of Comparative Counterfactual Explanations for Recommendation (CoCountER). CoCountER creates counterfactual data based on soft swap operations, enabling explanations for recommendations of arbitrary pairs of comparative items. Empirical experiments validate the effectiveness of our approach.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11713v1": {
    "title": "Are Large Reasoning Models Interruptible?",
    "url": "https://www.alphaxiv.org/abs/2510.11713v1",
    "arxiv_id": "2510.11713v1",
    "authors": "Tsung-Han Wu, Mihran Miroyan, David M. Chan, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-13 17:59:35",
    "ori_summary": "Large Reasoning Models (LRMs) excel at complex reasoning but are traditionally evaluated in static, \"frozen world\" settings: model responses are assumed to be instantaneous, and the context of a request is presumed to be immutable over the duration of the response. While generally true for short-term tasks, the \"frozen world\" assumption breaks down in modern reasoning tasks such as assistive programming, where models may take hours to think through problems and code may change dramatically from the time the model starts thinking to the model's final output. In this work, we challenge the frozen world assumption and evaluate LRM robustness under two realistic dynamic scenarios: interruptions, which test the quality of the model's partial outputs on a limited budget, and dynamic context, which tests model adaptation to in-flight changes. Across mathematics and programming benchmarks that require long-form reasoning, static evaluations consistently overestimate robustness: even state-of-the-art LRMs, which achieve high accuracy in static settings, can fail unpredictably when interrupted or exposed to changing context, with performance dropping by up to 60% when updates are introduced late in the reasoning process. Our analysis further reveals several novel failure modes, including reasoning leakage, where models fold the reasoning into their final answer when interrupted; panic, where under time pressure models abandon reasoning entirely and return incorrect answers; and self-doubt, where performance degrades while incorporating updated information.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11701v1": {
    "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.11701v1",
    "arxiv_id": "2510.11701v1",
    "authors": "Zhaochen Yu, Ling Yang, Jiaru Zou, Shuicheng Yan, Mengdi Wang",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 17:57:15",
    "ori_summary": "Recently, the emergence of agentic RL has showcased that RL could also effectively improve the agentic reasoning ability of LLMs, yet the key design principles and optimal practices remain unclear. In this work, we conduct a comprehensive and systematic investigation to demystify reinforcement learning in agentic reasoning from three key perspectives: data, algorithm, and reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic trajectories with real end-to-end tool-use trajectories yields a far stronger SFT initialization; high-diversity, model-aware datasets sustain exploration and markedly improve RL performance. (ii) Exploration-friendly techniques are crucial for agentic RL, such as clip higher, overlong reward shaping, and maintaining adequate policy entropy could improve the training efficiency. (iii) A deliberative strategy with fewer tool calls outperforms frequent tool calls or verbose self-reasoning, improving tool efficiency and final accuracy. Together, these simple practices consistently enhance agentic reasoning and training efficiency, achieving strong results on challenging benchmarks with smaller models, and establishing a practical baseline for future agentic RL research. Beyond these empirical insights, we further contribute a high-quality, real end-to-end agentic SFT dataset along with a high-quality RL dataset, and demonstrate the effectiveness of our insights in boosting the agentic reasoning ability of LLMs across four challenging benchmarks, including AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes, 4B-sized models could also achieve superior agentic reasoning performance compared to 32B-sized models. Code and models: https://github.com/Gen-Verse/Open-AgentRL",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11696v1": {
    "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.11696v1",
    "arxiv_id": "2510.11696v1",
    "authors": "Wei Huang, Yi Ge, Shuai Yang, Yicheng Xiao, Huizi Mao, Yujun Lin, Hanrong Ye, Sifei Liu, Ka Chun Cheung, Hongxu Yin, Yao Lu, Xiaojuan Qi, Song Han, Yukang Chen",
    "categories": "cs.LG, cs.CL, cs.CV",
    "pub_date": "2025-10-13 17:55:09",
    "ori_summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for large language models (LLMs). While RL is essential for LLMs' reasoning capabilities, it is resource-intensive, requiring substantial GPU memory and long rollout durations. QeRL addresses these issues by combining NVFP4 quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL while reducing memory overhead. Beyond efficiency, our findings show that quantization noise increases policy entropy, enhancing exploration, and enabling the discovery of better strategies during RL. To further optimize exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism, which dynamically adjusts noise during training. Experiments demonstrate that QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is the first framework to enable RL training of a 32B LLM on a single H100 80GB GPU, while delivering overall speedups for RL training. It also achieves faster reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while matching the performance of full-parameter fine-tuning on mathematical benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These results establish QeRL as an efficient and effective framework for RL training in LLMs.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11695v1": {
    "title": "When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents",
    "url": "https://www.alphaxiv.org/abs/2510.11695v1",
    "arxiv_id": "2510.11695v1",
    "authors": "Lingfei Qian, Xueqing Peng, Yan Wang, Vincent Jim Zhang, Huan He, Hanley Smith, Yi Han, Yueru He, Haohang Li, Yupeng Cao, Yangyang Yu, Alejandro Lopez-Lira, Peng Lu, Jian-Yun Nie, Guojun Xiong, Jimin Huang, Sophia Ananiadou",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 17:54:09",
    "ori_summary": "Although Large Language Model (LLM)-based agents are increasingly used in financial trading, it remains unclear whether they can reason and adapt in live markets, as most studies test models instead of agents, cover limited periods and assets, and rely on unverified data. To address these gaps, we introduce Agent Market Arena (AMA), the first lifelong, real-time benchmark for evaluating LLM-based trading agents across multiple markets. AMA integrates verified trading data, expert-checked news, and diverse agent architectures within a unified trading framework, enabling fair and continuous comparison under real conditions. It implements four agents, including InvestorAgent as a single-agent baseline, TradeAgent and HedgeFundAgent with different risk styles, and DeepFundAgent with memory-based reasoning, and evaluates them across GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and Gemini-2.0-flash. Live experiments on both cryptocurrency and stock markets demonstrate that agent frameworks display markedly distinct behavioral patterns, spanning from aggressive risk-taking to conservative decision-making, whereas model backbones contribute less to outcome variation. AMA thus establishes a foundation for rigorous, reproducible, and continuously evolving evaluation of financial reasoning and trading intelligence in LLM-based agents.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11693v1": {
    "title": "Scaling Language-Centric Omnimodal Representation Learning",
    "url": "https://www.alphaxiv.org/abs/2510.11693v1",
    "arxiv_id": "2510.11693v1",
    "authors": "Chenghao Xiao, Hou Pong Chan, Hao Zhang, Weiwen Xu, Mahani Aljunied, Yu Rong",
    "categories": "cs.CL, cs.AI, cs.CV",
    "pub_date": "2025-10-13 17:53:52",
    "ori_summary": "Recent multimodal embedding approaches leveraging multimodal large language models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising results, yet the underlying reasons behind their superiority remain underexplored. This work argues that a crucial advantage of MLLM-based approaches stems from implicit cross-modal alignment achieved during generative pretraining, where the language decoder learns to exploit multimodal signals within a shared representation space for generating unimodal outputs. Through analysis of anisotropy and kernel similarity structure, we empirically confirm that latent alignment emerges within MLLM representations, allowing CL to serve as a lightweight refinement stage. Leveraging this insight, we propose a Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive experiments across diverse backbones and benchmarks demonstrate its effectiveness, achieving state-of-the-art performance across modalities. Furthermore, we identify a Generation-Representation Scaling Law (GRSL), showing that the representational capabilities gained through contrastive refinement scales positively with the MLLM's generative capabilities. This suggests that improving generative abilities evolves as an effective paradigm for enhancing representation quality. We provide a theoretical explanation of GRSL, which formally links the MLLM's generative quality to the upper bound on its representation performance, and validate it on a challenging, low-resource visual-document retrieval task, showing that continual generative pretraining before CL can further enhance the potential of a model's embedding capabilities. Codes, models, and resources are available at https://github.com/LCO-Embedding/LCO-Embedding.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11683v1": {
    "title": "Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11683v1",
    "arxiv_id": "2510.11683v1",
    "authors": "Nianyi Lin, Jiajie Zhang, Lei Hou, Juanzi Li",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-13 17:47:50",
    "ori_summary": "A key challenge in applying reinforcement learning (RL) to diffusion large language models (dLLMs) lies in the intractability of their likelihood functions, which are essential for the RL objective, necessitating corresponding approximation in each training step. While existing methods approximate the log-likelihoods by their evidence lower bounds (ELBOs) via customized Monte Carlo (MC) sampling, the forward computational graphs of all MC samples need to be retained for the gradient computation of non-linear terms in the RL objective, resulting in significant memory overhead. This constraint restricts feasible sample sizes, leading to imprecise likelihood approximations and ultimately distorting the RL objective. To overcome this limitation, we propose \\emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient RL algorithm that maximizes a specially constructed lower bound of the ELBO-based objective. This lower bound is carefully designed to satisfy two key properties: (1) Linearity: it is formulated in a linear sum where each term depends only on a single MC sample, thereby enabling gradient accumulation across samples and ensuring constant memory usage; (2) Equivalence: Both the value and gradient of this lower bound are equal to those of the ELBO-based objective in on-policy training, making it also an effective approximation for the original RL objective. These properties allow BGPO to adopt a large MC sample size, resulting in more accurate likelihood approximations and improved RL objective estimation, which in turn leads to enhanced performance. Experiments show that BGPO significantly outperforms previous RL algorithms for dLLMs in math problem solving, code generation, and planning tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11652v1": {
    "title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems",
    "url": "https://www.alphaxiv.org/abs/2510.11652v1",
    "arxiv_id": "2510.11652v1",
    "authors": "Xin Gui, King Zhu, JinCheng Ren, Qianben Chen, Zekun Moore Wang, Yizhi LI, Xinpeng Liu, Xiaowan Li, Wenli Ren, Linyu Miao, Tianrui Qin, Ziqi Shu, He Zhu, Xiangru Tang, Dingfeng Shi, Jiaheng Liu, Yuchen Eleanor Jiang, Minghao Liu, Ge Zhang, Wangchunshu Zhou",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 17:30:36",
    "ori_summary": "In recent years, the research focus of large language models (LLMs) and agents has shifted increasingly from demonstrating novel capabilities to complex reasoning and tackling challenging tasks. However, existing evaluations focus mainly on math/code contests or general tasks, while existing multi-domain academic benchmarks lack sufficient reasoning depth, leaving the field without a rigorous benchmark for high-level reasoning. To fill this gap, we introduce the Acadreason benchmark, designed to evaluate the ability of LLMs and agents to acquire and reason over academic knowledge. It consists of 50 expert-annotated academic problems across five high-reasoning domains, including computer science, economics, law, mathematics, and philosophy. All questions are sourced from top-tier publications in recent years and undergo rigorous annotation and quality control to ensure they are both challenging and answerable. We conduct systematic evaluations of over 10 mainstream LLMs and agents. The results show that most LLMs scored below 20 points, with even the cutting-edge GPT-5 achieving only 16 points. While agents achieved higher scores, none exceeded 40 points. This demonstrates the current capability gap between LLMs and agents in super-intelligent academic research tasks and highlights the challenges of Acadreason.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11620v1": {
    "title": "Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan Aggregation",
    "url": "https://www.alphaxiv.org/abs/2510.11620v1",
    "arxiv_id": "2510.11620v1",
    "authors": "Siheng Xiong, Ali Payani, Faramarz Fekri",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 17:02:41",
    "ori_summary": "Inference-time scaling enhances the reasoning ability of a language model (LM) by extending its chain-of-thought (CoT). However, existing approaches typically generate the entire reasoning chain in a single forward pass, which often leads to CoT derailment, i.e., the reasoning trajectory drifting off course due to compounding errors. This problem is particularly severe for smaller LMs with long CoTs due to their limited capacity. To address this, we analyze raw long CoTs and uncover a reasoning hierarchy consisting of planning and execution steps. Our analysis reveals that most reasoning errors stem from incorrect planning. Motivated by this observation, we propose Multi-Path Plan Aggregation (MPPA), a framework that augments single-pass reasoning with plan exploration and aggregation. Following a variable interval schedule based on the token position, MPPA generates multiple candidate plans and aggregates them into a refined planning step. To maintain efficiency, we adopt a minimal design in which the base LM serves as the primary policy, while a lightweight LoRA module implements the plan aggregation policy. We further observe that outcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K tokens). To overcome this, we introduce online Step-DPO, a process-level preference optimization scheme that leverages Twisted Sequential Monte Carlo (TSMC) to provide scalable stepwise supervision using small LMs. This yields more efficient training, improved stability, and higher accuracy. Extensive experiments on challenging math, science, and logical reasoning benchmarks demonstrate that, with only 10% SFT data and 5% of preference pairs, our method outperforms both the DeepSeek-R1 distillation baseline and the outcome-reward RL baseline across multiple base models and tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11618v1": {
    "title": "StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11618v1",
    "arxiv_id": "2510.11618v1",
    "authors": "Zehao Chen, Rong Pan, Haoran Li",
    "categories": "cs.CL, cs.MA",
    "pub_date": "2025-10-13 16:57:32",
    "ori_summary": "Human writers often begin their stories with an overarching mental scene, where they envision the interactions between characters and their environment. Inspired by this creative process, we propose a novel approach to long-form story generation, termed hybrid bottom-up long-form story generation, using multi-agent simulations. In our method, agents interact within a dynamic sandbox environment, where their behaviors and interactions with one another and the environment generate emergent events. These events form the foundation for the story, enabling organic character development and plot progression. Unlike traditional top-down approaches that impose rigid structures, our hybrid bottom-up approach allows for the natural unfolding of events, fostering more spontaneous and engaging storytelling. The system is capable of generating stories exceeding 10,000 words while maintaining coherence and consistency, addressing some of the key challenges faced by current story generation models. We achieve state-of-the-art performance across several metrics. This approach offers a scalable and innovative solution for creating dynamic, immersive long-form stories that evolve organically from agent-driven interactions.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11615v1": {
    "title": "LLM-Oriented Token-Adaptive Knowledge Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.11615v1",
    "arxiv_id": "2510.11615v1",
    "authors": "Xurong Xie, Zhucun Xue, Jiafu Wu, Jian Li, Yabiao Wang, Xiaobin Hu, Yong Liu, Jiangning Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 16:55:07",
    "ori_summary": "Knowledge distillation (KD) is a key technique for compressing large-scale language models (LLMs), yet prevailing logit-based methods typically employ static strategies that are misaligned with the dynamic learning process of student models. These methods typically treat all tokens indiscriminately and apply a single, fixed temperature, resulting in suboptimal knowledge transfer. To address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge Distillation (AdaKD), a novel framework that adapts the distillation process to the real-time learning state of each token. AdaKD consists of two synergistic modules driven by a unified token difficulty metric. First, our Loss-Driven Adaptive Token Focusing (LATF) module dynamically adjusts the distillation focus by monitoring the student's learning stability, concentrating computational resources on the most valuable tokens at each training phase. Second, we introduce Inverse Difficulty Temperature Scaling (IDTS), a counterintuitive yet effective token-level temperature strategy. It employs low temperatures for difficult tokens for targeted error correction, and high temperatures for easy tokens to encourage students to learn from the teacher's complete and smooth output distribution, thereby enhancing generalization. As a plug-and-play framework, AdaKD can consistently improve the performance of various distillation methods on multiple model architectures and benchmarks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11602v1": {
    "title": "Deconstructing Attention: Investigating Design Principles for Effective Language Modeling",
    "url": "https://www.alphaxiv.org/abs/2510.11602v1",
    "arxiv_id": "2510.11602v1",
    "authors": "Huiyin Xue, Nafise Sadat Moosavi, Nikolaos Aletras",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-13 16:42:14",
    "ori_summary": "The success of Transformer language models is widely credited to their dot-product attention mechanism, which interweaves a set of key design principles: mixing information across positions (enabling multi-token interactions), sequence-dependent activations (where attention weights adapt to each input), a specific mathematical form (dot-product similarities plus softmax weighting), and coupling of queries and keys to evolving hidden states (grounding attention in the current layer). However, the necessity of each of these principles remains largely untested. In this work, we systematically deconstruct attention by designing controlled variants that selectively relax these principles, applied both uniformly across all layers and in hybrid architectures where only some layers retain standard attention. Our empirical analysis reveals that mechanisms for mixing tokens are indispensable, as their absence collapses models to near-random behavior, while the exact mathematical form and sequence dependency can be substantially relaxed, especially when preserved in just a subset of layers. Surprisingly, even variants that fail in isolation can achieve robust performance when interleaved with standard attention, highlighting a cooperative effect. These findings deepen our understanding of what truly underpins attention's effectiveness and open new avenues for simplifying language models without sacrificing performance.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11598v1": {
    "title": "MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11598v1",
    "arxiv_id": "2510.11598v1",
    "authors": "Bo Cheng, Xu Wang, Jinda Liu, Yi Chang, Yuan Wu",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 16:37:40",
    "ori_summary": "Low-Rank Adaptation (LoRA) has emerged as one of the most widely used parameter-efficient fine-tuning (PEFT) methods for adapting large language models (LLMs) to downstream tasks. While highly effective in single-task settings, it struggles to efficiently leverage inter-task knowledge in complex multi-task learning scenarios, often requiring substantial task-specific data to achieve optimal performance. To address this limitation, we introduce MeTA-LoRA, a two-stage optimization framework that significantly improves data efficiency in multi-task adaptation. In the first stage, task-specific LoRA adapters are learned using only a few samples from each involved dataset, enabling rapid adaptation without large-scale supervision. In the second stage, the shared LoRA adapter is updated by aggregating gradients from multiple tasks to promote knowledge transfer across tasks, further reducing data usage by leveraging common patterns. In both multi-task learning and multilingual learning scenarios, our method matches or surpasses the performance of traditional full-data LoRA fine-tuning approaches, while using significantly less task-specific data.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11586v1": {
    "title": "Survey Response Generation: Generating Closed-Ended Survey Responses In-Silico with Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11586v1",
    "arxiv_id": "2510.11586v1",
    "authors": "Georg Ahnert, Anna-Carolina Haensch, Barbara Plank, Markus Strohmaier",
    "categories": "cs.CL, cs.CY",
    "pub_date": "2025-10-13 16:29:19",
    "ori_summary": "Many in-silico simulations of human survey responses with large language models (LLMs) focus on generating closed-ended survey responses, whereas LLMs are typically trained to generate open-ended text instead. Previous research has used a diverse range of methods for generating closed-ended survey responses with LLMs, and a standard practice remains to be identified. In this paper, we systematically investigate the impact that various Survey Response Generation Methods have on predicted survey responses. We present the results of 32 mio. simulated survey responses across 8 Survey Response Generation Methods, 4 political attitude surveys, and 10 open-weight language models. We find significant differences between the Survey Response Generation Methods in both individual-level and subpopulation-level alignment. Our results show that Restricted Generation Methods perform best overall, and that reasoning output does not consistently improve alignment. Our work underlines the significant impact that Survey Response Generation Methods have on simulated survey responses, and we develop practical recommendations on the application of Survey Response Generation Methods.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11584v1": {
    "title": "LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings",
    "url": "https://www.alphaxiv.org/abs/2510.11584v1",
    "arxiv_id": "2510.11584v1",
    "authors": "Ting Li, Yang Yang, Yipeng Yu, Liang Yao, Guoqing Chao, Ruifeng Xu",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 16:29:17",
    "ori_summary": "Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the model's ability of link prediction by removing or inserting triples. A recent black-box method has attempted to incorporate textual and structural information to enhance attack performance. However, it is unable to generate human-readable explanations, and exhibits poor generalizability. In the past few years, large language models (LLMs) have demonstrated powerful capabilities in text comprehension, generation, and reasoning. In this paper, we propose LLMAtKGE, a novel LLM-based framework that selects attack targets and generates human-readable explanations. To provide the LLM with sufficient factual context under limited input constraints, we design a structured prompting scheme that explicitly formulates the attack as multiple-choice questions while incorporating KG factual evidence. To address the context-window limitation and hesitation issues, we introduce semantics-based and centrality-based filters, which compress the candidate set while preserving high recall of attack-relevant information. Furthermore, to efficiently integrate both semantic and structural information into the filter, we precompute high-order adjacency and fine-tune the LLM with a triple classification task to enhance filtering performance. Experiments on two widely used knowledge graph datasets demonstrate that our attack outperforms the strongest black-box baselines and provides explanations via reasoning, and showing competitive performance compared with white-box methods. Comprehensive ablation and case studies further validate its capability to generate explanations.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11570v1": {
    "title": "Bag of Tricks for Subverting Reasoning-based Safety Guardrails",
    "url": "https://www.alphaxiv.org/abs/2510.11570v1",
    "arxiv_id": "2510.11570v1",
    "authors": "Shuo Chen, Zhen Han, Haokun Chen, Bailan He, Shengyun Si, Jingpei Wu, Philip Torr, Volker Tresp, Jindong Gu",
    "categories": "cs.CR, cs.CL",
    "pub_date": "2025-10-13 16:16:44",
    "ori_summary": "Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs), such as deliberative alignment, have shown strong defense against jailbreak attacks. By leveraging LRMs' reasoning ability, these guardrails help the models to assess the safety of user inputs before generating final responses. The powerful reasoning ability can analyze the intention of the input query and will refuse to assist once it detects the harmful intent hidden by the jailbreak methods. Such guardrails have shown a significant boost in defense, such as the near-perfect refusal rates on the open-source gpt-oss series. Unfortunately, we find that these powerful reasoning-based guardrails can be extremely vulnerable to subtle manipulation of the input prompts, and once hijacked, can lead to even more harmful results. Specifically, we first uncover a surprisingly fragile aspect of these guardrails: simply adding a few template tokens to the input prompt can successfully bypass the seemingly powerful guardrails and lead to explicit and harmful responses. To explore further, we introduce a bag of jailbreak methods that subvert the reasoning-based guardrails. Our attacks span white-, gray-, and black-box settings and range from effortless template manipulations to fully automated optimization. Along with the potential for scalable implementation, these methods also achieve alarmingly high attack success rates (e.g., exceeding 90% across 5 different benchmarks on gpt-oss series on both local host models and online API services). Evaluations across various leading open-source LRMs confirm that these vulnerabilities are systemic, underscoring the urgent need for stronger alignment techniques for open-sourced LRMs to prevent malicious misuse. Code is open-sourced at https://chenxshuo.github.io/bag-of-tricks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11563v1": {
    "title": "Culturally-Aware Conversations: A Framework & Benchmark for LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.11563v1",
    "arxiv_id": "2510.11563v1",
    "authors": "Shreya Havaldar, Sunny Rai, Young-Min Cho, Lyle Ungar",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 16:06:14",
    "ori_summary": "Existing benchmarks that measure cultural adaptation in LLMs are misaligned with the actual challenges these models face when interacting with users from diverse cultural backgrounds. In this work, we introduce the first framework and benchmark designed to evaluate LLMs in realistic, multicultural conversational settings. Grounded in sociocultural theory, our framework formalizes how linguistic style - a key element of cultural communication - is shaped by situational, relational, and cultural context. We construct a benchmark dataset based on this framework, annotated by culturally diverse raters, and propose a new set of desiderata for cross-cultural evaluation in NLP: conversational framing, stylistic sensitivity, and subjective correctness. We evaluate today's top LLMs on our benchmark and show that these models struggle with cultural adaptation in a conversational setting.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11557v1": {
    "title": "Invisible Languages of the LLM Universe",
    "url": "https://www.alphaxiv.org/abs/2510.11557v1",
    "arxiv_id": "2510.11557v1",
    "authors": "Saurabh Khanna, Xinxu Li",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 16:00:15",
    "ori_summary": "Large Language Models are trained on massive multilingual corpora, yet this abundance masks a profound crisis: of the world's 7,613 living languages, approximately 2,000 languages with millions of speakers remain effectively invisible in digital ecosystems. We propose a critical framework connecting empirical measurements of language vitality (real world demographic strength) and digitality (online presence) with postcolonial theory and epistemic injustice to explain why linguistic inequality in AI systems is not incidental but structural. Analyzing data across all documented human languages, we identify four categories: Strongholds (33%, high vitality and digitality), Digital Echoes (6%, high digitality despite declining vitality), Fading Voices (36%, low on both dimensions), and critically, Invisible Giants (27%, high vitality but near-zero digitality) - languages spoken by millions yet absent from the LLM universe. We demonstrate that these patterns reflect continuities from colonial-era linguistic hierarchies to contemporary AI development, constituting what we term digital epistemic injustice. Our analysis reveals that English dominance in AI is not a technical necessity but an artifact of power structures that systematically exclude marginalized linguistic knowledge. We conclude with implications for decolonizing language technology and democratizing access to AI benefits.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11545v1": {
    "title": "Information-Preserving Reformulation of Reasoning Traces for Antidistillation",
    "url": "https://www.alphaxiv.org/abs/2510.11545v1",
    "arxiv_id": "2510.11545v1",
    "authors": "Jiayu Ding, Lei Cui, Li Dong, Nanning Zheng, Furu Wei",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 15:42:11",
    "ori_summary": "Recent advances in Large Language Models (LLMs) show that extending the length of reasoning chains significantly improves performance on complex tasks. While revealing these reasoning traces helps users better follow, verify, and learn from the model's problem-solving process, it also makes them highly vulnerable to unauthorized distillation. To mitigate this risk, proprietary model providers often adopt aggressive protection strategies, such as replacing detailed reasoning with brief summaries, which deprive users of valuable intermediate information. To address this trade-off, we propose PART, an information-preserving antidistillation reformulation of reasoning traces. Motivated by the difference between how humans understand reasoning traces and how LLMs exploit them for supervised fine-tuning, we design a simple but effective two-step reformulation: removing self-talk behaviors and reordering sub-conclusions. A small auxiliary model is trained to perform this reformulation, incurring minimal computational overhead. Extensive experiments demonstrate that PART consistently disrupts distillation across student models of different sizes and types on various reasoning benchmarks. For instance, when training on reformulated traces, even the performance of a large 32B student model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a 13.5% degradation.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11537v1": {
    "title": "An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese Token-Level Classification",
    "url": "https://www.alphaxiv.org/abs/2510.11537v1",
    "arxiv_id": "2510.11537v1",
    "authors": "Ba-Quang Nguyen",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 15:39:09",
    "ori_summary": "We propose a novel neural architecture named TextGraphFuseGAT, which integrates a pretrained transformer encoder (PhoBERT) with Graph Attention Networks for token-level classification tasks. The proposed model constructs a fully connected graph over the token embeddings produced by PhoBERT, enabling the GAT layer to capture rich inter-token dependencies beyond those modeled by sequential context alone. To further enhance contextualization, a Transformer-style self-attention layer is applied on top of the graph-enhanced embeddings. The final token representations are passed through a classification head to perform sequence labeling. We evaluate our approach on three Vietnamese benchmark datasets: PhoNER-COVID19 for named entity recognition in the COVID-19 domain, PhoDisfluency for speech disfluency detection, and VietMed-NER for medical-domain NER. VietMed-NER is the first Vietnamese medical spoken NER dataset, featuring 18 entity types collected from real-world medical speech transcripts and annotated with the BIO tagging scheme. Its specialized vocabulary and domain-specific expressions make it a challenging benchmark for token-level classification models. Experimental results show that our method consistently outperforms strong baselines, including transformer-only and hybrid neural models such as BiLSTM + CNN + CRF, confirming the effectiveness of combining pretrained semantic features with graph-based relational modeling for improved token classification across multiple domains.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11529v1": {
    "title": "Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11529v1",
    "arxiv_id": "2510.11529v1",
    "authors": "Yusheng Song, Lirong Qiu, Xi Zhang, Zhihao Tang",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 15:31:21",
    "ori_summary": "The detection of sophisticated hallucinations in Large Language Models (LLMs) is hampered by a ``Detection Dilemma'': methods probing internal states (Internal State Probing) excel at identifying factual inconsistencies but fail on logical fallacies, while those verifying externalized reasoning (Chain-of-Thought Verification) show the opposite behavior. This schism creates a task-dependent blind spot: Chain-of-Thought Verification fails on fact-intensive tasks like open-domain QA where reasoning is ungrounded, while Internal State Probing is ineffective on logic-intensive tasks like mathematical reasoning where models are confidently wrong. We resolve this with a unified framework that bridges this critical gap. However, unification is hindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse symbolic reasoning chains lack signals directly comparable to fine-grained internal states, and the Representational Alignment Barrier, a deep-seated mismatch between their underlying semantic spaces. To overcome these, we introduce a multi-path reasoning mechanism to obtain more comparable, fine-grained signals, and a segment-aware temporalized cross-attention module to adaptively fuse these now-aligned representations, pinpointing subtle dissonances. Extensive experiments on three diverse benchmarks and two leading LLMs demonstrate that our framework consistently and significantly outperforms strong baselines. Our code is available: https://github.com/peach918/HalluDet.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11498v1": {
    "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding",
    "url": "https://www.alphaxiv.org/abs/2510.11498v1",
    "arxiv_id": "2510.11498v1",
    "authors": "Yuhang Li, Chenchen Zhang, Ruilin Lv, Ao Liu, Ken Deng, Yuanxing Zhang, Jiaheng Liu, Wiggin Zhou, Bo Zhou",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-13 15:05:50",
    "ori_summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they struggle with front-end development, where correctness is judged on rendered pixels and interaction. We present ReLook, an agentic, vision-grounded reinforcement learning framework that empowers an agent to close a robust generate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool. During training, the agent uses the MLLM-in-the-loop both as a visual critic--scoring code with screenshots--and as a source of actionable, vision-grounded feedback; a strict zero-reward rule for invalid renders anchors renderability and prevents reward hacking. To prevent behavioral collapse, we introduce Forced Optimization, a strict acceptance rule that admits only improving revisions, yielding monotonically better trajectories. At inference, we decouple the critic and run a lightweight, critic-free self-edit cycle, keeping latency comparable to base decoding while retaining most of the gains. Across three widely used benchmarks, ReLook consistently outperforms strong baselines in vision-grounded front-end code generation, highlighting the benefits of agentic perception, visual rewards, and training-inference decoupling.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11482v1": {
    "title": "Investigating Large Language Models' Linguistic Abilities for Text Preprocessing",
    "url": "https://www.alphaxiv.org/abs/2510.11482v1",
    "arxiv_id": "2510.11482v1",
    "authors": "Marco Braga, Gian Carlo Milanese, Gabriella Pasi",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 14:53:44",
    "ori_summary": "Text preprocessing is a fundamental component of Natural Language Processing, involving techniques such as stopword removal, stemming, and lemmatization to prepare text as input for further processing and analysis. Despite the context-dependent nature of the above techniques, traditional methods usually ignore contextual information. In this paper, we investigate the idea of using Large Language Models (LLMs) to perform various preprocessing tasks, due to their ability to take context into account without requiring extensive language-specific annotated resources. Through a comprehensive evaluation on web-sourced data, we compare LLM-based preprocessing (specifically stopword removal, lemmatization and stemming) to traditional algorithms across multiple text classification tasks in six European languages. Our analysis indicates that LLMs are capable of replicating traditional stopword removal, lemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%, respectively. Additionally, we show that ML algorithms trained on texts preprocessed by LLMs achieve an improvement of up to 6% with respect to the $F_1$ measure compared to traditional techniques. Our code, prompts, and results are publicly available at https://github.com/GianCarloMilanese/llm_pipeline_wi-iat.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11444v1": {
    "title": "GenCNER: A Generative Framework for Continual Named Entity Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.11444v1",
    "arxiv_id": "2510.11444v1",
    "authors": "Yawen Yang, Fukun Ma, Shiao Meng, Aiwei Liu, Lijie Wen",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 14:15:31",
    "ori_summary": "Traditional named entity recognition (NER) aims to identify text mentions into pre-defined entity types. Continual Named Entity Recognition (CNER) is introduced since entity categories are continuously increasing in various real-world scenarios. However, existing continual learning (CL) methods for NER face challenges of catastrophic forgetting and semantic shift of non-entity type. In this paper, we propose GenCNER, a simple but effective Generative framework for CNER to mitigate the above drawbacks. Specifically, we skillfully convert the CNER task into sustained entity triplet sequence generation problem and utilize a powerful pre-trained seq2seq model to solve it. Additionally, we design a type-specific confidence-based pseudo labeling strategy along with knowledge distillation (KD) to preserve learned knowledge and alleviate the impact of label noise at the triplet level. Experimental results on two benchmark datasets show that our framework outperforms previous state-of-the-art methods in multiple CNER settings, and achieves the smallest gap compared with non-CL results.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11434v1": {
    "title": "Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated Content",
    "url": "https://www.alphaxiv.org/abs/2510.11434v1",
    "arxiv_id": "2510.11434v1",
    "authors": "Dana Sotto Porat, Ella Rabinovich",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 14:06:17",
    "ori_summary": "Generative large language models (LLMs) have become central to everyday life, producing human-like text across diverse domains. A growing body of research investigates whether these models also exhibit personality- and demographic-like characteristics in their language. In this work, we introduce a novel, data-driven methodology for assessing LLM personality without relying on self-report questionnaires, applying instead automatic personality and gender classifiers to model replies on open-ended questions collected from Reddit. Comparing six widely used models to human-authored responses, we find that LLMs systematically express higher Agreeableness and lower Neuroticism, reflecting cooperative and stable conversational tendencies. Gendered language patterns in model text broadly resemble those of human writers, though with reduced variation, echoing prior findings on automated agents. We contribute a new dataset of human and model responses, along with large-scale comparative analyses, shedding new light on the topic of personality and demographic patterns of generative AI.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11423v1": {
    "title": "Beyond the Crowd: LLM-Augmented Community Notes for Governing Health Misinformation",
    "url": "https://www.alphaxiv.org/abs/2510.11423v1",
    "arxiv_id": "2510.11423v1",
    "authors": "Jiaying Wu, Zihang Fu, Haonan Wang, Fanxiao Li, Min-Yen Kan",
    "categories": "cs.SI, cs.CL",
    "pub_date": "2025-10-13 13:57:23",
    "ori_summary": "Community Notes, the crowd-sourced misinformation governance system on X (formerly Twitter), enables users to flag misleading posts, attach contextual notes, and vote on their helpfulness. However, our analysis of 30.8K health-related notes reveals significant latency, with a median delay of 17.6 hours before the first note receives a helpfulness status. To improve responsiveness during real-world misinformation surges, we propose CrowdNotes+, a unified framework that leverages large language models (LLMs) to augment Community Notes for faster and more reliable health misinformation governance. CrowdNotes+ integrates two complementary modes: (1) evidence-grounded note augmentation and (2) utility-guided note automation, along with a hierarchical three-step evaluation that progressively assesses relevance, correctness, and helpfulness. We instantiate the framework through HealthNotes, a benchmark of 1.2K helpfulness-annotated health notes paired with a fine-tuned helpfulness judge. Experiments on fifteen LLMs reveal an overlooked loophole in current helpfulness evaluation, where stylistic fluency is mistaken for factual accuracy, and demonstrate that our hierarchical evaluation and LLM-augmented generation jointly enhance factual precision and evidence utility. These results point toward a hybrid human-AI governance model that improves both the rigor and timeliness of crowd-sourced fact-checking.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11408v1": {
    "title": "Valid Survey Simulations with Limited Human Data: The Roles of Prompting, Fine-Tuning, and Rectification",
    "url": "https://www.alphaxiv.org/abs/2510.11408v1",
    "arxiv_id": "2510.11408v1",
    "authors": "Stefan Krsteski, Giuseppe Russo, Serina Chang, Robert West, Kristina Gligorić",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 13:48:07",
    "ori_summary": "Surveys provide valuable insights into public opinion and behavior, but their execution is costly and slow. Large language models (LLMs) have been proposed as a scalable, low-cost substitute for human respondents, but their outputs are often biased and yield invalid estimates. We study the interplay between synthesis methods that use LLMs to generate survey responses and rectification methods that debias population estimates, and explore how human responses are best allocated between them. Using two panel surveys with questions on nutrition, politics, and economics, we find that synthesis alone introduces substantial bias (24-86%), whereas combining it with rectification reduces bias below 5% and increases effective sample size by up to 14%. Overall, we challenge the common practice of using all human responses for fine-tuning, showing that under a fixed budget, allocating most to rectification results in far more effective estimation.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11407v1": {
    "title": "KnowRL: Teaching Language Models to Know What They Know",
    "url": "https://www.alphaxiv.org/abs/2510.11407v1",
    "arxiv_id": "2510.11407v1",
    "authors": "Sahil Kale, Devendra Singh Dhami",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 13:47:14",
    "ori_summary": "Truly reliable AI requires more than simply scaling up knowledge; it demands the ability to know what it knows and when it does not. Yet recent research shows that even the best LLMs misjudge their own competence in more than one in five cases, making any response born of such internal uncertainty impossible to fully trust. Inspired by self-improvement reinforcement learning techniques that require minimal data, we present a simple but powerful framework KnowRL that strengthens a model's internal understanding of its own feasibility boundaries, enabling safer and more responsible behaviour. Our framework combines two components: (i) introspection, where the model generates and classifies tasks it judges feasible or infeasible, and (ii) consensus-based rewarding, where stability of self-knowledge assessment is reinforced through internal agreement. By using internally generated data, this design strengthens consistency in self-knowledge and entirely avoids costly external supervision. In experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved self-knowledge, validated by both intrinsic self-consistency and extrinsic benchmarking. With nothing more than a small seed set and no external supervision, our method drove gains as high as 28% in accuracy and 12% in F1, outperforming baselines in just a few iterations. Our framework essentially unlocks the untapped capacity of LLMs to self-improve their knowledge awareness, opening the door to reliable, more accountable AI and safer deployment in critical applications. Owing to its simplicity and independence from external effort, we encourage applying this reliability-enhancing process to all future models.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11391v1": {
    "title": "DocReward: A Document Reward Model for Structuring and Stylizing",
    "url": "https://www.alphaxiv.org/abs/2510.11391v1",
    "arxiv_id": "2510.11391v1",
    "authors": "Junpeng Liu, Yuzhong Zhao, Bowen Cao, Jiayu Ding, Yilin Jia, Tengchao Lv, Yupan Huang, Shaohan Huang, Nan Yang, Li Dong, Lei Cui, Tao Ge, Xun Wang, Huitian Jiao, Sun Mao, FNU Kartik, Si-Qing Chen, Wai Lam, Furu Wei",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-10-13 13:36:32",
    "ori_summary": "Recent advances in agentic workflows have enabled the automation of tasks such as professional document generation. However, they primarily focus on textual quality, neglecting visual structure and style, which are crucial for readability and engagement. This gap arises mainly from the absence of suitable reward models to guide agentic workflows toward producing documents with stronger structural and stylistic quality. To address this, we propose DocReward, a document reward model that evaluates documents based on their structure and style. We construct a multi-domain dataset DocPair of 117K paired documents, covering 32 domains and 267 document types, each including a high- and low-professionalism document with identical content but different structure and style. This enables the model to evaluate professionalism comprehensively, and in a textual-quality-agnostic way. DocReward is trained using the Bradley-Terry loss to score documents, penalizing predictions that contradict the annotated ranking. To assess the performance of reward models, we create a test dataset containing document bundles ranked by well-educated human evaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6 and 19.4 percentage points, respectively, demonstrating its superiority over baselines. In an extrinsic evaluation of document generation, DocReward achieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7% win rate, demonstrating its utility in guiding generation agents toward producing human-preferred documents.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11389v1": {
    "title": "Beyond Survival: Evaluating LLMs in Social Deduction Games with Human-Aligned Strategies",
    "url": "https://www.alphaxiv.org/abs/2510.11389v1",
    "arxiv_id": "2510.11389v1",
    "authors": "Zirui Song, Yuan Huang, Junchang Liu, Haozhe Luo, Chenxi Wang, Lang Gao, Zixiang Xu, Mingfei Han, Xiaojun Chang, Xiuying Chen",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 13:33:30",
    "ori_summary": "Social deduction games like Werewolf combine language, reasoning, and strategy, providing a testbed for studying natural language and social intelligence. However, most studies reduce the game to LLM-based self-play, yielding templated utterances and anecdotal cases that overlook the richness of social gameplay. Evaluation further relies on coarse metrics such as survival time or subjective scoring due to the lack of quality reference data. To address these gaps, we curate a high-quality, human-verified multimodal Werewolf dataset containing over 100 hours of video, 32.4M utterance tokens, and 15 rule variants. Based on this dataset, we propose a novel strategy-alignment evaluation that leverages the winning faction's strategies as ground truth in two stages: 1) Speech evaluation, formulated as multiple-choice-style tasks that assess whether the model can adopt appropriate stances across five dimensions of social ability; and 2) Decision evaluation, which assesses the model's voting choices and opponent-role inferences. This framework enables a fine-grained evaluation of models' linguistic and reasoning capabilities, while capturing their ability to generate strategically coherent gameplay. Our experiments show that state-of-the-art LLMs show diverse performance, with roughly half remain below 0.50, revealing clear gaps in deception and counterfactual reasoning. We hope our dataset further inspires research on language, reasoning, and strategy in multi-agent interaction.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11372v1": {
    "title": "Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction Tuning",
    "url": "https://www.alphaxiv.org/abs/2510.11372v1",
    "arxiv_id": "2510.11372v1",
    "authors": "Dean L. Slack, Noura Al Moubayed",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 13:12:46",
    "ori_summary": "Although large language models excel across many tasks, they can memorise training data and thereby expose private or copyrighted text. Most defences target the pre-training stage, leaving memorisation during fine-tuning, especially for domain adaptation and instruction tuning, poorly understood. We fine-tune Pythia, Llama3, and Mistral models spanning 1.4B-70B parameters on common evaluation datasets and track verbatim memorisation throughout training. We find that memorisation increases dramatically in the first few epochs, often significantly before either validation perplexity or evaluation performance is optimised. We use a simple but effective n-gram memorisation score which reliably precedes verbatim memorisation; using it as an early-stopping criterion mitigates memorisation with minimal performance loss. Further, we introduce an n-gram-aware loss regulariser and show that it reduces memorisation across all model families tested by up to 40% while minimising evaluation performance trade-offs when compared to an existing memorisation mitigation strategy. These results yield practical, scalable insights into memorisation dynamics during language model fine-tuning.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11370v1": {
    "title": "Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers",
    "url": "https://www.alphaxiv.org/abs/2510.11370v1",
    "arxiv_id": "2510.11370v1",
    "authors": "Wenhan Ma, Hailin Zhang, Liang Zhao, Yifan Song, Yudong Wang, Zhifang Sui, Fuli Luo",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-13 13:11:27",
    "ori_summary": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing the capabilities of large language models. However, in Mixture-of-Experts (MoE) models, the routing mechanism often introduces instability, even leading to catastrophic RL training collapse. We analyze the training-inference consistency of MoE models and identify a notable discrepancy in routing behaviors between the two phases. Moreover, even under identical conditions, the routing framework can yield divergent expert selections across repeated forward passes. To address this foundational inconsistency, we propose Rollout Routing Replay (R3), a method that records routing distributions from the inference engine and replays them during training. R3 significantly reduces training-inference policy KL divergence and mitigates extreme discrepancies without compromising training speed. Extensive experiments on various settings confirm that R3 succeeds in stabilizing RL training, preventing collapse and outperforming methods such as GSPO and TIS. We believe this work can offer a new solution for stabilizing RL in MoE models.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11330v1": {
    "title": "Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap",
    "url": "https://www.alphaxiv.org/abs/2510.11330v1",
    "arxiv_id": "2510.11330v1",
    "authors": "KiHyun Nam, Jongmin Choi, Hyeongkeun Lee, Jungwoo Heo, Joon Son Chung",
    "categories": "cs.SD, cs.AI, cs.CL, cs.LG, eess.AS",
    "pub_date": "2025-10-13 12:25:33",
    "ori_summary": "Contrastive audio-language pretraining yields powerful joint representations, yet a persistent audio-text modality gap limits the benefits of coupling multimodal encoders with large language models (LLMs). We present Diffusion-Link, a diffusion-based modality-bridging module that generatively maps audio embeddings into the text-embedding distribution. The module is trained at the output embedding from the frozen multimodal encoder and implemented as a lightweight network with three residual MLP blocks. To assess the effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on Automatic Audio Captioning (AAC); to our knowledge, this is the first application of diffusion-based modality bridging to AAC. We report two results. (1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link reduces the modality gap the most among prior diffusion-based methods and shows a collective migration of audio embeddings toward the text distribution. (2) Downstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline achieves state-of-the-art on AudioCaps in both zero-shot and fully supervised captioning without external knowledge, with relative gains up to 52.5% and 7.5%, respectively. These findings show that closing the modality gap is pivotal for effective coupling between multimodal encoders and LLMs, and diffusion-based modality bridging offers a promising direction beyond knowledge-retrieval-centric designs. Code will be released upon acceptance https://github.com/DevKiHyun/Diffusion-Link",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11328v1": {
    "title": "Do LLMs \"Feel\"? Emotion Circuits Discovery and Control",
    "url": "https://www.alphaxiv.org/abs/2510.11328v1",
    "arxiv_id": "2510.11328v1",
    "authors": "Chenxi Wang, Yixuan Zhang, Ruiji Yu, Yufei Zheng, Lang Gao, Zirui Song, Zixiang Xu, Gus Xia, Huishuai Zhang, Dongyan Zhao, Xiuying Chen",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 12:24:24",
    "ori_summary": "As the demand for emotional intelligence in large language models (LLMs) grows, a key challenge lies in understanding the internal mechanisms that give rise to emotional expression and in controlling emotions in generated text. This study addresses three core questions: (1) Do LLMs contain context-agnostic mechanisms shaping emotional expression? (2) What form do these mechanisms take? (3) Can they be harnessed for universal emotion control? We first construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit comparable internal states across emotions. Subsequently, we extract context-agnostic emotion directions that reveal consistent, cross-context encoding of emotion (Q1). We identify neurons and attention heads that locally implement emotional computation through analytical decomposition and causal analysis, and validate their causal roles via ablation and enhancement interventions. Next, we quantify each sublayer's causal influence on the model's final emotion representation and integrate the identified local components into coherent global emotion circuits that drive emotional expression (Q2). Directly modulating these circuits achieves 99.65% emotion-expression accuracy on the test set, surpassing prompting- and steering-based methods (Q3). To our knowledge, this is the first systematic study to uncover and validate emotion circuits in LLMs, offering new insights into interpretability and controllable emotional intelligence.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11314v1": {
    "title": "Template-Based Text-to-Image Alignment for Language Accessibility: A Study on Visualizing Text Simplifications",
    "url": "https://www.alphaxiv.org/abs/2510.11314v1",
    "arxiv_id": "2510.11314v1",
    "authors": "Belkiss Souayed, Sarah Ebling, Yingqiang Gao",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 12:03:36",
    "ori_summary": "Individuals with intellectual disabilities often have difficulties in comprehending complex texts. While many text-to-image models prioritize aesthetics over accessibility, it is not clear how visual illustrations relate to text simplifications (TS) generated from them. This paper presents a structured vision-language model (VLM) prompting framework for generating accessible images from simplified texts. We designed five prompt templates, i.e., Basic Object Focus, Contextual Scene, Educational Layout, Multi-Level Detail, and Grid Layout, each following distinct spatial arrangements while adhering to accessibility constraints such as object count limits, spatial separation, and content restrictions. Using 400 sentence-level simplifications from four established TS datasets (OneStopEnglish, SimPA, Wikipedia, and ASSET), we conducted a two-phase evaluation: Phase 1 assessed prompt template effectiveness with CLIPScores, and Phase 2 involved human annotation of generated images across ten visual styles by four accessibility experts. Results show that the Basic Object Focus prompt template achieved the highest semantic alignment, indicating that visual minimalism enhances language accessibility. Expert evaluation further identified Retro style as the most accessible and Wikipedia as the most effective data source. Inter-annotator agreement varied across dimensions, with Text Simplicity showing strong reliability and Image Quality proving more subjective. Overall, our framework offers practical guidelines for accessible content generation and underscores the importance of structured prompting in AI-generated visual accessibility tools.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11307v1": {
    "title": "FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks",
    "url": "https://www.alphaxiv.org/abs/2510.11307v1",
    "arxiv_id": "2510.11307v1",
    "authors": "Sabrina McCallum, Amit Parekh, Alessandro Suglia",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 11:55:21",
    "ori_summary": "Current approaches to embodied AI tend to learn policies from expert demonstrations. However, without a mechanism to evaluate the quality of demonstrated actions, they are limited to learning from optimal behaviour, or they risk replicating errors and inefficiencies. While reinforcement learning offers one alternative, the associated exploration typically results in sacrificing data efficiency. This work explores how agents trained with imitation learning can learn robust representations from both optimal and suboptimal demonstrations when given access to constructive language feedback as a means to contextualise different modes of behaviour. We directly provide language feedback embeddings as part of the input sequence into a Transformer-based policy, and optionally complement the traditional next action prediction objective with auxiliary self-supervised learning objectives for feedback prediction. We test our approach on a range of embodied Vision-and-Language tasks in our custom BabyAI-XGen environment and show significant improvements in agents' compositional generalisation abilities and robustness, suggesting that our data-efficient method allows models to successfully convert suboptimal behaviour into learning opportunities. Overall, our results suggest that language feedback is a competitive and intuitive alternative to intermediate scalar rewards for language-specified embodied tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11297v1": {
    "title": "Are Large Language Models Effective Knowledge Graph Constructors?",
    "url": "https://www.alphaxiv.org/abs/2510.11297v1",
    "arxiv_id": "2510.11297v1",
    "authors": "Ruirui Chen, Weifeng Jiang, Chengwei Qin, Bo Xiong, Fiona Liausvia, Dongkyu Choi, Boon Kiat Quek",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 11:37:48",
    "ori_summary": "Knowledge graphs (KGs) are vital for knowledge-intensive tasks and have shown promise in reducing hallucinations in large language models (LLMs). However, constructing high-quality KGs remains difficult, requiring accurate information extraction and structured representations that support interpretability and downstream utility. Existing LLM-based approaches often focus narrowly on entity and relation extraction, limiting coverage to sentence-level contexts or relying on predefined schemas. We propose a hierarchical extraction framework that organizes information at multiple levels, enabling the creation of semantically rich and well-structured KGs. Using state-of-the-art LLMs, we extract and construct knowledge graphs and evaluate them comprehensively from both structural and semantic perspectives. Our results highlight the strengths and shortcomings of current LLMs in KG construction and identify key challenges for future work. To advance research in this area, we also release a curated dataset of LLM-generated KGs derived from research papers on children's mental well-being. This resource aims to foster more transparent, reliable, and impactful applications in high-stakes domains such as healthcare.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11288v1": {
    "title": "Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.11288v1",
    "arxiv_id": "2510.11288v1",
    "authors": "Nikita Afonin, Nikita Andriyanov, Nikhil Bageshpura, Kyle Liu, Kevin Zhu, Sunishchal Dev, Ashwinee Panda, Alexander Panchenko, Oleg Rogov, Elena Tutubalina, Mikhail Seleznyov",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 11:23:56",
    "ori_summary": "Recent work has shown that narrow finetuning can produce broadly misaligned LLMs, a phenomenon termed emergent misalignment (EM). While concerning, these findings were limited to finetuning and activation steering, leaving out in-context learning (ICL). We therefore ask: does EM emerge in ICL? We find that it does: across three datasets, three frontier models produce broadly misaligned responses at rates between 2% and 17% given 64 narrow in-context examples, and up to 58% with 256 examples. We also examine mechanisms of EM by eliciting step-by-step reasoning (while leaving in-context examples unchanged). Manual analysis of the resulting chain-of-thought shows that 67.5% of misaligned traces explicitly rationalize harmful outputs by adopting a reckless or dangerous ''persona'', echoing prior results on finetuning-induced EM.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11278v1": {
    "title": "ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11278v1",
    "arxiv_id": "2510.11278v1",
    "authors": "Gareth Seneque, Lap-Hang Ho, Nafise Erfanian Saeedi, Jeffrey Molendijk, Ariel Kupermann, Tim Elson",
    "categories": "cs.LG, cs.AI, cs.CL, 68T50, I.2.7",
    "pub_date": "2025-10-13 11:13:09",
    "ori_summary": "We present Entropic Mutual-Information Geometry Large-Language Model Alignment (ENIGMA), a novel approach to Large-Language Model (LLM) training that jointly improves reasoning, alignment and robustness by treating an organisation's policies/principles as directions to move on a model's information manifold. Our single-loop trainer combines Group-Relative Policy Optimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought (CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information (SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn optimal-transport regulariser on hidden-state distributions to bound geometry drift. We also introduce infoNCE metrics that specialise to a standard MI lower bound under matched negatives to measure how strongly a model's CoT encodes these policies. These metrics include a Sufficiency Index (SI) that enables the selection and creation of principles that maximise downstream performance prior to training. In our experiments using small (1B) LLMs, high-SI principles predict steadier training dynamics and improved benchmark performance over GRPO ablations. Our information-geometry analysis of trained models validates desirable structural change in the manifold. These results support our hypothesis that reasoning, alignment, and robustness are projections of a single informationgeometric objective, and that models trained using ENIGMA demonstrate principled reasoning without the use of a reward model, offering a path to trusted capability",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11277v1": {
    "title": "Towards Real-Time Fake News Detection under Evidence Scarcity",
    "url": "https://www.alphaxiv.org/abs/2510.11277v1",
    "arxiv_id": "2510.11277v1",
    "authors": "Guangyu Wei, Ke Han, Yueming Lyu, Yu Luo, Yue Jiang, Caifeng Shan, Nicu Sebe",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 11:11:46",
    "ori_summary": "Fake news detection becomes particularly challenging in real-time scenarios, where emerging events often lack sufficient supporting evidence. Existing approaches often rely heavily on external evidence and therefore struggle to generalize under evidence scarcity. To address this issue, we propose Evaluation-Aware Selection of Experts (EASE), a novel framework for real-time fake news detection that dynamically adapts its decision-making process according to the assessed sufficiency of available evidence. EASE introduces a sequential evaluation mechanism comprising three independent perspectives: (1) Evidence-based evaluation, which assesses evidence and incorporates it into decision-making only when the evidence is sufficiently supportive; (2) Reasoning-based evaluation, which leverages the world knowledge of large language models (LLMs) and applies them only when their reliability is adequately established; and (3) Sentiment-based fallback, which integrates sentiment cues when neither evidence nor reasoning is reliable. To enhance the accuracy of evaluation processes, EASE employs instruction tuning with pseudo labels to guide each evaluator in justifying its perspective-specific knowledge through interpretable reasoning. Furthermore, the expert modules integrate the evaluators' justified assessments with the news content to enable evaluation-aware decision-making, thereby enhancing overall detection accuracy. Moreover, we introduce RealTimeNews-25, a new benchmark comprising recent news for evaluating model generalization on emerging news with limited evidence. Extensive experiments demonstrate that EASE not only achieves state-of-the-art performance across multiple benchmarks, but also significantly improves generalization to real-time news. The code and dataset are available: https://github.com/wgyhhhh/EASE.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11254v1": {
    "title": "Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality",
    "url": "https://www.alphaxiv.org/abs/2510.11254v1",
    "arxiv_id": "2510.11254v1",
    "authors": "Jana Jung, Marlene Lutz, Indira Sen, Markus Strohmaier",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 10:43:49",
    "ori_summary": "Psychometric tests are increasingly used to assess psychological constructs in large language models (LLMs). However, it remains unclear whether these tests -- originally developed for humans -- yield meaningful results when applied to LLMs. In this study, we systematically evaluate the reliability and validity of human psychometric tests for three constructs: sexism, racism, and morality. We find moderate reliability across multiple item and prompt variations. Validity is evaluated through both convergent (i.e., testing theory-based inter-test correlations) and ecological approaches (i.e., testing the alignment between tests scores and behavior in real-world downstream tasks). Crucially, we find that psychometric test scores do not align, and in some cases even negatively correlate with, model behavior in downstream tasks, indicating low ecological validity. Our results highlight that systematic evaluations of psychometric tests is essential before interpreting their scores. They also suggest that psychometric tests designed for humans cannot be applied directly to LLMs without adaptation.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11238v1": {
    "title": "Attacks by Content: Automated Fact-checking is an AI Security Issue",
    "url": "https://www.alphaxiv.org/abs/2510.11238v1",
    "arxiv_id": "2510.11238v1",
    "authors": "Michael Schlichtkrull",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 10:18:48",
    "ori_summary": "When AI agents retrieve and reason over external documents, adversaries can manipulate the data they receive to subvert their behaviour. Previous research has studied indirect prompt injection, where the attacker injects malicious instructions. We argue that injection of instructions is not necessary to manipulate agents - attackers could instead supply biased, misleading, or false information. We term this an attack by content. Existing defenses, which focus on detecting hidden commands, are ineffective against attacks by content. To defend themselves and their users, agents must critically evaluate retrieved information, corroborating claims with external evidence and evaluating source trustworthiness. We argue that this is analogous to an existing NLP task, automated fact-checking, which we propose to repurpose as a cognitive self-defense tool for agents.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11236v1": {
    "title": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression",
    "url": "https://www.alphaxiv.org/abs/2510.11236v1",
    "arxiv_id": "2510.11236v1",
    "authors": "Haoqi Yang, Yao Yao, Zuchao Li, Baoyuan Qi, Guoming Liu, Hai Zhao",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 10:17:21",
    "ori_summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks. However, their extensive memory requirements, particularly due to KV cache growth during long-text understanding and generation, present significant challenges for deployment in resource-constrained environments. Quantization has emerged as a promising solution to reduce memory consumption while preserving historical information. We propose XQuant, a training-free and plug-and-play framework that achieves ultra-low equivalent bit-width KV cache quantization. XQuant introduces two key innovations: a computationally negligible data-free calibration method and cross-layer KV cache compression, enabling quantization to sub-1.4 bits. Extensive experiments on TruthfulQA and LongBench demonstrate that XQuant outperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by achieving lower bit-width while maintaining superior performance, establishing a better trade-off between memory efficiency and model accuracy.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11233v1": {
    "title": "CNSocialDepress: A Chinese Social Media Dataset for Depression Risk Detection and Structured Analysis",
    "url": "https://www.alphaxiv.org/abs/2510.11233v1",
    "arxiv_id": "2510.11233v1",
    "authors": "Jinyuan Xu, Tian Lan, Xintao Yu, Xue He, Hezhi Zhang, Ying Wang, Pierre Magistry, Mathieu Valette, Lei Li",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 10:14:18",
    "ori_summary": "Depression is a pressing global public health issue, yet publicly available Chinese-language resources for risk detection remain scarce and are mostly limited to binary classification. To address this limitation, we release CNSocialDepress, a benchmark dataset for depression risk detection from Chinese social media posts. The dataset contains 44,178 texts from 233 users, within which psychological experts annotated 10,306 depression-related segments. CNSocialDepress provides binary risk labels together with structured multi-dimensional psychological attributes, enabling interpretable and fine-grained analysis of depressive signals. Experimental results demonstrate its utility across a wide range of NLP tasks, including structured psychological profiling and fine-tuning of large language models for depression detection. Comprehensive evaluations highlight the dataset's effectiveness and practical value for depression risk identification and psychological analysis, thereby providing insights to mental health applications tailored for Chinese-speaking populations.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11225v1": {
    "title": "A Theorem-Proving-Based Evaluation of Neural Semantic Parsing",
    "url": "https://www.alphaxiv.org/abs/2510.11225v1",
    "arxiv_id": "2510.11225v1",
    "authors": "Hayate Funakura, Hyunsoo Kim, Koji Mineshima",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 10:09:38",
    "ori_summary": "Graph-matching metrics such as Smatch are the de facto standard for evaluating neural semantic parsers, yet they capture surface overlap rather than logical equivalence. We reassess evaluation by pairing graph-matching with automated theorem proving. We compare two approaches to building parsers: supervised fine-tuning (T5-Small/Base) and few-shot in-context learning (GPT-4o/4.1/5), under normalized and unnormalized targets. We evaluate outputs using graph-matching, bidirectional entailment between source and target formulas with a first-order logic theorem prover, and well-formedness. Across settings, we find that models performing well on graph-matching often fail to produce logically equivalent formulas. Normalization reduces incidental target variability, improves well-formedness, and strengthens logical adequacy. Error analysis shows performance degrades with increasing formula complexity and with coordination, prepositional phrases, and passive voice; the dominant failures involve variable binding and indexing, and predicate naming. These findings highlight limits of graph-based metrics for reasoning-oriented applications and motivate logic-sensitive evaluation and training objectives together with simplified, normalized target representations. All code and data for our experiments are publicly available.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11222v1": {
    "title": "Fairness Metric Design Exploration in Multi-Domain Moral Sentiment Classification using Transformer-Based Models",
    "url": "https://www.alphaxiv.org/abs/2510.11222v1",
    "arxiv_id": "2510.11222v1",
    "authors": "Battemuulen Naranbat, Seyed Sahand Mohammadi Ziabari, Yousuf Nasser Al Husaini, Ali Mohammed Mansoor Alsahag",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 10:05:57",
    "ori_summary": "Ensuring fairness in natural language processing for moral sentiment classification is challenging, particularly under cross-domain shifts where transformer models are increasingly deployed. Using the Moral Foundations Twitter Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work evaluates BERT and DistilBERT in a multi-label setting with in-domain and cross-domain protocols. Aggregate performance can mask disparities: we observe pronounced asymmetry in transfer, with Twitter->Reddit degrading micro-F1 by 14.9% versus only 1.5% for Reddit->Twitter. Per-label analysis reveals fairness violations hidden by overall scores; notably, the authority label exhibits Demographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of 0.40-0.41. To address this gap, we introduce the Moral Fairness Consistency (MFC) metric, which quantifies the cross-domain stability of moral foundation detection. MFC shows strong empirical validity, achieving a perfect negative correlation with Demographic Parity Difference (rho = -1.000, p < 0.001) while remaining independent of standard performance metrics. Across labels, loyalty demonstrates the highest consistency (MFC = 0.96) and authority the lowest (MFC = 0.78). These findings establish MFC as a complementary, diagnosis-oriented metric for fairness-aware evaluation of moral reasoning models, enabling more reliable deployment across heterogeneous linguistic contexts. .",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11221v1": {
    "title": "WebRouter: Query-specific Router via Variational Information Bottleneck for Cost-sensitive Web Agent",
    "url": "https://www.alphaxiv.org/abs/2510.11221v1",
    "arxiv_id": "2510.11221v1",
    "authors": "Tao Li, Jinlong Hu, Yang Wang, Junfeng Liu, Xuejun Liu",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 10:05:43",
    "ori_summary": "LLM-brained web agents offer powerful capabilities for web automation but face a critical cost-performance trade-off. The challenge is amplified by web agents' inherently complex prompts that include goals, action histories, and environmental states, leading to degraded LLM ensemble performance. To address this, we introduce WebRouter, a novel query-specific router trained from an information-theoretic perspective. Our core contribution is a cost-aware Variational Information Bottleneck (ca-VIB) objective, which learns a compressed representation of the input prompt while explicitly penalizing the expected operational cost. Experiments on five real-world websites from the WebVoyager benchmark show that WebRouter reduces operational costs by a striking 87.8\\% compared to a GPT-4o baseline, while incurring only a 3.8\\% accuracy drop.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11218v1": {
    "title": "The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers",
    "url": "https://www.alphaxiv.org/abs/2510.11218v1",
    "arxiv_id": "2510.11218v1",
    "authors": "Saad Obaid ul Islam, Anne Lauscher, Goran Glavaš",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 10:00:58",
    "ori_summary": "Large language models (LLMs) can correctly answer \"When was Einstein born?\" yet fail to provide the same date when writing about Einstein's life revealing a fundamental inconsistency in how models access factual knowledge across task complexities. While models display impressive accuracy on factual question-answering benchmarks, the reliability gap between simple and complex queries remains poorly understood, eroding their trustworthiness. In this work, we introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a controlled evaluation framework that compares LLMs' answers to the same factual questions asked (a) in isolation (short) vs. (b) integrated into complex queries (long). Looking at 16 LLMs across 600 queries, we find a systematic misalignment of answers to the corresponding short and long queries. We further uncover position-dependent accuracy loss and momentum effects where consecutive correct or incorrect answers create self-reinforcing patterns. Through mechanistic analysis, we find that aligned facts activate overlapping model internals, and that metrics based on mechanistic similarity can predict short-long answer alignment with up to 78% accuracy. Our work establishes factual consistency over query complexity as an important aspect of LLMs' trustworthiness and challenges current evaluation practices, which implicitly assume that good performance for simple factual queries implies reliability in more complex knowledge-seeking tasks too.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11217v1": {
    "title": "Domain-Specific Data Generation Framework for RAG Adaptation",
    "url": "https://www.alphaxiv.org/abs/2510.11217v1",
    "arxiv_id": "2510.11217v1",
    "authors": "Chris Xing Tian, Weihao Xie, Zhen Chen, Zhengyuan Yi, Hui Liu, Haoliang Li, Shiqi Wang, Siwei Ma",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 09:59:49",
    "ori_summary": "Retrieval-Augmented Generation (RAG) combines the language understanding and reasoning power of large language models (LLMs) with external retrieval to enable domain-grounded responses. Effectively adapting RAG systems to domain-specific settings requires specialized, context-rich training data beyond general-purpose question-answering. Here, we propose RAGen, a scalable and modular framework for generating domain-grounded question-answer-context (QAC) triples tailored to diverse RAG adaptation approaches. RAGen produces these QAC triples by identifying key concepts in documents, generating diverse questions guided by Bloom's Taxonomy-inspired principles, and pairing them with precise answers extracted from relevant contexts. RAGen supports multiple RAG adaptation strategies, including the optimization of key components such as the LLM, retriever, and embedding model, etc. Its modular pipeline features semantic chunking, hierarchical concept extraction, and multi-chunk retrieval, along with the introduction of curated distractor contexts to promote robust reasoning. Designed for scalability, RAGen efficiently handles large and evolving document corpora without redundant processing, making it especially suitable for dynamic evolving domains such as scientific research and enterprise knowledge bases.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11210v1": {
    "title": "Discursive Circuits: How Do Language Models Understand Discourse Relations?",
    "url": "https://www.alphaxiv.org/abs/2510.11210v1",
    "arxiv_id": "2510.11210v1",
    "authors": "Yisong Miao, Min-Yen Kan",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-13 09:45:49",
    "ori_summary": "Which components in transformer language models are responsible for discourse understanding? We hypothesize that sparse computational graphs, termed as discursive circuits, control how models process discourse relations. Unlike simpler tasks, discourse relations involve longer spans and complex reasoning. To make circuit discovery feasible, we introduce a task called Completion under Discourse Relation (CuDR), where a model completes a discourse given a specified relation. To support this task, we construct a corpus of minimal contrastive pairs tailored for activation patching in circuit discovery. Experiments show that sparse circuits ($\\approx 0.2\\%$ of a full GPT-2 model) recover discourse understanding in the English PDTB-based CuDR task. These circuits generalize well to unseen discourse frameworks such as RST and SDRT. Further analysis shows lower layers capture linguistic features such as lexical semantics and coreference, while upper layers encode discourse-level abstractions. Feature utility is consistent across frameworks (e.g., coreference supports Expansion-like relations).",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11196v1": {
    "title": "Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations",
    "url": "https://www.alphaxiv.org/abs/2510.11196v1",
    "arxiv_id": "2510.11196v1",
    "authors": "Johannes Moll, Markus Graf, Tristan Lemke, Nicolas Lenhart, Daniel Truhn, Jean-Benoit Delbrouck, Jiazhen Pan, Daniel Rueckert, Lisa C. Adams, Keno K. Bressem",
    "categories": "cs.CL, cs.CV",
    "pub_date": "2025-10-13 09:28:22",
    "ori_summary": "Vision-language models (VLMs) often produce chain-of-thought (CoT) explanations that sound plausible yet fail to reflect the underlying decision process, undermining trust in high-stakes clinical use. Existing evaluations rarely catch this misalignment, prioritizing answer accuracy or adherence to formats. We present a clinically grounded framework for chest X-ray visual question answering (VQA) that probes CoT faithfulness via controlled text and image modifications across three axes: clinical fidelity, causal attribution, and confidence calibration. In a reader study (n=4), evaluator-radiologist correlations fall within the observed inter-radiologist range for all axes, with strong alignment for attribution (Kendall's $\\tau_b=0.670$), moderate alignment for fidelity ($\\tau_b=0.387$), and weak alignment for confidence tone ($\\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows that answer accuracy and explanation quality are decoupled, acknowledging injected cues does not ensure grounding, and text cues shift explanations more than visual cues. While some open-source models match final answer accuracy, proprietary models score higher on attribution (25.0% vs. 1.4%) and often on fidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to evaluate beyond final answer accuracy.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11184v1": {
    "title": "Can Tool-Integrated Reinforcement Learning Generalize Across Diverse Domains?",
    "url": "https://www.alphaxiv.org/abs/2510.11184v1",
    "arxiv_id": "2510.11184v1",
    "authors": "Zhengyu Chen, Jinluan Yang, Teng Xiao, Ruochen Zhou, Luan Zhang, Xiangyu Xi, Xiaowei Shi, Wei Wang, Jinggang Wang",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-13 09:19:13",
    "ori_summary": "Recent advances in large language models (LLMs) have demonstrated remarkable capabilities in reasoning and tool utilization. However, the generalization of tool-augmented reinforcement learning (RL) across diverse domains remains underexplored. In this work, we investigate the cross-domain generalization of an LLM agent equipped with a code interpreter tool, which is exclusively trained on mathematical problem-solving tasks. Despite the restricted training domain, we evaluate the agent's performance across several distinct reasoning domains. The results reveal that RL-based tool usage learned from mathematical tasks can be effectively transferred to complex tasks in other domains, enabling great task performance and high token efficiency. To facilitate this cross-domain transfer, we propose a Tool Generalization Reinforcement Learning (TGRL) framework designed to promote domain-agnostic learning and skill migration, encompassing: (i) a standardized tool interface that abstracts domain-specific nuances through consistent formatting and explicit termination, fostering transferable invocation patterns; (ii) a dual-component reward system that decomposes rewards to incentivize generalizable behaviors like tool efficiency and reasoning abstraction, ensuring alignment and robustness across domain shifts; and (iii) an XML-based prompt template that separates thinking, tool calls, and responses to encourage modular, domain-invariant planning and coherent multi-turn interactions. Extensive experiments across diverse benchmarks validate our approach, achieving state-of-the-art performance and highlighting the cross-domain potential of Tool RL for LLM reasoning.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11170v1": {
    "title": "EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling",
    "url": "https://www.alphaxiv.org/abs/2510.11170v1",
    "arxiv_id": "2510.11170v1",
    "authors": "Daniel Scalena, Leonidas Zotos, Elisabetta Fersini, Malvina Nissim, Ahmet Üstün",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-13 09:04:28",
    "ori_summary": "With the rise of reasoning language models and test-time scaling methods as a paradigm for improving model performance, substantial computation is often required to generate multiple candidate sequences from the same prompt. This enables exploration of different reasoning paths toward the correct solution, however, allocates the same compute budget for each prompt. Grounded on the assumption that different prompts carry different degrees of complexity, and thus different computation needs, we propose EAGer, a training-free generation method that leverages model uncertainty through token-wise entropy distribution to reduce redundant computation and concurrently improve overall performance. EAGer allows branching to multiple reasoning paths only in the presence of high-entropy tokens, and then reallocates the saved compute budget to the instances where exploration of alternative paths is most needed. We find that across multiple open-source models on complex reasoning benchmarks such as AIME 2025, EAGer can reallocate the budget without accessing target labels, achieving the best efficiency-performance trade-off in terms of reasoning length and Pass@k. When target labels are accessible, EAGer generates up to 65% fewer tokens (hence saving compute) and achieves up to 37% improvement in Pass@k compared to the Full Parallel Sampling.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11167v1": {
    "title": "Bridging Gaps in Hate Speech Detection: Meta-Collections and Benchmarks for Low-Resource Iberian Languages",
    "url": "https://www.alphaxiv.org/abs/2510.11167v1",
    "arxiv_id": "2510.11167v1",
    "authors": "Paloma Piot, José Ramom Pichel Campos, Javier Parapar",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 08:58:02",
    "ori_summary": "Hate speech poses a serious threat to social cohesion and individual well-being, particularly on social media, where it spreads rapidly. While research on hate speech detection has progressed, it remains largely focused on English, resulting in limited resources and benchmarks for low-resource languages. Moreover, many of these languages have multiple linguistic varieties, a factor often overlooked in current approaches. At the same time, large language models require substantial amounts of data to perform reliably, a requirement that low-resource languages often cannot meet. In this work, we address these gaps by compiling a meta-collection of hate speech datasets for European Spanish, standardised with unified labels and metadata. This collection is based on a systematic analysis and integration of existing resources, aiming to bridge the data gap and support more consistent and scalable hate speech detection. We extended this collection by translating it into European Portuguese and into a Galician standard that is more convergent with Spanish and another Galician variant that is more convergent with Portuguese, creating aligned multilingual corpora. Using these resources, we establish new benchmarks for hate speech detection in Iberian languages. We evaluate state-of-the-art large language models in zero-shot, few-shot, and fine-tuning settings, providing baseline results for future research. Moreover, we perform a cross-lingual analysis with our target languages. Our findings underscore the importance of multilingual and variety-aware approaches in hate speech detection and offer a foundation for improved benchmarking in underrepresented European languages.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11160v1": {
    "title": "One Size Does Not Fit All: Exploring Variable Thresholds for Distance-Based Multi-Label Text Classification",
    "url": "https://www.alphaxiv.org/abs/2510.11160v1",
    "arxiv_id": "2510.11160v1",
    "authors": "Jens Van Nooten, Andriy Kosar, Guy De Pauw, Walter Daelemans",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 08:52:14",
    "ori_summary": "Distance-based unsupervised text classification is a method within text classification that leverages the semantic similarity between a label and a text to determine label relevance. This method provides numerous benefits, including fast inference and adaptability to expanding label sets, as opposed to zero-shot, few-shot, and fine-tuned neural networks that require re-training in such cases. In multi-label distance-based classification and information retrieval algorithms, thresholds are required to determine whether a text instance is \"similar\" to a label or query. Similarity between a text and label is determined in a dense embedding space, usually generated by state-of-the-art sentence encoders. Multi-label classification complicates matters, as a text instance can have multiple true labels, unlike in multi-class or binary classification, where each instance is assigned only one label. We expand upon previous literature on this underexplored topic by thoroughly examining and evaluating the ability of sentence encoders to perform distance-based classification. First, we perform an exploratory study to verify whether the semantic relationships between texts and labels vary across models, datasets, and label sets by conducting experiments on a diverse collection of realistic multi-label text classification (MLTC) datasets. We find that similarity distributions show statistically significant differences across models, datasets and even label sets. We propose a novel method for optimizing label-specific thresholds using a validation set. Our label-specific thresholding method achieves an average improvement of 46% over normalized 0.5 thresholding and outperforms uniform thresholding approaches from previous work by an average of 14%. Additionally, the method demonstrates strong performance even with limited labeled examples.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11151v1": {
    "title": "TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code",
    "url": "https://www.alphaxiv.org/abs/2510.11151v1",
    "arxiv_id": "2510.11151v1",
    "authors": "Alexander Sternfeld, Andrei Kucharavy, Ljiljana Dolamic",
    "categories": "cs.CL, cs.CR",
    "pub_date": "2025-10-13 08:44:01",
    "ori_summary": "Large language Models (LLMs) have shown remarkable proficiency in code generation tasks across various programming languages. However, their outputs often contain subtle but critical vulnerabilities, posing significant risks when deployed in security-sensitive or mission-critical systems. This paper introduces TypePilot, an agentic AI framework designed to enhance the security and robustness of LLM-generated code by leveraging strongly typed and verifiable languages, using Scala as a representative example. We evaluate the effectiveness of our approach in two settings: formal verification with the Stainless framework and general-purpose secure code generation. Our experiments with leading open-source LLMs reveal that while direct code generation often fails to enforce safety constraints, just as naive prompting for more secure code, our type-focused agentic pipeline substantially mitigates input validation and injection vulnerabilities. The results demonstrate the potential of structured, type-guided LLM workflows to improve the SotA of the trustworthiness of automated code generation in high-assurance domains.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11144v1": {
    "title": "$How^{2}$: How to learn from procedural How-to questions",
    "url": "https://www.alphaxiv.org/abs/2510.11144v1",
    "arxiv_id": "2510.11144v1",
    "authors": "Gautier Dagan, Frank Keller, Alex Lascarides",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-13 08:35:20",
    "ori_summary": "An agent facing a planning problem can use answers to how-to questions to reduce uncertainty and fill knowledge gaps, helping it solve both current and future tasks. However, their open ended nature, where valid answers to \"How do I X?\" range from executable actions to high-level descriptions of X's sub-goals, makes them challenging for AI agents to ask, and for AI experts to answer, in ways that support efficient planning. We introduce $How^{2}$, a memory agent framework that enables agents to ask how-to questions, store the answers, and reuse them for lifelong learning in interactive environments. We evaluate our approach in Plancraft, a Minecraft crafting environment, where agents must complete an assembly task by manipulating inventory items. Using teacher models that answer at varying levels of abstraction, from executable action sequences to high-level subgoal descriptions, we show that lifelong learning agents benefit most from answers that are abstracted and decoupled from the current state. $How^{2}$ offers a way for LLM-based agents to improve their planning capabilities over time by asking questions in interactive environments.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11104v1": {
    "title": "Enhancing LLM Reasoning via Non-Human-Like Reasoning Path Preference Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.11104v1",
    "arxiv_id": "2510.11104v1",
    "authors": "Junjie Lu, Yuliang Liu, Chaofeng Qu, Wei Shen, Zhouhan Lin, Min Xu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 07:51:16",
    "ori_summary": "Current approaches for strengthening LLM reasoning tend to introduce a training bias toward human-like reasoning trajectories. In step-wise preference optimization, in particular, dependence on human or higher-capacity model annotations for intermediate steps limits exploration of alternative, non-human-like reasoning paths and thus constrains achievable performance. Furthermore, through a small-scale pilot study, we observed that in approximately 75% of cases, the model's first erroneous step occurs after the lowest-confidence point. This suggests that guiding the model at its lowest-confidence point before an error provides more accurate supervision than locating the first explicit error. In this paper, we propose Confidence-Guided Reasoning Path Preference Optimization (CGPO), a method that leverages a confidence signal to identify points of maximal uncertainty in the model's reasoning process and applies self-generated, non-human-like reasoning-path guidance to mitigate trajectory drift. Our experiments span diverse models applied to both code and mathematical reasoning tasks. The results show that, with the same amount of training data, our method using data generated by a small model can achieve better performance in most cases compared with approaches using data generated by a strong model or human-annotated.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11098v1": {
    "title": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
    "url": "https://www.alphaxiv.org/abs/2510.11098v1",
    "arxiv_id": "2510.11098v1",
    "authors": "Jiliang Hu, Wenfu Wang, Zuchao Li, Chenxing Li, Yiyang Zhao, Hanzhao Li, Liqiang Zhang, Meng Yu, Dong Yu",
    "categories": "cs.SD, cs.CL",
    "pub_date": "2025-10-13 07:45:52",
    "ori_summary": "Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited -- they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench) -- a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11052v1": {
    "title": "Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States",
    "url": "https://www.alphaxiv.org/abs/2510.11052v1",
    "arxiv_id": "2510.11052v1",
    "authors": "Qinglin Zhu, Yizhen Yao, Runcong Zhao, Yanzheng Xiang, Amrutha Saseendran, Chen Jin, Philip Alexander Teare, Bin Liang, Yulan He, Lin Gui",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 06:38:13",
    "ori_summary": "Autoregressive (AR) models remain the standard for natural language generation but still suffer from high latency due to strictly sequential decoding. Recent diffusion-inspired approaches, such as LlaDA and Dream, mitigate this by generating in parallel, yet they suffer from two core limitations: information loss, as predictive distributions for non-finalized tokens are discarded at each step, and premature commitment, where local decisions are made without sufficient global coordination. We introduce Latent Refinement Decoding (LRD), a two-stage framework with Latent Refinement and a Predictive Feedback Loop. The first stage maintains masked positions as distributional mixtures of predicted tokens and the mask embedding, allowing the model to establish more globally consistent beliefs. The second stage progressively finalizes confident tokens while retaining uncertain ones for iterative feedback. KL-divergence dynamics provide a principled and reliable criterion for convergence and early stopping. Experiments across coding (HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that LRD improves accuracy while delivering speedups of up to 10.6x, making it a strong and versatile alternative for parallel sequence generation.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11040v1": {
    "title": "Enabling Doctor-Centric Medical AI with LLMs through Workflow-Aligned Tasks and Benchmarks",
    "url": "https://www.alphaxiv.org/abs/2510.11040v1",
    "arxiv_id": "2510.11040v1",
    "authors": "Wenya Xie, Qingying Xiao, Yu Zheng, Xidong Wang, Junying Chen, Ke Ji, Anningzhe Gao, Prayag Tiwari, Xiang Wan, Feng Jiang, Benyou Wang",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 06:18:27",
    "ori_summary": "The rise of large language models (LLMs) has transformed healthcare by offering clinical guidance, yet their direct deployment to patients poses safety risks due to limited domain expertise. To mitigate this, we propose repositioning LLMs as clinical assistants that collaborate with experienced physicians rather than interacting with patients directly. We conduct a two-stage inspiration-feedback survey to identify real-world needs in clinical workflows. Guided by this, we construct DoctorFLAN, a large-scale Chinese medical dataset comprising 92,000 Q&A instances across 22 clinical tasks and 27 specialties. To evaluate model performance in doctor-facing applications, we introduce DoctorFLAN-test (550 single-turn Q&A items) and DotaBench (74 multi-turn conversations). Experimental results with over ten popular LLMs demonstrate that DoctorFLAN notably improves the performance of open-source LLMs in medical contexts, facilitating their alignment with physician workflows and complementing existing patient-oriented models. This work contributes a valuable resource and framework for advancing doctor-centered medical LLM development",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11031v1": {
    "title": "LogiNumSynth: Synthesizing Joint Logical-Numerical Reasoning Problems for Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11031v1",
    "arxiv_id": "2510.11031v1",
    "authors": "Yiwei Liu, Yucheng Li, Xiao Li, Gong Cheng",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 06:01:02",
    "ori_summary": "Joint logical-numerical reasoning remains a major challenge for language models, yet existing datasets rely on fixed rule sets and offer limited control over task complexity, constraining their generalizability for evaluation and training. We present LogiNumSynth, a flexible natural language problem synthesizer that synthesizes tasks requiring proficiency in joint logical reasoning (e.g., rule-based reasoning) and numerical reasoning (e.g., arithmetic computation). LogiNumSynth supports fine-grained control over reasoning world richness, logical reasoning depth, and the complexity of numerical computations, enabling flexible data synthesis across difficulty levels. We demonstrate three key contributions: (1) Synthesizer -- synthesizing fully controllable joint reasoning tasks over natural language; (2) Evaluation & Process Analysis -- evaluating both process accuracy and answer accuracy; (3) Targeted Training -- using synthesized data to enhance LLMs' reasoning performance. Experiments with multiple LLMs highlight persistent weaknesses in logical-numerical reasoning, showing that LogiNumSynth can serve as both a diagnostic tool and a source of targeted supervision for advancing integrated reasoning skills.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11004v1": {
    "title": "Automating Structural Engineering Workflows with Large Language Model Agents",
    "url": "https://www.alphaxiv.org/abs/2510.11004v1",
    "arxiv_id": "2510.11004v1",
    "authors": "Haoran Liang, Yufa Zhou, Mohammad Talebi Kalaleh, Qipei Mei",
    "categories": "cs.MA, cs.AI, cs.CE, cs.CL",
    "pub_date": "2025-10-13 04:38:46",
    "ori_summary": "We introduce $\\textbf{MASSE}$, the first Multi-Agent System for Structural Engineering, effectively integrating large language model (LLM)-based agents with real-world engineering workflows. Structural engineering is a fundamental yet traditionally stagnant domain, with core workflows remaining largely unchanged for decades despite its substantial economic impact and global market size. Recent advancements in LLMs have significantly enhanced their ability to perform complex reasoning, long-horizon planning, and precise tool utilization -- capabilities well aligned with structural engineering tasks such as interpreting design codes, executing load calculations, and verifying structural capacities. We present a proof-of-concept showing that most real-world structural engineering workflows can be fully automated through a training-free LLM-based multi-agent system. MASSE enables immediate deployment in professional environments, and our comprehensive validation on real-world case studies demonstrates that it can reduce expert workload from approximately two hours to mere minutes, while enhancing both reliability and accuracy in practical engineering scenarios.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11001v1": {
    "title": "DND: Boosting Large Language Models with Dynamic Nested Depth",
    "url": "https://www.alphaxiv.org/abs/2510.11001v1",
    "arxiv_id": "2510.11001v1",
    "authors": "Tieyuan Chen, Xiaodong Chen, Haoxing Chen, Zhenzhong Lan, Weiyao Lin, Jianguo Li",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 04:22:57",
    "ori_summary": "We introduce Dynamic Nested Depth (DND), a novel method that improves performance for off-the-shelf LLMs by selecting critical tokens to reprocess in a nested depth manner. Specifically, at the end of the given transformer layer, DND identifies more critical tokens with a router and feeds them back for an extra round of processing, effectively ``reviewing\" difficult tokens while avoiding redundant computation for easier ones. The dynamic selection mechanism is tailored for precise control via two novel strategies: a router controlling loss to enhance token selection distinguishability, and a threshold control scheme to ensure selection stability. We demonstrate the effectiveness of DND by directly integrating it into pre-trained dense and MoE models during a post-training phase. On diverse benchmarks, this approach boosts the performances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by 0.87%, all with a minimal parameter and computing increase.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10998v1": {
    "title": "ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring Scenarios",
    "url": "https://www.alphaxiv.org/abs/2510.10998v1",
    "arxiv_id": "2510.10998v1",
    "authors": "Mahika Phutane, Hayoung Jung, Matthew Kim, Tanushree Mitra, Aditya Vashistha",
    "categories": "cs.CL, cs.AI, cs.CY, cs.HC, cs.LG",
    "pub_date": "2025-10-13 04:18:23",
    "ori_summary": "Large language models (LLMs) are increasingly under scrutiny for perpetuating identity-based discrimination in high-stakes domains such as hiring, particularly against people with disabilities (PwD). However, existing research remains largely Western-centric, overlooking how intersecting forms of marginalization--such as gender and caste--shape experiences of PwD in the Global South. We conduct a comprehensive audit of six LLMs across 2,820 hiring scenarios spanning diverse disability, gender, nationality, and caste profiles. To capture subtle intersectional harms and biases, we introduce ABLEIST (Ableism, Inspiration, Superhumanization, and Tokenism), a set of five ableism-specific and three intersectional harm metrics grounded in disability studies literature. Our results reveal significant increases in ABLEIST harms towards disabled candidates--harms that many state-of-the-art models failed to detect. These harms were further amplified by sharp increases in intersectional harms (e.g., Tokenism) for gender and caste-marginalized disabled candidates, highlighting critical blind spots in current safety tools and the need for intersectional safety evaluations of frontier models in high-stakes domains like hiring.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10994v1": {
    "title": "DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety",
    "url": "https://www.alphaxiv.org/abs/2510.10994v1",
    "arxiv_id": "2510.10994v1",
    "authors": "Wei-Chieh Huang, Henry Peng Zou, Yaozu Wu, Dongyuan Li, Yankai Chen, Weizhi Zhang, Yangning Li, Angelo Zangari, Jizhou Guo, Chunyu Miao, Liancheng Fang, Langzhou He, Renhe Jiang, Philip S. Yu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 04:11:21",
    "ori_summary": "Deep research frameworks have shown promising capabilities in synthesizing comprehensive reports from web sources. While deep research possesses significant potential to address complex issues through planning and research cycles, existing frameworks are deficient in sufficient evaluation procedures and stage-specific protections. They typically treat evaluation as exact match accuracy of question-answering, but overlook crucial aspects of report quality such as credibility, coherence, breadth, depth, and safety. This oversight may result in hazardous or malicious sources being integrated into the final report. To address these issues, we introduce DEEPRESEARCHGUARD, a comprehensive framework featuring four-stage safeguards with open-domain evaluation of references and reports. We assess performance across multiple metrics, e.g., defense success rate and over-refusal rate, and five key report dimensions. In the absence of a suitable safety benchmark, we introduce DRSAFEBENCH, a stage-wise benchmark for deep research safety. Our evaluation spans diverse state-of-the-art LLMs, including GPT-4o, Gemini-2.5-flash, DeepSeek-v3, and o4-mini. DEEPRESEARCHGUARD achieves an average defense success rate improvement of 18.16% while reducing over-refusal rate by 6%. The input guard provides the most substantial early-stage protection by filtering out obvious risks, while the plan and research guards enhance citation discipline and source credibility. Through extensive experiments, we show that DEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware defenses that effectively block harmful content propagation, while systematically improving report quality without excessive over-refusal rates. The code can be found via https://github.com/Jasonya/DeepResearchGuard.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10991v1": {
    "title": "A Survey on Agentic Multimodal Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.10991v1",
    "arxiv_id": "2510.10991v1",
    "authors": "Huanjin Yao, Ruifei Zhang, Jiaxing Huang, Jingyi Zhang, Yibo Wang, Bo Fang, Ruolin Zhu, Yongcheng Jing, Shunyu Liu, Guanbin Li, Dacheng Tao",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-10-13 04:07:01",
    "ori_summary": "With the recent emergence of revolutionary autonomous agentic systems, research community is witnessing a significant shift from traditional static, passive, and domain-specific AI agents toward more dynamic, proactive, and generalizable agentic AI. Motivated by the growing interest in agentic AI and its potential trajectory toward AGI, we present a comprehensive survey on Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we explore the emerging paradigm of agentic MLLMs, delineating their conceptual foundations and distinguishing characteristics from conventional MLLM-based agents. We establish a conceptual framework that organizes agentic MLLMs along three fundamental dimensions: (i) Agentic internal intelligence functions as the system's commander, enabling accurate long-horizon planning through reasoning, reflection, and memory; (ii) Agentic external tool invocation, whereby models proactively use various external tools to extend their problem-solving capabilities beyond their intrinsic knowledge; and (iii) Agentic environment interaction further situates models within virtual or physical environments, allowing them to take actions, adapt strategies, and sustain goal-directed behavior in dynamic real-world scenarios. To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs. Finally, we review the downstream applications of agentic MLLMs and outline future research directions for this rapidly evolving field. To continuously track developments in this rapidly evolving field, we will also actively update a public repository at https://github.com/HJYao00/Awesome-Agentic-MLLMs.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10990v1": {
    "title": "Secret-Protected Evolution for Differentially Private Synthetic Text Generation",
    "url": "https://www.alphaxiv.org/abs/2510.10990v1",
    "arxiv_id": "2510.10990v1",
    "authors": "Tianze Wang, Zhaoyu Chen, Jian Du, Yingtai Xiao, Linjun Zhang, Qiang Yan",
    "categories": "cs.CR, cs.CL, cs.NE",
    "pub_date": "2025-10-13 04:05:42",
    "ori_summary": "Text data has become extremely valuable on large language models (LLMs) and even lead to general artificial intelligence (AGI). A lot of high-quality text in the real world is private and cannot be freely used due to privacy concerns. Therefore, differentially private (DP) synthetic text generation has been proposed, aiming to produce high-utility synthetic data while protecting sensitive information. However, existing DP synthetic text generation imposes uniform guarantees that often overprotect non-sensitive content, resulting in substantial utility loss and computational overhead. Therefore, we propose Secret-Protected Evolution (SecPE), a novel framework that extends private evolution with secret-aware protection. Theoretically, we show that SecPE satisfies $(\\mathrm{p}, \\mathrm{r})$-secret protection, constituting a relaxation of Gaussian DP that enables tighter utility-privacy trade-offs, while also substantially reducing computational complexity relative to baseline methods. Empirically, across the OpenReview, PubMed, and Yelp benchmarks, SecPE consistently achieves lower Fr\\'echet Inception Distance (FID) and higher downstream task accuracy than GDP-based Aug-PE baselines, while requiring less noise to attain the same level of protection. Our results highlight that secret-aware guarantees can unlock more practical and effective privacy-preserving synthetic text generation.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10977v1": {
    "title": "Revisiting Model Interpolation for Efficient Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.10977v1",
    "arxiv_id": "2510.10977v1",
    "authors": "Taiqiang Wu, Runming Yang, Tao Liu, Jiahao Wang, Ngai Wong",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-13 03:30:01",
    "ori_summary": "Model merging, typically on Instruct and Thinking models, has shown remarkable performance for efficient reasoning. In this paper, we systematically revisit the simplest merging method that interpolates two weights directly. Particularly, we observe that model interpolation follows a three-stage evolutionary paradigm with distinct behaviors on the reasoning trajectory. These dynamics provide a principled guide for navigating the performance-cost trade-off. Empirical results demonstrate that a strategically interpolated model surprisingly surpasses sophisticated model merging baselines on both efficiency and effectiveness. We further validate our findings with extensive ablation studies on model layers, modules, and decoding strategies. Ultimately, this work demystifies model interpolation and offers a practical framework for crafting models with precisely targeted reasoning capabilities. Code is available at \\href{https://github.com/wutaiqiang/MI}{Github}.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10974v1": {
    "title": "Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning",
    "url": "https://www.alphaxiv.org/abs/2510.10974v1",
    "arxiv_id": "2510.10974v1",
    "authors": "Zhiwen Ruan, Yixia Li, He Zhu, Yun Chen, Peng Li, Yang Liu, Guanhua Chen",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 03:25:36",
    "ori_summary": "Large language models (LLMs) primarily rely on supervised fine-tuning (SFT) as a key method to adapt pre-trained models to domain-specific tasks such as mathematical reasoning. However, standard SFT uniformly penalizes all tokens, neglecting that only a small subset of critical tokens determines reasoning correctness. This uniform supervision often causes reduced output diversity and limited generalization. We propose Critical Token Fine-tuning (CFT), a simple yet effective approach that updates only tokens identified as functionally indispensable via counterfactual perturbations. By focusing gradient signals on these decisive reasoning steps while preserving the diversity of non-critical tokens, CFT can enhance both generation and diversity. Extensive experiments on five models across three families (Qwen, OLMo, LLaMA) and eleven mathematical reasoning benchmarks show that CFT, despite fine-tuning on less than 12% of tokens, consistently outperforms standard SFT. Moreover, CFT enables test-time scaling through improved sampling diversity and provides a stronger initialization for reinforcement learning, sustaining performance gains in later training stages while maintaining higher entropy for better exploration. These results highlight CFT as a practical and general framework for efficient and robust LLM fine-tuning.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10971v1": {
    "title": "RV-HATE: Reinforced Multi-Module Voting for Implicit Hate Speech Detection",
    "url": "https://www.alphaxiv.org/abs/2510.10971v1",
    "arxiv_id": "2510.10971v1",
    "authors": "Yejin Lee, Hyeseon Ahn, Yo-Sub Han",
    "categories": "cs.CL, cs.AI, 68T50, I.2.7",
    "pub_date": "2025-10-13 03:21:51",
    "ori_summary": "Hate speech remains prevalent in human society and continues to evolve in its forms and expressions. Modern advancements in internet and online anonymity accelerate its rapid spread and complicate its detection. However, hate speech datasets exhibit diverse characteristics primarily because they are constructed from different sources and platforms, each reflecting different linguistic styles and social contexts. Despite this diversity, prior studies on hate speech detection often rely on fixed methodologies without adapting to data-specific features. We introduce RV-HATE, a detection framework designed to account for the dataset-specific characteristics of each hate speech dataset. RV-HATE consists of multiple specialized modules, where each module focuses on distinct linguistic or contextual features of hate speech. The framework employs reinforcement learning to optimize weights that determine the contribution of each module for a given dataset. A voting mechanism then aggregates the module outputs to produce the final decision. RV-HATE offers two primary advantages: (1)~it improves detection accuracy by tailoring the detection process to dataset-specific attributes, and (2)~it also provides interpretable insights into the distinctive features of each dataset. Consequently, our approach effectively addresses implicit hate speech and achieves superior performance compared to conventional static methods. Our code is available at https://github.com/leeyejin1231/RV-HATE.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10965v1": {
    "title": "Judge Before Answer: Can MLLM Discern the False Premise in Question?",
    "url": "https://www.alphaxiv.org/abs/2510.10965v1",
    "arxiv_id": "2510.10965v1",
    "authors": "Jidong Li, Lingyong Fang, Haodong Zhao, Sufeng Duan, Gongshen Liu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 03:17:00",
    "ori_summary": "Multimodal large language models (MLLMs) have witnessed astonishing advancements in recent years. Despite these successes, MLLMs remain vulnerable to flase premise problems. However, existing benchmarks targeting this issue are limited in scope: they often lack fine-grained categorization, exhibit insufficient coverage, and thus fail to provide a rigorous evaluation of the ability of models to recognize false premises. To bridge this gap, we introduce a fully automated pipeline for constructing a comprehensive benchmark of false premise questions. Our method systematically categorizes the premises into three main types and thirteen subtypes according to the abilities required to identify the premises, resulting in the JBA dataset.Results show current MLLMs still struggle with false premise recognition. Building upon this benchmark, we further propose a recognition enhancement framework tailored to strengthen the robustness of MLLMs to detect false premises. Extensive experiments demonstrate that models trained with our framework achieve significant improvements in false premise recognition.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10961v1": {
    "title": "KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification",
    "url": "https://www.alphaxiv.org/abs/2510.10961v1",
    "arxiv_id": "2510.10961v1",
    "authors": "Yejin Lee, Su-Hyeon Kim, Hyundong Jin, Dayoung Kim, Yeonsoo Kim, Yo-Sub Han",
    "categories": "cs.CL, cs.AI, 68T50, I.2.7",
    "pub_date": "2025-10-13 03:12:37",
    "ori_summary": "Toxic content has become an increasingly critical social issue with the rapid expansion of online communication. While numerous studies explored methods for detecting and detoxifying such content, most have focused primarily on English, leaving low-resource language underrepresented. Consequently, Large Language Models~(LLMs) often struggle to identify and neutralize toxic expressions in these languages. This challenge becomes even more pronounced when user employ obfuscation techniques to evade detection systems. Therefore, we propose a \\textbf{KOTOX: Korean Toxic Dataset} for deobfuscation and detoxicification to address this issue. We categorize various obfuscation approaches based on linguistic characteristics of Korean and define a set of transformation rules grounded in real-word examples. Using these rules, we construct three dataset versions (easy, normal, and hard) representing different levels of obfuscation difficulty. This is the first dataset that simultaneously supports deobfuscation and detoxification for the Korean language. We expect it to facilitate better understanding and mitigating of obfuscated toxic content in LLM for low-resource languages. Our code and data are available at https://github.com/leeyejin1231/KOTOX.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10959v1": {
    "title": "Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its Potential for LLM Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.10959v1",
    "arxiv_id": "2510.10959v1",
    "authors": "Xiaoyun Zhang, Xiaojian Yuan, Di Huang, Wang You, Chen Hu, Jingqing Ruan, Kejiang Chen, Xing Hu",
    "categories": "cs.LG, cs.AI, cs.CL, stat.ML",
    "pub_date": "2025-10-13 03:10:26",
    "ori_summary": "Reasoning ability has become a defining capability of Large Language Models (LLMs), with Reinforcement Learning with Verifiable Rewards (RLVR) emerging as a key paradigm to enhance it. However, RLVR training often suffers from policy entropy collapse, where the policy becomes overly deterministic, hindering exploration and limiting reasoning performance. While entropy regularization is a common remedy, its effectiveness is highly sensitive to the fixed coefficient, making it unstable across tasks and models. In this work, we revisit entropy regularization in RLVR and argue that its potential has been largely underestimated. Our analysis shows that (i) tasks of varying difficulty demand distinct exploration intensities, and (ii) balanced exploration may require the policy entropy to be maintained within a moderate range below its initial level. Therefore, we propose Adaptive Entropy Regularization (AER)--a framework that dynamically balances exploration and exploitation via three components: difficulty-aware coefficient allocation, initial-anchored target entropy, and dynamic global coefficient adjustment. Experiments on multiple mathematical reasoning benchmarks show that AER consistently outperforms baselines, improving both reasoning accuracy and exploration capability.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10951v1": {
    "title": "Punctuation-aware treebank tree binarization",
    "url": "https://www.alphaxiv.org/abs/2510.10951v1",
    "arxiv_id": "2510.10951v1",
    "authors": "Eitan Klinger, Vivaan Wadhwa, Jungyeul Park",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 03:02:38",
    "ori_summary": "This article presents a curated resource and evaluation suite for punctuation-aware treebank binarization. Standard binarization pipelines drop punctuation before head selection, which alters constituent shape and harms head-child identification. We release (1) a reproducible pipeline that preserves punctuation as sibling nodes prior to binarization, (2) derived artifacts and metadata (intermediate @X markers, reversibility signatures, alignment indices), and (3) an accompanying evaluation suite covering head-child prediction, round-trip reversibility, and structural compatibility with derivational resources (CCGbank). On the Penn Treebank, punctuation-aware preprocessing improves head prediction accuracy from 73.66\\% (Collins rules) and 86.66\\% (MLP) to 91.85\\% with the same classifier, and achieves competitive alignment against CCGbank derivations. All code, configuration files, and documentation are released to enable replication and extension to other corpora.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10943v1": {
    "title": "The Social Cost of Intelligence: Emergence, Propagation, and Amplification of Stereotypical Bias in Multi-Agent Systems",
    "url": "https://www.alphaxiv.org/abs/2510.10943v1",
    "arxiv_id": "2510.10943v1",
    "authors": "Thi-Nhung Nguyen, Linhao Luo, Thuy-Trang Vu, Dinh Phung",
    "categories": "cs.MA, cs.CL",
    "pub_date": "2025-10-13 02:56:42",
    "ori_summary": "Bias in large language models (LLMs) remains a persistent challenge, manifesting in stereotyping and unfair treatment across social groups. While prior research has primarily focused on individual models, the rise of multi-agent systems (MAS), where multiple LLMs collaborate and communicate, introduces new and largely unexplored dynamics in bias emergence and propagation. In this work, we present a comprehensive study of stereotypical bias in MAS, examining how internal specialization, underlying LLMs and inter-agent communication protocols influence bias robustness, propagation, and amplification. We simulate social contexts where agents represent different social groups and evaluate system behavior under various interaction and adversarial scenarios. Experiments on three bias benchmarks reveal that MAS are generally less robust than single-agent systems, with bias often emerging early through in-group favoritism. However, cooperative and debate-based communication can mitigate bias amplification, while more robust underlying LLMs improve overall system stability. Our findings highlight critical factors shaping fairness and resilience in multi-agent LLM systems.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10936v1": {
    "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF: A Reproducibility Study",
    "url": "https://www.alphaxiv.org/abs/2510.10936v1",
    "arxiv_id": "2510.10936v1",
    "authors": "Anirudh Ganesh, Jayavardhan Reddy",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-13 02:49:21",
    "ori_summary": "We present a reproducibility study of the state-of-the-art neural architecture for sequence labeling proposed by Ma and Hovy (2016)\\cite{ma2016end}. The original BiLSTM-CNN-CRF model combines character-level representations via Convolutional Neural Networks (CNNs), word-level context modeling through Bi-directional Long Short-Term Memory networks (BiLSTMs), and structured prediction using Conditional Random Fields (CRFs). This end-to-end approach eliminates the need for hand-crafted features while achieving excellent performance on named entity recognition (NER) and part-of-speech (POS) tagging tasks. Our implementation successfully reproduces the key results, achieving 91.18\\% F1-score on CoNLL-2003 NER and demonstrating the model's effectiveness across sequence labeling tasks. We provide a detailed analysis of the architecture components and release an open-source PyTorch implementation to facilitate further research.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10930v1": {
    "title": "Evaluating Language Models' Evaluations of Games",
    "url": "https://www.alphaxiv.org/abs/2510.10930v1",
    "arxiv_id": "2510.10930v1",
    "authors": "Katherine M. Collins, Cedegao E. Zhang, Graham Todd, Lance Ying, Mauricio Barba da Costa, Ryan Liu, Prafull Sharma, Adrian Weller, Ionatan Kuperwajs, Lionel Wong, Joshua B. Tenenbaum, Thomas L. Griffiths",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-13 02:45:37",
    "ori_summary": "Reasoning is not just about solving problems -- it is also about evaluating which problems are worth solving at all. Evaluations of artificial intelligence (AI) systems primarily focused on problem solving, historically by studying how models play games such as chess and Go. In this paper, we advocate for a new paradigm that assesses AI systems' evaluation of games. First, we introduce a formalism for evaluating such evaluations. We then leverage a large-scale dataset of over $100$ novel board games and over 450 human judgments to compare evaluations produced by modern language and reasoning models against those of people and symbolic computational agents. We consider two kinds of evaluative queries: assessing the payoff (or fairness) and the funness of games. These queries span two dimensions relevant to the design of evaluations of AI evaluations: how complex a query is to compute and how difficult a query is to quantify. Our results show that reasoning models are generally more aligned to people in their evaluations of games than non-reasoning language models. However, we observe a non-monotonic relationship: as models get closer to game-theoretic optimal, their fit to human data weakens. We also observe more \"jaggedness\" across models for assessing funness, in line with the greater difficulty of quantifying this query. Across queries and games, reasoning models show highly variable and unpredictable resource usage when assessing queries, pointing to the importance of imbuing more resource-rational meta-reasoning in language and reasoning models.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10927v1": {
    "title": "GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.10927v1",
    "arxiv_id": "2510.10927v1",
    "authors": "Yawen Yang, Fukun Ma, Shiao Meng, Aiwei Liu, Lijie Wen",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 02:40:14",
    "ori_summary": "In biomedical fields, one named entity may consist of a series of non-adjacent tokens and overlap with other entities. Previous methods recognize discontinuous entities by connecting entity fragments or internal tokens, which face challenges of error propagation and decoding ambiguity due to the wide variety of span or word combinations. To address these issues, we deeply explore discontinuous entity structures and propose an effective Gap-aware grid tagging model for Discontinuous Named Entity Recognition, named GapDNER. Our GapDNER innovatively applies representation learning on the context gaps between entity fragments to resolve decoding ambiguity and enhance discontinuous NER performance. Specifically, we treat the context gap as an additional type of span and convert span classification into a token-pair grid tagging task. Subsequently, we design two interactive components to comprehensively model token-pair grid features from both intra- and inter-span perspectives. The intra-span regularity extraction module employs the biaffine mechanism along with linear attention to capture the internal regularity of each span, while the inter-span relation enhancement module utilizes criss-cross attention to obtain semantic relations among different spans. At the inference stage of entity decoding, we assign a directed edge to each entity fragment and context gap, then use the BFS algorithm to search for all valid paths from the head to tail of grids with entity tags. Experimental results on three datasets demonstrate that our GapDNER achieves new state-of-the-art performance on discontinuous NER and exhibits remarkable advantages in recognizing complex entity structures.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10925v1": {
    "title": "Find Your Optimal Teacher: Personalized Data Synthesis via Router-Guided Multi-Teacher Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.10925v1",
    "arxiv_id": "2510.10925v1",
    "authors": "Hengyuan Zhang, Shiping Yang, Xiao Liang, Chenming Shang, Yuxuan Jiang, Chaofan Tao, Jing Xiong, Hayden Kwok-Hay So, Ruobing Xie, Angel X. Chang, Ngai Wong",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-13 02:36:36",
    "ori_summary": "Training student models on synthetic data generated by strong teacher models is a promising way to distilling the capabilities of teachers. However, recent studies show that stronger models are not always optimal teachers, revealing a mismatch between teacher outputs and student learnability. To address this issue, we propose PerSyn (Personalized data Synthesis), a novel synthesis strategy that operates under a new ``Route then Generate'' paradigm to create data tailored to each student model, enabling it to learn more effectively. Specifically, PerSyn first assigns each prompt to its optimal teacher via a query-level router that jointly considers student learnability and teacher response quality. Each teacher then synthesizes data only for its assigned prompts, making the process more efficient than the conventional ``Generate then Select'' paradigm, where all teachers must generate parallel responses for the entire prompt set before constructing the final dataset. Extensive experiments across different model families and scales demonstrate that PerSyn consistently achieves superior or comparable performance to all baselines in instruct tuning and math reasoning settings. Further analysis verifies the effectiveness of PerSyn and offers extra insights to propel future research.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10913v1": {
    "title": "ADVICE: Answer-Dependent Verbalized Confidence Estimation",
    "url": "https://www.alphaxiv.org/abs/2510.10913v1",
    "arxiv_id": "2510.10913v1",
    "authors": "Ki Jung Seo, Sehun Lim, Taeuk Kim",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 02:18:33",
    "ori_summary": "Recent progress in large language models (LLMs) has enabled them to express their confidence in natural language, enhancing transparency and reliability. However, their confidence often exhibits overconfidence, the cause of which remains poorly understood. In this work, we conduct a detailed analysis of the dynamics underlying verbalized confidence and identify answer-independence as a key factor, defined as the model's failure to condition confidence on its own answer. To address this, we propose ADVICE (Answer-Dependent Verbalized Confidence Estimation), a fine-tuning framework that facilitates answer-grounded confidence estimation. Extensive experiments show that ADVICE substantially improves confidence calibration while preserving task performance. Further analyses confirm that ADVICE strengthens answer-groundedness, leading to more balanced and well-calibrated confidence distributions. Our findings shed light on the origin of overconfidence and establish a framework for more trustworthy confidence verbalization.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10890v1": {
    "title": "LLM$\\times$MapReduce-V3: Enabling Interactive In-Depth Survey Generation through a MCP-Driven Hierarchically Modular Agent System",
    "url": "https://www.alphaxiv.org/abs/2510.10890v1",
    "arxiv_id": "2510.10890v1",
    "authors": "Yu Chao, Siyu Lin, xiaorong wang, Zhu Zhang, Zihan Zhou, Haoyu Wang, Shuo Wang, Jie Zhou, Zhiyuan Liu, Maosong Sun",
    "categories": "cs.CL",
    "pub_date": "2025-10-13 01:38:37",
    "ori_summary": "We introduce LLM x MapReduce-V3, a hierarchically modular agent system designed for long-form survey generation. Building on the prior work, LLM x MapReduce-V2, this version incorporates a multi-agent architecture where individual functional components, such as skeleton initialization, digest construction, and skeleton refinement, are implemented as independent model-context-protocol (MCP) servers. These atomic servers can be aggregated into higher-level servers, creating a hierarchically structured system. A high-level planner agent dynamically orchestrates the workflow by selecting appropriate modules based on their MCP tool descriptions and the execution history. This modular decomposition facilitates human-in-the-loop intervention, affording users greater control and customization over the research process. Through a multi-turn interaction, the system precisely captures the intended research perspectives to generate a comprehensive skeleton, which is then developed into an in-depth survey. Human evaluations demonstrate that our system surpasses representative baselines in both content depth and length, highlighting the strength of MCP-based modular planning.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10885v1": {
    "title": "Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling Strategies in Text2SQL Tasks",
    "url": "https://www.alphaxiv.org/abs/2510.10885v1",
    "arxiv_id": "2510.10885v1",
    "authors": "Jiajing Guo, Kenil Patel, Jorge Piazentin Ono, Wenbin He, Liu Ren",
    "categories": "cs.CL, cs.DB",
    "pub_date": "2025-10-13 01:29:54",
    "ori_summary": "Large language models (LLMs) are increasingly powering Text-to-SQL (Text2SQL) systems, enabling non-expert users to query industrial databases using natural language. While test-time scaling strategies have shown promise in LLM-based solutions, their effectiveness in real-world applications, especially with the latest reasoning models, remains uncertain. In this work, we benchmark six lightweight, industry-oriented test-time scaling strategies and four LLMs, including two reasoning models, evaluating their performance on the BIRD Mini-Dev benchmark. Beyond standard accuracy metrics, we also report inference latency and token consumption, providing insights relevant for practical system deployment. Our findings reveal that Divide-and-Conquer prompting and few-shot demonstrations consistently enhance performance for both general-purpose and reasoning-focused LLMs. However, introducing additional workflow steps yields mixed results, and base model selection plays a critical role. This work sheds light on the practical trade-offs between accuracy, efficiency, and complexity when deploying Text2SQL systems.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11717v1": {
    "title": "Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams",
    "url": "https://www.alphaxiv.org/abs/2510.11717v1",
    "arxiv_id": "2510.11717v1",
    "authors": "Takuya Nakabayashi, Navami Kairanda, Hideo Saito, Vladislav Golyanik",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 17:59:55",
    "ori_summary": "Event cameras offer various advantages for novel view rendering compared to synchronously operating RGB cameras, and efficient event-based techniques supporting rigid scenes have been recently demonstrated in the literature. In the case of non-rigid objects, however, existing approaches additionally require sparse RGB inputs, which can be a substantial practical limitation; it remains unknown if similar models could be learned from event streams only. This paper sheds light on this challenging open question and introduces Ev4DGS, i.e., the first approach for novel view rendering of non-rigidly deforming objects in the explicit observation space (i.e., as RGB or greyscale images) from monocular event streams. Our method regresses a deformable 3D Gaussian Splatting representation through 1) a loss relating the outputs of the estimated model with the 2D event observation space, and 2) a coarse 3D deformation model trained from binary masks generated from events. We perform experimental comparisons on existing synthetic and newly recorded real datasets with non-rigid objects. The results demonstrate the validity of Ev4DGS and its superior performance compared to multiple naive baselines that can be applied in our setting. We will release our models and the datasets used in the evaluation for research purposes; see the project webpage: https://4dqv.mpi-inf.mpg.de/Ev4DGS/.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11718v1": {
    "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images",
    "url": "https://www.alphaxiv.org/abs/2510.11718v1",
    "arxiv_id": "2510.11718v1",
    "authors": "Chengqi Duan, Kaiyue Sun, Rongyao Fang, Manyuan Zhang, Yan Feng, Ying Luo, Yufang Liu, Ke Wang, Peng Pei, Xunliang Cai, Hongsheng Li, Yi Ma, Xihui Liu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 17:59:55",
    "ori_summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models (VLMs) have shown significant progress in mathematical reasoning, yet they still face a critical bottleneck with problems requiring visual assistance, such as drawing auxiliary lines or plotting functions to solve the problems. Most LLMs and VLMs are constrained to text-only reasoning chains, while multimodal unified models that can generate interleaved text and images lack the necessary precision and controllability for such tasks. To address this, we propose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking with images\" in mathematics. Our approach leverages the VLM to generate text reasoning as well as executable plotting code, which is then rendered into images as \"visual thought\", to solve mathematical problems. To achieve this, we first construct Math-VR, the first large-scale, bilingual dataset and benchmark for Mathematics problems with Visual Reasoning, comprising 178K samples. Second, to create high-quality training data, we develop a state-of-the-art image-to-code converter specialized for parsing complex mathematical figures into codes. Finally, using these training data, we train the CodePlot-CoT model for solving mathematical problems. Experimental results show that our model achieves up to 21% increase over base model on our new benchmark, fully validating the efficacy of our proposed code-driven reasoning paradigm. Our work opens a new direction for multimodal mathematical reasoning and provides the community with the first large-scale dataset, comprehensive benchmark, and strong approach for such problems. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11715v1": {
    "title": "Point Prompting: Counterfactual Tracking with Video Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2510.11715v1",
    "arxiv_id": "2510.11715v1",
    "authors": "Ayush Shrivastava, Sanyam Mehta, Daniel Geng, Andrew Owens",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 17:59:46",
    "ori_summary": "Trackers and video generators solve closely related problems: the former analyze motion, while the latter synthesize it. We show that this connection enables pretrained video diffusion models to perform zero-shot point tracking by simply prompting them to visually mark points as they move over time. We place a distinctively colored marker at the query point, then regenerate the rest of the video from an intermediate noise level. This propagates the marker across frames, tracing the point's trajectory. To ensure that the marker remains visible in this counterfactual generation, despite such markers being unlikely in natural videos, we use the unedited initial frame as a negative prompt. Through experiments with multiple image-conditioned video diffusion models, we find that these \"emergent\" tracks outperform those of prior zero-shot methods and persist through occlusions, often obtaining performance that is competitive with specialized self-supervised models.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11712v1": {
    "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training",
    "url": "https://www.alphaxiv.org/abs/2510.11712v1",
    "arxiv_id": "2510.11712v1",
    "authors": "Haoran Feng, Dizhe Zhang, Xiangtai Li, Bo Du, Lu Qi",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 17:59:15",
    "ori_summary": "In this work, we propose DiT360, a DiT-based framework that performs hybrid training on perspective and panoramic data for panoramic image generation. For the issues of maintaining geometric fidelity and photorealism in generation quality, we attribute the main reason to the lack of large-scale, high-quality, real-world panoramic data, where such a data-centric view differs from prior methods that focus on model design. Basically, DiT360 has several key modules for inter-domain transformation and intra-domain augmentation, applied at both the pre-VAE image level and the post-VAE token level. At the image level, we incorporate cross-domain knowledge through perspective image guidance and panoramic refinement, which enhance perceptual quality while regularizing diversity and photorealism. At the token level, hybrid supervision is applied across multiple modules, which include circular padding for boundary continuity, yaw loss for rotational robustness, and cube loss for distortion awareness. Extensive experiments on text-to-panorama, inpainting, and outpainting tasks demonstrate that our method achieves better boundary consistency and image fidelity across eleven quantitative metrics. Our code is available at https://github.com/Insta360-Research-Team/DiT360.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11709v1": {
    "title": "Adversarial Attacks Leverage Interference Between Features in Superposition",
    "url": "https://www.alphaxiv.org/abs/2510.11709v1",
    "arxiv_id": "2510.11709v1",
    "authors": "Edward Stevinson, Lucas Prieto, Melih Barsbey, Tolga Birdal",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-13 17:59:02",
    "ori_summary": "Fundamental questions remain about when and why adversarial examples arise in neural networks, with competing views characterising them either as artifacts of the irregularities in the decision landscape or as products of sensitivity to non-robust input features. In this paper, we instead argue that adversarial vulnerability can stem from efficient information encoding in neural networks. Specifically, we show how superposition - where networks represent more features than they have dimensions - creates arrangements of latent representations that adversaries can exploit. We demonstrate that adversarial perturbations leverage interference between superposed features, making attack patterns predictable from feature arrangements. Our framework provides a mechanistic explanation for two known phenomena: adversarial attack transferability between models with similar training regimes and class-specific vulnerability patterns. In synthetic settings with precisely controlled superposition, we establish that superposition suffices to create adversarial vulnerability. We then demonstrate that these findings persist in a ViT trained on CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct of networks' representational compression, rather than flaws in the learning process or non-robust inputs.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11704v1": {
    "title": "Bayesian Topological Convolutional Neural Nets",
    "url": "https://www.alphaxiv.org/abs/2510.11704v1",
    "arxiv_id": "2510.11704v1",
    "authors": "Sarah Harkins Dayton, Hayden Everett, Ioannis Schizas, David L. Boothe Jr., Vasileios Maroulas",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 17:57:43",
    "ori_summary": "Convolutional neural networks (CNNs) have been established as the main workhorse in image data processing; nonetheless, they require large amounts of data to train, often produce overconfident predictions, and frequently lack the ability to quantify the uncertainty of their predictions. To address these concerns, we propose a new Bayesian topological CNN that promotes a novel interplay between topology-aware learning and Bayesian sampling. Specifically, it utilizes information from important manifolds to accelerate training while reducing calibration error by placing prior distributions on network parameters and properly learning appropriate posteriors. One important contribution of our work is the inclusion of a consistency condition in the learning cost, which can effectively modify the prior distributions to improve the performance of our novel network architecture. We evaluate the model on benchmark image classification datasets and demonstrate its superiority over conventional CNNs, Bayesian neural networks (BNNs), and topological CNNs. In particular, we supply evidence that our method provides an advantage in situations where training data is limited or corrupted. Furthermore, we show that the new model allows for better uncertainty quantification than standard BNNs since it can more readily identify examples of out-of-distribution data on which it has not been trained. Our results highlight the potential of our novel hybrid approach for more efficient and robust image classification.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11690v1": {
    "title": "Diffusion Transformers with Representation Autoencoders",
    "url": "https://www.alphaxiv.org/abs/2510.11690v1",
    "arxiv_id": "2510.11690v1",
    "authors": "Boyang Zheng, Nanye Ma, Shengbang Tong, Saining Xie",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-13 17:51:39",
    "ori_summary": "Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-dimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality. In this work, we explore replacing the VAE with pretrained representation encoders (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term Representation Autoencoders (RAEs). These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for a scalable transformer-based architecture. Since these latent spaces are typically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically. Our approach achieves faster convergence without auxiliary representation alignment losses. Using a DiT variant equipped with a lightweight, wide DDT head, we achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers clear advantages and should be the new default for diffusion transformer training.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11687v1": {
    "title": "Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View",
    "url": "https://www.alphaxiv.org/abs/2510.11687v1",
    "arxiv_id": "2510.11687v1",
    "authors": "Jinyu Zhang, Haitao Lin, Jiashu Hou, Xiangyang Xue, Yanwei Fu",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 17:49:15",
    "ori_summary": "Estimating an object's 6D pose, size, and shape from visual input is a fundamental problem in computer vision, with critical applications in robotic grasping and manipulation. Existing methods either rely on object-specific priors such as CAD models or templates, or suffer from limited generalization across categories due to pose-shape entanglement and multi-stage pipelines. In this work, we propose a unified, category-agnostic framework that simultaneously predicts 6D pose, size, and dense shape from a single RGB-D image, without requiring templates, CAD models, or category labels at test time. Our model fuses dense 2D features from vision foundation models with partial 3D point clouds using a Transformer encoder enhanced by a Mixture-of-Experts, and employs parallel decoders for pose-size estimation and shape reconstruction, achieving real-time inference at 28 FPS. Trained solely on synthetic data from 149 categories in the SOPE dataset, our framework is evaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL, spanning over 300 categories. It achieves state-of-the-art accuracy on seen categories while demonstrating remarkably strong zero-shot generalization to unseen real-world objects, establishing a new standard for open-set 6D understanding in robotics and embodied AI.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11675v1": {
    "title": "FACE: Faithful Automatic Concept Extraction",
    "url": "https://www.alphaxiv.org/abs/2510.11675v1",
    "arxiv_id": "2510.11675v1",
    "authors": "Dipkamal Bhusal, Michael Clifford, Sara Rampazzi, Nidhi Rastogi",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 17:44:45",
    "ori_summary": "Interpreting deep neural networks through concept-based explanations offers a bridge between low-level features and high-level human-understandable semantics. However, existing automatic concept discovery methods often fail to align these extracted concepts with the model's true decision-making process, thereby compromising explanation faithfulness. In this work, we propose FACE (Faithful Automatic Concept Extraction), a novel framework that augments Non-negative Matrix Factorization (NMF) with a Kullback-Leibler (KL) divergence regularization term to ensure alignment between the model's original and concept-based predictions. Unlike prior methods that operate solely on encoder activations, FACE incorporates classifier supervision during concept learning, enforcing predictive consistency and enabling faithful explanations. We provide theoretical guarantees showing that minimizing the KL divergence bounds the deviation in predictive distributions, thereby promoting faithful local linearity in the learned concept space. Systematic evaluations on ImageNet, COCO, and CelebA datasets demonstrate that FACE outperforms existing methods across faithfulness and sparsity metrics.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11650v1": {
    "title": "InfiniHuman: Infinite 3D Human Creation with Precise Control",
    "url": "https://www.alphaxiv.org/abs/2510.11650v1",
    "arxiv_id": "2510.11650v1",
    "authors": "Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 17:29:55",
    "ori_summary": "Generating realistic and controllable 3D human avatars is a long-standing challenge, particularly when covering broad attribute ranges such as ethnicity, age, clothing styles, and detailed body shapes. Capturing and annotating large-scale human datasets for training generative models is prohibitively expensive and limited in scale and diversity. The central question we address in this paper is: Can existing foundation models be distilled to generate theoretically unbounded, richly annotated 3D human data? We introduce InfiniHuman, a framework that synergistically distills these models to produce richly annotated human data at minimal cost and with theoretically unlimited scalability. We propose InfiniHumanData, a fully automatic pipeline that leverages vision-language and image generation models to create a large-scale multi-modal dataset. User study shows our automatically generated identities are undistinguishable from scan renderings. InfiniHumanData contains 111K identities spanning unprecedented diversity. Each identity is annotated with multi-granularity text descriptions, multi-view RGB images, detailed clothing images, and SMPL body-shape parameters. Building on this dataset, we propose InfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body shape, and clothing assets. InfiniHumanGen enables fast, realistic, and precisely controllable avatar generation. Extensive experiments demonstrate significant improvements over state-of-the-art methods in visual quality, generation speed, and controllability. Our approach enables high-quality avatar generation with fine-grained control at effectively unbounded scale through a practical and affordable solution. We will publicly release the automatic data generation pipeline, the comprehensive InfiniHumanData dataset, and the InfiniHumanGen models at https://yuxuan-xue.com/infini-human.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11649v1": {
    "title": "PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image",
    "url": "https://www.alphaxiv.org/abs/2510.11649v1",
    "arxiv_id": "2510.11649v1",
    "authors": "Pradyumna Yalandur Muralidhar, Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 17:29:51",
    "ori_summary": "Reconstructing metrically accurate humans and their surrounding scenes from a single image is crucial for virtual reality, robotics, and comprehensive 3D scene understanding. However, existing methods struggle with depth ambiguity, occlusions, and physically inconsistent contacts. To address these challenges, we introduce PhySIC, a framework for physically plausible Human-Scene Interaction and Contact reconstruction. PhySIC recovers metrically consistent SMPL-X human meshes, dense scene surfaces, and vertex-level contact maps within a shared coordinate frame from a single RGB image. Starting from coarse monocular depth and body estimates, PhySIC performs occlusion-aware inpainting, fuses visible depth with unscaled geometry for a robust metric scaffold, and synthesizes missing support surfaces like floors. A confidence-weighted optimization refines body pose, camera parameters, and global scale by jointly enforcing depth alignment, contact priors, interpenetration avoidance, and 2D reprojection consistency. Explicit occlusion masking safeguards invisible regions against implausible configurations. PhySIC is efficient, requiring only 9 seconds for joint human-scene optimization and under 27 seconds end-to-end. It naturally handles multiple humans, enabling reconstruction of diverse interactions. Empirically, PhySIC outperforms single-image baselines, reducing mean per-vertex scene error from 641 mm to 227 mm, halving PA-MPJPE to 42 mm, and improving contact F1 from 0.09 to 0.51. Qualitative results show realistic foot-floor interactions, natural seating, and plausible reconstructions of heavily occluded furniture. By converting a single image into a physically plausible 3D human-scene pair, PhySIC advances scalable 3D scene understanding. Our implementation is publicly available at https://yuxuan-xue.com/physic.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11647v1": {
    "title": "IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing Assessment",
    "url": "https://www.alphaxiv.org/abs/2510.11647v1",
    "arxiv_id": "2510.11647v1",
    "authors": "Yinan Chen, Jiangning Zhang, Teng Hu, Yuxiang Zeng, Zhucun Xue, Qingdong He, Chengjie Wang, Yong Liu, Xiaobin Hu, Shuicheng Yan",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 17:27:08",
    "ori_summary": "Instruction-guided video editing has emerged as a rapidly advancing research direction, offering new opportunities for intuitive content transformation while also posing significant challenges for systematic evaluation. Existing video editing benchmarks fail to support the evaluation of instruction-guided video editing adequately and further suffer from limited source diversity, narrow task coverage and incomplete evaluation metrics. To address the above limitations, we introduce IVEBench, a modern benchmark suite specifically designed for instruction-guided video editing assessment. IVEBench comprises a diverse database of 600 high-quality source videos, spanning seven semantic dimensions, and covering video lengths ranging from 32 to 1,024 frames. It further includes 8 categories of editing tasks with 35 subcategories, whose prompts are generated and refined through large language models and expert review. Crucially, IVEBench establishes a three-dimensional evaluation protocol encompassing video quality, instruction compliance and video fidelity, integrating both traditional metrics and multimodal large language model-based assessments. Extensive experiments demonstrate the effectiveness of IVEBench in benchmarking state-of-the-art instruction-guided video editing methods, showing its ability to provide comprehensive and human-aligned evaluation outcomes.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11632v1": {
    "title": "NV3D: Leveraging Spatial Shape Through Normal Vector-based 3D Object Detection",
    "url": "https://www.alphaxiv.org/abs/2510.11632v1",
    "arxiv_id": "2510.11632v1",
    "authors": "Krittin Chaowakarn, Paramin Sangwongngam, Nang Htet Htet Aung, Chalie Charoenlarpnopparut",
    "categories": "cs.CV, cs.AI, cs.LG, I.2.6; I.2.9; I.2.10; I.4.8; I.4.10; I.5.1; I.5.4",
    "pub_date": "2025-10-13 17:13:06",
    "ori_summary": "Recent studies in 3D object detection for autonomous vehicles aim to enrich features through the utilization of multi-modal setups or the extraction of local patterns within LiDAR point clouds. However, multi-modal methods face significant challenges in feature alignment, and gaining features locally can be oversimplified for complex 3D object detection tasks. In this paper, we propose a novel model, NV3D, which utilizes local features acquired from voxel neighbors, as normal vectors computed per voxel basis using K-nearest neighbors (KNN) and principal component analysis (PCA). This informative feature enables NV3D to determine the relationship between the surface and pertinent target entities, including cars, pedestrians, or cyclists. During the normal vector extraction process, NV3D offers two distinct sampling strategies: normal vector density-based sampling and FOV-aware bin-based sampling, allowing elimination of up to 55% of data while maintaining performance. In addition, we applied element-wise attention fusion, which accepts voxel features as the query and value and normal vector features as the key, similar to the attention mechanism. Our method is trained on the KITTI dataset and has demonstrated superior performance in car and cyclist detection owing to their spatial shapes. In the validation set, NV3D without sampling achieves 86.60% and 80.18% mean Average Precision (mAP), greater than the baseline Voxel R-CNN by 2.61% and 4.23% mAP, respectively. With both samplings, NV3D achieves 85.54% mAP in car detection, exceeding the baseline by 1.56% mAP, despite roughly 55% of voxels being filtered out.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11631v1": {
    "title": "EvoCAD: Evolutionary CAD Code Generation with Vision Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11631v1",
    "arxiv_id": "2510.11631v1",
    "authors": "Tobias Preintner, Weixuan Yuan, Adrian König, Thomas Bäck, Elena Raponi, Niki van Stein",
    "categories": "cs.CV, cs.AI, cs.NE",
    "pub_date": "2025-10-13 17:12:02",
    "ori_summary": "Combining large language models with evolutionary computation algorithms represents a promising research direction leveraging the remarkable generative and in-context learning capabilities of LLMs with the strengths of evolutionary algorithms. In this work, we present EvoCAD, a method for generating computer-aided design (CAD) objects through their symbolic representations using vision language models and evolutionary optimization. Our method samples multiple CAD objects, which are then optimized using an evolutionary approach with vision language and reasoning language models. We assess our method using GPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and comparing it to prior methods. Additionally, we introduce two new metrics based on topological properties defined by the Euler characteristic, which capture a form of semantic similarity between 3D objects. Our results demonstrate that EvoCAD outperforms previous approaches on multiple metrics, particularly in generating topologically correct objects, which can be efficiently evaluated using our two novel metrics that complement existing spatial metrics.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11613v1": {
    "title": "High-resolution Photo Enhancement in Real-time: A Laplacian Pyramid Network",
    "url": "https://www.alphaxiv.org/abs/2510.11613v1",
    "arxiv_id": "2510.11613v1",
    "authors": "Feng Zhang, Haoyou Deng, Zhiqiang Li, Lida Li, Bin Xu, Qingbo Lu, Zisheng Cao, Minchen Wei, Changxin Gao, Nong Sang, Xiang Bai",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 16:52:32",
    "ori_summary": "Photo enhancement plays a crucial role in augmenting the visual aesthetics of a photograph. In recent years, photo enhancement methods have either focused on enhancement performance, producing powerful models that cannot be deployed on edge devices, or prioritized computational efficiency, resulting in inadequate performance for real-world applications. To this end, this paper introduces a pyramid network called LLF-LUT++, which integrates global and local operators through closed-form Laplacian pyramid decomposition and reconstruction. This approach enables fast processing of high-resolution images while also achieving excellent performance. Specifically, we utilize an image-adaptive 3D LUT that capitalizes on the global tonal characteristics of downsampled images, while incorporating two distinct weight fusion strategies to achieve coarse global image enhancement. To implement this strategy, we designed a spatial-frequency transformer weight predictor that effectively extracts the desired distinct weights by leveraging frequency features. Additionally, we apply local Laplacian filters to adaptively refine edge details in high-frequency components. After meticulously redesigning the network structure and transformer model, LLF-LUT++ not only achieves a 2.64 dB improvement in PSNR on the HDR+ dataset, but also further reduces runtime, with 4K resolution images processed in just 13 ms on a single GPU. Extensive experimental results on two benchmark datasets further show that the proposed approach performs favorably compared to state-of-the-art methods. The source code will be made publicly available at https://github.com/fengzhang427/LLF-LUT.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11606v1": {
    "title": "ExpVid: A Benchmark for Experiment Video Understanding & Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.11606v1",
    "arxiv_id": "2510.11606v1",
    "authors": "Yicheng Xu, Yue Wu, Jiashuo Yu, Ziang Yan, Tianxiang Jiang, Yinan He, Qingsong Zhao, Kai Chen, Yu Qiao, Limin Wang, Manabu Okumura, Yi Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 16:45:28",
    "ori_summary": "Multimodal Large Language Models (MLLMs) hold promise for accelerating scientific discovery by interpreting complex experimental procedures. However, their true capabilities are poorly understood, as existing benchmarks neglect the fine-grained and long-horizon nature of authentic laboratory work, especially in wet-lab settings. To bridge this gap, we introduce ExpVid, the first benchmark designed to systematically evaluate MLLMs on scientific experiment videos. Curated from peer-reviewed video publications, ExpVid features a new three-level task hierarchy that mirrors the scientific process: (1) Fine-grained Perception of tools, materials, and actions; (2) Procedural Understanding of step order and completeness; and (3) Scientific Reasoning that connects the full experiment to its published conclusions. Our vision-centric annotation pipeline, combining automated generation with multi-disciplinary expert validation, ensures that tasks require visual grounding. We evaluate 19 leading MLLMs on ExpVid and find that while they excel at coarse-grained recognition, they struggle with disambiguating fine details, tracking state changes over time, and linking experimental procedures to scientific outcomes. Our results reveal a notable performance gap between proprietary and open-source models, particularly in high-order reasoning. ExpVid not only provides a diagnostic tool but also charts a roadmap for developing MLLMs capable of becoming trustworthy partners in scientific experimentation.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11605v1": {
    "title": "ACE-G: Improving Generalization of Scene Coordinate Regression Through Query Pre-Training",
    "url": "https://www.alphaxiv.org/abs/2510.11605v1",
    "arxiv_id": "2510.11605v1",
    "authors": "Leonard Bruns, Axel Barroso-Laguna, Tommaso Cavallari, Áron Monszpart, Sowmya Munukutla, Victor Adrian Prisacariu, Eric Brachmann",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 16:45:17",
    "ori_summary": "Scene coordinate regression (SCR) has established itself as a promising learning-based approach to visual relocalization. After mere minutes of scene-specific training, SCR models estimate camera poses of query images with high accuracy. Still, SCR methods fall short of the generalization capabilities of more classical feature-matching approaches. When imaging conditions of query images, such as lighting or viewpoint, are too different from the training views, SCR models fail. Failing to generalize is an inherent limitation of previous SCR frameworks, since their training objective is to encode the training views in the weights of the coordinate regressor itself. The regressor essentially overfits to the training views, by design. We propose to separate the coordinate regressor and the map representation into a generic transformer and a scene-specific map code. This separation allows us to pre-train the transformer on tens of thousands of scenes. More importantly, it allows us to train the transformer to generalize from mapping images to unseen query images during pre-training. We demonstrate on multiple challenging relocalization datasets that our method, ACE-G, leads to significantly increased robustness while keeping the computational footprint attractive.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11579v1": {
    "title": "MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis",
    "url": "https://www.alphaxiv.org/abs/2510.11579v1",
    "arxiv_id": "2510.11579v1",
    "authors": "Hongyu Zhu, Lin Chen, Mounim A. El-Yacoubi, Mingsheng Shang",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-13 16:23:32",
    "ori_summary": "Multimodal Sentiment Analysis (MSA) aims to identify and interpret human emotions by integrating information from heterogeneous data sources such as text, video, and audio. While deep learning models have advanced in network architecture design, they remain heavily limited by scarce multimodal annotated data. Although Mixup-based augmentation improves generalization in unimodal tasks, its direct application to MSA introduces critical challenges: random mixing often amplifies label ambiguity and semantic inconsistency due to the lack of emotion-aware mixing mechanisms. To overcome these issues, we propose MS-Mix, an adaptive, emotion-sensitive augmentation framework that automatically optimizes sample mixing in multimodal settings. The key components of MS-Mix include: (1) a Sentiment-Aware Sample Selection (SASS) strategy that effectively prevents semantic confusion caused by mixing samples with contradictory emotions. (2) a Sentiment Intensity Guided (SIG) module using multi-head self-attention to compute modality-specific mixing ratios dynamically based on their respective emotional intensities. (3) a Sentiment Alignment Loss (SAL) that aligns the prediction distributions across modalities, and incorporates the Kullback-Leibler-based loss as an additional regularization term to train the emotion intensity predictor and the backbone network jointly. Extensive experiments on three benchmark datasets with six state-of-the-art backbones confirm that MS-Mix consistently outperforms existing methods, establishing a new standard for robust multimodal sentiment augmentation. The source code is available at: https://github.com/HongyuZhu-s/MS-Mix.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11576v1": {
    "title": "Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping",
    "url": "https://www.alphaxiv.org/abs/2510.11576v1",
    "arxiv_id": "2510.11576v1",
    "authors": "Walid Elbarz, Mohamed Bourriz, Hicham Hajji, Hamd Ait Abdelali, François Bourzeix",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 16:21:59",
    "ori_summary": "Foundation models are transforming Earth observation, but their potential for hyperspectral crop mapping remains underexplored. This study benchmarks three foundation models for cereal crop mapping using hyperspectral imagery: HyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarth dataset (a large multitemporal hyperspectral archive). Models were fine-tuned on manually labeled data from a training region and evaluated on an independent test region. Performance was measured with overall accuracy (OA), average accuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%), DOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of 93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved 91%, highlighting the importance of model architecture for strong generalization across geographic regions and sensor platforms. These results provide a systematic evaluation of foundation models for operational hyperspectral crop mapping and outline directions for future model development.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11567v1": {
    "title": "A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.11567v1",
    "arxiv_id": "2510.11567v1",
    "authors": "Denis Zavadski, Damjan Kalšan, Tim Küchler, Haebom Lee, Stefan Roth, Carsten Rother",
    "categories": "cs.CV, cs.GR, cs.LG",
    "pub_date": "2025-10-13 16:12:29",
    "ori_summary": "Synthetic datasets are widely used for training urban scene recognition models, but even highly realistic renderings show a noticeable gap to real imagery. This gap is particularly pronounced when adapting to a specific target domain, such as Cityscapes, where differences in architecture, vegetation, object appearance, and camera characteristics limit downstream performance. Closing this gap with more detailed 3D modelling would require expensive asset and scene design, defeating the purpose of low-cost labelled data. To address this, we present a new framework that adapts an off-the-shelf diffusion model to a target domain using only imperfect pseudo-labels. Once trained, it generates high-fidelity, target-aligned images from semantic maps of any synthetic dataset, including low-effort sources created in hours rather than months. The method filters suboptimal generations, rectifies image-label misalignments, and standardises semantics across datasets, transforming weak synthetic data into competitive real-domain training sets. Experiments on five synthetic datasets and two real target datasets show segmentation gains of up to +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly constructed synthetic datasets as effective as high-effort, time-intensive synthetic datasets requiring extensive manual design. This work highlights a valuable collaborative paradigm where fast semantic prototyping, combined with generative models, enables scalable, high-quality training data creation for urban scene understanding.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11566v1": {
    "title": "SCOOP'D: Learning Mixed-Liquid-Solid Scooping via Sim2Real Generative Policy",
    "url": "https://www.alphaxiv.org/abs/2510.11566v1",
    "arxiv_id": "2510.11566v1",
    "authors": "Kuanning Wang, Yongchong Gu, Yuqian Fu, Zeyu Shangguan, Sicheng He, Xiangyang Xue, Yanwei Fu, Daniel Seita",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-13 16:11:34",
    "ori_summary": "Scooping items with tools such as spoons and ladles is common in daily life, ranging from assistive feeding to retrieving items from environmental disaster sites. However, developing a general and autonomous robotic scooping policy is challenging since it requires reasoning about complex tool-object interactions. Furthermore, scooping often involves manipulating deformable objects, such as granular media or liquids, which is challenging due to their infinite-dimensional configuration spaces and complex dynamics. We propose a method, SCOOP'D, which uses simulation from OmniGibson (built on NVIDIA Omniverse) to collect scooping demonstrations using algorithmic procedures that rely on privileged state information. Then, we use generative policies via diffusion to imitate demonstrations from observational input. We directly apply the learned policy in diverse real-world scenarios, testing its performance on various item quantities, item characteristics, and container types. In zero-shot deployment, our method demonstrates promising results across 465 trials in diverse scenarios, including objects of different difficulty levels that we categorize as \"Level 1\" and \"Level 2.\" SCOOP'D outperforms all baselines and ablations, suggesting that this is a promising approach to acquiring robotic scooping skills. Project page is at https://scoopdiff.github.io/.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11565v1": {
    "title": "SNAP: Towards Segmenting Anything in Any Point Cloud",
    "url": "https://www.alphaxiv.org/abs/2510.11565v1",
    "arxiv_id": "2510.11565v1",
    "authors": "Aniket Gupta, Hanhui Wang, Charles Saunders, Aruni RoyChowdhury, Hanumant Singh, Huaizu Jiang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 16:07:00",
    "ori_summary": "Interactive 3D point cloud segmentation enables efficient annotation of complex 3D scenes through user-guided prompts. However, current approaches are typically restricted in scope to a single domain (indoor or outdoor), and to a single form of user interaction (either spatial clicks or textual prompts). Moreover, training on multiple datasets often leads to negative transfer, resulting in domain-specific tools that lack generalizability. To address these limitations, we present \\textbf{SNAP} (\\textbf{S}egment a\\textbf{N}ything in \\textbf{A}ny \\textbf{P}oint cloud), a unified model for interactive 3D segmentation that supports both point-based and text-based prompts across diverse domains. Our approach achieves cross-domain generalizability by training on 7 datasets spanning indoor, outdoor, and aerial environments, while employing domain-adaptive normalization to prevent negative transfer. For text-prompted segmentation, we automatically generate mask proposals without human intervention and match them against CLIP embeddings of textual queries, enabling both panoptic and open-vocabulary segmentation. Extensive experiments demonstrate that SNAP consistently delivers high-quality segmentation results. We achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for spatial-prompted segmentation and demonstrate competitive results on all 5 text-prompted benchmarks. These results show that a unified model can match or exceed specialized domain-specific approaches, providing a practical tool for scalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11553v1": {
    "title": "How many samples to label for an application given a foundation model? Chest X-ray classification study",
    "url": "https://www.alphaxiv.org/abs/2510.11553v1",
    "arxiv_id": "2510.11553v1",
    "authors": "Nikolay Nechaev, Evgenia Przhezdzetskaya, Viktor Gombolevskiy, Dmitry Umerenkov, Dmitry Dylov",
    "categories": "cs.CV, 68T07 (Primary) 68T45, 62H30, 62P10 (Secondary)",
    "pub_date": "2025-10-13 15:53:55",
    "ori_summary": "Chest X-ray classification is vital yet resource-intensive, typically demanding extensive annotated data for accurate diagnosis. Foundation models mitigate this reliance, but how many labeled samples are required remains unclear. We systematically evaluate the use of power-law fits to predict the training size necessary for specific ROC-AUC thresholds. Testing multiple pathologies and foundation models, we find XrayCLIP and XraySigLIP achieve strong performance with significantly fewer labeled examples than a ResNet-50 baseline. Importantly, learning curve slopes from just 50 labeled cases accurately forecast final performance plateaus. Our results enable practitioners to minimize annotation costs by labeling only the essential samples for targeted performance.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11549v1": {
    "title": "ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?",
    "url": "https://www.alphaxiv.org/abs/2510.11549v1",
    "arxiv_id": "2510.11549v1",
    "authors": "Liu Yang, Huiyu Duan, Ran Tao, Juntao Cheng, Sijing Wu, Yunhao Li, Jing Liu, Xiongkuo Min, Guangtao Zhai",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 15:51:47",
    "ori_summary": "Omnidirectional images (ODIs) provide full 360x180 view which are widely adopted in VR, AR and embodied intelligence applications. While multi-modal large language models (MLLMs) have demonstrated remarkable performance on conventional 2D image and video understanding benchmarks, their ability to comprehend the immersive environments captured by ODIs remains largely unexplored. To address this gap, we first present ODI-Bench, a novel comprehensive benchmark specifically designed for omnidirectional image understanding. ODI-Bench contains 2,000 high-quality omnidirectional images and over 4,000 manually annotated question-answering (QA) pairs across 10 fine-grained tasks, covering both general-level and spatial-level ODI understanding. Extensive experiments are conducted to benchmark 20 representative MLLMs, including proprietary and open-source models, under both close-ended and open-ended settings. Experimental results reveal that current MLLMs still struggle to capture the immersive context provided by ODIs. To this end, we further introduce Omni-CoT, a training-free method which significantly enhances MLLMs' comprehension ability in the omnidirectional environment through chain-of-thought reasoning across both textual information and visual cues. Both the benchmark and the code will be released upon the publication.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11538v1": {
    "title": "Massive Activations are the Key to Local Detail Synthesis in Diffusion Transformers",
    "url": "https://www.alphaxiv.org/abs/2510.11538v1",
    "arxiv_id": "2510.11538v1",
    "authors": "Chaofan Gan, Zicheng Zhao, Yuanpeng Tu, Xi Chen, Ziran Qin, Tieyuan Chen, Mehrtash Harandi, Weiyao Lin",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 15:39:13",
    "ori_summary": "Diffusion Transformers (DiTs) have recently emerged as a powerful backbone for visual generation. Recent observations reveal \\emph{Massive Activations} (MAs) in their internal feature maps, yet their function remains poorly understood. In this work, we systematically investigate these activations to elucidate their role in visual generation. We found that these massive activations occur across all spatial tokens, and their distribution is modulated by the input timestep embeddings. Importantly, our investigations further demonstrate that these massive activations play a key role in local detail synthesis, while having minimal impact on the overall semantic content of output. Building on these insights, we propose \\textbf{D}etail \\textbf{G}uidance (\\textbf{DG}), a MAs-driven, training-free self-guidance strategy to explicitly enhance local detail fidelity for DiTs. Specifically, DG constructs a degraded ``detail-deficient'' model by disrupting MAs and leverages it to guide the original network toward higher-quality detail synthesis. Our DG can seamlessly integrate with Classifier-Free Guidance (CFG), enabling further refinements of fine-grained details. Extensive experiments demonstrate that our DG consistently improves fine-grained detail quality across various pre-trained DiTs (\\eg, SD3, SD3.5, and Flux).",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11520v1": {
    "title": "mmWalk: Towards Multi-modal Multi-view Walking Assistance",
    "url": "https://www.alphaxiv.org/abs/2510.11520v1",
    "arxiv_id": "2510.11520v1",
    "authors": "Kedi Ying, Ruiping Liu, Chongyan Chen, Mingzhe Tao, Hao Shi, Kailun Yang, Jiaming Zhang, Rainer Stiefelhagen",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 15:25:52",
    "ori_summary": "Walking assistance in extreme or complex environments remains a significant challenge for people with blindness or low vision (BLV), largely due to the lack of a holistic scene understanding. Motivated by the real-world needs of the BLV community, we build mmWalk, a simulated multi-modal dataset that integrates multi-view sensor and accessibility-oriented features for outdoor safe navigation. Our dataset comprises 120 manually controlled, scenario-categorized walking trajectories with 62k synchronized frames. It contains over 559k panoramic images across RGB, depth, and semantic modalities. Furthermore, to emphasize real-world relevance, each trajectory involves outdoor corner cases and accessibility-specific landmarks for BLV users. Additionally, we generate mmWalkVQA, a VQA benchmark with over 69k visual question-answer triplets across 9 categories tailored for safe and informed walking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs) using zero- and few-shot settings and found they struggle with our risk assessment and navigational tasks. We validate our mmWalk-finetuned model on real-world datasets and show the effectiveness of our dataset for advancing multi-modal walking assistance.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11512v1": {
    "title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference",
    "url": "https://www.alphaxiv.org/abs/2510.11512v1",
    "arxiv_id": "2510.11512v1",
    "authors": "Jianhao Yuan, Fabio Pizzati, Francesco Pinto, Lars Kunze, Ivan Laptev, Paul Newman, Philip Torr, Daniele De Martini",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 15:19:07",
    "ori_summary": "Intuitive physics understanding in video diffusion models plays an essential role in building general-purpose physically plausible world simulators, yet accurately evaluating such capacity remains a challenging task due to the difficulty in disentangling physics correctness from visual appearance in generation. To the end, we introduce LikePhys, a training-free method that evaluates intuitive physics in video diffusion models by distinguishing physically valid and impossible videos using the denoising objective as an ELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By testing on our constructed benchmark of twelve scenarios spanning over four physics domains, we show that our evaluation metric, Plausibility Preference Error (PPE), demonstrates strong alignment with human preference, outperforming state-of-the-art evaluator baselines. We then systematically benchmark intuitive physics understanding in current video diffusion models. Our study further analyses how model design and inference settings affect intuitive physics understanding and highlights domain-specific capacity variations across physical laws. Empirical results show that, despite current models struggling with complex and chaotic dynamics, there is a clear trend of improvement in physics understanding as model capacity and inference settings scale.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11509v1": {
    "title": "Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal Large Language Model",
    "url": "https://www.alphaxiv.org/abs/2510.11509v1",
    "arxiv_id": "2510.11509v1",
    "authors": "Ruiping Liu, Junwei Zheng, Yufan Chen, Zirui Wang, Kunyu Peng, Kailun Yang, Jiaming Zhang, Marc Pollefeys, Rainer Stiefelhagen",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 15:17:18",
    "ori_summary": "Physical environments and circumstances are fundamentally dynamic, yet current 3D datasets and evaluation benchmarks tend to concentrate on either dynamic scenarios or dynamic situations in isolation, resulting in incomplete comprehension. To overcome these constraints, we introduce Situat3DChange, an extensive dataset supporting three situation-aware change understanding tasks following the perception-action model: 121K question-answer pairs, 36K change descriptions for perception tasks, and 17K rearrangement instructions for the action task. To construct this large-scale dataset, Situat3DChange leverages 11K human observations of environmental changes to establish shared mental models and shared situational awareness for human-AI collaboration. These observations, enriched with egocentric and allocentric perspectives as well as categorical and coordinate spatial relations, are integrated using an LLM to support understanding of situated changes. To address the challenge of comparing pairs of point clouds from the same scene with minor changes, we propose SCReasoner, an efficient 3D MLLM approach that enables effective point cloud comparison with minimal parameter overhead and no additional tokens required for the language decoder. Comprehensive evaluation on Situat3DChange tasks highlights both the progress and limitations of MLLMs in dynamic scene and situation understanding. Additional experiments on data scaling and cross-domain transfer demonstrate the task-agnostic effectiveness of using Situat3DChange as a training dataset for MLLMs.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11508v1": {
    "title": "Towards Fast and Scalable Normal Integration using Continuous Components",
    "url": "https://www.alphaxiv.org/abs/2510.11508v1",
    "arxiv_id": "2510.11508v1",
    "authors": "Francesco Milano, Jen Jen Chung, Lionel Ott, Roland Siegwart",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 15:17:16",
    "ori_summary": "Surface normal integration is a fundamental problem in computer vision, dealing with the objective of reconstructing a surface from its corresponding normal map. Existing approaches require an iterative global optimization to jointly estimate the depth of each pixel, which scales poorly to larger normal maps. In this paper, we address this problem by recasting normal integration as the estimation of relative scales of continuous components. By constraining pixels belonging to the same component to jointly vary their scale, we drastically reduce the number of optimization variables. Our framework includes a heuristic to accurately estimate continuous components from the start, a strategy to rebalance optimization terms, and a technique to iteratively merge components to further reduce the size of the problem. Our method achieves state-of-the-art results on the standard normal integration benchmark in as little as a few seconds and achieves one-order-of-magnitude speedup over pixel-level approaches on large-resolution normal maps.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11496v1": {
    "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model",
    "url": "https://www.alphaxiv.org/abs/2510.11496v1",
    "arxiv_id": "2510.11496v1",
    "authors": "Zhiwei Jin, Xiaohui Song, Nan Wang, Yafei Liu, Chao Li, Xin Li, Ruichen Wang, Zhihao Li, Qi Qi, Long Cheng, Dongze Hao, Quanlong Zheng, Yanhao Zhang, Haobo Ji, Jian Ma, Zhitong Zheng, Zhenyi Lin, Haolin Deng, Xin Zou, Xiaojie Yin, Ruilin Wang, Liankai Cai, Haijing Liu, Yuqing Qiu, Ke Chen, Zixian Li, Chi Xie, Huafei Li, Chenxing Li, Chuangchuang Wang, Kai Tang, Zhiguang Zhu, Kai Tang, Wenmei Gao, Rui Wang, Jun Wu, Chao Liu, Qin Xie, Chen Chen, Haonan Lu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 15:04:38",
    "ori_summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoR",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11473v1": {
    "title": "VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment",
    "url": "https://www.alphaxiv.org/abs/2510.11473v1",
    "arxiv_id": "2510.11473v1",
    "authors": "Qing Li, Huifang Feng, Xun Gong, Yu-Shen Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 14:44:50",
    "ori_summary": "3D Gaussian Splatting has recently emerged as an efficient solution for high-quality and real-time novel view synthesis. However, its capability for accurate surface reconstruction remains underexplored. Due to the discrete and unstructured nature of Gaussians, supervision based solely on image rendering loss often leads to inaccurate geometry and inconsistent multi-view alignment. In this work, we propose a novel method that enhances the geometric representation of 3D Gaussians through view alignment (VA). Specifically, we incorporate edge-aware image cues into the rendering loss to improve surface boundary delineation. To enforce geometric consistency across views, we introduce a visibility-aware photometric alignment loss that models occlusions and encourages accurate spatial relationships among Gaussians. To further mitigate ambiguities caused by lighting variations, we incorporate normal-based constraints to refine the spatial orientation of Gaussians and improve local surface estimation. Additionally, we leverage deep image feature embeddings to enforce cross-view consistency, enhancing the robustness of the learned geometry under varying viewpoints and illumination. Extensive experiments on standard benchmarks demonstrate that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis. The source code is available at https://github.com/LeoQLi/VA-GS.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11456v1": {
    "title": "Coupled Degradation Modeling and Fusion: A VLM-Guided Degradation-Coupled Network for Degradation-Aware Infrared and Visible Image Fusion",
    "url": "https://www.alphaxiv.org/abs/2510.11456v1",
    "arxiv_id": "2510.11456v1",
    "authors": "Tianpei Zhang, Jufeng Zhao, Yiming Zhu, Guangmang Cui",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 14:26:33",
    "ori_summary": "Existing Infrared and Visible Image Fusion (IVIF) methods typically assume high-quality inputs. However, when handing degraded images, these methods heavily rely on manually switching between different pre-processing techniques. This decoupling of degradation handling and image fusion leads to significant performance degradation. In this paper, we propose a novel VLM-Guided Degradation-Coupled Fusion network (VGDCFusion), which tightly couples degradation modeling with the fusion process and leverages vision-language models (VLMs) for degradation-aware perception and guided suppression. Specifically, the proposed Specific-Prompt Degradation-Coupled Extractor (SPDCE) enables modality-specific degradation awareness and establishes a joint modeling of degradation suppression and intra-modal feature extraction. In parallel, the Joint-Prompt Degradation-Coupled Fusion (JPDCF) facilitates cross-modal degradation perception and couples residual degradation filtering with complementary cross-modal feature fusion. Extensive experiments demonstrate that our VGDCFusion significantly outperforms existing state-of-the-art fusion approaches under various degraded image scenarios. Our code is available at https://github.com/Lmmh058/VGDCFusion.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11449v1": {
    "title": "Enhancing Maritime Domain Awareness on Inland Waterways: A YOLO-Based Fusion of Satellite and AIS for Vessel Characterization",
    "url": "https://www.alphaxiv.org/abs/2510.11449v1",
    "arxiv_id": "2510.11449v1",
    "authors": "Geoffery Agorku, Sarah Hernandez, Hayley Hames, Cade Wagner",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 14:19:58",
    "ori_summary": "Maritime Domain Awareness (MDA) for inland waterways remains challenged by cooperative system vulnerabilities. This paper presents a novel framework that fuses high-resolution satellite imagery with vessel trajectory data from the Automatic Identification System (AIS). This work addresses the limitations of AIS-based monitoring by leveraging non-cooperative satellite imagery and implementing a fusion approach that links visual detections with AIS data to identify dark vessels, validate cooperative traffic, and support advanced MDA. The You Only Look Once (YOLO) v11 object detection model is used to detect and characterize vessels and barges by vessel type, barge cover, operational status, barge count, and direction of travel. An annotated data set of 4,550 instances was developed from $5{,}973~\\mathrm{mi}^2$ of Lower Mississippi River imagery. Evaluation on a held-out test set demonstrated vessel classification (tugboat, crane barge, bulk carrier, cargo ship, and hopper barge) with an F1 score of 95.8\\%; barge cover (covered or uncovered) detection yielded an F1 score of 91.6\\%; operational status (staged or in motion) classification reached an F1 score of 99.4\\%. Directionality (upstream, downstream) yielded 93.8\\% accuracy. The barge count estimation resulted in a mean absolute error (MAE) of 2.4 barges. Spatial transferability analysis across geographically disjoint river segments showed accuracy was maintained as high as 98\\%. These results underscore the viability of integrating non-cooperative satellite sensing with AIS fusion. This approach enables near-real-time fleet inventories, supports anomaly detection, and generates high-quality data for inland waterway surveillance. Future work will expand annotated datasets, incorporate temporal tracking, and explore multi-modal deep learning to further enhance operational scalability.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11417v1": {
    "title": "Robust Ego-Exo Correspondence with Long-Term Memory",
    "url": "https://www.alphaxiv.org/abs/2510.11417v1",
    "arxiv_id": "2510.11417v1",
    "authors": "Yijun Hu, Bing Fan, Xin Gu, Haiqing Ren, Dongfang Liu, Heng Fan, Libo Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 13:54:12",
    "ori_summary": "Establishing object-level correspondence between egocentric and exocentric views is essential for intelligent assistants to deliver precise and intuitive visual guidance. However, this task faces numerous challenges, including extreme viewpoint variations, occlusions, and the presence of small objects. Existing approaches usually borrow solutions from video object segmentation models, but still suffer from the aforementioned challenges. Recently, the Segment Anything Model 2 (SAM 2) has shown strong generalization capabilities and excellent performance in video object segmentation. Yet, when simply applied to the ego-exo correspondence (EEC) task, SAM 2 encounters severe difficulties due to ineffective ego-exo feature fusion and limited long-term memory capacity, especially for long videos. Addressing these problems, we propose a novel EEC framework based on SAM 2 with long-term memories by presenting a dual-memory architecture and an adaptive feature routing module inspired by Mixture-of-Experts (MoE). Compared to SAM 2, our approach features (i) a Memory-View MoE module which consists of a dual-branch routing mechanism to adaptively assign contribution weights to each expert feature along both channel and spatial dimensions, and (ii) a dual-memory bank system with a simple yet effective compression strategy to retain critical long-term information while eliminating redundancy. In the extensive experiments on the challenging EgoExo4D benchmark, our method, dubbed LM-EEC, achieves new state-of-the-art results and significantly outperforms existing methods and the SAM 2 baseline, showcasing its strong generalization across diverse scenarios. Our code and model are available at https://github.com/juneyeeHu/LM-EEC.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11387v1": {
    "title": "MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference",
    "url": "https://www.alphaxiv.org/abs/2510.11387v1",
    "arxiv_id": "2510.11387v1",
    "authors": "Wenyuan Zhang, Jimin Tang, Weiqi Zhang, Yi Fang, Yu-Shen Liu, Zhizhong Han",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 13:29:20",
    "ori_summary": "Modeling reflections from 2D images is essential for photorealistic rendering and novel view synthesis. Recent approaches enhance Gaussian primitives with reflection-related material attributes to enable physically based rendering (PBR) with Gaussian Splatting. However, the material inference often lacks sufficient constraints, especially under limited environment modeling, resulting in illumination aliasing and reduced generalization. In this work, we revisit the problem from a multi-view perspective and show that multi-view consistent material inference with more physically-based environment modeling is key to learning accurate reflections with Gaussian Splatting. To this end, we enforce 2D Gaussians to produce multi-view consistent material maps during deferred shading. We also track photometric variations across views to identify highly reflective regions, which serve as strong priors for reflection strength terms. To handle indirect illumination caused by inter-object occlusions, we further introduce an environment modeling strategy through ray tracing with 2DGS, enabling photorealistic rendering of indirect radiance. Experiments on widely used benchmarks show that our method faithfully recovers both illumination and geometry, achieving state-of-the-art rendering quality in novel views synthesis.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11369v1": {
    "title": "Reasoning as Representation: Rethinking Visual Reinforcement Learning in Image Quality Assessment",
    "url": "https://www.alphaxiv.org/abs/2510.11369v1",
    "arxiv_id": "2510.11369v1",
    "authors": "Shijie Zhao, Xuanyu Zhang, Weiqi Li, Junlin Li, Li Zhang, Tianfan Xue, Jian Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 13:11:08",
    "ori_summary": "Reasoning-based image quality assessment (IQA) models trained through reinforcement learning (RL) exhibit exceptional generalization, yet the underlying mechanisms and critical factors driving this capability remain underexplored in current research. Moreover, despite their superior performance, these models incur inference energy usage and latency orders of magnitude higher than their earlier counterparts, restricting their deployment in specific scenarios. Through extensive experiments, this paper verifies and elaborates that through RL training, MLLMs leverage their reasoning capability to convert redundant visual representations into compact, cross-domain aligned text representations. This conversion is precisely the source of the generalization exhibited by these reasoning-based IQA models. Building on this fundamental insight, we propose a novel algorithm, RALI, which employs contrastive learning to directly align images with these generalizable text representations learned by RL. This approach eliminates the reliance on reasoning processes and even obviates the need to load an LLM. For the quality scoring task, this framework achieves generalization performance comparable to reasoning-based models while requiring less than 5% of their model parameters and inference time.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11346v1": {
    "title": "Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.11346v1",
    "arxiv_id": "2510.11346v1",
    "authors": "Joshua Niemeijer, Jan Ehrhardt, Heinz Handels, Hristina Uzunova",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 12:41:28",
    "ori_summary": "Generative Models are a valuable tool for the controlled creation of high-quality image data. Controlled diffusion models like the ControlNet have allowed the creation of labeled distributions. Such synthetic datasets can augment the original training distribution when discriminative models, like semantic segmentation, are trained. However, this augmentation effect is limited since ControlNets tend to reproduce the original training distribution. This work introduces a method to utilize data from unlabeled domains to train ControlNets by introducing the concept of uncertainty into the control mechanism. The uncertainty indicates that a given image was not part of the training distribution of a downstream task, e.g., segmentation. Thus, two types of control are engaged in the final network: an uncertainty control from an unlabeled dataset and a semantic control from the labeled dataset. The resulting ControlNet allows us to create annotated data with high uncertainty from the target domain, i.e., synthetic data from the unlabeled distribution with labels. In our scenario, we consider retinal OCTs, where typically high-quality Spectralis images are available with given ground truth segmentations, enabling the training of segmentation networks. The recent development in Home-OCT devices, however, yields retinal OCTs with lower quality and a large domain shift, such that out-of-the-pocket segmentation networks cannot be applied for this type of data. Synthesizing annotated images from the Home-OCT domain using the proposed approach closes this gap and leads to significantly improved segmentation results without adding any further supervision. The advantage of uncertainty-guidance becomes obvious when compared to style transfer: it enables arbitrary domain shifts without any strict learning of an image style. This is also demonstrated in a traffic scene experiment.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11344v1": {
    "title": "MMAP: A Multi-Magnification and Prototype-Aware Architecture for Predicting Spatial Gene Expression",
    "url": "https://www.alphaxiv.org/abs/2510.11344v1",
    "arxiv_id": "2510.11344v1",
    "authors": "Hai Dang Nguyen, Nguyen Dang Huy Pham, The Minh Duc Nguyen, Dac Thai Nguyen, Hang Thi Nguyen, Duong M. Nguyen",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 12:41:09",
    "ori_summary": "Spatial Transcriptomics (ST) enables the measurement of gene expression while preserving spatial information, offering critical insights into tissue architecture and disease pathology. Recent developments have explored the use of hematoxylin and eosin (H&E)-stained whole-slide images (WSIs) to predict transcriptome-wide gene expression profiles through deep neural networks. This task is commonly framed as a regression problem, where each input corresponds to a localized image patch extracted from the WSI. However, predicting spatial gene expression from histological images remains a challenging problem due to the significant modality gap between visual features and molecular signals. Recent studies have attempted to incorporate both local and global information into predictive models. Nevertheless, existing methods still suffer from two key limitations: (1) insufficient granularity in local feature extraction, and (2) inadequate coverage of global spatial context. In this work, we propose a novel framework, MMAP (Multi-MAgnification and Prototype-enhanced architecture), that addresses both challenges simultaneously. To enhance local feature granularity, MMAP leverages multi-magnification patch representations that capture fine-grained histological details. To improve global contextual understanding, it learns a set of latent prototype embeddings that serve as compact representations of slide-level information. Extensive experimental results demonstrate that MMAP consistently outperforms all existing state-of-the-art methods across multiple evaluation metrics, including Mean Absolute Error (MAE), Mean Squared Error (MSE), and Pearson Correlation Coefficient (PCC).",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11341v1": {
    "title": "InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11341v1",
    "arxiv_id": "2510.11341v1",
    "authors": "Haomin Wang, Jinhui Yin, Qi Wei, Wenguang Zeng, Lixin Gu, Shenglong Ye, Zhangwei Gao, Yaohui Wang, Yanting Zhang, Yuanqi Li, Yanwen Guo, Wenhai Wang, Kai Chen, Yu Qiao, Hongjie Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 12:38:04",
    "ori_summary": "General SVG modeling remains challenging due to fragmented datasets, limited transferability of methods across tasks, and the difficulty of handling structural complexity. In response, we leverage the strong transfer and generalization capabilities of multimodal large language models (MLLMs) to achieve unified modeling for SVG understanding, editing, and generation. We present the InternSVG family, an integrated data-benchmark-model suite. At its core is SAgoge, the largest and most comprehensive multimodal dataset for SVG tasks, encompassing both static graphics and dynamic animations. It covers icons, long-sequence illustrations, scientific diagrams, and dynamic animations, supporting tasks of varied difficulty levels and providing deeper hierarchies with richer attributes compared to previous datasets. Based on this resource, we introduce SArena, a companion benchmark with comprehensive task definitions and standardized evaluation that aligns with the domains and difficulty spectrum covered by SAgoge. Building on these foundations, we propose InternSVG, a unified MLLM for SVG understanding, editing, and generation with SVG-specific special tokens, subword-based embedding initialization, and a two-stage training strategy that progresses from short static SVGs to long-sequence illustrations and complex animations. This unified formulation induces positive transfer and improves overall performance. Experiments on SArena and prior benchmark confirm that InternSVG achieves substantial gains and consistently outperforms leading open and proprietary counterparts.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11340v1": {
    "title": "REACT3D: Recovering Articulations for Interactive Physical 3D Scenes",
    "url": "https://www.alphaxiv.org/abs/2510.11340v1",
    "arxiv_id": "2510.11340v1",
    "authors": "Zhao Huang, Boyang Sun, Alexandros Delitzas, Jiaqi Chen, Marc Pollefeys",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-13 12:37:59",
    "ori_summary": "Interactive 3D scenes are increasingly vital for embodied intelligence, yet existing datasets remain limited due to the labor-intensive process of annotating part segmentation, kinematic types, and motion trajectories. We present REACT3D, a scalable zero-shot framework that converts static 3D scenes into simulation-ready interactive replicas with consistent geometry, enabling direct use in diverse downstream tasks. Our contributions include: (i) openable-object detection and segmentation to extract candidate movable parts from static scenes, (ii) articulation estimation that infers joint types and motion parameters, (iii) hidden-geometry completion followed by interactive object assembly, and (iv) interactive scene integration in widely supported formats to ensure compatibility with standard simulation platforms. We achieve state-of-the-art performance on detection/segmentation and articulation metrics across diverse indoor scenes, demonstrating the effectiveness of our framework and providing a practical foundation for scalable interactive scene generation, thereby lowering the barrier to large-scale research on articulated scene understanding. Our project page is \\textit{\\hypersetup{urlcolor=black}\\href{https://react3d.github.io/}{react3d.github.io}}.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11305v1": {
    "title": "Evaluating the effects of preprocessing, method selection, and hyperparameter tuning on SAR-based flood mapping and water depth estimation",
    "url": "https://www.alphaxiv.org/abs/2510.11305v1",
    "arxiv_id": "2510.11305v1",
    "authors": "Jean-Paul Travert, Cédric Goeury, Sébastien Boyaval, Vito Bacchi, Fabrice Zaoui",
    "categories": "cs.CV, physics.geo-ph",
    "pub_date": "2025-10-13 11:54:42",
    "ori_summary": "Flood mapping and water depth estimation from Synthetic Aperture Radar (SAR) imagery are crucial for calibrating and validating hydraulic models. This study uses SAR imagery to evaluate various preprocessing (especially speckle noise reduction), flood mapping, and water depth estimation methods. The impact of the choice of method at different steps and its hyperparameters is studied by considering an ensemble of preprocessed images, flood maps, and water depth fields. The evaluation is conducted for two flood events on the Garonne River (France) in 2019 and 2021, using hydrodynamic simulations and in-situ observations as reference data. Results show that the choice of speckle filter alters flood extent estimations with variations of several square kilometers. Furthermore, the selection and tuning of flood mapping methods also affect performance. While supervised methods outperformed unsupervised ones, tuned unsupervised approaches (such as local thresholding or change detection) can achieve comparable results. The compounded uncertainty from preprocessing and flood mapping steps also introduces high variability in the water depth field estimates. This study highlights the importance of considering the entire processing pipeline, encompassing preprocessing, flood mapping, and water depth estimation methods and their associated hyperparameters. Rather than relying on a single configuration, adopting an ensemble approach and accounting for methodological uncertainty should be privileged. For flood mapping, the method choice has the most influence. For water depth estimation, the most influential processing step was the flood map input resulting from the flood mapping step and the hyperparameters of the methods.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11303v1": {
    "title": "sketch2symm: Symmetry-aware sketch-to-shape generation via semantic bridging",
    "url": "https://www.alphaxiv.org/abs/2510.11303v1",
    "arxiv_id": "2510.11303v1",
    "authors": "Yan Zhou, Mingji Li, Xiantao Zeng, Jie Lin, Yuexia Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 11:49:45",
    "ori_summary": "Sketch-based 3D reconstruction remains a challenging task due to the abstract and sparse nature of sketch inputs, which often lack sufficient semantic and geometric information. To address this, we propose Sketch2Symm, a two-stage generation method that produces geometrically consistent 3D shapes from sketches. Our approach introduces semantic bridging via sketch-to-image translation to enrich sparse sketch representations, and incorporates symmetry constraints as geometric priors to leverage the structural regularity commonly found in everyday objects. Experiments on mainstream sketch datasets demonstrate that our method achieves superior performance compared to existing sketch-based reconstruction methods in terms of Chamfer Distance, Earth Mover's Distance, and F-Score, verifying the effectiveness of the proposed semantic bridging and symmetry-aware design.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11302v1": {
    "title": "When Does Supervised Training Pay Off? The Hidden Economics of Object Detection in the Era of Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11302v1",
    "arxiv_id": "2510.11302v1",
    "authors": "Samer Al-Hamadani",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-13 11:48:48",
    "ori_summary": "Object detection systems have traditionally relied on supervised learning with manually annotated bounding boxes, achieving high accuracy at the cost of substantial annotation investment. The emergence of Vision-Language Models (VLMs) offers an alternative paradigm enabling zero-shot detection through natural language queries, eliminating annotation requirements but operating with reduced accuracy. This paper presents the first comprehensive cost-effectiveness analysis comparing supervised detection (YOLO) with zero-shot VLM inference (Gemini Flash 2.5). Through systematic evaluation on 1,000 stratified COCO images and 200 diverse product images spanning consumer electronics and rare categories, combined with detailed Total Cost of Ownership modeling, we establish quantitative break-even thresholds governing architecture selection. Our findings reveal that supervised YOLO achieves 91.2% accuracy versus 68.5% for zero-shot Gemini on standard categories, representing a 22.7 percentage point advantage that costs $10,800 in annotation for 100-category systems. However, this advantage justifies investment only beyond 55 million inferences, equivalent to 151,000 images daily for one year. Zero-shot Gemini demonstrates 52.3% accuracy on diverse product categories (ranging from highly web-prevalent consumer electronics at 75-85% to rare specialized equipment at 25-40%) where supervised YOLO achieves 0% due to architectural constraints preventing detection of untrained classes. Cost per Correct Detection analysis reveals substantially lower per-detection costs for Gemini ($0.00050 vs $0.143) at 100,000 inferences despite accuracy deficits. We develop decision frameworks demonstrating that optimal architecture selection depends critically on deployment volume, category stability, budget constraints, and accuracy requirements rather than purely technical performance metrics.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11296v1": {
    "title": "$Δ\\mathrm{Energy}$: Optimizing Energy Change During Vision-Language Alignment Improves both OOD Detection and OOD Generalization",
    "url": "https://www.alphaxiv.org/abs/2510.11296v1",
    "arxiv_id": "2510.11296v1",
    "authors": "Lin Zhu, Yifeng Yang, Xinbing Wang, Qinying Gu, Nanyang Ye",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-13 11:36:58",
    "ori_summary": "Recent approaches for vision-language models (VLMs) have shown remarkable success in achieving fast downstream adaptation. When applied to real-world downstream tasks, VLMs inevitably encounter both the in-distribution (ID) data and out-of-distribution (OOD) data. The OOD datasets often include both covariate shifts (e.g., known classes with changes in image styles) and semantic shifts (e.g., test-time unseen classes). This highlights the importance of improving VLMs' generalization ability to covariate-shifted OOD data, while effectively detecting open-set semantic-shifted OOD classes. In this paper, inspired by the substantial energy change observed in closed-set data when re-aligning vision-language modalities (specifically by directly reducing the maximum cosine similarity to a low value), we introduce a novel OOD score, named {\\Delta}Energy. {\\Delta}Energy significantly outperforms the vanilla energy-based OOD score and provides a more reliable approach for OOD detection. Furthermore, {\\Delta}Energy can simultaneously improve OOD generalization under covariate shifts, which is achieved by lower-bound maximization for {\\Delta}Energy (termed EBM). EBM is theoretically proven to not only enhance OOD detection but also yields a domain-consistent Hessian, which serves as a strong indicator for OOD generalization. Based on this finding, we developed a unified fine-tuning framework that allows for improving VLMs' robustness in both OOD generalization and OOD detection. Extensive experiments on challenging OOD detection and generalization benchmarks demonstrate the superiority of our method, outperforming recent approaches by 10% to 25% in AUROC.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11295v1": {
    "title": "Human Uncertainty-Aware Data Selection and Automatic Labeling in Visual Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.11295v1",
    "arxiv_id": "2510.11295v1",
    "authors": "Jian Lan, Zhicheng Liu, Udo Schlegel, Raoyuan Zhao, Yihong Liu, Hinrich Schütze, Michael A. Hedderich, Thomas Seidl",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 11:35:30",
    "ori_summary": "Large vision-language models (VLMs) achieve strong performance in Visual Question Answering but still rely heavily on supervised fine-tuning (SFT) with massive labeled datasets, which is costly due to human annotations. Crucially, real-world datasets often exhibit human uncertainty (HU) -- variation in human confidence across annotations -- but standard SFT simply optimizes toward the most frequent label, disregarding HU distributions. This leaves two open questions: How does HU affect SFT, and how can HU be effectively leveraged in training? In this work, we first conduct a systematic evaluation of VLMs across varying HU levels. We have two key findings: (i) surprisingly, high-HU samples contribute little or even degrade model performance, and (ii) naively training on the full dataset yields under-calibrated models that fail to capture HU distributions. Motivated by these findings, we introduce HaDola, a human uncertainty-aware data selection and automatic labeling framework. HaDola operates in four stages -- discriminate, self-annotate, error trigger, and training -- to iteratively identify harmful samples, prioritize informative ones, and bootstrap from a small seed set (5\\% of data). Our approach substantially reduces reliance on costly HU annotations and makes VLMs more accurate and better calibrated. Extensive experiments on VQAv2 and VizWiz datasets demonstrate that HaDola consistently matches or outperforms state-of-the-art baselines with less training data. Our work highlights the importance of explicitly modeling HU in SFT, suggesting that better utilization of HU is more effective than merely scaling up dataset size.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11287v1": {
    "title": "EEMS: Edge-Prompt Enhanced Medical Image Segmentation Based on Learnable Gating Mechanism",
    "url": "https://www.alphaxiv.org/abs/2510.11287v1",
    "arxiv_id": "2510.11287v1",
    "authors": "Han Xia, Quanjun Li, Qian Li, Zimeng Li, Hongbin Ye, Yupeng Liu, Haolun Li, Xuhang Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 11:21:57",
    "ori_summary": "Medical image segmentation is vital for diagnosis, treatment planning, and disease monitoring but is challenged by complex factors like ambiguous edges and background noise. We introduce EEMS, a new model for segmentation, combining an Edge-Aware Enhancement Unit (EAEU) and a Multi-scale Prompt Generation Unit (MSPGU). EAEU enhances edge perception via multi-frequency feature extraction, accurately defining boundaries. MSPGU integrates high-level semantic and low-level spatial features using a prompt-guided approach, ensuring precise target localization. The Dual-Source Adaptive Gated Fusion Unit (DAGFU) merges edge features from EAEU with semantic features from MSPGU, enhancing segmentation accuracy and robustness. Tests on datasets like ISIC2018 confirm EEMS's superior performance and reliability as a clinical tool.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11268v1": {
    "title": "Exploring and Leveraging Class Vectors for Classifier Editing",
    "url": "https://www.alphaxiv.org/abs/2510.11268v1",
    "arxiv_id": "2510.11268v1",
    "authors": "Jaeik Kim, Jaeyoung Do",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 10:57:51",
    "ori_summary": "Image classifiers play a critical role in detecting diseases in medical imaging and identifying anomalies in manufacturing processes. However, their predefined behaviors after extensive training make post hoc model editing difficult, especially when it comes to forgetting specific classes or adapting to distribution shifts. Existing classifier editing methods either focus narrowly on correcting errors or incur extensive retraining costs, creating a bottleneck for flexible editing. Moreover, such editing has seen limited investigation in image classification. To overcome these challenges, we introduce Class Vectors, which capture class-specific representation adjustments during fine-tuning. Whereas task vectors encode task-level changes in weight space, Class Vectors disentangle each class's adaptation in the latent space. We show that Class Vectors capture each class's semantic shift and that classifier editing can be achieved either by steering latent features along these vectors or by mapping them into weight space to update the decision boundaries. We also demonstrate that the inherent linearity and orthogonality of Class Vectors support efficient, flexible, and high-level concept editing via simple class arithmetic. Finally, we validate their utility in applications such as unlearning, environmental adaptation, adversarial defense, and adversarial trigger optimization.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11260v1": {
    "title": "A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images",
    "url": "https://www.alphaxiv.org/abs/2510.11260v1",
    "arxiv_id": "2510.11260v1",
    "authors": "Yuxuan Chen, Ruotong Yang, Zhengyang Zhang, Mehreen Ahmed, Yanming Wang",
    "categories": "cs.CV, cond-mat.mtrl-sci, cs.AI, physics.data-an",
    "pub_date": "2025-10-13 10:50:54",
    "ori_summary": "Microscopic characterizations, such as Scanning Electron Microscopy (SEM), are widely used in scientific research for visualizing and analyzing microstructures. Determining the scale bars is an important first step of accurate SEM analysis; however, currently, it mainly relies on manual operations, which is both time-consuming and prone to errors. To address this issue, we propose a multi-modal and automated scale bar detection and extraction framework that provides concurrent object detection, text detection and text recognition with a Large Language Model (LLM) agent. The proposed framework operates in four phases; i) Automatic Dataset Generation (Auto-DG) model to synthesize a diverse dataset of SEM images ensuring robust training and high generalizability of the model, ii) scale bar object detection, iii) information extraction using a hybrid Optical Character Recognition (OCR) system with DenseNet and Convolutional Recurrent Neural Network (CRNN) based algorithms, iv) an LLM agent to analyze and verify accuracy of the results. The proposed model demonstrates a strong performance in object detection and accurate localization with a precision of 100%, recall of 95.8%, and a mean Average Precision (mAP) of 99.2% at IoU=0.5 and 69.1% at IoU=0.5:0.95. The hybrid OCR system achieved 89% precision, 65% recall, and a 75% F1 score on the Auto-DG dataset, significantly outperforming several mainstream standalone engines, highlighting its reliability for scientific image analysis. The LLM is introduced as a reasoning engine as well as an intelligent assistant that suggests follow-up steps and verifies the results. This automated method powered by an LLM agent significantly enhances the efficiency and accuracy of scale bar detection and extraction in SEM images, providing a valuable tool for microscopic analysis and advancing the field of scientific imaging.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11259v1": {
    "title": "DTEA: Dynamic Topology Weaving and Instability-Driven Entropic Attenuation for Medical Image Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.11259v1",
    "arxiv_id": "2510.11259v1",
    "authors": "Weixuan Li, Quanjun Li, Guang Yu, Song Yang, Zimeng Li, Chi-Man Pun, Yupeng Liu, Xuhang Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 10:50:41",
    "ori_summary": "In medical image segmentation, skip connections are used to merge global context and reduce the semantic gap between encoder and decoder. Current methods often struggle with limited structural representation and insufficient contextual modeling, affecting generalization in complex clinical scenarios. We propose the DTEA model, featuring a new skip connection framework with the Semantic Topology Reconfiguration (STR) and Entropic Perturbation Gating (EPG) modules. STR reorganizes multi-scale semantic features into a dynamic hypergraph to better model cross-resolution anatomical dependencies, enhancing structural and semantic representation. EPG assesses channel stability after perturbation and filters high-entropy channels to emphasize clinically important regions and improve spatial attention. Extensive experiments on three benchmark datasets show our framework achieves superior segmentation accuracy and better generalization across various clinical settings. The code is available at \\href{https://github.com/LWX-Research/DTEA}{https://github.com/LWX-Research/DTEA}.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11243v1": {
    "title": "Nepali Sign Language Characters Recognition: Dataset Development and Deep Learning Approaches",
    "url": "https://www.alphaxiv.org/abs/2510.11243v1",
    "arxiv_id": "2510.11243v1",
    "authors": "Birat Poudel, Satyam Ghimire, Sijan Bhattarai, Saurav Bhandari, Suramya Sharma Dahal",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 10:29:08",
    "ori_summary": "Sign languages serve as essential communication systems for individuals with hearing and speech impairments. However, digital linguistic dataset resources for underrepresented sign languages, such as Nepali Sign Language (NSL), remain scarce. This study introduces the first benchmark dataset for NSL, consisting of 36 gesture classes with 1,500 samples per class, designed to capture the structural and visual features of the language. To evaluate recognition performance, we fine-tuned MobileNetV2 and ResNet50 architectures on the dataset, achieving classification accuracies of 90.45% and 88.78%, respectively. These findings demonstrate the effectiveness of convolutional neural networks in sign recognition tasks, particularly within low-resource settings. To the best of our knowledge, this work represents the first systematic effort to construct a benchmark dataset and assess deep learning approaches for NSL recognition, highlighting the potential of transfer learning and fine-tuning for advancing research in underexplored sign languages.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11232v1": {
    "title": "LightPneumoNet: Lightweight Pneumonia Classifier",
    "url": "https://www.alphaxiv.org/abs/2510.11232v1",
    "arxiv_id": "2510.11232v1",
    "authors": "Neilansh Chauhan, Piyush Kumar Gupta, Faraz Doja",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-13 10:14:17",
    "ori_summary": "Effective pneumonia diagnosis is often challenged by the difficulty of deploying large, computationally expensive deep learning models in resource-limited settings. This study introduces LightPneumoNet, an efficient, lightweight convolutional neural network (CNN) built from scratch to provide an accessible and accurate diagnostic solution for pneumonia detection from chest X-rays. Our model was trained on a public dataset of 5,856 chest X-ray images. Preprocessing included image resizing to 224x224, grayscale conversion, and pixel normalization, with data augmentation (rotation, zoom, shear) to prevent overfitting. The custom architecture features four blocks of stacked convolutional layers and contains only 388,082 trainable parameters, resulting in a minimal 1.48 MB memory footprint. On the independent test set, our model delivered exceptional performance, achieving an overall accuracy of 0.942, precision of 0.92, and an F1-Score of 0.96. Critically, it obtained a sensitivity (recall) of 0.99, demonstrating a near-perfect ability to identify true pneumonia cases and minimize clinically significant false negatives. Notably, LightPneumoNet achieves this high recall on the same dataset where existing approaches typically require significantly heavier architectures or fail to reach comparable sensitivity levels. The model's efficiency enables deployment on low-cost hardware, making advanced computer-aided diagnosis accessible in underserved clinics and serving as a reliable second-opinion tool to improve patient outcomes.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11223v1": {
    "title": "Investigating Identity Signals in Conversational Facial Dynamics via Disentangled Expression Features",
    "url": "https://www.alphaxiv.org/abs/2510.11223v1",
    "arxiv_id": "2510.11223v1",
    "authors": "Masoumeh Chapariniya, Pierre Vuillecard, Jean-Marc Odobez, Volker Dellwo, Teodora Vukovic",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 10:06:25",
    "ori_summary": "This work investigates whether individuals can be identified solely through the pure dynamical components of their facial expressions, independent of static facial appearance. We leverage the FLAME 3D morphable model to achieve explicit disentanglement between facial shape and expression dynamics, extracting frame-by-frame parameters from conversational videos while retaining only expression and jaw coefficients. On the CANDOR dataset of 1,429 speakers in naturalistic conversations, our Conformer model with supervised contrastive learning achieves 61.14\\%accuracy on 1,429-way classification -- 458 times above chance -- demonstrating that facial dynamics carry strong identity signatures. We introduce a drift-to-noise ratio (DNR) that quantifies the reliability of shape expression separation by measuring across-session shape changes relative to within-session variability. DNR strongly negatively correlates with recognition performance, confirming that unstable shape estimation compromises dynamic identification. Our findings reveal person-specific signatures in conversational facial dynamics, with implications for social perception and clinical assessment.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11204v1": {
    "title": "Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos",
    "url": "https://www.alphaxiv.org/abs/2510.11204v1",
    "arxiv_id": "2510.11204v1",
    "authors": "Rohit Gupta, Anirban Roy, Claire Christensen, Sujeong Kim, Sarah Gerard, Madeline Cincebeaux, Ajay Divakaran, Todd Grindal, Mubarak Shah",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 09:36:26",
    "ori_summary": "The recent growth in the consumption of online media by children during early childhood necessitates data-driven tools enabling educators to filter out appropriate educational content for young learners. This paper presents an approach for detecting educational content in online videos. We focus on two widely used educational content classes: literacy and math. For each class, we choose prominent codes (sub-classes) based on the Common Core Standards. For example, literacy codes include `letter names', `letter sounds', and math codes include `counting', `sorting'. We pose this as a fine-grained multilabel classification problem as videos can contain multiple types of educational content and the content classes can get visually similar (e.g., `letter names' vs `letter sounds'). We propose a novel class prototypes based supervised contrastive learning approach that can handle fine-grained samples associated with multiple labels. We learn a class prototype for each class and a loss function is employed to minimize the distances between a class prototype and the samples from the class. Similarly, distances between a class prototype and the samples from other classes are maximized. As the alignment between visual and audio cues are crucial for effective comprehension, we consider a multimodal transformer network to capture the interaction between visual and audio cues in videos while learning the embedding for videos. For evaluation, we present a dataset, APPROVE, employing educational videos from YouTube labeled with fine-grained education classes by education researchers. APPROVE consists of 193 hours of expert-annotated videos with 19 classes. The proposed approach outperforms strong baselines on APPROVE and other benchmarks such as Youtube-8M, and COIN. The dataset is available at https://github.com/rohit-gupta/MMContrast/tree/main/APPROVE",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11190v1": {
    "title": "FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11190v1",
    "arxiv_id": "2510.11190v1",
    "authors": "Shengming Yuan, Xinyu Lyu, Shuailong Wang, Beitao Chen, Jingkuan Song, Lianli Gao",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 09:22:12",
    "ori_summary": "Multimodal large language models (MLLMs) face an inherent trade-off between faithfulness and creativity, as different tasks require varying degrees of associative reasoning. However, existing methods lack the flexibility to modulate this reasoning strength, limiting MLLMs' adaptability across factual and creative scenarios. To bridge this gap, we propose equipping MLLMs with mechanisms that enable flexible control over associative reasoning. We begin by investigating the internal mechanisms underlying associative behavior in MLLMs and find that: (1) middle layers play a pivotal role in shaping model's associative tendencies, (2) modifying representations in these layers effectively regulates associative reasoning strength, and (3) hallucinations can be exploited to derive steering vectors that guide this modulation. Building on these findings, we introduce Flexible Association Control (FlexAC), a lightweight and training-free framework for modulating associative behavior in MLLMs. FlexAC first induces hallucination-guided intermediate representations to encode associative directions. Then, it selects high-association instances to construct effective associative steering vectors, whose strengths are adaptively calibrated to balance creative guidance with output stability. Finally, recognizing the multi-dimensional nature of associative reasoning, FlexAC incorporates task-specific associative vectors derived from a forward pass on a few target-domain samples, enabling models to follow diverse associative directions and better adapt to creative tasks. Notably, our method achieves up to a 5.8x improvement in creativity on Creation-MMBench and a 29% reduction in hallucination rate on CHAIR, surpassing existing baselines and demonstrating its effectiveness in enabling flexible control over associative reasoning in MLLMs. Our code is available at https://github.com/ylhz/FlexAC.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11183v1": {
    "title": "Saudi Sign Language Translation Using T5",
    "url": "https://www.alphaxiv.org/abs/2510.11183v1",
    "arxiv_id": "2510.11183v1",
    "authors": "Ali Alhejab, Tomas Zelezny, Lamya Alkanhal, Ivan Gruber, Yazeed Alharbi, Jakub Straka, Vaclav Javorek, Marek Hruz, Badriah Alkalifah, Ahmed Ali",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 09:18:34",
    "ori_summary": "This paper explores the application of T5 models for Saudi Sign Language (SSL) translation using a novel dataset. The SSL dataset includes three challenging testing protocols, enabling comprehensive evaluation across different scenarios. Additionally, it captures unique SSL characteristics, such as face coverings, which pose challenges for sign recognition and translation. In our experiments, we investigate the impact of pre-training on American Sign Language (ASL) data by comparing T5 models pre-trained on the YouTubeASL dataset with models trained directly on the SSL dataset. Experimental results demonstrate that pre-training on YouTubeASL significantly improves models' performance (roughly $3\\times$ in BLEU-4), indicating cross-linguistic transferability in sign language models. Our findings highlight the benefits of leveraging large-scale ASL data to improve SSL translation and provide insights into the development of more effective sign language translation systems. Our code is publicly available at our GitHub repository.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11182v1": {
    "title": "Generalisation of automatic tumour segmentation in histopathological whole-slide images across multiple cancer types",
    "url": "https://www.alphaxiv.org/abs/2510.11182v1",
    "arxiv_id": "2510.11182v1",
    "authors": "Ole-Johan Skrede, Manohar Pradhan, Maria Xepapadakis Isaksen, Tarjei Sveinsgjerd Hveem, Ljiljana Vlatkovic, Arild Nesbakken, Kristina Lindemann, Gunnar B Kristensen, Jenneke Kasius, Alain G Zeimet, Odd Terje Brustugun, Lill-Tove Rasmussen Busund, Elin H Richardsen, Erik Skaaheim Haug, Bjørn Brennhovd, Emma Rewcastle, Melinda Lillesand, Vebjørn Kvikstad, Emiel Janssen, David J Kerr, Knut Liestøl, Fritz Albregtsen, Andreas Kleppe",
    "categories": "eess.IV, cs.AI, cs.CV",
    "pub_date": "2025-10-13 09:18:15",
    "ori_summary": "Deep learning is expected to aid pathologists by automating tasks such as tumour segmentation. We aimed to develop one universal tumour segmentation model for histopathological images and examine its performance in different cancer types. The model was developed using over 20 000 whole-slide images from over 4 000 patients with colorectal, endometrial, lung, or prostate carcinoma. Performance was validated in pre-planned analyses on external cohorts with over 3 000 patients across six cancer types. Exploratory analyses included over 1 500 additional patients from The Cancer Genome Atlas. Average Dice coefficient was over 80% in all validation cohorts with en bloc resection specimens and in The Cancer Genome Atlas cohorts. No loss of performance was observed when comparing the universal model with models specialised on single cancer types. In conclusion, extensive and rigorous evaluations demonstrate that generic tumour segmentation by a single model is possible across cancer types, patient populations, sample preparations, and slide scanners.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11178v1": {
    "title": "BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11178v1",
    "arxiv_id": "2510.11178v1",
    "authors": "Bryan Chen Zhengyu Tan, Zheng Weihua, Zhengyuan Liu, Nancy F. Chen, Hwaran Lee, Kenny Tsu Wei Choo, Roy Ka-Wei Lee",
    "categories": "cs.CV, cs.CY",
    "pub_date": "2025-10-13 09:10:05",
    "ori_summary": "As vision-language models (VLMs) are deployed globally, their ability to understand culturally situated knowledge becomes essential. Yet, existing evaluations largely assess static recall or isolated visual grounding, leaving unanswered whether VLMs possess robust and transferable cultural understanding. We introduce BLEnD-Vis, a multimodal, multicultural benchmark designed to evaluate the robustness of everyday cultural knowledge in VLMs across linguistic rephrasings and visual modalities. Building on the BLEnD dataset, BLEnD-Vis constructs 313 culturally grounded question templates spanning 16 regions and generates three aligned multiple-choice formats: (i) a text-only baseline querying from Region $\\to$ Entity, (ii) an inverted text-only variant (Entity $\\to$ Region), and (iii) a VQA-style version of (ii) with generated images. The resulting benchmark comprises 4,916 images and over 21,000 multiple-choice question (MCQ) instances, validated through human annotation. BLEnD-Vis reveals significant fragility in current VLM cultural knowledge; models exhibit performance drops under linguistic rephrasing and, whilst visual cues often aid performance, low cross-modal consistency highlights challenges in robustly integrating textual and visual understanding, particularly for lower-resource regions. BLEnD-Vis thus provides a crucial testbed for systematically analysing cultural robustness and multimodal grounding, exposing limitations and guiding the development of more culturally competent VLMs.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11176v1": {
    "title": "G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.11176v1",
    "arxiv_id": "2510.11176v1",
    "authors": "Yesung Cho, Sungmin Lee, Geongyu Lee, Minkyung Lee, Jongbae Park, Dongmyung Shin",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 09:08:59",
    "ori_summary": "Recent studies in pathology foundation models have shown that scaling training data, diversifying cancer types, and increasing model size consistently improve their performance. However, giga-scale foundation models, which are trained on hundreds of thousands of slides covering tens of cancer types and contain billions of parameters, pose significant challenges for practical use due to their tremendous computational costs in both development and deployment. In this work, we present a novel strategy, named the G2L framework, to increase the performance of large-scale foundation models, which consist of only $15\\%$ of the parameters of giga-scale models, to a comparable performance level of giga-scale models in cancer-specific tasks. Our approach applies knowledge distillation, transferring the capabilities of a giga-scale model to a large-scale model, using just 1K pathology slides of a target cancer (e.g., breast, prostate, etc.). The resulting distilled model not only outperformed state-of-the-art models of the same size (i.e., large-scale) across several benchmarks but also, interestingly, surpassed the giga-scale teacher and huge-scale models in some benchmarks. In addition, the distilled model exhibited a higher robustness index, indicating improved resilience to image variations originating from multiple institutions. These findings suggest that the proposed distillation approach for a large-scale model is a data- and parameter-efficient way to achieve giga-scale-level performance for cancer-specific applications without prohibitive computational burden.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11175v1": {
    "title": "Reliable Cross-modal Alignment via Prototype Iterative Construction",
    "url": "https://www.alphaxiv.org/abs/2510.11175v1",
    "arxiv_id": "2510.11175v1",
    "authors": "Xiang Ma, Litian Xu, Lexin Fang, Caiming Zhang, Lizhen Cui",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 09:08:27",
    "ori_summary": "Cross-modal alignment is an important multi-modal task, aiming to bridge the semantic gap between different modalities. The most reliable fundamention for achieving this objective lies in the semantic consistency between matched pairs. Conventional methods implicitly assume embeddings contain solely semantic information, ignoring the impact of non-semantic information during alignment, which inevitably leads to information bias or even loss. These non-semantic information primarily manifest as stylistic variations in the data, which we formally define as style information. An intuitive approach is to separate style from semantics, aligning only the semantic information. However, most existing methods distinguish them based on feature columns, which cannot represent the complex coupling relationship between semantic and style information. In this paper, we propose PICO, a novel framework for suppressing style interference during embedding interaction. Specifically, we quantify the probability of each feature column representing semantic information, and regard it as the weight during the embedding interaction. To ensure the reliability of the semantic probability, we propose a prototype iterative construction method. The key operation of this method is a performance feedback-based weighting function, and we have theoretically proven that the function can assign higher weight to prototypes that bring higher performance improvements. Extensive experiments on various benchmarks and model backbones demonstrate the superiority of PICO, outperforming state-of-the-art methods by 5.2\\%-14.1\\%.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11173v1": {
    "title": "CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.11173v1",
    "arxiv_id": "2510.11173v1",
    "authors": "Zhenyu Lu, Liupeng Li, Jinpeng Wang, Yan Feng, Bin Chen, Ke Chen, Yaowei Wang",
    "categories": "cs.CV, cs.MM",
    "pub_date": "2025-10-13 09:07:54",
    "ori_summary": "Existing works on reasoning segmentation either connect hidden features from a language model directly to a mask decoder or represent positions in text, which limits interpretability and semantic detail. To solve this, we present CoPRS, a Multi-modal Chain-of-Thought (MCoT)-based positional perception model that bridges language reasoning to segmentation through a differentiable and interpretable positional prior instantiated as a heatmap. By making the reasoning process clear via MCoT and expressing it as a dense, differentiable heatmap, this interface enhances interpretability and diagnostic analysis and yields more concentrated evidence on the target. A learnable concentration token aggregates features of the image and reasoning text to generate this positional prior, which is decoded to precise masks through a lightweight decoder, providing a direct connection between reasoning and segmentation. Across the RefCOCO series and ReasonSeg, CoPRS matches or surpasses the best reported metrics on each standard split under comparable protocols, with performance at or above prior state of the art across both validation and test partitions. Extensive experiments reveal that the quality of the heatmap strongly influences the resulting mask quality, supporting a consistent association between the reasoning output and downstream mask generation. Collectively, these findings support the utility of this paradigm in bridging reasoning and segmentation and show advantages in concentration driven by reasoning and predicting masks more precisely. Code, checkpoints and logs are released at https://github.com/ZhenyuLU-Heliodore/CoPRS.git.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11171v1": {
    "title": "Multiview Manifold Evidential Fusion for PolSAR Image Classification",
    "url": "https://www.alphaxiv.org/abs/2510.11171v1",
    "arxiv_id": "2510.11171v1",
    "authors": "Junfei Shi, Haojia Zhang, Haiyan Jin, Junhuai Li, Xiaogang Song, Yuanfan Guo, Haonan Su, Weisi Lin",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 09:05:51",
    "ori_summary": "Polarimetric Synthetic Aperture Radar (PolSAR) covariance matrices and their extracted multi-features - such as scattering angle, entropy, texture, and boundary descriptors - provide complementary and physically interpretable information for image classification. Traditional fusion strategies typically concatenate these features or employ deep learning networks to combine them. However, the covariance matrices and multi-features, as two complementary views, lie on different manifolds with distinct geometric structures. Existing fusion methods also overlook the varying importance of different views and ignore uncertainty, often leading to unreliable predictions. To address these issues, we propose a Multiview Manifold Evidential Fusion (MMEFnet) method to effectively fuse these two views. It gives a new framework to integrate PolSAR manifold learning and evidence fusion into a unified architecture. Specifically, covariance matrices are represented on the Hermitian Positive Definite (HPD) manifold, while multi-features are modeled on the Grassmann manifold. Two different kernel metric learning networks are constructed to learn their manifold representations. Subsequently, a trusted multiview evidence fusion, replacing the conventional softmax classifier, estimates belief mass and quantifies the uncertainty of each view from the learned deep features. Finally, a Dempster-Shafer theory-based fusion strategy combines evidence, enabling a more reliable and interpretable classification. Extensive experiments on three real-world PolSAR datasets demonstrate that the proposed method consistently outperforms existing approaches in accuracy, robustness, and interpretability.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11142v1": {
    "title": "Validation of an Artificial Intelligence Tool for the Detection of Sperm DNA Fragmentation Using the TUNEL In Situ Hybridization Assay",
    "url": "https://www.alphaxiv.org/abs/2510.11142v1",
    "arxiv_id": "2510.11142v1",
    "authors": "Byron Alexander Jacobs, Aqeel Morris, Ifthakaar Shaik, Frando Lin",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 08:32:11",
    "ori_summary": "Sperm DNA fragmentation (SDF) is a critical parameter in male fertility assessment that conventional semen analysis fails to evaluate. This study presents the validation of a novel artificial intelligence (AI) tool designed to detect SDF through digital analysis of phase contrast microscopy images, using the terminal deoxynucleotidyl transferase dUTP nick end labeling (TUNEL) assay as the gold standard reference. Utilising the established link between sperm morphology and DNA integrity, the present work proposes a morphology assisted ensemble AI model that combines image processing techniques with state-of-the-art transformer based machine learning models (GC-ViT) for the prediction of DNA fragmentation in sperm from phase contrast images. The ensemble model is benchmarked against a pure transformer `vision' model as well as a `morphology-only` model. Promising results show the proposed framework is able to achieve sensitivity of 60\\% and specificity of 75\\%. This non-destructive methodology represents a significant advancement in reproductive medicine by enabling real-time sperm selection based on DNA integrity for clinical diagnostic and therapeutic applications.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11129v1": {
    "title": "video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via Memory",
    "url": "https://www.alphaxiv.org/abs/2510.11129v1",
    "arxiv_id": "2510.11129v1",
    "authors": "Guangzhi Sun, Yixuan Li, Xiaodong Wu, Yudong Yang, Wei Li, Zejun Ma, Chao Zhang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 08:20:15",
    "ori_summary": "Continuous, high-frame-rate, high-resolution processing of long video streams is critical for future AI agents, yet current video-understanding LLMs struggle to scale. Offline, fixed-frame-number methods require the stream length to adapt frame rates; streaming methods constrain memory by merging or discarding tokens, losing information. We propose video-SALMONN S, a streaming audio-visual LLM that, to our knowledge, is the first to process 3-hour videos at 1 FPS and 360p resolution under a fixed memory budget. Our model introduces (i) a test-time-training (TTT) memory module that continually updates token representations to capture long-range dependencies by replacing token merging, and (ii) a prompt-dependent memory reader that selectively retrieves context-relevant content from fixed-size memory. The TTT module is optimised with a Hessian-free conjugate-gradient procedure (TTT_HF) for efficient adaptation. On long-video benchmarks (Video-MME, LVBench, VideoEvalPro), video-SALMONN S sustains high-quality understanding on multi-hour videos with 10k frames and 1M tokens. Our 8B-parameter model achieves 74.2% overall and 67.8% on the Video-MME long split, outperforming both offline and streaming baselines.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11128v1": {
    "title": "Lightweight Facial Landmark Detection in Thermal Images via Multi-Level Cross-Modal Knowledge Transfer",
    "url": "https://www.alphaxiv.org/abs/2510.11128v1",
    "arxiv_id": "2510.11128v1",
    "authors": "Qiyi Tong, Olivia Nocentini, Marta Lagomarsino, Kuanqi Cai, Marta Lorenzini, Arash Ajoudani",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-13 08:19:56",
    "ori_summary": "Facial Landmark Detection (FLD) in thermal imagery is critical for applications in challenging lighting conditions, but it is hampered by the lack of rich visual cues. Conventional cross-modal solutions, like feature fusion or image translation from RGB data, are often computationally expensive or introduce structural artifacts, limiting their practical deployment. To address this, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a novel framework that decouples high-fidelity RGB-to-thermal knowledge transfer from model compression to create both accurate and efficient thermal FLD models. A central challenge during knowledge transfer is the profound modality gap between RGB and thermal data, where traditional unidirectional distillation fails to enforce semantic consistency across disparate feature spaces. To overcome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a bidirectional mechanism designed specifically for this task. DIKD establishes a connection between modalities: it not only guides the thermal student with rich RGB features but also validates the student's learned representations by feeding them back into the frozen teacher's prediction head. This closed-loop supervision forces the student to learn modality-invariant features that are semantically aligned with the teacher, ensuring a robust and profound knowledge transfer. Experiments show that our approach sets a new state-of-the-art on public thermal FLD benchmarks, notably outperforming previous methods while drastically reducing computational overhead.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11117v1": {
    "title": "Demystifying Numerosity in Diffusion Models -- Limitations and Remedies",
    "url": "https://www.alphaxiv.org/abs/2510.11117v1",
    "arxiv_id": "2510.11117v1",
    "authors": "Yaqi Zhao, Xiaochen Wang, Li Dong, Wentao Zhang, Yuhui Yuan",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 08:07:24",
    "ori_summary": "Numerosity remains a challenge for state-of-the-art text-to-image generation models like FLUX and GPT-4o, which often fail to accurately follow counting instructions in text prompts. In this paper, we aim to study a fundamental yet often overlooked question: Can diffusion models inherently generate the correct number of objects specified by a textual prompt simply by scaling up the dataset and model size? To enable rigorous and reproducible evaluation, we construct a clean synthetic numerosity benchmark comprising two complementary datasets: GrayCount250 for controlled scaling studies, and NaturalCount6 featuring complex naturalistic scenes. Second, we empirically show that the scaling hypothesis does not hold: larger models and datasets alone fail to improve counting accuracy on our benchmark. Our analysis identifies a key reason: diffusion models tend to rely heavily on the noise initialization rather than the explicit numerosity specified in the prompt. We observe that noise priors exhibit biases toward specific object counts. In addition, we propose an effective strategy for controlling numerosity by injecting count-aware layout information into the noise prior. Our method achieves significant gains, improving accuracy on GrayCount250 from 20.0\\% to 85.3\\% and on NaturalCount6 from 74.8\\% to 86.3\\%, demonstrating effective generalization across settings.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11115v1": {
    "title": "Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning",
    "url": "https://www.alphaxiv.org/abs/2510.11115v1",
    "arxiv_id": "2510.11115v1",
    "authors": "Hao Tang, Shengfeng He, Jing Qin",
    "categories": "cs.CV, cs.MM",
    "pub_date": "2025-10-13 08:06:23",
    "ori_summary": "Few-shot learning (FSL) addresses the challenge of classifying novel classes with limited training samples. While some methods leverage semantic knowledge from smaller-scale models to mitigate data scarcity, these approaches often introduce noise and bias due to the data's inherent simplicity. In this paper, we propose a novel framework, Synergistic Knowledge Transfer (SynTrans), which effectively transfers diverse and complementary knowledge from large multimodal models to empower the off-the-shelf few-shot learner. Specifically, SynTrans employs CLIP as a robust teacher and uses a few-shot vision encoder as a weak student, distilling semantic-aligned visual knowledge via an unsupervised proxy task. Subsequently, a training-free synergistic knowledge mining module facilitates collaboration among large multimodal models to extract high-quality semantic knowledge. Building upon this, a visual-semantic bridging module enables bi-directional knowledge transfer between visual and semantic spaces, transforming explicit visual and implicit semantic knowledge into category-specific classifier weights. Finally, SynTrans introduces a visual weight generator and a semantic weight reconstructor to adaptively construct optimal multimodal FSL classifiers. Experimental results on four FSL datasets demonstrate that SynTrans, even when paired with a simple few-shot vision encoder, significantly outperforms current state-of-the-art methods.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11112v1": {
    "title": "Multimodal Disease Progression Modeling via Spatiotemporal Disentanglement and Multiscale Alignment",
    "url": "https://www.alphaxiv.org/abs/2510.11112v1",
    "arxiv_id": "2510.11112v1",
    "authors": "Chen Liu, Wenfang Yao, Kejing Yin, William K. Cheung, Jing Qin",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 08:02:36",
    "ori_summary": "Longitudinal multimodal data, including electronic health records (EHR) and sequential chest X-rays (CXRs), is critical for modeling disease progression, yet remains underutilized due to two key challenges: (1) redundancy in consecutive CXR sequences, where static anatomical regions dominate over clinically-meaningful dynamics, and (2) temporal misalignment between sparse, irregular imaging and continuous EHR data. We introduce $\\texttt{DiPro}$, a novel framework that addresses these challenges through region-aware disentanglement and multi-timescale alignment. First, we disentangle static (anatomy) and dynamic (pathology progression) features in sequential CXRs, prioritizing disease-relevant changes. Second, we hierarchically align these static and dynamic CXR features with asynchronous EHR data via local (pairwise interval-level) and global (full-sequence) synchronization to model coherent progression pathways. Extensive experiments on the MIMIC dataset demonstrate that $\\texttt{DiPro}$ could effectively extract temporal clinical dynamics and achieve state-of-the-art performance on both disease progression identification and general ICU prediction tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11107v1": {
    "title": "MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps",
    "url": "https://www.alphaxiv.org/abs/2510.11107v1",
    "arxiv_id": "2510.11107v1",
    "authors": "Jiahui Lei, Kyle Genova, George Kopanas, Noah Snavely, Leonidas Guibas",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 07:56:19",
    "ori_summary": "This paper addresses the challenge of learning semantically and functionally meaningful 3D motion priors from real-world videos, in order to enable prediction of future 3D scene motion from a single input image. We propose a novel pixel-aligned Motion Map (MoMap) representation for 3D scene motion, which can be generated from existing generative image models to facilitate efficient and effective motion prediction. To learn meaningful distributions over motion, we create a large-scale database of MoMaps from over 50,000 real videos and train a diffusion model on these representations. Our motion generation not only synthesizes trajectories in 3D but also suggests a new pipeline for 2D video synthesis: first generate a MoMap, then warp an image accordingly and complete the warped point-based renderings. Experimental results demonstrate that our approach generates plausible and semantically consistent 3D scene motion.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11106v1": {
    "title": "Compositional Zero-Shot Learning: A Survey",
    "url": "https://www.alphaxiv.org/abs/2510.11106v1",
    "arxiv_id": "2510.11106v1",
    "authors": "Ans Munir, Faisal Z. Qureshi, Mohsen Ali, Muhammad Haris Khan",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 07:54:47",
    "ori_summary": "Compositional Zero-Shot Learning (CZSL) is a critical task in computer vision that enables models to recognize unseen combinations of known attributes and objects during inference, addressing the combinatorial challenge of requiring training data for every possible composition. This is particularly challenging because the visual appearance of primitives is highly contextual; for example, ``small'' cats appear visually distinct from ``older'' ones, and ``wet'' cars differ significantly from ``wet'' cats. Effectively modeling this contextuality and the inherent compositionality is crucial for robust compositional zero-shot recognition. This paper presents, to our knowledge, the first comprehensive survey specifically focused on Compositional Zero-Shot Learning. We systematically review the state-of-the-art CZSL methods, introducing a taxonomy grounded in disentanglement, with four families of approaches: no explicit disentanglement, textual disentanglement, visual disentanglement, and cross-modal disentanglement. We provide a detailed comparative analysis of these methods, highlighting their core advantages and limitations in different problem settings, such as closed-world and open-world CZSL. Finally, we identify the most significant open challenges and outline promising future research directions. This survey aims to serve as a foundational resource to guide and inspire further advancements in this fascinating and important field. Papers studied in this survey with their official code are available on our github: https://github.com/ans92/Compositional-Zero-Shot-Learning",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11096v1": {
    "title": "CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification and Prompt Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.11096v1",
    "arxiv_id": "2510.11096v1",
    "authors": "Fengling Zhu, Boshi Liu, Jingyu Hua, Sheng Zhong",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 07:44:54",
    "ori_summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in tasks such as image captioning, visual question answering, and cross-modal reasoning by integrating visual and textual modalities. However, their multimodal nature also exposes them to adversarial threats, where attackers can perturb either modality or both jointly to induce harmful, misleading, or policy violating outputs. Existing defense strategies, such as adversarial training and input purification, face notable limitations: adversarial training typically improves robustness only against known attacks while incurring high computational costs, whereas conventional purification approaches often suffer from degraded image quality and insufficient generalization to complex multimodal tasks. In this work, we focus on defending the visual modality, which frequently serves as the primary entry point for adversarial manipulation. We propose a supervised diffusion based denoising framework that leverages paired adversarial clean image datasets to fine-tune diffusion models with directional, task specific guidance. Unlike prior unsupervised purification methods such as DiffPure, our approach achieves higher quality reconstructions while significantly improving defense robustness in multimodal tasks. Furthermore, we incorporate prompt optimization as a complementary defense mechanism, enhancing resistance against diverse and unseen attack strategies. Extensive experiments on image captioning and visual question answering demonstrate that our method not only substantially improves robustness but also exhibits strong transferability to unknown adversarial attacks. These results highlight the effectiveness of supervised diffusion based denoising for multimodal defense, paving the way for more reliable and secure deployment of MLLMs in real world applications.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11092v1": {
    "title": "Future-Aware End-to-End Driving: Bidirectional Modeling of Trajectory Planning and Scene Evolution",
    "url": "https://www.alphaxiv.org/abs/2510.11092v1",
    "arxiv_id": "2510.11092v1",
    "authors": "Bozhou Zhang, Nan Song, Jingyu Li, Xiatian Zhu, Jiankang Deng, Li Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 07:41:47",
    "ori_summary": "End-to-end autonomous driving methods aim to directly map raw sensor inputs to future driving actions such as planned trajectories, bypassing traditional modular pipelines. While these approaches have shown promise, they often operate under a one-shot paradigm that relies heavily on the current scene context, potentially underestimating the importance of scene dynamics and their temporal evolution. This limitation restricts the model's ability to make informed and adaptive decisions in complex driving scenarios. We propose a new perspective: the future trajectory of an autonomous vehicle is closely intertwined with the evolving dynamics of its environment, and conversely, the vehicle's own future states can influence how the surrounding scene unfolds. Motivated by this bidirectional relationship, we introduce SeerDrive, a novel end-to-end framework that jointly models future scene evolution and trajectory planning in a closed-loop manner. Our method first predicts future bird's-eye view (BEV) representations to anticipate the dynamics of the surrounding scene, then leverages this foresight to generate future-context-aware trajectories. Two key components enable this: (1) future-aware planning, which injects predicted BEV features into the trajectory planner, and (2) iterative scene modeling and vehicle planning, which refines both future scene prediction and trajectory generation through collaborative optimization. Extensive experiments on the NAVSIM and nuScenes benchmarks show that SeerDrive significantly outperforms existing state-of-the-art methods.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11091v1": {
    "title": "Text-Enhanced Panoptic Symbol Spotting in CAD Drawings",
    "url": "https://www.alphaxiv.org/abs/2510.11091v1",
    "arxiv_id": "2510.11091v1",
    "authors": "Xianlin Liu, Yan Gong, Bohao Li, Jiajing Huang, Bowen Du, Junchen Ye, Liyan Xu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 07:41:15",
    "ori_summary": "With the widespread adoption of Computer-Aided Design(CAD) drawings in engineering, architecture, and industrial design, the ability to accurately interpret and analyze these drawings has become increasingly critical. Among various subtasks, panoptic symbol spotting plays a vital role in enabling downstream applications such as CAD automation and design retrieval. Existing methods primarily focus on geometric primitives within the CAD drawings to address this task, but they face following major problems: they usually overlook the rich textual annotations present in CAD drawings and they lack explicit modeling of relationships among primitives, resulting in incomprehensive understanding of the holistic drawings. To fill this gap, we propose a panoptic symbol spotting framework that incorporates textual annotations. The framework constructs unified representations by jointly modeling geometric and textual primitives. Then, using visual features extract by pretrained CNN as the initial representations, a Transformer-based backbone is employed, enhanced with a type-aware attention mechanism to explicitly model the different types of spatial dependencies between various primitives. Extensive experiments on the real-world dataset demonstrate that the proposed method outperforms existing approaches on symbol spotting tasks involving textual annotations, and exhibits superior robustness when applied to complex CAD drawings.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11090v1": {
    "title": "Source-Free Object Detection with Detection Transformer",
    "url": "https://www.alphaxiv.org/abs/2510.11090v1",
    "arxiv_id": "2510.11090v1",
    "authors": "Huizai Yao, Sicheng Zhao, Shuo Lu, Hui Chen, Yangyang Li, Guoping Liu, Tengfei Xing, Chenggang Yan, Jianhua Tao, Guiguang Ding",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 07:35:04",
    "ori_summary": "Source-Free Object Detection (SFOD) enables knowledge transfer from a source domain to an unsupervised target domain for object detection without access to source data. Most existing SFOD approaches are either confined to conventional object detection (OD) models like Faster R-CNN or designed as general solutions without tailored adaptations for novel OD architectures, especially Detection Transformer (DETR). In this paper, we introduce Feature Reweighting ANd Contrastive Learning NetworK (FRANCK), a novel SFOD framework specifically designed to perform query-centric feature enhancement for DETRs. FRANCK comprises four key components: (1) an Objectness Score-based Sample Reweighting (OSSR) module that computes attention-based objectness scores on multi-scale encoder feature maps, reweighting the detection loss to emphasize less-recognized regions; (2) a Contrastive Learning with Matching-based Memory Bank (CMMB) module that integrates multi-level features into memory banks, enhancing class-wise contrastive learning; (3) an Uncertainty-weighted Query-fused Feature Distillation (UQFD) module that improves feature distillation through prediction quality reweighting and query feature fusion; and (4) an improved self-training pipeline with a Dynamic Teacher Updating Interval (DTUI) that optimizes pseudo-label quality. By leveraging these components, FRANCK effectively adapts a source-pre-trained DETR model to a target domain with enhanced robustness and generalization. Extensive experiments on several widely used benchmarks demonstrate that our method achieves state-of-the-art performance, highlighting its effectiveness and compatibility with DETR-based SFOD models.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11073v1": {
    "title": "ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer",
    "url": "https://www.alphaxiv.org/abs/2510.11073v1",
    "arxiv_id": "2510.11073v1",
    "authors": "Yuan Tian, Min Zhou, Yitong Chen, Fang Li, Lingzi Qi, Shuo Wang, Xieyang Xu, Yu Yu, Shiqiong Xu, Chaoyu Lei, Yankai Jiang, Rongzhao Zhang, Jia Tan, Li Wu, Hong Chen, Xiaowei Liu, Wei Lu, Lin Li, Huifang Zhou, Xuefei Song, Guangtao Zhai, Xianqun Fan",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 07:12:23",
    "ori_summary": "Patient face images provide a convenient mean for evaluating eye diseases, while also raising privacy concerns. Here, we introduce ROFI, a deep learning-based privacy protection framework for ophthalmology. Using weakly supervised learning and neural identity translation, ROFI anonymizes facial features while retaining disease features (over 98\\% accuracy, $\\kappa > 0.90$). It achieves 100\\% diagnostic sensitivity and high agreement ($\\kappa > 0.90$) across eleven eye diseases in three cohorts, anonymizing over 95\\% of images. ROFI works with AI systems, maintaining original diagnoses ($\\kappa > 0.80$), and supports secure image reversal (over 98\\% similarity), enabling audits and long-term care. These results show ROFI's effectiveness of protecting patient privacy in the digital medicine era.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11063v1": {
    "title": "LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.11063v1",
    "arxiv_id": "2510.11063v1",
    "authors": "Chang Liu, Henghui Ding, Kaining Ying, Lingyi Hong, Ning Xu, Linjie Yang, Yuchen Fan, Mingqi Gao, Jingkun Chen, Yunqi Miao, Gengshen Wu, Zhijin Qin, Jungong Han, Zhixiong Zhang, Shuangrui Ding, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Chang Soo Lim, Joonyoung Moon, Donghyeon Cho, Tingmin Li, Yixuan Li, Yang Yang, An Yan, Leilei Cao, Feng Lu, Ran Hong, Youhai Jiang, Fengjie Zhu, Yujie Xie, Hongyang Zhang, Zhihui Liu, Shihai Ruan, Quanzhu Niu, Dengxian Gong, Shihao Chen, Tao Zhang, Yikang Zhou, Haobo Yuan, Lu Qi, Xiangtai Li, Shunping Ji, Ran Hong, Feng Lu, Leilei Cao, An Yan, Alexey Nekrasov, Ali Athar, Daan de Geus, Alexander Hermans, Bastian Leibe",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 07:02:09",
    "ori_summary": "This report presents an overview of the 7th Large-scale Video Object Segmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the two traditional tracks of LSVOS that jointly target robustness in realistic video scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition features a newly introduced track, Complex VOS (MOSEv2). Building upon prior insights, MOSEv2 substantially increases difficulty, introducing more challenging but realistic scenarios including denser small objects, frequent disappear/reappear events, severe occlusions, adverse weather and lighting, etc., pushing long-term consistency and generalization beyond curated benchmarks. The challenge retains standard ${J}$, $F$, and ${J\\&F}$ metrics for VOS and RVOS, while MOSEv2 adopts ${J\\&\\dot{F}}$ as the primary ranking metric to better evaluate objects across scales and disappearance cases. We summarize datasets and protocols, highlight top-performing solutions, and distill emerging trends, such as the growing role of LLM/MLLM components and memory-aware propagation, aiming to chart future directions for resilient, language-aware video segmentation in the wild.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11050v1": {
    "title": "Zero-shot Face Editing via ID-Attribute Decoupled Inversion",
    "url": "https://www.alphaxiv.org/abs/2510.11050v1",
    "arxiv_id": "2510.11050v1",
    "authors": "Yang Hou, Minggu Wang, Jianjun Zhao",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 06:34:40",
    "ori_summary": "Recent advancements in text-guided diffusion models have shown promise for general image editing via inversion techniques, but often struggle to maintain ID and structural consistency in real face editing tasks. To address this limitation, we propose a zero-shot face editing method based on ID-Attribute Decoupled Inversion. Specifically, we decompose the face representation into ID and attribute features, using them as joint conditions to guide both the inversion and the reverse diffusion processes. This allows independent control over ID and attributes, ensuring strong ID preservation and structural consistency while enabling precise facial attribute manipulation. Our method supports a wide range of complex multi-attribute face editing tasks using only text prompts, without requiring region-specific input, and operates at a speed comparable to DDIM inversion. Comprehensive experiments demonstrate its practicality and effectiveness.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11047v1": {
    "title": "Benchmarking Deep Learning Models for Laryngeal Cancer Staging Using the LaryngealCT Dataset",
    "url": "https://www.alphaxiv.org/abs/2510.11047v1",
    "arxiv_id": "2510.11047v1",
    "authors": "Nivea Roy, Son Tran, Atul Sajjanhar, K. Devaraja, Prakashini Koteshwara, Yong Xiang, Divya Rao",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 06:25:19",
    "ori_summary": "Laryngeal cancer imaging research lacks standardised datasets to enable reproducible deep learning (DL) model development. We present LaryngealCT, a curated benchmark of 1,029 computed tomography (CT) scans aggregated from six collections from The Cancer Imaging Archive (TCIA). Uniform 1 mm isotropic volumes of interest encompassing the larynx were extracted using a weakly supervised parameter search framework validated by clinical experts. 3D DL architectures (3D CNN, ResNet18,50,101, DenseNet121) were benchmarked on (i) early (Tis,T1,T2) vs. advanced (T3,T4) and (ii) T4 vs. non-T4 classification tasks. 3D CNN (AUC-0.881, F1-macro-0.821) and ResNet18 (AUC-0.892, F1-macro-0.646) respectively outperformed the other models in the two tasks. Model explainability assessed using 3D GradCAMs with thyroid cartilage overlays revealed greater peri-cartilage attention in non-T4 cases and focal activations in T4 predictions. Through open-source data, pretrained models, and integrated explainability tools, LaryngealCT offers a reproducible foundation for AI-driven research to support clinical decisions in laryngeal oncology.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11028v1": {
    "title": "Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with Cascaded Prompts",
    "url": "https://www.alphaxiv.org/abs/2510.11028v1",
    "arxiv_id": "2510.11028v1",
    "authors": "Yanning Hou, Ke Xu, Junfa Li, Yanran Ruan, Jianfeng Qiu",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 05:53:49",
    "ori_summary": "Recently, the powerful generalization ability exhibited by foundation models has brought forth new solutions for zero-shot anomaly segmentation tasks. However, guiding these foundation models correctly to address downstream tasks remains a challenge. This paper proposes a novel two-stage framework, for zero-shot anomaly segmentation tasks in industrial anomaly detection. This framework excellently leverages the powerful anomaly localization capability of CLIP and the boundary perception ability of SAM.(1) To mitigate SAM's inclination towards object segmentation, we propose the Co-Feature Point Prompt Generation (PPG) module. This module collaboratively utilizes CLIP and SAM to generate positive and negative point prompts, guiding SAM to focus on segmenting anomalous regions rather than the entire object. (2) To further optimize SAM's segmentation results and mitigate rough boundaries and isolated noise, we introduce the Cascaded Prompts for SAM (CPS) module. This module employs hybrid prompts cascaded with a lightweight decoder of SAM, achieving precise segmentation of anomalous regions. Across multiple datasets, consistent experimental validation demonstrates that our approach achieves state-of-the-art zero-shot anomaly segmentation results. Particularly noteworthy is our performance on the Visa dataset, where we outperform the state-of-the-art methods by 10.3\\% and 7.7\\% in terms of {$F_1$-max} and AP metrics, respectively.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11027v1": {
    "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.11027v1",
    "arxiv_id": "2510.11027v1",
    "authors": "Ganlin Yang, Tianyi Zhang, Haoran Hao, Weiyun Wang, Yibin Liu, Dehui Wang, Guanzhou Chen, Zijian Cai, Junting Chen, Weijie Su, Wengang Zhou, Yu Qiao, Jifeng Dai, Jiangmiao Pang, Gen Luo, Wenhai Wang, Yao Mu, Zhi Hou",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 05:51:22",
    "ori_summary": "While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11026v1": {
    "title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.11026v1",
    "arxiv_id": "2510.11026v1",
    "authors": "Hongxiang Li, Yaowei Li, Bin Lin, Yuwei Niu, Yuhang Yang, Xiaoshuang Huang, Jiayin Cai, Xiaolong Jiang, Yao Hu, Long Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 05:50:44",
    "ori_summary": "Unified multimodal models integrate the reasoning capacity of large language models with both image understanding and generation, showing great promise for advanced multimodal intelligence. However, the community still lacks a rigorous reasoning-centric benchmark to systematically evaluate the alignment between understanding and generation, and their generalization potential in complex visual tasks. To this end, we introduce \\textbf{GIR-Bench}, a comprehensive benchmark that evaluates unified models across three complementary perspectives. Firstly, we investigate understanding-generation consistency (GIR-Bench-UGC), asking whether models can consistently leverage the same knowledge in both understanding and generation tasks. Secondly, we investigate whether models can perform reasoning-centric text-to-image generation that requires applying logical constraints and implicit knowledge to generate faithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models can handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset, we carefully design different task-specific evaluation pipelines tailored for each task. This enables fine-grained and interpretable evaluation while mitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive ablations over various unified models and generation-only systems have shown that: Although unified models are more capable of reasoning-driven visual tasks, they still exhibit a persistent gap between understanding and generation. The data and code for GIR-Bench are available at \\href{https://hkust-longgroup.github.io/GIR-Bench}{https://hkust-longgroup.github.io/GIR-Bench}.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11020v1": {
    "title": "GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via Cross-Modal Reward for Auxiliary Line Creation",
    "url": "https://www.alphaxiv.org/abs/2510.11020v1",
    "arxiv_id": "2510.11020v1",
    "authors": "Shasha Guo, Liang Pang, Xi Wang, Yanling Wang, Huawei Shen, Jing Zhang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-13 05:33:51",
    "ori_summary": "Auxiliary lines are essential for solving complex geometric problems but remain challenging for large vision-language models (LVLMs). Rather than editing diagrams to draw auxiliary lines, which current image editing models struggle to render with geometric precision, we generate textual descriptions of auxiliary-line constructions to better align with the representational strengths of LVLMs. To bridge the gap between textual descriptions and spatial structure, we propose a reinforcement learning framework that enhances diagram-text alignment. At the core of our approach is a cross-modal reward that evaluates how well the generated auxiliary-line description for an original diagram matches a ground-truth auxiliary-line diagram. Built on this reward, we present GeoVLMath, an open-source LVLM tailored to auxiliary-line reasoning in solid geometry. This fine-grained signal drives a GRPO-based RL stage, yielding precise diagram-text alignment. To support training, we develop a scalable data creation pipeline and construct AuxSolidMath, a dataset of 3,018 real-exam geometry problems with paired diagrams and aligned textual fields. At the 3B and 7B scales, GeoVLMath achieves competitive and often superior performance compared with strong open-source and proprietary LVLMs on auxiliary-line reasoning benchmarks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11018v1": {
    "title": "The Easy Path to Robustness: Coreset Selection using Sample Hardness",
    "url": "https://www.alphaxiv.org/abs/2510.11018v1",
    "arxiv_id": "2510.11018v1",
    "authors": "Pranav Ramesh, Arjun Roy, Deepak Ravikumar, Kaushik Roy, Gopalakrishnan Srinivasan",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-13 05:28:16",
    "ori_summary": "Designing adversarially robust models from a data-centric perspective requires understanding which input samples are most crucial for learning resilient features. While coreset selection provides a mechanism for efficient training on data subsets, current algorithms are designed for clean accuracy and fall short in preserving robustness. To address this, we propose a framework linking a sample's adversarial vulnerability to its \\textit{hardness}, which we quantify using the average input gradient norm (AIGN) over training. We demonstrate that \\textit{easy} samples (with low AIGN) are less vulnerable and occupy regions further from the decision boundary. Leveraging this insight, we present EasyCore, a coreset selection algorithm that retains only the samples with low AIGN for training. We empirically show that models trained on EasyCore-selected data achieve significantly higher adversarial accuracy than those trained with competing coreset methods under both standard and adversarial training. As AIGN is a model-agnostic dataset property, EasyCore is an efficient and widely applicable data-centric method for improving adversarial robustness. We show that EasyCore achieves up to 7\\% and 5\\% improvement in adversarial accuracy under standard training and TRADES adversarial training, respectively, compared to existing coreset methods.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11017v1": {
    "title": "High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation",
    "url": "https://www.alphaxiv.org/abs/2510.11017v1",
    "arxiv_id": "2510.11017v1",
    "authors": "Runyang Feng, Hyung Jin Chang, Tze Ho Elden Tse, Boeun Kim, Yi Chang, Yixing Gao",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 05:18:27",
    "ori_summary": "Modeling high-resolution spatiotemporal representations, including both global dynamic contexts (e.g., holistic human motion tendencies) and local motion details (e.g., high-frequency changes of keypoints), is essential for video-based human pose estimation (VHPE). Current state-of-the-art methods typically unify spatiotemporal learning within a single type of modeling structure (convolution or attention-based blocks), which inherently have difficulties in balancing global and local dynamic modeling and may bias the network to one of them, leading to suboptimal performance. Moreover, existing VHPE models suffer from quadratic complexity when capturing global dependencies, limiting their applicability especially for high-resolution sequences. Recently, the state space models (known as Mamba) have demonstrated significant potential in modeling long-range contexts with linear complexity; however, they are restricted to 1D sequential data. In this paper, we present a novel framework that extends Mamba from two aspects to separately learn global and local high-resolution spatiotemporal representations for VHPE. Specifically, we first propose a Global Spatiotemporal Mamba, which performs 6D selective space-time scan and spatial- and temporal-modulated scan merging to efficiently extract global representations from high-resolution sequences. We further introduce a windowed space-time scan-based Local Refinement Mamba to enhance the high-frequency details of localized keypoint motions. Extensive experiments on four benchmark datasets demonstrate that the proposed model outperforms state-of-the-art VHPE approaches while achieving better computational trade-offs.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11014v1": {
    "title": "Into the Unknown: Towards using Generative Models for Sampling Priors of Environment Uncertainty for Planning in Configuration Spaces",
    "url": "https://www.alphaxiv.org/abs/2510.11014v1",
    "arxiv_id": "2510.11014v1",
    "authors": "Subhransu S. Bhattacharjee, Hao Lu, Dylan Campbell, Rahul Shome",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2025-10-13 05:08:48",
    "ori_summary": "Priors are vital for planning under partial observability, yet difficult to obtain in practice. We present a sampling-based pipeline that leverages large-scale pretrained generative models to produce probabilistic priors capturing environmental uncertainty and spatio-semantic relationships in a zero-shot manner. Conditioned on partial observations, the pipeline recovers complete RGB-D point cloud samples with occupancy and target semantics, formulated to be directly useful in configuration-space planning. We establish a Matterport3D benchmark of rooms partially visible through doorways, where a robot must navigate to an unobserved target object. Effective priors for this setting must represent both occupancy and target-location uncertainty in unobserved regions. Experiments show that our approach recovers commonsense spatial semantics consistent with ground truth, yielding diverse, clean 3D point clouds usable in motion planning, highlight the promise of generative models as a rich source of priors for robotic planning.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11012v1": {
    "title": "COCO-Tree: Compositional Hierarchical Concept Trees for Enhanced Reasoning in Vision Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.11012v1",
    "arxiv_id": "2510.11012v1",
    "authors": "Sanchit Sinha, Guangzhi Xiong, Aidong Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 05:07:13",
    "ori_summary": "Compositional reasoning remains a persistent weakness of modern vision language models (VLMs): they often falter when a task hinges on understanding how multiple objects, attributes, and relations interact within an image. Multiple research works have attempted to improve compositionality performance by creative tricks such as improving prompt structure, chain of thought reasoning, etc. A more recent line of work attempts to impart additional reasoning in VLMs using well-trained Large Language Models (LLMs), which are far superior in linguistic understanding than VLMs to compensate for the limited linguistic prowess of VLMs. However, these approaches are either resource-intensive or do not provide an interpretable reasoning process. In this paper, we present 'COCO-Tree' - a novel approach that augments VLM outputs with carefully designed neurosymbolic concept trees learned from LLMs to improve VLM's linguistic reasoning. COCO-Tree's beam search-inspired reasoning process boosts compositionality performance and provides a rationale behind VLM predictions. Empirical results on four compositionality benchmarks, Winoground, EqBench, ColorSwap, and SugarCrepe, in seven different open-source VLMs with varying sizes, demonstrate that COCO-Tree significantly improves compositional generalization by 5-10% over baselines.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11005v1": {
    "title": "Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.11005v1",
    "arxiv_id": "2510.11005v1",
    "authors": "Kai Han, Siqi Ma, Chengxuan Qian, Jun Chen, Chongwen Lyu, Yuqing Song, Zhe Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 04:44:43",
    "ori_summary": "Accurate segmentation of tumors and adjacent normal tissues in medical images is essential for surgical planning and tumor staging. Although foundation models generally perform well in segmentation tasks, they often struggle to focus on foreground areas in complex, low-contrast backgrounds, where some malignant tumors closely resemble normal organs, complicating contextual differentiation. To address these challenges, we propose the Foreground-Aware Spectrum Segmentation (FASS) framework. First, we introduce a foreground-aware module to amplify the distinction between background and the entire volume space, allowing the model to concentrate more effectively on target areas. Next, a feature-level frequency enhancement module, based on wavelet transform, extracts discriminative high-frequency features to enhance boundary recognition and detail perception. Eventually, we introduce an edge constraint module to preserve geometric continuity in segmentation boundaries. Extensive experiments on multiple medical datasets demonstrate superior performance across all metrics, validating the effectiveness of our framework, particularly in robustness under complex conditions and fine structure recognition. Our framework significantly enhances segmentation of low-contrast images, paving the way for applications in more diverse and complex medical imaging scenarios.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.11000v1": {
    "title": "ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation",
    "url": "https://www.alphaxiv.org/abs/2510.11000v1",
    "arxiv_id": "2510.11000v1",
    "authors": "Ruihang Xu, Dewei Zhou, Fan Ma, Yi Yang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 04:21:19",
    "ori_summary": "Multi-instance image generation (MIG) remains a significant challenge for modern diffusion models due to key limitations in achieving precise control over object layout and preserving the identity of multiple distinct subjects. To address these limitations, we introduce ContextGen, a novel Diffusion Transformer framework for multi-instance generation that is guided by both layout and reference images. Our approach integrates two key technical contributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates the composite layout image into the generation context to robustly anchor the objects in their desired positions, and Identity Consistency Attention (ICA), an innovative attention mechanism that leverages contextual reference images to ensure the identity consistency of multiple instances. Recognizing the lack of large-scale, hierarchically-structured datasets for this task, we introduce IMIG-100K, the first dataset with detailed layout and identity annotations. Extensive experiments demonstrate that ContextGen sets a new state-of-the-art, outperforming existing methods in control precision, identity fidelity, and overall visual quality.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10993v1": {
    "title": "Perspective-aware 3D Gaussian Inpainting with Multi-view Consistency",
    "url": "https://www.alphaxiv.org/abs/2510.10993v1",
    "arxiv_id": "2510.10993v1",
    "authors": "Yuxin Cheng, Binxiao Huang, Taiqiang Wu, Wenyong Zhou, Chenchen Ding, Zhengwu Liu, Graziano Chesi, Ngai Wong",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 04:10:39",
    "ori_summary": "3D Gaussian inpainting, a critical technique for numerous applications in virtual reality and multimedia, has made significant progress with pretrained diffusion models. However, ensuring multi-view consistency, an essential requirement for high-quality inpainting, remains a key challenge. In this work, we present PAInpainter, a novel approach designed to advance 3D Gaussian inpainting by leveraging perspective-aware content propagation and consistency verification across multi-view inpainted images. Our method iteratively refines inpainting and optimizes the 3D Gaussian representation with multiple views adaptively sampled from a perspective graph. By propagating inpainted images as prior information and verifying consistency across neighboring views, PAInpainter substantially enhances global consistency and texture fidelity in restored 3D scenes. Extensive experiments demonstrate the superiority of PAInpainter over existing methods. Our approach achieves superior 3D inpainting quality, with PSNR scores of 26.03 dB and 29.51 dB on the SPIn-NeRF and NeRFiller datasets, respectively, highlighting its effectiveness and generalization capability.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10986v1": {
    "title": "Mixup Helps Understanding Multimodal Video Better",
    "url": "https://www.alphaxiv.org/abs/2510.10986v1",
    "arxiv_id": "2510.10986v1",
    "authors": "Xiaoyu Ma, Ding Ding, Hao Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 03:53:25",
    "ori_summary": "Multimodal video understanding plays a crucial role in tasks such as action recognition and emotion classification by combining information from different modalities. However, multimodal models are prone to overfitting strong modalities, which can dominate learning and suppress the contributions of weaker ones. To address this challenge, we first propose Multimodal Mixup (MM), which applies the Mixup strategy at the aggregated multimodal feature level to mitigate overfitting by generating virtual feature-label pairs. While MM effectively improves generalization, it treats all modalities uniformly and does not account for modality imbalance during training. Building on MM, we further introduce Balanced Multimodal Mixup (B-MM), which dynamically adjusts the mixing ratios for each modality based on their relative contributions to the learning objective. Extensive experiments on several datasets demonstrate the effectiveness of our methods in improving generalization and multimodal robustness.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10980v1": {
    "title": "On the Optimal Representation Efficiency of Barlow Twins: An Information-Geometric Interpretation",
    "url": "https://www.alphaxiv.org/abs/2510.10980v1",
    "arxiv_id": "2510.10980v1",
    "authors": "Di Zhang",
    "categories": "cs.LG, cs.CV, cs.IT, math.IT, math.ST, stat.ML, stat.TH, 68T07, 62B11, 94A17, 53B12, I.2.6; I.5.1; G.3; H.1.1",
    "pub_date": "2025-10-13 03:41:27",
    "ori_summary": "Self-supervised learning (SSL) has achieved remarkable success by learning meaningful representations without labeled data. However, a unified theoretical framework for understanding and comparing the efficiency of different SSL paradigms remains elusive. In this paper, we introduce a novel information-geometric framework to quantify representation efficiency. We define representation efficiency $\\eta$ as the ratio between the effective intrinsic dimension of the learned representation space and its ambient dimension, where the effective dimension is derived from the spectral properties of the Fisher Information Matrix (FIM) on the statistical manifold induced by the encoder. Within this framework, we present a theoretical analysis of the Barlow Twins method. Under specific but natural assumptions, we prove that Barlow Twins achieves optimal representation efficiency ($\\eta = 1$) by driving the cross-correlation matrix of representations towards the identity matrix, which in turn induces an isotropic FIM. This work provides a rigorous theoretical foundation for understanding the effectiveness of Barlow Twins and offers a new geometric perspective for analyzing SSL algorithms.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10973v1": {
    "title": "Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.10973v1",
    "arxiv_id": "2510.10973v1",
    "authors": "Sanchit Sinha, Oana Frunza, Kashif Rasul, Yuriy Nevmyvaka, Aidong Zhang",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-13 03:25:35",
    "ori_summary": "The capabilities of Large Vision-Language Models (LVLMs) have reached state-of-the-art on many visual reasoning tasks, including chart reasoning, yet they still falter on out-of-distribution (OOD) data, and degrade further when asked to produce their chain-of-thought (CoT) rationales, limiting explainability. We present Chart-RVR, a general framework that fine-tunes LVLMs to be more robust and explainable for chart reasoning by coupling Group Relative Policy Optimization (GRPO) with automatically verifiable rewards. Our framework comprises of three rewards that maximize: (i) correct chart-type classification, (ii) faithful chart table reconstruction, and (iii) process conformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently outperforms standard supervised fine-tuning (SFT) on both in-distribution and out-of-distribution datasets, closing the OOD performance gap while improving rationale fidelity. The resulting models, the Chart-RVR-3B series, achieve state-of-the-art results on six chart-reasoning benchmarks spanning in-domain and OOD settings, surpassing all existing models of comparable size. Beyond accuracy, Chart-RVR yields more interpretable CoT rationales, strengthening trust and reliability - showcasing the power of verifiable rewards with GRPO for training reliable, interpretable chart-reasoning models.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10969v1": {
    "title": "IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation",
    "url": "https://www.alphaxiv.org/abs/2510.10969v1",
    "arxiv_id": "2510.10969v1",
    "authors": "Zeteng Lin, Xingxing Li, Wen You, Xiaoyang Li, Zehan Lu, Yujun Cai, Jing Tang",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 03:19:45",
    "ori_summary": "Existing vision language models (VLMs), including GPT-4 and DALL-E, often struggle to preserve logic, object identity, and style in multimodal image-text generation. This limitation significantly hinders the generalization capability of VLMs in complex image-text input-output scenarios. To address this issue, we propose IUT-Plug, a module grounded in an Image Understanding Tree (IUT), which enhances existing interleaved VLMs through explicit structured reasoning, thereby mitigating context drift in logic, entity identity, and style. The proposed framework operates in two stages. (1) A dynamic IUT-Plug extraction module parses visual scenes into hierarchical symbolic structures. (2) A coordinated narrative-flow and image synthesis mechanism ensures cross-modal consistency. To evaluate our approach, we construct a novel benchmark based on 3,000 real human-generated question-answer pairs over fine-tuned large models, introducing a dynamic evaluation protocol for quantifying context drift in interleaved VLMs. Experimental results demonstrate that IUT-Plug not only improves accuracy on established benchmarks but also effectively alleviates the three critical forms of context drift across diverse multimodal question answering (QA) scenarios.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10954v1": {
    "title": "Comparative Evaluation of Neural Network Architectures for Generalizable Human Spatial Preference Prediction in Unseen Built Environments",
    "url": "https://www.alphaxiv.org/abs/2510.10954v1",
    "arxiv_id": "2510.10954v1",
    "authors": "Maral Doctorarastoo, Katherine A. Flanigan, Mario Bergés, Christopher McComb",
    "categories": "cs.CE, cs.CV, cs.LG, cs.MA",
    "pub_date": "2025-10-13 03:04:48",
    "ori_summary": "The capacity to predict human spatial preferences within built environments is instrumental for developing Cyber-Physical-Social Infrastructure Systems (CPSIS). A significant challenge in this domain is the generalizability of preference models, particularly their efficacy in predicting preferences within environmental configurations not encountered during training. While deep learning models have shown promise in learning complex spatial and contextual dependencies, it remains unclear which neural network architectures are most effective at generalizing to unseen layouts. To address this, we conduct a comparative study of Graph Neural Networks, Convolutional Neural Networks, and standard feedforward Neural Networks using synthetic data generated from a simplified and synthetic pocket park environment. Beginning with this illustrative case study, allows for controlled analysis of each model's ability to transfer learned preference patterns to unseen spatial scenarios. The models are evaluated based on their capacity to predict preferences influenced by heterogeneous physical, environmental, and social features. Generalizability score is calculated using the area under the precision-recall curve for the seen and unseen layouts. This generalizability score is appropriate for imbalanced data, providing insights into the suitability of each neural network architecture for preference-aware human behavior modeling in unseen built environments.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10947v1": {
    "title": "Towards Distribution-Shift Uncertainty Estimation for Inverse Problems with Generative Priors",
    "url": "https://www.alphaxiv.org/abs/2510.10947v1",
    "arxiv_id": "2510.10947v1",
    "authors": "Namhoon Kim, Sara Fridovich-Keil",
    "categories": "cs.CV",
    "pub_date": "2025-10-13 02:58:26",
    "ori_summary": "Generative models have shown strong potential as data-driven priors for solving inverse problems such as reconstructing medical images from undersampled measurements. While these priors improve reconstruction quality with fewer measurements, they risk hallucinating features when test images lie outside the training distribution. Existing uncertainty quantification methods in this setting (i) require an in-distribution calibration dataset, which may not be available, (ii) provide heuristic rather than statistical estimates, or (iii) quantify uncertainty from model capacity or limited measurements rather than distribution shift. We propose an instance-level, calibration-free uncertainty indicator that is sensitive to distribution shift, requires no knowledge of the training distribution, and incurs no retraining cost. Our key hypothesis is that reconstructions of in-distribution images remain stable under random measurement variations, while reconstructions of out-of-distribution (OOD) images exhibit greater instability. We use this stability as a proxy for detecting distribution shift. Our proposed OOD indicator is efficiently computable for any computational imaging inverse problem; we demonstrate it on tomographic reconstruction of MNIST digits, where a learned proximal network trained only on digit \"0\" is evaluated on all ten digits. Reconstructions of OOD digits show higher variability and correspondingly higher reconstruction error, validating this indicator. These results suggest a deployment strategy that pairs generative priors with lightweight guardrails, enabling aggressive measurement reduction for in-distribution cases while automatically warning when priors are applied out of distribution.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.10933v1": {
    "title": "DKPMV: Dense Keypoints Fusion from Multi-View RGB Frames for 6D Pose Estimation of Textureless Objects",
    "url": "https://www.alphaxiv.org/abs/2510.10933v1",
    "arxiv_id": "2510.10933v1",
    "authors": "Jiahong Chen, Jinghao Wang, Zi Wang, Ziwen Wang, Banglei Guan, Qifeng Yu",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-13 02:45:55",
    "ori_summary": "6D pose estimation of textureless objects is valuable for industrial robotic applications, yet remains challenging due to the frequent loss of depth information. Current multi-view methods either rely on depth data or insufficiently exploit multi-view geometric cues, limiting their performance. In this paper, we propose DKPMV, a pipeline that achieves dense keypoint-level fusion using only multi-view RGB images as input. We design a three-stage progressive pose optimization strategy that leverages dense multi-view keypoint geometry information. To enable effective dense keypoint fusion, we enhance the keypoint network with attentional aggregation and symmetry-aware training, improving prediction accuracy and resolving ambiguities on symmetric objects. Extensive experiments on the ROBI dataset demonstrate that DKPMV outperforms state-of-the-art multi-view RGB approaches and even surpasses the RGB-D methods in the majority of cases. The code will be available soon.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12801v1": {
    "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search",
    "url": "https://www.alphaxiv.org/abs/2510.12801v1",
    "arxiv_id": "2510.12801v1",
    "authors": "Kartik Narayan, Yang Xu, Tian Cao, Kavya Nerella, Vishal M. Patel, Navid Shiee, Peter Grasch, Chao Jia, Yinfei Yang, Zhe Gan",
    "categories": "cs.CV, cs.IR",
    "pub_date": "2025-10-14 17:59:58",
    "ori_summary": "Multimodal Large Language Models (MLLMs) in real-world applications require access to external knowledge sources and must remain responsive to the dynamic and ever-changing real-world information in order to address information-seeking and knowledge-intensive user queries. Existing approaches, such as retrieval augmented generation (RAG) methods, search agents, and search equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and poorly constructed search queries, which result in inefficiencies and suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1, the first multimodal LLM capable of performing on-demand, multi-turn web searches and dynamically crafting queries for both image and text search tools. Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops of the input image making the image search more effective, and can iteratively adapt text search queries based on retrieved information, thereby enabling self-reflection and self-correction. Our approach relies on a two-stage training pipeline: a cold start supervised finetuning phase followed by an online reinforcement learning optimization. For training, we introduce DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated pipeline intermixed with real-world information from web search tools. This dataset contains diverse, multi-hop queries that integrate textual and visual information, teaching the model when to search, what to search for, which search tool to use and how to reason over the retrieved information. We conduct extensive experiments across a range of knowledge-intensive benchmarks to demonstrate the superiority of our approach. Finally, we analyze the results and provide insights that are valuable for advancing multimodal web-search.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12742v1": {
    "title": "CTRL-Rec: Controlling Recommender Systems With Natural Language",
    "url": "https://www.alphaxiv.org/abs/2510.12742v1",
    "arxiv_id": "2510.12742v1",
    "authors": "Micah Carroll, Adeline Foote, Kevin Feng, Marcus Williams, Anca Dragan, W. Bradley Knox, Smitha Milli",
    "categories": "cs.AI, cs.IR",
    "pub_date": "2025-10-14 17:20:04",
    "ori_summary": "When users are dissatisfied with recommendations from a recommender system, they often lack fine-grained controls for changing them. Large language models (LLMs) offer a solution by allowing users to guide their recommendations through natural language requests (e.g., \"I want to see respectful posts with a different perspective than mine\"). We propose a method, CTRL-Rec, that allows for natural language control of traditional recommender systems in real-time with computational efficiency. Specifically, at training time, we use an LLM to simulate whether users would approve of items based on their language requests, and we train embedding models that approximate such simulated judgments. We then integrate these user-request-based predictions into the standard weighting of signals that traditional recommender systems optimize. At deployment time, we require only a single LLM embedding computation per user request, allowing for real-time control of recommendations. In experiments with the MovieLens dataset, our method consistently allows for fine-grained control across a diversity of requests. In a study with 19 Letterboxd users, we find that CTRL-Rec was positively received by users and significantly enhanced users' sense of control and satisfaction with recommendations compared to traditional controls.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12709v1": {
    "title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model",
    "url": "https://www.alphaxiv.org/abs/2510.12709v1",
    "arxiv_id": "2510.12709v1",
    "authors": "Lin Lin, Jiefeng Long, Zhihe Wan, Yuchi Wang, Dingkang Yang, Shuang Yang, Yueyang Yao, Xu Chen, Zirui Guo, Shengqiang Li, Weiran Li, Hanyu Li, Yaling Mou, Yan Qiu, Haiyang Yu, Xiao Liang, Hongsheng Li, Chao Feng",
    "categories": "cs.IR, cs.CV",
    "pub_date": "2025-10-14 16:43:22",
    "ori_summary": "Multimodal embedding models aim to yield informative unified representations that empower diverse cross-modal tasks. Despite promising developments in the evolution from CLIP-based dual-tower architectures to large vision-language models, prior works still face unavoidable challenges in real-world applications and business scenarios, such as the limited modality support, unstable training mechanisms, and industrial domain gaps. In this work, we introduce SAIL-Embedding, an omni-modal embedding foundation model that addresses these issues through tailored training strategies and architectural design. In the optimization procedure, we propose a multi-stage training scheme to boost the multifaceted effectiveness of representation learning. Specifically, the content-aware progressive training aims to enhance the model's adaptability to diverse downstream tasks and master enriched cross-modal proficiency. The collaboration-aware recommendation enhancement training further adapts multimodal representations for recommendation scenarios by distilling knowledge from sequence-to-item and ID-to-item embeddings while mining user historical interests. Concurrently, we develop the stochastic specialization and dataset-driven pattern matching to strengthen model training flexibility and generalizability. Experimental results show that SAIL-Embedding achieves SOTA performance compared to other methods in different retrieval tasks. In online experiments across various real-world scenarios integrated with our model, we observe a significant increase in Lifetime (LT), which is a crucial indicator for the recommendation experience. For instance, the model delivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the Douyin-Selected scenario. For the Douyin feed rank model, the match features produced by SAIL-Embedding yield a +0.08% AUC gain.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12668v1": {
    "title": "The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12668v1",
    "arxiv_id": "2510.12668v1",
    "authors": "Minghao Tang, Shiyu Ni, Jingtong Wu, Zengxin Han, Keping Bi",
    "categories": "cs.IR, cs.CL",
    "pub_date": "2025-10-14 16:05:01",
    "ori_summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving external documents. As an emerging form of RAG, parametric retrieval-augmented generation (PRAG) encodes documents as model parameters (i.e., LoRA modules) and injects these representations into the model during inference, enabling interaction between the LLM and documents at parametric level. Compared with directly placing documents in the input context, PRAG is more efficient and has the potential to offer deeper model-document interaction. Despite its growing attention, the mechanism underlying parametric injection remains poorly understood. In this work, we present a systematic study of PRAG to clarify the role of parametric injection, showing that parameterized documents capture only partial semantic information of documents, and relying on them alone yields inferior performance compared to interaction at text level. However, these parametric representations encode high-level document information that can enhance the model's understanding of documents within the input context. When combined parameterized documents with textual documents, the model can leverage relevant information more effectively and become more robust to noisy inputs, achieving better performance than either source alone. We recommend jointly using parameterized and textual documents and advocate for increasing the information content of parametric representations to advance PRAG.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12604v1": {
    "title": "SMILE: SeMantic Ids Enhanced CoLd Item Representation for Click-through Rate Prediction in E-commerce SEarch",
    "url": "https://www.alphaxiv.org/abs/2510.12604v1",
    "arxiv_id": "2510.12604v1",
    "authors": "Qihang Zhao, Zhongbo Sun, Xiaoyang Zheng, Xian Guo, Siyuan Wang, Zihan Liang, Mingcan Peng, Ben Chen, Chenyi Lei",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-14 14:58:50",
    "ori_summary": "With the rise of modern search and recommendation platforms, insufficient collaborative information of cold-start items exacerbates the Matthew effect of existing platform items, challenging platform diversity and becoming a longstanding issue. Existing methods align items' side content with collaborative information to transfer collaborative signals from high-popularity items to cold-start items. However, these methods fail to account for the asymmetry between collaboration and content, nor the fine-grained differences among items. To address these issues, we propose SMILE, an item representation enhancement approach based on fused alignment of semantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and collaborative information, followed by a two-step alignment: RQ encoding transfers shared collaborative signals across items, while OPQ encoding learns differentiated information of items. Comprehensive offline experiments on large-scale industrial datasets demonstrate superiority of SMILE, and rigorous online A/B tests confirm statistically significant improvements: item CTR +1.66%, buyers +1.57%, and order volume +2.17%.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12461v1": {
    "title": "Leveraging Language Semantics for Collaborative Filtering with TextGCN and TextGCN-MLP: Zero-Shot vs In-Domain Performance",
    "url": "https://www.alphaxiv.org/abs/2510.12461v1",
    "arxiv_id": "2510.12461v1",
    "authors": "Andrei Chernov, Haroon Wahab, Oleg Novitskij",
    "categories": "cs.IR",
    "pub_date": "2025-10-14 12:50:11",
    "ori_summary": "In recent years, various approaches have been proposed to leverage large language models (LLMs) for incorporating textual information about items into recommender systems. Existing methods primarily focus on either fine-tuning LLMs to generate recommendations or integrating LLM-based embeddings into downstream models. In this work, we follow the latter direction and propose \\textbf{TextGCN}, which applies parameter-free graph convolution layers directly over LLM-based item-title embeddings, instead of learning ID-based embeddings as in traditional methods. By combining language semantics with graph message passing, this architecture achieves state-of-the-art zero-shot performance, significantly outperforming prior approaches. Furthermore, we introduce \\textbf{TextGCN-MLP}, which extends TextGCN with a trainable multilayer perceptron trained using a contrastive loss, achieving state-of-the-art in-domain performance on recommendation benchmarks. However, the zero-shot performance of TextGCN-MLP remains lower than that of TextGCN, highlighting the trade-off between in-domain specialization and zero-shot generalization. We release our code on github at \\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12369v1": {
    "title": "A Hierarchical Quantized Tokenization Framework for Task-Adaptive Graph Representation Learning",
    "url": "https://www.alphaxiv.org/abs/2510.12369v1",
    "arxiv_id": "2510.12369v1",
    "authors": "Yang Xiang, Li Fan, Chenke Yin, Chengtao Ji",
    "categories": "cs.IR",
    "pub_date": "2025-10-14 10:36:43",
    "ori_summary": "Recent progress in language and vision foundation models demonstrates the importance of discrete token interfaces that transform complex inputs into compact sequences for large-scale modeling. Extending this paradigm to graphs requires a tokenization scheme that handles non-Euclidean structures and multi-scale dependencies efficiently. Existing approaches to graph tokenization, linearized, continuous, and quantized, remain limited in adaptability and efficiency. In particular, most current quantization-based tokenizers organize hierarchical information in fixed or task-agnostic ways, which may either over-represent or under-utilize structural cues, and lack the ability to dynamically reweight contributions from different levels without retraining the encoder. This work presents a hierarchical quantization framework that introduces a self-weighted mechanism for task-adaptive aggregation across multiple scales. The proposed method maintains a frozen encoder while modulating information flow through a lightweight gating process, enabling parameter-efficient adaptation to diverse downstream tasks. Experiments on benchmark datasets for node classification and link prediction demonstrate consistent improvements over strong baselines under comparable computational budgets.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12327v1": {
    "title": "Simple Projection Variants Improve ColBERT Performance",
    "url": "https://www.alphaxiv.org/abs/2510.12327v1",
    "arxiv_id": "2510.12327v1",
    "authors": "Benjamin Clavié, Sean Lee, Rikiya Takehi, Aamir Shakir, Makoto P. Kato",
    "categories": "cs.IR, cs.AI, cs.CL",
    "pub_date": "2025-10-14 09:34:05",
    "ori_summary": "Multi-vector dense retrieval methods like ColBERT systematically use a single-layer linear projection to reduce the dimensionality of individual vectors. In this study, we explore the implications of the MaxSim operator on the gradient flows of the training of multi-vector models and show that such a simple linear projection has inherent, if non-critical, limitations in this setting. We then discuss the theoretical improvements that could result from replacing this single-layer projection with well-studied alternative feedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU blocks, and skip-connections, could alleviate these limitations. Through the design and systematic evaluation of alternate projection blocks, we show that better-designed final projections positively impact the downstream performance of ColBERT models. We highlight that many projection variants outperform the original linear projections, with the best-performing variants increasing average performance on a range of retrieval benchmarks across domains by over 2 NDCG@10 points. We then conduct further exploration on the individual parameters of these projections block in order to understand what drives this empirical performance, highlighting the particular importance of upscaled intermediate projections and residual connections. As part of these ablation studies, we show that numerous suboptimal projection variants still outperform the traditional single-layer projection across multiple benchmarks, confirming our hypothesis. Finally, we observe that this effect is consistent across random seeds, further confirming that replacing the linear layer of ColBERT models is a robust, drop-in upgrade.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12325v1": {
    "title": "Causal Inspired Multi Modal Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.12325v1",
    "arxiv_id": "2510.12325v1",
    "authors": "Jie Yang, Chenyang Gu, Zixuan Liu",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-14 09:29:07",
    "ori_summary": "Multimodal recommender systems enhance personalized recommendations in e-commerce and online advertising by integrating visual, textual, and user-item interaction data. However, existing methods often overlook two critical biases: (i) modal confounding, where latent factors (e.g., brand style or product category) simultaneously drive multiple modalities and influence user preference, leading to spurious feature-preference associations; (ii) interaction bias, where genuine user preferences are mixed with noise from exposure effects and accidental clicks. To address these challenges, we propose a Causal-inspired multimodal Recommendation framework. Specifically, we introduce a dual-channel cross-modal diffusion module to identify hidden modal confounders, utilize back-door adjustment with hierarchical matching and vector-quantized codebooks to block confounding paths, and apply front-door adjustment combined with causal topology reconstruction to build a deconfounded causal subgraph. Extensive experiments on three real-world e-commerce datasets demonstrate that our method significantly outperforms state-of-the-art baselines while maintaining strong interpretability.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12299v1": {
    "title": "An Empirical Study for Representations of Videos in Video Question Answering via MLLMs",
    "url": "https://www.alphaxiv.org/abs/2510.12299v1",
    "arxiv_id": "2510.12299v1",
    "authors": "Zhi Li, Yanan Wang, Hao Niu, Julio Vizcarra, Masato Taya",
    "categories": "cs.IR, I.2.10",
    "pub_date": "2025-10-14 09:02:22",
    "ori_summary": "Multimodal large language models have recently achieved remarkable progress in video question answering (VideoQA) by jointly processing visual, textual, and audio information. However, it remains unclear which video representations are most effective for MLLMs, and how different modalities balance task accuracy against computational efficiency. In this work, we present a comprehensive empirical study of video representation methods for VideoQA with MLLMs. We systematically evaluate single modality inputs question only, subtitles, visual frames, and audio signals as well as multimodal combinations, on two widely used benchmarks: VideoMME and LongVideoBench. Our results show that visual frames substantially enhance accuracy but impose heavy costs in GPU memory and inference latency, while subtitles provide a lightweight yet effective alternative, particularly for long videos. These findings highlight clear trade-offs between effectiveness and efficiency and provide practical insights for designing resource-aware MLLM-based VideoQA systems.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12211v1": {
    "title": "Reinforced Preference Optimization for Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.12211v1",
    "arxiv_id": "2510.12211v1",
    "authors": "Junfei Tan, Yuxin Chen, An Zhang, Junguang Jiang, Bin Liu, Ziru Xu, Han Zhu, Jian Xu, Bo Zheng, Xiang Wang",
    "categories": "cs.IR",
    "pub_date": "2025-10-14 07:04:33",
    "ori_summary": "Recent breakthroughs in large language models (LLMs) have fundamentally shifted recommender systems from discriminative to generative paradigms, where user behavior modeling is achieved by generating target items conditioned on historical interactions. Yet current generative recommenders still suffer from two core limitations: the lack of high-quality negative modeling and the reliance on implicit rewards. Reinforcement learning with verifiable rewards (RLVR) offers a natural solution by enabling on-policy sampling of harder negatives and grounding optimization in explicit reward signals. However, applying RLVR to generative recommenders remains non-trivial. Its unique generation space often leads to invalid or repetitive items that undermine sampling efficiency, and ranking supervision is sparse since most items receive identical zero rewards. To address these challenges, we propose Reinforced Preference Optimization for Recommendation (ReRe), a reinforcement-based paradigm tailored to LLM-based recommenders, an important direction in generative recommendation. ReRe incorporates constrained beam search to improve sampling efficiency and diversify hard negatives, while augmenting rule-based accuracy rewards with auxiliary ranking rewards for finer-grained supervision. Extensive experiments on three real-world datasets demonstrate that ReRe consistently outperforms both traditional and LLM-based recommenders in ranking performance. Further analysis shows that ReRe not only enhances performance across both base and SFT-initialized models but also generalizes robustly across different backbone families and scales. Beyond empirical gains, we systematically investigate the design space of RLVR in recommendation across generation, sampling strategy, reward modeling, and optimization algorithm, offering insights for future research.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12054v1": {
    "title": "MIARec: Mutual-influence-aware Heterogeneous Network Embedding for Scientific Paper Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.12054v1",
    "arxiv_id": "2510.12054v1",
    "authors": "Wenjin Xie, Tao Jia",
    "categories": "cs.IR, cs.LG",
    "pub_date": "2025-10-14 01:47:25",
    "ori_summary": "With the rapid expansion of scientific literature, scholars increasingly demand precise and high-quality paper recommendations. Among various recommendation methodologies, graph-based approaches have garnered attention by effectively exploiting the structural characteristics inherent in scholarly networks. However, these methods often overlook the asymmetric academic influence that is prevalent in scholarly networks when learning graph representations. To address this limitation, this study proposes the Mutual-Influence-Aware Recommendation (MIARec) model, which employs a gravity-based approach to measure the mutual academic influence between scholars and incorporates this influence into the feature aggregation process during message propagation in graph representation learning. Additionally, the model utilizes a multi-channel aggregation method to capture both individual embeddings of distinct single relational sub-networks and their interdependent embeddings, thereby enabling a more comprehensive understanding of the heterogeneous scholarly network. Extensive experiments conducted on real-world datasets demonstrate that the MIARec model outperforms baseline models across three primary evaluation metrics, indicating its effectiveness in scientific paper recommendation tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12784v1": {
    "title": "SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models",
    "url": "https://www.alphaxiv.org/abs/2510.12784v1",
    "arxiv_id": "2510.12784v1",
    "authors": "Weiyang Jin, Yuwei Niu, Jiaqi Liao, Chengqi Duan, Aoxue Li, Shenghua Gao, Xihui Liu",
    "categories": "cs.CV, cs.CL, I.4.0",
    "pub_date": "2025-10-14 17:56:11",
    "ori_summary": "Recently, remarkable progress has been made in Unified Multimodal Models (UMMs), which integrate vision-language generation and understanding capabilities within a single framework. However, a significant gap exists where a model's strong visual understanding often fails to transfer to its visual generation. A model might correctly understand an image based on user instructions, yet be unable to generate a faithful image from text prompts. This phenomenon directly raises a compelling question: Can a model achieve self-improvement by using its understanding module to reward its generation module? To bridge this gap and achieve self-improvement, we introduce SRUM, a self-rewarding post-training framework that can be directly applied to existing UMMs of various designs. SRUM creates a feedback loop where the model's own understanding module acts as an internal ``evaluator'', providing corrective signals to improve its generation module, without requiring additional human-labeled data. To ensure this feedback is comprehensive, we designed a global-local dual reward system. To tackle the inherent structural complexity of images, this system offers multi-scale guidance: a \\textbf{global reward} ensures the correctness of the overall visual semantics and layout, while a \\textbf{local reward} refines fine-grained, object-level fidelity. SRUM leads to powerful capabilities and shows strong generalization, boosting performance on T2I-CompBench from 82.18 to \\textbf{88.37} and on T2I-ReasonBench from 43.82 to \\textbf{46.75}. Overall, our work establishes a powerful new paradigm for enabling a UMMs' understanding module to guide and enhance its own generation via self-rewarding.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12781v1": {
    "title": "Cost Analysis of Human-corrected Transcription for Predominately Oral Languages",
    "url": "https://www.alphaxiv.org/abs/2510.12781v1",
    "arxiv_id": "2510.12781v1",
    "authors": "Yacouba Diarra, Nouhoum Souleymane Coulibaly, Michael Leventhal",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 17:53:11",
    "ori_summary": "Creating speech datasets for low-resource languages is a critical yet poorly understood challenge, particularly regarding the actual cost in human labor. This paper investigates the time and complexity required to produce high-quality annotated speech data for a subset of low-resource languages, low literacy Predominately Oral Languages, focusing on Bambara, a Manding language of Mali. Through a one-month field study involving ten transcribers with native proficiency, we analyze the correction of ASR-generated transcriptions of 53 hours of Bambara voice data. We report that it takes, on average, 30 hours of human labor to accurately transcribe one hour of speech data under laboratory conditions and 36 hours under field conditions. The study provides a baseline and practical insights for a large class of languages with comparable profiles undertaking the creation of NLP resources.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12780v1": {
    "title": "Content Anonymization for Privacy in Long-form Audio",
    "url": "https://www.alphaxiv.org/abs/2510.12780v1",
    "arxiv_id": "2510.12780v1",
    "authors": "Cristina Aggazzotti, Ashi Garg, Zexin Cai, Nicholas Andrews",
    "categories": "cs.SD, cs.CL",
    "pub_date": "2025-10-14 17:52:50",
    "ori_summary": "Voice anonymization techniques have been found to successfully obscure a speaker's acoustic identity in short, isolated utterances in benchmarks such as the VoicePrivacy Challenge. In practice, however, utterances seldom occur in isolation: long-form audio is commonplace in domains such as interviews, phone calls, and meetings. In these cases, many utterances from the same speaker are available, which pose a significantly greater privacy risk: given multiple utterances from the same speaker, an attacker could exploit an individual's vocabulary, syntax, and turns of phrase to re-identify them, even when their voice is completely disguised. To address this risk, we propose new content anonymization approaches. Our approach performs a contextual rewriting of the transcripts in an ASR-TTS pipeline to eliminate speaker-specific style while preserving meaning. We present results in a long-form telephone conversation setting demonstrating the effectiveness of a content-based attack on voice-anonymized speech. Then we show how the proposed content-based anonymization methods can mitigate this risk while preserving speech utility. Overall, we find that paraphrasing is an effective defense against content-based attacks and recommend that stakeholders adopt this step to ensure anonymity in long-form audio.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12773v1": {
    "title": "Dr.LLM: Dynamic Layer Routing in LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.12773v1",
    "arxiv_id": "2510.12773v1",
    "authors": "Ahmed Heakl, Martin Gubri, Salman Khan, Sangdoo Yun, Seong Joon Oh",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-14 17:51:26",
    "ori_summary": "Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inference-time search, architectural changes, or large-scale retraining, and in practice often degrade accuracy despite efficiency gains. We introduce Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that equips pretrained models with lightweight per-layer routers deciding to skip, execute, or repeat a block. Routers are trained with explicit supervision: using Monte Carlo Tree Search (MCTS), we derive high-quality layer configurations that preserve or improve accuracy under a compute budget. Our design, windowed pooling for stable routing, focal loss with class balancing, and bottleneck MLP routers, ensures robustness under class imbalance and long sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to +3.4%p while saving 5 layers per example on average. Routers generalize to out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval) with only 0.85% accuracy drop while retaining efficiency, and outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that explicitly supervised routers retrofit frozen LLMs for budget-aware, accuracy-driven inference without altering base weights.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12766v1": {
    "title": "Language Models Model Language",
    "url": "https://www.alphaxiv.org/abs/2510.12766v1",
    "arxiv_id": "2510.12766v1",
    "authors": "Łukasz Borchmann",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 17:45:31",
    "ori_summary": "Linguistic commentary on LLMs, heavily influenced by the theoretical frameworks of de Saussure and Chomsky, is often speculative and unproductive. Critics challenge whether LLMs can legitimately model language, citing the need for \"deep structure\" or \"grounding\" to achieve an idealized linguistic \"competence.\" We argue for a radical shift in perspective towards the empiricist principles of Witold Ma\\'nczak, a prominent general and historical linguist. He defines language not as a \"system of signs\" or a \"computational system of the brain\" but as the totality of all that is said and written. Above all, he identifies frequency of use of particular language elements as language's primary governing principle. Using his framework, we challenge prior critiques of LLMs and provide a constructive guide for designing, evaluating, and interpreting language models.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12740v1": {
    "title": "Hey, wait a minute: on at-issue sensitivity in Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.12740v1",
    "arxiv_id": "2510.12740v1",
    "authors": "Sanghee J. Kim, Kanishka Misra",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 17:17:20",
    "ori_summary": "Evaluating the naturalness of dialogue in language models (LMs) is not trivial: notions of 'naturalness' vary, and scalable quantitative metrics remain limited. This study leverages the linguistic notion of 'at-issueness' to assess dialogue naturalness and introduces a new method: Divide, Generate, Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii) generates continuations for subparts using LMs, (iii) recombines the dialogue and continuations, and (iv) compares the likelihoods of the recombined sequences. This approach mitigates bias in linguistic analyses of LMs and enables systematic testing of discourse-sensitive behavior. Applying DGRC, we find that LMs prefer to continue dialogue on at-issue content, with this effect enhanced in instruct-tuned models. They also reduce their at-issue preference when relevant cues (e.g., \"Hey, wait a minute\") are present. Although instruct-tuning does not further amplify this modulation, the pattern reflects a hallmark of successful dialogue dynamics.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12722v1": {
    "title": "Which Word Orders Facilitate Length Generalization in LMs? An Investigation with GCG-Based Artificial Languages",
    "url": "https://www.alphaxiv.org/abs/2510.12722v1",
    "arxiv_id": "2510.12722v1",
    "authors": "Nadine El-Naggar, Tatsuki Kuribayashi, Ted Briscoe",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 17:00:19",
    "ori_summary": "Whether language models (LMs) have inductive biases that favor typologically frequent grammatical properties over rare, implausible ones has been investigated, typically using artificial languages (ALs) (White and Cotterell, 2021; Kuribayashi et al., 2024). In this paper, we extend these works from two perspectives. First, we extend their context-free AL formalization by adopting Generalized Categorial Grammar (GCG) (Wood, 2014), which allows ALs to cover attested but previously overlooked constructions, such as unbounded dependency and mildly context-sensitive structures. Second, our evaluation focuses more on the generalization ability of LMs to process unseen longer test sentences. Thus, our ALs better capture features of natural languages and our experimental paradigm leads to clearer conclusions -- typologically plausible word orders tend to be easier for LMs to productively generalize.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12720v1": {
    "title": "Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception",
    "url": "https://www.alphaxiv.org/abs/2510.12720v1",
    "arxiv_id": "2510.12720v1",
    "authors": "Ziyang Ma, Ruiyang Xu, Zhenghao Xing, Yunfei Chu, Yuxuan Wang, Jinzheng He, Jin Xu, Pheng-Ann Heng, Kai Yu, Junyang Lin, Eng Siong Chng, Xie Chen",
    "categories": "cs.CL, cs.CV, cs.MM, cs.SD",
    "pub_date": "2025-10-14 17:00:09",
    "ori_summary": "Fine-grained perception of multimodal information is critical for advancing human-AI interaction. With recent progress in audio-visual technologies, Omni Language Models (OLMs), capable of processing audio and video signals in parallel, have emerged as a promising paradigm for achieving richer understanding and reasoning. However, their capacity to capture and describe fine-grained details remains limited explored. In this work, we present a systematic and comprehensive investigation of omni detailed perception from the perspectives of the data pipeline, models, and benchmark. We first identify an inherent \"co-growth\" between detail and hallucination in current OLMs. To address this, we propose Omni-Detective, an agentic data generation pipeline integrating tool-calling, to autonomously produce highly detailed yet minimally hallucinatory multimodal data. Based on the data generated with Omni-Detective, we train two captioning models: Audio-Captioner for audio-only detailed perception, and Omni-Captioner for audio-visual detailed perception. Under the cascade evaluation protocol, Audio-Captioner achieves the best performance on MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and delivering performance comparable to Gemini 2.5 Pro. On existing detailed captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and achieves the best trade-off between detail and hallucination on the video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for detailed audio, visual, and audio-visual captioning that ensures stable, efficient, and reliable assessment. Experimental results and analysis demonstrate the effectiveness of Omni-Detective in generating high-quality detailed captions, as well as the superiority of Omni-Cloze in evaluating such detailed captions.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12699v1": {
    "title": "Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations",
    "url": "https://www.alphaxiv.org/abs/2510.12699v1",
    "arxiv_id": "2510.12699v1",
    "authors": "Sunny Yu, Ahmad Jabbar, Robert Hawkins, Dan Jurafsky, Myra Cheng",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 16:31:34",
    "ori_summary": "Different open-ended generation tasks require different degrees of output diversity. However, current LLMs are often miscalibrated. They collapse to overly homogeneous outputs for creative tasks and hallucinate diverse but incorrect responses for factual tasks. We argue that these two failure modes are unified by, and can both be addressed by, the notion of effective generation space size (GSS) -- the set of semantically distinct outputs a model considers for a prompt. We present GSSBench, a task suite of prompt pairs with ground-truth GSS relationships to assess different metrics and understand where models diverge from desired behavior. We find that hallucination detection metrics, particularly EigenScore, consistently outperform standard diversity and uncertainty quantification metrics, while using only model internals, providing interpretable insights into a model's internal task representations. We demonstrate three applications of GSS: (1) detecting prompt ambiguity and predicting clarification questions for better grounding, (2) interpreting overthinking and underthinking in reasoning models, and (3) steering models to expand their generation space to yield high-quality and diverse outputs.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12692v1": {
    "title": "Who is a Better Matchmaker? Human vs. Algorithmic Judge Assignment in a High-Stakes Startup Competition",
    "url": "https://www.alphaxiv.org/abs/2510.12692v1",
    "arxiv_id": "2510.12692v1",
    "authors": "Sarina Xi, Orelia Pi, Miaomiao Zhang, Becca Xiong, Jacqueline Ng Lane, Nihar B. Shah",
    "categories": "cs.HC, cs.AI, cs.CL, cs.CY, cs.LG",
    "pub_date": "2025-10-14 16:25:09",
    "ori_summary": "There is growing interest in applying artificial intelligence (AI) to automate and support complex decision-making tasks. However, it remains unclear how algorithms compare to human judgment in contexts requiring semantic understanding and domain expertise. We examine this in the context of the judge assignment problem, matching submissions to suitably qualified judges. Specifically, we tackled this problem at the Harvard President's Innovation Challenge, the university's premier venture competition awarding over \\$500,000 to student and alumni startups. This represents a real-world environment where high-quality judge assignment is essential. We developed an AI-based judge-assignment algorithm, Hybrid Lexical-Semantic Similarity Ensemble (HLSE), and deployed it at the competition. We then evaluated its performance against human expert assignments using blinded match-quality scores from judges on $309$ judge-venture pairs. Using a Mann-Whitney U statistic based test, we found no statistically significant difference in assignment quality between the two approaches ($AUC=0.48, p=0.40$); on average, algorithmic matches are rated $3.90$ and manual matches $3.94$ on a 5-point scale, where 5 indicates an excellent match. Furthermore, manual assignments that previously required a full week could be automated in several hours by the algorithm during deployment. These results demonstrate that HLSE achieves human-expert-level matching quality while offering greater scalability and efficiency, underscoring the potential of AI-driven solutions to support and enhance human decision-making for judge assignment in high-stakes settings.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12680v1": {
    "title": "Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and No-Think?",
    "url": "https://www.alphaxiv.org/abs/2510.12680v1",
    "arxiv_id": "2510.12680v1",
    "authors": "Shouren Wang, Wang Yang, Xianxuan Long, Qifan Wang, Vipin Chaudhary, Xiaotian Han",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-14 16:19:44",
    "ori_summary": "Hybrid thinking enables LLMs to switch between reasoning and direct answering, offering a balance between efficiency and reasoning capability. Yet our experiments reveal that current hybrid thinking LLMs only achieve partial mode separation: reasoning behaviors often leak into the no-think mode. To understand and mitigate this, we analyze the factors influencing controllability and identify four that matter most: (1) larger data scale, (2) using think and no-think answers from different questions rather than the same question, (3) a moderate increase in no-think data number, and (4) a two-phase strategy that first trains reasoning ability and then applies hybrid think training. Building on these findings, we propose a practical recipe that, compared to standard training, can maintain accuracy in both modes while significantly reducing no-think output length (from $1085$ to $585$ on MATH500) and occurrences of reasoning-supportive tokens such as ``\\texttt{wait}'' (from $5917$ to $522$ on MATH500). Our findings highlight the limitations of current hybrid thinking and offer directions for strengthening its controllability.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12643v1": {
    "title": "Reasoning Pattern Matters: Learning to Reason without Human Rationales",
    "url": "https://www.alphaxiv.org/abs/2510.12643v1",
    "arxiv_id": "2510.12643v1",
    "authors": "Chaoxu Pang, Yixuan Cao, Ping Luo",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 15:34:38",
    "ori_summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities under the widely adopted SFT+RLVR paradigm, which first performs Supervised Fine-Tuning (SFT) on human-annotated reasoning trajectories (rationales) to establish initial reasoning behaviors, then applies Reinforcement Learning with Verifiable Rewards (RLVR) to optimize the model using verifiable signals without golden rationales. However, annotating high-quality rationales for the SFT stage remains prohibitively expensive. This paper investigates when and how rationale annotation costs can be substantially reduced without compromising reasoning performance. We identify a broad class of problems, termed patterned reasoning tasks, where reasoning follows a fixed, procedural strategy consistent across instances. Although instances vary in content such as domain knowledge, factual information, or numeric values, the solution derives from applying a shared reasoning pattern. We argue that the success of SFT+RLVR on such tasks primarily stems from its ability to enable models to internalize these reasoning patterns. Using numerical semantic matching as a representative task, we provide both causal and behavioral evidence showing that reasoning patterns rather than the quantity or quality of rationales are the key determinant of performance. Building on these insights, we propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet effective framework that enables LLMs to generate rationales aligned with task-specific reasoning patterns without requiring human rationale annotations. Experiments show that PARO-generated rationales achieve comparable SFT+RLVR performance to human rationales that are 10 times larger. These results suggest that large-scale human rationale annotations can be replaced with LLM-based automatic annotations requiring only limited human supervision over reasoning patterns.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12637v1": {
    "title": "COSTAR-A: A prompting framework for enhancing Large Language Model performance on Point-of-View questions",
    "url": "https://www.alphaxiv.org/abs/2510.12637v1",
    "arxiv_id": "2510.12637v1",
    "authors": "Nzubechukwu C. Ohalete, Kevin B. Gittner, Lauren M. Matheny",
    "categories": "cs.CL, I.2.7",
    "pub_date": "2025-10-14 15:31:21",
    "ori_summary": "Large Language Models (LLMs) are highly sensitive to prompt design, and making optimized prompting techniques is crucial for generating consistent, high-quality outputs. In this study, we introduce COSTAR-A, a novel prompt engineering framework that enhances the existing COSTAR method, which stands for Context, Objective, Style, Tone, Audience, and Response, by adding the 'Answer' component at the end. We demonstrate that while the original COSTAR framework improves prompt clarity and aligns outputs for larger LLMs, its performance is less consistent with smaller, locally optimized models, particularly in tasks that require more directive or constrained outputs. Through a series of controlled prompt-output assessments with smaller (at most 8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance the output structure and decisiveness of localized LLMs for certain tasks, although its effectiveness varies across models and use cases. Notably, the Llama 3.1-8B model exhibited performance improvements when prompted with COSTAR-A compared to COSTAR alone. These findings emphasize the adaptability and scalability of COSTAR-A as a prompting framework, particularly in computationally efficient AI deployments on resource-constrained hardware.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12621v1": {
    "title": "ACADATA: Parallel Dataset of Academic Data for Machine Translation",
    "url": "https://www.alphaxiv.org/abs/2510.12621v1",
    "arxiv_id": "2510.12621v1",
    "authors": "Iñaki Lacunza, Javier Garcia Gilabert, Francesca De Luca Fornaciari, Javier Aula-Blasco, Aitor Gonzalez-Agirre, Maite Melero, Marta Villegas",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 15:20:06",
    "ori_summary": "We present ACADATA, a high-quality parallel dataset for academic translation, that consists of two subsets: ACAD-TRAIN, which contains approximately 1.5 million author-generated paragraph pairs across 96 language directions and ACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12 directions. To validate its utility, we fine-tune two Large Language Models (LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized machine-translation systems, general-purpose, open-weight LLMs, and several large-scale proprietary models. Experimental results demonstrate that fine-tuning on ACAD-TRAIN leads to improvements in academic translation quality by +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively, while also improving long-context translation in a general domain by up to 24.9% when translating out of English. The fine-tuned top-performing model surpasses the best propietary and open-weight models on academic translation domain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we provide the community with a valuable resource to advance research in academic domain and long-context translation.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12608v1": {
    "title": "StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis",
    "url": "https://www.alphaxiv.org/abs/2510.12608v1",
    "arxiv_id": "2510.12608v1",
    "authors": "Siyuan Li, Aodu Wulianghai, Xi Lin, Guangyan Li, Xiang Chen, Jun Wu, Jianhua Li",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 15:07:27",
    "ori_summary": "With the increasing integration of large language models (LLMs) into open-domain writing, detecting machine-generated text has become a critical task for ensuring content authenticity and trust. Existing approaches rely on statistical discrepancies or model-specific heuristics to distinguish between LLM-generated and human-written text. However, these methods struggle in real-world scenarios due to limited generalization, vulnerability to paraphrasing, and lack of explainability, particularly when facing stylistic diversity or hybrid human-AI authorship. In this work, we propose StyleDecipher, a robust and explainable detection framework that revisits LLM-generated text detection using combined feature extractors to quantify stylistic differences. By jointly modeling discrete stylistic indicators and continuous stylistic representations derived from semantic embeddings, StyleDecipher captures distinctive style-level divergences between human and LLM outputs within a unified representation space. This framework enables accurate, explainable, and domain-agnostic detection without requiring access to model internals or labeled segments. Extensive experiments across five diverse domains, including news, code, essays, reviews, and academic abstracts, demonstrate that StyleDecipher consistently achieves state-of-the-art in-domain accuracy. Moreover, in cross-domain evaluations, it surpasses existing baselines by up to 36.30%, while maintaining robustness against adversarial perturbations and mixed human-AI content. Further qualitative and quantitative analysis confirms that stylistic signals provide explainable evidence for distinguishing machine-generated text. Our source code can be accessed at https://github.com/SiyuanLi00/StyleDecipher.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12603v1": {
    "title": "Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space",
    "url": "https://www.alphaxiv.org/abs/2510.12603v1",
    "arxiv_id": "2510.12603v1",
    "authors": "Chao Chen, Zhixin Ma, Yongqi Li, Yupeng Hu, Yinwei Wei, Wenjie Li, Liqiang Nie",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-10-14 14:58:25",
    "ori_summary": "Multimodal reasoning aims to enhance the capabilities of MLLMs by incorporating intermediate reasoning steps before reaching the final answer. It has evolved from text-only reasoning to the integration of visual information, enabling the thought process to be conveyed through both images and text. Despite its effectiveness, current multimodal reasoning methods depend on explicit reasoning steps that require labor-intensive vision-text annotations and inherently introduce significant inference latency. To address these issues, we introduce multimodal latent reasoning with the advantages of multimodal representation, reduced annotation, and inference efficiency. To facilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR), which injects both visual and textual information in the reasoning process within the latent space. Specifically, IVT-LR represents each reasoning step by combining two implicit parts: latent text (the hidden states from the previous step) and latent vision (a set of selected image embeddings). We further introduce a progressive multi-stage training strategy to enable MLLMs to perform the above multimodal latent reasoning steps. Experiments on M3CoT and ScienceQA demonstrate that our IVT-LR method achieves an average performance increase of 5.45% in accuracy, while simultaneously achieving a speed increase of over 5 times compared to existing approaches. Code available at https://github.com/FYYDCC/IVT-LR.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12587v1": {
    "title": "Teaching Language Models to Faithfully Express their Uncertainty",
    "url": "https://www.alphaxiv.org/abs/2510.12587v1",
    "arxiv_id": "2510.12587v1",
    "authors": "Bryan Eikema, Evgenia Ilia, José G. C. de Souza, Chrysoula Zerva, Wilker Aziz",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 14:42:40",
    "ori_summary": "Large language models (LLMs) often miscommunicate their uncertainty: repeated queries can produce divergent answers, yet generated responses are typically unhedged or hedged in ways that do not reflect this variability. This conveys unfaithful information about the uncertain state of the LLMs' knowledge, creating a faithfulness gap that affects even strong LLMs. We introduce Faithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches instruction-tuned LLMs to express uncertainty faithfully without altering their underlying answer distribution. We construct training data by augmenting model samples with uncertainty hedges (i.e. verbal cues such as 'possibly' or 'likely') aligned with sample consistency, requiring no supervision beyond the model and a set of prompts. We evaluate FUT on open-domain question answering (QA) across multiple models and datasets. Our results show that FUT substantially reduces the faithfulness gap, while preserving QA accuracy and introducing minimal semantic distribution shift. Further analyses demonstrate robustness across decoding strategies, choice of hedgers, and other forms of uncertainty expression (i.e. numerical). These findings establish FUT as a simple and effective way to teach LLMs to communicate uncertainty faithfully.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12548v1": {
    "title": "VISaGE: Understanding Visual Generics and Exceptions",
    "url": "https://www.alphaxiv.org/abs/2510.12548v1",
    "arxiv_id": "2510.12548v1",
    "authors": "Stella Frank, Emily Allaway",
    "categories": "cs.CL, cs.CV",
    "pub_date": "2025-10-14 14:13:06",
    "ori_summary": "While Vision Language Models (VLMs) learn conceptual representations, in the form of generalized knowledge, during training, they are typically used to analyze individual instances. When evaluation instances are atypical, this paradigm results in tension between two priors in the model. The first is a pragmatic prior that the textual and visual input are both relevant, arising from VLM finetuning on congruent inputs; the second is a semantic prior that the conceptual representation is generally true for instances of the category. In order to understand how VLMs trade off these priors, we introduce a new evaluation dataset, VISaGE, consisting of both typical and exceptional images. In carefully balanced experiments, we show that conceptual understanding degrades when the assumption of congruency underlying the pragmatic prior is violated with incongruent images. This effect is stronger than the effect of the semantic prior when querying about individual instances.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12516v1": {
    "title": "BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)",
    "url": "https://www.alphaxiv.org/abs/2510.12516v1",
    "arxiv_id": "2510.12516v1",
    "authors": "Tomas Ruiz, Siyao Peng, Barbara Plank, Carsten Schwemmer",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 13:43:08",
    "ori_summary": "Test-time scaling is a family of techniques to improve LLM outputs at inference time by performing extra computation. To the best of our knowledge, test-time scaling has been limited to domains with verifiably correct answers, like mathematics and coding. We transfer test-time scaling to the LeWiDi-2025 tasks to evaluate annotation disagreements. We experiment with three test-time scaling methods: two benchmark algorithms (Model Averaging and Majority Voting), and a Best-of-N sampling method. The two benchmark methods improve LLM performance consistently on the LeWiDi tasks, but the Best-of-N method does not. Our experiments suggest that the Best-of-N method does not currently transfer from mathematics to LeWiDi tasks, and we analyze potential reasons for this gap.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12476v1": {
    "title": "When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection",
    "url": "https://www.alphaxiv.org/abs/2510.12476v1",
    "arxiv_id": "2510.12476v1",
    "authors": "Lang Gao, Xuhui Li, Chenxi Wang, Mingzhe Li, Wei Liu, Zirui Song, Jinghui Zhang, Rui Yan, Preslav Nakov, Xiuying Chen",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 13:10:23",
    "ori_summary": "Large language models (LLMs) have grown more powerful in language generation, producing fluent text and even imitating personal style. Yet, this ability also heightens the risk of identity impersonation. To the best of our knowledge, no prior work has examined personalized machine-generated text (MGT) detection. In this paper, we introduce \\dataset, the first benchmark for evaluating detector robustness in personalized settings, built from literary and blog texts paired with their LLM-generated imitations. Our experimental results demonstrate large performance gaps across detectors in personalized settings: some state-of-the-art models suffer significant drops. We attribute this limitation to the \\textit{feature-inversion trap}, where features that are discriminative in general domains become inverted and misleading when applied to personalized text. Based on this finding, we propose \\method, a simple and reliable way to predict detector performance changes in personalized settings. \\method identifies latent directions corresponding to inverted features and constructs probe datasets that differ primarily along these features to evaluate detector dependence. Our experiments show that \\method can accurately predict both the direction and the magnitude of post-transfer changes, showing 85\\% correlation with the actual performance gaps. We hope that this work will encourage further research on personalized text detection.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12474v1": {
    "title": "SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression",
    "url": "https://www.alphaxiv.org/abs/2510.12474v1",
    "arxiv_id": "2510.12474v1",
    "authors": "Biao Zhang, Lixin Chen, Tong Liu, Bo Zheng",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-14 13:04:22",
    "ori_summary": "Large language models (LLMs) generate high-dimensional embeddings that capture rich semantic and syntactic information. However, high-dimensional embeddings exacerbate computational complexity and storage requirements, thereby hindering practical deployment. To address these challenges, we propose a novel training framework named Sequential Matryoshka Embedding Compression (SMEC). This framework introduces the Sequential Matryoshka Representation Learning(SMRL) method to mitigate gradient variance during training, the Adaptive Dimension Selection (ADS) module to reduce information degradation during dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module to enhance unsupervised learning between high- and low-dimensional embeddings. Experiments on image, text, and multimodal datasets demonstrate that SMEC achieves significant dimensionality reduction while maintaining performance. For instance, on the BEIR dataset, our approach improves the performance of compressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points compared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12463v1": {
    "title": "Resource-sensitive but language-blind: Community size and not grammatical complexity better predicts the accuracy of Large Language Models in a novel Wug Test",
    "url": "https://www.alphaxiv.org/abs/2510.12463v1",
    "arxiv_id": "2510.12463v1",
    "authors": "Nikoleta Pantelidou, Evelina Leivada, Paolo Morosi",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 12:52:57",
    "ori_summary": "The linguistic abilities of Large Language Models are a matter of ongoing debate. This study contributes to this discussion by investigating model performance in a morphological generalization task that involves novel words. Using a multilingual adaptation of the Wug Test, six models were tested across four partially unrelated languages (Catalan, English, Greek, and Spanish) and compared with human speakers. The aim is to determine whether model accuracy approximates human competence and whether it is shaped primarily by linguistic complexity or by the quantity of available training data. Consistent with previous research, the results show that the models are able to generalize morphological processes to unseen words with human-like accuracy. However, accuracy patterns align more closely with community size and data availability than with structural complexity, refining earlier claims in the literature. In particular, languages with larger speaker communities and stronger digital representation, such as Spanish and English, revealed higher accuracy than less-resourced ones like Catalan and Greek. Overall, our findings suggest that model behavior is mainly driven by the richness of linguistic resources rather than by sensitivity to grammatical complexity, reflecting a form of performance that resembles human linguistic competence only superficially.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12460v1": {
    "title": "Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12460v1",
    "arxiv_id": "2510.12460v1",
    "authors": "Linfeng Gao, Baolong Bi, Zheng Yuan, Le Wang, Zerui Chen, Zhimin Wei, Shenghua Liu, Qinggang Zhang, Jinsong Su",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 12:48:24",
    "ori_summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to enhance the factuality of Large Language Models (LLMs). However, existing RAG systems often suffer from an unfaithfulness issue, where the model's response contradicts evidence from the retrieved context. Existing approaches to improving contextual faithfulness largely rely on external interventions, such as prompt engineering, decoding constraints, or reward-based fine-tuning. These works treat the LLM as a black box and overlook a crucial question: how does the LLM internally integrate retrieved evidence with its parametric memory, particularly under knowledge conflicts? To address this gap, we conduct a probing-based analysis of hidden-state representations in LLMs and observe three findings: knowledge integration occurs hierarchically, conflicts manifest as latent signals at the sentence level, and irrelevant context is often amplified when aligned with parametric knowledge. Building on these findings, we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a framework that (i) decomposes context into fine-grained sentence-level knowledge, (ii) employs hidden-state probing to localize conflicting knowledge, and (iii) introduces conflict-aware fine-tuning to guide the model to accurately integrate retrieved evidence. Extensive experiments across three benchmarks demonstrate that CLEAR substantially improves both accuracy and contextual faithfulness, consistently outperforming strong baselines under diverse conflict conditions. The related resources are available at https://github.com/LinfengGao/CLEAR.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12434v1": {
    "title": "PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for Retrieval-Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12434v1",
    "arxiv_id": "2510.12434v1",
    "authors": "Xiangjun Zai, Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Wenjie Zhang",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 12:13:23",
    "ori_summary": "Knowledge Hypergraphs (KHs) have recently emerged as a knowledge representation for retrieval-augmented generation (RAG), offering a paradigm to model multi-entity relations into a structured form. However, existing KH-based RAG methods suffer from three major limitations: static retrieval planning, non-adaptive retrieval execution, and superficial use of KH structure and semantics, which constrain their ability to perform effective multi-hop question answering. To overcome these limitations, we propose PRoH, a dynamic Planning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates three core innovations: (i) a context-aware planning module that sketches the local KH neighborhood to guide structurally grounded reasoning plan generation; (ii) a structured question decomposition process that organizes subquestions as a dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive, multi-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided reasoning path retrieval algorithm that prioritizes semantically coherent hyperedge traversals. Experiments across multiple domains demonstrate that PRoH achieves state-of-the-art performance, surpassing the prior SOTA model HyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation (G-E) score, while maintaining strong robustness in long-range multi-hop reasoning tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12389v1": {
    "title": "Tokenization Disparities as Infrastructure Bias: How Subword Systems Create Inequities in LLM Access and Efficiency",
    "url": "https://www.alphaxiv.org/abs/2510.12389v1",
    "arxiv_id": "2510.12389v1",
    "authors": "Hailay Kidu Teklehaymanot, Wolfgang Nejdl",
    "categories": "cs.CL, cs.AI, I.2.7; I.2.1; H.3.3; F.2.2",
    "pub_date": "2025-10-14 11:14:38",
    "ori_summary": "Tokenization disparities pose a significant barrier to achieving equitable access to artificial intelligence across linguistically diverse populations. This study conducts a large-scale cross-linguistic evaluation of tokenization efficiency in over 200 languages to systematically quantify computational inequities in large language models (LLMs). Using a standardized experimental framework, we applied consistent preprocessing and normalization protocols, followed by uniform tokenization through the tiktoken library across all language samples. Comprehensive tokenization statistics were collected using established evaluation metrics, including Tokens Per Sentence (TPS) and Relative Tokenization Cost (RTC), benchmarked against English baselines. Our cross-linguistic analysis reveals substantial and systematic disparities: Latin-script languages consistently exhibit higher tokenization efficiency, while non-Latin and morphologically complex languages incur significantly greater token inflation, often 3-5 times higher RTC ratios. These inefficiencies translate into increased computational costs and reduced effective context utilization for underrepresented languages. Overall, the findings highlight structural inequities in current AI systems, where speakers of low-resource and non-Latin languages face disproportionate computational disadvantages. Future research should prioritize the development of linguistically informed tokenization strategies and adaptive vocabulary construction methods that incorporate typological diversity, ensuring more inclusive and computationally equitable multilingual AI systems.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12367v1": {
    "title": "LLM-REVal: Can We Trust LLM Reviewers Yet?",
    "url": "https://www.alphaxiv.org/abs/2510.12367v1",
    "arxiv_id": "2510.12367v1",
    "authors": "Rui Li, Jia-Chen Gu, Po-Nien Kung, Heming Xia, Junfeng liu, Xiangwen Kong, Zhifang Sui, Nanyun Peng",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 10:30:20",
    "ori_summary": "The rapid advancement of large language models (LLMs) has inspired researchers to integrate them extensively into the academic workflow, potentially reshaping how research is practiced and reviewed. While previous studies highlight the potential of LLMs in supporting research and peer review, their dual roles in the academic workflow and the complex interplay between research and review bring new risks that remain largely underexplored. In this study, we focus on how the deep integration of LLMs into both peer-review and research processes may influence scholarly fairness, examining the potential risks of using LLMs as reviewers by simulation. This simulation incorporates a research agent, which generates papers and revises, alongside a review agent, which assesses the submissions. Based on the simulation results, we conduct human annotations and identify pronounced misalignment between LLM-based reviews and human judgments: (1) LLM reviewers systematically inflate scores for LLM-authored papers, assigning them markedly higher scores than human-authored ones; (2) LLM reviewers persistently underrate human-authored papers with critical statements (e.g., risk, fairness), even after multiple revisions. Our analysis reveals that these stem from two primary biases in LLM reviewers: a linguistic feature bias favoring LLM-generated writing styles, and an aversion toward critical statements. These results highlight the risks and equity concerns posed to human authors and academic research if LLMs are deployed in the peer review cycle without adequate caution. On the other hand, revisions guided by LLM reviews yield quality gains in both LLM-based and human evaluations, illustrating the potential of the LLMs-as-reviewers for early-stage researchers and enhancing low-quality papers.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12357v1": {
    "title": "MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture of Big Little Experts",
    "url": "https://www.alphaxiv.org/abs/2510.12357v1",
    "arxiv_id": "2510.12357v1",
    "authors": "Yushu Zhao, Yubin Qin, Yang Wang, Xiaolong Yang, Huiming Han, Shaojun Wei, Yang Hu, Shouyi Yin",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 10:22:44",
    "ori_summary": "Mixture-of-Experts (MoE) models have recently demonstrated exceptional performance across a diverse range of applications. The principle of sparse activation in MoE models facilitates an offloading strategy, wherein active experts are maintained in GPU HBM, while inactive experts are stored in CPU DRAM. The efficacy of this approach, however, is fundamentally constrained by the limited bandwidth of the CPU-GPU interconnect. To mitigate this bottleneck, existing approaches have employed prefetching to accelerate MoE inference. These methods attempt to predict and prefetch the required experts using specially trained modules. Nevertheless, such techniques are often encumbered by significant training overhead and have shown diminished effectiveness on recent MoE models with fine-grained expert segmentation. In this paper, we propose MoBiLE, a plug-and-play offloading-based MoE inference framework with \\textit{mixture of big-little experts}. It reduces the number of experts for unimportant tokens to half for acceleration while maintaining full experts for important tokens to guarantee model quality. Further, a dedicated fallback and prefetching mechanism is designed for switching between little and big experts to improve memory efficiency. We evaluate MoBiLE on four typical modern MoE architectures and challenging generative tasks. Our results show that MoBiLE achieves a speedup of 1.60x to 1.72x compared to the baseline on a consumer GPU system, with negligible degradation in accuracy.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12355v1": {
    "title": "Fine-grained Analysis of Brain-LLM Alignment through Input Attribution",
    "url": "https://www.alphaxiv.org/abs/2510.12355v1",
    "arxiv_id": "2510.12355v1",
    "authors": "Michela Proietti, Roberto Capobianco, Mariya Toneva",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 10:19:01",
    "ori_summary": "Understanding the alignment between large language models (LLMs) and human brain activity can reveal computational principles underlying language processing. We introduce a fine-grained input attribution method to identify the specific words most important for brain-LLM alignment, and leverage it to study a contentious research question about brain-LLM alignment: the relationship between brain alignment (BA) and next-word prediction (NWP). Our findings reveal that BA and NWP rely on largely distinct word subsets: NWP exhibits recency and primacy biases with a focus on syntax, while BA prioritizes semantic and discourse-level information with a more targeted recency effect. This work advances our understanding of how LLMs relate to human language processing and highlights differences in feature reliance between BA and NWP. Beyond this study, our attribution method can be broadly applied to explore the cognitive relevance of model predictions in diverse language processing tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12316v1": {
    "title": "Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12316v1",
    "arxiv_id": "2510.12316v1",
    "authors": "Greta Damo, Elena Cabrio, Serena Villata",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 09:20:01",
    "ori_summary": "Counter-speech generation is at the core of many expert activities, such as fact-checking and hate speech, to counter harmful content. Yet, existing work treats counter-speech generation as pure text generation task, mainly based on Large Language Models or NGO experts. These approaches show severe drawbacks due to the limited reliability and coherence in the generated countering text, and in scalability, respectively. To close this gap, we introduce a novel framework to model counter-speech generation as knowledge-wise text generation process. Our framework integrates advanced Retrieval-Augmented Generation (RAG) pipelines to ensure the generation of trustworthy counter-speech for 8 main target groups identified in the hate speech literature, including women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, and other. We built a knowledge base over the United Nations Digital Library, EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792 texts. We use the MultiTarget-CONAN dataset to empirically assess the quality of the generated counter-speech, both through standard metrics (i.e., JudgeLM) and a human evaluation. Results show that our framework outperforms standard LLM baselines and competitive approach, on both assessments. The resulting framework and the knowledge base pave the way for studying trustworthy and sound counter-speech generation, in hate speech and beyond.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12306v1": {
    "title": "A large-scale, unsupervised pipeline for automatic corpus annotation using LLMs: variation and change in the English consider construction",
    "url": "https://www.alphaxiv.org/abs/2510.12306v1",
    "arxiv_id": "2510.12306v1",
    "authors": "Cameron Morin, Matti Marttinen Larsson",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 09:06:14",
    "ori_summary": "As natural language corpora expand at an unprecedented rate, manual annotation remains a significant methodological bottleneck in corpus linguistic work. We address this challenge by presenting a scalable, unsupervised pipeline for automating grammatical annotation in voluminous corpora using large language models (LLMs). Unlike previous supervised and iterative approaches, our method employs a four-phase workflow: prompt engineering, pre-hoc evaluation, automated batch processing, and post-hoc validation. We demonstrate the pipeline's accessibility and effectiveness through a diachronic case study of variation in the English consider construction. Using GPT-5 through the OpenAI API, we annotate 143,933 sentences from the Corpus of Historical American English (COHA) in under 60 hours, achieving 98%+ accuracy on two sophisticated annotation procedures. Our results suggest that LLMs can perform a range of data preparation tasks at scale with minimal human intervention, opening new possibilities for corpus-based research, though implementation requires attention to costs, licensing, and other ethical considerations.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12287v1": {
    "title": "Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual Projector",
    "url": "https://www.alphaxiv.org/abs/2510.12287v1",
    "arxiv_id": "2510.12287v1",
    "authors": "Sifan Li, Hongkai Chen, Yujun Cai, Qingwen Ye, Liyang Chen, Junsong Yuan, Yiwei Wang",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-14 08:42:58",
    "ori_summary": "Vision Language Models (VLMs) have achieved impressive progress in multimodal reasoning; yet, they remain vulnerable to hallucinations, where outputs are not grounded in visual evidence. In this paper, we investigate a previously overlooked setting: logo hallucination, where models generate brand names or textual content despite logos containing no visible words. Using curated splits of pure symbols, hybrids, and text-bearing logos, as well as the challenging Hard-60 subset, we systematically measure hallucination across leading VLMs. We further probe robustness through nine structured perturbations and show that hallucinations persist even under strong distortions, with occlusion exposing the sharpest weaknesses. Embedding-level analysis with open-weight LLaVA demonstrates that hallucination is tied to a small subset of projector dimensions, and targeted ablation substantially reduces errors while preserving OCR accuracy. Together, these findings reveal that VLMs often rely on symbolic priors rather than genuine glyph perception, particularly for iconic circular logos, and that projector subspaces play a decisive role in this failure mode. Our work contributes both a novel diagnostic lens and actionable mitigation insights, highlighting projector disentanglement and OCR-guided decoding as promising directions for building more trustworthy multimodal systems.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12285v1": {
    "title": "Chinese ModernBERT with Whole-Word Masking",
    "url": "https://www.alphaxiv.org/abs/2510.12285v1",
    "arxiv_id": "2510.12285v1",
    "authors": "Zeyu Zhao, Ningtao Wang, Xing Fu, Yu Cheng",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 08:41:22",
    "ori_summary": "Encoder-only Transformers have advanced along three axes -- architecture, data, and systems -- yielding Pareto gains in accuracy, speed, and memory efficiency. Yet these improvements have not fully transferred to Chinese, where tokenization and morphology differ markedly from English. We introduce Chinese ModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware 32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the embedding budget; (ii) whole-word masking (WWM) with a dynamic masking curriculum (30% -> 15%) to align task difficulty with training progress; (iii) a two-stage pre-training pipeline that extends the native context from 1,024 to 8,192 tokens using RoPE and alternating local/global attention; and (iv) a damped-cosine learning-rate schedule for stable long-horizon optimization. We pre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and Cosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong Chinese encoders under a unified fine-tuning protocol. Under bf16 it achieves high long-sequence throughput while maintaining strong short-sequence speed, reflecting benefits from budget allocation and attention design. To probe retrieval-oriented quality, we add a small amount of open contrastive data: fine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking (~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set. Under this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding on SimCLUE, suggesting a clear scaling path for STS with additional curated pairs. We will release tokenizer and weights to facilitate reproducible research.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12255v1": {
    "title": "Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.12255v1",
    "arxiv_id": "2510.12255v1",
    "authors": "Blazej Manczak, Eric Lin, Francisco Eiras, James O' Neill, Vaikkunth Mugunthan",
    "categories": "cs.CL, cs.AI, I.2.7; I.2.6; J.3",
    "pub_date": "2025-10-14 08:04:18",
    "ori_summary": "Large language models (LLMs) are rapidly transitioning into medical clinical use, yet their reliability under realistic, multi-turn interactions remains poorly understood. Existing evaluation frameworks typically assess single-turn question answering under idealized conditions, overlooking the complexities of medical consultations where conflicting input, misleading context, and authority influence are common. We introduce MedQA-Followup, a framework for systematically evaluating multi-turn robustness in medical question answering. Our approach distinguishes between shallow robustness (resisting misleading initial context) and deep robustness (maintaining accuracy when answers are challenged across turns), while also introducing an indirect-direct axis that separates contextual framing (indirect) from explicit suggestion (direct). Using controlled interventions on the MedQA dataset, we evaluate five state-of-the-art LLMs and find that while models perform reasonably well under shallow perturbations, they exhibit severe vulnerabilities in multi-turn settings, with accuracy dropping from 91.2% to as low as 13.5% for Claude Sonnet 4. Counterintuitively, indirect, context-based interventions are often more harmful than direct suggestions, yielding larger accuracy drops across models and exposing a significant vulnerability for clinical deployment. Further compounding analyses reveal model differences, with some showing additional performance drops under repeated interventions while others partially recovering or even improving. These findings highlight multi-turn robustness as a critical but underexplored dimension for safe and reliable deployment of medical LLMs.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12251v1": {
    "title": "DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.12251v1",
    "arxiv_id": "2510.12251v1",
    "authors": "Jiakai Li, Rongzheng Wang, Yizhuo Ma, Shuang Liang, Guangchun Luo, Ke Qin",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 08:01:59",
    "ori_summary": "While large language models (LLMs) show considerable promise across various fields, they have notable limitations in handling multi-document question answering (Multi-doc QA) tasks. The first challenge is long-range dependency modeling, where LLMs struggle to focus on key information in long texts, which weakens important semantic connections. Second, most LLMs suffer from the ''lost-in-the-middle'' issue, where they have difficulty processing information in the middle of long inputs. Current solutions either truncate global dependencies or demand costly finetuning, ultimately lacking a universal and simple solution for these challenges. To resolve these limitations, we propose Dual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The Contextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by assessing paragraph relevance through layer-wise attention tracking and position-aware weighting. (ii) The Reciprocal Attention Suppression (RAS) module enhances focus on critical paragraphs by suppressing information exchange between key and irrelevant texts, thus mitigating the limitations in long-range dependency modeling. Notably, DSAS functions as a plug-and-play solution requiring no architectural modifications or extra training parameters. Extensive experiments on four benchmarks demonstrate DSAS's efficacy across mainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score improvement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and Qwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of both the CGW and RAS modules. In addition, detailed discussions in the Appendix further validate the robustness and scalability of DSAS.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12229v1": {
    "title": "Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability",
    "url": "https://www.alphaxiv.org/abs/2510.12229v1",
    "arxiv_id": "2510.12229v1",
    "authors": "Bianca Raimondi, Daniela Dalbagno, Maurizio Gabbrielli",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 07:31:29",
    "ori_summary": "Large language models (LLMs) have been shown to internalize human-like biases during finetuning, yet the mechanisms by which these biases manifest remain unclear. In this work, we investigated whether the well-known Knobe effect, a moral bias in intentionality judgements, emerges in finetuned LLMs and whether it can be traced back to specific components of the model. We conducted a Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the bias is not only learned during finetuning but also localized in a specific set of layers. Surprisingly, we found that patching activations from the corresponding pretrained model into just a few critical layers is sufficient to eliminate the effect. Our findings offer new evidence that social biases in LLMs can be interpreted, localized, and mitigated through targeted interventions, without the need for model retraining.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12217v1": {
    "title": "HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment",
    "url": "https://www.alphaxiv.org/abs/2510.12217v1",
    "arxiv_id": "2510.12217v1",
    "authors": "Ali Mekky, Omar El Herraoui, Preslav Nakov, Yuxia Wang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 07:13:26",
    "ori_summary": "Large language models (LLMs) are increasingly deployed across high-impact domains, from clinical decision support and legal analysis to hiring and education, making fairness and bias evaluation before deployment critical. However, existing evaluations lack grounding in real-world scenarios and do not account for differences in harm severity, e.g., a biased decision in surgery should not be weighed the same as a stylistic bias in text summarization. To address this gap, we introduce HALF (Harm-Aware LLM Fairness), a deployment-aligned framework that assesses model bias in realistic applications and weighs the outcomes by harm severity. HALF organizes nine application domains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline. Our evaluation results across eight LLMs show that (1) LLMs are not consistently fair across domains, (2) model size or performance do not guarantee fairness, and (3) reasoning models perform better in medical decision support but worse in education. We conclude that HALF exposes a clear gap between previous benchmarking success and deployment readiness.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12210v1": {
    "title": "DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12210v1",
    "arxiv_id": "2510.12210v1",
    "authors": "Yakun Song, Xiaobin Zhuang, Jiawei Chen, Zhikang Niu, Guanrou Yang, Chenpeng Du, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen",
    "categories": "eess.AS, cs.CL, cs.LG",
    "pub_date": "2025-10-14 07:03:29",
    "ori_summary": "Recent attempts to interleave autoregressive (AR) sketchers with diffusion-based refiners over continuous speech representations have shown promise, but they remain brittle under distribution shift and offer limited levers for controllability. We introduce DISTAR, a zero-shot text-to-speech framework that operates entirely in a discrete residual vector quantization (RVQ) code space and tightly couples an AR language model with a masked diffusion model, without forced alignment or a duration predictor. Concretely, DISTAR drafts block-level RVQ tokens with an AR language model and then performs parallel masked-diffusion infilling conditioned on the draft to complete the next block, yielding long-form synthesis with blockwise parallelism while mitigating classic AR exposure bias. The discrete code space affords explicit control at inference: DISTAR produces high-quality audio under both greedy and sample-based decoding using classifier-free guidance, supports trade-offs between robustness and diversity, and enables variable bit-rate and controllable computation via RVQ layer pruning at test time. Extensive experiments and ablations demonstrate that DISTAR surpasses state-of-the-art zero-shot TTS systems in robustness, naturalness, and speaker/style consistency, while maintaining rich output diversity. Audio samples are provided on https://anonymous.4open.science/w/DiSTAR_demo.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12200v1": {
    "title": "HackWorld: Evaluating Computer-Use Agents on Exploiting Web Application Vulnerabilities",
    "url": "https://www.alphaxiv.org/abs/2510.12200v1",
    "arxiv_id": "2510.12200v1",
    "authors": "Xiaoxue Ren, Penghao Jiang, Kaixin Li, Zhiyong Huang, Xiaoning Du, Jiaojiao Jiang, Zhenchang Xing, Jiamou Sun, Terry Yue Zhuo",
    "categories": "cs.CR, cs.CL",
    "pub_date": "2025-10-14 06:52:15",
    "ori_summary": "Web applications are prime targets for cyberattacks as gateways to critical services and sensitive data. Traditional penetration testing is costly and expertise-intensive, making it difficult to scale with the growing web ecosystem. While language model agents show promise in cybersecurity, modern web applications demand visual understanding, dynamic content handling, and multi-step interactions that only computer-use agents (CUAs) can perform. Yet, their ability to discover and exploit vulnerabilities through graphical interfaces remains largely unexplored. We present HackWorld, the first framework for systematically evaluating CUAs' capabilities to exploit web application vulnerabilities via visual interaction. Unlike sanitized benchmarks, HackWorld includes 36 real-world applications across 11 frameworks and 7 languages, featuring realistic flaws such as injection vulnerabilities, authentication bypasses, and unsafe input handling. Using a Capture-the-Flag (CTF) setup, it tests CUAs' capacity to identify and exploit these weaknesses while navigating complex web interfaces. Evaluation of state-of-the-art CUAs reveals concerning trends: exploitation rates below 12% and low cybersecurity awareness. CUAs often fail at multi-step attack planning and misuse security tools. These results expose the current limitations of CUAs in web security contexts and highlight opportunities for developing more security-aware agents capable of effective vulnerability detection and exploitation.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12195v1": {
    "title": "DPO-Tuned Large Language Models for Segmentation in Simultaneous Speech Translation",
    "url": "https://www.alphaxiv.org/abs/2510.12195v1",
    "arxiv_id": "2510.12195v1",
    "authors": "Zeyu Yang, Satoshi Nakamura",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 06:41:36",
    "ori_summary": "Simultaneous speech translation requires accurate segmentation to balance translation quality and latency. Recent studies such as SHAS have introduced pretrained segmentation models, achieving stronger performance than heuristic rules. However, segmentation models such as SHAS, though pretrained and more robust than heuristic methods, are still constrained by supervised learning objectives and do not incorporate human preference alignment, which is crucial for natural real-time interpretation. In this work, we propose a segmentation framework based on large language models (LLMs) trained with Direct Preference Optimization (DPO). By leveraging preference alignment, our method enables LLMs to predict natural segmentation points that better meet the demands of real-time translation. We evaluate the system on the ACL 60/60 corpus across three language pairs (English-Japanese, Chinese, German), using SeamlessM4T v2 as the translation backbone. Experimental results show that our DPO-tuned LLM achieves higher segmentation accuracy than SHAS and yields consistent improvements in translation quality (BLEU, COMET) as well as latency (Average Lagging). Furthermore, our system benefits from IWSLT baselines for direct comparison. These findings highlight the potential of preference-tuned LLMs to surpass existing pretrained segmentation models and advance adaptive, human-aligned simultaneous interpretation.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12185v1": {
    "title": "Not in Sync: Unveiling Temporal Bias in Audio Chat Models",
    "url": "https://www.alphaxiv.org/abs/2510.12185v1",
    "arxiv_id": "2510.12185v1",
    "authors": "Jiayu Yao, Shenghua Liu, Yiwei Wang, Rundong Cheng, Lingrui Mei, Baolong Bi, Zhen Xiong, Xueqi Cheng",
    "categories": "cs.CL, cs.SD",
    "pub_date": "2025-10-14 06:29:40",
    "ori_summary": "Large Audio Language Models (LALMs) are increasingly applied to audio understanding and multimodal reasoning, yet their ability to locate when events occur remains underexplored. We present the first systematic study of temporal bias in LALMs, revealing a key limitation in their timestamp prediction. For example, when asked \"At which second does the lecturer introduce the key formula?\", models often predict timestamps that are consistently earlier or later than the ground truth. Through controlled experiments on timestamped datasets, we find that temporal bias (i) is prevalent across datasets and models, (ii) increases with audio length - even accumulating to tens of seconds in extended recordings, and (iii) varies across event types and positions. We quantify this effect with the Temporal Bias Index (TBI), measuring systematic misalignment in predicted event timings, and complement it with a visualization framework. Our findings highlight a fundamental limitation in current LALMs and call for the development of temporally robust architectures.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12181v1": {
    "title": "From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing",
    "url": "https://www.alphaxiv.org/abs/2510.12181v1",
    "arxiv_id": "2510.12181v1",
    "authors": "Chengrui Xiang, Tengfei Ma, Xiangzheng Fu, Yiping Liu, Bosheng Song, Xiangxiang Zeng",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 06:15:36",
    "ori_summary": "Drug repurposing plays a critical role in accelerating treatment discovery, especially for complex and rare diseases. Biomedical knowledge graphs (KGs), which encode rich clinical associations, have been widely adopted to support this task. However, existing methods largely overlook common-sense biomedical concept knowledge in real-world labs, such as mechanistic priors indicating that certain drugs are fundamentally incompatible with specific treatments. To address this gap, we propose LLaDR, a Large Language Model-assisted framework for Drug Repurposing, which improves the representation of biomedical concepts within KGs. Specifically, we extract semantically enriched treatment-related textual representations of biomedical entities from large language models (LLMs) and use them to fine-tune knowledge graph embedding (KGE) models. By injecting treatment-relevant knowledge into KGE, LLaDR largely improves the representation of biomedical concepts, enhancing semantic understanding of under-studied or complex indications. Experiments based on benchmarks demonstrate that LLaDR achieves state-of-the-art performance across different scenarios, with case studies on Alzheimer's disease further confirming its robustness and effectiveness. Code is available at https://github.com/xiaomingaaa/LLaDR.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12178v1": {
    "title": "Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey",
    "url": "https://www.alphaxiv.org/abs/2510.12178v1",
    "arxiv_id": "2510.12178v1",
    "authors": "Abdulhady Abas Abdullah, Arkaitz Zubiaga, Seyedali Mirjalili, Amir H. Gandomi, Fatemeh Daneshfar, Mohammadsadra Amini, Alan Salam Mohammed, Hadi Veisi",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-14 06:12:44",
    "ori_summary": "This review surveys the rapid evolution of Meta AI's LLaMA (Large Language Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized parameter-efficient fine-tuning (PEFT) methods developed for these models. We first describe the LLaMA family of foundation models (7B-65B to 288B parameters), their architectures (including native multimodal and Mixtureof-Experts variants), and key performance characteristics. We then describe and discuss the concept of PEFT, which adapts large pre-trained models by updating only a small subset of parameters, and review five PEFT methods that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1 and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's mechanism, parameter savings, and example application to LLaMA (e.g., instruction tuning, multimodal tasks). We provide structured discussion and analysis of model and adapter architectures, parameter counts, and benchmark results (including examples where fine-tuned LLaMA models outperform larger baselines). Finally, we examine real-world use cases where LLaMA-based models and PEFT have been successfully applied (e.g., legal and medical domains), and we discuss ongoing challenges and future research directions (such as scaling to even larger contexts and improving robustness). This survey paper provides a one-stop resource for ML researchers and practitioners interested in LLaMA models and efficient fine-tuning strategies.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12167v1": {
    "title": "Towards Inference-time Scaling for Continuous Space Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.12167v1",
    "arxiv_id": "2510.12167v1",
    "authors": "Minghan Wang, Thuy-Trang Vu, Ehsan Shareghi, Gholamreza Haffari",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 05:53:41",
    "ori_summary": "Inference-time scaling through multiple sample generation in combination with Process- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective for text-based reasoning in large language models. This paper investigates whether such established techniques can be successfully adapted to reasoning in the continuous space, using COCONUT (Hao et al. 2024) continuous space reasoning LM as the backbone. We demonstrate the feasibility of generating diverse reasoning paths through dropout-based sampling. Our Pass@N analysis on the generated samples reveals the potential that could enable a significant gain in performance akin to observed gain in the discrete space. However, we highlight unique challenges faced for materializing this gain in the continuous thought space. In particular, working recipes for data generation and training PRM and ORM models in the discrete space unlocks only marginal improvements in the continuous space. Through probing various aspects including geometric properties and trajectory dynamics we identify the underlying reasons that prevent effective discrimination between correct and incorrect reasoning (essential for the functioning of PRM and ORM). Our findings reveal that current limitations stem from the absence of key inductive biases in continuous thought representations. We argue that the training frameworks for continuous reasoning LMs require not only to optimize for accuracy but also to explicitly incorporate inductive biases that could be utilized during inference-time for discrimination of correct and incorrect thoughts.\\footnote{Our code and data will be publicly available.}",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12164v1": {
    "title": "A Survey on Parallel Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.12164v1",
    "arxiv_id": "2510.12164v1",
    "authors": "Ziqi Wang, Boye Niu, Zipeng Gao, Zhi Zheng, Tong Xu, Linghui Meng, Zhongli Li, Jing Liu, Yilong Chen, Chen Zhu, Hua Wu, Haifeng Wang, Enhong Chen",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 05:42:19",
    "ori_summary": "With the increasing capabilities of Large Language Models (LLMs), parallel reasoning has emerged as a new inference paradigm that enhances reasoning robustness by concurrently exploring multiple lines of thought before converging on a final answer. It has become a significant trend to explore parallel reasoning to overcome the fragility of standard sequential methods and improve practical performance. In this paper, we aim to survey and summarize the progress and challenges of parallel reasoning. We first present a formal definition of parallel reasoning and clarify its distinction from related concepts like Chain-of-Thought. Then, we organize and discuss advanced techniques based on a novel taxonomy, including non-interactive reasoning, interactive reasoning, and efficiency-focused decoding strategies. Additionally, we explore various application scenarios, such as solving complex problems and enhancing the reliability of LLM outputs.Finally, we highlight the core challenges of parallel reasoning and suggest potential directions for future research. We hope that our work can provide a useful roadmap for beginners and encourage more research on improving parallel reasoning methods. Related source can be avaliable in https://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12137v1": {
    "title": "Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.12137v1",
    "arxiv_id": "2510.12137v1",
    "authors": "Shihao Ji, Zihui Song, Jiajie Huang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 04:31:49",
    "ori_summary": "Large Language Models (LLMs) hallucinate, generating factually incorrect yet confident assertions. We argue this stems from the Transformer's Softmax function, which creates \"Artificial Certainty\" by collapsing ambiguous attention scores into a single probability distribution, discarding uncertainty information at each layer. To fix this, we introduce the Credal Transformer, which replaces standard attention with a Credal Attention Mechanism (CAM) based on evidential theory. CAM produces a \"credal set\" (a set of distributions) instead of a single attention vector, with the set's size directly measuring model uncertainty. We implement this by re-conceptualizing attention scores as evidence masses for a Dirichlet distribution: sufficient evidence recovers standard attention, while insufficient evidence yields a diffuse distribution, representing ambiguity. Empirically, the Credal Transformer identifies out-of-distribution inputs, quantifies ambiguity, and significantly reduces confident errors on unanswerable questions by abstaining. Our contribution is a new architecture to mitigate hallucinations and a design paradigm that integrates uncertainty quantification directly into the model, providing a foundation for more reliable AI.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12133v1": {
    "title": "SafeMT: Multi-turn Safety for Multimodal Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.12133v1",
    "arxiv_id": "2510.12133v1",
    "authors": "Han Zhu, Juntao Dai, Jiaming Ji, Haoran Li, Chengkun Cai, Pengcheng Wen, Chi-Min Chan, Boyuan Chen, Yaodong Yang, Sirui Han, Yike Guo",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 04:24:07",
    "ori_summary": "With the widespread use of multi-modal Large Language models (MLLMs), safety issues have become a growing concern. Multi-turn dialogues, which are more common in everyday interactions, pose a greater risk than single prompts; however, existing benchmarks do not adequately consider this situation. To encourage the community to focus on the safety issues of these models in multi-turn dialogues, we introduce SafeMT, a benchmark that features dialogues of varying lengths generated from harmful queries accompanied by images. This benchmark consists of 10,000 samples in total, encompassing 17 different scenarios and four jailbreak methods. Additionally, we propose Safety Index (SI) to evaluate the general safety of MLLMs during conversations. We assess the safety of 17 models using this benchmark and discover that the risk of successful attacks on these models increases as the number of turns in harmful dialogues rises. This observation indicates that the safety mechanisms of these models are inadequate for recognizing the hazard in dialogue interactions. We propose a dialogue safety moderator capable of detecting malicious intent concealed within conversations and providing MLLMs with relevant safety policies. Experimental results from several open-source models indicate that this moderator is more effective in reducing multi-turn ASR compared to existed guard models.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12121v1": {
    "title": "Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing",
    "url": "https://www.alphaxiv.org/abs/2510.12121v1",
    "arxiv_id": "2510.12121v1",
    "authors": "Rongzhi Zhang, Liqin Ye, Yuzhao Heng, Xiang Chen, Tong Yu, Lingkai Kong, Sudheer Chava, Chao Zhang",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-14 03:50:22",
    "ori_summary": "Precise attribute intensity control--generating Large Language Model (LLM) outputs with specific, user-defined attribute intensities--is crucial for AI systems adaptable to diverse user expectations. Current LLM alignment methods, however, typically provide only directional or open-ended guidance, failing to reliably achieve exact attribute intensities. We address this limitation with three key designs: (1) reformulating precise attribute intensity control as a target-reaching problem, rather than simple maximization; (2) training a lightweight value function via temporal-difference learning to predict final attribute intensity scores from partial generations, thereby steering LLM outputs; and (3) employing gradient-based interventions on hidden representations to navigate the model precisely towards specific attribute intensity targets. Our method enables fine-grained, continuous control over attribute intensities, moving beyond simple directional alignment. Experiments on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text generation to user-specified attribute intensities with high accuracy. Finally, we demonstrate efficiency enhancements across three downstream tasks: preference data synthesis, Pareto frontier approximation and optimization, and distillation of aligned behaviors for intervention-free inference. Our code is available on https://github.com/Pre-Control/pre-control",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12116v1": {
    "title": "Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.12116v1",
    "arxiv_id": "2510.12116v1",
    "authors": "Bajian Xiang, Shuaijiang Zhao, Tingwei Guo, Wei Zou",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 03:34:38",
    "ori_summary": "End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive conversational generation abilities, yet consistently fall short of traditional pipeline systems on semantic understanding benchmarks. In this work, we reveal through systematic experimentation that although LSLMs lose some text input performance after speech-text alignment training, the performance gap between speech and text inputs is more pronounced, which we refer to as the modality gap. To understand this gap, we analyze both coarse- and fine-grained text and speech representations. At the coarse-grained level, representations of speech and text in deeper layers are found to be increasingly aligned in direction (cosine similarity), while concurrently diverging in magnitude (Euclidean distance). We further find that representation similarity is strongly correlated with the modality gap. At the fine-grained level, a spontaneous token-level alignment pattern between text and speech representations is observed. Based on this, we introduce the Alignment Path Score to quantify token-level alignment quality, which exhibits stronger correlation with the modality gap. Building on these insights, we design targeted interventions on critical tokens through angle projection and length normalization. These strategies demonstrate the potential to improve correctness for speech inputs. Our study provides the first systematic empirical analysis of the modality gap and alignment mechanisms in LSLMs, offering both theoretical and methodological guidance for future optimization.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12115v1": {
    "title": "Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation",
    "url": "https://www.alphaxiv.org/abs/2510.12115v1",
    "arxiv_id": "2510.12115v1",
    "authors": "Xin Zhao, Naoki Yoshinaga, Yuma Tsuta, Akiko Aizawa",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 03:34:17",
    "ori_summary": "Multilingual domain adaptation (ML-DA) is widely used to learn new domain knowledge across languages into large language models (LLMs). Although many methods have been proposed to improve domain adaptation, the mechanisms of multilingual knowledge acquisition, how domain knowledge is learned within a language and transferred across languages, remain underexplored. This gap leads to suboptimal performance, particularly in low-resource settings. This work examines the learning dynamics of LLMs during ML-DA. Because prior ML-DA studies often train and evaluate on datasets with mismatched knowledge coverage, we propose AdaXEval, an adaptive evaluation method that builds multiple-choice QA datasets from the same bilingual domain corpus used for training, thereby directly studying multilingual knowledge acquisition. Through continual training of LLMs with diverse data recipes, we track how LLMs acquire domain facts and pinpoint the mechanism behind the transformation process from domain training data to knowledge. Our experiments on a 13B English-Japanese bilingual LLM reveal that cross-lingual transfer remains challenging despite a high-quality bilingual corpus. The code has been released.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12110v1": {
    "title": "Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.12110v1",
    "arxiv_id": "2510.12110v1",
    "authors": "Ziliang Qiu, Renfen Hu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 03:26:28",
    "ori_summary": "The evaluation of LLMs' creativity represents a crucial research domain, though challenges such as data contamination and costly human assessments often impede progress. Drawing inspiration from human creativity assessment, we propose PACE, asking LLMs to generate Parallel Association Chains to Evaluate their creativity. PACE minimizes the risk of data contamination and offers a straightforward, highly efficient evaluation, as evidenced by its strong correlation with Chatbot Arena Creative Writing rankings (Spearman's $\\rho = 0.739$, $p < 0.001$) across various proprietary and open-source models. A comparative analysis of associative creativity between LLMs and humans reveals that while high-performing LLMs achieve scores comparable to average human performance, professional humans consistently outperform LLMs. Furthermore, linguistic analysis reveals that both humans and LLMs exhibit a trend of decreasing concreteness in their associations, and humans demonstrating a greater diversity of associative patterns.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12088v1": {
    "title": "One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration",
    "url": "https://www.alphaxiv.org/abs/2510.12088v1",
    "arxiv_id": "2510.12088v1",
    "authors": "Zaid Khan, Archiki Prasad, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-14 02:49:32",
    "ori_summary": "Symbolic world modeling requires inferring and representing an environment's transitional dynamics as an executable program. Prior work has focused on largely deterministic environments with abundant interaction data, simple mechanics, and human guidance. We address a more realistic and challenging setting, learning in a complex, stochastic environment where the agent has only \"one life\" to explore a hostile environment without human guidance. We introduce OneLife, a framework that models world dynamics through conditionally-activated programmatic laws within a probabilistic programming framework. Each law operates through a precondition-effect structure, activating in relevant world states. This creates a dynamic computation graph that routes inference and optimization only through relevant laws, avoiding scaling challenges when all laws contribute to predictions about a complex, hierarchical state, and enabling the learning of stochastic dynamics even with sparse rule activation. To evaluate our approach under these demanding constraints, we introduce a new evaluation protocol that measures (a) state ranking, the ability to distinguish plausible future states from implausible ones, and (b) state fidelity, the ability to generate future states that closely resemble reality. We develop and evaluate our framework on Crafter-OO, our reimplementation of the Crafter environment that exposes a structured, object-oriented symbolic state and a pure transition function that operates on that state alone. OneLife can successfully learn key environment dynamics from minimal, unguided interaction, outperforming a strong baseline on 16 out of 23 scenarios tested. We also test OneLife's planning ability, with simulated rollouts successfully identifying superior strategies. Our work establishes a foundation for autonomously constructing programmatic world models of unknown, complex environments.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12083v1": {
    "title": "An AI-Based Behavioral Health Safety Filter and Dataset for Identifying Mental Health Crises in Text-Based Conversations",
    "url": "https://www.alphaxiv.org/abs/2510.12083v1",
    "arxiv_id": "2510.12083v1",
    "authors": "Benjamin W. Nelson, Celeste Wong, Matthew T. Silvestrini, Sooyoon Shin, Alanna Robinson, Jessica Lee, Eric Yang, John Torous, Andrew Trister",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 02:47:52",
    "ori_summary": "Large language models often mishandle psychiatric emergencies, offering harmful or inappropriate advice and enabling destructive behaviors. This study evaluated the Verily behavioral health safety filter (VBHSF) on two datasets: the Verily Mental Health Crisis Dataset containing 1,800 simulated messages and the NVIDIA Aegis AI Content Safety Dataset subsetted to 794 mental health-related messages. The two datasets were clinician-labelled and we evaluated performance using the clinician labels. Additionally, we carried out comparative performance analyses against two open source, content moderation guardrails: OpenAI Omni Moderation Latest and NVIDIA NeMo Guardrails. The VBHSF demonstrated, well-balanced performance on the Verily Mental Health Crisis Dataset v1.0, achieving high sensitivity (0.990) and specificity (0.992) in detecting any mental health crises. It achieved an F1-score of 0.939, sensitivity ranged from 0.917-0.992, and specificity was >= 0.978 in identifying specific crisis categories. When evaluated against the NVIDIA Aegis AI Content Safety Dataset 2.0, VBHSF performance remained highly sensitive (0.982) and accuracy (0.921) with reduced specificity (0.859). When compared with the NVIDIA NeMo and OpenAI Omni Moderation Latest guardrails, the VBHSF demonstrated superior performance metrics across both datasets, achieving significantly higher sensitivity in all cases (all p < 0.001) and higher specificity relative to NVIDIA NeMo (p < 0.001), but not to OpenAI Omni Moderation Latest (p = 0.094). NVIDIA NeMo and OpenAI Omni Moderation Latest exhibited inconsistent performance across specific crisis types, with sensitivity for some categories falling below 0.10. Overall, the VBHSF demonstrated robust, generalizable performance that prioritizes sensitivity to minimize missed crises, a crucial feature for healthcare applications.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12063v1": {
    "title": "ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.12063v1",
    "arxiv_id": "2510.12063v1",
    "authors": "Sunzhu Li, Zhiyu Lin, Shuling Yang, Jiale Zhao, Wei Chen",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-14 02:02:19",
    "ori_summary": "Large Reasoning Models (LRMs) are powerful, but they still suffer from inefficient and off-target reasoning. Currently, training-free methods are limited to either rigid heuristics or descriptive, non-actionable analyses. In this paper, we introduce ThinkPilot, a training-free framework that automatically optimizes LRMs reasoning. It uses an evolutionary process to generate think-prefixes, which are instructions that evolve driven by a taxonomy of reasoning behaviors to guide models toward superior performance. Extensive experiments demonstrate ThinkPilot's broad effectiveness: it significantly improves the accuracy-length trade-off for efficient reasoning, drastically improves safety (for example, cutting the StrongREJECT score of DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction following. It also synergizes with existing training-based methods. Our analysis reveals that think-prefixes can reliably control LRMs' reasoning behaviors, and that different tasks have strong preferences for specific behavioral distributions. By automatically identifying and eliciting these behaviors, ThinkPilot provides a generalizable framework for aligning LRMs reasoning with task demands. Data and code are available at https://github.com/teqkilla/ThinkPilot",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12051v1": {
    "title": "APCE: Adaptive Progressive Context Expansion for Long Context Processing",
    "url": "https://www.alphaxiv.org/abs/2510.12051v1",
    "arxiv_id": "2510.12051v1",
    "authors": "Baisub Lee, Sanghyun Byun, Mohanad Odema, Jung Guack, Jacob Song, Woo Seong Chung",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 01:26:36",
    "ori_summary": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing two key challenges: (1) A growing memory footprint due to quadratic self-attention and linear KV-cache scaling in memory as sequence length increases; (2) the ContextRot phenomena where empirical evidence suggests that transformer architecture's performance degrades with increasing context length. Given the shared dependency on the input, a natural question arises: Can we surgically select the most important input chunks for processing to synergistically (a) reduce the memory footprint, and (b) mitigate the ContextRot effects? In this paper, we answer this question in the affirmative for long-context summarization tasks. We propose APCE as a context-aware solution to select the most important input chunks through low-dimensional semantic similarity matching with the current query. By directly operating on the input, APCE decouples from strict dependency on underlying hardware or CUDA environments, promising a compatible solution scalable to different deployment systems. Our empirical evaluations have demonstrated superior or on-par summarization performance for APCE compared to the full dense baseline using a fraction (50%-70%) of the input sequence resulting in KV-cache and self-attention memory efficiency improvements. We hope our findings inspire further research on context-aware efficiency solutions for LCTMs geared towards other relevant long-context tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12044v1": {
    "title": "Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer Specialization in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.12044v1",
    "arxiv_id": "2510.12044v1",
    "authors": "Yukun Zhang, Qi Dong",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-14 00:58:34",
    "ori_summary": "Existing alignment techniques for Large Language Models (LLMs), such as Direct Preference Optimization (DPO), typically treat the model as a monolithic entity, applying uniform optimization pressure across all layers. This approach overlooks the functional specialization within the Transformer architecture, where different layers are known to handle distinct tasks from syntax to abstract reasoning. In this paper, we challenge this one-size-fits-all paradigm by introducing Hierarchical Alignment, a novel method that applies targeted DPO to distinct functional blocks of a model's layers: local (syntax), intermediate (logic), and global (factuality). Through a series of controlled experiments on state-of-the-art models like Llama-3.1-8B and Qwen1.5-7B using LoRA for surgical fine-tuning, our results, evaluated by a powerful LLM-as-Judge, demonstrate significant and predictable improvements. Specifically, aligning the local layers (Local-Align) enhances grammatical fluency. More importantly, aligning the global layers (Global-Align) not only improves factual consistency as hypothesized but also proves to be the most effective strategy for enhancing logical coherence, outperforming all baselines. Critically, all hierarchical strategies successfully avoid the \"alignment tax\" observed in standard DPO, where gains in fluency come at the cost of degraded logical reasoning. These findings establish a more resource-efficient, controllable, and interpretable path for model alignment, highlighting the immense potential of shifting from monolithic optimization to structure-aware surgical fine-tuning to build more advanced and reliable LLMs.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12041v1": {
    "title": "Improving Text-to-Image Generation with Input-Side Inference-Time Scaling",
    "url": "https://www.alphaxiv.org/abs/2510.12041v1",
    "arxiv_id": "2510.12041v1",
    "authors": "Ruibo Chen, Jiacheng Pan, Heng Huang, Zhenheng Yang",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 00:51:39",
    "ori_summary": "Recent advances in text-to-image (T2I) generation have achieved impressive results, yet existing models often struggle with simple or underspecified prompts, leading to suboptimal image-text alignment, aesthetics, and quality. We propose a prompt rewriting framework that leverages large language models (LLMs) to refine user inputs before feeding them into T2I backbones. Our approach introduces a carefully designed reward system and an iterative direct preference optimization (DPO) training pipeline, enabling the rewriter to enhance prompts without requiring supervised fine-tuning data. We evaluate our method across diverse T2I models and benchmarks. Results show that our prompt rewriter consistently improves image-text alignment, visual quality, and aesthetics, outperforming strong baselines. Furthermore, we demonstrate strong transferability by showing that a prompt rewriter trained on one T2I backbone generalizes effectively to others without needing to be retrained. We also systematically study scalability, evaluating how performance gains scale with the capacity of the large LLM used as the rewriter. These findings highlight that prompt rewriting is an effective, scalable, and practical model-agnostic strategy for improving T2I systems. We plan to release the code and trained prompt rewriters soon.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12040v1": {
    "title": "Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions",
    "url": "https://www.alphaxiv.org/abs/2510.12040v1",
    "arxiv_id": "2510.12040v1",
    "authors": "Sungmin Kang, Yavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Salman Avestimehr",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 00:49:04",
    "ori_summary": "The rapid advancement of large language models (LLMs) has transformed the landscape of natural language processing, enabling breakthroughs across a wide range of areas including question answering, machine translation, and text summarization. Yet, their deployment in real-world applications has raised concerns over reliability and trustworthiness, as LLMs remain prone to hallucinations that produce plausible but factually incorrect outputs. Uncertainty quantification (UQ) has emerged as a central research direction to address this issue, offering principled measures for assessing the trustworthiness of model generations. We begin by introducing the foundations of UQ, from its formal definition to the traditional distinction between epistemic and aleatoric uncertainty, and then highlight how these concepts have been adapted to the context of LLMs. Building on this, we examine the role of UQ in hallucination detection, where quantifying uncertainty provides a mechanism for identifying unreliable generations and improving reliability. We systematically categorize a wide spectrum of existing methods along multiple dimensions and present empirical results for several representative approaches. Finally, we discuss current limitations and outline promising future research directions, providing a clearer picture of the current landscape of LLM UQ for hallucination detection.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12036v1": {
    "title": "On the Interplay between Human Label Variation and Model Fairness",
    "url": "https://www.alphaxiv.org/abs/2510.12036v1",
    "arxiv_id": "2510.12036v1",
    "authors": "Kemal Kurniawan, Meladel Mistica, Timothy Baldwin, Jey Han Lau",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 00:43:48",
    "ori_summary": "The impact of human label variation (HLV) on model fairness is an unexplored topic. This paper examines the interplay by comparing training on majority-vote labels with a range of HLV methods. Our experiments show that without explicit debiasing, HLV training methods have a positive impact on fairness.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12032v1": {
    "title": "Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.12032v1",
    "arxiv_id": "2510.12032v1",
    "authors": "Jung-Woo Shim, Yeong-Joon Ju, Ji-Hoon Park, Seong-Whan Lee",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-14 00:31:36",
    "ori_summary": "Recent advancements in large language models (LLMs) have shown strong performance in natural language understanding and generation tasks. However, LLMs continue to encounter challenges with hallucinations, where models generate plausible but incorrect information. While several factors contribute to hallucinations, the impact of ill-formed prompts, prompts with ambiguous wording, incorrect grammar, or incomplete information, was relatively under explored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a framework designed to systematically improve these ill-formed prompts across multiple stages. Each stage addresses specific errors such as punctuation, typographical mistakes, and misuse of key terms, using small language models (SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of prompts with additional context and employs a self-reflection mechanism with ranking to prioritize the most relevant input. Experimental results on hallucination benchmarks show that prompts refined by MPR achieve over an 85~\\% win rate compared to their original forms, demonstrating its effectiveness in reducing hallucinations and improving LLM output accuracy. Interestingly, we reveal that MPR can be combined with existing post-hoc hallucination mitigation frameworks, further enhancing its versatility. MPR provides a lightweight and adaptable solution for enhancing LLM reliability across various domains.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12029v1": {
    "title": "CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement",
    "url": "https://www.alphaxiv.org/abs/2510.12029v1",
    "arxiv_id": "2510.12029v1",
    "authors": "Jung-Woo Shim, Yeong-Joon Ju, Ji-Hoon Park, Seong-Whan Lee",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-14 00:27:46",
    "ori_summary": "Recent advancements in large language models (LLMs) highlight their fluency in generating responses to diverse prompts. However, these models sometimes generate plausible yet incorrect ``hallucinated\" facts, undermining trust. A frequent but often overlooked cause of such errors is the use of poorly structured or vague prompts by users, leading LLMs to base responses on assumed rather than actual intentions. To mitigate hallucinations induced by these ill-formed prompts, we introduce Curative Prompt Refinement (CPR), a plug-and-play framework for curative prompt refinement that 1) cleans ill-formed prompts, and 2) generates additional informative task descriptions to align the intention of the user and the prompt using a fine-tuned small language model. When applied to language models, we discover that CPR significantly increases the quality of generation while also mitigating hallucination. Empirical studies show that prompts with CPR applied achieves over a 90\\% win rate over the original prompts without any external knowledge.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12023v1": {
    "title": "Information Extraction from Conversation Transcripts: Neuro-Symbolic vs. LLM",
    "url": "https://www.alphaxiv.org/abs/2510.12023v1",
    "arxiv_id": "2510.12023v1",
    "authors": "Alice Saebom Kwak, Maria Alexeeva, Gus Hahn-Powell, Keith Alcock, Kevin McLaughlin, Doug McCorkle, Gabe McNunn, Mihai Surdeanu",
    "categories": "cs.CL",
    "pub_date": "2025-10-14 00:10:24",
    "ori_summary": "The current trend in information extraction (IE) is to rely extensively on large language models, effectively discarding decades of experience in building symbolic or statistical IE systems. This paper compares a neuro-symbolic (NS) and an LLM-based IE system in the agricultural domain, evaluating them on nine interviews across pork, dairy, and crop subdomains. The LLM-based system outperforms the NS one (F1 total: 69.4 vs. 52.7; core: 63.0 vs. 47.2), where total includes all extracted information and core focuses on essential details. However, each system has trade-offs: the NS approach offers faster runtime, greater control, and high accuracy in context-free tasks but lacks generalizability, struggles with contextual nuances, and requires significant resources to develop and maintain. The LLM-based system achieves higher performance, faster deployment, and easier maintenance but has slower runtime, limited control, model dependency and hallucination risks. Our findings highlight the \"hidden cost\" of deploying NLP systems in real-world applications, emphasizing the need to balance performance, efficiency, and control.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12798v1": {
    "title": "Detect Anything via Next Point Prediction",
    "url": "https://www.alphaxiv.org/abs/2510.12798v1",
    "arxiv_id": "2510.12798v1",
    "authors": "Qing Jiang, Junan Huo, Xingyu Chen, Yuda Xiong, Zhaoyang Zeng, Yihao Chen, Tianhe Ren, Junzhi Yu, Lei Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:59:54",
    "ori_summary": "Object detection has long been dominated by traditional coordinate regression-based models, such as YOLO, DETR, and Grounding DINO. Although recent efforts have attempted to leverage MLLMs to tackle this task, they face challenges like low recall rate, duplicate predictions, coordinate misalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a 3B-scale MLLM that achieves state-of-the-art object perception performance. On benchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or exceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot setting. This is enabled by three key designs: 1) Task Formulation: we use special tokens to represent quantized coordinates from 0 to 999, reducing the model's learning difficulty and improving token efficiency for coordinate prediction; 2) Data Engines: we construct multiple data engines to generate high-quality grounding, referring, and pointing data, providing semantically rich supervision for training; \\3) Training Pipelines: we employ a two-stage training process, combining supervised fine-tuning on 22 million data with GRPO-based reinforcement post-training. This RL post-training leverages geometry-aware rewards to effectively bridge the discrete-to-continuous coordinate prediction gap, improve box accuracy, and mitigate undesirable behaviors like duplicate predictions that stem from the teacher-guided nature of the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent language understanding enables versatile capabilities such as object referring, pointing, visual prompting, GUI grounding, spatial referring, OCR and key-pointing, all systematically evaluated on dedicated benchmarks. We believe that Rex-Omni paves the way for more versatile and language-aware visual perception systems.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12796v1": {
    "title": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
    "url": "https://www.alphaxiv.org/abs/2510.12796v1",
    "arxiv_id": "2510.12796v1",
    "authors": "Yingyan Li, Shuyao Shang, Weisong Liu, Bing Zhan, Haochen Wang, Yuqi Wang, Yuntao Chen, Xiaoman Wang, Yasong An, Chufeng Tang, Lu Hou, Lue Fan, Zhaoxiang Zhang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-14 17:59:47",
    "ori_summary": "Scaling Vision-Language-Action (VLA) models on large-scale data offers a promising path to achieving a more generalized driving intelligence. However, VLA models are limited by a ``supervision deficit'': the vast model capacity is supervised by sparse, low-dimensional actions, leaving much of their representational power underutilized. To remedy this, we propose \\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to predict future images. This task generates a dense, self-supervised signal that compels the model to learn the underlying dynamics of the driving environment. We showcase the paradigm's versatility by instantiating it for two dominant VLA archetypes: an autoregressive world model for VLAs that use discrete visual tokens, and a diffusion world model for those operating on continuous visual features. Building on the rich representations learned from world modeling, we introduce a lightweight action expert to address the inference latency for real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a 680x larger in-house dataset demonstrate that DriveVLA-W0 significantly outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling law, showing that performance gains accelerate as the training dataset size increases.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12795v1": {
    "title": "CuMPerLay: Learning Cubical Multiparameter Persistence Vectorizations",
    "url": "https://www.alphaxiv.org/abs/2510.12795v1",
    "arxiv_id": "2510.12795v1",
    "authors": "Caner Korkmaz, Brighton Nuwagira, Barış Coşkunuzer, Tolga Birdal",
    "categories": "cs.CV, cs.AI, cs.LG, math.AT, stat.ML",
    "pub_date": "2025-10-14 17:59:01",
    "ori_summary": "We present CuMPerLay, a novel differentiable vectorization layer that enables the integration of Cubical Multiparameter Persistence (CMP) into deep learning pipelines. While CMP presents a natural and powerful way to topologically work with images, its use is hindered by the complexity of multifiltration structures as well as the vectorization of CMP. In face of these challenges, we introduce a new algorithm for vectorizing MP homologies of cubical complexes. Our CuMPerLay decomposes the CMP into a combination of individual, learnable single-parameter persistence, where the bifiltration functions are jointly learned. Thanks to the differentiability, its robust topological feature vectors can be seamlessly used within state-of-the-art architectures such as Swin Transformers. We establish theoretical guarantees for the stability of our vectorization under generalized Wasserstein metrics. Our experiments on benchmark medical imaging and computer vision datasets show the benefit CuMPerLay on classification and segmentation performance, particularly in limited-data scenarios. Overall, CuMPerLay offers a promising direction for integrating global structural information into deep networks for structured image analysis.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12793v1": {
    "title": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution",
    "url": "https://www.alphaxiv.org/abs/2510.12793v1",
    "arxiv_id": "2510.12793v1",
    "authors": "Long Cui, Weiyun Wang, Jie Shao, Zichen Wen, Gen Luo, Linfeng Zhang, Yanting Zhang, Yu Qiao, Wenhai Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:58:10",
    "ori_summary": "Existing Multimodal Large Language Models (MLLMs) suffer from increased inference costs due to the additional vision tokens introduced by image inputs. In this work, we propose Visual Consistency Learning (ViCO), a novel training algorithm that enables the model to represent images of varying semantic complexities using different numbers of vision tokens. The key idea behind our method is to employ multiple MLP connectors, each with a different image compression ratio, to downsample the vision tokens based on the semantic complexity of the image. During training, we minimize the KL divergence between the responses conditioned on different MLP connectors. At inference time, we introduce an image router, termed Visual Resolution Router (ViR), that automatically selects the appropriate compression rate for each image patch. Compared with existing dynamic high-resolution strategies, which adjust the number of visual tokens based on image resolutions, our method dynamically adapts the number of visual tokens according to semantic complexity. Experimental results demonstrate that our method can reduce the number of vision tokens by up to 50% while maintaining the model's perception, reasoning, and OCR capabilities. We hope this work will contribute to the development of more efficient MLLMs. The code and models will be released to facilitate future research.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12789v1": {
    "title": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12789v1",
    "arxiv_id": "2510.12789v1",
    "authors": "Kevin Li, Manuel Brack, Sudeep Katakol, Hareesh Ravi, Ajinkya Kale",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-14 17:57:56",
    "ori_summary": "Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images and text. This separation constrains diffusion models' ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often use the last layer information from VLM, employ multiple visual encoders, or train large unified models jointly for text and image generation, which demands substantial computational resources and large-scale data, limiting its accessibility.We present UniFusion, a diffusion-based generative model conditioned on a frozen large vision-language model (VLM) that serves as a unified multimodal encoder. At the core of UniFusion is the Layerwise Attention Pooling (LAP) mechanism that extracts both high level semantics and low level details from text and visual tokens of a frozen VLM to condition a diffusion generative model. We demonstrate that LAP outperforms other shallow fusion architectures on text-image alignment for generation and faithful transfer of visual information from VLM to the diffusion model which is key for editing. We propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI), which conditions a diffusion transformer (DiT) only on the text tokens generated by the VLM during in-model prompt rewriting. VERIFI combines the alignment of the conditioning distribution with the VLM's reasoning capabilities for increased capabilities and flexibility at inference. In addition, finetuning on editing task not only improves text-image alignment for generation, indicative of cross-modality knowledge transfer, but also exhibits tremendous generalization capabilities. Our model when trained on single image editing, zero-shot generalizes to multiple image references further motivating the unified encoder design of UniFusion.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12788v1": {
    "title": "Efficient Real-World Deblurring using Single Images: AIM 2025 Challenge Report",
    "url": "https://www.alphaxiv.org/abs/2510.12788v1",
    "arxiv_id": "2510.12788v1",
    "authors": "Daniel Feijoo, Paula Garrido-Mellado, Marcos V. Conde, Jaesung Rim, Alvaro Garcia, Sunghyun Cho, Radu Timofte",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:57:04",
    "ori_summary": "This paper reviews the AIM 2025 Efficient Real-World Deblurring using Single Images Challenge, which aims to advance in efficient real-blur restoration. The challenge is based on a new test set based on the well known RSBlur dataset. Pairs of blur and degraded images in this dataset are captured using a double-camera system. Participant were tasked with developing solutions to effectively deblur these type of images while fulfilling strict efficiency constraints: fewer than 5 million model parameters and a computational budget under 200 GMACs. A total of 71 participants registered, with 4 teams finally submitting valid solutions. The top-performing approach achieved a PSNR of 31.1298 dB, showcasing the potential of efficient methods in this domain. This paper provides a comprehensive overview of the challenge, compares the proposed solutions, and serves as a valuable reference for researchers in efficient real-world image deblurring.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12785v1": {
    "title": "MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars",
    "url": "https://www.alphaxiv.org/abs/2510.12785v1",
    "arxiv_id": "2510.12785v1",
    "authors": "Felix Taubner, Ruihang Zhang, Mathieu Tuli, Sherwin Bahmani, David B. Lindell",
    "categories": "cs.CV, cs.AI, cs.GR",
    "pub_date": "2025-10-14 17:56:14",
    "ori_summary": "Digital human avatars aim to simulate the dynamic appearance of humans in virtual environments, enabling immersive experiences across gaming, film, virtual reality, and more. However, the conventional process for creating and animating photorealistic human avatars is expensive and time-consuming, requiring large camera capture rigs and significant manual effort from professional 3D artists. With the advent of capable image and video generation models, recent methods enable automatic rendering of realistic animated avatars from a single casually captured reference image of a target subject. While these techniques significantly lower barriers to avatar creation and offer compelling realism, they lack constraints provided by multi-view information or an explicit 3D representation. So, image quality and realism degrade when rendered from viewpoints that deviate strongly from the reference image. Here, we build a video model that generates animatable multi-view videos of digital humans based on a single reference image and target expressions. Our model, MVP4D, is based on a state-of-the-art pre-trained video diffusion model and generates hundreds of frames simultaneously from viewpoints varying by up to 360 degrees around a target subject. We show how to distill the outputs of this model into a 4D avatar that can be rendered in real-time. Our approach significantly improves the realism, temporal consistency, and 3D consistency of generated avatars compared to previous methods.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12777v1": {
    "title": "What If : Understanding Motion Through Sparse Interactions",
    "url": "https://www.alphaxiv.org/abs/2510.12777v1",
    "arxiv_id": "2510.12777v1",
    "authors": "Stefan Andreas Baumann, Nick Stracke, Timy Phan, Björn Ommer",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:52:17",
    "ori_summary": "Understanding the dynamics of a physical scene involves reasoning about the diverse ways it can potentially change, especially as a result of local interactions. We present the Flow Poke Transformer (FPT), a novel framework for directly predicting the distribution of local motion, conditioned on sparse interactions termed \"pokes\". Unlike traditional methods that typically only enable dense sampling of a single realization of scene dynamics, FPT provides an interpretable directly accessible representation of multi-modal scene motion, its dependency on physical interactions and the inherent uncertainties of scene dynamics. We also evaluate our model on several downstream tasks to enable comparisons with prior methods and highlight the flexibility of our approach. On dense face motion generation, our generic pre-trained model surpasses specialized baselines. FPT can be fine-tuned in strongly out-of-distribution tasks such as synthetic datasets to enable significant improvements over in-domain methods in articulated object motion estimation. Additionally, predicting explicit motion distributions directly enables our method to achieve competitive performance on tasks like moving part segmentation from pokes which further demonstrates the versatility of our FPT. Code and models are publicly available at https://compvis.github.io/flow-poke-transformer.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12768v1": {
    "title": "Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2510.12768v1",
    "arxiv_id": "2510.12768v1",
    "authors": "Fengzhi Guo, Chih-Chuan Hsu, Sihao Ding, Cheng Zhang",
    "categories": "cs.CV, cs.AI, cs.GR",
    "pub_date": "2025-10-14 17:47:11",
    "ori_summary": "Reconstructing dynamic 3D scenes from monocular input is fundamentally under-constrained, with ambiguities arising from occlusion and extreme novel views. While dynamic Gaussian Splatting offers an efficient representation, vanilla models optimize all Gaussian primitives uniformly, ignoring whether they are well or poorly observed. This limitation leads to motion drifts under occlusion and degraded synthesis when extrapolating to unseen views. We argue that uncertainty matters: Gaussians with recurring observations across views and time act as reliable anchors to guide motion, whereas those with limited visibility are treated as less reliable. To this end, we introduce USplat4D, a novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates reliable motion cues to enhance 4D reconstruction. Our key insight is to estimate time-varying per-Gaussian uncertainty and leverages it to construct a spatio-temporal graph for uncertainty-aware optimization. Experiments on diverse real and synthetic datasets show that explicitly modeling uncertainty consistently improves dynamic Gaussian Splatting models, yielding more stable geometry under occlusion and high-quality synthesis at extreme viewpoints.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12765v1": {
    "title": "Efficient Perceptual Image Super Resolution: AIM 2025 Study and Benchmark",
    "url": "https://www.alphaxiv.org/abs/2510.12765v1",
    "arxiv_id": "2510.12765v1",
    "authors": "Bruno Longarela, Marcos V. Conde, Alvaro Garcia, Radu Timofte",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:45:22",
    "ori_summary": "This paper presents a comprehensive study and benchmark on Efficient Perceptual Super-Resolution (EPSR). While significant progress has been made in efficient PSNR-oriented super resolution, approaches focusing on perceptual quality metrics remain relatively inefficient. Motivated by this gap, we aim to replicate or improve the perceptual results of Real-ESRGAN while meeting strict efficiency constraints: a maximum of 5M parameters and 2000 GFLOPs, calculated for an input size of 960x540 pixels. The proposed solutions were evaluated on a novel dataset consisting of 500 test images of 4K resolution, each degraded using multiple degradation types, without providing the original high-quality counterparts. This design aims to reflect realistic deployment conditions and serves as a diverse and challenging benchmark. The top-performing approach manages to outperform Real-ESRGAN across all benchmark datasets, demonstrating the potential of efficient methods in the perceptual domain. This paper establishes the modern baselines for efficient perceptual super resolution.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12764v1": {
    "title": "AnyUp: Universal Feature Upsampling",
    "url": "https://www.alphaxiv.org/abs/2510.12764v1",
    "arxiv_id": "2510.12764v1",
    "authors": "Thomas Wimmer, Prune Truong, Marie-Julie Rakotosaona, Michael Oechsle, Federico Tombari, Bernt Schiele, Jan Eric Lenssen",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-14 17:45:17",
    "ori_summary": "We introduce AnyUp, a method for feature upsampling that can be applied to any vision feature at any resolution, without encoder-specific training. Existing learning-based upsamplers for features like DINO or CLIP need to be re-trained for every feature extractor and thus do not generalize to different feature types at inference time. In this work, we propose an inference-time feature-agnostic upsampling architecture to alleviate this limitation and improve upsampling quality. In our experiments, AnyUp sets a new state of the art for upsampled features, generalizes to different feature types, and preserves feature semantics while being efficient and easy to apply to a wide range of downstream tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12758v1": {
    "title": "PET Head Motion Estimation Using Supervised Deep Learning with Attention",
    "url": "https://www.alphaxiv.org/abs/2510.12758v1",
    "arxiv_id": "2510.12758v1",
    "authors": "Zhuotong Cai, Tianyi Zeng, Jiazhen Zhang, Eléonore V. Lieffrig, Kathryn Fontaine, Chenyu You, Enette Mae Revilla, James S. Duncan, Jingmin Xin, Yihuan Lu, John A. Onofrey",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:37:12",
    "ori_summary": "Head movement poses a significant challenge in brain positron emission tomography (PET) imaging, resulting in image artifacts and tracer uptake quantification inaccuracies. Effective head motion estimation and correction are crucial for precise quantitative image analysis and accurate diagnosis of neurological disorders. Hardware-based motion tracking (HMT) has limited applicability in real-world clinical practice. To overcome this limitation, we propose a deep-learning head motion correction approach with cross-attention (DL-HMC++) to predict rigid head motion from one-second 3D PET raw data. DL-HMC++ is trained in a supervised manner by leveraging existing dynamic PET scans with gold-standard motion measurements from external HMT. We evaluate DL-HMC++ on two PET scanners (HRRT and mCT) and four radiotracers (18F-FDG, 18F-FPEB, 11C-UCB-J, and 11C-LSN3172176) to demonstrate the effectiveness and generalization of the approach in large cohort PET studies. Quantitative and qualitative results demonstrate that DL-HMC++ consistently outperforms state-of-the-art data-driven motion estimation methods, producing motion-free images with clear delineation of brain structures and reduced motion artifacts that are indistinguishable from gold-standard HMT. Brain region of interest standard uptake value analysis exhibits average difference ratios between DL-HMC++ and gold-standard HMT to be 1.2 plus-minus 0.5% for HRRT and 0.5 plus-minus 0.2% for mCT. DL-HMC++ demonstrates the potential for data-driven PET head motion correction to remove the burden of HMT, making motion correction accessible to clinical populations beyond research settings. The code is available at https://github.com/maxxxxxxcai/DL-HMC-TMI.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12753v1": {
    "title": "E-MoFlow: Learning Egomotion and Optical Flow from Event Data via Implicit Regularization",
    "url": "https://www.alphaxiv.org/abs/2510.12753v1",
    "arxiv_id": "2510.12753v1",
    "authors": "Wenpu Li, Bangyan Liao, Yi Zhou, Qi Xu, Pian Wan, Peidong Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:33:44",
    "ori_summary": "The estimation of optical flow and 6-DoF ego-motion, two fundamental tasks in 3D vision, has typically been addressed independently. For neuromorphic vision (e.g., event cameras), however, the lack of robust data association makes solving the two problems separately an ill-posed challenge, especially in the absence of supervision via ground truth. Existing works mitigate this ill-posedness by either enforcing the smoothness of the flow field via an explicit variational regularizer or leveraging explicit structure-and-motion priors in the parametrization to improve event alignment. The former notably introduces bias in results and computational overhead, while the latter, which parametrizes the optical flow in terms of the scene depth and the camera motion, often converges to suboptimal local minima. To address these issues, we propose an unsupervised framework that jointly optimizes egomotion and optical flow via implicit spatial-temporal and geometric regularization. First, by modeling camera's egomotion as a continuous spline and optical flow as an implicit neural representation, our method inherently embeds spatial-temporal coherence through inductive biases. Second, we incorporate structure-and-motion priors through differential geometric constraints, bypassing explicit depth estimation while maintaining rigorous geometric consistency. As a result, our framework (called E-MoFlow) unifies egomotion and optical flow estimation via implicit regularization under a fully unsupervised paradigm. Experiments demonstrate its versatility to general 6-DoF motion scenarios, achieving state-of-the-art performance among unsupervised methods and competitive even with supervised approaches.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12750v1": {
    "title": "VQArt-Bench: A semantically rich VQA Benchmark for Art and Cultural Heritage",
    "url": "https://www.alphaxiv.org/abs/2510.12750v1",
    "arxiv_id": "2510.12750v1",
    "authors": "A. Alfarano, L. Venturoli, D. Negueruela del Castillo",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-14 17:29:52",
    "ori_summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant capabilities in joint visual and linguistic tasks. However, existing Visual Question Answering (VQA) benchmarks often fail to evaluate deep semantic understanding, particularly in complex domains like visual art analysis. Confined to simple syntactic structures and surface-level attributes, these questions fail to capture the diversity and depth of human visual inquiry. This limitation incentivizes models to exploit statistical shortcuts rather than engage in visual reasoning. To address this gap, we introduce VQArt-Bench, a new, large-scale VQA benchmark for the cultural heritage domain. This benchmark is constructed using a novel multi-agent pipeline where specialized agents collaborate to generate nuanced, validated, and linguistically diverse questions. The resulting benchmark is structured along relevant visual understanding dimensions that probe a model's ability to interpret symbolic meaning, narratives, and complex visual relationships. Our evaluation of 14 state-of-the-art MLLMs on this benchmark reveals significant limitations in current models, including a surprising weakness in simple counting tasks and a clear performance gap between proprietary and open-source models.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12749v1": {
    "title": "SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.12749v1",
    "arxiv_id": "2510.12749v1",
    "authors": "Zhiliu Yang, Jinyu Dai, Jianyuan Zhang, Zhu Yang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:28:19",
    "ori_summary": "The scene perception, understanding, and simulation are fundamental techniques for embodied-AI agents, while existing solutions are still prone to segmentation deficiency, dynamic objects' interference, sensor data sparsity, and view-limitation problems. This paper proposes a novel framework, named SPORTS, for holistic scene understanding via tightly integrating Video Panoptic Segmentation (VPS), Visual Odometry (VO), and Scene Rendering (SR) tasks into an iterative and unified perspective. Firstly, VPS designs an adaptive attention-based geometric fusion mechanism to align cross-frame features via enrolling the pose, depth, and optical flow modality, which automatically adjust feature maps for different decoding stages. And a post-matching strategy is integrated to improve identities tracking. In VO, panoptic segmentation results from VPS are combined with the optical flow map to improve the confidence estimation of dynamic objects, which enhances the accuracy of the camera pose estimation and completeness of the depth map generation via the learning-based paradigm. Furthermore, the point-based rendering of SR is beneficial from VO, transforming sparse point clouds into neural fields to synthesize high-fidelity RGB views and twin panoptic views. Extensive experiments on three public datasets demonstrate that our attention-based feature fusion outperforms most existing state-of-the-art methods on the odometry, tracking, segmentation, and novel view synthesis tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12747v1": {
    "title": "FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution",
    "url": "https://www.alphaxiv.org/abs/2510.12747v1",
    "arxiv_id": "2510.12747v1",
    "authors": "Junhao Zhuang, Shi Guo, Xin Cai, Xiaohui Li, Yihao Liu, Chun Yuan, Tianfan Xue",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 17:25:54",
    "ori_summary": "Diffusion models have recently advanced video restoration, but applying them to real-world video super-resolution (VSR) remains challenging due to high latency, prohibitive computation, and poor generalization to ultra-high resolutions. Our goal in this work is to make diffusion-based VSR practical by achieving efficiency, scalability, and real-time performance. To this end, we propose FlashVSR, the first diffusion-based one-step streaming framework towards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408 videos on a single A100 GPU by combining three complementary innovations: (i) a train-friendly three-stage distillation pipeline that enables streaming super-resolution, (ii) locality-constrained sparse attention that cuts redundant computation while bridging the train-test resolution gap, and (iii) a tiny conditional decoder that accelerates reconstruction without sacrificing quality. To support large-scale training, we also construct VSR-120K, a new dataset with 120k videos and 180k images. Extensive experiments show that FlashVSR scales reliably to ultra-high resolutions and achieves state-of-the-art performance with up to 12x speedup over prior one-step diffusion VSR models. We will release the code, pretrained models, and dataset to foster future research in efficient diffusion-based VSR.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12741v1": {
    "title": "Personalized Federated Fine-Tuning of Vision Foundation Models for Healthcare",
    "url": "https://www.alphaxiv.org/abs/2510.12741v1",
    "arxiv_id": "2510.12741v1",
    "authors": "Adam Tupper, Christian Gagné",
    "categories": "cs.CV, cs.DC",
    "pub_date": "2025-10-14 17:18:12",
    "ori_summary": "Foundation models open up new possibilities for the use of AI in healthcare. However, even when pre-trained on health data, they still need to be fine-tuned for specific downstream tasks. Furthermore, although foundation models reduce the amount of training data required to achieve good performance, obtaining sufficient data is still a challenge. This is due, in part, to restrictions on sharing and aggregating data from different sources to protect patients' privacy. One possible solution to this is to fine-tune foundation models via federated learning across multiple participating clients (i.e., hospitals, clinics, etc.). In this work, we propose a new personalized federated fine-tuning method that learns orthogonal LoRA adapters to disentangle general and client-specific knowledge, enabling each client to fully exploit both their own data and the data of others. Our preliminary results on real-world federated medical imaging tasks demonstrate that our approach is competitive against current federated fine-tuning methods.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12712v1": {
    "title": "Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.12712v1",
    "arxiv_id": "2510.12712v1",
    "authors": "Xingang Guo, Utkarsh Tyagi, Advait Gosai, Paula Vergara, Ernesto Gabriel Hernández Montoya, Chen Bo Calvin Zhang, Bin Hu, Yunzhong He, Bing Liu, Rakshith Sharma Srinivasa",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-14 16:50:49",
    "ori_summary": "Multimodal Large Language Models (MLLMs) are increasingly applied in real-world scenarios where user-provided images are often imperfect, requiring active image manipulations such as cropping, editing, or enhancement to uncover salient visual cues. Beyond static visual perception, MLLMs must also think with images: dynamically transforming visual content and integrating it with other tools to solve complex tasks. However, this shift from treating vision as passive context to a manipulable cognitive workspace remains underexplored. Most existing benchmarks still follow a think about images paradigm, where images are regarded as static inputs. To address this gap, we introduce IRIS, an Interactive Reasoning with Images and Systems that evaluates MLLMs' ability to perceive, transform, and reason across complex visual-textual tasks under the think with images paradigm. IRIS comprises 1,204 challenging, open-ended vision tasks (603 single-turn, 601 multi-turn) spanning across five diverse domains, each paired with detailed rubrics to enable systematic evaluation. Our evaluation shows that current MLLMs struggle with tasks requiring effective integration of vision and general-purpose tools. Even the strongest model (GPT-5-think) reaches only 18.68% pass rate. We further observe divergent tool-use behaviors, with OpenAI models benefiting from diverse image manipulations while Gemini-2.5-pro shows no improvement. By introducing the first benchmark centered on think with images, IRIS offers critical insights for advancing visual intelligence in MLLMs.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12704v1": {
    "title": "Hybrid Explanation-Guided Learning for Transformer-Based Chest X-Ray Diagnosis",
    "url": "https://www.alphaxiv.org/abs/2510.12704v1",
    "arxiv_id": "2510.12704v1",
    "authors": "Shelley Zixin Shu, Haozhe Luo, Alexander Poellinger, Mauricio Reyes",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-14 16:39:02",
    "ori_summary": "Transformer-based deep learning models have demonstrated exceptional performance in medical imaging by leveraging attention mechanisms for feature representation and interpretability. However, these models are prone to learning spurious correlations, leading to biases and limited generalization. While human-AI attention alignment can mitigate these issues, it often depends on costly manual supervision. In this work, we propose a Hybrid Explanation-Guided Learning (H-EGL) framework that combines self-supervised and human-guided constraints to enhance attention alignment and improve generalization. The self-supervised component of H-EGL leverages class-distinctive attention without relying on restrictive priors, promoting robustness and flexibility. We validate our approach on chest X-ray classification using the Vision Transformer (ViT), where H-EGL outperforms two state-of-the-art Explanation-Guided Learning (EGL) methods, demonstrating superior classification accuracy and generalization capability. Additionally, it produces attention maps that are better aligned with human expertise.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12691v1": {
    "title": "DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization",
    "url": "https://www.alphaxiv.org/abs/2510.12691v1",
    "arxiv_id": "2510.12691v1",
    "authors": "Danial Hosseintabar, Fan Chen, Giannis Daras, Antonio Torralba, Constantinos Daskalakis",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-14 16:25:02",
    "ori_summary": "Diffusion models have emerged as powerful generative priors for high-dimensional inverse problems, yet learning them when only corrupted or noisy observations are available remains challenging. In this work, we propose a new method for training diffusion models with Expectation-Maximization (EM) from corrupted data. Our proposed method, DiffEM, utilizes conditional diffusion models to reconstruct clean data from observations in the E-step, and then uses the reconstructed data to refine the conditional diffusion model in the M-step. Theoretically, we provide monotonic convergence guarantees for the DiffEM iteration, assuming appropriate statistical conditions. We demonstrate the effectiveness of our approach through experiments on various image reconstruction tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12687v1": {
    "title": "EReLiFM: Evidential Reliability-Aware Residual Flow Meta-Learning for Open-Set Domain Generalization under Noisy Labels",
    "url": "https://www.alphaxiv.org/abs/2510.12687v1",
    "arxiv_id": "2510.12687v1",
    "authors": "Kunyu Peng, Di Wen, Kailun Yang, Jia Fu, Yufan Chen, Ruiping Liu, Jiamin Wu, Junwei Zheng, M. Saquib Sarfraz, Luc Van Gool, Danda Pani Paudel, Rainer Stiefelhagen",
    "categories": "cs.CV, cs.LG, cs.RO",
    "pub_date": "2025-10-14 16:23:11",
    "ori_summary": "Open-Set Domain Generalization (OSDG) aims to enable deep learning models to recognize unseen categories in new domains, which is crucial for real-world applications. Label noise hinders open-set domain generalization by corrupting source-domain knowledge, making it harder to recognize known classes and reject unseen ones. While existing methods address OSDG under Noisy Labels (OSDG-NL) using hyperbolic prototype-guided meta-learning, they struggle to bridge domain gaps, especially with limited clean labeled data. In this paper, we propose Evidential Reliability-Aware Residual Flow Meta-Learning (EReLiFM). We first introduce an unsupervised two-stage evidential loss clustering method to promote label reliability awareness. Then, we propose a residual flow matching mechanism that models structured domain- and category-conditioned residuals, enabling diverse and uncertainty-aware transfer paths beyond interpolation-based augmentation. During this meta-learning process, the model is optimized such that the update direction on the clean set maximizes the loss decrease on the noisy set, using pseudo labels derived from the most confident predicted class for supervision. Experimental results show that EReLiFM outperforms existing methods on OSDG-NL, achieving state-of-the-art performance. The source code is available at https://github.com/KPeng9510/ERELIFM.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12679v1": {
    "title": "MCOP: Multi-UAV Collaborative Occupancy Prediction",
    "url": "https://www.alphaxiv.org/abs/2510.12679v1",
    "arxiv_id": "2510.12679v1",
    "authors": "Zefu Lin, Wenbo Chen, Xiaojuan Jin, Yuran Yang, Lue Fan, Yixin Zhang, Yufeng Zhang, Zhaoxiang Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 16:17:42",
    "ori_summary": "Unmanned Aerial Vehicle (UAV) swarm systems necessitate efficient collaborative perception mechanisms for diverse operational scenarios. Current Bird's Eye View (BEV)-based approaches exhibit two main limitations: bounding-box representations fail to capture complete semantic and geometric information of the scene, and their performance significantly degrades when encountering undefined or occluded objects. To address these limitations, we propose a novel multi-UAV collaborative occupancy prediction framework. Our framework effectively preserves 3D spatial structures and semantics through integrating a Spatial-Aware Feature Encoder and Cross-Agent Feature Integration. To enhance efficiency, we further introduce Altitude-Aware Feature Reduction to compactly represent scene information, along with a Dual-Mask Perceptual Guidance mechanism to adaptively select features and reduce communication overhead. Due to the absence of suitable benchmark datasets, we extend three datasets for evaluation: two virtual datasets (Air-to-Pred-Occ and UAV3D-Occ) and one real-world dataset (GauUScene-Occ). Experiments results demonstrate that our method achieves state-of-the-art accuracy, significantly outperforming existing collaborative methods while reducing communication overhead to only a fraction of previous approaches.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12670v1": {
    "title": "TerraCodec: Compressing Earth Observations",
    "url": "https://www.alphaxiv.org/abs/2510.12670v1",
    "arxiv_id": "2510.12670v1",
    "authors": "Julen Costa-Watanabe, Isabelle Wittmann, Benedikt Blumenstiel, Konrad Schindler",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 16:05:31",
    "ori_summary": "Earth observation (EO) satellites produce massive streams of multispectral image time series, posing pressing challenges for storage and transmission. Yet, learned EO compression remains fragmented, lacking publicly available pretrained models and misaligned with advances in compression for natural imagery. Image codecs overlook temporal redundancy, while video codecs rely on motion priors that fail to capture the radiometric evolution of largely static scenes. We introduce TerraCodec (TEC), a family of learned codecs tailored to EO. TEC includes efficient image-based variants adapted to multispectral inputs, as well as a Temporal Transformer model (TEC-TT) that leverages dependencies across time. To overcome the fixed-rate setting of today's neural codecs, we present Latent Repacking, a novel method for training flexible-rate transformer models that operate on varying rate-distortion settings. Trained on Sentinel-2 data, TerraCodec outperforms classical codecs, achieving 3-10x stronger compression at equivalent image quality. Beyond compression, TEC-TT enables zero-shot cloud inpainting, surpassing state-of-the-art methods on the AllClear benchmark. Our results establish bespoke, learned compression algorithms as a promising direction for Earth observation. Code and model weights will be released under a permissive license.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12660v1": {
    "title": "On the Use of Hierarchical Vision Foundation Models for Low-Cost Human Mesh Recovery and Pose Estimation",
    "url": "https://www.alphaxiv.org/abs/2510.12660v1",
    "arxiv_id": "2510.12660v1",
    "authors": "Shuhei Tarashima, Yushan Wang, Norio Tagawa",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 15:57:40",
    "ori_summary": "In this work, we aim to develop simple and efficient models for human mesh recovery (HMR) and its predecessor task, human pose estimation (HPE). State-of-the-art HMR methods, such as HMR2.0 and its successors, rely on large, non-hierarchical vision transformers as encoders, which are inherited from the corresponding HPE models like ViTPose. To establish baselines across varying computational budgets, we first construct three lightweight HMR2.0 variants by adapting the corresponding ViTPose models. In addition, we propose leveraging the early stages of hierarchical vision foundation models (VFMs), including Swin Transformer, GroupMixFormer, and VMamba, as encoders. This design is motivated by the observation that intermediate stages of hierarchical VFMs produce feature maps with resolutions comparable to or higher than those of non-hierarchical counterparts. We conduct a comprehensive evaluation of 27 hierarchical-VFM-based HMR and HPE models, demonstrating that using only the first two or three stages achieves performance on par with full-stage models. Moreover, we show that the resulting truncated models exhibit better trade-offs between accuracy and computational efficiency compared to existing lightweight alternatives.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12646v1": {
    "title": "Zero-Shot CFC: Fast Real-World Image Denoising based on Cross-Frequency Consistency",
    "url": "https://www.alphaxiv.org/abs/2510.12646v1",
    "arxiv_id": "2510.12646v1",
    "authors": "Yanlin Jiang, Yuchen Liu, Mingren Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 15:35:59",
    "ori_summary": "Zero-shot denoisers address the dataset dependency of deep-learning-based denoisers, enabling the denoising of unseen single images. Nonetheless, existing zero-shot methods suffer from long training times and rely on the assumption of noise independence and a zero-mean property, limiting their effectiveness in real-world denoising scenarios where noise characteristics are more complicated. This paper proposes an efficient and effective method for real-world denoising, the Zero-Shot denoiser based on Cross-Frequency Consistency (ZSCFC), which enables training and denoising with a single noisy image and does not rely on assumptions about noise distribution. Specifically, image textures exhibit position similarity and content consistency across different frequency bands, while noise does not. Based on this property, we developed cross-frequency consistency loss and an ultralight network to realize image denoising. Experiments on various real-world image datasets demonstrate that our ZSCFC outperforms other state-of-the-art zero-shot methods in terms of computational efficiency and denoising performance.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12605v1": {
    "title": "WaterFlow: Explicit Physics-Prior Rectified Flow for Underwater Saliency Mask Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12605v1",
    "arxiv_id": "2510.12605v1",
    "authors": "Runting Li, Shijie Lian, Hua Li, Yutong Li, Wenhui Wu, Sam Kwong",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 15:02:24",
    "ori_summary": "Underwater Salient Object Detection (USOD) faces significant challenges, including underwater image quality degradation and domain gaps. Existing methods tend to ignore the physical principles of underwater imaging or simply treat degradation phenomena in underwater images as interference factors that must be eliminated, failing to fully exploit the valuable information they contain. We propose WaterFlow, a rectified flow-based framework for underwater salient object detection that innovatively incorporates underwater physical imaging information as explicit priors directly into the network training process and introduces temporal dimension modeling, significantly enhancing the model's capability for salient object identification. On the USOD10K dataset, WaterFlow achieves a 0.072 gain in S_m, demonstrating the effectiveness and superiority of our method. The code will be published after the acceptance.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12586v1": {
    "title": "Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training",
    "url": "https://www.alphaxiv.org/abs/2510.12586v1",
    "arxiv_id": "2510.12586v1",
    "authors": "Jiachen Lei, Keli Liu, Julius Berner, Haiming Yu, Hongkai Zheng, Jiahong Wu, Xiangxiang Chu",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 14:41:16",
    "ori_summary": "Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion and consistency models. In the first stage, we pre-train encoders to capture meaningful semantics from clean images while aligning them with points along the same deterministic sampling trajectory, which evolves points from the prior to the data distribution. In the second stage, we integrate the encoder with a randomly initialized decoder and fine-tune the complete model end-to-end for both diffusion and consistency models. Our training framework demonstrates strong empirical performance on ImageNet dataset. Specifically, our diffusion model reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75 number of function evaluations (NFE), surpassing prior pixel-space methods by a large margin in both generation quality and efficiency while rivaling leading VAE-based models at comparable training cost. Furthermore, on ImageNet-256, our consistency model achieves an impressive FID of 8.82 in a single sampling step, significantly surpassing its latent-space counterpart. To the best of our knowledge, this marks the first successful training of a consistency model directly on high-resolution images without relying on pre-trained VAEs or diffusion models.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12581v1": {
    "title": "LayerSync: Self-aligning Intermediate Layers",
    "url": "https://www.alphaxiv.org/abs/2510.12581v1",
    "arxiv_id": "2510.12581v1",
    "authors": "Yasaman Haghighi, Bastien van Delft, Mariam Hassan, Alexandre Alahi",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-14 14:39:14",
    "ori_summary": "We propose LayerSync, a domain-agnostic approach for improving the generation quality and the training efficiency of diffusion models. Prior studies have highlighted the connection between the quality of generation and the representations learned by diffusion models, showing that external guidance on model intermediate representations accelerates training. We reconceptualize this paradigm by regularizing diffusion models with their own intermediate representations. Building on the observation that representation quality varies across diffusion model layers, we show that the most semantically rich representations can act as an intrinsic guidance for weaker ones, reducing the need for external supervision. Our approach, LayerSync, is a self-sufficient, plug-and-play regularizer term with no overhead on diffusion model training and generalizes beyond the visual domain to other modalities. LayerSync requires no pretrained models nor additional data. We extensively evaluate the method on image generation and demonstrate its applicability to other domains such as audio, video, and motion generation. We show that it consistently improves the generation quality and the training efficiency. For example, we speed up the training of flow-based transformer by over 8.75x on ImageNet dataset and improved the generation quality by 23.6%. The code is available at https://github.com/vita-epfl/LayerSync.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12579v1": {
    "title": "Unlocking Zero-Shot Plant Segmentation with Pl@ntNet Intelligence",
    "url": "https://www.alphaxiv.org/abs/2510.12579v1",
    "arxiv_id": "2510.12579v1",
    "authors": "Simon Ravé, Jean-Christophe Lombardo, Pejman Rasti, Alexis Joly, David Rousseau",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 14:38:32",
    "ori_summary": "We present a zero-shot segmentation approach for agricultural imagery that leverages Plantnet, a large-scale plant classification model, in conjunction with its DinoV2 backbone and the Segment Anything Model (SAM). Rather than collecting and annotating new datasets, our method exploits Plantnet's specialized plant representations to identify plant regions and produce coarse segmentation masks. These masks are then refined by SAM to yield detailed segmentations. We evaluate on four publicly available datasets of various complexity in terms of contrast including some where the limited size of the training data and complex field conditions often hinder purely supervised methods. Our results show consistent performance gains when using Plantnet-fine-tuned DinoV2 over the base DinoV2 model, as measured by the Jaccard Index (IoU). These findings highlight the potential of combining foundation models with specialized plant-centric models to alleviate the annotation bottleneck and enable effective segmentation in diverse agricultural scenarios.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12573v1": {
    "title": "Learning Human Motion with Temporally Conditional Mamba",
    "url": "https://www.alphaxiv.org/abs/2510.12573v1",
    "arxiv_id": "2510.12573v1",
    "authors": "Quang Nguyen, Tri Le, Baoru Huang, Minh Nhat Vu, Ngan Le, Thieu Vo, Anh Nguyen",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 14:29:51",
    "ori_summary": "Learning human motion based on a time-dependent input signal presents a challenging yet impactful task with various applications. The goal of this task is to generate or estimate human movement that consistently reflects the temporal patterns of conditioning inputs. Existing methods typically rely on cross-attention mechanisms to fuse the condition with motion. However, this approach primarily captures global interactions and struggles to maintain step-by-step temporal alignment. To address this limitation, we introduce Temporally Conditional Mamba, a new mamba-based model for human motion generation. Our approach integrates conditional information into the recurrent dynamics of the Mamba block, enabling better temporally aligned motion. To validate the effectiveness of our method, we evaluate it on a variety of human motion tasks. Extensive experiments demonstrate that our model significantly improves temporal alignment, motion realism, and condition consistency over state-of-the-art approaches. Our project page is available at https://zquang2202.github.io/TCM.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12565v1": {
    "title": "MMOT: The First Challenging Benchmark for Drone-based Multispectral Multi-Object Tracking",
    "url": "https://www.alphaxiv.org/abs/2510.12565v1",
    "arxiv_id": "2510.12565v1",
    "authors": "Tianhao Li, Tingfa Xu, Ying Wang, Haolin Qin, Xu Lin, Jianan Li",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 14:25:17",
    "ori_summary": "Drone-based multi-object tracking is essential yet highly challenging due to small targets, severe occlusions, and cluttered backgrounds. Existing RGB-based tracking algorithms heavily depend on spatial appearance cues such as color and texture, which often degrade in aerial views, compromising reliability. Multispectral imagery, capturing pixel-level spectral reflectance, provides crucial cues that enhance object discriminability under degraded spatial conditions. However, the lack of dedicated multispectral UAV datasets has hindered progress in this domain. To bridge this gap, we introduce MMOT, the first challenging benchmark for drone-based multispectral multi-object tracking. It features three key characteristics: (i) Large Scale - 125 video sequences with over 488.8K annotations across eight categories; (ii) Comprehensive Challenges - covering diverse conditions such as extreme small targets, high-density scenarios, severe occlusions, and complex motion; and (iii) Precise Oriented Annotations - enabling accurate localization and reduced ambiguity under aerial perspectives. To better extract spectral features and leverage oriented annotations, we further present a multispectral and orientation-aware MOT scheme adapting existing methods, featuring: (i) a lightweight Spectral 3D-Stem integrating spectral features while preserving compatibility with RGB pretraining; (ii) an orientation-aware Kalman filter for precise state estimation; and (iii) an end-to-end orientation-adaptive transformer. Extensive experiments across representative trackers consistently show that multispectral input markedly improves tracking performance over RGB baselines, particularly for small and densely packed objects. We believe our work will advance drone-based multispectral multi-object tracking research. Our MMOT, code, and benchmarks are publicly available at https://github.com/Annzstbl/MMOT.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12560v1": {
    "title": "CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving",
    "url": "https://www.alphaxiv.org/abs/2510.12560v1",
    "arxiv_id": "2510.12560v1",
    "authors": "Xiaoji Zheng, Ziyuan Yang, Yanhao Chen, Yuhang Peng, Yuanrong Tang, Gengyuan Liu, Bokui Chen, Jiangtao Gong",
    "categories": "cs.CV, cs.LG, cs.RO",
    "pub_date": "2025-10-14 14:21:52",
    "ori_summary": "End-to-end autonomous driving models trained solely with imitation learning (IL) often suffer from poor generalization. In contrast, reinforcement learning (RL) promotes exploration through reward maximization but faces challenges such as sample inefficiency and unstable convergence. A natural solution is to combine IL and RL. Moving beyond the conventional two-stage paradigm (IL pretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive dual-policy framework that enables IL and RL agents to interact during training. CoIRL-AD introduces a competition-based mechanism that facilitates knowledge exchange while preventing gradient conflicts. Experiments on the nuScenes dataset show an 18% reduction in collision rate compared to baselines, along with stronger generalization and improved performance on long-tail scenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12537v1": {
    "title": "Unconditional Human Motion and Shape Generation via Balanced Score-Based Diffusion",
    "url": "https://www.alphaxiv.org/abs/2510.12537v1",
    "arxiv_id": "2510.12537v1",
    "authors": "David Björkstrand, Tiesheng Wang, Lars Bretzner, Josephine Sullivan",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-14 14:02:22",
    "ori_summary": "Recent work has explored a range of model families for human motion generation, including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and diffusion-based models. Despite their differences, many methods rely on over-parameterized input features and auxiliary losses to improve empirical results. These strategies should not be strictly necessary for diffusion models to match the human motion distribution. We show that on par with state-of-the-art results in unconditional human motion generation are achievable with a score-based diffusion model using only careful feature-space normalization and analytically derived weightings for the standard L2 score-matching loss, while generating both motion and shape directly, thereby avoiding slow post hoc shape recovery from joints. We build the method step by step, with a clear theoretical motivation for each component, and provide targeted ablations demonstrating the effectiveness of each proposed addition in isolation.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12524v1": {
    "title": "Voronoi-Assisted Diffusion for Computing Unsigned Distance Fields from Unoriented Points",
    "url": "https://www.alphaxiv.org/abs/2510.12524v1",
    "arxiv_id": "2510.12524v1",
    "authors": "Jiayi Kong, Chen Zong, Junkai Deng, Xuhui Chen, Fei Hou, Shiqing Xin, Junhui Hou, Chen Qian, Ying He",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 13:49:53",
    "ori_summary": "Unsigned Distance Fields (UDFs) provide a flexible representation for 3D shapes with arbitrary topology, including open and closed surfaces, orientable and non-orientable geometries, and non-manifold structures. While recent neural approaches have shown promise in learning UDFs, they often suffer from numerical instability, high computational cost, and limited controllability. We present a lightweight, network-free method, Voronoi-Assisted Diffusion (VAD), for computing UDFs directly from unoriented point clouds. Our approach begins by assigning bi-directional normals to input points, guided by two Voronoi-based geometric criteria encoded in an energy function for optimal alignment. The aligned normals are then diffused to form an approximate UDF gradient field, which is subsequently integrated to recover the final UDF. Experiments demonstrate that VAD robustly handles watertight and open surfaces, as well as complex non-manifold and non-orientable geometries, while remaining computationally efficient and stable.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12493v1": {
    "title": "BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring",
    "url": "https://www.alphaxiv.org/abs/2510.12493v1",
    "arxiv_id": "2510.12493v1",
    "authors": "An Zhao, Piaopiao Yu, Zhe Zhu, Mingqiang Wei",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 13:26:56",
    "ori_summary": "3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene reconstruction.However, reconstructing high-quality 3D scenes from motion-blurred images caused by camera motion poses a significant challenge.The performance of existing 3DGS-based deblurring methods are limited due to their inherent mechanisms, such as extreme dependence on the accuracy of camera poses and inability to effectively control erroneous Gaussian primitives densification caused by motion blur.To solve these problems, we introduce a novel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D scenes from motion-blurred images.BSGS contains two stages. First, Camera Pose Refinement roughly optimizes camera poses to reduce motion-induced distortions. Second, with fixed rough camera poses, Global RigidTransformation further corrects motion-induced blur distortions.To alleviate multi-subframe gradient conflicts, we propose a subframe gradient aggregation strategy to optimize both stages.Furthermore, a space-time bi-stage optimization strategy is introduced to dynamically adjust primitive densification thresholds and prevent premature noisy Gaussian generation in blurred regions. Comprehensive experiments verify the effectiveness of our proposed deblurring method and show its superiority over the state of the arts.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12483v1": {
    "title": "Fast Visuomotor Policy for Robotic Manipulation",
    "url": "https://www.alphaxiv.org/abs/2510.12483v1",
    "arxiv_id": "2510.12483v1",
    "authors": "Jingkai Jia, Tong Yang, Xueyao Chen, Chenhuan Liu, Wenqiang Zhang",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-14 13:18:45",
    "ori_summary": "We present a fast and effective policy framework for robotic manipulation, named Energy Policy, designed for high-frequency robotic tasks and resource-constrained systems. Unlike existing robotic policies, Energy Policy natively predicts multimodal actions in a single forward pass, enabling high-precision manipulation at high speed. The framework is built upon two core components. First, we adopt the energy score as the learning objective to facilitate multimodal action modeling. Second, we introduce an energy MLP to implement the proposed objective while keeping the architecture simple and efficient. We conduct comprehensive experiments in both simulated environments and real-world robotic tasks to evaluate the effectiveness of Energy Policy. The results show that Energy Policy matches or surpasses the performance of state-of-the-art manipulation methods while significantly reducing computational overhead. Notably, on the MimicGen benchmark, Energy Policy achieves superior performance with at a faster inference compared to existing approaches.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12482v1": {
    "title": "A Text-Image Fusion Method with Data Augmentation Capabilities for Referring Medical Image Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.12482v1",
    "arxiv_id": "2510.12482v1",
    "authors": "Shurong Chai, Rahul Kumar JAIN, Rui Xu, Shaocong Mo, Ruibo Hou, Shiyu Teng, Jiaqing Liu, Lanfen Lin, Yen-Wei Chen",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-14 13:18:34",
    "ori_summary": "Deep learning relies heavily on data augmentation to mitigate limited data, especially in medical imaging. Recent multimodal learning integrates text and images for segmentation, known as referring or text-guided image segmentation. However, common augmentations like rotation and flipping disrupt spatial alignment between image and text, weakening performance. To address this, we propose an early fusion framework that combines text and visual features before augmentation, preserving spatial consistency. We also design a lightweight generator that projects text embeddings into visual space, bridging semantic gaps. Visualization of generated pseudo-images shows accurate region localization. Our method is evaluated on three medical imaging tasks and four segmentation frameworks, achieving state-of-the-art results. Code is publicly available on GitHub: https://github.com/11yxk/MedSeg_EarlyFusion.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12468v1": {
    "title": "MS-GAGA: Metric-Selective Guided Adversarial Generation Attack",
    "url": "https://www.alphaxiv.org/abs/2510.12468v1",
    "arxiv_id": "2510.12468v1",
    "authors": "Dion J. X. Ho, Gabriel Lee Jun Rong, Niharika Shrivastava, Harshavardhan Abichandani, Pai Chet Ng, Xiaoxiao Miao",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 13:01:40",
    "ori_summary": "We present MS-GAGA (Metric-Selective Guided Adversarial Generation Attack), a two-stage framework for crafting transferable and visually imperceptible adversarial examples against deepfake detectors in black-box settings. In Stage 1, a dual-stream attack module generates adversarial candidates: MNTD-PGD applies enhanced gradient calculations optimized for small perturbation budgets, while SG-PGD focuses perturbations on visually salient regions. This complementary design expands the adversarial search space and improves transferability across unseen models. In Stage 2, a metric-aware selection module evaluates candidates based on both their success against black-box models and their structural similarity (SSIM) to the original image. By jointly optimizing transferability and imperceptibility, MS-GAGA achieves up to 27% higher misclassification rates on unseen detectors compared to state-of-the-art attacks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12451v1": {
    "title": "A Function Centric Perspective On Flat and Sharp Minima",
    "url": "https://www.alphaxiv.org/abs/2510.12451v1",
    "arxiv_id": "2510.12451v1",
    "authors": "Israel Mason-Williams, Gabryel Mason-Williams, Helen Yannakoudakis",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-14 12:33:14",
    "ori_summary": "Flat minima are widely believed to correlate with improved generalisation in deep neural networks. However, this connection has proven more nuanced in recent studies, with both theoretical counterexamples and empirical exceptions emerging in the literature. In this paper, we revisit the role of sharpness in model performance, proposing that sharpness is better understood as a function-dependent property rather than a reliable indicator of poor generalisation. We conduct extensive empirical studies, from single-objective optimisation to modern image classification tasks, showing that sharper minima often emerge when models are regularised (e.g., via SAM, weight decay, or data augmentation), and that these sharp minima can coincide with better generalisation, calibration, robustness, and functional consistency. Across a range of models and datasets, we find that baselines without regularisation tend to converge to flatter minima yet often perform worse across all safety metrics. Our findings demonstrate that function complexity, rather than flatness alone, governs the geometry of solutions, and that sharper minima can reflect more appropriate inductive biases (especially under regularisation), calling for a function-centric reappraisal of loss landscape geometry.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12444v1": {
    "title": "A Review of Longitudinal Radiology Report Generation: Dataset Composition, Methods, and Performance Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.12444v1",
    "arxiv_id": "2510.12444v1",
    "authors": "Shaoyang Zhou, Yingshu Li, Yunyi Liu, Lingqiao Liu, Lei Wang, Luping Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 12:26:23",
    "ori_summary": "Chest Xray imaging is a widely used diagnostic tool in modern medicine, and its high utilization creates substantial workloads for radiologists. To alleviate this burden, vision language models are increasingly applied to automate Chest Xray radiology report generation (CXRRRG), aiming for clinically accurate descriptions while reducing manual effort. Conventional approaches, however, typically rely on single images, failing to capture the longitudinal context necessary for producing clinically faithful comparison statements. Recently, growing attention has been directed toward incorporating longitudinal data into CXR RRG, enabling models to leverage historical studies in ways that mirror radiologists diagnostic workflows. Nevertheless, existing surveys primarily address single image CXRRRG and offer limited guidance for longitudinal settings, leaving researchers without a systematic framework for model design. To address this gap, this survey provides the first comprehensive review of longitudinal radiology report generation (LRRG). Specifically, we examine dataset construction strategies, report generation architectures alongside longitudinally tailored designs, and evaluation protocols encompassing both longitudinal specific measures and widely used benchmarks. We further summarize LRRG methods performance, alongside analyses of different ablation studies, which collectively highlight the critical role of longitudinal information and architectural design choices in improving model performance. Finally, we summarize five major limitations of current research and outline promising directions for future development, aiming to lay a foundation for advancing this emerging field.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12425v1": {
    "title": "Tensor Completion via Monotone Inclusion: Generalized Low-Rank Priors Meet Deep Denoisers",
    "url": "https://www.alphaxiv.org/abs/2510.12425v1",
    "arxiv_id": "2510.12425v1",
    "authors": "Peng Chen, Deliang Wei, Jiale Yao, Fang Li",
    "categories": "math.OC, cs.CV, 65K10, 68T07, 94A08",
    "pub_date": "2025-10-14 12:01:32",
    "ori_summary": "Missing entries in multi dimensional data pose significant challenges for downstream analysis across diverse real world applications. These data are naturally modeled as tensors, and recent completion methods integrating global low rank priors with plug and play denoisers have demonstrated strong empirical performance. However, these approaches often rely on empirical convergence alone or unrealistic assumptions, such as deep denoisers acting as proximal operators of implicit regularizers, which generally does not hold. To address these limitations, we propose a novel tensor completion framework grounded in the monotone inclusion paradigm, which unifies generalized low rank priors with deep pseudo contractive denoisers and extends beyond traditional convex optimization. Building on the Davis Yin splitting scheme, we develop the GTCTV DPC algorithm and rigorously establish its global convergence. Extensive experiments demonstrate that GTCTV DPC consistently outperforms existing methods in both quantitative metrics and visual quality, particularly at low sampling rates.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12422v1": {
    "title": "VideoLucy: Deep Memory Backtracking for Long Video Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.12422v1",
    "arxiv_id": "2510.12422v1",
    "authors": "Jialong Zuo, Yongtai Deng, Lingdong Kong, Jingkang Yang, Rui Jin, Yiwei Zhang, Nong Sang, Liang Pan, Ziwei Liu, Changxin Gao",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 11:59:19",
    "ori_summary": "Recent studies have shown that agent-based systems leveraging large language models (LLMs) for key information retrieval and integration have emerged as a promising approach for long video understanding. However, these systems face two major challenges. First, they typically perform modeling and reasoning on individual frames, struggling to capture the temporal context of consecutive frames. Second, to reduce the cost of dense frame-level captioning, they adopt sparse frame sampling, which risks discarding crucial information. To overcome these limitations, we propose VideoLucy, a deep memory backtracking framework for long video understanding. Inspired by the human recollection process from coarse to fine, VideoLucy employs a hierarchical memory structure with progressive granularity. This structure explicitly defines the detail level and temporal scope of memory at different hierarchical depths. Through an agent-based iterative backtracking mechanism, VideoLucy systematically mines video-wide, question-relevant deep memories until sufficient information is gathered to provide a confident answer. This design enables effective temporal understanding of consecutive frames while preserving critical details. In addition, we introduce EgoMem, a new benchmark for long video understanding. EgoMem is designed to comprehensively evaluate a model's ability to understand complex events that unfold over time and capture fine-grained details in extremely long videos. Extensive experiments demonstrate the superiority of VideoLucy. Built on open-source models, VideoLucy significantly outperforms state-of-the-art methods on multiple long video understanding benchmarks, achieving performance even surpassing the latest proprietary models such as GPT-4o. Our code and dataset will be made publicly at https://videolucy.github.io",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12408v1": {
    "title": "Low-Field Magnetic Resonance Image Quality Enhancement using a Conditional Flow Matching Model",
    "url": "https://www.alphaxiv.org/abs/2510.12408v1",
    "arxiv_id": "2510.12408v1",
    "authors": "Huu Tien Nguyen, Ahmed Karam Eldaly",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-14 11:41:27",
    "ori_summary": "This paper introduces a novel framework for image quality transfer based on conditional flow matching (CFM). Unlike conventional generative models that rely on iterative sampling or adversarial objectives, CFM learns a continuous flow between a noise distribution and target data distributions through the direct regression of an optimal velocity field. We evaluate this approach in the context of low-field magnetic resonance imaging (LF-MRI), a rapidly emerging modality that offers affordable and portable scanning but suffers from inherently low signal-to-noise ratio and reduced diagnostic quality. Our framework is designed to reconstruct high-field-like MR images from their corresponding low-field inputs, thereby bridging the quality gap without requiring expensive infrastructure. Experiments demonstrate that CFM not only achieves state-of-the-art performance, but also generalizes robustly to both in-distribution and out-of-distribution data. Importantly, it does so while utilizing significantly fewer parameters than competing deep learning methods. These results underline the potential of CFM as a powerful and scalable tool for MRI reconstruction, particularly in resource-limited clinical environments.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12400v1": {
    "title": "Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda",
    "url": "https://www.alphaxiv.org/abs/2510.12400v1",
    "arxiv_id": "2510.12400v1",
    "authors": "André Torneiro, Diogo Monteiro, Paulo Novais, Pedro Rangel Henriques, Nuno F. Rodrigues",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 11:27:46",
    "ori_summary": "Urban monitoring of public infrastructure (such as waste bins, road signs, vegetation, sidewalks, and construction sites) poses significant challenges due to the diversity of objects, environments, and contextual conditions involved. Current state-of-the-art approaches typically rely on a combination of IoT sensors and manual inspections, which are costly, difficult to scale, and often misaligned with citizens' perception formed through direct visual observation. This raises a critical question: Can machines now \"see\" like citizens and infer informed opinions about the condition of urban infrastructure? Vision-Language Models (VLMs), which integrate visual understanding with natural language reasoning, have recently demonstrated impressive capabilities in processing complex visual information, turning them into a promising technology to address this challenge. This systematic review investigates the role of VLMs in urban monitoring, with particular emphasis on zero-shot applications. Following the PRISMA methodology, we analyzed 32 peer-reviewed studies published between 2021 and 2025 to address four core research questions: (1) What urban monitoring tasks have been effectively addressed using VLMs? (2) Which VLM architectures and frameworks are most commonly used and demonstrate superior performance? (3) What datasets and resources support this emerging field? (4) How are VLM-based applications evaluated, and what performance levels have been reported?",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12387v1": {
    "title": "Scene Coordinate Reconstruction Priors",
    "url": "https://www.alphaxiv.org/abs/2510.12387v1",
    "arxiv_id": "2510.12387v1",
    "authors": "Wenjing Bian, Axel Barroso-Laguna, Tommaso Cavallari, Victor Adrian Prisacariu, Eric Brachmann",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 11:13:31",
    "ori_summary": "Scene coordinate regression (SCR) models have proven to be powerful implicit scene representations for 3D vision, enabling visual relocalization and structure-from-motion. SCR models are trained specifically for one scene. If training images imply insufficient multi-view constraints SCR models degenerate. We present a probabilistic reinterpretation of training SCR models, which allows us to infuse high-level reconstruction priors. We investigate multiple such priors, ranging from simple priors over the distribution of reconstructed depth values to learned priors over plausible scene coordinate configurations. For the latter, we train a 3D point cloud diffusion model on a large corpus of indoor scans. Our priors push predicted 3D scene points towards plausible geometry at each training step to increase their likelihood. On three indoor datasets our priors help learning better scene representations, resulting in more coherent scene point clouds, higher registration rates and better camera poses, with a positive effect on down-stream tasks such as novel view synthesis and camera relocalization.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12385v1": {
    "title": "Learning to Recognize Correctly Completed Procedure Steps in Egocentric Assembly Videos through Spatio-Temporal Modeling",
    "url": "https://www.alphaxiv.org/abs/2510.12385v1",
    "arxiv_id": "2510.12385v1",
    "authors": "Tim J. Schoonbeek, Shao-Hsuan Hung, Dan Lehman, Hans Onvlee, Jacek Kustra, Peter H. N. de With, Fons van der Sommen",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 11:03:30",
    "ori_summary": "Procedure step recognition (PSR) aims to identify all correctly completed steps and their sequential order in videos of procedural tasks. The existing state-of-the-art models rely solely on detecting assembly object states in individual video frames. By neglecting temporal features, model robustness and accuracy are limited, especially when objects are partially occluded. To overcome these limitations, we propose Spatio-Temporal Occlusion-Resilient Modeling for Procedure Step Recognition (STORM-PSR), a dual-stream framework for PSR that leverages both spatial and temporal features. The assembly state detection stream operates effectively with unobstructed views of the object, while the spatio-temporal stream captures both spatial and temporal features to recognize step completions even under partial occlusion. This stream includes a spatial encoder, pre-trained using a novel weakly supervised approach to capture meaningful spatial representations, and a transformer-based temporal encoder that learns how these spatial features relate over time. STORM-PSR is evaluated on the MECCANO and IndustReal datasets, reducing the average delay between actual and predicted assembly step completions by 11.2% and 26.1%, respectively, compared to prior methods. We demonstrate that this reduction in delay is driven by the spatio-temporal stream, which does not rely on unobstructed views of the object to infer completed steps. The code for STORM-PSR, along with the newly annotated MECCANO labels, is made publicly available at https://timschoonbeek.github.io/stormpsr .",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12376v1": {
    "title": "Deep Attention-guided Adaptive Subsampling",
    "url": "https://www.alphaxiv.org/abs/2510.12376v1",
    "arxiv_id": "2510.12376v1",
    "authors": "Sharath M Shankaranarayana, Soumava Kumar Roy, Prasad Sudhakar, Chandan Aladahalli",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-14 10:50:45",
    "ori_summary": "Although deep neural networks have provided impressive gains in performance, these improvements often come at the cost of increased computational complexity and expense. In many cases, such as 3D volume or video classification tasks, not all slices or frames are necessary due to inherent redundancies. To address this issue, we propose a novel learnable subsampling framework that can be integrated into any neural network architecture. Subsampling, being a nondifferentiable operation, poses significant challenges for direct adaptation into deep learning models. While some works, have proposed solutions using the Gumbel-max trick to overcome the problem of non-differentiability, they fall short in a crucial aspect: they are only task-adaptive and not inputadaptive. Once the sampling mechanism is learned, it remains static and does not adjust to different inputs, making it unsuitable for real-world applications. To this end, we propose an attention-guided sampling module that adapts to inputs even during inference. This dynamic adaptation results in performance gains and reduces complexity in deep neural network models. We demonstrate the effectiveness of our method on 3D medical imaging datasets from MedMNIST3D as well as two ultrasound video datasets for classification tasks, one of them being a challenging in-house dataset collected under real-world clinical conditions.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12362v1": {
    "title": "CurriFlow: Curriculum-Guided Depth Fusion with Optical Flow-Based Temporal Alignment for 3D Semantic Scene Completion",
    "url": "https://www.alphaxiv.org/abs/2510.12362v1",
    "arxiv_id": "2510.12362v1",
    "authors": "Jinzhou Lin, Jie Zhou, Wenhao Xu, Rongtao Xu, Changwei Wang, Shunpeng Chen, Kexue Fu, Yihua Shao, Li Guo, Shibiao Xu",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 10:25:26",
    "ori_summary": "Semantic Scene Completion (SSC) aims to infer complete 3D geometry and semantics from monocular images, serving as a crucial capability for camera-based perception in autonomous driving. However, existing SSC methods relying on temporal stacking or depth projection often lack explicit motion reasoning and struggle with occlusions and noisy depth supervision. We propose CurriFlow, a novel semantic occupancy prediction framework that integrates optical flow-based temporal alignment with curriculum-guided depth fusion. CurriFlow employs a multi-level fusion strategy to align segmentation, visual, and depth features across frames using pre-trained optical flow, thereby improving temporal consistency and dynamic object understanding. To enhance geometric robustness, a curriculum learning mechanism progressively transitions from sparse yet accurate LiDAR depth to dense but noisy stereo depth during training, ensuring stable optimization and seamless adaptation to real-world deployment. Furthermore, semantic priors from the Segment Anything Model (SAM) provide category-agnostic supervision, strengthening voxel-level semantic learning and spatial consistency. Experiments on the SemanticKITTI benchmark demonstrate that CurriFlow achieves state-of-the-art performance with a mean IoU of 16.9, validating the effectiveness of our motion-guided and curriculum-aware design for camera-based 3D semantic scene completion.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12308v1": {
    "title": "Hybrid Gaussian Splatting for Novel Urban View Synthesis",
    "url": "https://www.alphaxiv.org/abs/2510.12308v1",
    "arxiv_id": "2510.12308v1",
    "authors": "Mohamed Omran, Farhad Zanjani, Davide Abati, Jens Petersen, Amirhossein Habibian",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 09:09:13",
    "ori_summary": "This paper describes the Qualcomm AI Research solution to the RealADSim-NVS challenge, hosted at the RealADSim Workshop at ICCV 2025. The challenge concerns novel view synthesis in street scenes, and participants are required to generate, starting from car-centric frames captured during some training traversals, renders of the same urban environment as viewed from a different traversal (e.g. different street lane or car direction). Our solution is inspired by hybrid methods in scene generation and generative simulators merging gaussian splatting and diffusion models, and it is composed of two stages: First, we fit a 3D reconstruction of the scene and render novel views as seen from the target cameras. Then, we enhance the resulting frames with a dedicated single-step diffusion model. We discuss specific choices made in the initialization of gaussian primitives as well as the finetuning of the enhancer model and its training data curation. We report the performance of our model design and we ablate its components in terms of novel view quality as measured by PSNR, SSIM and LPIPS. On the public leaderboard reporting test results, our proposal reaches an aggregated score of 0.432, achieving the second place overall.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12283v1": {
    "title": "Dual Learning with Dynamic Knowledge Distillation and Soft Alignment for Partially Relevant Video Retrieval",
    "url": "https://www.alphaxiv.org/abs/2510.12283v1",
    "arxiv_id": "2510.12283v1",
    "authors": "Jianfeng Dong, Lei Huang, Daizong Liu, Xianke Chen, Xun Yang, Changting Lin, Xun Wang, Meng Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 08:38:20",
    "ori_summary": "Almost all previous text-to-video retrieval works ideally assume that videos are pre-trimmed with short durations containing solely text-related content. However, in practice, videos are typically untrimmed in long durations with much more complicated background content. Therefore, in this paper, we focus on the more practical yet challenging task of Partially Relevant Video Retrieval (PRVR), which aims to retrieve partially relevant untrimmed videos with the given query. To tackle this task, we propose a novel framework that distills generalization knowledge from a powerful large-scale vision-language pre-trained model and transfers it to a lightweight, task-specific PRVR network. Specifically, we introduce a Dual Learning framework with Dynamic Knowledge Distillation (DL-DKD++), where a large teacher model provides supervision to a compact dual-branch student network. The student model comprises two branches: an inheritance branch that absorbs transferable knowledge from the teacher, and an exploration branch that learns task-specific information from the PRVR dataset to address domain gaps. To further enhance learning, we incorporate a dynamic soft-target construction mechanism. By replacing rigid hard-target supervision with adaptive soft targets that evolve during training, our method enables the model to better capture the fine-grained, partial relevance between videos and queries. Experiment results demonstrate that our proposed model achieves state-of-the-art performance on TVR, ActivityNet, and Charades-STA datasets for PRVR. The code is available at https://github.com/HuiGuanLab/DL-DKD.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12282v1": {
    "title": "PAGS: Priority-Adaptive Gaussian Splatting for Dynamic Driving Scenes",
    "url": "https://www.alphaxiv.org/abs/2510.12282v1",
    "arxiv_id": "2510.12282v1",
    "authors": "Ying A, Wenzhang Sun, Chang Zeng, Chunfeng Wang, Hao Li, Jianxun Cui",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 08:36:09",
    "ori_summary": "Reconstructing dynamic 3D urban scenes is crucial for autonomous driving, yet current methods face a stark trade-off between fidelity and computational cost. This inefficiency stems from their semantically agnostic design, which allocates resources uniformly, treating static backgrounds and safety-critical objects with equal importance. To address this, we introduce Priority-Adaptive Gaussian Splatting (PAGS), a framework that injects task-aware semantic priorities directly into the 3D reconstruction and rendering pipeline. PAGS introduces two core contributions: (1) Semantically-Guided Pruning and Regularization strategy, which employs a hybrid importance metric to aggressively simplify non-critical scene elements while preserving fine-grained details on objects vital for navigation. (2) Priority-Driven Rendering pipeline, which employs a priority-based depth pre-pass to aggressively cull occluded primitives and accelerate the final shading computations. Extensive experiments on the Waymo and KITTI datasets demonstrate that PAGS achieves exceptional reconstruction quality, particularly on safety-critical objects, while significantly reducing training time and boosting rendering speeds to over 350 FPS.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12267v1": {
    "title": "SpineBench: Benchmarking Multimodal LLMs for Spinal Pathology Analysis",
    "url": "https://www.alphaxiv.org/abs/2510.12267v1",
    "arxiv_id": "2510.12267v1",
    "authors": "Chenghanyu Zhang, Zekun Li, Peipei Li, Xing Cui, Shuhan Xia, Weixiang Yan, Yiqiao Zhang, Qianyu Zhuang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 08:19:22",
    "ori_summary": "With the increasing integration of Multimodal Large Language Models (MLLMs) into the medical field, comprehensive evaluation of their performance in various medical domains becomes critical. However, existing benchmarks primarily assess general medical tasks, inadequately capturing performance in nuanced areas like the spine, which relies heavily on visual input. To address this, we introduce SpineBench, a comprehensive Visual Question Answering (VQA) benchmark designed for fine-grained analysis and evaluation of MLLMs in the spinal domain. SpineBench comprises 64,878 QA pairs from 40,263 spine images, covering 11 spinal diseases through two critical clinical tasks: spinal disease diagnosis and spinal lesion localization, both in multiple-choice format. SpineBench is built by integrating and standardizing image-label pairs from open-source spinal disease datasets, and samples challenging hard negative options for each VQA pair based on visual similarity (similar but not the same disease), simulating real-world challenging scenarios. We evaluate 12 leading MLLMs on SpineBench. The results reveal that these models exhibit poor performance in spinal tasks, highlighting limitations of current MLLM in the spine domain and guiding future improvements in spinal medicine applications. SpineBench is publicly available at https://zhangchenghanyu.github.io/SpineBench.github.io/.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12260v1": {
    "title": "AngularFuse: A Closer Look at Angle-based Perception for Spatial-Sensitive Multi-Modality Image Fusion",
    "url": "https://www.alphaxiv.org/abs/2510.12260v1",
    "arxiv_id": "2510.12260v1",
    "authors": "Xiaopeng Liu, Yupei Lin, Sen Zhang, Xiao Wang, Yukai Shi, Liang Lin",
    "categories": "cs.CV, cs.LG, eess.IV",
    "pub_date": "2025-10-14 08:13:15",
    "ori_summary": "Visible-infrared image fusion is crucial in key applications such as autonomous driving and nighttime surveillance. Its main goal is to integrate multimodal information to produce enhanced images that are better suited for downstream tasks. Although deep learning based fusion methods have made significant progress, mainstream unsupervised approaches still face serious challenges in practical applications. Existing methods mostly rely on manually designed loss functions to guide the fusion process. However, these loss functions have obvious limitations. On one hand, the reference images constructed by existing methods often lack details and have uneven brightness. On the other hand, the widely used gradient losses focus only on gradient magnitude. To address these challenges, this paper proposes an angle-based perception framework for spatial-sensitive image fusion (AngularFuse). At first, we design a cross-modal complementary mask module to force the network to learn complementary information between modalities. Then, a fine-grained reference image synthesis strategy is introduced. By combining Laplacian edge enhancement with adaptive histogram equalization, reference images with richer details and more balanced brightness are generated. Last but not least, we introduce an angle-aware loss, which for the first time constrains both gradient magnitude and direction simultaneously in the gradient domain. AngularFuse ensures that the fused images preserve both texture intensity and correct edge orientation. Comprehensive experiments on the MSRS, RoadScene, and M3FD public datasets show that AngularFuse outperforms existing mainstream methods with clear margin. Visual comparisons further confirm that our method produces sharper and more detailed results in challenging scenes, demonstrating superior fusion capability.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12259v1": {
    "title": "Local Background Features Matter in Out-of-Distribution Detection",
    "url": "https://www.alphaxiv.org/abs/2510.12259v1",
    "arxiv_id": "2510.12259v1",
    "authors": "Jinlun Ye, Zhuohao Sun, Yiqiao Qiu, Qiu Li, Zhijun Tan, Ruixuan Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 08:12:49",
    "ori_summary": "Out-of-distribution (OOD) detection is crucial when deploying deep neural networks in the real world to ensure the reliability and safety of their applications. One main challenge in OOD detection is that neural network models often produce overconfident predictions on OOD data. While some methods using auxiliary OOD datasets or generating fake OOD images have shown promising OOD detection performance, they are limited by the high costs of data collection and training. In this study, we propose a novel and effective OOD detection method that utilizes local background features as fake OOD features for model training. Inspired by the observation that OOD images generally share similar background regions with ID images, the background features are extracted from ID images as simulated OOD visual representations during training based on the local invariance of convolution. Through being optimized to reduce the $L_2$-norm of these background features, the neural networks are able to alleviate the overconfidence issue on OOD data. Extensive experiments on multiple standard OOD detection benchmarks confirm the effectiveness of our method and its wide combinatorial compatibility with existing post-hoc methods, with new state-of-the-art performance achieved from our method.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12258v1": {
    "title": "Multiplicative Loss for Enhancing Semantic Segmentation in Medical and Cellular Images",
    "url": "https://www.alphaxiv.org/abs/2510.12258v1",
    "arxiv_id": "2510.12258v1",
    "authors": "Yuto Yokoi, Kazuhiro Hotta",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 08:07:02",
    "ori_summary": "We propose two novel loss functions, Multiplicative Loss and Confidence-Adaptive Multiplicative Loss, for semantic segmentation in medical and cellular images. Although Cross Entropy and Dice Loss are widely used, their additive combination is sensitive to hyperparameters and often performs suboptimally, especially with limited data. Medical images suffer from data scarcity due to privacy, ethics, and costly annotations, requiring robust and efficient training objectives. Our Multiplicative Loss combines Cross Entropy and Dice losses multiplicatively, dynamically modulating gradients based on prediction confidence. This reduces penalties for confident correct predictions and amplifies gradients for incorrect overconfident ones, stabilizing optimization. Building on this, Confidence-Adaptive Multiplicative Loss applies a confidence-driven exponential scaling inspired by Focal Loss, integrating predicted probabilities and Dice coefficients to emphasize difficult samples. This enhances learning under extreme data scarcity by strengthening gradients when confidence is low. Experiments on cellular and medical segmentation benchmarks show our framework consistently outperforms tuned additive and existing loss functions, offering a simple, effective, and hyperparameter-free mechanism for robust segmentation under challenging data limitations.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12256v1": {
    "title": "Vectorized Video Representation with Easy Editing via Hierarchical Spatio-Temporally Consistent Proxy Embedding",
    "url": "https://www.alphaxiv.org/abs/2510.12256v1",
    "arxiv_id": "2510.12256v1",
    "authors": "Ye Chen, Liming Tan, Yupeng Zhu, Yuanbin Wang, Bingbing Ni",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 08:05:30",
    "ori_summary": "Current video representations heavily rely on unstable and over-grained priors for motion and appearance modelling, \\emph{i.e.}, pixel-level matching and tracking. A tracking error of just a few pixels would lead to the collapse of the visual object representation, not to mention occlusions and large motion frequently occurring in videos. To overcome the above mentioned vulnerability, this work proposes spatio-temporally consistent proxy nodes to represent dynamically changing objects/scenes in the video. On the one hand, the hierarchical proxy nodes have the ability to stably express the multi-scale structure of visual objects, so they are not affected by accumulated tracking error, long-term motion, occlusion, and viewpoint variation. On the other hand, the dynamic representation update mechanism of the proxy nodes adequately leverages spatio-temporal priors of the video to mitigate the impact of inaccurate trackers, thereby effectively handling drastic changes in scenes and objects. Additionally, the decoupled encoding manner of the shape and texture representations across different visual objects in the video facilitates controllable and fine-grained appearance editing capability. Extensive experiments demonstrate that the proposed representation achieves high video reconstruction accuracy with fewer parameters and supports complex video processing tasks, including video in-painting and keyframe-based temporally consistent video editing.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12241v1": {
    "title": "Ivan-ISTD: Rethinking Cross-domain Heteroscedastic Noise Perturbations in Infrared Small Target Detection",
    "url": "https://www.alphaxiv.org/abs/2510.12241v1",
    "arxiv_id": "2510.12241v1",
    "authors": "Yuehui Li, Yahao Lu, Haoyuan Wu, Sen Zhang, Liang Lin, Yukai Shi",
    "categories": "cs.CV, eess.IV",
    "pub_date": "2025-10-14 07:48:31",
    "ori_summary": "In the multimedia domain, Infrared Small Target Detection (ISTD) plays a important role in drone-based multi-modality sensing. To address the dual challenges of cross-domain shift and heteroscedastic noise perturbations in ISTD, we propose a doubly wavelet-guided Invariance learning framework(Ivan-ISTD). In the first stage, we generate training samples aligned with the target domain using Wavelet-guided Cross-domain Synthesis. This wavelet-guided alignment machine accurately separates the target background through multi-frequency wavelet filtering. In the second stage, we introduce Real-domain Noise Invariance Learning, which extracts real noise characteristics from the target domain to build a dynamic noise library. The model learns noise invariance through self-supervised loss, thereby overcoming the limitations of distribution bias in traditional artificial noise modeling. Finally, we create the Dynamic-ISTD Benchmark, a cross-domain dynamic degradation dataset that simulates the distribution shifts encountered in real-world applications. Additionally, we validate the versatility of our method using other real-world datasets. Experimental results demonstrate that our approach outperforms existing state-of-the-art methods in terms of many quantitative metrics. In particular, Ivan-ISTD demonstrates excellent robustness in cross-domain scenarios. The code for this work can be found at: https://github.com/nanjin1/Ivan-ISTD.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12231v1": {
    "title": "BIGFix: Bidirectional Image Generation with Token Fixing",
    "url": "https://www.alphaxiv.org/abs/2510.12231v1",
    "arxiv_id": "2510.12231v1",
    "authors": "Victor Besnier, David Hurych, Andrei Bursuc, Eduardo Valle",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 07:34:44",
    "ori_summary": "Recent advances in image and video generation have raised significant interest from both academia and industry. A key challenge in this field is improving inference efficiency, as model size and the number of inference steps directly impact the commercial viability of generative models while also posing fundamental scientific challenges. A promising direction involves combining auto-regressive sequential token modeling with multi-token prediction per step, reducing inference time by up to an order of magnitude. However, predicting multiple tokens in parallel can introduce structural inconsistencies due to token incompatibilities, as capturing complex joint dependencies during training remains challenging. Traditionally, once tokens are sampled, there is no mechanism to backtrack and refine erroneous predictions. We propose a method for self-correcting image generation by iteratively refining sampled tokens. We achieve this with a novel training scheme that injects random tokens in the context, improving robustness and enabling token fixing during sampling. Our method preserves the efficiency benefits of parallel token prediction while significantly enhancing generation quality. We evaluate our approach on image generation using the ImageNet-256 and CIFAR-10 datasets, as well as on video generation with UCF-101 and NuScenes, demonstrating substantial improvements across both modalities.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12225v1": {
    "title": "HoneyBee: Data Recipes for Vision-Language Reasoners",
    "url": "https://www.alphaxiv.org/abs/2510.12225v1",
    "arxiv_id": "2510.12225v1",
    "authors": "Hritik Bansal, Devandra Singh Sachan, Kai-Wei Chang, Aditya Grover, Gargi Ghosh, Wen-tau Yih, Ramakanth Pasunuru",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-14 07:23:44",
    "ori_summary": "Recent advances in vision-language models (VLMs) have made them highly effective at reasoning tasks. However, the principles underlying the construction of performant VL reasoning training datasets remain poorly understood. In this work, we introduce several data curation approaches and study their impacts on VL reasoning capabilities by carefully controlling training and evaluation setups. We analyze the effects of context (image and question pair) sources, implement targeted data interventions, and explore scaling up images, questions, and chain-of-thought (CoT) solutions. Our findings reveal that (a) context source strategies significantly affect VLM performance, (b) interventions such as auxiliary signals from image captions and the inclusion of text-only reasoning yield substantial gains, and (c) scaling all data dimensions (e.g., unique questions per image and unique CoTs per image-question pair) consistently improves reasoning capability. Motivated by these insights, we introduce HoneyBee, a large-scale, high-quality CoT reasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs trained with HoneyBee outperform state-of-the-art models across model sizes. For instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA model and the base model by 7.8% and 24.8%, respectively, on MathVerse. Furthermore, we propose a test-time scaling strategy that reduces decoding cost by 73% without sacrificing accuracy. Overall, this work presents improved strategies for VL reasoning dataset curation research.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12219v1": {
    "title": "DIANet: A Phase-Aware Dual-Stream Network for Micro-Expression Recognition via Dynamic Images",
    "url": "https://www.alphaxiv.org/abs/2510.12219v1",
    "arxiv_id": "2510.12219v1",
    "authors": "Vu Tram Anh Khuong, Luu Tu Nguyen, Thi Bich Phuong Man, Thanh Ha Le, Thi Duyen Ngo",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 07:15:29",
    "ori_summary": "Micro-expressions are brief, involuntary facial movements that typically last less than half a second and often reveal genuine emotions. Accurately recognizing these subtle expressions is critical for applications in psychology, security, and behavioral analysis. However, micro-expression recognition (MER) remains a challenging task due to the subtle and transient nature of facial cues and the limited availability of annotated data. While dynamic image (DI) representations have been introduced to summarize temporal motion into a single frame, conventional DI-based methods often overlook the distinct characteristics of different temporal phases within a micro-expression. To address this issue, this paper proposes a novel dual-stream framework, DIANet, which leverages phase-aware dynamic images - one encoding the onset-to-apex phase and the other capturing the apex-to-offset phase. Each stream is processed by a dedicated convolutional neural network, and a cross-attention fusion module is employed to adaptively integrate features from both streams based on their contextual relevance. Extensive experiments conducted on three benchmark MER datasets (CASME-II, SAMM, and MMEW) demonstrate that the proposed method consistently outperforms conventional single-phase DI-based approaches. The results highlight the importance of modeling temporal phase information explicitly and suggest a promising direction for advancing MER.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12208v1": {
    "title": "The Impact of Synthetic Data on Object Detection Model Performance: A Comparative Analysis with Real-World Data",
    "url": "https://www.alphaxiv.org/abs/2510.12208v1",
    "arxiv_id": "2510.12208v1",
    "authors": "Muammer Bay, Timo von Marcard, Dren Fazlija",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 06:59:51",
    "ori_summary": "Recent advances in generative AI, particularly in computer vision (CV), offer new opportunities to optimize workflows across industries, including logistics and manufacturing. However, many AI applications are limited by a lack of expertise and resources, which forces a reliance on general-purpose models. Success with these models often requires domain-specific data for fine-tuning, which can be costly and inefficient. Thus, using synthetic data for fine-tuning is a popular, cost-effective alternative to gathering real-world data. This work investigates the impact of synthetic data on the performance of object detection models, compared to models trained on real-world data only, specifically within the domain of warehouse logistics. To this end, we examined the impact of synthetic data generated using the NVIDIA Omniverse Replicator tool on the effectiveness of object detection models in real-world scenarios. It comprises experiments focused on pallet detection in a warehouse setting, utilizing both real and various synthetic dataset generation strategies. Our findings provide valuable insights into the practical applications of synthetic image data in computer vision, suggesting that a balanced integration of synthetic and real data can lead to robust and efficient object detection models.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12190v1": {
    "title": "Hierarchical Reasoning with Vision-Language Models for Incident Reports from Dashcam Videos",
    "url": "https://www.alphaxiv.org/abs/2510.12190v1",
    "arxiv_id": "2510.12190v1",
    "authors": "Shingo Yokoi, Kento Sasaki, Yu Yamaguchi",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 06:36:41",
    "ori_summary": "Recent advances in end-to-end (E2E) autonomous driving have been enabled by training on diverse large-scale driving datasets, yet autonomous driving models still struggle in out-of-distribution (OOD) scenarios. The COOOL benchmark targets this gap by encouraging hazard understanding beyond closed taxonomies, and the 2COOOL challenge extends it to generating human-interpretable incident reports. We present a hierarchical reasoning framework for incident report generation from dashcam videos that integrates frame-level captioning, incident frame detection, and fine-grained reasoning within vision-language models (VLMs). We further improve factual accuracy and readability through model ensembling and a Blind A/B Scoring selection protocol. On the official 2COOOL open leaderboard, our method ranks 2nd among 29 teams and achieves the best CIDEr-D score, producing accurate and coherent incident narratives. These results indicate that hierarchical reasoning with VLMs is a promising direction for accident analysis and for broader understanding of safety-critical traffic events. The implementation and code are available at https://github.com/riron1206/kaggle-2COOOL-2nd-Place-Solution.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12184v1": {
    "title": "CompoDistill: Attention Distillation for Compositional Reasoning in Multimodal LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.12184v1",
    "arxiv_id": "2510.12184v1",
    "authors": "Jiwan Kim, Kibum Kim, Sangwoo Seo, Chanyoung Park",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-14 06:27:26",
    "ori_summary": "Recently, efficient Multimodal Large Language Models (MLLMs) have gained significant attention as a solution to their high computational complexity, making them more practical for real-world applications. In this regard, the knowledge distillation (KD) approach has emerged as a promising alternative, which transfers the rich visual and linguistic knowledge from a larger model (teacher) to a smaller model (student). However, we observe that existing KD methods struggle to effectively distill the teacher MLLM's rich visual perception abilities to the student, a challenge that has been largely overlooked in previous studies. Through a systematic analysis, we identify visual attention misalignment between student and teacher as the main cause of this issue. Based on this insight, we propose CompoDistill, a novel KD framework that explicitly aligns the student's visual attention with that of the teacher to enhance the student's visual perception abilities. Our extensive experiments show that CompoDistill significantly improves performance on compositional reasoning tasks that require visual perception abilities while maintaining strong performance on visual question answering tasks, as done in existing studies. Furthermore, CompoDistill demonstrates effectiveness with a more advanced backbone, highlighting its generalizability.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12182v1": {
    "title": "BEEP3D: Box-Supervised End-to-End Pseudo-Mask Generation for 3D Instance Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.12182v1",
    "arxiv_id": "2510.12182v1",
    "authors": "Youngju Yoo, Seho Kim, Changick Kim",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 06:23:18",
    "ori_summary": "3D instance segmentation is crucial for understanding complex 3D environments, yet fully supervised methods require dense point-level annotations, resulting in substantial annotation costs and labor overhead. To mitigate this, box-level annotations have been explored as a weaker but more scalable form of supervision. However, box annotations inherently introduce ambiguity in overlapping regions, making accurate point-to-instance assignment challenging. Recent methods address this ambiguity by generating pseudo-masks through training a dedicated pseudo-labeler in an additional training stage. However, such two-stage pipelines often increase overall training time and complexity, hinder end-to-end optimization. To overcome these challenges, we propose BEEP3D-Box-supervised End-to-End Pseudo-mask generation for 3D instance segmentation. BEEP3D adopts a student-teacher framework, where the teacher model serves as a pseudo-labeler and is updated by the student model via an Exponential Moving Average. To better guide the teacher model to generate precise pseudo-masks, we introduce an instance center-based query refinement that enhances position query localization and leverages features near instance centers. Additionally, we design two novel losses-query consistency loss and masked feature consistency loss-to align semantic and geometric signals between predictions and pseudo-masks. Extensive experiments on ScanNetV2 and S3DIS datasets demonstrate that BEEP3D achieves competitive or superior performance compared to state-of-the-art weakly supervised methods while remaining computationally efficient.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12174v1": {
    "title": "UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering",
    "url": "https://www.alphaxiv.org/abs/2510.12174v1",
    "arxiv_id": "2510.12174v1",
    "authors": "Yusen Xie, Zhenmin Huang, Jianhao Jiao, Dimitrios Kanoulas, Jun Ma",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-14 06:07:57",
    "ori_summary": "In this paper, we propose UniGS, a unified map representation and differentiable framework for high-fidelity multimodal 3D reconstruction based on 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated rasterization pipeline capable of rendering photo-realistic RGB images, geometrically accurate depth maps, consistent surface normals, and semantic logits simultaneously. We redesign the rasterization to render depth via differentiable ray-ellipsoid intersection rather than using Gaussian centers, enabling effective optimization of rotation and scale attribute through analytic depth gradients. Furthermore, we derive the analytic gradient formulation for surface normal rendering, ensuring geometric consistency among reconstructed 3D scenes. To improve computational and storage efficiency, we introduce a learnable attribute that enables differentiable pruning of Gaussians with minimal contribution during training. Quantitative and qualitative experiments demonstrate state-of-the-art reconstruction accuracy across all modalities, validating the efficacy of our geometry-aware paradigm. Source code and multimodal viewer will be available on GitHub.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12160v1": {
    "title": "State Space Prompting via Gathering and Spreading Spatio-Temporal Information for Video Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.12160v1",
    "arxiv_id": "2510.12160v1",
    "authors": "Jiahuan Zhou, Kai Zhu, Zhenyu Cui, Zichen Liu, Xu Zou, Gang Hua",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 05:30:36",
    "ori_summary": "Recently, pre-trained state space models have shown great potential for video classification, which sequentially compresses visual tokens in videos with linear complexity, thereby improving the processing efficiency of video data while maintaining high performance. To apply powerful pre-trained models to downstream tasks, prompt learning is proposed to achieve efficient downstream task adaptation with only a small number of fine-tuned parameters. However, the sequentially compressed visual prompt tokens fail to capture the spatial and temporal contextual information in the video, thus limiting the effective propagation of spatial information within a video frame and temporal information between frames in the state compression model and the extraction of discriminative information. To tackle the above issue, we proposed a State Space Prompting (SSP) method for video understanding, which combines intra-frame and inter-frame prompts to aggregate and propagate key spatiotemporal information in the video. Specifically, an Intra-Frame Gathering (IFG) module is designed to aggregate spatial key information within each frame. Besides, an Inter-Frame Spreading (IFS) module is designed to spread discriminative spatio-temporal information across different frames. By adaptively balancing and compressing key spatio-temporal information within and between frames, our SSP effectively propagates discriminative information in videos in a complementary manner. Extensive experiments on four video benchmark datasets verify that our SSP significantly outperforms existing SOTA methods by 2.76% on average while reducing the overhead of fine-tuning parameters.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12159v1": {
    "title": "DPL: Spatial-Conditioned Diffusion Prototype Enhancement for One-Shot Medical Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.12159v1",
    "arxiv_id": "2510.12159v1",
    "authors": "Ziyuan Gao, Philippe Morel",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 05:28:58",
    "ori_summary": "One-shot medical image segmentation faces fundamental challenges in prototype representation due to limited annotated data and significant anatomical variability across patients. Traditional prototype-based methods rely on deterministic averaging of support features, creating brittle representations that fail to capture intra-class diversity essential for robust generalization. This work introduces Diffusion Prototype Learning (DPL), a novel framework that reformulates prototype construction through diffusion-based feature space exploration. DPL models one-shot prototypes as learnable probability distributions, enabling controlled generation of diverse yet semantically coherent prototype variants from minimal labeled data. The framework operates through three core innovations: (1) a diffusion-based prototype enhancement module that transforms single support prototypes into diverse variant sets via forward-reverse diffusion processes, (2) a spatial-aware conditioning mechanism that leverages geometric properties derived from prototype feature statistics, and (3) a conservative fusion strategy that preserves prototype fidelity while maximizing representational diversity. DPL ensures training-inference consistency by using the same diffusion enhancement and fusion pipeline in both phases. This process generates enhanced prototypes that serve as the final representations for similarity calculations, while the diffusion process itself acts as a regularizer. Extensive experiments on abdominal MRI and CT datasets demonstrate significant improvements respectively, establishing new state-of-the-art performance in one-shot medical image segmentation.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12150v1": {
    "title": "Class-aware Domain Knowledge Fusion and Fission for Continual Test-Time Adaptation",
    "url": "https://www.alphaxiv.org/abs/2510.12150v1",
    "arxiv_id": "2510.12150v1",
    "authors": "Jiahuan Zhou, Chao Zhu, Zhenyu Cui, Zichen Liu, Xu Zou, Gang Hua",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 05:09:50",
    "ori_summary": "Continual Test-Time Adaptation (CTTA) aims to quickly fine-tune the model during the test phase so that it can adapt to multiple unknown downstream domain distributions without pre-acquiring downstream domain data. To this end, existing advanced CTTA methods mainly reduce the catastrophic forgetting of historical knowledge caused by irregular switching of downstream domain data by restoring the initial model or reusing historical models. However, these methods are usually accompanied by serious insufficient learning of new knowledge and interference from potentially harmful historical knowledge, resulting in severe performance degradation. To this end, we propose a class-aware domain Knowledge Fusion and Fission method for continual test-time adaptation, called KFF, which adaptively expands and merges class-aware domain knowledge in old and new domains according to the test-time data from different domains, where discriminative historical knowledge can be dynamically accumulated. Specifically, considering the huge domain gap within streaming data, a domain Knowledge FIssion (KFI) module is designed to adaptively separate new domain knowledge from a paired class-aware domain prompt pool, alleviating the impact of negative knowledge brought by old domains that are distinct from the current domain. Besides, to avoid the cumulative computation and storage overheads from continuously fissioning new knowledge, a domain Knowledge FUsion (KFU) module is further designed to merge the fissioned new knowledge into the existing knowledge pool with minimal cost, where a greedy knowledge dynamic merging strategy is designed to improve the compatibility of new and old knowledge while keeping the computational efficiency. Extensive experiments on the ImageNet-C dataset verify the effectiveness of our proposed method against other methods.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12141v1": {
    "title": "MAPS: Masked Attribution-based Probing of Strategies- A computational framework to align human and model explanations",
    "url": "https://www.alphaxiv.org/abs/2510.12141v1",
    "arxiv_id": "2510.12141v1",
    "authors": "Sabine Muzellec, Yousif Kashef Alghetaa, Simon Kornblith, Kohitij Kar",
    "categories": "q-bio.NC, cs.CV",
    "pub_date": "2025-10-14 04:40:23",
    "ori_summary": "Human core object recognition depends on the selective use of visual information, but the strategies guiding these choices are difficult to measure directly. We present MAPS (Masked Attribution-based Probing of Strategies), a behaviorally validated computational tool that tests whether explanations derived from artificial neural networks (ANNs) can also explain human vision. MAPS converts attribution maps into explanation-masked images (EMIs) and compares image-by-image human accuracies on these minimal images with limited pixel budgets with accuracies on the full stimuli. MAPS provides a principled way to evaluate and choose among competing ANN interpretability methods. In silico, EMI-based behavioral similarity between models reliably recovers the ground-truth similarity computed from their attribution maps, establishing which explanation methods best capture the model's strategy. When applied to humans and macaques, MAPS identifies ANN-explanation combinations whose explanations align most closely with biological vision, achieving the behavioral validity of Bubble masks while requiring far fewer behavioral trials. Because it needs only access to model attributions and a modest set of behavioral data on the original images, MAPS avoids exhaustive psychophysics while offering a scalable tool for adjudicating explanations and linking human behavior, neural activity, and model decisions under a common standard.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12132v1": {
    "title": "FedHUG: Federated Heterogeneous Unsupervised Generalization for Remote Physiological Measurements",
    "url": "https://www.alphaxiv.org/abs/2510.12132v1",
    "arxiv_id": "2510.12132v1",
    "authors": "Xiao Yang, Jiyao Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 04:17:25",
    "ori_summary": "Remote physiological measurement gained wide attention, while it requires collecting users' privacy-sensitive information, and existing contactless measurements still rely on labeled client data. This presents challenges when we want to further update real-world deployed models with numerous user data lacking labels. To resolve these challenges, we instantiate a new protocol called Federated Unsupervised Domain Generalization (FUDG) in this work. Subsequently, the \\textbf{Fed}erated \\textbf{H}eterogeneous \\textbf{U}nsupervised \\textbf{G}eneralization (\\textbf{FedHUG}) framework is proposed and consists of: (1) Minimal Bias Aggregation module dynamically adjusts aggregation weights based on prior-driven bias evaluation to cope with heterogeneous non-IID features from multiple domains. (2) The Global Distribution-aware Learning Controller parameterizes the label distribution and dynamically manipulates client-specific training strategies, thereby mitigating the server-client label distribution skew and long-tail issue. The proposal shows superior performance across state-of-the-art techniques in estimation with either RGB video or mmWave radar. The code will be released.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12126v1": {
    "title": "MetaCaptioner: Towards Generalist Visual Captioning with Open-source Suites",
    "url": "https://www.alphaxiv.org/abs/2510.12126v1",
    "arxiv_id": "2510.12126v1",
    "authors": "Zhenxin Lei, Zhangwei Gao, Changyao Tian, Erfei Cui, Guanzhou Chen, Danni Yang, Yuchen Duan, Zhaokai Wang, Wenhao Li, Weiyun Wang, Xiangyu Zhao, Jiayi Ji, Yu Qiao, Wenhai Wang, Gen Luo",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 04:03:25",
    "ori_summary": "Generalist visual captioning goes beyond a simple appearance description task, but requires integrating a series of visual cues into a caption and handling various visual domains. In this task, current open-source models present a large performance gap with commercial ones, which limits various applications such as data synthesis. To bridge the gap, this paper proposes CapFlow, a novel multi-agent collaboration workflow. CapFlow demonstrates for the first time that, by capitalizing on open-source models, it is possible to achieve caption quality on par with GPT-4.1 in various domains with an 89.5% reduction in costs. By leveraging CapFlow as the data synthesizer, we produce high-quality visual captions from image and video domains at scale, and obtain a generalist visual captioner via fine-tuning, namely MetaCaptioner. Through extensive experiments, we show that MetaCaptioner not only achieves comparable captioning capabilities with commercial models but also reaches top-tier multimodal performance in the open-source community. We hope CapFlow and MetaCaptioner can benefit future multimodal research by providing a strong and cost-effective visual captioning solution.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12123v1": {
    "title": "Hardware-aware Coding Function Design for Compressive Single-Photon 3D Cameras",
    "url": "https://www.alphaxiv.org/abs/2510.12123v1",
    "arxiv_id": "2510.12123v1",
    "authors": "David Parra, Felipe Gutierrez-Barragan, Trevor Seets, Andreas Velten",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 03:52:24",
    "ori_summary": "Single-photon cameras are becoming increasingly popular in time-of-flight 3D imaging because they can time-tag individual photons with extreme resolution. However, their performance is susceptible to hardware limitations, such as system bandwidth, maximum laser power, sensor data rates, and in-sensor memory and compute resources. Compressive histograms were recently introduced as a solution to the challenge of data rates through an online in-sensor compression of photon timestamp data. Although compressive histograms work within limited in-sensor memory and computational resources, they underperform when subjected to real-world illumination hardware constraints. To address this, we present a constrained optimization approach for designing practical coding functions for compressive single-photon 3D imaging. Using gradient descent, we jointly optimize an illumination and coding matrix (i.e., the coding functions) that adheres to hardware constraints. We show through extensive simulations that our coding functions consistently outperform traditional coding designs under both bandwidth and peak power constraints. This advantage is particularly pronounced in systems constrained by peak power. Finally, we show that our approach adapts to arbitrary parameterized impulse responses by evaluating it on a real-world system with a non-ideal impulse response function.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12119v1": {
    "title": "ImageSentinel: Protecting Visual Datasets from Unauthorized Retrieval-Augmented Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12119v1",
    "arxiv_id": "2510.12119v1",
    "authors": "Ziyuan Luo, Yangyi Zhao, Ka Chun Cheung, Simon See, Renjie Wan",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 03:45:19",
    "ori_summary": "The widespread adoption of Retrieval-Augmented Image Generation (RAIG) has raised significant concerns about the unauthorized use of private image datasets. While these systems have shown remarkable capabilities in enhancing generation quality through reference images, protecting visual datasets from unauthorized use in such systems remains a challenging problem. Traditional digital watermarking approaches face limitations in RAIG systems, as the complex feature extraction and recombination processes fail to preserve watermark signals during generation. To address these challenges, we propose ImageSentinel, a novel framework for protecting visual datasets in RAIG. Our framework synthesizes sentinel images that maintain visual consistency with the original dataset. These sentinels enable protection verification through randomly generated character sequences that serve as retrieval keys. To ensure seamless integration, we leverage vision-language models to generate the sentinel images. Experimental results demonstrate that ImageSentinel effectively detects unauthorized dataset usage while preserving generation quality for authorized applications. Code is available at https://github.com/luo-ziyuan/ImageSentinel.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12114v1": {
    "title": "Self-Supervised Selective-Guided Diffusion Model for Old-Photo Face Restoration",
    "url": "https://www.alphaxiv.org/abs/2510.12114v1",
    "arxiv_id": "2510.12114v1",
    "authors": "Wenjie Li, Xiangyi Wang, Heng Guo, Guangwei Gao, Zhanyu Ma",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 03:34:15",
    "ori_summary": "Old-photo face restoration poses significant challenges due to compounded degradations such as breakage, fading, and severe blur. Existing pre-trained diffusion-guided methods either rely on explicit degradation priors or global statistical guidance, which struggle with localized artifacts or face color. We propose Self-Supervised Selective-Guided Diffusion (SSDiff), which leverages pseudo-reference faces generated by a pre-trained diffusion model under weak guidance. These pseudo-labels exhibit structurally aligned contours and natural colors, enabling region-specific restoration via staged supervision: structural guidance applied throughout the denoising process and color refinement in later steps, aligned with the coarse-to-fine nature of diffusion. By incorporating face parsing maps and scratch masks, our method selectively restores breakage regions while avoiding identity mismatch. We further construct VintageFace, a 300-image benchmark of real old face photos with varying degradation levels. SSDiff outperforms existing GAN-based and diffusion-based methods in perceptual quality, fidelity, and regional controllability. Code link: https://github.com/PRIS-CV/SSDiff.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12107v1": {
    "title": "DRL: Discriminative Representation Learning with Parallel Adapters for Class Incremental Learning",
    "url": "https://www.alphaxiv.org/abs/2510.12107v1",
    "arxiv_id": "2510.12107v1",
    "authors": "Jiawei Zhan, Jun Liu, Jinlong Peng, Xiaochen Chen, Bin-Bin Gao, Yong Liu, Chengjie Wang",
    "categories": "cs.CV, 68T05, 68T07, I.2.6; I.5.4",
    "pub_date": "2025-10-14 03:19:15",
    "ori_summary": "With the excellent representation capabilities of Pre-Trained Models (PTMs), remarkable progress has been made in non-rehearsal Class-Incremental Learning (CIL) research. However, it remains an extremely challenging task due to three conundrums: increasingly large model complexity, non-smooth representation shift during incremental learning and inconsistency between stage-wise sub-problem optimization and global inference. In this work, we propose the Discriminative Representation Learning (DRL) framework to specifically address these challenges. To conduct incremental learning effectively and yet efficiently, the DRL's network, called Incremental Parallel Adapter (IPA) network, is built upon a PTM and increasingly augments the model by learning a lightweight adapter with a small amount of parameter learning overhead in each incremental stage. The adapter is responsible for adapting the model to new classes, it can inherit and propagate the representation capability from the current model through parallel connection between them by a transfer gate. As a result, this design guarantees a smooth representation shift between different incremental stages. Furthermore, to alleviate inconsistency and enable comparable feature representations across incremental stages, we design the Decoupled Anchor Supervision (DAS). It decouples constraints of positive and negative samples by respectively comparing them with the virtual anchor. This decoupling promotes discriminative representation learning and aligns the feature spaces learned at different stages, thereby narrowing the gap between stage-wise local optimization over a subset of data and global inference across all classes. Extensive experiments on six benchmarks reveal that our DRL consistently outperforms other state-of-the-art methods throughout the entire CIL period while maintaining high efficiency in both training and inference phases.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12101v1": {
    "title": "Gaussian Semantic Field for One-shot LiDAR Global Localization",
    "url": "https://www.alphaxiv.org/abs/2510.12101v1",
    "arxiv_id": "2510.12101v1",
    "authors": "Pengyu Yin, Shenghai Yuan, Haozhi Cao, Xingyu Ji, Ruofei Bai, Siyu Chen, Lihua Xie",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-14 03:08:02",
    "ori_summary": "We present a one-shot LiDAR global localization algorithm featuring semantic disambiguation ability based on a lightweight tri-layered scene graph. While landmark semantic registration-based methods have shown promising performance improvements in global localization compared with geometric-only methods, landmarks can be repetitive and misleading for correspondence establishment. We propose to mitigate this problem by modeling semantic distributions with continuous functions learned from a population of Gaussian processes. Compared with discrete semantic labels, the continuous functions capture finer-grained geo-semantic information and also provide more detailed metric information for correspondence establishment. We insert this continuous function as the middle layer between the object layer and the metric-semantic layer, forming a tri-layered 3D scene graph, serving as a light-weight yet performant backend for one-shot localization. We term our global localization pipeline Outram-GSF (Gaussian semantic field) and conduct a wide range of experiments on publicly available data sets, validating the superior performance against the current state-of-the-art.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12099v1": {
    "title": "G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior",
    "url": "https://www.alphaxiv.org/abs/2510.12099v1",
    "arxiv_id": "2510.12099v1",
    "authors": "Junfeng Ni, Yixin Chen, Zhifei Yang, Yu Liu, Ruijie Lu, Song-Chun Zhu, Siyuan Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 03:06:28",
    "ori_summary": "Despite recent advances in leveraging generative prior from pre-trained diffusion models for 3D scene reconstruction, existing methods still face two critical limitations. First, due to the lack of reliable geometric supervision, they struggle to produce high-quality reconstructions even in observed regions, let alone in unobserved areas. Second, they lack effective mechanisms to mitigate multi-view inconsistencies in the generated images, leading to severe shape-appearance ambiguities and degraded scene geometry. In this paper, we identify accurate geometry as the fundamental prerequisite for effectively exploiting generative models to enhance 3D scene reconstruction. We first propose to leverage the prevalence of planar structures to derive accurate metric-scale depth maps, providing reliable supervision in both observed and unobserved regions. Furthermore, we incorporate this geometry guidance throughout the generative pipeline to improve visibility mask estimation, guide novel view selection, and enhance multi-view consistency when inpainting with video diffusion models, resulting in accurate and consistent scene completion. Extensive experiments on Replica, ScanNet++, and DeepBlending show that our method consistently outperforms existing baselines in both geometry and appearance reconstruction, particularly for unobserved regions. Moreover, our method naturally supports single-view inputs and unposed videos, with strong generalizability in both indoor and outdoor scenarios with practical real-world applicability. The project page is available at https://dali-jack.github.io/g4splat-web/.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12098v1": {
    "title": "An Adaptive Edge-Guided Dual-Network Framework for Fast QR Code Motion Deblurring",
    "url": "https://www.alphaxiv.org/abs/2510.12098v1",
    "arxiv_id": "2510.12098v1",
    "authors": "Jianping Li, Dongyang Guo, Wenjie Li, Wei Zhao",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 03:03:47",
    "ori_summary": "Unlike general image deblurring that prioritizes perceptual quality, QR code deblurring focuses on ensuring successful decoding. QR codes are characterized by highly structured patterns with sharp edges, a robust prior for restoration. Yet existing deep learning methods rarely exploit these priors explicitly. To address this gap, we propose the Edge-Guided Attention Block (EGAB), which embeds explicit edge priors into a Transformer architecture. Based on EGAB, we develop Edge-Guided Restormer (EG-Restormer), an effective network that significantly boosts the decoding rate of severely blurred QR codes. For mildly blurred inputs, we design the Lightweight and Efficient Network (LENet) for fast deblurring. We further integrate these two networks into an Adaptive Dual-network (ADNet), which dynamically selects the suitable network based on input blur severity, making it ideal for resource-constrained mobile devices. Extensive experiments show that our EG-Restormer and ADNet achieve state-of-the-art performance with a competitive speed. Project page: https://github.com/leejianping/ADNet",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12095v1": {
    "title": "IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation",
    "url": "https://www.alphaxiv.org/abs/2510.12095v1",
    "arxiv_id": "2510.12095v1",
    "authors": "Wenxu Zhou, Kaixuan Nie, Hang Du, Dong Yin, Wei Huang, Siqiang Guo, Xiaobo Zhang, Pengbo Hu",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 03:02:33",
    "ori_summary": "In this study, we present IL3D, a large-scale dataset meticulously designed for large language model (LLM)-driven 3D scene generation, addressing the pressing demand for diverse, high-quality training data in indoor layout design. Comprising 27,816 indoor layouts across 18 prevalent room types and a library of 29,215 high-fidelity 3D object assets, IL3D is enriched with instance-level natural language annotations to support robust multimodal learning for vision-language tasks. We establish rigorous benchmarks to evaluate LLM-driven scene generation. Experimental results show that supervised fine-tuning (SFT) of LLMs on IL3D significantly improves generalization and surpasses the performance of SFT on other datasets. IL3D offers flexible multimodal data export capabilities, including point clouds, 3D bounding boxes, multiview images, depth maps, normal maps, and semantic masks, enabling seamless adaptation to various visual tasks. As a versatile and robust resource, IL3D significantly advances research in 3D scene generation and embodied intelligence, by providing high-fidelity scene data to support environment perception tasks of embodied agents.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12089v1": {
    "title": "Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback",
    "url": "https://www.alphaxiv.org/abs/2510.12089v1",
    "arxiv_id": "2510.12089v1",
    "authors": "Xingpei Ma, Shenneng Huang, Jiaran Cai, Yuansheng Guan, Shen Zheng, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 02:50:05",
    "ori_summary": "Recent advances in diffusion models have significantly improved audio-driven human video generation, surpassing traditional methods in both quality and controllability. However, existing approaches still face challenges in lip-sync accuracy, temporal coherence for long video generation, and multi-character animation. In this work, we propose a diffusion transformer (DiT)-based framework for generating lifelike talking videos of arbitrary length, and introduce a training-free method for multi-character audio-driven animation. First, we employ a LoRA-based training strategy combined with a position shift inference approach, which enables efficient long video generation while preserving the capabilities of the foundation model. Moreover, we combine partial parameter updates with reward feedback to enhance both lip synchronization and natural body motion. Finally, we propose a training-free approach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character animation, which requires no specialized datasets or model modifications and supports audio-driven animation for three or more characters. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches, achieving high-quality, temporally coherent, and multi-character audio-driven video generation in a simple, efficient, and cost-effective manner.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12075v1": {
    "title": "A Review on Domain Adaption and Generative Adversarial Networks(GANs)",
    "url": "https://www.alphaxiv.org/abs/2510.12075v1",
    "arxiv_id": "2510.12075v1",
    "authors": "Aashish Dhawan, Divyanshu Mudgal",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-14 02:32:10",
    "ori_summary": "The major challenge in today's computer vision scenario is the availability of good quality labeled data. In a field of study like image classification, where data is of utmost importance, we need to find more reliable methods which can overcome the scarcity of data to produce results comparable to previous benchmark results. In most cases, obtaining labeled data is very difficult because of the high cost of human labor and in some cases impossible. The purpose of this paper is to discuss Domain Adaptation and various methods to implement it. The main idea is to use a model trained on a particular dataset to predict on data from a different domain of the same kind, for example - a model trained on paintings of airplanes predicting on real images of airplanes",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12069v1": {
    "title": "VIDMP3: Video Editing by Representing Motion with Pose and Position Priors",
    "url": "https://www.alphaxiv.org/abs/2510.12069v1",
    "arxiv_id": "2510.12069v1",
    "authors": "Sandeep Mishra, Oindrila Saha, Alan C. Bovik",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 02:20:12",
    "ori_summary": "Motion-preserved video editing is crucial for creators, particularly in scenarios that demand flexibility in both the structure and semantics of swapped objects. Despite its potential, this area remains underexplored. Existing diffusion-based editing methods excel in structure-preserving tasks, using dense guidance signals to ensure content integrity. While some recent methods attempt to address structure-variable editing, they often suffer from issues such as temporal inconsistency, subject identity drift, and the need for human intervention. To address these challenges, we introduce VidMP3, a novel approach that leverages pose and position priors to learn a generalized motion representation from source videos. Our method enables the generation of new videos that maintain the original motion while allowing for structural and semantic flexibility. Both qualitative and quantitative evaluations demonstrate the superiority of our approach over existing methods. The code will be made publicly available at https://github.com/sandeep-sm/VidMP3.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12060v1": {
    "title": "Your VAR Model is Secretly an Efficient and Explainable Generative Classifier",
    "url": "https://www.alphaxiv.org/abs/2510.12060v1",
    "arxiv_id": "2510.12060v1",
    "authors": "Yi-Chung Chen, David I. Inouye, Jing Gao",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-14 01:59:01",
    "ori_summary": "Generative classifiers, which leverage conditional generative models for classification, have recently demonstrated desirable properties such as robustness to distribution shifts. However, recent progress in this area has been largely driven by diffusion-based models, whose substantial computational cost severely limits scalability. This exclusive focus on diffusion-based methods has also constrained our understanding of generative classifiers. In this work, we propose a novel generative classifier built on recent advances in visual autoregressive (VAR) modeling, which offers a new perspective for studying generative classifiers. To further enhance its performance, we introduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which achieves a superior trade-off between accuracy and inference speed, thereby significantly improving practical applicability. Moreover, we show that the VAR-based method exhibits fundamentally different properties from diffusion-based methods. In particular, due to its tractable likelihood, the VAR-based classifier enables visual explainability via token-wise mutual information and demonstrates inherent resistance to catastrophic forgetting in class-incremental learning tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.12056v1": {
    "title": "APGNet: Adaptive Prior-Guided for Underwater Camouflaged Object Detection",
    "url": "https://www.alphaxiv.org/abs/2510.12056v1",
    "arxiv_id": "2510.12056v1",
    "authors": "Xinxin Huang, Han Sun, Junmin Cai, Ningzhong Liu, Huiyu Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-10-14 01:51:44",
    "ori_summary": "Detecting camouflaged objects in underwater environments is crucial for marine ecological research and resource exploration. However, existing methods face two key challenges: underwater image degradation, including low contrast and color distortion, and the natural camouflage of marine organisms. Traditional image enhancement techniques struggle to restore critical features in degraded images, while camouflaged object detection (COD) methods developed for terrestrial scenes often fail to adapt to underwater environments due to the lack of consideration for underwater optical characteristics. To address these issues, we propose APGNet, an Adaptive Prior-Guided Network, which integrates a Siamese architecture with a novel prior-guided mechanism to enhance robustness and detection accuracy. First, we employ the Multi-Scale Retinex with Color Restoration (MSRCR) algorithm for data augmentation, generating illumination-invariant images to mitigate degradation effects. Second, we design an Extended Receptive Field (ERF) module combined with a Multi-Scale Progressive Decoder (MPD) to capture multi-scale contextual information and refine feature representations. Furthermore, we propose an adaptive prior-guided mechanism that hierarchically fuses position and boundary priors by embedding spatial attention in high-level features for coarse localization and using deformable convolution to refine contours in low-level features. Extensive experimental results on two public MAS datasets demonstrate that our proposed method APGNet outperforms 15 state-of-art methods under widely used evaluation metrics.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  }
}