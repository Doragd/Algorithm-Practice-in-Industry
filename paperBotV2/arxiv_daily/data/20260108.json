{
  "2601.04019v1": {
    "title": "Modeling Behavioral Patterns in News Recommendations Using Fuzzy Neural Networks",
    "url": "https://www.alphaxiv.org/abs/2601.04019v1",
    "arxiv_id": "2601.04019v1",
    "authors": "Kevin Innerebner, Stephan Bartl, Markus Reiter-Haas, Elisabeth Lex",
    "categories": "cs.LG, cs.IR",
    "pub_date": "2026-01-07 15:34:15",
    "ori_summary": "News recommender systems are increasingly driven by black-box models, offering little transparency for editorial decision-making. In this work, we introduce a transparent recommender system that uses fuzzy neural networks to learn human-readable rules from behavioral data for predicting article clicks. By extracting the rules at configurable thresholds, we can control rule complexity and thus, the level of interpretability. We evaluate our approach on two publicly available news datasets (i.e., MIND and EB-NeRD) and show that we can accurately predict click behavior compared to several established baselines, while learning human-readable rules. Furthermore, we show that the learned rules reveal news consumption patterns, enabling editors to align content curation goals with target audience behavior.",
    "summary": "该论文研究新闻推荐系统中黑盒模型缺乏透明度的问题，核心思想是使用模糊神经网络从行为数据中学习人类可读的规则来预测文章点击，并通过可配置阈值控制规则复杂度和可解释性水平。",
    "translation": "基于模糊神经网络建模新闻推荐中的行为模式",
    "relevance_score": 8,
    "reasoning": "该论文属于'核心领域进展'范畴，直接针对推荐系统（RecSys）中的新闻推荐场景，研究用户行为模式建模这一核心问题。模糊神经网络作为建模方法，能够处理推荐系统中的不确定性和模糊性，具有明确的实用价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对推荐系统的可解释性核心问题，提出使用模糊神经网络学习人类可读规则的方法，与推荐系统核心领域进展高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03903v1": {
    "title": "Unleashing the Potential of Neighbors: Diffusion-based Latent Neighbor Generation for Session-based Recommendation",
    "url": "https://www.alphaxiv.org/abs/2601.03903v1",
    "arxiv_id": "2601.03903v1",
    "authors": "Yuhan Yang, Jie Zou, Guojia An, Jiwei Wei, Yang Yang, Heng Tao Shen",
    "categories": "cs.IR",
    "pub_date": "2026-01-07 13:14:12",
    "ori_summary": "Session-based recommendation aims to predict the next item that anonymous users may be interested in, based on their current session interactions. Recent studies have demonstrated that retrieving neighbor sessions to augment the current session can effectively alleviate the data sparsity issue and improve recommendation performance. However, existing methods typically rely on explicitly observed session data, neglecting latent neighbors - not directly observed but potentially relevant within the interest space - thereby failing to fully exploit the potential of neighbor sessions in recommendation. To address the above limitation, we propose a novel model of diffusion-based latent neighbor generation for session-based recommendation, named DiffSBR. Specifically, DiffSBR leverages two diffusion modules, including retrieval-augmented diffusion and self-augmented diffusion, to generate high-quality latent neighbors. In the retrieval-augmented diffusion module, we leverage retrieved neighbors as guiding signals to constrain and reconstruct the distribution of latent neighbors. Meanwhile, we adopt a training strategy that enables the retriever to learn from the feedback provided by the generator. In the self-augmented diffusion module, we explicitly guide the generation of latent neighbors by injecting the current session's multi-modal signals through contrastive learning. After obtaining the generated latent neighbors, we utilize them to enhance session representations for improving session-based recommendation. Extensive experiments on four public datasets show that DiffSBR generates effective latent neighbors and improves recommendation performance against state-of-the-art baselines.",
    "summary": "该论文研究会话推荐中数据稀疏性问题，其核心思想是提出DiffSBR模型，通过检索增强和自增强两个扩散模块生成未直接观察但语义相关的潜在邻居会话，以增强会话表示。",
    "translation": "释放邻居的潜力：基于扩散的潜在邻居生成用于会话推荐",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及会话推荐系统（Session-based Recommendation），这是推荐系统领域的核心研究方向。基于扩散的潜在邻居生成方法可能代表推荐系统中序列建模或图神经网络的新架构进展，具有明确的RecSys应用价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对会话推荐的核心问题，提出利用扩散模型生成潜在邻居会话的创新方法，属于推荐系统领域的前沿技术探索。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03793v1": {
    "title": "Prompt Tuning without Labeled Samples for Zero-Shot Node Classification in Text-Attributed Graphs",
    "url": "https://www.alphaxiv.org/abs/2601.03793v1",
    "arxiv_id": "2601.03793v1",
    "authors": "Sethupathy Parameswaran, Suresh Sundaram, Yuan Fang",
    "categories": "cs.LG, cs.IR",
    "pub_date": "2026-01-07 10:50:18",
    "ori_summary": "Node classification is a fundamental problem in information retrieval with many real-world applications, such as community detection in social networks, grouping articles published online and product categorization in e-commerce. Zero-shot node classification in text-attributed graphs (TAGs) presents a significant challenge, particularly due to the absence of labeled data. In this paper, we propose a novel Zero-shot Prompt Tuning (ZPT) framework to address this problem by leveraging a Universal Bimodal Conditional Generator (UBCG). Our approach begins with pre-training a graph-language model to capture both the graph structure and the associated textual descriptions of each node. Following this, a conditional generative model is trained to learn the joint distribution of nodes in both graph and text modalities, enabling the generation of synthetic samples for each class based solely on the class name. These synthetic node and text embeddings are subsequently used to perform continuous prompt tuning, facilitating effective node classification in a zero-shot setting. Furthermore, we conduct extensive experiments on multiple benchmark datasets, demonstrating that our framework performs better than existing state-of-the-art baselines. We also provide ablation studies to validate the contribution of the bimodal generator. The code is provided at: https://github.com/Sethup123/ZPT.",
    "summary": "该论文研究零样本场景下图文本属性节点的分类问题，核心思想是预训练图语言模型捕获结构与文本信息，并训练条件生成器基于类别名生成合成样本，用于连续提示调优以实现分类。",
    "translation": "面向文本属性图中零样本节点分类的无标注样本提示调优",
    "relevance_score": 8,
    "reasoning": "该论文涉及图神经网络与提示调优技术，属于核心LLM技术进展，可直接应用于推荐系统中的冷启动用户/物品建模、搜索中的查询-文档匹配以及广告中的用户兴趣建模，通过零样本学习处理新类别节点，具有明确的实践价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出零样本提示调优框架，直接应用LLM技术于图节点分类，并利用条件生成器处理图与文本异构数据，与LLM应用和异构数据建模两个焦点高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03748v1": {
    "title": "Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning",
    "url": "https://www.alphaxiv.org/abs/2601.03748v1",
    "arxiv_id": "2601.03748v1",
    "authors": "Dario Maio, Stefano Rizzi",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2026-01-07 09:37:36",
    "ori_summary": "Retrieval-Augmented Generation (RAG) systems are increasingly deployed on large-scale document collections, often comprising millions of documents and tens of millions of text chunks. In industrial-scale retrieval platforms, scalability is typically addressed through horizontal sharding and a combination of Approximate Nearest-Neighbor search, hybrid indexing, and optimized metadata filtering. Although effective from an efficiency perspective, these mechanisms rely on bottom-up, similarity-driven organization and lack a conceptual rationale for corpus partitioning. In this paper, we claim that the design of large-scale RAG systems may benefit from the combination of two orthogonal strategies: semantic clustering, which optimizes locality in embedding space, and multidimensional partitioning, which governs where retrieval should occur based on conceptual dimensions such as time and organizational context. Although such dimensions are already implicitly present in current systems, they are used in an ad hoc and poorly structured manner. We propose the Dimensional Fact Model (DFM) as a conceptual framework to guide the design of multidimensional partitions for RAG corpora. The DFM provides a principled way to reason about facts, dimensions, hierarchies, and granularity in retrieval-oriented settings. This framework naturally supports hierarchical routing and controlled fallback strategies, ensuring that retrieval remains robust even in the presence of incomplete metadata, while transforming the search process from a 'black-box' similarity matching into a governable and deterministic workflow. This work is intended as a position paper; its goal is to bridge the gap between OLAP-style multidimensional modeling and modern RAG architectures, and to stimulate further research on principled, explainable, and governable retrieval strategies at scale.",
    "summary": "",
    "translation": "桥接OLAP与RAG：面向语料库分区的多维设计方法",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及OLAP（联机分析处理）和RAG（检索增强生成）的桥接技术，主要关注语料库分区设计。虽然RAG技术可能间接应用于搜索系统，但论文核心聚焦于数据库/数据仓库的语料分区架构设计，与推荐系统、搜索排序、广告投放等核心领域关联较弱。标题未明确体现对Transformer架构、LLM技术或异构数据统一建模的直接贡献，潜在应用场景不够明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03730v1": {
    "title": "Perception-Aware Bias Detection for Query Suggestions",
    "url": "https://www.alphaxiv.org/abs/2601.03730v1",
    "arxiv_id": "2601.03730v1",
    "authors": "Fabian Haak, Philipp Schaer",
    "categories": "cs.IR",
    "pub_date": "2026-01-07 09:21:59",
    "ori_summary": "Bias in web search has been in the spotlight of bias detection research for quite a while. At the same time, little attention has been paid to query suggestions in this regard. Awareness of the problem of biased query suggestions has been raised. Likewise, there is a rising need for automatic bias detection approaches. This paper adds on the bias detection pipeline for bias detection in query suggestions of person-related search developed by Bonart et al. \\cite{Bonart_2019a}. The sparseness and lack of contextual metadata of query suggestions make them a difficult subject for bias detection. Furthermore, query suggestions are perceived very briefly and subliminally. To overcome these issues, perception-aware metrics are introduced. Consequently, the enhanced pipeline is able to better detect systematic topical bias in search engine query suggestions for person-related searches. The results of an analysis performed with the developed pipeline confirm this assumption. Due to the perception-aware bias detection metrics, findings produced by the pipeline can be assumed to reflect bias that users would discern.",
    "summary": "该论文研究搜索引擎中人物相关搜索查询建议的系统性偏见检测问题。核心方法是引入感知感知的度量指标来增强现有检测流程，以克服查询建议数据稀疏、缺乏上下文元数据且用户感知短暂的问题，从而更好地识别用户可能察觉到的偏见。",
    "translation": "面向查询建议的感知感知偏差检测",
    "relevance_score": 7,
    "reasoning": "该论文标题直接涉及搜索系统中的查询建议，这是搜索领域的核心组成部分。虽然标题未明确提及LLM技术，但偏差检测方法可能应用于改进搜索排名或推荐系统的公平性，属于核心领域进展。感知感知方法可能借鉴多模态建模思想，与VLM类比处理异构数据的理念有一定关联。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文直接针对搜索领域的查询建议偏见检测问题，提出了感知感知的度量方法，与搜索领域核心进展相关，但未涉及LLM或Transformer技术。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03628v1": {
    "title": "Global research trends and collaborations in Fibrodysplasia Ossificans Progressiva: A bibliometric analysis (1989-2023)",
    "url": "https://www.alphaxiv.org/abs/2601.03628v1",
    "arxiv_id": "2601.03628v1",
    "authors": "Muneer Ahmad, Undie Felicia Nkatv, Sajid Saleem",
    "categories": "cs.DL, cs.IR",
    "pub_date": "2026-01-07 06:17:04",
    "ori_summary": "Fibrodysplasia Ossificans Progressiva (FOP) is a rare and debilitating genetic disorder characterized by the progressive formation of bone in muscles and connective tissues. This scientometric analysis examines the global research trends on FOP between 1989 and 2023 using bibliographic data from Web of Science. The study highlights key patterns in publication productivity, influential journals, institutions, and the geographical distribution of research. The findings reveal that the United States leads both in terms of total publications and citation impact, with significant contributions from the UK, Italy, Japan, and other European countries. Additionally, the analysis identifies the major document types, including articles and reviews, and evaluates the collaborative efforts across institutions. The study offers valuable insights into the global research landscape of FOP, providing a foundation for future studies and international collaborations.",
    "summary": "",
    "translation": "进行性骨化性纤维发育不良（FOP）的全球研究趋势与合作：文献计量学分析（1989-2023年）",
    "relevance_score": 1,
    "reasoning": "该论文标题明确表明这是一篇关于罕见医学疾病（进行性骨化性纤维发育不良）的文献计量分析研究，属于“Irrelevant Topics”中明确排除的“Medical, Biology, Chemistry, Physics or other domain-specific applications”范畴。论文内容完全聚焦于特定医学领域的学术产出分析，与推荐系统、搜索、广告、LLM或Transformer架构等当前关注的技术领域没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03600v1": {
    "title": "ALERT: Zero-shot LLM Jailbreak Detection via Internal Discrepancy Amplification",
    "url": "https://www.alphaxiv.org/abs/2601.03600v1",
    "arxiv_id": "2601.03600v1",
    "authors": "Xiao Lin, Philip Li, Zhichen Zeng, Tingwei Li, Tianxin Wei, Xuying Ning, Gaotang Li, Yuzhong Chen, Hanghang Tong",
    "categories": "cs.LG, cs.AI, cs.IR",
    "pub_date": "2026-01-07 05:30:53",
    "ori_summary": "Despite rich safety alignment strategies, large language models (LLMs) remain highly susceptible to jailbreak attacks, which compromise safety guardrails and pose serious security risks. Existing detection methods mainly detect jailbreak status relying on jailbreak templates present in the training data. However, few studies address the more realistic and challenging zero-shot jailbreak detection setting, where no jailbreak templates are available during training. This setting better reflects real-world scenarios where new attacks continually emerge and evolve. To address this challenge, we propose a layer-wise, module-wise, and token-wise amplification framework that progressively magnifies internal feature discrepancies between benign and jailbreak prompts. We uncover safety-relevant layers, identify specific modules that inherently encode zero-shot discriminative signals, and localize informative safety tokens. Building upon these insights, we introduce ALERT (Amplification-based Jailbreak Detector), an efficient and effective zero-shot jailbreak detector that introduces two independent yet complementary classifiers on amplified representations. Extensive experiments on three safety benchmarks demonstrate that ALERT achieves consistently strong zero-shot detection performance. Specifically, (i) across all datasets and attack strategies, ALERT reliably ranks among the top two methods, and (ii) it outperforms the second-best baseline by at least 10% in average Accuracy and F1-score, and sometimes by up to 40%.",
    "summary": "",
    "translation": "ALERT：通过内部不一致性放大的零样本大语言模型越狱检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM安全性和对抗性攻击检测（越狱检测），这属于安全/隐私领域，被明确列为无关主题。虽然涉及LLM技术，但核心关注点是安全防护而非推荐系统、搜索或广告的应用潜力。内部不一致性放大方法可能具有技术价值，但论文方向与当前关注的RecSys/Search/Ads技术进展无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03496v1": {
    "title": "STELLA: Self-Reflective Terminology-Aware Framework for Building an Aerospace Information Retrieval Benchmark",
    "url": "https://www.alphaxiv.org/abs/2601.03496v1",
    "arxiv_id": "2601.03496v1",
    "authors": "Bongmin Kim",
    "categories": "cs.IR, cs.CL",
    "pub_date": "2026-01-07 01:23:44",
    "ori_summary": "Tasks in the aerospace industry heavily rely on searching and reusing large volumes of technical documents, yet there is no public information retrieval (IR) benchmark that reflects the terminology- and query-intent characteristics of this domain. To address this gap, this paper proposes the STELLA (Self-Reflective TErminoLogy-Aware Framework for BuiLding an Aerospace Information Retrieval Benchmark) framework. Using this framework, we introduce the STELLA benchmark, an aerospace-specific IR evaluation set constructed from NASA Technical Reports Server (NTRS) documents via a systematic pipeline that comprises document layout detection, passage chunking, terminology dictionary construction, synthetic query generation, and cross-lingual extension. The framework generates two types of queries: the Terminology Concordant Query (TCQ), which includes the terminology verbatim to evaluate lexical matching, and the Terminology Agnostic Query (TAQ), which utilizes the terminology's description to assess semantic matching. This enables a disentangled evaluation of the lexical and semantic matching capabilities of embedding models. In addition, we combine Chain-of-Density (CoD) and the Self-Reflection method with query generation to improve quality and implement a hybrid cross-lingual extension that reflects real user querying practices. Evaluation of seven embedding models on the STELLA benchmark shows that large decoder-based embedding models exhibit the strongest semantic understanding, while lexical matching methods such as BM25 remain highly competitive in domains where exact lexical matching technical term is crucial. The STELLA benchmark provides a reproducible foundation for reliable performance evaluation and improvement of embedding models in aerospace-domain IR tasks. The STELLA benchmark can be found in https://huggingface.co/datasets/telepix/STELLA.",
    "summary": "",
    "translation": "STELLA：用于构建航空航天信息检索基准的自反思术语感知框架",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及特定领域（航空航天）的信息检索基准构建，这属于领域特定的应用，与您关注的通用推荐系统、搜索或广告核心进展无关。虽然涉及信息检索，但限定在航空航天领域，且框架性质（基准构建）更偏向评估而非核心算法或架构创新，因此相关性很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03479v1": {
    "title": "Efficient Sequential Recommendation for Long Term User Interest Via Personalization",
    "url": "https://www.alphaxiv.org/abs/2601.03479v1",
    "arxiv_id": "2601.03479v1",
    "authors": "Qiang Zhang, Hanchao Yu, Ivan Ji, Chen Yuan, Yi Zhang, Chihuang Liu, Xiaolong Wang, Christopher E. Lambert, Ren Chen, Chen Kovacs, Xinzhu Bei, Renqin Cai, Rui Li, Lizhu Zhang, Xiangjun Fan, Qunshu Zhang, Benyu Zhang",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2026-01-07 00:15:44",
    "ori_summary": "Recent years have witnessed success of sequential modeling, generative recommender, and large language model for recommendation. Though the scaling law has been validated for sequential models, it showed inefficiency in computational capacity when considering real-world applications like recommendation, due to the non-linear(quadratic) increasing nature of the transformer model. To improve the efficiency of the sequential model, we introduced a novel approach to sequential recommendation that leverages personalization techniques to enhance efficiency and performance. Our method compresses long user interaction histories into learnable tokens, which are then combined with recent interactions to generate recommendations. This approach significantly reduces computational costs while maintaining high recommendation accuracy. Our method could be applied to existing transformer based recommendation models, e.g., HSTU and HLLM. Extensive experiments on multiple sequential models demonstrate its versatility and effectiveness. Source code is available at \\href{https://github.com/facebookresearch/PerSRec}{https://github.com/facebookresearch/PerSRec}.",
    "summary": "主题：解决基于Transformer的顺序推荐模型在处理长用户交互历史时计算效率低下的问题。核心思想：引入个性化可学习令牌来压缩长序列历史，将其与近期交互结合进行推荐，从而降低计算成本。",
    "translation": "基于个性化建模的高效序列推荐：面向长期用户兴趣",
    "relevance_score": 9,
    "reasoning": "该论文直接聚焦于序列推荐这一核心推荐系统领域，特别是针对长期用户兴趣建模这一关键挑战。高效的序列推荐算法对提升推荐系统性能有直接应用价值，符合'核心领域进展'和'直接LLM应用'的关注点。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对顺序推荐中的Transformer效率问题，提出通过个性化可学习令牌压缩长序列的核心方法，高度契合效率提升和直接LLM应用两大焦点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03474v1": {
    "title": "SegNSP: Revisiting Next Sentence Prediction for Linear Text Segmentation",
    "url": "https://www.alphaxiv.org/abs/2601.03474v1",
    "arxiv_id": "2601.03474v1",
    "authors": "José Isidro, Filipe Cunha, Purificação Silvano, Alípio Jorge, Nuno Guimarães, Sérgio Nunes, Ricardo Campos",
    "categories": "cs.CL, cs.AI, cs.IR",
    "pub_date": "2026-01-07 00:02:30",
    "ori_summary": "Linear text segmentation is a long-standing problem in natural language processing (NLP), focused on dividing continuous text into coherent and semantically meaningful units. Despite its importance, the task remains challenging due to the complexity of defining topic boundaries, the variability in discourse structure, and the need to balance local coherence with global context. These difficulties hinder downstream applications such as summarization, information retrieval, and question answering. In this work, we introduce SegNSP, framing linear text segmentation as a next sentence prediction (NSP) task. Although NSP has largely been abandoned in modern pre-training, its explicit modeling of sentence-to-sentence continuity makes it a natural fit for detecting topic boundaries. We propose a label-agnostic NSP approach, which predicts whether the next sentence continues the current topic without requiring explicit topic labels, and enhance it with a segmentation-aware loss combined with harder negative sampling to better capture discourse continuity. Unlike recent proposals that leverage NSP alongside auxiliary topic classification, our approach avoids task-specific supervision. We evaluate our model against established baselines on two datasets, CitiLink-Minutes, for which we establish the first segmentation benchmark, and WikiSection. On CitiLink-Minutes, SegNSP achieves a B-$F_1$ of 0.79, closely aligning with human-annotated topic transitions, while on WikiSection it attains a B-F$_1$ of 0.65, outperforming the strongest reproducible baseline, TopSeg, by 0.17 absolute points. These results demonstrate competitive and robust performance, highlighting the effectiveness of modeling sentence-to-sentence continuity for improving segmentation quality and supporting downstream NLP applications.",
    "summary": "",
    "translation": "SegNSP：重新审视用于线性文本分割的下句预测",
    "relevance_score": 2,
    "reasoning": "该论文标题表明它专注于文本分割任务中的下句预测（NSP），这是NLP领域的一个具体技术问题。虽然NSP是BERT等Transformer模型的预训练任务之一，但论文标题没有显示出对Transformer架构效率、新注意力机制或MoE等核心架构进展的探索，也没有明确指向其在推荐系统、搜索或广告中的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04160v1": {
    "title": "All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection",
    "url": "https://www.alphaxiv.org/abs/2601.04160v1",
    "arxiv_id": "2601.04160v1",
    "authors": "Yuechen Jiang, Zhiwei Liu, Yupeng Cao, Yueru He, Ziyang Xu, Chen Xu, Zhiyang Deng, Prayag Tiwari, Xi Chen, Alejandro Lopez-Lira, Jimin Huang, Junichi Tsujii, Sophia Ananiadou",
    "categories": "cs.CL, cs.CE, q-fin.CP",
    "pub_date": "2026-01-07 18:18:28",
    "ori_summary": "We introduce RFC Bench, a benchmark for evaluating large language models on financial misinformation under realistic news. RFC Bench operates at the paragraph level and captures the contextual complexity of financial news where meaning emerges from dispersed cues. The benchmark defines two complementary tasks: reference free misinformation detection and comparison based diagnosis using paired original perturbed inputs. Experiments reveal a consistent pattern: performance is substantially stronger when comparative context is available, while reference free settings expose significant weaknesses, including unstable predictions and elevated invalid outputs. These results indicate that current models struggle to maintain coherent belief states without external grounding. By highlighting this gap, RFC Bench provides a structured testbed for studying reference free reasoning and advancing more reliable financial misinformation detection in real world settings.",
    "summary": "",
    "translation": "闪光的不都是金子：一种无参考反事实金融虚假信息检测基准",
    "relevance_score": 1,
    "reasoning": "该论文标题表明其关注金融领域的虚假信息检测基准，这属于特定领域应用（金融），而非推荐系统、搜索或广告的核心技术。论文内容可能涉及NLP评估基准，这与当前关注的LLM技术趋势、Transformer架构改进或直接应用于推荐/搜索/广告的LLM应用无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04157v1": {
    "title": "FLEx: Language Modeling with Few-shot Language Explanations",
    "url": "https://www.alphaxiv.org/abs/2601.04157v1",
    "arxiv_id": "2601.04157v1",
    "authors": "Adar Avsian, Christopher Richardson, Anirudh Sundar, Larry Heck",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2026-01-07 18:12:05",
    "ori_summary": "Language models have become effective at a wide range of tasks, from math problem solving to open-domain question answering. However, they still make mistakes, and these mistakes are often repeated across related queries. Natural language explanations can help correct these errors, but collecting them at scale may be infeasible, particularly in domains where expert annotators are required. To address this issue, we introduce FLEx ($\\textbf{F}$ew-shot $\\textbf{L}$anguage $\\textbf{Ex}$planations), a method for improving model behavior using a small number of explanatory examples. FLEx selects representative model errors using embedding-based clustering, verifies that the associated explanations correct those errors, and summarizes them into a prompt prefix that is prepended at inference-time. This summary guides the model to avoid similar errors on new inputs, without modifying model weights. We evaluate FLEx on CounterBench, GSM8K, and ReasonIF. We find that FLEx consistently outperforms chain-of-thought (CoT) prompting across all three datasets and reduces up to 83\\% of CoT's remaining errors.",
    "summary": "",
    "translation": "FLEx：基于少样本语言解释的语言建模",
    "relevance_score": 3,
    "reasoning": "该论文主要关注语言建模中的少样本解释技术，属于核心LLM技术范畴。虽然少样本学习在推荐/搜索系统中具有潜在应用价值（如冷启动、个性化解释），但论文标题未明确指向推荐/搜索/广告领域的应用场景，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04135v1": {
    "title": "LLMberjack: Guided Trimming of Debate Trees for Multi-Party Conversation Creation",
    "url": "https://www.alphaxiv.org/abs/2601.04135v1",
    "arxiv_id": "2601.04135v1",
    "authors": "Leonardo Bottona, Nicolò Penzo, Bruno Lepri, Marco Guerini, Sara Tonelli",
    "categories": "cs.CL, cs.HC",
    "pub_date": "2026-01-07 17:49:17",
    "ori_summary": "We present LLMberjack, a platform for creating multi-party conversations starting from existing debates, originally structured as reply trees. The system offers an interactive interface that visualizes discussion trees and enables users to construct coherent linearized dialogue sequences while preserving participant identity and discourse relations. It integrates optional large language model (LLM) assistance to support automatic editing of the messages and speakers' descriptions. We demonstrate the platform's utility by showing how tree visualization facilitates the creation of coherent, meaningful conversation threads and how LLM support enhances output quality while reducing human effort. The tool is open-source and designed to promote transparent and reproducible workflows to create multi-party conversations, addressing a lack of resources of this type.",
    "summary": "",
    "translation": "LLMberjack：用于多方对话生成的有指导辩论树修剪",
    "relevance_score": 3,
    "reasoning": "该论文涉及LLM在对话生成中的应用，属于直接LLM应用范畴，但多方对话创建与推荐系统、搜索或广告的核心任务（如排序、匹配、个性化）关联较弱。辩论树修剪技术可能对某些对话式推荐场景有间接启发，但缺乏明确的RecSys/Search/Ads应用指向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04131v1": {
    "title": "ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2601.04131v1",
    "arxiv_id": "2601.04131v1",
    "authors": "Nikhil Anand, Shwetha Somasundaram, Anirudh Phukan, Apoorv Saxena, Koyel Mukherjee",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2026-01-07 17:45:20",
    "ori_summary": "Large Language Models (LLMs) encode vast amounts of parametric knowledge during pre-training. As world knowledge evolves, effective deployment increasingly depends on their ability to faithfully follow externally retrieved context. When such evidence conflicts with the model's internal knowledge, LLMs often default to memorized facts, producing unfaithful outputs. In this work, we introduce ContextFocus, a lightweight activation steering approach that improves context faithfulness in such knowledge-conflict settings while preserving fluency and efficiency. Unlike prior approaches, our solution requires no model finetuning and incurs minimal inference-time overhead, making it highly efficient. We evaluate ContextFocus on the ConFiQA benchmark, comparing it against strong baselines including ContextDPO, COIECD, and prompting-based methods. Furthermore, we show that our method is complementary to prompting strategies and remains effective on larger models. Extensive experiments show that ContextFocus significantly improves contextual-faithfulness. Our results highlight the effectiveness, robustness, and efficiency of ContextFocus in improving contextual-faithfulness of LLM outputs.",
    "summary": "",
    "translation": "ContextFocus：大型语言模型中基于激活导向的上下文忠实性控制",
    "relevance_score": 2,
    "reasoning": "该论文关注LLM的激活导向技术以提高上下文忠实性，属于LLM内部机制研究。虽然可能间接影响推荐/搜索系统中LLM的可靠性，但论文本身未明确涉及这些应用领域，且更偏向NLP-centric的评估问题（如忠实性/幻觉），与当前关注的直接应用或使能技术关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04126v1": {
    "title": "InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training",
    "url": "https://www.alphaxiv.org/abs/2601.04126v1",
    "arxiv_id": "2601.04126v1",
    "authors": "Ziyun Zhang, Zezhou Wang, Xiaoyi Zhang, Zongyu Guo, Jiahao Li, Bin Li, Yan Lu",
    "categories": "cs.CL, cs.AI, cs.CV",
    "pub_date": "2026-01-07 17:40:08",
    "ori_summary": "GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.",
    "summary": "",
    "translation": "InfiniteWeb：用于GUI智能体训练的可扩展Web环境合成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注GUI智能体训练的环境合成技术，属于强化学习/智能体训练领域。虽然Web环境可能与搜索系统有一定关联，但论文标题未表明其直接应用于推荐系统、搜索或广告的核心排名任务，也未涉及LLM、Transformer架构或异构数据统一建模等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04098v1": {
    "title": "Layer-wise Positional Bias in Short-Context Language Modeling",
    "url": "https://www.alphaxiv.org/abs/2601.04098v1",
    "arxiv_id": "2601.04098v1",
    "authors": "Maryam Rahimi, Mahdi Nouri, Yadollah Yaghoobzadeh",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 17:04:30",
    "ori_summary": "Language models often show a preference for using information from specific positions in the input regardless of semantic relevance. While positional bias has been studied in various contexts, from attention sinks to task performance degradation in long-context settings, prior work has not established how these biases evolve across individual layers and input positions, or how they vary independent of task complexity. We introduce an attribution-based framework to analyze positional effects in short-context language modeling. Using layer conductance with a sliding-window approach, we quantify how each layer distributes importance across input positions, yielding layer-wise positional importance profiles. We find that these profiles are architecture-specific, stable across inputs, and invariant to lexical scrambling. Characterizing these profiles, we find prominent recency bias that increases with depth and subtle primacy bias that diminishes through model depth. Beyond positional structure, we also show that early layers preferentially weight content words over function words across all positions, while later layers lose this word-type differentiation.",
    "summary": "该论文研究语言模型中与语义无关的位置偏好问题，核心方法是提出基于归因的框架，通过层传导和滑动窗口量化各层对输入位置的重要性分布，发现位置重要性模式具有架构特异性、输入稳定性和词汇无关性，并揭示了随深度变化的近因偏好和首因偏好演化规律。",
    "translation": "短上下文语言建模中的层级位置偏置",
    "relevance_score": 7,
    "reasoning": "该论文研究语言模型中的位置偏置问题，属于'核心LLM技术进展'范畴，对Transformer架构中的注意力机制有直接影响。位置偏置研究对搜索和推荐系统中的序列建模有潜在应用价值，能够改进用户行为序列和上下文特征的处理。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文通过层间分析揭示了Transformer模型的位置偏好机制，直接关联Transformer架构效率与注意力机制优化，对推荐和搜索系统的序列建模有启发性。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.04093v1": {
    "title": "SearchAttack: Red-Teaming LLMs against Real-World Threats via Framing Unsafe Web Information-Seeking Tasks",
    "url": "https://www.alphaxiv.org/abs/2601.04093v1",
    "arxiv_id": "2601.04093v1",
    "authors": "Yu Yan, Sheng Sun, Mingfeng Li, Zheming Yang, Chiwei Zhu, Fei Ma, Benfeng Xu, Min Liu",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 16:59:34",
    "ori_summary": "Recently, people have suffered and become increasingly aware of the unreliability gap in LLMs for open and knowledge-intensive tasks, and thus turn to search-augmented LLMs to mitigate this issue. However, when the search engine is triggered for harmful tasks, the outcome is no longer under the LLM's control. Once the returned content directly contains targeted, ready-to-use harmful takeaways, the LLM's safeguards cannot withdraw that exposure. Motivated by this dilemma, we identify web search as a critical attack surface and propose \\textbf{\\textit{SearchAttack}} for red-teaming. SearchAttack outsources the harmful semantics to web search, retaining only the query's skeleton and fragmented clues, and further steers LLMs to reconstruct the retrieved content via structural rubrics to achieve malicious goals. Extensive experiments are conducted to red-team the search-augmented LLMs for responsible vulnerability assessment. Empirically, SearchAttack demonstrates strong effectiveness in attacking these systems.",
    "summary": "",
    "translation": "SearchAttack：通过构建不安全的网络信息寻求任务，对大型语言模型进行红队测试以应对现实世界威胁",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM的安全测试和红队攻击，属于安全/对抗性测试范畴，这在您的无关主题列表中明确排除。虽然涉及搜索/信息寻求任务，但核心焦点是安全威胁而非搜索系统本身的技术进步或LLM应用。对于'赋能技术'方面，该研究可能间接帮助构建更安全的搜索系统，但这并非其主要贡献或直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04086v1": {
    "title": "KDCM: Reducing Hallucination in LLMs through Explicit Reasoning Structures",
    "url": "https://www.alphaxiv.org/abs/2601.04086v1",
    "arxiv_id": "2601.04086v1",
    "authors": "Jinbo Hao, Kai Yang, Qingzhen Su, Yifan Li, Chao Jiang",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 16:54:20",
    "ori_summary": "To mitigate hallucinations in large language models (LLMs), we propose a framework that focuses on errors induced by prompts. Our method extends a chain-style knowledge distillation approach by incorporating a programmable module that guides knowledge graph exploration. This module is embedded as executable code within the reasoning prompt, allowing the model to leverage external structured knowledge during inference. Based on this design, we develop an enhanced distillation-based reasoning framework that explicitly regulates intermediate reasoning steps, resulting in more reliable predictions. We evaluate the proposed approach on multiple public benchmarks using GPT-4 and LLaMA-3.3. Experimental results show that code-guided reasoning significantly improves contextual modeling and reduces prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 increase by 15.64%, 13.38%, and 13.28%, respectively, with scores exceeding 95% across several evaluation settings. These findings indicate that the proposed method effectively constrains erroneous reasoning while improving both accuracy and interpretability.",
    "summary": "",
    "translation": "KDCM：通过显式推理结构减少大语言模型中的幻觉",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于减少LLM中的幻觉，这属于您指定的无关主题“幻觉、评估基准或其他纯NLP中心主题”。虽然幻觉减少技术可能间接影响推荐系统或搜索中的内容质量，但论文标题并未表明与RecSys/Search/Ads的直接应用或潜在应用，且核心关注点与您的焦点领域不匹配。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04073v1": {
    "title": "Analyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts",
    "url": "https://www.alphaxiv.org/abs/2601.04073v1",
    "arxiv_id": "2601.04073v1",
    "authors": "Zhihao Zhu, Jiafeng Liang, Shixin Jiang, Jinlan Fu, Ming Liu, Guanglu Sun, See-Kiong Ng, Bing Qin",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2026-01-07 16:39:34",
    "ori_summary": "Large Multimodal Models (LMMs) have demonstrated impressive capabilities in video reasoning via Chain-of-Thought (CoT). However, the robustness of their reasoning chains remains questionable. In this paper, we identify a critical failure mode termed textual inertia, where once a textual hallucination occurs in the thinking process, models tend to blindly adhere to the erroneous text while neglecting conflicting visual evidence. To systematically investigate this, we propose the LogicGraph Perturbation Protocol that structurally injects perturbations into the reasoning chains of diverse LMMs spanning both native reasoning architectures and prompt-driven paradigms to evaluate their self-reflection capabilities. The results reveal that models successfully self-correct in less than 10% of cases and predominantly succumb to blind textual error propagation. To mitigate this, we introduce Active Visual-Context Refinement, a training-free inference paradigm which orchestrates an active visual re-grounding mechanism to enforce fine-grained verification coupled with an adaptive context refinement strategy to summarize and denoise the reasoning history. Experiments demonstrate that our approach significantly stifles hallucination propagation and enhances reasoning robustness.",
    "summary": "",
    "translation": "跨模态冲突下大型多模态模型推理一致性分析",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态模型（特别是视觉-语言模型）的推理一致性评估，属于模型评估和基准测试范畴。虽然提到了多模态冲突，但核心是评估方法而非直接应用于推荐/搜索/广告系统的技术。对于您的关注点，它可能间接启发异构数据处理，但缺乏明确的RecSys/Search/Ads应用潜力说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04056v1": {
    "title": "Bridging the Discrete-Continuous Gap: Unified Multimodal Generation via Coupled Manifold Discrete Absorbing Diffusion",
    "url": "https://www.alphaxiv.org/abs/2601.04056v1",
    "arxiv_id": "2601.04056v1",
    "authors": "Yuanfeng Xu, Yuhao Chen, Liang Lin, Guangrun Wang",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 16:21:19",
    "ori_summary": "The bifurcation of generative modeling into autoregressive approaches for discrete data (text) and diffusion approaches for continuous data (images) hinders the development of truly unified multimodal systems. While Masked Language Models (MLMs) offer efficient bidirectional context, they traditionally lack the generative fidelity of autoregressive models and the semantic continuity of diffusion models. Furthermore, extending masked generation to multimodal settings introduces severe alignment challenges and training instability. In this work, we propose \\textbf{CoM-DAD} (\\textbf{Co}upled \\textbf{M}anifold \\textbf{D}iscrete \\textbf{A}bsorbing \\textbf{D}iffusion), a novel probabilistic framework that reformulates multimodal generation as a hierarchical dual-process. CoM-DAD decouples high-level semantic planning from low-level token synthesis. First, we model the semantic manifold via a continuous latent diffusion process; second, we treat token generation as a discrete absorbing diffusion process, regulated by a \\textbf{Variable-Rate Noise Schedule}, conditioned on these evolving semantic priors. Crucially, we introduce a \\textbf{Stochastic Mixed-Modal Transport} strategy that aligns disparate modalities without requiring heavy contrastive dual-encoders. Our method demonstrates superior stability over standard masked modeling, establishing a new paradigm for scalable, unified text-image generation.",
    "summary": "",
    "translation": "弥合离散-连续鸿沟：通过耦合流形离散吸收扩散实现统一多模态生成",
    "relevance_score": 3,
    "reasoning": "该论文标题涉及多模态生成和扩散模型技术，属于LLM/Transformer的使能技术范畴，可能通过改进生成模型在推荐或搜索中用于内容理解或生成。然而，标题未明确指向推荐/搜索/广告应用，且多模态生成更接近AIGC领域，与当前关注点的直接关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04055v1": {
    "title": "Modular Prompt Optimization: Optimizing Structured Prompts with Section-Local Textual Gradients",
    "url": "https://www.alphaxiv.org/abs/2601.04055v1",
    "arxiv_id": "2601.04055v1",
    "authors": "Prith Sharma, Austin Z. Henley",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 16:20:08",
    "ori_summary": "Prompt quality plays a central role in controlling the behavior, reliability, and reasoning performance of large language models (LLMs), particularly for smaller open-source instruction-tuned models that depend heavily on explicit structure. While recent work has explored automatic prompt optimization using textual gradients and self-refinement, most existing methods treat prompts as monolithic blocks of text, making it difficult to localize errors, preserve critical instructions, or prevent uncontrolled prompt growth. We introduce Modular Prompt Optimization (MPO), a schema-based prompt optimization framework that treats prompts as structured objects composed of fixed semantic sections, including system role, context, task description, constraints, and output format. MPO applies section-local textual gradients, generated by a critic language model, to refine each section independently while keeping the overall prompt schema fixed. Section updates are consolidated through de-duplication to reduce redundancy and interference between components, yielding an interpretable and robust optimization process. We evaluate MPO on two reasoning benchmarks, ARC-Challenge and MMLU, using LLaMA-3 8B-Instruct and Mistral-7B-Instruct as solver models. Across both benchmarks and models, MPO consistently outperforms an untuned structured prompt and the TextGrad baseline, achieving substantial accuracy gains without modifying model parameters or altering prompt structure. These results demonstrate that maintaining a fixed prompt schema while applying localized, section-wise optimization is an effective and practical approach for improving reasoning performance in small open-source LMs.",
    "summary": "论文研究如何优化大型语言模型的提示质量以提升推理性能。其核心方法是提出模块化提示优化框架，将提示视为由固定语义模块组成的结构化对象，并应用基于批评模型的局部文本梯度对每个模块进行独立优化，同时保持整体提示结构不变。",
    "translation": "模块化提示优化：利用局部文本梯度优化结构化提示",
    "relevance_score": 6,
    "reasoning": "该论文涉及提示优化技术，属于LLM应用方法，可能应用于搜索或推荐系统中的查询优化、提示工程等场景。虽然直接应用性有限，但优化提示的方法可提升LLM在相关任务中的性能。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出模块化提示优化框架，直接针对LLM提示工程的核心技术，属于LLM在应用层面的方法创新，与用户焦点中的“直接LLM应用”和“使能LLM技术”高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.04052v1": {
    "title": "Stable Language Guidance for Vision-Language-Action Models",
    "url": "https://www.alphaxiv.org/abs/2601.04052v1",
    "arxiv_id": "2601.04052v1",
    "authors": "Zhihao Zhan, Yuhao Chen, Jiaying Zhou, Qinhan Lv, Hao Liu, Keze Wang, Liang Lin, Guangrun Wang",
    "categories": "cs.RO, cs.CL",
    "pub_date": "2026-01-07 16:16:10",
    "ori_summary": "Vision-Language-Action (VLA) models have demonstrated impressive capabilities in generalized robotic control; however, they remain notoriously brittle to linguistic perturbations. We identify a critical ``modality collapse'' phenomenon where strong visual priors overwhelm sparse linguistic signals, causing agents to overfit to specific instruction phrasings while ignoring the underlying semantic intent. To address this, we propose \\textbf{Residual Semantic Steering (RSS)}, a probabilistic framework that disentangles physical affordance from semantic execution. RSS introduces two theoretical innovations: (1) \\textbf{Monte Carlo Syntactic Integration}, which approximates the true semantic posterior via dense, LLM-driven distributional expansion, and (2) \\textbf{Residual Affordance Steering}, a dual-stream decoding mechanism that explicitly isolates the causal influence of language by subtracting the visual affordance prior. Theoretical analysis suggests that RSS effectively maximizes the mutual information between action and intent while suppressing visual distractors. Empirical results across diverse manipulation benchmarks demonstrate that RSS achieves state-of-the-art robustness, maintaining performance even under adversarial linguistic perturbations.",
    "summary": "",
    "translation": "视觉-语言-动作模型的稳定语言引导",
    "relevance_score": 3,
    "reasoning": "该论文涉及视觉-语言-动作模型，属于多模态领域，与您关注的VLM异质数据建模有一定概念关联，但主要针对动作控制而非推荐/搜索/广告。虽然语言引导技术可能启发推荐系统的指令跟随，但论文焦点在机器人/控制应用，与您明确排除的领域特定应用重叠，直接相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04043v1": {
    "title": "When Helpers Become Hazards: A Benchmark for Analyzing Multimodal LLM-Powered Safety in Daily Life",
    "url": "https://www.alphaxiv.org/abs/2601.04043v1",
    "arxiv_id": "2601.04043v1",
    "authors": "Xinyue Lou, Jinan Xu, Jingyi Yin, Xiaolong Wang, Zhaolu Kang, Youwei Liao, Yixuan Wang, Xiangyu Shi, Fengran Mo, Su Yao, Kaiyu Huang",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 15:59:07",
    "ori_summary": "As Multimodal Large Language Models (MLLMs) become an indispensable assistant in human life, the unsafe content generated by MLLMs poses a danger to human behavior, perpetually overhanging human society like a sword of Damocles. To investigate and evaluate the safety impact of MLLMs responses on human behavior in daily life, we introduce SaLAD, a multimodal safety benchmark which contains 2,013 real-world image-text samples across 10 common categories, with a balanced design covering both unsafe scenarios and cases of oversensitivity. It emphasizes realistic risk exposure, authentic visual inputs, and fine-grained cross-modal reasoning, ensuring that safety risks cannot be inferred from text alone. We further propose a safety-warning-based evaluation framework that encourages models to provide clear and informative safety warnings, rather than generic refusals. Results on 18 MLLMs demonstrate that the top-performing models achieve a safe response rate of only 57.2% on unsafe queries. Moreover, even popular safety alignment methods limit effectiveness of the models in our scenario, revealing the vulnerabilities of current MLLMs in identifying dangerous behaviors in daily life. Our dataset is available at https://github.com/xinyuelou/SaLAD.",
    "summary": "",
    "translation": "当助手变为隐患：分析多模态大语言模型在日常生活中的安全性基准",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于多模态LLM的安全性评估基准，属于安全性和伦理范畴，这在您的无关主题列表中明确排除。虽然涉及多模态LLM，但核心关注点是安全评估而非技术进展或应用，与您的所有关注领域（核心领域进展、使能技术、直接应用等）均无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04036v1": {
    "title": "Analyzing and Improving Cross-lingual Knowledge Transfer for Machine Translation",
    "url": "https://www.alphaxiv.org/abs/2601.04036v1",
    "arxiv_id": "2601.04036v1",
    "authors": "David Stap",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 15:51:54",
    "ori_summary": "Multilingual machine translation systems aim to make knowledge accessible across languages, yet learning effective cross-lingual representations remains challenging. These challenges are especially pronounced for low-resource languages, where limited parallel data constrains generalization and transfer. Understanding how multilingual models share knowledge across languages requires examining the interaction between representations, data availability, and training strategies. In this thesis, we study cross-lingual knowledge transfer in neural models and develop methods to improve robustness and generalization in multilingual settings, using machine translation as a central testbed. We analyze how similarity between languages influences transfer, how retrieval and auxiliary supervision can strengthen low-resource translation, and how fine-tuning on parallel data can introduce unintended trade-offs in large language models. We further examine the role of language diversity during training and show that increasing translation coverage improves generalization and reduces off-target behavior. Together, this work highlights how modeling choices and data composition shape multilingual learning and offers insights toward more inclusive and resilient multilingual NLP systems.",
    "summary": "",
    "translation": "分析与改进机器翻译中的跨语言知识迁移",
    "relevance_score": 2,
    "reasoning": "该论文主要关注机器翻译领域的跨语言知识迁移，属于NLP特定应用，与推荐系统、搜索或广告的核心关注点没有直接关联。虽然知识迁移技术可能具有通用性，但论文标题明确限定在机器翻译领域，没有表明对RecSys/Search/Ads的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04029v1": {
    "title": "SpeakerSleuth: Evaluating Large Audio-Language Models as Judges for Multi-turn Speaker Consistency",
    "url": "https://www.alphaxiv.org/abs/2601.04029v1",
    "arxiv_id": "2601.04029v1",
    "authors": "Jonggeun Lee, Junseong Pyo, Gyuhyeon Seo, Yohan Jo",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 15:45:41",
    "ori_summary": "Large Audio-Language Models (LALMs) as judges have emerged as a prominent approach for evaluating speech generation quality, yet their ability to assess speaker consistency across multi-turn conversations remains unexplored. We present SpeakerSleuth, a benchmark evaluating whether LALMs can reliably judge speaker consistency in multi-turn dialogues through three tasks reflecting real-world requirements. We construct 1,818 human-verified evaluation instances across four diverse datasets spanning synthetic and real speech, with controlled acoustic difficulty. Evaluating nine widely-used LALMs, we find that models struggle to reliably detect acoustic inconsistencies. For instance, given audio samples of the same speaker's turns, some models overpredict inconsistency, whereas others are overly lenient. Models further struggle to identify the exact turns that are problematic. When other interlocutors' turns are provided together, performance degrades dramatically as models prioritize textual coherence over acoustic cues, failing to detect even obvious gender switches for a speaker. On the other hand, models perform substantially better in choosing the audio that best matches the speaker among several acoustic variants, demonstrating inherent acoustic discrimination capabilities. These findings expose a significant bias in LALMs: they tend to prioritize text over acoustics, revealing fundamental modality imbalances that need to be addressed to build reliable audio-language judges.",
    "summary": "",
    "translation": "SpeakerSleuth：评估大型音频-语言模型作为多轮对话说话人一致性评判者的能力",
    "relevance_score": 1,
    "reasoning": "该论文专注于音频-语言模型在说话人一致性评估方面的能力，属于语音处理领域。虽然涉及多轮对话，但核心是语音模态的评估，与推荐系统、搜索或广告的技术焦点无关，也没有展示在异构数据处理方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04025v1": {
    "title": "Simulated Students in Tutoring Dialogues: Substance or Illusion?",
    "url": "https://www.alphaxiv.org/abs/2601.04025v1",
    "arxiv_id": "2601.04025v1",
    "authors": "Alexander Scarlatos, Jaewook Lee, Simon Woodhead, Andrew Lan",
    "categories": "cs.CL, cs.CY",
    "pub_date": "2026-01-07 15:44:11",
    "ori_summary": "Advances in large language models (LLMs) enable many new innovations in education. However, evaluating the effectiveness of new technology requires real students, which is time-consuming and hard to scale up. Therefore, many recent works on LLM-powered tutoring solutions have used simulated students for both training and evaluation, often via simple prompting. Surprisingly, little work has been done to ensure or even measure the quality of simulated students. In this work, we formally define the student simulation task, propose a set of evaluation metrics that span linguistic, behavioral, and cognitive aspects, and benchmark a wide range of student simulation methods on these metrics. We experiment on a real-world math tutoring dialogue dataset, where both automated and human evaluation results show that prompting strategies for student simulation perform poorly; supervised fine-tuning and preference optimization yield much better but still limited performance, motivating future work on this challenging task.",
    "summary": "",
    "translation": "辅导对话中的模拟学生：实质还是幻觉？",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及教育领域的对话系统（辅导对话），属于特定领域应用（教育技术），与我的关注点（推荐系统、搜索、广告）无关。标题中的“模拟学生”可能涉及对话生成或模拟，但这属于教育或通用对话系统范畴，没有明确指向推荐、搜索或广告领域的应用或技术进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03997v1": {
    "title": "VotIE: Information Extraction from Meeting Minutes",
    "url": "https://www.alphaxiv.org/abs/2601.03997v1",
    "arxiv_id": "2601.03997v1",
    "authors": "José Pedro Evans, Luís Filipe Cunha, Purificação Silvano, Alípio Jorge, Nuno Guimarães, Sérgio Nunes, Ricardo Campos",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 15:06:53",
    "ori_summary": "Municipal meeting minutes record key decisions in local democratic processes. Unlike parliamentary proceedings, which typically adhere to standardized formats, they encode voting outcomes in highly heterogeneous, free-form narrative text that varies widely across municipalities, posing significant challenges for automated extraction. In this paper, we introduce VotIE (Voting Information Extraction), a new information extraction task aimed at identifying structured voting events in narrative deliberative records, and establish the first benchmark for this task using Portuguese municipal minutes, building on the recently introduced CitiLink corpus. Our experiments yield two key findings. First, under standard in-domain evaluation, fine-tuned encoders, specifically XLM-R-CRF, achieve the strongest performance, reaching 93.2\\% macro F1, outperforming generative approaches. Second, in a cross-municipality setting that evaluates transfer to unseen administrative contexts, these models suffer substantial performance degradation, whereas few-shot LLMs demonstrate greater robustness, with significantly smaller declines in performance. Despite this generalization advantage, the high computational cost of generative models currently constrains their practicality. As a result, lightweight fine-tuned encoders remain a more practical option for large-scale, real-world deployment. To support reproducible research in administrative NLP, we publicly release our benchmark, trained models, and evaluation framework.",
    "summary": "",
    "translation": "VotIE：会议纪要信息抽取",
    "relevance_score": 1,
    "reasoning": "该论文专注于会议纪要的信息抽取，属于特定领域的信息提取任务，与推荐系统、搜索或广告的核心进展、LLM技术应用、Transformer架构改进或异构数据统一建模均无直接关联。该主题更接近文档处理或特定领域NLP应用，不在当前关注的任何技术范畴内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03986v1": {
    "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks",
    "url": "https://www.alphaxiv.org/abs/2601.03986v1",
    "arxiv_id": "2601.03986v1",
    "authors": "Qi Qian, Chengsong Huang, Jingwen Xu, Changze Lv, Muling Wu, Wenhao Liu, Xiaohua Wang, Zhenghua Wang, Zisu Huang, Muzhao Tian, Jianhan Xu, Kun Hu, He-Da Wang, Yao Hu, Xuanjing Huang, Xiaoqing Zheng",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 14:59:03",
    "ori_summary": "The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.",
    "summary": "",
    "translation": "基准测试的基准测试：对大型语言模型基准的系统性评估",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于LLM基准测试的评估，这属于纯粹的NLP评估基准主题，与您当前关注的推荐系统、搜索、广告领域的技术进展、LLM应用或Transformer架构改进等核心方向完全无关。根据您的排除标准，此类'Evaluation benchmarks'属于明确的不相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03981v1": {
    "title": "RADAR: Retrieval-Augmented Detector with Adversarial Refinement for Robust Fake News Detection",
    "url": "https://www.alphaxiv.org/abs/2601.03981v1",
    "arxiv_id": "2601.03981v1",
    "authors": "Song-Duo Ma, Yi-Hung Liu, Hsin-Yu Lin, Pin-Yu Chen, Hong-Yan Huang, Shau-Yung Hsu, Yun-Nung Chen",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 14:52:15",
    "ori_summary": "To efficiently combat the spread of LLM-generated misinformation, we present RADAR, a retrieval-augmented detector with adversarial refinement for robust fake news detection. Our approach employs a generator that rewrites real articles with factual perturbations, paired with a lightweight detector that verifies claims using dense passage retrieval. To enable effective co-evolution, we introduce verbal adversarial feedback (VAF). Rather than relying on scalar rewards, VAF issues structured natural-language critiques; these guide the generator toward more sophisticated evasion attempts, compelling the detector to adapt and improve. On a fake news detection benchmark, RADAR achieves 86.98% ROC-AUC, significantly outperforming general-purpose LLMs with retrieval. Ablation studies confirm that detector-side retrieval yields the largest gains, while VAF and few-shot demonstrations provide critical signals for robust training.",
    "summary": "",
    "translation": "RADAR：基于检索增强与对抗性精化的鲁棒虚假新闻检测器",
    "relevance_score": 2,
    "reasoning": "该论文主要关注虚假新闻检测，属于内容安全/可信度评估领域，而非推荐系统、搜索或广告的核心排序/匹配任务。虽然检索增强技术（RAG）在搜索中有应用潜力，但论文标题明确指向“虚假新闻检测”这一具体应用场景，与您关注的RecSys/Search/Ads核心进展（如排序模型、用户行为建模、广告点击率预测等）直接相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03979v1": {
    "title": "SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems",
    "url": "https://www.alphaxiv.org/abs/2601.03979v1",
    "arxiv_id": "2601.03979v1",
    "authors": "Andreea-Elena Bodea, Stephen Meisenbacher, Alexandra Klymenko, Florian Matthes",
    "categories": "cs.CR, cs.CL",
    "pub_date": "2026-01-07 14:50:41",
    "ori_summary": "The continued promise of Large Language Models (LLMs), particularly in their natural language understanding and generation capabilities, has driven a rapidly increasing interest in identifying and developing LLM use cases. In an effort to complement the ingrained \"knowledge\" of LLMs, Retrieval-Augmented Generation (RAG) techniques have become widely popular. At its core, RAG involves the coupling of LLMs with domain-specific knowledge bases, whereby the generation of a response to a user question is augmented with contextual and up-to-date information. The proliferation of RAG has sparked concerns about data privacy, particularly with the inherent risks that arise when leveraging databases with potentially sensitive information. Numerous recent works have explored various aspects of privacy risks in RAG systems, from adversarial attacks to proposed mitigations. With the goal of surveying and unifying these works, we ask one simple question: What are the privacy risks in RAG, and how can they be measured and mitigated? To answer this question, we conduct a systematic literature review of RAG works addressing privacy, and we systematize our findings into a comprehensive set of privacy risks, mitigation techniques, and evaluation strategies. We supplement these findings with two primary artifacts: a Taxonomy of RAG Privacy Risks and a RAG Privacy Process Diagram. Our work contributes to the study of privacy in RAG not only by conducting the first systematization of risks and mitigations, but also by uncovering important considerations when mitigating privacy risks in RAG systems and assessing the current maturity of proposed mitigations.",
    "summary": "",
    "translation": "SoK：检索增强生成系统中的隐私风险与缓解措施",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于隐私风险与缓解措施，这属于明确的无关主题（Irrelevant Topics）中的“Privacy”范畴。虽然检索增强生成（RAG）系统可能与搜索相关，但论文的核心关注点是隐私保护而非技术架构、模型改进或直接应用，因此与当前关注的RecSys/Search/Ads技术进展无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03973v1": {
    "title": "Muse: Towards Reproducible Long-Form Song Generation with Fine-Grained Style Control",
    "url": "https://www.alphaxiv.org/abs/2601.03973v1",
    "arxiv_id": "2601.03973v1",
    "authors": "Changhao Jiang, Jiahao Chen, Zhenghao Xiang, Zhixiong Yang, Hanchen Wang, Jiabao Zhuang, Xinmeng Che, Jiajun Sun, Hui Li, Yifei Cao, Shihan Dou, Ming Zhang, Junjie Ye, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang",
    "categories": "cs.SD, cs.CL",
    "pub_date": "2026-01-07 14:40:48",
    "ori_summary": "Recent commercial systems such as Suno demonstrate strong capabilities in long-form song generation, while academic research remains largely non-reproducible due to the lack of publicly available training data, hindering fair comparison and progress. To this end, we release a fully open-source system for long-form song generation with fine-grained style conditioning, including a licensed synthetic dataset, training and evaluation pipelines, and Muse, an easy-to-deploy song generation model. The dataset consists of 116k fully licensed synthetic songs with automatically generated lyrics and style descriptions paired with audio synthesized by SunoV5. We train Muse via single-stage supervised finetuning of a Qwen-based language model extended with discrete audio tokens using MuCodec, without task-specific losses, auxiliary objectives, or additional architectural components. Our evaluations find that although Muse is trained with a modest data scale and model size, it achieves competitive performance on phoneme error rate, text--music style similarity, and audio aesthetic quality, while enabling controllable segment-level generation across different musical structures. All data, model weights, and training and evaluation pipelines will be publicly released, paving the way for continued progress in controllable long-form song generation research. The project repository is available at https://github.com/yuhui1038/Muse.",
    "summary": "",
    "translation": "Muse：面向可复现的长篇幅歌曲生成与细粒度风格控制",
    "relevance_score": 1,
    "reasoning": "该论文专注于音乐生成这一特定内容创作领域，属于纯粹的AIGC/内容生成范畴。虽然涉及生成模型，但音乐生成与推荐系统、搜索或广告中的排名、匹配、用户建模等核心问题没有直接关联，也不涉及Transformer架构改进或异构数据处理等使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03969v1": {
    "title": "Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models",
    "url": "https://www.alphaxiv.org/abs/2601.03969v1",
    "arxiv_id": "2601.03969v1",
    "authors": "Wei Wu, Liyi Chen, Congxi Xiao, Tianfu Wang, Qimeng Wang, Chengqiang Lu, Yan Gao, Yi Wu, Yao Hu, Hui Xiong",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2026-01-07 14:31:07",
    "ori_summary": "Large reasoning models enhanced by reinforcement learning with verifiable rewards have achieved significant performance gains by extending their chain-of-thought. However, this paradigm incurs substantial deployment costs as models often exhibit excessive verbosity on simple queries. Existing efficient reasoning methods relying on explicit length penalties often introduce optimization conflicts and leave the generative mechanisms driving overthinking largely unexamined. In this paper, we identify a phenomenon termed length shift where models increasingly generate unnecessary reasoning on trivial inputs during training. To address this, we introduce Dynamic Outlier Truncation (DOT), a training-time intervention that selectively suppresses redundant tokens. This method targets only the extreme tail of response lengths within fully correct rollout groups while preserving long-horizon reasoning capabilities for complex problems. To complement this intervention and ensure stable convergence, we further incorporate auxiliary KL regularization and predictive dynamic sampling. Experimental results across multiple model scales demonstrate that our approach significantly pushes the efficiency-performance Pareto frontier outward. Notably, on the AIME-24, our method reduces inference token usage by 78% while simultaneously increasing accuracy compared to the initial policy and surpassing state-of-the-art efficient reasoning methods.",
    "summary": "该论文研究推理模型在训练中因简单查询产生冗余推理（长度偏移）导致的部署效率问题。核心方法是提出动态异常值截断，在训练时选择性抑制完全正确但过长的响应中的极端尾部冗余标记，同时保留复杂问题的长程推理能力。",
    "translation": "抗长度偏移：动态异常截断用于训练高效推理模型",
    "relevance_score": 8,
    "reasoning": "该论文涉及训练效率优化，属于Transformer架构效率提升的范畴（Enabling Transformer Tech）。动态异常截断技术可应用于处理推荐/搜索系统中用户行为序列的长度变化，提高模型训练效率，对大规模序列建模有直接价值。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文针对推理模型训练中的长度偏移问题提出动态截断方法，直接优化模型效率，与推荐/搜索系统对响应长度和计算效率的关注高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03940v1": {
    "title": "Large-Scale Aspect-Based Sentiment Analysis with Reasoning-Infused LLMs",
    "url": "https://www.alphaxiv.org/abs/2601.03940v1",
    "arxiv_id": "2601.03940v1",
    "authors": "Paweł Liskowski, Krzysztof Jankowski",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 13:58:29",
    "ori_summary": "We introduce Arctic-ABSA, a collection of powerful models for real-life aspect-based sentiment analysis (ABSA). Our models are tailored to commercial needs, trained on a large corpus of public data alongside carefully generated synthetic data, resulting in a dataset 20 times larger than SemEval14. We extend typical ABSA models by expanding the number of sentiment classes from the standard three (positive, negative, neutral) to five, adding mixed and unknown classes, while also jointly predicting overall text sentiment and supporting multiple languages. We experiment with reasoning injection by fine-tuning on Chain-of-Thought (CoT) examples and introduce a novel reasoning pretraining technique for encoder-only models that significantly improves downstream fine-tuning and generalization. Our 395M-parameter encoder and 8B-parameter decoder achieve up to 10 percentage points higher accuracy than GPT-4o and Claude 3.5 Sonnet, while setting new state-of-the-art results on the SemEval14 benchmark. A single multilingual model maintains 87-91% accuracy across six languages without degrading English performance. We release ABSA-mix, a large-scale benchmark aggregating 17 public ABSA datasets across 92 domains.",
    "summary": "",
    "translation": "基于推理增强型大语言模型的大规模方面级情感分析",
    "relevance_score": 3,
    "reasoning": "该论文虽然涉及LLM技术，但主要聚焦于情感分析这一特定NLP任务，属于纯粹的NLP应用领域。虽然LLM推理能力可能间接应用于推荐系统中的用户评论理解，但论文标题明确指向情感分析而非RecSys/Search/Ads的直接应用，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03938v1": {
    "title": "FOREVER: Forgetting Curve-Inspired Memory Replay for Language Model Continual Learning",
    "url": "https://www.alphaxiv.org/abs/2601.03938v1",
    "arxiv_id": "2601.03938v1",
    "authors": "Yujie Feng, Hao Wang, Jian Li, Xu Chu, Zhaolu Kang, Yiran Liu, Yasha Wang, Philip S. Yu, Xiao-Ming Wu",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2026-01-07 13:55:14",
    "ori_summary": "Continual learning (CL) for large language models (LLMs) aims to enable sequential knowledge acquisition without catastrophic forgetting. Memory replay methods are widely used for their practicality and effectiveness, but most rely on fixed, step-based heuristics that often misalign with the model's actual learning progress, since identical training steps can result in varying degrees of parameter change. Motivated by recent findings that LLM forgetting mirrors the Ebbinghaus human forgetting curve, we propose FOREVER (FORgEtting curVe-inspired mEmory Replay), a novel CL framework that aligns replay schedules with a model-centric notion of time. FOREVER defines model time using the magnitude of optimizer updates, allowing forgetting curve-inspired replay intervals to align with the model's internal evolution rather than raw training steps. Building on this approach, FOREVER incorporates a forgetting curve-based replay scheduler to determine when to replay and an intensity-aware regularization mechanism to adaptively control how to replay. Extensive experiments on three CL benchmarks and models ranging from 0.6B to 13B parameters demonstrate that FOREVER consistently mitigates catastrophic forgetting.",
    "summary": "该论文研究大型语言模型持续学习中的灾难性遗忘问题，核心思想是借鉴人类遗忘曲线理论，通过基于优化器更新幅度定义的“模型时间”来动态调整记忆回放时机，使回放间隔与模型内部演化过程对齐。",
    "translation": "FOREVER：基于遗忘曲线的记忆回放用于语言模型持续学习",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能LLM技术'范畴，研究语言模型的持续学习机制，这对推荐/搜索/广告系统至关重要，因为这些系统需要持续适应新用户行为、商品和趋势。基于遗忘曲线的记忆回放技术可以优化LLM在动态环境中的知识更新，防止灾难性遗忘，直接提升推荐模型对用户兴趣演变的跟踪能力。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出基于遗忘曲线的记忆回放框架，直接针对LLM持续学习中的灾难性遗忘问题，属于LLM核心技术进步，对推荐/搜索系统中的模型持续更新有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03928v1": {
    "title": "FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection",
    "url": "https://www.alphaxiv.org/abs/2601.03928v1",
    "arxiv_id": "2601.03928v1",
    "authors": "Mingyu Ouyang, Kevin Qinghong Lin, Mike Zheng Shou, Hwee Tou Ng",
    "categories": "cs.CV, cs.AI, cs.CL, cs.HC",
    "pub_date": "2026-01-07 13:48:12",
    "ori_summary": "Vision-Language Models (VLMs) have shown remarkable performance in User Interface (UI) grounding tasks, driven by their ability to process increasingly high-resolution screenshots. However, screenshots are tokenized into thousands of visual tokens (e.g., about 4700 for 2K resolution), incurring significant computational overhead and diluting attention. In contrast, humans typically focus on regions of interest when interacting with UI. In this work, we pioneer the task of efficient UI grounding. Guided by practical analysis of the task's characteristics and challenges, we propose FocusUI, an efficient UI grounding framework that selects patches most relevant to the instruction while preserving positional continuity for precise grounding. FocusUI addresses two key challenges: (1) Eliminating redundant tokens in visual encoding. We construct patch-level supervision by fusing an instruction-conditioned score with a rule-based UI-graph score that down-weights large homogeneous regions to select distinct and instruction-relevant visual tokens. (2) Preserving positional continuity during visual token selection. We find that general visual token pruning methods suffer from severe accuracy degradation on UI grounding tasks due to broken positional information. We introduce a novel PosPad strategy, which compresses each contiguous sequence of dropped visual tokens into a single special marker placed at the sequence's last index to preserve positional continuity. Comprehensive experiments on four grounding benchmarks demonstrate that FocusUI surpasses GUI-specific baselines. On the ScreenSpot-Pro benchmark, FocusUI-7B achieves a performance improvement of 3.7% over GUI-Actor-7B. Even with only 30% visual token retention, FocusUI-7B drops by only 3.2% while achieving up to 1.44x faster inference and 17% lower peak GPU memory.",
    "summary": "",
    "translation": "FocusUI：通过位置保持的视觉标记选择实现高效的UI界面定位",
    "relevance_score": 2,
    "reasoning": "该论文主要关注UI界面中的视觉元素定位技术，属于计算机视觉与用户界面交互的交叉领域。虽然视觉定位技术在某些搜索或推荐场景中可能有间接应用（如移动端UI理解），但论文标题未明确表明与推荐系统、搜索或广告的核心技术直接相关，也未提及LLM、Transformer架构或异构数据处理等当前关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03926v1": {
    "title": "Doc-PP: Document Policy Preservation Benchmark for Large Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2601.03926v1",
    "arxiv_id": "2601.03926v1",
    "authors": "Haeun Jang, Hwan Chang, Hwanhee Lee",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 13:45:39",
    "ori_summary": "The deployment of Large Vision-Language Models (LVLMs) for real-world document question answering is often constrained by dynamic, user-defined policies that dictate information disclosure based on context. While ensuring adherence to these explicit constraints is critical, existing safety research primarily focuses on implicit social norms or text-only settings, overlooking the complexities of multimodal documents. In this paper, we introduce Doc-PP (Document Policy Preservation Benchmark), a novel benchmark constructed from real-world reports requiring reasoning across heterogeneous visual and textual elements under strict non-disclosure policies. Our evaluation highlights a systemic Reasoning-Induced Safety Gap: models frequently leak sensitive information when answers must be inferred through complex synthesis or aggregated across modalities, effectively circumventing existing safety constraints. Furthermore, we identify that providing extracted text improves perception but inadvertently facilitates leakage. To address these vulnerabilities, we propose DVA (Decompose-Verify-Aggregation), a structural inference framework that decouples reasoning from policy verification. Experimental results demonstrate that DVA significantly outperforms standard prompting defenses, offering a robust baseline for policy-compliant document understanding",
    "summary": "",
    "translation": "Doc-PP：面向大型视觉语言模型的文档策略保持基准",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于视觉语言模型的评估基准（Document Policy Preservation Benchmark），属于纯粹的评估基准研究。虽然涉及视觉语言模型，但论文关注的是文档策略保持这一特定评估任务，与推荐系统、搜索或广告中的异构数据统一建模、LLM应用或Transformer架构进展等核心关注点没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03914v1": {
    "title": "When Models Decide and When They Bind: A Two-Stage Computation for Multiple-Choice Question-Answering",
    "url": "https://www.alphaxiv.org/abs/2601.03914v1",
    "arxiv_id": "2601.03914v1",
    "authors": "Hugh Mee Wong, Rick Nouwen, Albert Gatt",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 13:27:48",
    "ori_summary": "Multiple-choice question answering (MCQA) is easy to evaluate but adds a meta-task: models must both solve the problem and output the symbol that *represents* the answer, conflating reasoning errors with symbol-binding failures. We study how language models implement MCQA internally using representational analyses (PCA, linear probes) as well as causal interventions. We find that option-boundary (newline) residual states often contain strong linearly decodable signals related to per-option correctness. Winner-identity probing reveals a two-stage progression: the winning *content position* becomes decodable immediately after the final option is processed, while the *output symbol* is represented closer to the answer emission position. Tests under symbol and content permutations support a two-stage mechanism in which models first select a winner in content space and then bind or route that winner to the appropriate symbol to emit.",
    "summary": "",
    "translation": "模型何时决策与何时约束：一种用于多项选择题问答的两阶段计算框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多项选择题问答中的两阶段计算框架，属于NLP问答任务的具体方法研究。虽然涉及模型决策机制，但缺乏明确的RecSys/Search/Ads应用场景，且未提及Transformer架构改进或LLM在推荐/搜索/广告中的直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03908v1": {
    "title": "Decide Then Retrieve: A Training-Free Framework with Uncertainty-Guided Triggering and Dual-Path Retrieval",
    "url": "https://www.alphaxiv.org/abs/2601.03908v1",
    "arxiv_id": "2601.03908v1",
    "authors": "Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Deguo Xia, Jizhou Huang",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 13:20:59",
    "ori_summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, but existing approaches indiscriminately trigger retrieval and rely on single-path evidence construction, often introducing noise and limiting performance gains. In this work, we propose Decide Then Retrieve (DTR), a training-free framework that adaptively determines when retrieval is necessary and how external information should be selected. DTR leverages generation uncertainty to guide retrieval triggering and introduces a dual-path retrieval mechanism with adaptive information selection to better handle sparse and ambiguous queries. Extensive experiments across five open-domain QA benchmarks, multiple model scales, and different retrievers demonstrate that DTR consistently improves EM and F1 over standard RAG and strong retrieval-enhanced baselines, while reducing unnecessary retrievals. The code and data used in this paper are available at https://github.com/ChenWangHKU/DTR.",
    "summary": "该论文研究检索增强生成（RAG）中检索触发时机和证据构建的优化问题。其核心方法是提出一个无需训练的框架，利用生成不确定性自适应触发检索，并采用双路径检索机制进行自适应信息选择，以更好地处理稀疏和模糊查询。",
    "translation": "先决策后检索：一种免训练框架，基于不确定性引导的触发机制与双路径检索",
    "relevance_score": 8,
    "reasoning": "该论文标题涉及检索增强生成（RAG）框架，属于LLM在搜索/推荐系统中的直接应用。不确定性引导触发机制可用于优化检索时机，双路径检索可提升召回质量，这些技术可直接应用于搜索和推荐系统的检索阶段，提高系统效率和准确性。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出了一种无需训练的检索增强生成框架，通过不确定性引导触发和双路径检索机制，直接优化LLM在检索增强场景下的应用，与“直接LLM应用”和“核心领域进展”高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03905v1": {
    "title": "Current Agents Fail to Leverage World Model as Tool for Foresight",
    "url": "https://www.alphaxiv.org/abs/2601.03905v1",
    "arxiv_id": "2601.03905v1",
    "authors": "Cheng Qian, Emre Can Acikgoz, Bingxuan Li, Xiusi Chen, Yuji Zhang, Bingxiang He, Qinyu Luo, Dilek Hakkani-Tür, Gokhan Tur, Yunzhu Li, Heng Ji, Heng Ji",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2026-01-07 13:15:23",
    "ori_summary": "Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.",
    "summary": "",
    "translation": "当前智能体未能将世界模型作为前瞻性工具加以利用",
    "relevance_score": 2,
    "reasoning": "该论文标题聚焦于智能体与世界模型的前瞻能力问题，属于强化学习或智能体研究的范畴。虽然世界模型概念在强化学习中常见，但标题未明确涉及推荐系统、搜索或广告领域的应用，也未提及LLM、Transformer架构或异质数据建模等当前关注的核心技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03895v1": {
    "title": "Adaptive-Boundary-Clipping GRPO: Ensuring Bounded Ratios for Stable and Generalizable Training",
    "url": "https://www.alphaxiv.org/abs/2601.03895v1",
    "arxiv_id": "2601.03895v1",
    "authors": "Chi Liu, Xin Chen",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2026-01-07 13:04:52",
    "ori_summary": "Group Relative Policy Optimization (GRPO) has emerged as a popular algorithm for reinforcement learning with large language models (LLMs). However, upon analyzing its clipping mechanism, we argue that it is suboptimal in certain scenarios. With appropriate modifications, GRPO can be significantly enhanced to improve both flexibility and generalization. To this end, we propose Adaptive-Boundary-Clipping GRPO (ABC-GRPO), an asymmetric and adaptive refinement of the original GRPO framework. We demonstrate that ABC-GRPO achieves superior performance over standard GRPO on mathematical reasoning tasks using the Qwen3 LLMs. Moreover, ABC-GRPO maintains substantially higher entropy throughout training, thereby preserving the model's exploration capacity and mitigating premature convergence. The implementation code is available online to ease reproducibility https://github.com/chi2liu/ABC-GRPO.",
    "summary": "",
    "translation": "自适应边界裁剪GRPO：确保有界比率以实现稳定且可泛化的训练",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及强化学习中的策略优化方法（GRPO可能指某种策略优化算法），主要关注训练稳定性和泛化性。虽然强化学习在推荐/搜索系统中可能有应用，但标题没有明确指向这些领域，也没有涉及LLM、Transformer架构或异构数据建模等当前关注的核心技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03874v1": {
    "title": "Evaluating Small Decoder-Only Language Models for Grammar Correction and Text Simplification",
    "url": "https://www.alphaxiv.org/abs/2601.03874v1",
    "arxiv_id": "2601.03874v1",
    "authors": "Anthony Lamelas",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 12:39:31",
    "ori_summary": "Large language models have become extremely popular recently due to their ability to achieve strong performance on a variety of tasks, such as text generation and rewriting, but their size and computation cost make them difficult to access, deploy, and secure in many settings. This paper investigates whether small, decoder-only language models can provide an efficient alternative for the tasks of grammar correction and text simplification. The experiments in this paper focus on testing small language models out of the box, fine-tuned, and run sequentially on the JFLEG and ASSET datasets using established metrics. The results show that while SLMs may learn certain behaviors well, their performance remains below strong baselines and current LLMs. The results also show that SLMs struggle with retaining meaning and hallucinations. These findings suggest that despite their efficiency advantages, current SLMs are not yet competitive enough with modern LLMs for rewriting, and further advances in training are required for SLMs to close the performance gap between them and today's LLMs.",
    "summary": "",
    "translation": "评估小型仅解码器语言模型在语法纠错和文本简化任务中的性能",
    "relevance_score": 2,
    "reasoning": "该论文主要关注小型语言模型在语法纠错和文本简化任务上的评估，这属于纯粹的NLP评估基准范畴，与推荐系统、搜索或广告的核心技术进展无关。虽然涉及语言模型，但论文重点在于特定NLP任务的评估方法，而非能够应用于推荐/搜索/广告领域的底层技术或架构创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03872v1": {
    "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning",
    "url": "https://www.alphaxiv.org/abs/2601.03872v1",
    "arxiv_id": "2601.03872v1",
    "authors": "Jinyang Wu, Guocheng Zhai, Ruihan Jin, Jiahao Yuan, Yuhao Shen, Shuai Zhang, Zhengqi Wen, Jianhua Tao",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 12:38:33",
    "ori_summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) \\textbf{training-free cluster-based routing} that exploits empirical priors for domain-specific alignment, and (2) \\textbf{RL-based multi-step routing} that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.",
    "summary": "",
    "translation": "Atlas：面向多领域复杂推理的异构模型与工具编排系统",
    "relevance_score": 3,
    "reasoning": "该论文标题涉及异构模型编排和多领域推理，可能涉及模型集成或系统架构，与“异构数据的VLM类比”有一定间接关联，但未明确提及推荐、搜索或广告应用。对于“赋能技术”方面，其潜在应用可能在于构建更强大的推荐系统，通过整合多种模型来处理复杂的用户行为和上下文特征，但标题本身未直接体现这些具体应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03868v1": {
    "title": "What Matters For Safety Alignment?",
    "url": "https://www.alphaxiv.org/abs/2601.03868v1",
    "arxiv_id": "2601.03868v1",
    "authors": "Xing Li, Hui-Ling Zhen, Lihao Yin, Xianzhi Yu, Zhenhua Dong, Mingxuan Yuan",
    "categories": "cs.CL, cs.AI, cs.CR",
    "pub_date": "2026-01-07 12:31:52",
    "ori_summary": "This paper presents a comprehensive empirical study on the safety alignment capabilities. We evaluate what matters for safety alignment in LLMs and LRMs to provide essential insights for developing more secure and reliable AI systems. We systematically investigate and compare the influence of six critical intrinsic model characteristics and three external attack techniques. Our large-scale evaluation is conducted using 32 recent, popular LLMs and LRMs across thirteen distinct model families, spanning a parameter scale from 3B to 235B. The assessment leverages five established safety datasets and probes model vulnerabilities with 56 jailbreak techniques and four CoT attack strategies, resulting in 4.6M API calls. Our key empirical findings are fourfold. First, we identify the LRMs GPT-OSS-20B, Qwen3-Next-80B-A3B-Thinking, and GPT-OSS-120B as the top-three safest models, which substantiates the significant advantage of integrated reasoning and self-reflection mechanisms for robust safety alignment. Second, post-training and knowledge distillation may lead to a systematic degradation of safety alignment. We thus argue that safety must be treated as an explicit constraint or a core optimization objective during these stages, not merely subordinated to the pursuit of general capability. Third, we reveal a pronounced vulnerability: employing a CoT attack via a response prefix can elevate the attack success rate by 3.34x on average and from 0.6% to 96.3% for Seed-OSS-36B-Instruct. This critical finding underscores the safety risks inherent in text-completion interfaces and features that allow user-defined response prefixes in LLM services, highlighting an urgent need for architectural and deployment safeguards. Fourth, roleplay, prompt injection, and gradient-based search for adversarial prompts are the predominant methodologies for eliciting unaligned behaviors in modern models.",
    "summary": "",
    "translation": "安全对齐的关键因素是什么？",
    "relevance_score": 1,
    "reasoning": "该标题关注的是LLM安全对齐问题，属于伦理/安全范畴，这在您的无关主题列表中明确排除。虽然安全对齐是LLM领域的重要话题，但它没有直接涉及推荐系统、搜索或广告的核心技术、架构改进或应用创新，也没有展示在您关注的领域中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03860v1": {
    "title": "PartisanLens: A Multilingual Dataset of Hyperpartisan and Conspiratorial Immigration Narratives in European Media",
    "url": "https://www.alphaxiv.org/abs/2601.03860v1",
    "arxiv_id": "2601.03860v1",
    "authors": "Michele Joshua Maggini, Paloma Piot, Anxo Pérez, Erik Bran Marino, Lúa Santamaría Montesinos, Ana Lisboa, Marta Vázquez Abuín, Javier Parapar, Pablo Gamallo",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 12:18:14",
    "ori_summary": "Detecting hyperpartisan narratives and Population Replacement Conspiracy Theories (PRCT) is essential to addressing the spread of misinformation. These complex narratives pose a significant threat, as hyperpartisanship drives political polarisation and institutional distrust, while PRCTs directly motivate real-world extremist violence, making their identification critical for social cohesion and public safety. However, existing resources are scarce, predominantly English-centric, and often analyse hyperpartisanship, stance, and rhetorical bias in isolation rather than as interrelated aspects of political discourse. To bridge this gap, we introduce \\textsc{PartisanLens}, the first multilingual dataset of \\num{1617} hyperpartisan news headlines in Spanish, Italian, and Portuguese, annotated in multiple political discourse aspects. We first evaluate the classification performance of widely used Large Language Models (LLMs) on this dataset, establishing robust baselines for the classification of hyperpartisan and PRCT narratives. In addition, we assess the viability of using LLMs as automatic annotators for this task, analysing their ability to approximate human annotation. Results highlight both their potential and current limitations. Next, moving beyond standard judgments, we explore whether LLMs can emulate human annotation patterns by conditioning them on socio-economic and ideological profiles that simulate annotator perspectives. At last, we provide our resources and evaluation, \\textsc{PartisanLens} supports future research on detecting partisan and conspiratorial narratives in European contexts.",
    "summary": "",
    "translation": "PartisanLens：欧洲媒体中关于移民的党派偏见与阴谋论叙事的多语言数据集",
    "relevance_score": 1,
    "reasoning": "该论文标题表明这是一个关于媒体叙事和党派偏见的数据集，属于社会科学或政治传播领域。它不涉及推荐系统、搜索或广告的核心技术进展，也不涉及LLM、Transformer架构或异构数据建模等使能技术。该主题与用户当前关注的技术焦点完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03858v1": {
    "title": "What Does Loss Optimization Actually Teach, If Anything? Knowledge Dynamics in Continual Pre-training of LLMs",
    "url": "https://www.alphaxiv.org/abs/2601.03858v1",
    "arxiv_id": "2601.03858v1",
    "authors": "Seyed Mahed Mousavi, Simone Alghisi, Giuseppe Riccardi",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 12:14:33",
    "ori_summary": "Continual Pre-Training (CPT) is widely used for acquiring and updating factual knowledge in LLMs. This practice treats loss as a proxy for knowledge learning, while offering no grounding into how it changes during training. We study CPT as a knowledge learning process rather than a solely optimization problem. We construct a controlled, distribution-matched benchmark of factual documents and interleave diagnostic probes directly into the CPT loop, enabling epoch-level measurement of knowledge acquisition dynamics and changes in Out-Of-Domain (OOD) general skills (e.g., math). We further analyze how CPT reshapes knowledge circuits during training. Across three instruction-tuned LLMs and multiple CPT strategies, optimization and learning systematically diverge as loss decreases monotonically while factual learning is unstable and non-monotonic. Acquired facts are rarely consolidated, learning is strongly conditioned on prior exposure, and OOD performance degrades from early epochs. Circuit analysis reveals rapid reconfiguration of knowledge pathways across epochs, providing an explanation for narrow acquisition windows and systematic forgetting. These results show that loss optimization is misaligned with learning progress in CPT and motivate evaluation of stopping criteria based on task-level learning dynamics.",
    "summary": "",
    "translation": "损失优化究竟教会了什么，如果有的话？大语言模型持续预训练中的知识动态",
    "relevance_score": 3,
    "reasoning": "该论文探讨LLM持续预训练中的知识动态和损失优化，属于核心LLM技术进展范畴。虽然持续预训练技术本身可能对RecSys/Search/Ads有潜在应用价值（如模型持续适应新数据分布），但论文标题未明确指向这些领域的具体应用，且可能更偏向理论分析而非明确的实用技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03851v1": {
    "title": "Rethinking Table Pruning in TableQA: From Sequential Revisions to Gold Trajectory-Supervised Parallel Search",
    "url": "https://www.alphaxiv.org/abs/2601.03851v1",
    "arxiv_id": "2601.03851v1",
    "authors": "Yu Guo, Shenghao Ye, Shuangwu Chen, Zijian Wen, Tao Zhang, Qirui Bai, Dong Jin, Yunpeng Hou, Huasen He, Jian Yang, Xiaobin Tan",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 12:08:59",
    "ori_summary": "Table Question Answering (TableQA) benefits significantly from table pruning, which extracts compact sub-tables by eliminating redundant cells to streamline downstream reasoning. However, existing pruning methods typically rely on sequential revisions driven by unreliable critique signals, often failing to detect the loss of answer-critical data. To address this limitation, we propose TabTrim, a novel table pruning framework which transforms table pruning from sequential revisions to gold trajectory-supervised parallel search. TabTrim derives a gold pruning trajectory using the intermediate sub-tables in the execution process of gold SQL queries, and trains a pruner and a verifier to make the step-wise pruning result align with the gold pruning trajectory. During inference, TabTrim performs parallel search to explore multiple candidate pruning trajectories and identify the optimal sub-table. Extensive experiments demonstrate that TabTrim achieves state-of-the-art performance across diverse tabular reasoning tasks: TabTrim-8B reaches 73.5% average accuracy, outperforming the strongest baseline by 3.2%, including 79.4% on WikiTQ and 61.2% on TableBench.",
    "summary": "",
    "translation": "重新思考TableQA中的表格剪枝：从顺序修订到基于黄金轨迹监督的并行搜索",
    "relevance_score": 2,
    "reasoning": "该论文标题聚焦于表格问答（TableQA）中的表格剪枝技术，属于特定NLP任务优化，与推荐系统、搜索或广告的核心领域进展、LLM技术应用或Transformer架构改进无直接关联。虽然表格数据处理在理论上可能涉及信息检索，但论文未明确展示其在RecSys/Search/Ads中的实际应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03823v1": {
    "title": "Step Potential Advantage Estimation: Harnessing Intermediate Confidence and Correctness for Efficient Mathematical Reasoning",
    "url": "https://www.alphaxiv.org/abs/2601.03823v1",
    "arxiv_id": "2601.03823v1",
    "authors": "Fei Wu, Zhenrong Zhang, Qikai Chang, Jianshu Zhang, Quan Liu, Jun Du",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 11:36:01",
    "ori_summary": "Reinforcement Learning with Verifiable Rewards (RLVR) elicits long chain-of-thought reasoning in large language models (LLMs), but outcome-based rewards lead to coarse-grained advantage estimation. While existing approaches improve RLVR via token-level entropy or sequence-level length control, they lack a semantically grounded, step-level measure of reasoning progress. As a result, LLMs fail to distinguish necessary deduction from redundant verification: they may continue checking after reaching a correct solution and, in extreme cases, overturn a correct trajectory into an incorrect final answer. To remedy the lack of process supervision, we introduce a training-free probing mechanism that extracts intermediate confidence and correctness and combines them into a Step Potential signal that explicitly estimates the reasoning state at each step. Building on this signal, we propose Step Potential Advantage Estimation (SPAE), a fine-grained credit assignment method that amplifies potential gains, penalizes potential drops, and applies penalty after potential saturates to encourage timely termination. Experiments across multiple benchmarks show SPAE consistently improves accuracy while substantially reducing response length, outperforming strong RL baselines and recent efficient reasoning and token-level advantage estimation methods. The code is available at https://github.com/cii030/SPAE-RL.",
    "summary": "",
    "translation": "步骤潜在优势估计：利用中间置信度与正确性实现高效数学推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注数学推理中的效率优化方法，属于特定领域的推理技术。虽然涉及置信度评估，但未明确展示在推荐系统、搜索或广告领域的潜在应用价值，与当前关注的LLM技术应用、Transformer架构进展或异构数据统一建模等方向关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03812v1": {
    "title": "AI Generated Text Detection",
    "url": "https://www.alphaxiv.org/abs/2601.03812v1",
    "arxiv_id": "2601.03812v1",
    "authors": "Adilkhan Alikhanov, Aidar Amangeldi, Diar Demeubay, Dilnaz Akhmetzhan, Nurbek Moldakhmetov, Omar Polat, Galymzhan Zharas",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 11:18:10",
    "ori_summary": "The rapid development of large language models has led to an increase in AI-generated text, with students increasingly using LLM-generated content as their own work, which violates academic integrity. This paper presents an evaluation of AI text detection methods, including both traditional machine learning models and transformer-based architectures. We utilize two datasets, HC3 and DAIGT v2, to build a unified benchmark and apply a topic-based data split to prevent information leakage. This approach ensures robust generalization across unseen domains. Our experiments show that TF-IDF logistic regression achieves a reasonable baseline accuracy of 82.87%. However, deep learning models outperform it. The BiLSTM classifier achieves an accuracy of 88.86%, while DistilBERT achieves a similar accuracy of 88.11% with the highest ROC-AUC score of 0.96, demonstrating the strongest overall performance. The results indicate that contextual semantic modeling is significantly superior to lexical features and highlight the importance of mitigating topic memorization through appropriate evaluation protocols. The limitations of this work are primarily related to dataset diversity and computational constraints. In future work, we plan to expand dataset diversity and utilize parameter-efficient fine-tuning methods such as LoRA. We also plan to explore smaller or distilled models and employ more efficient batching strategies and hardware-aware optimization.",
    "summary": "",
    "translation": "AI生成文本检测",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及AI生成文本的检测，这属于纯粹的NLP评估基准或检测技术，与推荐系统、搜索或广告的核心技术进展无关。虽然检测技术可能在内容安全方面有应用，但论文本身并未直接涉及推荐、搜索或广告的建模、架构或应用创新，因此与当前关注点高度不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03798v1": {
    "title": "Where meaning lives: Layer-wise accessibility of psycholinguistic features in encoder and decoder language models",
    "url": "https://www.alphaxiv.org/abs/2601.03798v1",
    "arxiv_id": "2601.03798v1",
    "authors": "Taisiia Tikhomirova, Dirk U. Wulff",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 10:55:04",
    "ori_summary": "Understanding where transformer language models encode psychologically meaningful aspects of meaning is essential for both theory and practice. We conduct a systematic layer-wise probing study of 58 psycholinguistic features across 10 transformer models, spanning encoder-only and decoder-only architectures, and compare three embedding extraction methods. We find that apparent localization of meaning is strongly method-dependent: contextualized embeddings yield higher feature-specific selectivity and different layer-wise profiles than isolated embeddings. Across models and methods, final-layer representations are rarely optimal for recovering psycholinguistic information with linear probes. Despite these differences, models exhibit a shared depth ordering of meaning dimensions, with lexical properties peaking earlier and experiential and affective dimensions peaking later. Together, these results show that where meaning \"lives\" in transformer models reflects an interaction between methodological choices and architectural constraints.",
    "summary": "",
    "translation": "意义栖居何处：编码器与解码器语言模型中心理语言学特征的逐层可访问性",
    "relevance_score": 2,
    "reasoning": "该论文主要研究语言模型内部表示与心理语言学特征的关系，属于NLP基础研究范畴。虽然涉及语言模型分析，但未明确展示在推荐系统、搜索或广告领域的潜在应用价值，与当前关注的直接应用或赋能技术关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03792v1": {
    "title": "VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for Vietnamese Traditional Medicine Evaluation",
    "url": "https://www.alphaxiv.org/abs/2601.03792v1",
    "arxiv_id": "2601.03792v1",
    "authors": "Huynh Trung Kiet, Dao Sy Duy Minh, Nguyen Dinh Ha Duong, Le Hoang Minh Huy, Long Nguyen, Dien Dinh",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 10:49:56",
    "ori_summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in general medical domains. However, their performance significantly degrades in specialized, culturally specific domains such as Vietnamese Traditional Medicine (VTM), primarily due to the scarcity of high-quality, structured benchmarks. In this paper, we introduce VietMed-MCQ, a novel multiple-choice question dataset generated via a Retrieval-Augmented Generation (RAG) pipeline with an automated consistency check mechanism. Unlike previous synthetic datasets, our framework incorporates a dual-model validation approach to ensure reasoning consistency through independent answer verification, though the substring-based evidence checking has known limitations. The complete dataset of 3,190 questions spans three difficulty levels and underwent validation by one medical expert and four students, achieving 94.2 percent approval with substantial inter-rater agreement (Fleiss' kappa = 0.82). We benchmark seven open-source models on VietMed-MCQ. Results reveal that general-purpose models with strong Chinese priors outperform Vietnamese-centric models, highlighting cross-lingual conceptual transfer, while all models still struggle with complex diagnostic reasoning. Our code and dataset are publicly available to foster research in low-resource medical domains.",
    "summary": "",
    "translation": "VietMed-MCQ：一种用于越南传统医学评估的基于一致性过滤的数据合成框架",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向越南传统医学评估这一特定医学领域应用，属于明确的医学/生物学领域特定应用。标题中提到的数据合成框架可能涉及技术方法，但其核心应用领域（传统医学评估）与搜索、推荐、广告系统完全无关，且属于明确列出的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03791v1": {
    "title": "Do LLMs Really Memorize Personally Identifiable Information? Revisiting PII Leakage with a Cue-Controlled Memorization Framework",
    "url": "https://www.alphaxiv.org/abs/2601.03791v1",
    "arxiv_id": "2601.03791v1",
    "authors": "Xiaoyu Luo, Yiyi Chen, Qiongxiu Li, Johannes Bjerva",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 10:49:36",
    "ori_summary": "Large Language Models (LLMs) have been reported to \"leak\" Personally Identifiable Information (PII), with successful PII reconstruction often interpreted as evidence of memorization. We propose a principled revision of memorization evaluation for LLMs, arguing that PII leakage should be evaluated under low lexical cue conditions, where target PII cannot be reconstructed through prompt-induced generalization or pattern completion. We formalize Cue-Resistant Memorization (CRM) as a cue-controlled evaluation framework and a necessary condition for valid memorization evaluation, explicitly conditioning on prompt-target overlap cues. Using CRM, we conduct a large-scale multilingual re-evaluation of PII leakage across 32 languages and multiple memorization paradigms. Revisiting reconstruction-based settings, including verbatim prefix-suffix completion and associative reconstruction, we find that their apparent effectiveness is driven primarily by direct surface-form cues rather than by true memorization. When such cues are controlled for, reconstruction success diminishes substantially. We further examine cue-free generation and membership inference, both of which exhibit extremely low true positive rates. Overall, our results suggest that previously reported PII leakage is better explained by cue-driven behavior than by genuine memorization, highlighting the importance of cue-controlled evaluation for reliably quantifying privacy-relevant memorization in LLMs.",
    "summary": "",
    "translation": "大语言模型真的会记忆个人身份信息吗？基于线索控制记忆框架重新审视PII泄露问题",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于LLMs中的隐私泄露和记忆问题，这属于明确的无关主题（隐私、安全）。虽然涉及LLMs技术，但研究的是隐私保护而非技术应用，对推荐系统、搜索或广告的核心技术发展没有直接贡献。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03790v1": {
    "title": "NeoAMT: Neologism-Aware Agentic Machine Translation with Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2601.03790v1",
    "arxiv_id": "2601.03790v1",
    "authors": "Zhongtao Miao, Kaiyan Zhao, Masaaki Nagata, Yoshimasa Tsuruoka",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 10:49:00",
    "ori_summary": "Neologism-aware machine translation aims to translate source sentences containing neologisms into target languages. This field remains underexplored compared with general machine translation (MT). In this paper, we propose an agentic framework, NeoAMT, for neologism-aware machine translation using a Wiktionary search tool. Specifically, we first create a new dataset for neologism-aware machine translation and develop a search tool based on Wiktionary. The new dataset covers 16 languages and 75 translation directions and is derived from approximately 10 million records of an English Wiktionary dump. The retrieval corpus of the search tool is also constructed from around 3 million cleaned records of the Wiktionary dump. We then use it for training the translation agent with reinforcement learning (RL) and evaluating the accuracy of neologism-aware machine translation. Based on this, we also propose an RL training framework that contains a novel reward design and an adaptive rollout generation approach by leveraging \"translation difficulty\" to further improve the translation quality of translation agents using our search tool.",
    "summary": "",
    "translation": "NeoAMT：基于强化学习的、具备新词感知能力的智能机器翻译代理",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及机器翻译、强化学习和代理智能，这些主题与推荐系统、搜索或广告的核心进展、LLM技术应用或Transformer架构改进均无直接关联。强化学习在此语境下是用于机器翻译而非推荐/搜索/广告场景，且未提及任何潜在应用可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03786v1": {
    "title": "Compact Example-Based Explanations for Language Models",
    "url": "https://www.alphaxiv.org/abs/2601.03786v1",
    "arxiv_id": "2601.03786v1",
    "authors": "Loris Schoenegger, Benjamin Roth",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2026-01-07 10:36:46",
    "ori_summary": "Training data influence estimation methods quantify the contribution of training documents to a model's output, making them a promising source of information for example-based explanations. As humans cannot interpret thousands of documents, only a small subset of the training data can be presented as an explanation. Although the choice of which documents to include directly affects explanation quality, previous evaluations of such systems have largely ignored any selection strategies. To address this, we propose a novel selection relevance score, a retraining-free metric that quantifies how useful a set of examples is for explaining a model's output. We validate this score through fine-tuning experiments, confirming that it can predict whether a set of examples supports or undermines the model's predictions. Using this metric, we further show that common selection strategies often underperform random selection. Motivated by this finding, we propose a strategy that balances influence and representativeness, enabling better use of selection budgets than naively selecting the highest-ranking examples.",
    "summary": "",
    "translation": "语言模型的紧凑示例式解释",
    "relevance_score": 3,
    "reasoning": "该论文涉及语言模型的解释性方法，属于LLM技术范畴，但未明确说明其在推荐系统、搜索或广告领域的直接应用潜力。示例式解释技术可能有助于提升推荐系统的可解释性，但标题本身未体现这种具体应用方向，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03785v1": {
    "title": "Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents",
    "url": "https://www.alphaxiv.org/abs/2601.03785v1",
    "arxiv_id": "2601.03785v1",
    "authors": "Dehao Tao, Guoliang Ma, Yongfeng Huang, Minghu Jiang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 10:36:29",
    "ori_summary": "Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent \"memory boxes\" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.",
    "summary": "",
    "translation": "Membox：将主题连续性编织进LLM智能体的长程记忆中",
    "relevance_score": 4,
    "reasoning": "该论文涉及LLM智能体的长程记忆机制，属于核心LLM技术进展，可能通过改进记忆管理来增强LLM在推荐/搜索中的上下文理解能力。然而，标题未明确说明其在推荐系统、搜索或广告中的具体应用，相关性较为间接。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2601.03783v1": {
    "title": "HearSay Benchmark: Do Audio LLMs Leak What They Hear?",
    "url": "https://www.alphaxiv.org/abs/2601.03783v1",
    "arxiv_id": "2601.03783v1",
    "authors": "Jin Wang, Liang Lin, Kaiwen Luo, Weiliu Wang, Yitian Chen, Moayad Aloqaily, Xuehai Tang, Zhenhong Zhou, Kun Wang, Li Sun, Qingsong Wen",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 10:33:44",
    "ori_summary": "While Audio Large Language Models (ALLMs) have achieved remarkable progress in understanding and generation, their potential privacy implications remain largely unexplored. This paper takes the first step to investigate whether ALLMs inadvertently leak user privacy solely through acoustic voiceprints and introduces $\\textit{HearSay}$, a comprehensive benchmark constructed from over 22,000 real-world audio clips. To ensure data quality, the benchmark is meticulously curated through a rigorous pipeline involving automated profiling and human verification, guaranteeing that all privacy labels are grounded in factual records. Extensive experiments on $\\textit{HearSay}$ yield three critical findings: $\\textbf{Significant Privacy Leakage}$: ALLMs inherently extract private attributes from voiceprints, reaching 92.89% accuracy on gender and effectively profiling social attributes. $\\textbf{Insufficient Safety Mechanisms}$: Alarmingly, existing safeguards are severely inadequate; most models fail to refuse privacy-intruding requests, exhibiting near-zero refusal rates for physiological traits. $\\textbf{Reasoning Amplifies Risk}$: Chain-of-Thought (CoT) reasoning exacerbates privacy risks in capable models by uncovering deeper acoustic correlations. These findings expose critical vulnerabilities in ALLMs, underscoring the urgent need for targeted privacy alignment. The codes and dataset are available at https://github.com/JinWang79/HearSay_Benchmark",
    "summary": "",
    "translation": "HearSay基准测试：音频大语言模型会泄露它们听到的内容吗？",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及音频大语言模型的安全性和隐私泄露问题，这属于明确的无关主题（安全、隐私）。虽然提到了LLMs，但核心关注点是音频模态的隐私风险，而非LLM在推荐系统、搜索或广告中的技术应用、架构改进或直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03779v1": {
    "title": "Tracing the complexity profiles of different linguistic phenomena through the intrinsic dimension of LLM representations",
    "url": "https://www.alphaxiv.org/abs/2601.03779v1",
    "arxiv_id": "2601.03779v1",
    "authors": "Marco Baroni, Emily Cheng, Iria deDios-Flores, Francesca Franzon",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 10:16:59",
    "ori_summary": "We explore the intrinsic dimension (ID) of LLM representations as a marker of linguistic complexity, asking if different ID profiles across LLM layers differentially characterize formal and functional complexity. We find the formal contrast between sentences with multiple coordinated or subordinated clauses to be reflected in ID differences whose onset aligns with a phase of more abstract linguistic processing independently identified in earlier work. The functional contrasts between sentences characterized by right branching vs. center embedding or unambiguous vs. ambiguous relative clause attachment are also picked up by ID, but in a less marked way, and they do not correlate with the same processing phase. Further experiments using representational similarity and layer ablation confirm the same trends. We conclude that ID is a useful marker of linguistic complexity in LLMs, that it allows to differentiate between different types of complexity, and that it points to similar stages of linguistic processing across disparate LLMs.",
    "summary": "",
    "translation": "通过大语言模型表示的内在维度追踪不同语言现象的复杂度分布",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLM表示的内在维度与语言现象复杂度的关系，属于LLM表示分析的基础研究。虽然涉及LLM技术，但论文聚焦于语言现象分析这一NLP中心话题，没有明确讨论在推荐系统、搜索或广告领域的潜在应用，与当前关注的直接应用或赋能技术方向关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03775v1": {
    "title": "Do LLM Self-Explanations Help Users Predict Model Behavior? Evaluating Counterfactual Simulatability with Pragmatic Perturbations",
    "url": "https://www.alphaxiv.org/abs/2601.03775v1",
    "arxiv_id": "2601.03775v1",
    "authors": "Pingjun Hong, Benjamin Roth",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 10:13:26",
    "ori_summary": "Large Language Models (LLMs) can produce verbalized self-explanations, yet prior studies suggest that such rationales may not reliably reflect the model's true decision process. We ask whether these explanations nevertheless help users predict model behavior, operationalized as counterfactual simulatability. Using StrategyQA, we evaluate how well humans and LLM judges can predict a model's answers to counterfactual follow-up questions, with and without access to the model's chain-of-thought or post-hoc explanations. We compare LLM-generated counterfactuals with pragmatics-based perturbations as alternative ways to construct test cases for assessing the potential usefulness of explanations. Our results show that self-explanations consistently improve simulation accuracy for both LLM judges and humans, but the degree and stability of gains depend strongly on the perturbation strategy and judge strength. We also conduct a qualitative analysis of free-text justifications written by human users when predicting the model's behavior, which provides evidence that access to explanations helps humans form more accurate predictions on the perturbed questions.",
    "summary": "",
    "translation": "LLM自我解释是否有助于用户预测模型行为？基于实用扰动的反事实可模拟性评估",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLM自我解释对用户理解模型行为的影响，属于模型可解释性和用户交互评估范畴。虽然涉及LLM技术，但其核心关注点（模型行为预测、反事实模拟）与推荐系统、搜索或广告中的核心进展、架构创新或直接应用关联较弱，更偏向通用NLP评估而非特定领域的技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03752v1": {
    "title": "Evaluation of Multilingual LLMs Personalized Text Generation Capabilities Targeting Groups and Social-Media Platforms",
    "url": "https://www.alphaxiv.org/abs/2601.03752v1",
    "arxiv_id": "2601.03752v1",
    "authors": "Dominik Macko",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 09:43:13",
    "ori_summary": "Capabilities of large language models to generate multilingual coherent text have continuously enhanced in recent years, which opens concerns about their potential misuse. Previous research has shown that they can be misused for generation of personalized disinformation in multiple languages. It has also been observed that personalization negatively affects detectability of machine-generated texts; however, this has been studied in the English language only. In this work, we examine this phenomenon across 10 languages, while we focus not only on potential misuse of personalization capabilities, but also on potential benefits they offer. Overall, we cover 1080 combinations of various personalization aspects in the prompts, for which the texts are generated by 16 distinct language models (17,280 texts in total). Our results indicate that there are differences in personalization quality of the generated texts when targeting demographic groups and when targeting social-media platforms across languages. Personalization towards platforms affects detectability of the generated texts in a higher scale, especially in English, where the personalization quality is the highest.",
    "summary": "",
    "translation": "面向群体与社交媒体平台的多语言大语言模型个性化文本生成能力评估",
    "relevance_score": 3,
    "reasoning": "该论文标题主要关注多语言LLM的个性化文本生成能力评估，属于纯粹的LLM评估范畴，与您关注的RecSys/Search/Ads核心进展、Transformer架构创新或LLM直接应用关联较弱。虽然提及社交媒体平台可能涉及用户交互，但核心焦点是文本生成能力评估而非推荐或搜索系统应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03746v1": {
    "title": "Whose Facts Win? LLM Source Preferences under Knowledge Conflicts",
    "url": "https://www.alphaxiv.org/abs/2601.03746v1",
    "arxiv_id": "2601.03746v1",
    "authors": "Jakob Schuster, Vagrant Gautam, Katja Markert",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 09:35:35",
    "ori_summary": "As large language models (LLMs) are more frequently used in retrieval-augmented generation pipelines, it is increasingly relevant to study their behavior under knowledge conflicts. Thus far, the role of the source of the retrieved information has gone unexamined. We address this gap with a novel framework to investigate how source preferences affect LLM resolution of inter-context knowledge conflicts in English, motivated by interdisciplinary research on credibility. With a comprehensive, tightly-controlled evaluation of 13 open-weight LLMs, we find that LLMs prefer institutionally-corroborated information (e.g., government or newspaper sources) over information from people and social media. However, these source preferences can be reversed by simply repeating information from less credible sources. To mitigate repetition effects and maintain consistent preferences, we propose a novel method that reduces repetition bias by up to 99.8%, while also maintaining at least 88.8% of original preferences. We release all data and code to encourage future work on credibility and source preferences in knowledge-intensive NLP.",
    "summary": "",
    "translation": "知识冲突下大语言模型的信源偏好：谁的事实胜出？",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLM在面临知识冲突时的信源偏好问题，这属于LLM评估和可靠性研究的范畴。虽然涉及LLM技术，但论文焦点是知识冲突下的行为分析，而非直接应用于推荐系统、搜索或广告的架构、效率或建模方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03743v1": {
    "title": "O-Researcher: An Open Ended Deep Research Model via Multi-Agent Distillation and Agentic RL",
    "url": "https://www.alphaxiv.org/abs/2601.03743v1",
    "arxiv_id": "2601.03743v1",
    "authors": "Yi Yao, He Zhu, Piaohong Wang, Jincheng Ren, Xinlong Yang, Qianben Chen, Xiaowan Li, Dingfeng Shi, Jiaxian Li, Qiexiang Wang, Sinuo Wang, Xinpeng Liu, Jiaqi Wu, Minghao Liu, Wangchunshu Zhou",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 09:31:10",
    "ori_summary": "The performance gap between closed-source and open-source large language models (LLMs) is largely attributed to disparities in access to high-quality training data. To bridge this gap, we introduce a novel framework for the automated synthesis of sophisticated, research-grade instructional data. Our approach centers on a multi-agent workflow where collaborative AI agents simulate complex tool-integrated reasoning to generate diverse and high-fidelity data end-to-end. Leveraging this synthesized data, we develop a two-stage training strategy that integrates supervised fine-tuning with a novel reinforcement learning method, designed to maximize model alignment and capability. Extensive experiments demonstrate that our framework empowers open-source models across multiple scales, enabling them to achieve new state-of-the-art performance on the major deep research benchmark. This work provides a scalable and effective pathway for advancing open-source LLMs without relying on proprietary data or models.",
    "summary": "",
    "translation": "O-Researcher：一种通过多智能体蒸馏与智能体强化学习实现的开端式深度研究模型",
    "relevance_score": 3,
    "reasoning": "该论文标题涉及多智能体系统和强化学习，但未明确说明与推荐系统、搜索或广告的直接关联。虽然强化学习在推荐系统中有应用，但标题中的“Open Ended Deep Research Model”和“Multi-Agent Distillation”更偏向通用AI研究框架，缺乏对RecSys/Search/Ads领域的明确指向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03733v1": {
    "title": "RadDiff: Describing Differences in Radiology Image Sets with Natural Language",
    "url": "https://www.alphaxiv.org/abs/2601.03733v1",
    "arxiv_id": "2601.03733v1",
    "authors": "Xiaoxian Shen, Yuhui Zhang, Sahithi Ankireddy, Xiaohan Wang, Maya Varma, Henry Guo, Curtis Langlotz, Serena Yeung-Levy",
    "categories": "cs.CV, cs.AI, cs.CL, cs.CY, cs.LG",
    "pub_date": "2026-01-07 09:25:04",
    "ori_summary": "Understanding how two radiology image sets differ is critical for generating clinical insights and for interpreting medical AI systems. We introduce RadDiff, a multimodal agentic system that performs radiologist-style comparative reasoning to describe clinically meaningful differences between paired radiology studies. RadDiff builds on a proposer-ranker framework from VisDiff, and incorporates four innovations inspired by real diagnostic workflows: (1) medical knowledge injection through domain-adapted vision-language models; (2) multimodal reasoning that integrates images with their clinical reports; (3) iterative hypothesis refinement across multiple reasoning rounds; and (4) targeted visual search that localizes and zooms in on salient regions to capture subtle findings. To evaluate RadDiff, we construct RadDiffBench, a challenging benchmark comprising 57 expert-validated radiology study pairs with ground-truth difference descriptions. On RadDiffBench, RadDiff achieves 47% accuracy, and 50% accuracy when guided by ground-truth reports, significantly outperforming the general-domain VisDiff baseline. We further demonstrate RadDiff's versatility across diverse clinical tasks, including COVID-19 phenotype comparison, racial subgroup analysis, and discovery of survival-related imaging features. Together, RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data.",
    "summary": "",
    "translation": "RadDiff：使用自然语言描述放射学图像集之间的差异",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（放射学）领域的特定应用，这属于明确的无关主题。虽然涉及自然语言处理，但其核心是医学领域的图像差异描述，与推荐系统、搜索或广告的技术进步、LLM应用或Transformer架构发展没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03727v1": {
    "title": "Stuttering-Aware Automatic Speech Recognition for Indonesian Language",
    "url": "https://www.alphaxiv.org/abs/2601.03727v1",
    "arxiv_id": "2601.03727v1",
    "authors": "Fadhil Muhammad, Alwin Djuliansah, Adrian Aryaputra Hamzah, Kurniawati Azizah",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 09:21:12",
    "ori_summary": "Automatic speech recognition systems have achieved remarkable performance on fluent speech but continue to degrade significantly when processing stuttered speech, a limitation that is particularly acute for low-resource languages like Indonesian where specialized datasets are virtually non-existent. To overcome this scarcity, we propose a data augmentation framework that generates synthetic stuttered audio by injecting repetitions and prolongations into fluent text through a combination of rule-based transformations and large language models followed by text-to-speech synthesis. We apply this synthetic data to fine-tune a pre-trained Indonesian Whisper model using transfer learning, enabling the architecture to adapt to dysfluent acoustic patterns without requiring large-scale real-world recordings. Our experiments demonstrate that this targeted synthetic exposure consistently reduces recognition errors on stuttered speech while maintaining performance on fluent segments, validating the utility of synthetic data pipelines for developing more inclusive speech technologies in under-represented languages.",
    "summary": "",
    "translation": "面向印尼语的结巴感知自动语音识别",
    "relevance_score": 1,
    "reasoning": "该论文专注于特定语言（印尼语）的自动语音识别技术，属于纯粹的语音处理领域。虽然语音识别在广义上可能与搜索系统相关，但该论文标题明确聚焦于语音识别中的结巴问题处理，没有提及任何与推荐系统、搜索排名、广告或LLM/Transformer架构相关的技术，完全属于被排除的'纯粹语音论文'类别。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03717v1": {
    "title": "MIND: From Passive Mimicry to Active Reasoning through Capability-Aware Multi-Perspective CoT Distillation",
    "url": "https://www.alphaxiv.org/abs/2601.03717v1",
    "arxiv_id": "2601.03717v1",
    "authors": "Jin Cui, Jiaqi Guo, Jiepeng Zhou, Ruixuan Yang, Jiayi Lu, Jiajun Xu, Jiangcheng Song, Boran Zhao, Pengju Ren",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 09:08:59",
    "ori_summary": "While Large Language Models (LLMs) have emerged with remarkable capabilities in complex tasks through Chain-of-Thought reasoning, practical resource constraints have sparked interest in transferring these abilities to smaller models. However, achieving both domain performance and cross-domain generalization remains challenging. Existing approaches typically restrict students to following a single golden rationale and treat different reasoning paths independently. Due to distinct inductive biases and intrinsic preferences, alongside the student's evolving capacity and reasoning preferences during training, a teacher's \"optimal\" rationale could act as out-of-distribution noise. This misalignment leads to a degeneration of the student's latent reasoning distribution, causing suboptimal performance. To bridge this gap, we propose MIND, a capability-adaptive framework that transitions distillation from passive mimicry to active cognitive construction. We synthesize diverse teacher perspectives through a novel \"Teaching Assistant\" network. By employing a Feedback-Driven Inertia Calibration mechanism, this network utilizes inertia-filtered training loss to align supervision with the student's current adaptability, effectively enhancing performance while mitigating catastrophic forgetting. Extensive experiments demonstrate that MIND achieves state-of-the-art performance on both in-distribution and out-of-distribution benchmarks, and our sophisticated latent space analysis further confirms the mechanism of reasoning ability internalization.",
    "summary": "主题：研究如何将大语言模型的复杂推理能力更有效地蒸馏到小模型中，解决现有方法因师生能力与偏好不匹配导致的性能退化问题。核心思想：提出MIND框架，通过“教学助理”网络合成多样化的教师推理视角，并采用反馈驱动的惯性校准机制，使监督信号动态适配学生模型当前的学习能力，实现从被动模仿到主动认知构建的转变。",
    "translation": "MIND：从被动模仿到主动推理——基于能力感知的多视角思维链蒸馏",
    "relevance_score": 8,
    "reasoning": "该论文涉及思维链（CoT）蒸馏技术，属于'赋能LLM技术'范畴，旨在提升LLM的推理能力。在推荐系统、搜索或广告领域，增强的推理能力可应用于复杂用户意图理解、多步骤决策制定（如会话式推荐）以及跨模态信息整合，从而提高系统智能性和准确性。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出一种从被动模仿转向主动认知构建的蒸馏框架，通过能力感知和多视角合成来提升小模型的推理能力，直接对应“使能LLM技术”中模型压缩与知识迁移的核心趋势。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03714v1": {
    "title": "Visual Merit or Linguistic Crutch? A Close Look at DeepSeek-OCR",
    "url": "https://www.alphaxiv.org/abs/2601.03714v1",
    "arxiv_id": "2601.03714v1",
    "authors": "Yunhao Liang, Ruixuan Ying, Bo Li, Hong Li, Kai Yan, Qingwen Li, Min Yang, Okamoto Satoshi, Zhe Cui, Shiwen Ni",
    "categories": "cs.CL, cs.CV",
    "pub_date": "2026-01-07 09:01:23",
    "ori_summary": "DeepSeek-OCR utilizes an optical 2D mapping approach to achieve high-ratio vision-text compression, claiming to decode text tokens exceeding ten times the input visual tokens. While this suggests a promising solution for the LLM long-context bottleneck, we investigate a critical question: \"Visual merit or linguistic crutch - which drives DeepSeek-OCR's performance?\" By employing sentence-level and word-level semantic corruption, we isolate the model's intrinsic OCR capabilities from its language priors. Results demonstrate that without linguistic support, DeepSeek-OCR's performance plummets from approximately 90% to 20%. Comparative benchmarking against 13 baseline models reveals that traditional pipeline OCR methods exhibit significantly higher robustness to such semantic perturbations than end-to-end methods. Furthermore, we find that lower visual token counts correlate with increased reliance on priors, exacerbating hallucination risks. Context stress testing also reveals a total model collapse around 10,000 text tokens, suggesting that current optical compression techniques may paradoxically aggravate the long-context bottleneck. This study empirically defines DeepSeek-OCR's capability boundaries and offers essential insights for future optimizations of the vision-text compression paradigm. We release all data, results and scripts used in this study at https://github.com/dududuck00/DeepSeekOCR.",
    "summary": "",
    "translation": "视觉优势还是语言拐杖？深度剖析DeepSeek-OCR",
    "relevance_score": 2,
    "reasoning": "该论文标题聚焦于OCR（光学字符识别）技术分析，属于计算机视觉领域。虽然OCR在特定场景下可能辅助文档处理，但标题未表明与推荐系统、搜索或广告的核心技术（如排序、召回、用户建模）有直接关联，也未提及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03707v1": {
    "title": "AirNav: A Large-Scale Real-World UAV Vision-and-Language Navigation Dataset with Natural and Diverse Instructions",
    "url": "https://www.alphaxiv.org/abs/2601.03707v1",
    "arxiv_id": "2601.03707v1",
    "authors": "Hengxing Cai, Yijie Rao, Ligang Huang, Zanyang Zhong, Jinhan Dong, Jingjun Tan, Wenhao Lu, Renxin Zhong",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 08:46:09",
    "ori_summary": "Existing Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) datasets face issues such as dependence on virtual environments, lack of naturalness in instructions, and limited scale. To address these challenges, we propose AirNav, a large-scale UAV VLN benchmark constructed from real urban aerial data, rather than synthetic environments, with natural and diverse instructions. Additionally, we introduce the AirVLN-R1, which combines Supervised Fine-Tuning and Reinforcement Fine-Tuning to enhance performance and generalization. The feasibility of the model is preliminarily evaluated through real-world tests. Our dataset and code are publicly available.",
    "summary": "",
    "translation": "AirNav：一个大规模真实世界无人机视觉与语言导航数据集，包含自然且多样化的指令",
    "relevance_score": 2,
    "reasoning": "该论文主要关注无人机视觉与语言导航数据集，属于计算机视觉和机器人导航领域。虽然涉及视觉-语言多模态建模，但其应用场景（无人机导航）与推荐系统、搜索或广告的核心领域没有直接关联，也不符合您关注的LLM在RecSys/Search/Ads中的直接应用或异构数据统一建模的要求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03700v1": {
    "title": "ADEPT: Adaptive Dynamic Early-Exit Process for Transformers",
    "url": "https://www.alphaxiv.org/abs/2601.03700v1",
    "arxiv_id": "2601.03700v1",
    "authors": "Sangmin Yoo, Srikanth Malla, Chiho Choi, Wei D. Lu, Joon Hee Choi",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 08:34:41",
    "ori_summary": "The inference of large language models imposes significant computational workloads, often requiring the processing of billions of parameters. Although early-exit strategies have proven effective in reducing computational demands by halting inference earlier, they apply either to only the first token in the generation phase or at the prompt level in the prefill phase. Thus, the Key-Value (KV) cache for skipped layers remains a bottleneck for subsequent token generation, limiting the benefits of early exit. We introduce ADEPT (Adaptive Dynamic Early-exit Process for Transformers), a novel approach designed to overcome this issue and enable dynamic early exit in both the prefill and generation phases. The proposed adaptive token-level early-exit mechanism adjusts computation dynamically based on token complexity, optimizing efficiency without compromising performance. ADEPT further enhances KV generation procedure by decoupling sequential dependencies in skipped layers, making token-level early exit more practical. Experimental results demonstrate that ADEPT improves efficiency by up to 25% in language generation tasks and achieves a 4x speed-up in downstream classification tasks, with up to a 45% improvement in performance.",
    "summary": "该论文研究Transformer模型推理时的计算效率瓶颈问题，核心思想是提出一种自适应令牌级动态提前退出机制，通过解耦跳过层中的序列依赖关系，在预填充和生成阶段都实现动态计算优化。",
    "translation": "ADEPT：面向Transformer的自适应动态提前退出处理机制",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能Transformer技术'范畴，专注于Transformer架构的效率优化。自适应动态提前退出机制可显著降低推理延迟和计算成本，这对推荐系统、搜索和广告中的实时大规模部署至关重要，特别是在需要低延迟响应的在线服务场景中。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的ADEPT方法通过自适应动态提前退出机制优化Transformer推理效率，直接针对Transformer架构的效率瓶颈，属于核心的Transformer技术进步，对搜索、推荐等大规模系统具有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03699v1": {
    "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2601.03699v1",
    "arxiv_id": "2601.03699v1",
    "authors": "Quy-Anh Dang, Chris Ngo, Truong-Son Hy",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 08:34:17",
    "ori_summary": "As large language models (LLMs) become integral to safety-critical applications, ensuring their robustness against adversarial prompts is paramount. However, existing red teaming datasets suffer from inconsistent risk categorizations, limited domain coverage, and outdated evaluations, hindering systematic vulnerability assessments. To address these challenges, we introduce RedBench, a universal dataset aggregating 37 benchmark datasets from leading conferences and repositories, comprising 29,362 samples across attack and refusal prompts. RedBench employs a standardized taxonomy with 22 risk categories and 19 domains, enabling consistent and comprehensive evaluations of LLM vulnerabilities. We provide a detailed analysis of existing datasets, establish baselines for modern LLMs, and open-source the dataset and evaluation code. Our contributions facilitate robust comparisons, foster future research, and promote the development of secure and reliable LLMs for real-world deployment. Code: https://github.com/knoveleng/redeval",
    "summary": "",
    "translation": "RedBench：用于大型语言模型全面红队测试的通用数据集",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于LLM红队测试数据集，属于纯粹的LLM评估基准范畴。虽然红队测试在安全领域有应用，但论文本身不涉及推荐系统、搜索或广告的核心技术进展、架构改进或直接应用，完全属于您列出的不相关主题中的“评估基准”类别。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03698v1": {
    "title": "Evaluation Framework for AI Creativity: A Case Study Based on Story Generation",
    "url": "https://www.alphaxiv.org/abs/2601.03698v1",
    "arxiv_id": "2601.03698v1",
    "authors": "Pharath Sathya, Yin Jou Huang, Fei Cheng",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 08:31:08",
    "ori_summary": "Evaluating creative text generation remains a challenge because existing reference-based metrics fail to capture the subjective nature of creativity. We propose a structured evaluation framework for AI story generation comprising four components (Novelty, Value, Adherence, and Resonance) and eleven sub-components. Using controlled story generation via ``Spike Prompting'' and a crowdsourced study of 115 readers, we examine how different creative components shape both immediate and reflective human creativity judgments. Our findings show that creativity is evaluated hierarchically rather than cumulatively, with different dimensions becoming salient at different stages of judgment, and that reflective evaluation substantially alters both ratings and inter-rater agreement. Together, these results support the effectiveness of our framework in revealing dimensions of creativity that are obscured by reference-based evaluation.",
    "summary": "",
    "translation": "人工智能创造力评估框架：基于故事生成的案例研究",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于AI创造力评估和故事生成，属于纯粹的LLM内容生成和评估基准范畴。根据用户列出的不相关主题，这直接属于'纯粹LLM中心主题'和'评估基准'类别，与推荐系统、搜索或广告的核心技术进展、LLM使能技术或直接应用完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03682v1": {
    "title": "From Implicit to Explicit: Token-Efficient Logical Supervision for Mathematical Reasoning in LLMs",
    "url": "https://www.alphaxiv.org/abs/2601.03682v1",
    "arxiv_id": "2601.03682v1",
    "authors": "Shaojie Wang, Liang Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 08:15:01",
    "ori_summary": "Recent studies reveal that large language models (LLMs) exhibit limited logical reasoning abilities in mathematical problem-solving, instead often relying on pattern-matching and memorization. We systematically analyze this limitation, focusing on logical relationship understanding, which is a core capability underlying genuine logical reasoning, and reveal that errors related to this capability account for over 90\\% of incorrect predictions, with Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) failing to substantially reduce these errors. To address this bottleneck, we propose First-Step Logical Reasoning (FSLR), a lightweight training framework targeting logical relationship understanding. Our key insight is that the first planning step-identifying which variables to use and which operation to apply-encourages the model to derive logical relationships directly from the problem statement. By training models on this isolated step, FSLR provides explicit supervision for logical relationship understanding, unlike CoT-SFT which implicitly embeds such relationships within complete solution trajectories. Extensive experiments across multiple models and datasets demonstrate that FSLR consistently outperforms CoT-SFT under both in-distribution and out-of-distribution settings, with average improvements of 3.2\\% and 4.6\\%, respectively. Moreover, FSLR achieves 4-6x faster training and reduces training token consumption by over 80\\%.",
    "summary": "",
    "translation": "从隐式到显式：面向大语言模型数学推理的令牌高效逻辑监督",
    "relevance_score": 2,
    "reasoning": "该论文专注于数学推理这一特定领域，属于纯粹的NLP应用，与推荐系统、搜索或广告的核心技术没有直接关联。虽然涉及LLM训练技术，但未展示在RecSys/Search/Ads领域的潜在应用价值，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03676v1": {
    "title": "Towards Compositional Generalization of LLMs via Skill Taxonomy Guided Data Synthesis",
    "url": "https://www.alphaxiv.org/abs/2601.03676v1",
    "arxiv_id": "2601.03676v1",
    "authors": "Yifan Wei, Li Du, Xiaoyan Yu, Yang Feng, Angsheng Li",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 07:58:51",
    "ori_summary": "Large Language Models (LLMs) and agent-based systems often struggle with compositional generalization due to a data bottleneck in which complex skill combinations follow a long-tailed, power-law distribution, limiting both instruction-following performance and generalization in agent-centric tasks. To address this challenge, we propose STEPS, a Skill Taxonomy guided Entropy-based Post-training data Synthesis framework for generating compositionally challenging data. STEPS explicitly targets compositional generalization by uncovering latent relationships among skills and organizing them into an interpretable, hierarchical skill taxonomy using structural information theory. Building on this taxonomy, we formulate data synthesis as a constrained information maximization problem, selecting skill combinations that maximize marginal structural information within the hierarchy while preserving semantic coherence. Experiments on challenging instruction-following benchmarks show that STEPS outperforms existing data synthesis baselines, while also yielding improved compositional generalization in downstream agent-based evaluations.",
    "summary": "该论文研究LLM在组合技能泛化上的数据瓶颈问题，其核心方法是构建可解释的层次化技能分类学，并基于此将数据合成建模为约束信息最大化问题，以生成具有挑战性的组合数据。",
    "translation": "面向技能分类指导数据合成的LLM组合泛化研究",
    "relevance_score": 8,
    "reasoning": "该论文属于'Enabling LLM Tech'范畴，专注于通过数据合成提升LLM的组合泛化能力。组合泛化对推荐系统至关重要，可增强模型处理新用户-物品组合、理解复杂查询以及生成个性化推荐的能力。技能分类指导的数据合成方法可直接应用于构建更鲁棒的推荐模型，提升对未见用户偏好和物品特征的泛化性能。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM的组合泛化瓶颈，提出基于技能分类学的数据合成框架，属于核心LLM技术进步，对搜索推荐系统中的复杂任务分解与泛化有直接应用潜力。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03672v1": {
    "title": "Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction",
    "url": "https://www.alphaxiv.org/abs/2601.03672v1",
    "arxiv_id": "2601.03672v1",
    "authors": "Chen Zhang, Kepu Zhang, Jiatong Zhang, Xiao Zhang, Jun Xu",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2026-01-07 07:52:30",
    "ori_summary": "Query correction is a critical entry point in modern search pipelines, demanding high accuracy strictly within real-time latency constraints. Chain-of-Thought (CoT) reasoning improves accuracy but incurs prohibitive latency for real-time query correction. A potential solution is to output an answer before reasoning to reduce latency; however, under autoregressive decoding, the early answer is independent of subsequent reasoning, preventing the model from leveraging its reasoning capability to improve accuracy. To address this issue, we propose Sandwich Reasoning (SandwichR), a novel approach that explicitly aligns a fast initial answer with post-hoc reasoning, enabling low-latency query correction without sacrificing reasoning-aware accuracy. SandwichR follows an Answer-Reasoning-Answer paradigm, producing an initial correction, an explicit reasoning process, and a final refined correction. To align the initial answer with post-reasoning insights, we design a consistency-aware reinforcement learning (RL) strategy: a dedicated consistency reward enforces alignment between the initial and final corrections, while margin-based rejection sampling prioritizes borderline samples where reasoning drives the most impactful corrective gains. Additionally, we construct a high-quality query correction dataset, addressing the lack of specialized benchmarks for complex query correction. Experimental results demonstrate that SandwichR achieves SOTA accuracy comparable to standard CoT while delivering a 40-70% latency reduction, resolving the latency-accuracy trade-off in online search.",
    "summary": "",
    "translation": "三明治推理：一种用于低延迟查询纠正的答案-推理-答案方法",
    "relevance_score": 4,
    "reasoning": "该论文涉及查询纠正技术，与搜索领域直接相关，但标题未明确说明是否采用LLM或Transformer架构。低延迟优化可能对搜索系统有实用价值，但需要更多细节判断是否属于核心领域进展或直接LLM应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2601.03671v1": {
    "title": "NeuronScope: A Multi-Agent Framework for Explaining Polysemantic Neurons in Language Models",
    "url": "https://www.alphaxiv.org/abs/2601.03671v1",
    "arxiv_id": "2601.03671v1",
    "authors": "Weiqi Liu, Yongliang Miao, Haiyan Zhao, Yanguang Liu, Mengnan Du",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2026-01-07 07:50:47",
    "ori_summary": "Neuron-level interpretation in large language models (LLMs) is fundamentally challenged by widespread polysemanticity, where individual neurons respond to multiple distinct semantic concepts. Existing single-pass interpretation methods struggle to faithfully capture such multi-concept behavior. In this work, we propose NeuronScope, a multi-agent framework that reformulates neuron interpretation as an iterative, activation-guided process. NeuronScope explicitly deconstructs neuron activations into atomic semantic components, clusters them into distinct semantic modes, and iteratively refines each explanation using neuron activation feedback. Experiments demonstrate that NeuronScope uncovers hidden polysemanticity and produces explanations with significantly higher activation correlation compared to single-pass baselines.",
    "summary": "",
    "translation": "NeuronScope：一种用于解释语言模型中多义神经元的多智能体框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注语言模型的可解释性，属于模型分析领域，与推荐系统、搜索或广告的核心技术进展或直接应用关联较弱。虽然涉及语言模型，但未明确展示在推荐/搜索/广告领域的潜在应用价值，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03670v1": {
    "title": "DisastQA: A Comprehensive Benchmark for Evaluating Question Answering in Disaster Management",
    "url": "https://www.alphaxiv.org/abs/2601.03670v1",
    "arxiv_id": "2601.03670v1",
    "authors": "Zhitong Chen, Kai Yin, Xiangjue Dong, Chengkai Liu, Xiangpeng Li, Yiming Xiao, Bo Li, Junwei Ma, Ali Mostafavi, James Caverlee",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 07:46:42",
    "ori_summary": "Accurate question answering (QA) in disaster management requires reasoning over uncertain and conflicting information, a setting poorly captured by existing benchmarks built on clean evidence. We introduce DisastQA, a large-scale benchmark of 3,000 rigorously verified questions (2,000 multiple-choice and 1,000 open-ended) spanning eight disaster types. The benchmark is constructed via a human-LLM collaboration pipeline with stratified sampling to ensure balanced coverage. Models are evaluated under varying evidence conditions, from closed-book to noisy evidence integration, enabling separation of internal knowledge from reasoning under imperfect information. For open-ended QA, we propose a human-verified keypoint-based evaluation protocol emphasizing factual completeness over verbosity. Experiments with 20 models reveal substantial divergences from general-purpose leaderboards such as MMLU-Pro. While recent open-weight models approach proprietary systems in clean settings, performance degrades sharply under realistic noise, exposing critical reliability gaps for disaster response. All code, data, and evaluation resources are available at https://github.com/TamuChen18/DisastQA_open.",
    "summary": "",
    "translation": "DisastQA：用于评估灾害管理领域问答系统的综合基准",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向灾害管理领域的问答系统评估基准，属于特定领域应用（灾害管理），与用户关注的推荐系统、搜索、广告等核心领域无关。同时，它属于纯粹的评估基准研究，属于用户明确排除的“Hallucination, Evaluation benchmarks, or other purely NLP-centric topics”范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03669v1": {
    "title": "eTracer: Towards Traceable Text Generation via Claim-Level Grounding",
    "url": "https://www.alphaxiv.org/abs/2601.03669v1",
    "arxiv_id": "2601.03669v1",
    "authors": "Bohao Chu, Qianli Wang, Hendrik Damm, Hui Wang, Ula Muhabbek, Elisabeth Livingstone, Christoph M. Friedrich, Norbert Fuhr",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 07:44:30",
    "ori_summary": "How can system-generated responses be efficiently verified, especially in the high-stakes biomedical domain? To address this challenge, we introduce eTracer, a plug-and-play framework that enables traceable text generation by grounding claims against contextual evidence. Through post-hoc grounding, each response claim is aligned with contextual evidence that either supports or contradicts it. Building on claim-level grounding results, eTracer not only enables users to precisely trace responses back to their contextual source but also quantifies response faithfulness, thereby enabling the verifiability and trustworthiness of generated responses. Experiments show that our claim-level grounding approach alleviates the limitations of conventional grounding methods in aligning generated statements with contextual sentence-level evidence, resulting in substantial improvements in overall grounding quality and user verification efficiency. The code and data are available at https://github.com/chubohao/eTracer.",
    "summary": "",
    "translation": "eTracer：通过声明级基础实现可追溯文本生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文本生成的可追溯性和事实基础，这属于LLM的幻觉缓解和评估范畴，属于您指定的无关主题。虽然声明级基础技术可能间接影响推荐/搜索系统中生成内容的可信度，但论文本身没有明确指向RecSys/Search/Ads应用，且更偏向纯NLP技术而非直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03666v1": {
    "title": "e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings",
    "url": "https://www.alphaxiv.org/abs/2601.03666v1",
    "arxiv_id": "2601.03666v1",
    "authors": "Haonan Chen, Sicheng Gao, Radu Timofte, Tetsuya Sakai, Zhicheng Dou",
    "categories": "cs.CL, cs.AI, cs.CV",
    "pub_date": "2026-01-07 07:39:40",
    "ori_summary": "Modern information systems often involve different types of items, e.g., a text query, an image, a video clip, or an audio segment. This motivates omni-modal embedding models that map heterogeneous modalities into a shared space for direct comparison. However, most recent omni-modal embeddings still rely heavily on implicit alignment inherited from pretrained vision-language model (VLM) backbones. In practice, this causes three common issues: (i) similarity logits have modality-dependent sharpness, so scores are not on a consistent scale; (ii) in-batch negatives become less effective over time because mixed-modality batches create an imbalanced hardness distribution; as a result, many negatives quickly become trivial and contribute little gradient; and (iii) embeddings across modalities show mismatched first- and second-order statistics, which makes rankings less stable. To tackle these problems, we propose e5-omni, a lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. e5-omni combines three simple components: (1) modality-aware temperature calibration to align similarity scales, (2) a controllable negative curriculum with debiasing to focus on confusing negatives while reducing the impact of false negatives, and (3) batch whitening with covariance regularization to better match cross-modal geometry in the shared embedding space. Experiments on MMEB-V2 and AudioCaps show consistent gains over strong bi-modal and omni-modal baselines, and the same recipe also transfers well to other VLM backbones. We release our model checkpoint at https://huggingface.co/Haon-Chen/e5-omni-7B.",
    "summary": "该论文研究多模态嵌入模型中因隐式对齐导致的相似度尺度不一致、负样本效率低和统计特性不匹配问题。核心方法是提出包含模态感知温度校准、可控负样本课程学习和批量白化的轻量级显式对齐方案，将现有视觉语言模型适配为稳健的多模态嵌入模型。",
    "translation": "e5-omni：面向全模态嵌入的显式跨模态对齐",
    "relevance_score": 9,
    "reasoning": "该论文标题直接涉及跨模态对齐技术，与'VLM Analogy for Heterogeneous Data'高度相关，可将搜索/推荐中的用户序列、上下文特征等异构数据视为不同模态进行统一建模。这种显式对齐方法在提升多模态检索、跨域推荐等任务性能方面具有明确的应用潜力。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对多模态嵌入对齐问题，提出显式对齐方法，与VLM类比异质数据建模高度相关，且方法具有通用性可应用于推荐搜索场景。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03649v1": {
    "title": "SyncThink: A Training-Free Strategy to Align Inference Termination with Reasoning Saturation",
    "url": "https://www.alphaxiv.org/abs/2601.03649v1",
    "arxiv_id": "2601.03649v1",
    "authors": "Gengyang Li, Wang Cai, Yifeng Gao, Yunfang Wu",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 07:00:15",
    "ori_summary": "Chain-of-Thought (CoT) prompting improves reasoning but often produces long and redundant traces that substantially increase inference cost. We present SyncThink, a training-free and plug-and-play decoding method that reduces CoT overhead without modifying model weights. We find that answer tokens attend weakly to early reasoning and instead focus on the special token \"/think\", indicating an information bottleneck. Building on this observation, SyncThink monitors the model's own reasoning-transition signal and terminates reasoning. Experiments on GSM8K, MMLU, GPQA, and BBH across three DeepSeek-R1 distilled models show that SyncThink achieves 62.00 percent average Top-1 accuracy using 656 generated tokens and 28.68 s latency, compared to 61.22 percent, 2141 tokens, and 92.01 s for full CoT decoding. On long-horizon tasks such as GPQA, SyncThink can further yield up to +8.1 absolute accuracy by preventing over-thinking.",
    "summary": "",
    "translation": "SyncThink：一种无需训练的策略，用于将推理终止与推理饱和对齐",
    "relevance_score": 3,
    "reasoning": "该论文提出了一种在推理过程中确定何时终止生成的方法，这属于LLM推理效率的范畴。虽然推理效率是LLM技术的基础进步，可能间接应用于推荐系统或搜索中的响应生成优化，但论文标题未明确说明与RecSys/Search/Ads的具体应用联系，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03648v1": {
    "title": "ELO: Efficient Layer-Specific Optimization for Continual Pretraining of Multilingual LLMs",
    "url": "https://www.alphaxiv.org/abs/2601.03648v1",
    "arxiv_id": "2601.03648v1",
    "authors": "HanGyeol Yoo, ChangSu Choi, Minjun Kim, Seohyun Song, SeungWoo Song, Inho Won, Jongyoul Park, Cheoneum Park, KyungTae Lim",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 06:55:29",
    "ori_summary": "We propose an efficient layer-specific optimization (ELO) method designed to enhance continual pretraining (CP) for specific languages in multilingual large language models (MLLMs). This approach addresses the common challenges of high computational cost and degradation of source language performance associated with traditional CP. The ELO method consists of two main stages: (1) ELO Pretraining, where a small subset of specific layers, identified in our experiments as the critically important first and last layers, are detached from the original MLLM and trained with the target language. This significantly reduces not only the number of trainable parameters but also the total parameters computed during the forward pass, minimizing GPU memory consumption and accelerating the training process. (2) Layer Alignment, where the newly trained layers are reintegrated into the original model, followed by a brief full fine-tuning step on a small dataset to align the parameters. Experimental results demonstrate that the ELO method achieves a training speedup of up to 6.46 times compared to existing methods, while improving target language performance by up to 6.2\\% on qualitative benchmarks and effectively preserving source language (English) capabilities.",
    "summary": "",
    "translation": "ELO：面向多语言大语言模型持续预训练的高效层特定优化",
    "relevance_score": 6,
    "reasoning": "该论文属于'使能LLM技术'范畴，专注于通过层特定优化提升多语言LLM持续预训练的效率。虽然不直接应用于推荐/搜索/广告，但高效的多语言模型优化技术可显著降低多语言场景（如跨境电商推荐、多语言搜索广告）的模型更新成本，具有明确的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2601.03645v1": {
    "title": "LLM-MC-Affect: LLM-Based Monte Carlo Modeling of Affective Trajectories and Latent Ambiguity for Interpersonal Dynamic Insight",
    "url": "https://www.alphaxiv.org/abs/2601.03645v1",
    "arxiv_id": "2601.03645v1",
    "authors": "Yu-Zheng Lin, Bono Po-Jen Shih, John Paul Martin Encinas, Elizabeth Victoria Abraham Achom, Karan Himanshu Patel, Jesus Horacio Pacheco, Sicong Shao, Jyotikrishna Dass, Soheil Salehi, Pratik Satam",
    "categories": "cs.CL, cs.CY",
    "pub_date": "2026-01-07 06:50:41",
    "ori_summary": "Emotional coordination is a core property of human interaction that shapes how relational meaning is constructed in real time. While text-based affect inference has become increasingly feasible, prior approaches often treat sentiment as a deterministic point estimate for individual speakers, failing to capture the inherent subjectivity, latent ambiguity, and sequential coupling found in mutual exchanges. We introduce LLM-MC-Affect, a probabilistic framework that characterizes emotion not as a static label, but as a continuous latent probability distribution defined over an affective space. By leveraging stochastic LLM decoding and Monte Carlo estimation, the methodology approximates these distributions to derive high-fidelity sentiment trajectories that explicitly quantify both central affective tendencies and perceptual ambiguity. These trajectories enable a structured analysis of interpersonal coupling through sequential cross-correlation and slope-based indicators, identifying leading or lagging influences between interlocutors. To validate the interpretive capacity of this approach, we utilize teacher-student instructional dialogues as a representative case study, where our quantitative indicators successfully distill high-level interaction insights such as effective scaffolding. This work establishes a scalable and deployable pathway for understanding interpersonal dynamics, offering a generalizable solution that extends beyond education to broader social and behavioral research.",
    "summary": "",
    "translation": "LLM-MC-Affect：基于大语言模型的蒙特卡洛情感轨迹建模与潜在模糊性分析用于人际动态洞察",
    "relevance_score": 2,
    "reasoning": "该论文标题主要涉及情感轨迹建模和人际动态分析，这属于心理学或社交计算领域，与推荐系统、搜索或广告的核心技术进展没有直接关联。虽然使用了LLM技术，但应用场景（人际动态洞察）不符合当前关注的RecSys/Search/Ads领域，且未明确展示在推荐、搜索或广告中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03641v1": {
    "title": "Agent-Dice: Disentangling Knowledge Updates via Geometric Consensus for Agent Continual Learning",
    "url": "https://www.alphaxiv.org/abs/2601.03641v1",
    "arxiv_id": "2601.03641v1",
    "authors": "Zheng Wu, Xingyu Lou, Xinbei Ma, Yansi Li, Weiwen Liu, Weinan Zhang, Jun Wang, Zhuosheng Zhang",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 06:43:50",
    "ori_summary": "Large Language Model (LLM)-based agents significantly extend the utility of LLMs by interacting with dynamic environments. However, enabling agents to continually learn new tasks without catastrophic forgetting remains a critical challenge, known as the stability-plasticity dilemma. In this work, we argue that this dilemma fundamentally arises from the failure to explicitly distinguish between common knowledge shared across tasks and conflicting knowledge introduced by task-specific interference. To address this, we propose Agent-Dice, a parameter fusion framework based on directional consensus evaluation. Concretely, Agent-Dice disentangles knowledge updates through a two-stage process: geometric consensus filtering to prune conflicting gradients, and curvature-based importance weighting to amplify shared semantics. We provide a rigorous theoretical analysis that establishes the validity of the proposed fusion scheme and offers insight into the origins of the stability-plasticity dilemma. Extensive experiments on GUI agents and tool-use agent domains demonstrate that Agent-Dice exhibits outstanding continual learning performance with minimal computational overhead and parameter updates.",
    "summary": "",
    "translation": "Agent-Dice：基于几何共识解耦知识更新的智能体持续学习方法",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及智能体持续学习，主要关注知识更新和几何共识，属于强化学习/智能体学习领域。虽然持续学习技术可能间接应用于推荐系统或搜索中的用户偏好演化建模，但论文标题未明确指向推荐系统、搜索或广告的核心问题，也未涉及LLM、Transformer架构或异构数据统一建模等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03630v1": {
    "title": "Reasoning Model Is Superior LLM-Judge, Yet Suffers from Biases",
    "url": "https://www.alphaxiv.org/abs/2601.03630v1",
    "arxiv_id": "2601.03630v1",
    "authors": "Hui Huang, Xuanxin Wu, Muyun Yang, Yuki Arase",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 06:19:26",
    "ori_summary": "This paper presents the first systematic comparison investigating whether Large Reasoning Models (LRMs) are superior judge to non-reasoning LLMs. Our empirical analysis yields four key findings: 1) LRMs outperform non-reasoning LLMs in terms of judgment accuracy, particularly on reasoning-intensive tasks; 2) LRMs demonstrate superior instruction-following capabilities in evaluation contexts; 3) LRMs exhibit enhanced robustness against adversarial attacks targeting judgment tasks; 4) However, LRMs still exhibit strong biases in superficial quality. To improve the robustness against biases, we propose PlanJudge, an evaluation strategy that prompts the model to generate an explicit evaluation plan before execution. Despite its simplicity, our experiments demonstrate that PlanJudge significantly mitigates biases in both LRMs and standard LLMs.",
    "summary": "",
    "translation": "推理模型优于LLM-Judge，但仍存在偏见问题",
    "relevance_score": 2,
    "reasoning": "该论文标题主要涉及LLM评估和偏见问题，属于NLP-centric的评估基准话题，与您关注的RecSys/Search/Ads核心进展、LLM技术应用或Transformer架构改进等焦点领域关联较弱。虽然LLM评估技术可能间接影响推荐系统，但论文标题明确指向评估和偏见，这属于您列出的不相关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03627v1": {
    "title": "Evaluating the Pre-Consultation Ability of LLMs using Diagnostic Guidelines",
    "url": "https://www.alphaxiv.org/abs/2601.03627v1",
    "arxiv_id": "2601.03627v1",
    "authors": "Jean Seo, Gibaeg Kim, Kihun Shin, Seungseop Lim, Hyunkyung Lee, Wooseok Han, Jongwon Lee, Eunho Yang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 06:15:21",
    "ori_summary": "We introduce EPAG, a benchmark dataset and framework designed for Evaluating the Pre-consultation Ability of LLMs using diagnostic Guidelines. LLMs are evaluated directly through HPI-diagnostic guideline comparison and indirectly through disease diagnosis. In our experiments, we observe that small open-source models fine-tuned with a well-curated, task-specific dataset can outperform frontier LLMs in pre-consultation. Additionally, we find that increased amount of HPI (History of Present Illness) does not necessarily lead to improved diagnostic performance. Further experiments reveal that the language of pre-consultation influences the characteristics of the dialogue. By open-sourcing our dataset and evaluation pipeline on https://github.com/seemdog/EPAG, we aim to contribute to the evaluation and further development of LLM applications in real-world clinical settings.",
    "summary": "",
    "translation": "基于诊断指南评估大语言模型的预咨询能力",
    "relevance_score": 2,
    "reasoning": "该论文标题明确聚焦于LLM在医疗诊断咨询场景下的评估，属于医疗领域特定应用，与您关注的搜索/推荐/广告核心领域无关。虽然涉及LLM能力评估，但属于医疗领域评估而非通用技术进展，且没有表明对RecSys/Search/Ads的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03615v1": {
    "title": "Analyzing Reasoning Shifts in Audio Deepfake Detection under Adversarial Attacks: The Reasoning Tax versus Shield Bifurcation",
    "url": "https://www.alphaxiv.org/abs/2601.03615v1",
    "arxiv_id": "2601.03615v1",
    "authors": "Binh Nguyen, Thai Le",
    "categories": "cs.CL, cs.SD, eess.AS",
    "pub_date": "2026-01-07 05:46:45",
    "ori_summary": "Audio Language Models (ALMs) offer a promising shift towards explainable audio deepfake detections (ADDs), moving beyond \\textit{black-box} classifiers by providing some level of transparency into their predictions via reasoning traces. This necessitates a new class of model robustness analysis: robustness of the predictive reasoning under adversarial attacks, which goes beyond existing paradigm that mainly focuses on the shifts of the final predictions (e.g., fake v.s. real). To analyze such reasoning shifts, we introduce a forensic auditing framework to evaluate the robustness of ALMs' reasoning under adversarial attacks in three inter-connected dimensions: acoustic perception, cognitive coherence, and cognitive dissonance. Our systematic analysis reveals that explicit reasoning does not universally enhance robustness. Instead, we observe a bifurcation: for models exhibiting robust acoustic perception, reasoning acts as a defensive \\textit{``shield''}, protecting them from adversarial attacks. However, for others, it imposes a performance \\textit{``tax''}, particularly under linguistic attacks which reduce cognitive coherence and increase attack success rate. Crucially, even when classification fails, high cognitive dissonance can serve as a \\textit{silent alarm}, flagging potential manipulation. Overall, this work provides a critical evaluation of the role of reasoning in forensic audio deepfake analysis and its vulnerabilities.",
    "summary": "",
    "translation": "对抗攻击下音频深度伪造检测的推理偏移分析：推理税与防护盾分叉现象",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及音频深度伪造检测和对抗攻击，属于音频安全领域。虽然标题提到“推理偏移”，但这主要针对检测模型的鲁棒性分析，与推荐系统、搜索、广告的核心技术（如排序算法、用户建模、特征工程）或LLM/Transformer的架构创新没有直接关联。该主题更接近安全/隐私领域，属于明确的无关话题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03605v1": {
    "title": "DiVA: Fine-grained Factuality Verification with Agentic-Discriminative Verifier",
    "url": "https://www.alphaxiv.org/abs/2601.03605v1",
    "arxiv_id": "2601.03605v1",
    "authors": "Hui Huang, Muyun Yang, Yuki Arase",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 05:35:01",
    "ori_summary": "Despite the significant advancements of Large Language Models (LLMs), their factuality remains a critical challenge, fueling growing interest in factuality verification. Existing research on factuality verification primarily conducts binary judgments (e.g., correct or incorrect), which fails to distinguish varying degrees of error severity. This limits its utility for applications such as fine-grained evaluation and preference optimization. To bridge this gap, we propose the Agentic Discriminative Verifier (DiVA), a hybrid framework that synergizes the agentic search capabilities of generative models with the precise scoring aptitude of discriminative models. We also construct a new benchmark, FGVeriBench, as a robust testbed for fine-grained factuality verification. Experimental results on FGVeriBench demonstrate that our DiVA significantly outperforms existing methods on factuality verification for both general and multi-hop questions.",
    "summary": "",
    "translation": "DiVA：基于智能判别式验证器的细粒度事实性验证",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于事实性验证这一特定任务，属于纯粹NLP领域的评估和验证范畴，与您关注的推荐系统、搜索、广告核心进展或相关使能技术无关。该研究没有展示在异构数据处理、Transformer架构改进或LLM在推荐/搜索/广告中直接应用的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03597v1": {
    "title": "From Chains to Graphs: Self-Structured Reasoning for General-Domain LLMs",
    "url": "https://www.alphaxiv.org/abs/2601.03597v1",
    "arxiv_id": "2601.03597v1",
    "authors": "Yingjian Chen, Haoran Liu, Yinhong Liu, Sherry T. Tong, Aosong Feng, Jinghui Lu, Juntao Zhang, Yusuke Iwasawa, Yutaka Matsuo, Irene Li",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 05:27:41",
    "ori_summary": "Large Language Models (LLMs) show strong reasoning ability in open-domain question answering, yet their reasoning processes are typically linear and often logically inconsistent. In contrast, real-world reasoning requires integrating multiple premises and solving subproblems in parallel. Existing methods, such as Chain-of-Thought (CoT), express reasoning in a linear textual form, which may appear coherent but frequently leads to inconsistent conclusions. Recent approaches rely on externally provided graphs and do not explore how LLMs can construct and use their own graph-structured reasoning, particularly in open-domain QA. To fill this gap, we novelly explore graph-structured reasoning of LLMs in general-domain question answering. We propose Self-Graph Reasoning (SGR), a framework that enables LLMs to explicitly represent their reasoning process as a structured graph before producing the final answer. We further construct a graph-structured reasoning dataset that merges multiple candidate reasoning graphs into refined graph structures for model training. Experiments on five QA benchmarks across both general and specialized domains show that SGR consistently improves reasoning consistency and yields a 17.74% gain over the base model. The LLaMA-3.3-70B model fine-tuned with SGR performs comparably to GPT-4o and surpasses Claude-3.5-Haiku, demonstrating the effectiveness of graph-structured reasoning.",
    "summary": "论文研究LLM在开放域问答中线性推理导致逻辑不一致的问题。核心方法是提出自构图推理框架，让LLM在生成最终答案前，先将推理过程显式表示为结构化图，以整合多前提并解决并行子问题。",
    "translation": "从链式到图式：通用领域大语言模型的自结构化推理",
    "relevance_score": 7,
    "reasoning": "该论文探讨LLM推理架构的演进（从链式到图式），属于'Enabling LLM Tech'范畴。这种自结构化推理技术可以增强LLM在推荐和搜索中的复杂决策能力，例如处理多路径用户意图或构建更精细的推荐逻辑图。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出让LLM自主构建图结构推理框架，直接提升LLM的推理能力，属于核心LLM技术进步，对搜索和推荐系统的复杂推理任务有直接应用潜力。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03595v1": {
    "title": "Controllable LLM Reasoning via Sparse Autoencoder-Based Steering",
    "url": "https://www.alphaxiv.org/abs/2601.03595v1",
    "arxiv_id": "2601.03595v1",
    "authors": "Yi Fang, Wenjie Wang, Mingfeng Xue, Boyi Deng, Fengli Xu, Dayiheng Liu, Fuli Feng",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2026-01-07 05:26:26",
    "ori_summary": "Large Reasoning Models (LRMs) exhibit human-like cognitive reasoning strategies (e.g. backtracking, cross-verification) during reasoning process, which improves their performance on complex tasks. Currently, reasoning strategies are autonomously selected by LRMs themselves. However, such autonomous selection often produces inefficient or even erroneous reasoning paths. To make reasoning more reliable and flexible, it is important to develop methods for controlling reasoning strategies. Existing methods struggle to control fine-grained reasoning strategies due to conceptual entanglement in LRMs' hidden states. To address this, we leverage Sparse Autoencoders (SAEs) to decompose strategy-entangled hidden states into a disentangled feature space. To identify the few strategy-specific features from the vast pool of SAE features, we propose SAE-Steering, an efficient two-stage feature identification pipeline. SAE-Steering first recalls features that amplify the logits of strategy-specific keywords, filtering out over 99\\% of features, and then ranks the remaining features by their control effectiveness. Using the identified strategy-specific features as control vectors, SAE-Steering outperforms existing methods by over 15\\% in control effectiveness. Furthermore, controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones, achieving a 7\\% absolute accuracy improvement.",
    "summary": "该论文研究如何控制大语言模型的推理策略选择问题。其核心方法是利用稀疏自编码器分解隐藏状态中的策略纠缠特征，并通过两阶段特征识别流程提取策略特定特征作为控制向量。",
    "translation": "基于稀疏自编码器的可控大语言模型推理引导",
    "relevance_score": 8,
    "reasoning": "该论文属于'核心LLM技术进展'范畴，聚焦于LLM推理过程的控制机制。稀疏自编码器方法在提升LLM推理的可控性和可解释性方面具有潜力，这对于搜索和推荐系统中的查询理解、结果解释以及个性化响应生成等应用至关重要。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出利用稀疏自编码器控制大语言模型推理策略的方法，直接属于'直接LLM应用'和'核心LLM技术'范畴，对推荐/搜索系统的可控推理有重要应用潜力。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03589v1": {
    "title": "OLA: Output Language Alignment in Code-Switched LLM Interactions",
    "url": "https://www.alphaxiv.org/abs/2601.03589v1",
    "arxiv_id": "2601.03589v1",
    "authors": "Juhyun Oh, Haneul Yoo, Faiz Ghifari Haznitrama, Alice Oh",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 05:07:22",
    "ori_summary": "Code-switching, alternating between languages within a conversation, is natural for multilingual users, yet poses fundamental challenges for large language models (LLMs). When a user code-switches in their prompt to an LLM, they typically do not specify the expected language of the LLM response, and thus LLMs must infer the output language from contextual and pragmatic cues. We find that current LLMs systematically fail to align with this expectation, responding in undesired languages even when cues are clear to humans. We introduce OLA, a benchmark to evaluate LLMs' Output Language Alignment in code-switched interactions. OLA focuses on Korean--English code-switching and spans simple intra-sentential mixing to instruction-content mismatches. Even frontier models frequently misinterpret implicit language expectation, exhibiting a bias toward non-English responses. We further show this bias generalizes beyond Korean to Chinese and Indonesian pairs. Models also show instability through mid-response switching and language intrusions. Chain-of-Thought prompting fails to resolve these errors, indicating weak pragmatic reasoning about output language. However, Code-Switching Aware DPO with minimal data (about 1K examples) substantially reduces misalignment, suggesting these failures stem from insufficient alignment rather than fundamental limitations. Our results highlight the need to align multilingual LLMs with users' implicit expectations in real-world code-switched interactions.",
    "summary": "",
    "translation": "OLA：代码切换LLM交互中的输出语言对齐",
    "relevance_score": 2,
    "reasoning": "该论文关注代码切换（多语言混合）场景下的LLM输出语言对齐，这属于语言模型交互的特定技术问题。虽然涉及LLM技术，但论文主要解决多语言处理中的语言对齐问题，与推荐系统、搜索或广告中的核心应用（如排序、召回、特征建模）没有直接关联，也不涉及Transformer架构改进或异构数据统一建模等关键方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03578v1": {
    "title": "PsychEthicsBench: Evaluating Large Language Models Against Australian Mental Health Ethics",
    "url": "https://www.alphaxiv.org/abs/2601.03578v1",
    "arxiv_id": "2601.03578v1",
    "authors": "Yaling Shen, Stephanie Fong, Yiwen Jiang, Zimu Wang, Feilong Tang, Qingyang Xu, Xiangyu Zhao, Zhongxing Xu, Jiahe Liu, Jinpeng Hu, Dominic Dwyer, Zongyuan Ge",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 04:49:02",
    "ori_summary": "The increasing integration of large language models (LLMs) into mental health applications necessitates robust frameworks for evaluating professional safety alignment. Current evaluative approaches primarily rely on refusal-based safety signals, which offer limited insight into the nuanced behaviors required in clinical practice. In mental health, clinically inadequate refusals can be perceived as unempathetic and discourage help-seeking. To address this gap, we move beyond refusal-centric metrics and introduce \\texttt{PsychEthicsBench}, the first principle-grounded benchmark based on Australian psychology and psychiatry guidelines, designed to evaluate LLMs' ethical knowledge and behavioral responses through multiple-choice and open-ended tasks with fine-grained ethicality annotations. Empirical results across 14 models reveal that refusal rates are poor indicators of ethical behavior, revealing a significant divergence between safety triggers and clinical appropriateness. Notably, we find that domain-specific fine-tuning can degrade ethical robustness, as several specialized models underperform their base backbones in ethical alignment. PsychEthicsBench provides a foundation for systematic, jurisdiction-aware evaluation of LLMs in mental health, encouraging more responsible development in this domain.",
    "summary": "",
    "translation": "PsychEthicsBench：针对澳大利亚心理健康伦理评估大型语言模型",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及伦理评估，属于明确的无关主题。论文专注于特定国家（澳大利亚）的特定领域（心理健康）伦理评估，与推荐系统、搜索、广告的技术核心进展、LLM/Transformer架构改进或直接应用完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03570v1": {
    "title": "How Do Large Language Models Learn Concepts During Continual Pre-Training?",
    "url": "https://www.alphaxiv.org/abs/2601.03570v1",
    "arxiv_id": "2601.03570v1",
    "authors": "Barry Menglong Yao, Sha Li, Yunzhi Yao, Minqian Liu, Zaishuo Xia, Qifan Wang, Lifu Huang",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 04:29:15",
    "ori_summary": "Human beings primarily understand the world through concepts (e.g., dog), abstract mental representations that structure perception, reasoning, and learning. However, how large language models (LLMs) acquire, retain, and forget such concepts during continual pretraining remains poorly understood. In this work, we study how individual concepts are acquired and forgotten, as well as how multiple concepts interact through interference and synergy. We link these behavioral dynamics to LLMs' internal Concept Circuits, computational subgraphs associated with specific concepts, and incorporate Graph Metrics to characterize circuit structure. Our analysis reveals: (1) LLMs concept circuits provide a non-trivial, statistically significant signal of concept learning and forgetting; (2) Concept circuits exhibit a stage-wise temporal pattern during continual pretraining, with an early increase followed by gradual decrease and stabilization; (3) concepts with larger learning gains tend to exhibit greater forgetting under subsequent training; (4) semantically similar concepts induce stronger interference than weakly related ones; (5) conceptual knowledge differs in their transferability, with some significantly facilitating the learning of others. Together, our findings offer a circuit-level view of concept learning dynamics and inform the design of more interpretable and robust concept-aware training strategies for LLMs.",
    "summary": "论文研究LLM在持续预训练中如何学习、遗忘和交互概念。核心方法是提出'概念电路'作为与特定概念相关的计算子图，并利用图度量分析其结构动态，以揭示概念获取、干扰和迁移的机制。",
    "translation": "大型语言模型在持续预训练中如何学习概念？",
    "relevance_score": 8,
    "reasoning": "该论文探讨LLM在持续预训练中的概念学习机制，属于'Enabling LLM Tech'范畴，对理解模型如何适应新领域知识至关重要。在推荐/搜索/广告系统中，持续学习能力可帮助模型动态整合用户行为模式、新兴商品特征或实时趋势，提升个性化服务的时效性和准确性。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文通过概念电路分析LLM在持续预训练中的概念学习动态，其核心方法（概念电路与图度量）为理解LLM内部表征演化提供了新视角，与LLM核心技术进步和可解释性应用高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03559v1": {
    "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
    "url": "https://www.alphaxiv.org/abs/2601.03559v1",
    "arxiv_id": "2601.03559v1",
    "authors": "Shidong Cao, Hongzhan Lin, Yuxuan Gu, Ziyang Luo, Jing Ma",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 03:58:42",
    "ori_summary": "Chain-of-Thought (CoT) reasoning improves multi-step mathematical problem solving in large language models but remains vulnerable to exposure bias and error accumulation, as early mistakes propagate irreversibly through autoregressive decoding. In this work, we propose DiffCoT, a diffusion-styled CoT framework that reformulates CoT reasoning as an iterative denoising process. DiffCoT integrates diffusion principles at the reasoning-step level via a sliding-window mechanism, enabling unified generation and retrospective correction of intermediate steps while preserving token-level autoregression. To maintain causal consistency, we further introduce a causal diffusion noise schedule that respects the temporal structure of reasoning chains. Extensive experiments on three multi-step CoT reasoning benchmarks across diverse model backbones demonstrate that DiffCoT consistently outperforms existing CoT preference optimization methods, yielding improved robustness and error-correction capability in CoT reasoning.",
    "summary": "",
    "translation": "DiffCoT：大型语言模型中的扩散风格思维链推理",
    "relevance_score": 3,
    "reasoning": "该论文涉及LLM推理技术（思维链），属于核心LLM技术进展，可能通过改进推理能力间接应用于推荐/搜索系统的决策过程。然而，论文标题未明确展示与推荐系统、搜索或广告的直接关联，应用潜力较为间接。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03553v1": {
    "title": "Evaluating LLMs for Police Decision-Making: A Framework Based on Police Action Scenarios",
    "url": "https://www.alphaxiv.org/abs/2601.03553v1",
    "arxiv_id": "2601.03553v1",
    "authors": "Sangyub Lee, Heedou Kim, Hyeoncheol Kim",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 03:44:12",
    "ori_summary": "The use of Large Language Models (LLMs) in police operations is growing, yet an evaluation framework tailored to police operations remains absent. While LLM's responses may not always be legally incorrect, their unverified use still can lead to severe issues such as unlawful arrests and improper evidence collection. To address this, we propose PAS (Police Action Scenarios), a systematic framework covering the entire evaluation process. Applying this framework, we constructed a novel QA dataset from over 8,000 official documents and established key metrics validated through statistical analysis with police expert judgements. Experimental results show that commercial LLMs struggle with our new police-related tasks, particularly in providing fact-based recommendations. This study highlights the necessity of an expandable evaluation framework to ensure reliable AI-driven police operations. We release our data and prompt template.",
    "summary": "",
    "translation": "评估大型语言模型在警务决策中的应用：基于警务行动场景的框架",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于警务决策这一特定领域应用，属于明确的领域特定应用（医学、生物、化学、物理等）。虽然涉及LLM评估，但其应用场景与搜索、推荐、广告等核心领域完全无关，属于明确的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03549v1": {
    "title": "EASLT: Emotion-Aware Sign Language Translation",
    "url": "https://www.alphaxiv.org/abs/2601.03549v1",
    "arxiv_id": "2601.03549v1",
    "authors": "Guobin Tu, Di Weng",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2026-01-07 03:32:28",
    "ori_summary": "Sign Language Translation (SLT) is a complex cross-modal task requiring the integration of Manual Signals (MS) and Non-Manual Signals (NMS). While recent gloss-free SLT methods have made strides in translating manual gestures, they frequently overlook the semantic criticality of facial expressions, resulting in ambiguity when distinct concepts share identical manual articulations. To address this, we present **EASLT** (**E**motion-**A**ware **S**ign **L**anguage **T**ranslation), a framework that treats facial affect not as auxiliary information, but as a robust semantic anchor. Unlike methods that relegate facial expressions to a secondary role, EASLT incorporates a dedicated emotional encoder to capture continuous affective dynamics. These representations are integrated via a novel *Emotion-Aware Fusion* (EAF) module, which adaptively recalibrates spatio-temporal sign features based on affective context to resolve semantic ambiguities. Extensive evaluations on the PHOENIX14T and CSL-Daily benchmarks demonstrate that EASLT establishes advanced performance among gloss-free methods, achieving BLEU-4 scores of 26.15 and 22.80, and BLEURT scores of 61.0 and 57.8, respectively. Ablation studies confirm that explicitly modeling emotion effectively decouples affective semantics from manual dynamics, significantly enhancing translation fidelity. Code is available at https://github.com/TuGuobin/EASLT.",
    "summary": "",
    "translation": "EASLT：情感感知手语翻译",
    "relevance_score": 1,
    "reasoning": "该论文专注于手语翻译这一特定领域，属于纯粹的视觉或多模态任务，与推荐系统、搜索或广告的核心技术领域没有直接关联。标题中提到的情感感知技术可能涉及多模态理解，但缺乏明确的连接点表明其在RecSys/Search/Ads中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03546v1": {
    "title": "Value-Action Alignment in Large Language Models under Privacy-Prosocial Conflict",
    "url": "https://www.alphaxiv.org/abs/2601.03546v1",
    "arxiv_id": "2601.03546v1",
    "authors": "Guanyu Chen, Chenxiao Yu, Xiyang Hu",
    "categories": "cs.CL, cs.AI, cs.HC, cs.LG",
    "pub_date": "2026-01-07 03:30:42",
    "ori_summary": "Large language models (LLMs) are increasingly used to simulate decision-making tasks involving personal data sharing, where privacy concerns and prosocial motivations can push choices in opposite directions. Existing evaluations often measure privacy-related attitudes or sharing intentions in isolation, which makes it difficult to determine whether a model's expressed values jointly predict its downstream data-sharing actions as in real human behaviors. We introduce a context-based assessment protocol that sequentially administers standardized questionnaires for privacy attitudes, prosocialness, and acceptance of data sharing within a bounded, history-carrying session. To evaluate value-action alignments under competing attitudes, we use multi-group structural equation modeling (MGSEM) to identify relations from privacy concerns and prosocialness to data sharing. We propose Value-Action Alignment Rate (VAAR), a human-referenced directional agreement metric that aggregates path-level evidence for expected signs. Across multiple LLMs, we observe stable but model-specific Privacy-PSA-AoDS profiles, and substantial heterogeneity in value-action alignment.",
    "summary": "",
    "translation": "隐私-亲社会冲突下大型语言模型的价值-行为对齐",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及隐私和伦理对齐，属于明确的无关主题。虽然提到了大型语言模型，但核心关注点是隐私与亲社会行为的冲突，这属于伦理、隐私等非技术性范畴，与您关注的推荐系统、搜索、广告等核心领域技术进展无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03543v1": {
    "title": "EvolMem: A Cognitive-Driven Benchmark for Multi-Session Dialogue Memory",
    "url": "https://www.alphaxiv.org/abs/2601.03543v1",
    "arxiv_id": "2601.03543v1",
    "authors": "Ye Shen, Dun Pei, Yiqiu Guo, Junying Wang, Yijin Guo, Zicheng Zhang, Qi Jia, Jun Zhou, Guangtao Zhai",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 03:14:42",
    "ori_summary": "Despite recent advances in understanding and leveraging long-range conversational memory, existing benchmarks still lack systematic evaluation of large language models(LLMs) across diverse memory dimensions, particularly in multi-session settings. In this work, we propose EvolMem, a new benchmark for assessing multi-session memory capabilities of LLMs and agent systems. EvolMem is grounded in cognitive psychology and encompasses both declarative and non-declarative memory, further decomposed into multiple fine-grained abilities. To construct the benchmark, we introduce a hybrid data synthesis framework that consists of topic-initiated generation and narrative-inspired transformations. This framework enables scalable generation of multi-session conversations with controllable complexity, accompanied by sample-specific evaluation guidelines. Extensive evaluation reveals that no LLM consistently outperforms others across all memory dimensions. Moreover, agent memory mechanisms do not necessarily enhance LLMs' capabilities and often exhibit notable efficiency limitations. Data and code will be released at https://github.com/shenye7436/EvolMem.",
    "summary": "",
    "translation": "EvolMem：面向多轮对话记忆的认知驱动基准",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于对话记忆的基准测试，属于纯粹的自然语言处理（NLP）评估领域，与推荐系统、搜索或广告的核心技术进展、LLM/Transformer架构改进或直接应用无关。尽管涉及记忆机制，但未表明其技术或方法在推荐/搜索/广告场景中有潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03542v1": {
    "title": "Layer-Order Inversion: Rethinking Latent Multi-Hop Reasoning in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2601.03542v1",
    "arxiv_id": "2601.03542v1",
    "authors": "Xukai Liu, Ye Liu, Jipeng Zhang, Yanghai Zhang, Kai Zhang, Qi Liu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 03:13:03",
    "ori_summary": "Large language models (LLMs) perform well on multi-hop reasoning, yet how they internally compose multiple facts remains unclear. Recent work proposes \\emph{hop-aligned circuit hypothesis}, suggesting that bridge entities are computed sequentially across layers before later-hop answers. Through systematic analyses on real-world multi-hop queries, we show that this hop-aligned assumption does not generalize: later-hop answer entities can become decodable earlier than bridge entities, a phenomenon we call \\emph{layer-order inversion}, which strengthens with total hops. To explain this behavior, we propose a \\emph{probabilistic recall-and-extract} framework that models multi-hop reasoning as broad probabilistic recall in shallow MLP layers followed by selective extraction in deeper attention layers. This framework is empirically validated through systematic probing analyses, reinterpreting prior layer-wise decoding evidence, explaining chain-of-thought gains, and providing a mechanistic diagnosis of multi-hop failures despite correct single-hop knowledge. Code is available at https://github.com/laquabe/Layer-Order-Inversion.",
    "summary": "该论文研究LLM内部如何进行多跳事实推理的核心问题。其核心思想是提出概率召回与提取框架，认为浅层MLP进行广泛概率召回，深层注意力层进行选择性提取，从而解释层序反转现象。",
    "translation": "层序反转：重新思考大型语言模型中的潜在多跳推理",
    "relevance_score": 8,
    "reasoning": "该论文探讨LLM内部推理机制（多跳推理），属于'核心LLM技术进展'范畴，对理解LLM在搜索/推荐中的复杂推理能力（如多步查询理解、用户意图分解）有直接应用潜力。层序反转可能揭示更高效的推理架构，可类比推荐系统中的序列建模优化。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接研究LLM内部推理机制，其提出的概率召回与提取框架对理解LLM在推荐、搜索等任务中的多跳推理行为具有核心价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03540v1": {
    "title": "DeepSynth-Eval: Objectively Evaluating Information Consolidation in Deep Survey Writing",
    "url": "https://www.alphaxiv.org/abs/2601.03540v1",
    "arxiv_id": "2601.03540v1",
    "authors": "Hongzhi Zhang, Yuanze Hu, Tinghai Zhang, Jia Fu, Tao Wang, Junwei Jing, Zhaoxin Fan, Qi Wang, Ruiming Tang, Han Li, Guorui Zhou, Kun Gai",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 03:07:52",
    "ori_summary": "The evolution of Large Language Models (LLMs) towards autonomous agents has catalyzed progress in Deep Research. While retrieval capabilities are well-benchmarked, the post-retrieval synthesis stage--where agents must digest massive amounts of context and consolidate fragmented evidence into coherent, long-form reports--remains under-evaluated due to the subjectivity of open-ended writing. To bridge this gap, we introduce DeepSynth-Eval, a benchmark designed to objectively evaluate information consolidation capabilities. We leverage high-quality survey papers as gold standards, reverse-engineering research requests and constructing \"Oracle Contexts\" from their bibliographies to isolate synthesis from retrieval noise. We propose a fine-grained evaluation protocol using General Checklists (for factual coverage) and Constraint Checklists (for structural organization), transforming subjective judgment into verifiable metrics. Experiments across 96 tasks reveal that synthesizing information from hundreds of references remains a significant challenge. Our results demonstrate that agentic plan-and-write workflows significantly outperform single-turn generation, effectively reducing hallucinations and improving adherence to complex structural constraints.",
    "summary": "",
    "translation": "DeepSynth-Eval：深度综述写作中信息整合的客观评估",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于评估深度综述写作中的信息整合，这属于内容生成和评估的范畴，与AIGC、内容生成等纯LLM中心话题相关。论文没有涉及推荐系统、搜索或广告的核心进展、使能技术或直接应用，也没有展示与异构数据统一建模的明显关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03537v1": {
    "title": "STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules",
    "url": "https://www.alphaxiv.org/abs/2601.03537v1",
    "arxiv_id": "2601.03537v1",
    "authors": "Di Wu, Yanyan Zhao, Xin Lu, Mingzhe Li, Bing Qin",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2026-01-07 03:06:55",
    "ori_summary": "Defending against jailbreak attacks is crucial for the safe deployment of Large Language Models (LLMs). Recent research has attempted to improve safety by training models to reason over safety rules before responding. However, a key issue lies in determining what form of safety reasoning effectively defends against jailbreak attacks, which is difficult to explicitly design or directly obtain. To address this, we propose \\textbf{STAR-S} (\\textbf{S}elf-\\textbf{TA}ught \\textbf{R}easoning based on \\textbf{S}afety rules), a framework that integrates the learning of safety rule reasoning into a self-taught loop. The core of STAR-S involves eliciting reasoning and reflection guided by safety rules, then leveraging fine-tuning to enhance safety reasoning. Repeating this process creates a synergistic cycle. Improvements in the model's reasoning and interpretation of safety rules allow it to produce better reasoning data under safety rule prompts, which is then utilized for further training. Experiments show that STAR-S effectively defends against jailbreak attacks, outperforming baselines. Code is available at: https://github.com/pikepokenew/STAR_S.git.",
    "summary": "",
    "translation": "STAR-S：通过基于安全规则的自学推理提升安全对齐",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于LLM的安全对齐，属于安全、伦理等非技术性话题范畴，已被明确列为不相关主题。虽然涉及LLM技术，但其核心关注点（安全对齐）与推荐系统、搜索或广告领域的技术进展、架构创新或直接应用无关，无法建立与当前关注领域的有效联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03534v1": {
    "title": "Persona-aware and Explainable Bikeability Assessment: A Vision-Language Model Approach",
    "url": "https://www.alphaxiv.org/abs/2601.03534v1",
    "arxiv_id": "2601.03534v1",
    "authors": "Yilong Dai, Ziyi Wang, Chenguang Wang, Kexin Zhou, Yiheng Qian, Susu Xu, Xiang Yan",
    "categories": "cs.CL, cs.CV, cs.HC, cs.LG",
    "pub_date": "2026-01-07 02:46:51",
    "ori_summary": "Bikeability assessment is essential for advancing sustainable urban transportation and creating cyclist-friendly cities, and it requires incorporating users' perceptions of safety and comfort. Yet existing perception-based bikeability assessment approaches face key limitations in capturing the complexity of road environments and adequately accounting for heterogeneity in subjective user perceptions. This paper proposes a persona-aware Vision-Language Model framework for bikeability assessment with three novel contributions: (i) theory-grounded persona conditioning based on established cyclist typology that generates persona-specific explanations via chain-of-thought reasoning; (ii) multi-granularity supervised fine-tuning that combines scarce expert-annotated reasoning with abundant user ratings for joint prediction and explainable assessment; and (iii) AI-enabled data augmentation that creates controlled paired data to isolate infrastructure variable impacts. To test and validate this framework, we developed a panoramic image-based crowdsourcing system and collected 12,400 persona-conditioned assessments from 427 cyclists. Experiment results show that the proposed framework offers competitive bikeability rating prediction while uniquely enabling explainable factor attribution.",
    "summary": "",
    "translation": "基于角色感知与可解释性的骑行适宜度评估：一种视觉-语言模型方法",
    "relevance_score": 2,
    "reasoning": "该论文虽然涉及视觉-语言模型（VLM），但其核心应用领域是城市交通/骑行评估，而非搜索、推荐或广告系统。论文标题中的“Persona-aware”可能涉及用户建模，但整体应用场景与指定领域无关，且未明确展示如何将VLM技术应用于异构数据处理或推荐系统。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03531v1": {
    "title": "PALM-Bench: A Comprehensive Benchmark for Personalized Audio-Language Models",
    "url": "https://www.alphaxiv.org/abs/2601.03531v1",
    "arxiv_id": "2601.03531v1",
    "authors": "Yuwen Wang, Xinyuan Qian, Tian-Hao Zhang, Jiaran Gao, Yuchen Pan, Xin Wang, Zhou Pan, Chen Wei, Yiming Wang",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 02:44:38",
    "ori_summary": "Large Audio-Language Models (LALMs) have demonstrated strong performance in audio understanding and generation. Yet, our extensive benchmarking reveals that their behavior is largely generic (e.g., summarizing spoken content) and fails to adequately support personalized question answering (e.g., summarizing what my best friend says). In contrast, human conditions their interpretation and decision-making on each individual's personal context. To bridge this gap, we formalize the task of Personalized LALMs (PALM) for recognizing personal concepts and reasoning within personal context. Moreover, we create the first benchmark (PALM-Bench) to foster the methodological advances in PALM and enable structured evaluation on several tasks across multi-speaker scenarios. Our extensive experiments on representative open-source LALMs, show that existing training-free prompting and supervised fine-tuning strategies, while yield improvements, remains limited in modeling personalized knowledge and transferring them across tasks robustly. Data and code will be released.",
    "summary": "",
    "translation": "PALM-Bench：个性化音频-语言模型综合基准",
    "relevance_score": 2,
    "reasoning": "该论文专注于音频-语言模型的个性化基准测试，属于多模态评估范畴。虽然个性化是推荐系统的核心要素，但音频模态与当前关注的搜索、推荐、广告领域相关性较弱，且缺乏明确的Transformer架构或LLM技术应用说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03515v1": {
    "title": "Mem-Gallery: Benchmarking Multimodal Long-Term Conversational Memory for MLLM Agents",
    "url": "https://www.alphaxiv.org/abs/2601.03515v1",
    "arxiv_id": "2601.03515v1",
    "authors": "Yuanchen Bei, Tianxin Wei, Xuying Ning, Yanjun Zhao, Zhining Liu, Xiao Lin, Yada Zhu, Hendrik Hamann, Jingrui He, Hanghang Tong",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 02:03:13",
    "ori_summary": "Long-term memory is a critical capability for multimodal large language model (MLLM) agents, particularly in conversational settings where information accumulates and evolves over time. However, existing benchmarks either evaluate multi-session memory in text-only conversations or assess multimodal understanding within localized contexts, failing to evaluate how multimodal memory is preserved, organized, and evolved across long-term conversational trajectories. Thus, we introduce Mem-Gallery, a new benchmark for evaluating multimodal long-term conversational memory in MLLM agents. Mem-Gallery features high-quality multi-session conversations grounded in both visual and textual information, with long interaction horizons and rich multimodal dependencies. Building on this dataset, we propose a systematic evaluation framework that assesses key memory capabilities along three functional dimensions: memory extraction and test-time adaptation, memory reasoning, and memory knowledge management. Extensive benchmarking across thirteen memory systems reveals several key findings, highlighting the necessity of explicit multimodal information retention and memory organization, the persistent limitations in memory reasoning and knowledge management, as well as the efficiency bottleneck of current models.",
    "summary": "",
    "translation": "Mem-Gallery：面向多模态大语言模型智能体的多模态长时对话记忆基准测试",
    "relevance_score": 2,
    "reasoning": "该论文标题主要涉及多模态大语言模型的对话记忆基准测试，属于纯粹的LLM评估基准范畴，与您关注的推荐系统、搜索或广告核心进展无关。虽然提到了多模态和长时记忆，但缺乏将这些技术应用于推荐/搜索/广告领域的明确潜力说明，因此相关性很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03511v1": {
    "title": "IntroLM: Introspective Language Models via Prefilling-Time Self-Evaluation",
    "url": "https://www.alphaxiv.org/abs/2601.03511v1",
    "arxiv_id": "2601.03511v1",
    "authors": "Hossein Hosseini Kasnavieh, Gholamreza Haffari, Chris Leckie, Adel N. Toosi",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2026-01-07 01:48:17",
    "ori_summary": "A major challenge for the operation of large language models (LLMs) is how to predict whether a specific LLM will produce sufficiently high-quality output for a given query. Existing approaches rely on external classifiers, most commonly BERT based models, which suffer from limited context windows, constrained representational capacity, and additional computational overhead. We propose IntroLM, a method that enables causal language models to predict their own output quality during the prefilling phase without affecting generation using introspective tokens. By introducing token conditional LoRA that activates only for the introspective token, the model learns to predict the output quality for a given query while preserving the original backbone behavior and avoiding external evaluators. On question answering benchmarks, IntroLM applied to Qwen3 8B achieves a ROC AUC of 90 precent for success prediction, outperforming a DeBERTa classifier by 14 precent. When integrated into multi model routing systems, IntroLM achieves superior cost performance tradeoffs, reducing latency by up to 33 precent and large model usage by up to 50 precent at matched reliability.",
    "summary": "该论文研究如何预测大型语言模型对特定查询的输出质量。其核心方法是让因果语言模型在预填充阶段通过引入仅对自省令牌激活的token conditional LoRA，在不影响生成的情况下预测自身输出质量，从而避免依赖外部评估器。",
    "translation": "IntroLM：通过预填充时自评估实现自省式语言模型",
    "relevance_score": 7,
    "reasoning": "该论文属于'赋能LLM技术'范畴，研究语言模型的自评估机制，这是提升LLM可靠性和可控性的核心进展。在推荐系统、搜索和广告应用中，这种自评估能力可用于提高推荐结果的可解释性、过滤低质量内容，或优化查询理解与响应生成的质量控制。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的IntroLM方法通过自评估机制预测LLM输出质量，直接应用于多模型路由系统优化成本与延迟，与推荐/搜索系统中的模型选择与资源分配高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03506v1": {
    "title": "Reasoning Pattern Alignment Merging for Adaptive Reasoning",
    "url": "https://www.alphaxiv.org/abs/2601.03506v1",
    "arxiv_id": "2601.03506v1",
    "authors": "Zhaofeng Zhong, Wei Yuan, Tong Chen, Xiangyu Zhao, Quoc Viet Hung Nguyen, Hongzhi Yin",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 01:36:39",
    "ori_summary": "Recent large reasoning models (LRMs) have made substantial progress in complex reasoning tasks, yet they often generate lengthy reasoning paths for every query, incurring unnecessary computation and latency. Existing speed-up approaches typically rely on retraining the model or designing sophisticated prompting, which are either prohibitively expensive or highly sensitive to the input and prompt formulation. In this work, we study model merging as a lightweight alternative for efficient reasoning: by combining a long chain-of-thought (Long-CoT) reasoning model with a Short-CoT instruction model, we obtain an adaptive reasoner without training from scratch or requiring large-scale additional data. Building on this idea, we propose Reasoning Pattern Alignment Merging (RPAM), a layer-wise model merging framework based on feature alignment to facilitate query-adaptive reasoning. RPAM first constructs a small pattern-labeled calibration set that assigns each query an appropriate reasoning pattern. It then optimizes layer-wise merging coefficients by aligning the merged model's intermediate representations with those of the selected model, while a contrastive objective explicitly pushes them away from the non-selected model. Experiments on seven widely used reasoning benchmarks show that RPAM substantially reduces inference cost while maintaining strong performance. Upon article acceptance, we will provide open-source code to reproduce experiments for RPAM.",
    "summary": "",
    "translation": "自适应推理的推理模式对齐融合",
    "relevance_score": 3,
    "reasoning": "该标题涉及推理模式对齐与自适应推理，可能属于LLM推理优化技术，与'Enabling LLM Tech'有一定关联。然而，标题过于宽泛，未明确说明具体技术或其在推荐/搜索/广告领域的应用潜力，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03505v1": {
    "title": "Beyond Perplexity: A Lightweight Benchmark for Knowledge Retention in Supervised Fine-Tuning",
    "url": "https://www.alphaxiv.org/abs/2601.03505v1",
    "arxiv_id": "2601.03505v1",
    "authors": "Soheil Zibakhsh Shabgahi, Pedram Aghazadeh, Farinaz Koushanfar",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 01:34:28",
    "ori_summary": "Supervised Fine-Tuning (SFT) is a standard approach for injecting domain knowledge into Large Language Models (LLMs). However, relying on validation perplexity to monitor training is often insufficient, as it confounds stylistic mimicry with genuine factual internalization. To address this, we introduce the Knowledge Retention (KR) Test , a lightweight, corpus-grounded evaluation framework designed to distinguish factual learning from linguistics. KR-Test utilizes automatically generated contrastive examples to measure likelihood preferences for correct versus incorrect continuations, requiring no instruction tuning or generative decoding. We validate the framework's integrity through a \"blind vs. oracle\" baseline analysis. Furthermore, we demonstrate the diagnostic capabilities of KR-Test by analyzing the training dynamics of Low-Rank Adaptation (LoRA). By exposing the fine-grained dissociation between linguistic convergence and knowledge retention, KR-Test enhances the interpretability of fine-tuning dynamics.",
    "summary": "",
    "translation": "超越困惑度：监督微调中知识保持的轻量级基准",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM评估基准（困惑度替代方案）和知识保持，属于纯粹的LLM评估方法研究。虽然涉及监督微调，但核心是评估而非应用，没有明确展示在推荐系统、搜索或广告中的潜在应用价值，因此与当前关注点相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03493v1": {
    "title": "Submodular Evaluation Subset Selection in Automatic Prompt Optimization",
    "url": "https://www.alphaxiv.org/abs/2601.03493v1",
    "arxiv_id": "2601.03493v1",
    "authors": "Jinming Nian, Zhiyuan Peng, Hongwei Shang, Dae Hoon Park, Yi Fang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-07 01:12:45",
    "ori_summary": "Automatic prompt optimization reduces manual prompt engineering, but relies on task performance measured on a small, often randomly sampled evaluation subset as its main source of feedback signal. Despite this, how to select that evaluation subset is usually treated as an implementation detail. We study evaluation subset selection for prompt optimization from a principled perspective and propose SESS, a submodular evaluation subset selection method. We frame selection as maximizing an objective set function and show that, under mild conditions, it is monotone and submodular, enabling greedy selection with theoretical guarantees. Across GSM8K, MATH, and GPQA-Diamond, submodularly selected evaluation subsets can yield better optimized prompts than random or heuristic baselines.",
    "summary": "",
    "translation": "自动提示优化中的子模评估子集选择",
    "relevance_score": 3,
    "reasoning": "该论文涉及提示优化技术，属于LLM应用范畴，但主要关注自动提示优化的评估子集选择方法，与推荐系统、搜索或广告的直接应用关联较弱。虽然子模优化在推荐系统中可能有潜在应用（如内容选择），但论文标题未明确表明这种跨领域应用，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03483v1": {
    "title": "CALM: Culturally Self-Aware Language Models",
    "url": "https://www.alphaxiv.org/abs/2601.03483v1",
    "arxiv_id": "2601.03483v1",
    "authors": "Lingzhi Shen, Xiaohao Cai, Yunfei Long, Imran Razzak, Guanming Chen, Shoaib Jameel",
    "categories": "cs.CL, cs.CY, cs.LG",
    "pub_date": "2026-01-07 00:28:33",
    "ori_summary": "Cultural awareness in language models is the capacity to understand and adapt to diverse cultural contexts. However, most existing approaches treat culture as static background knowledge, overlooking its dynamic and evolving nature. This limitation reduces their reliability in downstream tasks that demand genuine cultural sensitivity. In this work, we introduce CALM, a novel framework designed to endow language models with cultural self-awareness. CALM disentangles task semantics from explicit cultural concepts and latent cultural signals, shaping them into structured cultural clusters through contrastive learning. These clusters are then aligned via cross-attention to establish fine-grained interactions among related cultural features and are adaptively integrated through a Mixture-of-Experts mechanism along culture-specific dimensions. The resulting unified representation is fused with the model's original knowledge to construct a culturally grounded internal identity state, which is further enhanced through self-prompted reflective learning, enabling continual adaptation and self-correction. Extensive experiments conducted on multiple cross-cultural benchmark datasets demonstrate that CALM consistently outperforms state-of-the-art methods.",
    "summary": "",
    "translation": "CALM：具备文化自我意识的语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注语言模型的文化意识，这属于模型能力或评估范畴，与您关注的推荐系统、搜索、广告领域的核心进展、Transformer架构效率提升或直接应用无关。虽然文化意识可能在某些特定场景下影响内容推荐，但论文标题未表明其与您关注的技术方向（如架构创新、多模态建模、推荐算法等）有直接联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03481v1": {
    "title": "Self-Explaining Hate Speech Detection with Moral Rationales",
    "url": "https://www.alphaxiv.org/abs/2601.03481v1",
    "arxiv_id": "2601.03481v1",
    "authors": "Francielle Vargas, Jackson Trager, Diego Alves, Surendrabikram Thapa, Matteo Guida, Berk Atil, Daryna Dementieva, Andrew Smart, Ameeta Agrawal",
    "categories": "cs.CL",
    "pub_date": "2026-01-07 00:17:16",
    "ori_summary": "Hate speech detection models rely on surface-level lexical features, increasing vulnerability to spurious correlations and limiting robustness, cultural contextualization, and interpretability. We propose Supervised Moral Rationale Attention (SMRA), the first self-explaining hate speech detection framework to incorporate moral rationales as direct supervision for attention alignment. Based on Moral Foundations Theory, SMRA aligns token-level attention with expert-annotated moral rationales, guiding models to attend to morally salient spans rather than spurious lexical patterns. Unlike prior rationale-supervised or post-hoc approaches, SMRA integrates moral rationale supervision directly into the training objective, producing inherently interpretable and contextualized explanations. To support our framework, we also introduce HateBRMoralXplain, a Brazilian Portuguese benchmark dataset annotated with hate labels, moral categories, token-level moral rationales, and socio-political metadata. Across binary hate speech detection and multi-label moral sentiment classification, SMRA consistently improves performance (e.g., +0.9 and +1.5 F1, respectively) while substantially enhancing explanation faithfulness, increasing IoU F1 (+7.4 pp) and Token F1 (+5.0 pp). Although explanations become more concise, sufficiency improves (+2.3 pp) and fairness remains stable, indicating more faithful rationales without performance or bias trade-offs",
    "summary": "",
    "translation": "基于道德推理的自解释仇恨言论检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于仇恨言论检测，这属于内容安全/审核领域，与推荐系统、搜索或广告的核心技术（如排序、检索、用户建模）无关。虽然可能涉及文本分类，但缺乏明确的RecSys/Search/Ads应用场景，且属于被排除的“非技术性主题”（如伦理/安全）。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04194v1": {
    "title": "Choreographing a World of Dynamic Objects",
    "url": "https://www.alphaxiv.org/abs/2601.04194v1",
    "arxiv_id": "2601.04194v1",
    "authors": "Yanzhe Lyu, Chen Geng, Karthik Dharmarajan, Yunzhi Zhang, Hadi Alzayer, Shangzhe Wu, Jiajun Wu",
    "categories": "cs.CV, cs.GR, cs.RO",
    "pub_date": "2026-01-07 18:59:40",
    "ori_summary": "Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord",
    "summary": "",
    "translation": "编排动态对象世界",
    "relevance_score": 1,
    "reasoning": "该标题涉及动态对象编排，可能属于机器人学、物理模拟或游戏AI领域，与推荐系统、搜索或广告的核心技术无直接关联。标题未提及任何与LLM、Transformer架构、多模态建模或推荐/搜索/广告应用相关的关键词，因此判断为不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04185v1": {
    "title": "ImLoc: Revisiting Visual Localization with Image-based Representation",
    "url": "https://www.alphaxiv.org/abs/2601.04185v1",
    "arxiv_id": "2601.04185v1",
    "authors": "Xudong Jiang, Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Marc Pollefeys",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 18:51:51",
    "ori_summary": "Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update. In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure. Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions. With compact compression and a GPU-accelerated LO-RANSAC implementation, the whole pipeline is efficient in both storage and computation and allows for a flexible trade-off between accuracy and highest memory efficiency. Our method achieves a new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes. Code will be available at https://github.com/cvg/Hierarchical-Localization.",
    "summary": "",
    "translation": "ImLoc：基于图像表示的视觉定位方法再探",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向视觉定位（Visual Localization），属于计算机视觉领域，专注于从图像中确定相机位置。虽然标题中提到“图像表示”，但这与推荐系统、搜索或广告中的异构数据处理没有直接关联。该研究缺乏明确的跨模态建模或对推荐/搜索/广告应用的潜在适用性说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04163v1": {
    "title": "Scanner-Induced Domain Shifts Undermine the Robustness of Pathology Foundation Models",
    "url": "https://www.alphaxiv.org/abs/2601.04163v1",
    "arxiv_id": "2601.04163v1",
    "authors": "Erik Thiringer, Fredrik K. Gustafsson, Kajsa Ledesma Eriksson, Mattias Rantalainen",
    "categories": "eess.IV, cs.CV, cs.LG",
    "pub_date": "2026-01-07 18:24:12",
    "ori_summary": "Pathology foundation models (PFMs) have become central to computational pathology, aiming to offer general encoders for feature extraction from whole-slide images (WSIs). Despite strong benchmark performance, PFM robustness to real-world technical domain shifts, such as variability from whole-slide scanner devices, remains poorly understood. We systematically evaluated the robustness of 14 PFMs to scanner-induced variability, including state-of-the-art models, earlier self-supervised models, and a baseline trained on natural images. Using a multiscanner dataset of 384 breast cancer WSIs scanned on five devices, we isolated scanner effects independently from biological and laboratory confounders. Robustness is assessed via complementary unsupervised embedding analyses and a set of clinicopathological supervised prediction tasks. Our results demonstrate that current PFMs are not invariant to scanner-induced domain shifts. Most models encode pronounced scanner-specific variability in their embedding spaces. While AUC often remains stable, this masks a critical failure mode: scanner variability systematically alters the embedding space and impacts calibration of downstream model predictions, resulting in scanner-dependent bias that can impact reliability in clinical use cases. We further show that robustness is not a simple function of training data scale, model size, or model recency. None of the models provided reliable robustness against scanner-induced variability. While the models trained on the most diverse data, here represented by vision-language models, appear to have an advantage with respect to robustness, they underperformed on downstream supervised tasks. We conclude that development and evaluation of PFMs requires moving beyond accuracy-centric benchmarks toward explicit evaluation and optimisation of embedding stability and calibration under realistic acquisition variability.",
    "summary": "",
    "translation": "扫描仪诱导的领域偏移削弱了病理学基础模型的鲁棒性",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学/病理学领域的特定应用，属于明确的无关主题范畴。虽然提到了'基础模型'这一通用术语，但上下文完全限定在医疗领域，与推荐系统、搜索或广告技术无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04159v1": {
    "title": "ToTMNet: FFT-Accelerated Toeplitz Temporal Mixing Network for Lightweight Remote Photoplethysmography",
    "url": "https://www.alphaxiv.org/abs/2601.04159v1",
    "arxiv_id": "2601.04159v1",
    "authors": "Vladimir Frants, Sos Agaian, Karen Panetta",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 18:15:09",
    "ori_summary": "Remote photoplethysmography (rPPG) estimates a blood volume pulse (BVP) waveform from facial videos captured by commodity cameras. Although recent deep models improve robustness compared to classical signal-processing approaches, many methods increase computational cost and parameter count, and attention-based temporal modeling introduces quadratic scaling with respect to the temporal length. This paper proposes ToTMNet, a lightweight rPPG architecture that replaces temporal attention with an FFT-accelerated Toeplitz temporal mixing layer. The Toeplitz operator provides full-sequence temporal receptive field using a linear number of parameters in the clip length and can be applied in near-linear time using circulant embedding and FFT-based convolution. ToTMNet integrates the global Toeplitz temporal operator into a compact gated temporal mixer that combines a local depthwise temporal convolution branch with gated global Toeplitz mixing, enabling efficient long-range temporal filtering while only having 63k parameters. Experiments on two datasets, UBFC-rPPG (real videos) and SCAMPS (synthetic videos), show that ToTMNet achieves strong heart-rate estimation accuracy with a compact design. On UBFC-rPPG intra-dataset evaluation, ToTMNet reaches 1.055 bpm MAE with Pearson correlation 0.996. In a synthetic-to-real setting (SCAMPS to UBFC-rPPG), ToTMNet reaches 1.582 bpm MAE with Pearson correlation 0.994. Ablation results confirm that the gating mechanism is important for effectively using global Toeplitz mixing, especially under domain shift. The main limitation of this preprint study is the use of only two datasets; nevertheless, the results indicate that Toeplitz-structured temporal mixing is a practical and efficient alternative to attention for rPPG.",
    "summary": "",
    "translation": "ToTMNet：基于FFT加速的Toeplitz时序混合网络，用于轻量级远程光电容积描记术",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向远程光电容积描记术（rPPG）这一生物医学信号处理领域，属于医疗/生物医学应用范畴，与RecSys/Search/Ads的核心技术领域完全无关。论文中提到的FFT加速和Toeplitz结构虽然涉及计算优化，但整体应用场景被严格限定在医疗健康监测方向，没有任何潜在的应用于推荐系统、搜索或广告的可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04153v1": {
    "title": "Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning",
    "url": "https://www.alphaxiv.org/abs/2601.04153v1",
    "arxiv_id": "2601.04153v1",
    "authors": "Yifan Wang, Yanyu Li, Sergey Tulyakov, Yun Fu, Anil Kag",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 18:05:08",
    "ori_summary": "Direct Preference Optimization (DPO) has recently improved Text-to-Video (T2V) generation by enhancing visual fidelity and text alignment. However, current methods rely on non-differentiable preference signals from human annotations or learned reward models. This reliance makes training label-intensive, bias-prone, and easy-to-game, which often triggers reward hacking and unstable training. We propose Diffusion-DRF, a differentiable reward flow for fine-tuning video diffusion models using a frozen, off-the-shelf Vision-Language Model (VLM) as a training-free critic. Diffusion-DRF directly backpropagates VLM feedback through the diffusion denoising chain, converting logit-level responses into token-aware gradients for optimization. We propose an automated, aspect-structured prompting pipeline to obtain reliable multi-dimensional VLM feedback, while gradient checkpointing enables efficient updates through the final denoising steps. Diffusion-DRF improves video quality and semantic alignment while mitigating reward hacking and collapse -- without additional reward models or preference datasets. It is model-agnostic and readily generalizes to other diffusion-based generative tasks.",
    "summary": "",
    "translation": "Diffusion-DRF：用于视频扩散模型微分的可微分奖励流",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频生成领域的扩散模型微调技术，属于纯粹的视觉内容生成范畴。虽然提到了奖励机制，但未建立与推荐系统、搜索或广告中排序、匹配、用户建模等核心问题的明确联系。论文主题更接近AIGC/内容生成领域，而非用户行为建模或信息检索技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04151v1": {
    "title": "Klear: Unified Multi-Task Audio-Video Joint Generation",
    "url": "https://www.alphaxiv.org/abs/2601.04151v1",
    "arxiv_id": "2601.04151v1",
    "authors": "Jun Wang, Chunyu Qiang, Yuxin Guo, Yiran Wang, Xijuan Zeng, Chen Zhang, Pengfei Wan",
    "categories": "cs.CV, cs.AI, cs.MM, cs.SD",
    "pub_date": "2026-01-07 18:03:45",
    "ori_summary": "Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.",
    "summary": "",
    "translation": "Klear：统一的多任务音视频联合生成",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于音视频联合生成，属于AIGC/内容生成领域，与用户指定的“Irrelevant Topics”中“AIGC, Content generation, Summarization, or other purely LLM-centric topics”直接冲突。尽管标题包含“Unified Multi-Task”，但其核心是音视频生成，与推荐系统、搜索或广告中的排序、检索、用户建模等核心任务没有明确关联，也未提及任何可能应用于这些领域的技术路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04137v1": {
    "title": "Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test",
    "url": "https://www.alphaxiv.org/abs/2601.04137v1",
    "arxiv_id": "2601.04137v1",
    "authors": "Chun-Kai Fan, Xiaowei Chi, Xiaozhu Ju, Hao Li, Yong Bao, Yu-Kai Wang, Lizhang Chen, Zhiyuan Jiang, Kuangzhi Ge, Ying Li, Weishi Mi, Qingpo Wuwu, Peidong Jia, Yulin Luo, Kevin Zhang, Zhiyuan Qin, Yong Dai, Sirui Han, Yike Guo, Shanghang Zhang, Jian Tang",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2026-01-07 17:50:37",
    "ori_summary": "As world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation. However, before exploring these downstream tasks, video foundation models still have two critical questions unanswered: (1) whether their generative generalization is sufficient to maintain perceptual fidelity in the eyes of human observers, and (2) whether they are robust enough to serve as a universal prior for real-world embodied agents. To provide a standardized framework for answering these questions, we introduce the Embodied Turing Test benchmark: WoW-World-Eval (Wow,wo,val). Building upon 609 robot manipulation data, Wow-wo-val examines five core abilities, including perception, planning, prediction, generalization, and execution. We propose a comprehensive evaluation protocol with 22 metrics to assess the models' generation ability, which achieves a high Pearson Correlation between the overall score and human preference (>0.93) and establishes a reliable foundation for the Human Turing Test. On Wow-wo-val, models achieve only 17.27 on long-horizon planning and at best 68.02 on physical consistency, indicating limited spatiotemporal consistency and physical reasoning. For the Inverse Dynamic Model Turing Test, we first use an IDM to evaluate the video foundation models' execution accuracy in the real world. However, most models collapse to $\\approx$ 0% success, while WoW maintains a 40.74% success rate. These findings point to a noticeable gap between the generated videos and the real world, highlighting the urgency and necessity of benchmarking World Model in Embodied AI.",
    "summary": "",
    "translation": "哇，我，验证！一个全面的具身世界模型评估图灵测试",
    "relevance_score": 1,
    "reasoning": "这篇论文标题表明它专注于具身世界模型的评估和图灵测试，这属于机器人学或具身AI领域，与推荐系统、搜索或广告没有直接关联。标题中提到的'具身世界模型'和'图灵测试'表明这是关于智能体在物理环境中交互能力的评估，没有显示出对推荐系统、搜索或广告的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04127v1": {
    "title": "Pixel-Wise Multimodal Contrastive Learning for Remote Sensing Images",
    "url": "https://www.alphaxiv.org/abs/2601.04127v1",
    "arxiv_id": "2601.04127v1",
    "authors": "Leandro Stival, Ricardo da Silva Torres, Helio Pedrini",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-07 17:41:11",
    "ori_summary": "Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimensional (2D) representations to encode visual property variations from SITS more effectively. Specifically, we generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, and SAVI) as an alternative to using raw pixel values, creating more informative representations. Additionally, we introduce PIxel-wise Multimodal Contrastive (PIMC), a new multimodal self-supervision approach that produces effective encoders based on two-dimensional pixel time series representations and remote sensing imagery (RSI). To validate our approach, we assess its performance on three downstream tasks: pixel-level forecasting and classification using the PASTIS dataset, and land cover classification on the EuroSAT dataset. Moreover, we compare our results to state-of-the-art (SOTA) methods on all downstream tasks. Our experimental results show that the use of 2D representations significantly enhances feature extraction from SITS, while contrastive learning improves the quality of representations for both pixel time series and RSI. These findings suggest that our multimodal method outperforms existing models in various Earth observation tasks, establishing it as a robust self-supervision framework for processing both SITS and RSI. Code avaliable on",
    "summary": "",
    "translation": "遥感图像的像素级多模态对比学习",
    "relevance_score": 2,
    "reasoning": "该论文主要关注遥感图像的多模态对比学习，这属于计算机视觉领域，与您的核心关注点（推荐系统、搜索、广告）的直接关联性较弱。虽然多模态学习的概念可能启发异构数据处理，但该论文的具体应用场景（遥感）和技术重点（像素级处理）与您关注的领域相距甚远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04121v1": {
    "title": "MORPHFED: Federated Learning for Cross-institutional Blood Morphology Analysis",
    "url": "https://www.alphaxiv.org/abs/2601.04121v1",
    "arxiv_id": "2601.04121v1",
    "authors": "Gabriel Ansah, Eden Ruffell, Delmiro Fernandez-Reyes, Petru Manescu",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2026-01-07 17:32:24",
    "ori_summary": "Automated blood morphology analysis can support hematological diagnostics in low- and middle-income countries (LMICs) but remains sensitive to dataset shifts from staining variability, imaging differences, and rare morphologies. Building centralized datasets to capture this diversity is often infeasible due to privacy regulations and data-sharing restrictions. We introduce a federated learning framework for white blood cell morphology analysis that enables collaborative training across institutions without exchanging training data. Using blood films from multiple clinical sites, our federated models learn robust, domain-invariant representations while preserving complete data privacy. Evaluations across convolutional and transformer-based architectures show that federated training achieves strong cross-site performance and improved generalization to unseen institutions compared to centralized training. These findings highlight federated learning as a practical and privacy-preserving approach for developing equitable, scalable, and generalizable medical imaging AI in resource-limited healthcare environments.",
    "summary": "",
    "translation": "MORPHFED：用于跨机构血液形态学分析的联邦学习",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及联邦学习，这属于明确列出的无关主题。虽然标题包含“分析”一词，但具体应用领域是血液形态学，属于医学/生物学领域，同样属于无关主题。该论文与推荐系统、搜索、广告或相关使能技术没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04118v1": {
    "title": "GeoReason: Aligning Thinking And Answering In Remote Sensing Vision-Language Models Via Logical Consistency Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2601.04118v1",
    "arxiv_id": "2601.04118v1",
    "authors": "Wenshuai Li, Xiantai Xiang, Zixiao Wen, Guangyao Zhou, Ben Niu, Feng Wang, Lijia Huang, Qiantong Wang, Yuxin Hu",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 17:26:41",
    "ori_summary": "The evolution of Remote Sensing Vision-Language Models(RS-VLMs) emphasizes the importance of transitioning from perception-centric recognition toward high-level deductive reasoning to enhance cognitive reliability in complex spatial tasks. However, current models often suffer from logical hallucinations, where correct answers are derived from flawed reasoning chains or rely on positional shortcuts rather than spatial logic. This decoupling undermines reliability in strategic spatial decision-making. To address this, we present GeoReason, a framework designed to synchronize internal thinking with final decisions. We first construct GeoReason-Bench, a logic-driven dataset containing 4,000 reasoning trajectories synthesized from geometric primitives and expert knowledge. We then formulate a two-stage training strategy: (1) Supervised Knowledge Initialization to equip the model with reasoning syntax and domain expertise, and (2) Consistency-Aware Reinforcement Learning to refine deductive reliability. This second stage integrates a novel Logical Consistency Reward, which penalizes logical drift via an option permutation strategy to anchor decisions in verifiable reasoning traces. Experimental results demonstrate that our framework significantly enhances the cognitive reliability and interpretability of RS-VLMs, achieving state-of-the-art performance compared to other advanced methods.",
    "summary": "",
    "translation": "GeoReason：通过逻辑一致性强化学习在遥感视觉语言模型中实现思维与回答的对齐",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及遥感领域的视觉语言模型（VLM），属于特定领域应用（遥感），与RecSys/Search/Ads的核心关注点无关。虽然提到了VLM和强化学习，但未明确展示其在推荐、搜索或广告中的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04090v1": {
    "title": "Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2601.04090v1",
    "arxiv_id": "2601.04090v1",
    "authors": "Jiaxin Huang, Yuanbo Yang, Bangbang Yang, Lin Ma, Yuewen Ma, Yiyi Liao",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 16:57:30",
    "ori_summary": "We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation. We repurpose the VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, which are regularized to align with the appearance latents of pre-trained video diffusion models. By jointly generating these disentangled yet aligned latents, Gen3R produces both RGB videos and corresponding 3D geometry, including camera poses, depth maps, and global point clouds. Experiments demonstrate that our approach achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Additionally, our method can enhance the robustness of reconstruction by leveraging generative priors, demonstrating the mutual benefit of tightly coupling reconstruction and generative models.",
    "summary": "",
    "translation": "Gen3R：三维场景生成与前馈式重建的融合",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及三维场景生成和重建，属于计算机视觉领域。虽然前馈式架构可能具有效率优势，但论文明确聚焦于3D场景生成，这与推荐系统、搜索或广告的核心技术（如排序、检索、用户建模）缺乏直接关联。三维场景生成在广告创意生成、虚拟试衣等非排序广告应用中可能有间接价值，但根据用户指定的无关主题（包括“非排序广告话题”和“纯视觉论文”），此类应用应被排除。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04068v1": {
    "title": "Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2601.04068v1",
    "arxiv_id": "2601.04068v1",
    "authors": "Zitong Huang, Kaidong Zhang, Yukang Ding, Chao Gao, Rui Ding, Ying Chen, Wangmeng Zuo",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-07 16:32:17",
    "ori_summary": "Aligning text-to-video diffusion models with human preferences is crucial for generating high-quality videos. Existing Direct Preference Otimization (DPO) methods rely on multi-sample ranking and task-specific critic models, which is inefficient and often yields ambiguous global supervision. To address these limitations, we propose LocalDPO, a novel post-training framework that constructs localized preference pairs from real videos and optimizes alignment at the spatio-temporal region level. We design an automated pipeline to efficiently collect preference pair data that generates preference pairs with a single inference per prompt, eliminating the need for external critic models or manual annotation. Specifically, we treat high-quality real videos as positive samples and generate corresponding negatives by locally corrupting them with random spatio-temporal masks and restoring only the masked regions using the frozen base model. During training, we introduce a region-aware DPO loss that restricts preference learning to corrupted areas for rapid convergence. Experiments on Wan2.1 and CogVideoX demonstrate that LocalDPO consistently improves video fidelity, temporal coherence and human preference scores over other post-training approaches, establishing a more efficient and fine-grained paradigm for video generator alignment.",
    "summary": "",
    "translation": "关注生成细节：面向视频扩散模型的直接局部细节偏好优化",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频扩散模型的细节优化技术，属于AIGC/内容生成领域。虽然扩散模型是生成式AI的重要技术，但论文标题明确限定于视频生成应用，没有表明与推荐系统、搜索或广告的潜在关联。根据用户指定的无关主题列表，这属于'纯粹LLM中心化主题'和'AIGC/内容生成'范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04065v1": {
    "title": "Unsupervised Modular Adaptive Region Growing and RegionMix Classification for Wind Turbine Segmentation",
    "url": "https://www.alphaxiv.org/abs/2601.04065v1",
    "arxiv_id": "2601.04065v1",
    "authors": "Raül Pérez-Gonzalo, Riccardo Magro, Andreas Espersen, Antonio Agudo",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-01-07 16:29:52",
    "ori_summary": "Reliable operation of wind turbines requires frequent inspections, as even minor surface damages can degrade aerodynamic performance, reduce energy output, and accelerate blade wear. Central to automating these inspections is the accurate segmentation of turbine blades from visual data. This task is traditionally addressed through dense, pixel-wise deep learning models. However, such methods demand extensive annotated datasets, posing scalability challenges. In this work, we introduce an annotation-efficient segmentation approach that reframes the pixel-level task into a binary region classification problem. Image regions are generated using a fully unsupervised, interpretable Modular Adaptive Region Growing technique, guided by image-specific Adaptive Thresholding and enhanced by a Region Merging process that consolidates fragmented areas into coherent segments. To improve generalization and classification robustness, we introduce RegionMix, an augmentation strategy that synthesizes new training samples by combining distinct regions. Our framework demonstrates state-of-the-art segmentation accuracy and strong cross-site generalization by consistently segmenting turbine blades across distinct windfarms.",
    "summary": "",
    "translation": "无监督模块化自适应区域生长与RegionMix分类的风力涡轮机分割",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的风力涡轮机分割，属于纯粹的视觉任务，与推荐系统、搜索或广告领域没有直接关联。虽然提到了无监督学习和分类方法，但缺乏将这些技术应用于异构数据建模或推荐系统相关场景的明确潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04061v1": {
    "title": "CLAP: Contrastive Latent Action Pretraining for Learning Vision-Language-Action Models from Human Videos",
    "url": "https://www.alphaxiv.org/abs/2601.04061v1",
    "arxiv_id": "2601.04061v1",
    "authors": "Chubin Zhang, Jianan Wang, Zifeng Gao, Yue Su, Tianru Dai, Cai Zhou, Jiwen Lu, Yansong Tang",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2026-01-07 16:26:33",
    "ori_summary": "Generalist Vision-Language-Action models are currently hindered by the scarcity of robotic data compared to the abundance of human video demonstrations. Existing Latent Action Models attempt to leverage video data but often suffer from visual entanglement, capturing noise rather than manipulation skills. To address this, we propose Contrastive Latent Action Pretraining (CLAP), a framework that aligns the visual latent space from videos with a proprioceptive latent space from robot trajectories. By employing contrastive learning, CLAP maps video transitions onto a quantized, physically executable codebook. Building on this representation, we introduce a dual-formulation VLA framework offering both CLAP-NTP, an autoregressive model excelling at instruction following and object generalization, and CLAP-RF, a Rectified Flow-based policy designed for high-frequency, precise manipulation. Furthermore, we propose a Knowledge Matching (KM) regularization strategy to mitigate catastrophic forgetting during fine-tuning. Extensive experiments demonstrate that CLAP significantly outperforms strong baselines, enabling the effective transfer of skills from human videos to robotic execution. Project page: https://lin-shan.com/CLAP/.",
    "summary": "",
    "translation": "CLAP：基于人类视频学习视觉-语言-动作模型的对比潜在动作预训练",
    "relevance_score": 3,
    "reasoning": "虽然该论文涉及多模态学习（视觉-语言-动作），这类似于您关注的异构数据统一建模思路，但其核心焦点是动作学习与机器人控制，而非推荐系统、搜索或广告的直接应用。论文标题明确指向“人类视频”和“动作模型”，这更接近机器人学或具身AI领域，与您列出的核心领域（RecSys/Search/Ads）关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04033v1": {
    "title": "Thinking with Frames: Generative Video Distortion Evaluation via Frame Reward Model",
    "url": "https://www.alphaxiv.org/abs/2601.04033v1",
    "arxiv_id": "2601.04033v1",
    "authors": "Yuan Wang, Borui Liao, Huijuan Huang, Jinda Lu, Ouxiang Li, Kuien Liu, Meng Wang, Xiang Wang",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 15:47:14",
    "ori_summary": "Recent advances in video reward models and post-training strategies have improved text-to-video (T2V) generation. While these models typically assess visual quality, motion quality, and text alignment, they often overlook key structural distortions, such as abnormal object appearances and interactions, which can degrade the overall quality of the generative video. To address this gap, we introduce REACT, a frame-level reward model designed specifically for structural distortions evaluation in generative videos. REACT assigns point-wise scores and attribution labels by reasoning over video frames, focusing on recognizing distortions. To support this, we construct a large-scale human preference dataset, annotated based on our proposed taxonomy of structural distortions, and generate additional data using a efficient Chain-of-Thought (CoT) synthesis pipeline. REACT is trained with a two-stage framework: ((1) supervised fine-tuning with masked loss for domain knowledge injection, followed by (2) reinforcement learning with Group Relative Policy Optimization (GRPO) and pairwise rewards to enhance reasoning capability and align output scores with human preferences. During inference, a dynamic sampling mechanism is introduced to focus on frames most likely to exhibit distortion. We also present REACT-Bench, a benchmark for generative video distortion evaluation. Experimental results demonstrate that REACT complements existing reward models in assessing structutal distortion, achieving both accurate quantitative evaluations and interpretable attribution analysis.",
    "summary": "",
    "translation": "基于框架思考：通过帧奖励模型进行生成式视频失真评估",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于视频失真评估和生成式视频，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然提到了奖励模型，但上下文是视频质量评估，而非推荐/搜索/广告中的用户反馈建模，因此不符合任何关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.04005v1": {
    "title": "Padé Neurons for Efficient Neural Models",
    "url": "https://www.alphaxiv.org/abs/2601.04005v1",
    "arxiv_id": "2601.04005v1",
    "authors": "Onur Keleş, A. Murat Tekalp",
    "categories": "cs.CV, eess.IV",
    "pub_date": "2026-01-07 15:15:30",
    "ori_summary": "Neural networks commonly employ the McCulloch-Pitts neuron model, which is a linear model followed by a point-wise non-linear activation. Various researchers have already advanced inherently non-linear neuron models, such as quadratic neurons, generalized operational neurons, generative neurons, and super neurons, which offer stronger non-linearity compared to point-wise activation functions. In this paper, we introduce a novel and better non-linear neuron model called Padé neurons (Paons), inspired by Padé approximants. Paons offer several advantages, such as diversity of non-linearity, since each Paon learns a different non-linear function of its inputs, and layer efficiency, since Paons provide stronger non-linearity in much fewer layers compared to piecewise linear approximation. Furthermore, Paons include all previously proposed neuron models as special cases, thus any neuron model in any network can be replaced by Paons. We note that there has been a proposal to employ the Padé approximation as a generalized point-wise activation function, which is fundamentally different from our model. To validate the efficacy of Paons, in our experiments, we replace classic neurons in some well-known neural image super-resolution, compression, and classification models based on the ResNet architecture with Paons. Our comprehensive experimental results and analyses demonstrate that neural models built by Paons provide better or equal performance than their classic counterparts with a smaller number of layers. The PyTorch implementation code for Paon is open-sourced at https://github.com/onur-keles/Paon.",
    "summary": "",
    "translation": "用于高效神经模型的帕德神经元",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及神经网络架构的特定组件（帕德神经元），属于通用的神经模型效率改进。虽然效率提升可能间接有益于推荐系统、搜索或广告中的大规模模型部署，但标题本身未明确指向Transformer架构、LLM技术或推荐/搜索/广告领域的直接应用，也未提及异构数据处理或多模态建模。因此，它仅与当前关注点有微弱关联，缺乏明确的领域针对性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03993v1": {
    "title": "PosterVerse: A Full-Workflow Framework for Commercial-Grade Poster Generation with HTML-Based Scalable Typography",
    "url": "https://www.alphaxiv.org/abs/2601.03993v1",
    "arxiv_id": "2601.03993v1",
    "authors": "Junle Liu, Peirong Zhang, Yuyi Zhang, Pengyu Yan, Hui Zhou, Xinyue Zhou, Fengjun Guo, Lianwen Jin",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 15:04:24",
    "ori_summary": "Commercial-grade poster design demands the seamless integration of aesthetic appeal with precise, informative content delivery. Current automated poster generation systems face significant limitations, including incomplete design workflows, poor text rendering accuracy, and insufficient flexibility for commercial applications. To address these challenges, we propose PosterVerse, a full-workflow, commercial-grade poster generation method that seamlessly automates the entire design process while delivering high-density and scalable text rendering. PosterVerse replicates professional design through three key stages: (1) blueprint creation using fine-tuned LLMs to extract key design elements from user requirements, (2) graphical background generation via customized diffusion models to create visually appealing imagery, and (3) unified layout-text rendering with an MLLM-powered HTML engine to guarantee high text accuracy and flexible customization. In addition, we introduce PosterDNA, a commercial-grade, HTML-based dataset tailored for training and validating poster design models. To the best of our knowledge, PosterDNA is the first Chinese poster generation dataset to introduce HTML typography files, enabling scalable text rendering and fundamentally solving the challenges of rendering small and high-density text. Experimental results demonstrate that PosterVerse consistently produces commercial-grade posters with appealing visuals, accurate text alignment, and customizable layouts, making it a promising solution for automating commercial poster design. The code and model are available at https://github.com/wuhaer/PosterVerse.",
    "summary": "",
    "translation": "PosterVerse：基于HTML可扩展排版技术的商业级海报生成全流程框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于商业海报生成和HTML排版技术，属于AIGC和内容生成领域。虽然涉及商业应用，但主要关注视觉内容创作而非推荐系统、搜索或广告的排名与匹配核心问题。该技术缺乏在推荐、搜索或广告领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03959v1": {
    "title": "FUSION: Full-Body Unified Motion Prior for Body and Hands via Diffusion",
    "url": "https://www.alphaxiv.org/abs/2601.03959v1",
    "arxiv_id": "2601.03959v1",
    "authors": "Enes Duran, Nikos Athanasiou, Muhammed Kocabas, Michael J. Black, Omid Taheri",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 14:18:59",
    "ori_summary": "Hands are central to interacting with our surroundings and conveying gestures, making their inclusion essential for full-body motion synthesis. Despite this, existing human motion synthesis methods fall short: some ignore hand motions entirely, while others generate full-body motions only for narrowly scoped tasks under highly constrained settings. A key obstacle is the lack of large-scale datasets that jointly capture diverse full-body motion with detailed hand articulation. While some datasets capture both, they are limited in scale and diversity. Conversely, large-scale datasets typically focus either on body motion without hands or on hand motions without the body. To overcome this, we curate and unify existing hand motion datasets with large-scale body motion data to generate full-body sequences that capture both hand and body. We then propose the first diffusion-based unconditional full-body motion prior, FUSION, which jointly models body and hand motion. Despite using a pose-based motion representation, FUSION surpasses state-of-the-art skeletal control models on the Keypoint Tracking task in the HumanML3D dataset and achieves superior motion naturalness. Beyond standard benchmarks, we demonstrate that FUSION can go beyond typical uses of motion priors through two applications: (1) generating detailed full-body motion including fingers during interaction given the motion of an object, and (2) generating Self-Interaction motions using an LLM to transform natural language cues into actionable motion constraints. For these applications, we develop an optimization pipeline that refines the latent space of our diffusion model to generate task-specific motions. Experiments on these tasks highlight precise control over hand motion while maintaining plausible full-body coordination. The code will be public.",
    "summary": "",
    "translation": "FUSION：基于扩散模型的全身体统一运动先验，适用于身体和手部",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的运动生成和人体建模，属于纯粹的视觉/图形学领域，与推荐系统、搜索或广告的核心技术栈无直接关联。虽然扩散模型是生成式AI的重要技术，但该研究针对特定的人体运动模态，缺乏明确的跨模态应用场景或与用户行为序列、上下文特征等推荐系统核心数据的关联性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03955v1": {
    "title": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation",
    "url": "https://www.alphaxiv.org/abs/2601.03955v1",
    "arxiv_id": "2601.03955v1",
    "authors": "Xu Zhang, Cheng Da, Huan Yang, Kun Gai, Ming Lu, Zhan Ma",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 14:09:18",
    "ori_summary": "Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring \"vision\" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok.",
    "summary": "",
    "translation": "ResTok：用于自回归图像生成的1D视觉分词器中学习层次化残差",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉分词器和图像生成技术，属于计算机视觉领域。虽然提到了自回归生成模型，但其核心是图像生成而非推荐/搜索/广告应用。视觉语言模型的类比可能有一定关联，但该论文缺乏明确的推荐/搜索/广告应用潜力说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03924v1": {
    "title": "A low-complexity method for efficient depth-guided image deblurring",
    "url": "https://www.alphaxiv.org/abs/2601.03924v1",
    "arxiv_id": "2601.03924v1",
    "authors": "Ziyao Yi, Diego Valsesia, Tiziano Bianchi, Enrico Magli",
    "categories": "eess.IV, cs.CV",
    "pub_date": "2026-01-07 13:45:20",
    "ori_summary": "Image deblurring is a challenging problem in imaging due to its highly ill-posed nature. Deep learning models have shown great success in tackling this problem but the quest for the best image quality has brought their computational complexity up, making them impractical on anything but powerful servers. Meanwhile, recent works have shown that mobile Lidars can provide complementary information in the form of depth maps that enhance deblurring quality. In this paper, we introduce a novel low-complexity neural network for depth-guided image deblurring. We show that the use of the wavelet transform to separate structural details and reduce spatial redundancy as well as efficient feature conditioning on the depth information are essential ingredients in developing a low-complexity model. Experimental results show competitive image quality against recent state-of-the-art models while reducing complexity by up to two orders of magnitude.",
    "summary": "",
    "translation": "一种用于高效深度引导图像去模糊的低复杂度方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的图像去模糊技术，属于纯粹的视觉处理任务。虽然提到了深度信息，但这指的是图像深度图而非深度学习。该研究没有涉及推荐系统、搜索、广告、LLM技术或Transformer架构，也没有展示如何将视觉技术应用于这些领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03915v1": {
    "title": "HemBLIP: A Vision-Language Model for Interpretable Leukemia Cell Morphology Analysis",
    "url": "https://www.alphaxiv.org/abs/2601.03915v1",
    "arxiv_id": "2601.03915v1",
    "authors": "Julie van Logtestijn, Petru Manescu",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 13:31:33",
    "ori_summary": "Microscopic evaluation of white blood cell morphology is central to leukemia diagnosis, yet current deep learning models often act as black boxes, limiting clinical trust and adoption. We introduce HemBLIP, a vision language model designed to generate interpretable, morphology aware descriptions of peripheral blood cells. Using a newly constructed dataset of 14k healthy and leukemic cells paired with expert-derived attribute captions, we adapt a general-purpose VLM via both full fine-tuning and LoRA based parameter efficient training, and benchmark against the biomedical foundation model MedGEMMA. HemBLIP achieves higher caption quality and morphological accuracy, while LoRA adaptation provides further gains with significantly reduced computational cost. These results highlight the promise of vision language models for transparent and scalable hematological diagnostics.",
    "summary": "",
    "translation": "HemBLIP：用于可解释性白血病细胞形态学分析的视觉语言模型",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学领域（白血病细胞形态学分析），属于明确的无关主题。虽然涉及视觉语言模型技术，但其应用场景是医学诊断而非推荐系统、搜索或广告领域，与当前关注点无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03884v1": {
    "title": "FLNet: Flood-Induced Agriculture Damage Assessment using Super Resolution of Satellite Images",
    "url": "https://www.alphaxiv.org/abs/2601.03884v1",
    "arxiv_id": "2601.03884v1",
    "authors": "Sanidhya Ghosal, Anurag Sharma, Sushil Ghildiyal, Mukesh Saini",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-07 12:51:28",
    "ori_summary": "Distributing government relief efforts after a flood is challenging. In India, the crops are widely affected by floods; therefore, making rapid and accurate crop damage assessment is crucial for effective post-disaster agricultural management. Traditional manual surveys are slow and biased, while current satellite-based methods face challenges like cloud cover and low spatial resolution. Therefore, to bridge this gap, this paper introduced FLNet, a novel deep learning based architecture that used super-resolution to enhance the 10 m spatial resolution of Sentinel-2 satellite images into 3 m resolution before classifying damage. We tested our model on the Bihar Flood Impacted Croplands Dataset (BFCD-22), and the results showed an improved critical \"Full Damage\" F1-score from 0.83 to 0.89, nearly matching the 0.89 score of commercial high-resolution imagery. This work presented a cost-effective and scalable solution, paving the way for a nationwide shift from manual to automated, high-fidelity damage assessment.",
    "summary": "",
    "translation": "FLNet：基于卫星图像超分辨率技术的洪水诱发农业损害评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于卫星图像超分辨率在农业损害评估中的应用，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术无关。其应用场景（农业损害评估）属于特定领域应用，不在当前关注的RecSys/Search/Ads范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03875v1": {
    "title": "Staged Voxel-Level Deep Reinforcement Learning for 3D Medical Image Segmentation with Noisy Annotations",
    "url": "https://www.alphaxiv.org/abs/2601.03875v1",
    "arxiv_id": "2601.03875v1",
    "authors": "Yuyang Fu, Xiuzhen Guo, Ji Shi",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 12:39:54",
    "ori_summary": "Deep learning has achieved significant advancements in medical image segmentation. Currently, obtaining accurate segmentation outcomes is critically reliant on large-scale datasets with high-quality annotations. However, noisy annotations are frequently encountered owing to the complex morphological structures of organs in medical images and variations among different annotators, which can substantially limit the efficacy of segmentation models. Motivated by the fact that medical imaging annotator can correct labeling errors during segmentation based on prior knowledge, we propose an end-to-end Staged Voxel-Level Deep Reinforcement Learning (SVL-DRL) framework for robust medical image segmentation under noisy annotations. This framework employs a dynamic iterative update strategy to automatically mitigate the impact of erroneous labels without requiring manual intervention. The key advancements of SVL-DRL over existing works include: i) formulating noisy annotations as a voxel-dependent problem and addressing it through a novel staged reinforcement learning framework which guarantees robust model convergence; ii) incorporating a voxel-level asynchronous advantage actor-critic (vA3C) module that conceptualizes each voxel as an autonomous agent, which allows each agent to dynamically refine its own state representation during training, thereby directly mitigating the influence of erroneous labels; iii) designing a novel action space for the agents, along with a composite reward function that strategically combines the Dice value and a spatial continuity metric to significantly boost segmentation accuracy while maintain semantic integrity. Experiments on three public medical image datasets demonstrates State-of-The-Art (SoTA) performance under various experimental settings, with an average improvement of over 3\\% in both Dice and IoU scores.",
    "summary": "",
    "translation": "基于噪声标注的三维医学图像分割的阶段性体素级深度强化学习",
    "relevance_score": 1,
    "reasoning": "该论文涉及医学图像分割和3D视觉，属于明确的无关领域（医学、3D视觉）。虽然提到了强化学习，但应用于医学图像处理，与推荐系统、搜索或广告无直接关联，也不符合任何当前关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03869v1": {
    "title": "Bayesian Monocular Depth Refinement via Neural Radiance Fields",
    "url": "https://www.alphaxiv.org/abs/2601.03869v1",
    "arxiv_id": "2601.03869v1",
    "authors": "Arun Muthukkumar",
    "categories": "cs.CV, cs.GR, cs.LG, cs.RO",
    "pub_date": "2026-01-07 12:32:39",
    "ori_summary": "Monocular depth estimation has applications in many fields, such as autonomous navigation and extended reality, making it an essential computer vision task. However, current methods often produce smooth depth maps that lack the fine geometric detail needed for accurate scene understanding. We propose MDENeRF, an iterative framework that refines monocular depth estimates using depth information from Neural Radiance Fields (NeRFs). MDENeRF consists of three components: (1) an initial monocular estimate for global structure, (2) a NeRF trained on perturbed viewpoints, with per-pixel uncertainty, and (3) Bayesian fusion of the noisy monocular and NeRF depths. We derive NeRF uncertainty from the volume rendering process to iteratively inject high-frequency fine details. Meanwhile, our monocular prior maintains global structure. We demonstrate superior performance on key metrics and experiments using indoor scenes from the SUN RGB-D dataset.",
    "summary": "",
    "translation": "基于神经辐射场的贝叶斯单目深度优化",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及计算机视觉中的深度估计和3D重建技术，属于纯粹的视觉领域研究。虽然神经辐射场（NeRF）是先进的3D表示方法，但论文标题明确聚焦于单目深度优化，没有显示出与推荐系统、搜索或广告领域的直接关联或潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03824v1": {
    "title": "IDESplat: Iterative Depth Probability Estimation for Generalizable 3D Gaussian Splatting",
    "url": "https://www.alphaxiv.org/abs/2601.03824v1",
    "arxiv_id": "2601.03824v1",
    "authors": "Wei Long, Haifeng Wu, Shiyin Jiang, Jinhua Zhang, Xinchun Ji, Shuhang Gu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-07 11:37:57",
    "ori_summary": "Generalizable 3D Gaussian Splatting aims to directly predict Gaussian parameters using a feed-forward network for scene reconstruction. Among these parameters, Gaussian means are particularly difficult to predict, so depth is usually estimated first and then unprojected to obtain the Gaussian sphere centers. Existing methods typically rely solely on a single warp to estimate depth probability, which hinders their ability to fully leverage cross-view geometric cues, resulting in unstable and coarse depth maps. To address this limitation, we propose IDESplat, which iteratively applies warp operations to boost depth probability estimation for accurate Gaussian mean prediction. First, to eliminate the inherent instability of a single warp, we introduce a Depth Probability Boosting Unit (DPBU) that integrates epipolar attention maps produced by cascading warp operations in a multiplicative manner. Next, we construct an iterative depth estimation process by stacking multiple DPBUs, progressively identifying potential depth candidates with high likelihood. As IDESplat iteratively boosts depth probability estimates and updates the depth candidates, the depth map is gradually refined, resulting in accurate Gaussian means. We conduct experiments on RealEstate10K, ACID, and DL3DV. IDESplat achieves outstanding reconstruction quality and state-of-the-art performance with real-time efficiency. On RE10K, it outperforms DepthSplat by 0.33 dB in PSNR, using only 10.7% of the parameters and 70% of the memory. Additionally, our IDESplat improves PSNR by 2.95 dB over DepthSplat on the DTU dataset in cross-dataset experiments, demonstrating its strong generalization ability.",
    "summary": "",
    "translation": "IDESplat：用于可泛化3D高斯泼溅的迭代深度概率估计",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及3D高斯泼溅和深度概率估计，属于计算机视觉和3D重建领域。虽然提到了可泛化性，但未表明与推荐系统、搜索或广告的直接关联。根据用户指定的无关主题，该论文属于'纯粹视觉、3D视觉或图形学论文，且未明确显示与RecSys/搜索/广告的相关性'的范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03811v1": {
    "title": "EvalBlocks: A Modular Pipeline for Rapidly Evaluating Foundation Models in Medical Imaging",
    "url": "https://www.alphaxiv.org/abs/2601.03811v1",
    "arxiv_id": "2601.03811v1",
    "authors": "Jan Tagscherer, Sarah de Boer, Lena Philipp, Fennie van der Graaf, Dré Peeters, Joeran Bosma, Lars Leijten, Bogdan Obreja, Ewoud Smit, Alessa Hering",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-01-07 11:16:49",
    "ori_summary": "Developing foundation models in medical imaging requires continuous monitoring of downstream performance. Researchers are burdened with tracking numerous experiments, design choices, and their effects on performance, often relying on ad-hoc, manual workflows that are inherently slow and error-prone. We introduce EvalBlocks, a modular, plug-and-play framework for efficient evaluation of foundation models during development. Built on Snakemake, EvalBlocks supports seamless integration of new datasets, foundation models, aggregation methods, and evaluation strategies. All experiments and results are tracked centrally and are reproducible with a single command, while efficient caching and parallel execution enable scalable use on shared compute infrastructure. Demonstrated on five state-of-the-art foundation models and three medical imaging classification tasks, EvalBlocks streamlines model evaluation, enabling researchers to iterate faster and focus on model innovation rather than evaluation logistics. The framework is released as open source software at https://github.com/DIAGNijmegen/eval-blocks.",
    "summary": "",
    "translation": "EvalBlocks：用于快速评估医学影像基础模型的模块化流水线",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医学影像领域的评估流水线，属于明确的无关主题（医学/生物学领域特定应用）。标题中提到的'基础模型'可能指医学影像领域的专用模型，而非通用LLM技术，且没有表明与推荐系统、搜索或广告的任何潜在应用关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03808v1": {
    "title": "From Brute Force to Semantic Insight: Performance-Guided Data Transformation Design with LLMs",
    "url": "https://www.alphaxiv.org/abs/2601.03808v1",
    "arxiv_id": "2601.03808v1",
    "authors": "Usha Shrestha, Dmitry Ignatov, Radu Timofte",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-01-07 11:13:02",
    "ori_summary": "Large language models (LLMs) have achieved notable performance in code synthesis; however, data-aware augmentation remains a limiting factor, handled via heuristic design or brute-force approaches. We introduce a performance-aware, closed-loop solution in the NNGPT ecosystem of projects that enables LLMs to autonomously engineer optimal transformations by internalizing empirical performance cues. We fine-tune LLMs with Low-Rank Adaptation on a novel repository of more than 6,000 empirically evaluated PyTorch augmentation functions, each annotated solely by downstream model accuracy. Training uses pairwise performance ordering (better-worse transformations), enabling alignment through empirical feedback without reinforcement learning, reward models, or symbolic objectives. This reduces the need for exhaustive search, achieving up to 600x times fewer evaluated candidates than brute-force discovery while maintaining competitive peak accuracy and shifting generation from random synthesis to task-aligned design. Ablation studies show that structured Chain-of-Thought prompting introduces syntactic noise and degrades performance, whereas direct prompting ensures stable optimization in performance-critical code tasks. Qualitative and quantitative analyses demonstrate that the model internalizes semantic performance cues rather than memorizing syntax. These results show that LLMs can exhibit task-level reasoning through non-textual feedback loops, bypassing explicit symbolic rewards.",
    "summary": "该论文研究如何让大语言模型自主设计最优的数据增强变换。其核心思想是通过对数千个标注了下游模型准确率的增强函数进行微调，让LLM能够从性能反馈中学习语义线索，从而替代传统的启发式或暴力搜索方法。",
    "translation": "从暴力搜索到语义洞察：基于LLM的性能导向数据转换设计",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM在数据转换设计中的应用，属于'直接LLM应用'范畴，对推荐系统、搜索和广告中的特征工程和数据预处理有直接价值。性能导向的方法可以优化推荐/搜索系统中的特征表示，提升模型效果，具有明确的实践意义。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出利用LLMs通过性能反馈闭环自主设计数据增强，直接应用于推荐/搜索系统的特征工程优化，属于LLM在系统优化中的创新应用。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.03784v1": {
    "title": "A Comparative Study of 3D Model Acquisition Methods for Synthetic Data Generation of Agricultural Products",
    "url": "https://www.alphaxiv.org/abs/2601.03784v1",
    "arxiv_id": "2601.03784v1",
    "authors": "Steven Moonen, Rob Salaets, Kenneth Batstone, Abdellatif Bey-Temsamani, Nick Michiels",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 10:34:26",
    "ori_summary": "In the manufacturing industry, computer vision systems based on artificial intelligence (AI) are widely used to reduce costs and increase production. Training these AI models requires a large amount of training data that is costly to acquire and annotate, especially in high-variance, low-volume manufacturing environments. A popular approach to reduce the need for real data is the use of synthetic data that is generated by leveraging computer-aided design (CAD) models available in the industry. However, in the agricultural industry these models are not readily available, increasing the difficulty in leveraging synthetic data. In this paper, we present different techniques for substituting CAD files to create synthetic datasets. We measure their relative performance when used to train an AI object detection model to separate stones and potatoes in a bin picking environment. We demonstrate that using highly representative 3D models acquired by scanning or using image-to-3D approaches can be used to generate synthetic data for training object detection models. Finetuning on a small real dataset can significantly improve the performance of the models and even get similar performance when less representative models are used.",
    "summary": "",
    "translation": "农业产品合成数据生成中三维模型获取方法的比较研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于农业领域的三维模型获取和合成数据生成，属于明确的领域特定应用（农业），与推荐系统、搜索或广告的核心技术无关。论文内容涉及计算机视觉和3D建模，但没有展示与推荐/搜索/广告系统的潜在联系，完全属于被排除的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03782v1": {
    "title": "PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation",
    "url": "https://www.alphaxiv.org/abs/2601.03782v1",
    "arxiv_id": "2601.03782v1",
    "authors": "Wenlong Huang, Yu-Wei Chao, Arsalan Mousavian, Ming-Yu Liu, Dieter Fox, Kaichun Mo, Li Fei-Fei",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2026-01-07 10:29:12",
    "ori_summary": "Humans anticipate, from a glance and a contemplated action of their bodies, how the 3D world will respond, a capability that is equally vital for robotic manipulation. We introduce PointWorld, a large pre-trained 3D world model that unifies state and action in a shared 3D space as 3D point flows: given one or few RGB-D images and a sequence of low-level robot action commands, PointWorld forecasts per-pixel displacements in 3D that respond to the given actions. By representing actions as 3D point flows instead of embodiment-specific action spaces (e.g., joint positions), this formulation directly conditions on physical geometries of robots while seamlessly integrating learning across embodiments. To train our 3D world model, we curate a large-scale dataset spanning real and simulated robotic manipulation in open-world environments, enabled by recent advances in 3D vision and simulated environments, totaling about 2M trajectories and 500 hours across a single-arm Franka and a bimanual humanoid. Through rigorous, large-scale empirical studies of backbones, action representations, learning objectives, partial observability, data mixtures, domain transfers, and scaling, we distill design principles for large-scale 3D world modeling. With a real-time (0.1s) inference speed, PointWorld can be efficiently integrated in the model-predictive control (MPC) framework for manipulation. We demonstrate that a single pre-trained checkpoint enables a real-world Franka robot to perform rigid-body pushing, deformable and articulated object manipulation, and tool use, without requiring any demonstrations or post-training and all from a single image captured in-the-wild. Project website at https://point-world.github.io/.",
    "summary": "",
    "translation": "PointWorld：面向野外机器人操作的3D世界模型规模化构建",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人操作和3D世界建模，属于机器人学领域。虽然涉及3D数据处理，但未提及与推荐系统、搜索或广告相关的应用场景。论文内容与所列的当前关注领域（推荐系统、搜索、广告、LLM技术、Transformer架构、VLM类比等）均无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03781v1": {
    "title": "MVP: Enhancing Video Large Language Models via Self-supervised Masked Video Prediction",
    "url": "https://www.alphaxiv.org/abs/2601.03781v1",
    "arxiv_id": "2601.03781v1",
    "authors": "Xiaokun Sun, Zezhong Wu, Zewen Ding, Linli Xu",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 10:25:48",
    "ori_summary": "Reinforcement learning based post-training paradigms for Video Large Language Models (VideoLLMs) have achieved significant success by optimizing for visual-semantic tasks such as captioning or VideoQA. However, while these approaches effectively enhance perception abilities, they primarily target holistic content understanding, often lacking explicit supervision for intrinsic temporal coherence and inter-frame correlations. This tendency limits the models' ability to capture intricate dynamics and fine-grained visual causality. To explicitly bridge this gap, we propose a novel post-training objective: Masked Video Prediction (MVP). By requiring the model to reconstruct a masked continuous segment from a set of challenging distractors, MVP forces the model to attend to the sequential logic and temporal context of events. To support scalable training, we introduce a scalable data synthesis pipeline capable of transforming arbitrary video corpora into MVP training samples, and further employ Group Relative Policy Optimization (GRPO) with a fine-grained reward function to enhance the model's understanding of video context and temporal properties. Comprehensive evaluations demonstrate that MVP enhances video reasoning capabilities by directly reinforcing temporal reasoning and causal understanding.",
    "summary": "",
    "translation": "MVP：通过自监督掩码视频预测增强视频大语言模型",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视频大语言模型的增强技术，属于计算机视觉与语言模型的交叉领域，与纯粹的推荐系统、搜索或广告核心领域没有直接关联。虽然自监督学习和掩码预测技术可能对处理序列数据有启发，但论文明确聚焦于视频模态，缺乏明确的推荐/搜索/广告应用场景说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03741v1": {
    "title": "I2E: From Image Pixels to Actionable Interactive Environments for Text-Guided Image Editing",
    "url": "https://www.alphaxiv.org/abs/2601.03741v1",
    "arxiv_id": "2601.03741v1",
    "authors": "Jinghan Yu, Junhao Xiao, Chenyu Zhu, Jiaming Li, Jia Li, HanMing Deng, Xirui Wang, Guoli Jia, Jianjun Li, Zhiyuan Ma, Xiang Bai, Bowen Zhou",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 09:29:57",
    "ori_summary": "Existing text-guided image editing methods primarily rely on end-to-end pixel-level inpainting paradigm. Despite its success in simple scenarios, this paradigm still significantly struggles with compositional editing tasks that require precise local control and complex multi-object spatial reasoning. This paradigm is severely limited by 1) the implicit coupling of planning and execution, 2) the lack of object-level control granularity, and 3) the reliance on unstructured, pixel-centric modeling. To address these limitations, we propose I2E, a novel \"Decompose-then-Action\" paradigm that revisits image editing as an actionable interaction process within a structured environment. I2E utilizes a Decomposer to transform unstructured images into discrete, manipulable object layers and then introduces a physics-aware Vision-Language-Action Agent to parse complex instructions into a series of atomic actions via Chain-of-Thought reasoning. Further, we also construct I2E-Bench, a benchmark designed for multi-instance spatial reasoning and high-precision editing. Experimental results on I2E-Bench and multiple public benchmarks demonstrate that I2E significantly outperforms state-of-the-art methods in handling complex compositional instructions, maintaining physical plausibility, and ensuring multi-turn editing stability.",
    "summary": "",
    "translation": "I2E：从图像像素到可操作的交互式环境，用于文本引导的图像编辑",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于文本引导的图像编辑技术，属于纯粹的计算机视觉和图像生成领域。虽然提到了文本交互，但核心是图像像素操作而非推荐系统、搜索或广告应用。该主题属于明确的无关话题范畴（纯粹视觉/图像生成），与当前关注的推荐系统、搜索广告、LLM技术或Transformer架构进展无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03736v1": {
    "title": "HyperCOD: The First Challenging Benchmark and Baseline for Hyperspectral Camouflaged Object Detection",
    "url": "https://www.alphaxiv.org/abs/2601.03736v1",
    "arxiv_id": "2601.03736v1",
    "authors": "Shuyan Bai, Tingfa Xu, Peifu Liu, Yuhao Qiu, Huiyan Bai, Huan Chen, Yanyan Peng, Jianan Li",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 09:26:32",
    "ori_summary": "RGB-based camouflaged object detection struggles in real-world scenarios where color and texture cues are ambiguous. While hyperspectral image offers a powerful alternative by capturing fine-grained spectral signatures, progress in hyperspectral camouflaged object detection (HCOD) has been critically hampered by the absence of a dedicated, large-scale benchmark. To spur innovation, we introduce HyperCOD, the first challenging benchmark for HCOD. Comprising 350 high-resolution hyperspectral images, It features complex real-world scenarios with minimal objects, intricate shapes, severe occlusions, and dynamic lighting to challenge current models. The advent of foundation models like the Segment Anything Model (SAM) presents a compelling opportunity. To adapt the Segment Anything Model (SAM) for HCOD, we propose HyperSpectral Camouflage-aware SAM (HSC-SAM). HSC-SAM ingeniously reformulates the hyperspectral image by decoupling it into a spatial map fed to SAM's image encoder and a spectral saliency map that serves as an adaptive prompt. This translation effectively bridges the modality gap. Extensive experiments show that HSC-SAM sets a new state-of-the-art on HyperCOD and generalizes robustly to other public HSI datasets. The HyperCOD dataset and our HSC-SAM baseline provide a robust foundation to foster future research in this emerging area.",
    "summary": "",
    "translation": "HyperCOD：首个高光谱伪装目标检测的挑战性基准与基线方法",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向高光谱图像处理和伪装目标检测，属于计算机视觉的特定领域。虽然涉及基准和基线方法，但核心内容与推荐系统、搜索、广告或相关LLM技术无直接关联，也不符合异构数据统一建模的VLM类比方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03729v1": {
    "title": "MATANet: A Multi-context Attention and Taxonomy-Aware Network for Fine-Grained Underwater Recognition of Marine Species",
    "url": "https://www.alphaxiv.org/abs/2601.03729v1",
    "arxiv_id": "2601.03729v1",
    "authors": "Donghwan Lee, Byeongjin Kim, Geunhee Kim, Hyukjin Kwon, Nahyeon Maeng, Wooju Kim",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 09:21:45",
    "ori_summary": "Fine-grained classification of marine animals supports ecology, biodiversity and habitat conservation, and evidence-based policy-making. However, existing methods often overlook contextual interactions from the surrounding environment and insufficiently incorporate the hierarchical structure of marine biological taxonomy. To address these challenges, we propose MATANet (Multi-context Attention and Taxonomy-Aware Network), a novel model designed for fine-grained marine species classification. MATANet mimics expert strategies by using taxonomy and environmental context to interpret ambiguous features of underwater animals. It consists of two key components: a Multi-Context Environmental Attention Module (MCEAM), which learns relationships between regions of interest (ROIs) and their surrounding environments, and a Hierarchical Separation-Induced Learning Module (HSLM), which encodes taxonomic hierarchy into the feature space. MATANet combines instance and environmental features with taxonomic structure to enhance fine-grained classification. Experiments on the FathomNet2025, FAIR1M, and LifeCLEF2015-Fish datasets demonstrate state-of-the-art performance. The source code is available at: https://github.com/dhlee-work/fathomnet-cvpr2025-ssl",
    "summary": "",
    "translation": "MATANet：一种用于海洋物种细粒度水下识别的多上下文注意力与分类感知网络",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于水下海洋物种识别这一计算机视觉应用，属于纯粹的视觉识别任务。虽然提到了注意力机制，但其应用场景（海洋生物学识别）与推荐系统、搜索或广告领域完全无关，且没有表明任何向这些领域迁移的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03728v1": {
    "title": "CSMCIR: CoT-Enhanced Symmetric Alignment with Memory Bank for Composed Image Retrieval",
    "url": "https://www.alphaxiv.org/abs/2601.03728v1",
    "arxiv_id": "2601.03728v1",
    "authors": "Zhipeng Qian, Zihan Liang, Yufei Ma, Ben Chen, Huangyu Dai, Yiwei Ma, Jiayi Ji, Chenyi Lei, Han Li, Xiaoshuai Sun",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-07 09:21:38",
    "ori_summary": "Composed Image Retrieval (CIR) enables users to search for target images using both a reference image and manipulation text, offering substantial advantages over single-modality retrieval systems. However, existing CIR methods suffer from representation space fragmentation: queries and targets comprise heterogeneous modalities and are processed by distinct encoders, forcing models to bridge misaligned representation spaces only through post-hoc alignment, which fundamentally limits retrieval performance. This architectural asymmetry manifests as three distinct, well-separated clusters in the feature space, directly demonstrating how heterogeneous modalities create fundamentally misaligned representation spaces from initialization. In this work, we propose CSMCIR, a unified representation framework that achieves efficient query-target alignment through three synergistic components. First, we introduce a Multi-level Chain-of-Thought (MCoT) prompting strategy that guides Multimodal Large Language Models to generate discriminative, semantically compatible captions for target images, establishing modal symmetry. Building upon this, we design a symmetric dual-tower architecture where both query and target sides utilize the identical shared-parameter Q-Former for cross-modal encoding, ensuring consistent feature representations and further reducing the alignment gap. Finally, this architectural symmetry enables an entropy-based, temporally dynamic Memory Bank strategy that provides high-quality negative samples while maintaining consistency with the evolving model state. Extensive experiments on four benchmark datasets demonstrate that our CSMCIR achieves state-of-the-art performance with superior training efficiency. Comprehensive ablation studies further validate the effectiveness of each proposed component.",
    "summary": "",
    "translation": "CSMCIR：基于思维链增强的对称对齐与记忆库的组合图像检索",
    "relevance_score": 3,
    "reasoning": "该论文主要关注组合图像检索（CIR），这是一个计算机视觉任务，涉及根据文本修改查询图像。虽然它使用了类似LLM的思维链（CoT）技术，但其核心是视觉-语言跨模态检索，与推荐系统、搜索或广告中的排序和检索任务没有直接关联。尽管CoT技术可能对理解复杂用户查询有潜在启发，但论文的视觉焦点和特定任务使其与当前关注点相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03718v1": {
    "title": "Towards Real-world Lens Active Alignment with Unlabeled Data via Domain Adaptation",
    "url": "https://www.alphaxiv.org/abs/2601.03718v1",
    "arxiv_id": "2601.03718v1",
    "authors": "Wenyong Lia, Qi Jiang, Weijian Hu, Kailun Yang, Zhanjun Zhang, Wenjun Tian, Kaiwei Wang, Jian Bai",
    "categories": "cs.CV, eess.IV, physics.optics",
    "pub_date": "2026-01-07 09:13:20",
    "ori_summary": "Active Alignment (AA) is a key technology for the large-scale automated assembly of high-precision optical systems. Compared with labor-intensive per-model on-device calibration, a digital-twin pipeline built on optical simulation offers a substantial advantage in generating large-scale labeled data. However, complex imaging conditions induce a domain gap between simulation and real-world images, limiting the generalization of simulation-trained models. To address this, we propose augmenting a simulation baseline with minimal unlabeled real-world images captured at random misalignment positions, mitigating the gap from a domain adaptation perspective. We introduce Domain Adaptive Active Alignment (DA3), which utilizes an autoregressive domain transformation generator and an adversarial-based feature alignment strategy to distill real-world domain information via self-supervised learning. This enables the extraction of domain-invariant image degradation features to facilitate robust misalignment prediction. Experiments on two lens types reveal that DA3 improves accuracy by 46% over a purely simulation pipeline. Notably, it approaches the performance achieved with precisely labeled real-world data collected on 3 lens samples, while reducing on-device data collection time by 98.7%. The results demonstrate that domain adaptation effectively endows simulation-trained models with robust real-world performance, validating the digital-twin pipeline as a practical solution to significantly enhance the efficiency of large-scale optical assembly.",
    "summary": "",
    "translation": "基于领域自适应的无标签数据实现真实世界镜头主动对准",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及计算机视觉中的镜头对准和领域自适应技术，属于纯粹的视觉处理范畴。虽然提到了真实世界应用，但未表明与推荐系统、搜索或广告有任何关联。标题中没有任何元素指向LLM技术、Transformer架构、异构数据建模或推荐/搜索/广告领域的直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03713v1": {
    "title": "BREATH-VL: Vision-Language-Guided 6-DoF Bronchoscopy Localization via Semantic-Geometric Fusion",
    "url": "https://www.alphaxiv.org/abs/2601.03713v1",
    "arxiv_id": "2601.03713v1",
    "authors": "Qingyao Tian, Bingyu Yang, Huai Liao, Xinyan Huang, Junyong Li, Dong Yi, Hongbin Liu",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 09:00:52",
    "ori_summary": "Vision-language models (VLMs) have recently shown remarkable performance in navigation and localization tasks by leveraging large-scale pretraining for semantic understanding. However, applying VLMs to 6-DoF endoscopic camera localization presents several challenges: 1) the lack of large-scale, high-quality, densely annotated, and localization-oriented vision-language datasets in real-world medical settings; 2) limited capability for fine-grained pose regression; and 3) high computational latency when extracting temporal features from past frames. To address these issues, we first construct BREATH dataset, the largest in-vivo endoscopic localization dataset to date, collected in the complex human airway. Building on this dataset, we propose BREATH-VL, a hybrid framework that integrates semantic cues from VLMs with geometric information from vision-based registration methods for accurate 6-DoF pose estimation. Our motivation lies in the complementary strengths of both approaches: VLMs offer generalizable semantic understanding, while registration methods provide precise geometric alignment. To further enhance the VLM's ability to capture temporal context, we introduce a lightweight context-learning mechanism that encodes motion history as linguistic prompts, enabling efficient temporal reasoning without expensive video-level computation. Extensive experiments demonstrate that the vision-language module delivers robust semantic localization in challenging surgical scenes. Building on this, our BREATH-VL outperforms state-of-the-art vision-only localization methods in both accuracy and generalization, reducing translational error by 25.5% compared with the best-performing baseline, while achieving competitive computational latency.",
    "summary": "",
    "translation": "BREATH-VL：通过语义-几何融合实现视觉-语言引导的6自由度支气管镜定位",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学领域（支气管镜定位）的视觉-语言模型应用，属于明确的医学/生物医学应用范畴。虽然涉及视觉-语言模型技术，但其应用场景（支气管镜检查）与推荐系统、搜索或广告领域完全无关，且没有表明该技术可迁移到这些商业领域的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03667v1": {
    "title": "TRec: Egocentric Action Recognition using 2D Point Tracks",
    "url": "https://www.alphaxiv.org/abs/2601.03667v1",
    "arxiv_id": "2601.03667v1",
    "authors": "Dennis Holzmann, Sven Wachsmuth",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-01-07 07:41:57",
    "ori_summary": "We present a novel approach for egocentric action recognition that leverages 2D point tracks as an additional motion cue. While most existing methods rely on RGB appearance, human pose estimation, or their combination, our work demonstrates that tracking randomly sampled image points across video frames can substantially improve recognition accuracy. Unlike prior approaches, we do not detect hands, objects, or interaction regions. Instead, we employ CoTracker to follow a set of randomly initialized points through each video and use the resulting trajectories, together with the corresponding image frames, as input to a Transformer-based recognition model. Surprisingly, our method achieves notable gains even when only the initial frame and its associated point tracks are provided, without incorporating the full video sequence. Experimental results confirm that integrating 2D point tracks consistently enhances performance compared to the same model trained without motion information, highlighting their potential as a lightweight yet effective representation for egocentric action understanding.",
    "summary": "",
    "translation": "TRec：基于二维点轨迹的自我中心动作识别",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的自我中心动作识别，使用二维点轨迹技术。虽然涉及动作识别，但属于纯粹的视觉研究范畴，没有展示与推荐系统、搜索或广告的明确关联。论文标题中未提及任何与推荐、搜索、广告、Transformer架构或LLM技术相关的元素。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03665v1": {
    "title": "PhysVideoGenerator: Towards Physically Aware Video Generation via Latent Physics Guidance",
    "url": "https://www.alphaxiv.org/abs/2601.03665v1",
    "arxiv_id": "2601.03665v1",
    "authors": "Siddarth Nilol Kundur Satish, Devesh Jaiswal, Hongyu Chen, Abhishek Bakshi",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 07:38:58",
    "ori_summary": "Current video generation models produce high-quality aesthetic videos but often struggle to learn representations of real-world physics dynamics, resulting in artifacts such as unnatural object collisions, inconsistent gravity, and temporal flickering. In this work, we propose PhysVideoGenerator, a proof-of-concept framework that explicitly embeds a learnable physics prior into the video generation process. We introduce a lightweight predictor network, PredictorP, which regresses high-level physical features extracted from a pre-trained Video Joint Embedding Predictive Architecture (V-JEPA 2) directly from noisy diffusion latents. These predicted physics tokens are injected into the temporal attention layers of a DiT-based generator (Latte) via a dedicated cross-attention mechanism. Our primary contribution is demonstrating the technical feasibility of this joint training paradigm: we show that diffusion latents contain sufficient information to recover V-JEPA 2 physical representations, and that multi-task optimization remains stable over training. This report documents the architectural design, technical challenges, and validation of training stability, establishing a foundation for future large-scale evaluation of physics-aware generative models.",
    "summary": "",
    "translation": "PhysVideoGenerator：通过潜在物理引导实现物理感知的视频生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于物理感知的视频生成，属于纯粹的视觉内容生成领域，与推荐系统、搜索或广告的核心技术无关。虽然标题提到“物理引导”，但这属于计算机视觉中的物理模拟方向，没有明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03660v1": {
    "title": "MGPC: Multimodal Network for Generalizable Point Cloud Completion With Modality Dropout and Progressive Decoding",
    "url": "https://www.alphaxiv.org/abs/2601.03660v1",
    "arxiv_id": "2601.03660v1",
    "authors": "Jiangyuan Liu, Hongxuan Ma, Yuhao Zhao, Zhe Liu, Jian Wang, Wei Zou",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 07:16:46",
    "ori_summary": "Point cloud completion aims to recover complete 3D geometry from partial observations caused by limited viewpoints and occlusions. Existing learning-based works, including 3D Convolutional Neural Network (CNN)-based, point-based, and Transformer-based methods, have achieved strong performance on synthetic benchmarks. However, due to the limitations of modality, scalability, and generative capacity, their generalization to novel objects and real-world scenarios remains challenging. In this paper, we propose MGPC, a generalizable multimodal point cloud completion framework that integrates point clouds, RGB images, and text within a unified architecture. MGPC introduces an innovative modality dropout strategy, a Transformer-based fusion module, and a novel progressive generator to improve robustness, scalability, and geometric modeling capability. We further develop an automatic data generation pipeline and construct MGPC-1M, a large-scale benchmark with over 1,000 categories and one million training pairs. Extensive experiments on MGPC-1M and in-the-wild data demonstrate that the proposed method consistently outperforms prior baselines and exhibits strong generalization under real-world conditions.",
    "summary": "",
    "translation": "MGPC：一种具有模态丢弃和渐进解码功能的多模态网络，用于可泛化的点云补全",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D点云补全，属于纯粹的3D视觉领域，与推荐系统、搜索或广告没有直接关联。虽然提到了多模态网络，但这是针对3D点云数据的特定模态处理，而非推荐/搜索/广告领域所需的异构数据（如上下文特征和用户序列）的统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03655v1": {
    "title": "VideoMemory: Toward Consistent Video Generation via Memory Integration",
    "url": "https://www.alphaxiv.org/abs/2601.03655v1",
    "arxiv_id": "2601.03655v1",
    "authors": "Jinsong Zhou, Yihua Du, Xinli Xu, Luozhou Wang, Zijie Zhuang, Yehang Zhang, Shuaibo Li, Xiaojun Hu, Bolan Su, Ying-cong Chen",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 07:10:32",
    "ori_summary": "Maintaining consistent characters, props, and environments across multiple shots is a central challenge in narrative video generation. Existing models can produce high-quality short clips but often fail to preserve entity identity and appearance when scenes change or when entities reappear after long temporal gaps. We present VideoMemory, an entity-centric framework that integrates narrative planning with visual generation through a Dynamic Memory Bank. Given a structured script, a multi-agent system decomposes the narrative into shots, retrieves entity representations from memory, and synthesizes keyframes and videos conditioned on these retrieved states. The Dynamic Memory Bank stores explicit visual and semantic descriptors for characters, props, and backgrounds, and is updated after each shot to reflect story-driven changes while preserving identity. This retrieval-update mechanism enables consistent portrayal of entities across distant shots and supports coherent long-form generation. To evaluate this setting, we construct a 54-case multi-shot consistency benchmark covering character-, prop-, and background-persistent scenarios. Extensive experiments show that VideoMemory achieves strong entity-level coherence and high perceptual quality across diverse narrative sequences.",
    "summary": "",
    "translation": "VideoMemory：通过记忆集成实现一致性的视频生成",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于视频生成技术，属于AIGC/内容生成领域，这在无关主题中被明确排除。虽然记忆机制在推荐系统中可能有潜在应用，但标题本身并未表明与推荐系统、搜索或广告的任何直接关联，也没有展示出对Transformer架构或LLM核心技术的贡献。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03637v1": {
    "title": "CrackSegFlow: Controllable Flow-Matching Synthesis for Generalizable Crack Segmentation with the CSF-50K Benchmark",
    "url": "https://www.alphaxiv.org/abs/2601.03637v1",
    "arxiv_id": "2601.03637v1",
    "authors": "Babak Asadi, Peiyang Wu, Mani Golparvar-Fard, Ramez Hajj",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 06:28:16",
    "ori_summary": "Automated crack segmentation is essential for scalable condition assessment of pavements and civil infrastructure, yet practical deployment is limited by scarce pixel-level labels and severe domain shift across sensors, illumination, textures, and annotation conventions. This paper presents CrackSegFlow, a controllable flow-matching synthesis framework that generates photorealistic crack images conditioned on binary masks while preserving strict mask-image alignment. The generator combines topology-preserving mask injection with boundary-gated modulation to maintain thin-structure continuity and suppress texture-driven false positives. A second class-conditional flow-matching model synthesizes crack masks with explicit control over crack coverage, enabling balanced, topology-diverse paired data without additional manual annotation. We further inject crack masks into crack-free backgrounds to diversify illumination and surface artifacts and reduce false positives caused by shadows, joints, and pavement markings. Experiments on five benchmarks spanning four asphalt datasets and the crack class of a concrete-domain dataset demonstrate consistent improvements under an established hybrid CNN--Transformer segmentation backbone and a fixed training protocol. With real plus synthesized pairs, in-domain performance improves on average by 5.37 mIoU and 5.13 F1, and target-guided cross-domain synthesis yields average gains of 13.12 mIoU and 14.82 F1 using only limited target mask statistics. Compared with diffusion-based semantic synthesis, CrackSegFlow provides substantially faster deterministic sampling and improves fidelity and mask-image alignment for thin-structure crack geometry. Finally, we release CSF-50K, a public dataset of 50,000 paired crack images and pixel-accurate masks for large-scale benchmarking of generalizable crack segmentation.",
    "summary": "",
    "translation": "CrackSegFlow：基于可控流匹配合成的可泛化裂缝分割方法及CSF-50K基准数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的裂缝分割任务，属于纯粹的视觉应用领域。虽然标题中提到了'可泛化'和'基准数据集'，但核心内容与推荐系统、搜索或广告的技术栈（如Transformer架构、LLM应用、异构数据建模）没有任何关联。该研究属于土木工程或工业检测的特定视觉应用，完全不符合当前关注的任何技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03633v1": {
    "title": "MFC-RFNet: A Multi-scale Guided Rectified Flow Network for Radar Sequence Prediction",
    "url": "https://www.alphaxiv.org/abs/2601.03633v1",
    "arxiv_id": "2601.03633v1",
    "authors": "Wenjie Luo, Chuanhu Deng, Chaorong Li, Rongyao Deng, Qiang Yang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-07 06:24:26",
    "ori_summary": "Accurate and high-resolution precipitation nowcasting from radar echo sequences is crucial for disaster mitigation and economic planning, yet it remains a significant challenge. Key difficulties include modeling complex multi-scale evolution, correcting inter-frame feature misalignment caused by displacement, and efficiently capturing long-range spatiotemporal context without sacrificing spatial fidelity. To address these issues, we present the Multi-scale Feature Communication Rectified Flow (RF) Network (MFC-RFNet), a generative framework that integrates multi-scale communication with guided feature fusion. To enhance multi-scale fusion while retaining fine detail, a Wavelet-Guided Skip Connection (WGSC) preserves high-frequency components, and a Feature Communication Module (FCM) promotes bidirectional cross-scale interaction. To correct inter-frame displacement, a Condition-Guided Spatial Transform Fusion (CGSTF) learns spatial transforms from conditioning echoes to align shallow features. The backbone adopts rectified flow training to learn near-linear probability-flow trajectories, enabling few-step sampling with stable fidelity. Additionally, lightweight Vision-RWKV (RWKV) blocks are placed at the encoder tail, the bottleneck, and the first decoder layer to capture long-range spatiotemporal dependencies at low spatial resolutions with moderate compute. Evaluations on four public datasets (SEVIR, MeteoNet, Shanghai, and CIKM) demonstrate consistent improvements over strong baselines, yielding clearer echo morphology at higher rain-rate thresholds and sustained skill at longer lead times. These results suggest that the proposed synergy of RF training with scale-aware communication, spatial alignment, and frequency-aware fusion presents an effective and robust approach for radar-based nowcasting.",
    "summary": "",
    "translation": "MFC-RFNet：一种用于雷达序列预测的多尺度引导整流流网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于雷达序列预测，属于特定传感器数据处理领域，与推荐系统、搜索或广告的核心技术无关。虽然涉及序列预测，但雷达数据与用户行为序列、上下文特征等异构数据在性质和应用场景上完全不同，无法直接类比VLM方法。论文没有展示任何与Transformer架构、LLM技术或推荐/搜索/广告领域的潜在联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03625v1": {
    "title": "Shape Classification using Approximately Convex Segment Features",
    "url": "https://www.alphaxiv.org/abs/2601.03625v1",
    "arxiv_id": "2601.03625v1",
    "authors": "Bimal Kumar Ray",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 06:12:05",
    "ori_summary": "The existing object classification techniques based on descriptive features rely on object alignment to compute the similarity of objects for classification. This paper replaces the necessity of object alignment through sorting of feature. The object boundary is normalized and segmented into approximately convex segments and the segments are then sorted in descending order of their length. The segment length, number of extreme points in segments, area of segments, the base and the width of the segments - a bag of features - is used to measure the similarity between image boundaries. The proposed method is tested on datasets and acceptable results are observed.",
    "summary": "",
    "translation": "基于近似凸分割特征的形状分类",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的形状分类任务，属于纯粹的视觉领域研究。论文标题中提到的“近似凸分割特征”是计算机视觉中的特定技术，没有显示出与推荐系统、搜索或广告领域的直接关联或潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03617v1": {
    "title": "Systematic Evaluation of Depth Backbones and Semantic Cues for Monocular Pseudo-LiDAR 3D Detection",
    "url": "https://www.alphaxiv.org/abs/2601.03617v1",
    "arxiv_id": "2601.03617v1",
    "authors": "Samson Oseiwe Ajadalu",
    "categories": "cs.CV, cs.LG, cs.RO",
    "pub_date": "2026-01-07 05:57:19",
    "ori_summary": "Monocular 3D object detection offers a low-cost alternative to LiDAR, yet remains less accurate due to the difficulty of estimating metric depth from a single image. We systematically evaluate how depth backbones and feature engineering affect a monocular Pseudo-LiDAR pipeline on the KITTI validation split. Specifically, we compare NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under an identical pseudo-LiDAR generation and PointRCNN detection protocol. NeWCRFs yields stronger downstream 3D detection, achieving 10.50\\% AP$_{3D}$ at IoU$=0.7$ on the Moderate split using grayscale intensity (Exp~2). We further test point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Contrary to the expectation that semantics would substantially close the gap, these features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Finally, we report a depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes (including Ped/Cyc), highlighting that coarse depth correctness does not fully predict strict 3D IoU. Overall, under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection.",
    "summary": "",
    "translation": "用于单目伪激光雷达三维检测的深度主干网络与语义线索的系统性评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的3D检测技术，特别是单目伪激光雷达方法，这属于纯粹的视觉领域研究。论文内容涉及深度主干网络架构和语义线索评估，与推荐系统、搜索或广告的核心技术没有直接关联，也没有展示在异构数据处理或多模态建模方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03609v1": {
    "title": "Unveiling Text in Challenging Stone Inscriptions: A Character-Context-Aware Patching Strategy for Binarization",
    "url": "https://www.alphaxiv.org/abs/2601.03609v1",
    "arxiv_id": "2601.03609v1",
    "authors": "Pratyush Jena, Amal Joseph, Arnav Sharma, Ravi Kiran Sarvadevabhatla",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 05:37:29",
    "ori_summary": "Binarization is a popular first step towards text extraction in historical artifacts. Stone inscription images pose severe challenges for binarization due to poor contrast between etched characters and the stone background, non-uniform surface degradation, distracting artifacts, and highly variable text density and layouts. These conditions frequently cause existing binarization techniques to fail and struggle to isolate coherent character regions. Many approaches sub-divide the image into patches to improve text fragment resolution and improve binarization performance. With this in mind, we present a robust and adaptive patching strategy to binarize challenging Indic inscriptions. The patches from our approach are used to train an Attention U-Net for binarization. The attention mechanism allows the model to focus on subtle structural cues, while our dynamic sampling and patch selection method ensures that the model learns to overcome surface noise and layout irregularities. We also introduce a carefully annotated, pixel-precise dataset of Indic stone inscriptions at the character-fragment level. We demonstrate that our novel patching mechanism significantly boosts binarization performance across classical and deep learning baselines. Despite training only on single script Indic dataset, our model exhibits strong zero-shot generalization to other Indic and non-indic scripts, highlighting its robustness and script-agnostic generalization capabilities. By producing clean, structured representations of inscription content, our method lays the foundation for downstream tasks such as script identification, OCR, and historical text analysis. Project page: https://ihdia.iiit.ac.in/shilalekhya-binarization/",
    "summary": "",
    "translation": "揭示挑战性石刻铭文中的文字：一种面向二值化的字符-上下文感知分块策略",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的特定图像处理任务（石刻铭文二值化），属于纯粹的视觉技术范畴。虽然提到了“上下文感知”，但这指的是图像中的局部上下文而非推荐/搜索/广告所需的用户行为或特征上下文。该技术没有展示在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03596v1": {
    "title": "Adaptive Attention Distillation for Robust Few-Shot Segmentation under Environmental Perturbations",
    "url": "https://www.alphaxiv.org/abs/2601.03596v1",
    "arxiv_id": "2601.03596v1",
    "authors": "Qianyu Guo, Jingrong Wu, Jieji Ren, Weifeng Ge, Wenqiang Zhang",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 05:27:12",
    "ori_summary": "Few-shot segmentation (FSS) aims to rapidly learn novel class concepts from limited examples to segment specific targets in unseen images, and has been widely applied in areas such as medical diagnosis and industrial inspection. However, existing studies largely overlook the complex environmental factors encountered in real world scenarios-such as illumination, background, and camera viewpoint-which can substantially increase the difficulty of test images. As a result, models trained under laboratory conditions often fall short of practical deployment requirements. To bridge this gap, in this paper, an environment-robust FSS setting is introduced that explicitly incorporates challenging test cases arising from complex environments-such as motion blur, small objects, and camouflaged targets-to enhance model's robustness under realistic, dynamic conditions. An environment robust FSS benchmark (ER-FSS) is established, covering eight datasets across multiple real world scenarios. In addition, an Adaptive Attention Distillation (AAD) method is proposed, which repeatedly contrasts and distills key shared semantics between known (support) and unknown (query) images to derive class-specific attention for novel categories. This strengthens the model's ability to focus on the correct targets in complex environments, thereby improving environmental robustness. Comparative experiments show that AAD improves mIoU by 3.3% - 8.5% across all datasets and settings, demonstrating superior performance and strong generalization. The source code and dataset are available at: https://github.com/guoqianyu-alberta/Adaptive-Attention-Distillation-for-FSS.",
    "summary": "",
    "translation": "面向环境扰动下鲁棒少样本分割的自适应注意力蒸馏",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的少样本分割任务，核心创新在于注意力蒸馏机制和环境扰动下的鲁棒性改进。虽然注意力机制是Transformer架构的关键组件，但本文聚焦于纯粹的视觉分割问题，未涉及推荐系统、搜索或广告领域的应用场景，也未探讨将异构数据作为不同模态进行统一建模的可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03590v1": {
    "title": "Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions",
    "url": "https://www.alphaxiv.org/abs/2601.03590v1",
    "arxiv_id": "2601.03590v1",
    "authors": "Zhongbin Guo, Zhen Yang, Yushan Li, Xinyue Zhang, Wenyu Gao, Jiacheng Wang, Chengzhi Li, Xiangrui Liu, Ping Jian",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-07 05:13:52",
    "ori_summary": "Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant \"spatial gap\" remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .",
    "summary": "",
    "translation": "大语言模型能否不依赖像素进行视觉感知？基于文本描述的空间智能基准测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM的空间智能基准测试，这属于纯粹的NLP评估基准范畴，与您关注的推荐/搜索/广告核心领域进展、使能技术或直接应用无关。虽然涉及LLM能力评估，但缺乏明确的推荐/搜索/广告应用场景或技术迁移路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03586v1": {
    "title": "Detecting AI-Generated Images via Distributional Deviations from Real Images",
    "url": "https://www.alphaxiv.org/abs/2601.03586v1",
    "arxiv_id": "2601.03586v1",
    "authors": "Yakun Niu, Yingjian Chen, Lei Zhang",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 05:00:13",
    "ori_summary": "The rapid advancement of generative models has significantly enhanced the quality of AI-generated images, raising concerns about misinformation and the erosion of public trust. Detecting AI-generated images has thus become a critical challenge, particularly in terms of generalizing to unseen generative models. Existing methods using frozen pre-trained CLIP models show promise in generalization but treat the image encoder as a basic feature extractor, failing to fully exploit its potential. In this paper, we perform an in-depth analysis of the frozen CLIP image encoder (CLIP-ViT), revealing that it effectively clusters real images in a high-level, abstract feature space. However, it does not truly possess the ability to distinguish between real and AI-generated images. Based on this analysis, we propose a Masking-based Pre-trained model Fine-Tuning (MPFT) strategy, which introduces a Texture-Aware Masking (TAM) mechanism to mask textured areas containing generative model-specific patterns during fine-tuning. This approach compels CLIP-ViT to attend to the \"distributional deviations\"from authentic images for AI-generated image detection, thereby achieving enhanced generalization performance. Extensive experiments on the GenImage and UniversalFakeDetect datasets demonstrate that our method, fine-tuned with only a minimal number of images, significantly outperforms existing approaches, achieving up to 98.2% and 94.6% average accuracy on the two datasets, respectively.",
    "summary": "",
    "translation": "通过检测与真实图像的分布偏差来识别AI生成图像",
    "relevance_score": 1,
    "reasoning": "该论文专注于AI生成图像检测，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然涉及AI生成内容，但属于纯粹的视觉检测任务，不符合当前关注的LLM技术应用、Transformer架构进展或异构数据统一建模等方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03579v1": {
    "title": "SpatiaLoc: Leveraging Multi-Level Spatial Enhanced Descriptors for Cross-Modal Localization",
    "url": "https://www.alphaxiv.org/abs/2601.03579v1",
    "arxiv_id": "2601.03579v1",
    "authors": "Tianyi Shang, Pengjie Xu, Zhaojun Deng, Zhenyu Li, Zhicong Chen, Lijun Wu",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 04:50:39",
    "ori_summary": "Cross-modal localization using text and point clouds enables robots to localize themselves via natural language descriptions, with applications in autonomous navigation and interaction between humans and robots. In this task, objects often recur across text and point clouds, making spatial relationships the most discriminative cues for localization. Given this characteristic, we present SpatiaLoc, a framework utilizing a coarse-to-fine strategy that emphasizes spatial relationships at both the instance and global levels. In the coarse stage, we introduce a Bezier Enhanced Object Spatial Encoder (BEOSE) that models spatial relationships at the instance level using quadratic Bezier curves. Additionally, a Frequency Aware Encoder (FAE) generates spatial representations in the frequency domain at the global level. In the fine stage, an Uncertainty Aware Gaussian Fine Localizer (UGFL) regresses 2D positions by modeling predictions as Gaussian distributions with a loss function aware of uncertainty. Extensive experiments on KITTI360Pose demonstrate that SpatiaLoc significantly outperforms existing state-of-the-art (SOTA) methods.",
    "summary": "",
    "translation": "SpatiaLoc：利用多级空间增强描述符进行跨模态定位",
    "relevance_score": 3,
    "reasoning": "该论文标题涉及跨模态定位和空间描述符，可能属于计算机视觉或机器人领域。虽然跨模态技术（如VLM处理异构数据）与“VLM Analogy for Heterogeneous Data”有一定关联，但“定位”通常指物理空间定位（如视觉-语言定位），而非推荐/搜索/广告中的用户-项目匹配。缺乏明确指向RecSys/Search/Ads的应用暗示，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03528v1": {
    "title": "CloudMatch: Weak-to-Strong Consistency Learning for Semi-Supervised Cloud Detection",
    "url": "https://www.alphaxiv.org/abs/2601.03528v1",
    "arxiv_id": "2601.03528v1",
    "authors": "Jiayi Zhao, Changlu Chen, Jingsheng Li, Tianxiang Xue, Kun Zhan",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 02:31:17",
    "ori_summary": "Due to the high cost of annotating accurate pixel-level labels, semi-supervised learning has emerged as a promising approach for cloud detection. In this paper, we propose CloudMatch, a semi-supervised framework that effectively leverages unlabeled remote sensing imagery through view-consistency learning combined with scene-mixing augmentations. An observation behind CloudMatch is that cloud patterns exhibit structural diversity and contextual variability across different scenes and within the same scene category. Our key insight is that enforcing prediction consistency across diversely augmented views, incorporating both inter-scene and intra-scene mixing, enables the model to capture the structural diversity and contextual richness of cloud patterns. Specifically, CloudMatch generates one weakly augmented view along with two complementary strongly augmented views for each unlabeled image: one integrates inter-scene patches to simulate contextual variety, while the other employs intra-scene mixing to preserve semantic coherence. This approach guides pseudolabel generation and enhances generalization. Extensive experiments show that CloudMatch achieves good performance, demonstrating its capability to utilize unlabeled data efficiently and advance semi-supervised cloud detection.",
    "summary": "",
    "translation": "CloudMatch：用于半监督云检测的弱到强一致性学习",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及云检测（遥感/计算机视觉领域），与推荐系统、搜索或广告的核心技术领域无关。虽然提到了弱到强一致性学习，但这是针对特定视觉任务的方法，没有显示出在推荐/搜索/广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03526v1": {
    "title": "Physics-Constrained Cross-Resolution Enhancement Network for Optics-Guided Thermal UAV Image Super-Resolution",
    "url": "https://www.alphaxiv.org/abs/2601.03526v1",
    "arxiv_id": "2601.03526v1",
    "authors": "Zhicheng Zhao, Fengjiao Peng, Jinquan Yan, Wei Lu, Chenglong Li, Jin Tang",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 02:30:27",
    "ori_summary": "Optics-guided thermal UAV image super-resolution has attracted significant research interest due to its potential in all-weather monitoring applications. However, existing methods typically compress optical features to match thermal feature dimensions for cross-modal alignment and fusion, which not only causes the loss of high-frequency information that is beneficial for thermal super-resolution, but also introduces physically inconsistent artifacts such as texture distortions and edge blurring by overlooking differences in the imaging physics between modalities. To address these challenges, we propose PCNet to achieve cross-resolution mutual enhancement between optical and thermal modalities, while physically constraining the optical guidance process via thermal conduction to enable robust thermal UAV image super-resolution. In particular, we design a Cross-Resolution Mutual Enhancement Module (CRME) to jointly optimize thermal image super-resolution and optical-to-thermal modality conversion, facilitating effective bidirectional feature interaction across resolutions while preserving high-frequency optical priors. Moreover, we propose a Physics-Driven Thermal Conduction Module (PDTM) that incorporates two-dimensional heat conduction into optical guidance, modeling spatially-varying heat conduction properties to prevent inconsistent artifacts. In addition, we introduce a temperature consistency loss that enforces regional distribution consistency and boundary gradient smoothness to ensure generated thermal images align with real-world thermal radiation principles. Extensive experiments on VGTSR2.0 and DroneVehicle datasets demonstrate that PCNet significantly outperforms state-of-the-art methods on both reconstruction quality and downstream tasks including semantic segmentation and object detection.",
    "summary": "",
    "translation": "面向光学引导热成像无人机图像超分辨率的物理约束跨分辨率增强网络",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及无人机热成像和超分辨率技术，属于计算机视觉领域，与推荐系统、搜索或广告的核心关注点无关。虽然提到了网络架构，但这是针对特定传感器数据处理的专用方法，没有显示出在异构数据统一建模或Transformer架构方面的通用潜力，因此与当前关注点基本不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03517v1": {
    "title": "Semantic Belief-State World Model for 3D Human Motion Prediction",
    "url": "https://www.alphaxiv.org/abs/2601.03517v1",
    "arxiv_id": "2601.03517v1",
    "authors": "Sarim Chaudhry",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 02:06:26",
    "ori_summary": "Human motion prediction has traditionally been framed as a sequence regression problem where models extrapolate future joint coordinates from observed pose histories. While effective over short horizons this approach does not separate observation reconstruction with dynamics modeling and offers no explicit representation of the latent causes governing motion. As a result, existing methods exhibit compounding drift, mean-pose collapse, and poorly calibrated uncertainty when rolled forward beyond the training regime. Here we propose a Semantic Belief-State World Model (SBWM) that reframes human motion prediction as latent dynamical simulation on the human body manifold. Rather than predicting poses directly, SBWM maintains a recurrent probabilistic belief state whose evolution is learned independently of pose reconstruction and explicitly aligned with the SMPL-X anatomical parameterization. This alignment imposes a structural information bottleneck that prevents the latent state from encoding static geometry or sensor noise, forcing it to capture motion dynamics, intent, and control-relevant structure. Inspired by belief-state world models developed for model-based reinforcement learning, SBWM adapts stochastic latent transitions and rollout-centric training to the domain of human motion. In contrast to RSSM-based, transformer, and diffusion approaches optimized for reconstruction fidelity, SBWM prioritizes stable forward simulation. We demonstrate coherent long-horizon rollouts, and competitive accuracy at substantially lower computational cost. These results suggest that treating the human body as part of the world models state space rather than its output fundamentally changes how motion is simulated, and predicted.",
    "summary": "",
    "translation": "用于3D人体运动预测的语义信念状态世界模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D人体运动预测，属于计算机视觉和图形学领域，与推荐系统、搜索或广告没有直接关联。标题中提到的\"世界模型\"和\"信念状态\"暗示可能涉及强化学习或序列建模，但缺乏明确的推荐/搜索/广告应用场景，因此与当前关注点高度不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03510v1": {
    "title": "G2P: Gaussian-to-Point Attribute Alignment for Boundary-Aware 3D Semantic Segmentation",
    "url": "https://www.alphaxiv.org/abs/2601.03510v1",
    "arxiv_id": "2601.03510v1",
    "authors": "Hojun Song, Chae-yeong Song, Jeong-hun Hong, Chaewon Moon, Dong-hwi Kim, Gahyeon Kim, Soo Ye Kim, Yiyi Liao, Jaehyup Lee, Sang-hyo Park",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 01:44:29",
    "ori_summary": "Semantic segmentation on point clouds is critical for 3D scene understanding. However, sparse and irregular point distributions provide limited appearance evidence, making geometry-only features insufficient to distinguish objects with similar shapes but distinct appearances (e.g., color, texture, material). We propose Gaussian-to-Point (G2P), which transfers appearance-aware attributes from 3D Gaussian Splatting to point clouds for more discriminative and appearance-consistent segmentation. Our G2P address the misalignment between optimized Gaussians and original point geometry by establishing point-wise correspondences. By leveraging Gaussian opacity attributes, we resolve the geometric ambiguity that limits existing models. Additionally, Gaussian scale attributes enable precise boundary localization in complex 3D scenes. Extensive experiments demonstrate that our approach achieves superior performance on standard benchmarks and shows significant improvements on geometrically challenging classes, all without any 2D or language supervision.",
    "summary": "",
    "translation": "G2P：面向边界感知三维语义分割的高斯分布到点属性对齐方法",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于3D视觉中的语义分割技术，属于纯粹的计算机视觉领域研究。虽然提到了边界感知和属性对齐，但缺乏与推荐系统、搜索或广告领域的明确关联。该技术主要针对3D点云数据处理，没有展示在异构数据统一建模或Transformer架构改进方面的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03507v1": {
    "title": "REFA: Real-time Egocentric Facial Animations for Virtual Reality",
    "url": "https://www.alphaxiv.org/abs/2601.03507v1",
    "arxiv_id": "2601.03507v1",
    "authors": "Qiang Zhang, Tong Xiao, Haroun Habeeb, Larissa Laich, Sofien Bouaziz, Patrick Snape, Wenjing Zhang, Matthew Cioffi, Peizhao Zhang, Pavel Pidlypenskyi, Winnie Lin, Luming Ma, Mengjiao Wang, Kunpeng Li, Chengjiang Long, Steven Song, Martin Prazak, Alexander Sjoholm, Ajinkya Deogade, Jaebong Lee, Julio Delgado Mangas, Amaury Aubel",
    "categories": "cs.CV",
    "pub_date": "2026-01-07 01:41:46",
    "ori_summary": "We present a novel system for real-time tracking of facial expressions using egocentric views captured from a set of infrared cameras embedded in a virtual reality (VR) headset. Our technology facilitates any user to accurately drive the facial expressions of virtual characters in a non-intrusive manner and without the need of a lengthy calibration step. At the core of our system is a distillation based approach to train a machine learning model on heterogeneous data and labels coming form multiple sources, \\eg synthetic and real images. As part of our dataset, we collected 18k diverse subjects using a lightweight capture setup consisting of a mobile phone and a custom VR headset with extra cameras. To process this data, we developed a robust differentiable rendering pipeline enabling us to automatically extract facial expression labels. Our system opens up new avenues for communication and expression in virtual environments, with applications in video conferencing, gaming, entertainment, and remote collaboration.",
    "summary": "",
    "translation": "REFA：面向虚拟现实的实时第一人称视角面部动画",
    "relevance_score": 1,
    "reasoning": "该论文专注于虚拟现实中的面部动画技术，属于计算机图形学/视觉领域，与推荐系统、搜索或广告的核心技术栈无直接关联。其内容涉及实时渲染和VR应用，属于明确的无关主题（Purely Vision/Graphic papers without clear relevance to RecSys/Search/Ads）。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03500v1": {
    "title": "SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2601.03500v1",
    "arxiv_id": "2601.03500v1",
    "authors": "Yuxuan Xia, Siheng Wang, Peng Li",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-07 01:27:58",
    "ori_summary": "Large Vision-Language Models (LVLMs) demonstrate significant progress in multimodal understanding and reasoning, yet object hallucination remains a critical challenge. While existing research focuses on mitigating language priors or high-level statistical biases, they often overlook the internal complexities of the visual encoding process. We identify that visual statistical bias, arising from the inherent Bag-of-Patches behavior of Vision Encoders under weak structural supervision, acts as a contributing factor of object hallucinations. Under this bias, models prioritize local texture features within individual patches over holistic geometric structures. This tendency may induce spurious visual confidence and result in hallucinations. To address this, we introduce a training-free algorithm called Structure-Disrupted Contrastive Decoding (SDCD), which performs contrastive calibration of the output distribution by introducing a shuffled structure-disrupted view. By penalizing tokens that maintain high confidence under this structure-less view, SDCD effectively suppresses the texture-driven bias. Experimental results demonstrate that SDCD significantly mitigates hallucinations across multiple benchmarks and enhances the overall multimodal capabilities of LVLMs.",
    "summary": "",
    "translation": "SDCD：面向大型视觉语言模型幻觉缓解的结构破坏对比解码",
    "relevance_score": 1,
    "reasoning": "该论文专注于解决视觉语言模型的幻觉问题，这属于纯粹的NLP/视觉中心话题，与推荐系统、搜索或广告的核心技术进展无关。论文标题明确表明其关注点是幻觉缓解，这在您的无关主题列表中已被明确排除。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03499v1": {
    "title": "GeoDiff-SAR: A Geometric Prior Guided Diffusion Model for SAR Image Generation",
    "url": "https://www.alphaxiv.org/abs/2601.03499v1",
    "arxiv_id": "2601.03499v1",
    "authors": "Fan Zhang, Xuanting Wu, Fei Ma, Qiang Yin, Yuxin Hu",
    "categories": "eess.IV, cs.CV",
    "pub_date": "2026-01-07 01:27:20",
    "ori_summary": "Synthetic Aperture Radar (SAR) imaging results are highly sensitive to observation geometries and the geometric parameters of targets. However, existing generative methods primarily operate within the image domain, neglecting explicit geometric information. This limitation often leads to unsatisfactory generation quality and the inability to precisely control critical parameters such as azimuth angles. To address these challenges, we propose GeoDiff-SAR, a geometric prior guided diffusion model for high-fidelity SAR image generation. Specifically, GeoDiff-SAR first efficiently simulates the geometric structures and scattering relationships inherent in real SAR imaging by calculating SAR point clouds at specific azimuths, which serves as a robust physical guidance. Secondly, to effectively fuse multi-modal information, we employ a feature fusion gating network based on Feature-wise Linear Modulation (FiLM) to dynamically regulate the weight distribution of 3D physical information, image control parameters, and textual description parameters. Thirdly, we utilize the Low-Rank Adaptation (LoRA) architecture to perform lightweight fine-tuning on the advanced Stable Diffusion 3.5 (SD3.5) model, enabling it to rapidly adapt to the distribution characteristics of the SAR domain. To validate the effectiveness of GeoDiff-SAR, extensive comparative experiments were conducted on real-world SAR datasets. The results demonstrate that data generated by GeoDiff-SAR exhibits high fidelity and effectively enhances the accuracy of downstream classification tasks. In particular, it significantly improves recognition performance across different azimuth angles, thereby underscoring the superiority of physics-guided generation.",
    "summary": "",
    "translation": "GeoDiff-SAR：一种几何先验引导的扩散模型用于合成孔径雷达图像生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注合成孔径雷达（SAR）图像生成，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然扩散模型是生成模型的一种，但该研究专注于特定传感器数据生成，缺乏在推荐/搜索/广告场景中应用的明确路径或潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.03490v1": {
    "title": "CroBIM-U: Uncertainty-Driven Referring Remote Sensing Image Segmentation",
    "url": "https://www.alphaxiv.org/abs/2601.03490v1",
    "arxiv_id": "2601.03490v1",
    "authors": "Yuzhe Sun, Zhe Dong, Haochen Jiang, Tianzhu Liu, Yanfeng Gu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-07 01:02:39",
    "ori_summary": "Referring remote sensing image segmentation aims to localize specific targets described by natural language within complex overhead imagery. However, due to extreme scale variations, dense similar distractors, and intricate boundary structures, the reliability of cross-modal alignment exhibits significant \\textbf{spatial non-uniformity}. Existing methods typically employ uniform fusion and refinement strategies across the entire image, which often introduces unnecessary linguistic perturbations in visually clear regions while failing to provide sufficient disambiguation in confused areas. To address this, we propose an \\textbf{uncertainty-guided framework} that explicitly leverages a pixel-wise \\textbf{referring uncertainty map} as a spatial prior to orchestrate adaptive inference. Specifically, we introduce a plug-and-play \\textbf{Referring Uncertainty Scorer (RUS)}, which is trained via an online error-consistency supervision strategy to interpretably predict the spatial distribution of referential ambiguity. Building on this prior, we design two plug-and-play modules: 1) \\textbf{Uncertainty-Gated Fusion (UGF)}, which dynamically modulates language injection strength to enhance constraints in high-uncertainty regions while suppressing noise in low-uncertainty ones; and 2) \\textbf{Uncertainty-Driven Local Refinement (UDLR)}, which utilizes uncertainty-derived soft masks to focus refinement on error-prone boundaries and fine details. Extensive experiments demonstrate that our method functions as a unified, plug-and-play solution that significantly improves robustness and geometric fidelity in complex remote sensing scenes without altering the backbone architecture.",
    "summary": "",
    "translation": "CroBIM-U：不确定性驱动的遥感图像指代分割",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向遥感图像分割这一计算机视觉任务，属于纯粹的视觉领域研究。虽然涉及不确定性建模，但未提及任何与推荐系统、搜索或广告相关的应用场景、架构创新或技术迁移可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  }
}