{
  "2510.23544v1": {
    "title": "LimRank: Less is More for Reasoning-Intensive Information Reranking",
    "url": "https://www.alphaxiv.org/abs/2510.23544v1",
    "arxiv_id": "2510.23544v1",
    "authors": "Tingyu Song, Yilun Zhao, Siyue Zhang, Chen Zhao, Arman Cohan",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2025-10-27 17:19:37",
    "ori_summary": "Existing approaches typically rely on large-scale fine-tuning to adapt LLMs for information reranking tasks, which is computationally expensive. In this work, we demonstrate that modern LLMs can be effectively adapted using only minimal, high-quality supervision. To enable this, we design LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating diverse, challenging, and realistic reranking examples. Using this synthetic data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and FollowIR for instruction-following retrieval. Our experiments demonstrate that LIMRANK achieves competitive performance, while being trained on less than 5% of the data typically used in prior work. Further ablation studies demonstrate the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization capabilities of LIMRANK across downstream tasks, including scientific literature search and retrieval-augmented generation for knowledge-intensive problem solving.",
    "summary": "论文研究LLM在信息重排序任务中的高效适应问题，核心思想是通过设计可复用的合成数据生成管道，仅用极少量高质量监督数据就能有效微调LLM进行重排序。",
    "translation": "LimRank：少即是多——面向推理密集型信息重排",
    "relevance_score": 8,
    "reasoning": "该论文聚焦信息重排（reranking），这是搜索和推荐系统中的核心任务。标题中'推理密集型'表明该方法可能涉及复杂的推理能力，这与LLM在搜索重排中的应用高度相关，属于'直接LLM应用'范畴。'少即是多'的表述暗示了效率优化，可能对大规模推荐和搜索系统的实际部署有重要意义。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对搜索领域的重排序任务，提出用少量高质量数据微调LLM的方法，属于LLM在搜索中的直接应用，与核心领域高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.23224v1": {
    "title": "Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment",
    "url": "https://www.alphaxiv.org/abs/2510.23224v1",
    "arxiv_id": "2510.23224v1",
    "authors": "Hongyi Wang, Zhengjie Zhu, Jiabo Ma, Fang Wang, Yue Shi, Bo Luo, Jili Wang, Qiuyu Cai, Xiuming Zhang, Yen-Wei Chen, Lanfen Lin, Hao Chen",
    "categories": "cs.CV, cs.IR",
    "pub_date": "2025-10-27 11:22:28",
    "ori_summary": "The rapid digitization of histopathology slides has opened up new possibilities for computational tools in clinical and research workflows. Among these, content-based slide retrieval stands out, enabling pathologists to identify morphologically and semantically similar cases, thereby supporting precise diagnoses, enhancing consistency across observers, and assisting example-based education. However, effective retrieval of whole slide images (WSIs) remains challenging due to their gigapixel scale and the difficulty of capturing subtle semantic differences amid abundant irrelevant content. To overcome these challenges, we present PathSearch, a retrieval framework that unifies fine-grained attentive mosaic representations with global-wise slide embeddings aligned through vision-language contrastive learning. Trained on a corpus of 6,926 slide-report pairs, PathSearch captures both fine-grained morphological cues and high-level semantic patterns to enable accurate and flexible retrieval. The framework supports two key functionalities: (1) mosaic-based image-to-image retrieval, ensuring accurate and efficient slide research; and (2) multi-modal retrieval, where text queries can directly retrieve relevant slides. PathSearch was rigorously evaluated on four public pathology datasets and three in-house cohorts, covering tasks including anatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination, and grading across diverse organs such as breast, lung, kidney, liver, and stomach. External results show that PathSearch outperforms traditional image-to-image retrieval frameworks. A multi-center reader study further demonstrates that PathSearch improves diagnostic accuracy, boosts confidence, and enhances inter-observer agreement among pathologists in real clinical scenarios. These results establish PathSearch as a scalable and generalizable retrieval solution for digital pathology.",
    "summary": "",
    "translation": "通过注意力视觉语言对齐实现精准可扩展的多模态病理检索",
    "relevance_score": 2,
    "reasoning": "该论文专注于病理学领域的多模态检索，属于医疗应用范畴，与RecSys/Search/Ads核心领域无关。虽然涉及视觉语言对齐技术，但其医疗病理学的特定领域应用使其与推荐、搜索和广告系统的技术需求相距甚远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23104v1": {
    "title": "Leveraging Hierarchical Organization for Medical Multi-document Summarization",
    "url": "https://www.alphaxiv.org/abs/2510.23104v1",
    "arxiv_id": "2510.23104v1",
    "authors": "Yi-Li Hsu, Katelyn X. Mei, Lucy Lu Wang",
    "categories": "cs.CL, cs.AI, cs.IR",
    "pub_date": "2025-10-27 08:18:02",
    "ori_summary": "Medical multi-document summarization (MDS) is a complex task that requires effectively managing cross-document relationships. This paper investigates whether incorporating hierarchical structures in the inputs of MDS can improve a model's ability to organize and contextualize information across documents compared to traditional flat summarization methods. We investigate two ways of incorporating hierarchical organization across three large language models (LLMs), and conduct comprehensive evaluations of the resulting summaries using automated metrics, model-based metrics, and domain expert evaluation of preference, understandability, clarity, complexity, relevance, coverage, factuality, and coherence. Our results show that human experts prefer model-generated summaries over human-written summaries. Hierarchical approaches generally preserve factuality, coverage, and coherence of information, while also increasing human preference for summaries. Additionally, we examine whether simulated judgments from GPT-4 align with human judgments, finding higher agreement along more objective evaluation facets. Our findings demonstrate that hierarchical structures can improve the clarity of medical summaries generated by models while maintaining content coverage, providing a practical way to improve human preference for generated summaries.",
    "summary": "",
    "translation": "利用层次化组织进行医学多文档摘要",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的多文档摘要，这属于明确的无关主题（医学领域特定应用）。虽然摘要技术本身可能有通用性，但论文明确限定在医学应用场景，且没有证据表明其技术对推荐系统、搜索或广告有直接或间接的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23077v1": {
    "title": "Think before Recommendation: Autonomous Reasoning-enhanced Recommender",
    "url": "https://www.alphaxiv.org/abs/2510.23077v1",
    "arxiv_id": "2510.23077v1",
    "authors": "Xiaoyu Kong, Junguang Jiang, Bin Liu, Ziru Xu, Han Zhu, Jian Xu, Bo Zheng, Jiancan Wu, Xiang Wang",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-27 07:26:32",
    "ori_summary": "The core task of recommender systems is to learn user preferences from historical user-item interactions. With the rapid development of large language models (LLMs), recent research has explored leveraging the reasoning capabilities of LLMs to enhance rating prediction tasks. However, existing distillation-based methods suffer from limitations such as the teacher model's insufficient recommendation capability, costly and static supervision, and superficial transfer of reasoning ability. To address these issues, this paper proposes RecZero, a reinforcement learning (RL)-based recommendation paradigm that abandons the traditional multi-model and multi-stage distillation approach. Instead, RecZero trains a single LLM through pure RL to autonomously develop reasoning capabilities for rating prediction. RecZero consists of two key components: (1) \"Think-before-Recommendation\" prompt construction, which employs a structured reasoning template to guide the model in step-wise analysis of user interests, item features, and user-item compatibility; and (2) rule-based reward modeling, which adopts group relative policy optimization (GRPO) to compute rewards for reasoning trajectories and optimize the LLM. Additionally, the paper explores a hybrid paradigm, RecOne, which combines supervised fine-tuning with RL, initializing the model with cold-start reasoning samples and further optimizing it with RL. Experimental results demonstrate that RecZero and RecOne significantly outperform existing baseline methods on multiple benchmark datasets, validating the superiority of the RL paradigm in achieving autonomous reasoning-enhanced recommender systems.",
    "summary": "论文研究如何解决现有LLM蒸馏方法在推荐系统中的局限性问题，核心思想是通过纯强化学习训练单一LLM，使其自主发展出基于结构化推理模板的评分预测能力。",
    "translation": "推荐前思考：自主推理增强的推荐系统",
    "relevance_score": 9,
    "reasoning": "该论文直接属于'直接LLM应用'范畴，提出将自主推理能力集成到推荐系统中。推理增强的推荐系统可以显著提升个性化推荐质量，通过模拟人类决策过程来更好地理解用户意图和上下文，这在搜索和广告领域具有直接应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用LLM推理能力增强推荐系统，提出自主推理的RL训练范式，完全契合直接LLM应用和核心推荐系统改进的研究方向。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.23066v1": {
    "title": "Multi-Stage Field Extraction of Financial Documents with OCR and Compact Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.23066v1",
    "arxiv_id": "2510.23066v1",
    "authors": "Yichao Jin, Yushuo Wang, Qishuai Zhong, Kent Chiu Jin-Chun, Kenneth Zhu Ke, Donald MacDonald",
    "categories": "cs.IR",
    "pub_date": "2025-10-27 06:56:08",
    "ori_summary": "Financial documents are essential sources of information for regulators, auditors, and financial institutions, particularly for assessing the wealth and compliance of Small and Medium-sized Businesses. However, SMB documents are often difficult to parse. They are rarely born digital and instead are distributed as scanned images that are none machine readable. The scans themselves are low in resolution, affected by skew or rotation, and often contain noisy backgrounds. These documents also tend to be heterogeneous, mixing narratives, tables, figures, and multilingual content within the same report. Such characteristics pose major challenges for automated information extraction, especially when relying on end to end large Vision Language Models, which are computationally expensive, sensitive to noise, and slow when applied to files with hundreds of pages. We propose a multistage pipeline that leverages traditional image processing models and OCR extraction, together with compact VLMs for structured field extraction of large-scale financial documents. Our approach begins with image pre-processing, including segmentation, orientation detection, and size normalization. Multilingual OCR is then applied to recover page-level text. Upon analyzing the text information, pages are retrieved for coherent sections. Finally, compact VLMs are operated within these narrowed-down scopes to extract structured financial indicators. Our approach is evaluated using an internal corpus of multi-lingual, scanned financial documents. The results demonstrate that compact VLMs, together with a multistage pipeline, achieves 8.8 times higher field level accuracy relative to directly feeding the whole document into large VLMs, only at 0.7 percent of the GPU cost and 92.6 percent less end-to-end service latency.",
    "summary": "",
    "translation": "使用OCR和紧凑视觉语言模型对金融文档进行多阶段字段提取",
    "relevance_score": 2,
    "reasoning": "该论文主要关注金融文档处理，属于特定领域应用，与推荐系统、搜索或广告的核心进展无关。虽然涉及视觉语言模型，但应用场景局限于金融文档字段提取，没有展示在推荐/搜索/广告领域的通用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23606v1": {
    "title": "Variational Masked Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2510.23606v1",
    "arxiv_id": "2510.23606v1",
    "authors": "Yichi Zhang, Alex Schwing, Zhizhen Zhao",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-27 17:59:57",
    "ori_summary": "Masked diffusion models have recently emerged as a flexible framework for discrete generative modeling. However, a key limitation of standard masked diffusion is its inability to effectively capture dependencies among tokens that are predicted concurrently, leading to degraded generation quality when dependencies among tokens are important. To explicitly model dependencies among tokens, we propose Variational Masked Diffusion (VMD), a framework that introduces latent variables into the masked diffusion process. Through controlled experiments on synthetic datasets, we demonstrate that VMD successfully learns dependencies that conventional masked diffusion fails to capture. We further validate the effectiveness of our approach on Sudoku puzzles and text datasets, where learning of dependencies among tokens improves global consistency. Across these domains, VMD enhances both generation quality and dependency awareness, highlighting the value of integrating variational inference into masked diffusion. Our code is available at: https://riccizz.github.io/VMD.",
    "summary": "该论文研究离散生成建模中掩码扩散模型无法有效捕捉并发预测token间依赖关系的问题，核心思想是引入隐变量到掩码扩散过程中，通过变分推断显式建模token间的依赖关系。",
    "translation": "变分掩码扩散模型",
    "relevance_score": 6,
    "reasoning": "该论文属于扩散模型领域，这是生成式AI的核心技术之一，属于'Enabling LLM Tech'范畴。扩散模型在推荐和搜索系统中具有潜在应用，例如用于生成个性化内容、数据增强或处理异构数据模态，类似于VLM处理不同数据源的方式。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出的变分掩码扩散模型通过引入隐变量建模token间依赖关系，这种增强生成一致性的方法对推荐系统中的序列生成和搜索中的文档生成具有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.23596v1": {
    "title": "Think Twice: Branch-and-Rethink Reasoning Reward Model",
    "url": "https://www.alphaxiv.org/abs/2510.23596v1",
    "arxiv_id": "2510.23596v1",
    "authors": "Yizhu Jiao, Jiaqi Zeng, Julien Veron Vialard, Oleksii Kuchaiev, Jiawei Han, Olivier Delalleau",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 17:58:07",
    "ori_summary": "Large language models (LLMs) increasingly rely on thinking models that externalize intermediate steps and allocate extra test-time compute, with think-twice strategies showing that a deliberate second pass can elicit stronger reasoning. In contrast, most reward models (RMs) still compress many quality dimensions into a single scalar in one shot, a design that induces judgment diffusion: attention spreads across evaluation criteria, yielding diluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a two-turn RM that transfers the think-twice principle to reward modeling. Turn 1 performs adaptive branching, selecting a small set of instance-critical dimensions (such as factuality and safety) and sketching concise, evidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a targeted reread that tests those hypotheses and scrutinizes only what matters most. We train with GRPO-style reinforcement learning over structured two-turn traces using a simple binary outcome reward with strict format checks, making the approach compatible with standard RLHF pipelines. By converting all-at-oncescoringintofocused, second-lookreasoning, BR-RMreducesjudgmentdiffusionandimproves sensitivity to subtle yet consequential errors while remaining practical and scalable. Experimental results demonstrate that our model achieves state-of-the-art performance on three challenging reward modeling benchmarks across diverse domains. The code and the model will be released soon.",
    "summary": "",
    "translation": "三思而后行：分支与反思推理奖励模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注推理奖励模型，属于LLM推理优化领域，与推荐系统、搜索或广告的核心技术关联较弱。虽然推理技术可能间接影响LLM在推荐/搜索中的应用质量，但缺乏明确的直接应用场景或架构创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23585v1": {
    "title": "Hope Speech Detection in Social Media English Corpora: Performance of Traditional and Transformer Models",
    "url": "https://www.alphaxiv.org/abs/2510.23585v1",
    "arxiv_id": "2510.23585v1",
    "authors": "Luis Ramos, Hiram Calvo, Olga Kolesnikova",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 17:53:40",
    "ori_summary": "The identification of hope speech has become a promised NLP task, considering the need to detect motivational expressions of agency and goal-directed behaviour on social media platforms. This proposal evaluates traditional machine learning models and fine-tuned transformers for a previously split hope speech dataset as train, development and test set. On development test, a linear-kernel SVM and logistic regression both reached a macro-F1 of 0.78; SVM with RBF kernel reached 0.77, and Na\\\"ive Bayes hit 0.75. Transformer models delivered better results, the best model achieved weighted precision of 0.82, weighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and 0.80 accuracy. These results suggest that while optimally configured traditional machine learning models remain agile, transformer architectures detect some subtle semantics of hope to achieve higher precision and recall in hope speech detection, suggesting that larges transformers and LLMs could perform better in small datasets.",
    "summary": "",
    "translation": "社交媒体英文语料库中希望言论检测：传统模型与Transformer模型的性能对比",
    "relevance_score": 2,
    "reasoning": "该论文主要关注社交媒体中的希望言论检测，这属于内容安全/内容审核领域，与推荐系统、搜索或广告的核心技术无关。虽然使用了Transformer模型，但应用场景是文本分类而非推荐/搜索/广告中的排序、匹配或用户建模等核心问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23564v1": {
    "title": "ReCode: Unify Plan and Action for Universal Granularity Control",
    "url": "https://www.alphaxiv.org/abs/2510.23564v1",
    "arxiv_id": "2510.23564v1",
    "authors": "Zhaoyang Yu, Jiayi Zhang, Huixue Su, Yufan Zhao, Yifan Wu, Mingyi Deng, Jinyu Xiang, Yizhang Lin, Lingxiao Tang, Yingchao Li, Yuyu Luo, Bang Liu, Chenglin Wu",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-27 17:35:15",
    "ori_summary": "Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.",
    "summary": "",
    "translation": "ReCode：统一规划与行动以实现通用粒度控制",
    "relevance_score": 3,
    "reasoning": "该论文标题暗示了规划与行动的统一框架，可能涉及序列决策或多步骤任务分解，这在推荐系统中可能应用于多轮对话推荐或复杂用户意图理解。然而，标题过于模糊，没有明确指向Transformer架构、LLM技术或推荐/搜索/广告的具体应用，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23558v1": {
    "title": "ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.23558v1",
    "arxiv_id": "2510.23558v1",
    "authors": "Bohan Li, Wenbin Huang, Yuhang Qiu, Yiwei Guo, Hankun Wang, Zhihan Li, Jing Peng, Ziyang Ma, Xie Chen, Kai Yu",
    "categories": "cs.SD, cs.CL, eess.AS",
    "pub_date": "2025-10-27 17:31:25",
    "ori_summary": "Large Audio Language Models (LALMs), which couple acoustic perception with large language models (LLMs) to extract and understand diverse information from audio, have attracted intense interest from both academic and industrial communities. However, existing LALMs are highly sensitive to how instructions are phrased, affecting both (i) instruction-following rates and (ii) task performance. Yet, no existing benchmarks offer a systematic and comprehensive evaluation of this sensitivity. We introduce ISA-Bench, a dynamic benchmark evaluating instruction sensitivity for LALMs along three axes: instruction description, output format, and task composition. We assess recent open-source and proprietary LALMs using ISA-Bench, profiling both compliance and accuracy under controlled instruction variations. Experimental results reveal that even state-of-the-art LALMs suffer significant instruction sensitivity, leading to degraded performance on fundamental audio understanding tasks. To mitigate this issue, we fine-tune Qwen2-Audio on a specifically constructed complex instruction-variant dataset, achieving a marked improvement in instruction-following performance. However, this also induces nontrivial catastrophic forgetting: the model loses some previously mastered task capabilities when exposed to new instruction styles. Our benchmark provides a standardized basis for assessing and improving instruction sensitivity in LALMs, underscoring the need for instruction-robust audio understanding in real-world pipelines.",
    "summary": "",
    "translation": "ISA-Bench：面向大型音频语言模型的指令敏感性基准测试",
    "relevance_score": 2,
    "reasoning": "该论文专注于音频语言模型的基准测试和指令敏感性评估，属于纯粹的评估基准范畴。虽然音频模态在理论上可能与推荐系统中的多媒体内容相关，但论文的核心焦点是基准测试而非实际应用，且没有明确说明在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23554v1": {
    "title": "A U-Net and Transformer Pipeline for Multilingual Image Translation",
    "url": "https://www.alphaxiv.org/abs/2510.23554v1",
    "arxiv_id": "2510.23554v1",
    "authors": "Siddharth Sahay, Radhika Agarwal",
    "categories": "cs.LG, cs.CL, cs.CV",
    "pub_date": "2025-10-27 17:28:55",
    "ori_summary": "This paper presents an end-to-end multilingual translation pipeline that integrates a custom U-Net for text detection, the Tesseract engine for text recognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for Neural Machine Translation (NMT). Our approach first utilizes a U-Net model, trained on a synthetic dataset , to accurately segment and detect text regions from an image. These detected regions are then processed by Tesseract to extract the source text. This extracted text is fed into a custom Transformer model trained from scratch on a multilingual parallel corpus spanning 5 languages. Unlike systems reliant on monolithic pre-trained models, our architecture emphasizes full customization and adaptability. The system is evaluated on its text detection accuracy, text recognition quality, and translation performance via BLEU scores. The complete pipeline demonstrates promising results, validating the viability of a custom-built system for translating text directly from images.",
    "summary": "",
    "translation": "基于U-Net和Transformer的多语言图像翻译流水线",
    "relevance_score": 1,
    "reasoning": "该论文专注于多语言图像翻译，属于计算机视觉领域的特定应用，与推荐系统、搜索或广告没有直接关联。虽然提到了Transformer架构，但应用场景是纯粹的图像翻译任务，没有展现出在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23538v1": {
    "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence",
    "url": "https://www.alphaxiv.org/abs/2510.23538v1",
    "arxiv_id": "2510.23538v1",
    "authors": "Qiushi Sun, Jingyang Gong, Yang Liu, Qiaosheng Chen, Lei Li, Kai Chen, Qipeng Guo, Ben Kao, Fei Yuan",
    "categories": "cs.AI, cs.CL, cs.CV, cs.SE",
    "pub_date": "2025-10-27 17:13:49",
    "ori_summary": "The scope of neural code intelligence is rapidly expanding beyond text-based source code to encompass the rich visual outputs that programs generate. This visual dimension is critical for advanced applications like flexible content generation and precise, program-driven editing of visualizations. However, progress has been impeded by the scarcity of high-quality multimodal code data, a bottleneck stemming from challenges in synthesis and quality assessment. To address these challenges, we make contributions from both a data and modeling perspective. We first introduce a complete synthesis toolkit that leverages reciprocal synergies between data modalities to efficiently produce a large-scale, high-quality corpus spanning from standard charts to complex interactive web UIs and code-driven animations. Leveraging this toolkit, we construct JanusCode-800K, the largest multimodal code corpus to date. This powers the training of our models, JanusCoder and JanusCoderV, which establish a visual-programmatic interface for generating code from textual instructions, visual inputs, or a combination of both. Our unified model is a departure from existing approaches that build specialized models for isolated tasks. Extensive experiments on both text-centric and vision-centric coding tasks demonstrate the superior performance of the JanusCoder series, with our 7B to 14B scale models approaching or even exceeding the performance of commercial models. Furthermore, extensive analysis provides key insights into harmonizing programmatic logic with its visual expression. Our code and checkpoints will are available at https://github.com/InternLM/JanusCoder.",
    "summary": "",
    "translation": "JanusCoder：迈向代码智能的基础性视觉-程序化接口",
    "relevance_score": 2,
    "reasoning": "该论文主要关注代码智能领域的视觉-程序化接口开发，属于编程辅助工具的范畴。虽然提到了视觉模态，但其核心应用场景是代码理解和生成，与推荐系统、搜索或广告的异构数据处理没有直接关联，且缺乏明确的跨模态建模在RecSys/Search/Ads中的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23536v1": {
    "title": "IPQA: A Benchmark for Core Intent Identification in Personalized Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.23536v1",
    "arxiv_id": "2510.23536v1",
    "authors": "Jieyong Kim, Maryam Amirizaniani, Soojin Yoon, Dongha Lee",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 17:12:49",
    "ori_summary": "Intent identification serves as the foundation for generating appropriate responses in personalized question answering (PQA). However, existing benchmarks evaluate only response quality or retrieval performance without directly measuring intent identification capabilities. This gap is critical because without understanding which intents users prioritize, systems cannot generate responses satisfying individual information needs. To address this, we introduce the concept of core intents: intents users prioritize when selecting answers to satisfy their information needs. To evaluate these core intents, we propose IPQA, a benchmark for core Intent identification in Personalized Question Answering. Since users do not explicitly state their prioritized intents, we derive core intents from observable behavior patterns in answer selection, grounded in satisficing theory where users choose answers meeting their acceptance thresholds. We construct a dataset with various domains through systematic filtering, LLM-based annotation, and rigorous quality control combining automated verification with human validation. Experimental evaluations across state-of-the-art language models reveal that current systems struggle with core intent identification in personalized contexts. Models fail to identify core intents from user histories, with performance degrading as question complexity increases. The code and dataset will be made publicly available to facilitate future research in this direction.",
    "summary": "",
    "translation": "IPQA：个性化问答中核心意图识别的基准",
    "relevance_score": 3,
    "reasoning": "该论文关注个性化问答中的意图识别基准，这与搜索和推荐系统中的用户意图理解有一定关联。然而，它主要侧重于问答基准构建和评估，而非核心推荐系统算法、Transformer架构改进或LLM技术的直接应用，应用潜力有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23508v1": {
    "title": "M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World Fact-Checking Dataset",
    "url": "https://www.alphaxiv.org/abs/2510.23508v1",
    "arxiv_id": "2510.23508v1",
    "authors": "Jiahui Geng, Jonathan Tonglet, Iryna Gurevych",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 16:44:35",
    "ori_summary": "Existing real-world datasets for multimodal automated fact-checking have multiple limitations: they contain few instances, focus on only one or two languages and tasks, suffer from evidence leakage, or depend on external sets of news articles for sourcing true claims. To address these shortcomings, we introduce M4FC, a new real-world dataset comprising 4,982 images paired with 6,980 claims. The images, verified by professional fact-checkers from 22 organizations, represent diverse cultural and geographic contexts. Each claim is available in one or two out of ten languages. M4FC spans six multimodal fact-checking tasks: visual claim extraction, claimant intent prediction, fake detection, image contextualization, location verification, and verdict prediction. We provide baseline results for all tasks and analyze how combining intermediate tasks influence downstream verdict prediction performance. We make our dataset and code available.",
    "summary": "",
    "translation": "M4FC：一个多模态、多语言、多文化、多任务的真实世界事实核查数据集",
    "relevance_score": 2,
    "reasoning": "该论文主要关注事实核查数据集构建，属于内容可信度验证领域。虽然多模态和多语言特性可能在某些搜索场景中有间接应用，但该工作核心是事实核查而非推荐、搜索或广告系统的核心排序或建模技术，与当前关注点关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23477v1": {
    "title": "MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring",
    "url": "https://www.alphaxiv.org/abs/2510.23477v1",
    "arxiv_id": "2510.23477v1",
    "authors": "Tengchao Yang, Sichen Guo, Mengzhao Jia, Jiaming Su, Yuanyang Liu, Zhihan Zhang, Meng Jiang",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 16:11:49",
    "ori_summary": "Effective math tutoring requires not only solving problems but also diagnosing students' difficulties and guiding them step by step. While multimodal large language models (MLLMs) show promise, existing benchmarks largely overlook these tutoring skills. We introduce MMTutorBench, the first benchmark for AI math tutoring, consisting of 685 problems built around pedagogically significant key-steps. Each problem is paired with problem-specific rubrics that enable fine-grained evaluation across six dimensions, and structured into three tasks-Insight Discovery, Operation Formulation, and Operation Execution. We evaluate 12 leading MLLMs and find clear performance gaps between proprietary and open-source systems, substantial room compared to human tutors, and consistent trends across input variants: OCR pipelines degrade tutoring quality, few-shot prompting yields limited gains, and our rubric-based LLM-as-a-Judge proves highly reliable. These results highlight both the difficulty and diagnostic value of MMTutorBench for advancing AI tutoring.",
    "summary": "",
    "translation": "MMTutorBench：首个面向AI数学辅导的多模态基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于数学辅导领域的多模态基准测试，属于教育技术领域的特定应用。这与我的核心关注点（推荐系统、搜索、广告及其相关技术）完全无关，且不涉及任何可能在这些领域应用的使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23464v1": {
    "title": "Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts",
    "url": "https://www.alphaxiv.org/abs/2510.23464v1",
    "arxiv_id": "2510.23464v1",
    "authors": "Nikesh Gyawali, Doina Caragea, Alex Vasenkov, Cornelia Caragea",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 16:03:20",
    "ori_summary": "Financial narratives from U.S. Securities and Exchange Commission (SEC) filing reports and quarterly earnings call transcripts (ECTs) are very important for investors, auditors, and regulators. However, their length, financial jargon, and nuanced language make fine-grained analysis difficult. Prior sentiment analysis in the financial domain required a large, expensive labeled dataset, making the sentence-level stance towards specific financial targets challenging. In this work, we introduce a sentence-level corpus for stance detection focused on three core financial metrics: debt, earnings per share (EPS), and sales. The sentences were extracted from Form 10-K annual reports and ECTs, and labeled for stance (positive, negative, neutral) using the advanced ChatGPT-o3-pro model under rigorous human validation. Using this corpus, we conduct a systematic evaluation of modern large language models (LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting strategies. Our results show that few-shot with CoT prompting performs best compared to supervised baselines, and LLMs' performance varies across the SEC and ECT datasets. Our findings highlight the practical viability of leveraging LLMs for target-specific stance in the financial domain without requiring extensive labeled data.",
    "summary": "",
    "translation": "基于SEC申报报告和财报电话会议记录评估大型语言模型在金融目标立场检测中的表现",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM在金融领域特定任务（立场检测）上的评估，这属于纯粹的NLP评估基准范畴。虽然使用了SEC文件和财报数据，但核心是立场检测这一NLP任务，没有展示在推荐系统、搜索或广告中的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23458v1": {
    "title": "BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents",
    "url": "https://www.alphaxiv.org/abs/2510.23458v1",
    "arxiv_id": "2510.23458v1",
    "authors": "Litu Ou, Kuan Li, Huifeng Yin, Liwen Zhang, Zhongwang Zhang, Xixi Wu, Rui Ye, Zile Qiao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 15:58:51",
    "ori_summary": "Confidence in LLMs is a useful indicator of model uncertainty and answer reliability. Existing work mainly focused on single-turn scenarios, while research on confidence in complex multi-turn interactions is limited. In this paper, we investigate whether LLM-based search agents have the ability to communicate their own confidence through verbalized confidence scores after long sequences of actions, a significantly more challenging task compared to outputting confidence in a single interaction. Experimenting on open-source agentic models, we first find that models exhibit much higher task accuracy at high confidence while having near-zero accuracy when confidence is low. Based on this observation, we propose Test-Time Scaling (TTS) methods that use confidence scores to determine answer quality, encourage the model to try again until reaching a satisfactory confidence level. Results show that our proposed methods significantly reduce token consumption while demonstrating competitive performance compared to baseline fixed budget TTS methods.",
    "summary": "",
    "translation": "BrowseConf：面向网页智能体的置信度引导测试时缩放方法",
    "relevance_score": 3,
    "reasoning": "该论文主要关注网页智能体的测试时缩放技术，属于特定应用领域的系统优化。虽然置信度引导方法在推荐系统中可能有潜在应用价值，但论文标题明确限定于网页智能体这一特定场景，与推荐系统、搜索或广告的核心技术关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23451v1": {
    "title": "Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences",
    "url": "https://www.alphaxiv.org/abs/2510.23451v1",
    "arxiv_id": "2510.23451v1",
    "authors": "Zhuoran Jin, Hongbang Yuan, Kejian Zhu, Jiachun Li, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao",
    "categories": "cs.CL, cs.AI, cs.CV",
    "pub_date": "2025-10-27 15:53:20",
    "ori_summary": "Reward models (RMs) play a critical role in aligning AI behaviors with human preferences, yet they face two fundamental challenges: (1) Modality Imbalance, where most RMs are mainly focused on text and image modalities, offering limited support for video, audio, and other modalities; and (2) Preference Rigidity, where training on fixed binary preference pairs fails to capture the complexity and diversity of personalized preferences. To address the above challenges, we propose Omni-Reward, a step toward generalist omni-modal reward modeling with support for free-form preferences, consisting of: (1) Evaluation: We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form preferences, covering nine tasks across five modalities including text, image, video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal preference dataset comprising 248K general preference pairs and 69K instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We propose Omni-RewardModel, which includes both discriminative and generative RMs, and achieves strong performance on Omni-RewardBench as well as other widely used reward modeling benchmarks.",
    "summary": "该论文研究多模态奖励建模的泛化性问题，核心思想是构建支持文本、图像、视频、音频和3D五种模态的统一奖励模型框架，通过自由形式偏好数据来捕捉复杂个性化偏好。",
    "translation": "Omni-Reward：面向通用全能模态奖励建模与自由形式偏好的研究",
    "relevance_score": 8,
    "reasoning": "该论文涉及通用多模态奖励建模，属于核心LLM技术进展，可直接应用于推荐系统和搜索中的偏好建模与排序优化。自由形式偏好的处理能力特别适用于个性化推荐和用户意图理解等场景。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出的多模态奖励建模框架可直接应用于推荐系统的多模态内容偏好建模，其自由形式偏好处理能力与个性化推荐需求高度契合。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.23443v1": {
    "title": "A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration",
    "url": "https://www.alphaxiv.org/abs/2510.23443v1",
    "arxiv_id": "2510.23443v1",
    "authors": "Chiara Bonfanti, Alessandro Druetto, Cataldo Basile, Tharindu Ranasinghe, Marcos Zampieri",
    "categories": "cs.AI, cs.CL, cs.CR, cs.MA",
    "pub_date": "2025-10-27 15:46:02",
    "ori_summary": "The growing intersection of cybersecurity and law creates a complex information space where traditional legal research tools struggle to deal with nuanced connections between cases, statutes, and technical vulnerabilities. This knowledge divide hinders collaboration between legal experts and cybersecurity professionals. To address this important gap, this work provides a first step towards intelligent systems capable of navigating the increasingly intricate cyber-legal domain. We demonstrate promising initial results on multilingual tasks.",
    "summary": "",
    "translation": "一种神经符号多智能体法律-网络安全知识集成方法",
    "relevance_score": 1,
    "reasoning": "该论文涉及法律与网络安全领域的知识集成，属于特定领域应用，与推荐系统、搜索或广告的核心技术进展无关。神经符号和多智能体方法在此上下文中没有明显的RecSys/Search/Ads应用潜力，主要关注法律网络安全这一非相关领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23396v1": {
    "title": "EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting",
    "url": "https://www.alphaxiv.org/abs/2510.23396v1",
    "arxiv_id": "2510.23396v1",
    "authors": "Musleh Alharthi, Kaleel Mahmood, Sarosh Patel, Ausif Mahmood",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 14:55:30",
    "ori_summary": "The immense success of the Transformer architecture in Natural Language Processing has led to its adoption in Time Se ries Forecasting (TSF), where superior performance has been shown. However, a recent important paper questioned their effectiveness by demonstrating that a simple single layer linear model outperforms Transformer-based models. This was soon shown to be not as valid, by a better transformer-based model termed PatchTST. More re cently, TimeLLM demonstrated even better results by repurposing a Large Language Model (LLM) for the TSF domain. Again, a follow up paper challenged this by demonstrating that removing the LLM component or replacing it with a basic attention layer in fact yields better performance. One of the challenges in forecasting is the fact that TSF data favors the more recent past, and is sometimes subject to unpredictable events. Based upon these recent insights in TSF, we propose a strong Mixture of Experts (MoE) framework. Our method combines the state-of-the-art (SOTA) models including xLSTM, en hanced Linear, PatchTST, and minGRU, among others. This set of complimentary and diverse models for TSF are integrated in a Trans former based MoE gating network. Our proposed model outperforms all existing TSF models on standard benchmarks, surpassing even the latest approaches based on MoE frameworks.",
    "summary": "",
    "translation": "EMTSF：用于时间序列预测的顶尖模型非凡混合方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注时间序列预测的模型混合技术，属于通用时序分析领域，与推荐系统、搜索或广告的核心技术关联较弱。虽然时序预测在用户行为建模中有潜在应用，但论文标题未表明其专门针对RecSys/Search/Ads场景，也未涉及Transformer架构改进或LLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23395v1": {
    "title": "Detecting Religious Language in Climate Discourse",
    "url": "https://www.alphaxiv.org/abs/2510.23395v1",
    "arxiv_id": "2510.23395v1",
    "authors": "Evy Beijen, Pien Pieterse, Yusuf Çelik, Willem Th. van Peursen, Sandjai Bhulai, Meike Morren",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 14:54:51",
    "ori_summary": "Religious language continues to permeate contemporary discourse, even in ostensibly secular domains such as environmental activism and climate change debates. This paper investigates how explicit and implicit forms of religious language appear in climate-related texts produced by secular and religious nongovernmental organizations (NGOs). We introduce a dual methodological approach: a rule-based model using a hierarchical tree of religious terms derived from ecotheology literature, and large language models (LLMs) operating in a zero-shot setting. Using a dataset of more than 880,000 sentences, we compare how these methods detect religious language and analyze points of agreement and divergence. The results show that the rule-based method consistently labels more sentences as religious than LLMs. These findings highlight not only the methodological challenges of computationally detecting religious language but also the broader tension over whether religious language should be defined by vocabulary alone or by contextual meaning. This study contributes to digital methods in religious studies by demonstrating both the potential and the limitations of approaches for analyzing how the sacred persists in climate discourse.",
    "summary": "",
    "translation": "检测气候讨论中的宗教语言",
    "relevance_score": 1,
    "reasoning": "该论文专注于气候话语中的宗教语言检测，这属于特定领域（宗教、气候）的NLP应用，与推荐系统、搜索或广告的核心技术进展完全无关。论文内容不涉及LLM技术、Transformer架构改进，也没有任何明显的应用场景可以扩展到RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23358v1": {
    "title": "How AI Forecasts AI Jobs: Benchmarking LLM Predictions of Labor Market Changes",
    "url": "https://www.alphaxiv.org/abs/2510.23358v1",
    "arxiv_id": "2510.23358v1",
    "authors": "Sheri Osborn, Rohit Valecha, H. Raghav Rao, Dan Sass, Anthony Rios",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 14:08:27",
    "ori_summary": "Artificial intelligence is reshaping labor markets, yet we lack tools to systematically forecast its effects on employment. This paper introduces a benchmark for evaluating how well large language models (LLMs) can anticipate changes in job demand, especially in occupations affected by AI. Existing research has shown that LLMs can extract sentiment, summarize economic reports, and emulate forecaster behavior, but little work has assessed their use for forward-looking labor prediction. Our benchmark combines two complementary datasets: a high-frequency index of sector-level job postings in the United States, and a global dataset of projected occupational changes due to AI adoption. We format these data into forecasting tasks with clear temporal splits, minimizing the risk of information leakage. We then evaluate LLMs using multiple prompting strategies, comparing task-scaffolded, persona-driven, and hybrid approaches across model families. We assess both quantitative accuracy and qualitative consistency over time. Results show that structured task prompts consistently improve forecast stability, while persona prompts offer advantages on short-term trends. However, performance varies significantly across sectors and horizons, highlighting the need for domain-aware prompting and rigorous evaluation protocols. By releasing our benchmark, we aim to support future research on labor forecasting, prompt design, and LLM-based economic reasoning. This work contributes to a growing body of research on how LLMs interact with real-world economic data, and provides a reproducible testbed for studying the limits and opportunities of AI as a forecasting tool in the context of labor markets.",
    "summary": "",
    "translation": "AI如何预测AI岗位：基准测试LLM对劳动力市场变化的预测能力",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM在劳动力市场预测方面的应用，这属于经济学和社会科学领域，而非推荐系统、搜索或广告的核心技术。虽然涉及LLM预测能力，但没有明确的技术创新或应用场景与RecSys/Search/Ads相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23341v1": {
    "title": "LightKGG: Simple and Efficient Knowledge Graph Generation from Textual Data",
    "url": "https://www.alphaxiv.org/abs/2510.23341v1",
    "arxiv_id": "2510.23341v1",
    "authors": "Teng Lin",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 13:55:13",
    "ori_summary": "The scarcity of high-quality knowledge graphs (KGs) remains a critical bottleneck for downstream AI applications, as existing extraction methods rely heavily on error-prone pattern-matching techniques or resource-intensive large language models (LLMs). While recent tools leverage LLMs to generate KGs, their computational demands limit accessibility for low-resource environments. Our paper introduces LightKGG, a novel framework that enables efficient KG extraction from textual data using small-scale language models (SLMs) through two key technical innovations: (1) Context-integrated Graph extraction integrates contextual information with nodes and edges into a unified graph structure, reducing the reliance on complex semantic processing while maintaining more key information; (2) Topology-enhanced relationship inference leverages the inherent topology of the extracted graph to efficiently infer relationships, enabling relationship discovery without relying on complex language understanding capabilities of LLMs. By enabling accurate KG construction with minimal hardware requirements, this work bridges the gap between automated knowledge extraction and practical deployment scenarios while introducing scientifically rigorous methods for optimizing SLM efficiency in structured NLP tasks.",
    "summary": "该论文研究如何从文本数据高效构建知识图谱的核心问题，其核心方法是利用上下文集成图提取和拓扑增强关系推理，使小规模语言模型能够在不依赖复杂语义处理的情况下完成结构化信息提取。",
    "translation": "LightKGG：从文本数据生成知识图谱的简单高效方法",
    "relevance_score": 7,
    "reasoning": "知识图谱生成技术是搜索和推荐系统中的核心使能技术，能够从文本数据中构建结构化知识表示。该方法在推荐系统中可用于构建用户兴趣图谱和商品关系图谱，在搜索中可用于增强语义理解和实体关系建模，其简单高效的特点特别适合大规模工业应用场景。",
    "rerank_relevance_score": 6,
    "rerank_reasoning": "该论文专注于知识图谱生成的高效方法，虽非直接应用于推荐系统，但其轻量化模型优化和结构化信息提取技术对资源受限的推荐场景具有潜在迁移价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.23340v1": {
    "title": "Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps",
    "url": "https://www.alphaxiv.org/abs/2510.23340v1",
    "arxiv_id": "2510.23340v1",
    "authors": "Anwesha Das, John Duff, Jörg Hoffmann, Vera Demberg",
    "categories": "cs.AI, cs.CL, cs.HC",
    "pub_date": "2025-10-27 13:54:54",
    "ori_summary": "Adaptive agent design offers a way to improve human-AI collaboration on time-sensitive tasks in rapidly changing environments. In such cases, to ensure the human maintains an accurate understanding of critical task elements, an assistive agent must not only identify the highest priority information but also estimate how and when this information can be communicated most effectively, given that human attention represents a zero-sum cognitive resource where focus on one message diminishes awareness of other or upcoming information. We introduce a theoretical framework for adaptive signalling which meets these challenges by using principles of rational communication, formalised as Bayesian reference resolution using the Rational Speech Act (RSA) modelling framework, to plan a sequence of messages which optimise timely alignment between user belief and a dynamic environment. The agent adapts message specificity and timing to the particulars of a user and scenario based on projections of how prior-guided interpretation of messages will influence attention to the interface and subsequent belief update, across several timesteps out to a fixed horizon. In a comparison to baseline methods, we show that this effectiveness depends crucially on combining multi-step planning with a realistic model of user awareness. As the first application of RSA for communication in a dynamic environment, and for human-AI interaction in general, we establish theoretical foundations for pragmatic communication in human-agent teams, highlighting how insights from cognitive science can be capitalised to inform the design of assistive agents.",
    "summary": "",
    "translation": "通过RSA进行前瞻规划：通过跨未来时间步投影用户意识实现动态环境中的高效信令",
    "relevance_score": 3,
    "reasoning": "该论文标题涉及动态环境中的信令和用户意识投影，可能属于多智能体系统或通信优化领域。虽然提到了用户意识，但没有明确连接到推荐系统、搜索或广告的核心技术。对于潜在的使能技术应用，该信令机制可能适用于动态推荐环境中的用户意图建模，但连接较为间接且不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23337v1": {
    "title": "BaZi-Based Character Simulation Benchmark: Evaluating AI on Temporal and Persona Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.23337v1",
    "arxiv_id": "2510.23337v1",
    "authors": "Siyuan Zheng, Pai Liu, Xi Chen, Jizheng Dong, Sihan Jia",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 13:51:13",
    "ori_summary": "Human-like virtual characters are crucial for games, storytelling, and virtual reality, yet current methods rely heavily on annotated data or handcrafted persona prompts, making it difficult to scale up and generate realistic, contextually coherent personas. We create the first QA dataset for BaZi-based persona reasoning, where real human experiences categorized into wealth, health, kinship, career, and relationships are represented as life-event questions and answers. Furthermore, we propose the first BaZi-LLM system that integrates symbolic reasoning with large language models to generate temporally dynamic and fine-grained virtual personas. Compared with mainstream LLMs such as DeepSeek-v3 and GPT-5-mini, our method achieves a 30.3%-62.6% accuracy improvement. In addition, when incorrect BaZi information is used, our model's accuracy drops by 20%-45%, showing the potential of culturally grounded symbolic-LLM integration for realistic character simulation.",
    "summary": "",
    "translation": "基于八字的人物模拟基准：评估AI在时序和人物角色推理上的能力",
    "relevance_score": 1,
    "reasoning": "该论文专注于基于中国传统八字理论的人物模拟基准测试，属于特定文化领域的AI评估，与推荐系统、搜索或广告的核心技术进展完全无关。论文内容涉及时序和人物角色推理，但这些是特定文化背景下的应用，没有明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23334v1": {
    "title": "Adaptive Blockwise Search: Inference-Time Alignment for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.23334v1",
    "arxiv_id": "2510.23334v1",
    "authors": "Mohammad Atif Quamar, Mohammad Areeb, Nishant Sharma, Ananth Shreekumar, Jonathan Rosenthal, Muslum Ozgur Ozmen, Mikhail Kuznetsov, Z. Berkay Celik",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 13:48:59",
    "ori_summary": "LLM alignment remains a critical challenge. Inference-time methods provide a flexible alternative to fine-tuning, but their uniform computational effort often yields suboptimal alignment. We hypothesize that for many alignment tasks, the initial tokens of a response are disproportionately more critical. To leverage this principle, we introduce AdaSearch, a novel blockwise search strategy. It adaptively allocates a fixed computational budget using a sampling schedule, focusing search effort on these critical tokens. We apply AdaSearch to sequential decoding and introduce its tree-search counterpart, AdaBeam. Our comprehensive evaluation across eight LLMs demonstrates that AdaSearch outperforms strong Best-of-N and fine-tuning baselines. Specifically, win-rates improve by over 10% for harmlessness generation, controlled sentiment generation, and for mathematical reasoning tasks relative to Best-of-N.",
    "summary": "研究LLM对齐中的计算效率问题，核心思想是根据响应中不同token的重要性差异，自适应分配固定计算预算来优化对齐效果。",
    "translation": "自适应分块搜索：大型语言模型的推理时对齐",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM推理时对齐技术，属于'Enabling LLM Tech'范畴。推理时对齐方法可直接应用于搜索和推荐系统中的实时内容生成与排序优化，通过调整模型在推理阶段的输出行为，能够提升推荐结果的相关性和搜索结果的准确性，同时保持模型效率。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出推理时对齐方法，通过自适应计算分配优化LLM对齐效果，直接适用于搜索和推荐系统的实时优化场景。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.23320v1": {
    "title": "LibriConvo: Simulating Conversations from Read Literature for ASR and Diarization",
    "url": "https://www.alphaxiv.org/abs/2510.23320v1",
    "arxiv_id": "2510.23320v1",
    "authors": "Máté Gedeon, Péter Mihajlik",
    "categories": "eess.AS, cs.CL, cs.SD",
    "pub_date": "2025-10-27 13:35:22",
    "ori_summary": "We introduce LibriConvo, a simulated multi-speaker conversational dataset based on speaker-aware conversation simulation (SASC), designed to support training and evaluation of speaker diarization and automatic speech recognition (ASR) systems. Unlike prior resources that mostly rely on semantically disconnected utterances and implausible temporal gaps, LibriConvo ensures semantic coherence and realistic conversational timing. Our pipeline leverages CallHome with external VAD for reliable boundaries, applies compression to reduce unnaturally long silences, and organizes LibriTTS utterances by book to maintain contextual consistency. Acoustic realism is enhanced via a novel room impulse response selection procedure that ranks speaker-microphone configurations by spatial plausibility, balancing realism and diversity. The dataset comprises 240.1 hours across 1,496 dialogues with 830 unique speakers, split in a speaker-disjoint manner for robust evaluation. Baselines show that the sortformer model outperforms the pyannote pipeline in diarization, while a fine-tuned Fast Conformer-CTC XLarge with Serialized Output Training achieves 7.29\\% WER for ASR, surpassing zero-shot Whisper-large-v3. LibriConvo provides a valuable resource for advancing multi-speaker speech processing research with realistic conversational dynamics and controlled experimental conditions.",
    "summary": "",
    "translation": "LibriConvo：从阅读文献中模拟对话用于自动语音识别和说话人日志",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音处理领域的自动语音识别(ASR)和说话人日志，属于纯语音技术范畴。这些技术虽然可能在某些边缘场景中与搜索相关，但缺乏与推荐系统、广告或核心LLM/Transformer技术的直接关联，也不涉及异构数据建模或推荐/搜索/广告领域的核心进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23319v1": {
    "title": "Arabic Little STT: Arabic Children Speech Recognition Dataset",
    "url": "https://www.alphaxiv.org/abs/2510.23319v1",
    "arxiv_id": "2510.23319v1",
    "authors": "Mouhand Alkadri, Dania Desouki, Khloud Al Jallad",
    "categories": "cs.CL, cs.AI, cs.HC, cs.LG, cs.SD",
    "pub_date": "2025-10-27 13:30:54",
    "ori_summary": "The performance of Artificial Intelligence (AI) systems fundamentally depends on high-quality training data. However, low-resource languages like Arabic suffer from severe data scarcity. Moreover, the absence of child-specific speech corpora is an essential gap that poses significant challenges. To address this gap, we present our created dataset, Arabic Little STT, a dataset of Levantine Arabic child speech recorded in classrooms, containing 355 utterances from 288 children (ages 6 - 13). We further conduct a systematic assessment of Whisper, a state-of-the-art automatic speech recognition (ASR) model, on this dataset and compare its performance with adult Arabic benchmarks. Our evaluation across eight Whisper variants reveals that even the best-performing model (Large_v3) struggles significantly, achieving a 0.66 word error rate (WER) on child speech, starkly contrasting with its sub 0.20 WER on adult datasets. These results align with other research on English speech. Results highlight the critical need for dedicated child speech benchmarks and inclusive training data in ASR development. Emphasizing that such data must be governed by strict ethical and privacy frameworks to protect sensitive child information. We hope that this study provides an initial step for future work on equitable speech technologies for Arabic-speaking children. We hope that our publicly available dataset enrich the children's demographic representation in ASR datasets.",
    "summary": "",
    "translation": "阿拉伯语儿童语音识别数据集：Arabic Little STT",
    "relevance_score": 1,
    "reasoning": "该论文专注于阿拉伯语儿童语音识别数据集，属于语音处理领域，与搜索、推荐或广告系统的核心进展无关。语音识别技术本身在搜索或广告中可能有边缘应用，但该特定数据集针对儿童阿拉伯语，应用范围极其有限，且不属于当前关注的任何技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23284v1": {
    "title": "DCMM-SQL: Automated Data-Centric Pipeline and Multi-Model Collaboration Training for Text-to-SQL Model",
    "url": "https://www.alphaxiv.org/abs/2510.23284v1",
    "arxiv_id": "2510.23284v1",
    "authors": "Yuanzhen Xie, Liu Ye, Jiqun Chu, Mochi Gao, Hehuan Liu, Yunzhi Tan, Bo Hu, Zang Li",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 12:53:39",
    "ori_summary": "Text-to-SQL tasks have gained attractive improvements since the release of ChatGPT. Among them, agent-based frameworks have been widely used in this field. However, the impact of data-centric strategies on text-to-SQL tasks has rarely been explored. In this paper, we systemically design a fully automated data-centric pipeline for text-to-SQL tasks, including \\emph{adaptive data repair}, which can automatically find and fix errors in the training dataset; and \\emph{error data augmentation}, where we specifically diffuse and enhance erroneous data predicted by the initially trained models. Meanwhile, we propose a Multi-Model collaboration training schema, aiming to train multiple models with different augmented data, enabling them to possess distinct capabilities and work together to complement each other, because it has been found that the capability of a single fine-tuned model is very limited. Furthermore, we utilize an ensemble strategy to integrate the capabilities of multiple models to solve a multiple-choice question, aiming to further improve the accuracy of text-to-SQL tasks. The experiment results and ablation study have demonstrated the effectiveness of data-centric pipeline and Multi-Model(MM) interactive iterative strategies, achieving first place in lightweight text-to-SQL models (within 70B).",
    "summary": "",
    "translation": "DCMM-SQL：面向文本到SQL模型的自动化数据为中心流程与多模型协作训练",
    "relevance_score": 3,
    "reasoning": "该论文主要关注文本到SQL任务的自动化数据流程和多模型协作训练，属于特定领域的数据库查询应用。虽然多模型协作训练与多模态建模有一定关联，但其核心应用场景（文本到SQL）与推荐系统、搜索或广告的关联度较低，且未明确展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23276v1": {
    "title": "A Cocktail-Party Benchmark: Multi-Modal dataset and Comparative Evaluation Results",
    "url": "https://www.alphaxiv.org/abs/2510.23276v1",
    "arxiv_id": "2510.23276v1",
    "authors": "Thai-Binh Nguyen, Katerina Zmolikova, Pingchuan Ma, Ngoc Quan Pham, Christian Fuegen, Alexander Waibel",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 12:36:43",
    "ori_summary": "We introduce the task of Multi-Modal Context-Aware Recognition (MCoRec) in the ninth CHiME Challenge, which addresses the cocktail-party problem of overlapping conversations in a single-room setting using audio, visual, and contextual cues. MCoRec captures natural multi-party conversations where the recordings focus on unscripted, casual group chats, leading to extreme speech overlap of up to 100% and highly fragmented conversational turns. The task requires systems to answer the question \"Who speaks when, what, and with whom?\" by jointly transcribing each speaker's speech and clustering them into their respective conversations from audio-visual recordings. Audio-only baselines exceed 100% word error rate, whereas incorporating visual cues yields substantial 50% improvements, highlighting the importance of multi-modality. In this manuscript, we present the motivation behind the task, outline the data collection process, and report the baseline systems developed for the MCoRec.",
    "summary": "",
    "translation": "鸡尾酒会基准：多模态数据集与对比评估结果",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态数据集构建和基准评估，属于通用评估基准范畴。虽然多模态数据在推荐和搜索中有应用潜力，但论文标题明确聚焦于基准测试和评估结果，这属于您排除的'评估基准'和'纯粹NLP中心主题'范畴，缺乏对核心架构或直接应用的明确指向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23272v1": {
    "title": "Code Aesthetics with Agentic Reward Feedback",
    "url": "https://www.alphaxiv.org/abs/2510.23272v1",
    "arxiv_id": "2510.23272v1",
    "authors": "Bang Xiao, Lingjie Jiang, Shaohan Huang, Tengchao Lv, Yupan Huang, Xun Wu, Lei Cui, Furu Wei",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 12:32:33",
    "ori_summary": "Large Language Models (LLMs) have become valuable assistants for developers in code-related tasks. While LLMs excel at traditional programming tasks such as code generation and bug fixing, they struggle with visually-oriented coding tasks, often producing suboptimal aesthetics. In this paper, we introduce a new pipeline to enhance the aesthetic quality of LLM-generated code. We first construct AesCode-358K, a large-scale instruction-tuning dataset focused on code aesthetics. Next, we propose agentic reward feedback, a multi-agent system that evaluates executability, static aesthetics, and interactive aesthetics. Building on this, we develop GRPO-AR, which integrates these signals into the GRPO algorithm for joint optimization of functionality and code aesthetics. Finally, we develop OpenDesign, a benchmark for assessing code aesthetics. Experimental results show that combining supervised fine-tuning on AesCode-358K with reinforcement learning using agentic reward feedback significantly improves performance on OpenDesign and also enhances results on existing benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o and GPT-4.1, and achieves performance comparable to large open-source models with 480B-685B parameters, underscoring the effectiveness of our approach.",
    "summary": "",
    "translation": "基于智能体奖励反馈的代码美学",
    "relevance_score": 1,
    "reasoning": "该论文关注代码美学和智能体奖励反馈，属于AIGC和代码生成领域，与推荐系统、搜索或广告的核心技术无关。论文主题更偏向编程辅助和代码质量评估，没有明显的应用场景可以迁移到RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23271v1": {
    "title": "Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation and User Intent Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.23271v1",
    "arxiv_id": "2510.23271v1",
    "authors": "Mohammed Aljafari, Ismail Alturki, Ahmed Mori, Yehya Kadumi",
    "categories": "cs.CL, 68T50 (68T50 Natural language processing), I.2.7; I.2.6; I.2.0; H.3.3",
    "pub_date": "2025-10-27 12:29:27",
    "ori_summary": "Mubeen is a proprietary Arabic language model developed by MASARAT SA, optimized for deep understanding of Arabic linguistics, Islamic studies, and cultural heritage. Trained on an extensive collection of authentic Arabic sources significantly expanded by digitizing historical manuscripts via a proprietary Arabic OCR engine, the model incorporates seminal scholarly works in linguistics, jurisprudence, hadith, and Quranic exegesis, alongside thousands of academic theses and peer-reviewed research papers. Conditioned through a deep linguistic engineering framework, Mubeen masters not just the meaning but the eloquence of Arabic, enabling precise understanding across classical texts, contemporary writing, and regional dialects with focus on comprehending user intent and delivering accurate, contextually relevant responses. Unlike other Arabic models relying on translated English data that often fail in intent detection or retrieval-augmented generation (RAG), Mubeen uses native Arabic sources to ensure cultural authenticity and accuracy. Its core innovation is the Practical Closure Architecture, designed to solve the \"Utility Gap Crisis\" where factually correct answers fail to resolve users' core needs, forcing them into frustrating cycles of re-prompting. By prioritizing clarity and decisive guidance, Mubeen transforms from an information repository into a decisive guide, aligning with Saudi Vision 2030. The model's architecture combines deep heritage specialization with multi-disciplinary expert modules, enabling robust performance across both cultural preservation and general knowledge domains.",
    "summary": "",
    "translation": "Mubeen AI：面向文化遗产保护与用户意图理解的专用阿拉伯语语言模型",
    "relevance_score": 3,
    "reasoning": "该论文涉及特定领域语言模型和用户意图理解，这与搜索和推荐系统中的查询理解有一定相关性。然而，它主要专注于阿拉伯语文化遗产保护这一特定应用领域，与核心推荐系统、搜索或广告的直接关联有限，且未明确涉及Transformer架构创新或跨模态建模等关键技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23252v1": {
    "title": "Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?",
    "url": "https://www.alphaxiv.org/abs/2510.23252v1",
    "arxiv_id": "2510.23252v1",
    "authors": "Tawsif Tashwar Dipto, Azmol Hossain, Rubayet Sabbir Faruque, Md. Rezuwan Hassan, Kanij Fatema, Tanmoy Shome, Ruwad Naswan, Md. Foriduzzaman Zihad, Mohaymen Ul Anam, Nazia Tasnim, Hasan Mahmud, Md Kamrul Hasan, Md. Mehedi Hasan Shawon, Farig Sadeque, Tahsin Reasat",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 12:14:52",
    "ori_summary": "Conventional research on speech recognition modeling relies on the canonical form for most low-resource languages while automatic speech recognition (ASR) for regional dialects is treated as a fine-tuning task. To investigate the effects of dialectal variations on ASR we develop a 78-hour annotated Bengali Speech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and data-driven perspectives shows that speech foundation models struggle heavily in regional dialect ASR, both in zero-shot and fine-tuned settings. We observe that all deep learning methods struggle to model speech data under dialectal variations but dialect specific model training alleviates the issue. Our dataset also serves as a out of-distribution (OOD) resource for ASR modeling under constrained resources in ASR algorithms. The dataset and code developed for this project are publicly available",
    "summary": "",
    "translation": "ASR基础模型是否足够泛化以捕捉低资源语言区域方言的特征？",
    "relevance_score": 2,
    "reasoning": "该论文主要关注语音识别(ASR)基础模型在低资源语言方言上的泛化能力，这属于语音处理领域。虽然语音识别在搜索中有潜在应用，但论文焦点是方言和低资源语言的技术挑战，与推荐系统、广告或搜索排名的核心关注点关联较弱，且未明确涉及LLM或Transformer架构在推荐/搜索/广告中的直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23217v1": {
    "title": "Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports",
    "url": "https://www.alphaxiv.org/abs/2510.23217v1",
    "arxiv_id": "2510.23217v1",
    "authors": "Alois Thomas, Maya Varma, Jean-Benoit Delbrouck, Curtis P. Langlotz",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 11:08:05",
    "ori_summary": "Automating radiology report generation with Large Vision-Language Models (LVLMs) holds great potential, yet these models often produce clinically critical hallucinations, posing serious risks. Existing hallucination detection methods frequently lack the necessary sentence-level granularity or robust generalization across different LVLM generators. We introduce a novel approach: a sentence-level Process Reward Model (PRM) adapted for this vision-language task. Our PRM predicts the factual correctness of each generated sentence, conditioned on clinical context and preceding text. When fine-tuned on MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM outperforms existing verification techniques, demonstrating, for instance, relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods reliant on internal model states, our PRM demonstrates strong generalization to an unseen LVLM. We further show its practical utility: PRM scores effectively filter low-quality reports, improving F1-CheXbert scores by 4.5% (when discarding the worst 10% of reports). Moreover, when guiding a novel weighted best-of-N selection process on the MIMIC-CXR test set, our PRM show relative improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for BERTScore. These results demonstrate that a lightweight, context-aware PRM provides a model-agnostic safety layer for clinical LVLMs without access to internal activations",
    "summary": "",
    "translation": "用于LVLM放射学报告句子级验证的过程奖励模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于放射学领域的医学报告验证，这属于明确的无关主题（医学/生物学应用）。虽然提到了LVLM（大型视觉语言模型），但应用场景是医学放射学报告，与推荐系统、搜索或广告领域没有任何关联。该技术没有明显的潜力应用于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23198v1": {
    "title": "PTPP-Aware Adaptation Scaling Laws: Predicting Domain-Adaptation Performance at Unseen Pre-Training Budgets",
    "url": "https://www.alphaxiv.org/abs/2510.23198v1",
    "arxiv_id": "2510.23198v1",
    "authors": "Etienne Goffinet, Shane Bergsma, Avraham Sheinin, Natalia Vassilieva, Shaheer Muhammad, Preslav Nakov, Gurpreet Gosal",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-27 10:36:15",
    "ori_summary": "Continual pre-training (CPT) for domain adaptation must balance target-domain gains with stability on the base domain. Existing CPT scaling laws typically assume a fixed pre-training budget, which limits their ability to forecast adaptation outcomes for models trained at different tokens-per-parameter (PTPP). We present \\emph{PTPP-aware} adaptation scaling laws that make the pre-training budget an explicit variable, enabling accurate \\emph{prediction} of adaptation loss at unseen \\ptpp. On a multilingual setup (English/Arabic $\\rightarrow$ French), PTPP-aware formulations trained on early stages (\\ptpp{}=\\{15,31\\}) predict target loss at \\ptpp{}=279 and outperform a PTPP-agnostic \\dcpt{} transfer baseline on metrics (Huber-on-log, MAE$_\\mathrm{rel}$, calibration slope); full diagnostics (RMSE, MAPE) are in the appendix. Beyond forecasting, we show a practical use case: planning replay ratios and adaptation token budgets that satisfy target and forgetting constraints under compute limits.",
    "summary": "",
    "translation": "PTPP感知的自适应缩放定律：在未见过的预训练预算下预测领域自适应性能",
    "relevance_score": 3,
    "reasoning": "该论文关注预训练预算与领域自适应性能的缩放关系，属于LLM效率优化的范畴。虽然缩放定律研究对LLM技术发展有贡献，但其与推荐/搜索/广告系统的直接应用关联较弱，主要聚焦于预训练阶段的计算效率，而非下游任务的具体应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23189v1": {
    "title": "DREaM: Drug-Drug Relation Extraction via Transfer Learning Method",
    "url": "https://www.alphaxiv.org/abs/2510.23189v1",
    "arxiv_id": "2510.23189v1",
    "authors": "Ali Fata, Hossein Rahmani, Parinaz Soltanzadeh, Amirhossein Derakhshan, Behrouz Minaei Bidgoli",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-27 10:27:00",
    "ori_summary": "Relation extraction between drugs plays a crucial role in identifying drug drug interactions and predicting side effects. The advancement of machine learning methods in relation extraction, along with the development of large medical text databases, has enabled the low cost extraction of such relations compared to other approaches that typically require expert knowledge. However, to the best of our knowledge, there are limited datasets specifically designed for drug drug relation extraction currently available. Therefore, employing transfer learning becomes necessary to apply machine learning methods in this domain. In this study, we propose DREAM, a method that first employs a trained relation extraction model to discover relations between entities and then applies this model to a corpus of medical texts to construct an ontology of drug relationships. The extracted relations are subsequently validated using a large language model. Quantitative results indicate that the LLM agreed with 71 of the relations extracted from a subset of PubMed abstracts. Furthermore, our qualitative analysis indicates that this approach can uncover ambiguities in the medical domain, highlighting the challenges inherent in relation extraction in this field.",
    "summary": "",
    "translation": "DREaM：基于迁移学习方法的药物-药物关系抽取",
    "relevance_score": 1,
    "reasoning": "该论文专注于药物-药物关系抽取，属于生物医学领域的特定应用，与推荐系统、搜索或广告的核心技术无关。迁移学习方法虽然通用，但论文的应用场景明确限定在药物关系这一非相关领域，没有任何潜在的应用于RecSys/Search/Ads的可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23182v1": {
    "title": "SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations",
    "url": "https://www.alphaxiv.org/abs/2510.23182v1",
    "arxiv_id": "2510.23182v1",
    "authors": "Shuai Huang, Wenxuan Zhao, Jun Gao",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 10:21:46",
    "ori_summary": "As large language models (LLMs) develop anthropomorphic abilities, they are increasingly being deployed as autonomous agents to interact with humans. However, evaluating their performance in realistic and complex social interactions remains a significant challenge. Most previous research built datasets through simulated agent-to-agent interactions, which fails to capture the authentic linguistic styles and relational dynamics found in real human conversations. To address this gap, we introduce SI-Bench, a novel benchmark designed to evaluate aspects of social intelligence in LLMs. Grounded in broad social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues collected from a social networking application. We further selected a subset of 312 dialogues for manual annotation across 8 major models. The experiments show that SOTA models have surpassed the human expert in process reasoning under complex social situations, yet they still fall behind humans in reply quality. Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the performance of LLMs in social dialogue tasks. All datasets are openly available at https://github.com/SI-Bench/SI-Bench.git.",
    "summary": "",
    "translation": "SI-Bench：评估大语言模型在人与人对话中的社交智能基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于评估LLM在人类对话中的社交智能，属于纯粹的评估基准研究。虽然涉及LLM技术，但其核心关注点（社交智能评估、对话能力）与推荐系统、搜索或广告领域没有直接关联，且明确排除了评估基准等纯粹NLP中心化主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23169v1": {
    "title": "MATCH: Task-Driven Code Evaluation through Contrastive Learning",
    "url": "https://www.alphaxiv.org/abs/2510.23169v1",
    "arxiv_id": "2510.23169v1",
    "authors": "Marah Ghoummaid, Vladimir Tchuiev, Ofek Glick, Michal Moschkovitz, Dotan Di Castro",
    "categories": "cs.CL, cs.SE",
    "pub_date": "2025-10-27 09:51:49",
    "ori_summary": "AI-based code generation is increasingly prevalent, with GitHub Copilot estimated to generate 46% of the code on GitHub. Accurately evaluating how well generated code aligns with developer intent remains a critical challenge. Traditional evaluation methods, such as unit tests, are often unscalable and costly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code functionality, and metrics like CodeBERTScore require reference code, which is not always available. To address the gap in reference-free evaluation, with few alternatives such as ICE-Score, this paper introduces MATCH, a novel reference-free metric. MATCH uses Contrastive Learning to generate meaningful embeddings for code and natural language task descriptions, enabling similarity scoring that reflects how well generated code implements the task. We show that MATCH achieves stronger correlations with functional correctness and human preference than existing metrics across multiple programming languages.",
    "summary": "",
    "translation": "MATCH：基于对比学习的任务驱动代码评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于代码评估和对比学习，属于软件工程和程序分析领域，与推荐系统、搜索或广告的核心技术无直接关联。论文内容涉及代码质量评估和任务驱动的编程分析，这些主题不在当前关注的LLM技术、Transformer架构或推荐系统应用范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23163v1": {
    "title": "Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.23163v1",
    "arxiv_id": "2510.23163v1",
    "authors": "Hang Lei, Shengyi Zong, Zhaoyan Li, Ziren Zhou, Hao Liu",
    "categories": "cs.CL, cs.AI, I.2.0",
    "pub_date": "2025-10-27 09:41:29",
    "ori_summary": "The screenplay serves as the foundation for television production, defining narrative structure, character development, and dialogue. While Large Language Models (LLMs) show great potential in creative writing, direct end-to-end generation approaches often fail to produce well-crafted screenplays. We argue this failure stems from forcing a single model to simultaneously master two disparate capabilities: creative narrative construction and rigid format adherence. The resulting outputs may mimic superficial style but lack the deep structural integrity and storytelling substance required for professional use. To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage Refinement (DSR), a decomposed framework that decouples creative narrative generation from format conversion. The first stage transforms a brief outline into rich, novel-style prose. The second stage refines this narrative into a professionally formatted screenplay. This separation enables the model to specialize in one distinct capability at each stage. A key challenge in implementing DSR is the scarcity of paired outline-to-novel training data. We address this through hybrid data synthesis: reverse synthesis deconstructs existing screenplays into structured inputs, while forward synthesis leverages these inputs to generate high-quality narrative texts as training targets. Blind evaluations by professional screenwriters show that DSR achieves a 75% win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of human-level performance. Our work demonstrates that decomposed generation architecture with tailored data synthesis effectively specializes LLMs in complex creative domains.",
    "summary": "",
    "translation": "超越直接生成：基于大语言模型的分解式精心剧本创作方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于使用LLMs进行剧本创作的AIGC应用，这属于纯粹的LLM内容生成范畴。虽然涉及分解方法，但其核心应用领域（剧本创作）与搜索、推荐或广告系统没有任何关联，完全落在被排除的AIGC和内容生成主题范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23160v1": {
    "title": "ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix",
    "url": "https://www.alphaxiv.org/abs/2510.23160v1",
    "arxiv_id": "2510.23160v1",
    "authors": "Zile Yang, Ling Li, Na Di, Jinlong Pang, Yao Zhou, Hao Cheng, Bo Han, Jiaheng Wei",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 09:39:22",
    "ori_summary": "Supervised Fine-Tuning (SFT) adapts pre-trained Large Language Models (LLMs) to domain-specific instructions by training on a carefully curated subset of high-quality instruction-response pairs, typically drawn from a larger dataset that often contains many low-quality or noisy samples. However, existing quality-first paradigms often overlook valuable signals in discarded low-quality data and rely on imperfect quality filters. We introduce ENTP (Enhancing low-quality SFT data via Neural-symbolic Text Purge-Mix), a framework that revitalizes low-quality corpora through symbolic purification and neural reconstruction. The symbolic module identifies and prunes noisy samples based on statistical priors, while the neural component synthesizes enriched instruction-response pairs by leveraging latent representations and model knowledge. This neural-symbolic synergy enhances data informativeness and diversity. Experiments show that ENTP-augmented datasets, constructed exclusively from low-quality data, outperform 13 established data-selection baselines across five instruction-following benchmarks, and even surpass fine-tuning on the full original dataset (approximately 300K examples). Our results highlight the untapped potential of low-quality data and underscore the importance of intelligent purification and synthesis for efficient instruction alignment.",
    "summary": "论文研究如何利用被丢弃的低质量SFT数据提升指令微调效果。核心方法是通过符号模块识别并修剪噪声样本，结合神经模块利用潜在表示和模型知识合成增强的指令-响应对，实现神经符号协同的数据净化与重建。",
    "translation": "ENTP：通过神经符号文本净化混合增强低质量监督微调数据",
    "relevance_score": 8,
    "reasoning": "该论文涉及提升低质量监督微调(SFT)数据质量的方法，这直接属于'Enabling LLM Tech'范畴。在推荐系统、搜索和广告领域，高质量的训练数据对于构建准确的用户意图理解和内容匹配模型至关重要，该方法可以提升这些领域LLM应用的性能基础。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出通过神经符号方法净化低质量SFT数据，直接提升LLM指令微调效果，属于LLM核心技术进步，对搜索推荐系统有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.23142v1": {
    "title": "Rethinking GSPO: The Perplexity-Entropy Equivalence",
    "url": "https://www.alphaxiv.org/abs/2510.23142v1",
    "arxiv_id": "2510.23142v1",
    "authors": "Chi Liu",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-27 09:19:10",
    "ori_summary": "We provide a new perspective on GSPO's length-normalized importance ratios by establishing their connection to information-theoretic quantities. We show that GSPO's sequence-level weight $s(\\theta) = (\\pi_\\theta/\\pi_{\\theta_{\\text{old}}})^{1/|y|}$ can be equivalently expressed as the inverse perplexity ratio $\\text{PPL}_{\\theta_{\\text{old}}}/\\text{PPL}_\\theta$ and as the exponential cross-entropy change $\\exp(\\Delta H)$. While the perplexity-entropy relationship follows from standard definitions, this observation provides a useful lens for understanding GSPO: the algorithm weights policy gradient updates by perplexity ratios, offering an information-theoretic interpretation of the importance weights. This perspective helps explain GSPO's empirical properties, including log-domain variance reduction through geometric averaging and stability in training mixture-of-experts models. We validate the mathematical equivalences and variance predictions through controlled experiments on mathematical reasoning tasks.",
    "summary": "",
    "translation": "重新思考GSPO：困惑度与熵等价性",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及困惑度与熵的理论等价性研究，这属于信息论和概率论的基础理论范畴。论文没有显示出与推荐系统、搜索、广告或相关使能技术的直接关联，也没有表明这些理论概念在推荐系统、搜索或广告领域有具体的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23131v1": {
    "title": "Corpus Frequencies in Morphological Inflection: Do They Matter?",
    "url": "https://www.alphaxiv.org/abs/2510.23131v1",
    "arxiv_id": "2510.23131v1",
    "authors": "Tomáš Sourada, Jana Straková",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 09:12:04",
    "ori_summary": "The traditional approach to morphological inflection (the task of modifying a base word (lemma) to express grammatical categories) has been, for decades, to consider lexical entries of lemma-tag-form triples uniformly, lacking any information about their frequency distribution. However, in production deployment, one might expect the user inputs to reflect a real-world distribution of frequencies in natural texts. With future deployment in mind, we explore the incorporation of corpus frequency information into the task of morphological inflection along three key dimensions during system development: (i) for train-dev-test split, we combine a lemma-disjoint approach, which evaluates the model's generalization capabilities, with a frequency-weighted strategy to better reflect the realistic distribution of items across different frequency bands in training and test sets; (ii) for evaluation, we complement the standard type accuracy (often referred to simply as accuracy), which treats all items equally regardless of frequency, with token accuracy, which assigns greater weight to frequent words and better approximates performance on running text; (iii) for training data sampling, we introduce a method novel in the context of inflection, frequency-aware training, which explicitly incorporates word frequency into the sampling process. We show that frequency-aware training outperforms uniform sampling in 26 out of 43 languages.",
    "summary": "",
    "translation": "语料库频率在形态屈折变化中的作用：它们重要吗？",
    "relevance_score": 1,
    "reasoning": "这篇论文关注形态屈折变化中的语料库频率问题，属于纯粹的语言学/NLP领域，与推荐系统、搜索或广告的核心技术进展没有直接关联。该研究不涉及Transformer架构改进、多模态建模或任何可能应用于RecSys/Search/Ads的LLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23123v1": {
    "title": "Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation",
    "url": "https://www.alphaxiv.org/abs/2510.23123v1",
    "arxiv_id": "2510.23123v1",
    "authors": "Shiwei Li, Xiandi Luo, Haozhao Wang, Xing Tang, Ziqiang Cui, Dugang Liu, Yuhua Li, Xiuqiang He, Ruixuan Li",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-27 08:57:24",
    "ori_summary": "Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method widely used in large language models (LLMs). LoRA essentially describes the projection of an input space into a low-dimensional output space, with the dimensionality determined by the LoRA rank. In standard LoRA, all input tokens share the same weights and undergo an identical input-output projection. This limits LoRA's ability to capture token-specific information due to the inherent semantic differences among tokens. To address this limitation, we propose Token-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts LoRA weights according to the input token, thereby learning token-wise input-output projections in an end-to-end manner. Formally, the weights of TopLoRA can be expressed as $B\\Sigma_X A$, where $A$ and $B$ are low-rank matrices (as in standard LoRA), and $\\Sigma_X$ is a diagonal matrix generated from each input token $X$. Notably, TopLoRA does not increase the rank of LoRA weights but achieves more granular adaptation by learning token-wise LoRA weights (i.e., token-wise input-output projections). Extensive experiments across multiple models and datasets demonstrate that TopLoRA consistently outperforms LoRA and its variants. The code is available at https://github.com/Leopold1423/toplora-neurips25.",
    "summary": "论文研究标准LoRA方法中所有输入token共享相同权重导致无法捕获token特定信息的问题，核心创新是提出TopLoRA方法，通过为每个输入token动态生成对角矩阵来学习token-wise的输入-输出投影，实现更细粒度的适配而不增加LoRA秩。",
    "translation": "超越更高秩：面向高效低秩适应的逐令牌输入输出投影",
    "relevance_score": 8,
    "reasoning": "该论文专注于Transformer架构的效率优化，属于'赋能Transformer技术'类别。通过逐令牌输入输出投影的低秩适应方法，可以显著降低LLM微调和推理的计算成本，这对于需要频繁更新和实时服务的推荐系统、搜索和广告应用具有直接价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出TopLoRA方法改进LoRA的权重共享限制，通过token-wise投影实现更细粒度的适配，直接属于Transformer架构效率优化和LLM高效微调技术范畴。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.23114v1": {
    "title": "Flexing in 73 Languages: A Single Small Model for Multilingual Inflection",
    "url": "https://www.alphaxiv.org/abs/2510.23114v1",
    "arxiv_id": "2510.23114v1",
    "authors": "Tomáš Sourada, Jana Straková",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 08:34:41",
    "ori_summary": "We present a compact, single-model approach to multilingual inflection, the task of generating inflected word forms from base lemmas to express grammatical categories. Our model, trained jointly on data from 73 languages, is lightweight, robust to unseen words, and outperforms monolingual baselines in most languages. This demonstrates the effectiveness of multilingual modeling for inflection and highlights its practical benefits: simplifying deployment by eliminating the need to manage and retrain dozens of separate monolingual models. In addition to the standard SIGMORPHON shared task benchmarks, we evaluate our monolingual and multilingual models on 73 Universal Dependencies (UD) treebanks, extracting lemma-tag-form triples and their frequency counts. To ensure realistic data splits, we introduce a novel frequency-weighted, lemma-disjoint train-dev-test resampling procedure. Our work addresses the lack of an open-source, general-purpose, multilingual morphological inflection system capable of handling unseen words across a wide range of languages, including Czech. All code is publicly released at: https://github.com/tomsouri/multilingual-inflection.",
    "summary": "",
    "translation": "在73种语言中灵活变形：用于多语言词形变化的单一小型模型",
    "relevance_score": 2,
    "reasoning": "该论文专注于多语言词形变化这一特定NLP任务，属于语言建模的专门应用领域。虽然涉及多语言能力，但词形变化任务与推荐系统、搜索或广告的核心技术需求（如排序、召回、用户建模）没有直接关联，且没有明确的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23090v1": {
    "title": "MAP4TS: A Multi-Aspect Prompting Framework for Time-Series Forecasting with Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.23090v1",
    "arxiv_id": "2510.23090v1",
    "authors": "Suchan Lee, Jihoon Choi, Sohyeon Lee, Minseok Song, Bong-Gyu Jang, Hwanjo Yu, Soyeon Caren Han",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 07:51:54",
    "ori_summary": "Recent advances have investigated the use of pretrained large language models (LLMs) for time-series forecasting by aligning numerical inputs with LLM embedding spaces. However, existing multimodal approaches often overlook the distinct statistical properties and temporal dependencies that are fundamental to time-series data. To bridge this gap, we propose MAP4TS, a novel Multi-Aspect Prompting Framework that explicitly incorporates classical time-series analysis into the prompt design. Our framework introduces four specialized prompt components: a Global Domain Prompt that conveys dataset-level context, a Local Domain Prompt that encodes recent trends and series-specific behaviors, and a pair of Statistical and Temporal Prompts that embed handcrafted insights derived from autocorrelation (ACF), partial autocorrelation (PACF), and Fourier analysis. Multi-Aspect Prompts are combined with raw time-series embeddings and passed through a cross-modality alignment module to produce unified representations, which are then processed by an LLM and projected for final forecasting. Extensive experiments across eight diverse datasets show that MAP4TS consistently outperforms state-of-the-art LLM-based methods. Our ablation studies further reveal that prompt-aware designs significantly enhance performance stability and that GPT-2 backbones, when paired with structured prompts, outperform larger models like LLaMA in long-term forecasting tasks.",
    "summary": "该论文研究如何改进LLM在时间序列预测中的表现，核心方法是设计包含全局领域、局部领域、统计和时序四个专门提示组件的多模态提示框架，将传统时间序列分析（如自相关、傅里叶分析）的统计特性融入LLM输入设计。",
    "translation": "MAP4TS：基于大语言模型的时间序列预测多角度提示框架",
    "relevance_score": 8,
    "reasoning": "该论文提出的大语言模型提示框架直接属于'直接LLM应用'范畴，时间序列预测在推荐系统和广告领域有广泛应用，如用户行为预测、广告点击率预测等。多角度提示框架的创新方法可以提升预测准确性，为推荐和广告系统的时序建模提供新的技术路径。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用LLM技术解决时间序列预测问题，通过多模态提示设计将传统统计分析与LLM结合，在搜索推荐广告的时序预测场景中具有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.23081v1": {
    "title": "A Survey on LLM Mid-training",
    "url": "https://www.alphaxiv.org/abs/2510.23081v1",
    "arxiv_id": "2510.23081v1",
    "authors": "Chengying Tu, Xuemiao Zhang, Rongxiang Weng, Rumei Li, Chen Zhang, Yang Bai, Hongfei Yan, Jingang Wang, Xunliang Cai",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 07:32:19",
    "ori_summary": "Recent advances in foundation models have highlighted the significant benefits of multi-stage training, with a particular emphasis on the emergence of mid-training as a vital stage that bridges pre-training and post-training. Mid-training is distinguished by its use of intermediate data and computational resources, systematically enhancing specified capabilities such as mathematics, coding, reasoning, and long-context extension, while maintaining foundational competencies. This survey provides a formal definition of mid-training for large language models (LLMs) and investigates optimization frameworks that encompass data curation, training strategies, and model architecture optimization. We analyze mainstream model implementations in the context of objective-driven interventions, illustrating how mid-training serves as a distinct and critical stage in the progressive development of LLM capabilities. By clarifying the unique contributions of mid-training, this survey offers a comprehensive taxonomy and actionable insights, supporting future research and innovation in the advancement of LLMs.",
    "summary": "论文研究LLM多阶段训练中的中期训练阶段，核心思想是通过中间数据和计算资源系统增强特定能力（如推理、长上下文），同时保持基础能力，为LLM能力渐进发展提供优化框架。",
    "translation": "大语言模型中期训练研究综述",
    "relevance_score": 9,
    "reasoning": "这篇关于LLM中期训练的综述直接属于'赋能LLM技术'范畴，探讨LLM训练过程中的核心进展。中期训练技术（如持续预训练、领域适应）对于在搜索、推荐和广告领域构建专业化的LLM模型具有重要应用价值，能够显著提升模型在特定业务场景下的性能表现。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文系统研究LLM中期训练阶段，直接关联LLM核心技术进步，为搜索推荐系统的模型能力定制化提供重要方法论支撑。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.23074v1": {
    "title": "Fast-MIA: Efficient and Scalable Membership Inference for LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.23074v1",
    "arxiv_id": "2510.23074v1",
    "authors": "Hiromu Takahashi, Shotaro Ishihara",
    "categories": "cs.CR, cs.CL",
    "pub_date": "2025-10-27 07:18:32",
    "ori_summary": "We propose Fast-MIA (https://github.com/Nikkei/fast-mia), a Python library for efficiently evaluating membership inference attacks (MIA) against Large Language Models (LLMs). MIA against LLMs has emerged as a crucial challenge due to growing concerns over copyright, security, and data privacy, and has attracted increasing research attention. However, the progress of this research is significantly hindered by two main obstacles: (1) the high computational cost of inference in LLMs, and (2) the lack of standardized and maintained implementations of MIA methods, which makes large-scale empirical comparison difficult. To address these challenges, our library provides fast batch inference and includes implementations of representative MIA methods under a unified evaluation framework. This library supports easy implementation of reproducible benchmarks with simple configuration and extensibility. We release Fast-MIA as an open-source (Apache License 2.0) tool to support scalable and transparent research on LLMs.",
    "summary": "",
    "translation": "Fast-MIA：面向大型语言模型的高效可扩展成员推理攻击",
    "relevance_score": 1,
    "reasoning": "该论文专注于成员推理攻击，这属于隐私和安全领域，被明确列为无关主题。虽然涉及LLMs，但核心关注的是隐私攻击方法而非LLM技术本身在推荐系统、搜索或广告中的应用。该工作没有展示在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23070v1": {
    "title": "Quality-Aware Translation Tagging in Multilingual RAG system",
    "url": "https://www.alphaxiv.org/abs/2510.23070v1",
    "arxiv_id": "2510.23070v1",
    "authors": "Hoyeon Moon, Byeolhee Kim, Nikhil Verma",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 07:11:01",
    "ori_summary": "Multilingual Retrieval-Augmented Generation (mRAG) often retrieves English documents and translates them into the query language for low-resource settings. However, poor translation quality degrades response generation performance. Existing approaches either assume sufficient translation quality or utilize the rewriting method, which introduces factual distortion and hallucinations. To mitigate these problems, we propose Quality-Aware Translation Tagging in mRAG (QTT-RAG), which explicitly evaluates translation quality along three dimensions-semantic equivalence, grammatical accuracy, and naturalness&fluency-and attach these scores as metadata without altering the original content. We evaluate QTT-RAG against CrossRAG and DKM-RAG as baselines in two open-domain QA benchmarks (XORQA, MKQA) using six instruction-tuned LLMs ranging from 2.4B to 14B parameters, covering two low-resource languages (Korean and Finnish) and one high-resource language (Chinese). QTT-RAG outperforms the baselines by preserving factual integrity while enabling generator models to make informed decisions based on translation reliability. This approach allows for effective usage of cross-lingual documents in low-resource settings with limited native language documents, offering a practical and robust solution across multilingual domains.",
    "summary": "",
    "translation": "多语言RAG系统中的质量感知翻译标记",
    "relevance_score": 3,
    "reasoning": "该论文主要关注多语言RAG系统中的翻译质量标记，这属于检索增强生成(RAG)的特定应用场景。虽然RAG技术在搜索系统中有所应用，但论文聚焦于翻译质量这一相对狭窄的NLP问题，与推荐系统、广告或核心Transformer架构进展的直接关联性较弱。其潜在应用仅限于多语言搜索的特定子领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23052v1": {
    "title": "Knocking-Heads Attention",
    "url": "https://www.alphaxiv.org/abs/2510.23052v1",
    "arxiv_id": "2510.23052v1",
    "authors": "Zhanchao Zhou, Xiaodong Chen, Haoxing Chen, Zhenzhong Lan, Jianguo Li",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 06:28:58",
    "ori_summary": "Multi-head attention (MHA) has become the cornerstone of modern large language models, enhancing representational capacity through parallel attention heads. However, increasing the number of heads inherently weakens individual head capacity, and existing attention mechanisms - whether standard MHA or its variants like grouped-query attention (GQA) and grouped-tied attention (GTA) - simply concatenate outputs from isolated heads without strong interaction. To address this limitation, we propose knocking-heads attention (KHA), which enables attention heads to \"knock\" on each other - facilitating cross-head feature-level interactions before the scaled dot-product attention. This is achieved by applying a shared, diagonally-initialized projection matrix across all heads. The diagonal initialization preserves head-specific specialization at the start of training while allowing the model to progressively learn integrated cross-head representations. KHA adds only minimal parameters and FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention variants. We validate KHA by training a 6.1B parameter MoE model (1.01B activated) on 1T high-quality tokens. Compared to baseline attention mechanisms, KHA brings superior and more stable training dynamics, achieving better performance across downstream tasks.",
    "summary": "该论文研究多头注意力机制中头间交互不足的问题，核心思想是通过共享对角初始化投影矩阵，在缩放点积注意力之前实现跨头特征级交互，从而增强表示能力。",
    "translation": "碰撞头注意力机制",
    "relevance_score": 8,
    "reasoning": "该论文标题表明涉及注意力机制改进，属于'使能Transformer技术'范畴。新的注意力机制可以直接应用于推荐系统和搜索中的序列建模与特征交互，提升模型效率与性能，对大规模推荐和搜索系统具有重要价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出了一种新的注意力机制改进方法，直接针对Transformer架构的核心组件进行优化，属于Enabling Transformer Tech的范畴，对LLM和推荐系统都有重要意义。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.23038v1": {
    "title": "Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.23038v1",
    "arxiv_id": "2510.23038v1",
    "authors": "Ran Xu, Jingjing Chen, Jiayu Ye, Yu Wu, Jun Yan, Carl Yang, Hongkun Yu",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-27 06:03:37",
    "ori_summary": "Large Language Models (LLMs) are widely used as judges to evaluate response quality, providing a scalable alternative to human evaluation. However, most LLM judges operate solely on intrinsic text-based reasoning, limiting their ability to verify complex constraints or perform accurate computation. Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks, we propose TIR-Judge, an end-to-end RL framework for training LLM judges that integrates a code executor for precise evaluation. TIR-Judge is built on three principles: (i) diverse training across verifiable and non-verifiable domains, (ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii) iterative RL that bootstraps directly from the initial model without distillation. On seven public benchmarks, TIR-Judge surpasses strong reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and achieves listwise performance comparable to Claude-Opus-4 despite having only 8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled judge trajectories, matches the performance of distilled variants, demonstrating that tool-augmented judges can self-evolve through iterative reinforcement learning.",
    "summary": "",
    "translation": "通过工具集成强化学习激励LLM评判者进行自主推理",
    "relevance_score": 3,
    "reasoning": "该论文主要关注LLM评判者的推理能力提升和强化学习应用，属于纯粹的LLM能力优化范畴。虽然强化学习技术理论上可用于推荐系统的策略优化，但论文标题明确聚焦于LLM评判者的推理激励，缺乏与推荐、搜索或广告系统的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23027v1": {
    "title": "Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts",
    "url": "https://www.alphaxiv.org/abs/2510.23027v1",
    "arxiv_id": "2510.23027v1",
    "authors": "Di Zhang, Xun Wu, Shaohan Huang, Yaru Hao, Li Dong, Zewen Chi, Zhifang Sui, Furu Wei",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-27 05:47:48",
    "ori_summary": "Recent advances in reinforcement learning (RL) have substantially improved the training of large-scale language models, leading to significant gains in generation quality and reasoning ability. However, most existing research focuses on dense models, while RL training for Mixture-of-Experts (MoE) architectures remains underexplored. To address the instability commonly observed in MoE training, we propose a novel router-aware approach to optimize importance sampling (IS) weights in off-policy RL. Specifically, we design a rescaling strategy guided by router logits, which effectively reduces gradient variance and mitigates training divergence. Experimental results demonstrate that our method significantly improves both the convergence stability and the final performance of MoE models, highlighting the potential of RL algorithmic innovations tailored to MoE architectures and providing a promising direction for efficient training of large-scale expert models.",
    "summary": "",
    "translation": "迈向稳定有效的混合专家强化学习",
    "relevance_score": 3,
    "reasoning": "该论文虽然涉及混合专家（MoE）这一Transformer架构的重要扩展技术，但其核心焦点是强化学习（RL）方法。根据用户明确的排除标准，'没有明确相关性的强化学习论文'应被视为不相关。尽管MoE技术本身具有潜在应用价值，但论文的RL导向使其与搜索、推荐和广告系统的直接关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23023v1": {
    "title": "UniAIDet: A Unified and Universal Benchmark for AI-Generated Image Content Detection and Localization",
    "url": "https://www.alphaxiv.org/abs/2510.23023v1",
    "arxiv_id": "2510.23023v1",
    "authors": "Huixuan Zhang, Xiaojun Wan",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-27 05:37:23",
    "ori_summary": "With the rapid proliferation of image generative models, the authenticity of digital images has become a significant concern. While existing studies have proposed various methods for detecting AI-generated content, current benchmarks are limited in their coverage of diverse generative models and image categories, often overlooking end-to-end image editing and artistic images. To address these limitations, we introduce UniAIDet, a unified and comprehensive benchmark that includes both photographic and artistic images. UniAIDet covers a wide range of generative models, including text-to-image, image-to-image, image inpainting, image editing, and deepfake models. Using UniAIDet, we conduct a comprehensive evaluation of various detection methods and answer three key research questions regarding generalization capability and the relation between detection and localization. Our benchmark and analysis provide a robust foundation for future research.",
    "summary": "",
    "translation": "UniAIDet：用于AI生成图像内容检测与定位的统一通用基准",
    "relevance_score": 2,
    "reasoning": "该论文主要关注AI生成图像内容的检测与定位基准，属于AIGC和内容生成领域。虽然检测技术可能间接应用于广告或推荐系统中的内容审核，但这并非核心的推荐系统、搜索或广告排名技术，也不涉及LLM在推荐/搜索中的直接应用或Transformer架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23020v1": {
    "title": "M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance, Multi-Relation Text-to-Image Benchmark",
    "url": "https://www.alphaxiv.org/abs/2510.23020v1",
    "arxiv_id": "2510.23020v1",
    "authors": "Huixuan Zhang, Xiaojun Wan",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-27 05:32:50",
    "ori_summary": "Text-to-image models are known to struggle with generating images that perfectly align with textual prompts. Several previous studies have focused on evaluating image-text alignment in text-to-image generation. However, these evaluations either address overly simple scenarios, especially overlooking the difficulty of prompts with multiple different instances belonging to the same category, or they introduce metrics that do not correlate well with human evaluation. In this study, we introduce M$^3$T2IBench, a large-scale, multi-category, multi-instance, multi-relation along with an object-detection-based evaluation metric, $AlignScore$, which aligns closely with human evaluation. Our findings reveal that current open-source text-to-image models perform poorly on this challenging benchmark. Additionally, we propose the Revise-Then-Enforce approach to enhance image-text alignment. This training-free post-editing method demonstrates improvements in image-text alignment across a broad range of diffusion models. \\footnote{Our code and data has been released in supplementary material and will be made publicly available after the paper is accepted.}",
    "summary": "",
    "translation": "M³T2IBench：一个大规模多类别、多实例、多关系文本到图像基准",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文本到图像生成基准的构建和评估，属于纯粹的视觉内容生成领域。虽然标题提到多类别和多关系，但这主要针对图像生成任务本身，没有明确涉及推荐系统、搜索或广告中的异构数据建模或排名应用。文本到图像基准与RecSys/Search/Ads的核心技术需求关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23011v1": {
    "title": "LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.23011v1",
    "arxiv_id": "2510.23011v1",
    "authors": "Sammriddh Gupta, Sonit Singh, Aditya Joshi, Mira Kim",
    "categories": "cs.CL, cs.CY",
    "pub_date": "2025-10-27 05:11:07",
    "ori_summary": "Language educators strive to create a rich experience for learners, while they may be restricted in the extend of feedback and practice they can provide. We present the design and development of LangLingual, a conversational agent built using the LangChain framework and powered by Large Language Models. The system is specifically designed to provide real-time, grammar-focused feedback, generate context-aware language exercises and track learner proficiency over time. The paper discusses the architecture, implementation and evaluation of LangLingual in detail. The results indicate strong usability, positive learning outcomes and encouraging learner engagement.",
    "summary": "",
    "translation": "LangLingual：一种基于大型语言模型的个性化、练习导向的英语语言学习工具",
    "relevance_score": 2,
    "reasoning": "该论文虽然涉及LLM技术，但专注于语言学习这一特定应用领域，属于教育技术范畴。论文内容与推荐系统、搜索或广告的核心技术进展没有直接关联，也不涉及Transformer架构改进或异构数据统一建模等关键技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23006v1": {
    "title": "Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures",
    "url": "https://www.alphaxiv.org/abs/2510.23006v1",
    "arxiv_id": "2510.23006v1",
    "authors": "Shenran Wang, Timothy Tin-Long Tse, Jian Zhu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 04:49:01",
    "ori_summary": "We perform in-depth evaluations of in-context learning (ICL) on state-of-the-art transformer, state-space, and hybrid large language models over two categories of knowledge-based ICL tasks. Using a combination of behavioral probing and intervention-based methods, we have discovered that, while LLMs of different architectures can behave similarly in task performance, their internals could remain different. We discover that function vectors (FVs) responsible for ICL are primarily located in the self-attention and Mamba layers, and speculate that Mamba2 uses a different mechanism from FVs to perform ICL. FVs are more important for ICL involving parametric knowledge retrieval, but not for contextual knowledge understanding. Our work contributes to a more nuanced understanding across architectures and task types. Methodologically, our approach also highlights the importance of combining both behavioural and mechanistic analyses to investigate LLM capabilities.",
    "summary": "论文研究不同架构大语言模型在上下文学习中的内部机制差异问题，核心发现是Transformer的自注意力层和Mamba的状态空间层使用不同机制实现上下文学习，其中函数向量主要处理参数化知识检索而非上下文知识理解。",
    "translation": "超越Transformer理解上下文学习：状态空间与混合架构研究",
    "relevance_score": 8,
    "reasoning": "该论文研究Transformer替代架构（状态空间模型和混合架构）中的上下文学习能力，属于'使能Transformer技术'范畴。这些高效架构可直接应用于推荐系统和搜索中的序列建模，通过降低计算复杂度实现更长的用户行为序列处理，同时保持强大的上下文理解能力。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接研究Transformer替代架构（状态空间模型）在上下文学习中的机制差异，属于Transformer架构效率与机制分析的核心前沿，对推荐系统中模型选择与优化具有重要参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.22993v1": {
    "title": "Can Language Models Compose Skills In-Context?",
    "url": "https://www.alphaxiv.org/abs/2510.22993v1",
    "arxiv_id": "2510.22993v1",
    "authors": "Zidong Liu, Zhuoyan Xu, Zhenmei Shi, Yingyu Liang",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-27 04:18:59",
    "ori_summary": "Composing basic skills from simple tasks to accomplish composite tasks is crucial for modern intelligent systems. We investigate the in-context composition ability of language models to perform composite tasks that combine basic skills demonstrated in in-context examples. This is more challenging than the standard setting, where skills and their composition can be learned in training. We conduct systematic experiments on various representative open-source language models, utilizing linguistic and logical tasks designed to probe composition abilities. The results reveal that simple task examples can have a surprising negative impact on the performance, because the models generally struggle to recognize and assemble the skills correctly, even with Chain-of-Thought examples. Theoretical analysis further shows that it is crucial to align examples with the corresponding steps in the composition. This inspires a method for the probing tasks, whose improved performance provides positive support for our insights.",
    "summary": "论文研究语言模型在上下文中组合基本技能以完成复合任务的能力，核心发现是模型难以正确识别和组装技能，需要将示例与组合步骤对齐。",
    "translation": "语言模型能否在上下文中组合技能？",
    "relevance_score": 8,
    "reasoning": "该论文探讨语言模型的上下文学习能力，特别是技能组合机制，这属于'核心LLM技术进展'范畴。在推荐系统和搜索中，这种能力可用于动态组合用户偏好、上下文特征和领域知识，实现更精准的个性化推荐和复杂查询理解。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文研究LLM的上下文技能组合能力，直接关联LLM核心技术进步，对推荐和搜索系统中的复杂任务分解具有应用潜力。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.22968v1": {
    "title": "Measuring Teaching with LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.22968v1",
    "arxiv_id": "2510.22968v1",
    "authors": "Michael Hardy",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 03:42:04",
    "ori_summary": "Objective and scalable measurement of teaching quality is a persistent challenge in education. While Large Language Models (LLMs) offer potential, general-purpose models have struggled to reliably apply complex, authentic classroom observation instruments. This paper uses custom LLMs built on sentence-level embeddings, an architecture better suited for the long-form, interpretive nature of classroom transcripts than conventional subword tokenization. We systematically evaluate five different sentence embeddings under a data-efficient training regime designed to prevent overfitting. Our results demonstrate that these specialized models can achieve human-level and even super-human performance with expert human ratings above 0.65 and surpassing the average human-human rater correlation. Further, through analysis of annotation context windows, we find that more advanced models-those better aligned with human judgments-attribute a larger share of score variation to lesson-level features rather than isolated utterances, challenging the sufficiency of single-turn annotation paradigms. Finally, to assess external validity, we find that aggregate model scores align with teacher value-added measures, indicating they are capturing features relevant to student learning. However, this trend does not hold at the individual item level, suggesting that while the models learn useful signals, they have not yet achieved full generalization. This work establishes a viable and powerful new methodology for AI-driven instructional measurement, offering a path toward providing scalable, reliable, and valid feedback for educator development.",
    "summary": "",
    "translation": "使用大语言模型衡量教学",
    "relevance_score": 2,
    "reasoning": "该论文标题明确聚焦于教育领域的教学评估应用，属于特定领域应用而非核心推荐系统、搜索或广告技术。虽然涉及LLMs，但缺乏与推荐系统、搜索或广告的潜在应用联系，且教育评估属于明确的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22967v1": {
    "title": "MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.22967v1",
    "arxiv_id": "2510.22967v1",
    "authors": "Yucheng Ning, Xixun Lin, Fang Fang, Yanan Cao",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 03:41:32",
    "ori_summary": "The widespread adoption of Large Language Models (LLMs) raises critical concerns about the factual accuracy of their outputs, especially in high-risk domains such as biomedicine, law, and education. Existing evaluation methods for short texts often fail on long-form content due to complex reasoning chains, intertwined perspectives, and cumulative information. To address this, we propose a systematic approach integrating large-scale long-form datasets, multi-agent verification mechanisms, and weighted evaluation metrics. We construct LongHalluQA, a Chinese long-form factuality dataset; and develop MAD-Fact, a debate-based multi-agent verification system. We introduce a fact importance hierarchy to capture the varying significance of claims in long-form texts. Experiments on two benchmarks show that larger LLMs generally maintain higher factual consistency, while domestic models excel on Chinese content. Our work provides a structured framework for evaluating and enhancing factual reliability in long-form LLM outputs, guiding their safe deployment in sensitive domains.",
    "summary": "",
    "translation": "MAD-Fact：一种用于大语言模型长文本事实性评估的多智能体辩论框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM的事实性评估方法，属于纯粹的评估基准和NLP中心话题，与推荐系统、搜索或广告的核心技术无关。论文提出的多智能体辩论框架没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22956v1": {
    "title": "Tagging-Augmented Generation: Assisting Language Models in Finding Intricate Knowledge In Long Contexts",
    "url": "https://www.alphaxiv.org/abs/2510.22956v1",
    "arxiv_id": "2510.22956v1",
    "authors": "Anwesan Pal, Karen Hovsepian, Tinghao Guo, Mengnan Zhao, Somendra Tripathi, Nikos Kanakaris, George Mihaila, Sumit Nigam",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2025-10-27 03:23:25",
    "ori_summary": "Recent investigations into effective context lengths of modern flagship large language models (LLMs) have revealed major limitations in effective question answering (QA) and reasoning over long and complex contexts for even the largest and most impressive cadre of models. While approaches like retrieval-augmented generation (RAG) and chunk-based re-ranking attempt to mitigate this issue, they are sensitive to chunking, embedding and retrieval strategies and models, and furthermore, rely on extensive pre-processing, knowledge acquisition and indexing steps. In this paper, we propose Tagging-Augmented Generation (TAG), a lightweight data augmentation strategy that boosts LLM performance in long-context scenarios, without degrading and altering the integrity and composition of retrieved documents. We validate our hypothesis by augmenting two challenging and directly relevant question-answering benchmarks -- NoLima and NovelQA -- and show that tagging the context or even just adding tag definitions into QA prompts leads to consistent performance gains over the baseline -- up to 17% for 32K token contexts, and 2.9% in complex reasoning question-answering for multi-hop queries requiring knowledge across a wide span of text. Additional details are available at https://sites.google.com/view/tag-emnlp.",
    "summary": "论文研究LLM在长复杂上下文中查找精细知识的性能限制问题，核心方法是提出标记增强生成技术，通过轻量级数据标注策略增强上下文表示而不改变文档完整性。",
    "translation": "标签增强生成：辅助语言模型在长上下文中查找复杂知识",
    "relevance_score": 7,
    "reasoning": "该论文属于'赋能LLM技术'类别，专注于提升LLM在长上下文中的知识检索能力。这对于搜索和推荐系统至关重要，能够帮助模型更有效地从长用户历史、产品描述或内容文档中提取相关信息，从而提升检索质量和推荐准确性。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出标记增强生成方法解决LLM长上下文知识查找问题，直接属于LLM在搜索问答领域的应用创新，与核心关注点高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.22954v1": {
    "title": "Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)",
    "url": "https://www.alphaxiv.org/abs/2510.22954v1",
    "arxiv_id": "2510.22954v1",
    "authors": "Liwei Jiang, Yuanjun Chai, Margaret Li, Mickel Liu, Raymond Fok, Nouha Dziri, Yulia Tsvetkov, Maarten Sap, Alon Albalak, Yejin Choi",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 03:16:21",
    "ori_summary": "Language models (LMs) often struggle to generate diverse, human-like creative content, raising concerns about the long-term homogenization of human thought through repeated exposure to similar outputs. Yet scalable methods for evaluating LM output diversity remain limited, especially beyond narrow tasks such as random number or name generation, or beyond repeated sampling from a single model. We introduce Infinity-Chat, a large-scale dataset of 26K diverse, real-world, open-ended user queries that admit a wide range of plausible answers with no single ground truth. We introduce the first comprehensive taxonomy for characterizing the full spectrum of open-ended prompts posed to LMs, comprising 6 top-level categories (e.g., brainstorm & ideation) that further breaks down to 17 subcategories. Using Infinity-Chat, we present a large-scale study of mode collapse in LMs, revealing a pronounced Artificial Hivemind effect in open-ended generation of LMs, characterized by (1) intra-model repetition, where a single model consistently generates similar responses, and more so (2) inter-model homogeneity, where different models produce strikingly similar outputs. Infinity-Chat also includes 31,250 human annotations, across absolute ratings and pairwise preferences, with 25 independent human annotations per example. This enables studying collective and individual-specific human preferences in response to open-ended queries. Our findings show that LMs, reward models, and LM judges are less well calibrated to human ratings on model generations that elicit differing idiosyncratic annotator preferences, despite maintaining comparable overall quality. Overall, INFINITY-CHAT presents the first large-scale resource for systematically studying real-world open-ended queries to LMs, revealing critical insights to guide future research for mitigating long-term AI safety risks posed by the Artificial Hivemind.",
    "summary": "",
    "translation": "人工蜂群思维：语言模型（及更广范围）的开放式同质性",
    "relevance_score": 2,
    "reasoning": "该论文标题聚焦于语言模型的同质化现象及其更广泛影响，这属于LLM基础特性分析而非技术进展。虽然涉及LLM，但未提出新的架构、效率改进或具体应用方法，对推荐系统、搜索或广告领域的潜在应用价值有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22907v1": {
    "title": "Language Server CLI Empowers Language Agents with Process Rewards",
    "url": "https://www.alphaxiv.org/abs/2510.22907v1",
    "arxiv_id": "2510.22907v1",
    "authors": "Yifan Zhang, Lanser Contributors",
    "categories": "cs.CL, cs.AI, cs.PL, cs.SE",
    "pub_date": "2025-10-27 01:25:20",
    "ori_summary": "Large language models routinely hallucinate APIs and mislocalize edits, while language servers compute verified, IDE-grade facts about real code. We present Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language Server Protocol (LSP) server for coding agents and CI, exposing deterministic, replayable workflows. Our position is that language servers provide not only structural information (definitions, references, types, diagnostics) but also an actionable process reward: machine-checked, step-wise signals that align an agent's planning loop with program reality. In this work, Lanser-CLI contributes: (i) a robust addressing scheme beyond brittle \"file:line:col\" via a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a principled relocation algorithm; (ii) deterministic Analysis Bundles that normalize Language Server responses and capture environment/capability metadata with stable content hashes; (iii) a safety envelope for mutating operations (rename, code actions) with preview, workspace jails, and Git-aware, transactional apply; and (iv) a process-reward functional derived from Language Server facts (diagnostic deltas, disambiguation confidence, and safe-apply checks) that is computable online and replayable offline. We formalize determinism under frozen snapshots and establish a monotonicity property for the process reward, making it suitable for process supervision and counterfactual analysis. Project Page: https://github.com/yifanzhang-pro/lanser-cli",
    "summary": "",
    "translation": "语言服务器命令行界面通过过程奖励赋能语言智能体",
    "relevance_score": 2,
    "reasoning": "该论文主要关注语言智能体的开发工具和过程奖励机制，属于语言模型工具链优化范畴。虽然涉及语言智能体技术，但其核心焦点是开发工具和过程奖励，与推荐系统、搜索或广告的核心技术进展关联度较低，缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22904v1": {
    "title": "Modeling Political Discourse with Sentence-BERT and BERTopic",
    "url": "https://www.alphaxiv.org/abs/2510.22904v1",
    "arxiv_id": "2510.22904v1",
    "authors": "Margarida Mendonca, Alvaro Figueira",
    "categories": "cs.SI, cs.CL, cs.CY, 68T50, 91D30, I.2.7; H.3.1; J.4",
    "pub_date": "2025-10-27 01:19:42",
    "ori_summary": "Social media has reshaped political discourse, offering politicians a platform for direct engagement while reinforcing polarization and ideological divides. This study introduces a novel topic evolution framework that integrates BERTopic-based topic modeling with Moral Foundations Theory (MFT) to analyze the longevity and moral dimensions of political topics in Twitter activity during the 117th U.S. Congress. We propose a methodology for tracking dynamic topic shifts over time and measuring their association with moral values and quantifying topic persistence. Our findings reveal that while overarching themes remain stable, granular topics tend to dissolve rapidly, limiting their long-term influence. Moreover, moral foundations play a critical role in topic longevity, with Care and Loyalty dominating durable topics, while partisan differences manifest in distinct moral framing strategies. This work contributes to the field of social network analysis and computational political discourse by offering a scalable, interpretable approach to understanding moral-driven topic evolution on social media.",
    "summary": "",
    "translation": "使用Sentence-BERT和BERTopic建模政治话语",
    "relevance_score": 1,
    "reasoning": "该论文专注于政治话语分析这一特定领域应用，与推荐系统、搜索或广告的核心技术进展无关。虽然使用了BERT相关技术，但属于纯粹的NLP应用领域，没有展示在RecSys/Search/Ads中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22881v1": {
    "title": "Offline Preference Optimization via Maximum Marginal Likelihood Estimation",
    "url": "https://www.alphaxiv.org/abs/2510.22881v1",
    "arxiv_id": "2510.22881v1",
    "authors": "Saeed Najafi, Alona Fyshe",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-27 00:15:57",
    "ori_summary": "Aligning Large Language Models (LLMs) with human preferences is crucial, but standard methods like Reinforcement Learning from Human Feedback (RLHF) are often complex and unstable. In this work, we propose a new, simpler approach that recasts alignment through the lens of Maximum Marginal Likelihood (MML) estimation. Our new MML based Preference Optimization (MMPO) maximizes the marginal log-likelihood of a preferred text output, using the preference pair as samples for approximation, and forgoes the need for both an explicit reward model and entropy maximization. We theoretically demonstrate that MMPO implicitly performs preference optimization, producing a weighted gradient that naturally up-weights chosen responses over rejected ones. Across models ranging from 135M to 8B parameters, we empirically show that MMPO: 1) is more stable with respect to the hyperparameter $\\beta$ compared to alternative baselines, and 2) achieves competitive or superior preference alignment while better preserving the base model's general language capabilities. Through a series of ablation experiments, we show that this improved performance is indeed attributable to MMPO's implicit preference optimization within the gradient updates.",
    "summary": "论文研究LLM与人类偏好的对齐问题，核心思想是通过最大边际似然估计重新构建对齐框架，无需显式奖励模型即可隐式执行偏好优化。",
    "translation": "基于最大边际似然估计的离线偏好优化",
    "relevance_score": 8,
    "reasoning": "该论文提出的离线偏好优化方法直接适用于推荐系统和搜索中的用户偏好学习问题。最大边际似然估计技术可以作为核心推荐算法的基础，用于从用户行为数据中学习更准确的偏好模型，提升个性化推荐和搜索结果的质量。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出了一种新的LLM对齐方法MMPO，直接替代复杂的RLHF流程，对推荐系统和搜索中的偏好建模具有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.23607v1": {
    "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations",
    "url": "https://www.alphaxiv.org/abs/2510.23607v1",
    "arxiv_id": "2510.23607v1",
    "authors": "Yujia Zhang, Xiaoyang Wu, Yixing Lao, Chengyao Wang, Zhuotao Tian, Naiyan Wang, Hengshuang Zhao",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 17:59:59",
    "ori_summary": "Humans learn abstract concepts through multisensory synergy, and once formed, such representations can often be recalled from a single modality. Inspired by this principle, we introduce Concerto, a minimalist simulation of human concept learning for spatial cognition, combining 3D intra-modal self-distillation with 2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more coherent and informative spatial features, as demonstrated by zero-shot visualizations. It outperforms both standalone SOTA 2D and 3D self-supervised models by 14.2% and 4.8%, respectively, as well as their feature concatenation, in linear probing for 3D scene perception. With full fine-tuning, Concerto sets new SOTA results across multiple scene understanding benchmarks (e.g., 80.7% mIoU on ScanNet). We further present a variant of Concerto tailored for video-lifted point cloud spatial understanding, and a translator that linearly projects Concerto representations into CLIP's language space, enabling open-world perception. These results highlight that Concerto emerges spatial representations with superior fine-grained geometric and semantic consistency.",
    "summary": "",
    "translation": "协奏曲：联合2D-3D自监督学习涌现空间表征",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉领域的2D-3D空间表征学习，虽然涉及自监督学习技术，但其核心应用场景明显偏向视觉理解和3D场景理解。在推荐系统、搜索或广告领域，这种空间表征的直接应用潜力非常有限，除非专门针对空间感知的推荐场景（如AR/VR购物），否则相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23605v1": {
    "title": "Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling",
    "url": "https://www.alphaxiv.org/abs/2510.23605v1",
    "arxiv_id": "2510.23605v1",
    "authors": "Shuhong Zheng, Ashkan Mirzaei, Igor Gilitschenski",
    "categories": "cs.CV, cs.AI, cs.GR, cs.LG, cs.RO",
    "pub_date": "2025-10-27 17:59:51",
    "ori_summary": "Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at https://zsh2000.github.io/track-inpaint-resplat.github.io/.",
    "summary": "",
    "translation": "追踪、修复、重映射：基于渐进式纹理填充的主体驱动3D与4D生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D/4D内容生成和纹理填充技术，属于纯粹的计算机视觉和图形学领域。虽然标题提到'主体驱动'，但这指的是视觉对象跟踪和生成，与推荐系统、搜索或广告中的用户行为建模和内容排序没有直接关联。该技术缺乏在异构数据处理、序列建模或推荐排序方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23603v1": {
    "title": "PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity",
    "url": "https://www.alphaxiv.org/abs/2510.23603v1",
    "arxiv_id": "2510.23603v1",
    "authors": "Yuqian Yuan, Wenqiao Zhang, Xin Li, Shihao Wang, Kehan Li, Wentong Li, Jun Xiao, Lei Zhang, Beng Chin Ooi",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 17:59:32",
    "ori_summary": "Multimodal large language models (MLLMs) have demonstrated strong general-purpose capabilities in open-world visual comprehension. However, most existing MLLMs primarily focus on holistic, scene-level understanding, often overlooking the need for fine-grained, object-centric reasoning. In this paper, we present PixelRefer, a unified region-level MLLM framework that enables advanced fine-grained understanding over user-specified regions across both images and videos. Motivated by the observation that LLM attention predominantly focuses on object-level tokens, we propose a Scale-Adaptive Object Tokenizer (SAOT) to generate compact and semantically rich object representations from free-form regions. Our analysis reveals that global visual tokens contribute mainly in early LLM layers, inspiring the design of PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion module to pre-fuse global context into object tokens. This yields a lightweight Object-Only Framework that substantially reduces computational cost while maintaining high semantic fidelity. To facilitate fine-grained instruction tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction dataset. Extensive experiments across a range of benchmarks validate that PixelRefer achieves leading performance with fewer training samples, while PixelRefer-Lite offers competitive accuracy with notable gains in efficiency.",
    "summary": "",
    "translation": "PixelRefer：一种具有任意粒度的时空对象指代统一框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的时空对象指代任务，属于纯粹的视觉技术范畴。虽然标题提到'统一框架'和'任意粒度'，但这些概念在推荐系统、搜索或广告领域缺乏明确的直接应用场景，且论文内容明显偏向视觉理解而非文本或序列建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23594v1": {
    "title": "PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection",
    "url": "https://www.alphaxiv.org/abs/2510.23594v1",
    "arxiv_id": "2510.23594v1",
    "authors": "Yusu Qian, Cheng Wan, Chao Jia, Yinfei Yang, Qingyu Zhao, Zhe Gan",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 17:57:52",
    "ori_summary": "We introduce \\textbf{PRISM-Bench}, a benchmark of puzzle-based visual challenges designed to evaluate not only whether models can solve problems, but how their reasoning unfolds. Unlike prior evaluations that measure only final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error, models must identify the first incorrect step. This setting enables fine-grained assessment of logical consistency, error detection, and visual reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric, and analogical reasoning, resisting shortcuts based on superficial pattern matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap between fluent generation and faithful reasoning: models that produce plausible CoTs often fail to locate simple logical faults. By disentangling answer generation from reasoning verification, PRISM-Bench offers a sharper lens on multimodal reasoning competence and underscores the need for diagnostic evaluation protocols in the development of trustworthy MLLMs.",
    "summary": "",
    "translation": "PRISM-Bench：基于谜题的视觉任务基准，附带思维链错误检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉谜题基准和思维链错误检测，属于纯粹的视觉和NLP评估基准范畴。虽然思维链技术可能间接影响推荐系统的推理能力，但论文的视觉焦点和基准性质使其与搜索、推荐、广告的核心技术进展关联度极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23589v1": {
    "title": "InFlux: A Benchmark for Self-Calibration of Dynamic Intrinsics of Video Cameras",
    "url": "https://www.alphaxiv.org/abs/2510.23589v1",
    "arxiv_id": "2510.23589v1",
    "authors": "Erich Liang, Roma Bhattacharjee, Sreemanti Dey, Rafael Moschopoulos, Caitlin Wang, Michel Liao, Grace Tan, Andrew Wang, Karhan Kayan, Stamatis Alexandropoulos, Jia Deng",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 17:54:57",
    "ori_summary": "Accurately tracking camera intrinsics is crucial for achieving 3D understanding from 2D video. However, most 3D algorithms assume that camera intrinsics stay constant throughout a video, which is often not true for many real-world in-the-wild videos. A major obstacle in this field is a lack of dynamic camera intrinsics benchmarks--existing benchmarks typically offer limited diversity in scene content and intrinsics variation, and none provide per-frame intrinsic changes for consecutive video frames. In this paper, we present Intrinsics in Flux (InFlux), a real-world benchmark that provides per-frame ground truth intrinsics annotations for videos with dynamic intrinsics. Compared to prior benchmarks, InFlux captures a wider range of intrinsic variations and scene diversity, featuring 143K+ annotated frames from 386 high-resolution indoor and outdoor videos with dynamic camera intrinsics. To ensure accurate per-frame intrinsics, we build a comprehensive lookup table of calibration experiments and extend the Kalibr toolbox to improve its accuracy and robustness. Using our benchmark, we evaluate existing baseline methods for predicting camera intrinsics and find that most struggle to achieve accurate predictions on videos with dynamic intrinsics. For the dataset, code, videos, and submission, please visit https://influx.cs.princeton.edu/.",
    "summary": "",
    "translation": "InFlux：视频相机动态内参自校准基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的相机内参校准基准测试，属于纯粹的视觉技术研究。虽然标题提到\"动态内参\"，但这主要涉及相机硬件参数的自适应调整，与推荐系统、搜索或广告的核心技术栈没有直接关联。该研究属于视觉传感器标定范畴，不符合当前关注的LLM技术、推荐算法或Transformer架构等核心方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23588v1": {
    "title": "FARMER: Flow AutoRegressive Transformer over Pixels",
    "url": "https://www.alphaxiv.org/abs/2510.23588v1",
    "arxiv_id": "2510.23588v1",
    "authors": "Guangting Zheng, Qinyu Zhao, Tao Yang, Fei Xiao, Zhijie Lin, Jie Wu, Jiajun Deng, Yanyong Zhang, Rui Zhu",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 17:54:08",
    "ori_summary": "Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training.",
    "summary": "",
    "translation": "FARMER：基于像素的流自回归Transformer",
    "relevance_score": 3,
    "reasoning": "该论文提出了基于像素的流自回归Transformer，属于Transformer架构的效率优化技术，与'使能Transformer技术'相关。然而，该方法专注于像素级生成任务，在推荐系统、搜索或广告中的潜在应用有限，可能适用于图像内容理解或生成式推荐场景，但直接相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23581v1": {
    "title": "Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation",
    "url": "https://www.alphaxiv.org/abs/2510.23581v1",
    "arxiv_id": "2510.23581v1",
    "authors": "Junyoung Seo, Rodrigo Mira, Alexandros Haliassos, Stella Bounareli, Honglie Chen, Linh Tran, Seungryong Kim, Zoe Landgraf, Jie Shen",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-27 17:50:19",
    "ori_summary": "Audio-driven human animation models often suffer from identity drift during temporal autoregressive generation, where characters gradually lose their identity over time. One solution is to generate keyframes as intermediate temporal anchors that prevent degradation, but this requires an additional keyframe generation stage and can restrict natural motion dynamics. To address this, we propose Lookahead Anchoring, which leverages keyframes from future timesteps ahead of the current generation window, rather than within it. This transforms keyframes from fixed boundaries into directional beacons: the model continuously pursues these future anchors while responding to immediate audio cues, maintaining consistent identity through persistent guidance. This also enables self-keyframing, where the reference image serves as the lookahead target, eliminating the need for keyframe generation entirely. We find that the temporal lookahead distance naturally controls the balance between expressivity and consistency: larger distances allow for greater motion freedom, while smaller ones strengthen identity adherence. When applied to three recent human animation models, Lookahead Anchoring achieves superior lip synchronization, identity preservation, and visual quality, demonstrating improved temporal conditioning across several different architectures. Video results are available at the following link: https://lookahead-anchoring.github.io.",
    "summary": "",
    "translation": "前瞻锚定：在音频驱动人体动画中保持角色身份特征",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机图形学中的音频驱动人体动画技术，属于纯粹的视觉/图形领域应用。论文内容涉及角色动画生成和身份保持，与推荐系统、搜索、广告或相关使能技术没有任何直接或间接的关联，完全超出了关注范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23576v1": {
    "title": "UrbanVLA: A Vision-Language-Action Model for Urban Micromobility",
    "url": "https://www.alphaxiv.org/abs/2510.23576v1",
    "arxiv_id": "2510.23576v1",
    "authors": "Anqi Li, Zhiyong Wang, Jiazhao Zhang, Minghan Li, Yunpeng Qi, Zhibo Chen, Zhizheng Zhang, He Wang",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2025-10-27 17:46:43",
    "ori_summary": "Urban micromobility applications, such as delivery robots, demand reliable navigation across large-scale urban environments while following long-horizon route instructions. This task is particularly challenging due to the dynamic and unstructured nature of real-world city areas, yet most existing navigation methods remain tailored to short-scale and controllable scenarios. Effective urban micromobility requires two complementary levels of navigation skills: low-level capabilities such as point-goal reaching and obstacle avoidance, and high-level capabilities, such as route-visual alignment. To this end, we propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework designed for scalable urban navigation. Our method explicitly aligns noisy route waypoints with visual observations during execution, and subsequently plans trajectories to drive the robot. To enable UrbanVLA to master both levels of navigation, we employ a two-stage training pipeline. The process begins with Supervised Fine-Tuning (SFT) using simulated environments and trajectories parsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on a mixture of simulation and real-world data, which enhances the model's safety and adaptability in real-world settings. Experiments demonstrate that UrbanVLA surpasses strong baselines by more than 55% in the SocialNav task on MetaUrban. Furthermore, UrbanVLA achieves reliable real-world navigation, showcasing both scalability to large-scale urban environments and robustness against real-world uncertainties.",
    "summary": "",
    "translation": "UrbanVLA：面向城市微交通的视觉-语言-动作模型",
    "relevance_score": 3,
    "reasoning": "该论文虽然涉及视觉-语言模型技术，但其应用领域是城市微交通（如共享单车、电动滑板车等）的动作控制，与推荐系统、搜索或广告的核心关注点相距甚远。视觉-语言模型技术本身具有潜在应用价值，但该论文的具体应用场景缺乏与RecSys/Search/Ads的明确关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23574v1": {
    "title": "More Than Generation: Unifying Generation and Depth Estimation via Text-to-Image Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2510.23574v1",
    "arxiv_id": "2510.23574v1",
    "authors": "Hongkai Lin, Dingkang Liang, Mingyang Du, Xin Zhou, Xiang Bai",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 17:44:56",
    "ori_summary": "Generative depth estimation methods leverage the rich visual priors stored in pre-trained text-to-image diffusion models, demonstrating astonishing zero-shot capability. However, parameter updates during training lead to catastrophic degra- dation in the image generation capability of the pre-trained model. We introduce MERGE, a unified model for image generation and depth estimation, starting from a fixed pre-trained text-to-image model. MERGE demonstrates that the pre-trained text-to-image model can do more than image generation, but also expand to depth estimation effortlessly. Specifically, MERGE introduces a play- and-plug framework that enables seamless switching between image generation and depth estimation modes through simple and pluggable converters. Meanwhile, we propose a Group Reuse Mechanism to encourage parameter reuse and im- prove the utilization of the additional learnable parameters. MERGE unleashes the powerful depth estimation capability of the pre-trained text-to-image model while preserving its original image generation ability. Compared to other unified models for image generation and depth estimation, MERGE achieves state-of- the-art performance across multiple depth estimation benchmarks. The code will be made available at https://github.com/H-EmbodVis/MERGE",
    "summary": "",
    "translation": "超越生成：通过文本到图像扩散模型统一生成与深度估计",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的扩散模型应用，特别是图像生成与深度估计的统一。虽然扩散模型本身是重要的生成技术，但该工作聚焦于纯粹的视觉任务，没有明确展示在推荐系统、搜索或广告中的潜在应用，与当前关注的核心领域相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23571v1": {
    "title": "RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation",
    "url": "https://www.alphaxiv.org/abs/2510.23571v1",
    "arxiv_id": "2510.23571v1",
    "authors": "Yash Jangir, Yidi Zhang, Kashu Yamazaki, Chenyu Zhang, Kuan-Hsun Tu, Tsung-Wei Ke, Lei Ke, Yonatan Bisk, Katerina Fragkiadaki",
    "categories": "cs.RO, cs.AI, cs.CV, cs.LG",
    "pub_date": "2025-10-27 17:41:38",
    "ori_summary": "The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies within the same synthetic domains and cannot assess models trained from real-world demonstrations or alternative simulation environments. As policies expand in scope and complexity, these barriers only intensify, since defining \"success\" in robotics often hinges on nuanced human judgments of execution quality. In this paper, we introduce a new benchmarking framework that overcomes these challenges by shifting VLA evaluation into large-scale simulated environments augmented with online human feedback. Leveraging advances in vision-language models, 2D-to-3D generative modeling, and differentiable rendering, our approach automatically converts video demonstrations from widely used robot datasets into simulated counterparts. Within these digital twins, we assess VLA policies using both automated VLM-guided scoring and scalable human preference judgments collected from crowdworkers, transforming human involvement from tedious scene setup, resetting, and safety supervision into lightweight preference comparisons. To measure robustness, we systematically perturb simulated environments along multiple axes, such as textures and object placements, stress-testing policy generalization under controlled variation. The result is a continuously evolving, reproducible, and scalable benchmark for real-world trained robot manipulation policies, addressing a critical missing capability in today's robotics landscape.",
    "summary": "",
    "translation": "RobotArena ∞：通过实景到仿真转换实现可扩展的机器人基准测试",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人领域的基准测试和实景到仿真转换技术，与推荐系统、搜索或广告的核心领域进展、LLM技术、Transformer架构或异构数据建模完全无关。机器人基准测试和仿真转换技术没有明显的潜在应用可以迁移到推荐系统、搜索或广告领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23569v1": {
    "title": "EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT",
    "url": "https://www.alphaxiv.org/abs/2510.23569v1",
    "arxiv_id": "2510.23569v1",
    "authors": "Baoqi Pei, Yifei Huang, Jilan Xu, Yuping He, Guo Chen, Fei Wu, Yu Qiao, Jiangmiao Pang",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 17:38:17",
    "ori_summary": "Egocentric video reasoning centers on an unobservable agent behind the camera who dynamically shapes the environment, requiring inference of hidden intentions and recognition of fine-grained interactions. This core challenge limits current multimodal large language models MLLMs, which excel at visible event reasoning but lack embodied, first-person understanding. To bridge this gap, we introduce EgoThinker, a novel framework that endows MLLMs with robust egocentric reasoning capabilities through spatio-temporal chain-of-thought supervision and a two-stage learning curriculum. First, we introduce EgoRe-5M, a large-scale egocentric QA dataset constructed from 13M diverse egocentric video clips. This dataset features multi-minute segments annotated with detailed CoT rationales and dense hand-object grounding. Second, we employ SFT on EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning RFT to further enhance spatio-temporal localization. Experimental results show that EgoThinker outperforms existing methods across multiple egocentric benchmarks, while achieving substantial improvements in fine-grained spatio-temporal localization tasks. Full code and data are released at https://github.com/InternRobotics/EgoThinker.",
    "summary": "",
    "translation": "EgoThinker：通过时空思维链揭示自我中心推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注自我中心视角下的时空推理和思维链技术，这属于计算机视觉和推理领域的特定应用。虽然思维链技术本身是LLM的重要能力，但该论文的自我中心视角和时空推理焦点与推荐系统、搜索或广告的核心技术需求关联度较低，缺乏明确的跨模态建模或序列处理的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23561v1": {
    "title": "Revising Second Order Terms in Deep Animation Video Coding",
    "url": "https://www.alphaxiv.org/abs/2510.23561v1",
    "arxiv_id": "2510.23561v1",
    "authors": "Konstantin Schmidt, Thomas Richter",
    "categories": "eess.IV, cs.CV",
    "pub_date": "2025-10-27 17:32:08",
    "ori_summary": "First Order Motion Model is a generative model that animates human heads based on very little motion information derived from keypoints. It is a promising solution for video communication because first it operates at very low bitrate and second its computational complexity is moderate compared to other learning based video codecs. However, it has strong limitations by design. Since it generates facial animations by warping source-images, it fails to recreate videos with strong head movements. This works concentrates on one specific kind of head movements, namely head rotations. We show that replacing the Jacobian transformations in FOMM by a global rotation helps the system to perform better on items with head-rotations while saving 40% to 80% of bitrate on P-frames. Moreover, we apply state-of-the-art normalization techniques to the discriminator to stabilize the adversarial training which is essential for generating visually appealing videos. We evaluate the performance by the learned metics LPIPS and DISTS to show the success our optimizations.",
    "summary": "",
    "translation": "深度动画视频编码中二阶项的修正",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频编码技术中的二阶项修正，属于纯视频压缩领域，与推荐系统、搜索或广告的核心技术无直接关联。论文内容涉及动画视频编码优化，属于计算机视觉中的视频处理范畴，不符合当前关注的LLM技术、Transformer架构或推荐系统相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23525v1": {
    "title": "DPGLA: Bridging the Gap between Synthetic and Real Data for Unsupervised Domain Adaptation in 3D LiDAR Semantic Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.23525v1",
    "arxiv_id": "2510.23525v1",
    "authors": "Wanmeng Li, Simone Mosco, Daniel Fusaro, Alberto Pretto",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-27 17:05:59",
    "ori_summary": "Annotating real-world LiDAR point clouds for use in intelligent autonomous systems is costly. To overcome this limitation, self-training-based Unsupervised Domain Adaptation (UDA) has been widely used to improve point cloud semantic segmentation by leveraging synthetic point cloud data. However, we argue that existing methods do not effectively utilize unlabeled data, as they either rely on predefined or fixed confidence thresholds, resulting in suboptimal performance. In this paper, we propose a Dynamic Pseudo-Label Filtering (DPLF) scheme to enhance real data utilization in point cloud UDA semantic segmentation. Additionally, we design a simple and efficient Prior-Guided Data Augmentation Pipeline (PG-DAP) to mitigate domain shift between synthetic and real-world point clouds. Finally, we utilize data mixing consistency loss to push the model to learn context-free representations. We implement and thoroughly evaluate our approach through extensive comparisons with state-of-the-art methods. Experiments on two challenging synthetic-to-real point cloud semantic segmentation tasks demonstrate that our approach achieves superior performance. Ablation studies confirm the effectiveness of the DPLF and PG-DAP modules. We release the code of our method in this paper.",
    "summary": "",
    "translation": "DPGLA：弥合合成与真实数据之间的差距，用于3D LiDAR语义分割的无监督领域自适应",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D LiDAR语义分割和领域自适应技术，这属于纯粹的3D视觉领域，与推荐系统、搜索或广告没有直接关联。论文内容涉及计算机视觉中的特定传感器数据处理，无法找到在RecSys/Search/Ads领域的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23515v1": {
    "title": "FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time",
    "url": "https://www.alphaxiv.org/abs/2510.23515v1",
    "arxiv_id": "2510.23515v1",
    "authors": "Yaoli Liu, Yao-Xiang Ding, Kun Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 16:54:08",
    "ori_summary": "This paper proposes FreeFuse, a novel training-free approach for multi-subject text-to-image generation through automatic fusion of multiple subject LoRAs. In contrast to existing methods that either focus on pre-inference LoRA weight merging or rely on segmentation models and complex techniques like noise blending to isolate LoRA outputs, our key insight is that context-aware dynamic subject masks can be automatically derived from cross-attention layer weights. Mathematical analysis shows that directly applying these masks to LoRA outputs during inference well approximates the case where the subject LoRA is integrated into the diffusion model and used individually for the masked region. FreeFuse demonstrates superior practicality and efficiency as it requires no additional training, no modification to LoRAs, no auxiliary models, and no user-defined prompt templates or region specifications. Alternatively, it only requires users to provide the LoRA activation words for seamless integration into standard workflows. Extensive experiments validate that FreeFuse outperforms existing approaches in both generation quality and usability under the multi-subject generation tasks. The project page is at https://future-item.github.io/FreeFuse/",
    "summary": "",
    "translation": "FreeFuse：通过测试时自动掩码实现多主体LoRA融合",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LoRA（Low-Rank Adaptation）融合技术，属于模型微调优化领域。虽然LoRA技术本身在LLM高效适配中有应用，但该论文聚焦于多主体融合和测试时自动掩码，更偏向于图像生成或多模态生成任务，与推荐系统、搜索或广告的核心技术栈关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23512v1": {
    "title": "Localising under the drape: proprioception in the era of distributed surgical robotic system",
    "url": "https://www.alphaxiv.org/abs/2510.23512v1",
    "arxiv_id": "2510.23512v1",
    "authors": "Martin Huber, Nicola A. Cavalcanti, Ayoob Davoodi, Ruixuan Li, Christopher E. Mower, Fabio Carrillo, Christoph J. Laux, Francois Teyssere, Thibault Chandanson, Antoine Harlé, Elie Saghbiny, Mazda Farshad, Guillaume Morel, Emmanuel Vander Poorten, Philipp Fürnstahl, Sébastien Ourselin, Christos Bergeles, Tom Vercauteren",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-27 16:50:12",
    "ori_summary": "Despite their mechanical sophistication, surgical robots remain blind to their surroundings. This lack of spatial awareness causes collisions, system recoveries, and workflow disruptions, issues that will intensify with the introduction of distributed robots with independent interacting arms. Existing tracking systems rely on bulky infrared cameras and reflective markers, providing only limited views of the surgical scene and adding hardware burden in crowded operating rooms. We present a marker-free proprioception method that enables precise localisation of surgical robots under their sterile draping despite associated obstruction of visual cues. Our method solely relies on lightweight stereo-RGB cameras and novel transformer-based deep learning models. It builds on the largest multi-centre spatial robotic surgery dataset to date (1.4M self-annotated images from human cadaveric and preclinical in vivo studies). By tracking the entire robot and surgical scene, rather than individual markers, our approach provides a holistic view robust to occlusions, supporting surgical scene understanding and context-aware control. We demonstrate an example of potential clinical benefits during in vivo breathing compensation with access to tissue dynamics, unobservable under state of the art tracking, and accurately locate in multi-robot systems for future intelligent interaction. In addition, and compared with existing systems, our method eliminates markers and improves tracking visibility by 25%. To our knowledge, this is the first demonstration of marker-free proprioception for fully draped surgical robots, reducing setup complexity, enhancing safety, and paving the way toward modular and autonomous robotic surgery.",
    "summary": "",
    "translation": "分布式手术机器人系统时代下的本体感觉定位",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于医疗领域的手术机器人系统，涉及机器人定位和本体感觉技术。这些主题与推荐系统、搜索或广告领域没有任何关联，完全超出了我的关注范围。论文内容纯粹属于医疗机器人应用，没有任何潜在的技术可以应用于推荐、搜索或广告系统。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23504v1": {
    "title": "iPac: Incorporating Intra-image Patch Context into Graph Neural Networks for Medical Image Classification",
    "url": "https://www.alphaxiv.org/abs/2510.23504v1",
    "arxiv_id": "2510.23504v1",
    "authors": "Usama Zidan, Mohamed Gaber, Mohammed M. Abdelsamea",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 16:37:16",
    "ori_summary": "Graph neural networks have emerged as a promising paradigm for image processing, yet their performance in image classification tasks is hindered by a limited consideration of the underlying structure and relationships among visual entities. This work presents iPac, a novel approach to introduce a new graph representation of images to enhance graph neural network image classification by recognizing the importance of underlying structure and relationships in medical image classification. iPac integrates various stages, including patch partitioning, feature extraction, clustering, graph construction, and graph-based learning, into a unified network to advance graph neural network image classification. By capturing relevant features and organising them into clusters, we construct a meaningful graph representation that effectively encapsulates the semantics of the image. Experimental evaluation on diverse medical image datasets demonstrates the efficacy of iPac, exhibiting an average accuracy improvement of up to 5% over baseline methods. Our approach offers a versatile and generic solution for image classification, particularly in the realm of medical images, by leveraging the graph representation and accounting for the inherent structure and relationships among visual entities.",
    "summary": "",
    "translation": "iPac：将图像内块上下文整合到图神经网络中用于医学图像分类",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分类这一明确排除的领域，与推荐系统、搜索或广告无关。虽然提到了图神经网络架构，但其应用场景完全属于医疗领域，没有任何潜在的应用于RecSys/Search/Ads的可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23497v1": {
    "title": "VOLD: Reasoning Transfer from LLMs to Vision-Language Models via On-Policy Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.23497v1",
    "arxiv_id": "2510.23497v1",
    "authors": "Walid Bousselham, Hilde Kuehne, Cordelia Schmid",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 16:32:12",
    "ori_summary": "Training vision-language models (VLMs) for complex reasoning remains a challenging task, i.a. due to the scarcity of high-quality image-text reasoning data. Conversely, text-based reasoning resources are abundant and scalable, but it is still an open question how to leveraging them for VLM reasoning. To address this problem, we propose VOLD, a framework to transfer reasoning capabilities from text-only teacher models to VLM student models. To this end, VOLD combines reinforcement learning via Group Relative Policy Optimization (GRPO) with on-policy distillation, which allows the student reasoning traces to be guided by the teacher model, resulting in a significant gain over using GRPO alone. We further show that a cold-start alignment is essential for an effective transfer during the online training phase in this scenario and that without sufficient distributional alignment between teacher and student, on-policy distillation fails to provide meaningful guidance. We evaluate VOLD across diverse benchmarks including MMMU-Pro, MathVision, MathVista, and LogicVista, showing that VOLD outperforms the baseline model significantly and improves over the state of the art by a margin. Our ablation shows the importance of a cold-start alignment via SFT for on-policy distillation with a text-only teacher.",
    "summary": "",
    "translation": "VOLD：通过策略蒸馏实现从大语言模型到视觉语言模型的推理迁移",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视觉语言模型之间的知识蒸馏技术，虽然涉及LLM到VLM的迁移，但其核心应用场景偏向视觉推理任务。对于推荐、搜索或广告领域，这种跨模态蒸馏技术的潜在应用较为间接，可能用于多模态内容理解，但缺乏明确的直接应用路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23494v1": {
    "title": "Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap",
    "url": "https://www.alphaxiv.org/abs/2510.23494v1",
    "arxiv_id": "2510.23494v1",
    "authors": "Elisabeth Jüttner, Leona Krath, Stefan Korfhage, Hannah Dröge, Matthias B. Hullin, Markus Plack",
    "categories": "cs.CV, cs.GR",
    "pub_date": "2025-10-27 16:28:55",
    "ori_summary": "Volumetric video relighting is essential for bringing captured performances into virtual worlds, but current approaches struggle to deliver temporally stable, production-ready results. Diffusion-based intrinsic decomposition methods show promise for single frames, yet suffer from stochastic noise and instability when extended to sequences, while video diffusion models remain constrained by memory and scale. We propose a hybrid relighting framework that combines diffusion-derived material priors with temporal regularization and physically motivated rendering. Our method aggregates multiple stochastic estimates of per-frame material properties into temporally consistent shading components, using optical-flow-guided regularization. For indirect effects such as shadows and reflections, we extract a mesh proxy from Gaussian Opacity Fields and render it within a standard graphics pipeline. Experiments on real and synthetic captures show that this hybrid strategy achieves substantially more stable relighting across sequences than diffusion-only baselines, while scaling beyond the clip lengths feasible for video diffusion. These results indicate that hybrid approaches, which balance learned priors with physically grounded constraints, are a practical step toward production-ready volumetric video relighting.",
    "summary": "",
    "translation": "Yesnt：扩散重光照模型是否已准备好用于拍摄阶段合成？一种弥合差距的混合替代方案",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的扩散模型重光照和图像合成技术，主要涉及图形学和视觉特效应用。虽然提到了扩散模型，但其应用场景是拍摄阶段的视觉合成，与推荐系统、搜索或广告的排名和建模需求没有直接关联。该技术方向属于纯粹的视觉应用范畴，不符合当前关注的核心领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23484v1": {
    "title": "T-REGS: Minimum Spanning Tree Regularization for Self-Supervised Learning",
    "url": "https://www.alphaxiv.org/abs/2510.23484v1",
    "arxiv_id": "2510.23484v1",
    "authors": "Julie Mordacq, David Loiseaux, Vicky Kalogeiton, Steve Oudot",
    "categories": "cs.LG, cs.CG, cs.CV",
    "pub_date": "2025-10-27 16:16:40",
    "ori_summary": "Self-supervised learning (SSL) has emerged as a powerful paradigm for learning representations without labeled data, often by enforcing invariance to input transformations such as rotations or blurring. Recent studies have highlighted two pivotal properties for effective representations: (i) avoiding dimensional collapse-where the learned features occupy only a low-dimensional subspace, and (ii) enhancing uniformity of the induced distribution. In this work, we introduce T-REGS, a simple regularization framework for SSL based on the length of the Minimum Spanning Tree (MST) over the learned representation. We provide theoretical analysis demonstrating that T-REGS simultaneously mitigates dimensional collapse and promotes distribution uniformity on arbitrary compact Riemannian manifolds. Several experiments on synthetic data and on classical SSL benchmarks validate the effectiveness of our approach at enhancing representation quality.",
    "summary": "",
    "translation": "T-REGS：自监督学习中的最小生成树正则化",
    "relevance_score": 2,
    "reasoning": "该论文提出了一种自监督学习中的正则化方法，虽然自监督学习是机器学习的重要方向，但论文聚焦于最小生成树这一特定技术，与推荐系统、搜索或广告的核心进展缺乏直接关联。该方法在推荐系统中的应用潜力有限，主要适用于图结构数据的表示学习，而非当前关注的LLM技术、Transformer架构或异构数据统一建模等核心方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23482v1": {
    "title": "On the Faithfulness of Visual Thinking: Measurement and Enhancement",
    "url": "https://www.alphaxiv.org/abs/2510.23482v1",
    "arxiv_id": "2510.23482v1",
    "authors": "Zujing Liu, Junwen Pan, Qi She, Yuan Gao, Guisong Xia",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-27 16:15:54",
    "ori_summary": "Recent large vision-language models (LVLMs) can generate vision-text multimodal chain-of-thought (MCoT) traces after reinforcement fine-tuning (RFT). However, we observe that the visual information incorporated in MCoT is often inaccurate, though still yield correct answers, indicating a lack of faithfulness in the MCoT reasoning process. We attribute this unfaithfulness to the RL reward in RFT, which solely incentivizes the format of interleaved vision-text cues, ie, it encourages the model to incorporate visual information into its text reasoning steps without considering the correctness of the visual information. In this paper, we first probe the faithfulness of MCoT by measuring how much the prediction changes when its visual and textual thoughts are intervened. Surprisingly, the model's predictions remain nearly unchanged under visual intervention but change significantly under textual intervention, indicating that the visual evidence is largely ignored. To further analyze visual information, we introduce an automated LVLM-based evaluation metric that quantifies the faithfulness of visual cues from two perspectives: reliability and sufficiency. Our evaluation reveals that the visual information in current MCoT traces is simultaneously unreliable and insufficient. To address this issue, we propose a novel MCoT learning strategy termed Sufficient-Component Cause Model (SCCM) learning. This approach encourages the MCoT to generate sufficient yet minimal visual components that are independently capable of leading to correct answers. We note that the proposed SCCM is annotation-free and compatible with various RFT for MCoT in a plug-and-play manner. Empirical results demonstrate that SCCM consistently improves the visual faithfulness across a suite of fine-grained perception and reasoning benchmarks. Code is available at https://github.com/EugeneLiu01/Faithful_Thinking_with_Image.",
    "summary": "",
    "translation": "论视觉思维的忠实性：测量与增强",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于视觉思维的忠实性测量与增强，属于认知科学和心理学领域，与推荐系统、搜索或广告的核心技术进展无关。该主题不涉及Transformer架构、LLM技术或异构数据统一建模，也没有任何明显的应用场景可以关联到我的关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23479v1": {
    "title": "MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.23479v1",
    "arxiv_id": "2510.23479v1",
    "authors": "Xin Jin, Siyuan Li, Siyong Jian, Kai Yu, Huan Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 16:12:40",
    "ori_summary": "Vision-language alignment in multi-modal large language models (MLLMs) typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). SFT is stable and efficient but requires large-scale human annotations and cannot capture subtle preferences, while RL brings in a reward signal for training, but suffers from overhead and instability. These limitations highlight a trade-off between scalability, robustness, and alignment quality. To address this, we propose MergeMix, a training-time augmentation paradigm that bridges SFT and RL. It first applies an attention-aware image mixing via token merge with more cluster representation and spatial context, and then presents a preference-driven training paradigm for MLLMs by building preference pairs with mixed images and raw images, and optimizing via SimPO loss. As a mixup augmentation, MergeMix enhances attention consistency and efficiency, surpassing other heuristic-based methods in classification. Extensive experiments demonstrate that MergeMix achieves competitive accuracy with improved efficiency, providing a scalable approach to preference alignment in classification and MLLMs.",
    "summary": "",
    "translation": "MergeMix：一种用于视觉与多模态理解的统一增强范式",
    "relevance_score": 3,
    "reasoning": "该论文提出了一种多模态数据增强方法，虽然涉及多模态理解，但主要聚焦于视觉领域而非推荐系统、搜索或广告的核心技术。其潜在的跨模态数据增强技术可能适用于处理推荐系统中的异构特征，但应用场景不够直接和明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23478v1": {
    "title": "UrbanIng-V2X: A Large-Scale Multi-Vehicle, Multi-Infrastructure Dataset Across Multiple Intersections for Cooperative Perception",
    "url": "https://www.alphaxiv.org/abs/2510.23478v1",
    "arxiv_id": "2510.23478v1",
    "authors": "Karthikeyan Chandra Sekaran, Markus Geisler, Dominik Rößle, Adithya Mohan, Daniel Cremers, Wolfgang Utschick, Michael Botsch, Werner Huber, Torsten Schön",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 16:12:12",
    "ori_summary": "Recent cooperative perception datasets have played a crucial role in advancing smart mobility applications by enabling information exchange between intelligent agents, helping to overcome challenges such as occlusions and improving overall scene understanding. While some existing real-world datasets incorporate both vehicle-to-vehicle and vehicle-to-infrastructure interactions, they are typically limited to a single intersection or a single vehicle. A comprehensive perception dataset featuring multiple connected vehicles and infrastructure sensors across several intersections remains unavailable, limiting the benchmarking of algorithms in diverse traffic environments. Consequently, overfitting can occur, and models may demonstrate misleadingly high performance due to similar intersection layouts and traffic participant behavior. To address this gap, we introduce UrbanIng-V2X, the first large-scale, multi-modal dataset supporting cooperative perception involving vehicles and infrastructure sensors deployed across three urban intersections in Ingolstadt, Germany. UrbanIng-V2X consists of 34 temporally aligned and spatially calibrated sensor sequences, each lasting 20 seconds. All sequences contain recordings from one of three intersections, involving two vehicles and up to three infrastructure-mounted sensor poles operating in coordinated scenarios. In total, UrbanIng-V2X provides data from 12 vehicle-mounted RGB cameras, 2 vehicle LiDARs, 17 infrastructure thermal cameras, and 12 infrastructure LiDARs. All sequences are annotated at a frequency of 10 Hz with 3D bounding boxes spanning 13 object classes, resulting in approximately 712k annotated instances across the dataset. We provide comprehensive evaluations using state-of-the-art cooperative perception methods and publicly release the codebase, dataset, HD map, and a digital twin of the complete data collection environment.",
    "summary": "",
    "translation": "UrbanIng-V2X：一个用于协同感知的大规模多车辆、多基础设施跨多个路口数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域的协同感知数据集，涉及车辆与基础设施通信(V2X)技术。这与推荐系统、搜索或广告的核心领域完全无关，也不涉及LLM技术、Transformer架构进展或异构数据统一建模。该数据集纯粹面向自动驾驶应用，没有任何潜在的应用于RecSys/Search/Ads的途径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23473v1": {
    "title": "Video-Thinker: Sparking \"Thinking with Videos\" via Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.23473v1",
    "arxiv_id": "2510.23473v1",
    "authors": "Shijian Wang, Jiarui Jin, Xingjian Wang, Linxin Song, Runhao Fu, Hecheng Wang, Zongyuan Ge, Yuan Lu, Xuelian Cheng",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 16:10:45",
    "ori_summary": "Recent advances in image reasoning methods, particularly \"Thinking with Images\", have demonstrated remarkable success in Multimodal Large Language Models (MLLMs); however, this dynamic reasoning paradigm has not yet been extended to video reasoning tasks. In this paper, we propose Video-Thinker, which empowers MLLMs to think with videos by autonomously leveraging their intrinsic \"grounding\" and \"captioning\" capabilities to generate reasoning clues throughout the inference process. To spark this capability, we construct Video-Thinker-10K, a curated dataset featuring autonomous tool usage within chain-of-thought reasoning sequences. Our training strategy begins with Supervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group Relative Policy Optimization (GRPO) to strengthen this reasoning capability. Through this approach, Video-Thinker enables MLLMs to autonomously navigate grounding and captioning tasks for video reasoning, eliminating the need for constructing and calling external tools. Extensive experiments demonstrate that Video-Thinker achieves significant performance gains on both in-domain tasks and challenging out-of-domain video reasoning benchmarks, including Video-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B substantially outperforms existing baselines such as Video-R1 and establishes state-of-the-art performance among 7B-sized MLLMs.",
    "summary": "",
    "translation": "Video-Thinker：通过强化学习激发“视频思考”",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及视频理解和强化学习，但未明确展示与推荐系统、搜索或广告的关联。虽然强化学习在推荐系统中可能有应用，但标题强调“视频思考”这一概念，更偏向通用视频理解任务，而非特定于RecSys/Search/Ads领域的直接应用或核心进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23444v1": {
    "title": "FRBNet: Revisiting Low-Light Vision through Frequency-Domain Radial Basis Network",
    "url": "https://www.alphaxiv.org/abs/2510.23444v1",
    "arxiv_id": "2510.23444v1",
    "authors": "Fangtong Sun, Congyu Li, Ke Yang, Yuchen Pan, Hanwen Yu, Xichuan Zhang, Yiying Li",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-27 15:46:07",
    "ori_summary": "Low-light vision remains a fundamental challenge in computer vision due to severe illumination degradation, which significantly affects the performance of downstream tasks such as detection and segmentation. While recent state-of-the-art methods have improved performance through invariant feature learning modules, they still fall short due to incomplete modeling of low-light conditions. Therefore, we revisit low-light image formation and extend the classical Lambertian model to better characterize low-light conditions. By shifting our analysis to the frequency domain, we theoretically prove that the frequency-domain channel ratio can be leveraged to extract illumination-invariant features via a structured filtering process. We then propose a novel and end-to-end trainable module named \\textbf{F}requency-domain \\textbf{R}adial \\textbf{B}asis \\textbf{Net}work (\\textbf{FRBNet}), which integrates the frequency-domain channel ratio operation with a learnable frequency domain filter for the overall illumination-invariant feature enhancement. As a plug-and-play module, FRBNet can be integrated into existing networks for low-light downstream tasks without modifying loss functions. Extensive experiments across various downstream tasks demonstrate that FRBNet achieves superior performance, including +2.2 mAP for dark object detection and +2.9 mIoU for nighttime segmentation. Code is available at: https://github.com/Sing-Forevet/FRBNet.",
    "summary": "",
    "translation": "FRBNet：通过频域径向基网络重新审视低光视觉",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的低光图像处理，使用频域分析和径向基网络来解决特定视觉问题。这与推荐系统、搜索或广告的核心技术焦点无关，也不涉及Transformer架构、LLM技术或异构数据建模，无法找到在RecSys/Search/Ads领域的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23442v1": {
    "title": "CURVETE: Curriculum Learning and Progressive Self-supervised Training for Medical Image Classification",
    "url": "https://www.alphaxiv.org/abs/2510.23442v1",
    "arxiv_id": "2510.23442v1",
    "authors": "Asmaa Abbas, Mohamed Gaber, Mohammed M. Abdelsamea",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 15:46:02",
    "ori_summary": "Identifying high-quality and easily accessible annotated samples poses a notable challenge in medical image analysis. Transfer learning techniques, leveraging pre-training data, offer a flexible solution to this issue. However, the impact of fine-tuning diminishes when the dataset exhibits an irregular distribution between classes. This paper introduces a novel deep convolutional neural network, named Curriculum Learning and Progressive Self-supervised Training (CURVETE). CURVETE addresses challenges related to limited samples, enhances model generalisability, and improves overall classification performance. It achieves this by employing a curriculum learning strategy based on the granularity of sample decomposition during the training of generic unlabelled samples. Moreover, CURVETE address the challenge of irregular class distribution by incorporating a class decomposition approach in the downstream task. The proposed method undergoes evaluation on three distinct medical image datasets: brain tumour, digital knee x-ray, and Mini-DDSM datasets. We investigate the classification performance using a generic self-supervised sample decomposition approach with and without the curriculum learning component in training the pretext task. Experimental results demonstrate that the CURVETE model achieves superior performance on test sets with an accuracy of 96.60% on the brain tumour dataset, 75.60% on the digital knee x-ray dataset, and 93.35% on the Mini-DDSM dataset using the baseline ResNet-50. Furthermore, with the baseline DenseNet-121, it achieved accuracies of 95.77%, 80.36%, and 93.22% on the brain tumour, digital knee x-ray, and Mini-DDSM datasets, respectively, outperforming other training strategies.",
    "summary": "",
    "translation": "CURVETE：面向医学图像分类的课程学习与渐进式自监督训练",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分类这一特定领域应用，属于明确的无关主题范畴。虽然提到了课程学习和自监督训练等技术，但这些方法在医学图像领域的应用与推荐系统、搜索或广告的核心技术发展没有直接关联，也不具备在这些领域应用的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23429v1": {
    "title": "MiCADangelo: Fine-Grained Reconstruction of Constrained CAD Models from 3D Scans",
    "url": "https://www.alphaxiv.org/abs/2510.23429v1",
    "arxiv_id": "2510.23429v1",
    "authors": "Ahmet Serdar Karadeniz, Dimitrios Mallis, Danila Rukhovich, Kseniya Cherenkova, Anis Kacem, Djamila Aouada",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 15:33:51",
    "ori_summary": "Computer-Aided Design (CAD) plays a foundational role in modern manufacturing and product development, often requiring designers to modify or build upon existing models. Converting 3D scans into parametric CAD representations--a process known as CAD reverse engineering--remains a significant challenge due to the high precision and structural complexity of CAD models. Existing deep learning-based approaches typically fall into two categories: bottom-up, geometry-driven methods, which often fail to produce fully parametric outputs, and top-down strategies, which tend to overlook fine-grained geometric details. Moreover, current methods neglect an essential aspect of CAD modeling: sketch-level constraints. In this work, we introduce a novel approach to CAD reverse engineering inspired by how human designers manually perform the task. Our method leverages multi-plane cross-sections to extract 2D patterns and capture fine parametric details more effectively. It enables the reconstruction of detailed and editable CAD models, outperforming state-of-the-art methods and, for the first time, incorporating sketch constraints directly into the reconstruction process.",
    "summary": "",
    "translation": "MiCADangelo：从3D扫描中精细重建受约束的CAD模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D扫描到CAD模型的重建技术，属于计算机图形学和3D建模领域。虽然涉及3D数据处理，但与推荐系统、搜索或广告的核心技术（如排序、用户建模、内容理解）没有直接关联，也不属于Transformer架构改进或LLM应用范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23416v1": {
    "title": "Quality-controlled registration of urban MLS point clouds reducing drift effects by adaptive fragmentation",
    "url": "https://www.alphaxiv.org/abs/2510.23416v1",
    "arxiv_id": "2510.23416v1",
    "authors": "Marco Antonio Ortiz Rincon, Yihui Yang, Christoph Holst",
    "categories": "cs.CV, eess.SP",
    "pub_date": "2025-10-27 15:21:39",
    "ori_summary": "This study presents a novel workflow designed to efficiently and accurately register large-scale mobile laser scanning (MLS) point clouds to a target model point cloud in urban street scenarios. This workflow specifically targets the complexities inherent in urban environments and adeptly addresses the challenges of integrating point clouds that vary in density, noise characteristics, and occlusion scenarios, which are common in bustling city centers. Two methodological advancements are introduced. First, the proposed Semi-sphere Check (SSC) preprocessing technique optimally fragments MLS trajectory data by identifying mutually orthogonal planar surfaces. This step reduces the impact of MLS drift on the accuracy of the entire point cloud registration, while ensuring sufficient geometric features within each fragment to avoid local minima. Second, we propose Planar Voxel-based Generalized Iterative Closest Point (PV-GICP), a fine registration method that selectively utilizes planar surfaces within voxel partitions. This pre-process strategy not only improves registration accuracy but also reduces computation time by more than 50% compared to conventional point-to-plane ICP methods. Experiments on real-world datasets from Munich's inner city demonstrate that our workflow achieves sub-0.01 m average registration accuracy while significantly shortening processing times. The results underscore the potential of the proposed methods to advance automated 3D urban modeling and updating, with direct applications in urban planning, infrastructure management, and dynamic city monitoring.",
    "summary": "",
    "translation": "通过自适应分块减少漂移效应的城市移动激光扫描点云质量控制配准",
    "relevance_score": 1,
    "reasoning": "该论文专注于移动激光扫描点云的配准技术，属于计算机视觉和3D数据处理领域。虽然标题提到质量控制，但这与推荐系统、搜索或广告的核心技术没有直接关联，也不涉及LLM、Transformer架构或异构数据建模。该技术主要应用于城市测绘和地理信息系统，无法找到在RecSys/Search/Ads领域的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23415v1": {
    "title": "Towards Generalisable Foundation Models for 3D Brain MRI",
    "url": "https://www.alphaxiv.org/abs/2510.23415v1",
    "arxiv_id": "2510.23415v1",
    "authors": "Moona Mazher, Geoff J. M. Parker, Daniel C. Alexander",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 15:19:46",
    "ori_summary": "Foundation models in artificial intelligence (AI) are transforming medical imaging by enabling general-purpose feature learning from large-scale, unlabeled datasets. In this work, we introduce BrainFound, a self-supervised foundation model for brain MRI, built by extending DINO-v2, a vision transformer originally designed for 2D natural images. BrainFound adapts DINO-v2 to model full 3D brain anatomy by incorporating volumetric information from sequential MRI slices, moving beyond conventional single-slice paradigms. It supports both single- and multimodal inputs, enabling a broad range of downstream tasks, including disease detection and image segmentation, while generalising across varied imaging protocols and clinical scenarios. We show that BrainFound consistently outperforms existing self-supervised pretraining strategies and supervised baselines, particularly in label-scarce and multi-contrast settings. By integrating information from diverse 3D MRI modalities (e.g., T1, T2, FLAIR), it enhances diagnostic accuracy and reduces dependency on extensive expert annotations. This flexibility makes BrainFound a scalable and practical solution for 3D neuroimaging pipelines, with significant potential for clinical deployment and research innovation.",
    "summary": "",
    "translation": "面向3D脑部核磁共振成像的通用基础模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像领域的3D脑部MRI，属于明确的医学/生物学应用范畴，与搜索、推荐、广告系统完全无关。论文标题明确指向医疗影像分析，没有任何技术要素表明其与推荐系统、搜索或广告领域存在潜在关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23414v1": {
    "title": "Symmetria: A Synthetic Dataset for Learning in Point Clouds",
    "url": "https://www.alphaxiv.org/abs/2510.23414v1",
    "arxiv_id": "2510.23414v1",
    "authors": "Ivan Sipiran, Gustavo Santelices, Lucas Oyarzún, Andrea Ranieri, Chiara Romanengo, Silvia Biasotti, Bianca Falcidieno",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 15:18:26",
    "ori_summary": "Unlike image or text domains that benefit from an abundance of large-scale datasets, point cloud learning techniques frequently encounter limitations due to the scarcity of extensive datasets. To overcome this limitation, we present Symmetria, a formula-driven dataset that can be generated at any arbitrary scale. By construction, it ensures the absolute availability of precise ground truth, promotes data-efficient experimentation by requiring fewer samples, enables broad generalization across diverse geometric settings, and offers easy extensibility to new tasks and modalities. Using the concept of symmetry, we create shapes with known structure and high variability, enabling neural networks to learn point cloud features effectively. Our results demonstrate that this dataset is highly effective for point cloud self-supervised pre-training, yielding models with strong performance in downstream tasks such as classification and segmentation, which also show good few-shot learning capabilities. Additionally, our dataset can support fine-tuning models to classify real-world objects, highlighting our approach's practical utility and application. We also introduce a challenging task for symmetry detection and provide a benchmark for baseline comparisons. A significant advantage of our approach is the public availability of the dataset, the accompanying code, and the ability to generate very large collections, promoting further research and innovation in point cloud learning.",
    "summary": "",
    "translation": "Symmetria：用于点云学习的合成数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于点云数据集的创建，属于计算机视觉中的3D视觉领域。点云处理与推荐系统、搜索或广告的核心技术没有直接关联，也不涉及Transformer架构改进或LLM技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23399v1": {
    "title": "Color and Frequency Correction for Image Colorization",
    "url": "https://www.alphaxiv.org/abs/2510.23399v1",
    "arxiv_id": "2510.23399v1",
    "authors": "Yun Kai Zhuang",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 14:57:14",
    "ori_summary": "The project has carried out the re-optimization of image coloring in accordance with the existing Autocolorization direction model DDColor. For the experiments on the existing weights of DDColor, we found that it has limitations in some frequency bands and the color cast problem caused by insufficient input dimension. We construct two optimization schemes and combine them, which achieves the performance improvement of indicators such as PSNR and SSIM of the images after DDColor.",
    "summary": "",
    "translation": "图像着色中的颜色与频率校正",
    "relevance_score": 1,
    "reasoning": "该论文专注于图像颜色校正的计算机视觉技术，属于纯粹的图像处理领域。虽然图像在推荐和搜索系统中可能作为内容特征出现，但该论文的核心技术（颜色和频率校正）与推荐系统、搜索或广告的核心排名、建模或架构进步没有直接关联，也不涉及LLM或Transformer技术的应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23397v1": {
    "title": "VideoTG-R1: Boosting Video Temporal Grounding via Curriculum Reinforcement Learning on Reflected Boundary Annotations",
    "url": "https://www.alphaxiv.org/abs/2510.23397v1",
    "arxiv_id": "2510.23397v1",
    "authors": "Lu Dong, Haiyu Zhang, Han Lin, Ziang Yan, Xiangyu Zeng, Hongjie Zhang, Yifei Huang, Yi Wang, Zhen-Hua Ling, Limin Wang, Yali Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 14:55:38",
    "ori_summary": "Video temporal grounding (VTG) aims to locate precise segments in videos based on language queries, which is a fundamental challenge in video understanding. While recent Multimodal Large Language Models (MLLMs) have shown promise in tackling VTG through reinforcement learning (RL), they overlook the challenges arising from both the quality and difficulty of training samples. (1) Partially annotated samples. Many samples contain relevant segments beyond the annotated interval, introducing ambiguous supervision. (2) Hard-to-ground samples. Samples with poor zero-shot performance produce consistently low and indistinguishable rewards during RL training, exhibiting no clear preference among multiple outputs and thus hindering learning efficiency. To address these challenges, we propose VideoTG-R1, a novel curriculum RL framework with reflected boundary annotations, enabling data-efficient training. Specifically, we propose a Boundary Reflection Agent that utilizes MLLMs to predict query-relevant timestamps outside the annotated intervals, allowing us to identify and filter out partially annotated samples, thereby reducing ambiguity. Furthermore, we introduce a Difficulty Estimation Agent to assess the training difficulty of each sample and design a curriculum RL strategy that dynamically masks the videos of hard-to-ground samples according to the training steps, easing the training difficulty and providing clearer preference. Experiments on the VTG and grounded VideoQA tasks demonstrate the effectiveness of our method. Remarkably, with only 10% of the training samples and 21% of the computational budget, VideoTG-R1 outperforms full-data counterparts under both group relative policy optimization (GRPO) and supervised fine-tuning (SFT). The code is available at https://github.com/ldong1111/VideoTG-R1.",
    "summary": "",
    "translation": "VideoTG-R1：通过基于反射边界标注的课程强化学习提升视频时序定位性能",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频时序定位这一计算机视觉任务，虽然使用了强化学习技术，但与推荐系统、搜索或广告领域没有直接关联。强化学习在视频时序定位中的应用难以迁移到RecSys/Search/Ads的核心排序或推荐任务中，缺乏明确的跨领域应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23382v1": {
    "title": "An Efficient Remote Sensing Super Resolution Method Exploring Diffusion Priors and Multi-Modal Constraints for Crop Type Mapping",
    "url": "https://www.alphaxiv.org/abs/2510.23382v1",
    "arxiv_id": "2510.23382v1",
    "authors": "Songxi Yang, Tang Sui, Qunying Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 14:34:52",
    "ori_summary": "Super resolution offers a way to harness medium even lowresolution but historically valuable remote sensing image archives. Generative models, especially diffusion models, have recently been applied to remote sensing super resolution (RSSR), yet several challenges exist. First, diffusion models are effective but require expensive training from scratch resources and have slow inference speeds. Second, current methods have limited utilization of auxiliary information as real-world constraints to reconstruct scientifically realistic images. Finally, most current methods lack evaluation on downstream tasks. In this study, we present a efficient LSSR framework for RSSR, supported by a new multimodal dataset of paired 30 m Landsat 8 and 10 m Sentinel 2 imagery. Built on frozen pretrained Stable Diffusion, LSSR integrates crossmodal attention with auxiliary knowledge (Digital Elevation Model, land cover, month) and Synthetic Aperture Radar guidance, enhanced by adapters and a tailored Fourier NDVI loss to balance spatial details and spectral fidelity. Extensive experiments demonstrate that LSSR significantly improves crop boundary delineation and recovery, achieving state-of-the-art performance with Peak Signal-to-Noise Ratio/Structural Similarity Index Measure of 32.63/0.84 (RGB) and 23.99/0.78 (IR), and the lowest NDVI Mean Squared Error (0.042), while maintaining efficient inference (0.39 sec/image). Moreover, LSSR transfers effectively to NASA Harmonized Landsat and Sentinel (HLS) super resolution, yielding more reliable crop classification (F1: 0.86) than Sentinel-2 (F1: 0.85). These results highlight the potential of RSSR to advance precision agriculture.",
    "summary": "",
    "translation": "一种探索扩散先验和多模态约束的高效遥感超分辨率方法用于作物类型制图",
    "relevance_score": 2,
    "reasoning": "该论文主要关注遥感图像超分辨率和作物类型映射，属于计算机视觉在农业领域的特定应用。虽然提到了多模态约束，但这与推荐系统、搜索或广告中的异构数据建模没有直接关联。论文的核心技术（扩散先验、超分辨率）在推荐系统、搜索或广告领域缺乏明确的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23368v1": {
    "title": "PlanarTrack: A high-quality and challenging benchmark for large-scale planar object tracking",
    "url": "https://www.alphaxiv.org/abs/2510.23368v1",
    "arxiv_id": "2510.23368v1",
    "authors": "Yifan Jiao, Xinran Liu, Xiaoqiong Liu, Xiaohui Yuan, Heng Fan, Libo Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 14:18:13",
    "ori_summary": "Planar tracking has drawn increasing interest owing to its key roles in robotics and augmented reality. Despite recent great advancement, further development of planar tracking, particularly in the deep learning era, is largely limited compared to generic tracking due to the lack of large-scale platforms. To mitigate this, we propose PlanarTrack, a large-scale high-quality and challenging benchmark for planar tracking. Specifically, PlanarTrack consists of 1,150 sequences with over 733K frames, including 1,000 short-term and 150 new long-term videos, which enables comprehensive evaluation of short- and long-term tracking performance. All videos in PlanarTrack are recorded in unconstrained conditions from the wild, which makes PlanarTrack challenging but more realistic for real-world applications. To ensure high-quality annotations, each video frame is manually annotated by four corner points with multi-round meticulous inspection and refinement. To enhance target diversity of PlanarTrack, we only capture a unique target in one sequence, which is different from existing benchmarks. To our best knowledge, PlanarTrack is by far the largest and most diverse and challenging dataset dedicated to planar tracking. To understand performance of existing methods on PlanarTrack and to provide a comparison for future research, we evaluate 10 representative planar trackers with extensive comparison and in-depth analysis. Our evaluation reveals that, unsurprisingly, the top planar trackers heavily degrade on the challenging PlanarTrack, which indicates more efforts are required for improving planar tracking. Our data and results will be released at https://github.com/HengLan/PlanarTrack",
    "summary": "",
    "translation": "PlanarTrack：一个面向大规模平面目标跟踪的高质量且具有挑战性的基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的平面目标跟踪基准，属于纯粹的视觉研究领域。虽然标题提到\"大规模\"，但这是指视觉跟踪任务的数据规模，与推荐系统、搜索或广告中的大规模用户行为建模没有直接关联。该工作没有展示在异构数据统一建模或Transformer架构改进方面的潜力，无法应用于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23363v1": {
    "title": "Interpretable Tile-Based Classification of Paclitaxel Exposure",
    "url": "https://www.alphaxiv.org/abs/2510.23363v1",
    "arxiv_id": "2510.23363v1",
    "authors": "Sean Fletcher, Gabby Scott, Douglas Currie, Xin Zhang, Yuqi Song, Bruce MacLeod",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 14:13:51",
    "ori_summary": "Medical image analysis is central to drug discovery and preclinical evaluation, where scalable, objective readouts can accelerate decision-making. We address classification of paclitaxel (Taxol) exposure from phase-contrast microscopy of C6 glioma cells -- a task with subtle dose differences that challenges full-image models. We propose a simple tiling-and-aggregation pipeline that operates on local patches and combines tile outputs into an image label, achieving state-of-the-art accuracy on the benchmark dataset and improving over the published baseline by around 20 percentage points, with trends confirmed by cross-validation. To understand why tiling is effective, we further apply Grad-CAM and Score-CAM and attention analyses, which enhance model interpretability and point toward robustness-oriented directions for future medical image research. Code is released to facilitate reproduction and extension.",
    "summary": "",
    "translation": "基于可解释图块分类的紫杉醇暴露预测",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向紫杉醇（一种化疗药物）的医学/生物学应用，这属于明确的无关主题范畴。论文内容涉及药物暴露分类的医学领域，与推荐系统、搜索、广告或相关使能技术没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23325v1": {
    "title": "Multitask Multimodal Self-Supervised Learning for Medical Images",
    "url": "https://www.alphaxiv.org/abs/2510.23325v1",
    "arxiv_id": "2510.23325v1",
    "authors": "Cristian Simionescu",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-27 13:42:16",
    "ori_summary": "This thesis works to address a pivotal challenge in medical image analysis: the reliance on extensive labeled datasets, which are often limited due to the need for expert annotation and constrained by privacy and legal issues. By focusing on the development of self-supervised learning techniques and domain adaptation methods, this research aims to circumvent these limitations, presenting a novel approach to enhance the utility and efficacy of deep learning in medical imaging. Central to this thesis is the development of the Medformer, an innovative neural network architecture designed for multitask learning and deep domain adaptation. This model is adept at pre-training on diverse medical image datasets, handling varying sizes and modalities, and is equipped with a dynamic input-output adaptation mechanism. This enables efficient processing and integration of a wide range of medical image types, from 2D X-rays to complex 3D MRIs, thus mitigating the dependency on large labeled datasets. Further, the thesis explores the current state of self-supervised learning in medical imaging. It introduces novel pretext tasks that are capable of extracting meaningful information from unlabeled data, significantly advancing the model's interpretative abilities. This approach is validated through rigorous experimentation, including the use of the MedMNIST dataset, demonstrating the model's proficiency in learning generalized features applicable to various downstream tasks. In summary, this thesis contributes to the advancement of medical image analysis by offering a scalable, adaptable framework that reduces reliance on labeled data. It paves the way for more accurate, efficient diagnostic tools in healthcare, signifying a major step forward in the application of deep learning in medical imaging.",
    "summary": "",
    "translation": "医学图像的多任务多模态自监督学习",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像领域，属于明确的无关主题范畴。虽然涉及多模态学习，但其特定于医学应用，与推荐系统、搜索或广告领域没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23306v1": {
    "title": "ReconViaGen: Towards Accurate Multi-view 3D Object Reconstruction via Generation",
    "url": "https://www.alphaxiv.org/abs/2510.23306v1",
    "arxiv_id": "2510.23306v1",
    "authors": "Jiahao Chang, Chongjie Ye, Yushuang Wu, Yuantao Chen, Yidan Zhang, Zhongjin Luo, Chenghong Li, Yihao Zhi, Xiaoguang Han",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-27 13:15:06",
    "ori_summary": "Existing multi-view 3D object reconstruction methods heavily rely on sufficient overlap between input views, where occlusions and sparse coverage in practice frequently yield severe reconstruction incompleteness. Recent advancements in diffusion-based 3D generative techniques offer the potential to address these limitations by leveraging learned generative priors to hallucinate invisible parts of objects, thereby generating plausible 3D structures. However, the stochastic nature of the inference process limits the accuracy and reliability of generation results, preventing existing reconstruction frameworks from integrating such 3D generative priors. In this work, we comprehensively analyze the reasons why diffusion-based 3D generative methods fail to achieve high consistency, including (a) the insufficiency in constructing and leveraging cross-view connections when extracting multi-view image features as conditions, and (b) the poor controllability of iterative denoising during local detail generation, which easily leads to plausible but inconsistent fine geometric and texture details with inputs. Accordingly, we propose ReconViaGen to innovatively integrate reconstruction priors into the generative framework and devise several strategies that effectively address these issues. Extensive experiments demonstrate that our ReconViaGen can reconstruct complete and accurate 3D models consistent with input views in both global structure and local details.Project page: https://jiahao620.github.io/reconviagen.",
    "summary": "",
    "translation": "ReconViaGen：通过生成实现精确的多视角3D物体重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的3D物体重建技术，属于纯粹的视觉研究方向。虽然提到了生成方法，但其核心是3D几何重建而非推荐系统、搜索或广告相关的技术。该工作没有展示与异构数据处理、序列建模或推荐系统应用的明显关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23301v1": {
    "title": "MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification",
    "url": "https://www.alphaxiv.org/abs/2510.23301v1",
    "arxiv_id": "2510.23301v1",
    "authors": "Yingying Feng, Jie Li, Jie Hu, Yukang Zhang, Lei Tan, Jiayi Ji",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 13:08:46",
    "ori_summary": "Real-world object re-identification (ReID) systems often face modality inconsistencies, where query and gallery images come from different sensors (e.g., RGB, NIR, TIR). However, most existing methods assume modality-matched conditions, which limits their robustness and scalability in practical applications. To address this challenge, we propose MDReID, a flexible any-to-any image-level ReID framework designed to operate under both modality-matched and modality-mismatched scenarios. MDReID builds on the insight that modality information can be decomposed into two components: modality-shared features that are predictable and transferable, and modality-specific features that capture unique, modality-dependent characteristics. To effectively leverage this, MDReID introduces two key components: the Modality Decoupling Learning (MDL) and Modality-aware Metric Learning (MML). Specifically, MDL explicitly decomposes modality features into modality-shared and modality-specific representations, enabling effective retrieval in both modality-aligned and mismatched scenarios. MML, a tailored metric learning strategy, further enforces orthogonality and complementarity between the two components to enhance discriminative power across modalities. Extensive experiments conducted on three challenging multi-modality ReID benchmarks (RGBNT201, RGBNT100, MSVR310) consistently demonstrate the superiority of MDReID. Notably, MDReID achieves significant mAP improvements of 9.8\\%, 3.0\\%, and 11.5\\% in general modality-matched scenarios, and average gains of 3.4\\%, 11.8\\%, and 10.9\\% in modality-mismatched scenarios, respectively. The code is available at: \\textcolor{magenta}{https://github.com/stone96123/MDReID}.",
    "summary": "",
    "translation": "MDReID：面向任意到任意多模态对象重识别的模态解耦学习",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉中的多模态对象重识别任务，虽然涉及多模态学习，但其核心应用领域是视觉对象识别而非推荐系统、搜索或广告。模态解耦学习技术理论上可能启发推荐系统中处理异构用户行为数据的方法，但这种关联性较弱且间接。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23299v1": {
    "title": "MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm Detection",
    "url": "https://www.alphaxiv.org/abs/2510.23299v1",
    "arxiv_id": "2510.23299v1",
    "authors": "Haochen Zhao, Yuyao Kong, Yongxiu Xu, Gaopeng Gou, Hongbo Xu, Yubin Wang, Haoliang Zhang",
    "categories": "cs.CV, cs.MM",
    "pub_date": "2025-10-27 13:05:27",
    "ori_summary": "Despite progress in multimodal sarcasm detection, existing datasets and methods predominantly focus on single-image scenarios, overlooking potential semantic and affective relations across multiple images. This leaves a gap in modeling cases where sarcasm is triggered by multi-image cues in real-world settings. To bridge this gap, we introduce MMSD3.0, a new benchmark composed entirely of multi-image samples curated from tweets and Amazon reviews. We further propose the Cross-Image Reasoning Model (CIRM), which performs targeted cross-image sequence modeling to capture latent inter-image connections. In addition, we introduce a relevance-guided, fine-grained cross-modal fusion mechanism based on text-image correspondence to reduce information loss during integration. We establish a comprehensive suite of strong and representative baselines and conduct extensive experiments, showing that MMSD3.0 is an effective and reliable benchmark that better reflects real-world conditions. Moreover, CIRM demonstrates state-of-the-art performance across MMSD, MMSD2.0 and MMSD3.0, validating its effectiveness in both single-image and multi-image scenarios.",
    "summary": "",
    "translation": "MMSD3.0：面向真实世界多模态讽刺检测的多图像基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于多模态讽刺检测基准构建，属于情感分析和内容理解领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然涉及多模态数据处理，但其应用场景（讽刺检测）与RecSys/Search/Ads的排序、匹配或用户意图建模需求相距甚远，且未体现对Transformer架构改进或LLM技术应用的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23285v1": {
    "title": "Adaptive Stochastic Coefficients for Accelerating Diffusion Sampling",
    "url": "https://www.alphaxiv.org/abs/2510.23285v1",
    "arxiv_id": "2510.23285v1",
    "authors": "Ruoyu Wang, Beier Zhu, Junzhi Li, Liangyu Yuan, Chi Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 12:53:48",
    "ori_summary": "Diffusion-based generative processes, formulated as differential equation solving, frequently balance computational speed with sample quality. Our theoretical investigation of ODE- and SDE-based solvers reveals complementary weaknesses: ODE solvers accumulate irreducible gradient error along deterministic trajectories, while SDE methods suffer from amplified discretization errors when the step budget is limited. Building upon this insight, we introduce AdaSDE, a novel single-step SDE solver that aims to unify the efficiency of ODEs with the error resilience of SDEs. Specifically, we introduce a single per-step learnable coefficient, estimated via lightweight distillation, which dynamically regulates the error correction strength to accelerate diffusion sampling. Notably, our framework can be integrated with existing solvers to enhance their capabilities. Extensive experiments demonstrate state-of-the-art performance: at 5 NFE, AdaSDE achieves FID scores of 4.18 on CIFAR-10, 8.05 on FFHQ and 6.96 on LSUN Bedroom. Codes are available in https://github.com/WLU-wry02/AdaSDE.",
    "summary": "",
    "translation": "用于加速扩散采样的自适应随机系数",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于扩散模型的采样加速技术，属于生成式AI领域。虽然扩散模型在AIGC和内容生成中有应用，但根据用户明确的排除标准，这属于'AIGC, Content generation, Summarization, or other purely LLM-centric topics'的无关主题，与推荐系统、搜索或广告的核心技术没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23278v1": {
    "title": "hYOLO Model: Enhancing Object Classification with Hierarchical Context in YOLOv8",
    "url": "https://www.alphaxiv.org/abs/2510.23278v1",
    "arxiv_id": "2510.23278v1",
    "authors": "Veska Tsenkova, Peter Stanchev, Daniel Petrov, Deyan Lazarov",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 12:39:50",
    "ori_summary": "Current convolution neural network (CNN) classification methods are predominantly focused on flat classification which aims solely to identify a specified object within an image. However, real-world objects often possess a natural hierarchical organization that can significantly help classification tasks. Capturing the presence of relations between objects enables better contextual understanding as well as control over the severity of mistakes. Considering these aspects, this paper proposes an end-to-end hierarchical model for image detection and classification built upon the YOLO model family. A novel hierarchical architecture, a modified loss function, and a performance metric tailored to the hierarchical nature of the model are introduced. The proposed model is trained and evaluated on two different hierarchical categorizations of the same dataset: a systematic categorization that disregards visual similarities between objects and a categorization accounting for common visual characteristics across classes. The results illustrate how the suggested methodology addresses the inherent hierarchical structure present in real-world objects, which conventional flat classification algorithms often overlook.",
    "summary": "",
    "translation": "hYOLO模型：在YOLOv8中通过层次化上下文增强目标分类",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的目标检测和分类，属于纯粹的视觉技术领域。虽然提到了层次化上下文的概念，但这仅限于视觉对象识别，没有展示与推荐系统、搜索或广告相关的潜在应用或技术迁移可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23253v1": {
    "title": "A Video Is Not Worth a Thousand Words",
    "url": "https://www.alphaxiv.org/abs/2510.23253v1",
    "arxiv_id": "2510.23253v1",
    "authors": "Sam Pollard, Michael Wray",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 12:15:02",
    "ori_summary": "As we become increasingly dependent on vision language models (VLMs) to answer questions about the world around us, there is a significant amount of research devoted to increasing both the difficulty of video question answering (VQA) datasets, and the context lengths of the models that they evaluate. The reliance on large language models as backbones has lead to concerns about potential text dominance, and the exploration of interactions between modalities is underdeveloped. How do we measure whether we're heading in the right direction, with the complexity that multi-modal models introduce? We propose a joint method of computing both feature attributions and modality scores based on Shapley values, where both the features and modalities are arbitrarily definable. Using these metrics, we compare $6$ VLM models of varying context lengths on $4$ representative datasets, focusing on multiple-choice VQA. In particular, we consider video frames and whole textual elements as equal features in the hierarchy, and the multiple-choice VQA task as an interaction between three modalities: video, question and answer. Our results demonstrate a dependence on text and show that the multiple-choice VQA task devolves into a model's ability to ignore distractors. Code available at https://github.com/sjpollard/a-video-is-not-worth-a-thousand-words.",
    "summary": "",
    "translation": "视频并非千言万语",
    "relevance_score": 2,
    "reasoning": "该标题暗示对视频内容价值的质疑，可能涉及多模态表示学习或模态间信息差异分析。虽然这与VLM处理异质数据的理念有微弱关联，但缺乏明确的RecSys/Search/Ads应用场景，且更偏向纯粹的视觉-语言模态研究而非推荐系统核心问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23241v1": {
    "title": "Progressive Growing of Patch Size: Curriculum Learning for Accelerated and Improved Medical Image Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.23241v1",
    "arxiv_id": "2510.23241v1",
    "authors": "Stefan M. Fischer, Johannes Kiechle, Laura Daza, Lina Felsner, Richard Osuala, Daniel M. Lang, Karim Lekadir, Jan C. Peeken, Julia A. Schnabel",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-27 11:55:12",
    "ori_summary": "In this work, we introduce Progressive Growing of Patch Size, an automatic curriculum learning approach for 3D medical image segmentation. Our approach progressively increases the patch size during model training, resulting in an improved class balance for smaller patch sizes and accelerated convergence of the training process. We evaluate our curriculum approach in two settings: a resource-efficient mode and a performance mode, both regarding Dice score performance and computational costs across 15 diverse and popular 3D medical image segmentation tasks. The resource-efficient mode matches the Dice score performance of the conventional constant patch size sampling baseline with a notable reduction in training time to only 44%. The performance mode improves upon constant patch size segmentation results, achieving a statistically significant relative mean performance gain of 1.28% in Dice Score. Remarkably, across all 15 tasks, our proposed performance mode manages to surpass the constant patch size baseline in Dice Score performance, while simultaneously reducing training time to only 89%. The benefits are particularly pronounced for highly imbalanced tasks such as lesion segmentation tasks. Rigorous experiments demonstrate that our performance mode not only improves mean segmentation performance but also reduces performance variance, yielding more trustworthy model comparison. Furthermore, our findings reveal that the proposed curriculum sampling is not tied to a specific architecture but represents a broadly applicable strategy that consistently boosts performance across diverse segmentation models, including UNet, UNETR, and SwinUNETR. In summary, we show that this simple yet elegant transformation on input data substantially improves both Dice Score performance and training runtime, while being compatible across diverse segmentation backbones.",
    "summary": "",
    "translation": "补丁尺寸的渐进式增长：用于加速和改善医学图像分割的课程学习",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于医学图像分割，这是一个与我的关注领域无关的特定领域应用。虽然它提到了课程学习这一通用技术，但该方法专门针对医学图像处理，在推荐系统、搜索或广告方面没有明显的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23240v1": {
    "title": "Autoregressive Styled Text Image Generation, but Make it Reliable",
    "url": "https://www.alphaxiv.org/abs/2510.23240v1",
    "arxiv_id": "2510.23240v1",
    "authors": "Carmine Zaccagnino, Fabio Quattrini, Vittorio Pippi, Silvia Cascianelli, Alessio Tonioni, Rita Cucchiara",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 11:54:23",
    "ori_summary": "Generating faithful and readable styled text images (especially for Styled Handwritten Text generation - HTG) is an open problem with several possible applications across graphic design, document understanding, and image editing. A lot of research effort in this task is dedicated to developing strategies that reproduce the stylistic characteristics of a given writer, with promising results in terms of style fidelity and generalization achieved by the recently proposed Autoregressive Transformer paradigm for HTG. However, this method requires additional inputs, lacks a proper stop mechanism, and might end up in repetition loops, generating visual artifacts. In this work, we rethink the autoregressive formulation by framing HTG as a multimodal prompt-conditioned generation task, and tackle the content controllability issues by introducing special textual input tokens for better alignment with the visual ones. Moreover, we devise a Classifier-Free-Guidance-based strategy for our autoregressive model. Through extensive experimental validation, we demonstrate that our approach, dubbed Eruku, compared to previous solutions requires fewer inputs, generalizes better to unseen styles, and follows more faithfully the textual prompt, improving content adherence.",
    "summary": "",
    "translation": "自回归风格化文本图像生成，但确保其可靠性",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本到图像的生成任务，属于纯粹的AIGC和内容生成领域，与推荐系统、搜索或广告的核心技术无关。虽然涉及生成模型，但缺乏与排名、用户行为建模或异构数据处理等RecSys/Search/Ads核心问题的直接联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23225v1": {
    "title": "Through the Lens: Benchmarking Deepfake Detectors Against Moiré-Induced Distortions",
    "url": "https://www.alphaxiv.org/abs/2510.23225v1",
    "arxiv_id": "2510.23225v1",
    "authors": "Razaib Tariq, Minji Heo, Simon S. Woo, Shahroz Tariq",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 11:23:04",
    "ori_summary": "Deepfake detection remains a pressing challenge, particularly in real-world settings where smartphone-captured media from digital screens often introduces Moir\\'e artifacts that can distort detection outcomes. This study systematically evaluates state-of-the-art (SOTA) deepfake detectors on Moir\\'e-affected videos, an issue that has received little attention. We collected a dataset of 12,832 videos, spanning 35.64 hours, from the Celeb-DF, DFD, DFDC, UADFV, and FF++ datasets, capturing footage under diverse real-world conditions, including varying screens, smartphones, lighting setups, and camera angles. To further examine the influence of Moir\\'e patterns on deepfake detection, we conducted additional experiments using our DeepMoir\\'eFake, referred to as (DMF) dataset and two synthetic Moir\\'e generation techniques. Across 15 top-performing detectors, our results show that Moir\\'e artifacts degrade performance by as much as 25.4%, while synthetically generated Moir\\'e patterns lead to a 21.4% drop in accuracy. Surprisingly, demoir\\'eing methods, intended as a mitigation approach, instead worsened the problem, reducing accuracy by up to 17.2%. These findings underscore the urgent need for detection models that can robustly handle Moir\\'e distortions alongside other realworld challenges, such as compression, sharpening, and blurring. By introducing the DMF dataset, we aim to drive future research toward closing the gap between controlled experiments and practical deepfake detection.",
    "summary": "",
    "translation": "透过镜头：针对莫尔条纹诱导失真的深度伪造检测器基准测试",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于深度伪造检测和莫尔条纹失真，属于计算机视觉安全领域。这与我的关注点（推荐系统、搜索、广告及相关的LLM/Transformer技术）完全无关，不涉及任何推荐算法、用户建模或内容排序的核心技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23205v1": {
    "title": "VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D Gaussian Splatting",
    "url": "https://www.alphaxiv.org/abs/2510.23205v1",
    "arxiv_id": "2510.23205v1",
    "authors": "Hoonhee Cho, Jae-Young Kang, Giwon Lee, Hyemin Yang, Heejun Park, Seokwoo Jung, Kuk-Jin Yoon",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 10:49:39",
    "ori_summary": "End-to-end autonomous driving (E2E-AD) has emerged as a promising paradigm that unifies perception, prediction, and planning into a holistic, data-driven framework. However, achieving robustness to varying camera viewpoints, a common real-world challenge due to diverse vehicle configurations, remains an open problem. In this work, we propose VR-Drive, a novel E2E-AD framework that addresses viewpoint generalization by jointly learning 3D scene reconstruction as an auxiliary task to enable planning-aware view synthesis. Unlike prior scene-specific synthesis approaches, VR-Drive adopts a feed-forward inference strategy that supports online training-time augmentation from sparse views without additional annotations. To further improve viewpoint consistency, we introduce a viewpoint-mixed memory bank that facilitates temporal interaction across multiple viewpoints and a viewpoint-consistent distillation strategy that transfers knowledge from original to synthesized views. Trained in a fully end-to-end manner, VR-Drive effectively mitigates synthesis-induced noise and improves planning under viewpoint shifts. In addition, we release a new benchmark dataset to evaluate E2E-AD performance under novel camera viewpoints, enabling comprehensive analysis. Our results demonstrate that VR-Drive is a scalable and robust solution for the real-world deployment of end-to-end autonomous driving systems.",
    "summary": "",
    "translation": "VR-Drive：基于前馈3D高斯泼溅的视点鲁棒端到端驾驶",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域，涉及3D场景重建和视点鲁棒性技术，属于纯粹的计算机视觉应用。虽然3D高斯泼溅是先进的3D表示技术，但其在推荐系统、搜索或广告领域的潜在应用并不明确，与我的关注焦点完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23203v1": {
    "title": "DecoDINO: 3D Human-Scene Contact Prediction with Semantic Classification",
    "url": "https://www.alphaxiv.org/abs/2510.23203v1",
    "arxiv_id": "2510.23203v1",
    "authors": "Lukas Bierling, Davide Pasero, Fleur Dolmans, Helia Ghasemi, Angelo Broere",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 10:46:22",
    "ori_summary": "Accurate vertex-level contact prediction between humans and surrounding objects is a prerequisite for high fidelity human object interaction models used in robotics, AR/VR, and behavioral simulation. DECO was the first in the wild estimator for this task but is limited to binary contact maps and struggles with soft surfaces, occlusions, children, and false-positive foot contacts. We address these issues and introduce DecoDINO, a three-branch network based on DECO's framework. It uses two DINOv2 ViT-g/14 encoders, class-balanced loss weighting to reduce bias, and patch-level cross-attention for improved local reasoning. Vertex features are finally passed through a lightweight MLP with a softmax to assign semantic contact labels. We also tested a vision-language model (VLM) to integrate text features, but the simpler architecture performed better and was used instead. On the DAMON benchmark, DecoDINO (i) raises the binary-contact F1 score by 7$\\%$, (ii) halves the geodesic error, and (iii) augments predictions with object-level semantic labels. Ablation studies show that LoRA fine-tuning and the dual encoders are key to these improvements. DecoDINO outperformed the challenge baseline in both tasks of the DAMON Challenge. Our code is available at https://github.com/DavidePasero/deco/tree/main.",
    "summary": "",
    "translation": "DecoDINO：基于语义分类的3D人-场景接触预测",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D视觉中的人-场景接触预测和语义分类，属于纯粹的计算机视觉领域研究。虽然涉及3D场景理解，但缺乏与推荐系统、搜索或广告领域的直接关联，也没有展示出在异构数据统一建模方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23190v1": {
    "title": "Evaluation of Vision-LLMs in Surveillance Video",
    "url": "https://www.alphaxiv.org/abs/2510.23190v1",
    "arxiv_id": "2510.23190v1",
    "authors": "Pascal Benschop, Cristian Meo, Justin Dauwels, Jelte P. Mense",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 10:27:02",
    "ori_summary": "The widespread use of cameras in our society has created an overwhelming amount of video data, far exceeding the capacity for human monitoring. This presents a critical challenge for public safety and security, as the timely detection of anomalous or criminal events is crucial for effective response and prevention. The ability for an embodied agent to recognize unexpected events is fundamentally tied to its capacity for spatial reasoning. This paper investigates the spatial reasoning of vision-language models (VLMs) by framing anomalous action recognition as a zero-shot, language-grounded task, addressing the embodied perception challenge of interpreting dynamic 3D scenes from sparse 2D video. Specifically, we investigate whether small, pre-trained vision--LLMs can act as spatially-grounded, zero-shot anomaly detectors by converting video into text descriptions and scoring labels via textual entailment. We evaluate four open models on UCF-Crime and RWF-2000 under prompting and privacy-preserving conditions. Few-shot exemplars can improve accuracy for some models, but may increase false positives, and privacy filters -- especially full-body GAN transforms -- introduce inconsistencies that degrade accuracy. These results chart where current vision--LLMs succeed (simple, spatially salient events) and where they falter (noisy spatial cues, identity obfuscation). Looking forward, we outline concrete paths to strengthen spatial grounding without task-specific training: structure-aware prompts, lightweight spatial memory across clips, scene-graph or 3D-pose priors during description, and privacy methods that preserve action-relevant geometry. This positions zero-shot, language-grounded pipelines as adaptable building blocks for embodied, real-world video understanding. Our implementation for evaluating VLMs is publicly available at: https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition",
    "summary": "",
    "translation": "视觉大语言模型在监控视频中的评估",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉大语言模型在监控视频领域的评估，这属于纯粹的视觉应用范畴，与推荐系统、搜索或广告没有直接关联。虽然标题提到了LLMs，但应用场景是监控视频分析，属于明确的无关主题，无法看出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23184v1": {
    "title": "Finding 3D Scene Analogies with Multimodal Foundation Models",
    "url": "https://www.alphaxiv.org/abs/2510.23184v1",
    "arxiv_id": "2510.23184v1",
    "authors": "Junho Kim, Young Min Kim",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 10:23:31",
    "ori_summary": "Connecting current observations with prior experiences helps robots adapt and plan in new, unseen 3D environments. Recently, 3D scene analogies have been proposed to connect two 3D scenes, which are smooth maps that align scene regions with common spatial relationships. These maps enable detailed transfer of trajectories or waypoints, potentially supporting demonstration transfer for imitation learning or task plan transfer across scenes. However, existing methods for the task require additional training and fixed object vocabularies. In this work, we propose to use multimodal foundation models for finding 3D scene analogies in a zero-shot, open-vocabulary setting. Central to our approach is a hybrid neural representation of scenes that consists of a sparse graph based on vision-language model features and a feature field derived from 3D shape foundation models. 3D scene analogies are then found in a coarse-to-fine manner, by first aligning the graph and refining the correspondence with feature fields. Our method can establish accurate correspondences between complex scenes, and we showcase applications in trajectory and waypoint transfer.",
    "summary": "",
    "translation": "利用多模态基础模型寻找3D场景类比",
    "relevance_score": 2,
    "reasoning": "该论文主要关注3D场景理解和多模态基础模型，这属于纯粹的视觉和3D视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然多模态建模的概念在理论上可以类比到异构数据处理，但论文的具体应用场景（3D场景）和我的关注领域相距甚远，缺乏明确的实用关联性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23151v1": {
    "title": "AG-Fusion: adaptive gated multimodal fusion for 3d object detection in complex scenes",
    "url": "https://www.alphaxiv.org/abs/2510.23151v1",
    "arxiv_id": "2510.23151v1",
    "authors": "Sixian Liu, Chen Xu, Qiang Wang, Donghai Shi, Yiwen Li",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-27 09:26:27",
    "ori_summary": "Multimodal camera-LiDAR fusion technology has found extensive application in 3D object detection, demonstrating encouraging performance. However, existing methods exhibit significant performance degradation in challenging scenarios characterized by sensor degradation or environmental disturbances. We propose a novel Adaptive Gated Fusion (AG-Fusion) approach that selectively integrates cross-modal knowledge by identifying reliable patterns for robust detection in complex scenes. Specifically, we first project features from each modality into a unified BEV space and enhance them using a window-based attention mechanism. Subsequently, an adaptive gated fusion module based on cross-modal attention is designed to integrate these features into reliable BEV representations robust to challenging environments. Furthermore, we construct a new dataset named Excavator3D (E3D) focusing on challenging excavator operation scenarios to benchmark performance in complex conditions. Our method not only achieves competitive performance on the standard KITTI dataset with 93.92% accuracy, but also significantly outperforms the baseline by 24.88% on the challenging E3D dataset, demonstrating superior robustness to unreliable modal information in complex industrial scenes.",
    "summary": "",
    "translation": "AG-Fusion：面向复杂场景中3D目标检测的自适应门控多模态融合",
    "relevance_score": 2,
    "reasoning": "该论文专注于3D目标检测和计算机视觉中的多模态融合技术，属于纯粹的视觉领域研究。虽然门控融合机制在概念上可能与推荐系统中的特征融合有相似之处，但论文明确针对3D视觉任务，没有提供任何与推荐系统、搜索或广告相关的潜在应用连接。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23145v1": {
    "title": "Implicit Modeling for Transferability Estimation of Vision Foundation Models",
    "url": "https://www.alphaxiv.org/abs/2510.23145v1",
    "arxiv_id": "2510.23145v1",
    "authors": "Yaoyan Zheng, Huiqun Wang, Nan Zhou, Di Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 09:21:19",
    "ori_summary": "Transferability estimation identifies the best pre-trained models for downstream tasks without incurring the high computational cost of full fine-tuning. This capability facilitates deployment and advances the pre-training and fine-tuning paradigm. However, existing methods often struggle to accurately assess transferability for emerging pre-trained models with diverse architectures, training strategies, and task alignments. In this work, we propose Implicit Transferability Modeling (ITM), a novel framework that implicitly models each model's intrinsic transferability, coupled with a Divide-and-Conquer Variational Approximation (DVA) strategy to efficiently approximate embedding space evolution. This design enables generalization across a broader range of models and downstream tasks. Extensive experiments on a comprehensive benchmark--spanning extensive training regimes and a wider variety of model types--demonstrate that ITM consistently outperforms existing methods in terms of stability, effectiveness, and efficiency.",
    "summary": "",
    "translation": "视觉基础模型可迁移性估计的隐式建模",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉基础模型的可迁移性估计，属于纯粹的计算机视觉领域研究。虽然基础模型的概念在推荐和搜索系统中也有应用，但该论文明确聚焦于视觉模型，且没有提及任何与推荐系统、搜索或广告相关的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23144v1": {
    "title": "DQ3D: Depth-guided Query for Transformer-Based 3D Object Detection in Traffic Scenarios",
    "url": "https://www.alphaxiv.org/abs/2510.23144v1",
    "arxiv_id": "2510.23144v1",
    "authors": "Ziyu Wang, Wenhao Li, Ji Wu",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 09:20:59",
    "ori_summary": "3D object detection from multi-view images in traffic scenarios has garnered significant attention in recent years. Many existing approaches rely on object queries that are generated from 3D reference points to localize objects. However, a limitation of these methods is that some reference points are often far from the target object, which can lead to false positive detections. In this paper, we propose a depth-guided query generator for 3D object detection (DQ3D) that leverages depth information and 2D detections to ensure that reference points are sampled from the surface or interior of the object. Furthermore, to address partially occluded objects in current frame, we introduce a hybrid attention mechanism that fuses historical detection results with depth-guided queries, thereby forming hybrid queries. Evaluation on the nuScenes dataset demonstrates that our method outperforms the baseline by 6.3\\% in terms of mean Average Precision (mAP) and 4.3\\% in the NuScenes Detection Score (NDS).",
    "summary": "",
    "translation": "DQ3D：基于Transformer的交通场景3D目标检测中的深度引导查询",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D视觉和自动驾驶领域的特定应用，与推荐系统、搜索或广告的核心技术没有直接关联。虽然涉及Transformer架构，但其应用场景（交通场景3D目标检测）过于特定，无法为RecSys/Search/Ads领域提供可转移的技术洞察或潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23140v1": {
    "title": "Fast Voxel-Wise Kinetic Modeling in Dynamic PET using a Physics-Informed CycleGAN",
    "url": "https://www.alphaxiv.org/abs/2510.23140v1",
    "arxiv_id": "2510.23140v1",
    "authors": "Christian Salomonsen, Samuel Kuttner, Michael Kampffmeyer, Robert Jenssen, Kristoffer Wickstrøm, Jong Chul Ye, Elisabeth Wetzer",
    "categories": "cs.CV, q-bio.OT",
    "pub_date": "2025-10-27 09:17:02",
    "ori_summary": "Tracer kinetic modeling serves a vital role in diagnosis, treatment planning, tracer development and oncology, but burdens practitioners with complex and invasive arterial input function estimation (AIF). We adopt a physics-informed CycleGAN showing promise in DCE-MRI quantification to dynamic PET quantification. Our experiments demonstrate sound AIF predictions and parameter maps closely resembling the reference.",
    "summary": "",
    "translation": "基于物理信息循环生成对抗网络的动态PET快速体素级动力学建模",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像领域（动态PET）的物理建模和生成对抗网络应用，属于医学影像处理范畴。虽然涉及生成模型技术，但其应用场景和问题领域与推荐系统、搜索或广告的核心技术栈完全无关，没有任何潜在的应用关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23137v1": {
    "title": "Note on the Construction of Structure Tensor",
    "url": "https://www.alphaxiv.org/abs/2510.23137v1",
    "arxiv_id": "2510.23137v1",
    "authors": "Josef Bigun, Fernado Alonso-Fernandez",
    "categories": "cs.CV, math.SP",
    "pub_date": "2025-10-27 09:16:34",
    "ori_summary": "This note presents a theoretical discussion of two structure tensor constructions: one proposed by Bigun and Granlund 1987, and the other by Granlund and Knutsson 1995. At first glance, these approaches may appear quite different--the former is implemented by averaging outer products of gradient filter responses, while the latter constructs the tensor from weighted outer products of tune-in frequency vectors of quadrature filters. We argue that when both constructions are viewed through the common lens of Total Least Squares (TLS) line fitting to the power spectrum, they can be reconciled to a large extent, and additional benefits emerge. From this perspective, the correction term introduced in Granlund and Knutsson 1995 becomes unnecessary. Omitting it ensures that the resulting tensor remains positive semi-definite, thereby simplifying the interpretation of its eigenvalues. Furthermore, this interpretation allows fitting more than a single 0rientation to the input by reinterpreting quadrature filter responses without relying on a structure tensor. It also removes the constraint that responses must originate strictly from quadrature filters, allowing the use of alternative filter types and non-angular tessellations. These alternatives include Gabor filters--which, although not strictly quadrature, are still suitable for structure tensor construction--even when they tessellate the spectrum in a Cartesian fashion, provided they are sufficiently concentrated.",
    "summary": "",
    "translation": "关于结构张量构造的注记",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及结构张量的数学构造，属于纯粹的数学或信号处理领域，与推荐系统、搜索、广告或LLM技术没有任何直接关联。结构张量主要用于图像处理和计算机视觉中的纹理分析、边缘检测等任务，完全超出了您关注的领域范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23124v1": {
    "title": "DeepSalt: Bridging Laboratory and Satellite Spectra through Domain Adaptation and Knowledge Distillation for Large-Scale Soil Salinity Estimation",
    "url": "https://www.alphaxiv.org/abs/2510.23124v1",
    "arxiv_id": "2510.23124v1",
    "authors": "Rupasree Dey, Abdul Matin, Everett Lewark, Tanjim Bin Faruk, Andrei Bachinin, Sam Leuthold, M. Francesca Cotrufo, Shrideep Pallickara, Sangmi Lee Pallickara",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-27 08:57:59",
    "ori_summary": "Soil salinization poses a significant threat to both ecosystems and agriculture because it limits plants' ability to absorb water and, in doing so, reduces crop productivity. This phenomenon alters the soil's spectral properties, creating a measurable relationship between salinity and light reflectance that enables remote monitoring. While laboratory spectroscopy provides precise measurements, its reliance on in-situ sampling limits scalability to regional or global levels. Conversely, hyperspectral satellite imagery enables wide-area observation but lacks the fine-grained interpretability of laboratory instruments. To bridge this gap, we introduce DeepSalt, a deep-learning-based spectral transfer framework that leverages knowledge distillation and a novel Spectral Adaptation Unit to transfer high-resolution spectral insights from laboratory-based spectroscopy to satellite-based hyperspectral sensing. Our approach eliminates the need for extensive ground sampling while enabling accurate, large-scale salinity estimation, as demonstrated through comprehensive empirical benchmarks. DeepSalt achieves significant performance gains over methods without explicit domain adaptation, underscoring the impact of the proposed Spectral Adaptation Unit and the knowledge distillation strategy. The model also effectively generalized to unseen geographic regions, explaining a substantial portion of the salinity variance.",
    "summary": "",
    "translation": "DeepSalt：通过领域自适应和知识蒸馏连接实验室与卫星光谱，实现大规模土壤盐度估算",
    "relevance_score": 1,
    "reasoning": "该论文专注于遥感领域的土壤盐度估算，属于地球科学和农业应用范畴。其技术方法（领域自适应、知识蒸馏）虽然通用，但论文的应用场景与推荐系统、搜索或广告领域完全无关，没有任何潜在的应用关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23118v1": {
    "title": "Task-Agnostic Fusion of Time Series and Imagery for Earth Observation",
    "url": "https://www.alphaxiv.org/abs/2510.23118v1",
    "arxiv_id": "2510.23118v1",
    "authors": "Gianfranco Basile, Johannes Jakubik, Benedikt Blumenstiel, Thomas Brunschwiler, Juan Bernabe Moreno",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 08:38:52",
    "ori_summary": "We propose a task-agnostic framework for multimodal fusion of time series and single timestamp images, enabling cross-modal generation and robust downstream performance. Our approach explores deterministic and learned strategies for time series quantization and then leverages a masked correlation learning objective, aligning discrete image and time series tokens in a unified representation space. Instantiated in the Earth observation domain, the pretrained model generates consistent global temperature profiles from satellite imagery and is validated through counterfactual experiments. Across downstream tasks, our task-agnostic pretraining outperforms task-specific fusion by 6\\% in R$^2$ and 2\\% in RMSE on average, and exceeds baseline methods by 50\\% in R$^2$ and 12\\% in RMSE. Finally, we analyze gradient sensitivity across modalities, providing insights into model robustness. Code, data, and weights will be released under a permissive license.",
    "summary": "",
    "translation": "面向地球观测任务无关的时间序列与图像数据融合",
    "relevance_score": 2,
    "reasoning": "该论文专注于地球观测领域的多模态融合技术，虽然涉及时间序列和图像数据的融合方法，但其应用场景局限于地球观测这一特定领域。论文标题未表明该方法在推荐系统、搜索或广告中有直接应用潜力，也不涉及Transformer架构改进或LLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23117v1": {
    "title": "Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction",
    "url": "https://www.alphaxiv.org/abs/2510.23117v1",
    "arxiv_id": "2510.23117v1",
    "authors": "Omer Jauhar Khan, Sudais Khan, Hafeez Anwar",
    "categories": "cs.LG, cs.CV, 65M70 (Primary), 68T07 (Secondary), I.2.6; I.4.8; G.1.8",
    "pub_date": "2025-10-27 08:38:17",
    "ori_summary": "Physics Informed Neural Networks (PINNs) are gaining attention for their ability to embed physical laws into deep learning models, which is particularly useful in structural engineering tasks with limited data. This paper aims to explore the use of PINNs to predict the weight of small scale spaghetti bridges, a task relevant to understanding load limits and potential failure modes in simplified structural models. Our proposed framework incorporates physics-based constraints to the prediction model for improved performance. In addition to standard PINNs, we introduce a novel architecture named Physics Informed Kolmogorov Arnold Network (PIKAN), which blends universal function approximation theory with physical insights. The structural parameters provided as input to the model are collected either manually or through computer vision methods. Our dataset includes 15 real bridges, augmented to 100 samples, and our best model achieves an $R^2$ score of 0.9603 and a mean absolute error (MAE) of 10.50 units. From applied perspective, we also provide a web based interface for parameter entry and prediction. These results show that PINNs can offer reliable estimates of structural weight, even with limited data, and may help inform early stage failure analysis in lightweight bridge designs. The complete data and code are available at https://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.",
    "summary": "",
    "translation": "预见结构失效：基于图像的物理信息神经网络（PINN）用于意大利面桥梁荷载预测",
    "relevance_score": 1,
    "reasoning": "该论文专注于物理信息神经网络在结构工程领域的应用，特别是意大利面桥梁的荷载预测。这与推荐系统、搜索或广告的核心领域进展、使能LLM技术、Transformer架构或异构数据建模完全无关。该研究属于纯粹的工程物理应用，没有任何潜在的应用于RecSys/Search/Ads的关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23116v1": {
    "title": "Residual Diffusion Bridge Model for Image Restoration",
    "url": "https://www.alphaxiv.org/abs/2510.23116v1",
    "arxiv_id": "2510.23116v1",
    "authors": "Hebaixu Wang, Jing Zhang, Haoyang Chen, Haonan Guo, Di Wang, Jiayi Ma, Bo Du",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 08:35:49",
    "ori_summary": "Diffusion bridge models establish probabilistic paths between arbitrary paired distributions and exhibit great potential for universal image restoration. Most existing methods merely treat them as simple variants of stochastic interpolants, lacking a unified analytical perspective. Besides, they indiscriminately reconstruct images through global noise injection and removal, inevitably distorting undegraded regions due to imperfect reconstruction. To address these challenges, we propose the Residual Diffusion Bridge Model (RDBM). Specifically, we theoretically reformulate the stochastic differential equations of generalized diffusion bridge and derive the analytical formulas of its forward and reverse processes. Crucially, we leverage the residuals from given distributions to modulate the noise injection and removal, enabling adaptive restoration of degraded regions while preserving intact others. Moreover, we unravel the fundamental mathematical essence of existing bridge models, all of which are special cases of RDBM and empirically demonstrate the optimality of our proposed models. Extensive experiments are conducted to demonstrate the state-of-the-art performance of our method both qualitatively and quantitatively across diverse image restoration tasks. Code is publicly available at https://github.com/MiliLab/RDBM.",
    "summary": "",
    "translation": "用于图像复原的残差扩散桥模型",
    "relevance_score": 2,
    "reasoning": "该论文专注于图像复原的扩散模型技术，属于计算机视觉领域的特定应用。虽然扩散模型是生成式AI的重要分支，但该工作没有明确展示在推荐系统、搜索或广告中的潜在应用价值，与当前关注的LLM技术、Transformer架构或异构数据建模等方向关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23095v1": {
    "title": "Revisiting Multimodal Positional Encoding in Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.23095v1",
    "arxiv_id": "2510.23095v1",
    "authors": "Jie Huang, Xuejing Liu, Sibo Song, Ruibing Hou, Hong Chang, Junyang Lin, Shuai Bai",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 08:00:46",
    "ori_summary": "Multimodal position encoding is essential for vision-language models, yet there has been little systematic investigation into multimodal position encoding. We conduct a comprehensive analysis of multimodal Rotary Positional Embedding (RoPE) by examining its two core components: position design and frequency allocation. Through extensive experiments, we identify three key guidelines: positional coherence, full frequency utilization, and preservation of textual priors-ensuring unambiguous layout, rich representation, and faithful transfer from the pre-trained LLM. Based on these insights, we propose Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and plug-and-play variants that require no architectural changes. Our methods consistently outperform existing approaches across diverse benchmarks, with significant improvements in both general and fine-grained multimodal understanding. Code will be avaliable at https://github.com/JJJYmmm/Multimodal-RoPEs.",
    "summary": "该论文研究多模态视觉语言模型中的位置编码问题，核心思想是通过分析位置设计和频率分配两个关键组件，提出保持位置一致性、充分利用频率和保留文本先验的指导原则，并开发了无需架构修改的即插即用RoPE变体。",
    "translation": "重新审视视觉语言模型中的多模态位置编码",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及VLM（视觉语言模型）中的多模态建模技术，这与'VLM类比用于异构数据'焦点高度相关。位置编码的改进可以应用于处理推荐系统中的用户行为序列和上下文特征等异构数据模态，实现更统一和有效的表示学习。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文系统研究多模态位置编码，提出新的RoPE变体，对处理异构数据的统一建模具有直接启发价值，符合VLM类比和Transformer架构优化的关注点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.23087v1": {
    "title": "EndoWave: Rational-Wavelet 4D Gaussian Splatting for Endoscopic Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2510.23087v1",
    "arxiv_id": "2510.23087v1",
    "authors": "Taoyu Wu, Yiyi Miao, Jiaxin Guo, Ziyan Chen, Sihang Zhao, Zhuoxiao Li, Zhe Tang, Baoru Huang, Limin Yu",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-27 07:45:17",
    "ori_summary": "In robot-assisted minimally invasive surgery, accurate 3D reconstruction from endoscopic video is vital for downstream tasks and improved outcomes. However, endoscopic scenarios present unique challenges, including photometric inconsistencies, non-rigid tissue motion, and view-dependent highlights. Most 3DGS-based methods that rely solely on appearance constraints for optimizing 3DGS are often insufficient in this context, as these dynamic visual artifacts can mislead the optimization process and lead to inaccurate reconstructions. To address these limitations, we present EndoWave, a unified spatio-temporal Gaussian Splatting framework by incorporating an optical flow-based geometric constraint and a multi-resolution rational wavelet supervision. First, we adopt a unified spatio-temporal Gaussian representation that directly optimizes primitives in a 4D domain. Second, we propose a geometric constraint derived from optical flow to enhance temporal coherence and effectively constrain the 3D structure of the scene. Third, we propose a multi-resolution rational orthogonal wavelet as a constraint, which can effectively separate the details of the endoscope and enhance the rendering performance. Extensive evaluations on two real surgical datasets, EndoNeRF and StereoMIS, demonstrate that our method EndoWave achieves state-of-the-art reconstruction quality and visual accuracy compared to the baseline method.",
    "summary": "",
    "translation": "EndoWave：用于内窥镜重建的有理小波四维高斯泼溅技术",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学内窥镜重建的计算机视觉技术，属于医疗领域的特定应用。其核心方法（4D高斯泼溅、小波变换）与推荐系统、搜索或广告领域没有直接关联，也不涉及LLM技术或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23079v1": {
    "title": "Strategies for Robust Deep Learning Based Deformable Registration",
    "url": "https://www.alphaxiv.org/abs/2510.23079v1",
    "arxiv_id": "2510.23079v1",
    "authors": "Joel Honkamaa, Pekka Marttinen",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 07:29:28",
    "ori_summary": "Deep learning based deformable registration methods have become popular in recent years. However, their ability to generalize beyond training data distribution can be poor, significantly hindering their usability. LUMIR brain registration challenge for Learn2Reg 2025 aims to advance the field by evaluating the performance of the registration on contrasts and modalities different from those included in the training set. Here we describe our submission to the challenge, which proposes a very simple idea for significantly improving robustness by transforming the images into MIND feature space before feeding them into the model. In addition, a special ensembling strategy is proposed that shows a small but consistent improvement.",
    "summary": "",
    "translation": "基于深度学习的可变形配准鲁棒性策略",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像配准中的可变形配准技术，属于计算机视觉在医疗领域的特定应用。这与搜索、推荐、广告系统的核心需求无关，也不涉及Transformer架构、LLM技术或异构数据建模。论文内容纯粹是计算机视觉技术，没有明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23057v1": {
    "title": "Seq-DeepIPC: Sequential Sensing for End-to-End Control in Legged Robot Navigation",
    "url": "https://www.alphaxiv.org/abs/2510.23057v1",
    "arxiv_id": "2510.23057v1",
    "authors": "Oskar Natan, Jun Miura",
    "categories": "cs.RO, cs.CV, cs.SY, eess.IV, eess.SY",
    "pub_date": "2025-10-27 06:39:57",
    "ori_summary": "We present Seq-DeepIPC, a sequential end-to-end perception-to-control model for legged robot navigation in realworld environments. Seq-DeepIPC advances intelligent sensing for autonomous legged navigation by tightly integrating multi-modal perception (RGB-D + GNSS) with temporal fusion and control. The model jointly predicts semantic segmentation and depth estimation, giving richer spatial features for planning and control. For efficient deployment on edge devices, we use EfficientNet-B0 as the encoder, reducing computation while maintaining accuracy. Heading estimation is simplified by removing the noisy IMU and instead computing the bearing angle directly from consecutive GNSS positions. We collected a larger and more diverse dataset that includes both road and grass terrains, and validated Seq-DeepIPC on a robot dog. Comparative and ablation studies show that sequential inputs improve perception and control in our models, while other baselines do not benefit. Seq-DeepIPC achieves competitive or better results with reasonable model size; although GNSS-only heading is less reliable near tall buildings, it is robust in open areas. Overall, Seq-DeepIPC extends end-to-end navigation beyond wheeled robots to more versatile and temporally-aware systems. To support future research, we will release the codes to our GitHub repository at https://github.com/oskarnatan/Seq-DeepIPC.",
    "summary": "",
    "translation": "Seq-DeepIPC：用于腿式机器人导航端到端控制的序列感知",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人导航和端到端控制，属于机器人技术领域。虽然标题提到序列感知，但这与推荐系统、搜索或广告中的用户行为序列建模没有关联。该研究没有展示在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23043v1": {
    "title": "HieraMamba: Video Temporal Grounding via Hierarchical Anchor-Mamba Pooling",
    "url": "https://www.alphaxiv.org/abs/2510.23043v1",
    "arxiv_id": "2510.23043v1",
    "authors": "Joungbin An, Kristen Grauman",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 06:13:07",
    "ori_summary": "Video temporal grounding, the task of localizing the start and end times of a natural language query in untrimmed video, requires capturing both global context and fine-grained temporal detail. This challenge is particularly pronounced in long videos, where existing methods often compromise temporal fidelity by over-downsampling or relying on fixed windows. We present HieraMamba, a hierarchical architecture that preserves temporal structure and semantic richness across scales. At its core are Anchor-MambaPooling (AMP) blocks, which utilize Mamba's selective scanning to produce compact anchor tokens that summarize video content at multiple granularities. Two complementary objectives, anchor-conditioned and segment-pooled contrastive losses, encourage anchors to retain local detail while remaining globally discriminative. HieraMamba sets a new state-of-the-art on Ego4D-NLQ, MAD, and TACoS, demonstrating precise, temporally faithful localization in long, untrimmed videos.",
    "summary": "",
    "translation": "HieraMamba：通过分层锚点-Mamba池化实现视频时序定位",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频时序定位这一计算机视觉任务，虽然使用了Mamba架构（一种序列建模技术），但其核心应用领域是视频理解而非推荐系统、搜索或广告。Mamba架构本身在序列建模方面有潜力，但论文的具体应用场景与我的关注领域相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23028v1": {
    "title": "Nested AutoRegressive Models",
    "url": "https://www.alphaxiv.org/abs/2510.23028v1",
    "arxiv_id": "2510.23028v1",
    "authors": "Hongyu Wu, Xuhui Fan, Zhangkai Wu, Longbing Cao",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-27 05:49:02",
    "ori_summary": "AutoRegressive (AR) models have demonstrated competitive performance in image generation, achieving results comparable to those of diffusion models. However, their token-by-token image generation mechanism remains computationally intensive and existing solutions such as VAR often lead to limited sample diversity. In this work, we propose a Nested AutoRegressive~(NestAR) model, which proposes nested AutoRegressive architectures in generating images. NestAR designs multi-scale modules in a hierarchical order. These different scaled modules are constructed in an AR architecture, where one larger-scale module is conditioned on outputs from its previous smaller-scale module. Within each module, NestAR uses another AR structure to generate ``patches'' of tokens. The proposed nested AR architecture reduces the overall complexity from $\\mathcal{O}(n)$ to $\\mathcal{O}(\\log n)$ in generating $n$ image tokens, as well as increases image diversities. NestAR further incorporates flow matching loss to use continuous tokens, and develops objectives to coordinate these multi-scale modules in model training. NestAR achieves competitive image generation performance while significantly lowering computational cost.",
    "summary": "论文研究自回归图像生成模型的计算效率问题，核心思想是通过构建多尺度嵌套自回归架构，将大尺度模块条件依赖于小尺度模块输出，实现从O(n)到O(log n)的复杂度降低。",
    "translation": "嵌套自回归模型",
    "relevance_score": 7,
    "reasoning": "自回归模型是LLM和序列建模的核心技术，嵌套结构可能带来效率提升或更复杂的依赖建模。这种架构改进可以直接应用于推荐系统中的用户行为序列建模和搜索中的查询-文档相关性建模，通过更高效的序列生成提升系统性能。",
    "rerank_relevance_score": 3,
    "rerank_reasoning": "该论文专注于图像生成领域的自回归模型效率优化，虽然Transformer架构改进具有通用性，但与推荐系统、搜索和广告的核心领域关联较弱。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.23009v1": {
    "title": "UGAE: Unified Geometry and Attribute Enhancement for G-PCC Compressed Point Clouds",
    "url": "https://www.alphaxiv.org/abs/2510.23009v1",
    "arxiv_id": "2510.23009v1",
    "authors": "Pan Zhao, Hui Yuan, Chongzhen Tian, Tian Guo, Raouf Hamzaoui, Zhigeng Pan",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 05:01:57",
    "ori_summary": "Lossy compression of point clouds reduces storage and transmission costs; however, it inevitably leads to irreversible distortion in geometry structure and attribute information. To address these issues, we propose a unified geometry and attribute enhancement (UGAE) framework, which consists of three core components: post-geometry enhancement (PoGE), pre-attribute enhancement (PAE), and post-attribute enhancement (PoAE). In PoGE, a Transformer-based sparse convolutional U-Net is used to reconstruct the geometry structure with high precision by predicting voxel occupancy probabilities. Building on the refined geometry structure, PAE introduces an innovative enhanced geometry-guided recoloring strategy, which uses a detail-aware K-Nearest Neighbors (DA-KNN) method to achieve accurate recoloring and effectively preserve high-frequency details before attribute compression. Finally, at the decoder side, PoAE uses an attribute residual prediction network with a weighted mean squared error (W-MSE) loss to enhance the quality of high-frequency regions while maintaining the fidelity of low-frequency regions. UGAE significantly outperformed existing methods on three benchmark datasets: 8iVFB, Owlii, and MVUB. Compared to the latest G-PCC test model (TMC13v29), UGAE achieved an average BD-PSNR gain of 9.98 dB and 90.98% BD-bitrate savings for geometry under the D1 metric, as well as a 3.67 dB BD-PSNR improvement with 56.88% BD-bitrate savings for attributes on the Y component. Additionally, it improved perceptual quality significantly.",
    "summary": "",
    "translation": "UGAE：面向G-PCC压缩点云的统一几何与属性增强",
    "relevance_score": 1,
    "reasoning": "该论文专注于点云压缩的几何和属性增强技术，属于计算机图形学和3D视觉领域。虽然提到了统一建模的概念，但其核心应用场景是点云数据压缩和重建，与推荐系统、搜索或广告的异构数据处理没有直接关联，也不涉及Transformer架构或LLM技术的应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23007v1": {
    "title": "CoMo: Compositional Motion Customization for Text-to-Video Generation",
    "url": "https://www.alphaxiv.org/abs/2510.23007v1",
    "arxiv_id": "2510.23007v1",
    "authors": "Youcan Xu, Zhen Wang, Jiaxin Shi, Kexin Li, Feifei Shao, Jun Xiao, Yi Yang, Jun Yu, Long Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 04:57:09",
    "ori_summary": "While recent text-to-video models excel at generating diverse scenes, they struggle with precise motion control, particularly for complex, multi-subject motions. Although methods for single-motion customization have been developed to address this gap, they fail in compositional scenarios due to two primary challenges: motion-appearance entanglement and ineffective multi-motion blending. This paper introduces CoMo, a novel framework for $\\textbf{compositional motion customization}$ in text-to-video generation, enabling the synthesis of multiple, distinct motions within a single video. CoMo addresses these issues through a two-phase approach. First, in the single-motion learning phase, a static-dynamic decoupled tuning paradigm disentangles motion from appearance to learn a motion-specific module. Second, in the multi-motion composition phase, a plug-and-play divide-and-merge strategy composes these learned motions without additional training by spatially isolating their influence during the denoising process. To facilitate research in this new domain, we also introduce a new benchmark and a novel evaluation metric designed to assess multi-motion fidelity and blending. Extensive experiments demonstrate that CoMo achieves state-of-the-art performance, significantly advancing the capabilities of controllable video generation. Our project page is at https://como6.github.io/.",
    "summary": "",
    "translation": "CoMo：面向文本到视频生成的组合式运动定制",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频生成中的运动定制技术，属于AIGC和内容生成领域，与推荐系统、搜索或广告的核心技术焦点无关。虽然组合式定制概念在技术上具有创新性，但缺乏明确的RecSys/Search/Ads应用场景，属于纯粹的生成式AI研究方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23003v1": {
    "title": "An Intelligent Water-Saving Irrigation System Based on Multi-Sensor Fusion and Visual Servoing Control",
    "url": "https://www.alphaxiv.org/abs/2510.23003v1",
    "arxiv_id": "2510.23003v1",
    "authors": "ZhengKai Huang, YiKun Wang, ChenYu Hui, XiaoCheng",
    "categories": "cs.RO, cs.CV, cs.SY, eess.SY",
    "pub_date": "2025-10-27 04:43:20",
    "ori_summary": "This paper introduces an intelligent water-saving irrigation system designed to address critical challenges in precision agriculture, such as inefficient water use and poor terrain adaptability. The system integrates advanced computer vision, robotic control, and real-time stabilization technologies via a multi-sensor fusion approach. A lightweight YOLO model, deployed on an embedded vision processor (K210), enables real-time plant container detection with over 96% accuracy under varying lighting conditions. A simplified hand-eye calibration algorithm-designed for 'handheld camera' robot arm configurations-ensures that the end effector can be precisely positioned, with a success rate exceeding 90%. The active leveling system, driven by the STM32F103ZET6 main control chip and JY901S inertial measurement data, can stabilize the irrigation platform on slopes up to 10 degrees, with a response time of 1.8 seconds. Experimental results across three simulated agricultural environments (standard greenhouse, hilly terrain, complex lighting) demonstrate a 30-50% reduction in water consumption compared to conventional flood irrigation, with water use efficiency exceeding 92% in all test cases.",
    "summary": "",
    "translation": "基于多传感器融合与视觉伺服控制的智能节水灌溉系统",
    "relevance_score": 1,
    "reasoning": "该论文专注于农业灌溉系统，涉及传感器融合和视觉伺服控制技术，这些与推荐系统、搜索或广告领域完全无关。该研究属于农业工程应用，没有任何技术组件或方法可以应用于我的核心关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22995v1": {
    "title": "LoMix: Learnable Weighted Multi-Scale Logits Mixing for Medical Image Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.22995v1",
    "arxiv_id": "2510.22995v1",
    "authors": "Md Mostafijur Rahman, Radu Marculescu",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 04:25:57",
    "ori_summary": "U-shaped networks output logits at multiple spatial scales, each capturing a different blend of coarse context and fine detail. Yet, training still treats these logits in isolation - either supervising only the final, highest-resolution logits or applying deep supervision with identical loss weights at every scale - without exploring mixed-scale combinations. Consequently, the decoder output misses the complementary cues that arise only when coarse and fine predictions are fused. To address this issue, we introduce LoMix (Logits Mixing), a NAS-inspired, differentiable plug-and-play module that generates new mixed-scale outputs and learns how exactly each of them should guide the training process. More precisely, LoMix mixes the multi-scale decoder logits with four lightweight fusion operators: addition, multiplication, concatenation, and attention-based weighted fusion, yielding a rich set of synthetic mutant maps. Every original or mutant map is given a softplus loss weight that is co-optimized with network parameters, mimicking a one-step architecture search that automatically discovers the most useful scales, mixtures, and operators. Plugging LoMix into recent U-shaped architectures (i.e., PVT-V2-B2 backbone with EMCAD decoder) on Synapse 8-organ dataset improves DICE by +4.2% over single-output supervision, +2.2% over deep supervision, and +1.5% over equally weighted additive fusion, all with zero inference overhead. When training data are scarce (e.g., one or two labeled scans), the advantage grows to +9.23%, underscoring LoMix's data efficiency. Across four benchmarks and diverse U-shaped networks, LoMiX improves DICE by up to +13.5% over single-output supervision, confirming that learnable weighted mixed-scale fusion generalizes broadly while remaining data efficient, fully interpretable, and overhead-free at inference. Our code is available at https://github.com/SLDGroup/LoMix.",
    "summary": "",
    "translation": "LoMix：用于医学图像分割的可学习加权多尺度逻辑混合",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分割领域，属于明确的无关主题（医学应用）。虽然涉及多尺度特征混合技术，但论文标题明确限定于医学图像处理，没有显示出在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22994v1": {
    "title": "SceneDecorator: Towards Scene-Oriented Story Generation with Scene Planning and Scene Consistency",
    "url": "https://www.alphaxiv.org/abs/2510.22994v1",
    "arxiv_id": "2510.22994v1",
    "authors": "Quanjian Song, Donghao Zhou, Jingyu Lin, Fei Shen, Jiaze Wang, Xiaowei Hu, Cunjian Chen, Pheng-Ann Heng",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 04:19:22",
    "ori_summary": "Recent text-to-image models have revolutionized image generation, but they still struggle with maintaining concept consistency across generated images. While existing works focus on character consistency, they often overlook the crucial role of scenes in storytelling, which restricts their creativity in practice. This paper introduces scene-oriented story generation, addressing two key challenges: (i) scene planning, where current methods fail to ensure scene-level narrative coherence by relying solely on text descriptions, and (ii) scene consistency, which remains largely unexplored in terms of maintaining scene consistency across multiple stories. We propose SceneDecorator, a training-free framework that employs VLM-Guided Scene Planning to ensure narrative coherence across different scenes in a ``global-to-local'' manner, and Long-Term Scene-Sharing Attention to maintain long-term scene consistency and subject diversity across generated stories. Extensive experiments demonstrate the superior performance of SceneDecorator, highlighting its potential to unleash creativity in the fields of arts, films, and games.",
    "summary": "",
    "translation": "SceneDecorator：面向场景导向的故事生成，通过场景规划与场景一致性实现",
    "relevance_score": 2,
    "reasoning": "该论文主要关注故事生成中的场景规划与一致性，属于内容生成领域。虽然涉及序列建模，但其核心应用是创意写作而非推荐、搜索或广告系统。在推荐/搜索场景中，这种技术可能用于生成商品描述或内容摘要，但这不是论文的主要焦点，且与当前关注的LLM在推荐系统中的应用关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22990v1": {
    "title": "USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked Autoencoding",
    "url": "https://www.alphaxiv.org/abs/2510.22990v1",
    "arxiv_id": "2510.22990v1",
    "authors": "Youssef Megahed, Robin Ducharme, Mark Walker, Steven Hawken, Adrian D. C. Chan",
    "categories": "eess.IV, cs.AI, cs.CV",
    "pub_date": "2025-10-27 04:16:43",
    "ori_summary": "Ultrasound imaging is one of the most widely used diagnostic modalities, offering real-time, radiation-free assessment across diverse clinical domains. However, interpretation of ultrasound images remains challenging due to high noise levels, operator dependence, and limited field of view, resulting in substantial inter-observer variability. Current Deep Learning approaches are hindered by the scarcity of large labeled datasets and the domain gap between general and sonographic images, which limits the transferability of models pretrained on non-medical data. To address these challenges, we introduce the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), the first large-scale self-supervised MAE framework pretrained exclusively on ultrasound data. The model was pre-trained on 370,000 2D and 3D ultrasound images curated from 46 open-source datasets, collectively termed OpenUS-46, spanning over twenty anatomical regions. This curated dataset has been made publicly available to facilitate further research and reproducibility. Using a Vision Transformer encoder-decoder architecture, USF-MAE reconstructs masked image patches, enabling it to learn rich, modality-specific representations directly from unlabeled data. The pretrained encoder was fine-tuned on three public downstream classification benchmarks: BUS-BRA (breast cancer), MMOTU-2D (ovarian tumors), and GIST514-DB (gastrointestinal stromal tumors). Across all tasks, USF-MAE consistently outperformed conventional CNN and ViT baselines, achieving F1-scores of 81.6%, 79.6%, and 82.4%, respectively. Despite not using labels during pretraining, USF-MAE approached the performance of the supervised foundation model UltraSam on breast cancer classification and surpassed it on the other tasks, demonstrating strong cross-anatomical generalization.",
    "summary": "",
    "translation": "USF-MAE：基于掩码自编码的超声自监督基础模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学超声领域的自监督学习，属于医疗影像的特定领域应用。虽然涉及基础模型和掩码自编码技术，但其应用场景明确限定在医疗超声领域，与推荐系统、搜索或广告的核心技术发展没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22981v1": {
    "title": "Exploring Semantic-constrained Adversarial Example with Instruction Uncertainty Reduction",
    "url": "https://www.alphaxiv.org/abs/2510.22981v1",
    "arxiv_id": "2510.22981v1",
    "authors": "Jin Hu, Jiakai Wang, Linna Jing, Haolin Li, Haodong Liu, Haotong Qin, Aishan Liu, Ke Xu, Xianglong Liu",
    "categories": "cs.AI, cs.CV",
    "pub_date": "2025-10-27 04:02:52",
    "ori_summary": "Recently, semantically constrained adversarial examples (SemanticAE), which are directly generated from natural language instructions, have become a promising avenue for future research due to their flexible attacking forms. To generate SemanticAEs, current methods fall short of satisfactory attacking ability as the key underlying factors of semantic uncertainty in human instructions, such as referring diversity, descriptive incompleteness, and boundary ambiguity, have not been fully investigated. To tackle the issues, this paper develops a multi-dimensional instruction uncertainty reduction (InSUR) framework to generate more satisfactory SemanticAE, i.e., transferable, adaptive, and effective. Specifically, in the dimension of the sampling method, we propose the residual-driven attacking direction stabilization to alleviate the unstable adversarial optimization caused by the diversity of language references. By coarsely predicting the language-guided sampling process, the optimization process will be stabilized by the designed ResAdv-DDIM sampler, therefore releasing the transferable and robust adversarial capability of multi-step diffusion models. In task modeling, we propose the context-encoded attacking scenario constraint to supplement the missing knowledge from incomplete human instructions. Guidance masking and renderer integration are proposed to regulate the constraints of 2D/3D SemanticAE, activating stronger scenario-adapted attacks. Moreover, in the dimension of generator evaluation, we propose the semantic-abstracted attacking evaluation enhancement by clarifying the evaluation boundary, facilitating the development of more effective SemanticAE generators. Extensive experiments demonstrate the superiority of the transfer attack performance of InSUR. Moreover, we realize the reference-free generation of semantically constrained 3D adversarial examples for the first time.",
    "summary": "",
    "translation": "探索语义约束下通过指令不确定性降低的对抗样本",
    "relevance_score": 2,
    "reasoning": "该论文主要研究对抗样本和指令不确定性降低，属于安全性和鲁棒性领域，与我的核心关注点（推荐系统、搜索、广告的技术进展）相关性较低。虽然对抗训练可能间接影响模型性能，但论文焦点不在推荐/搜索/广告的直接应用或核心LLM技术进展上。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22975v1": {
    "title": "VoMP: Predicting Volumetric Mechanical Property Fields",
    "url": "https://www.alphaxiv.org/abs/2510.22975v1",
    "arxiv_id": "2510.22975v1",
    "authors": "Rishit Dagli, Donglai Xiang, Vismay Modi, Charles Loop, Clement Fuji Tsang, Anka He Chen, Anita Hu, Gavriel State, David I. W. Levin, Maria Shugrina",
    "categories": "cs.CV, cs.GR, cs.LG",
    "pub_date": "2025-10-27 03:56:25",
    "ori_summary": "Physical simulation relies on spatially-varying mechanical properties, often laboriously hand-crafted. VoMP is a feed-forward method trained to predict Young's modulus ($E$), Poisson's ratio ($\\nu$), and density ($\\rho$) throughout the volume of 3D objects, in any representation that can be rendered and voxelized. VoMP aggregates per-voxel multi-view features and passes them to our trained Geometry Transformer to predict per-voxel material latent codes. These latents reside on a manifold of physically plausible materials, which we learn from a real-world dataset, guaranteeing the validity of decoded per-voxel materials. To obtain object-level training data, we propose an annotation pipeline combining knowledge from segmented 3D datasets, material databases, and a vision-language model, along with a new benchmark. Experiments show that VoMP estimates accurate volumetric properties, far outperforming prior art in accuracy and speed.",
    "summary": "",
    "translation": "VoMP：预测体积力学性能场",
    "relevance_score": 1,
    "reasoning": "该论文标题表明研究内容为材料科学或工程力学领域的体积力学性能预测，属于物理科学范畴。这与推荐系统、搜索、广告或相关使能技术没有任何直接或间接关联，完全超出了当前关注的技术领域范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22973v1": {
    "title": "Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method",
    "url": "https://www.alphaxiv.org/abs/2510.22973v1",
    "arxiv_id": "2510.22973v1",
    "authors": "Bohan Li, Xin Jin, Hu Zhu, Hongsi Liu, Ruikai Li, Jiazhe Guo, Kaiwen Cai, Chao Ma, Yueming Jin, Hao Zhao, Xiaokang Yang, Wenjun Zeng",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 03:52:45",
    "ori_summary": "Driving scene generation is a critical domain for autonomous driving, enabling downstream applications, including perception and planning evaluation. Occupancy-centric methods have recently achieved state-of-the-art results by offering consistent conditioning across frames and modalities; however, their performance heavily depends on annotated occupancy data, which still remains scarce. To overcome this limitation, we curate Nuplan-Occ, the largest semantic occupancy dataset to date, constructed from the widely used Nuplan benchmark. Its scale and diversity facilitate not only large-scale generative modeling but also autonomous driving downstream applications. Based on this dataset, we develop a unified framework that jointly synthesizes high-quality semantic occupancy, multi-view videos, and LiDAR point clouds. Our approach incorporates a spatio-temporal disentangled architecture to support high-fidelity spatial expansion and temporal forecasting of 4D dynamic occupancy. To bridge modal gaps, we further propose two novel techniques: a Gaussian splatting-based sparse point map rendering strategy that enhances multi-view video generation, and a sensor-aware embedding strategy that explicitly models LiDAR sensor properties for realistic multi-LiDAR simulation. Extensive experiments demonstrate that our method achieves superior generation fidelity and scalability compared to existing approaches, and validates its practical value in downstream tasks. Repo: https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2",
    "summary": "",
    "translation": "扩展以占据率为中心的驾驶场景生成：数据集与方法",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于自动驾驶领域的场景生成，属于纯粹的计算机视觉应用。虽然提到了生成技术，但内容完全围绕驾驶场景，与推荐系统、搜索或广告没有任何关联。该论文不涉及任何Transformer架构、LLM技术或推荐系统的应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22970v1": {
    "title": "VALA: Learning Latent Anchors for Training-Free and Temporally Consistent",
    "url": "https://www.alphaxiv.org/abs/2510.22970v1",
    "arxiv_id": "2510.22970v1",
    "authors": "Zhangkai Wu, Xuhui Fan, Zhongyuan Xie, Kaize Shi, Longbing Cao",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 03:44:11",
    "ori_summary": "Recent advances in training-free video editing have enabled lightweight and precise cross-frame generation by leveraging pre-trained text-to-image diffusion models. However, existing methods often rely on heuristic frame selection to maintain temporal consistency during DDIM inversion, which introduces manual bias and reduces the scalability of end-to-end inference. In this paper, we propose~\\textbf{VALA} (\\textbf{V}ariational \\textbf{A}lignment for \\textbf{L}atent \\textbf{A}nchors), a variational alignment module that adaptively selects key frames and compresses their latent features into semantic anchors for consistent video editing. To learn meaningful assignments, VALA propose a variational framework with a contrastive learning objective. Therefore, it can transform cross-frame latent representations into compressed latent anchors that preserve both content and temporal coherence. Our method can be fully integrated into training-free text-to-image based video editing models. Extensive experiments on real-world video editing benchmarks show that VALA achieves state-of-the-art performance in inversion fidelity, editing quality, and temporal consistency, while offering improved efficiency over prior methods.",
    "summary": "",
    "translation": "VALA：学习潜在锚点以实现免训练和时间一致性",
    "relevance_score": 3,
    "reasoning": "该论文标题暗示了学习潜在锚点的方法，可能涉及表示学习或特征提取技术，这与推荐系统中的用户/物品表示学习有一定关联。然而，标题中明确提到的'免训练'和'时间一致性'更偏向于计算机视觉或视频处理领域的时间一致性保持，而非直接针对推荐、搜索或广告系统的核心问题，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22964v1": {
    "title": "Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges",
    "url": "https://www.alphaxiv.org/abs/2510.22964v1",
    "arxiv_id": "2510.22964v1",
    "authors": "Liling Yang, Ning Chen, Jun Yue, Yidan Liu, Jiayi Ma, Pedram Ghamisi, Antonio Plaza, Leyuan Fang",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 03:40:00",
    "ori_summary": "Foundation models have transformed natural language processing and computer vision, and their impact is now reshaping remote sensing image analysis. With powerful generalization and transfer learning capabilities, they align naturally with the multimodal, multi-resolution, and multi-temporal characteristics of remote sensing data. To address unique challenges in the field, multimodal geospatial foundation models (GFMs) have emerged as a dedicated research frontier. This survey delivers a comprehensive review of multimodal GFMs from a modality-driven perspective, covering five core visual and vision-language modalities. We examine how differences in imaging physics and data representation shape interaction design, and we analyze key techniques for alignment, integration, and knowledge transfer to tackle modality heterogeneity, distribution shifts, and semantic gaps. Advances in training paradigms, architectures, and task-specific adaptation strategies are systematically assessed alongside a wealth of emerging benchmarks. Representative multimodal visual and vision-language GFMs are evaluated across ten downstream tasks, with insights into their architectures, performance, and application scenarios. Real-world case studies, spanning land cover mapping, agricultural monitoring, disaster response, climate studies, and geospatial intelligence, demonstrate the practical potential of GFMs. Finally, we outline pressing challenges in domain generalization, interpretability, efficiency, and privacy, and chart promising avenues for future research.",
    "summary": "",
    "translation": "多模态地理空间基础模型综述：技术、应用与挑战",
    "relevance_score": 2,
    "reasoning": "虽然该论文涉及多模态基础模型，但其核心焦点是地理空间应用，与推荐系统、搜索或广告的直接关联性较弱。地理空间模型在位置感知推荐或本地化搜索中可能有间接应用，但论文的综述性质和应用领域使其主要相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22960v1": {
    "title": "FAME: Fairness-aware Attention-modulated Video Editing",
    "url": "https://www.alphaxiv.org/abs/2510.22960v1",
    "arxiv_id": "2510.22960v1",
    "authors": "Zhangkai Wu, Xuhui Fan, Zhongyuan Xie, Kaize Shi, Zhidong Li, Longbing Cao",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-27 03:34:15",
    "ori_summary": "Training-free video editing (VE) models tend to fall back on gender stereotypes when rendering profession-related prompts. We propose \\textbf{FAME} for \\textit{Fairness-aware Attention-modulated Video Editing} that mitigates profession-related gender biases while preserving prompt alignment and temporal consistency for coherent VE. We derive fairness embeddings from existing minority representations by softly injecting debiasing tokens into the text encoder. Simultaneously, FAME integrates fairness modulation into both temporal self attention and prompt-to-region cross attention to mitigate the motion corruption and temporal inconsistency caused by directly introducing fairness cues. For temporal self attention, FAME introduces a region constrained attention mask combined with time decay weighting, which enhances intra-region coherence while suppressing irrelevant inter-region interactions. For cross attention, it reweights tokens to region matching scores by incorporating fairness sensitive similarity masks derived from debiasing prompt embeddings. Together, these modulations keep fairness-sensitive semantics tied to the right visual regions and prevent temporal drift across frames. Extensive experiments on new VE fairness-oriented benchmark \\textit{FairVE} demonstrate that FAME achieves stronger fairness alignment and semantic fidelity, surpassing existing VE baselines.",
    "summary": "",
    "translation": "FAME：公平性感知的注意力调制视频编辑",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及公平性和视频编辑，这两个主题均属于明确排除的无关主题范畴。公平性属于非技术性伦理话题，而视频编辑属于纯粹的视觉应用，与推荐系统、搜索或广告的排名技术没有明确关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22946v1": {
    "title": "LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation",
    "url": "https://www.alphaxiv.org/abs/2510.22946v1",
    "arxiv_id": "2510.22946v1",
    "authors": "Zeyu Wang, Zilong Chen, Chenhui Gou, Feng Li, Chaorui Deng, Deyao Zhu, Kunchang Li, Weihao Yu, Haoqin Tu, Haoqi Fan, Cihang Xie",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 02:59:57",
    "ori_summary": "Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources. In this paper, we show that competitive performance can be obtained far more efficiently by strategically fusing publicly available models specialized for either generation or understanding. Our key design is to retain the original blocks while additionally interleaving multimodal self-attention blocks throughout the networks. This double fusion mechanism (1) effectively enables rich multi-modal fusion while largely preserving the original strengths of the base models, and (2) catalyzes synergistic fusion of high-level semantic representations from the understanding encoder with low-level spatial signals from the generation encoder. By training with only ~ 35B tokens, this approach achieves strong results across multiple benchmarks: 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By fully releasing the entire suite of code, model weights, and datasets, we hope to support future research on unified multimodal modeling.",
    "summary": "研究如何高效构建统一多模态模型；核心方法是保留预训练模型原始块的同时插入多模态自注意力块，实现理解编码器与生成编码器的语义-空间信号协同融合。",
    "translation": "LightBagel：一种轻量级、双重融合框架，用于统一多模态理解与生成",
    "relevance_score": 8,
    "reasoning": "该论文提出的统一多模态理解与生成框架直接对应'VLM类比用于异构数据'这一焦点领域，其中双重融合机制可类比于处理推荐系统中的用户序列和上下文特征等异构模态。轻量级设计对于大规模推荐系统的实际部署具有重要价值，能够高效处理多源用户行为数据。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文的双融合框架和轻量化设计直接关联Transformer架构效率优化，其统一多模态建模思想对推荐系统中的异构数据处理具有启发性。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.22943v1": {
    "title": "Switchable Token-Specific Codebook Quantization For Face Image Compression",
    "url": "https://www.alphaxiv.org/abs/2510.22943v1",
    "arxiv_id": "2510.22943v1",
    "authors": "Yongbo Wang, Haonan Wang, Guodong Mu, Ruixin Zhang, Jiaqi Chen, Jingyun Zhang, Jun Wang, Yuan Xie, Zhizhong Zhang, Shouhong Ding",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 02:56:17",
    "ori_summary": "With the ever-increasing volume of visual data, the efficient and lossless transmission, along with its subsequent interpretation and understanding, has become a critical bottleneck in modern information systems. The emerged codebook-based solution utilize a globally shared codebook to quantize and dequantize each token, controlling the bpp by adjusting the number of tokens or the codebook size. However, for facial images, which are rich in attributes, such global codebook strategies overlook both the category-specific correlations within images and the semantic differences among tokens, resulting in suboptimal performance, especially at low bpp. Motivated by these observations, we propose a Switchable Token-Specific Codebook Quantization for face image compression, which learns distinct codebook groups for different image categories and assigns an independent codebook to each token. By recording the codebook group to which each token belongs with a small number of bits, our method can reduce the loss incurred when decreasing the size of each codebook group. This enables a larger total number of codebooks under a lower overall bpp, thereby enhancing the expressive capability and improving reconstruction performance. Owing to its generalizable design, our method can be integrated into any existing codebook-based representation learning approach and has demonstrated its effectiveness on face recognition datasets, achieving an average accuracy of 93.51% for reconstructed images at 0.05 bpp.",
    "summary": "",
    "translation": "面向人脸图像压缩的可切换令牌特定码本量化",
    "relevance_score": 1,
    "reasoning": "该论文专注于人脸图像压缩的特定计算机视觉任务，属于纯粹的视觉应用领域。虽然提到了量化技术，但该技术专门针对人脸图像压缩优化，与推荐系统、搜索或广告的核心技术栈没有明显关联，也不涉及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22937v1": {
    "title": "Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics",
    "url": "https://www.alphaxiv.org/abs/2510.22937v1",
    "arxiv_id": "2510.22937v1",
    "authors": "Matthew So, Judah Goldfeder, Mark Lis, Hod Lipson",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-27 02:41:43",
    "ori_summary": "There has been a historic assumption that the biometrics of an individual are statistically uncorrelated. We test this assumption by training Bi-Encoder networks on three verification tasks, including fingerprint-to-fingerprint matching, iris-to-iris matching, and cross-modal fingerprint-to-iris matching using 274 subjects with $\\sim$100k fingerprints and 7k iris images. We trained ResNet-50 and Vision Transformer backbones in Bi-Encoder architectures such that the contrastive loss between images sampled from the same individual is minimized. The iris ResNet architecture reaches 91 ROC AUC score for iris-to-iris matching, providing clear evidence that the left and right irises of an individual are correlated. Fingerprint models reproduce the positive intra-subject suggested by prior work in this space. This is the first work attempting to use Vision Transformers for this matching. Cross-modal matching rises only slightly above chance, which suggests that more data and a more sophisticated pipeline is needed to obtain compelling results. These findings continue challenge independence assumptions of biometrics and we plan to extend this work to other biometrics in the future. Code available: https://github.com/MatthewSo/bio_fingerprints_iris.",
    "summary": "",
    "translation": "用于指纹和虹膜生物识别的双编码器对比学习",
    "relevance_score": 1,
    "reasoning": "该论文涉及生物识别技术（指纹和虹膜），这属于安全认证领域，与推荐系统、搜索或广告的核心技术无关。对比学习虽然是一种通用技术，但论文的应用场景明确限定在生物特征识别，没有显示与推荐、搜索或广告系统的潜在联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22936v1": {
    "title": "Positional Preservation Embedding for Multimodal Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.22936v1",
    "arxiv_id": "2510.22936v1",
    "authors": "Mouxiao Huang, Borui Jiang, Dehua Zheng, Hailin Hu, Kai Han, Xinghao Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 02:40:02",
    "ori_summary": "Multimodal large language models (MLLMs) have achieved strong performance on vision-language tasks, yet often suffer from inefficiencies due to redundant visual tokens. Existing token merging methods reduce sequence length but frequently disrupt spatial layouts and temporal continuity by disregarding positional relationships. In this work, we propose a novel encoding operator dubbed as \\textbf{P}ositional \\textbf{P}reservation \\textbf{E}mbedding (\\textbf{PPE}), which has the main hallmark of preservation of spatiotemporal structure during visual token compression. PPE explicitly introduces the disentangled encoding of 3D positions in the token dimension, enabling each compressed token to encapsulate different positions from multiple original tokens. Furthermore, we show that PPE can effectively support cascade clustering -- a progressive token compression strategy that leads to better performance retention. PPE is a parameter-free and generic operator that can be seamlessly integrated into existing token merging methods without any adjustments. Applied to state-of-the-art token merging framework, PPE achieves consistent improvements of $2\\%\\sim5\\%$ across multiple vision-language benchmarks, including MMBench (general vision understanding), TextVQA (layout understanding) and VideoMME (temporal understanding). These results demonstrate that preserving positional cues is critical for efficient and effective MLLM reasoning.",
    "summary": "研究多模态大语言模型中视觉令牌压缩导致空间布局和时间连续性破坏的问题；核心方法是提出位置保持嵌入算子，通过令牌维度上3D位置的解耦编码，使每个压缩令牌能够封装来自多个原始令牌的不同位置信息。",
    "translation": "多模态大语言模型的位置保持嵌入",
    "relevance_score": 6,
    "reasoning": "该论文涉及位置嵌入技术，这是Transformer架构的核心组件，属于'Enabling Transformer Tech'范畴。在推荐系统和搜索中，位置信息对于理解用户行为序列、商品浏览顺序等至关重要，改进的位置嵌入可以增强序列建模能力，提升推荐和搜索的准确性。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的位置保持嵌入方法直接针对Transformer架构的效率优化，通过位置信息解耦编码解决多模态序列压缩中的空间布局和时间连续性保持问题，对推荐和搜索系统中的序列建模具有重要借鉴意义。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.22930v1": {
    "title": "Gen-LangSplat: Generalized Language Gaussian Splatting with Pre-Trained Feature Compression",
    "url": "https://www.alphaxiv.org/abs/2510.22930v1",
    "arxiv_id": "2510.22930v1",
    "authors": "Pranav Saxena",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-27 02:13:38",
    "ori_summary": "Modeling open-vocabulary language fields in 3D is essential for intuitive human-AI interaction and querying within physical environments. State-of-the-art approaches, such as LangSplat, leverage 3D Gaussian Splatting to efficiently construct these language fields, encoding features distilled from high-dimensional models like CLIP. However, this efficiency is currently offset by the requirement to train a scene-specific language autoencoder for feature compression, introducing a costly, per-scene optimization bottleneck that hinders deployment scalability. In this work, we introduce Gen-LangSplat, that eliminates this requirement by replacing the scene-wise autoencoder with a generalized autoencoder, pre-trained extensively on the large-scale ScanNet dataset. This architectural shift enables the use of a fixed, compact latent space for language features across any new scene without any scene-specific training. By removing this dependency, our entire language field construction process achieves a efficiency boost while delivering querying performance comparable to, or exceeding, the original LangSplat method. To validate our design choice, we perform a thorough ablation study empirically determining the optimal latent embedding dimension and quantifying representational fidelity using Mean Squared Error and cosine similarity between the original and reprojected 512-dimensional CLIP embeddings. Our results demonstrate that generalized embeddings can efficiently and accurately support open-vocabulary querying in novel 3D scenes, paving the way for scalable, real-time interactive 3D AI applications.",
    "summary": "",
    "translation": "Gen-LangSplat：基于预训练特征压缩的广义语言高斯泼溅",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及3D场景重建与语言模型的结合（高斯泼溅是3D重建技术），属于计算机视觉与语言交叉领域。虽然标题提到预训练特征压缩，但其核心应用方向是3D场景理解而非推荐/搜索/广告系统，与当前关注的技术领域关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22916v1": {
    "title": "Estimating Pasture Biomass from Top-View Images: A Dataset for Precision Agriculture",
    "url": "https://www.alphaxiv.org/abs/2510.22916v1",
    "arxiv_id": "2510.22916v1",
    "authors": "Qiyu Liao, Dadong Wang, Rebecca Haling, Jiajun Liu, Xun Li, Martyna Plomecka, Andrew Robson, Matthew Pringle, Rhys Pirie, Megan Walker, Joshua Whelan",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 01:35:00",
    "ori_summary": "Accurate estimation of pasture biomass is important for decision-making in livestock production systems. Estimates of pasture biomass can be used to manage stocking rates to maximise pasture utilisation, while minimising the risk of overgrazing and promoting overall system health. We present a comprehensive dataset of 1,162 annotated top-view images of pastures collected across 19 locations in Australia. The images were taken across multiple seasons and include a range of temperate pasture species. Each image captures a 70cm * 30cm quadrat and is paired with on-ground measurements including biomass sorted by component (green, dead, and legume fraction), vegetation height, and Normalized Difference Vegetation Index (NDVI) from Active Optical Sensors (AOS). The multidimensional nature of the data, which combines visual, spectral, and structural information, opens up new possibilities for advancing the use of precision grazing management. The dataset is released and hosted in a Kaggle competition that challenges the international Machine Learning community with the task of pasture biomass estimation. The dataset is available on the official Kaggle webpage: https://www.kaggle.com/competitions/csiro-biomass",
    "summary": "",
    "translation": "基于俯视图像估算牧场生物量：精准农业数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于农业领域的计算机视觉应用，涉及从图像估算生物量这一特定任务。这与我的关注领域（推荐系统、搜索、广告）完全无关，且没有明显的技术迁移潜力。论文内容属于精准农业这一垂直领域，不在任何相关技术范畴内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  }
}