{
  "2511.07328v1": {
    "title": "Q-RAG: Long Context Multi-step Retrieval via Value-based Embedder Training",
    "url": "https://www.alphaxiv.org/abs/2511.07328v1",
    "arxiv_id": "2511.07328v1",
    "authors": "Artyom Sorokin, Nazar Buzun, Alexander Anokhin, Oleg Inozemcev, Egor Vedernikov, Petr Anokhin, Mikhail Burtsev, Trushkov Alexey, Yin Wenshuai, Evgeny Burnaev",
    "categories": "cs.LG, cs.IR",
    "pub_date": "2025-11-10 17:31:02",
    "ori_summary": "Retrieval-Augmented Generation (RAG) methods enhance LLM performance by efficiently filtering relevant context for LLMs, reducing hallucinations and inference cost. However, most existing RAG methods focus on single-step retrieval, which is often insufficient for answering complex questions that require multi-step search. Recently, multi-step retrieval approaches have emerged, typically involving the fine-tuning of small LLMs to perform multi-step retrieval. This type of fine-tuning is highly resource-intensive and does not enable the use of larger LLMs. In this work, we propose Q-RAG, a novel approach that fine-tunes the Embedder model for multi-step retrieval using reinforcement learning (RL). Q-RAG offers a competitive, resource-efficient alternative to existing multi-step retrieval methods for open-domain question answering and achieves state-of-the-art results on the popular long-context benchmarks Babilong and RULER for contexts up to 10M tokens.",
    "summary": "论文研究长上下文多步检索的核心问题，核心方法是使用强化学习微调嵌入模型来实现高效的多步检索，避免传统方法需要微调小语言模型的高资源消耗。",
    "translation": "Q-RAG：基于价值嵌入器训练的长上下文多步检索",
    "relevance_score": 8,
    "reasoning": "该论文专注于检索增强生成(RAG)中的长上下文多步检索，直接适用于搜索和推荐系统的检索组件。基于价值的嵌入器训练方法可以显著提升检索效率和准确性，对于处理长用户序列和复杂上下文特征的推荐与搜索场景具有直接应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出通过强化学习训练嵌入模型实现多步检索，直接针对搜索和推荐系统中的核心检索效率问题，属于LLM在搜索领域的直接应用。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.07295v1": {
    "title": "Hard vs. Noise: Resolving Hard-Noisy Sample Confusion in Recommender Systems via Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2511.07295v1",
    "arxiv_id": "2511.07295v1",
    "authors": "Tianrui Song, Wen-Shuo Chao, Hao Liu",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-11-10 16:51:03",
    "ori_summary": "Implicit feedback, employed in training recommender systems, unavoidably confronts noise due to factors such as misclicks and position bias. Previous studies have attempted to identify noisy samples through their diverged data patterns, such as higher loss values, and mitigate their influence through sample dropping or reweighting. However, we observed that noisy samples and hard samples display similar patterns, leading to hard-noisy confusion issue. Such confusion is problematic as hard samples are vital for modeling user preferences. To solve this problem, we propose LLMHNI framework, leveraging two auxiliary user-item relevance signals generated by Large Language Models (LLMs) to differentiate hard and noisy samples. LLMHNI obtains user-item semantic relevance from LLM-encoded embeddings, which is used in negative sampling to select hard negatives while filtering out noisy false negatives. An objective alignment strategy is proposed to project LLM-encoded embeddings, originally for general language tasks, into a representation space optimized for user-item relevance modeling. LLMHNI also exploits LLM-inferred logical relevance within user-item interactions to identify hard and noisy samples. These LLM-inferred interactions are integrated into the interaction graph and guide denoising with cross-graph contrastive alignment. To eliminate the impact of unreliable interactions induced by LLM hallucination, we propose a graph contrastive learning strategy that aligns representations from randomly edge-dropped views to suppress unreliable edges. Empirical results demonstrate that LLMHNI significantly improves denoising and recommendation performance.",
    "summary": "论文研究推荐系统中硬样本与噪声样本因相似数据模式而产生的混淆问题。核心方法是利用LLM生成用户-项目语义相关性和逻辑相关性两种辅助信号，通过负采样优化和跨图对比对齐来区分硬样本与噪声样本。",
    "translation": "困难样本 vs. 噪声样本：基于大语言模型解决推荐系统中困难-噪声样本混淆问题",
    "relevance_score": 8,
    "reasoning": "该论文直接应用LLM技术解决推荐系统中的核心问题——样本质量区分，属于'Direct LLM Applications'范畴。通过LLM区分困难样本与噪声样本，可以显著提升推荐模型的训练效果和泛化能力，这对于推荐系统的排序质量至关重要。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统中的噪声处理问题，利用LLM生成辅助信号解决硬样本与噪声样本混淆的核心挑战，完美契合LLM在推荐系统中的应用研究。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.07280v1": {
    "title": "The Value of Personalized Recommendations: Evidence from Netflix",
    "url": "https://www.alphaxiv.org/abs/2511.07280v1",
    "arxiv_id": "2511.07280v1",
    "authors": "Kevin Zielnicki, Guy Aridor, Aurélien Bibaut, Allen Tran, Winston Chou, Nathan Kallus",
    "categories": "econ.GN, cs.IR, cs.LG, q-fin.EC",
    "pub_date": "2025-11-10 16:27:01",
    "ori_summary": "Personalized recommendation systems shape much of user choice online, yet their targeted nature makes separating out the value of recommendation and the underlying goods challenging. We build a discrete choice model that embeds recommendation-induced utility, low-rank heterogeneity, and flexible state dependence and apply the model to viewership data at Netflix. We exploit idiosyncratic variation introduced by the recommendation algorithm to identify and separately value these components as well as to recover model-free diversion ratios that we can use to validate our structural model. We use the model to evaluate counterfactuals that quantify the incremental engagement generated by personalized recommendations. First, we show that replacing the current recommender system with a matrix factorization or popularity-based algorithm would lead to 4% and 12% reduction in engagement, respectively, and decreased consumption diversity. Second, most of the consumption increase from recommendations comes from effective targeting, not mechanical exposure, with the largest gains for mid-popularity goods (as opposed to broadly appealing or very niche goods).",
    "summary": "论文研究如何分离和量化个性化推荐系统的价值，核心方法是构建包含推荐诱导效用、低秩异质性和灵活状态依赖的离散选择模型，用于评估推荐系统对用户参与度的增量贡献。",
    "translation": "个性化推荐的价值：来自Netflix的证据",
    "relevance_score": 10,
    "reasoning": "该论文直接研究个性化推荐系统的实际价值，属于核心推荐系统领域的进展。通过Netflix这一主要推荐平台的实证证据，该研究对理解推荐系统的商业影响和用户价值具有直接相关性，完全符合核心推荐系统领域的研究重点。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接研究推荐系统的价值评估，通过结构模型量化个性化推荐对用户参与度的影响，属于推荐系统核心领域的重要实证研究。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.07055v1": {
    "title": "When Sufficient is not Enough: Utilizing the Rashomon Effect for Complete Evidence Extraction",
    "url": "https://www.alphaxiv.org/abs/2511.07055v1",
    "arxiv_id": "2511.07055v1",
    "authors": "Katharina Beckh, Stefan Rüping",
    "categories": "cs.CL, cs.IR, cs.LG",
    "pub_date": "2025-11-10 12:46:39",
    "ori_summary": "Feature attribution methods typically provide minimal sufficient evidence justifying a model decision. However, in many applications this is inadequate. For compliance and cataloging, the full set of contributing features must be identified - complete evidence. We perform a case study on a medical dataset which contains human-annotated complete evidence. We show that individual models typically recover only subsets of complete evidence and that aggregating evidence from several models improves evidence recall from $\\sim$0.60 (single best model) to $\\sim$0.86 (ensemble). We analyze the recall-precision trade-off, the role of training with evidence, dynamic ensembles with certainty thresholds, and discuss implications.",
    "summary": "",
    "translation": "当充分性不足时：利用拉什omon效应进行完整证据提取",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及统计学中的拉什omon效应（即多个模型对相同数据具有相似性能的现象）和证据提取，这些概念与推荐系统、搜索或广告的核心技术领域没有直接关联。标题暗示的是模型解释性或因果推断方向，而非LLM技术、Transformer架构改进或直接应用于排名系统的创新方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07028v1": {
    "title": "Wavelet Enhanced Adaptive Frequency Filter for Sequential Recommendation",
    "url": "https://www.alphaxiv.org/abs/2511.07028v1",
    "arxiv_id": "2511.07028v1",
    "authors": "Huayang Xu, Huanhuan Yuan, Guanfeng Liu, Junhua Fang, Lei Zhao, Pengpeng Zhao",
    "categories": "cs.IR",
    "pub_date": "2025-11-10 12:22:33",
    "ori_summary": "Sequential recommendation has garnered significant attention for its ability to capture dynamic preferences by mining users' historical interaction data. Given that users' complex and intertwined periodic preferences are difficult to disentangle in the time domain, recent research is exploring frequency domain analysis to identify these hidden patterns. However, current frequency-domain-based methods suffer from two key limitations: (i) They primarily employ static filters with fixed characteristics, overlooking the personalized nature of behavioral patterns; (ii) While the global discrete Fourier transform excels at modeling long-range dependencies, it can blur non-stationary signals and short-term fluctuations. To overcome these limitations, we propose a novel method called Wavelet Enhanced Adaptive Frequency Filter for Sequential Recommendation. Specifically, it consists of two vital modules: dynamic frequency-domain filtering and wavelet feature enhancement. The former is used to dynamically adjust filtering operations based on behavioral sequences to extract personalized global information, and the latter integrates wavelet transform to reconstruct sequences, enhancing blurred non-stationary signals and short-term fluctuations. Finally, these two modules work to achieve comprehensive performance and efficiency optimization in long sequential recommendation scenarios. Extensive experiments on four widely-used benchmark datasets demonstrate the superiority of our work.",
    "summary": "论文研究序列推荐中用户复杂周期性偏好的建模问题，核心思想是结合动态频率域滤波提取个性化全局信息，并集成小波变换重构序列以增强非平稳信号和短期波动。",
    "translation": "用于序列推荐的小波增强自适应频率滤波器",
    "relevance_score": 8,
    "reasoning": "该论文属于序列推荐的核心领域进展，直接针对推荐系统中的序列建模问题。小波变换和频率滤波技术可以用于提取用户行为序列中的多尺度时序模式，这对于理解用户长期和短期兴趣、处理噪声序列数据具有直接应用价值，能够提升推荐系统的准确性和鲁棒性。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文针对序列推荐中的频率域分析进行创新，提出动态滤波和小波增强方法，直接涉及Transformer架构的效率优化和异质数据处理，与核心领域高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.07025v1": {
    "title": "Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks",
    "url": "https://www.alphaxiv.org/abs/2511.07025v1",
    "arxiv_id": "2511.07025v1",
    "authors": "Yauhen Babakhin, Radek Osmulski, Ronay Ak, Gabriel Moreira, Mengyao Xu, Benedikt Schifferer, Bo Liu, Even Oldridge",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2025-11-10 12:13:16",
    "ori_summary": "We introduce llama-embed-nemotron-8b, an open-weights text embedding model that achieves state-of-the-art performance on the Multilingual Massive Text Embedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent models show strong performance, their training data or methodologies are often not fully disclosed. We aim to address this by developing a fully open-source model, publicly releasing its weights and detailed ablation studies, and planning to share the curated training datasets. Our model demonstrates superior performance across all major embedding tasks -- including retrieval, classification and semantic textual similarity (STS) -- and excels in challenging multilingual scenarios, such as low-resource languages and cross-lingual setups. This state-of-the-art performance is driven by a novel data mix of 16.1 million query-document pairs, split between 7.7 million samples from public datasets and 8.4 million synthetically generated examples from various open-weight LLMs. One of our key contributions is a detailed ablation study analyzing core design choices, including a comparison of contrastive loss implementations, an evaluation of synthetic data generation (SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b is an instruction-aware model, supporting user-defined instructions to enhance performance for specific use-cases. This combination of top-tier performance, broad applicability, and user-driven flexibility enables it to serve as a universal text embedding solution.",
    "summary": "该论文研究如何构建高性能的多语言文本嵌入模型，核心方法是结合公开数据集和LLM生成合成数据的数据混合策略，并通过详细的消融分析优化对比损失实现和模型合并等关键设计选择，创建支持用户指令的通用嵌入解决方案。",
    "translation": "Llama-Embed-Nemotron-8B：面向多语言和跨语言任务的通用文本嵌入模型",
    "relevance_score": 8,
    "reasoning": "该论文专注于文本嵌入模型，这是搜索和推荐系统中的核心技术，用于语义匹配和内容理解。作为通用多语言嵌入模型，它可以直接应用于跨语言搜索、多语言内容推荐以及广告中的多语言语义匹配任务，提升国际化服务的质量。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文开发了通用文本嵌入模型，在检索、分类等核心推荐系统任务中具有直接应用价值，其多语言能力和指令感知特性对搜索和广告领域有重要意义。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.06982v1": {
    "title": "CGLE: Class-label Graph Link Estimator for Link Prediction",
    "url": "https://www.alphaxiv.org/abs/2511.06982v1",
    "arxiv_id": "2511.06982v1",
    "authors": "Ankit Mazumder, Srikanta Bedathur",
    "categories": "cs.SI, cs.IR",
    "pub_date": "2025-11-10 11:34:36",
    "ori_summary": "Link prediction is a pivotal task in graph mining with wide-ranging applications in social networks, recommendation systems, and knowledge graph completion. However, many leading Graph Neural Network (GNN) models often neglect the valuable semantic information aggregated at the class level. To address this limitation, this paper introduces CGLE (Class-label Graph Link Estimator), a novel framework designed to augment GNN-based link prediction models. CGLE operates by constructing a class-conditioned link probability matrix, where each entry represents the probability of a link forming between two node classes. This matrix is derived from either available ground-truth labels or from pseudo-labels obtained through clustering. The resulting class-based prior is then concatenated with the structural link embedding from a backbone GNN, and the combined representation is processed by a Multi-Layer Perceptron (MLP) for the final prediction. Crucially, CGLE's logic is encapsulated in an efficient preprocessing stage, leaving the computational complexity of the underlying GNN model unaffected. We validate our approach through extensive experiments on a broad suite of benchmark datasets, covering both homophilous and sparse heterophilous graphs. The results show that CGLE yields substantial performance gains over strong baselines such as NCN and NCNC, with improvements in HR@100 of over 10 percentage points on homophilous datasets like Pubmed and DBLP. On sparse heterophilous graphs, CGLE delivers an MRR improvement of over 4% on the Chameleon dataset. Our work underscores the efficacy of integrating global, data-driven semantic priors, presenting a compelling alternative to the pursuit of increasingly complex model architectures. Code to reproduce our findings is available at: https://github.com/data-iitd/cgle-icdm2025.",
    "summary": "",
    "translation": "CGLE：用于链接预测的类标签图链接估计器",
    "relevance_score": 2,
    "reasoning": "该论文专注于图神经网络中的链接预测技术，属于通用的图学习领域。虽然链接预测在推荐系统中可能有间接应用（如社交推荐），但该标题未表明任何与推荐系统、搜索或广告的直接关联，也未涉及LLM、Transformer架构或异构数据建模等核心关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06937v1": {
    "title": "Fine-Tuning Diffusion-Based Recommender Systems via Reinforcement Learning with Reward Function Optimization",
    "url": "https://www.alphaxiv.org/abs/2511.06937v1",
    "arxiv_id": "2511.06937v1",
    "authors": "Yu Hou, Hua Li, Ha Young Kim, Won-Yong Shin",
    "categories": "cs.IR, cs.AI, cs.LG, cs.NI, cs.SI",
    "pub_date": "2025-11-10 10:38:16",
    "ori_summary": "Diffusion models recently emerged as a powerful paradigm for recommender systems, offering state-of-the-art performance by modeling the generative process of user-item interactions. However, training such models from scratch is both computationally expensive and yields diminishing returns once convergence is reached. To remedy these challenges, we propose ReFiT, a new framework that integrates Reinforcement learning (RL)-based Fine-Tuning into diffusion-based recommender systems. In contrast to prior RL approaches for diffusion models depending on external reward models, ReFiT adopts a task-aligned design: it formulates the denoising trajectory as a Markov decision process (MDP) and incorporates a collaborative signal-aware reward function that directly reflects recommendation quality. By tightly coupling the MDP structure with this reward signal, ReFiT empowers the RL agent to exploit high-order connectivity for fine-grained optimization, while avoiding the noisy or uninformative feedback common in naive reward designs. Leveraging policy gradient optimization, ReFiT maximizes exact log-likelihood of observed interactions, thereby enabling effective post hoc fine-tuning of diffusion recommenders. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed ReFiT framework (a) exhibits substantial performance gains over strong competitors (up to 36.3% on sequential recommendation), (b) demonstrates strong efficiency with linear complexity in the number of users or items, and (c) generalizes well across multiple diffusion-based recommendation scenarios. The source code and datasets are publicly available at https://anonymous.4open.science/r/ReFiT-4C60.",
    "summary": "论文研究扩散模型推荐系统训练效率低和收敛后收益递减的问题，核心方法是设计基于马尔可夫决策过程的强化学习微调框架，通过协作信号感知的奖励函数直接优化推荐质量。",
    "translation": "基于奖励函数优化的强化学习微调扩散推荐系统",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及推荐系统的核心领域进展，将扩散模型应用于推荐系统，并采用强化学习进行微调。强化学习与奖励函数优化的结合在推荐系统中具有明确的应用价值，可用于优化长期用户参与度和业务指标，属于直接应用于推荐系统的前沿技术。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文将强化学习与扩散模型结合用于推荐系统优化，直接针对推荐系统核心领域，并引入创新的后训练微调方法，与关注点高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.06905v1": {
    "title": "Have We Really Understood Collaborative Information? An Empirical Investigation",
    "url": "https://www.alphaxiv.org/abs/2511.06905v1",
    "arxiv_id": "2511.06905v1",
    "authors": "Xiaokun Zhang, Zhaochun Ren, Bowei He, Ziqiang Cui, Chen Ma",
    "categories": "cs.IR",
    "pub_date": "2025-11-10 10:00:26",
    "ori_summary": "Collaborative information serves as the cornerstone of recommender systems which typically focus on capturing it from user-item interactions to deliver personalized services. However, current understanding of this crucial resource remains limited. Specifically, a quantitative definition of collaborative information is missing, its manifestation within user-item interactions remains unclear, and its impact on recommendation performance is largely unknown. To bridge this gap, this work conducts a systematic investigation of collaborative information. We begin by clarifying collaborative information in terms of item co-occurrence patterns, identifying its main characteristics, and presenting a quantitative definition. We then estimate the distribution of collaborative information from several aspects, shedding light on how collaborative information is structured in practice. Furthermore, we evaluate the impact of collaborative information on the performance of various recommendation algorithms. Finally, we highlight challenges in effectively capturing collaborative information and outlook promising directions for future research. By establishing an empirical framework, we uncover many insightful observations that advance our understanding of collaborative information and offer valuable guidelines for developing more effective recommender systems.",
    "summary": "论文研究推荐系统中协同信息的本质理解问题，核心思想是通过建立实证框架，从物品共现模式角度明确定义协同信息，分析其分布特征，并评估其对推荐算法的影响。",
    "translation": "我们真的理解了协同信息吗？一项实证研究",
    "relevance_score": 8,
    "reasoning": "该论文标题直接指向推荐系统中的核心概念——协同信息（通常指协同过滤），这是推荐系统领域的基础研究。实证调查方法有助于深入理解协同信息的本质和局限性，这对于改进推荐算法具有直接价值。虽然未明确提及LLM或Transformer，但对协同信息的深入理解可以为构建更有效的推荐模型提供理论基础。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文系统研究推荐系统核心基石——协同信息，明确其定义、分布特征及对算法性能的影响，直接对应核心领域进展和推荐系统基础研究。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.06803v1": {
    "title": "Learning to Fast Unrank in Collaborative Filtering Recommendation",
    "url": "https://www.alphaxiv.org/abs/2511.06803v1",
    "arxiv_id": "2511.06803v1",
    "authors": "Junpeng Zhao, Lin Li, Ming Li, Amran Bhuiyan, Jimmy Huang",
    "categories": "cs.IR, cs.AI, cs.LG",
    "pub_date": "2025-11-10 07:45:15",
    "ori_summary": "Modern data-driven recommendation systems risk memorizing sensitive user behavioral patterns, raising privacy concerns. Existing recommendation unlearning methods, while capable of removing target data influence, suffer from inefficient unlearning speed and degraded performance, failing to meet real-time unlearning demands. Considering the ranking-oriented nature of recommendation systems, we present unranking, the process of reducing the ranking positions of target items while ensuring the formal guarantees of recommendation unlearning. To achieve efficient unranking, we propose Learning to Fast Unrank in Collaborative Filtering Recommendation (L2UnRank), which operates through three key stages: (a) identifying the influenced scope via interaction-based p-hop propagation, (b) computing structural and semantic influences for entities within this scope, and (c) performing efficient, ranking-aware parameter updates guided by influence information. Extensive experiments across multiple datasets and backbone models demonstrate L2UnRank's model-agnostic nature, achieving state-of-the-art unranking effectiveness and maintaining recommendation quality comparable to retraining, while also delivering a 50x speedup over existing methods. Codes are available at https://github.com/Juniper42/L2UnRank.",
    "summary": "论文研究推荐系统中保护用户隐私的高效数据遗忘问题，核心方法是通过交互传播识别影响范围、计算结构语义影响、进行排名感知参数更新来实现快速降排。",
    "translation": "协同过滤推荐中的快速反排序学习",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及协同过滤推荐系统的核心算法优化，属于Core Domain Advances范畴。'反排序'技术可能指高效处理大规模候选集或动态调整排序结果，这对推荐系统的实时性和可扩展性具有重要价值。虽然未明确提及LLM，但此类排序优化技术是推荐系统的基础组件。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文针对推荐系统中的隐私保护问题，提出高效的unranking方法，直接属于推荐系统核心领域进展，并涉及实时性能优化。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.06688v1": {
    "title": "Accessibility Gaps in U.S. Government Dashboards for Blind and Low-Vision Residents",
    "url": "https://www.alphaxiv.org/abs/2511.06688v1",
    "arxiv_id": "2511.06688v1",
    "authors": "Chadani Acharya",
    "categories": "cs.HC, cs.CY, cs.DL, cs.IR",
    "pub_date": "2025-11-10 04:20:46",
    "ori_summary": "Public dashboards are now a common way for US government agencies to share high stakes information with residents. We audited six live systems at federal, state, and city levels: CDC respiratory illness, HUD homelessness PIT and HIC, California HCD Annual Progress Report, New York City Mayor's Management Report, Houston Permitting, and Chicago public health and budget dashboards. Using a rubric based on screen reader needs and WCAG, we checked five items: (1) discoverability of key metrics by assistive tech, (2) keyboard access without mouse hover, (3) clear semantic labels for axes, series, and categories, (4) short plain language status and trend notes, and (5) machine readable tables or CSVs that mirror what sighted users see. Findings are mixed. Many charts fail basic discoverability or depend on hover, which blocks keyboard and screen reader use. Plain language summaries are common in CDC and Chicago, but rare in HUD and Houston. Machine readable data is strong for NYC, California, and HUD; it is weaker or unclear for Houston. Several sites promise service for the public or for customers yet do not name accessibility in their descriptions. Across systems we also observe urgency inversion: faster, operational dashboards tend to provide fewer accessible affordances than slower accountability dashboards. These patterns matter for equal participation and for ADA Title II compliance that references WCAG 2.1 AA. We propose three steps for any public dashboard: add a brief status and trend text at the same update cadence, publish a matching table or CSV of the visual metrics, and state an explicit accessibility commitment.",
    "summary": "",
    "translation": "美国面向盲人和低视力居民的政府仪表板可访问性差距",
    "relevance_score": 1,
    "reasoning": "该论文关注政府仪表板的可访问性差距，属于无障碍访问和用户体验领域，与搜索、推荐或广告系统的核心技术进展、LLM技术或Transformer架构无关。虽然可访问性在广义用户体验中很重要，但该主题不涉及排名算法、模型架构或任何可能应用于推荐/搜索系统的技术方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06668v1": {
    "title": "When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in Healthcare",
    "url": "https://www.alphaxiv.org/abs/2511.06668v1",
    "arxiv_id": "2511.06668v1",
    "authors": "Saeedeh Javadi, Sara Mirabi, Manan Gangar, Bahadorreza Ofoghi",
    "categories": "cs.IR, cs.LG",
    "pub_date": "2025-11-10 03:27:54",
    "ori_summary": "In high-stakes information domains such as healthcare, where large language models (LLMs) can produce hallucinations or misinformation, retrieval-augmented generation (RAG) has been proposed as a mitigation strategy, grounding model outputs in external, domain-specific documents. Yet, this approach can introduce errors when source documents contain outdated or contradictory information. This work investigates the performance of five LLMs in generating RAG-based responses to medicine-related queries. Our contributions are three-fold: i) the creation of a benchmark dataset using consumer medicine information documents from the Australian Therapeutic Goods Administration (TGA), where headings are repurposed as natural language questions, ii) the retrieval of PubMed abstracts using TGA headings, stratified across multiple publication years, to enable controlled temporal evaluation of outdated evidence, and iii) a comparative analysis of the frequency and impact of outdated or contradictory content on model-generated responses, assessing how LLMs integrate and reconcile temporally inconsistent information. Our findings show that contradictions between highly similar abstracts do, in fact, degrade performance, leading to inconsistencies and reduced factual accuracy in model answers. These results highlight that retrieval similarity alone is insufficient for reliable medical RAG and underscore the need for contradiction-aware filtering strategies to ensure trustworthy responses in high-stakes domains.",
    "summary": "",
    "translation": "当证据相矛盾时：医疗保健中更安全的检索增强生成方法研究",
    "relevance_score": 2,
    "reasoning": "虽然论文涉及检索增强生成(RAG)技术，但其明确聚焦于医疗保健领域的应用，这属于被明确排除的医疗领域特定应用。论文标题强调医疗保健安全性，与推荐系统、搜索或广告的核心技术进展没有直接关联，且没有显示出对异构数据建模或Transformer架构改进的通用贡献。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06635v1": {
    "title": "Can LLM Annotations Replace User Clicks for Learning to Rank?",
    "url": "https://www.alphaxiv.org/abs/2511.06635v1",
    "arxiv_id": "2511.06635v1",
    "authors": "Lulu Yu, Keping Bi, Jiafeng Guo, Shihao Liu, Shuaiqiang Wang, Dawei Yin, Xueqi Cheng",
    "categories": "cs.IR",
    "pub_date": "2025-11-10 02:26:14",
    "ori_summary": "Large-scale supervised data is essential for training modern ranking models, but obtaining high-quality human annotations is costly. Click data has been widely used as a low-cost alternative, and with recent advances in large language models (LLMs), LLM-based relevance annotation has emerged as another promising annotation. This paper investigates whether LLM annotations can replace click data for learning to rank (LTR) by conducting a comprehensive comparison across multiple dimensions. Experiments on both a public dataset, TianGong-ST, and an industrial dataset, Baidu-Click, show that click-supervised models perform better on high-frequency queries, while LLM annotation-supervised models are more effective on medium- and low-frequency queries. Further analysis shows that click-supervised models are better at capturing document-level signals such as authority or quality, while LLM annotation-supervised models are more effective at modeling semantic matching between queries and documents and at distinguishing relevant from non-relevant documents. Motivated by these observations, we explore two training strategies -- data scheduling and frequency-aware multi-objective learning -- that integrate both supervision signals. Both approaches enhance ranking performance across queries at all frequency levels, with the latter being more effective. Our code is available at https://github.com/Trustworthy-Information-Access/LLMAnn_Click.",
    "summary": "该论文研究LLM标注是否能替代用户点击数据用于学习排序。核心思想是通过对比分析发现点击数据擅长捕捉文档级信号，而LLM标注更擅长语义匹配和相关性区分，并探索融合两种监督信号的训练策略。",
    "translation": "LLM标注能否替代用户点击用于学习排序？",
    "relevance_score": 9,
    "reasoning": "该论文直接探讨LLM在搜索排序系统中的应用，属于'直接LLM应用'范畴。它研究使用LLM生成的标注替代传统用户点击数据来训练排序模型，这直接解决了搜索和推荐系统中的核心排序问题，具有明确的实用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接研究LLM标注在推荐系统排序任务中的应用潜力，与用户点击数据对比分析，并探索融合策略，完全契合LLM在推荐系统中的应用研究重点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.06582v1": {
    "title": "TabRAG: Tabular Document Retrieval via Structured Language Representations",
    "url": "https://www.alphaxiv.org/abs/2511.06582v1",
    "arxiv_id": "2511.06582v1",
    "authors": "Jacob Si, Mike Qu, Michelle Lee, Yingzhen Li",
    "categories": "cs.CL, cs.AI, cs.CV, cs.IR, cs.LG",
    "pub_date": "2025-11-10 00:05:58",
    "ori_summary": "Ingesting data for Retrieval-Augmented Generation (RAG) involves either fine-tuning the embedding model directly on the target corpus or parsing documents for embedding model encoding. The former, while accurate, incurs high computational hardware requirements, while the latter suffers from suboptimal performance when extracting tabular data. In this work, we address the latter by presenting TabRAG, a parsing-based RAG pipeline designed to tackle table-heavy documents via structured language representations. TabRAG outperforms existing popular parsing-based methods for generation and retrieval. Code is available at https://github.com/jacobyhsi/TabRAG.",
    "summary": "该论文研究表格密集型文档在检索增强生成中的解析性能不足问题，核心思想是设计基于结构化语言表示的解析流程来提升表格数据的处理效果。",
    "translation": "TabRAG：基于结构化语言表示的表格式文档检索",
    "relevance_score": 7,
    "reasoning": "该论文涉及结构化文档检索和语言表示，与搜索领域的文档检索和结构化数据处理高度相关。结构化语言表示技术可以应用于搜索系统中的表格数据理解和检索，以及推荐系统中处理结构化用户特征和物品属性。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接解决检索增强生成中表格数据处理的核心挑战，提出了结构化语言表示方法，对搜索和推荐系统中的异构数据处理具有重要参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.07417v1": {
    "title": "Language Generation with Infinite Contamination",
    "url": "https://www.alphaxiv.org/abs/2511.07417v1",
    "arxiv_id": "2511.07417v1",
    "authors": "Anay Mehrotra, Grigoris Velegkas, Xifan Yu, Felix Zhou",
    "categories": "stat.ML, cs.AI, cs.CL, cs.DS, cs.LG",
    "pub_date": "2025-11-10 18:59:39",
    "ori_summary": "We study language generation in the limit, where an algorithm observes an adversarial enumeration of strings from an unknown target language $K$ and must eventually generate new, unseen strings from $K$. Kleinberg and Mullainathan [KM24] proved that generation is achievable in surprisingly general settings. But their generator suffers from ``mode collapse,'' producing from an ever-smaller subset of the target. To address this, Kleinberg and Wei [KW25] require the generator's output to be ``dense'' in the target language. They showed that generation with density, surprisingly, remains achievable at the same generality. Both results assume perfect data: no noisy insertions and no omissions. This raises a central question: how much contamination can generation tolerate? Recent works made partial progress on this question by studying (non-dense) generation with either finite amounts of noise (but no omissions) or omissions (but no noise). We characterize robustness under contaminated enumerations: 1. Generation under Contamination: Language generation in the limit is achievable for all countable collections iff the fraction of contaminated examples converges to zero. When this fails, we characterize which collections are generable. 2. Dense Generation under Contamination: Dense generation is strictly less robust to contamination than generation. As a byproduct, we resolve an open question of Raman and Raman [ICML25] by showing that generation is possible with only membership oracle access under finitely many contaminated examples. Finally, we introduce a beyond-worst-case model inspired by curriculum learning and prove that dense generation is achievable even with infinite contamination provided the fraction of contaminated examples converges to zero. This suggests curriculum learning may be crucial for learning from noisy web data.",
    "summary": "",
    "translation": "无限污染下的语言生成",
    "relevance_score": 2,
    "reasoning": "该标题涉及语言生成中的污染问题，这主要属于LLM训练数据质量和安全性的技术挑战。虽然语言生成是LLM的核心能力，但污染问题更偏向数据安全、模型鲁棒性和伦理范畴，与我的核心关注点（推荐系统、搜索广告中的架构创新和直接应用）关联较弱。没有明确的机制表明这种污染研究能直接应用于提升推荐、搜索或广告系统的性能。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07413v1": {
    "title": "DigiData: Training and Evaluating General-Purpose Mobile Control Agents",
    "url": "https://www.alphaxiv.org/abs/2511.07413v1",
    "arxiv_id": "2511.07413v1",
    "authors": "Yuxuan Sun, Manchen Wang, Shengyi Qian, William R. Wong, Eric Gan, Pierluca D'Oro, Alejandro Castillejo Munoz, Sneha Silwal, Pedro Matias, Nitin Kamra, Satwik Kottur, Nick Raines, Xuanyi Zhao, Joy Chen, Joseph Greer, Andrea Madotto, Allen Bolourchi, James Valori, Kevin Carlberg, Karl Ridgeway, Joseph Tighe",
    "categories": "cs.AI, cs.CL, cs.HC, cs.LG",
    "pub_date": "2025-11-10 18:57:35",
    "ori_summary": "AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions.",
    "summary": "",
    "translation": "DigiData：训练与评估通用移动控制智能体",
    "relevance_score": 2,
    "reasoning": "该论文聚焦于移动控制智能体的训练与评估，属于通用智能体技术范畴，与推荐系统、搜索或广告的核心技术关联度较低。虽然通用智能体技术可能间接影响用户交互界面，但缺乏明确的直接应用场景或技术迁移路径至当前关注的领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07405v1": {
    "title": "SPOT: An Annotated French Corpus and Benchmark for Detecting Critical Interventions in Online Conversations",
    "url": "https://www.alphaxiv.org/abs/2511.07405v1",
    "arxiv_id": "2511.07405v1",
    "authors": "Manon Berriche, Célia Nouri, Chloé Clavel, Jean-Philippe Cointet",
    "categories": "cs.CL, cs.CY",
    "pub_date": "2025-11-10 18:54:40",
    "ori_summary": "We introduce SPOT (Stopping Points in Online Threads), the first annotated corpus translating the sociological concept of stopping point into a reproducible NLP task. Stopping points are ordinary critical interventions that pause or redirect online discussions through a range of forms (irony, subtle doubt or fragmentary arguments) that frameworks like counterspeech or social correction often overlook. We operationalize this concept as a binary classification task and provide reliable annotation guidelines. The corpus contains 43,305 manually annotated French Facebook comments linked to URLs flagged as false information by social media users, enriched with contextual metadata (article, post, parent comment, page or group, and source). We benchmark fine-tuned encoder models (CamemBERT) and instruction-tuned LLMs under various prompting strategies. Results show that fine-tuned encoders outperform prompted LLMs in F1 score by more than 10 percentage points, confirming the importance of supervised learning for emerging non-English social media tasks. Incorporating contextual metadata further improves encoder models F1 scores from 0.75 to 0.78. We release the anonymized dataset, along with the annotation guidelines and code in our code repository, to foster transparency and reproducible research.",
    "summary": "",
    "translation": "SPOT：一个用于检测在线对话中关键干预的标注法语语料库与基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于法语对话中关键干预的检测，这属于对话分析和内容审核领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然在线对话可能出现在某些平台中，但论文关注的是特定语言（法语）的干预检测，缺乏对RecSys/Search/Ads领域的直接应用潜力或技术启发性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07403v1": {
    "title": "SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards",
    "url": "https://www.alphaxiv.org/abs/2511.07403v1",
    "arxiv_id": "2511.07403v1",
    "authors": "Hunar Batra, Haoqin Tu, Hardy Chen, Yuanze Lin, Cihang Xie, Ronald Clark",
    "categories": "cs.CV, cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-11-10 18:52:47",
    "ori_summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning.",
    "summary": "",
    "translation": "SpatialThinker：通过空间奖励在多媒体大语言模型中强化3D推理能力",
    "relevance_score": 2,
    "reasoning": "该论文专注于3D推理和视觉空间理解，属于纯粹的视觉领域研究。虽然提到了多模态LLM，但其核心是3D空间推理，与推荐系统、搜索或广告中的异构数据处理没有直接关联。在推荐/搜索/广告的典型应用中，3D空间推理能力缺乏明确的实用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07397v1": {
    "title": "ConvFill: Model Collaboration for Responsive Conversational Voice Agents",
    "url": "https://www.alphaxiv.org/abs/2511.07397v1",
    "arxiv_id": "2511.07397v1",
    "authors": "Vidya Srinivas, Zachary Englhardt, Maximus Powers, Shwetak Patel, Vikram Iyer",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 18:50:30",
    "ori_summary": "Deploying conversational voice agents with large language models faces a critical challenge: cloud-based foundation models provide deep reasoning and domain knowledge but introduce latency that disrupts natural conversation, while on-device models respond immediately but lack sophistication. We propose conversational infill, a task where a lightweight on-device model generates contextually appropriate dialogue while seamlessly incorporating streaming knowledge from a powerful backend model. This approach decouples response latency from model capability, enabling systems that feel responsive while accessing the full power of large-scale models. We present ConvFill, a 360M parameter model trained on synthetic multi-domain conversations. Evaluation across multiple backend models shows that conversational infill can be successfully learned, with ConvFill achieving accuracy improvements of 36-42% over standalone small models of the same size while consistently retaining sub-200ms response latencies. Our results demonstrate the promise of this approach for building on-device conversational agents that are both immediately responsive and knowledgeable.",
    "summary": "",
    "translation": "ConvFill：面向响应式对话语音智能体的模型协作方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注对话语音智能体和模型协作技术，属于语音交互和对话系统领域。虽然涉及模型协作概念，但与搜索、推荐、广告系统的核心进展或LLM/Transformer基础技术缺乏直接关联，且未明确展示在推荐系统或搜索广告中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07392v1": {
    "title": "Surgical Agent Orchestration Platform for Voice-directed Patient Data Interaction",
    "url": "https://www.alphaxiv.org/abs/2511.07392v1",
    "arxiv_id": "2511.07392v1",
    "authors": "Hyeryun Park, Byung Mo Gu, Jun Hee Lee, Byeong Hyeon Choi, Sekeun Kim, Hyun Koo Kim, Kyungsang Kim",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-11-10 18:47:24",
    "ori_summary": "In da Vinci robotic surgery, surgeons' hands and eyes are fully engaged in the procedure, making it difficult to access and manipulate multimodal patient data without interruption. We propose a voice-directed Surgical Agent Orchestrator Platform (SAOP) built on a hierarchical multi-agent framework, consisting of an orchestration agent and three task-specific agents driven by Large Language Models (LLMs). These LLM-based agents autonomously plan, refine, validate, and reason to map voice commands into specific tasks such as retrieving clinical information, manipulating CT scans, or navigating 3D anatomical models on the surgical video. We also introduce a Multi-level Orchestration Evaluation Metric (MOEM) to comprehensively assess the performance and robustness from command-level and category-level perspectives. The SAOP achieves high accuracy and success rates across 240 voice commands, while LLM-based agents improve robustness against speech recognition errors and diverse or ambiguous free-form commands, demonstrating strong potential to support minimally invasive da Vinci robotic surgery.",
    "summary": "",
    "translation": "用于语音导向患者数据交互的手术代理编排平台",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医疗领域的手术场景和患者数据交互，属于明确的医疗应用范畴。根据用户指定的无关主题，医疗、生物等特定领域应用应被排除，且该标题没有任何迹象表明与推荐系统、搜索、广告或相关使能技术有关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07384v1": {
    "title": "Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence",
    "url": "https://www.alphaxiv.org/abs/2511.07384v1",
    "arxiv_id": "2511.07384v1",
    "authors": "Sean McLeish, Ang Li, John Kirchenbauer, Dayal Singh Kalra, Brian R. Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Jonas Geiping, Tom Goldstein, Micah Goldblum",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-11-10 18:43:07",
    "ori_summary": "Recent advances in depth-recurrent language models show that recurrence can decouple train-time compute and parameter count from test-time compute. In this work, we study how to convert existing pretrained non-recurrent language models into depth-recurrent models. We find that using a curriculum of recurrences to increase the effective depth of the model over the course of training preserves performance while reducing total computational cost. In our experiments, on mathematics, we observe that converting pretrained models to recurrent ones results in better performance at a given compute budget than simply post-training the original non-recurrent language model.",
    "summary": "该论文研究如何将预训练的非循环语言模型转换为深度循环模型以解耦训练和推理计算。核心方法是采用循环课程学习策略，在训练过程中逐步增加模型的有效深度，从而在保持性能的同时降低计算成本。",
    "translation": "通过改造的循环机制教导预训练语言模型进行更深层次的思考",
    "relevance_score": 7,
    "reasoning": "该论文属于'使能LLM技术'范畴，通过引入循环机制增强预训练语言模型的推理深度。这种改进可以直接应用于搜索和推荐系统中的复杂查询理解、多轮对话推荐以及需要深度推理的用户意图建模，提升系统对复杂用户需求的响应能力。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出将预训练语言模型转换为深度循环模型的方法，通过课程学习策略增加有效深度，这属于Transformer架构效率优化和LLM核心技术进步，对推荐和搜索系统的推理效率有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.07382v1": {
    "title": "Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for Bangla-to-Python Code Generation",
    "url": "https://www.alphaxiv.org/abs/2511.07382v1",
    "arxiv_id": "2511.07382v1",
    "authors": "K M Nafi Asib, Sourav Saha, Mohammed Moshiul Hoque",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 18:41:44",
    "ori_summary": "Large Language Models (LLMs) have advanced the automated generation of code from natural language prompts. However, low-resource languages (LRLs) like Bangla remain underrepresented due to the limited availability of instruction-to-code datasets and evaluation benchmarks. To address this, the BLP Workshop at IJCNLP-AACL 2025 introduced a shared task on \"Code Generation in Bangla\". In this work, we propose a method that combines instruction prompting with a test-driven, feedback-guided iterative refinement process using a fine-tuned Qwen2.5-14B model. The model generates code from Bangla instructions, tests it against unit tests, and iteratively refines any failing outputs through three evaluation passes, using test feedback to guide each step. This approach helped our team \"Retriv\" to secure 2nd place in the shared task with a Pass@1 score of 0.934. The analysis highlights challenges in Bangla instruction understanding and Python code generation, emphasizing the need for targeted methods in LRLs. We made experimental scripts publicly available for the community.",
    "summary": "",
    "translation": "Retriv在BLP-2025任务2中的方法：面向孟加拉语到Python代码生成的测试驱动反馈引导框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于特定语言（孟加拉语）的代码生成任务，属于纯粹的NLP应用领域。虽然涉及代码生成技术，但缺乏与推荐系统、搜索或广告领域的直接关联，也没有展示出在这些领域应用的潜力。该研究主要关注编程语言翻译和代码生成，属于纯粹的LLM应用，不符合当前关注的任何技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07380v1": {
    "title": "Selecting Auxiliary Data via Neural Tangent Kernels for Low-Resource Domains",
    "url": "https://www.alphaxiv.org/abs/2511.07380v1",
    "arxiv_id": "2511.07380v1",
    "authors": "Pingjie Wang, Hongcheng Liu, Yusheng Liao, Ziqing Fan, Yaxin Du, Shuo Tang, Yanfeng Wang, Yu Wang",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 18:41:23",
    "ori_summary": "Large language models (LLMs) have achieved remarkable success across widespread tasks, yet their application in low-resource domains remains a significant challenge due to data scarcity and the high risk of overfitting. While in-domain data is limited, there exist vast amounts of similar general-domain data, and our initial findings reveal that they could potentially serve as auxiliary supervision for domain enhancement. This observation leads us to our central research question: \\textbf{\\textit{how to effectively select the most valuable auxiliary data to maximize domain-specific performance}}, particularly when traditional methods are inapplicable due to a lack of large in-domain data pools or validation sets. To address this, we propose \\textbf{NTK-Selector}, a principled and efficient framework for selecting general-domain auxiliary data to enhance domain-specific performance via neural tangent kernels (NTK). Our method tackles two challenges of directly applying NTK to LLMs, theoretical assumptions and prohibitive computational cost, by empirically demonstrating a stable NTK-like behavior in LLMs during LoRA fine-tuning and proposing a Jacobian-free approximation method. Extensive experiments across four low-resource domains (medical, financial, legal, and psychological) demonstrate that NTK-Selector consistently improves downstream performance. Specifically, fine-tuning on 1,000 in-domain samples alone only yielded +0.8 points for Llama3-8B-Instruct and +0.9 points for Qwen3-8B. In contrast, enriching with 9,000 auxiliary samples selected by NTK-Selector led to substantial \\textbf{gains of +8.7 and +5.1 points}, which corresponds to a \\textbf{10.9x and 5.7x improvement} over the domain-only setting.",
    "summary": "",
    "translation": "基于神经正切核为低资源领域选择辅助数据",
    "relevance_score": 2,
    "reasoning": "该论文主要关注低资源领域的数据选择方法，虽然涉及神经网络技术，但核心是数据选择而非模型架构或推荐系统应用。神经正切核理论主要应用于理论分析领域，与推荐系统、搜索或广告的直接关联性较弱，缺乏明确的实际应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07364v1": {
    "title": "Self-Evaluating LLMs for Multi-Step Tasks: Stepwise Confidence Estimation for Failure Detection",
    "url": "https://www.alphaxiv.org/abs/2511.07364v1",
    "arxiv_id": "2511.07364v1",
    "authors": "Vaibhav Mavi, Shubh Jaroria, Weiqi Sun",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-11-10 18:19:51",
    "ori_summary": "Reliability and failure detection of large language models (LLMs) is critical for their deployment in high-stakes, multi-step reasoning tasks. Prior work explores confidence estimation for self-evaluating LLM-scorer systems, with confidence scorers estimating the likelihood of errors in LLM responses. However, most methods focus on single-step outputs and overlook the challenges of multi-step reasoning. In this work, we extend self-evaluation techniques to multi-step tasks, testing two intuitive approaches: holistic scoring and step-by-step scoring. Using two multi-step benchmark datasets, we show that stepwise evaluation generally outperforms holistic scoring in detecting potential errors, with up to 15% relative increase in AUC-ROC. Our findings demonstrate that self-evaluating LLM systems provide meaningful confidence estimates in complex reasoning, improving their trustworthiness and providing a practical framework for failure detection.",
    "summary": "",
    "translation": "用于多步任务的自我评估大语言模型：用于故障检测的逐步置信度估计",
    "relevance_score": 3,
    "reasoning": "该论文主要关注LLM的自我评估和故障检测，属于LLM评估范畴，这在您的关注点中被列为不相关主题。虽然置信度估计在理论上可能应用于搜索或推荐系统的可靠性评估，但论文标题明确聚焦于多步任务和失败检测，这与您的核心关注点（核心领域进展、赋能技术、直接应用）关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07327v1": {
    "title": "IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2511.07327v1",
    "arxiv_id": "2511.07327v1",
    "authors": "Guoxin Chen, Zile Qiao, Xuanzhong Chen, Donglei Yu, Haotian Xu, Wayne Xin Zhao, Ruihua Song, Wenbiao Yin, Huifeng Yin, Liwen Zhang, Kuan Li, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-11-10 17:30:08",
    "ori_summary": "Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that reformulates long-horizon research as a Markov Decision Process with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. We further develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning framework that incentivizes efficient exploration through geometric reward discounting and enables stable distributed training via adaptive downsampling. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5\\% to 42.5\\%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.",
    "summary": "",
    "translation": "IterResearch：通过马尔可夫状态重构重新思考长视野智能体",
    "relevance_score": 2,
    "reasoning": "该论文主要关注长视野智能体和马尔可夫状态重构，属于强化学习领域。虽然强化学习在推荐系统中有所应用，但论文标题没有明确表明与推荐、搜索或广告系统的直接关联，也没有提到Transformer架构或LLM技术。该研究更偏向通用RL方法，缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07322v1": {
    "title": "FinRpt: Dataset, Evaluation System and LLM-based Multi-agent Framework for Equity Research Report Generation",
    "url": "https://www.alphaxiv.org/abs/2511.07322v1",
    "arxiv_id": "2511.07322v1",
    "authors": "Song Jin, Shuqi Li, Shukun Zhang, Rui Yan",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-11-10 17:22:32",
    "ori_summary": "While LLMs have shown great success in financial tasks like stock prediction and question answering, their application in fully automating Equity Research Report generation remains uncharted territory. In this paper, we formulate the Equity Research Report (ERR) Generation task for the first time. To address the data scarcity and the evaluation metrics absence, we present an open-source evaluation benchmark for ERR generation - FinRpt. We frame a Dataset Construction Pipeline that integrates 7 financial data types and produces a high-quality ERR dataset automatically, which could be used for model training and evaluation. We also introduce a comprehensive evaluation system including 11 metrics to assess the generated ERRs. Moreover, we propose a multi-agent framework specifically tailored to address this task, named FinRpt-Gen, and train several LLM-based agents on the proposed datasets using Supervised Fine-Tuning and Reinforcement Learning. Experimental results indicate the data quality and metrics effectiveness of the benchmark FinRpt and the strong performance of FinRpt-Gen, showcasing their potential to drive innovation in the ERR generation field. All code and datasets are publicly available.",
    "summary": "",
    "translation": "FinRpt：用于股权研究报告生成的数据集、评估系统及基于LLM的多智能体框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注金融领域的股权研究报告生成，属于特定领域的内容生成应用。虽然涉及LLM技术，但其应用场景（金融研究）与搜索、推荐、广告系统的核心业务没有直接关联。论文重点在于报告生成而非推荐/搜索/广告中的排名、匹配或用户建模等核心问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07318v1": {
    "title": "When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs",
    "url": "https://www.alphaxiv.org/abs/2511.07318v1",
    "arxiv_id": "2511.07318v1",
    "authors": "Shaowen Wang, Yiqi Dong, Ruinian Chang, Tansheng Zhu, Yuebo Sun, Kaifeng Lyu, Jian Li",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-11-10 17:19:27",
    "ori_summary": "Despite substantial advances, large language models (LLMs) continue to exhibit hallucinations, generating plausible yet incorrect responses. In this paper, we highlight a critical yet previously underexplored class of hallucinations driven by spurious correlations -- superficial but statistically prominent associations between features (e.g., surnames) and attributes (e.g., nationality) present in the training data. We demonstrate that these spurious correlations induce hallucinations that are confidently generated, immune to model scaling, evade current detection methods, and persist even after refusal fine-tuning. Through systematically controlled synthetic experiments and empirical evaluations on state-of-the-art open-source and proprietary LLMs (including GPT-5), we show that existing hallucination detection methods, such as confidence-based filtering and inner-state probing, fundamentally fail in the presence of spurious correlations. Our theoretical analysis further elucidates why these statistical biases intrinsically undermine confidence-based detection techniques. Our findings thus emphasize the urgent need for new approaches explicitly designed to address hallucinations caused by spurious correlations.",
    "summary": "",
    "translation": "当偏见伪装成真相：伪相关性如何破坏大语言模型中的幻觉检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM中的幻觉检测和偏见问题，这属于纯粹的NLP评估基准范畴，与推荐系统、搜索或广告的核心技术进展无关。虽然标题提到LLMs，但内容聚焦于幻觉检测这一非技术性评估问题，没有涉及任何可能应用于RecSys/Search/Ads的架构改进或建模技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07317v1": {
    "title": "RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments",
    "url": "https://www.alphaxiv.org/abs/2511.07317v1",
    "arxiv_id": "2511.07317v1",
    "authors": "Zhiyuan Zeng, Hamish Ivison, Yiping Wang, Lifan Yuan, Shuyue Stella Li, Zhuorui Ye, Siting Li, Jacqueline He, Runlong Zhou, Tong Chen, Chenyang Zhao, Yulia Tsvetkov, Simon Shaolei Du, Natasha Jaques, Hao Peng, Pang Wei Koh, Hannaneh Hajishirzi",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-11-10 17:18:35",
    "ori_summary": "We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). RLVE enables each verifiable environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses. In contrast, static data distributions often lead to vanishing learning signals when problems are either too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a large-scale suite of 400 verifiable environments carefully developed through manual environment engineering. Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs. By comparison, continuing this LM's original RL training yields only a 0.49% average absolute gain despite using over 3x more compute. We release our code publicly.",
    "summary": "",
    "translation": "RLVE：通过自适应可验证环境扩展语言模型的强化学习规模",
    "relevance_score": 3,
    "reasoning": "该论文主要关注强化学习在语言模型中的扩展技术，虽然强化学习本身可能应用于推荐系统或搜索的某些场景，但论文标题明确聚焦于语言模型的强化学习扩展，属于纯粹的LLM技术范畴。对于推荐/搜索/广告领域的直接应用潜力有限，因为核心关注点是语言模型的训练方法而非具体的应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07311v1": {
    "title": "ACE-ICD: Acronym Expansion As Data Augmentation For Automated ICD Coding",
    "url": "https://www.alphaxiv.org/abs/2511.07311v1",
    "arxiv_id": "2511.07311v1",
    "authors": "Tuan-Dung Le, Shohreh Haddadan, Thanh Q. Thieu",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 17:11:20",
    "ori_summary": "Automatic ICD coding, the task of assigning disease and procedure codes to electronic medical records, is crucial for clinical documentation and billing. While existing methods primarily enhance model understanding of code hierarchies and synonyms, they often overlook the pervasive use of medical acronyms in clinical notes, a key factor in ICD code inference. To address this gap, we propose a novel effective data augmentation technique that leverages large language models to expand medical acronyms, allowing models to be trained on their full form representations. Moreover, we incorporate consistency training to regularize predictions by enforcing agreement between the original and augmented documents. Extensive experiments on the MIMIC-III dataset demonstrate that our approach, ACE-ICD establishes new state-of-the-art performance across multiple settings, including common codes, rare codes, and full-code assignments. Our code is publicly available.",
    "summary": "",
    "translation": "ACE-ICD：将缩略语扩展作为自动化ICD编码的数据增强方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域的ICD编码自动化，属于医疗生物信息学应用，与推荐系统、搜索或广告的核心技术领域无关。虽然涉及数据增强技术，但其特定应用于医疗编码场景，没有明显的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07304v1": {
    "title": "Retriv at BLP-2025 Task 1: A Transformer Ensemble and Multi-Task Learning Approach for Bangla Hate Speech Identification",
    "url": "https://www.alphaxiv.org/abs/2511.07304v1",
    "arxiv_id": "2511.07304v1",
    "authors": "Sourav Saha, K M Nafi Asib, Mohammed Moshiul Hoque",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 17:07:09",
    "ori_summary": "This paper addresses the problem of Bangla hate speech identification, a socially impactful yet linguistically challenging task. As part of the \"Bangla Multi-task Hate Speech Identification\" shared task at the BLP Workshop, IJCNLP-AACL 2025, our team \"Retriv\" participated in all three subtasks: (1A) hate type classification, (1B) target group identification, and (1C) joint detection of type, severity, and target. For subtasks 1A and 1B, we employed a soft-voting ensemble of transformer models (BanglaBERT, MuRIL, IndicBERTv2). For subtask 1C, we trained three multitask variants and aggregated their predictions through a weighted voting ensemble. Our systems achieved micro-f1 scores of 72.75% (1A) and 72.69% (1B), and a weighted micro-f1 score of 72.62% (1C). On the shared task leaderboard, these corresponded to 9th, 10th, and 7th positions, respectively. These results highlight the promise of transformer ensembles and weighted multitask frameworks for advancing Bangla hate speech detection in low-resource contexts. We made experimental scripts publicly available for the community.",
    "summary": "",
    "translation": "Retriv在BLP-2025任务1中的方法：基于Transformer集成和多任务学习的孟加拉语仇恨言论识别",
    "relevance_score": 1,
    "reasoning": "该论文专注于仇恨言论识别这一特定NLP任务，属于内容安全领域，与推荐系统、搜索或广告的核心技术无关。虽然使用了Transformer架构，但应用场景是孟加拉语的内容审核，没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07296v1": {
    "title": "Who Is the Story About? Protagonist Entity Recognition in News",
    "url": "https://www.alphaxiv.org/abs/2511.07296v1",
    "arxiv_id": "2511.07296v1",
    "authors": "Jorge Gabín, M. Eduardo Ares, Javier Parapar",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 16:53:45",
    "ori_summary": "News articles often reference numerous organizations, but traditional Named Entity Recognition (NER) treats all mentions equally, obscuring which entities genuinely drive the narrative. This limits downstream tasks that rely on understanding event salience, influence, or narrative focus. We introduce Protagonist Entity Recognition (PER), a task that identifies the organizations that anchor a news story and shape its main developments. To validate PER, we compare he predictions of Large Language Models (LLMs) against annotations from four expert annotators over a gold corpus, establishing both inter-annotator consistency and human-LLM agreement. Leveraging these findings, we use state-of-the-art LLMs to automatically label large-scale news collections through NER-guided prompting, generating scalable, high-quality supervision. We then evaluate whether other LLMs, given reduced context and without explicit candidate guidance, can still infer the correct protagonists. Our results demonstrate that PER is a feasible and meaningful extension to narrative-centered information extraction, and that guided LLMs can approximate human judgments of narrative importance at scale.",
    "summary": "",
    "translation": "新闻中的主角实体识别：谁是故事的主角？",
    "relevance_score": 3,
    "reasoning": "该论文聚焦于新闻文本中的主角实体识别，属于信息提取任务。虽然实体识别技术可以应用于搜索系统中的文档理解，但该研究主要针对新闻领域的具体场景，与推荐系统、广告或核心LLM技术的直接关联性较弱，潜在应用较为有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07237v1": {
    "title": "The Few Govern the Many:Unveiling Few-Layer Dominance for Time Series Models",
    "url": "https://www.alphaxiv.org/abs/2511.07237v1",
    "arxiv_id": "2511.07237v1",
    "authors": "Xin Qiu, Junlong Tong, Yirong Sun, Yunpu Ma, Xiaoyu Shen",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-11-10 15:53:29",
    "ori_summary": "Large-scale models are at the forefront of time series (TS) forecasting, dominated by two paradigms: fine-tuning text-based Large Language Models (LLM4TS) and training Time Series Foundation Models (TSFMs) from scratch. Both approaches share a foundational assumption that scaling up model capacity and data volume leads to improved performance. However, we observe a \\textit{\\textbf{scaling paradox}} in TS models, revealing a puzzling phenomenon that larger models do \\emph{NOT} achieve better performance. Through extensive experiments on two model families across four scales (100M to 1.7B parameters) and diverse data (up to 6B observations), we rigorously confirm that the scaling paradox is a pervasive issue. We then diagnose its root cause by analyzing internal representations, identifying a phenomenon we call \\textit{few-layer dominance}: only a small subset of layers are functionally important, while the majority are redundant, under-utilized, and can even distract training. Based on this discovery, we propose a practical method to automatically identify and retain only these dominant layers. In our models, retaining only 21\\% of the parameters achieves up to a 12\\% accuracy improvement and a 2.7$\\times$ inference speedup. We validate the universality of our method on 8 prominent SOTA models (LLM4TS and TSFMs, 90M to 6B), showing that retaining less than 30\\% of layers achieves comparable or superior accuracy in over 95\\% of tasks.",
    "summary": "",
    "translation": "少数层主导多数：揭示时间序列模型中的少数层主导现象",
    "relevance_score": 2,
    "reasoning": "该论文主要关注时间序列模型的架构效率问题，虽然涉及模型效率这一通用主题，但时间序列建模与推荐系统、搜索或广告的核心关注点关联较弱。论文可能探讨层数减少对性能的影响，但这种效率改进在推荐/搜索/广告领域的直接应用潜力有限，因为这些系统通常处理的是用户-物品交互、查询-文档匹配等不同类型的数据模式。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07230v1": {
    "title": "Discourse Graph Guided Document Translation with Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2511.07230v1",
    "arxiv_id": "2511.07230v1",
    "authors": "Viet-Thanh Pham, Minghan Wang, Hao-Han Liao, Thuy-Trang Vu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-11-10 15:48:01",
    "ori_summary": "Adapting large language models to full document translation remains challenging due to the difficulty of capturing long-range dependencies and preserving discourse coherence throughout extended texts. While recent agentic machine translation systems mitigate context window constraints through multi-agent orchestration and persistent memory, they require substantial computational resources and are sensitive to memory retrieval strategies. We introduce TransGraph, a discourse-guided framework that explicitly models inter-chunk relationships through structured discourse graphs and selectively conditions each translation segment on relevant graph neighbourhoods rather than relying on sequential or exhaustive context. Across three document-level MT benchmarks spanning six languages and diverse domains, TransGraph consistently surpasses strong baselines in translation quality and terminology consistency while incurring significantly lower token overhead.",
    "summary": "",
    "translation": "基于大语言模型的语篇图引导文档翻译",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文档翻译任务，属于纯粹的NLP应用领域，与推荐系统、搜索或广告的核心技术进展无关。虽然涉及大语言模型，但其应用场景（翻译）和核心技术（语篇图）在RecSys/Search/Ads领域缺乏直接的应用潜力，无法为这些领域提供核心算法或架构上的改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07193v1": {
    "title": "EMODIS: A Benchmark for Context-Dependent Emoji Disambiguation in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2511.07193v1",
    "arxiv_id": "2511.07193v1",
    "authors": "Jiacheng Huang, Ning Yu, Xiaoyin Yi",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 15:24:01",
    "ori_summary": "Large language models (LLMs) are increasingly deployed in real-world communication settings, yet their ability to resolve context-dependent ambiguity remains underexplored. In this work, we present EMODIS, a new benchmark for evaluating LLMs' capacity to interpret ambiguous emoji expressions under minimal but contrastive textual contexts. Each instance in EMODIS comprises an ambiguous sentence containing an emoji, two distinct disambiguating contexts that lead to divergent interpretations, and a specific question that requires contextual reasoning. We evaluate both open-source and API-based LLMs, and find that even the strongest models frequently fail to distinguish meanings when only subtle contextual cues are present. Further analysis reveals systematic biases toward dominant interpretations and limited sensitivity to pragmatic contrast. EMODIS provides a rigorous testbed for assessing contextual disambiguation, and highlights the gap in semantic reasoning between humans and LLMs.",
    "summary": "",
    "translation": "Emodis：大型语言模型中上下文相关表情符号消歧的基准",
    "relevance_score": 2,
    "reasoning": "该论文主要关注表情符号消歧的基准测试，这属于纯粹的NLP评估任务，与推荐系统、搜索或广告的核心技术进展无关。虽然涉及LLM评估，但缺乏明确的RecSys/Search/Ads应用场景，表情符号理解在这些领域的实际应用价值有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07176v1": {
    "title": "Graph Representation-based Model Poisoning on the Heterogeneous Internet of Agents",
    "url": "https://www.alphaxiv.org/abs/2511.07176v1",
    "arxiv_id": "2511.07176v1",
    "authors": "Hanlin Cai, Houtianfu Wang, Haofan Dong, Kai Li, Ozgur B. Akan",
    "categories": "cs.NI, cs.CL",
    "pub_date": "2025-11-10 15:06:26",
    "ori_summary": "Internet of Agents (IoA) envisions a unified, agent-centric paradigm where heterogeneous large language model (LLM) agents can interconnect and collaborate at scale. Within this paradigm, federated learning (FL) serves as a key enabler that allows distributed LLM agents to co-train global models without centralizing data. However, the FL-enabled IoA system remains vulnerable to model poisoning attacks, and the prevailing distance and similarity-based defenses become fragile at billion-parameter scale and under heterogeneous data distributions. This paper proposes a graph representation-based model poisoning (GRMP) attack, which passively exploits observed benign local models to construct a parameter correlation graph and extends an adversarial variational graph autoencoder to capture and reshape higher-order dependencies. The GRMP attack synthesizes malicious local models that preserve benign-like statistics while embedding adversarial objectives, remaining elusive to detection at the server. Experiments demonstrate a gradual drop in system accuracy under the proposed attack and the ineffectiveness of the prevailing defense mechanism in detecting the attack, underscoring a severe threat to the ambitious IoA paradigm.",
    "summary": "",
    "translation": "基于图表示的异构智能体互联网模型投毒",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及模型投毒（安全攻击）和图表示学习，属于安全隐私领域，这被明确列为不相关主题。虽然提到了异构数据和图表示，但核心焦点是安全攻击而非推荐系统、搜索或广告的技术进步。没有证据表明该工作对推荐系统、搜索或广告有直接应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07166v1": {
    "title": "AdaRec: Adaptive Recommendation with LLMs via Narrative Profiling and Dual-Channel Reasoning",
    "url": "https://www.alphaxiv.org/abs/2511.07166v1",
    "arxiv_id": "2511.07166v1",
    "authors": "Meiyun Wang, Charin Polpanumas",
    "categories": "cs.CL, cs.AI, cs.CE",
    "pub_date": "2025-11-10 14:59:27",
    "ori_summary": "We propose AdaRec, a few-shot in-context learning framework that leverages large language models for an adaptive personalized recommendation. AdaRec introduces narrative profiling, transforming user-item interactions into natural language representations to enable unified task handling and enhance human readability. Centered on a bivariate reasoning paradigm, AdaRec employs a dual-channel architecture that integrates horizontal behavioral alignment, discovering peer-driven patterns, with vertical causal attribution, highlighting decisive factors behind user preferences. Unlike existing LLM-based approaches, AdaRec eliminates manual feature engineering through semantic representations and supports rapid cross-task adaptation with minimal supervision. Experiments on real ecommerce datasets demonstrate that AdaRec outperforms both machine learning models and LLM-based baselines by up to eight percent in few-shot settings. In zero-shot scenarios, it achieves up to a nineteen percent improvement over expert-crafted profiling, showing effectiveness for long-tail personalization with minimal interaction data. Furthermore, lightweight fine-tuning on synthetic data generated by AdaRec matches the performance of fully fine-tuned models, highlighting its efficiency and generalization across diverse tasks.",
    "summary": "论文研究如何利用大语言模型实现自适应个性化推荐，核心思想是通过叙事画像将用户-物品交互转化为自然语言表示，并采用双通道架构结合横向行为对齐和纵向因果归因进行推理。",
    "translation": "AdaRec：基于叙事画像和双通道推理的LLM自适应推荐",
    "relevance_score": 9,
    "reasoning": "该论文直接应用LLM技术进行推荐系统设计，属于'直接LLM应用'范畴。通过叙事画像和双通道推理实现自适应推荐，展示了LLM在个性化推荐中的创新应用。这种方法有潜力显著提升推荐系统的理解能力和推理质量。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用LLM技术于推荐系统，提出叙事画像和双通道推理方法，完美契合LLM在推荐领域的应用研究方向。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.07162v1": {
    "title": "Categorical Emotions or Appraisals - Which Emotion Model Explains Argument Convincingness Better?",
    "url": "https://www.alphaxiv.org/abs/2511.07162v1",
    "arxiv_id": "2511.07162v1",
    "authors": "Lynn Greschner, Meike Bauer, Sabine Weber, Roman Klinger",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 14:53:04",
    "ori_summary": "The convincingness of an argument does not only depend on its structure (logos), the person who makes the argument (ethos), but also on the emotion that it causes in the recipient (pathos). While the overall intensity and categorical values of emotions in arguments have received considerable attention in the research community, we argue that the emotion an argument evokes in a recipient is subjective. It depends on the recipient's goals, standards, prior knowledge, and stance. Appraisal theories lend themselves as a link between the subjective cognitive assessment of events and emotions. They have been used in event-centric emotion analysis, but their suitability for assessing argument convincingness remains unexplored. In this paper, we evaluate whether appraisal theories are suitable for emotion analysis in arguments by considering subjective cognitive evaluations of the importance and impact of an argument on its receiver. Based on the annotations in the recently published ContArgA corpus, we perform zero-shot prompting experiments to evaluate the importance of gold-annotated and predicted emotions and appraisals for the assessment of the subjective convincingness labels. We find that, while categorical emotion information does improve convincingness prediction, the improvement is more pronounced with appraisals. This work presents the first systematic comparison between emotion models for convincingness prediction, demonstrating the advantage of appraisals, providing insights for theoretical and practical applications in computational argumentation.",
    "summary": "",
    "translation": "分类情感与评价——哪种情感模型能更好地解释论点的说服力？",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于情感模型与论点说服力的关系，属于心理学和自然语言处理的情感分析领域。这与搜索、推荐或广告系统中的核心技术进步、LLM赋能技术或Transformer架构改进均无直接关联，也不涉及异构数据的统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07148v1": {
    "title": "TCM-Eval: An Expert-Level Dynamic and Extensible Benchmark for Traditional Chinese Medicine",
    "url": "https://www.alphaxiv.org/abs/2511.07148v1",
    "arxiv_id": "2511.07148v1",
    "authors": "Zihao Cheng, Yuheng Lu, Huaiqian Ye, Zeming Liu, Minqi Wang, Jingjing Liu, Zihan Li, Wei Fan, Yuanfang Guo, Ruiji Fu, Shifeng She, Gang Wang, Yunhong Wang",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 14:35:25",
    "ori_summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in modern medicine, yet their application in Traditional Chinese Medicine (TCM) remains severely limited by the absence of standardized benchmarks and the scarcity of high-quality training data. To address these challenges, we introduce TCM-Eval, the first dynamic and extensible benchmark for TCM, meticulously curated from national medical licensing examinations and validated by TCM experts. Furthermore, we construct a large-scale training corpus and propose Self-Iterative Chain-of-Thought Enhancement (SI-CoTE) to autonomously enrich question-answer pairs with validated reasoning chains through rejection sampling, establishing a virtuous cycle of data and model co-evolution. Using this enriched training data, we develop ZhiMingTang (ZMT), a state-of-the-art LLM specifically designed for TCM, which significantly exceeds the passing threshold for human practitioners. To encourage future research and development, we release a public leaderboard, fostering community engagement and continuous improvement.",
    "summary": "",
    "translation": "TCM-Eval：一个专家级的动态可扩展传统中医评估基准",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向传统中医领域的评估基准，属于医学/生物学特定领域应用，与推荐系统、搜索、广告或LLM技术完全无关。根据用户明确的无关主题列表，医学和生物学领域应用被明确排除在外，因此该论文完全不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07129v1": {
    "title": "LoRA on the Go: Instance-level Dynamic LoRA Selection and Merging",
    "url": "https://www.alphaxiv.org/abs/2511.07129v1",
    "arxiv_id": "2511.07129v1",
    "authors": "Seungeon Lee, Soumi Das, Manish Gupta, Krishna P. Gummadi",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-11-10 14:13:10",
    "ori_summary": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient approach for fine-tuning large language models.However, conventional LoRA adapters are typically trained for a single task, limiting their applicability in real-world settings where inputs may span diverse and unpredictable domains. At inference time, existing approaches combine multiple LoRAs for improving performance on diverse tasks, while usually requiring labeled data or additional task-specific training, which is expensive at scale. In this work, we introduce LoRA on the Go (LoGo), a training-free framework that dynamically selects and merges adapters at the instance level without any additional requirements. LoGo leverages signals extracted from a single forward pass through LoRA adapters, to identify the most relevant adapters and determine their contributions on-the-fly. Across 5 NLP benchmarks, 27 datasets, and 3 model families, LoGo outperforms training-based baselines on some tasks upto a margin of 3.6% while remaining competitive on other tasks and maintaining inference throughput, highlighting its effectiveness and practicality.",
    "summary": "论文研究单一LoRA适配器难以应对多样化推理任务的问题，核心思想是通过单次前向传播提取信号，实现无需额外训练的实例级动态适配器选择与融合。",
    "translation": "LoRA在路上：实例级动态LoRA选择与合并",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能LLM技术'范畴，专注于LoRA（低秩适应）的动态实例级选择与合并，这是参数高效微调的核心进展。在推荐系统、搜索和广告中，这种方法可以实现针对不同用户、查询或上下文动态选择和组合专家模型，显著提升个性化效果和模型适应性，同时保持高效推理。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出无需训练的动态LoRA选择与融合框架，直接提升LLM在多样化任务上的推理能力，与推荐和搜索系统中处理异构用户需求的核心挑战高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.07124v1": {
    "title": "Think Consistently, Reason Efficiently: Energy-Based Calibration for Implicit Chain-of-Thought",
    "url": "https://www.alphaxiv.org/abs/2511.07124v1",
    "arxiv_id": "2511.07124v1",
    "authors": "Zhikang Chen, Sen Cui, Deheng Ye, Yu Zhang, Yatao Bian, Tingting Zhu",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-11-10 14:10:58",
    "ori_summary": "Large Language Models (LLMs) have demonstrated strong reasoning capabilities through \\emph{Chain-of-Thought} (CoT) prompting, which enables step-by-step intermediate reasoning. However, explicit CoT methods rely on discrete token-level reasoning processes that are prone to error propagation and limited by vocabulary expressiveness, often resulting in rigid and inconsistent reasoning trajectories. Recent research has explored implicit or continuous reasoning in latent spaces, allowing models to perform internal reasoning before generating explicit output. Although such approaches alleviate some limitations of discrete CoT, they generally lack explicit mechanisms to enforce consistency among reasoning steps, leading to divergent reasoning paths and unstable outcomes. To address this issue, we propose EBM-CoT, an Energy-Based Chain-of-Thought Calibration framework that refines latent thought representations through an energy-based model (EBM). Our method dynamically adjusts latent reasoning trajectories toward lower-energy, high-consistency regions in the embedding space, improving both reasoning accuracy and consistency without modifying the base language model. Extensive experiments across mathematical, commonsense, and symbolic reasoning benchmarks demonstrate that the proposed framework significantly enhances the consistency and efficiency of multi-step reasoning in LLMs.",
    "summary": "论文研究隐式思维链推理中的一致性问题，核心思想是使用能量模型在潜在空间中动态调整推理轨迹，使其收敛到高一致性区域。",
    "translation": "思考一致，推理高效：基于能量的隐式思维链校准",
    "relevance_score": 7,
    "reasoning": "该论文聚焦于思维链推理的校准技术，属于核心LLM技术进步。在搜索和推荐系统中，高效的推理校准可以显著提升复杂查询理解和多步骤推荐决策的准确性与一致性，这对于处理用户复杂意图和多轮交互场景具有直接应用价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出基于能量的隐式思维链校准方法，直接改进LLM推理过程的一致性和效率，对搜索和推荐系统中的复杂推理任务具有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.07112v1": {
    "title": "More Agents Helps but Adversarial Robustness Gap Persists",
    "url": "https://www.alphaxiv.org/abs/2511.07112v1",
    "arxiv_id": "2511.07112v1",
    "authors": "Khashayar Alavi, Zhastay Yeltay, Lucie Flek, Akbar Karimi",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-11-10 13:58:17",
    "ori_summary": "When LLM agents work together, they seem to be more powerful than a single LLM in mathematical question answering. However, are they also more robust to adversarial inputs? We investigate this question using adversarially perturbed math questions. These perturbations include punctuation noise with three intensities (10, 30, and 50 percent), plus real-world and human-like typos (WikiTypo, R2ATA). Using a unified sampling-and-voting framework (Agent Forest), we evaluate six open-source models (Qwen3-4B/14B, Llama3.1-8B, Mistral-7B, Gemma3-4B/12B) across four benchmarks (GSM8K, MATH, MMLU-Math, MultiArith), with various numbers of agents n from one to 25 (1, 2, 5, 10, 15, 20, 25). Our findings show that (1) Noise type matters: punctuation noise harm scales with its severity, and the human typos remain the dominant bottleneck, yielding the largest gaps to Clean accuracy and the highest ASR even with a large number of agents. And (2) Collaboration reliably improves accuracy as the number of agents, n, increases, with the largest gains from one to five agents and diminishing returns beyond 10 agents. However, the adversarial robustness gap persists regardless of the agent count.",
    "summary": "",
    "translation": "更多智能体有助于提升性能，但对抗鲁棒性差距持续存在",
    "relevance_score": 2,
    "reasoning": "该论文标题主要涉及多智能体系统和对抗鲁棒性研究，属于通用机器学习安全领域。虽然对抗鲁棒性在推荐系统中可能有间接应用（如防御对抗攻击），但论文没有明确指向RecSys/Search/Ads领域的特定问题，且多智能体系统的焦点与当前关注的核心技术方向关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07107v1": {
    "title": "MENTOR: A Metacognition-Driven Self-Evolution Framework for Uncovering and Mitigating Implicit Risks in LLMs on Domain Tasks",
    "url": "https://www.alphaxiv.org/abs/2511.07107v1",
    "arxiv_id": "2511.07107v1",
    "authors": "Liang Shan, Kaicheng Shen, Wen Wu, Zhenyu Ying, Chaochao Lu, Guangze Ye, Liang He",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-11-10 13:51:51",
    "ori_summary": "Ensuring the safety and value alignment of large language models (LLMs) is critical for their deployment. Current alignment efforts primarily target explicit risks such as bias, hate speech, and violence. However, they often fail to address deeper, domain-specific implicit risks and lack a flexible, generalizable framework applicable across diverse specialized fields. Hence, we proposed MENTOR: A MEtacognition-driveN self-evoluTion framework for uncOvering and mitigating implicit Risks in LLMs on Domain Tasks. To address the limitations of labor-intensive human evaluation, we introduce a novel metacognitive self-assessment tool. This enables LLMs to reflect on potential value misalignments in their responses using strategies like perspective-taking and consequential thinking. We also release a supporting dataset of 9,000 risk queries spanning education, finance, and management to enhance domain-specific risk identification. Subsequently, based on the outcomes of metacognitive reflection, the framework dynamically generates supplementary rule knowledge graphs that extend predefined static rule trees. This enables models to actively apply validated rules to future similar challenges, establishing a continuous self-evolution cycle that enhances generalization by reducing maintenance costs and inflexibility of static systems. Finally, we employ activation steering during inference to guide LLMs in following the rules, a cost-effective method to robustly enhance enforcement across diverse contexts. Experimental results show MENTOR's effectiveness: In defensive testing across three vertical domains, the framework substantially reduces semantic attack success rates, enabling a new level of implicit risk mitigation for LLMs. Furthermore, metacognitive assessment not only aligns closely with baseline human evaluators but also delivers more thorough and insightful analysis of LLMs value alignment.",
    "summary": "",
    "translation": "MENTOR：一种元认知驱动的自进化框架，用于在领域任务中揭示和缓解大语言模型的隐含风险",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM的安全性和风险缓解，这属于模型安全范畴而非核心推荐系统、搜索或广告的技术进展。虽然提到了领域任务，但焦点在于风险识别而非实际应用改进，与当前关注的LLM效率、架构创新或直接应用无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07080v1": {
    "title": "Wasm: A Pipeline for Constructing Structured Arabic Interleaved Multimodal Corpora",
    "url": "https://www.alphaxiv.org/abs/2511.07080v1",
    "arxiv_id": "2511.07080v1",
    "authors": "Khalil Hennara, Ahmad Bastati, Muhammad Hreden, Mohamed Motasim Hamed, Zeina Aldallal, Sara Chrouf, Safwan AlModhayan",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-11-10 13:10:31",
    "ori_summary": "The performance of large language models (LLMs) and large multimodal models (LMMs) depends heavily on the quality and scale of their pre-training datasets. Recent research shows that large multimodal models trained on natural documents where images and text are interleaved outperform those trained only on image-text pairs across a wide range of benchmarks, leveraging advanced pre- trained models to enforce semantic alignment, image-sequence consistency, and textual coherence. For Arabic, however, the lack of high-quality multimodal datasets that preserve document structure has limited progress. In this paper, we present our pipeline Wasm for processing the Common Crawl dataset to create a new Arabic multimodal dataset that uniquely provides markdown output. Unlike existing Arabic corpora that focus solely on text extraction, our approach preserves the structural integrity of web content while maintaining flexibility for both text-only and multimodal pre-training scenarios. We provide a comprehensive comparative analysis of our data processing pipeline against those used for major existing datasets, highlighting the convergences in filtering strategies and justifying our specific design choices. To support future research, we publicly release a representative dataset dump along with the multimodal processing pipeline for Arabic.",
    "summary": "",
    "translation": "Wasm：构建结构化阿拉伯语交错多模态语料库的流水线",
    "relevance_score": 1,
    "reasoning": "该论文专注于阿拉伯语多模态语料库构建，属于特定语言的数据工程工作。虽然涉及多模态概念，但主要关注阿拉伯语这一特定语言的数据处理，与推荐系统、搜索或广告的核心技术进展没有直接关联。该工作缺乏明确的Transformer架构改进或LLM技术应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07077v1": {
    "title": "EmoBang: Detecting Emotion From Bengali Texts",
    "url": "https://www.alphaxiv.org/abs/2511.07077v1",
    "arxiv_id": "2511.07077v1",
    "authors": "Abdullah Al Maruf, Aditi Golder, Zakaria Masud Jiyad, Abdullah Al Numan, Tarannum Shaila Zaman",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 13:07:40",
    "ori_summary": "Emotion detection from text seeks to identify an individual's emotional or mental state - positive, negative, or neutral - based on linguistic cues. While significant progress has been made for English and other high-resource languages, Bengali remains underexplored despite being the world's fourth most spoken language. The lack of large, standardized datasets classifies Bengali as a low-resource language for emotion detection. Existing studies mainly employ classical machine learning models with traditional feature engineering, yielding limited performance. In this paper, we introduce a new Bengali emotion dataset annotated across eight emotion categories and propose two models for automatic emotion detection: (i) a hybrid Convolutional Recurrent Neural Network (CRNN) model (EmoBangHybrid) and (ii) an AdaBoost-Bidirectional Encoder Representations from Transformers (BERT) ensemble model (EmoBangEnsemble). Additionally, we evaluate six baseline models with five feature engineering techniques and assess zero-shot and few-shot large language models (LLMs) on the dataset. To the best of our knowledge, this is the first comprehensive benchmark for Bengali emotion detection. Experimental results show that EmoBangH and EmoBangE achieve accuracies of 92.86% and 93.69%, respectively, outperforming existing methods and establishing strong baselines for future research.",
    "summary": "",
    "translation": "EmoBang：从孟加拉语文本中检测情感",
    "relevance_score": 1,
    "reasoning": "该论文专注于孟加拉语的情感检测，属于特定语言的NLP任务，与推荐系统、搜索或广告的核心技术焦点无关。情感分析虽然在某些场景下可能作为辅助特征，但该论文没有展示与推荐/搜索/广告系统的直接关联或潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07074v1": {
    "title": "Importance-Aware Data Selection for Efficient LLM Instruction Tuning",
    "url": "https://www.alphaxiv.org/abs/2511.07074v1",
    "arxiv_id": "2511.07074v1",
    "authors": "Tingyu Jiang, Shen Li, Yiyao Song, Lan Zhang, Hualei Zhu, Yuan Zhao, Xiaohang Xu, Kenjiro Taura, Hao Henry Wang",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 13:06:30",
    "ori_summary": "Instruction tuning plays a critical role in enhancing the performance and efficiency of Large Language Models (LLMs). Its success depends not only on the quality of the instruction data but also on the inherent capabilities of the LLM itself. Some studies suggest that even a small amount of high-quality data can achieve instruction fine-tuning results that are on par with, or even exceed, those from using a full-scale dataset. However, rather than focusing solely on calculating data quality scores to evaluate instruction data, there is a growing need to select high-quality data that maximally enhances the performance of instruction tuning for a given LLM. In this paper, we propose the Model Instruction Weakness Value (MIWV) as a novel metric to quantify the importance of instruction data in enhancing model's capabilities. The MIWV metric is derived from the discrepancies in the model's responses when using In-Context Learning (ICL), helping identify the most beneficial data for enhancing instruction tuning performance. Our experimental results demonstrate that selecting only the top 1\\% of data based on MIWV can outperform training on the full dataset. Furthermore, this approach extends beyond existing research that focuses on data quality scoring for data selection, offering strong empirical evidence supporting the effectiveness of our proposed method.",
    "summary": "该论文研究如何为特定LLM选择最能提升其指令调优性能的高质量数据。核心思想是提出模型指令弱点值(MIWV)指标，通过分析模型在上下文学习中的响应差异来量化指令数据对增强模型能力的重要性。",
    "translation": "面向高效大语言模型指令调优的重要性感知数据选择",
    "relevance_score": 8,
    "reasoning": "该论文关注LLM指令调优的数据选择效率，这属于'使能LLM技术'范畴。高效的指令调优方法可以直接应用于提升推荐、搜索和广告系统中LLM组件的训练效率，特别是在处理大规模用户交互数据时，重要性感知选择可以显著降低计算成本并提高模型性能。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的数据选择方法直接优化LLM指令调优效率，属于LLM核心技术进步，对推荐和搜索系统的模型微调具有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.07065v1": {
    "title": "Aligning Attention with Human Rationales for Self-Explaining Hate Speech Detection",
    "url": "https://www.alphaxiv.org/abs/2511.07065v1",
    "arxiv_id": "2511.07065v1",
    "authors": "Brage Eilertsen, Røskva Bjørgfinsdóttir, Francielle Vargas, Ali Ramezani-Kebrya",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-11-10 12:57:56",
    "ori_summary": "The opaque nature of deep learning models presents significant challenges for the ethical deployment of hate speech detection systems. To address this limitation, we introduce Supervised Rational Attention (SRA), a framework that explicitly aligns model attention with human rationales, improving both interpretability and fairness in hate speech classification. SRA integrates a supervised attention mechanism into transformer-based classifiers, optimizing a joint objective that combines standard classification loss with an alignment loss term that minimizes the discrepancy between attention weights and human-annotated rationales. We evaluated SRA on hate speech benchmarks in English (HateXplain) and Portuguese (HateBRXplain) with rationale annotations. Empirically, SRA achieves 2.4x better explainability compared to current baselines, and produces token-level explanations that are more faithful and human-aligned. In terms of fairness, SRA achieves competitive fairness across all measures, with second-best performance in detecting toxic posts targeting identity groups, while maintaining comparable results on other metrics. These findings demonstrate that incorporating human rationales into attention mechanisms can enhance interpretability and faithfulness without compromising fairness.",
    "summary": "",
    "translation": "基于人类理性对齐注意力机制的自解释仇恨言论检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注仇恨言论检测这一特定NLP任务，属于内容安全领域，而非推荐系统、搜索或广告的核心技术。虽然涉及注意力机制，但其应用场景和优化目标与RecSys/Search/Ads的排序、匹配和个性化需求相关性较弱。论文重点在于模型可解释性和特定领域的检测任务，缺乏在推荐、搜索或广告场景中的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07044v1": {
    "title": "Evaluating LLMs for Anxiety, Depression, and Stress Detection Evaluating Large Language Models for Anxiety, Depression, and Stress Detection: Insights into Prompting Strategies and Synthetic Data",
    "url": "https://www.alphaxiv.org/abs/2511.07044v1",
    "arxiv_id": "2511.07044v1",
    "authors": "Mihael Arcan, David-Paul Niland",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 12:38:26",
    "ori_summary": "Mental health disorders affect over one-fifth of adults globally, yet detecting such conditions from text remains challenging due to the subtle and varied nature of symptom expression. This study evaluates multiple approaches for mental health detection, comparing Large Language Models (LLMs) such as Llama and GPT with classical machine learning and transformer-based architectures including BERT, XLNet, and Distil-RoBERTa. Using the DAIC-WOZ dataset of clinical interviews, we fine-tuned models for anxiety, depression, and stress classification and applied synthetic data generation to mitigate class imbalance. Results show that Distil-RoBERTa achieved the highest F1 score (0.883) for GAD-2, while XLNet outperformed others on PHQ tasks (F1 up to 0.891). For stress detection, a zero-shot synthetic approach (SD+Zero-Shot-Basic) reached an F1 of 0.884 and ROC AUC of 0.886. Findings demonstrate the effectiveness of transformer-based models and highlight the value of synthetic data in improving recall and generalization. However, careful calibration is required to prevent precision loss. Overall, this work emphasizes the potential of combining advanced language models and data augmentation to enhance automated mental health assessment from text.",
    "summary": "",
    "translation": "评估大型语言模型在焦虑、抑郁和压力检测中的应用：关于提示策略和合成数据的深入分析",
    "relevance_score": 1,
    "reasoning": "该论文专注于心理健康领域的LLM应用，属于医学/生物学领域的特定应用，明确在无关主题范围内。论文内容涉及焦虑、抑郁和压力检测，与推荐系统、搜索或广告领域没有任何技术关联，也不涉及任何可能在这些领域应用的使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07011v1": {
    "title": "Multilingual Lexical Feature Analysis of Spoken Language for Predicting Major Depression Symptom Severity",
    "url": "https://www.alphaxiv.org/abs/2511.07011v1",
    "arxiv_id": "2511.07011v1",
    "authors": "Anastasiia Tokareva, Judith Dineley, Zoe Firth, Pauline Conde, Faith Matcham, Sara Siddi, Femke Lamers, Ewan Carr, Carolin Oetzmann, Daniel Leightley, Yuezhou Zhang, Amos A. Folarin, Josep Maria Haro, Brenda W. J. H. Penninx, Raquel Bailon, Srinivasan Vairavan, Til Wykes, Richard J. B. Dobson, Vaibhav A. Narayan, Matthew Hotopf, Nicholas Cummins, The RADAR-CNS Consortium",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-11-10 12:03:16",
    "ori_summary": "Background: Captured between clinical appointments using mobile devices, spoken language has potential for objective, more regular assessment of symptom severity and earlier detection of relapse in major depressive disorder. However, research to date has largely been in non-clinical cross-sectional samples of written language using complex machine learning (ML) approaches with limited interpretability. Methods: We describe an initial exploratory analysis of longitudinal speech data and PHQ-8 assessments from 5,836 recordings of 586 participants in the UK, Netherlands, and Spain, collected in the RADAR-MDD study. We sought to identify interpretable lexical features associated with MDD symptom severity with linear mixed-effects modelling. Interpretable features and high-dimensional vector embeddings were also used to test the prediction performance of four regressor ML models. Results: In English data, MDD symptom severity was associated with 7 features including lexical diversity measures and absolutist language. In Dutch, associations were observed with words per sentence and positive word frequency; no associations were observed in recordings collected in Spain. The predictive power of lexical features and vector embeddings was near chance level across all languages. Limitations: Smaller samples in non-English speech and methodological choices, such as the elicitation prompt, may have also limited the effect sizes observable. A lack of NLP tools in languages other than English restricted our feature choice. Conclusion: To understand the value of lexical markers in clinical research and practice, further research is needed in larger samples across several languages using improved protocols, and ML models that account for within- and between-individual variations in language.",
    "summary": "",
    "translation": "基于口语多语言词汇特征分析预测重度抑郁症症状严重程度",
    "relevance_score": 1,
    "reasoning": "该论文专注于心理健康领域的医学诊断应用，使用语言特征分析来预测抑郁症严重程度。这与推荐系统、搜索或广告的核心技术领域完全无关，也不涉及任何LLM、Transformer架构或异质数据建模的相关技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07010v1": {
    "title": "A Picture is Worth a Thousand (Correct) Captions: A Vision-Guided Judge-Corrector System for Multimodal Machine Translation",
    "url": "https://www.alphaxiv.org/abs/2511.07010v1",
    "arxiv_id": "2511.07010v1",
    "authors": "Siddharth Betala, Kushan Raj, Vipul Betala, Rohan Saswade",
    "categories": "cs.CL, cs.CV, cs.HC",
    "pub_date": "2025-11-10 12:02:48",
    "ori_summary": "In this paper, we describe our system under the team name BLEU Monday for the English-to-Indic Multimodal Translation Task at WAT 2025. We participate in the text-only translation tasks for English-Hindi, English-Bengali, English-Malayalam, and English-Odia language pairs. We present a two-stage approach that addresses quality issues in the training data through automated error detection and correction, followed by parameter-efficient model fine-tuning. Our methodology introduces a vision-augmented judge-corrector pipeline that leverages multimodal language models to systematically identify and correct translation errors in the training data. The judge component classifies translations into three categories: correct, visually ambiguous (requiring image context), or mistranslated (poor translation quality). Identified errors are routed to specialized correctors: GPT-4o-mini regenerates captions requiring visual disambiguation, while IndicTrans2 retranslates cases with pure translation quality issues. This automated pipeline processes 28,928 training examples across four languages, correcting an average of 17.1% of captions per language. We then apply Low-Rank Adaptation (LoRA) to fine-tune the IndicTrans2 en-indic 200M distilled model on both original and corrected datasets. Training on corrected data yields consistent improvements, with BLEU score gains of +1.30 for English-Bengali on the evaluation set (42.00 -> 43.30) and +0.70 on the challenge set (44.90 -> 45.60), +0.60 for English-Odia on the evaluation set (41.00 -> 41.60), and +0.10 for English-Hindi on the challenge set (53.90 -> 54.00).",
    "summary": "",
    "translation": "一图胜千（正确）字幕：用于多模态机器翻译的视觉引导评判-修正系统",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态机器翻译中的视觉引导修正系统，属于计算机视觉与自然语言处理的交叉领域。虽然涉及多模态建模，但其核心应用场景是机器翻译而非推荐系统、搜索或广告领域。视觉语言模型的类比思想可能有一定启发，但缺乏明确的RecSys/Search/Ads应用路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07003v1": {
    "title": "Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs",
    "url": "https://www.alphaxiv.org/abs/2511.07003v1",
    "arxiv_id": "2511.07003v1",
    "authors": "Yingfeng Luo, Ziqiang Xu, Yuxuan Ouyang, Murun Yang, Dingyang Lin, Kaiyan Chang, Tong Zheng, Bei Li, Peinan Feng, Quan Du, Tong Xiao, Jingbo Zhu",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 11:54:53",
    "ori_summary": "Large language models have significantly advanced Multilingual Machine Translation (MMT), yet the broad language coverage, consistent translation quality, and English-centric bias remain open challenges. To address these challenges, we introduce \\textbf{LMT}, a suite of \\textbf{L}arge-scale \\textbf{M}ultilingual \\textbf{T}ranslation models centered on both Chinese and English, covering 60 languages and 234 translation directions. During development, we identify a previously overlooked phenomenon of \\textbf{directional degeneration}, where symmetric multi-way fine-tuning data overemphasize reverse directions (X $\\to$ En/Zh), leading to excessive many-to-one mappings and degraded translation quality. We propose \\textbf{Strategic Downsampling}, a simple yet effective method to mitigate this degeneration. In addition, we design \\textbf{Parallel Multilingual Prompting (PMP)}, which leverages typologically related auxiliary languages to enhance cross-lingual transfer. Through rigorous data curation and refined adaptation strategies, LMT achieves SOTA performance among models of comparable language coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B and NLLB-54B models by a substantial margin. We release LMT in four sizes (0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for inclusive, scalable, and high-quality MMT \\footnote{\\href{https://github.com/NiuTrans/LMT}{https://github.com/NiuTrans/LMT}}.",
    "summary": "",
    "translation": "超越英语：基于大语言模型实现包容且可扩展的多语言机器翻译",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多语言机器翻译，属于通用NLP领域，与推荐系统、搜索或广告的核心技术关联度较低。虽然多语言能力在全球化应用中具有价值，但论文未明确涉及推荐、搜索或广告的具体应用场景，也未涉及Transformer架构改进或异构数据统一建模等核心技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07002v1": {
    "title": "Automated Circuit Interpretation via Probe Prompting",
    "url": "https://www.alphaxiv.org/abs/2511.07002v1",
    "arxiv_id": "2511.07002v1",
    "authors": "Giuseppe Birardi",
    "categories": "cs.CL, I.2.0; I.2.6; I.2.7; I.2.4",
    "pub_date": "2025-11-10 11:53:36",
    "ori_summary": "Mechanistic interpretability aims to understand neural networks by identifying which learned features mediate specific behaviors. Attribution graphs reveal these feature pathways, but interpreting them requires extensive manual analysis -- a single prompt can take approximately 2 hours for an experienced circuit tracer. We present probe prompting, an automated pipeline that transforms attribution graphs into compact, interpretable subgraphs built from concept-aligned supernodes. Starting from a seed prompt and target logit, we select high-influence features, generate concept-targeted yet context-varying probes, and group features by cross-prompt activation signatures into Semantic, Relationship, and Say-X categories using transparent decision rules. Across five prompts including classic \"capitals\" circuits, probe-prompted subgraphs preserve high explanatory coverage while compressing complexity (Completeness 0.83, mean across circuits; Replacement 0.54). Compared to geometric clustering baselines, concept-aligned groups exhibit higher behavioral coherence: 2.3x higher peak-token consistency (0.425 vs 0.183) and 5.8x higher activation-pattern similarity (0.762 vs 0.130), despite lower geometric compactness. Entity-swap tests reveal a layerwise hierarchy: early-layer features transfer robustly (64% transfer rate, mean layer 6.3), while late-layer Say-X features specialize for output promotion (mean layer 16.4), supporting a backbone-and-specialization view of transformer computation. We release code (https://github.com/peppinob-ol/attribution-graph-probing), an interactive demo (https://huggingface.co/spaces/Peppinob/attribution-graph-probing), and minimal artifacts enabling immediate reproduction and community adoption.",
    "summary": "",
    "translation": "基于探针提示的自动化电路解释",
    "relevance_score": 3,
    "reasoning": "该论文关注LLM内部电路解释的自动化方法，属于核心LLM技术进展。虽然电路解释本身是LLM可解释性研究，但自动化电路分析技术可能应用于理解推荐/搜索系统中LLM的决策过程，帮助优化模型行为或识别潜在偏差。然而，这种应用相对间接且具体性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07001v1": {
    "title": "SCOPE: Intrinsic Semantic Space Control for Mitigating Copyright Infringement in LLMs",
    "url": "https://www.alphaxiv.org/abs/2511.07001v1",
    "arxiv_id": "2511.07001v1",
    "authors": "Zhenliang Zhang, Xinyu Hu, Xiaojun Wan",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 11:53:07",
    "ori_summary": "Large language models sometimes inadvertently reproduce passages that are copyrighted, exposing downstream applications to legal risk. Most existing studies for inference-time defences focus on surface-level token matching and rely on external blocklists or filters, which add deployment complexity and may overlook semantically paraphrased leakage. In this work, we reframe copyright infringement mitigation as intrinsic semantic-space control and introduce SCOPE, an inference-time method that requires no parameter updates or auxiliary filters. Specifically, the sparse autoencoder (SAE) projects hidden states into a high-dimensional, near-monosemantic space; benefiting from this representation, we identify a copyright-sensitive subspace and clamp its activations during decoding. Experiments on widely recognized benchmarks show that SCOPE mitigates copyright infringement without degrading general utility. Further interpretability analyses confirm that the isolated subspace captures high-level semantics.",
    "summary": "",
    "translation": "SCOPE：通过内在语义空间控制缓解大语言模型中的版权侵权问题",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM中的版权侵权缓解，这属于法律合规和内容安全范畴，而非核心推荐系统、搜索或广告的技术进展。虽然涉及LLM技术，但其应用方向是版权保护而非推荐/搜索/广告系统的性能提升或架构创新，与当前关注的技术焦点相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06942v1": {
    "title": "HLPD: Aligning LLMs to Human Language Preference for Machine-Revised Text Detection",
    "url": "https://www.alphaxiv.org/abs/2511.06942v1",
    "arxiv_id": "2511.06942v1",
    "authors": "Fangqi Dai, Xingjian Jiang, Zizhuang Deng",
    "categories": "cs.CL, cs.CR",
    "pub_date": "2025-11-10 10:47:34",
    "ori_summary": "To prevent misinformation and social issues arising from trustworthy-looking content generated by LLMs, it is crucial to develop efficient and reliable methods for identifying the source of texts. Previous approaches have demonstrated exceptional performance in detecting texts fully generated by LLMs. However, these methods struggle when confronting more advanced LLM output or text with adversarial multi-task machine revision, especially in the black-box setting, where the generating model is unknown. To address this challenge, grounded in the hypothesis that human writing possesses distinctive stylistic patterns, we propose Human Language Preference Detection (HLPD). HLPD employs a reward-based alignment process, Human Language Preference Optimization (HLPO), to shift the scoring model's token distribution toward human-like writing, making the model more sensitive to human writing, therefore enhancing the identification of machine-revised text. We test HLPD in an adversarial multi-task evaluation framework that leverages a five-dimensional prompt generator and multiple advanced LLMs to create diverse revision scenarios. When detecting texts revised by GPT-series models, HLPD achieves a 15.11% relative improvement in AUROC over ImBD, surpassing Fast-DetectGPT by 45.56%. When evaluated on texts generated by advanced LLMs, HLPD achieves the highest average AUROC, exceeding ImBD by 5.53% and Fast-DetectGPT by 34.14%. Code will be made available at https://github.com/dfq2021/HLPD.",
    "summary": "",
    "translation": "HLPD：将大语言模型与人类语言偏好对齐以实现机器修订文本检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注机器修订文本检测和人类语言偏好对齐，这属于LLM评估和检测领域，与我的核心关注点（推荐系统、搜索、广告中的核心进展、使能技术或直接应用）相关性较弱。虽然涉及LLM对齐，但缺乏明确的在RecSys/Search/Ads中的潜在应用场景，更偏向于内容检测和评估的NLP任务。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06899v1": {
    "title": "RPTS: Tree-Structured Reasoning Process Scoring for Faithful Multimodal Evaluation",
    "url": "https://www.alphaxiv.org/abs/2511.06899v1",
    "arxiv_id": "2511.06899v1",
    "authors": "Haofeng Wang, Yu Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-11-10 09:48:07",
    "ori_summary": "Large Vision-Language Models (LVLMs) excel in multimodal reasoning and have shown impressive performance on various multimodal benchmarks. However, most of these benchmarks evaluate models primarily through multiple-choice or short-answer formats, which do not take the reasoning process into account. Although some benchmarks assess the reasoning process, their methods are often overly simplistic and only examine reasoning when answers are incorrect. This approach overlooks scenarios where flawed reasoning leads to correct answers. In addition, these benchmarks do not consider the impact of intermodal relationships on reasoning. To address this issue, we propose the Reasoning Process Tree Score (RPTS), a tree structure-based metric to assess reasoning processes. Specifically, we organize the reasoning steps into a reasoning tree and leverage its hierarchical information to assign weighted faithfulness scores to each reasoning step. By dynamically adjusting these weights, RPTS not only evaluates the overall correctness of the reasoning, but also pinpoints where the model fails in the reasoning. To validate RPTS in real-world multimodal scenarios, we construct a new benchmark, RPTS-Eval, comprising 374 images and 390 reasoning instances. Each instance includes reliable visual-textual clues that serve as leaf nodes of the reasoning tree. Furthermore, we define three types of intermodal relationships to investigate how intermodal interactions influence the reasoning process. We evaluated representative LVLMs (e.g., GPT4o, Llava-Next), uncovering their limitations in multimodal reasoning and highlighting the differences between open-source and closed-source commercial LVLMs. We believe that this benchmark will contribute to the advancement of research in the field of multimodal reasoning.",
    "summary": "",
    "translation": "RPTS：基于树状推理过程评分的可信多模态评估",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态评估的忠实性（faithfulness）和推理过程评分，这属于评估基准范畴，在无关主题中明确排除。虽然提到了多模态，但缺乏与推荐系统、搜索或广告的直接关联，也没有展示在Transformer架构效率或LLM技术方面的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06890v1": {
    "title": "EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity and Adversarial Safety of LLMs as Simulated Teachers",
    "url": "https://www.alphaxiv.org/abs/2511.06890v1",
    "arxiv_id": "2511.06890v1",
    "authors": "Yilin Jiang, Mingzi Zhang, Xuanyu Yin, Sheng Jin, Suyu Lu, Zuocan Ying, Zengyi Yu, Xiangjie Kong",
    "categories": "cs.CL, I.2.7",
    "pub_date": "2025-11-10 09:42:24",
    "ori_summary": "Large Language Models for Simulating Professions (SP-LLMs), particularly as teachers, are pivotal for personalized education. However, ensuring their professional competence and ethical safety is a critical challenge, as existing benchmarks fail to measure role-playing fidelity or address the unique teaching harms inherent in educational scenarios. To address this, we propose EduGuardBench, a dual-component benchmark. It assesses professional fidelity using a Role-playing Fidelity Score (RFS) while diagnosing harms specific to the teaching profession. It also probes safety vulnerabilities using persona-based adversarial prompts targeting both general harms and, particularly, academic misconduct, evaluated with metrics including Attack Success Rate (ASR) and a three-tier Refusal Quality assessment. Our extensive experiments on 14 leading models reveal a stark polarization in performance. While reasoning-oriented models generally show superior fidelity, incompetence remains the dominant failure mode across most models. The adversarial tests uncovered a counterintuitive scaling paradox, where mid-sized models can be the most vulnerable, challenging monotonic safety assumptions. Critically, we identified a powerful Educational Transformation Effect: the safest models excel at converting harmful requests into teachable moments by providing ideal Educational Refusals. This capacity is strongly negatively correlated with ASR, revealing a new dimension of advanced AI safety. EduGuardBench thus provides a reproducible framework that moves beyond siloed knowledge tests toward a holistic assessment of professional, ethical, and pedagogical alignment, uncovering complex dynamics essential for deploying trustworthy AI in education. See https://github.com/YL1N/EduGuardBench for Materials.",
    "summary": "",
    "translation": "EduGuardBench：一个用于评估LLM作为模拟教师的教学保真度和对抗安全性的综合基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于教育领域的LLM评估基准，涉及教学保真度和对抗安全性评估，属于特定领域应用。这与我的关注点（推荐系统、搜索、广告中的核心进展、使能技术及其应用）完全不相关，且教育领域属于明确的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06886v1": {
    "title": "Inclusion of Role into Named Entity Recognition and Ranking",
    "url": "https://www.alphaxiv.org/abs/2511.06886v1",
    "arxiv_id": "2511.06886v1",
    "authors": "Neelesh Kumar Shukla, Sanasam Ranbir Singh",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-11-10 09:39:00",
    "ori_summary": "Most of the Natural Language Processing sys- tems are involved in entity-based processing for several tasks like Information Extraction, Question-Answering, Text-Summarization and so on. A new challenge comes when entities play roles according to their act or attributes in certain context. Entity Role Detection is the task of assigning such roles to the entities. Usu- ally real-world entities are of types: person, lo- cation and organization etc. Roles could be con- sidered as domain-dependent subtypes of these types. In the cases, where retrieving a subset of entities based on their roles is needed, poses the problem of defining the role and entities having those roles. This paper presents the study of study of solving Entity Role Detection prob- lem by modeling it as Named Entity Recogni- tion (NER) and Entity Retrieval/Ranking task. In NER, these roles could be considered as mutually exclusive classes and standard NER methods like sequence tagging could be used. For Entity Retrieval, Roles could be formulated as Query and entities as Collection on which the query needs to be executed. The aspect of Entity Retrieval task, which is different than document retrieval task is that the entities and roles against which they need to be retrieved are indirectly described. We have formulated au- tomated ways of learning representative words and phrases and building representations of roles and entities using them. We have also explored different contexts like sentence and document. Since the roles depend upon con- text, so it is not always possible to have large domain-specific dataset or knowledge bases for learning purposes, so we have tried to exploit the information from small dataset in domain- agnostic way.",
    "summary": "论文研究实体角色检测问题，核心方法是将角色检测建模为命名实体识别任务和实体检索排序任务，通过自动学习代表性词语构建角色和实体的表示来解决上下文相关的角色识别。",
    "translation": "将角色信息融入命名实体识别与排序",
    "relevance_score": 6,
    "reasoning": "该论文涉及命名实体识别和排序，这在搜索系统中具有直接应用价值，可用于改进实体搜索和结果排序。角色信息的融入可能增强对用户查询中实体意图的理解，从而提升搜索相关性。虽然不直接涉及LLM技术，但属于搜索领域的核心排序改进。",
    "rerank_relevance_score": 4,
    "rerank_reasoning": "该论文主要关注实体角色检测的NLP任务，虽然实体识别与搜索推荐有一定关联，但核心方法聚焦于序列标注和实体检索的NLP技术，与LLM、推荐系统或广告领域的直接关联较弱。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.06860v1": {
    "title": "CLiFT-ASR: A Cross-Lingual Fine-Tuning Framework for Low-Resource Taiwanese Hokkien Speech Recognition",
    "url": "https://www.alphaxiv.org/abs/2511.06860v1",
    "arxiv_id": "2511.06860v1",
    "authors": "Hung-Yang Sung, Chien-Chun Wang, Kuan-Tang Huang, Tien-Hong Lo, Yu-Sheng Tsao, Yung-Chang Hsu, Berlin Chen",
    "categories": "cs.CL, cs.SD",
    "pub_date": "2025-11-10 09:03:30",
    "ori_summary": "Automatic speech recognition (ASR) for low-resource languages such as Taiwanese Hokkien is difficult due to the scarcity of annotated data. However, direct fine-tuning on Han-character transcriptions often fails to capture detailed phonetic and tonal cues, while training only on romanization lacks lexical and syntactic coverage. In addition, prior studies have rarely explored staged strategies that integrate both annotation types. To address this gap, we present CLiFT-ASR, a cross-lingual fine-tuning framework that builds on Mandarin HuBERT models and progressively adapts them to Taiwanese Hokkien. The framework employs a two-stage process in which it first learns acoustic and tonal representations from phonetic Tai-lo annotations and then captures vocabulary and syntax from Han-character transcriptions. This progressive adaptation enables effective alignment between speech sounds and orthographic structures. Experiments on the TAT-MOE corpus demonstrate that CLiFT-ASR achieves a 24.88\\% relative reduction in character error rate (CER) compared with strong baselines. The results indicate that CLiFT-ASR provides an effective and parameter-efficient solution for Taiwanese Hokkien ASR and that it has potential to benefit other low-resource language scenarios.",
    "summary": "",
    "translation": "CLiFT-ASR：面向低资源台湾闽南语语音识别的跨语言微调框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于特定语言的语音识别技术，属于纯粹的语音处理领域。虽然涉及跨语言微调技术，但其应用场景仅限于台湾闽南语语音识别，与搜索、推荐或广告系统的核心需求没有直接关联。论文的技术方法在RecSys/Search/Ads领域缺乏明显的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06826v1": {
    "title": "Beyond Plain Demos: A Demo-centric Anchoring Paradigm for In-Context Learning in Alzheimer's Disease Detection",
    "url": "https://www.alphaxiv.org/abs/2511.06826v1",
    "arxiv_id": "2511.06826v1",
    "authors": "Puzhen Su, Haoran Yin, Yongzhu Miao, Jintao Tang, Shasha Li, Ting Wang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-11-10 08:13:42",
    "ori_summary": "Detecting Alzheimer's disease (AD) from narrative transcripts challenges large language models (LLMs): pre-training rarely covers this out-of-distribution task, and all transcript demos describe the same scene, producing highly homogeneous contexts. These factors cripple both the model's built-in task knowledge (\\textbf{task cognition}) and its ability to surface subtle, class-discriminative cues (\\textbf{contextual perception}). Because cognition is fixed after pre-training, improving in-context learning (ICL) for AD detection hinges on enriching perception through better demonstration (demo) sets. We demonstrate that standard ICL quickly saturates, its demos lack diversity (context width) and fail to convey fine-grained signals (context depth), and that recent task vector (TV) approaches improve broad task adaptation by injecting TV into the LLMs' hidden states (HSs), they are ill-suited for AD detection due to the mismatch of injection granularity, strength and position. To address these bottlenecks, we introduce \\textbf{DA4ICL}, a demo-centric anchoring framework that jointly expands context width via \\emph{\\textbf{Diverse and Contrastive Retrieval}} (DCR) and deepens each demo's signal via \\emph{\\textbf{Projected Vector Anchoring}} (PVA) at every Transformer layer. Across three AD benchmarks, DA4ICL achieves large, stable gains over both ICL and TV baselines, charting a new paradigm for fine-grained, OOD and low-resource LLM adaptation.",
    "summary": "",
    "translation": "超越简单演示：面向阿尔茨海默病检测中上下文学习的演示中心锚定范式",
    "relevance_score": 1,
    "reasoning": "该论文专注于阿尔茨海默病检测这一医疗领域应用，属于明确的无关主题。虽然提到了上下文学习这一LLM技术，但其应用场景完全在医疗诊断领域，与推荐系统、搜索或广告没有任何潜在关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06818v1": {
    "title": "Learning to Focus: Focal Attention for Selective and Scalable Transformers",
    "url": "https://www.alphaxiv.org/abs/2511.06818v1",
    "arxiv_id": "2511.06818v1",
    "authors": "Dhananjay Ram, Wei Xia, Stefano Soatto",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-11-10 08:02:58",
    "ori_summary": "Attention is a core component of transformer architecture, whether encoder-only, decoder-only, or encoder-decoder model. However, the standard softmax attention often produces noisy probability distribution, which can impair effective feature selection at every layer of these models, particularly for long contexts. We propose Focal Attention, a simple yet effective modification that sharpens the attention distribution by controlling the softmax temperature, either as a fixed hyperparameter or as a learnable parameter during training. This sharpening enables the model to concentrate on the most relevant tokens while suppressing irrelevant ones. Empirically, Focal Attention scales more favorably than standard transformer with respect to model size, training data, and context length. Across diverse benchmarks, it achieves the same accuracy with up to 42% fewer parameters or 33% less training data. On long-context tasks, it delivers substantial relative improvements ranging from 17% to 82%, demonstrating its effectiveness in real world applications.",
    "summary": "论文研究标准softmax注意力在长上下文中产生噪声分布的问题，核心思想是通过控制softmax温度参数来锐化注意力分布，使模型能够聚焦相关标记并抑制无关信息。",
    "translation": "学会聚焦：用于选择性可扩展Transformer的焦点注意力机制",
    "relevance_score": 9,
    "reasoning": "这篇论文直接涉及Transformer架构中的注意力机制创新，属于'Enabling Transformer Tech'范畴。焦点注意力机制通过选择性关注关键信息，可以显著提升推荐系统和搜索中长序列处理的效率和效果，特别是在处理用户历史行为序列和上下文特征时。这种可扩展的注意力机制对于大规模工业级推荐和搜索系统具有直接的应用价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的聚焦注意力机制直接改进Transformer核心组件，在长上下文处理、模型效率和特征选择方面的突破对搜索推荐系统具有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.06778v1": {
    "title": "SAFENLIDB: A Privacy-Preserving Safety Alignment Framework for LLM-based Natural Language Database Interfaces",
    "url": "https://www.alphaxiv.org/abs/2511.06778v1",
    "arxiv_id": "2511.06778v1",
    "authors": "Ruiheng Liu, XiaoBing Chen, Jinyu Zhang, Qiongwen Zhang, Yu Zhang, Bailong Yang",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 07:05:59",
    "ori_summary": "The rapid advancement of Large Language Models (LLMs) has driven significant progress in Natural Language Interface to Database (NLIDB). However, the widespread adoption of LLMs has raised critical privacy and security concerns. During interactions, LLMs may unintentionally expose confidential database contents or be manipulated by attackers to exfiltrate data through seemingly benign queries. While current efforts typically rely on rule-based heuristics or LLM agents to mitigate this leakage risk, these methods still struggle with complex inference-based attacks, suffer from high false positive rates, and often compromise the reliability of SQL queries. To address these challenges, we propose \\textsc{SafeNlidb}, a novel privacy-security alignment framework for LLM-based NLIDB. The framework features an automated pipeline that generates hybrid chain-of-thought interaction data from scratch, seamlessly combining implicit security reasoning with SQL generation. Additionally, we introduce reasoning warm-up and alternating preference optimization to overcome the multi-preference oscillations of Direct Preference Optimization (DPO), enabling LLMs to produce security-aware SQL through fine-grained reasoning without the need for human-annotated preference data. Extensive experiments demonstrate that our method outperforms both larger-scale LLMs and ideal-setting baselines, achieving significant security improvements while preserving high utility.WARNING: This work may contain content that is offensive and harmful!",
    "summary": "",
    "translation": "SAFENLIDB：一种面向基于LLM的自然语言数据库接口的隐私保护安全对齐框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注隐私保护和安全对齐，这两个主题明确属于被排除的隐私和安全相关话题。虽然提到了LLM-based自然语言数据库接口，但核心焦点是隐私保护而非LLM技术本身在推荐系统、搜索或广告中的应用潜力。论文的技术方向与当前关注的领域进展、LLM使能技术或直接应用不匹配。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06763v1": {
    "title": "Sensitivity of Small Language Models to Fine-tuning Data Contamination",
    "url": "https://www.alphaxiv.org/abs/2511.06763v1",
    "arxiv_id": "2511.06763v1",
    "authors": "Nicy Scaria, Silvester John Joseph Kennedy, Deepak Subramani",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-11-10 06:44:29",
    "ori_summary": "Small Language Models (SLMs) are increasingly being deployed in resource-constrained environments, yet their behavioral robustness to data contamination during instruction tuning remains poorly understood. We systematically investigate the contamination sensitivity of 23 SLMs (270M to 4B parameters) across multiple model families by measuring susceptibility to syntactic and semantic transformation types during instruction tuning: syntactic transformations (character and word reversal) and semantic transformations (irrelevant and counterfactual responses), each applied at contamination levels of 25\\%, 50\\%, 75\\%, and 100\\%. Our results reveal fundamental asymmetries in vulnerability patterns: syntactic transformations cause catastrophic performance degradation, with character reversal producing near-complete failure across all models regardless of size or family, while semantic transformations demonstrate distinct threshold behaviors and greater resilience in core linguistic capabilities. Critically, we discover a ``\\textit{capability curse}\" where larger, more capable models become more susceptible to learning semantic corruptions, effectively following harmful instructions more readily, while our analysis of base versus instruction-tuned variants reveals that alignment provides inconsistent robustness benefits, sometimes even reducing resilience. Our work establishes three core contributions: (1) empirical evidence of SLMs' disproportionate vulnerability to syntactic pattern contamination, (2) identification of asymmetric sensitivity patterns between syntactic and semantic transformations, and (3) systematic evaluation protocols for contamination robustness assessment. These findings have immediate deployment implications, suggesting that current robustness assumptions may not hold for smaller models and highlighting the need for contamination-aware training protocols.",
    "summary": "",
    "translation": "小语言模型对微调数据污染的敏感性",
    "relevance_score": 2,
    "reasoning": "该论文主要关注小语言模型在数据污染情况下的敏感性，属于模型鲁棒性和数据质量的技术问题。虽然LLM的鲁棒性可能间接影响推荐/搜索系统的可靠性，但论文本身没有明确涉及推荐系统、搜索或广告的直接应用，也没有提出新的架构或方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06738v1": {
    "title": "Rethinking Retrieval-Augmented Generation for Medicine: A Large-Scale, Systematic Expert Evaluation and Practical Insights",
    "url": "https://www.alphaxiv.org/abs/2511.06738v1",
    "arxiv_id": "2511.06738v1",
    "authors": "Hyunjae Kim, Jiwoong Sohn, Aidan Gilson, Nicholas Cochran-Caggiano, Serina Applebaum, Heeju Jin, Seihee Park, Yujin Park, Jiyeong Park, Seoyoung Choi, Brittany Alexandra Herrera Contreras, Thomas Huang, Jaehoon Yun, Ethan F. Wei, Roy Jiang, Leah Colucci, Eric Lai, Amisha Dave, Tuo Guo, Maxwell B. Singer, Yonghoe Koo, Ron A. Adelman, James Zou, Andrew Taylor, Arman Cohan, Hua Xu, Qingyu Chen",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 06:00:12",
    "ori_summary": "Large language models (LLMs) are transforming the landscape of medicine, yet two fundamental challenges persist: keeping up with rapidly evolving medical knowledge and providing verifiable, evidence-grounded reasoning. Retrieval-augmented generation (RAG) has been widely adopted to address these limitations by supplementing model outputs with retrieved evidence. However, whether RAG reliably achieves these goals remains unclear. Here, we present the most comprehensive expert evaluation of RAG in medicine to date. Eighteen medical experts contributed a total of 80,502 annotations, assessing 800 model outputs generated by GPT-4o and Llama-3.1-8B across 200 real-world patient and USMLE-style queries. We systematically decomposed the RAG pipeline into three components: (i) evidence retrieval (relevance of retrieved passages), (ii) evidence selection (accuracy of evidence usage), and (iii) response generation (factuality and completeness of outputs). Contrary to expectation, standard RAG often degraded performance: only 22% of top-16 passages were relevant, evidence selection remained weak (precision 41-43%, recall 27-49%), and factuality and completeness dropped by up to 6% and 5%, respectively, compared with non-RAG variants. Retrieval and evidence selection remain key failure points for the model, contributing to the overall performance drop. We further show that simple yet effective strategies, including evidence filtering and query reformulation, substantially mitigate these issues, improving performance on MedMCQA and MedXpertQA by up to 12% and 8.2%, respectively. These findings call for re-examining RAG's role in medicine and highlight the importance of stage-aware evaluation and deliberate system design for reliable medical LLM applications.",
    "summary": "",
    "translation": "重新思考医学领域的检索增强生成：大规模系统性专家评估与实践洞见",
    "relevance_score": 1,
    "reasoning": "该论文明确聚焦于医学领域的RAG应用，这属于明确的无关主题（医学领域特定应用）。虽然RAG技术本身在搜索和推荐系统中具有潜在价值，但论文的医学领域焦点使其与当前关注的通用RecSys/Search/Ads技术无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06722v1": {
    "title": "Revisiting the Data Sampling in Multimodal Post-training from a Difficulty-Distinguish View",
    "url": "https://www.alphaxiv.org/abs/2511.06722v1",
    "arxiv_id": "2511.06722v1",
    "authors": "Jianyu Qi, Ding Zou, Wenrui Yan, Rui Ma, Jiaxu Li, Zhijie Zheng, Zhiguo Yang, Rongchang Zhao",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-11-10 05:31:59",
    "ori_summary": "Recent advances in Multimodal Large Language Models (MLLMs) have spurred significant progress in Chain-of-Thought (CoT) reasoning. Building on the success of Deepseek-R1, researchers extended multimodal reasoning to post-training paradigms based on reinforcement learning (RL), focusing predominantly on mathematical datasets. However, existing post-training paradigms tend to neglect two critical aspects: (1) The lack of quantifiable difficulty metrics capable of strategically screening samples for post-training optimization. (2) Suboptimal post-training paradigms that fail to jointly optimize perception and reasoning capabilities. To address this gap, we propose two novel difficulty-aware sampling strategies: Progressive Image Semantic Masking (PISM) quantifies sample hardness through systematic image degradation, while Cross-Modality Attention Balance (CMAB) assesses cross-modal interaction complexity via attention distribution analysis. Leveraging these metrics, we design a hierarchical training framework that incorporates both GRPO-only and SFT+GRPO hybrid training paradigms, and evaluate them across six benchmark datasets. Experiments demonstrate consistent superiority of GRPO applied to difficulty-stratified samples compared to conventional SFT+GRPO pipelines, indicating that strategic data sampling can obviate the need for supervised fine-tuning while improving model accuracy. Our code will be released at https://github.com/qijianyu277/DifficultySampling.",
    "summary": "论文研究多模态大语言模型后训练中的数据采样优化问题，核心思想是通过渐进式图像语义掩码和跨模态注意力平衡两种策略量化样本难度，构建分层训练框架来联合优化感知和推理能力。",
    "translation": "从难度区分视角重新审视多模态后训练中的数据采样",
    "relevance_score": 7,
    "reasoning": "该论文涉及多模态训练中的数据采样策略，这与'VLM Analogy for Heterogeneous Data'焦点直接相关，其中可以将用户序列和上下文特征视为不同模态进行统一建模。改进的多模态数据采样方法可以应用于推荐系统中处理异构用户行为数据和内容特征，优化模型训练效率和质量。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文针对多模态后训练的数据采样策略提出创新方法，通过难度感知采样优化模型训练，直接适用于推荐和搜索系统中的多模态内容处理。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.06708v1": {
    "title": "Sentiment Analysis On YouTube Comments Using Machine Learning Techniques Based On Video Games Content",
    "url": "https://www.alphaxiv.org/abs/2511.06708v1",
    "arxiv_id": "2511.06708v1",
    "authors": "Adi Danish Bin Muhammad Amin, Mohaiminul Islam Bhuiyan, Nur Shazwani Kamarudin, Zulfahmi Toh, Nur Syafiqah Nafis",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 05:02:25",
    "ori_summary": "The rapid evolution of the gaming industry, driven by technological advancements and a burgeoning community, necessitates a deeper understanding of user sentiments, especially as expressed on popular social media platforms like YouTube. This study presents a sentiment analysis on video games based on YouTube comments, aiming to understand user sentiments within the gaming community. Utilizing YouTube API, comments related to various video games were collected and analyzed using the TextBlob sentiment analysis tool. The pre-processed data underwent classification using machine learning algorithms, including Na\\\"ive Bayes, Logistic Regression, and Support Vector Machine (SVM). Among these, SVM demonstrated superior performance, achieving the highest classification accuracy across different datasets. The analysis spanned multiple popular gaming videos, revealing trends and insights into user preferences and critiques. The findings underscore the importance of advanced sentiment analysis in capturing the nuanced emotions expressed in user comments, providing valuable feedback for game developers to enhance game design and user experience. Future research will focus on integrating more sophisticated natural language processing techniques and exploring additional data sources to further refine sentiment analysis in the gaming domain.",
    "summary": "",
    "translation": "基于视频游戏内容的YouTube评论情感分析使用机器学习技术",
    "relevance_score": 2,
    "reasoning": "该论文主要关注使用机器学习技术进行YouTube评论的情感分析，这属于通用的文本分析任务，与推荐系统、搜索或广告的核心进展没有直接关联。虽然情感分析在内容理解方面有潜在价值，但论文专注于视频游戏内容这一特定领域，且没有明确展示在RecSys/Search/Ads中的直接应用或核心架构创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06700v1": {
    "title": "Place Matters: Comparing LLM Hallucination Rates for Place-Based Legal Queries",
    "url": "https://www.alphaxiv.org/abs/2511.06700v1",
    "arxiv_id": "2511.06700v1",
    "authors": "Damian Curran, Vanessa Sporne, Lea Frermann, Jeannie Paterson",
    "categories": "cs.CY, cs.AI, cs.CL",
    "pub_date": "2025-11-10 04:42:00",
    "ori_summary": "How do we make a meaningful comparison of a large language model's knowledge of the law in one place compared to another? Quantifying these differences is critical to understanding if the quality of the legal information obtained by users of LLM-based chatbots varies depending on their location. However, obtaining meaningful comparative metrics is challenging because legal institutions in different places are not themselves easily comparable. In this work we propose a methodology to obtain place-to-place metrics based on the comparative law concept of functionalism. We construct a dataset of factual scenarios drawn from Reddit posts by users seeking legal advice for family, housing, employment, crime and traffic issues. We use these to elicit a summary of a law from the LLM relevant to each scenario in Los Angeles, London and Sydney. These summaries, typically of a legislative provision, are manually evaluated for hallucinations. We show that the rate of hallucination of legal information by leading closed-source LLMs is significantly associated with place. This suggests that the quality of legal solutions provided by these models is not evenly distributed across geography. Additionally, we show a strong negative correlation between hallucination rate and the frequency of the majority response when the LLM is sampled multiple times, suggesting a measure of uncertainty of model predictions of legal facts.",
    "summary": "",
    "translation": "地点至关重要：基于地点的法律查询中大型语言模型幻觉率比较",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM幻觉问题，这属于纯粹的NLP评估基准范畴，被明确列为不相关主题。虽然提到了法律查询，但这与搜索、推荐或广告系统的核心技术进步无关，也没有展示在推荐系统、搜索或广告中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06682v1": {
    "title": "Textual Self-attention Network: Test-Time Preference Optimization through Textual Gradient-based Attention",
    "url": "https://www.alphaxiv.org/abs/2511.06682v1",
    "arxiv_id": "2511.06682v1",
    "authors": "Shibing Mo, Haoyang Ruan, Kai Wu, Jing Liu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-11-10 04:01:46",
    "ori_summary": "Large Language Models (LLMs) have demonstrated remarkable generalization capabilities, but aligning their outputs with human preferences typically requires expensive supervised fine-tuning. Recent test-time methods leverage textual feedback to overcome this, but they often critique and revise a single candidate response, lacking a principled mechanism to systematically analyze, weigh, and synthesize the strengths of multiple promising candidates. Such a mechanism is crucial because different responses may excel in distinct aspects (e.g., clarity, factual accuracy, or tone), and combining their best elements may produce a far superior outcome. This paper proposes the Textual Self-Attention Network (TSAN), a new paradigm for test-time preference optimization that requires no parameter updates. TSAN emulates self-attention entirely in natural language to overcome this gap: it analyzes multiple candidates by formatting them into textual keys and values, weighs their relevance using an LLM-based attention module, and synthesizes their strengths into a new, preference-aligned response under the guidance of the learned textual attention. This entire process operates in a textual gradient space, enabling iterative and interpretable optimization. Empirical evaluations demonstrate that with just three test-time iterations on a base SFT model, TSAN outperforms supervised models like Llama-3.1-70B-Instruct and surpasses the current state-of-the-art test-time alignment method by effectively leveraging multiple candidate solutions.",
    "summary": "该论文研究如何在不更新模型参数的情况下优化LLM输出与人类偏好的对齐问题。核心方法是构建文本自注意力网络，将多个候选回答格式化为文本键值对，使用LLM计算注意力权重，在文本梯度空间中迭代合成各候选回答的优势元素。",
    "translation": "文本自注意力网络：基于文本梯度注意力的测试时偏好优化",
    "relevance_score": 8,
    "reasoning": "该论文涉及注意力机制优化和测试时偏好优化，直接属于'Enabling Transformer Tech'中的注意力机制进展。基于文本梯度的注意力方法可应用于搜索和推荐系统中的用户偏好建模与个性化排序，通过测试时优化提升模型对用户特定偏好的适应能力。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出无需参数更新的测试时偏好优化方法，通过文本自注意力机制分析多个候选回答的优势，直接应用于推荐系统、搜索和广告领域的偏好对齐任务。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.06680v1": {
    "title": "Steering LLMs toward Korean Local Speech: Iterative Refinement Framework for Faithful Dialect Translation",
    "url": "https://www.alphaxiv.org/abs/2511.06680v1",
    "arxiv_id": "2511.06680v1",
    "authors": "Keunhyeung Park, Seunguk Yu, Youngbin Kim",
    "categories": "cs.CL",
    "pub_date": "2025-11-10 03:52:24",
    "ori_summary": "Standard-to-dialect machine translation remains challenging due to a persistent dialect gap in large language models and evaluation distortions inherent in n-gram metrics, which favor source copying over authentic dialect translation. In this paper, we propose the dialect refinement (DIA-REFINE) framework, which guides LLMs toward faithful target dialect outputs through an iterative loop of translation, verification, and feedback using external dialect classifiers. To address the limitations of n-gram-based metrics, we introduce the dialect fidelity score (DFS) to quantify linguistic shift and the target dialect ratio (TDR) to measure the success of dialect translation. Experiments on Korean dialects across zero-shot and in-context learning baselines demonstrate that DIA-REFINE consistently enhances dialect fidelity. The proposed metrics distinguish between False Success cases, where high n-gram scores obscure failures in dialectal translation, and True Attempt cases, where genuine attempts at dialectal translation yield low n-gram scores. We also observed that models exhibit varying degrees of responsiveness to the framework, and that integrating in-context examples further improves the translation of dialectal expressions. Our work establishes a robust framework for goal-directed, inclusive dialect translation, providing both rigorous evaluation and critical insights into model performance.",
    "summary": "",
    "translation": "引导大语言模型朝向韩语方言：用于忠实方言翻译的迭代优化框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注特定语言（韩语）的方言翻译任务，属于纯NLP应用领域。虽然涉及LLM技术，但缺乏与推荐系统、搜索或广告领域的明确关联。方言翻译的应用场景主要局限于语言处理，而非排名、检索或用户建模等核心业务需求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06676v1": {
    "title": "How AI Fails: An Interactive Pedagogical Tool for Demonstrating Dialectal Bias in Automated Toxicity Models",
    "url": "https://www.alphaxiv.org/abs/2511.06676v1",
    "arxiv_id": "2511.06676v1",
    "authors": "Subhojit Ghimire",
    "categories": "cs.CL, cs.CY, cs.HC",
    "pub_date": "2025-11-10 03:49:58",
    "ori_summary": "Now that AI-driven moderation has become pervasive in everyday life, we often hear claims that \"the AI is biased\". While this is often said jokingly, the light-hearted remark reflects a deeper concern. How can we be certain that an online post flagged as \"inappropriate\" was not simply the victim of a biased algorithm? This paper investigates this problem using a dual approach. First, I conduct a quantitative benchmark of a widely used toxicity model (unitary/toxic-bert) to measure performance disparity between text in African-American English (AAE) and Standard American English (SAE). The benchmark reveals a clear, systematic bias: on average, the model scores AAE text as 1.8 times more toxic and 8.8 times higher for \"identity hate\". Second, I introduce an interactive pedagogical tool that makes these abstract biases tangible. The tool's core mechanic, a user-controlled \"sensitivity threshold,\" demonstrates that the biased score itself is not the only harm; instead, the more-concerning harm is the human-set, seemingly neutral policy that ultimately operationalises discrimination. This work provides both statistical evidence of disparate impact and a public-facing tool designed to foster critical AI literacy.",
    "summary": "",
    "translation": "AI如何失败：用于展示自动化毒性模型中方言偏见的交互式教学工具",
    "relevance_score": 1,
    "reasoning": "该论文关注AI系统中的偏见检测和教学工具开发，这属于公平性、伦理和评估范畴，属于明确的无关主题。论文内容聚焦于毒性检测中的方言偏见演示工具，与推荐系统、搜索、广告的核心技术进展或LLM/Transformer架构创新没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06653v1": {
    "title": "HiMo-CLIP: Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment",
    "url": "https://www.alphaxiv.org/abs/2511.06653v1",
    "arxiv_id": "2511.06653v1",
    "authors": "Ruijia Wu, Ping Chen, Fei Shen, Shaoan Zhao, Qiang Hui, Huanlin Gao, Ting Lu, Zhaoxiang Liu, Fang Zhao, Kai Wang, Shiguo Lian",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-11-10 03:04:36",
    "ori_summary": "Contrastive vision-language models like CLIP have achieved impressive results in image-text retrieval by aligning image and text representations in a shared embedding space. However, these models often treat text as flat sequences, limiting their ability to handle complex, compositional, and long-form descriptions. In particular, they fail to capture two essential properties of language: semantic hierarchy, which reflects the multi-level compositional structure of text, and semantic monotonicity, where richer descriptions should result in stronger alignment with visual content.To address these limitations, we propose HiMo-CLIP, a representation-level framework that enhances CLIP-style models without modifying the encoder architecture. HiMo-CLIP introduces two key components: a hierarchical decomposition (HiDe) module that extracts latent semantic components from long-form text via in-batch PCA, enabling flexible, batch-aware alignment across different semantic granularities, and a monotonicity-aware contrastive loss (MoLo) that jointly aligns global and component-level representations, encouraging the model to internalize semantic ordering and alignment strength as a function of textual completeness.These components work in concert to produce structured, cognitively-aligned cross-modal representations. Experiments on multiple image-text retrieval benchmarks show that HiMo-CLIP consistently outperforms strong baselines, particularly under long or compositional descriptions. The code is available at https://github.com/UnicomAI/HiMo-CLIP.",
    "summary": "该论文研究CLIP模型在处理复杂组合文本时的语义层次和单调性建模问题，核心思想是通过层次分解模块提取文本的潜在语义组件，并结合单调性感知对比损失实现多粒度语义对齐。",
    "translation": "HiMo-CLIP：在视觉-语言对齐中建模语义层次性与单调性",
    "relevance_score": 7,
    "reasoning": "该论文提出在视觉-语言模型中对语义层次性和单调性进行建模，这与'VLM类比处理异构数据'焦点高度相关。这种层次结构建模方法可直接应用于推荐系统中处理用户兴趣层次、商品类别层次等异构数据模态的对齐问题，为构建更结构化的多模态推荐模型提供技术基础。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的层次分解和单调性建模方法可直接类比于推荐系统中处理用户行为序列和上下文特征的异构数据融合问题，为多模态建模提供了新思路。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.06645v1": {
    "title": "Adaptive Testing for Segmenting Watermarked Texts From Language Models",
    "url": "https://www.alphaxiv.org/abs/2511.06645v1",
    "arxiv_id": "2511.06645v1",
    "authors": "Xingchi Li, Xiaochi Liu, Guanxun Li",
    "categories": "stat.ML, cs.CL, cs.LG",
    "pub_date": "2025-11-10 02:50:19",
    "ori_summary": "The rapid adoption of large language models (LLMs), such as GPT-4 and Claude 3.5, underscores the need to distinguish LLM-generated text from human-written content to mitigate the spread of misinformation and misuse in education. One promising approach to address this issue is the watermark technique, which embeds subtle statistical signals into LLM-generated text to enable reliable identification. In this paper, we first generalize the likelihood-based LLM detection method of a previous study by introducing a flexible weighted formulation, and further adapt this approach to the inverse transform sampling method. Moving beyond watermark detection, we extend this adaptive detection strategy to tackle the more challenging problem of segmenting a given text into watermarked and non-watermarked substrings. In contrast to the approach in a previous study, which relies on accurate estimation of next-token probabilities that are highly sensitive to prompt estimation, our proposed framework removes the need for precise prompt estimation. Extensive numerical experiments demonstrate that the proposed methodology is both effective and robust in accurately segmenting texts containing a mixture of watermarked and non-watermarked content.",
    "summary": "",
    "translation": "从语言模型中分割水印文本的自适应测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注水印检测和文本分割，这属于安全和水印技术领域，与我的核心关注点（推荐系统、搜索、广告的核心进展及LLM应用）无关。虽然涉及语言模型，但重点在于水印而非模型架构改进或直接应用，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06618v1": {
    "title": "GRAPH-GRPO-LEX: Contract Graph Modeling and Reinforcement Learning with Group Relative Policy Optimization",
    "url": "https://www.alphaxiv.org/abs/2511.06618v1",
    "arxiv_id": "2511.06618v1",
    "authors": "Moriya Dechtiar, Daniel Martin Katz, Mari Sundaresan, Sylvain Jaume, Hongming Wang",
    "categories": "cs.AI, cs.CL, cs.LG, cs.SE",
    "pub_date": "2025-11-10 01:57:51",
    "ori_summary": "Contracts are complex documents featuring detailed formal structures, explicit and implicit dependencies and rich semantic content. Given these document properties, contract drafting and manual examination of contracts have proven to be both arduous and susceptible to errors. This work aims to simplify and automate the task of contract review and analysis using a novel framework for transforming legal contracts into structured semantic graphs, enabling computational analysis and data-driven insights. We introduce a detailed ontology mapping core legal contract elements to their graph-theoretic equivalents of nodes and edges. We then present a reinforcement learning based Large Language Model (LLM) framework for segmentation and extraction of entities and relationships from contracts. Our method, GRAPH-GRPO-LEX, incorporates both LLMs and reinforcement learning with group relative policy optimization (GRPO). By applying a carefully drafted reward function of graph metrics, we demonstrate the ability to automatically identify direct relationships between clauses, and even uncover hidden dependencies. Our introduction of the gated GRPO approach shows a strong learning signal and can move contract analysis from a linear, manual reading process to an easily visualized graph. This allows for a more dynamic analysis, including building the groundwork for contract linting similar to what is now practiced in software engineering.",
    "summary": "",
    "translation": "GRAPH-GRPO-LEX：合约图建模与基于群体相对策略优化的强化学习",
    "relevance_score": 4,
    "reasoning": "该论文涉及图建模和强化学习（GRPO），这与推荐系统中的用户-物品图建模和序列决策有一定关联。然而，论文标题未明确说明其与推荐系统、搜索或广告的具体应用，且强化学习部分需要进一步明确是否直接适用于这些领域，因此相关性中等。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2511.06601v1": {
    "title": "Duality-based Mode Operations and Pyramid Multilayer Mapping for Rhetorical Modes",
    "url": "https://www.alphaxiv.org/abs/2511.06601v1",
    "arxiv_id": "2511.06601v1",
    "authors": "Zi-Niu Wu",
    "categories": "cs.CL, cs.FL, cs.PL",
    "pub_date": "2025-11-10 01:17:00",
    "ori_summary": "Rhetorical modes are useful in both academic and non-academic writing, and can be subjects to be studied within linguistic research and computational modeling. Establishing a conceptual bridge among these domains could enable each to benefit from the others. This paper proposes duality-based mode operations (split-unite, forward-backward, expansion-reduction and orthogonal dualities) to expand the set of rhetorical modes, introducing generated modes like combination and generalization, thereby enhancing epistemic diversity across multiple applications. It further presents a pyramid multilayer mapping framework (e.g., three layers from the rhetorical model layer, to cognitive layer, and to epistemic layers) that reduces the resulting cognitive complexity. The degrees of expressive diversity and complexity reduction are quantified through binomial combinatorics and Shannon entropy analysis. A Marginal Rhetorical Bit (MRB) is identified, permitting the definition of a rhetorical-scalable parameter that measures expressive growth speed in bits per stage. A direct entropy measure shows that hierarchical selection over smaller subsets markedly reduces choice uncertainty compared with flat selection across all modes. These considerations appear to transform static and non-measurable rhetorical taxonomies into more dynamic and more measurable systems for discourse design. From this work, it would be possible to identify a pathway for future AI systems to operate not only on language tokens but on layered rhetorical reasoning structures, bridging linguistic, pedagogical, academic, and computational research",
    "summary": "",
    "translation": "基于对偶的模式操作与金字塔多层映射用于修辞模式",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于修辞模式分析，属于语言学领域，与推荐系统、搜索或广告的核心技术无关。标题中提到的对偶操作和金字塔映射等技术概念在自然语言处理中可能用于文本分析，但缺乏明确的推荐、搜索或广告应用场景。该研究更偏向于纯语言学研究，而非可应用于大规模推荐或搜索系统的技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06592v1": {
    "title": "MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making",
    "url": "https://www.alphaxiv.org/abs/2511.06592v1",
    "arxiv_id": "2511.06592v1",
    "authors": "Zhi Rui Tam, Yun-Nung Chen",
    "categories": "cs.CL, eess.AS",
    "pub_date": "2025-11-10 00:44:37",
    "ori_summary": "As large language models transition from text-based interfaces to audio interactions in clinical settings, they might introduce new vulnerabilities through paralinguistic cues in audio. We evaluated these models on 170 clinical cases, each synthesized into speech from 36 distinct voice profiles spanning variations in age, gender, and emotion. Our findings reveal a severe modality bias: surgical recommendations for audio inputs varied by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer recommendations. Further analysis uncovered age disparities of up to 12% between young and elderly voices, which persisted in most models despite chain-of-thought prompting. While explicit reasoning successfully eliminated gender bias, the impact of emotion was not detected due to poor recognition performance. These results demonstrate that audio LLMs are susceptible to making clinical decisions based on a patient's voice characteristics rather than medical evidence, a flaw that risks perpetuating healthcare disparities. We conclude that bias-aware architectures are essential and urgently needed before the clinical deployment of these models.",
    "summary": "",
    "translation": "MedVoiceBias：临床决策中音频大语言模型行为的对照研究",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医疗领域的音频LLM应用和临床决策，属于明确的医疗领域特定应用。标题中没有任何元素表明与推荐系统、搜索或广告相关，也不涉及任何可能在这些领域应用的通用LLM技术进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07418v1": {
    "title": "Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields",
    "url": "https://www.alphaxiv.org/abs/2511.07418v1",
    "arxiv_id": "2511.07418v1",
    "authors": "Zhao-Heng Yin, Pieter Abbeel",
    "categories": "cs.RO, cs.AI, cs.CV, cs.DC, cs.GR",
    "pub_date": "2025-11-10 18:59:44",
    "ori_summary": "Despite years of research, real-time diverse grasp synthesis for dexterous hands remains an unsolved core challenge in robotics and computer graphics. We present Lightning Grasp, a novel high-performance procedural grasp synthesis algorithm that achieves orders-of-magnitude speedups over state-of-the-art approaches, while enabling unsupervised grasp generation for irregular, tool-like objects. The method avoids many limitations of prior approaches, such as the need for carefully tuned energy functions and sensitive initialization. This breakthrough is driven by a key insight: decoupling complex geometric computation from the search process via a simple, efficient data structure - the Contact Field. This abstraction collapses the problem complexity, enabling a procedural search at unprecedented speeds. We open-source our system to propel further innovation in robotic manipulation.",
    "summary": "",
    "translation": "闪电抓取：基于接触场的高性能程序化抓取合成",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人抓取合成技术，属于纯粹的机器人学和计算机视觉领域。虽然涉及高性能计算，但没有任何与推荐系统、搜索或广告相关的技术元素或潜在应用。论文内容完全在指定的无关主题范围内，特别是纯粹的视觉和机器人应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07416v1": {
    "title": "Robot Learning from a Physical World Model",
    "url": "https://www.alphaxiv.org/abs/2511.07416v1",
    "arxiv_id": "2511.07416v1",
    "authors": "Jiageng Mao, Sicheng He, Hao-Ning Wu, Yang You, Shuyang Sun, Zhicheng Wang, Yanan Bao, Huizhong Chen, Leonidas Guibas, Vitor Guizilini, Howard Zhou, Yue Wang",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2025-11-10 18:59:07",
    "ori_summary": "We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit \\href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage} for details.",
    "summary": "",
    "translation": "机器人从物理世界模型中学习",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于机器人学习和物理世界建模，这与推荐系统、搜索或广告领域没有直接关联。机器人学习属于机器人技术领域，物理世界建模涉及物理模拟和环境交互，无法看出其在RecSys/Search/Ads中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07412v1": {
    "title": "TwinOR: Photorealistic Digital Twins of Dynamic Operating Rooms for Embodied AI Research",
    "url": "https://www.alphaxiv.org/abs/2511.07412v1",
    "arxiv_id": "2511.07412v1",
    "authors": "Han Zhang, Yiqing Shen, Roger D. Soberanis-Mukul, Ankita Ghosh, Hao Ding, Lalithkumar Seenivasan, Jose L. Porras, Zhekai Mao, Chenjia Li, Wenjie Xiao, Lonny Yarmus, Angela Christine Argento, Masaru Ishii, Mathias Unberath",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-11-10 18:57:09",
    "ori_summary": "Developing embodied AI for intelligent surgical systems requires safe, controllable environments for continual learning and evaluation. However, safety regulations and operational constraints in operating rooms (ORs) limit embodied agents from freely perceiving and interacting in realistic settings. Digital twins provide high-fidelity, risk-free environments for exploration and training. How we may create photorealistic and dynamic digital representations of ORs that capture relevant spatial, visual, and behavioral complexity remains unclear. We introduce TwinOR, a framework for constructing photorealistic, dynamic digital twins of ORs for embodied AI research. The system reconstructs static geometry from pre-scan videos and continuously models human and equipment motion through multi-view perception of OR activities. The static and dynamic components are fused into an immersive 3D environment that supports controllable simulation and embodied exploration. The proposed framework reconstructs complete OR geometry with centimeter level accuracy while preserving dynamic interaction across surgical workflows, enabling realistic renderings and a virtual playground for embodied AI systems. In our experiments, TwinOR simulates stereo and monocular sensor streams for geometry understanding and visual localization tasks. Models such as FoundationStereo and ORB-SLAM3 on TwinOR-synthesized data achieve performance within their reported accuracy on real indoor datasets, demonstrating that TwinOR provides sensor-level realism sufficient for perception and localization challenges. By establishing a real-to-sim pipeline for constructing dynamic, photorealistic digital twins of OR environments, TwinOR enables the safe, scalable, and data-efficient development and benchmarking of embodied AI, ultimately accelerating the deployment of embodied AI from sim-to-real.",
    "summary": "",
    "translation": "TwinOR：用于具身AI研究的动态手术室真实数字孪生",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域手术室的数字孪生技术，属于医疗应用范畴，与推荐系统、搜索或广告领域完全无关。论文标题明确指向具身AI在医疗环境中的研究，不涉及任何推荐、搜索或广告相关的技术或应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07409v1": {
    "title": "DIMO: Diverse 3D Motion Generation for Arbitrary Objects",
    "url": "https://www.alphaxiv.org/abs/2511.07409v1",
    "arxiv_id": "2511.07409v1",
    "authors": "Linzhan Mou, Jiahui Lei, Chen Wang, Lingjie Liu, Kostas Daniilidis",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 18:56:49",
    "ori_summary": "We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation. Our project page is available at https://linzhanm.github.io/dimo.",
    "summary": "",
    "translation": "DIMO：面向任意物体的多样化3D运动生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D运动生成，属于计算机视觉和图形学领域，与推荐系统、搜索或广告的核心技术栈无直接关联。3D运动生成技术主要应用于动画、游戏、虚拟现实等场景，缺乏在RecSys/Search/Ads领域的明确应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07399v1": {
    "title": "StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation",
    "url": "https://www.alphaxiv.org/abs/2511.07399v1",
    "arxiv_id": "2511.07399v1",
    "authors": "Tianrui Feng, Zhi Li, Shuo Yang, Haocheng Xi, Muyang Li, Xiuyu Li, Lvmin Zhang, Keting Yang, Kelly Peng, Song Han, Maneesh Agrawala, Kurt Keutzer, Akio Kodaira, Chenfeng Xu",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-11-10 18:51:28",
    "ori_summary": "Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms.",
    "summary": "",
    "translation": "StreamDiffusionV2：用于动态交互式视频生成的流式系统",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频生成系统，属于纯粹的视觉内容生成领域。虽然标题提到'交互式'，但核心是视频生成技术，与推荐系统、搜索或广告的排名和匹配任务没有直接关联。这属于被明确排除的'纯粹视觉'和'AIGC/内容生成'类别。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07377v1": {
    "title": "Real-Time LiDAR Super-Resolution via Frequency-Aware Multi-Scale Fusion",
    "url": "https://www.alphaxiv.org/abs/2511.07377v1",
    "arxiv_id": "2511.07377v1",
    "authors": "June Moh Goo, Zichao Zeng, Jan Boehm",
    "categories": "cs.CV, cs.AI, cs.RO",
    "pub_date": "2025-11-10 18:38:15",
    "ori_summary": "LiDAR super-resolution addresses the challenge of achieving high-quality 3D perception from cost-effective, low-resolution sensors. While recent transformer-based approaches like TULIP show promise, they remain limited to spatial-domain processing with restricted receptive fields. We introduce FLASH (Frequency-aware LiDAR Adaptive Super-resolution with Hierarchical fusion), a novel framework that overcomes these limitations through dual-domain processing. FLASH integrates two key innovations: (i) Frequency-Aware Window Attention that combines local spatial attention with global frequency-domain analysis via FFT, capturing both fine-grained geometry and periodic scanning patterns at log-linear complexity. (ii) Adaptive Multi-Scale Fusion that replaces conventional skip connections with learned position-specific feature aggregation, enhanced by CBAM attention for dynamic feature selection. Extensive experiments on KITTI demonstrate that FLASH achieves state-of-the-art performance across all evaluation metrics, surpassing even uncertainty-enhanced baselines that require multiple forward passes. Notably, FLASH outperforms TULIP with Monte Carlo Dropout while maintaining single-pass efficiency, which enables real-time deployment. The consistent superiority across all distance ranges validates that our dual-domain approach effectively handles uncertainty through architectural design rather than computationally expensive stochastic inference, making it practical for autonomous systems.",
    "summary": "",
    "translation": "基于频率感知多尺度融合的实时激光雷达超分辨率",
    "relevance_score": 1,
    "reasoning": "该论文专注于激光雷达传感器的超分辨率技术，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术栈没有直接关联。激光雷达主要用于自动驾驶和机器人感知，其超分辨率技术难以转化为推荐、搜索或广告场景中的实际应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07362v1": {
    "title": "Inference-Time Scaling of Diffusion Models for Infrared Data Generation",
    "url": "https://www.alphaxiv.org/abs/2511.07362v1",
    "arxiv_id": "2511.07362v1",
    "authors": "Kai A. Horstmann, Maxim Clouser, Kia Khezeli",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-11-10 18:18:38",
    "ori_summary": "Infrared imagery enables temperature-based scene understanding using passive sensors, particularly under conditions of low visibility where traditional RGB imaging fails. Yet, developing downstream vision models for infrared applications is hindered by the scarcity of high-quality annotated data, due to the specialized expertise required for infrared annotation. While synthetic infrared image generation has the potential to accelerate model development by providing large-scale, diverse training data, training foundation-level generative diffusion models in the infrared domain has remained elusive due to limited datasets. In light of such data constraints, we explore an inference-time scaling approach using a domain-adapted CLIP-based verifier for enhanced infrared image generation quality. We adapt FLUX.1-dev, a state-of-the-art text-to-image diffusion model, to the infrared domain by finetuning it on a small sample of infrared images using parameter-efficient techniques. The trained verifier is then employed during inference to guide the diffusion sampling process toward higher quality infrared generations that better align with input text prompts. Empirically, we find that our approach leads to consistent improvements in generation quality, reducing FID scores on the KAIST Multispectral Pedestrian Detection Benchmark dataset by 10% compared to unguided baseline samples. Our results suggest that inference-time guidance offers a promising direction for bridging the domain gap in low-data infrared settings.",
    "summary": "",
    "translation": "扩散模型在推理时缩放用于红外数据生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于红外数据生成和扩散模型推理优化，这属于计算机视觉领域的特定应用。红外数据生成与推荐系统、搜索或广告没有直接关联，扩散模型主要用于内容生成而非排名或检索任务。论文内容属于纯粹的视觉生成技术，没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07329v1": {
    "title": "Preparation of Fractal-Inspired Computational Architectures for Advanced Large Language Model Analysis",
    "url": "https://www.alphaxiv.org/abs/2511.07329v1",
    "arxiv_id": "2511.07329v1",
    "authors": "Yash Mittal, Dmitry Ignatov, Radu Timofte",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-11-10 17:31:39",
    "ori_summary": "It introduces FractalNet, a fractal-inspired computational architectures for advanced large language model analysis that mainly challenges model diversity on a large scale in an efficient manner. The new set-up involves a template-driven generator, runner, and evaluation framework that, through systematic permutations of convolutional, normalization, activation, and dropout layers, can create more than 1,200 variants of neural networks. Fractal templates allow for structural recursion and multi-column pathways, thus, models become deeper and wider in a balanced way. Training utilizes PyTorch, Automatic Mixed Precision (AMP), and gradient checkpointing and is carried out on the CIFAR-10 dataset for five epochs. The outcomes show that fractal-based architectures are capable of strong performance and are computationally efficient. The paper positions fractal design as a feasible and resource-efficient method of automated architecture exploration.",
    "summary": "",
    "translation": "面向高级大语言模型分析的分形启发计算架构的构建",
    "relevance_score": 3,
    "reasoning": "该论文涉及计算架构设计，可能属于Transformer架构效率改进的范畴，但标题过于模糊，没有明确说明与推荐系统、搜索或广告的具体联系。分形启发架构可能带来计算效率提升，但缺乏明确的应用场景指向，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07325v1": {
    "title": "Garbage Vulnerable Point Monitoring using IoT and Computer Vision",
    "url": "https://www.alphaxiv.org/abs/2511.07325v1",
    "arxiv_id": "2511.07325v1",
    "authors": "R. Kumar, A. Lall, S. Chaudhari, M. Kale, A. Vattem",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-11-10 17:27:51",
    "ori_summary": "This paper proposes a smart way to manage municipal solid waste by using the Internet of Things (IoT) and computer vision (CV) to monitor illegal waste dumping at garbage vulnerable points (GVPs) in urban areas. The system can quickly detect and monitor dumped waste using a street-level camera and object detection algorithm. Data was collected from the Sangareddy district in Telangana, India. A series of comprehensive experiments was carried out using the proposed dataset to assess the accuracy and overall performance of various object detection models. Specifically, we performed an in-depth evaluation of YOLOv8, YOLOv10, YOLO11m, and RT-DETR on our dataset. Among these models, YOLO11m achieved the highest accuracy of 92.39\\% in waste detection, demonstrating its effectiveness in detecting waste. Additionally, it attains an mAP@50 of 0.91, highlighting its high precision. These findings confirm that the object detection model is well-suited for monitoring and tracking waste dumping events at GVP locations. Furthermore, the system effectively captures waste disposal patterns, including hourly, daily, and weekly dumping trends, ensuring comprehensive daily and nightly monitoring.",
    "summary": "",
    "translation": "基于物联网和计算机视觉的垃圾易损点监测",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于物联网和计算机视觉在垃圾监测领域的应用，属于环境监测或智慧城市范畴。标题中没有任何元素表明与推荐系统、搜索、广告、LLM技术或Transformer架构相关，完全超出了用户关注的技术领域范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07321v1": {
    "title": "YoNoSplat: You Only Need One Model for Feedforward 3D Gaussian Splatting",
    "url": "https://www.alphaxiv.org/abs/2511.07321v1",
    "arxiv_id": "2511.07321v1",
    "authors": "Botao Ye, Boqi Chen, Haofei Xu, Daniel Barath, Marc Pollefeys",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 17:21:54",
    "ori_summary": "Fast and flexible 3D scene reconstruction from unstructured image collections remains a significant challenge. We present YoNoSplat, a feedforward model that reconstructs high-quality 3D Gaussian Splatting representations from an arbitrary number of images. Our model is highly versatile, operating effectively with both posed and unposed, calibrated and uncalibrated inputs. YoNoSplat predicts local Gaussians and camera poses for each view, which are aggregated into a global representation using either predicted or provided poses. To overcome the inherent difficulty of jointly learning 3D Gaussians and camera parameters, we introduce a novel mixing training strategy. This approach mitigates the entanglement between the two tasks by initially using ground-truth poses to aggregate local Gaussians and gradually transitioning to a mix of predicted and ground-truth poses, which prevents both training instability and exposure bias. We further resolve the scale ambiguity problem by a novel pairwise camera-distance normalization scheme and by embedding camera intrinsics into the network. Moreover, YoNoSplat also predicts intrinsic parameters, making it feasible for uncalibrated inputs. YoNoSplat demonstrates exceptional efficiency, reconstructing a scene from 100 views (at 280x518 resolution) in just 2.69 seconds on an NVIDIA GH200 GPU. It achieves state-of-the-art performance on standard benchmarks in both pose-free and pose-dependent settings. Our project page is at https://botaoye.github.io/yonosplat/.",
    "summary": "",
    "translation": "YoNoSplat：仅需单一模型实现前向传播的3D高斯泼溅",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D视觉领域的渲染技术优化，属于纯粹的计算机图形学范畴。标题中提到的3D高斯泼溅和单一模型前向传播方法与推荐系统、搜索或广告的核心技术栈没有直接关联，也不涉及Transformer架构、LLM技术或异构数据建模等当前关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07301v1": {
    "title": "Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection",
    "url": "https://www.alphaxiv.org/abs/2511.07301v1",
    "arxiv_id": "2511.07301v1",
    "authors": "Huizai Yao, Sicheng Zhao, Pengteng Li, Yi Cui, Shuo Lu, Weiyu Guo, Yunfan Lu, Yijie Xu, Hui Xiong",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-10 17:06:01",
    "ori_summary": "Source-Free Object Detection (SFOD) aims to adapt a source-pretrained object detector to a target domain without access to source data. However, existing SFOD methods predominantly rely on internal knowledge from the source model, which limits their capacity to generalize across domains and often results in biased pseudo-labels, thereby hindering both transferability and discriminability. In contrast, Vision Foundation Models (VFMs), pretrained on massive and diverse data, exhibit strong perception capabilities and broad generalization, yet their potential remains largely untapped in the SFOD setting. In this paper, we propose a novel SFOD framework that leverages VFMs as external knowledge sources to jointly enhance feature alignment and label quality. Specifically, we design three VFM-based modules: (1) Patch-weighted Global Feature Alignment (PGFA) distills global features from VFMs using patch-similarity-based weighting to enhance global feature transferability; (2) Prototype-based Instance Feature Alignment (PIFA) performs instance-level contrastive learning guided by momentum-updated VFM prototypes; and (3) Dual-source Enhanced Pseudo-label Fusion (DEPF) fuses predictions from detection VFMs and teacher models via an entropy-aware strategy to yield more reliable supervision. Extensive experiments on six benchmarks demonstrate that our method achieves state-of-the-art SFOD performance, validating the effectiveness of integrating VFMs to simultaneously improve transferability and discriminability.",
    "summary": "",
    "translation": "超越边界：利用视觉基础模型进行无源目标检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的目标检测任务，特别是无源域适应场景。虽然提到了基础模型概念，但核心内容纯粹围绕视觉任务，没有任何与推荐系统、搜索或广告相关的技术连接或潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07298v1": {
    "title": "LMM-IQA: Image Quality Assessment for Low-Dose CT Imaging",
    "url": "https://www.alphaxiv.org/abs/2511.07298v1",
    "arxiv_id": "2511.07298v1",
    "authors": "Kagan Celik, Mehmet Ozan Unal, Metin Ertas, Isa Yildirim",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-10 16:56:11",
    "ori_summary": "Low-dose computed tomography (CT) represents a significant improvement in patient safety through lower radiation doses, but increased noise, blur, and contrast loss can diminish diagnostic quality. Therefore, consistency and robustness in image quality assessment become essential for clinical applications. In this study, we propose an LLM-based quality assessment system that generates both numerical scores and textual descriptions of degradations such as noise, blur, and contrast loss. Furthermore, various inference strategies - from the zero-shot approach to metadata integration and error feedback - are systematically examined, demonstrating the progressive contribution of each method to overall performance. The resultant assessments yield not only highly correlated scores but also interpretable output, thereby adding value to clinical workflows. The source codes of our study are available at https://github.com/itu-biai/lmms_ldct_iqa.",
    "summary": "",
    "translation": "LMM-IQA：低剂量CT成像的图像质量评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像领域的图像质量评估，属于医疗应用范畴，与推荐系统、搜索或广告无关。低剂量CT成像和图像质量评估技术没有明显的潜在应用可以转移到RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07299v1": {
    "title": "VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2511.07299v1",
    "arxiv_id": "2511.07299v1",
    "authors": "Ying Cheng, Yu-Ho Lin, Min-Hung Chen, Fu-En Yang, Shang-Hong Lai",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 16:56:11",
    "ori_summary": "Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. A Relation Feature Extractor and a COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis.",
    "summary": "",
    "translation": "VADER：基于关系感知大语言模型的因果视频异常理解",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频异常检测，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然提到了大语言模型，但其应用场景局限于视频理解，没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07293v1": {
    "title": "Verifying rich robustness properties for neural networks",
    "url": "https://www.alphaxiv.org/abs/2511.07293v1",
    "arxiv_id": "2511.07293v1",
    "authors": "Mohammad Afzal, S. Akshay, Ashutosh Gupta",
    "categories": "cs.LO, cs.AI, cs.CV",
    "pub_date": "2025-11-10 16:43:02",
    "ori_summary": "Robustness is a important problem in AI alignment and safety, with models such as neural networks being increasingly used in safety-critical systems. In the last decade, a large body of work has emerged on local robustness, i.e., checking if the decision of a neural network remains unchanged when the input is slightly perturbed. However, many of these approaches require specialized encoding and often ignore the confidence of a neural network on its output. In this paper, our goal is to build a generalized framework to specify and verify variants of robustness in neural network verification. We propose a specification framework using a simple grammar, which is flexible enough to capture most existing variants. This allows us to introduce new variants of robustness that take into account the confidence of the neural network in its outputs. Next, we develop a novel and powerful unified technique to verify all such variants in a homogeneous way, viz., by adding a few additional layers to the neural network. This enables us to use any state-of-the-art neural network verification tool, without having to tinker with the encoding within, while incurring an approximation error that we show is bounded. We perform an extensive experimental evaluation over a large suite of 8870 benchmarks having 138M parameters in a largest network, and show that we are able to capture a wide set of robustness variants and outperform direct encoding approaches by a significant margin.",
    "summary": "",
    "translation": "验证神经网络的丰富鲁棒性属性",
    "relevance_score": 2,
    "reasoning": "该论文聚焦于神经网络的形式化验证和鲁棒性分析，属于通用机器学习安全领域。虽然鲁棒性在推荐和搜索系统中具有一定重要性，但该工作主要关注理论验证方法而非具体的推荐/搜索应用，且缺乏与LLM、Transformer架构或异构数据建模的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07292v1": {
    "title": "PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving",
    "url": "https://www.alphaxiv.org/abs/2511.07292v1",
    "arxiv_id": "2511.07292v1",
    "authors": "Simon Gerstenecker, Andreas Geiger, Katrin Renz",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-11-10 16:41:47",
    "ori_summary": "Most recent work in autonomous driving has prioritized benchmark performance and methodological innovation over in-depth analysis of model failures, biases, and shortcut learning. This has led to incremental improvements without a deep understanding of the current failures. While it is straightforward to look at situations where the model fails, it is hard to understand the underlying reason. This motivates us to conduct a systematic study, where inputs to the model are perturbed and the predictions observed. We introduce PlanT 2.0, a lightweight, object-centric planning transformer designed for autonomous driving research in CARLA. The object-level representation enables controlled analysis, as the input can be easily perturbed (e.g., by changing the location or adding or removing certain objects), in contrast to sensor-based models. To tackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0, we introduce multiple upgrades to PlanT, achieving state-of-the-art performance on Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysis exposes insightful failures, such as a lack of scene understanding caused by low obstacle diversity, rigid expert behaviors leading to exploitable shortcuts, and overfitting to a fixed set of expert trajectories. Based on these findings, we argue for a shift toward data-centric development, with a focus on richer, more robust, and less biased datasets. We open-source our code and model at https://github.com/autonomousvision/plant2.",
    "summary": "",
    "translation": "PlanT 2.0：揭示闭环驾驶中的偏见与结构缺陷",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于自动驾驶领域的闭环系统评估，涉及偏见检测和系统结构分析。这与推荐系统、搜索或广告的核心技术、LLM应用或Transformer架构进展均无直接关联，且未提及任何可能应用于这些领域的潜在技术迁移。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07290v1": {
    "title": "CAMP-VQA: Caption-Embedded Multimodal Perception for No-Reference Quality Assessment of Compressed Video",
    "url": "https://www.alphaxiv.org/abs/2511.07290v1",
    "arxiv_id": "2511.07290v1",
    "authors": "Xinyi Wang, Angeliki Katsenou, Junxiao Shen, David Bull",
    "categories": "eess.IV, cs.CV, cs.MM",
    "pub_date": "2025-11-10 16:37:47",
    "ori_summary": "The prevalence of user-generated content (UGC) on platforms such as YouTube and TikTok has rendered no-reference (NR) perceptual video quality assessment (VQA) vital for optimizing video delivery. Nonetheless, the characteristics of non-professional acquisition and the subsequent transcoding of UGC video on sharing platforms present significant challenges for NR-VQA. Although NR-VQA models attempt to infer mean opinion scores (MOS), their modeling of subjective scores for compressed content remains limited due to the absence of fine-grained perceptual annotations of artifact types. To address these challenges, we propose CAMP-VQA, a novel NR-VQA framework that exploits the semantic understanding capabilities of large vision-language models. Our approach introduces a quality-aware prompting mechanism that integrates video metadata (e.g., resolution, frame rate, bitrate) with key fragments extracted from inter-frame variations to guide the BLIP-2 pretraining approach in generating fine-grained quality captions. A unified architecture has been designed to model perceptual quality across three dimensions: semantic alignment, temporal characteristics, and spatial characteristics. These multimodal features are extracted and fused, then regressed to video quality scores. Extensive experiments on a wide variety of UGC datasets demonstrate that our model consistently outperforms existing NR-VQA methods, achieving improved accuracy without the need for costly manual fine-grained annotations. Our method achieves the best performance in terms of average rank and linear correlation (SRCC: 0.928, PLCC: 0.938) compared to state-of-the-art methods. The source code and trained models, along with a user-friendly demo, are available at: https://github.com/xinyiW915/CAMP-VQA.",
    "summary": "",
    "translation": "CAMP-VQA：用于压缩视频无参考质量评估的标题嵌入多模态感知",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频压缩质量评估，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术无关。虽然涉及多模态处理，但主要应用于视频质量评估而非推荐或搜索场景中的异构数据建模。没有明确的潜在应用连接到RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07286v1": {
    "title": "Glioma C6: A Novel Dataset for Training and Benchmarking Cell Segmentation",
    "url": "https://www.alphaxiv.org/abs/2511.07286v1",
    "arxiv_id": "2511.07286v1",
    "authors": "Roman Malashin, Svetlana Pashkevich, Daniil Ilyukhin, Arseniy Volkov, Valeria Yachnaya, Andrey Denisov, Maria Mikhalkova",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-10 16:33:34",
    "ori_summary": "We present Glioma C6, a new open dataset for instance segmentation of glioma C6 cells, designed as both a benchmark and a training resource for deep learning models. The dataset comprises 75 high-resolution phase-contrast microscopy images with over 12,000 annotated cells, providing a realistic testbed for biomedical image analysis. It includes soma annotations and morphological cell categorization provided by biologists. Additional categorization of cells, based on morphology, aims to enhance the utilization of image data for cancer cell research. Glioma C6 consists of two parts: the first is curated with controlled parameters for benchmarking, while the second supports generalization testing under varying conditions. We evaluate the performance of several generalist segmentation models, highlighting their limitations on our dataset. Our experiments demonstrate that training on Glioma C6 significantly enhances segmentation performance, reinforcing its value for developing robust and generalizable models. The dataset is publicly available for researchers.",
    "summary": "",
    "translation": "胶质瘤C6：用于细胞分割训练与基准测试的新型数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的细胞分割数据集，属于生物医学应用范畴，与推荐系统、搜索或广告领域完全无关。论文内容涉及医学图像分析，属于明确排除的领域特定应用，没有任何技术要素与当前关注点相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07281v1": {
    "title": "Segmentation of Ischemic Stroke Lesions using Transfer Learning on Multi-sequence MRI",
    "url": "https://www.alphaxiv.org/abs/2511.07281v1",
    "arxiv_id": "2511.07281v1",
    "authors": "R. P. Chowdhury, T. Rahman",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 16:27:25",
    "ori_summary": "The accurate understanding of ischemic stroke lesions is critical for efficient therapy and prognosis of stroke patients. Magnetic resonance imaging (MRI) is sensitive to acute ischemic stroke and is a common diagnostic method for stroke. However, manual lesion segmentation performed by experts is tedious, time-consuming, and prone to observer inconsistency. Automatic medical image analysis methods have been proposed to overcome this challenge. However, previous approaches have relied on hand-crafted features that may not capture the irregular and physiologically complex shapes of ischemic stroke lesions. In this study, we present a novel framework for quickly and automatically segmenting ischemic stroke lesions on various MRI sequences, including T1-weighted, T2-weighted, DWI, and FLAIR. The proposed methodology is validated on the ISLES 2015 Brain Stroke sequence dataset, where we trained our model using the Res-Unet architecture twice: first, with pre-existing weights, and then without, to explore the benefits of transfer learning. Evaluation metrics, including the Dice score and sensitivity, were computed across 3D volumes. Finally, a Majority Voting Classifier was integrated to amalgamate the outcomes from each axis, resulting in a comprehensive segmentation method. Our efforts culminated in achieving a Dice score of 80.5\\% and an accuracy of 74.03\\%, showcasing the efficacy of our segmentation approach.",
    "summary": "",
    "translation": "基于多序列MRI的迁移学习在缺血性脑卒中病灶分割中的应用",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像分析中的脑卒中病灶分割，属于医学/生物学领域的特定应用。论文内容涉及医学图像处理和迁移学习，与推荐系统、搜索、广告或相关使能技术没有任何直接或间接关联。这完全属于被排除的领域特定应用范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07278v1": {
    "title": "StreamKV: Streaming Video Question-Answering with Segment-based KV Cache Retrieval and Compression",
    "url": "https://www.alphaxiv.org/abs/2511.07278v1",
    "arxiv_id": "2511.07278v1",
    "authors": "Yilong Chen, Xiang Bai, Zhibin Wang, Chengyu Bai, Yuhan Dai, Ming Lu, Shanghang Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 16:25:03",
    "ori_summary": "Video Large Language Models (Video-LLMs) have demonstrated significant potential in the areas of video captioning, search, and summarization. However, current Video-LLMs still face challenges with long real-world videos. Recent methods have introduced a retrieval mechanism that retrieves query-relevant KV caches for question answering, enhancing the efficiency and accuracy of long real-world videos. However, the compression and retrieval of KV caches are still not fully explored. In this paper, we propose \\textbf{StreamKV}, a training-free framework that seamlessly equips Video-LLMs with advanced KV cache retrieval and compression. Compared to previous methods that used uniform partitioning, StreamKV dynamically partitions video streams into semantic segments, which better preserves semantic information. For KV cache retrieval, StreamKV calculates a summary vector for each segment to retain segment-level information essential for retrieval. For KV cache compression, StreamKV introduces a guidance prompt designed to capture the key semantic elements within each segment, ensuring only the most informative KV caches are retained for answering questions. Moreover, StreamKV unifies KV cache retrieval and compression within a single module, performing both in a layer-adaptive manner, thereby further improving the effectiveness of streaming video question answering. Extensive experiments on public StreamingVQA benchmarks demonstrate that StreamKV significantly outperforms existing Online Video-LLMs, achieving superior accuracy while substantially improving both memory efficiency and computational latency. The code has been released at https://github.com/sou1p0wer/StreamKV.",
    "summary": "",
    "translation": "StreamKV：基于分段KV缓存检索与压缩的流式视频问答",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频问答任务，属于计算机视觉与自然语言处理的交叉领域，与推荐系统、搜索或广告的核心技术关联较弱。虽然KV缓存技术可能对Transformer效率有潜在贡献，但论文聚焦于视频流处理这一特定场景，在推荐/搜索/广告领域的直接应用潜力有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07253v1": {
    "title": "Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2511.07253v1",
    "arxiv_id": "2511.07253v1",
    "authors": "Umberto Cappellazzo, Xubo Liu, Pingchuan Ma, Stavros Petridis, Maja Pantic",
    "categories": "eess.AS, cs.CV, cs.SD",
    "pub_date": "2025-11-10 16:03:44",
    "ori_summary": "Large language models (LLMs) have recently achieved impressive results in speech recognition across multiple modalities, including Auditory Speech Recognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech Recognition (AVSR). Despite this progress, current LLM-based approaches typically address each task independently, training separate models that raise computational and deployment resource use while missing potential cross-task synergies. They also rely on fixed-rate token compression, which restricts flexibility in balancing accuracy with efficiency. These limitations highlight the need for a unified framework that can support ASR, VSR, and AVSR while enabling elastic inference. To this end, we present Omni-AVSR, a unified audio-visual LLM that combines efficient multi-granularity training with parameter-efficient adaptation. Specifically, we adapt the matryoshka representation learning paradigm to efficiently train across multiple audio and visual granularities, reducing its inherent training resource use. Furthermore, we explore three LoRA-based strategies for adapting the backbone LLM, balancing shared and task-specific specialization. Experiments on LRS2 and LRS3 show that Omni-AVSR achieves comparable or superior accuracy to state-of-the-art baselines while training a single model at substantially lower training and deployment resource use. The model also remains robust under acoustic noise, and we analyze its scaling behavior as LLM size increases, providing insights into the trade-off between performance and efficiency.",
    "summary": "",
    "translation": "Omni-AVSR：基于大型语言模型的统一多模态语音识别",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态语音识别，属于语音处理领域，与推荐系统、搜索或广告的核心技术栈关联度较低。虽然涉及大型语言模型，但其应用场景（语音识别）与文本排序、用户行为建模等RecSys/Search/Ads核心任务差异显著，缺乏明确的跨领域应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07250v1": {
    "title": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs",
    "url": "https://www.alphaxiv.org/abs/2511.07250v1",
    "arxiv_id": "2511.07250v1",
    "authors": "Tianhao Peng, Haochen Wang, Yuanxing Zhang, Zekun Wang, Zili Wang, Ge Zhang, Jian Yang, Shihao Li, Yanghai Wang, Xintao Wang, Houyi Li, Wei Ji, Pengfei Wan, Wenhao Huang, Zhaoxiang Zhang, Jiaheng Liu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-10 16:02:33",
    "ori_summary": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs' ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research.",
    "summary": "",
    "translation": "MVU-Eval：面向多模态大语言模型的多视频理解评估",
    "relevance_score": 2,
    "reasoning": "虽然该论文涉及多模态LLM评估，但其核心焦点是多视频理解评估，这主要属于纯视觉和视频理解领域。对于推荐系统、搜索或广告的潜在应用非常有限，因为多视频理解评估主要针对视觉内容理解能力，而非用户行为建模、特征工程或排序优化等核心任务。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07241v1": {
    "title": "4DSTR: Advancing Generative 4D Gaussians with Spatial-Temporal Rectification for High-Quality and Consistent 4D Generation",
    "url": "https://www.alphaxiv.org/abs/2511.07241v1",
    "arxiv_id": "2511.07241v1",
    "authors": "Mengmeng Liu, Jiuming Liu, Yunpeng Zhang, Jiangtao Li, Michael Ying Yang, Francesco Nex, Hao Cheng",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 15:57:03",
    "ori_summary": "Remarkable advances in recent 2D image and 3D shape generation have induced a significant focus on dynamic 4D content generation. However, previous 4D generation methods commonly struggle to maintain spatial-temporal consistency and adapt poorly to rapid temporal variations, due to the lack of effective spatial-temporal modeling. To address these problems, we propose a novel 4D generation network called 4DSTR, which modulates generative 4D Gaussian Splatting with spatial-temporal rectification. Specifically, temporal correlation across generated 4D sequences is designed to rectify deformable scales and rotations and guarantee temporal consistency. Furthermore, an adaptive spatial densification and pruning strategy is proposed to address significant temporal variations by dynamically adding or deleting Gaussian points with the awareness of their pre-frame movements. Extensive experiments demonstrate that our 4DSTR achieves state-of-the-art performance in video-to-4D generation, excelling in reconstruction quality, spatial-temporal consistency, and adaptation to rapid temporal movements.",
    "summary": "",
    "translation": "4DSTR：通过时空校正推进生成式4D高斯模型，实现高质量且一致的4D生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于4D生成和3D/4D视觉技术，涉及高斯模型和时空建模，但完全属于计算机视觉领域。虽然标题提到'生成'和'时空'概念，但这是纯粹的视觉生成任务，与推荐系统、搜索或广告中的排序、检索或用户建模没有直接关联。该技术没有明显的应用场景可以迁移到RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07238v1": {
    "title": "Leveraging Text-Driven Semantic Variation for Robust OOD Segmentation",
    "url": "https://www.alphaxiv.org/abs/2511.07238v1",
    "arxiv_id": "2511.07238v1",
    "authors": "Seungheon Song, Jaekoo Lee",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-10 15:54:23",
    "ori_summary": "In autonomous driving and robotics, ensuring road safety and reliable decision-making critically depends on out-of-distribution (OOD) segmentation. While numerous methods have been proposed to detect anomalous objects on the road, leveraging the vision-language space-which provides rich linguistic knowledge-remains an underexplored field. We hypothesize that incorporating these linguistic cues can be especially beneficial in the complex contexts found in real-world autonomous driving scenarios. To this end, we present a novel approach that trains a Text-Driven OOD Segmentation model to learn a semantically diverse set of objects in the vision-language space. Concretely, our approach combines a vision-language model's encoder with a transformer decoder, employs Distance-Based OOD prompts located at varying semantic distances from in-distribution (ID) classes, and utilizes OOD Semantic Augmentation for OOD representations. By aligning visual and textual information, our approach effectively generalizes to unseen objects and provides robust OOD segmentation in diverse driving environments. We conduct extensive experiments on publicly available OOD segmentation datasets such as Fishyscapes, Segment-Me-If-You-Can, and Road Anomaly datasets, demonstrating that our approach achieves state-of-the-art performance across both pixel-level and object-level evaluations. This result underscores the potential of vision-language-based OOD segmentation to bolster the safety and reliability of future autonomous driving systems.",
    "summary": "",
    "translation": "利用文本驱动的语义变化实现鲁棒性分布外分割",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的分布外分割问题，虽然涉及文本模态，但其核心是视觉分割任务，与推荐系统、搜索或广告的排序和建模需求关联度较低。文本驱动的语义变化可能对多模态理解有启发，但缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07233v1": {
    "title": "Noise & pattern: identity-anchored Tikhonov regularization for robust structural anomaly detection",
    "url": "https://www.alphaxiv.org/abs/2511.07233v1",
    "arxiv_id": "2511.07233v1",
    "authors": "Alexander Bauer, Klaus-Robert Müller",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-11-10 15:48:50",
    "ori_summary": "Anomaly detection plays a pivotal role in automated industrial inspection, aiming to identify subtle or rare defects in otherwise uniform visual patterns. As collecting representative examples of all possible anomalies is infeasible, we tackle structural anomaly detection using a self-supervised autoencoder that learns to repair corrupted inputs. To this end, we introduce a corruption model that injects artificial disruptions into training images to mimic structural defects. While reminiscent of denoising autoencoders, our approach differs in two key aspects. First, instead of unstructured i.i.d.\\ noise, we apply structured, spatially coherent perturbations that make the task a hybrid of segmentation and inpainting. Second, and counterintuitively, we add and preserve Gaussian noise on top of the occlusions, which acts as a Tikhonov regularizer anchoring the Jacobian of the reconstruction function toward identity. This identity-anchored regularization stabilizes reconstruction and further improves both detection and segmentation accuracy. On the MVTec AD benchmark, our method achieves state-of-the-art results (I/P-AUROC: 99.9/99.4), supporting our theoretical framework and demonstrating its practical relevance for automatic inspection.",
    "summary": "",
    "translation": "噪声与模式：基于身份锚定的Tikhonov正则化用于鲁棒结构异常检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注结构异常检测和正则化方法，属于通用的机器学习技术领域。虽然异常检测在推荐系统中可能有边缘应用（如检测异常用户行为），但论文标题没有明确表明与推荐系统、搜索或广告的直接关联，也没有涉及LLM、Transformer架构或异构数据建模等核心技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07231v1": {
    "title": "Mapping Reduced Accessibility to WASH Facilities in Rohingya Refugee Camps with Sub-Meter Imagery",
    "url": "https://www.alphaxiv.org/abs/2511.07231v1",
    "arxiv_id": "2511.07231v1",
    "authors": "Kyeongjin Ahn, YongHun Suh, Sungwon Han, Jeasurk Yang, Hannes Taubenböck, Meeyoung Cha",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 15:48:04",
    "ori_summary": "Access to Water, Sanitation, and Hygiene (WASH) services remains a major public health concern in refugee camps. This study introduces a remote sensing-driven framework to quantify WASH accessibility-specifically to water pumps, latrines, and bathing cubicles-in the Rohingya camps of Cox's Bazar, one of the world's most densely populated displacement settings. Detecting refugee shelters in such emergent camps presents substantial challenges, primarily due to their dense spatial configuration and irregular geometric patterns. Using sub-meter satellite images, we develop a semi-supervised segmentation framework that achieves an F1-score of 76.4% in detecting individual refugee shelters. Applying the framework across multi-year data reveals declining WASH accessibility, driven by rapid refugee population growth and reduced facility availability, rising from 25 people per facility in 2022 to 29.4 in 2025. Gender-disaggregated analysis further shows that women and girls experience reduced accessibility, in scenarios with inadequate safety-related segregation in WASH facilities. These findings suggest the importance of demand-responsive allocation strategies that can identify areas with under-served populations-such as women and girls-and ensure that limited infrastructure serves the greatest number of people in settings with fixed or shrinking budgets. We also discuss the value of high-resolution remote sensing and machine learning to detect inequality and inform equitable resource planning in complex humanitarian environments.",
    "summary": "",
    "translation": "利用亚米级影像绘制罗兴亚难民营中WASH设施可达性降低区域分布图",
    "relevance_score": 1,
    "reasoning": "该论文涉及难民卫生设施的地理测绘，属于人道主义应用领域，与推荐系统、搜索或广告技术完全无关。论文内容聚焦于卫星影像分析和公共卫生基础设施评估，没有任何技术元素与LLM、推荐系统或广告排名相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07222v1": {
    "title": "Omni-View: Unlocking How Generation Facilitates Understanding in Unified 3D Model based on Multiview images",
    "url": "https://www.alphaxiv.org/abs/2511.07222v1",
    "arxiv_id": "2511.07222v1",
    "authors": "JiaKui Hu, Shanshan Zhao, Qing-Guo Chen, Xuerui Qiu, Jialun Liu, Zhao Xu, Weihua Luo, Kaifu Zhang, Yanye Lu",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 15:44:48",
    "ori_summary": "This paper presents Omni-View, which extends the unified multimodal understanding and generation to 3D scenes based on multiview images, exploring the principle that \"generation facilitates understanding\". Consisting of understanding model, texture module, and geometry module, Omni-View jointly models scene understanding, novel view synthesis, and geometry estimation, enabling synergistic interaction between 3D scene understanding and generation tasks. By design, it leverages the spatiotemporal modeling capabilities of its texture module responsible for appearance synthesis, alongside the explicit geometric constraints provided by its dedicated geometry module, thereby enriching the model's holistic understanding of 3D scenes. Trained with a two-stage strategy, Omni-View achieves a state-of-the-art score of 55.4 on the VSI-Bench benchmark, outperforming existing specialized 3D understanding models, while simultaneously delivering strong performance in both novel view synthesis and 3D scene generation.",
    "summary": "",
    "translation": "Omni-View：基于多视图图像的统一3D模型中生成如何促进理解",
    "relevance_score": 2,
    "reasoning": "该论文主要关注3D视觉和生成模型，属于纯粹的视觉领域研究。虽然标题提到'生成促进理解'，但其核心是3D模型和多视图图像处理，与推荐系统、搜索或广告没有直接关联。即使考虑VLM类比，该工作专注于3D视觉而非处理推荐系统中的异构数据模态。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07210v1": {
    "title": "Breaking the Stealth-Potency Trade-off in Clean-Image Backdoors with Generative Trigger Optimization",
    "url": "https://www.alphaxiv.org/abs/2511.07210v1",
    "arxiv_id": "2511.07210v1",
    "authors": "Binyan Xu, Fan Yang, Di Tang, Xilin Dai, Kehuan Zhang",
    "categories": "cs.CV, cs.CR, cs.LG, 68T07, I.2.6",
    "pub_date": "2025-11-10 15:37:44",
    "ori_summary": "Clean-image backdoor attacks, which use only label manipulation in training datasets to compromise deep neural networks, pose a significant threat to security-critical applications. A critical flaw in existing methods is that the poison rate required for a successful attack induces a proportional, and thus noticeable, drop in Clean Accuracy (CA), undermining their stealthiness. This paper presents a new paradigm for clean-image attacks that minimizes this accuracy degradation by optimizing the trigger itself. We introduce Generative Clean-Image Backdoors (GCB), a framework that uses a conditional InfoGAN to identify naturally occurring image features that can serve as potent and stealthy triggers. By ensuring these triggers are easily separable from benign task-related features, GCB enables a victim model to learn the backdoor from an extremely small set of poisoned examples, resulting in a CA drop of less than 1%. Our experiments demonstrate GCB's remarkable versatility, successfully adapting to six datasets, five architectures, and four tasks, including the first demonstration of clean-image backdoors in regression and segmentation. GCB also exhibits resilience against most of the existing backdoor defenses.",
    "summary": "",
    "translation": "通过生成式触发器优化打破干净图像后门中的隐蔽性与有效性权衡",
    "relevance_score": 1,
    "reasoning": "这篇论文关注计算机视觉领域的后门攻击和生成式触发器优化，属于安全研究范畴。虽然提到了生成式方法，但核心是图像安全攻击技术，与推荐系统、搜索或广告的排名和建模技术没有直接关联。该研究属于明确排除的无关主题中的安全领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07206v1": {
    "title": "Geometric implicit neural representations for signed distance functions",
    "url": "https://www.alphaxiv.org/abs/2511.07206v1",
    "arxiv_id": "2511.07206v1",
    "authors": "Luiz Schirmer, Tiago Novello, Vinícius da Silva, Guilherme Schardong, Daniel Perazzo, Hélio Lopes, Nuno Gonçalves, Luiz Velho",
    "categories": "cs.CV, cs.CG, cs.GR",
    "pub_date": "2025-11-10 15:33:02",
    "ori_summary": "\\textit{Implicit neural representations} (INRs) have emerged as a promising framework for representing signals in low-dimensional spaces. This survey reviews the existing literature on the specialized INR problem of approximating \\textit{signed distance functions} (SDFs) for surface scenes, using either oriented point clouds or a set of posed images. We refer to neural SDFs that incorporate differential geometry tools, such as normals and curvatures, in their loss functions as \\textit{geometric} INRs. The key idea behind this 3D reconstruction approach is to include additional \\textit{regularization} terms in the loss function, ensuring that the INR satisfies certain global properties that the function should hold -- such as having unit gradient in the case of SDFs. We explore key methodological components, including the definition of INR, the construction of geometric loss functions, and sampling schemes from a differential geometry perspective. Our review highlights the significant advancements enabled by geometric INRs in surface reconstruction from oriented point clouds and posed images.",
    "summary": "",
    "translation": "用于符号距离函数的几何隐式神经表示",
    "relevance_score": 2,
    "reasoning": "该论文主要研究几何表示和3D重建中的隐式神经表示技术，属于计算机图形学和3D视觉领域。虽然隐式表示技术在某些方面具有数学上的先进性，但该工作缺乏与推荐系统、搜索或广告领域的直接或潜在应用关联，主要聚焦于纯粹的几何建模问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07199v1": {
    "title": "Automated Estimation of Anatomical Risk Metrics for Endoscopic Sinus Surgery Using Deep Learning",
    "url": "https://www.alphaxiv.org/abs/2511.07199v1",
    "arxiv_id": "2511.07199v1",
    "authors": "Konrad Reuter, Lennart Thaysen, Bilkay Doruk, Sarah Latus, Brigitte Holst, Benjamin Becker, Dennis Eggert, Christian Betz, Anna-Sophie Hoffmann, Alexander Schlaefer",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 15:28:02",
    "ori_summary": "Endoscopic sinus surgery requires careful preoperative assessment of the skull base anatomy to minimize risks such as cerebrospinal fluid leakage. Anatomical risk scores like the Keros, Gera and Thailand-Malaysia-Singapore score offer a standardized approach but require time-consuming manual measurements on coronal CT or CBCT scans. We propose an automated deep learning pipeline that estimates these risk scores by localizing key anatomical landmarks via heatmap regression. We compare a direct approach to a specialized global-to-local learning strategy and find mean absolute errors on the relevant anatomical measurements of 0.506mm for the Keros, 4.516{\\deg} for the Gera and 0.802mm / 0.777mm for the TMS classification.",
    "summary": "",
    "translation": "基于深度学习的鼻内窥镜手术解剖风险指标自动评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的内窥镜手术风险评估，属于明确的医学应用范畴。论文内容涉及解剖学风险指标和深度学习在医疗手术中的具体应用，与推荐系统、搜索、广告或相关使能技术没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07192v1": {
    "title": "LiteUpdate: A Lightweight Framework for Updating AI-Generated Image Detectors",
    "url": "https://www.alphaxiv.org/abs/2511.07192v1",
    "arxiv_id": "2511.07192v1",
    "authors": "Jiajie Lu, Zhenkan Fu, Na Zhao, Long Xing, Kejiang Chen, Weiming Zhang, Nenghai Yu",
    "categories": "cs.CV, cs.CR",
    "pub_date": "2025-11-10 15:22:28",
    "ori_summary": "The rapid progress of generative AI has led to the emergence of new generative models, while existing detection methods struggle to keep pace, resulting in significant degradation in the detection performance. This highlights the urgent need for continuously updating AI-generated image detectors to adapt to new generators. To overcome low efficiency and catastrophic forgetting in detector updates, we propose LiteUpdate, a lightweight framework for updating AI-generated image detectors. LiteUpdate employs a representative sample selection module that leverages image confidence and gradient-based discriminative features to precisely select boundary samples. This approach improves learning and detection accuracy on new distributions with limited generated images, significantly enhancing detector update efficiency. Additionally, LiteUpdate incorporates a model merging module that fuses weights from multiple fine-tuning trajectories, including pre-trained, representative, and random updates. This balances the adaptability to new generators and mitigates the catastrophic forgetting of prior knowledge. Experiments demonstrate that LiteUpdate substantially boosts detection performance in various detectors. Specifically, on AIDE, the average detection accuracy on Midjourney improved from 87.63% to 93.03%, a 6.16% relative increase.",
    "summary": "",
    "translation": "LiteUpdate：一种用于更新AI生成图像检测器的轻量级框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于AI生成图像检测器的更新框架，这属于内容检测和验证领域，与推荐系统、搜索或广告的核心技术无关。虽然AI生成图像可能与广告创意相关，但论文关注的是检测器更新而非排名、检索或用户建模等核心领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07171v1": {
    "title": "Federated Learning for Video Violence Detection: Complementary Roles of Lightweight CNNs and Vision-Language Models for Energy-Efficient Use",
    "url": "https://www.alphaxiv.org/abs/2511.07171v1",
    "arxiv_id": "2511.07171v1",
    "authors": "Sébastien Thuau, Siba Haidar, Rachid Chelouah",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-11-10 15:01:51",
    "ori_summary": "Deep learning-based video surveillance increasingly demands privacy-preserving architectures with low computational and environmental overhead. Federated learning preserves privacy but deploying large vision-language models (VLMs) introduces major energy and sustainability challenges. We compare three strategies for federated violence detection under realistic non-IID splits on the RWF-2000 and RLVS datasets: zero-shot inference with pretrained VLMs, LoRA-based fine-tuning of LLaVA-NeXT-Video-7B, and personalized federated learning of a 65.8M-parameter 3D CNN. All methods exceed 90% accuracy in binary violence detection. The 3D CNN achieves superior calibration (ROC AUC 92.59%) at roughly half the energy cost (240 Wh vs. 570 Wh) of federated LoRA, while VLMs provide richer multimodal reasoning. Hierarchical category grouping (based on semantic similarity and class exclusion) boosts VLM multiclass accuracy from 65.31% to 81% on the UCF-Crime dataset. To our knowledge, this is the first comparative simulation study of LoRA-tuned VLMs and personalized CNNs for federated violence detection, with explicit energy and CO2e quantification. Our results inform hybrid deployment strategies that default to efficient CNNs for routine inference and selectively engage VLMs for complex contextual reasoning.",
    "summary": "",
    "translation": "联邦学习在视频暴力检测中的应用：轻量级CNN与视觉语言模型在能效利用中的互补作用",
    "relevance_score": 1,
    "reasoning": "该论文涉及联邦学习和视频分析，这两个主题均被明确列为不相关主题。虽然提到了视觉语言模型，但其应用场景是视频暴力检测，与推荐系统、搜索或广告领域没有直接关联。该研究专注于特定领域的计算机视觉应用，不符合当前关注的核心领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07142v1": {
    "title": "ProcGen3D: Learning Neural Procedural Graph Representations for Image-to-3D Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2511.07142v1",
    "arxiv_id": "2511.07142v1",
    "authors": "Xinyi Zhang, Daoyi Gao, Naiqi Li, Angela Dai",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 14:23:55",
    "ori_summary": "We introduce ProcGen3D, a new approach for 3D content creation by generating procedural graph abstractions of 3D objects, which can then be decoded into rich, complex 3D assets. Inspired by the prevalent use of procedural generators in production 3D applications, we propose a sequentialized, graph-based procedural graph representation for 3D assets. We use this to learn to approximate the landscape of a procedural generator for image-based 3D reconstruction. We employ edge-based tokenization to encode the procedural graphs, and train a transformer prior to predict the next token conditioned on an input RGB image. Crucially, to enable better alignment of our generated outputs to an input image, we incorporate Monte Carlo Tree Search (MCTS) guided sampling into our generation process, steering output procedural graphs towards more image-faithful reconstructions. Our approach is applicable across a variety of objects that can be synthesized with procedural generators. Extensive experiments on cacti, trees, and bridges show that our neural procedural graph generation outperforms both state-of-the-art generative 3D methods and domain-specific modeling techniques. Furthermore, this enables improved generalization on real-world input images, despite training only on synthetic data.",
    "summary": "",
    "translation": "ProcGen3D：学习神经程序图表示用于图像到3D重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的3D重建技术，涉及程序生成和图形表示学习。虽然标题提到神经表示学习，但其核心应用是图像到3D的视觉任务，与推荐系统、搜索或广告的排名和建模需求没有直接关联。该技术缺乏在异构数据处理或序列建模方面的潜在应用，无法满足当前关注领域的要求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07137v1": {
    "title": "MPJudge: Towards Perceptual Assessment of Music-Induced Paintings",
    "url": "https://www.alphaxiv.org/abs/2511.07137v1",
    "arxiv_id": "2511.07137v1",
    "authors": "Shiqi Jiang, Tianyi Liang, Changbo Wang, Chenhui Li",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 14:18:27",
    "ori_summary": "Music induced painting is a unique artistic practice, where visual artworks are created under the influence of music. Evaluating whether a painting faithfully reflects the music that inspired it poses a challenging perceptual assessment task. Existing methods primarily rely on emotion recognition models to assess the similarity between music and painting, but such models introduce considerable noise and overlook broader perceptual cues beyond emotion. To address these limitations, we propose a novel framework for music induced painting assessment that directly models perceptual coherence between music and visual art. We introduce MPD, the first large scale dataset of music painting pairs annotated by domain experts based on perceptual coherence. To better handle ambiguous cases, we further collect pairwise preference annotations. Building on this dataset, we present MPJudge, a model that integrates music features into a visual encoder via a modulation based fusion mechanism. To effectively learn from ambiguous cases, we adopt Direct Preference Optimization for training. Extensive experiments demonstrate that our method outperforms existing approaches. Qualitative results further show that our model more accurately identifies music relevant regions in paintings.",
    "summary": "",
    "translation": "MPJudge：面向音乐诱发绘画的感知评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于音乐与绘画之间的跨模态感知评估，属于艺术生成和多媒体理解领域。这与推荐系统、搜索或广告的核心技术焦点完全无关，也不涉及LLM、Transformer架构或异构数据建模的相关进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07122v1": {
    "title": "Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2511.07122v1",
    "arxiv_id": "2511.07122v1",
    "authors": "Changyue Shi, Chuxiao Yang, Xinyuan Hu, Minghao Chen, Wenwen Pan, Yan Yang, Jiajun Ding, Zhou Yu, Jun Yu",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 14:10:43",
    "ori_summary": "Dynamic Gaussian Splatting approaches have achieved remarkable performance for 4D scene reconstruction. However, these approaches rely on dense-frame video sequences for photorealistic reconstruction. In real-world scenarios, due to equipment constraints, sometimes only sparse frames are accessible. In this paper, we propose Sparse4DGS, the first method for sparse-frame dynamic scene reconstruction. We observe that dynamic reconstruction methods fail in both canonical and deformed spaces under sparse-frame settings, especially in areas with high texture richness. Sparse4DGS tackles this challenge by focusing on texture-rich areas. For the deformation network, we propose Texture-Aware Deformation Regularization, which introduces a texture-based depth alignment loss to regulate Gaussian deformation. For the canonical Gaussian field, we introduce Texture-Aware Canonical Optimization, which incorporates texture-based noise into the gradient descent process of canonical Gaussians. Extensive experiments show that when taking sparse frames as inputs, our method outperforms existing dynamic or few-shot techniques on NeRF-Synthetic, HyperNeRF, NeRF-DS, and our iPhone-4D datasets.",
    "summary": "",
    "translation": "Sparse4DGS：用于稀疏帧动态场景重建的4D高斯泼溅技术",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于4D高斯泼溅技术用于动态场景重建，属于纯粹的计算机视觉和3D图形学领域。虽然标题提到'稀疏帧'概念，但该技术主要应用于3D场景重建和渲染，与推荐系统、搜索或广告的排名、建模或用户行为分析没有明确的关联。该研究属于纯粹的视觉技术范畴，没有展示在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07106v1": {
    "title": "HENet++: Hybrid Encoding and Multi-task Learning for 3D Perception and End-to-end Autonomous Driving",
    "url": "https://www.alphaxiv.org/abs/2511.07106v1",
    "arxiv_id": "2511.07106v1",
    "authors": "Zhongyu Xia, Zhiwei Lin, Yongtao Wang, Ming-Hsuan Yang",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 13:49:59",
    "ori_summary": "Three-dimensional feature extraction is a critical component of autonomous driving systems, where perception tasks such as 3D object detection, bird's-eye-view (BEV) semantic segmentation, and occupancy prediction serve as important constraints on 3D features. While large image encoders, high-resolution images, and long-term temporal inputs can significantly enhance feature quality and deliver remarkable performance gains, these techniques are often incompatible in both training and inference due to computational resource constraints. Moreover, different tasks favor distinct feature representations, making it difficult for a single model to perform end-to-end inference across multiple tasks while maintaining accuracy comparable to that of single-task models. To alleviate these issues, we present the HENet and HENet++ framework for multi-task 3D perception and end-to-end autonomous driving. Specifically, we propose a hybrid image encoding network that uses a large image encoder for short-term frames and a small one for long-term frames. Furthermore, our framework simultaneously extracts both dense and sparse features, providing more suitable representations for different tasks, reducing cumulative errors, and delivering more comprehensive information to the planning module. The proposed architecture maintains compatibility with various existing 3D feature extraction methods and supports multimodal inputs. HENet++ achieves state-of-the-art end-to-end multi-task 3D perception results on the nuScenes benchmark, while also attaining the lowest collision rate on the nuScenes end-to-end autonomous driving benchmark.",
    "summary": "",
    "translation": "HENet++：用于3D感知和端到端自动驾驶的混合编码与多任务学习",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D感知和自动驾驶领域，与推荐系统、搜索或广告的核心技术领域没有直接关联。虽然提到了多任务学习，但应用场景是纯粹的计算机视觉和自动驾驶，没有任何潜在的应用于RecSys/Search/Ads的迹象。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07103v1": {
    "title": "GEWDiff: Geometric Enhanced Wavelet-based Diffusion Model for Hyperspectral Image Super-resolution",
    "url": "https://www.alphaxiv.org/abs/2511.07103v1",
    "arxiv_id": "2511.07103v1",
    "authors": "Sirui Wang, Jiang He, Natàlia Blasco Andreo, Xiao Xiang Zhu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-10 13:44:16",
    "ori_summary": "Improving the quality of hyperspectral images (HSIs), such as through super-resolution, is a crucial research area. However, generative modeling for HSIs presents several challenges. Due to their high spectral dimensionality, HSIs are too memory-intensive for direct input into conventional diffusion models. Furthermore, general generative models lack an understanding of the topological and geometric structures of ground objects in remote sensing imagery. In addition, most diffusion models optimize loss functions at the noise level, leading to a non-intuitive convergence behavior and suboptimal generation quality for complex data. To address these challenges, we propose a Geometric Enhanced Wavelet-based Diffusion Model (GEWDiff), a novel framework for reconstructing hyperspectral images at 4-times super-resolution. A wavelet-based encoder-decoder is introduced that efficiently compresses HSIs into a latent space while preserving spectral-spatial information. To avoid distortion during generation, we incorporate a geometry-enhanced diffusion process that preserves the geometric features. Furthermore, a multi-level loss function was designed to guide the diffusion process, promoting stable convergence and improved reconstruction fidelity. Our model demonstrated state-of-the-art results across multiple dimensions, including fidelity, spectral accuracy, visual realism, and clarity.",
    "summary": "",
    "translation": "GEWDiff：基于几何增强小波的扩散模型用于高光谱图像超分辨率",
    "relevance_score": 1,
    "reasoning": "该论文专注于高光谱图像超分辨率，这是一个纯粹的计算机视觉任务，与推荐系统、搜索或广告没有明显关联。扩散模型虽然是一种生成技术，但该论文的应用场景（高光谱图像处理）在所列焦点领域中没有直接的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07094v1": {
    "title": "Task-Adaptive Low-Dose CT Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2511.07094v1",
    "arxiv_id": "2511.07094v1",
    "authors": "Necati Sefercioglu, Mehmet Ozan Unal, Metin Ertas, Isa Yildirim",
    "categories": "eess.IV, cs.CV",
    "pub_date": "2025-11-10 13:30:59",
    "ori_summary": "Deep learning-based low-dose computed tomography reconstruction methods already achieve high performance on standard image quality metrics like peak signal-to-noise ratio and structural similarity index measure. Yet, they frequently fail to preserve the critical anatomical details needed for diagnostic tasks. This fundamental limitation hinders their clinical applicability despite their high metric scores. We propose a novel task-adaptive reconstruction framework that addresses this gap by incorporating a frozen pre-trained task network as a regularization term in the reconstruction loss function. Unlike existing joint-training approaches that simultaneously optimize both reconstruction and task networks, and risk diverging from satisfactory reconstructions, our method leverages a pre-trained task model to guide reconstruction training while still maintaining diagnostic quality. We validate our framework on a liver and liver tumor segmentation task. Our task-adaptive models achieve Dice scores up to 0.707, approaching the performance of full-dose scans (0.874), and substantially outperforming joint-training approaches (0.331) and traditional reconstruction methods (0.626). Critically, our framework can be integrated into any existing deep learning-based reconstruction model through simple loss function modification, enabling widespread adoption for task-adaptive optimization in clinical practice. Our codes are available at: https://github.com/itu-biai/task_adaptive_ct",
    "summary": "",
    "translation": "任务自适应的低剂量CT重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像领域的CT重建技术，属于医学图像处理范畴。虽然涉及任务自适应方法，但这是针对特定医疗应用的图像重建问题，与推荐系统、搜索或广告的核心技术领域没有直接关联。论文内容完全属于医疗领域应用，属于明确排除的无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07091v1": {
    "title": "How Bias Binds: Measuring Hidden Associations for Bias Control in Text-to-Image Compositions",
    "url": "https://www.alphaxiv.org/abs/2511.07091v1",
    "arxiv_id": "2511.07091v1",
    "authors": "Jeng-Lin Li, Ming-Ching Chang, Wei-Chao Chen",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-10 13:27:05",
    "ori_summary": "Text-to-image generative models often exhibit bias related to sensitive attributes. However, current research tends to focus narrowly on single-object prompts with limited contextual diversity. In reality, each object or attribute within a prompt can contribute to bias. For example, the prompt \"an assistant wearing a pink hat\" may reflect female-inclined biases associated with a pink hat. The neglected joint effects of the semantic binding in the prompts cause significant failures in current debiasing approaches. This work initiates a preliminary investigation on how bias manifests under semantic binding, where contextual associations between objects and attributes influence generative outcomes. We demonstrate that the underlying bias distribution can be amplified based on these associations. Therefore, we introduce a bias adherence score that quantifies how specific object-attribute bindings activate bias. To delve deeper, we develop a training-free context-bias control framework to explore how token decoupling can facilitate the debiasing of semantic bindings. This framework achieves over 10% debiasing improvement in compositional generation tasks. Our analysis of bias scores across various attribute-object bindings and token decorrelation highlights a fundamental challenge: reducing bias without disrupting essential semantic relationships. These findings expose critical limitations in current debiasing approaches when applied to semantically bound contexts, underscoring the need to reassess prevailing bias mitigation strategies.",
    "summary": "",
    "translation": "偏见如何束缚：测量文本到图像组合中隐藏关联以实现偏见控制",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本到图像生成中的偏见测量和控制，这属于纯粹的AIGC和内容生成领域。虽然提到了偏见控制，但这属于伦理和公平性范畴，属于明确排除的非技术性话题。该研究没有展示与推荐系统、搜索或广告排名的直接相关性或潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07085v1": {
    "title": "Achieving Effective Virtual Reality Interactions via Acoustic Gesture Recognition based on Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2511.07085v1",
    "arxiv_id": "2511.07085v1",
    "authors": "Xijie Zhang, Fengliang He, Hong-Ning Dai",
    "categories": "cs.HC, cs.AI, cs.CV",
    "pub_date": "2025-11-10 13:19:58",
    "ori_summary": "Natural and efficient interaction remains a critical challenge for virtual reality and augmented reality (VR/AR) systems. Vision-based gesture recognition suffers from high computational cost, sensitivity to lighting conditions, and privacy leakage concerns. Acoustic sensing provides an attractive alternative: by emitting inaudible high-frequency signals and capturing their reflections, channel impulse response (CIR) encodes how gestures perturb the acoustic field in a low-cost and user-transparent manner. However, existing CIR-based gesture recognition methods often rely on extensive training of models on large labeled datasets, making them unsuitable for few-shot VR scenarios. In this work, we propose the first framework that leverages large language models (LLMs) for CIR-based gesture recognition in VR/AR systems. Despite LLMs' strengths, it is non-trivial to achieve few-shot and zero-shot learning of CIR gestures due to their inconspicuous features. To tackle this challenge, we collect differential CIR rather than original CIR data. Moreover, we construct a real-world dataset collected from 10 participants performing 15 gestures across three categories (digits, letters, and shapes), with 10 repetitions each. We then conduct extensive experiments on this dataset using an LLM-adopted classifier. Results show that our LLM-based framework achieves accuracy comparable to classical machine learning baselines, while requiring no domain-specific retraining.",
    "summary": "",
    "translation": "基于大语言模型实现声学手势识别以达成有效的虚拟现实交互",
    "relevance_score": 1,
    "reasoning": "该论文专注于虚拟现实交互和声学手势识别，属于人机交互领域，与推荐系统、搜索或广告的核心技术无关。虽然提到了大语言模型，但应用场景是VR交互，没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07084v1": {
    "title": "Pandar128 dataset for lane line detection",
    "url": "https://www.alphaxiv.org/abs/2511.07084v1",
    "arxiv_id": "2511.07084v1",
    "authors": "Filip Beránek, Václav Diviš, Ivan Gruber",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-10 13:18:36",
    "ori_summary": "We present Pandar128, the largest public dataset for lane line detection using a 128-beam LiDAR. It contains over 52,000 camera frames and 34,000 LiDAR scans, captured in diverse real-world conditions in Germany. The dataset includes full sensor calibration (intrinsics, extrinsics) and synchronized odometry, supporting tasks such as projection, fusion, and temporal modeling. To complement the dataset, we also introduce SimpleLidarLane, a light-weight baseline method for lane line reconstruction that combines BEV segmentation, clustering, and polyline fitting. Despite its simplicity, our method achieves strong performance under challenging various conditions (e.g., rain, sparse returns), showing that modular pipelines paired with high-quality data and principled evaluation can compete with more complex approaches. Furthermore, to address the lack of standardized evaluation, we propose a novel polyline-based metric - Interpolation-Aware Matching F1 (IAM-F1) - that employs interpolation-aware lateral matching in BEV space. All data and code are publicly released to support reproducibility in LiDAR-based lane detection.",
    "summary": "",
    "translation": "用于车道线检测的Pandar128数据集",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于车道线检测的计算机视觉数据集，属于纯粹的视觉领域研究。车道线检测主要用于自动驾驶和交通监控，与推荐系统、搜索或广告的核心技术领域没有任何直接或间接关联，也不涉及任何Transformer架构或LLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07078v1": {
    "title": "LeCoT: revisiting network architecture for two-view correspondence pruning",
    "url": "https://www.alphaxiv.org/abs/2511.07078v1",
    "arxiv_id": "2511.07078v1",
    "authors": "Luanyuan Dai, Xiaoyu Du, Jinhui Tang",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 13:08:15",
    "ori_summary": "Two-view correspondence pruning aims to accurately remove incorrect correspondences (outliers) from initial ones and is widely applied to various computer vision tasks. Current popular strategies adopt multilayer perceptron (MLP) as the backbone, supplemented by additional modules to enhance the network ability to handle context information, which is a known limitation of MLPs. In contrast, we introduce a novel perspective for capturing correspondence context information without extra design modules. To this end, we design a two-view correspondence pruning network called LeCoT, which can naturally leverage global context information at different stages. Specifically, the core design of LeCoT is the Spatial-Channel Fusion Transformer block, a newly proposed component that efficiently utilizes both spatial and channel global context information among sparse correspondences. In addition, we integrate the proposed prediction block that utilizes correspondence features from intermediate stages to generate a probability set, which acts as guiding information for subsequent learning phases, allowing the network to more effectively capture robust global context information. Notably, this prediction block progressively refines the probability set, thereby mitigating the issue of information loss that is common in the traditional one. Extensive experiments prove that the proposed LeCoT outperforms state-of-the-art methods in correspondence pruning, relative pose estimation, homography estimation, visual localization, and $3$D~reconstruction tasks. The code is provided in https://github.com/Dailuanyuan2024/LeCoT-Revisiting-Network-Architecture-for-Two-View-Correspondence-Pruning.",
    "summary": "",
    "translation": "LeCoT：重新审视用于双视图对应点剪枝的网络架构",
    "relevance_score": 2,
    "reasoning": "这篇论文关注计算机视觉中的双视图对应点剪枝问题，属于纯粹的视觉处理任务。虽然网络架构改进可能带来效率提升，但该技术主要针对视觉几何匹配，与推荐系统、搜索或广告中的排序、用户建模等核心问题缺乏直接关联。没有明确的机制表明这些架构改进能够迁移到RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07068v1": {
    "title": "ClusterMine: Robust Label-Free Visual Out-Of-Distribution Detection via Concept Mining from Text Corpora",
    "url": "https://www.alphaxiv.org/abs/2511.07068v1",
    "arxiv_id": "2511.07068v1",
    "authors": "Nikolas Adaloglou, Diana Petrusheva, Mohamed Asker, Felix Michels, Markus Kollmann",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-11-10 13:04:01",
    "ori_summary": "Large-scale visual out-of-distribution (OOD) detection has witnessed remarkable progress by leveraging vision-language models such as CLIP. However, a significant limitation of current methods is their reliance on a pre-defined set of in-distribution (ID) ground-truth label names (positives). These fixed label names can be unavailable, unreliable at scale, or become less relevant due to in-distribution shifts after deployment. Towards truly unsupervised OOD detection, we utilize widely available text corpora for positive label mining, bypassing the need for positives. In this paper, we utilize widely available text corpora for positive label mining under a general concept mining paradigm. Within this framework, we propose ClusterMine, a novel positive label mining method. ClusterMine is the first method to achieve state-of-the-art OOD detection performance without access to positive labels. It extracts positive concepts from a large text corpus by combining visual-only sample consistency (via clustering) and zero-shot image-text consistency. Our experimental study reveals that ClusterMine is scalable across a plethora of CLIP models and achieves state-of-the-art robustness to covariate in-distribution shifts. The code is available at https://github.com/HHU-MMBS/clustermine_wacv_official.",
    "summary": "",
    "translation": "ClusterMine：通过从文本语料库中挖掘概念实现鲁棒的无标签视觉分布外检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉领域的分布外检测问题，虽然提到了文本语料库的概念挖掘，但其核心是视觉异常检测而非推荐系统、搜索或广告应用。论文的技术方向更偏向计算机视觉和异常检测，与当前关注的LLM在推荐/搜索/广告中的核心应用缺乏直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07067v1": {
    "title": "RaLD: Generating High-Resolution 3D Radar Point Clouds with Latent Diffusion",
    "url": "https://www.alphaxiv.org/abs/2511.07067v1",
    "arxiv_id": "2511.07067v1",
    "authors": "Ruijie Zhang, Bixin Zeng, Shengpeng Wang, Fuhui Zhou, Wei Wang",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 13:03:58",
    "ori_summary": "Millimeter-wave radar offers a promising sensing modality for autonomous systems thanks to its robustness in adverse conditions and low cost. However, its utility is significantly limited by the sparsity and low resolution of radar point clouds, which poses challenges for tasks requiring dense and accurate 3D perception. Despite that recent efforts have shown great potential by exploring generative approaches to address this issue, they often rely on dense voxel representations that are inefficient and struggle to preserve structural detail. To fill this gap, we make the key observation that latent diffusion models (LDMs), though successful in other modalities, have not been effectively leveraged for radar-based 3D generation due to a lack of compatible representations and conditioning strategies. We introduce RaLD, a framework that bridges this gap by integrating scene-level frustum-based LiDAR autoencoding, order-invariant latent representations, and direct radar spectrum conditioning. These insights lead to a more compact and expressive generation process. Experiments show that RaLD produces dense and accurate 3D point clouds from raw radar spectrums, offering a promising solution for robust perception in challenging environments.",
    "summary": "",
    "translation": "RaLD：使用潜在扩散生成高分辨率3D雷达点云",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D雷达点云生成，属于计算机视觉和3D感知领域，与推荐系统、搜索或广告的核心技术栈没有直接关联。扩散模型技术本身具有通用性，但论文的应用场景（雷达点云生成）在RecSys/Search/Ads领域缺乏明确的适用场景和潜在应用路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07057v1": {
    "title": "TauFlow: Dynamic Causal Constraint for Complexity-Adaptive Lightweight Segmentation",
    "url": "https://www.alphaxiv.org/abs/2511.07057v1",
    "arxiv_id": "2511.07057v1",
    "authors": "Zidong Chen, Fadratul Hafinaz Hassan",
    "categories": "eess.IV, cs.AI, cs.CV, 68U10, 68T45, 92C55, 68T07, I.4.6; I.2.10; J.3; I.2.6",
    "pub_date": "2025-11-10 12:47:21",
    "ori_summary": "Deploying lightweight medical image segmentation models on edge devices presents two major challenges: 1) efficiently handling the stark contrast between lesion boundaries and background regions, and 2) the sharp drop in accuracy that occurs when pursuing extremely lightweight designs (e.g., <0.5M parameters). To address these problems, this paper proposes TauFlow, a novel lightweight segmentation model. The core of TauFlow is a dynamic feature response strategy inspired by brain-like mechanisms. This is achieved through two key innovations: the Convolutional Long-Time Constant Cell (ConvLTC), which dynamically regulates the feature update rate to \"slowly\" process low-frequency backgrounds and \"quickly\" respond to high-frequency boundaries; and the STDP Self-Organizing Module, which significantly mitigates feature conflicts between the encoder and decoder, reducing the conflict rate from approximately 35%-40% to 8%-10%.",
    "summary": "",
    "translation": "TauFlow：面向复杂度自适应轻量级分割的动态因果约束",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的图像分割任务，特别是轻量级架构设计。虽然提到了复杂度自适应和动态约束等技术概念，但这些方法与推荐系统、搜索或广告的核心技术栈关联度较低。论文没有明确展示这些技术如何应用于异构数据处理、序列建模或Transformer架构改进等与当前关注领域直接相关的场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07051v1": {
    "title": "Improving Deepfake Detection with Reinforcement Learning-Based Adaptive Data Augmentation",
    "url": "https://www.alphaxiv.org/abs/2511.07051v1",
    "arxiv_id": "2511.07051v1",
    "authors": "Yuxuan Zhou, Tao Yu, Wen Huang, Yuheng Zhang, Tao Dai, Shu-Tao Xia",
    "categories": "cs.CV, cs.CR",
    "pub_date": "2025-11-10 12:45:52",
    "ori_summary": "The generalization capability of deepfake detectors is critical for real-world use. Data augmentation via synthetic fake face generation effectively enhances generalization, yet current SoTA methods rely on fixed strategies-raising a key question: Is a single static augmentation sufficient, or does the diversity of forgery features demand dynamic approaches? We argue existing methods overlook the evolving complexity of real-world forgeries (e.g., facial warping, expression manipulation), which fixed policies cannot fully simulate. To address this, we propose CRDA (Curriculum Reinforcement-Learning Data Augmentation), a novel framework guiding detectors to progressively master multi-domain forgery features from simple to complex. CRDA synthesizes augmented samples via a configurable pool of forgery operations and dynamically generates adversarial samples tailored to the detector's current learning state. Central to our approach is integrating reinforcement learning (RL) and causal inference. An RL agent dynamically selects augmentation actions based on detector performance to efficiently explore the vast augmentation space, adapting to increasingly challenging forgeries. Simultaneously, the agent introduces action space variations to generate heterogeneous forgery patterns, guided by causal inference to mitigate spurious correlations-suppressing task-irrelevant biases and focusing on causally invariant features. This integration ensures robust generalization by decoupling synthetic augmentation patterns from the model's learned representations. Extensive experiments show our method significantly improves detector generalizability, outperforming SOTA methods across multiple cross-domain datasets.",
    "summary": "",
    "translation": "基于强化学习的自适应数据增强改进深度伪造检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于深度伪造检测，这是一个计算机视觉安全应用，与推荐系统、搜索或广告的核心领域无关。虽然提到了强化学习，但论文明确关注检测伪造内容，这与排名、个性化或用户建模等RecSys/Search/Ads核心任务没有直接关联。该主题属于被排除的纯粹视觉应用类别，没有明确的RecSys/Search/Ads相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07049v1": {
    "title": "From Pretrain to Pain: Adversarial Vulnerability of Video Foundation Models Without Task Knowledge",
    "url": "https://www.alphaxiv.org/abs/2511.07049v1",
    "arxiv_id": "2511.07049v1",
    "authors": "Hui Lu, Yi Yu, Song Xia, Yiming Yang, Deepu Rajan, Boon Poh Ng, Alex Kot, Xudong Jiang",
    "categories": "cs.CV, cs.CR",
    "pub_date": "2025-11-10 12:42:32",
    "ori_summary": "Large-scale Video Foundation Models (VFMs) has significantly advanced various video-related tasks, either through task-specific models or Multi-modal Large Language Models (MLLMs). However, the open accessibility of VFMs also introduces critical security risks, as adversaries can exploit full knowledge of the VFMs to launch potent attacks. This paper investigates a novel and practical adversarial threat scenario: attacking downstream models or MLLMs fine-tuned from open-source VFMs, without requiring access to the victim task, training data, model query, and architecture. In contrast to conventional transfer-based attacks that rely on task-aligned surrogate models, we demonstrate that adversarial vulnerabilities can be exploited directly from the VFMs. To this end, we propose the Transferable Video Attack (TVA), a temporal-aware adversarial attack method that leverages the temporal representation dynamics of VFMs to craft effective perturbations. TVA integrates a bidirectional contrastive learning mechanism to maximize the discrepancy between the clean and adversarial features, and introduces a temporal consistency loss that exploits motion cues to enhance the sequential impact of perturbations. TVA avoids the need to train expensive surrogate models or access to domain-specific data, thereby offering a more practical and efficient attack strategy. Extensive experiments across 24 video-related tasks demonstrate the efficacy of TVA against downstream models and MLLMs, revealing a previously underexplored security vulnerability in the deployment of video models.",
    "summary": "",
    "translation": "从预训练到痛苦：无任务知识的视频基础模型对抗性漏洞",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频基础模型的对抗性漏洞，属于安全领域，与我的核心关注点无关。虽然基础模型技术本身具有潜在应用价值，但论文专注于安全漏洞而非技术改进或应用，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07040v1": {
    "title": "3D-ANC: Adaptive Neural Collapse for Robust 3D Point Cloud Recognition",
    "url": "https://www.alphaxiv.org/abs/2511.07040v1",
    "arxiv_id": "2511.07040v1",
    "authors": "Yuanmin Huang, Wenxuan Li, Mi Zhang, Xiaohan Zhang, Xiaoyu You, Min Yang",
    "categories": "cs.CV, cs.CR",
    "pub_date": "2025-11-10 12:37:00",
    "ori_summary": "Deep neural networks have recently achieved notable progress in 3D point cloud recognition, yet their vulnerability to adversarial perturbations poses critical security challenges in practical deployments. Conventional defense mechanisms struggle to address the evolving landscape of multifaceted attack patterns. Through systematic analysis of existing defenses, we identify that their unsatisfactory performance primarily originates from an entangled feature space, where adversarial attacks can be performed easily. To this end, we present 3D-ANC, a novel approach that capitalizes on the Neural Collapse (NC) mechanism to orchestrate discriminative feature learning. In particular, NC depicts where last-layer features and classifier weights jointly evolve into a simplex equiangular tight frame (ETF) arrangement, establishing maximally separable class prototypes. However, leveraging this advantage in 3D recognition confronts two substantial challenges: (1) prevalent class imbalance in point cloud datasets, and (2) complex geometric similarities between object categories. To tackle these obstacles, our solution combines an ETF-aligned classification module with an adaptive training framework consisting of representation-balanced learning (RBL) and dynamic feature direction loss (FDL). 3D-ANC seamlessly empowers existing models to develop disentangled feature spaces despite the complexity in 3D data distribution. Comprehensive evaluations state that 3D-ANC significantly improves the robustness of models with various structures on two datasets. For instance, DGCNN's classification accuracy is elevated from 27.2% to 80.9% on ModelNet40 -- a 53.7% absolute gain that surpasses leading baselines by 34.0%.",
    "summary": "",
    "translation": "3D-ANC：用于鲁棒3D点云识别的自适应神经坍缩",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D点云识别，属于纯粹的3D视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。论文内容涉及3D数据处理的特定计算机视觉任务，不符合当前关注的任何技术方向，包括LLM应用、Transformer架构改进或异构数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07029v1": {
    "title": "Certified L2-Norm Robustness of 3D Point Cloud Recognition in the Frequency Domain",
    "url": "https://www.alphaxiv.org/abs/2511.07029v1",
    "arxiv_id": "2511.07029v1",
    "authors": "Liang Zhou, Qiming Wang, Tianze Chen",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 12:24:35",
    "ori_summary": "3D point cloud classification is a fundamental task in safety-critical applications such as autonomous driving, robotics, and augmented reality. However, recent studies reveal that point cloud classifiers are vulnerable to structured adversarial perturbations and geometric corruptions, posing risks to their deployment in safety-critical scenarios. Existing certified defenses limit point-wise perturbations but overlook subtle geometric distortions that preserve individual points yet alter the overall structure, potentially leading to misclassification. In this work, we propose FreqCert, a novel certification framework that departs from conventional spatial domain defenses by shifting robustness analysis to the frequency domain, enabling structured certification against global L2-bounded perturbations. FreqCert first transforms the input point cloud via the graph Fourier transform (GFT), then applies structured frequency-aware subsampling to generate multiple sub-point clouds. Each sub-cloud is independently classified by a standard model, and the final prediction is obtained through majority voting, where sub-clouds are constructed based on spectral similarity rather than spatial proximity, making the partitioning more stable under L2 perturbations and better aligned with the object's intrinsic structure. We derive a closed-form lower bound on the certified L2 robustness radius and prove its tightness under minimal and interpretable assumptions, establishing a theoretical foundation for frequency domain certification. Extensive experiments on the ModelNet40 and ScanObjectNN datasets demonstrate that FreqCert consistently achieves higher certified accuracy and empirical accuracy under strong perturbations. Our results suggest that spectral representations provide an effective pathway toward certifiable robustness in 3D point cloud recognition.",
    "summary": "",
    "translation": "频域中3D点云识别的L2范数鲁棒性认证",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于3D点云识别的鲁棒性认证，属于纯粹的3D视觉领域。虽然提到了认证和鲁棒性，但这些技术概念与推荐系统、搜索或广告的核心技术栈没有直接关联，也不涉及LLM、Transformer架构或异构数据建模等当前关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07009v1": {
    "title": "Performance Decay in Deepfake Detection: The Limitations of Training on Outdated Data",
    "url": "https://www.alphaxiv.org/abs/2511.07009v1",
    "arxiv_id": "2511.07009v1",
    "authors": "Jack Richings, Margaux Leblanc, Ian Groves, Victoria Nockles",
    "categories": "cs.CV, 68T07, 68T45",
    "pub_date": "2025-11-10 11:58:34",
    "ori_summary": "The continually advancing quality of deepfake technology exacerbates the threats of disinformation, fraud, and harassment by making maliciously-generated synthetic content increasingly difficult to distinguish from reality. We introduce a simple yet effective two-stage detection method that achieves an AUROC of over 99.8% on contemporary deepfakes. However, this high performance is short-lived. We show that models trained on this data suffer a recall drop of over 30% when evaluated on deepfakes created with generation techniques from just six months later, demonstrating significant decay as threats evolve. Our analysis reveals two key insights for robust detection. Firstly, continued performance requires the ongoing curation of large, diverse datasets. Second, predictive power comes primarily from static, frame-level artifacts, not temporal inconsistencies. The future of effective deepfake detection therefore depends on rapid data collection and the development of advanced frame-level feature detectors.",
    "summary": "",
    "translation": "深度伪造检测中的性能衰减：基于过时数据训练的局限性",
    "relevance_score": 1,
    "reasoning": "该论文专注于深度伪造检测领域，这属于计算机视觉和内容安全范畴，与推荐系统、搜索或广告的核心技术领域没有直接关联。论文讨论的是模型在过时数据上的训练局限性问题，不涉及任何推荐、搜索或广告相关的技术应用或架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07007v1": {
    "title": "TrueCity: Real and Simulated Urban Data for Cross-Domain 3D Scene Understanding",
    "url": "https://www.alphaxiv.org/abs/2511.07007v1",
    "arxiv_id": "2511.07007v1",
    "authors": "Duc Nguyen, Yan-Ling Lai, Qilin Zhang, Prabin Gyawali, Benedikt Schwab, Olaf Wysocki, Thomas H. Kolbe",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-11-10 11:57:50",
    "ori_summary": "3D semantic scene understanding remains a long-standing challenge in the 3D computer vision community. One of the key issues pertains to limited real-world annotated data to facilitate generalizable models. The common practice to tackle this issue is to simulate new data. Although synthetic datasets offer scalability and perfect labels, their designer-crafted scenes fail to capture real-world complexity and sensor noise, resulting in a synthetic-to-real domain gap. Moreover, no benchmark provides synchronized real and simulated point clouds for segmentation-oriented domain shift analysis. We introduce TrueCity, the first urban semantic segmentation benchmark with cm-accurate annotated real-world point clouds, semantic 3D city models, and annotated simulated point clouds representing the same city. TrueCity proposes segmentation classes aligned with international 3D city modeling standards, enabling consistent evaluation of synthetic-to-real gap. Our extensive experiments on common baselines quantify domain shift and highlight strategies for exploiting synthetic data to enhance real-world 3D scene understanding. We are convinced that the TrueCity dataset will foster further development of sim-to-real gap quantification and enable generalizable data-driven models. The data, code, and 3D models are available online: https://tum-gis.github.io/TrueCity/",
    "summary": "",
    "translation": "TrueCity：用于跨领域3D场景理解的真实与模拟城市数据",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D场景理解和城市数据，这属于纯粹的计算机视觉和3D视觉领域，与推荐系统、搜索或广告的核心技术栈没有直接关联。即使考虑跨领域应用，3D场景理解在RecSys/Search/Ads中的潜在应用场景极其有限且不明确，因此相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.07004v1": {
    "title": "Exploring the \"Great Unseen\" in Medieval Manuscripts: Instance-Level Labeling of Legacy Image Collections with Zero-Shot Models",
    "url": "https://www.alphaxiv.org/abs/2511.07004v1",
    "arxiv_id": "2511.07004v1",
    "authors": "Christofer Meinecke, Estelle Guéville, David Joseph Wrisley",
    "categories": "cs.CV, cs.HC",
    "pub_date": "2025-11-10 11:56:58",
    "ori_summary": "We aim to theorize the medieval manuscript page and its contents more holistically, using state-of-the-art techniques to segment and describe the entire manuscript folio, for the purpose of creating richer training data for computer vision techniques, namely instance segmentation, and multimodal models for medieval-specific visual content.",
    "summary": "",
    "translation": "探索中世纪手稿中的“伟大未见面”：使用零样本模型对遗留图像集进行实例级标注",
    "relevance_score": 1,
    "reasoning": "该论文专注于中世纪手稿图像分析和零样本模型在历史文档处理中的应用，这属于特定领域的历史文档分析。虽然涉及零样本学习技术，但其应用场景（中世纪手稿）与推荐系统、搜索或广告领域没有直接关联，且没有明确的跨模态建模或推荐系统应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06973v1": {
    "title": "Oh That Looks Familiar: A Novel Similarity Measure for Spreadsheet Template Discovery",
    "url": "https://www.alphaxiv.org/abs/2511.06973v1",
    "arxiv_id": "2511.06973v1",
    "authors": "Ananad Krishnakumar, Vengadesh Ravikumaran",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-11-10 11:25:55",
    "ori_summary": "Traditional methods for identifying structurally similar spreadsheets fail to capture the spatial layouts and type patterns defining templates. To quantify spreadsheet similarity, we introduce a hybrid distance metric that combines semantic embeddings, data type information, and spatial positioning. In order to calculate spreadsheet similarity, our method converts spreadsheets into cell-level embeddings and then uses aggregation techniques like Chamfer and Hausdorff distances. Experiments across template families demonstrate superior unsupervised clustering performance compared to the graph-based Mondrian baseline, achieving perfect template reconstruction (Adjusted Rand Index of 1.00 versus 0.90) on the FUSTE dataset. Our approach facilitates large-scale automated template discovery, which in turn enables downstream applications such as retrieval-augmented generation over tabular collections, model training, and bulk data cleaning.",
    "summary": "",
    "translation": "哦，这看起来熟悉：一种用于电子表格模板发现的新型相似性度量方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注电子表格模板发现的相似性度量，属于特定领域的文档处理技术。虽然相似性度量在推荐系统中具有基础作用，但电子表格模板这一特定应用场景与搜索、推荐或广告系统的核心需求关联较弱，且未涉及LLM、Transformer架构或异构数据统一建模等关键技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06958v1": {
    "title": "Learning from the Right Patches: A Two-Stage Wavelet-Driven Masked Autoencoder for Histopathology Representation Learning",
    "url": "https://www.alphaxiv.org/abs/2511.06958v1",
    "arxiv_id": "2511.06958v1",
    "authors": "Raneen Younis, Louay Hamdi, Lukas Chavez, Zahra Ahmadi",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 11:06:25",
    "ori_summary": "Whole-slide images are central to digital pathology, yet their extreme size and scarce annotations make self-supervised learning essential. Masked Autoencoders (MAEs) with Vision Transformer backbones have recently shown strong potential for histopathology representation learning. However, conventional random patch sampling during MAE pretraining often includes irrelevant or noisy regions, limiting the model's ability to capture meaningful tissue patterns. In this paper, we present a lightweight and domain-adapted framework that brings structure and biological relevance into MAE-based learning through a wavelet-informed patch selection strategy. WISE-MAE applies a two-step coarse-to-fine process: wavelet-based screening at low magnification to locate structurally rich regions, followed by high-resolution extraction for detailed modeling. This approach mirrors the diagnostic workflow of pathologists and improves the quality of learned representations. Evaluations across multiple cancer datasets, including lung, renal, and colorectal tissues, show that WISE-MAE achieves competitive representation quality and downstream classification performance while maintaining efficiency under weak supervision.",
    "summary": "",
    "translation": "从正确补丁中学习：用于病理学表征学习的双阶段小波驱动掩码自编码器",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于病理学图像的表征学习，这属于医学领域的特定应用，与我的关注点（推荐系统、搜索、广告）完全无关。虽然提到了掩码自编码器技术，但其应用场景严格限定在医疗图像处理领域，没有任何与推荐、搜索或广告相关的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06953v1": {
    "title": "GFix: Perceptually Enhanced Gaussian Splatting Video Compression",
    "url": "https://www.alphaxiv.org/abs/2511.06953v1",
    "arxiv_id": "2511.06953v1",
    "authors": "Siyue Teng, Ge Gao, Duolikun Danier, Yuxuan Jiang, Fan Zhang, Thomas Davis, Zoe Liu, David Bull",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 11:03:30",
    "ori_summary": "3D Gaussian Splatting (3DGS) enhances 3D scene reconstruction through explicit representation and fast rendering, demonstrating potential benefits for various low-level vision tasks, including video compression. However, existing 3DGS-based video codecs generally exhibit more noticeable visual artifacts and relatively low compression ratios. In this paper, we specifically target the perceptual enhancement of 3DGS-based video compression, based on the assumption that artifacts from 3DGS rendering and quantization resemble noisy latents sampled during diffusion training. Building on this premise, we propose a content-adaptive framework, GFix, comprising a streamlined, single-step diffusion model that serves as an off-the-shelf neural enhancer. Moreover, to increase compression efficiency, We propose a modulated LoRA scheme that freezes the low-rank decompositions and modulates the intermediate hidden states, thereby achieving efficient adaptation of the diffusion backbone with highly compressible updates. Experimental results show that GFix delivers strong perceptual quality enhancement, outperforming GSVC with up to 72.1% BD-rate savings in LPIPS and 21.4% in FID.",
    "summary": "",
    "translation": "GFix：感知增强的高斯泼溅视频压缩",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频压缩技术，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术栈没有直接关联。高斯泼溅是一种3D场景表示方法，视频压缩主要涉及存储和传输效率，这些技术在RecSys/Search/Ads领域的潜在应用场景非常有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06948v1": {
    "title": "PADM: A Physics-aware Diffusion Model for Attenuation Correction",
    "url": "https://www.alphaxiv.org/abs/2511.06948v1",
    "arxiv_id": "2511.06948v1",
    "authors": "Trung Kien Pham, Hoang Minh Vu, Anh Duc Chu, Dac Thai Nguyen, Trung Thanh Nguyen, Thao Nguyen Truong, Mai Hong Son, Thanh Trung Nguyen, Phi Le Nguyen",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 10:54:46",
    "ori_summary": "Attenuation artifacts remain a significant challenge in cardiac Myocardial Perfusion Imaging (MPI) using Single-Photon Emission Computed Tomography (SPECT), often compromising diagnostic accuracy and reducing clinical interpretability. While hybrid SPECT/CT systems mitigate these artifacts through CT-derived attenuation maps, their high cost, limited accessibility, and added radiation exposure hinder widespread clinical adoption. In this study, we propose a novel CT-free solution to attenuation correction in cardiac SPECT. Specifically, we introduce Physics-aware Attenuation Correction Diffusion Model (PADM), a diffusion-based generative method that incorporates explicit physics priors via a teacher--student distillation mechanism. This approach enables attenuation artifact correction using only Non-Attenuation-Corrected (NAC) input, while still benefiting from physics-informed supervision during training. To support this work, we also introduce CardiAC, a comprehensive dataset comprising 424 patient studies with paired NAC and Attenuation-Corrected (AC) reconstructions, alongside high-resolution CT-based attenuation maps. Extensive experiments demonstrate that PADM outperforms state-of-the-art generative models, delivering superior reconstruction fidelity across both quantitative metrics and visual assessment.",
    "summary": "",
    "translation": "PADM：一种用于衰减校正的物理感知扩散模型",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学影像处理领域（衰减校正），这属于明确的无关主题范畴。虽然涉及扩散模型技术，但其物理感知特性专门针对医学物理应用，与推荐系统、搜索或广告领域没有任何直接或间接的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06947v1": {
    "title": "FoCLIP: A Feature-Space Misalignment Framework for CLIP-Based Image Manipulation and Detection",
    "url": "https://www.alphaxiv.org/abs/2511.06947v1",
    "arxiv_id": "2511.06947v1",
    "authors": "Yulin Chen, Zeyuan Wang, Tianyuan Yu, Yingmei Wei, Liang Bai",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-10 10:54:35",
    "ori_summary": "The well-aligned attribute of CLIP-based models enables its effective application like CLIPscore as a widely adopted image quality assessment metric. However, such a CLIP-based metric is vulnerable for its delicate multimodal alignment. In this work, we propose \\textbf{FoCLIP}, a feature-space misalignment framework for fooling CLIP-based image quality metric. Based on the stochastic gradient descent technique, FoCLIP integrates three key components to construct fooling examples: feature alignment as the core module to reduce image-text modality gaps, the score distribution balance module and pixel-guard regularization, which collectively optimize multimodal output equilibrium between CLIPscore performance and image quality. Such a design can be engineered to maximize the CLIPscore predictions across diverse input prompts, despite exhibiting either visual unrecognizability or semantic incongruence with the corresponding adversarial prompts from human perceptual perspectives. Experiments on ten artistic masterpiece prompts and ImageNet subsets demonstrate that optimized images can achieve significant improvement in CLIPscore while preserving high visual fidelity. In addition, we found that grayscale conversion induces significant feature degradation in fooling images, exhibiting noticeable CLIPscore reduction while preserving statistical consistency with original images. Inspired by this phenomenon, we propose a color channel sensitivity-driven tampering detection mechanism that achieves 91% accuracy on standard benchmarks. In conclusion, this work establishes a practical pathway for feature misalignment in CLIP-based multimodal systems and the corresponding defense method.",
    "summary": "",
    "translation": "FoCLIP：一种基于CLIP的图像操作与检测的特征空间错位框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注CLIP模型在图像操作和检测方面的应用，属于计算机视觉领域。虽然CLIP是多模态模型，但论文聚焦于图像内容的修改和识别，与推荐系统、搜索或广告中的核心排名和用户建模问题缺乏直接关联。其技术框架难以直接应用于处理推荐系统中的异构数据或提升排序性能。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06944v1": {
    "title": "From Attribution to Action: Jointly ALIGNing Predictions and Explanations",
    "url": "https://www.alphaxiv.org/abs/2511.06944v1",
    "arxiv_id": "2511.06944v1",
    "authors": "Dongsheng Hong, Chao Chen, Yanhui Chen, Shanshan Lin, Zhihao Chen, Xiangwen Liao",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-10 10:52:17",
    "ori_summary": "Explanation-guided learning (EGL) has shown promise in aligning model predictions with interpretable reasoning, particularly in computer vision tasks. However, most approaches rely on external annotations or heuristic-based segmentation to supervise model explanations, which can be noisy, imprecise and difficult to scale. In this work, we provide both empirical and theoretical evidence that low-quality supervision signals can degrade model performance rather than improve it. In response, we propose ALIGN, a novel framework that jointly trains a classifier and a masker in an iterative manner. The masker learns to produce soft, task-relevant masks that highlight informative regions, while the classifier is optimized for both prediction accuracy and alignment between its saliency maps and the learned masks. By leveraging high-quality masks as guidance, ALIGN improves both interpretability and generalizability, showing its superiority across various settings. Experiments on the two domain generalization benchmarks, VLCS and Terra Incognita, show that ALIGN consistently outperforms six strong baselines in both in-distribution and out-of-distribution settings. Besides, ALIGN also yields superior explanation quality concerning sufficiency and comprehensiveness, highlighting its effectiveness in producing accurate and interpretable models.",
    "summary": "",
    "translation": "从归因到行动：联合对齐预测与解释",
    "relevance_score": 2,
    "reasoning": "该论文主要关注模型解释性和归因方法，属于可解释AI范畴，与推荐系统、搜索或广告的核心技术进展关联较弱。虽然模型解释在工业应用中具有一定价值，但该工作更偏向于通用ML/NLP的可解释性研究，缺乏明确的RecSys/Search/Ads应用场景或技术突破。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06943v1": {
    "title": "PlantTraitNet: An Uncertainty-Aware Multimodal Framework for Global-Scale Plant Trait Inference from Citizen Science Data",
    "url": "https://www.alphaxiv.org/abs/2511.06943v1",
    "arxiv_id": "2511.06943v1",
    "authors": "Ayushi Sharma, Johanna Trost, Daniel Lusk, Johannes Dollinger, Julian Schrader, Christian Rossi, Javier Lopatin, Etienne Laliberté, Simon Haberstroh, Jana Eichel, Daniel Mederer, Jose Miguel Cerda-Paredes, Shyam S. Phartyal, Lisa-Maricia Schwarz, Anja Linstädter, Maria Conceição Caldeira, Teja Kattenborn",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-10 10:51:04",
    "ori_summary": "Global plant maps of plant traits, such as leaf nitrogen or plant height, are essential for understanding ecosystem processes, including the carbon and energy cycles of the Earth system. However, existing trait maps remain limited by the high cost and sparse geographic coverage of field-based measurements. Citizen science initiatives offer a largely untapped resource to overcome these limitations, with over 50 million geotagged plant photographs worldwide capturing valuable visual information on plant morphology and physiology. In this study, we introduce PlantTraitNet, a multi-modal, multi-task uncertainty-aware deep learning framework that predictsfour key plant traits (plant height, leaf area, specific leaf area, and nitrogen content) from citizen science photos using weak supervision. By aggregating individual trait predictions across space, we generate global maps of trait distributions. We validate these maps against independent vegetation survey data (sPlotOpen) and benchmark them against leading global trait products. Our results show that PlantTraitNet consistently outperforms existing trait maps across all evaluated traits, demonstrating that citizen science imagery, when integrated with computer vision and geospatial AI, enables not only scalable but also more accurate global trait mapping. This approach offers a powerful new pathway for ecological research and Earth system modeling.",
    "summary": "",
    "translation": "PlantTraitNet：一种基于公民科学数据的全球尺度植物性状推断的不确定性感知多模态框架",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于植物性状推断和公民科学数据，属于生物学领域特定应用。虽然提到了多模态框架，但其应用场景（植物性状、公民科学）与推荐系统、搜索或广告领域完全无关，不涉及任何Transformer架构、LLM技术或推荐系统核心进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06925v1": {
    "title": "DTTNet: Improving Video Shadow Detection via Dark-Aware Guidance and Tokenized Temporal Modeling",
    "url": "https://www.alphaxiv.org/abs/2511.06925v1",
    "arxiv_id": "2511.06925v1",
    "authors": "Zhicheng Li, Kunyang Sun, Rui Yao, Hancheng Zhu, Fuyuan Hu, Jiaqi Zhao, Zhiwen Shao, Yong Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 10:18:26",
    "ori_summary": "Video shadow detection confronts two entwined difficulties: distinguishing shadows from complex backgrounds and modeling dynamic shadow deformations under varying illumination. To address shadow-background ambiguity, we leverage linguistic priors through the proposed Vision-language Match Module (VMM) and a Dark-aware Semantic Block (DSB), extracting text-guided features to explicitly differentiate shadows from dark objects. Furthermore, we introduce adaptive mask reweighting to downweight penumbra regions during training and apply edge masks at the final decoder stage for better supervision. For temporal modeling of variable shadow shapes, we propose a Tokenized Temporal Block (TTB) that decouples spatiotemporal learning. TTB summarizes cross-frame shadow semantics into learnable temporal tokens, enabling efficient sequence encoding with minimal computation overhead. Comprehensive Experiments on multiple benchmark datasets demonstrate state-of-the-art accuracy and real-time inference efficiency. Codes are available at https://github.com/city-cheng/DTTNet.",
    "summary": "",
    "translation": "DTTNet：通过暗区感知引导与令牌化时序建模改进视频阴影检测",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉中的视频阴影检测任务，属于纯粹的视觉处理领域。虽然提到了时序建模技术，但其应用场景（阴影检测）与推荐系统、搜索或广告的核心技术栈没有直接关联，也没有展示出在异构数据处理方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06908v1": {
    "title": "Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text Encoding for Monocular 3D Visual Grounding",
    "url": "https://www.alphaxiv.org/abs/2511.06908v1",
    "arxiv_id": "2511.06908v1",
    "authors": "Yuzhen Li, Min Liu, Zhaoyang Li, Yuan Bian, Xueping Wang, Erbo Zhai, Yaonan Wang",
    "categories": "cs.CV, cs.MM",
    "pub_date": "2025-11-10 10:02:30",
    "ori_summary": "Monocular 3D Visual Grounding (Mono3DVG) is an emerging task that locates 3D objects in RGB images using text descriptions with geometric cues. However, existing methods face two key limitations. Firstly, they often over-rely on high-certainty keywords that explicitly identify the target object while neglecting critical spatial descriptions. Secondly, generalized textual features contain both 2D and 3D descriptive information, thereby capturing an additional dimension of details compared to singular 2D or 3D visual features. This characteristic leads to cross-dimensional interference when refining visual features under text guidance. To overcome these challenges, we propose Mono3DVG-EnSD, a novel framework that integrates two key components: the CLIP-Guided Lexical Certainty Adapter (CLIP-LCA) and the Dimension-Decoupled Module (D2M). The CLIP-LCA dynamically masks high-certainty keywords while retaining low-certainty implicit spatial descriptions, thereby forcing the model to develop a deeper understanding of spatial relationships in captions for object localization. Meanwhile, the D2M decouples dimension-specific (2D/3D) textual features from generalized textual features to guide corresponding visual features at same dimension, which mitigates cross-dimensional interference by ensuring dimensionally-consistent cross-modal interactions. Through comprehensive comparisons and ablation studies on the Mono3DRefer dataset, our method achieves state-of-the-art (SOTA) performance across all metrics. Notably, it improves the challenging Far(Acc@0.5) scenario by a significant +13.54%.",
    "summary": "",
    "translation": "Mono3DVG-EnSD：用于单目3D视觉定位的增强空间感知与维度解耦文本编码",
    "relevance_score": 2,
    "reasoning": "该论文主要关注单目3D视觉定位中的文本编码技术，属于纯粹的计算机视觉领域。虽然提到了空间感知和文本编码，但其核心应用场景是3D视觉定位，与推荐系统、搜索或广告的排序任务没有直接关联。该技术缺乏明确的跨模态建模应用潜力，无法直接服务于RecSys/Search/Ads的核心需求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06901v1": {
    "title": "Classification of Microplastic Particles in Water using Polarized Light Scattering and Machine Learning Methods",
    "url": "https://www.alphaxiv.org/abs/2511.06901v1",
    "arxiv_id": "2511.06901v1",
    "authors": "Leonard Saur, Marc von Pawlowski, Ulrich Gengenbach, Ingo Sieber, Hossein Shirali, Lorenz Wührl, Rainer Kiko, Christian Pylatiuk",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 09:51:15",
    "ori_summary": "Facing the critical need for continuous, large-scale microplastic monitoring, which is hindered by the limitations of gold-standard methods in aquatic environments, this paper introduces and validates a novel, reflection-based approach for the in-situ classification and identification of microplastics directly in water bodies, which is based on polarized light scattering. In this experiment, we classify colorless microplastic particles (50-300 $\\mu$m) by illuminating them with linearly polarized laser light and capturing their reflected signals using a polarization-sensitive camera. This reflection-based technique successfully circumvents the transmission-based interference issues that plague many conventional methods when applied in water. Using a deep convolutional neural network (CNN) for image-based classification, we successfully identified three common polymer types, high-density polyethylene, low-density polyethylene, and polypropylene, achieving a peak mean classification accuracy of 80% on the test dataset. A subsequent feature hierarchy analysis demonstrated that the CNN's decision-making process relies mainly on the microstructural integrity and internal texture (polarization patterns) of the particle rather than its macroshape. Critically, we found that the Angle of Linear Polarization (AOLP) signal is significantly more robust against contextual noise than the Degree of Linear Polarization (DOLP) signal. While the AOLP-based classification achieved superior overall performance, its strength lies in distinguishing between the two polyethylene plastics, showing a lower confusion rate between high-density and low-density polyethylene. Conversely, the DOLP signal demonstrated slightly worse overall classification results but excels at accurately identifying the polypropylene class, which it isolated with greater success than AOLP.",
    "summary": "",
    "translation": "基于偏振光散射与机器学习方法的水中微塑料颗粒分类",
    "relevance_score": 1,
    "reasoning": "该论文专注于环境科学中的微塑料检测，使用偏振光散射和传统机器学习方法。这与推荐系统、搜索、广告或LLM技术完全无关，也不涉及任何Transformer架构或异构数据处理。该研究属于纯粹的领域特定应用，超出了所有相关主题范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06897v1": {
    "title": "Adaptive Morph-Patch Transformer for Arotic Vessel Segmentation",
    "url": "https://www.alphaxiv.org/abs/2511.06897v1",
    "arxiv_id": "2511.06897v1",
    "authors": "Zhenxi Zhang, Fuchen Zheng, Adnan Iltaf, Yifei Han, Zhenyu Cheng, Yue Du, Bin Li, Tianyong Liu, Shoujun Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 09:46:04",
    "ori_summary": "Accurate segmentation of aortic vascular structures is critical for diagnosing and treating cardiovascular diseases.Traditional Transformer-based models have shown promise in this domain by capturing long-range dependencies between vascular features. However, their reliance on fixed-size rectangular patches often influences the integrity of complex vascular structures, leading to suboptimal segmentation accuracy. To address this challenge, we propose the adaptive Morph Patch Transformer (MPT), a novel architecture specifically designed for aortic vascular segmentation. Specifically, MPT introduces an adaptive patch partitioning strategy that dynamically generates morphology-aware patches aligned with complex vascular structures. This strategy can preserve semantic integrity of complex vascular structures within individual patches. Moreover, a Semantic Clustering Attention (SCA) method is proposed to dynamically aggregate features from various patches with similar semantic characteristics. This method enhances the model's capability to segment vessels of varying sizes, preserving the integrity of vascular structures. Extensive experiments on three open-source dataset(AVT, AortaSeg24 and TBAD) demonstrate that MPT achieves state-of-the-art performance, with improvements in segmenting intricate vascular structures.",
    "summary": "",
    "translation": "用于主动脉血管分割的自适应形态-补丁Transformer",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像分割领域，具体针对主动脉血管分割，这属于明确的医学/生物学应用范畴。虽然使用了Transformer架构，但该应用与推荐系统、搜索或广告领域没有任何关联，完全落在指定的无关主题范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06888v1": {
    "title": "A Two-Stage System for Layout-Controlled Image Generation using Large Language Models and Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2511.06888v1",
    "arxiv_id": "2511.06888v1",
    "authors": "Jan-Hendrik Koch, Jonas Krumme, Konrad Gadzicki",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 09:40:48",
    "ori_summary": "Text-to-image diffusion models exhibit remarkable generative capabilities, but lack precise control over object counts and spatial arrangements. This work introduces a two-stage system to address these compositional limitations. The first stage employs a Large Language Model (LLM) to generate a structured layout from a list of objects. The second stage uses a layout-conditioned diffusion model to synthesize a photorealistic image adhering to this layout. We find that task decomposition is critical for LLM-based spatial planning; by simplifying the initial generation to core objects and completing the layout with rule-based insertion, we improve object recall from 57.2% to 99.9% for complex scenes. For image synthesis, we compare two leading conditioning methods: ControlNet and GLIGEN. After domain-specific finetuning on table-setting datasets, we identify a key trade-off: ControlNet preserves text-based stylistic control but suffers from object hallucination, while GLIGEN provides superior layout fidelity at the cost of reduced prompt-based controllability. Our end-to-end system successfully generates images with specified object counts and plausible spatial arrangements, demonstrating the viability of a decoupled approach for compositionally controlled synthesis.",
    "summary": "",
    "translation": "基于大语言模型和扩散模型的布局控制图像生成两阶段系统",
    "relevance_score": 2,
    "reasoning": "该论文主要关注图像生成任务，属于AIGC和内容生成范畴，这在无关主题中被明确排除。虽然涉及LLM技术，但应用场景是图像生成而非推荐系统、搜索或广告中的排名或建模任务。布局控制图像生成与RecSys/Search/Ads的核心技术需求没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06876v1": {
    "title": "Generating an Image From 1,000 Words: Enhancing Text-to-Image With Structured Captions",
    "url": "https://www.alphaxiv.org/abs/2511.06876v1",
    "arxiv_id": "2511.06876v1",
    "authors": "Eyal Gutflaish, Eliran Kachlon, Hezi Zisman, Tal Hacham, Nimrod Sarid, Alexander Visheratin, Saar Huberman, Gal Davidi, Guy Bukchin, Kfir Goldberg, Ron Mokady",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 09:25:25",
    "ori_summary": "Text-to-image models have rapidly evolved from casual creative tools to professional-grade systems, achieving unprecedented levels of image quality and realism. Yet, most models are trained to map short prompts into detailed images, creating a gap between sparse textual input and rich visual outputs. This mismatch reduces controllability, as models often fill in missing details arbitrarily, biasing toward average user preferences and limiting precision for professional use. We address this limitation by training the first open-source text-to-image model on long structured captions, where every training sample is annotated with the same set of fine-grained attributes. This design maximizes expressive coverage and enables disentangled control over visual factors. To process long captions efficiently, we propose DimFusion, a fusion mechanism that integrates intermediate tokens from a lightweight LLM without increasing token length. We also introduce the Text-as-a-Bottleneck Reconstruction (TaBR) evaluation protocol. By assessing how well real images can be reconstructed through a captioning-generation loop, TaBR directly measures controllability and expressiveness, even for very long captions where existing evaluation methods fail. Finally, we demonstrate our contributions by training the large-scale model FIBO, achieving state-of-the-art prompt alignment among open-source models. Model weights are publicly available at https://huggingface.co/briaai/FIBO",
    "summary": "",
    "translation": "从千词生成图像：通过结构化描述增强文本到图像生成",
    "relevance_score": 2,
    "reasoning": "该论文专注于文本到图像生成技术，属于纯粹的AIGC和内容生成领域。虽然结构化描述的概念在推荐系统中可能有类比应用，但论文本身没有展示与推荐系统、搜索或广告排名的直接相关性，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06863v1": {
    "title": "VAEVQ: Enhancing Discrete Visual Tokenization through Variational Modeling",
    "url": "https://www.alphaxiv.org/abs/2511.06863v1",
    "arxiv_id": "2511.06863v1",
    "authors": "Sicheng Yang, Xing Hu, Qiang Wu, Dawei Yang",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 09:07:23",
    "ori_summary": "Vector quantization (VQ) transforms continuous image features into discrete representations, providing compressed, tokenized inputs for generative models. However, VQ-based frameworks suffer from several issues, such as non-smooth latent spaces, weak alignment between representations before and after quantization, and poor coherence between the continuous and discrete domains. These issues lead to unstable codeword learning and underutilized codebooks, ultimately degrading the performance of both reconstruction and downstream generation tasks. To this end, we propose VAEVQ, which comprises three key components: (1) Variational Latent Quantization (VLQ), replacing the AE with a VAE for quantization to leverage its structured and smooth latent space, thereby facilitating more effective codeword activation; (2) Representation Coherence Strategy (RCS), adaptively modulating the alignment strength between pre- and post-quantization features to enhance consistency and prevent overfitting to noise; and (3) Distribution Consistency Regularization (DCR), aligning the entire codebook distribution with the continuous latent distribution to improve utilization. Extensive experiments on two benchmark datasets demonstrate that VAEVQ outperforms state-of-the-art methods.",
    "summary": "",
    "translation": "VAEVQ：通过变分建模增强离散视觉分词",
    "relevance_score": 2,
    "reasoning": "该论文专注于视觉领域的离散分词技术改进，属于计算机视觉范畴。虽然标题提到变分建模和离散化技术，但这些方法主要针对视觉数据处理，与推荐系统、搜索或广告中的文本序列处理、用户行为建模等核心问题缺乏直接关联。即使作为使能技术，其视觉导向的特性也难以直接应用于RecSys/Search/Ads领域中的主要数据类型和任务。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06857v1": {
    "title": "Ambiguity-aware Truncated Flow Matching for Ambiguous Medical Image Segmentation",
    "url": "https://www.alphaxiv.org/abs/2511.06857v1",
    "arxiv_id": "2511.06857v1",
    "authors": "Fanding Li, Xiangyu Li, Xianghe Su, Xingyu Qiu, Suyu Dong, Wei Wang, Kuanquan Wang, Gongning Luo, Shuo Li",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 08:57:06",
    "ori_summary": "A simultaneous enhancement of accuracy and diversity of predictions remains a challenge in ambiguous medical image segmentation (AMIS) due to the inherent trade-offs. While truncated diffusion probabilistic models (TDPMs) hold strong potential with a paradigm optimization, existing TDPMs suffer from entangled accuracy and diversity of predictions with insufficient fidelity and plausibility. To address the aforementioned challenges, we propose Ambiguity-aware Truncated Flow Matching (ATFM), which introduces a novel inference paradigm and dedicated model components. Firstly, we propose Data-Hierarchical Inference, a redefinition of AMIS-specific inference paradigm, which enhances accuracy and diversity at data-distribution and data-sample level, respectively, for an effective disentanglement. Secondly, Gaussian Truncation Representation (GTR) is introduced to enhance both fidelity of predictions and reliability of truncation distribution, by explicitly modeling it as a Gaussian distribution at $T_{\\text{trunc}}$ instead of using sampling-based approximations.Thirdly, Segmentation Flow Matching (SFM) is proposed to enhance the plausibility of diverse predictions by extending semantic-aware flow transformation in Flow Matching (FM). Comprehensive evaluations on LIDC and ISIC3 datasets demonstrate that ATFM outperforms SOTA methods and simultaneously achieves a more efficient inference. ATFM improves GED and HM-IoU by up to $12\\%$ and $7.3\\%$ compared to advanced methods.",
    "summary": "",
    "translation": "面向模糊医学图像分割的模糊感知截断流匹配",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分割这一特定领域应用，与推荐系统、搜索或广告领域完全无关。论文标题明确提及医学图像分割，这属于明确的无关主题范畴，没有任何潜在的应用于推荐系统、搜索或广告的技术路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06848v1": {
    "title": "Distillation Dynamics: Towards Understanding Feature-Based Distillation in Vision Transformers",
    "url": "https://www.alphaxiv.org/abs/2511.06848v1",
    "arxiv_id": "2511.06848v1",
    "authors": "Huiyuan Tian, Bonan Xu Shijian Li",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 08:46:30",
    "ori_summary": "While feature-based knowledge distillation has proven highly effective for compressing CNNs, these techniques unexpectedly fail when applied to Vision Transformers (ViTs), often performing worse than simple logit-based distillation. We provide the first comprehensive analysis of this phenomenon through a novel analytical framework termed as ``distillation dynamics\", combining frequency spectrum analysis, information entropy metrics, and activation magnitude tracking. Our investigation reveals that ViTs exhibit a distinctive U-shaped information processing pattern: initial compression followed by expansion. We identify the root cause of negative transfer in feature distillation: a fundamental representational paradigm mismatch between teacher and student models. Through frequency-domain analysis, we show that teacher models employ distributed, high-dimensional encoding strategies in later layers that smaller student models cannot replicate due to limited channel capacity. This mismatch causes late-layer feature alignment to actively harm student performance. Our findings reveal that successful knowledge transfer in ViTs requires moving beyond naive feature mimicry to methods that respect these fundamental representational constraints, providing essential theoretical guidance for designing effective ViTs compression strategies. All source code and experimental logs are provided in the supplementary material.",
    "summary": "",
    "translation": "蒸馏动态：理解视觉Transformer中基于特征的蒸馏",
    "relevance_score": 3,
    "reasoning": "该论文主要研究视觉Transformer中的特征蒸馏技术，属于模型压缩和知识迁移领域。虽然蒸馏技术本身可能作为使能技术应用于推荐或搜索系统中的模型优化，但论文明确聚焦于视觉Transformer，且未提及在推荐/搜索/广告领域的潜在应用，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06846v1": {
    "title": "Gaussian-Augmented Physics Simulation and System Identification with Complex Colliders",
    "url": "https://www.alphaxiv.org/abs/2511.06846v1",
    "arxiv_id": "2511.06846v1",
    "authors": "Federico Vasile, Ri-Zhao Qiu, Lorenzo Natale, Xiaolong Wang",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 08:44:19",
    "ori_summary": "System identification involving the geometry, appearance, and physical properties from video observations is a challenging task with applications in robotics and graphics. Recent approaches have relied on fully differentiable Material Point Method (MPM) and rendering for simultaneous optimization of these properties. However, they are limited to simplified object-environment interactions with planar colliders and fail in more challenging scenarios where objects collide with non-planar surfaces. We propose AS-DiffMPM, a differentiable MPM framework that enables physical property estimation with arbitrarily shaped colliders. Our approach extends existing methods by incorporating a differentiable collision handling mechanism, allowing the target object to interact with complex rigid bodies while maintaining end-to-end optimization. We show AS-DiffMPM can be easily interfaced with various novel view synthesis methods as a framework for system identification from visual observations.",
    "summary": "",
    "translation": "基于复杂碰撞体的高斯增强物理仿真与系统辨识",
    "relevance_score": 1,
    "reasoning": "该论文专注于物理仿真和系统辨识领域，涉及高斯增强方法和复杂碰撞体，属于物理学和工程仿真范畴。这些主题与推荐系统、搜索或广告的核心技术进展、LLM技术或Transformer架构改进没有直接关联，也没有明显的潜在应用路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06841v1": {
    "title": "Aerial Image Stitching Using IMU Data from a UAV",
    "url": "https://www.alphaxiv.org/abs/2511.06841v1",
    "arxiv_id": "2511.06841v1",
    "authors": "Selim Ahmet Iz, Mustafa Unel",
    "categories": "cs.CV, cs.RO, cs.SY, eess.SY, math.DS",
    "pub_date": "2025-11-10 08:33:00",
    "ori_summary": "Unmanned Aerial Vehicles (UAVs) are widely used for aerial photography and remote sensing applications. One of the main challenges is to stitch together multiple images into a single high-resolution image that covers a large area. Featurebased image stitching algorithms are commonly used but can suffer from errors and ambiguities in feature detection and matching. To address this, several approaches have been proposed, including using bundle adjustment techniques or direct image alignment. In this paper, we present a novel method that uses a combination of IMU data and computer vision techniques for stitching images captured by a UAV. Our method involves several steps such as estimating the displacement and rotation of the UAV between consecutive images, correcting for perspective distortion, and computing a homography matrix. We then use a standard image stitching algorithm to align and blend the images together. Our proposed method leverages the additional information provided by the IMU data, corrects for various sources of distortion, and can be easily integrated into existing UAV workflows. Our experiments demonstrate the effectiveness and robustness of our method, outperforming some of the existing feature-based image stitching algorithms in terms of accuracy and reliability, particularly in challenging scenarios such as large displacements, rotations, and variations in camera pose.",
    "summary": "",
    "translation": "基于无人机IMU数据的航拍图像拼接",
    "relevance_score": 1,
    "reasoning": "该论文专注于无人机航拍图像拼接技术，属于纯粹的计算机视觉应用领域。虽然使用了IMU数据，但这与推荐系统、搜索或广告的核心技术栈没有直接关联，也不涉及LLM、Transformer架构或异构数据建模等关键技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06840v1": {
    "title": "PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing and Dynamic Memory",
    "url": "https://www.alphaxiv.org/abs/2511.06840v1",
    "arxiv_id": "2511.06840v1",
    "authors": "Qunchao Jin, Yilin Wu, Changhao Chen",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-11-10 08:31:32",
    "ori_summary": "Zero-shot object navigation (ZSON) in unseen environments remains a challenging problem for household robots, requiring strong perceptual understanding and decision-making capabilities. While recent methods leverage metric maps and Large Language Models (LLMs), they often depend on depth sensors or prebuilt maps, limiting the spatial reasoning ability of Multimodal Large Language Models (MLLMs). Mapless ZSON approaches have emerged to address this, but they typically make short-sighted decisions, leading to local deadlocks due to a lack of historical context. We propose PanoNav, a fully RGB-only, mapless ZSON framework that integrates a Panoramic Scene Parsing module to unlock the spatial parsing potential of MLLMs from panoramic RGB inputs, and a Memory-guided Decision-Making mechanism enhanced by a Dynamic Bounded Memory Queue to incorporate exploration history and avoid local deadlocks. Experiments on the public navigation benchmark show that PanoNav significantly outperforms representative baselines in both SR and SPL metrics.",
    "summary": "",
    "translation": "PanoNav：通过全景场景解析与动态记忆实现无地图零样本目标导航",
    "relevance_score": 2,
    "reasoning": "该论文主要关注机器人导航和计算机视觉中的场景理解任务，属于视觉导航领域。虽然涉及场景解析和记忆机制，但这些技术主要针对物理环境导航，与推荐系统、搜索或广告中的用户行为建模、内容理解等核心问题缺乏直接关联。其技术路径和应用场景与当前关注的LLM赋能推荐/搜索技术路线差异较大。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06839v1": {
    "title": "Vision-Based System Identification of a Quadrotor",
    "url": "https://www.alphaxiv.org/abs/2511.06839v1",
    "arxiv_id": "2511.06839v1",
    "authors": "Selim Ahmet Iz, Mustafa Unel",
    "categories": "cs.RO, cs.CV, cs.SY, eess.SY, math.DS",
    "pub_date": "2025-11-10 08:31:28",
    "ori_summary": "This paper explores the application of vision-based system identification techniques in quadrotor modeling and control. Through experiments and analysis, we address the complexities and limitations of quadrotor modeling, particularly in relation to thrust and drag coefficients. Grey-box modeling is employed to mitigate uncertainties, and the effectiveness of an onboard vision system is evaluated. An LQR controller is designed based on a system identification model using data from the onboard vision system. The results demonstrate consistent performance between the models, validating the efficacy of vision based system identification. This study highlights the potential of vision-based techniques in enhancing quadrotor modeling and control, contributing to improved performance and operational capabilities. Our findings provide insights into the usability and consistency of these techniques, paving the way for future research in quadrotor performance enhancement, fault detection, and decision-making processes.",
    "summary": "",
    "translation": "基于视觉的四旋翼无人机系统辨识",
    "relevance_score": 1,
    "reasoning": "该论文专注于四旋翼无人机的视觉系统辨识，属于机器人控制和计算机视觉领域。这与推荐系统、搜索或广告的核心技术领域完全无关，也不涉及LLM、Transformer架构或异构数据处理等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06836v1": {
    "title": "NeuroBridge: Bio-Inspired Self-Supervised EEG-to-Image Decoding via Cognitive Priors and Bidirectional Semantic Alignment",
    "url": "https://www.alphaxiv.org/abs/2511.06836v1",
    "arxiv_id": "2511.06836v1",
    "authors": "Wenjiang Zhang, Sifeng Wang, Yuwei Su, Xinyu Li, Chen Zhang, Suyu Zhong",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-10 08:29:09",
    "ori_summary": "Visual neural decoding seeks to reconstruct or infer perceived visual stimuli from brain activity patterns, providing critical insights into human cognition and enabling transformative applications in brain-computer interfaces and artificial intelligence. Current approaches, however, remain constrained by the scarcity of high-quality stimulus-brain response pairs and the inherent semantic mismatch between neural representations and visual content. Inspired by perceptual variability and co-adaptive strategy of the biological systems, we propose a novel self-supervised architecture, named NeuroBridge, which integrates Cognitive Prior Augmentation (CPA) with Shared Semantic Projector (SSP) to promote effective cross-modality alignment. Specifically, CPA simulates perceptual variability by applying asymmetric, modality-specific transformations to both EEG signals and images, enhancing semantic diversity. Unlike previous approaches, SSP establishes a bidirectional alignment process through a co-adaptive strategy, which mutually aligns features from two modalities into a shared semantic space for effective cross-modal learning. NeuroBridge surpasses previous state-of-the-art methods under both intra-subject and inter-subject settings. In the intra-subject scenario, it achieves the improvements of 12.3% in top-1 accuracy and 10.2% in top-5 accuracy, reaching 63.2% and 89.9% respectively on a 200-way zero-shot retrieval task. Extensive experiments demonstrate the effectiveness, robustness, and scalability of the proposed framework for neural visual decoding.",
    "summary": "",
    "translation": "NeuroBridge：基于认知先验和双向语义对齐的生物启发式自监督脑电图到图像解码",
    "relevance_score": 1,
    "reasoning": "该论文专注于脑电图（EEG）到图像的生物医学解码，属于医疗/生物领域的特定应用。虽然提到了自监督学习和语义对齐，但其核心是脑机接口和神经科学应用，与推荐系统、搜索或广告领域没有任何直接或潜在的关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06833v1": {
    "title": "ConsistTalk: Intensity Controllable Temporally Consistent Talking Head Generation with Diffusion Noise Search",
    "url": "https://www.alphaxiv.org/abs/2511.06833v1",
    "arxiv_id": "2511.06833v1",
    "authors": "Zhenjie Liu, Jianzhang Lu, Renjie Lu, Cong Liang, Shangfei Wang",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 08:28:13",
    "ori_summary": "Recent advancements in video diffusion models have significantly enhanced audio-driven portrait animation. However, current methods still suffer from flickering, identity drift, and poor audio-visual synchronization. These issues primarily stem from entangled appearance-motion representations and unstable inference strategies. In this paper, we introduce \\textbf{ConsistTalk}, a novel intensity-controllable and temporally consistent talking head generation framework with diffusion noise search inference. First, we propose \\textbf{an optical flow-guided temporal module (OFT)} that decouples motion features from static appearance by leveraging facial optical flow, thereby reducing visual flicker and improving temporal consistency. Second, we present an \\textbf{Audio-to-Intensity (A2I) model} obtained through multimodal teacher-student knowledge distillation. By transforming audio and facial velocity features into a frame-wise intensity sequence, the A2I model enables joint modeling of audio and visual motion, resulting in more natural dynamics. This further enables fine-grained, frame-wise control of motion dynamics while maintaining tight audio-visual synchronization. Third, we introduce a \\textbf{diffusion noise initialization strategy (IC-Init)}. By enforcing explicit constraints on background coherence and motion continuity during inference-time noise search, we achieve better identity preservation and refine motion dynamics compared to the current autoregressive strategy. Extensive experiments demonstrate that ConsistTalk significantly outperforms prior methods in reducing flicker, preserving identity, and delivering temporally stable, high-fidelity talking head videos.",
    "summary": "",
    "translation": "ConsistTalk：通过扩散噪声搜索实现强度可控的时序一致性说话头部生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于说话头部生成的计算机视觉任务，属于纯粹的视觉内容生成领域。虽然使用了扩散模型技术，但论文的核心应用是视频生成和面部动画，与推荐系统、搜索或广告的排名和建模任务没有直接关联。这种视觉生成技术无法直接应用于RecSys/Search/Ads的核心业务场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06830v1": {
    "title": "MUGSQA: Novel Multi-Uncertainty-Based Gaussian Splatting Quality Assessment Method, Dataset, and Benchmarks",
    "url": "https://www.alphaxiv.org/abs/2511.06830v1",
    "arxiv_id": "2511.06830v1",
    "authors": "Tianang Chen, Jian Jin, Shilv Cai, Zhuangzi Li, Weisi Lin",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 08:21:11",
    "ori_summary": "Gaussian Splatting (GS) has recently emerged as a promising technique for 3D object reconstruction, delivering high-quality rendering results with significantly improved reconstruction speed. As variants continue to appear, assessing the perceptual quality of 3D objects reconstructed with different GS-based methods remains an open challenge. To address this issue, we first propose a unified multi-distance subjective quality assessment method that closely mimics human viewing behavior for objects reconstructed with GS-based methods in actual applications, thereby better collecting perceptual experiences. Based on it, we also construct a novel GS quality assessment dataset named MUGSQA, which is constructed considering multiple uncertainties of the input data. These uncertainties include the quantity and resolution of input views, the view distance, and the accuracy of the initial point cloud. Moreover, we construct two benchmarks: one to evaluate the robustness of various GS-based reconstruction methods under multiple uncertainties, and the other to evaluate the performance of existing quality assessment metrics. Our dataset and benchmark code will be released soon.",
    "summary": "",
    "translation": "MUGSQA：基于多重不确定性的新型高斯溅射质量评估方法、数据集与基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的3D高斯溅射质量评估，这是一个纯粹的视觉技术主题。虽然标题提到质量评估，但该方法专门针对3D视觉表示，与推荐系统、搜索或广告中的排名、用户建模或内容理解没有明显关联。该技术缺乏在RecSys/Search/Ads领域的潜在应用前景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06823v1": {
    "title": "Integrating Reweighted Least Squares with Plug-and-Play Diffusion Priors for Noisy Image Restoration",
    "url": "https://www.alphaxiv.org/abs/2511.06823v1",
    "arxiv_id": "2511.06823v1",
    "authors": "Ji Li, Chao Wang",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 08:11:20",
    "ori_summary": "Existing plug-and-play image restoration methods typically employ off-the-shelf Gaussian denoisers as proximal operators within classical optimization frameworks based on variable splitting. Recently, denoisers induced by generative priors have been successfully integrated into regularized optimization methods for image restoration under Gaussian noise. However, their application to non-Gaussian noise--such as impulse noise--remains largely unexplored. In this paper, we propose a plug-and-play image restoration framework based on generative diffusion priors for robust removal of general noise types, including impulse noise. Within the maximum a posteriori (MAP) estimation framework, the data fidelity term is adapted to the specific noise model. Departing from the conventional least-squares loss used for Gaussian noise, we introduce a generalized Gaussian scale mixture-based loss, which approximates a wide range of noise distributions and leads to an $\\ell_q$-norm ($0<q\\leq2$) fidelity term. This optimization problem is addressed using an iteratively reweighted least squares (IRLS) approach, wherein the proximal step involving the generative prior is efficiently performed via a diffusion-based denoiser. Experimental results on benchmark datasets demonstrate that the proposed method effectively removes non-Gaussian impulse noise and achieves superior restoration performance.",
    "summary": "",
    "translation": "将重加权最小二乘法与即插即用扩散先验相结合用于噪声图像恢复",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的噪声图像恢复问题，使用扩散模型和优化方法。虽然扩散模型是生成模型的一种，但该工作专注于纯粹的图像处理任务，没有展示与推荐系统、搜索或广告系统的潜在应用连接。该技术缺乏明确的路径来应用于处理用户行为序列、上下文特征或排名任务。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06817v1": {
    "title": "TiS-TSL: Image-Label Supervised Surgical Video Stereo Matching via Time-Switchable Teacher-Student Learning",
    "url": "https://www.alphaxiv.org/abs/2511.06817v1",
    "arxiv_id": "2511.06817v1",
    "authors": "Rui Wang, Ying Zhou, Hao Wang, Wenwei Zhang, Qiang Li, Zhiwei Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-10 08:01:26",
    "ori_summary": "Stereo matching in minimally invasive surgery (MIS) is essential for next-generation navigation and augmented reality. Yet, dense disparity supervision is nearly impossible due to anatomical constraints, typically limiting annotations to only a few image-level labels acquired before the endoscope enters deep body cavities. Teacher-Student Learning (TSL) offers a promising solution by leveraging a teacher trained on sparse labels to generate pseudo labels and associated confidence maps from abundant unlabeled surgical videos. However, existing TSL methods are confined to image-level supervision, providing only spatial confidence and lacking temporal consistency estimation. This absence of spatio-temporal reliability results in unstable disparity predictions and severe flickering artifacts across video frames. To overcome these challenges, we propose TiS-TSL, a novel time-switchable teacher-student learning framework for video stereo matching under minimal supervision. At its core is a unified model that operates in three distinct modes: Image-Prediction (IP), Forward Video-Prediction (FVP), and Backward Video-Prediction (BVP), enabling flexible temporal modeling within a single architecture. Enabled by this unified model, TiS-TSL adopts a two-stage learning strategy. The Image-to-Video (I2V) stage transfers sparse image-level knowledge to initialize temporal modeling. The subsequent Video-to-Video (V2V) stage refines temporal disparity predictions by comparing forward and backward predictions to calculate bidirectional spatio-temporal consistency. This consistency identifies unreliable regions across frames, filters noisy video-level pseudo labels, and enforces temporal coherence. Experimental results on two public datasets demonstrate that TiS-TSL exceeds other image-based state-of-the-arts by improving TEPE and EPE by at least 2.11% and 4.54%, respectively..",
    "summary": "",
    "translation": "TiS-TSL：基于图像-标签监督的通过时间可切换师生学习的手术视频立体匹配",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的立体匹配技术，应用于手术视频这一特定医疗领域。虽然提到了师生学习框架，但其核心是视觉几何问题，与推荐系统、搜索或广告中的排序、用户建模、内容理解等核心技术没有直接关联。论文的医疗应用场景和纯粹的视觉任务使其超出了当前关注的技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06810v1": {
    "title": "ConeGS: Error-Guided Densification Using Pixel Cones for Improved Reconstruction with Fewer Primitives",
    "url": "https://www.alphaxiv.org/abs/2511.06810v1",
    "arxiv_id": "2511.06810v1",
    "authors": "Bartłomiej Baranowski, Stefano Esposito, Patricia Gschoßmann, Anpei Chen, Andreas Geiger",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 07:54:58",
    "ori_summary": "3D Gaussian Splatting (3DGS) achieves state-of-the-art image quality and real-time performance in novel view synthesis but often suffers from a suboptimal spatial distribution of primitives. This issue stems from cloning-based densification, which propagates Gaussians along existing geometry, limiting exploration and requiring many primitives to adequately cover the scene. We present ConeGS, an image-space-informed densification framework that is independent of existing scene geometry state. ConeGS first creates a fast Instant Neural Graphics Primitives (iNGP) reconstruction as a geometric proxy to estimate per-pixel depth. During the subsequent 3DGS optimization, it identifies high-error pixels and inserts new Gaussians along the corresponding viewing cones at the predicted depth values, initializing their size according to the cone diameter. A pre-activation opacity penalty rapidly removes redundant Gaussians, while a primitive budgeting strategy controls the total number of primitives, either by a fixed budget or by adapting to scene complexity, ensuring high reconstruction quality. Experiments show that ConeGS consistently enhances reconstruction quality and rendering performance across Gaussian budgets, with especially strong gains under tight primitive constraints where efficient placement is crucial.",
    "summary": "",
    "translation": "ConeGS：使用像素锥体进行误差引导的密集化，以更少基元实现改进的重建",
    "relevance_score": 1,
    "reasoning": "该论文涉及计算机图形学中的3D重建技术，专注于通过像素锥体和误差引导的密集化方法来优化重建质量。这与推荐系统、搜索或广告的核心技术领域没有直接关联，也不涉及LLM、Transformer架构或异构数据处理。该技术主要面向纯粹的视觉和图形应用，属于明确排除的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06769v1": {
    "title": "RRTS Dataset: A Benchmark Colonoscopy Dataset from Resource-Limited Settings for Computer-Aided Diagnosis Research",
    "url": "https://www.alphaxiv.org/abs/2511.06769v1",
    "arxiv_id": "2511.06769v1",
    "authors": "Ridoy Chandra Shil, Ragib Abid, Tasnia Binte Mamun, Samiul Based Shuvo, Masfique Ahmed Bhuiyan, Jahid Ferdous",
    "categories": "eess.IV, cs.CV",
    "pub_date": "2025-11-10 06:51:41",
    "ori_summary": "Background and Objective: Colorectal cancer prevention relies on early detection of polyps during colonoscopy. Existing public datasets, such as CVC-ClinicDB and Kvasir-SEG, provide valuable benchmarks but are limited by small sample sizes, curated image selection, or lack of real-world artifacts. There remains a need for datasets that capture the complexity of clinical practice, particularly in resource-constrained settings. Methods: We introduce a dataset, BUET Polyp Dataset (BPD), of colonoscopy images collected using Olympus 170 and Pen- tax i-Scan series endoscopes under routine clinical conditions. The dataset contains images with corresponding expert-annotated binary masks, reflecting diverse challenges such as motion blur, specular highlights, stool artifacts, blood, and low-light frames. Annotations were manually reviewed by clinical experts to ensure quality. To demonstrate baseline performance, we provide bench- mark results for classification using VGG16, ResNet50, and InceptionV3, and for segmentation using UNet variants with VGG16, ResNet34, and InceptionV4 backbones. Results: The dataset comprises 1,288 images with polyps from 164 patients with corresponding ground-truth masks and 1,657 polyp-free images from 31 patients. Benchmarking experiments achieved up to 90.8% accuracy for binary classification (VGG16) and a maximum Dice score of 0.64 with InceptionV4-UNet for segmentation. Performance was lower compared to curated datasets, reflecting the real-world difficulty of images with artifacts and variable quality.",
    "summary": "",
    "translation": "RRTS数据集：来自资源受限环境的基准结肠镜检查数据集，用于计算机辅助诊断研究",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于医学领域的结肠镜检查数据集，属于明确的医疗应用范畴，与推荐系统、搜索或广告领域完全无关。论文标题明确指向计算机辅助诊断和医疗影像分析，这属于被明确排除的医学和生物学应用领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06765v1": {
    "title": "Robust and High-Fidelity 3D Gaussian Splatting: Fusing Pose Priors and Geometry Constraints for Texture-Deficient Outdoor Scenes",
    "url": "https://www.alphaxiv.org/abs/2511.06765v1",
    "arxiv_id": "2511.06765v1",
    "authors": "Meijun Guo, Yongliang Shi, Caiyun Liu, Yixiao Feng, Ming Ma, Tinghai Yan, Weining Lu, Bin Liang",
    "categories": "cs.CV, cs.GR",
    "pub_date": "2025-11-10 06:45:08",
    "ori_summary": "3D Gaussian Splatting (3DGS) has emerged as a key rendering pipeline for digital asset creation due to its balance between efficiency and visual quality. To address the issues of unstable pose estimation and scene representation distortion caused by geometric texture inconsistency in large outdoor scenes with weak or repetitive textures, we approach the problem from two aspects: pose estimation and scene representation. For pose estimation, we leverage LiDAR-IMU Odometry to provide prior poses for cameras in large-scale environments. These prior pose constraints are incorporated into COLMAP's triangulation process, with pose optimization performed via bundle adjustment. Ensuring consistency between pixel data association and prior poses helps maintain both robustness and accuracy. For scene representation, we introduce normal vector constraints and effective rank regularization to enforce consistency in the direction and shape of Gaussian primitives. These constraints are jointly optimized with the existing photometric loss to enhance the map quality. We evaluate our approach using both public and self-collected datasets. In terms of pose optimization, our method requires only one-third of the time while maintaining accuracy and robustness across both datasets. In terms of scene representation, the results show that our method significantly outperforms conventional 3DGS pipelines. Notably, on self-collected datasets characterized by weak or repetitive textures, our approach demonstrates enhanced visualization capabilities and achieves superior overall performance. Codes and data will be publicly available at https://github.com/justinyeah/normal_shape.git.",
    "summary": "",
    "translation": "鲁棒且高保真的3D高斯泼溅：融合位姿先验与几何约束用于纹理缺失的户外场景",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D场景重建和计算机视觉中的高斯泼溅技术，主要解决户外场景的3D重建问题。这与推荐系统、搜索或广告的核心技术领域没有直接关联，也不涉及Transformer架构、LLM技术或异构数据建模。虽然3D重建技术可能有潜在的AR/VR应用，但论文本身没有展示与RecSys/Search/Ads领域的明确联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06764v1": {
    "title": "CAST-LUT: Tokenizer-Guided HSV Look-Up Tables for Purple Flare Removal",
    "url": "https://www.alphaxiv.org/abs/2511.06764v1",
    "arxiv_id": "2511.06764v1",
    "authors": "Pu Wang, Shuning Sun, Jialang Lu, Chen Wu, Zhihua Zhang, Youshan Zhang, Chenggang Shan, Dianjie Lu, Guijuan Zhang, Zhuoran Zheng",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 06:45:03",
    "ori_summary": "Purple flare, a diffuse chromatic aberration artifact commonly found around highlight areas, severely degrades the tone transition and color of the image. Existing traditional methods are based on hand-crafted features, which lack flexibility and rely entirely on fixed priors, while the scarcity of paired training data critically hampers deep learning. To address this issue, we propose a novel network built upon decoupled HSV Look-Up Tables (LUTs). The method aims to simplify color correction by adjusting the Hue (H), Saturation (S), and Value (V) components independently. This approach resolves the inherent color coupling problems in traditional methods. Our model adopts a two-stage architecture: First, a Chroma-Aware Spectral Tokenizer (CAST) converts the input image from RGB space to HSV space and independently encodes the Hue (H) and Value (V) channels into a set of semantic tokens describing the Purple flare status; second, the HSV-LUT module takes these tokens as input and dynamically generates independent correction curves (1D-LUTs) for the three channels H, S, and V. To effectively train and validate our model, we built the first large-scale purple flare dataset with diverse scenes. We also proposed new metrics and a loss function specifically designed for this task. Extensive experiments demonstrate that our model not only significantly outperforms existing methods in visual effects but also achieves state-of-the-art performance on all quantitative metrics.",
    "summary": "",
    "translation": "CAST-LUT：基于分词器引导的HSV查找表用于紫色耀斑去除",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉中的特定图像处理任务（紫色耀斑去除），使用HSV颜色空间和查找表技术。该主题与推荐系统、搜索或广告的核心领域进展没有明显关联，也不涉及LLM技术、Transformer架构进展，或异构数据的统一建模。这是一个纯粹的视觉处理应用，没有明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06754v1": {
    "title": "SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation",
    "url": "https://www.alphaxiv.org/abs/2511.06754v1",
    "arxiv_id": "2511.06754v1",
    "authors": "Taisei Hanyu, Nhat Chung, Huy Le, Toan Nguyen, Yuki Ikebe, Anthony Gunderman, Duy Nguyen Ho Minh, Khoa Vo, Tung Kieu, Kashu Yamazaki, Chase Rainwater, Anh Nguyen, Ngan Le",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-11-10 06:33:44",
    "ori_summary": "Inspired by how humans reason over discrete objects and their relationships, we explore whether compact object-centric and object-relation representations can form a foundation for multitask robotic manipulation. Most existing robotic multitask models rely on dense embeddings that entangle both object and background cues, raising concerns about both efficiency and interpretability. In contrast, we study object-relation-centric representations as a pathway to more structured, efficient, and explainable visuomotor control. Our contributions are two-fold. First, we introduce LIBERO+, a fine-grained benchmark dataset designed to enable and evaluate object-relation reasoning in robotic manipulation. Unlike prior datasets, LIBERO+ provides object-centric annotations that enrich demonstrations with box- and mask-level labels as well as instance-level temporal tracking, supporting compact and interpretable visuomotor representations. Second, we propose SlotVLA, a slot-attention-based framework that captures both objects and their relations for action decoding. It uses a slot-based visual tokenizer to maintain consistent temporal object representations, a relation-centric decoder to produce task-relevant embeddings, and an LLM-driven module that translates these embeddings into executable actions. Experiments on LIBERO+ demonstrate that object-centric slot and object-relation slot representations drastically reduce the number of required visual tokens, while providing competitive generalization. Together, LIBERO+ and SlotVLA provide a compact, interpretable, and effective foundation for advancing object-relation-centric robotic manipulation.",
    "summary": "",
    "translation": "SlotVLA：面向机器人操作中对象关系表示建模的研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人操作领域，涉及对象关系表示建模，这与推荐系统、搜索或广告的核心领域进展、LLM技术或Transformer架构没有直接关联。机器人操作属于物理交互领域，不属于文本/序列建模或用户行为预测的范畴，因此与当前关注点高度不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06752v1": {
    "title": "Med-SORA: Symptom to Organ Reasoning in Abdomen CT Images",
    "url": "https://www.alphaxiv.org/abs/2511.06752v1",
    "arxiv_id": "2511.06752v1",
    "authors": "You-Kyoung Na, Yeong-Jun Cho",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 06:30:51",
    "ori_summary": "Understanding symptom-image associations is crucial for clinical reasoning. However, existing medical multimodal models often rely on simple one-to-one hard labeling, oversimplifying clinical reality where symptoms relate to multiple organs. In addition, they mainly use single-slice 2D features without incorporating 3D information, limiting their ability to capture full anatomical context. In this study, we propose Med-SORA, a framework for symptom-to-organ reasoning in abdominal CT images. Med-SORA introduces RAG-based dataset construction, soft labeling with learnable organ anchors to capture one-to-many symptom-organ relationships, and a 2D-3D cross-attention architecture to fuse local and global image features. To our knowledge, this is the first work to address symptom-to-organ reasoning in medical multimodal learning. Experimental results show that Med-SORA outperforms existing medical multimodal models and enables accurate 3D clinical reasoning.",
    "summary": "",
    "translation": "Med-SORA：腹部CT图像中的症状到器官推理",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医学影像分析领域（腹部CT图像），属于明确的医疗应用范畴。根据用户指定的无关主题，医疗、生物等特定领域应用应被排除，且该论文没有显示出与推荐系统、搜索或广告的任何潜在关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06751v1": {
    "title": "Hierarchical Spatial-Frequency Aggregation for Spectral Deconvolution Imaging",
    "url": "https://www.alphaxiv.org/abs/2511.06751v1",
    "arxiv_id": "2511.06751v1",
    "authors": "Tao Lv, Daoming Zhou, Chenglong Huang, Chongde Zi, Linsen Chen, Xun Cao",
    "categories": "eess.IV, cs.AI, cs.CV",
    "pub_date": "2025-11-10 06:29:34",
    "ori_summary": "Computational spectral imaging (CSI) achieves real-time hyperspectral imaging through co-designed optics and algorithms, but typical CSI methods suffer from a bulky footprint and limited fidelity. Therefore, Spectral Deconvolution imaging (SDI) methods based on PSF engineering have been proposed to achieve high-fidelity compact CSI design recently. However, the composite convolution-integration operations of SDI render the normal-equation coefficient matrix scene-dependent, which hampers the efficient exploitation of imaging priors and poses challenges for accurate reconstruction. To tackle the inherent data-dependent operators in SDI, we introduce a Hierarchical Spatial-Spectral Aggregation Unfolding Framework (HSFAUF). By decomposing subproblems and projecting them into the frequency domain, HSFAUF transforms nonlinear processes into linear mappings, thereby enabling efficient solutions. Furthermore, to integrate spatial-spectral priors during iterative refinement, we propose a Spatial-Frequency Aggregation Transformer (SFAT), which explicitly aggregates information across spatial and frequency domains. By integrating SFAT into HSFAUF, we develop a Transformer-based deep unfolding method, \\textbf{H}ierarchical \\textbf{S}patial-\\textbf{F}requency \\textbf{A}ggregation \\textbf{U}nfolding \\textbf{T}ransformer (HSFAUT), to solve the inverse problem of SDI. Systematic simulated and real experiments show that HSFAUT surpasses SOTA methods with cheaper memory and computational costs, while exhibiting optimal performance on different SDI systems.",
    "summary": "",
    "translation": "用于光谱解卷积成像的层次化空间-频率聚合",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉领域的光谱成像和图像处理技术，与推荐系统、搜索或广告的核心领域进展没有直接关联。虽然标题中的'层次化聚合'和'解卷积'听起来像通用技术，但论文明确聚焦于'光谱成像'这一特定视觉应用，没有显示出在推荐系统、搜索或广告中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06749v1": {
    "title": "Semi-distributed Cross-modal Air-Ground Relative Localization",
    "url": "https://www.alphaxiv.org/abs/2511.06749v1",
    "arxiv_id": "2511.06749v1",
    "authors": "Weining Lu, Deer Bin, Lian Ma, Ming Ma, Zhihao Ma, Xiangyang Chen, Longfei Wang, Yixiao Feng, Zhouxian Jiang, Yongliang Shi, Bin Liang",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-11-10 06:28:31",
    "ori_summary": "Efficient, accurate, and flexible relative localization is crucial in air-ground collaborative tasks. However, current approaches for robot relative localization are primarily realized in the form of distributed multi-robot SLAM systems with the same sensor configuration, which are tightly coupled with the state estimation of all robots, limiting both flexibility and accuracy. To this end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to integrate multiple sensors, enabling a semi-distributed cross-modal air-ground relative localization framework. In this work, both the UGV and the Unmanned Aerial Vehicle (UAV) independently perform SLAM while extracting deep learning-based keypoints and global descriptors, which decouples the relative localization from the state estimation of all agents. The UGV employs a local Bundle Adjustment (BA) with LiDAR, camera, and an IMU to rapidly obtain accurate relative pose estimates. The BA process adopts sparse keypoint optimization and is divided into two stages: First, optimizing camera poses interpolated from LiDAR-Inertial Odometry (LIO), followed by estimating the relative camera poses between the UGV and UAV. Additionally, we implement an incremental loop closure detection algorithm using deep learning-based descriptors to maintain and retrieve keyframes efficiently. Experimental results demonstrate that our method achieves outstanding performance in both accuracy and efficiency. Unlike traditional multi-robot SLAM approaches that transmit images or point clouds, our method only transmits keypoint pixels and their descriptors, effectively constraining the communication bandwidth under 0.3 Mbps. Codes and data will be publicly available on https://github.com/Ascbpiac/cross-model-relative-localization.git.",
    "summary": "",
    "translation": "半分布式跨模态空地相对定位",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及跨模态定位技术，主要聚焦于空中与地面平台的相对定位问题，这属于机器人学或自动驾驶领域。虽然提到了跨模态概念，但该技术主要应用于物理空间定位和导航，与推荐系统、搜索或广告中的异构数据处理没有直接关联，也没有明显的Transformer架构或LLM技术应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06748v1": {
    "title": "Image Restoration via Primal Dual Hybrid Gradient and Flow Generative Model",
    "url": "https://www.alphaxiv.org/abs/2511.06748v1",
    "arxiv_id": "2511.06748v1",
    "authors": "Ji Li, Chao Wang",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 06:26:36",
    "ori_summary": "Regularized optimization has been a classical approach to solving imaging inverse problems, where the regularization term enforces desirable properties of the unknown image. Recently, the integration of flow matching generative models into image restoration has garnered significant attention, owing to their powerful prior modeling capabilities. In this work, we incorporate such generative priors into a Plug-and-Play (PnP) framework based on proximal splitting, where the proximal operator associated with the regularizer is replaced by a time-dependent denoiser derived from the generative model. While existing PnP methods have achieved notable success in inverse problems with smooth squared $\\ell_2$ data fidelity--typically associated with Gaussian noise--their applicability to more general data fidelity terms remains underexplored. To address this, we propose a general and efficient PnP algorithm inspired by the primal-dual hybrid gradient (PDHG) method. Our approach is computationally efficient, memory-friendly, and accommodates a wide range of fidelity terms. In particular, it supports both $\\ell_1$ and $\\ell_2$ norm-based losses, enabling robustness to non-Gaussian noise types such as Poisson and impulse noise. We validate our method on several image restoration tasks, including denoising, super-resolution, deblurring, and inpainting, and demonstrate that $\\ell_1$ and $\\ell_2$ fidelity terms outperform the conventional squared $\\ell_2$ loss in the presence of non-Gaussian noise.",
    "summary": "",
    "translation": "基于原始对偶混合梯度与流生成模型的图像恢复",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉领域的图像恢复技术，主要涉及生成模型和优化算法。虽然流生成模型是生成式AI的重要分支，但该工作主要针对纯粹的图像处理任务，没有明确展示在推荐系统、搜索或广告领域的应用潜力。图像恢复本身属于视觉计算范畴，与文本/序列建模为主的RecSys/Search/Ads核心问题关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06744v1": {
    "title": "PointCubeNet: 3D Part-level Reasoning with 3x3x3 Point Cloud Blocks",
    "url": "https://www.alphaxiv.org/abs/2511.06744v1",
    "arxiv_id": "2511.06744v1",
    "authors": "Da-Yeong Kim, Yeong-Jun Cho",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 06:16:07",
    "ori_summary": "In this paper, we propose PointCubeNet, a novel multi-modal 3D understanding framework that achieves part-level reasoning without requiring any part annotations. PointCubeNet comprises global and local branches. The proposed local branch, structured into 3x3x3 local blocks, enables part-level analysis of point cloud sub-regions with the corresponding local text labels. Leveraging the proposed pseudo-labeling method and local loss function, PointCubeNet is effectively trained in an unsupervised manner. The experimental results demonstrate that understanding 3D object parts enhances the understanding of the overall 3D object. In addition, this is the first attempt to perform unsupervised 3D part-level reasoning and achieves reliable and meaningful results.",
    "summary": "",
    "translation": "PointCubeNet：使用3x3x3点云块进行3D部件级推理",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D点云处理和计算机视觉中的部件级推理，属于纯粹的3D视觉研究。虽然标题提到'推理'，但这指的是3D几何理解而非推荐或搜索系统中的推理。该工作没有展示与推荐系统、搜索或广告的明显关联，完全落在无关主题范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06741v1": {
    "title": "Otter: Mitigating Background Distractions of Wide-Angle Few-Shot Action Recognition with Enhanced RWKV",
    "url": "https://www.alphaxiv.org/abs/2511.06741v1",
    "arxiv_id": "2511.06741v1",
    "authors": "Wenbo Huang, Jinghui Zhang, Zhenghao Chen, Guang Li, Lei Zhang, Yang Cao, Fang Dong, Takahiro Ogawa, Miki Haseyama",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 06:05:56",
    "ori_summary": "Wide-angle videos in few-shot action recognition (FSAR) effectively express actions within specific scenarios. However, without a global understanding of both subjects and background, recognizing actions in such samples remains challenging because of the background distractions. Receptance Weighted Key Value (RWKV), which learns interaction between various dimensions, shows promise for global modeling. While directly applying RWKV to wide-angle FSAR may fail to highlight subjects due to excessive background information. Additionally, temporal relation degraded by frames with similar backgrounds is difficult to reconstruct, further impacting performance. Therefore, we design the CompOund SegmenTation and Temporal REconstructing RWKV (Otter). Specifically, the Compound Segmentation Module~(CSM) is devised to segment and emphasize key patches in each frame, effectively highlighting subjects against background information. The Temporal Reconstruction Module (TRM) is incorporated into the temporal-enhanced prototype construction to enable bidirectional scanning, allowing better reconstruct temporal relation. Furthermore, a regular prototype is combined with the temporal-enhanced prototype to simultaneously enhance subject emphasis and temporal modeling, improving wide-angle FSAR performance. Extensive experiments on benchmarks such as SSv2, Kinetics, UCF101, and HMDB51 demonstrate that Otter achieves state-of-the-art performance. Extra evaluation on the VideoBadminton dataset further validates the superiority of Otter in wide-angle FSAR.",
    "summary": "",
    "translation": "Otter：利用增强型RWKV缓解广角少样本动作识别中的背景干扰",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的动作识别任务，特别是广角场景下的背景干扰问题。虽然使用了RWKV架构（一种RNN-like的Transformer变体），但其应用场景是纯粹的视觉动作识别，没有明确与推荐系统、搜索或广告相关的潜在应用。论文的核心问题（背景干扰缓解）是视觉特有的挑战，难以迁移到文本或序列建模领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06740v1": {
    "title": "SinSEMI: A One-Shot Image Generation Model and Data-Efficient Evaluation Framework for Semiconductor Inspection Equipment",
    "url": "https://www.alphaxiv.org/abs/2511.06740v1",
    "arxiv_id": "2511.06740v1",
    "authors": "ChunLiang Wu, Xiaochun Li",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 06:01:10",
    "ori_summary": "In the early stages of semiconductor equipment development, obtaining large quantities of raw optical images poses a significant challenge. This data scarcity hinder the advancement of AI-powered solutions in semiconductor manufacturing. To address this challenge, we introduce SinSEMI, a novel one-shot learning approach that generates diverse and highly realistic images from single optical image. SinSEMI employs a multi-scale flow-based model enhanced with LPIPS (Learned Perceptual Image Patch Similarity) energy guidance during sampling, ensuring both perceptual realism and output variety. We also introduce a comprehensive evaluation framework tailored for this application, which enables a thorough assessment using just two reference images. Through the evaluation against multiple one-shot generation techniques, we demonstrate SinSEMI's superior performance in visual quality, quantitative measures, and downstream tasks. Our experimental results demonstrate that SinSEMI-generated images achieve both high fidelity and meaningful diversity, making them suitable as training data for semiconductor AI applications.",
    "summary": "",
    "translation": "SinSEMI：一种用于半导体检测设备的单次图像生成模型与数据高效评估框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于半导体检测领域的图像生成和评估框架，属于高度专业化的工业应用。虽然涉及生成模型技术，但其应用场景（半导体检测）与推荐系统、搜索或广告领域完全无关，且不涉及任何推荐、搜索或广告相关的技术组件或应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06734v1": {
    "title": "Rethinking Rainy 3D Scene Reconstruction via Perspective Transforming and Brightness Tuning",
    "url": "https://www.alphaxiv.org/abs/2511.06734v1",
    "arxiv_id": "2511.06734v1",
    "authors": "Qianfeng Yang, Xiang Chen, Pengpeng Li, Qiyuan Guan, Guiyue Jin, Jiyu Jin",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 05:57:55",
    "ori_summary": "Rain degrades the visual quality of multi-view images, which are essential for 3D scene reconstruction, resulting in inaccurate and incomplete reconstruction results. Existing datasets often overlook two critical characteristics of real rainy 3D scenes: the viewpoint-dependent variation in the appearance of rain streaks caused by their projection onto 2D images, and the reduction in ambient brightness resulting from cloud coverage during rainfall. To improve data realism, we construct a new dataset named OmniRain3D that incorporates perspective heterogeneity and brightness dynamicity, enabling more faithful simulation of rain degradation in 3D scenes. Based on this dataset, we propose an end-to-end reconstruction framework named REVR-GSNet (Rain Elimination and Visibility Recovery for 3D Gaussian Splatting). Specifically, REVR-GSNet integrates recursive brightness enhancement, Gaussian primitive optimization, and GS-guided rain elimination into a unified architecture through joint alternating optimization, achieving high-fidelity reconstruction of clean 3D scenes from rain-degraded inputs. Extensive experiments show the effectiveness of our dataset and method. Our dataset and method provide a foundation for future research on multi-view image deraining and rainy 3D scene reconstruction.",
    "summary": "",
    "translation": "通过透视变换与亮度调节重新思考雨天三维场景重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的三维场景重建，特别是针对雨天条件的视觉处理技术。虽然涉及透视变换和亮度调节等图像处理技术，但论文内容纯粹属于视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联，也没有展示在异构数据处理方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06724v1": {
    "title": "Argus: Quality-Aware High-Throughput Text-to-Image Inference Serving System",
    "url": "https://www.alphaxiv.org/abs/2511.06724v1",
    "arxiv_id": "2511.06724v1",
    "authors": "Shubham Agarwal, Subrata Mitra, Saud Iqbal",
    "categories": "cs.CV, cs.DC",
    "pub_date": "2025-11-10 05:34:39",
    "ori_summary": "Text-to-image (T2I) models have gained significant popularity. Most of these are diffusion models with unique computational characteristics, distinct from both traditional small-scale ML models and large language models. They are highly compute-bound and use an iterative denoising process to generate images, leading to very high inference time. This creates significant challenges in designing a high-throughput system. We discovered that a large fraction of prompts can be served using faster, approximated models. However, the approximation setting must be carefully calibrated for each prompt to avoid quality degradation. Designing a high-throughput system that assigns each prompt to the appropriate model and compatible approximation setting remains a challenging problem. We present Argus, a high-throughput T2I inference system that selects the right level of approximation for each prompt to maintain quality while meeting throughput targets on a fixed-size cluster. Argus intelligently switches between different approximation strategies to satisfy both throughput and quality requirements. Overall, Argus achieves 10x fewer latency service-level objective (SLO) violations, 10% higher average quality, and 40% higher throughput compared to baselines on two real-world workload traces.",
    "summary": "",
    "translation": "Argus：质量感知的高吞吐量文生图推理服务系统",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于文本到图像生成的推理服务系统优化，属于AIGC和内容生成领域。虽然涉及推理系统效率，但其核心应用是图像生成而非推荐、搜索或广告中的排名任务。根据筛选标准，AIGC和内容生成属于无关主题，因此相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06721v1": {
    "title": "AvatarTex: High-Fidelity Facial Texture Reconstruction from Single-Image Stylized Avatars",
    "url": "https://www.alphaxiv.org/abs/2511.06721v1",
    "arxiv_id": "2511.06721v1",
    "authors": "Yuda Qiu, Zitong Xiao, Yiwei Zuo, Zisheng Ye, Weikai Chen, Xiaoguang Han",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 05:31:15",
    "ori_summary": "We present AvatarTex, a high-fidelity facial texture reconstruction framework capable of generating both stylized and photorealistic textures from a single image. Existing methods struggle with stylized avatars due to the lack of diverse multi-style datasets and challenges in maintaining geometric consistency in non-standard textures. To address these limitations, AvatarTex introduces a novel three-stage diffusion-to-GAN pipeline. Our key insight is that while diffusion models excel at generating diversified textures, they lack explicit UV constraints, whereas GANs provide a well-structured latent space that ensures style and topology consistency. By integrating these strengths, AvatarTex achieves high-quality topology-aligned texture synthesis with both artistic and geometric coherence. Specifically, our three-stage pipeline first completes missing texture regions via diffusion-based inpainting, refines style and structure consistency using GAN-based latent optimization, and enhances fine details through diffusion-based repainting. To address the need for a stylized texture dataset, we introduce TexHub, a high-resolution collection of 20,000 multi-style UV textures with precise UV-aligned layouts. By leveraging TexHub and our structured diffusion-to-GAN pipeline, AvatarTex establishes a new state-of-the-art in multi-style facial texture reconstruction. TexHub will be released upon publication to facilitate future research in this field.",
    "summary": "",
    "translation": "AvatarTex：从单图像风格化虚拟角色进行高保真面部纹理重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的3D面部纹理重建技术，属于纯粹的视觉和图形学领域。虽然涉及虚拟角色创建，但没有任何与推荐系统、搜索或广告相关的技术元素或潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06720v1": {
    "title": "Relative Energy Learning for LiDAR Out-of-Distribution Detection",
    "url": "https://www.alphaxiv.org/abs/2511.06720v1",
    "arxiv_id": "2511.06720v1",
    "authors": "Zizhao Li, Zhengkang Xiang, Jiayang Ao, Joseph West, Kourosh Khoshelham",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 05:29:18",
    "ori_summary": "Out-of-distribution (OOD) detection is a critical requirement for reliable autonomous driving, where safety depends on recognizing road obstacles and unexpected objects beyond the training distribution. Despite extensive research on OOD detection in 2D images, direct transfer to 3D LiDAR point clouds has been proven ineffective. Current LiDAR OOD methods struggle to distinguish rare anomalies from common classes, leading to high false-positive rates and overconfident errors in safety-critical settings. We propose Relative Energy Learning (REL), a simple yet effective framework for OOD detection in LiDAR point clouds. REL leverages the energy gap between positive (in-distribution) and negative logits as a relative scoring function, mitigating calibration issues in raw energy values and improving robustness across various scenes. To address the absence of OOD samples during training, we propose a lightweight data synthesis strategy called Point Raise, which perturbs existing point clouds to generate auxiliary anomalies without altering the inlier semantics. Evaluated on SemanticKITTI and the Spotting the Unexpected (STU) benchmark, REL consistently outperforms existing methods by a large margin. Our results highlight that modeling relative energy, combined with simple synthetic outliers, provides a principled and scalable solution for reliable OOD detection in open-world autonomous driving.",
    "summary": "",
    "translation": "用于激光雷达分布外检测的相对能量学习",
    "relevance_score": 1,
    "reasoning": "该论文专注于激光雷达数据的分布外检测，属于纯粹的计算机视觉/感知领域，与推荐系统、搜索或广告没有任何直接或间接关联。相对能量学习方法在激光雷达这种特定传感器模态中的应用，无法迁移到推荐系统、搜索或广告中的用户行为建模、内容理解或排序任务。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06717v1": {
    "title": "MRT: Learning Compact Representations with Mixed RWKV-Transformer for Extreme Image Compression",
    "url": "https://www.alphaxiv.org/abs/2511.06717v1",
    "arxiv_id": "2511.06717v1",
    "authors": "Han Liu, Hengyu Man, Xingtao Wang, Wenrui Li, Debin Zhao",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 05:23:00",
    "ori_summary": "Recent advances in extreme image compression have revealed that mapping pixel data into highly compact latent representations can significantly improve coding efficiency. However, most existing methods compress images into 2-D latent spaces via convolutional neural networks (CNNs) or Swin Transformers, which tend to retain substantial spatial redundancy, thereby limiting overall compression performance. In this paper, we propose a novel Mixed RWKV-Transformer (MRT) architecture that encodes images into more compact 1-D latent representations by synergistically integrating the complementary strengths of linear-attention-based RWKV and self-attention-based Transformer models. Specifically, MRT partitions each image into fixed-size windows, utilizing RWKV modules to capture global dependencies across windows and Transformer blocks to model local redundancies within each window. The hierarchical attention mechanism enables more efficient and compact representation learning in the 1-D domain. To further enhance compression efficiency, we introduce a dedicated RWKV Compression Model (RCM) tailored to the structure characteristics of the intermediate 1-D latent features in MRT. Extensive experiments on standard image compression benchmarks validate the effectiveness of our approach. The proposed MRT framework consistently achieves superior reconstruction quality at bitrates below 0.02 bits per pixel (bpp). Quantitative results based on the DISTS metric show that MRT significantly outperforms the state-of-the-art 2-D architecture GLC, achieving bitrate savings of 43.75%, 30.59% on the Kodak and CLIC2020 test datasets, respectively.",
    "summary": "",
    "translation": "MRT：基于混合RWKV-Transformer的极端图像压缩中的紧凑表示学习",
    "relevance_score": 2,
    "reasoning": "该论文主要关注图像压缩领域，虽然使用了Transformer架构变体，但其应用场景与搜索、推荐或广告系统没有直接关联。极端图像压缩技术可能在某些边缘场景有潜在应用，但缺乏明确的RecSys/Search/Ads应用路径，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06716v1": {
    "title": "MirrorMamba: Towards Scalable and Robust Mirror Detection in Videos",
    "url": "https://www.alphaxiv.org/abs/2511.06716v1",
    "arxiv_id": "2511.06716v1",
    "authors": "Rui Song, Jiaying Lin, Rynson W. H. Lau",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-10 05:18:14",
    "ori_summary": "Video mirror detection has received significant research attention, yet existing methods suffer from limited performance and robustness. These approaches often over-rely on single, unreliable dynamic features, and are typically built on CNNs with limited receptive fields or Transformers with quadratic computational complexity. To address these limitations, we propose a new effective and scalable video mirror detection method, called MirrorMamba. Our approach leverages multiple cues to adapt to diverse conditions, incorporating perceived depth, correspondence and optical. We also introduce an innovative Mamba-based Multidirection Correspondence Extractor, which benefits from the global receptive field and linear complexity of the emerging Mamba spatial state model to effectively capture correspondence properties. Additionally, we design a Mamba-based layer-wise boundary enforcement decoder to resolve the unclear boundary caused by the blurred depth map. Notably, this work marks the first successful application of the Mamba-based architecture in the field of mirror detection. Extensive experiments demonstrate that our method outperforms existing state-of-the-art approaches for video mirror detection on the benchmark datasets. Furthermore, on the most challenging and representative image-based mirror detection dataset, our approach achieves state-of-the-art performance, proving its robustness and generalizability.",
    "summary": "",
    "translation": "MirrorMamba：面向视频中可扩展且鲁棒的镜面检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频中的镜面检测，这属于纯粹的计算机视觉任务，与推荐系统、搜索或广告的核心技术领域没有直接关联。虽然标题中包含了'Mamba'架构，但镜面检测的应用场景（如视频分析、AR/VR）与RecSys/Search/Ads的排序、匹配、用户建模等核心问题缺乏明确的联系路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06709v1": {
    "title": "K-Stain: Keypoint-Driven Correspondence for H&E-to-IHC Virtual Staining",
    "url": "https://www.alphaxiv.org/abs/2511.06709v1",
    "arxiv_id": "2511.06709v1",
    "authors": "Sicheng Yang, Zhaohu Xing, Haipeng Zhou, Lei Zhu",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 05:04:59",
    "ori_summary": "Virtual staining offers a promising method for converting Hematoxylin and Eosin (H&E) images into Immunohistochemical (IHC) images, eliminating the need for costly chemical processes. However, existing methods often struggle to utilize spatial information effectively due to misalignment in tissue slices. To overcome this challenge, we leverage keypoints as robust indicators of spatial correspondence, enabling more precise alignment and integration of structural details in synthesized IHC images. We introduce K-Stain, a novel framework that employs keypoint-based spatial and semantic relationships to enhance synthesized IHC image fidelity. K-Stain comprises three main components: (1) a Hierarchical Spatial Keypoint Detector (HSKD) for identifying keypoints in stain images, (2) a Keypoint-aware Enhancement Generator (KEG) that integrates these keypoints during image generation, and (3) a Keypoint Guided Discriminator (KGD) that improves the discriminator's sensitivity to spatial details. Our approach leverages contextual information from adjacent slices, resulting in more accurate and visually consistent IHC images. Extensive experiments show that K-Stain outperforms state-of-the-art methods in quantitative metrics and visual quality.",
    "summary": "",
    "translation": "K-Stain：基于关键点驱动的H&E至IHC虚拟染色对应关系",
    "relevance_score": 1,
    "reasoning": "这篇论文涉及医学图像处理中的虚拟染色技术，具体处理H&E（苏木精-伊红）到IHC（免疫组织化学）的染色转换。这属于医学/生物领域的特定应用，与推荐系统、搜索、广告或LLM技术没有任何关联，完全超出了当前关注的技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06702v1": {
    "title": "SPAN: Spatial-Projection Alignment for Monocular 3D Object Detection",
    "url": "https://www.alphaxiv.org/abs/2511.06702v1",
    "arxiv_id": "2511.06702v1",
    "authors": "Yifan Wang, Yian Zhao, Fanqi Pu, Xiaochen Yang, Yang Tang, Xi Chen, Wenming Yang",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 04:48:48",
    "ori_summary": "Existing monocular 3D detectors typically tame the pronounced nonlinear regression of 3D bounding box through decoupled prediction paradigm, which employs multiple branches to estimate geometric center, depth, dimensions, and rotation angle separately. Although this decoupling strategy simplifies the learning process, it inherently ignores the geometric collaborative constraints between different attributes, resulting in the lack of geometric consistency prior, thereby leading to suboptimal performance. To address this issue, we propose novel Spatial-Projection Alignment (SPAN) with two pivotal components: (i). Spatial Point Alignment enforces an explicit global spatial constraint between the predicted and ground-truth 3D bounding boxes, thereby rectifying spatial drift caused by decoupled attribute regression. (ii). 3D-2D Projection Alignment ensures that the projected 3D box is aligned tightly within its corresponding 2D detection bounding box on the image plane, mitigating projection misalignment overlooked in previous works. To ensure training stability, we further introduce a Hierarchical Task Learning strategy that progressively incorporates spatial-projection alignment as 3D attribute predictions refine, preventing early stage error propagation across attributes. Extensive experiments demonstrate that the proposed method can be easily integrated into any established monocular 3D detector and delivers significant performance improvements.",
    "summary": "",
    "translation": "SPAN：用于单目3D目标检测的空间投影对齐",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉中的3D目标检测技术，属于纯粹的视觉领域研究。虽然3D检测在自动驾驶等场景有应用，但论文标题未显示与推荐系统、搜索或广告的直接关联，也没有涉及LLM技术或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.06687v1": {
    "title": "AnoStyler: Text-Driven Localized Anomaly Generation via Lightweight Style Transfer",
    "url": "https://www.alphaxiv.org/abs/2511.06687v1",
    "arxiv_id": "2511.06687v1",
    "authors": "Yulim So, Seokho Kang",
    "categories": "cs.CV",
    "pub_date": "2025-11-10 04:20:31",
    "ori_summary": "Anomaly generation has been widely explored to address the scarcity of anomaly images in real-world data. However, existing methods typically suffer from at least one of the following limitations, hindering their practical deployment: (1) lack of visual realism in generated anomalies; (2) dependence on large amounts of real images; and (3) use of memory-intensive, heavyweight model architectures. To overcome these limitations, we propose AnoStyler, a lightweight yet effective method that frames zero-shot anomaly generation as text-guided style transfer. Given a single normal image along with its category label and expected defect type, an anomaly mask indicating the localized anomaly regions and two-class text prompts representing the normal and anomaly states are generated using generalizable category-agnostic procedures. A lightweight U-Net model trained with CLIP-based loss functions is used to stylize the normal image into a visually realistic anomaly image, where anomalies are localized by the anomaly mask and semantically aligned with the text prompts. Extensive experiments on the MVTec-AD and VisA datasets show that AnoStyler outperforms existing anomaly generation methods in generating high-quality and diverse anomaly images. Furthermore, using these generated anomalies helps enhance anomaly detection performance.",
    "summary": "",
    "translation": "AnoStyler：通过轻量级风格迁移实现文本驱动的局部异常生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注异常生成和风格迁移技术，属于计算机视觉和图像处理领域。虽然风格迁移技术在某些推荐系统中可能用于内容增强，但论文标题明确指向异常生成这一特定应用，与搜索、推荐或广告系统的核心排名和匹配任务关联度较低。缺乏明确的Transformer架构改进或LLM技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  }
}