{
  "2510.13738v1": {
    "title": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based Sequential Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.13738v1",
    "arxiv_id": "2510.13738v1",
    "authors": "Jingyi Zhou, Cheng Chen, Kai Zuo, Manjie Xu, Zhendong Fu, Yibo Chen, Xu Tang, Yao Hu",
    "categories": "cs.IR",
    "pub_date": "2025-10-15 16:45:59",
    "ori_summary": "Large language models (LLMs) have recently demonstrated strong potential for sequential recommendation. However, current LLM-based approaches face critical limitations in modeling users' long-term and diverse interests. First, due to inference latency and feature fetching bandwidth constraints, existing methods typically truncate user behavior sequences to include only the most recent interactions, resulting in the loss of valuable long-range preference signals. Second, most current methods rely on next-item prediction with a single predicted embedding, overlooking the multifaceted nature of user interests and limiting recommendation diversity. To address these challenges, we propose HyMiRec, a hybrid multi-interest sequential recommendation framework, which leverages a lightweight recommender to extracts coarse interest embeddings from long user sequences and an LLM-based recommender to captures refined interest embeddings. To alleviate the overhead of fetching features, we introduce a residual codebook based on cosine similarity, enabling efficient compression and reuse of user history embeddings. To model the diverse preferences of users, we design a disentangled multi-interest learning module, which leverages multiple interest queries to learn disentangles multiple interest signals adaptively, allowing the model to capture different facets of user intent. Extensive experiments are conducted on both benchmark datasets and a collected industrial dataset, demonstrating our effectiveness over existing state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec brings consistent improvements in real-world recommendation systems.",
    "summary": "论文研究LLM在序列推荐中长序列建模和多兴趣建模的挑战，核心思想是采用轻量推荐器提取粗粒度兴趣嵌入、LLM推荐器捕获细粒度兴趣嵌入的混合架构，并设计残差码本实现高效特征压缩和多兴趣解耦学习模块。",
    "translation": "HyMiRec：一种基于大语言模型的序列推荐混合多兴趣学习框架",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及LLM在推荐系统中的应用（Direct LLM Applications），提出了混合多兴趣学习框架用于序列推荐。多兴趣建模是推荐系统的核心问题，该工作将LLM技术与传统推荐方法结合，具有明确的RecSys应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM在推荐系统中的两大核心挑战——长序列建模和多兴趣建模，提出了混合架构和残差码本等创新方法，与我的所有关注领域高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13590v1": {
    "title": "RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for Evolving Knowledge",
    "url": "https://www.alphaxiv.org/abs/2510.13590v1",
    "arxiv_id": "2510.13590v1",
    "authors": "Jiale Han, Austin Cheung, Yubai Wei, Zheng Yu, Xusheng Wang, Bing Zhu, Yi Yang",
    "categories": "cs.IR",
    "pub_date": "2025-10-15 14:21:08",
    "ori_summary": "Knowledge is inherently time-sensitive and continuously evolves over time. Although current Retrieval-Augmented Generation (RAG) systems enrich LLMs with external knowledge, they largely ignore this temporal nature. This raises two challenges for RAG. First, current RAG methods lack effective time-aware representations. Same facts of different time are difficult to distinguish with vector embeddings or conventional knowledge graphs. Second, most RAG evaluations assume a static corpus, leaving a blind spot regarding update costs and retrieval stability as knowledge evolves. To make RAG time-aware, we propose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level temporal graph consisting of a temporal knowledge graph with timestamped relations and a hierarchical time graph. Multi-granularity temporal summaries are generated for each time node to capture both key events and broader trends at that time. The design supports incremental updates by extracting new temporal facts from the incoming corpus and merging them into the existing graph. The temporal graph explicitly represents identical facts at different times as distinct edges to avoid ambiguity, and the time hierarchy graph allows only generating reports for new leaf time nodes and their ancestors, ensuring effective and efficient updates. During inference, TG-RAG dynamically retrieves a subgraph within the temporal and semantic scope of the query, enabling precise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive question-answering dataset featuring both specific and abstract queries, along with a comprehensive evaluation protocol designed to assess incremental update capabilities of RAG systems. Extensive experiments show that TG-RAG significantly outperforms existing baselines, demonstrating the effectiveness of our method in handling temporal knowledge and incremental updates.",
    "summary": "论文研究RAG系统无法处理时间敏感知识的核心问题，核心方法是构建包含时序知识图和层次时间图的双层次时序图结构，通过多粒度时间摘要和时间层次设计实现知识的精确表示和高效增量更新。",
    "translation": "RAG与时间图相遇：面向演化知识的时间敏感建模与检索",
    "relevance_score": 8,
    "reasoning": "该论文结合RAG（检索增强生成）与时间图建模，直接适用于搜索和推荐系统中处理动态演化的知识。时间敏感建模能够增强推荐系统对用户兴趣变化和内容时效性的理解，而RAG技术可以提升搜索结果的准确性和时效性，这对于处理新闻、商品、社交内容等动态数据的推荐和搜索场景具有重要价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对RAG系统的时间感知问题，提出双层次时序图建模方法，对搜索和推荐系统中的动态知识更新具有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13371v1": {
    "title": "MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.13371v1",
    "arxiv_id": "2510.13371v1",
    "authors": "Jiin Park, Misuk Kim",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-15 10:03:29",
    "ori_summary": "Recent attempts to integrate large language models (LLMs) into recommender systems have gained momentum, but most remain limited to simple text generation or static prompt-based inference, failing to capture the complexity of user preferences and real-world interactions. This study proposes the Multi-Aspect Driven LLM Agent MADRec, an autonomous LLM-based recommender that constructs user and item profiles by unsupervised extraction of multi-aspect information from reviews and performs direct recommendation, sequential recommendation, and explanation generation. MADRec generates structured profiles via aspect-category-based summarization and applies Re-Ranking to construct high-density inputs. When the ground-truth item is missing from the output, the Self-Feedback mechanism dynamically adjusts the inference criteria. Experiments across multiple domains show that MADRec outperforms traditional and LLM-based baselines in both precision and explainability, with human evaluation further confirming the persuasiveness of the generated explanations.",
    "summary": "论文研究如何构建能够捕捉用户偏好复杂性的LLM推荐系统，核心方法是设计多方面驱动的自主代理，通过无监督提取评论中的多方面信息构建用户物品画像，并采用重排序和自反馈机制实现自适应推荐。",
    "translation": "MADREC：面向可解释与自适应推荐的多方面驱动大语言模型智能体",
    "relevance_score": 9,
    "reasoning": "该论文直接应用LLM技术构建推荐系统智能体，属于'Direct LLM Applications'范畴。论文标题明确涉及可解释推荐和自适应推荐，这些都是推荐系统的核心挑战，MADREC通过多方面驱动的方法展示了LLM在推荐系统中的直接应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用LLM技术构建推荐系统代理，实现了多任务推荐和可解释性生成，完全契合直接LLM应用和推荐系统核心领域。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13359v1": {
    "title": "Improving Visual Recommendation on E-commerce Platforms Using Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13359v1",
    "arxiv_id": "2510.13359v1",
    "authors": "Yuki Yada, Sho Akiyama, Ryo Watanabe, Yuta Ueno, Yusuke Shido, Andre Rusli",
    "categories": "cs.IR, cs.CV, cs.LG",
    "pub_date": "2025-10-15 09:46:27",
    "ori_summary": "On large-scale e-commerce platforms with tens of millions of active monthly users, recommending visually similar products is essential for enabling users to efficiently discover items that align with their preferences. This study presents the application of a vision-language model (VLM) -- which has demonstrated strong performance in image recognition and image-text retrieval tasks -- to product recommendations on Mercari, a major consumer-to-consumer marketplace used by more than 20 million monthly users in Japan. Specifically, we fine-tuned SigLIP, a VLM employing a sigmoid-based contrastive loss, using one million product image-title pairs from Mercari collected over a three-month period, and developed an image encoder for generating item embeddings used in the recommendation system. Our evaluation comprised an offline analysis of historical interaction logs and an online A/B test in a production environment. In offline analysis, the model achieved a 9.1% improvement in nDCG@5 compared with the baseline. In the online A/B test, the click-through rate improved by 50% whereas the conversion rate improved by 14% compared with the existing model. These results demonstrate the effectiveness of VLM-based encoders for e-commerce product recommendations and provide practical insights into the development of visual similarity-based recommendation systems.",
    "summary": "论文研究电商平台中基于视觉相似性的产品推荐问题，核心方法是微调SigLIP视觉语言模型生成商品嵌入，用于构建视觉推荐系统。",
    "translation": "利用视觉语言模型改进电商平台的视觉推荐",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及视觉推荐系统，属于推荐系统领域的核心进展。视觉语言模型技术作为使能技术，在电商推荐中可用于理解商品图像与文本描述的关联，提升多模态推荐效果，类似于VLM处理异构数据的思路。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用视觉语言模型解决电商平台的视觉推荐问题，完美契合VLM类比异构数据和直接LLM应用两个重点领域。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13312v1": {
    "title": "ChatR1: Reinforcement Learning for Conversational Reasoning and Retrieval Augmented Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.13312v1",
    "arxiv_id": "2510.13312v1",
    "authors": "Simon Lupart, Mohammad Aliannejadi, Evangelos Kanoulas",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2025-10-15 09:00:20",
    "ori_summary": "We present ChatR1, a reasoning framework based on reinforcement learning (RL) for conversational question answering (CQA). Reasoning plays an important role in CQA, where user intent evolves across dialogue turns, and utterances are often underspecified, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Unlike static `rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through RL. To address the challenge of sparse and delayed rewards in RL, we propose an intent-aware reward that provides turn-level feedback by aligning retrieval and reasoning with evolving user goals. Our proposed ChatR1 demonstrates strong performance on both 3B and 7B model backbones, outperforming competitive models on five CQA datasets, measured by different metrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA datasets to cover topic shifts, evolving intents, mixed-initiative dialogues, and multi-document grounding, testing ChatR1's performance from various aspects. Ablation studies confirm the effectiveness of the intent-aware reward. Our analyses further reveal diverse reasoning trajectories and effective use of the search tool. ChatR1 also generalizes robustly across domains, demonstrating that RL-based reasoning enables more flexible and context-sensitive behavior than static CQA pipelines.",
    "summary": "",
    "translation": "ChatR1：用于对话推理和检索增强问答的强化学习",
    "relevance_score": 3,
    "reasoning": "虽然论文涉及检索增强问答，这与搜索系统有一定关联，但核心焦点是强化学习和对话推理，这超出了当前关注范围。强化学习应用在对话系统中，与推荐系统、搜索或广告的排名优化没有直接关联，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13229v1": {
    "title": "Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning for Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.13229v1",
    "arxiv_id": "2510.13229v1",
    "authors": "Yi Zhang, Lili Xie, Ruihong Qiu, Jiajun Liu, Sen Wang",
    "categories": "cs.IR",
    "pub_date": "2025-10-15 07:28:29",
    "ori_summary": "Recommender systems (RecSys) have become critical tools for enhancing user engagement by delivering personalized content across diverse digital platforms. Recent advancements in large language models (LLMs) demonstrate significant potential for improving RecSys, primarily due to their exceptional generalization capabilities and sophisticated contextual understanding, which facilitate the generation of flexible and interpretable recommendations. However, the direct deployment of LLMs as primary recommendation policies presents notable challenges, including persistent latency issues stemming from frequent API calls and inherent model limitations such as hallucinations and biases. To address these issues, this paper proposes a novel offline reinforcement learning (RL) framework that leverages imitation learning from LLM-generated trajectories. Specifically, inverse reinforcement learning is employed to extract robust reward models from LLM demonstrations. This approach negates the need for LLM fine-tuning, thereby substantially reducing computational overhead. Simultaneously, the RL policy is guided by the cumulative rewards derived from these demonstrations, effectively transferring the semantic insights captured by the LLM. Comprehensive experiments conducted on two benchmark datasets validate the effectiveness of the proposed method, demonstrating superior performance when compared against state-of-the-art RL-based and in-context learning baselines. The code can be found at https://github.com/ArronDZhang/IL-Rec.",
    "summary": "论文研究如何克服LLM直接作为推荐策略的延迟和幻觉问题，核心思想是利用模仿学习从LLM生成的轨迹中提取奖励模型，通过离线强化学习框架将LLM的语义理解能力转移给更高效的推荐策略。",
    "translation": "超越静态大语言模型策略：用于推荐的模仿增强强化学习",
    "relevance_score": 9,
    "reasoning": "该论文直接结合了强化学习与LLM技术应用于推荐系统，属于'直接LLM应用'范畴。模仿增强方法可以提升推荐策略的适应性和个性化，通过结合专家演示与强化学习优化，能够解决推荐系统中的动态用户偏好和长期价值优化问题。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM在推荐系统中的应用瓶颈，提出结合模仿学习和强化学习的创新框架，完美契合直接LLM应用和核心推荐系统进展两个重点领域。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13217v1": {
    "title": "LLM-guided Hierarchical Retrieval",
    "url": "https://www.alphaxiv.org/abs/2510.13217v1",
    "arxiv_id": "2510.13217v1",
    "authors": "Nilesh Gupta, Wei-Cheng Chang, Ngot Bui, Cho-Jui Hsieh, Inderjit S. Dhillon",
    "categories": "cs.IR, cs.LG",
    "pub_date": "2025-10-15 07:05:17",
    "ori_summary": "Modern IR systems are increasingly tasked with answering complex, multi-faceted queries that require deep reasoning rather than simple keyword or semantic matching. While LLM-based IR has shown great promise, the prevailing retrieve-then-rerank paradigm inherits the limitations of embedding-based retrieval; parametric generative approaches are difficult to update with new information; and long-context methods that place the entire corpus in context are computationally infeasible for large document collections. To address these challenges, we introduce LATTICE, a hierarchical retrieval framework that enables an LLM to reason over and navigate large corpora with logarithmic search complexity by imposing a semantic tree structure on the corpus. Our approach consists of two stages: (1) an offline phase that organizes the corpus into a semantic hierarchy via either a bottom-up agglomerative strategy or a top-down divisive strategy using multi-level summaries and (2) an online traversal phase where a search LLM navigates this tree. A central challenge in such LLM-guided search is that the model's relevance judgments are noisy, context-dependent, and unaware of the hierarchy, making cross-branch and cross-level comparisons difficult. To overcome this, we propose a traversal algorithm that estimates calibrated latent relevance scores from local LLM outputs and aggregates them into a global path relevance metric. Our training-free framework achieves state-of-the-art zero-shot performance on the reasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in Recall@100 and 5% in nDCG@10 over the next best zero-shot baseline. Furthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains comparable results on BRIGHT subsets that use a static corpus for evaluation.",
    "summary": "研究复杂查询检索问题，核心思想是通过构建语义层次树结构，让LLM以对数复杂度在大规模语料库中进行层次化导航检索。",
    "translation": "大语言模型引导的层次化检索",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及LLM在检索系统中的应用，属于'Direct LLM Applications'范畴。层次化检索是搜索和推荐系统中的核心架构模式，LLM的引导作用可以显著提升检索效率和准确性，在搜索、推荐和广告排名中具有直接应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对搜索系统中的核心检索问题，提出LLM引导的层次检索框架，属于搜索领域的核心算法创新，与LLM在搜索中的应用高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13193v1": {
    "title": "ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient RAG",
    "url": "https://www.alphaxiv.org/abs/2510.13193v1",
    "arxiv_id": "2510.13193v1",
    "authors": "Yikuan Hu, Jifeng Zhu, Lanrui Tang, Chen Huang",
    "categories": "cs.IR",
    "pub_date": "2025-10-15 06:31:29",
    "ori_summary": "Knowledge graphs (KGs), with their structured representation capabilities, offer promising avenue for enhancing Retrieval Augmented Generation (RAG) systems, leading to the development of KG-RAG systems. Nevertheless, existing methods often struggle to achieve effective synergy between system effectiveness and cost efficiency, leading to neither unsatisfying performance nor excessive LLM prompt tokens and inference time. To this end, this paper proposes REMINDRAG, which employs an LLM-guided graph traversal featuring node exploration, node exploitation, and, most notably, memory replay, to improve both system effectiveness and cost efficiency. Specifically, REMINDRAG memorizes traversal experience within KG edge embeddings, mirroring the way LLMs \"memorize\" world knowledge within their parameters, but in a train-free manner. We theoretically and experimentally confirm the effectiveness of REMINDRAG, demonstrating its superiority over existing baselines across various benchmark datasets and LLM backbones. Our code is available at https://github.com/kilgrims/ReMindRAG.",
    "summary": "论文研究如何提升知识图谱增强检索生成系统的效率与成本平衡问题，核心思想是采用LLM引导的图谱遍历策略，通过节点探索、利用和记忆回放机制，在无需训练的情况下将遍历经验编码到边嵌入中。",
    "translation": "ReMindRAG：基于低成本大语言模型引导的知识图谱遍历实现高效检索增强生成",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM引导的知识图谱遍历技术，属于直接LLM应用范畴，可显著提升检索增强生成在推荐和搜索系统中的效率。通过低成本LLM优化知识图谱查询，能够为个性化推荐和语义搜索提供更精准的知识检索能力，直接应用于搜索和推荐系统的知识增强场景。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对检索增强生成系统的效率问题，提出LLM引导的知识图谱遍历方法，与搜索和推荐系统的核心优化目标高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13095v1": {
    "title": "Retrieval-in-the-Chain: Bootstrapping Large Language Models for Generative Retrieval",
    "url": "https://www.alphaxiv.org/abs/2510.13095v1",
    "arxiv_id": "2510.13095v1",
    "authors": "Yingchen zhang, Ruqing zhang, Jiafeng Guo, Wenjun Peng, Sen Li, Fuyu Lv",
    "categories": "cs.IR",
    "pub_date": "2025-10-15 02:29:10",
    "ori_summary": "Generative retrieval (GR) is an emerging paradigm that leverages large language models (LLMs) to autoregressively generate document identifiers (docids) relevant to a given query. Prior works have focused on leveraging the generative capabilities of LLMs to improve GR, while overlooking that their reasoning capabilities could likewise help. This raises a key question: Can explicit reasoning benefit GR? To investigate, we first conduct a preliminary study where an LLM is prompted to generate free-form chain-of-thought (CoT) reasoning before performing constrained docid decoding. Although this method outperforms standard GR, the generated reasoning tends to be verbose and poorly aligned with the docid space. These limitations motivate the development of a reasoning mechanism better tailored to GR. Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented framework for GR that converts free-form CoT reasoning into a compact, structured format, and iteratively refines the reasoning during the retrieval process. R4R augments an existing GR method by leveraging a reasoning-capable LLM that has been instruction-tuned for GR. At inference time, R4R first uses the LLM to generate an initial structured reasoning; then the same LLM alternates between (i) constrained decoding with the chosen GR method to produce candidate docids and (ii) updating the reasoning based on retrieval results to improve the next round. R4R does not require additional models or training, and instead a single LLM serves as both the reasoning generator and the retriever. Extensive experiments on Natural Questions, MS MARCO, and a real-world item-search benchmark validate the effectiveness of R4R.",
    "summary": "论文研究如何提升生成式检索的性能，核心思想是将自由形式的思维链推理转化为结构化格式，并在检索过程中迭代优化推理与文档标识符生成。",
    "translation": "链中检索：为生成式检索引导大型语言模型",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及生成式检索，这是搜索和推荐系统的核心进展。通过引导LLMs进行检索，该方法在搜索相关性建模和内容理解方面具有直接应用潜力，可显著提升搜索和推荐系统的检索质量与效率。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对生成式检索的核心问题，将LLM的推理能力与检索过程相结合，完全符合直接LLM应用和核心领域进展的关注点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13804v1": {
    "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
    "url": "https://www.alphaxiv.org/abs/2510.13804v1",
    "arxiv_id": "2510.13804v1",
    "authors": "Xinchen Zhang, Xiaoying Zhang, Youbin Wu, Yanbin Cao, Renrui Zhang, Ruihang Chu, Ling Yang, Yujiu Yang",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-10-15 17:59:24",
    "ori_summary": "We introduce Generative Universal Verifier, a novel concept and plugin designed for next-generation multimodal reasoning in vision-language models and unified multimodal models, providing the fundamental capability of reflection and refinement on visual outcomes during the reasoning and generation process. This work makes three main contributions: (1) We build ViVerBench, a comprehensive benchmark spanning 16 categories of critical tasks for evaluating visual outcomes in multimodal reasoning. Results show that existing VLMs consistently underperform across these tasks, underscoring a substantial gap from human-level capability in reliable visual verification. (2) We design two automated pipelines to construct large-scale visual verification data and train OmniVerifier-7B, the first omni-capable generative verifier trained for universal visual verification and achieves notable gains on ViVerBench(+8.3). Through training, we identify three atomic capabilities in visual verification and demonstrate how they generalize and interact synergistically. (3) We propose OmniVerifier-TTS, a sequential test-time scaling paradigm that leverages the universal verifier to bridge image generation and editing within unified models, enhancing the upper bound of generative ability through iterative fine-grained optimization. Beyond generation, we extend universal verifier to broader world-modeling interleaved reasoning scenarios. Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7), and GenEval++(+4.3), outperforming existing parallel test-time scaling methods, such as Best-of-N. By endowing multimodal reasoning with reliable visual verification, OmniVerifier advances both reliable reflection during generation and scalable test-time refinement, marking a step toward more trustworthy and controllable next-generation reasoning systems.",
    "summary": "",
    "translation": "生成式通用验证器作为多模态元推理器",
    "relevance_score": 3,
    "reasoning": "该论文涉及多模态和元推理技术，可能属于'VLM Analogy for Heterogeneous Data'范畴，可将异构数据视为不同模态进行统一建模。然而标题过于宽泛，未明确说明具体应用场景，对RecSys/Search/Ads的直接相关性较弱，潜在应用不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13799v1": {
    "title": "BRIEF-Pro: Universal Context Compression with Short-to-Long Synthesis for Fast and Accurate Multi-Hop Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.13799v1",
    "arxiv_id": "2510.13799v1",
    "authors": "Jia-Chen Gu, Junyi Zhang, Di Wu, Yuankai Li, Kai-Wei Chang, Nanyun Peng",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 17:57:45",
    "ori_summary": "As retrieval-augmented generation (RAG) tackles complex tasks, increasingly expanded contexts offer richer information, but at the cost of higher latency and increased cognitive load on the model. To mitigate this bottleneck, especially for intricate multi-hop questions, we introduce BRIEF-Pro. It is a universal, lightweight compressor that distills relevant evidence for a given query from retrieved documents into a concise summary for seamless integration into in-context RAG. Using seed data consisting of relatively short contexts (fewer than 1k words), BRIEF-Pro is trained to perform abstractive compression of extended contexts exceeding 10k words across a wide range of scenarios. Furthermore, BRIEF-Pro offers flexible user control over summary length by allowing users to specify the desired number of sentences. Experiments on four open-domain multi-hop question-answering datasets show that BRIEF-Pro generates more concise and relevant summaries, enhancing performance across small, large, and proprietary language models. With the 70B reader model, 32x compression by BRIEF-Pro improves QA performance by 4.67% on average over LongLLMLingua's 9x, while requiring only 23% of its computational overhead.",
    "summary": "该论文研究检索增强生成中长上下文导致的延迟和认知负荷问题，核心思想是训练轻量级压缩器从短上下文数据学习对超长文档进行抽象压缩，实现通用上下文压缩和用户可控摘要长度。",
    "translation": "BRIEF-Pro：通过短到长合成实现通用上下文压缩，用于快速准确的多跳推理",
    "relevance_score": 8,
    "reasoning": "该论文提出的通用上下文压缩技术属于核心LLM技术进展，能够显著提升推理效率。在搜索和推荐系统中，多跳推理对于理解复杂用户查询和上下文关系至关重要，这种压缩技术可以加速大规模检索和排序过程，同时保持准确性。短到长合成方法特别适用于处理用户历史行为和上下文特征的复杂交互。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出的上下文压缩技术直接解决检索增强生成中的延迟和认知负荷问题，对搜索和推荐系统中的长文档处理具有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13797v1": {
    "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons",
    "url": "https://www.alphaxiv.org/abs/2510.13797v1",
    "arxiv_id": "2510.13797v1",
    "authors": "Giovanni Monea, Yair Feldman, Shankar Padmanabhan, Kianté Brantley, Yoav Artzi",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 17:57:21",
    "ori_summary": "The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques.",
    "summary": "该论文研究Transformer模型长上下文推理中的内存效率问题，核心思想是通过学习特殊压缩标记周期性压缩生成过程中的KV缓存，利用蒸馏和强化学习框架训练模型实现智能缓存管理。",
    "translation": "面包屑推理：基于压缩信标的内存高效推理方法",
    "relevance_score": 8,
    "reasoning": "该论文聚焦于内存高效的推理技术，这对于在资源受限环境中部署大型语言模型至关重要。压缩信标技术可以显著降低推理内存需求，直接应用于搜索和推荐系统中的实时LLM推理场景，提高系统响应速度和可扩展性。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出KV缓存压缩方法，直接针对Transformer架构的效率瓶颈，对大规模推荐和搜索系统的推理优化具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13796v1": {
    "title": "The Mechanistic Emergence of Symbol Grounding in Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13796v1",
    "arxiv_id": "2510.13796v1",
    "authors": "Shuyu Wu, Ziqiao Ma, Xiaoxi Luo, Yidong Huang, Josue Torres-Fonseca, Freda Shi, Joyce Chai",
    "categories": "cs.CL, cs.CV",
    "pub_date": "2025-10-15 17:56:15",
    "ori_summary": "Symbol grounding (Harnad, 1990) describes how symbols such as words acquire their meanings by connecting to real-world sensorimotor experiences. Recent work has shown preliminary evidence that grounding may emerge in (vision-)language models trained at scale without using explicit grounding objectives. Yet, the specific loci of this emergence and the mechanisms that drive it remain largely unexplored. To address this problem, we introduce a controlled evaluation framework that systematically traces how symbol grounding arises within the internal computations through mechanistic and causal analysis. Our findings show that grounding concentrates in middle-layer computations and is implemented through the aggregate mechanism, where attention heads aggregate the environmental ground to support the prediction of linguistic forms. This phenomenon replicates in multimodal dialogue and across architectures (Transformers and state-space models), but not in unidirectional LSTMs. Our results provide behavioral and mechanistic evidence that symbol grounding can emerge in language models, with practical implications for predicting and potentially controlling the reliability of generation.",
    "summary": "",
    "translation": "语言模型中符号接地的机制性涌现",
    "relevance_score": 3,
    "reasoning": "该论文探讨语言模型如何建立符号与现实世界概念的联系，这属于核心LLM技术进展。虽然符号接地是LLM理解能力的基础，但其在推荐系统、搜索或广告中的具体应用路径尚不明确，主要侧重于语言理解机制本身而非直接的应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13750v1": {
    "title": "Confidence-Based Response Abstinence: Improving LLM Trustworthiness via Activation-Based Uncertainty Estimation",
    "url": "https://www.alphaxiv.org/abs/2510.13750v1",
    "arxiv_id": "2510.13750v1",
    "authors": "Zhiqi Huang, Vivek Datla, Chenyang Zhu, Alfy Samuel, Daben Liu, Anoop Kumar, Ritesh Soni",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 16:55:56",
    "ori_summary": "We propose a method for confidence estimation in retrieval-augmented generation (RAG) systems that aligns closely with the correctness of large language model (LLM) outputs. Confidence estimation is especially critical in high-stakes domains such as finance and healthcare, where the cost of an incorrect answer outweighs that of not answering the question. Our approach extends prior uncertainty quantification methods by leveraging raw feed-forward network (FFN) activations as auto-regressive signals, avoiding the information loss inherent in token logits and probabilities after projection and softmax normalization. We model confidence prediction as a sequence classification task, and regularize training with a Huber loss term to improve robustness against noisy supervision. Applied in a real-world financial industry customer-support setting with complex knowledge bases, our method outperforms strong baselines and maintains high accuracy under strict latency constraints. Experiments on Llama 3.1 8B model show that using activations from only the 16th layer preserves accuracy while reducing response latency. Our results demonstrate that activation-based confidence modeling offers a scalable, architecture-aware path toward trustworthy RAG deployment.",
    "summary": "",
    "translation": "基于置信度的响应弃权：通过基于激活的不确定性估计提升大语言模型可信度",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM可信度和不确定性估计，这属于纯粹的LLM评估和可靠性问题，而非核心推荐系统、搜索或广告领域的进展。虽然不确定性估计在理论上可以应用于可信推荐，但论文标题明确聚焦于LLM信任worthiness，属于评估基准范畴，与我的技术焦点相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13749v1": {
    "title": "Assessing Web Search Credibility and Response Groundedness in Chat Assistants",
    "url": "https://www.alphaxiv.org/abs/2510.13749v1",
    "arxiv_id": "2510.13749v1",
    "authors": "Ivan Vykopal, Matúš Pikuliak, Simon Ostermann, Marián Šimko",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 16:55:47",
    "ori_summary": "Chat assistants increasingly integrate web search functionality, enabling them to retrieve and cite external sources. While this promises more reliable answers, it also raises the risk of amplifying misinformation from low-credibility sources. In this paper, we introduce a novel methodology for evaluating assistants' web search behavior, focusing on source credibility and the groundedness of responses with respect to cited sources. Using 100 claims across five misinformation-prone topics, we assess GPT-4o, GPT-5, Perplexity, and Qwen Chat. Our findings reveal differences between the assistants, with Perplexity achieving the highest source credibility, whereas GPT-4o exhibits elevated citation of non-credibility sources on sensitive topics. This work provides the first systematic comparison of commonly used chat assistants for fact-checking behavior, offering a foundation for evaluating AI systems in high-stakes information environments.",
    "summary": "",
    "translation": "评估聊天助手中的网络搜索可信度与响应接地性",
    "relevance_score": 2,
    "reasoning": "该论文主要关注聊天助手中的可信度评估和响应接地性，这属于纯粹的NLP评估基准范畴，与我的核心关注点（推荐系统、搜索排名、广告技术中的核心进展或使能技术）无关。虽然涉及搜索，但焦点是可信度评估而非搜索算法改进或LLM在搜索中的直接应用，因此相关性很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13744v1": {
    "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math",
    "url": "https://www.alphaxiv.org/abs/2510.13744v1",
    "arxiv_id": "2510.13744v1",
    "authors": "Shrey Pandit, Austin Xu, Xuan-Phi Nguyen, Yifei Ming, Caiming Xiong, Shafiq Joty",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-15 16:50:54",
    "ori_summary": "Large language model (LLM)-based reasoning systems have recently achieved gold medal-level performance in the IMO 2025 competition, writing mathematical proofs where, to receive full credit, each step must be not only correct but also sufficiently supported. To train LLM-based reasoners in such challenging, open-ended settings, strong verifiers capable of catching step-level mistakes are necessary prerequisites. We introduce Hard2Verify, a human-annotated, step-level verification benchmark produced with over 500 hours of human labor. Hard2Verify is designed to rigorously assess step-level verifiers at the frontier: Verifiers must provide step-level annotations or identify the first error in responses generated by frontier LLMs for very recent, challenging, and open-ended math questions. We evaluate 29 generative critics and process reward models, demonstrating that, beyond a few standouts, open-source verifiers lag closed source models. We subsequently analyze what drives poor performance in step-level verification, the impacts of scaling verifier compute, as well as fundamental questions such as self-verification and verification-generation dynamics.",
    "summary": "",
    "translation": "Hard2Verify：面向开放式前沿数学问题的步骤级验证基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于数学问题验证基准，属于纯粹的NLP评估基准范畴，与推荐系统、搜索或广告的核心技术进展无关。论文内容涉及数学推理验证，没有展示在RecSys/Search/Ads领域的潜在应用价值，完全落在不相关主题范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13734v1": {
    "title": "GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI Clinicians",
    "url": "https://www.alphaxiv.org/abs/2510.13734v1",
    "arxiv_id": "2510.13734v1",
    "authors": "Xiuyuan Chen, Tao Sun, Dexin Su, Ailing Yu, Junwei Liu, Zhe Chen, Gangzeng Jin, Xin Wang, Jingnan Liu, Hansong Xiao, Hualei Zhou, Dongjie Tao, Chunxiao Guo, Minghui Yang, Yuan Xia, Jing Zhao, Qianrui Fan, Yanyun Wang, Shuai Zhen, Kezhong Chen, Jun Wang, Zewen Sun, Heng Zhao, Tian Guan, Shaodong Wang, Geyun Chang, Jiaming Deng, Hongchengcheng Chen, Kexin Feng, Ruzhen Li, Jiayi Geng, Changtai Zhao, Jun Wang, Guihu Lin, Peihao Li, Liqi Liu, Peng Wei, Jian Wang, Jinjie Gu, Ping Wang, Fan Yang",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 16:40:28",
    "ori_summary": "Current benchmarks for AI clinician systems, often based on multiple-choice exams or manual rubrics, fail to capture the depth, robustness, and safety required for real-world clinical practice. To address this, we introduce the GAPS framework, a multidimensional paradigm for evaluating \\textbf{G}rounding (cognitive depth), \\textbf{A}dequacy (answer completeness), \\textbf{P}erturbation (robustness), and \\textbf{S}afety. Critically, we developed a fully automated, guideline-anchored pipeline to construct a GAPS-aligned benchmark end-to-end, overcoming the scalability and subjectivity limitations of prior work. Our pipeline assembles an evidence neighborhood, creates dual graph and tree representations, and automatically generates questions across G-levels. Rubrics are synthesized by a DeepResearch agent that mimics GRADE-consistent, PICO-driven evidence review in a ReAct loop. Scoring is performed by an ensemble of large language model (LLM) judges. Validation confirmed our automated questions are high-quality and align with clinician judgment. Evaluating state-of-the-art models on the benchmark revealed key failure modes: performance degrades sharply with increased reasoning depth (G-axis), models struggle with answer completeness (A-axis), and they are highly vulnerable to adversarial perturbations (P-axis) as well as certain safety issues (S-axis). This automated, clinically-grounded approach provides a reproducible and scalable method for rigorously evaluating AI clinician systems and guiding their development toward safer, more reliable clinical practice.",
    "summary": "",
    "translation": "GAPS：基于临床基础的自动化基准，用于评估AI临床医生",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域AI临床医生的评估基准，属于明确的医学应用范畴。根据用户要求，医学、生物学等特定领域应用属于不相关主题，且该论文与推荐系统、搜索或广告没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13721v1": {
    "title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching",
    "url": "https://www.alphaxiv.org/abs/2510.13721v1",
    "arxiv_id": "2510.13721v1",
    "authors": "Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, Tat-Seng Chua",
    "categories": "cs.CL, cs.AI, cs.CV, cs.MM",
    "pub_date": "2025-10-15 16:25:18",
    "ori_summary": "Next-generation multimodal foundation models capable of any-to-any cross-modal generation and multi-turn interaction will serve as core components of artificial general intelligence systems, playing a pivotal role in human-machine interaction. However, most existing multimodal models remain constrained by autoregressive architectures, whose inherent limitations prevent a balanced integration of understanding and generation capabilities. Although hybrid and decoupling strategies have been explored to address these tasks within unified frameworks separately, their redundant, non-integrated designs limit their applicability to broader scenarios, such as cross-modal retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal foundation model that achieves unified modeling through discrete flow paradigms. By leveraging metric-induced probability paths and kinetic optimal velocities, NExT-OMNI natively supports any-to-any understanding and generation with enhanced response efficiency, while enabling broader application scenarios through concise unified representations rather than task-decoupled designs. Trained on large-scale interleaved text, image, video, and audio data, NExT-OMNI delivers competitive performance on multimodal generation and understanding benchmarks, while outperforming prior unified models in multi-turn multimodal interaction and cross-modal retrieval, highlighting its architectural advantages as a next-generation multimodal foundation model. To advance further research, we release training details, data protocols, and open-source both the code and model checkpoints.",
    "summary": "该论文研究多模态基础模型中理解与生成能力不平衡的核心问题，提出通过离散流范式实现统一建模，利用度量诱导概率路径和动力学最优速度支持任意模态间的双向转换。",
    "translation": "NExT-OMNI：基于离散流匹配的任意模态到任意模态全模态基础模型",
    "relevance_score": 8,
    "reasoning": "该论文提出的任意模态到任意模态全模态基础模型与VLM类比异质数据的理念高度相关，能够统一处理搜索、推荐系统中的多种数据类型（如文本、图像、用户行为序列）。离散流匹配技术作为Transformer架构的效率优化方法，在提升多模态建模效率方面具有直接应用价值，特别适合处理推荐系统中复杂的异构特征交互。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的离散流匹配统一建模方法可直接应用于推荐系统的多模态理解与生成任务，其任意到任意的跨模态能力与VLM异构数据处理理念高度契合。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13681v1": {
    "title": "How Sampling Affects the Detectability of Machine-written texts: A Comprehensive Study",
    "url": "https://www.alphaxiv.org/abs/2510.13681v1",
    "arxiv_id": "2510.13681v1",
    "authors": "Matthieu Dubois, François Yvon, Pablo Piantanida",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 15:36:45",
    "ori_summary": "As texts generated by Large Language Models (LLMs) are ever more common and often indistinguishable from human-written content, research on automatic text detection has attracted growing attention. Many recent detectors report near-perfect accuracy, often boasting AUROC scores above 99\\%. However, these claims typically assume fixed generation settings, leaving open the question of how robust such systems are to changes in decoding strategies. In this work, we systematically examine how sampling-based decoding impacts detectability, with a focus on how subtle variations in a model's (sub)word-level distribution affect detection performance. We find that even minor adjustments to decoding parameters - such as temperature, top-p, or nucleus sampling - can severely impair detector accuracy, with AUROC dropping from near-perfect levels to 1\\% in some settings. Our findings expose critical blind spots in current detection methods and emphasize the need for more comprehensive evaluation protocols. To facilitate future research, we release a large-scale dataset encompassing 37 decoding configurations, along with our code and evaluation framework https://github.com/BaggerOfWords/Sampling-and-Detection",
    "summary": "",
    "translation": "采样如何影响机器生成文本的可检测性：一项全面研究",
    "relevance_score": 2,
    "reasoning": "该论文主要关注机器生成文本的检测问题，这属于纯粹的NLP评估基准范畴，与推荐系统、搜索或广告的核心技术进展无关。虽然检测技术可能在某些边缘场景中有应用，但论文标题明确聚焦于文本检测性这一评估问题，而非推荐/搜索/广告领域的核心技术创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13632v1": {
    "title": "Closing the Gap Between Text and Speech Understanding in LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.13632v1",
    "arxiv_id": "2510.13632v1",
    "authors": "Santiago Cuervo, Skyler Seto, Maureen de Seyssel, Richard He Bai, Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly, Zakaria Aldeneh",
    "categories": "cs.CL, cs.AI, eess.AS",
    "pub_date": "2025-10-15 14:57:16",
    "ori_summary": "Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts--and even cascaded pipelines--on language understanding tasks. We term this shortfall the text-speech understanding gap: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce SALAD--Sample-efficient Alignment with Learning through Active selection and cross-modal Distillation--which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora.",
    "summary": "",
    "translation": "缩小大型语言模型中文本与语音理解之间的差距",
    "relevance_score": 2,
    "reasoning": "该论文关注文本与语音理解的对齐，属于多模态领域，但语音模态在推荐系统、搜索或广告中的直接应用有限。虽然多模态对齐技术可能启发异构数据建模，但语音并非当前关注的异构数据类型（如上下文特征、用户序列），因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13626v1": {
    "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models",
    "url": "https://www.alphaxiv.org/abs/2510.13626v1",
    "arxiv_id": "2510.13626v1",
    "authors": "Senyu Fei, Siyin Wang, Junhao Shi, Zihao Dai, Jikun Cai, Pengfang Qian, Li Ji, Xinzhe He, Shiduo Zhang, Zhaoye Fei, Jinlan Fu, Jingjing Gong, Xipeng Qiu",
    "categories": "cs.RO, cs.CL, cs.CV",
    "pub_date": "2025-10-15 14:51:36",
    "ori_summary": "Visual-Language-Action (VLA) models report impressive success rates on robotic manipulation benchmarks, yet these results may mask fundamental weaknesses in robustness. We perform a systematic vulnerability analysis by introducing controlled perturbations across seven dimensions: objects layout, camera viewpoints, robot initial states, language instructions, light conditions, background textures and sensor noise. We comprehensively analyzed multiple state-of-the-art models and revealed consistent brittleness beneath apparent competence. Our analysis exposes critical weaknesses: models exhibit extreme sensitivity to perturbation factors, including camera viewpoints and robot initial states, with performance dropping from 95% to below 30% under modest perturbations. Surprisingly, models are largely insensitive to language variations, with further experiments revealing that models tend to ignore language instructions completely. Our findings challenge the assumption that high benchmark scores equate to true competency and highlight the need for evaluation practices that assess reliability under realistic variation.",
    "summary": "",
    "translation": "LIBERO-Plus：视觉-语言-动作模型的深度鲁棒性分析",
    "relevance_score": 2,
    "reasoning": "该论文虽然涉及视觉-语言模型，但主要关注动作模型和鲁棒性分析，这与推荐系统、搜索或广告的核心技术关联较弱。视觉-语言-动作模型主要应用于机器人控制领域，其鲁棒性分析对RecSys/Search/Ads的潜在应用价值有限，因为我们的焦点在于处理异构数据的统一建模，而非动作控制系统的可靠性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13624v1": {
    "title": "Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of German Tumor Diagnoses",
    "url": "https://www.alphaxiv.org/abs/2510.13624v1",
    "arxiv_id": "2510.13624v1",
    "authors": "Stefan Lenz, Lakisha Ortiz Rosario, Georg Vollmar, Arsenij Ustjanzew, Fatma Alickovic, Thomas Kindler, Torsten Panholzer",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-15 14:51:28",
    "ori_summary": "Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential for structured cancer documentation in Germany. Smaller open-weight LLMs are appealing for privacy-preserving automation but often struggle with coding accuracy in German-language contexts. This study investigates whether instruction-based fine-tuning on public datasets improves the coding accuracy of open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded diagnoses from the local tumor documentation system as test data. In a systematic data quality assessment, the upper limit for ICD-10 coding performance was estimated at 60-79% for exact and 81-94% for partial (three-character codes only) derivation. As training data, over 500,000 question-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS catalogues. Eight open-weight models from the Qwen, Llama, and Mistral families (7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to 41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3 topography coding also improved but started and remained considerably lower with an exact accuracy of 22-40% and a partial accuracy of 56-67% after fine-tuning. Malformed code outputs dropped to 0% for all models. Tumor-diagnosis recognition reached 99%. Accuracy correlated positively with model size, but gaps between small and large models narrowed after fine-tuning. The reasoning mode in Qwen3 generally yielded a lower performance than fine-tuning and was over 100 times slower. Our findings highlight the potential of leveraging public catalogues to build instruction datasets that improve LLMs in medical documentation tasks. The complete training dataset and the best-performing checkpoints of the fine-tuned models are available from https://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024.",
    "summary": "",
    "translation": "解锁公共目录：为德国肿瘤诊断ICD编码进行指令微调大语言模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的ICD编码应用，属于医疗生物医学特定领域应用，与推荐系统、搜索或广告无关。指令微调技术虽然是LLM相关，但应用场景完全在医疗诊断编码领域，没有显示出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13614v1": {
    "title": "MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.13614v1",
    "arxiv_id": "2510.13614v1",
    "authors": "Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 14:43:31",
    "ori_summary": "Large Language Models (LLMs) have achieved impressive reasoning abilities, but struggle with temporal understanding, especially when questions involve multiple entities, compound operators, and evolving event sequences. Temporal Knowledge Graphs (TKGs), which capture vast amounts of temporal facts in a structured format, offer a reliable source for temporal reasoning. However, existing TKG-based LLM reasoning methods still struggle with four major challenges: maintaining temporal faithfulness in multi-hop reasoning, achieving multi-entity temporal synchronization, adapting retrieval to diverse temporal operators, and reusing prior reasoning experience for stability and efficiency. To address these issues, we propose MemoTime, a memory-augmented temporal knowledge graph framework that enhances LLM reasoning through structured grounding, recursive reasoning, and continual experience learning. MemoTime decomposes complex temporal questions into a hierarchical Tree of Time, enabling operator-aware reasoning that enforces monotonic timestamps and co-constrains multiple entities under unified temporal bounds. A dynamic evidence retrieval layer adaptively selects operator-specific retrieval strategies, while a self-evolving experience memory stores verified reasoning traces, toolkit decisions, and sub-question embeddings for cross-type reuse. Comprehensive experiments on multiple temporal QA benchmarks show that MemoTime achieves overall state-of-the-art results, outperforming the strong baseline by up to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo.",
    "summary": "",
    "translation": "MemoTime：内存增强时序知识图谱增强的大型语言模型推理",
    "relevance_score": 6,
    "reasoning": "该论文涉及时序知识图谱和LLM推理增强技术，属于'赋能LLM技术'范畴。时序知识图谱增强的LLM推理可应用于搜索和推荐系统中的时序用户行为建模、动态兴趣捕捉和上下文感知的个性化推荐，提升对用户行为序列的理解和预测能力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.13602v1": {
    "title": "NOSA: Native and Offloadable Sparse Attention",
    "url": "https://www.alphaxiv.org/abs/2510.13602v1",
    "arxiv_id": "2510.13602v1",
    "authors": "Yuxiang Huang, Chaojun Xiao, Xu Han, Zhiyuan Liu",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-15 14:33:16",
    "ori_summary": "Trainable sparse attention has emerged as a promising solution to address the decoding efficiency bottleneck of LLMs in long-context processing, significantly saving memory accesses while minimally impacting task performance. However, existing sparse attention methods leave a crucial limitation unresolved: the size of the key-value (KV) cache remains unreduced, which constrains on-GPU batch sizes and throttles decoding throughput, especially in large-scale batched inference. In this paper, we show that trainable sparse attention naturally exhibits strong locality in token selection across adjacent decoding steps, thereby enabling KV cache offloading without altering the underlying attention computation. However, the inherent locality remains insufficient to achieve efficient offloading, as the transfer of selected KV pairs between the CPU and GPU continues to dominate the overall decoding cost. Building on this insight, we present NOSA, a trainable sparse attention framework designed to natively support KV cache offloading. NOSA introduces explicit locality constraints by decomposing token selection into query-aware and query-agnostic components, thereby reducing KV transfers while preserving the same attention computation as used during training. We pretrain a 1B-parameter model with NOSA and conduct extensive benchmarks, showing that it preserves near-lossless performance while achieving up to a 2.3x improvement in decoding throughput compared with the vanilla trainable sparse attention baseline (InfLLM-V2).",
    "summary": "该论文研究LLM长上下文处理中KV缓存过大导致的解码效率瓶颈问题，核心思想是通过分解token选择为查询感知和查询无关组件，在保持训练时注意力计算不变的前提下，为可训练稀疏注意力引入显式局部性约束，从而原生支持KV缓存卸载。",
    "translation": "NOSA：原生与可卸载稀疏注意力",
    "relevance_score": 9,
    "reasoning": "该论文聚焦于Transformer架构中的稀疏注意力机制优化，属于'使能Transformer技术'范畴。稀疏注意力技术可直接应用于大规模推荐和搜索系统，通过减少计算复杂度来提升长序列处理效率，这对于处理用户长历史行为序列和上下文特征至关重要。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM解码效率瓶颈，提出创新的稀疏注意力框架，在Transformer架构效率和LLM推理优化方面具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13598v1": {
    "title": "FreshTab: Sourcing Fresh Data for Table-to-Text Generation Evaluation",
    "url": "https://www.alphaxiv.org/abs/2510.13598v1",
    "arxiv_id": "2510.13598v1",
    "authors": "Kristýna Onderková, Ondřej Plátek, Zdeněk Kasner, Ondřej Dušek",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 14:31:44",
    "ori_summary": "Table-to-text generation (insight generation from tables) is a challenging task that requires precision in analyzing the data. In addition, the evaluation of existing benchmarks is affected by contamination of Large Language Model (LLM) training data as well as domain imbalance. We introduce FreshTab, an on-the-fly table-to-text benchmark generation from Wikipedia, to combat the LLM data contamination problem and enable domain-sensitive evaluation. While non-English table-to-text datasets are limited, FreshTab collects datasets in different languages on demand (we experiment with German, Russian and French in addition to English). We find that insights generated by LLMs from recent tables collected by our method appear clearly worse by automatic metrics, but this does not translate into LLM and human evaluations. Domain effects are visible in all evaluations, showing that a~domain-balanced benchmark is more challenging.",
    "summary": "",
    "translation": "FreshTab：为表格到文本生成评估获取新鲜数据",
    "relevance_score": 1,
    "reasoning": "该论文关注表格到文本生成的评估数据获取，属于纯粹的文本生成和评估领域。虽然表格数据在推荐和搜索系统中存在，但论文焦点是文本生成评估而非实际应用，与当前关注的推荐系统、搜索广告核心进展、使能技术及直接应用均无关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13586v1": {
    "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs",
    "url": "https://www.alphaxiv.org/abs/2510.13586v1",
    "arxiv_id": "2510.13586v1",
    "authors": "Pasin Buakhaw, Kun Kerdthaisong, Phuree Phenhiran, Pitikorn Khlaisamniang, Supasate Vorathammathorn, Piyalitt Ittichaiwong, Nutchanon Yongsatianchot",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 14:17:23",
    "ori_summary": "The emergence of large language models (LLMs) has opened new opportunities for cre- ating dynamic non-player characters (NPCs) in gaming environments, enabling both func- tional task execution and persona-consistent dialogue generation. In this paper, we (Tu_Character_lab) report our participation in the Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which eval- uates agents across three tracks: task-oriented dialogue, context-aware dialogue, and their integration. Our approach combines two complementary strategies: (i) lightweight prompting techniques in the API track, including a Deflanderization prompting method to suppress excessive role-play and improve task fidelity, and (ii) fine-tuned large models in the GPU track, leveraging Qwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our best submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on Task 3 (GPU track).",
    "summary": "",
    "translation": "游戏对话的去扁平化：在基于大语言模型的NPC中平衡角色真实性与任务执行",
    "relevance_score": 2,
    "reasoning": "该论文主要关注游戏NPC对话系统的具体应用，属于垂直领域的内容生成问题。虽然涉及LLM技术，但其应用场景（游戏对话）与推荐系统、搜索或广告的核心业务领域没有直接关联，且属于被排除的AIGC/内容生成范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13580v1": {
    "title": "Sparse Subnetwork Enhancement for Underrepresented Languages in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13580v1",
    "arxiv_id": "2510.13580v1",
    "authors": "Daniil Gurgurov, Josef van Genabith, Simon Ostermann",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 14:14:49",
    "ori_summary": "Large language models exhibit uneven performance across languages, with substantial gaps between high- and low-resource languages. We present a framework for enhancing monolingual capabilities of LLMs in underrepresented languages while preserving their general-purpose performance through targeted fine-tuning of language-specific subnetworks. Our approach identifies language-specific neurons using Language Activation Probability Entropy and fine-tunes only the weights associated with these neurons, a dedicated subnetwork, on target-language data. Experiments on Llama-3.1-8B and Mistral-Nemo-12B across 12 mid- and low-resource languages demonstrate that our method consistently outperforms full fine-tuning, FFN-only fine-tuning, LoRA adaptation, and random subset fine-tuning baselines while efficiently updating only up to 1% of model parameters. Beyond performance improvements, we observe enhanced favorable training dynamics, cross-lingual representational alignment, and systematic weight update changes. To facilitate future research, we release language-specific neuron identifications for over 100 languages as well as our adaptation pipeline, offering a cost-effective pathway for adapting state-of-the-art models to underrepresented languages.",
    "summary": "",
    "translation": "大型语言模型中低资源语言的稀疏子网络增强",
    "relevance_score": 2,
    "reasoning": "该论文主要关注低资源语言的模型能力增强，属于特定语言领域的优化问题。虽然涉及LLM技术，但缺乏与推荐系统、搜索或广告领域的直接关联，也没有展示在异构数据处理或Transformer架构效率方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13554v1": {
    "title": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.13554v1",
    "arxiv_id": "2510.13554v1",
    "authors": "Yang Li, Zhichen Dong, Yuhan Sun, Weixun Wang, Shaopan Xiong, Yijia Luo, Jiashun Liu, Han Lu, Jiamang Wang, Wenbo Su, Bo Zheng, Junchi Yan",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-15 13:49:51",
    "ori_summary": "The reasoning pattern of Large language models (LLMs) remains opaque, and Reinforcement learning (RL) typically applies uniform credit across an entire generation, blurring the distinction between pivotal and routine steps. This work positions attention as a privileged substrate that renders the internal logic of LLMs legible, not merely as a byproduct of computation, but as a mechanistic blueprint of reasoning itself. We first distinguish attention heads between locally and globally focused information processing and reveal that locally focused heads produce a sawtooth pattern near the diagonal indicating phrasal chunks, while globally focused heads expose tokens that exert broad downstream influence over future tokens. We formalize these with two metrics: 1) Windowed Average Attention Distance, which measures the extent of backward attention within a clipped window; 2) Future Attention Influence, which quantifies a token's global importance as the average attention it receives from subsequent tokens. Taken together, these signals reveal a recurring preplan-and-anchor mechanism, where the model first performs a long-range contextual reference to generate an introductory token, which is immediately followed by or coincides with a semantic anchor token that organizes subsequent reasoning. Leveraging these insights, we introduce three novel RL strategies that dynamically perform targeted credit assignment to critical nodes (preplan tokens, anchor tokens, and their temporal coupling) and show consistent performance gains across various reasoning tasks. By aligning optimization with the model's intrinsic reasoning rhythm, we aim to transform opaque optimization into an actionable structure-aware process, hoping to offer a potential step toward more transparent and effective optimization of LLM reasoning.",
    "summary": "论文研究LLM推理过程中注意力机制的作用模式，核心发现是存在预规划-锚定节奏机制，即模型先进行长程上下文参考生成引导性token，随后出现组织后续推理的语义锚定token。",
    "translation": "注意力机制照亮LLM推理：预规划与锚定节奏实现细粒度策略优化",
    "relevance_score": 8,
    "reasoning": "该论文研究LLM推理机制和注意力模式，属于'赋能LLM技术'类别，探讨LLM内部推理过程的核心进展。在搜索和推荐系统中，理解LLM的推理机制可以显著改善查询理解、结果排序和个性化推荐的质量，通过优化推理策略提升系统性能和用户体验。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文通过注意力机制揭示LLM推理的内部机制，并基于此开发针对性的强化学习策略，直接关联Transformer架构优化和LLM推理应用。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13537v1": {
    "title": "K-Merge: Online Continual Merging of Adapters for On-device Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13537v1",
    "arxiv_id": "2510.13537v1",
    "authors": "Donald Shenaj, Ondrej Bohdal, Taha Ceritli, Mete Ozay, Pietro Zanuttigh, Umberto Michieli",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-15 13:32:25",
    "ori_summary": "On-device deployment of Large Language Models (LLMs) frequently leverages Low-Rank Adapters (LoRAs) to support diverse downstream tasks under tight resource constraints. To address the limited storage capacity of mobile devices, recent works have explored model merging techniques to fuse multiple LoRAs into a single one. In practice, however, LoRAs are often delivered incrementally, as users request support for new tasks (e.g., novel problem types or languages). This scenario introduces a new challenge: on-device online continual merging, where the objective is to incorporate new LoRAs while preserving the performance on previously supported tasks. In this paper, we propose a data-free and computationally efficient strategy for selecting and merging LoRAs when a new one becomes available, assuming the device can store only a limited number of adapters. Extensive experiments across real-world tasks demonstrate the superiority of our approach compared to alternative strategies while adhering to the storage budget and compute limitations of on-device settings.",
    "summary": "研究在设备端LLM存储受限环境下如何在线持续集成新的任务适配器；核心方法是开发数据无关且计算高效的适配器选择和合并策略，在存储预算内保持对历史任务性能。",
    "translation": "K-Merge：面向设备端大语言模型的适配器在线持续合并方法",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM适配器的在线持续合并技术，属于'使能LLM技术'范畴，专注于模型效率和部署优化。在推荐系统、搜索和广告场景中，这种技术可以实现更高效的个性化模型更新和在线学习，支持设备端实时适应不断变化的用户偏好和行为模式。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接解决设备端LLM在推荐/搜索场景中的存储限制问题，提出的在线持续合并适配器方法对资源受限环境下的模型部署具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13500v1": {
    "title": "MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts",
    "url": "https://www.alphaxiv.org/abs/2510.13500v1",
    "arxiv_id": "2510.13500v1",
    "authors": "Shujun Xia, Haokun Lin, Yichen Wu, Yinan Zhou, Zixuan Li, Zhongwei Wan, Xingrun Xing, Yefeng Zheng, Xiang Li, Caifeng Shan, Zhenan Sun, Quanzheng Li",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 12:50:33",
    "ori_summary": "LLMs hold great promise for healthcare applications, but the rapid evolution of medical knowledge and errors in training data often cause them to generate outdated or inaccurate information, limiting their applicability in high-stakes clinical practice. Model editing has emerged as a potential remedy without full retraining. While parameter-based editing often compromises locality and is thus ill-suited for the medical domain, retrieval-based editing offers a more viable alternative. However, it still faces two critical challenges: (1) representation overlap within the medical knowledge space often causes inaccurate retrieval and reduces editing accuracy; (2) existing methods are restricted to single-sample edits, while batch-editing remains largely unexplored despite its importance for real-world medical applications. To address these challenges, we first construct MedVersa, \\hk{an enhanced benchmark with broader coverage of medical subjects, designed to evaluate both single and batch edits under strict locality constraints}. We then propose MedREK, a retrieval-based editing framework that integrates a shared query-key module for precise matching with an attention-based prompt encoder for informative guidance. Experimental results on various medical benchmarks demonstrate that our MedREK achieves superior performance across different core metrics and provides the first validated solution for batch-editing in medical LLMs. Our code and dataset are available at https://github.com/mylittleriver/MedREK.",
    "summary": "",
    "translation": "MedREK：基于检索的医学大语言模型编辑与关键感知提示",
    "relevance_score": 2,
    "reasoning": "该论文聚焦于医学领域的LLM编辑技术，属于明确的医学领域特定应用，属于无关主题。虽然涉及检索增强和提示工程，但这些技术元素在医学领域的应用与推荐系统、搜索或广告的核心关注点无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13499v1": {
    "title": "ConsintBench: Evaluating Language Models on Real-World Consumer Intent Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.13499v1",
    "arxiv_id": "2510.13499v1",
    "authors": "Xiaozhe Li, TianYi Lyu, Siyi Yang, Yuxi Gong, Yizhao Yang, Jinxuan Huang, Ligao Zhang, Zhuoyi Huang, Qingwen Liu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 12:49:45",
    "ori_summary": "Understanding human intent is a complex, high-level task for large language models (LLMs), requiring analytical reasoning, contextual interpretation, dynamic information aggregation, and decision-making under uncertainty. Real-world public discussions, such as consumer product discussions, are rarely linear or involve a single user. Instead, they are characterized by interwoven and often conflicting perspectives, divergent concerns, goals, emotional tendencies, as well as implicit assumptions and background knowledge about usage scenarios. To accurately understand such explicit public intent, an LLM must go beyond parsing individual sentences; it must integrate multi-source signals, reason over inconsistencies, and adapt to evolving discourse, similar to how experts in fields like politics, economics, or finance approach complex, uncertain environments. Despite the importance of this capability, no large-scale benchmark currently exists for evaluating LLMs on real-world human intent understanding, primarily due to the challenges of collecting real-world public discussion data and constructing a robust evaluation pipeline. To bridge this gap, we introduce \\bench, the first dynamic, live evaluation benchmark specifically designed for intent understanding, particularly in the consumer domain. \\bench is the largest and most diverse benchmark of its kind, supporting real-time updates while preventing data contamination through an automated curation pipeline.",
    "summary": "该论文研究如何评估语言模型在真实世界消费者意图理解上的能力，核心方法是构建首个动态、实时的意图理解评估基准，专门针对消费者领域的复杂多源信号整合需求。",
    "translation": "ConsintBench：评估语言模型在真实世界消费者意图理解上的表现",
    "relevance_score": 8,
    "reasoning": "该论文专注于评估语言模型在消费者意图理解方面的能力，这直接关系到搜索和推荐系统的核心任务。消费者意图理解是搜索查询理解、用户兴趣建模和个性化推荐的关键技术，该基准测试的进展可以直接应用于提升搜索和推荐系统的性能。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文专注于语言模型的意图理解能力评估，虽然不直接涉及推荐系统，但意图理解是搜索和广告领域的核心技术基础。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13494v1": {
    "title": "LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA",
    "url": "https://www.alphaxiv.org/abs/2510.13494v1",
    "arxiv_id": "2510.13494v1",
    "authors": "Tommaso Bonomo, Luca Gioffré, Roberto Navigli",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 12:43:59",
    "ori_summary": "Question Answering (QA) on narrative text poses a unique challenge to current systems, requiring a deep understanding of long, complex documents. However, the reliability of NarrativeQA, the most widely used benchmark in this domain, is hindered by noisy documents and flawed QA pairs. In this work, we introduce LiteraryQA, a high-quality subset of NarrativeQA focused on literary works. Using a human- and LLM-validated pipeline, we identify and correct low-quality QA samples while removing extraneous text from source documents. We then carry out a meta-evaluation of automatic metrics to clarify how systems should be evaluated on LiteraryQA. This analysis reveals that all n-gram-based metrics have a low system-level correlation to human judgment, while LLM-as-a-Judge evaluations, even with small open-weight models, can strongly agree with the ranking identified by humans. Finally, we benchmark a set of long-context LLMs on LiteraryQA. We release our code and data at https://github.com/SapienzaNLP/LiteraryQA.",
    "summary": "",
    "translation": "LiteraryQA：面向长文档叙事问答的有效评估",
    "relevance_score": 2,
    "reasoning": "该论文主要关注长文档问答的评估基准，属于纯粹的NLP评估领域，与推荐系统、搜索或广告的核心技术进展无关。虽然问答技术可能间接影响搜索质量，但该论文专注于文学叙事领域的特定评估，缺乏对RecSys/Search/Ads的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13434v1": {
    "title": "Beyond Single-Reward: Multi-Pair, Multi-Perspective Preference Optimization for Machine Translation",
    "url": "https://www.alphaxiv.org/abs/2510.13434v1",
    "arxiv_id": "2510.13434v1",
    "authors": "Hao Wang, Linlong Xu, Heng Liu, Yangyang Liu, Xiaohu Zhao, Bo Zeng, Liangying Shao, Longyue Wang, Weihua Luo, Kaifu Zhang",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 11:30:49",
    "ori_summary": "Direct Preference Optimization (DPO) is a powerful paradigm for aligning Large Language Models (LLMs) to human preferences in Machine Translation (MT), but current methods are hindered by two fundamental challenges: (1) flawed reward signals from Quality Estimation (QE) models that overlook critical errors like translation hallucination, and (2) inefficient data utilization that discards valuable learning signals by selecting only a single win-loss pair. To address these limitations, we introduce M^2PO: Multi-Pair, Multi-Perspective Preference Optimization. Our framework integrates a multi-perspective reward engine that creates a more robust signal by combining two key viewpoints: a new hallucination penalty for factuality, and an innovative dynamic quality score that adaptively fuses external evaluations with the model's own evolving judgment. This is synergistically paired with a multi-pair construction strategy that systematically creates a comprehensive set of preference pairs from the entire pool of translation candidates. This synergistic approach ensures the model learns from a richer spectrum of quality trade-offs, leading to more robust and faithful translations. On challenging WMT21-22 benchmarks, M^2PO substantially outperforms existing preference optimization methods and demonstrates highly competitive performance against leading proprietary LLMs.",
    "summary": "",
    "translation": "超越单一奖励：面向机器翻译的多配对、多视角偏好优化",
    "relevance_score": 2,
    "reasoning": "该论文专注于机器翻译领域的偏好优化技术，属于纯NLP应用范畴。虽然多奖励优化概念在理论上可能对推荐系统的多目标优化有启发，但论文明确限定在机器翻译领域，缺乏与推荐、搜索或广告系统的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13430v1": {
    "title": "Evaluating Arabic Large Language Models: A Survey of Benchmarks, Methods, and Gaps",
    "url": "https://www.alphaxiv.org/abs/2510.13430v1",
    "arxiv_id": "2510.13430v1",
    "authors": "Ahmed Alzubaidi, Shaikha Alsuwaidi, Basma El Amel Boussaha, Leen AlQadi, Omar Alkaabi, Mohammed Alyafeai, Hamza Alobeidli, Hakim Hacid",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 11:25:33",
    "ori_summary": "This survey provides the first systematic review of Arabic LLM benchmarks, analyzing 40+ evaluation benchmarks across NLP tasks, knowledge domains, cultural understanding, and specialized capabilities. We propose a taxonomy organizing benchmarks into four categories: Knowledge, NLP Tasks, Culture and Dialects, and Target-Specific evaluations. Our analysis reveals significant progress in benchmark diversity while identifying critical gaps: limited temporal evaluation, insufficient multi-turn dialogue assessment, and cultural misalignment in translated datasets. We examine three primary approaches: native collection, translation, and synthetic generation discussing their trade-offs regarding authenticity, scale, and cost. This work serves as a comprehensive reference for Arabic NLP researchers, providing insights into benchmark methodologies, reproducibility standards, and evaluation metrics while offering recommendations for future development.",
    "summary": "",
    "translation": "评估阿拉伯语大语言模型：基准、方法与差距综述",
    "relevance_score": 2,
    "reasoning": "该论文主要关注阿拉伯语LLM的评估基准和方法，属于纯粹的评估基准研究。虽然涉及大语言模型，但焦点是特定语言的评估而非核心模型技术进展或直接应用。对于推荐系统、搜索或广告领域，这种语言特定的评估研究缺乏明确的实用价值和技术启发性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13417v1": {
    "title": "Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse",
    "url": "https://www.alphaxiv.org/abs/2510.13417v1",
    "arxiv_id": "2510.13417v1",
    "authors": "Liesbeth Allein, Nataly Pineda-Castañeda, Andrea Rocci, Marie-Francine Moens",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-15 11:15:00",
    "ori_summary": "How does a cause lead to an effect, and which intermediate causal steps explain their connection? This work scrutinizes the mechanistic causal reasoning capabilities of large language models (LLMs) to answer these questions through the task of implicit causal chain discovery. In a diagnostic evaluation framework, we instruct nine LLMs to generate all possible intermediate causal steps linking given cause-effect pairs in causal chain structures. These pairs are drawn from recent resources in argumentation studies featuring polarized discussion on climate change. Our analysis reveals that LLMs vary in the number and granularity of causal steps they produce. Although they are generally self-consistent and confident about the intermediate causal connections in the generated chains, their judgments are mainly driven by associative pattern matching rather than genuine causal reasoning. Nonetheless, human evaluations confirmed the logical coherence and integrity of the generated chains. Our baseline causal chain discovery approach, insights from our diagnostic evaluation, and benchmark dataset with causal chains lay a solid foundation for advancing future work in implicit, mechanistic causal reasoning in argumentation settings.",
    "summary": "",
    "translation": "通过气候论述中的隐式因果链发现评估大语言模型推理能力",
    "relevance_score": 1,
    "reasoning": "该论文专注于评估LLM在气候论述这一特定领域中的推理能力，属于纯粹的NLP评估和基准测试范畴。虽然涉及LLM推理，但主题是气候论述这一无关领域，且没有展示在推荐系统、搜索或广告中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13407v1": {
    "title": "Investigating Lexical Change through Cross-Linguistic Colexification Patterns",
    "url": "https://www.alphaxiv.org/abs/2510.13407v1",
    "arxiv_id": "2510.13407v1",
    "authors": "Kim Gfeller, Sabine Stoll, Chundra Cathcart, Paul Widmer",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 11:04:28",
    "ori_summary": "One of the most intriguing features of language is its constant change, with ongoing shifts in how meaning is expressed. Despite decades of research, the factors that determine how and why meanings evolve remain only partly understood. Colexification -- the phenomenon of expressing multiple distinct concepts using the same word form -- serves as a valuable window onto the dynamics of meaning change across languages. Here, we apply phylogenetic comparative models to dictionary data from three language families, Austronesian, Indo-European, and Uralic, in order to shed light on the evolutionary dynamics underlying the colexification of concept pairs. We assess the effects of three predictors: associativity, borrowability, and usage frequency. Our results show that more closely related concept pairs are colexified across a larger portion of the family tree and exhibit slower rates of change. In contrast, concept pairs that are more frequent and more prone to borrowing tend to change more rapidly and are less often colexified. We also find considerable differences between the language families under study, suggesting that areal and cultural factors may play a role.",
    "summary": "",
    "translation": "通过跨语言共词汇化模式研究词汇演变",
    "relevance_score": 1,
    "reasoning": "该论文研究词汇演变和跨语言共词汇化模式，属于纯粹的语言学和历史语言学领域，与推荐系统、搜索或广告的核心技术进展没有直接关联。论文内容不涉及Transformer架构、LLM技术应用，也没有明显的潜在应用场景在推荐、搜索或广告领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13395v1": {
    "title": "Doing Things with Words: Rethinking Theory of Mind Simulation in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13395v1",
    "arxiv_id": "2510.13395v1",
    "authors": "Agnese Lombardi, Alessandro Lenci",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 10:48:31",
    "ori_summary": "Language is fundamental to human cooperation, facilitating not only the exchange of information but also the coordination of actions through shared interpretations of situational contexts. This study explores whether the Generative Agent-Based Model (GABM) Concordia can effectively model Theory of Mind (ToM) within simulated real-world environments. Specifically, we assess whether this framework successfully simulates ToM abilities and whether GPT-4 can perform tasks by making genuine inferences from social context, rather than relying on linguistic memorization. Our findings reveal a critical limitation: GPT-4 frequently fails to select actions based on belief attribution, suggesting that apparent ToM-like abilities observed in previous studies may stem from shallow statistical associations rather than true reasoning. Additionally, the model struggles to generate coherent causal effects from agent actions, exposing difficulties in processing complex social interactions. These results challenge current statements about emergent ToM-like capabilities in LLMs and highlight the need for more rigorous, action-based evaluation frameworks.",
    "summary": "",
    "translation": "用语言做事：重新思考大型语言模型中的心智理论模拟",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM中的心智理论（Theory of Mind）模拟，这是一个纯粹的NLP认知能力研究主题。虽然心智理论在理解用户意图方面有潜在价值，但论文标题显示其焦点是基础认知机制而非具体的推荐、搜索或广告应用，属于纯粹的LLM理论分析范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13387v1": {
    "title": "Make an Offer They Can't Refuse: Grounding Bayesian Persuasion in Real-World Dialogues without Pre-Commitment",
    "url": "https://www.alphaxiv.org/abs/2510.13387v1",
    "arxiv_id": "2510.13387v1",
    "authors": "Buwei He, Yang Liu, Zhaowei Zhang, Zixia Jia, Huijia Wu, Zhaofeng He, Zilong Zheng, Yipeng Kang",
    "categories": "cs.CL, cs.GT",
    "pub_date": "2025-10-15 10:26:02",
    "ori_summary": "Persuasion, a fundamental social capability for humans, remains a challenge for AI systems such as large language models (LLMs). Current studies often overlook the strategic use of information asymmetry in message design or rely on strong assumptions regarding pre-commitment. In this work, we explore the application of Bayesian Persuasion (BP) in natural language within single-turn dialogue settings, to enhance the strategic persuasion capabilities of LLMs. Our framework incorporates a commitment-communication mechanism, where the persuader explicitly outlines an information schema by narrating their potential types (e.g., honest or dishonest), thereby guiding the persuadee in performing the intended Bayesian belief update. We evaluate two variants of our approach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language (FNL) BP, benchmarking them against both naive and strong non-BP (NBP) baselines within a comprehensive evaluation framework. This framework covers a diverse set of persuadees -- including LLM instances with varying prompts and fine-tuning and human participants -- across tasks ranging from specially designed persuasion scenarios to general everyday situations. Experimental results on LLM-based agents reveal three main findings: (1) LLMs guided by BP strategies consistently achieve higher persuasion success rates than NBP baselines; (2) SFNL exhibits greater credibility and logical coherence, while FNL shows stronger emotional resonance and robustness in naturalistic conversations; (3) with supervised fine-tuning, smaller models can attain BP performance comparable to that of larger models.",
    "summary": "",
    "translation": "提出无法拒绝的报价：在无需预先承诺的现实对话中实现贝叶斯说服",
    "relevance_score": 2,
    "reasoning": "该论文主要研究贝叶斯说服理论在现实对话中的应用，属于博弈论和对话系统的交叉领域。虽然对话系统可能与搜索推荐有一定关联，但论文焦点在于说服机制而非推荐算法或LLM技术，与当前关注的核心领域进展、LLM技术或Transformer架构的直接关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13366v1": {
    "title": "Document Intelligence in the Era of Large Language Models: A Survey",
    "url": "https://www.alphaxiv.org/abs/2510.13366v1",
    "arxiv_id": "2510.13366v1",
    "authors": "Weishi Wang, Hengchang Hu, Zhijie Zhang, Zhaochen Li, Hongxin Shao, Daniel Dahlmeier",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 09:57:03",
    "ori_summary": "Document AI (DAI) has emerged as a vital application area, and is significantly transformed by the advent of large language models (LLMs). While earlier approaches relied on encoder-decoder architectures, decoder-only LLMs have revolutionized DAI, bringing remarkable advancements in understanding and generation. This survey provides a comprehensive overview of DAI's evolution, highlighting current research attempts and future prospects of LLMs in this field. We explore key advancements and challenges in multimodal, multilingual, and retrieval-augmented DAI, while also suggesting future research directions, including agent-based approaches and document-specific foundation models. This paper aims to provide a structured analysis of the state-of-the-art in DAI and its implications for both academic and practical applications.",
    "summary": "该论文研究文档智能领域在大型语言模型时代的发展与挑战。核心思想是系统梳理LLM如何变革文档AI，重点关注多模态、多语言和检索增强方法，并探讨基于代理的文档处理及文档专用基础模型等未来方向。",
    "translation": "大语言模型时代的文档智能：综述",
    "relevance_score": 8,
    "reasoning": "该综述聚焦文档智能与大语言模型的结合，在搜索领域具有直接应用价值（如文档理解、信息检索），同时在推荐系统中可应用于用户行为文档分析。作为LLM应用研究，它属于'直接LLM应用'范畴，并为处理异构文档数据提供技术参考。",
    "rerank_relevance_score": 6,
    "rerank_reasoning": "该综述探讨LLM在文档智能领域的应用，与推荐系统和搜索有间接关联，但未直接聚焦于核心推荐算法或架构创新。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13363v1": {
    "title": "D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured Memory And Reasoning Tree",
    "url": "https://www.alphaxiv.org/abs/2510.13363v1",
    "arxiv_id": "2510.13363v1",
    "authors": "Xiang Lei, Qin Li, Min Zhang, Min Zhang",
    "categories": "cs.CL, 68T50, 68T30, I.2.7; I.2.4",
    "pub_date": "2025-10-15 09:53:11",
    "ori_summary": "Large Language Models (LLMs) often exhibit factual inconsistencies and logical decay in extended, multi-turn dialogues, a challenge stemming from their reliance on static, pre-trained knowledge and an inability to reason adaptively over the dialogue history. Prevailing mitigation strategies, such as Retrieval-Augmented Generation (RAG) and agentic working memories, improve information recall but still engage with fundamentally static knowledge sources and follow pre-defined single reasoning path. This hinders their ability to preserve factual and logical consistency of their responses in multi-turn dialogues while the context evolves over time. To address this issue, we propose D-SMART, a model-agnostic framework designed to maintain multi-turn dialogue consistency by enabling LLMs to build and reason over a dynamic, structured representation of the conversational context. This is achieved via two synergistic components: (1) a Dynamic Structured Memory (DSM), which incrementally constructs and maintains an authoritative, OWL-compliant knowledge graph of the conversation; and (2) a Reasoning Tree (RT), which executes inferences as an explicit and traceable multi-step search over the graph. As the popular-used quality score (judged by GPT-4) can overlook logical flaws, we introduce new NLI-based metrics to better measure multi-turn dialogue consistency. Comprehensive experiments on the MT-Bench-101 benchmark show that D-SMART significantly outperforms state-of-the-art baselines, elevating the dialogue consistency score by over 48\\% for both proprietary and open-source models, and notably improves the quality score of the latter by up to 10.1\\%.",
    "summary": "",
    "translation": "D-SMART：通过动态结构化记忆与推理树增强大语言模型对话一致性",
    "relevance_score": 3,
    "reasoning": "该论文主要关注对话系统的一致性增强，属于LLM对话优化的特定领域。虽然涉及记忆和推理机制，但这些技术主要针对对话连贯性，在推荐系统、搜索或广告中的直接应用潜力有限。动态记忆和推理树可能对会话推荐有一定启发，但核心焦点偏离了RecSys/Search/Ads的核心排序或检索任务。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13357v1": {
    "title": "Personal Attribute Leakage in Federated Speech Models",
    "url": "https://www.alphaxiv.org/abs/2510.13357v1",
    "arxiv_id": "2510.13357v1",
    "authors": "Hamdan Al-Ali, Ali Reza Ghavamipour, Tommaso Caselli, Fatih Turkmen, Zeerak Talat, Hanan Aldarmaki",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 09:43:10",
    "ori_summary": "Federated learning is a common method for privacy-preserving training of machine learning models. In this paper, we analyze the vulnerability of ASR models to attribute inference attacks in the federated setting. We test a non-parametric white-box attack method under a passive threat model on three ASR models: Wav2Vec2, HuBERT, and Whisper. The attack operates solely on weight differentials without access to raw speech from target speakers. We demonstrate attack feasibility on sensitive demographic and clinical attributes: gender, age, accent, emotion, and dysarthria. Our findings indicate that attributes that are underrepresented or absent in the pre-training data are more vulnerable to such inference attacks. In particular, information about accents can be reliably inferred from all models. Our findings expose previously undocumented vulnerabilities in federated ASR models and offer insights towards improved security.",
    "summary": "",
    "translation": "联邦语音模型中的个人属性泄露",
    "relevance_score": 1,
    "reasoning": "该论文涉及联邦学习和隐私泄露问题，这属于明确排除的隐私和安全相关主题。虽然提到了语音模型，但论文焦点是隐私泄露而非语音技术在推荐/搜索/广告中的应用，因此与当前关注点完全不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13351v1": {
    "title": "Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise LLM Systems",
    "url": "https://www.alphaxiv.org/abs/2510.13351v1",
    "arxiv_id": "2510.13351v1",
    "authors": "Karthik Avinash, Nikhil Pareek, Rishav Hada",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 09:40:24",
    "ori_summary": "The increasing deployment of Large Language Models (LLMs) across enterprise and mission-critical domains has underscored the urgent need for robust guardrailing systems that ensure safety, reliability, and compliance. Existing solutions often struggle with real-time oversight, multi-modal data handling, and explainability -- limitations that hinder their adoption in regulated environments. Existing guardrails largely operate in isolation, focused on text alone making them inadequate for multi-modal, production-scale environments. We introduce Protect, natively multi-modal guardrailing model designed to operate seamlessly across text, image, and audio inputs, designed for enterprise-grade deployment. Protect integrates fine-tuned, category-specific adapters trained via Low-Rank Adaptation (LoRA) on an extensive, multi-modal dataset covering four safety dimensions: toxicity, sexism, data privacy, and prompt injection. Our teacher-assisted annotation pipeline leverages reasoning and explanation traces to generate high-fidelity, context-aware labels across modalities. Experimental results demonstrate state-of-the-art performance across all safety dimensions, surpassing existing open and proprietary models such as WildGuard, LlamaGuard-4, and GPT-4.1. Protect establishes a strong foundation for trustworthy, auditable, and production-ready safety systems capable of operating across text, image, and audio modalities.",
    "summary": "",
    "translation": "Protect：面向可信企业大语言模型系统的鲁棒护栏技术栈",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于企业级LLM系统的护栏技术和可信性保障，属于安全、隐私和伦理范畴，这些主题在无关主题列表中明确排除。论文标题未提及任何与推荐系统、搜索、广告相关的技术或应用，也没有涉及LLM核心架构、Transformer效率或异构数据建模等关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13344v1": {
    "title": "UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE",
    "url": "https://www.alphaxiv.org/abs/2510.13344v1",
    "arxiv_id": "2510.13344v1",
    "authors": "Zhenyu Liu, Yunxin Li, Xuanyu Zhang, Qixun Teng, Shenyuan Jiang, Xinyu Chen, Haoyuan Shi, Jinchao Li, Qi Wang, Haolan Chen, Fanbo Meng, Mingjun Zhao, Yu Xu, Yancheng He, Baotian Hu, Min Zhang",
    "categories": "cs.SD, cs.CL",
    "pub_date": "2025-10-15 09:30:25",
    "ori_summary": "Recent advances in unified multimodal models indicate a clear trend towards comprehensive content generation. However, the auditory domain remains a significant challenge, with music and speech often developed in isolation, hindering progress towards universal audio synthesis. This separation stems from inherent task conflicts and severe data imbalances, which impede the development of a truly unified audio generation model. To address this challenge, we propose UniMoE-Audio, a unified speech and music generation model within a novel Dynamic-Capacity Mixture-of-Experts (MoE) framework. Architecturally, UniMoE-Audio introduces a Top-P routing strategy for dynamic expert number allocation, and a hybrid expert design comprising routed experts for domain-specific knowledge, shared experts for domain-agnostic features, and null experts for adaptive computation skipping. To tackle data imbalance, we introduce a three-stage training curriculum: 1) Independent Specialist Training leverages original datasets to instill domain-specific knowledge into each \"proto-expert\" without interference; 2) MoE Integration and Warmup incorporates these specialists into the UniMoE-Audio architecture, warming up the gate module and shared expert using a subset of balanced dataset; and 3) Synergistic Joint Training trains the entire model end-to-end on the fully balanced dataset, fostering enhanced cross-domain synergy. Extensive experiments show that UniMoE-Audio not only achieves state-of-the-art performance on major speech and music generation benchmarks, but also demonstrates superior synergistic learning, mitigating the performance degradation typically seen in naive joint training. Our findings highlight the substantial potential of specialized MoE architecture and curated training strategies in advancing the field of universal audio generation. Homepage: https://mukioxun.github.io/Uni-MoE-site/home.html",
    "summary": "",
    "translation": "UniMoE-Audio：基于动态容量专家混合模型的统一语音与音乐生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注语音和音乐生成的特定领域应用，属于音频生成范畴。虽然涉及MoE（专家混合）这一Transformer架构的扩展技术，但其应用场景局限于音频生成，与推荐系统、搜索或广告的核心领域缺乏直接关联，也没有明确的潜在应用路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13341v1": {
    "title": "Are Proverbs the New Pythian Oracles? Exploring Sentiment in Greek Sayings",
    "url": "https://www.alphaxiv.org/abs/2510.13341v1",
    "arxiv_id": "2510.13341v1",
    "authors": "Katerina Korre, John Pavlopoulos",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 09:26:52",
    "ori_summary": "Proverbs are among the most fascinating linguistic phenomena that transcend cultural and linguistic boundaries. Yet, much of the global landscape of proverbs remains underexplored, as many cultures preserve their traditional wisdom within their own communities due to the oral tradition of the phenomenon. Taking advantage of the current advances in Natural Language Processing (NLP), we focus on Greek proverbs, analyzing their sentiment. Departing from an annotated dataset of Greek proverbs, we expand it to include local dialects, effectively mapping the annotated sentiment. We present (1) a way to exploit LLMs in order to perform sentiment classification of proverbs, (2) a map of Greece that provides an overview of the distribution of sentiment, (3) a combinatory analysis in terms of the geographic position, dialect, and topic of proverbs. Our findings show that LLMs can provide us with an accurate enough picture of the sentiment of proverbs, especially when approached as a non-conventional sentiment polarity task. Moreover, in most areas of Greece negative sentiment is more prevalent.",
    "summary": "",
    "translation": "谚语是新的皮提亚神谕吗？探索希腊谚语中的情感",
    "relevance_score": 1,
    "reasoning": "该论文研究希腊谚语的情感分析，属于语言学和文化研究范畴，与推荐系统、搜索或广告的核心技术进展无关。论文内容不涉及LLM技术、Transformer架构创新，也没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13334v1": {
    "title": "Taming the Fragility of KV Cache Eviction in LLM Inference",
    "url": "https://www.alphaxiv.org/abs/2510.13334v1",
    "arxiv_id": "2510.13334v1",
    "authors": "Yuan Feng, Haoyu Guo, JunLin Lv, S. Kevin Zhou, Xike Xie",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 09:18:58",
    "ori_summary": "Large language models have revolutionized natural language processing, yet their deployment remains hampered by the substantial memory and runtime overhead of the transformer's Key-Value cache. To mitigate this, recent methods employ a scoring-aggregation framework to evict unimportant cache entries, based on the stability assumption-that a fixed subset of entries remains consistently important during generation. However, prior work has largely focused on refining importance indicators for scoring, while defaulting to mean aggregation due to a faithful trust in the stability assumption. In this work, we argue that this underlying assumption is inherently fragile, making mean aggregation highly vulnerable in extreme cases. To counter this, we propose a simple yet elegant defensive aggregation strategy: a two-step, linear-time approach that controls worst-case risk, thereby defending against extreme cases with negligible computational overhead. Embodying this strategy, we propose a novel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV, which incorporates layer-wise budget allocation. Across seven task domains (18 datasets), our methods reduce generation quality loss by 2.3x and 4.3x respectively, versus the strongest baseline under a 20% cache size. These results set new performance benchmarks and pioneer a promising direction for optimizing cache eviction against underlying fragility through worst-case risk management. Our code is available at https://github.com/FFY0/DefensiveKV.",
    "summary": "论文研究LLM推理中KV缓存驱逐的脆弱性问题，核心思想是通过控制最坏情况风险的两步防御性聚合策略来替代传统的均值聚合，提升缓存管理的鲁棒性。",
    "translation": "驯服LLM推理中KV缓存驱逐的脆弱性",
    "relevance_score": 8,
    "reasoning": "该论文聚焦于LLM推理效率的核心技术挑战——KV缓存管理，这直接属于'Enabling LLM Tech'范畴。KV缓存优化对于大规模推荐和搜索系统至关重要，能够显著降低推理延迟和计算成本，提升在线服务性能。高效的KV缓存管理可以支持更复杂的多轮对话推荐和个性化搜索交互。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对Transformer架构的KV缓存效率问题，提出了防御性聚合策略来优化LLM推理，这对搜索和推荐系统的实时性能至关重要。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13329v1": {
    "title": "Embedding-Based Context-Aware Reranker",
    "url": "https://www.alphaxiv.org/abs/2510.13329v1",
    "arxiv_id": "2510.13329v1",
    "authors": "Ye Yuan, Mohammad Amin Shabani, Siqi Liu",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 09:14:04",
    "ori_summary": "Retrieval-Augmented Generation (RAG) systems rely on retrieving relevant evidence from a corpus to support downstream generation. The common practice of splitting a long document into multiple shorter passages enables finer-grained and targeted information retrieval. However, it also introduces challenges when a correct retrieval would require inference across passages, such as resolving coreference, disambiguating entities, and aggregating evidence scattered across multiple sources. Many state-of-the-art (SOTA) reranking methods, despite utilizing powerful large pretrained language models with potentially high inference costs, still neglect the aforementioned challenges. Therefore, we propose Embedding-Based Context-Aware Reranker (EBCAR), a lightweight reranking framework operating directly on embeddings of retrieved passages with enhanced cross-passage understandings through the structural information of the passages and a hybrid attention mechanism, which captures both high-level interactions across documents and low-level relationships within each document. We evaluate EBCAR against SOTA rerankers on the ConTEB benchmark, demonstrating its effectiveness for information retrieval requiring cross-passage inference and its advantages in both accuracy and efficiency.",
    "summary": "论文研究检索系统中因文档分块导致的跨段落推理挑战；核心方法是基于嵌入的轻量级重排序框架，通过结构信息和混合注意力机制增强跨段落理解。",
    "translation": "基于嵌入的情境感知重排序器",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及推荐系统和搜索中的核心排序技术，嵌入方法和情境感知都是这些领域的关键组件。基于嵌入的重排序器可以应用于搜索结果的精排阶段和推荐系统的最终排序层，通过上下文信息优化项目排序质量。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对检索系统中的重排序问题，提出轻量级框架增强跨段落理解，与搜索和推荐系统的核心需求高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13302v1": {
    "title": "LLM one-shot style transfer for Authorship Attribution and Verification",
    "url": "https://www.alphaxiv.org/abs/2510.13302v1",
    "arxiv_id": "2510.13302v1",
    "authors": "Pablo Miralles-González, Javier Huertas-Tato, Alejandro Martín, David Camacho",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 08:43:24",
    "ori_summary": "Computational stylometry analyzes writing style through quantitative patterns in text, supporting applications from forensic tasks such as identity linking and plagiarism detection to literary attribution in the humanities. Supervised and contrastive approaches rely on data with spurious correlations and often confuse style with topic. Despite their natural use in AI-generated text detection, the CLM pre-training of modern LLMs has been scarcely leveraged for general authorship problems. We propose a novel unsupervised approach based on this extensive pre-training and the in-context learning capabilities of LLMs, employing the log-probabilities of an LLM to measure style transferability from one text to another. Our method significantly outperforms LLM prompting approaches of comparable scale and achieves higher accuracy than contrastively trained baselines when controlling for topical correlations. Moreover, performance scales fairly consistently with the size of the base model and, in the case of authorship verification, with an additional mechanism that increases test-time computation; enabling flexible trade-offs between computational cost and accuracy.",
    "summary": "",
    "translation": "基于大语言模型的单样本风格迁移用于作者归属与验证",
    "relevance_score": 2,
    "reasoning": "该论文主要关注作者归属和验证任务，这属于纯NLP应用领域，与推荐系统、搜索或广告的核心技术需求关联度较低。虽然风格迁移技术可能有潜在应用，但论文标题明确指向作者识别这一特定NLP任务，而非推荐、搜索或广告场景中的用户建模或内容理解。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13293v1": {
    "title": "Mismatch Aware Guidance for Robust Emotion Control in Auto-Regressive TTS Models",
    "url": "https://www.alphaxiv.org/abs/2510.13293v1",
    "arxiv_id": "2510.13293v1",
    "authors": "Yizhou Peng, Yukun Ma, Chong Zhang, Yi-Wen Chao, Chongjia Ni, Bin Ma",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 08:37:16",
    "ori_summary": "While Text-to-Speech (TTS) systems can achieve fine-grained control over emotional expression via natural language prompts, a significant challenge emerges when the desired emotion (style prompt) conflicts with the semantic content of the text. This mismatch often results in unnatural-sounding speech, undermining the goal of achieving fine-grained emotional control. Classifier-Free Guidance (CFG) is a key technique for enhancing prompt alignment; however, its application to auto-regressive (AR) TTS models remains underexplored, which can lead to degraded audio quality. This paper directly addresses the challenge of style-content mismatch in AR TTS models by proposing an adaptive CFG scheme that adjusts to different levels of the detected mismatch, as measured using large language models or natural language inference models. This solution is based on a comprehensive analysis of CFG's impact on emotional expressiveness in state-of-the-art AR TTS models. Our results demonstrate that the proposed adaptive CFG scheme improves the emotional expressiveness of the AR TTS model while maintaining audio quality and intelligibility.",
    "summary": "",
    "translation": "自回归TTS模型中用于鲁棒情绪控制的失配感知引导",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音合成(TTS)中的情绪控制技术，属于语音处理领域。虽然提到了自回归模型，但其应用场景仅限于TTS系统，与推荐系统、搜索或广告的核心技术没有直接关联。该技术没有明显的潜力应用于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13291v1": {
    "title": "Higher Satisfaction, Lower Cost: A Technical Report on How LLMs Revolutionize Meituan's Intelligent Interaction Systems",
    "url": "https://www.alphaxiv.org/abs/2510.13291v1",
    "arxiv_id": "2510.13291v1",
    "authors": "Xuxin Cheng, Ke Zeng, Zhiquan Cao, Linyi Dai, Wenxuan Gao, Fei Han, Ai Jian, Feng Hong, Wenxing Hu, Zihe Huang, Dejian Kong, Jia Leng, Zhuoyuan Liao, Pei Liu, Jiaye Lin, Xing Ma, Jingqing Ruan, Jiaxing Song, Xiaoyu Tan, Ruixuan Xiao, Wenhui Yu, Wenyu Zhan, Haoxing Zhang, Chao Zhou, Hao Zhou, Shaodong Zheng, Ruinian Chen, Siyuan Chen, Ziyang Chen, Yiwen Dong, Yaoyou Fan, Yangyi Fang, Yang Gan, Shiguang Guo, Qi He, Chaowen Hu, Binghui Li, Dailin Li, Xiangyu Li, Yan Li, Chengjian Liu, Xiangfeng Liu, Jiahui Lv, Qiao Ma, Jiang Pan, Cong Qin, Chenxing Sun, Wen Sun, Zhonghui Wang, Abudukelimu Wuerkaixi, Xin Yang, Fangyi Yuan, Yawen Zhu, Tianyi Zhai, Jie Zhang, Runlai Zhang, Yao Xu, Yiran Zhao, Yifan Wang, Xunliang Cai, Yangen Hu, Cao Liu, Lu Pan, Xiaoli Wang, Bo Xiao, Wenyuan Yao, Qianlin Zhou, Benchang Zhu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 08:35:51",
    "ori_summary": "Enhancing customer experience is essential for business success, particularly as service demands grow in scale and complexity. Generative artificial intelligence and Large Language Models (LLMs) have empowered intelligent interaction systems to deliver efficient, personalized, and 24/7 support. In practice, intelligent interaction systems encounter several challenges: (1) Constructing high-quality data for cold-start training is difficult, hindering self-evolution and raising labor costs. (2) Multi-turn dialogue performance remains suboptimal due to inadequate intent understanding, rule compliance, and solution extraction. (3) Frequent evolution of business rules affects system operability and transferability, constraining low-cost expansion and adaptability. (4) Reliance on a single LLM is insufficient in complex scenarios, where the absence of multi-agent frameworks and effective collaboration undermines process completeness and service quality. (5) The open-domain nature of multi-turn dialogues, lacking unified golden answers, hampers quantitative evaluation and continuous optimization. To address these challenges, we introduce WOWService, an intelligent interaction system tailored for industrial applications. With the integration of LLMs and multi-agent architectures, WOWService enables autonomous task management and collaborative problem-solving. Specifically, WOWService focuses on core modules including data construction, general capability enhancement, business scenario adaptation, multi-agent coordination, and automated evaluation. Currently, WOWService is deployed on the Meituan App, achieving significant gains in key metrics, e.g., User Satisfaction Metric 1 (USM 1) -27.53% and User Satisfaction Metric 2 (USM 2) +25.51%, demonstrating its effectiveness in capturing user needs and advancing personalized service.",
    "summary": "该论文研究工业级智能交互系统在多轮对话、冷启动数据构建和业务规则频繁演进等场景下的挑战。核心方法是集成LLM和多智能体架构，通过数据构建、通用能力增强、业务场景适应、多智能体协调和自动化评估等模块实现自主任务管理和协同问题解决。",
    "translation": "更高满意度，更低成本：关于大语言模型如何革新美团智能交互系统的技术报告",
    "relevance_score": 9,
    "reasoning": "该论文直接应用LLM技术于美团智能交互系统，属于'直接LLM应用'范畴。智能交互系统在搜索、推荐和广告领域具有广泛应用，如智能客服、个性化推荐对话等，能够显著提升用户体验和系统效率。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用LLM技术构建智能交互系统，解决推荐和搜索领域中的多轮对话、业务规则适应等核心问题，与用户关注的直接LLM应用和推荐系统进展高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13285v1": {
    "title": "In-Distribution Steering: Balancing Control and Coherence in Language Model Generation",
    "url": "https://www.alphaxiv.org/abs/2510.13285v1",
    "arxiv_id": "2510.13285v1",
    "authors": "Arthur Vogels, Benjamin Wong, Yann Choho, Annabelle Blangero, Milan Bhan",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 08:31:37",
    "ori_summary": "Activation steering methods control large language model (LLM) behavior by modifying internal activations at inference time. However, most existing activation steering methods rely on a fixed steering strength, leading to either insufficient control or unadapted intervention that degrades text plausibility and coherence. We introduce In-Distribution Steering (IDS), a novel method that adapts steering strength based on the input data distribution in representation space. IDS dynamically adjusts interventions according to how far a given input lies within the distribution, enabling adaptive intervention and generation stability during text generation. Experiments demonstrate that IDS achieves strong accuracy on classification tasks while producing coherent text without collapse, making IDS particularly well suited for real-world applications.",
    "summary": "",
    "translation": "分布内引导：在语言模型生成中平衡控制与连贯性",
    "relevance_score": 6,
    "reasoning": "该论文涉及语言模型生成控制技术，属于'Enabling LLM Tech'范畴。在推荐系统和搜索领域，这种控制技术可以用于生成更符合业务需求的内容（如个性化推荐理由、搜索摘要），同时保持内容的连贯性和质量，对提升用户体验有直接应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.13281v1": {
    "title": "Two Heads Are Better Than One: Audio-Visual Speech Error Correction with Dual Hypotheses",
    "url": "https://www.alphaxiv.org/abs/2510.13281v1",
    "arxiv_id": "2510.13281v1",
    "authors": "Sungnyun Kim, Kangwook Jang, Sungwoo Cho, Joon Son Chung, Hoirin Kim, Se-Young Yun",
    "categories": "eess.AS, cs.CL, cs.LG",
    "pub_date": "2025-10-15 08:27:16",
    "ori_summary": "This paper introduces a new paradigm for generative error correction (GER) framework in audio-visual speech recognition (AVSR) that reasons over modality-specific evidences directly in the language space. Our framework, DualHyp, empowers a large language model (LLM) to compose independent N-best hypotheses from separate automatic speech recognition (ASR) and visual speech recognition (VSR) models. To maximize the effectiveness of DualHyp, we further introduce RelPrompt, a noise-aware guidance mechanism that provides modality-grounded prompts to the LLM. RelPrompt offers the temporal reliability of each modality stream, guiding the model to dynamically switch its focus between ASR and VSR hypotheses for an accurate correction. Under various corruption scenarios, our framework attains up to 57.7% error rate gain on the LRS2 benchmark over standard ASR baseline, contrary to single-stream GER approaches that achieve only 10% gain. To facilitate research within our DualHyp framework, we release the code and the dataset comprising ASR and VSR hypotheses at https://github.com/sungnyun/dualhyp.",
    "summary": "",
    "translation": "双头胜于单头：基于双假设的视听语音纠错",
    "relevance_score": 1,
    "reasoning": "该论文专注于视听语音纠错，属于纯语音处理领域，与推荐系统、搜索或广告没有明显关联。论文内容涉及多模态融合（音频和视觉），但这种融合仅限于语音处理应用，无法转化为推荐系统、搜索或广告中的异构数据处理需求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13276v1": {
    "title": "MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13276v1",
    "arxiv_id": "2510.13276v1",
    "authors": "Keyan Zhou, Zecheng Tang, Lingfeng Ming, Guanghao Zhou, Qiguang Chen, Dan Qiao, Zheming Yang, Libo Qin, Minghui Qiu, Juntao Li, Min Zhang",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-15 08:22:03",
    "ori_summary": "The rapid advancement of large vision language models (LVLMs) has led to a significant expansion of their context windows. However, an extended context window does not guarantee the effective utilization of the context, posing a critical challenge for real-world applications. Current evaluations of such long-context faithfulness are predominantly focused on the text-only domain, while multimodal assessments remain limited to short contexts. To bridge this gap, we introduce MMLongCite, a comprehensive benchmark designed to evaluate the fidelity of LVLMs in long-context scenarios. MMLongCite comprises 8 distinct tasks spanning 6 context length intervals and incorporates diverse modalities, including text, images, and videos. Our evaluation of state-of-the-art LVLMs reveals their limited faithfulness in handling long multimodal contexts. Furthermore, we provide an in-depth analysis of how context length and the position of crucial content affect the faithfulness of these models.",
    "summary": "",
    "translation": "MMLongCite：用于评估长上下文视觉语言模型保真度的基准",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型的评估基准，属于纯粹的评估和基准测试范畴，这在无关主题中明确排除。虽然提到了长上下文处理，但核心焦点是评估保真度而非技术进展本身，且没有明确说明在推荐系统、搜索或广告中的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13272v1": {
    "title": "Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2510.13272v1",
    "arxiv_id": "2510.13272v1",
    "authors": "Zhichao Xu, Zongyu Wu, Yun Zhou, Aosong Feng, Kang Zhou, Sangmin Woo, Kiran Ramnath, Yijun Tian, Xuan Qi, Weikang Qiu, Lin Lee Cheong, Haibo Ding",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 08:17:52",
    "ori_summary": "Inspired by the success of reinforcement learning (RL) in Large Language Model (LLM) training for domains like math and code, recent works have begun exploring how to train LLMs to use search engines more effectively as tools for retrieval-augmented generation. Although these methods achieve performance improvement across QA benchmarks, many prioritize final answer correctness while overlooking the quality of intermediate reasoning steps, which may lead to chain-of-thought unfaithfulness. In this paper, we first introduce a comprehensive evaluation framework for evaluating RL-based search agents, covering three distinct faithfulness metrics: information-think faithfulness, think-answer faithfulness, and think-search faithfulness. Our evaluations reveal that a prototypical RL-based search agent, Search-R1, has significant room for improvement in this regard. To foster faithful reasoning, we introduce VERITAS (Verifying Entailed Reasoning through Intermediate Traceability in Agentic Search), a novel framework that integrates fine-grained faithfulness rewards into the reinforcement learning process. Our experiments show that models trained with VERITAS not only significantly improve reasoning faithfulness, but also achieve comparable task performance across seven QA benchmarks.",
    "summary": "",
    "translation": "超越正确性：在检索增强生成中奖励忠实推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注检索增强生成(RAG)中的忠实推理评估，这属于纯粹的LLM评估和可靠性问题，与我的核心关注点无关。虽然RAG技术本身在搜索中有应用，但本文聚焦于评估指标而非技术改进，且没有明确涉及推荐系统、广告或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13271v1": {
    "title": "Do You Get the Hint? Benchmarking LLMs on the Board Game Concept",
    "url": "https://www.alphaxiv.org/abs/2510.13271v1",
    "arxiv_id": "2510.13271v1",
    "authors": "Ine Gevers, Walter Daelemans",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 08:17:25",
    "ori_summary": "Large language models (LLMs) have achieved striking successes on many benchmarks, yet recent studies continue to expose fundamental weaknesses. In particular, tasks that require abstract reasoning remain challenging, often because they use representations such as grids, symbols, or visual patterns that differ from the natural language data LLMs are trained on. In this paper, we introduce Concept, a simple word-guessing board game, as a benchmark for probing abductive reasoning in a representation that is much closer to LLM pre-training data: natural language. Our results show that this game, easily solved by humans (with a success rate of over 90\\%), is still very challenging for state-of-the-art LLMs (no model exceeds 40\\% success rate). Specifically, we observe that LLMs struggle with interpreting other players' strategic intents, and with correcting initial hypotheses given sequential information updates. In addition, we extend the evaluation across multiple languages, and find that the LLM performance drops further in lower-resource languages (Dutch, French, and Spanish) compared to English.",
    "summary": "",
    "translation": "你理解提示了吗？在棋盘游戏概念上对大型语言模型进行基准测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM在棋盘游戏概念理解上的基准测试，这属于纯粹的NLP评估基准范畴，与推荐系统、搜索或广告的核心技术进展无关。虽然涉及LLM，但缺乏明确的RecSys/Search/Ads应用潜力，更偏向于通用语言理解能力的评估。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13255v1": {
    "title": "Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain",
    "url": "https://www.alphaxiv.org/abs/2510.13255v1",
    "arxiv_id": "2510.13255v1",
    "authors": "Jingmin An, Yilong Song, Ruolin Yang, Nai Ding, Lingxi Lu, Yuxuan Wang, Wei Wang, Chu Zhuang, Qian Wang, Fang Fang",
    "categories": "cs.CL, cs.NE",
    "pub_date": "2025-10-15 08:04:49",
    "ori_summary": "Large Language Models (LLMs) demonstrate human-level or even superior language abilities, effectively modeling syntactic structures, yet the specific computational modules responsible remain unclear. A key question is whether LLM behavioral capabilities stem from mechanisms akin to those in the human brain. To address these questions, we introduce the Hierarchical Frequency Tagging Probe (HFTP), a tool that utilizes frequency-domain analysis to identify neuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP) neurons) and cortical regions (via intracranial recordings) encoding syntactic structures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama 2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human brain relies on distinct cortical regions for different syntactic levels. Representational similarity analysis reveals a stronger alignment between LLM representations and the left hemisphere of the brain (dominant in language processing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows greater brain similarity than Gemma, while Llama 3.1 shows less alignment with the brain compared to Llama 2. These findings offer new insights into the interpretability of LLM behavioral improvements, raising questions about whether these advancements are driven by human-like or non-human-like mechanisms, and establish HFTP as a valuable tool bridging computational linguistics and cognitive neuroscience. This project is available at https://github.com/LilTiger/HFTP.",
    "summary": "",
    "translation": "层次频率标记探针（HFTP）：一种研究大型语言模型和人脑中句法结构表征的统一方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM中句法结构的神经表征分析，属于纯NLP基础研究范畴。虽然涉及LLM内部机制分析，但聚焦于语言学结构和人脑对比，与推荐系统、搜索或广告的实际应用场景缺乏直接关联，对核心领域进展的贡献有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13220v1": {
    "title": "EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems",
    "url": "https://www.alphaxiv.org/abs/2510.13220v1",
    "arxiv_id": "2510.13220v1",
    "authors": "Yufei He, Juncheng Liu, Yue Liu, Yibo Li, Tri Cao, Zhiyuan Hu, Xinxing Xu, Bryan Hooi",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-15 07:16:28",
    "ori_summary": "A fundamental limitation of current AI agents is their inability to learn complex skills on the fly at test time, often behaving like \"clever but clueless interns\" in novel environments. This severely limits their practical utility. To systematically measure and drive progress on this challenge, we first introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a new evaluation setup where an agent must play the same game for several consecutive episodes, attempting to improve its performance from one episode to the next. On J-TTL, we find that existing adaptation methods like reflection, memory, or reinforcement learning struggle. To address the challenges posed by our benchmark, we present EvoTest, an evolutionary test-time learning framework that improves an agent without any fine-tuning or gradients-by evolving the entire agentic system after every episode. EvoTest has two roles: the Actor Agent, which plays the game, and the Evolver Agent, which analyzes the episode transcript to propose a revised configuration for the next run. This configuration rewrites the prompt, updates memory by logging effective state-action choices, tunes hyperparameters, and learns the tool-use routines. On our J-TTL benchmark, EvoTest consistently increases performance, outperforming not only reflection and memory-only baselines but also more complex online fine-tuning methods. Notably, our method is the only one capable of winning two games (Detective and Library), while all baselines fail to win any.",
    "summary": "",
    "translation": "EvoTest：用于自改进智能体系统的进化测试时学习",
    "relevance_score": 2,
    "reasoning": "该论文主要关注智能体系统的测试时学习和自改进能力，这属于通用AI系统范畴，而非专门针对推荐、搜索或广告领域的核心进展。虽然测试时学习技术可能对在线学习系统有潜在价值，但论文标题未表明与推荐系统、搜索或广告的具体关联，也没有明确涉及LLM、Transformer架构或异构数据建模等焦点领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13215v1": {
    "title": "Personalized Learning Path Planning with Goal-Driven Learner State Modeling",
    "url": "https://www.alphaxiv.org/abs/2510.13215v1",
    "arxiv_id": "2510.13215v1",
    "authors": "Joy Jia Yin Lim, Ye He, Jifan Yu, Xin Cong, Daniel Zhang-Li, Zhiyuan Liu, Huiqin Liu, Lei Hou, Juanzi Li, Bin Xu",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-15 06:59:49",
    "ori_summary": "Personalized Learning Path Planning (PLPP) aims to design adaptive learning paths that align with individual goals. While large language models (LLMs) show potential in personalizing learning experiences, existing approaches often lack mechanisms for goal-aligned planning. We introduce Pxplore, a novel framework for PLPP that integrates a reinforcement-based training paradigm and an LLM-driven educational architecture. We design a structured learner state model and an automated reward function that transforms abstract objectives into computable signals. We train the policy combining supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), and deploy it within a real-world learning platform. Extensive experiments validate Pxplore's effectiveness in producing coherent, personalized, and goal-driven learning paths. We release our code and dataset to facilitate future research.",
    "summary": "",
    "translation": "基于目标驱动学习者状态建模的个性化学习路径规划",
    "relevance_score": 3,
    "reasoning": "虽然该论文涉及个性化建模和序列规划，但其核心应用领域是教育技术而非推荐系统、搜索或广告。论文中的学习者状态建模和路径规划技术可能对推荐系统中的用户状态建模和序列推荐有启发意义，但这种迁移应用并不直接明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13211v1": {
    "title": "A fully automated and scalable Parallel Data Augmentation for Low Resource Languages using Image and Text Analytics",
    "url": "https://www.alphaxiv.org/abs/2510.13211v1",
    "arxiv_id": "2510.13211v1",
    "authors": "Prawaal Sharma, Navneet Goyal, Poonam Goyal, Vishnupriyan R",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 06:57:23",
    "ori_summary": "Linguistic diversity across the world creates a disparity with the availability of good quality digital language resources thereby restricting the technological benefits to majority of human population. The lack or absence of data resources makes it difficult to perform NLP tasks for low-resource languages. This paper presents a novel scalable and fully automated methodology to extract bilingual parallel corpora from newspaper articles using image and text analytics. We validate our approach by building parallel data corpus for two different language combinations and demonstrate the value of this dataset through a downstream task of machine translation and improve over the current baseline by close to 3 BLEU points.",
    "summary": "",
    "translation": "基于图像和文本分析的完全自动化可扩展低资源语言并行数据增强方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注低资源语言的数据增强技术，这属于通用的NLP数据预处理范畴。虽然提到了图像和文本分析，但核心焦点是语言数据增强而非推荐系统、搜索或广告的直接应用。论文可能对多语言搜索有间接价值，但与当前关注的核心领域进展、Transformer架构改进或LLM直接应用关联度很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13202v1": {
    "title": "LLM-Guided Synthetic Augmentation (LGSA) for Mitigating Bias in AI Systems",
    "url": "https://www.alphaxiv.org/abs/2510.13202v1",
    "arxiv_id": "2510.13202v1",
    "authors": "Sai Suhruth Reddy Karri, Yashwanth Sai Nallapuneni, Laxmi Narasimha Reddy Mallireddy, Gopichand G",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 06:42:35",
    "ori_summary": "Bias in AI systems, especially those relying on natural language data, raises ethical and practical concerns. Underrepresentation of certain groups often leads to uneven performance across demographics. Traditional fairness methods, such as pre-processing, in-processing, and post-processing, depend on protected-attribute labels, involve accuracy-fairness trade-offs, and may not generalize across datasets. To address these challenges, we propose LLM-Guided Synthetic Augmentation (LGSA), which uses large language models to generate counterfactual examples for underrepresented groups while preserving label integrity. We evaluated LGSA on a controlled dataset of short English sentences with gendered pronouns, professions, and binary classification labels. Structured prompts were used to produce gender-swapped paraphrases, followed by quality control including semantic similarity checks, attribute verification, toxicity screening, and human spot checks. The augmented dataset expanded training coverage and was used to train a classifier under consistent conditions. Results show that LGSA reduces performance disparities without compromising accuracy. The baseline model achieved 96.7 percent accuracy with a 7.2 percent gender bias gap. Simple swap augmentation reduced the gap to 0.7 percent but lowered accuracy to 95.6 percent. LGSA achieved 99.1 percent accuracy with a 1.9 percent bias gap, improving performance on female-labeled examples. These findings demonstrate that LGSA is an effective strategy for bias mitigation, enhancing subgroup balance while maintaining high task accuracy and label fidelity.",
    "summary": "",
    "translation": "用于缓解AI系统偏差的LLM引导合成增强",
    "relevance_score": 3,
    "reasoning": "该论文主要关注偏差缓解，这属于公平性范畴，属于明确排除的非技术性话题。虽然提到了LLM和合成数据增强技术，但其核心应用方向（偏差缓解）与当前关注的推荐系统、搜索或广告的核心技术进展无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13197v1": {
    "title": "Text Anomaly Detection with Simplified Isolation Kernel",
    "url": "https://www.alphaxiv.org/abs/2510.13197v1",
    "arxiv_id": "2510.13197v1",
    "authors": "Yang Cao, Sikun Yang, Yujiu Yang, Lianyong Qi, Ming Liu",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 06:35:54",
    "ori_summary": "Two-step approaches combining pre-trained large language model embeddings and anomaly detectors demonstrate strong performance in text anomaly detection by leveraging rich semantic representations. However, high-dimensional dense embeddings extracted by large language models pose challenges due to substantial memory requirements and high computation time. To address this challenge, we introduce the Simplified Isolation Kernel (SIK), which maps high-dimensional dense embeddings to lower-dimensional sparse representations while preserving crucial anomaly characteristics. SIK has linear time complexity and significantly reduces space complexity through its innovative boundary-focused feature mapping. Experiments across 7 datasets demonstrate that SIK achieves better detection performance than 11 state-of-the-art (SOTA) anomaly detection algorithms while maintaining computational efficiency and low memory cost. All code and demonstrations are available at https://github.com/charles-cao/SIK.",
    "summary": "",
    "translation": "基于简化隔离核的文本异常检测",
    "relevance_score": 2,
    "reasoning": "该论文专注于文本异常检测这一基础NLP任务，虽然异常检测在推荐系统中可能有边缘应用（如检测异常用户行为或内容），但论文标题明确限定于文本数据且未提及任何与推荐系统、搜索或广告相关的应用场景。隔离核方法主要是一种通用的异常检测技术，缺乏明确的Transformer架构改进或LLM技术应用，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13194v1": {
    "title": "StressTransfer: Stress-Aware Speech-to-Speech Translation with Emphasis Preservation",
    "url": "https://www.alphaxiv.org/abs/2510.13194v1",
    "arxiv_id": "2510.13194v1",
    "authors": "Xi Chen, Yuchen Song, Satoshi Nakamura",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 06:32:24",
    "ori_summary": "We propose a stress-aware speech-to-speech translation (S2ST) system that preserves word-level emphasis by leveraging LLMs for cross-lingual emphasis conversion. Our method translates source-language stress into target-language tags that guide a controllable TTS model. To overcome data scarcity, we developed a pipeline to automatically generate aligned training data and introduce the \"LLM-as-Judge\" for evaluation. Experiments show our approach substantially outperforms baselines in preserving emphasis while maintaining comparable translation quality, speaker intent, and naturalness. Our work highlights the importance of prosody in translation and provides an effective, data-efficient solution for preserving paralinguistic cues in S2ST.",
    "summary": "",
    "translation": "StressTransfer：具有重音保持功能的压力感知语音到语音翻译",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音到语音翻译中的重音保留技术，这属于语音处理领域，与推荐系统、搜索或广告的核心技术栈没有直接关联。虽然语音技术在某些边缘场景可能有应用，但该论文没有展示在RecSys/Search/Ads领域的明确应用潜力，且不属于指定的关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13191v1": {
    "title": "Grounding Long-Context Reasoning with Contextual Normalization for Retrieval-Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2510.13191v1",
    "arxiv_id": "2510.13191v1",
    "authors": "Jiamin Chen, Yuchen Li, Xinyu Ma, Xinran Chen, Xiaokun Zhang, Shuaiqiang Wang, Chen Ma, Dawei Yin",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 06:28:25",
    "ori_summary": "Retrieval-Augmented Generation (RAG) has become an essential approach for extending the reasoning and knowledge capacity of large language models (LLMs). While prior research has primarily focused on retrieval quality and prompting strategies, the influence of how the retrieved documents are framed, i.e., context format, remains underexplored. We show that seemingly superficial choices, such as delimiters or structural markers in key-value extraction, can induce substantial shifts in accuracy and stability, even when semantic content is identical. To systematically investigate this effect, we design controlled experiments that vary context density, delimiter styles, and positional placement, revealing the underlying factors that govern performance differences. Building on these insights, we introduce Contextual Normalization, a lightweight strategy that adaptively standardizes context representations before generation. Extensive experiments on both controlled and real-world RAG benchmarks across diverse settings demonstrate that the proposed strategy consistently improves robustness to order variation and strengthens long-context utilization. These findings underscore that reliable RAG depends not only on retrieving the right content, but also on how that content is presented, offering both new empirical evidence and a practical technique for better long-context reasoning.",
    "summary": "论文研究检索增强生成中上下文格式对模型推理稳定性的影响问题，核心思想是通过上下文归一化方法自适应标准化检索文档的表示格式，以提升长上下文推理的鲁棒性。",
    "translation": "基于上下文归一化的检索增强生成长文本推理",
    "relevance_score": 8,
    "reasoning": "该论文聚焦于检索增强生成(RAG)技术，这是搜索和推荐系统的核心使能技术。上下文归一化方法可以显著提升长文本推理性能，这对于处理用户历史行为序列、多模态上下文等异构数据的推荐和搜索系统具有直接应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接研究RAG系统中上下文格式对推理性能的影响，并提出标准化方法提升长上下文利用，与推荐/搜索系统中内容呈现优化的核心问题高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.13190v1": {
    "title": "SHIELD: Classifier-Guided Prompting for Robust and Safer LVLMs",
    "url": "https://www.alphaxiv.org/abs/2510.13190v1",
    "arxiv_id": "2510.13190v1",
    "authors": "Juan Ren, Mark Dras, Usman Naseem",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 06:27:46",
    "ori_summary": "Large Vision-Language Models (LVLMs) unlock powerful multimodal reasoning but also expand the attack surface, particularly through adversarial inputs that conceal harmful goals in benign prompts. We propose SHIELD, a lightweight, model-agnostic preprocessing framework that couples fine-grained safety classification with category-specific guidance and explicit actions (Block, Reframe, Forward). Unlike binary moderators, SHIELD composes tailored safety prompts that enforce nuanced refusals or safe redirection without retraining. Across five benchmarks and five representative LVLMs, SHIELD consistently lowers jailbreak and non-following rates while preserving utility. Our method is plug-and-play, incurs negligible overhead, and is easily extendable to new attack types -- serving as a practical safety patch for both weakly and strongly aligned LVLMs.",
    "summary": "",
    "translation": "SHIELD：用于鲁棒且更安全大型视觉语言模型的分类器引导提示",
    "relevance_score": 2,
    "reasoning": "该论文主要关注大型视觉语言模型的安全性和鲁棒性，属于纯粹的视觉-语言模态研究。虽然提到了提示工程，但其应用场景是通用安全防护而非推荐、搜索或广告领域。该技术缺乏明确的RecSys/Search/Ads应用潜力，主要解决的是多模态模型的安全问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13183v1": {
    "title": "DSCD: Large Language Model Detoxification with Self-Constrained Decoding",
    "url": "https://www.alphaxiv.org/abs/2510.13183v1",
    "arxiv_id": "2510.13183v1",
    "authors": "Ming Dong, Jinkui Zhang, Bolong Zheng, Xinhui Tu, Po Hu, Tingting He",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 06:10:47",
    "ori_summary": "Detoxification in large language models (LLMs) remains a significant research challenge. Existing decoding detoxification methods are all based on external constraints, which require additional resource overhead and lose generation fluency. This work proposes Detoxification with Self-Constrained Decoding (DSCD), a novel method for LLM detoxification without parameter fine-tuning. DSCD strengthens the inner next-token distribution of the safety layer while weakening that of hallucination and toxic layers during output generation. This effectively diminishes toxicity and enhances output safety. DSCD offers lightweight, high compatibility, and plug-and-play capabilities, readily integrating with existing detoxification methods for further performance improvement. Extensive experiments on representative open-source LLMs and public datasets validate DSCD's effectiveness, demonstrating state-of-the-art (SOTA) performance in both detoxification and generation fluency, with superior efficiency compared to existing methods. These results highlight DSCD's potential as a practical and scalable solution for safer LLM deployments.",
    "summary": "",
    "translation": "DSCD：基于自约束解码的大语言模型去毒",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM安全性和内容审核（去毒），这属于伦理和安全范畴，在无关主题中明确排除。虽然LLM技术本身可能应用于推荐系统，但该工作的核心焦点是内容安全而非推荐、搜索或广告的架构创新或性能提升。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13170v1": {
    "title": "Putting on the Thinking Hats: A Survey on Chain of Thought Fine-tuning from the Perspective of Human Reasoning Mechanism",
    "url": "https://www.alphaxiv.org/abs/2510.13170v1",
    "arxiv_id": "2510.13170v1",
    "authors": "Xiaoshu Chen, Sihang Zhou, Ke Liang, Duanyang Yuan, Haoyuan Chen, Xiaoyu Sun, Linyuan Meng, Xinwang Liu",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 05:54:13",
    "ori_summary": "Chain of thought (CoT) fine-tuning aims to endow large language models (LLMs) with reasoning capabilities by training them on curated reasoning traces. It leverages both supervised and reinforced fine-tuning to cultivate human-like reasoning skills in LLMs, including detailed planning, divergent thinking, intuitive judgment, timely reflection, internal thinking, and fact perception, etc. As CoT fine-tuning has advanced, LLMs have demonstrated substantial improvements in tasks such as mathematical reasoning and code generation. However, existing surveys about CoT fine-tuning primarily focus on technical aspects and overlook a systematic analysis from the perspective of human reasoning mechanisms. Given that the ultimate goal of CoT fine-tuning is to enable LLMs to reason like humans, it is crucial to investigate this technique through the lens of human cognition. To fill this gap, we present the first comprehensive survey of CoT fine-tuning grounded in human reasoning theory. Specifically, inspired by the well-known Six Thinking Hats framework, which systematically characterizes common human thinking modes using six metaphorical hats, we classify and examine CoT fine-tuning methods through this lens. Furthermore, building upon this theory, we outline potential directions for future research in CoT fine-tuning. In addition, we compile a comprehensive overview of existing datasets and model performances, and a real-time GitHub repository \\footnote{https://github.com/AI-Chen/Awesome-CoT-Finetuning} that continuously tracks recent advances in this area is maintained. We hope this survey will serve as a valuable resource to inspire innovation and foster progress in this rapidly evolving field.",
    "summary": "",
    "translation": "戴上思考帽：从人类推理机制视角看思维链微调综述",
    "relevance_score": 8,
    "reasoning": "该论文聚焦思维链（Chain of Thought）微调，这是LLM推理能力的核心进展。在推荐系统和搜索领域，增强的推理能力可以显著改善复杂查询理解、多步骤决策制定和用户意图推理，从而提升个性化推荐和搜索结果的相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.13166v1": {
    "title": "CoT-Evo: Evolutionary Distillation of Chain-of-Thought for Scientific Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.13166v1",
    "arxiv_id": "2510.13166v1",
    "authors": "Kehua Feng, Keyan Ding, Zhihui Zhu, Lei Liang, Qiang Zhang, Huajun Chen",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 05:29:56",
    "ori_summary": "While chain-of-thought (CoT) distillation from advanced large language models (LLMs) has proven effective in general reasoning tasks, it struggles in scientific domains where even advanced models often produce incorrect or superficial reasoning due to high complexity and specialized knowledge requirements. Directly distilling from such flawed outputs results in low-quality training data and limits the performance of smaller student models. To overcome this, we propose CoT-Evo, an evolutionary CoT distillation framework. It begins by constructing a diverse pool of reasoning trajectories from multiple LLM thinkers, enriches them with automatically retrieved domain knowledge, and iteratively refines the trajectories using novelty-driven selection, reflective recombination and mutation. The refinement is guided by a fitness function that evaluates answer correctness, coherence, and effective knowledge utilization. This results in a high-quality CoT dataset tailored for scientific reasoning. We employ this evolved dataset to fine-tune a compact model, which achieves state-of-the-art performance on scientific reasoning benchmarks. Our work establishes a scalable approach to synthesizing high-fidelity scientific reasoning data from diverse and fallible LLMs.",
    "summary": "",
    "translation": "CoT-Evo：用于科学推理的思维链进化蒸馏",
    "relevance_score": 2,
    "reasoning": "该论文专注于科学推理领域的思维链蒸馏技术，属于纯粹的NLP推理优化范畴。虽然思维链技术本身是重要的LLM能力，但论文的应用场景限定在科学推理，与推荐系统、搜索或广告的核心技术需求没有直接关联，也没有展示在这些领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13163v1": {
    "title": "A Matter of Representation: Towards Graph-Based Abstract Code Generation",
    "url": "https://www.alphaxiv.org/abs/2510.13163v1",
    "arxiv_id": "2510.13163v1",
    "authors": "Nyx Iskandar, Hisham Bedri, Andy Tsen",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 05:26:36",
    "ori_summary": "Most large language models (LLMs) today excel at generating raw, sequential code with minimal abstractions and custom structures. However, there has been little work on graph-based abstract code generation, where significant logic is encapsulated in predefined nodes and execution flow is determined by edges. This is relevant for visual programming languages, and in cases where raw source code is inaccessible to users and LLM training sets. In this work, we propose and evaluate JSON representations for graphs to enable high accuracy graph-based abstract code generation. We evaluate these representations on ScratchTest, a mini-benchmark based on our custom Python re-implementation of Scratch, which tests the LLM in code graph space. Our findings demonstrate that LLMs can indeed perform the aforementioned generation task in a single pass without relying on specialized or complex pipelines, given the correct graph representations. We also show that different representations induce significantly different accuracies, highlighting the instrumental role of representations in this generation task. All in all, this work establishes the first steps towards representation learning for graph-based abstract code generation.",
    "summary": "",
    "translation": "表示问题：面向基于图的抽象代码生成",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于代码生成领域，属于专门的编程语言处理任务，与推荐系统、搜索或广告的核心技术无关。基于图的抽象代码生成技术没有明确的路径可以应用于RecSys/Search/Ads领域，也不涉及Transformer架构效率、多模态建模或LLM在推荐搜索中的直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13161v1": {
    "title": "Mirror Speculative Decoding: Breaking the Serial Barrier in LLM Inference",
    "url": "https://www.alphaxiv.org/abs/2510.13161v1",
    "arxiv_id": "2510.13161v1",
    "authors": "Nikhil Bhendawade, Kumari Nishu, Arnav Kundu, Chris Bartels, Minsik Cho, Irina Belousova",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 05:22:57",
    "ori_summary": "Speculative decoding accelerates LLM inference by using a draft model to look ahead, but gains are capped by the cost of autoregressive draft generation: increasing draft size elevates acceptance rates but introduces additional latency overhead exacerbating the speed-accuracy tradeoff. Prior methods (Medusa, Hydra, EAGLE) partially reduce draft cost but either degrade acceptance or introduce overheads that limit scaling. We present Mirror Speculative Decoding (Mirror-SD), an inference algorithm that breaks the latency-acceptance tradeoff. Mirror-SD launches branch-complete rollouts from early-exit signals in parallel with the target model's suffix and explicitly maps computation across heterogeneous accelerators (GPU and NPU) to exploit cross-device parallelism. The draft speculates forward continuations for the target to verify, while the target simultaneously speculates correction paths for the draft, converting speculation into two complementary execution pipelines. To further cut draft latency without weakening acceptance semantics, we add speculative streaming so the draft emits multiple tokens per step. This dual strategy of parallel heterogeneous execution plus multi-token speculative streaming pushes speculative decoding toward its ideal regime of high acceptance with low overhead. On SpecBench with server-scale models from 14B to 66B parameters, Mirror-SD delivers consistent end-to-end gains, achieving 2.8x-5.8x wall-time speedups across diverse tasks and a 30% average relative improvement over the strongest baseline, EAGLE3.",
    "summary": "",
    "translation": "镜像推测解码：打破大语言模型推理中的串行障碍",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能LLM技术'范畴，专注于提升LLM推理效率的推测解码方法。在搜索、推荐和广告系统中，更快的LLM推理速度可以显著降低服务延迟，提升用户体验，并支持更复杂的实时推理任务。打破串行障碍对于大规模部署LLM驱动的推荐和搜索服务至关重要。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.13157v1": {
    "title": "Program of Thoughts for Financial Reasoning: Leveraging Dynamic In-Context Examples and Generative Retrieval",
    "url": "https://www.alphaxiv.org/abs/2510.13157v1",
    "arxiv_id": "2510.13157v1",
    "authors": "Subhendu Khatuya, Shashwat Naidu, Pawan Goyal, Niloy Ganguly",
    "categories": "cs.CE, cs.AI, cs.CL",
    "pub_date": "2025-10-15 05:16:54",
    "ori_summary": "Despite continuous advancements in the capabilities of large language models (LLMs), numerical reasoning remains a challenging area. Techniques like chain-of-thought prompting, tree-of-thought prompting, and program-of-thought prompting guide LLMs through intermediate reasoning steps. Although in-context learning with few-shot prompting has improved performance, LLMs still lag behind state-of-the-art models on financial numerical reasoning datasets such as FinQA and ConvFinQA. In this work, we introduce FINDER, a novel two-step framework, to enhance LLMs' capabilities in financial numerical reasoning. The first step utilizes a generative retriever to extract relevant facts from unstructured data, including both text and tables. This is followed by context-aware Program of Thought prompting with dynamic selection of in-context examples. Our model FINDER achieves a new state-of-the-art performance on both the FinQA and ConvFinQA datasets, surpassing previous benchmarks with execution accuracy improvements of 5.98% and 4.05%, respectively.",
    "summary": "",
    "translation": "金融推理的思维程序化：利用动态上下文示例与生成式检索",
    "relevance_score": 3,
    "reasoning": "该论文主要关注金融领域的特定推理任务，属于领域特定应用，与RecSys/Search/Ads核心领域进展无关。虽然涉及动态上下文示例和生成式检索技术，但这些技术在金融推理中的应用缺乏向推荐、搜索或广告领域迁移的明确路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13154v1": {
    "title": "I Am Aligned, But With Whom? MENA Values Benchmark for Evaluating Cultural Alignment and Multilingual Bias in LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.13154v1",
    "arxiv_id": "2510.13154v1",
    "authors": "Pardis Sadat Zahraei, Ehsaneddin Asgari",
    "categories": "cs.CL",
    "pub_date": "2025-10-15 05:10:57",
    "ori_summary": "We introduce MENAValues, a novel benchmark designed to evaluate the cultural alignment and multilingual biases of large language models (LLMs) with respect to the beliefs and values of the Middle East and North Africa (MENA) region, an underrepresented area in current AI evaluation efforts. Drawing from large-scale, authoritative human surveys, we curate a structured dataset that captures the sociocultural landscape of MENA with population-level response distributions from 16 countries. To probe LLM behavior, we evaluate diverse models across multiple conditions formed by crossing three perspective framings (neutral, personalized, and third-person/cultural observer) with two language modes (English and localized native languages: Arabic, Persian, Turkish). Our analysis reveals three critical phenomena: \"Cross-Lingual Value Shifts\" where identical questions yield drastically different responses based on language, \"Reasoning-Induced Degradation\" where prompting models to explain their reasoning worsens cultural alignment, and \"Logit Leakage\" where models refuse sensitive questions while internal probabilities reveal strong hidden preferences. We further demonstrate that models collapse into simplistic linguistic categories when operating in native languages, treating diverse nations as monolithic entities. MENAValues offers a scalable framework for diagnosing cultural misalignment, providing both empirical insights and methodological tools for developing more culturally inclusive AI.",
    "summary": "",
    "translation": "我是对齐的，但和谁对齐？用于评估LLM文化对齐与多语言偏见的中东和北非价值观基准",
    "relevance_score": 1,
    "reasoning": "该论文主要关注LLM的文化对齐和多语言偏见评估，属于伦理、公平性和评估基准范畴。这些主题明确列在无关主题中，与推荐系统、搜索或广告的核心技术进展无关。论文没有涉及任何可能应用于RecSys/Search/Ads的使能技术或直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13143v1": {
    "title": "Stable LLM Ensemble: Interaction between Example Representativeness and Diversity",
    "url": "https://www.alphaxiv.org/abs/2510.13143v1",
    "arxiv_id": "2510.13143v1",
    "authors": "Junichiro Niimi",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 04:49:23",
    "ori_summary": "Large language models (LLMs) have achieved remarkable results in wide range of domains. However, the accuracy and robustness of one-shot LLM predictions remain highly sensitive to the examples and the diversity among ensemble members. This study systematically investigates the effects of example representativeness (one-shot strategy) and output diversity (sampling temperature) on LLM ensemble performance. Two one-shot strategies are compared: centroid-based representative examples (proposed) and randomly sampled examples (baseline) and sampling temperature also is varied. The proposed approach with higher temperature setting significantly outperforms random selection by +7.6% (macro-F1) and -10.5% (RMSE). Furthermore, the proposed model exceeds 5-shot prompting by +21.1% (macro-F1) and -24.0% (RMSE). Our findings demonstrate that combining representative example selection with increased temperature provides the appropriate level of diversity to the ensemble. This work highlights the practical importance of both example selection and controlled diversity in designing effective one-shot LLM ensembles.",
    "summary": "",
    "translation": "稳定大语言模型集成：示例代表性与多样性之间的交互作用",
    "relevance_score": 4,
    "reasoning": "该论文关注LLM集成方法中的示例选择策略，属于LLM技术的基础性研究。虽然集成方法可以提升模型鲁棒性，但其在推荐系统、搜索或广告中的具体应用路径不够明确，主要解决的是通用LLM的稳定性问题而非特定领域的应用创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.13139v1": {
    "title": "Addressing the alignment problem in transportation policy making: an LLM approach",
    "url": "https://www.alphaxiv.org/abs/2510.13139v1",
    "arxiv_id": "2510.13139v1",
    "authors": "Xiaoyu Yan, Tianxing Dai, Yu, Nie",
    "categories": "cs.CY, cs.CE, cs.CL, cs.MA",
    "pub_date": "2025-10-15 04:36:38",
    "ori_summary": "A key challenge in transportation planning is that the collective preferences of heterogeneous travelers often diverge from the policies produced by model-driven decision tools. This misalignment frequently results in implementation delays or failures. Here, we investigate whether large language models (LLMs), noted for their capabilities in reasoning and simulating human decision-making, can help inform and address this alignment problem. We develop a multi-agent simulation in which LLMs, acting as agents representing residents from different communities in a city, participate in a referendum on a set of transit policy proposals. Using chain-of-thought reasoning, LLM agents provide ranked-choice or approval-based preferences, which are aggregated using instant-runoff voting (IRV) to model democratic consensus. We implement this simulation framework with both GPT-4o and Claude-3.5, and apply it for Chicago and Houston. Our findings suggest that LLM agents are capable of approximating plausible collective preferences and responding to local context, while also displaying model-specific behavioral biases and modest divergences from optimization-based benchmarks. These capabilities underscore both the promise and limitations of LLMs as tools for solving the alignment problem in transportation decision-making.",
    "summary": "",
    "translation": "解决交通政策制定中的对齐问题：一种大语言模型方法",
    "relevance_score": 2,
    "reasoning": "该论文虽然涉及LLM应用，但聚焦于交通政策制定这一特定领域，与推荐系统、搜索或广告的核心领域无关。交通政策制定不属于My Current Focus中定义的核心领域，且没有明显证据表明该方法可以迁移到RecSys/Search/Ads应用中。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13117v1": {
    "title": "On the Reasoning Abilities of Masked Diffusion Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13117v1",
    "arxiv_id": "2510.13117v1",
    "authors": "Anej Svete, Ashish Sabharwal",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-15 03:29:26",
    "ori_summary": "Masked diffusion models (MDMs) for text offer a compelling alternative to traditional autoregressive language models. Parallel generation makes them efficient, but their computational capabilities and the limitations inherent to their parallelism remain largely unexplored. To this end, we characterize what types of reasoning problems MDMs can provably solve and how efficiently. We do this by connecting MDMs to the well-understood reasoning frameworks of chain of thought (CoT) and padded looped transformers (PLTs) in the finite-precision log-width setting: We show that MDMs and polynomially-padded PLTs are, in fact, equivalent in this setting, and that MDMs can solve all problems that CoT-augmented transformers can. Moreover, we showcase classes of problems (including regular languages) for which MDMs are inherently more efficient than CoT transformers, where parallel generation allows for substantially faster reasoning.",
    "summary": "",
    "translation": "关于掩码扩散语言模型推理能力的研究",
    "relevance_score": 3,
    "reasoning": "该论文主要研究扩散语言模型的推理能力，属于核心LLM技术进展。虽然扩散模型在生成质量方面有优势，但其在推荐系统、搜索或广告中的直接应用潜力有限，因为这些领域更关注理解、匹配和排序能力，而非生成式推理。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13115v1": {
    "title": "Multi-Label Clinical Text Eligibility Classification and Summarization System",
    "url": "https://www.alphaxiv.org/abs/2510.13115v1",
    "arxiv_id": "2510.13115v1",
    "authors": "Surya Tejaswi Yerramsetty, Almas Fathimah",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-15 03:21:43",
    "ori_summary": "Clinical trials are central to medical progress because they help improve understanding of human health and the healthcare system. They play a key role in discovering new ways to detect, prevent, or treat diseases, and it is essential that clinical trials include participants with appropriate and diverse medical backgrounds. In this paper, we propose a system that leverages Natural Language Processing (NLP) and Large Language Models (LLMs) to automate multi-label clinical text eligibility classification and summarization. The system combines feature extraction methods such as word embeddings (Word2Vec) and named entity recognition to identify relevant medical concepts, along with traditional vectorization techniques such as count vectorization and TF-IDF (Term Frequency-Inverse Document Frequency). We further explore weighted TF-IDF word embeddings that integrate both count-based and embedding-based strengths to capture term importance effectively. Multi-label classification using Random Forest and SVM models is applied to categorize documents based on eligibility criteria. Summarization techniques including TextRank, Luhn, and GPT-3 are evaluated to concisely summarize eligibility requirements. Evaluation with ROUGE scores demonstrates the effectiveness of the proposed methods. This system shows potential for automating clinical trial eligibility assessment using data-driven approaches, thereby improving research efficiency.",
    "summary": "",
    "translation": "多标签临床文本资格分类与摘要系统",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域的临床文本处理，属于明确的医疗领域特定应用，与推荐系统、搜索或广告领域完全无关。论文内容涉及医疗资格分类和摘要生成，这些都属于医疗NLP应用范畴，不在当前关注的技术范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13106v1": {
    "title": "TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13106v1",
    "arxiv_id": "2510.13106v1",
    "authors": "Ruoyu Sun, Da Song, Jiayang Song, Yuheng Huang, Lei Ma",
    "categories": "cs.SE, cs.AI, cs.CL",
    "pub_date": "2025-10-15 02:59:07",
    "ori_summary": "As Large Language Models (LLMs) continue to revolutionize Natural Language Processing (NLP) applications, critical concerns about their trustworthiness persist, particularly in safety and robustness. To address these challenges, we introduce TRUSTVIS, an automated evaluation framework that provides a comprehensive assessment of LLM trustworthiness. A key feature of our framework is its interactive user interface, designed to offer intuitive visualizations of trustworthiness metrics. By integrating well-known perturbation methods like AutoDAN and employing majority voting across various evaluation methods, TRUSTVIS not only provides reliable results but also makes complex evaluation processes accessible to users. Preliminary case studies on models like Vicuna-7b, Llama2-7b, and GPT-3.5 demonstrate the effectiveness of our framework in identifying safety and robustness vulnerabilities, while the interactive interface allows users to explore results in detail, empowering targeted model improvements. Video Link: https://youtu.be/k1TrBqNVg8g",
    "summary": "",
    "translation": "TRUSTVIS：面向大型语言模型的多维度可信度评估框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM的可信度评估框架，这属于纯粹的LLM评估和基准测试范畴，明确列在无关主题中。虽然可信度在理论上可能与推荐系统的可信推荐相关，但论文标题表明其核心是通用的LLM评估框架，没有显示出与RecSys/Search/Ads领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13103v1": {
    "title": "ESI: Epistemic Uncertainty Quantification via Semantic-preserving Intervention for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13103v1",
    "arxiv_id": "2510.13103v1",
    "authors": "Mingda Li, Xinyu Li, Weinan Zhang, Longxuan Ma",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-15 02:46:43",
    "ori_summary": "Uncertainty Quantification (UQ) is a promising approach to improve model reliability, yet quantifying the uncertainty of Large Language Models (LLMs) is non-trivial. In this work, we establish a connection between the uncertainty of LLMs and their invariance under semantic-preserving intervention from a causal perspective. Building on this foundation, we propose a novel grey-box uncertainty quantification method that measures the variation in model outputs before and after the semantic-preserving intervention. Through theoretical justification, we show that our method provides an effective estimate of epistemic uncertainty. Our extensive experiments, conducted across various LLMs and a variety of question-answering (QA) datasets, demonstrate that our method excels not only in terms of effectiveness but also in computational efficiency.",
    "summary": "",
    "translation": "ESI：通过语义保持干预对大型语言模型进行认知不确定性量化",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM的不确定性量化技术，这属于模型可靠性和可信度评估范畴。虽然不确定性估计在推荐系统中可能有潜在应用（如置信度校准），但该工作更偏向通用NLP评估而非直接面向RecSys/Search/Ads的使能技术，与当前关注的核心领域进展和直接应用关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13079v1": {
    "title": "GatePro: Parameter-Free Expert Selection Optimization for Mixture-of-Experts Models",
    "url": "https://www.alphaxiv.org/abs/2510.13079v1",
    "arxiv_id": "2510.13079v1",
    "authors": "Chen Zheng, Yuhang Cai, Deyi Liu, Jin Ma, Yiyuan Ma, Yuan Yang, Jing Liu, Yutao Zeng, Xun Zhou, Siyuan Qiao",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-15 01:47:45",
    "ori_summary": "Modern large language models leverage Mixture-of-Experts (MoE) architectures for efficient scaling, but face a critical challenge: functionally similar experts are often selected simultaneously, creating redundant computation and limiting effective model capacity. Existing auxiliary balance loss methods improve token distribution but fail to address the underlying expert diversity problem. We introduce GatePro, a novel parameter-free method that directly promotes expert selection diversity. GatePro identifies the most similar expert pairs and introduces localized competition mechanisms, preventing redundant expert co-activation while maintaining natural expert specialization. Our comprehensive evaluation demonstrates GatePro's effectiveness across model scales and benchmarks. Analysis demonstrates GatePro's ability to achieve enhanced expert diversity, where experts develop more distinct and complementary capabilities, avoiding functional redundancy. This approach can be deployed hot-swappable during any training phase without additional learnable parameters, offering a practical solution for improving MoE effectiveness.",
    "summary": "",
    "translation": "GatePro：面向专家混合模型的无参专家选择优化方法",
    "relevance_score": 8,
    "reasoning": "该论文专注于专家混合模型的专家选择优化，属于'赋能Transformer技术'范畴中的MoE架构效率优化。在推荐系统和搜索领域，MoE模型可以通过专家网络处理不同的用户兴趣模式或内容类型，无参专家选择优化能够显著提升模型推理效率并降低计算成本，这对于大规模工业部署至关重要。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.13809v1": {
    "title": "PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.13809v1",
    "arxiv_id": "2510.13809v1",
    "authors": "Sihui Ji, Xi Chen, Xin Tao, Pengfei Wan, Hengshuang Zhao",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 17:59:59",
    "ori_summary": "Video generation models nowadays are capable of generating visually realistic videos, but often fail to adhere to physical laws, limiting their ability to generate physically plausible videos and serve as ''world models''. To address this issue, we propose PhysMaster, which captures physical knowledge as a representation for guiding video generation models to enhance their physics-awareness. Specifically, PhysMaster is based on the image-to-video task where the model is expected to predict physically plausible dynamics from the input image. Since the input image provides physical priors like relative positions and potential interactions of objects in the scenario, we devise PhysEncoder to encode physical information from it as an extra condition to inject physical knowledge into the video generation process. The lack of proper supervision on the model's physical performance beyond mere appearance motivates PhysEncoder to apply reinforcement learning with human feedback to physical representation learning, which leverages feedback from generation models to optimize physical representations with Direct Preference Optimization (DPO) in an end-to-end manner. PhysMaster provides a feasible solution for improving physics-awareness of PhysEncoder and thus of video generation, proving its ability on a simple proxy task and generalizability to wide-ranging physical scenarios. This implies that our PhysMaster, which unifies solutions for various physical processes via representation learning in the reinforcement learning paradigm, can act as a generic and plug-in solution for physics-aware video generation and broader applications.",
    "summary": "",
    "translation": "PhysMaster：通过强化学习掌握视频生成的物理表征",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频生成中的物理表征学习，属于纯粹的视觉生成领域，与推荐系统、搜索或广告没有直接关联。虽然使用了强化学习技术，但论文内容明显属于被排除的'Purely Vision'和'AIGC/Content generation'范畴，没有任何明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13808v1": {
    "title": "VisCoP: Visual Probing for Video Domain Adaptation of Vision Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13808v1",
    "arxiv_id": "2510.13808v1",
    "authors": "Dominick Reilly, Manish Kumar Govind, Le Xue, Srijan Das",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 17:59:52",
    "ori_summary": "Large Vision-Language Models (VLMs) excel at general visual reasoning tasks but exhibit sharp performance degradation when applied to novel domains with substantial distribution shifts from pretraining data. Existing domain adaptation approaches finetune different VLM components, but this often results in limited domain-specific feature learning or catastrophic forgetting of prior capabilities. To address these issues, we introduce Vision Contextualized Probing (VisCoP), which augments the VLM's vision encoder with a compact set of learnable visual probes. These probes enable efficient domain-specific adaptation with minimal modification to pretrained parameters. We evaluate VisCoP across three challenging domain adaptation settings-cross-view (exocentric to egocentric), cross-modal (RGB to depth), and cross-task (human understanding to robot control). Experiments show that VisCoP consistently outperforms existing adaptation strategies, achieving superior performance on target domains while effectively retaining source-domain knowledge.",
    "summary": "",
    "translation": "VisCoP：面向视觉语言模型视频领域自适应的视觉探测方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型在视频领域的自适应问题，属于纯粹的视觉和多模态领域研究。虽然提到了视觉语言模型，但其应用场景局限于视频理解，与推荐系统、搜索或广告的核心技术需求缺乏直接关联，也没有展示出在异构数据处理方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13802v1": {
    "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
    "url": "https://www.alphaxiv.org/abs/2510.13802v1",
    "arxiv_id": "2510.13802v1",
    "authors": "Xinhang Liu, Yuxi Xiao, Donny Y. Chen, Jiashi Feng, Yu-Wing Tai, Chi-Keung Tang, Bingyi Kang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 17:59:04",
    "ori_summary": "Effective spatio-temporal representation is fundamental to modeling, understanding, and predicting dynamics in videos. The atomic unit of a video, the pixel, traces a continuous 3D trajectory over time, serving as the primitive element of dynamics. Based on this principle, we propose representing any video as a Trajectory Field: a dense mapping that assigns a continuous 3D trajectory function of time to each pixel in every frame. With this representation, we introduce Trace Anything, a neural network that predicts the entire trajectory field in a single feed-forward pass. Specifically, for each pixel in each frame, our model predicts a set of control points that parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at arbitrary query time instants. We trained the Trace Anything model on large-scale 4D data, including data from our new platform, and our experiments demonstrate that: (i) Trace Anything achieves state-of-the-art performance on our new benchmark for trajectory field estimation and performs competitively on established point-tracking benchmarks; (ii) it offers significant efficiency gains thanks to its one-pass paradigm, without requiring iterative optimization or auxiliary estimators; and (iii) it exhibits emergent abilities, including goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion. Project page: https://trace-anything.github.io/.",
    "summary": "",
    "translation": "追踪万物：通过轨迹场实现任意视频的4D表示",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频理解和4D表示学习，属于计算机视觉领域。虽然轨迹场技术可能对视频内容理解有贡献，但缺乏明确的推荐系统、搜索或广告应用场景。其核心是视觉表示学习，而非推荐、搜索或广告中的用户行为建模、内容排序等关键技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13800v1": {
    "title": "Reasoning in Space via Grounding in the World",
    "url": "https://www.alphaxiv.org/abs/2510.13800v1",
    "arxiv_id": "2510.13800v1",
    "authors": "Yiming Chen, Zekun Qi, Wenyao Zhang, Xin Jin, Li Zhang, Peidong Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 17:58:08",
    "ori_summary": "In this paper, we claim that 3D visual grounding is the cornerstone of spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to explore the effective spatial representations that bridge the gap between them. Existing 3D LLMs suffer from the absence of a unified 3D representation capable of jointly capturing semantic and geometric information. This deficiency is manifested either in poor performance on grounding or in an excessive reliance on external modules, ultimately hindering the seamless integration of grounding and spatial reasoning. To address this, we propose a simple yet effective dual-path pooling mechanism that tightly aligns geometric features with both semantic and positional cues, constructing a unified image patch-based 3D representation that encapsulates all essential information without increasing the number of input tokens. Leveraging this holistic representation, GS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely without external modules while delivering performance comparable to state-of-the-art models, establishing a unified and self-contained framework for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we introduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is meticulously curated to include both 3D bounding box annotations for objects referenced in reasoning questions and step-by-step reasoning paths that integrate grounding as a core component of the problem-solving process. Extensive experiments demonstrate that GS-Reasoner achieves impressive results on 3D visual grounding, which in turn significantly enhances its spatial reasoning capabilities, leading to state-of-the-art performance.",
    "summary": "",
    "translation": "通过世界中的基础实现空间推理",
    "relevance_score": 2,
    "reasoning": "该论文标题暗示了空间推理和基础（grounding）的概念，可能涉及将符号推理与现实世界空间信息连接。虽然基础技术在某些推荐/搜索场景中可能有潜在应用（如理解用户位置上下文），但标题过于抽象且没有明确指向推荐系统、搜索或广告的核心技术。缺乏明确的Transformer架构、LLM应用或异构数据建模的直接联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13795v1": {
    "title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs",
    "url": "https://www.alphaxiv.org/abs/2510.13795v1",
    "arxiv_id": "2510.13795v1",
    "authors": "Yi Zhang, Bolin Ni, Xin-Sheng Chen, Heng-Rui Zhang, Yongming Rao, Houwen Peng, Qinglin Lu, Han Hu, Meng-Hao Guo, Shi-Min Hu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 17:52:59",
    "ori_summary": "Fully open multimodal large language models (MLLMs) currently lag behind proprietary counterparts, primarily due to a significant gap in data quality for supervised fine-tuning (SFT). Existing open-source datasets are often plagued by widespread noise and a critical deficit in complex reasoning data, such as Chain-of-Thought (CoT), which hinders the development of advanced model capabilities. Addressing these challenges, our work makes three primary contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising approximately 15 million QA pairs, processed through multiple cleaning techniques and enhanced with a novel dual-level (short and long) CoT enrichment strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its underlying framework DataStudio, providing the community with a transparent and adaptable methodology for data curation that moves beyond static dataset releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B model on Honey-Data-15M. Experiments show that Bee-8B establishes a new state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is competitive with, and in some cases surpasses, recent semi-open models such as InternVL3.5-8B. Our work delivers to the community a suite of foundational resources, including: the Honey-Data-15M corpus; the full-stack suite comprising HoneyPipe and DataStudio; training recipes; an evaluation harness; and the model weights. This effort demonstrates that a principled focus on data quality is a key pathway to developing fully open MLLMs that are highly competitive with their semi-open counterparts.",
    "summary": "",
    "translation": "Bee：一个高质量语料库与全栈套件，用于解锁先进的完全开源多模态大语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态大语言模型（MLLMs）的语料库构建和开源工具套件，属于纯粹的LLM技术基础设施。虽然多模态建模与VLM类比有概念关联，但论文标题未表明任何在推荐系统、搜索或广告领域的特定应用潜力，缺乏明确的领域相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13793v1": {
    "title": "NoisePrints: Distortion-Free Watermarks for Authorship in Private Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2510.13793v1",
    "arxiv_id": "2510.13793v1",
    "authors": "Nir Goren, Oren Katzir, Abhinav Nakarmi, Eyal Ronen, Mahmood Sharif, Or Patashnik",
    "categories": "cs.CV, cs.CR, cs.LG",
    "pub_date": "2025-10-15 17:50:45",
    "ori_summary": "With the rapid adoption of diffusion models for visual content generation, proving authorship and protecting copyright have become critical. This challenge is particularly important when model owners keep their models private and may be unwilling or unable to handle authorship issues, making third-party verification essential. A natural solution is to embed watermarks for later verification. However, existing methods require access to model weights and rely on computationally heavy procedures, rendering them impractical and non-scalable. To address these challenges, we propose , a lightweight watermarking scheme that utilizes the random seed used to initialize the diffusion process as a proof of authorship without modifying the generation process. Our key observation is that the initial noise derived from a seed is highly correlated with the generated visual content. By incorporating a hash function into the noise sampling process, we further ensure that recovering a valid seed from the content is infeasible. We also show that sampling an alternative seed that passes verification is infeasible, and demonstrate the robustness of our method under various manipulations. Finally, we show how to use cryptographic zero-knowledge proofs to prove ownership without revealing the seed. By keeping the seed secret, we increase the difficulty of watermark removal. In our experiments, we validate NoisePrints on multiple state-of-the-art diffusion models for images and videos, demonstrating efficient verification using only the seed and output, without requiring access to model weights.",
    "summary": "",
    "translation": "NoisePrints：私有扩散模型中的无失真作者身份水印技术",
    "relevance_score": 1,
    "reasoning": "该论文专注于扩散模型中的水印技术，属于隐私保护范畴，这在我的无关主题列表中明确排除。虽然涉及生成模型，但核心关注点是作者身份验证和隐私保护，而非推荐系统、搜索或广告中的直接应用。该技术没有明显的潜力应用于我的关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13787v1": {
    "title": "Adaptive Visual Conditioning for Semantic Consistency in Diffusion-Based Story Continuation",
    "url": "https://www.alphaxiv.org/abs/2510.13787v1",
    "arxiv_id": "2510.13787v1",
    "authors": "Seyed Mohammad Mousavi, Morteza Analoui",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 17:43:22",
    "ori_summary": "Story continuation focuses on generating the next image in a narrative sequence so that it remains coherent with both the ongoing text description and the previously observed images. A central challenge in this setting lies in utilizing prior visual context effectively, while ensuring semantic alignment with the current textual input. In this work, we introduce AVC (Adaptive Visual Conditioning), a framework for diffusion-based story continuation. AVC employs the CLIP model to retrieve the most semantically aligned image from previous frames. Crucially, when no sufficiently relevant image is found, AVC adaptively restricts the influence of prior visuals to only the early stages of the diffusion process. This enables the model to exploit visual context when beneficial, while avoiding the injection of misleading or irrelevant information. Furthermore, we improve data quality by re-captioning a noisy dataset using large language models, thereby strengthening textual supervision and semantic alignment. Quantitative results and human evaluations demonstrate that AVC achieves superior coherence, semantic consistency, and visual fidelity compared to strong baselines, particularly in challenging cases where prior visuals conflict with the current input.",
    "summary": "",
    "translation": "基于扩散模型的故事延续中语义一致性的自适应视觉条件控制",
    "relevance_score": 1,
    "reasoning": "该论文专注于扩散模型在视觉故事生成中的应用，属于纯粹的视觉内容生成领域。虽然涉及语义一致性，但核心是视觉内容生成而非推荐、搜索或广告系统，与当前关注的LLM在推荐系统、搜索广告中的应用以及Transformer架构进展等焦点领域无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13778v1": {
    "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy",
    "url": "https://www.alphaxiv.org/abs/2510.13778v1",
    "arxiv_id": "2510.13778v1",
    "authors": "Xinyi Chen, Yilun Chen, Yanwei Fu, Ning Gao, Jiaya Jia, Weiyang Jin, Hao Li, Yao Mu, Jiangmiao Pang, Yu Qiao, Yang Tian, Bin Wang, Bolun Wang, Fangjing Wang, Hanqing Wang, Tai Wang, Ziqin Wang, Xueyuan Wei, Chao Wu, Shuai Yang, Jinhui Ye, Junqiu Yu, Jia Zeng, Jingjing Zhang, Jinyu Zhang, Shi Zhang, Feng Zheng, Bowen Zhou, Yangkun Zhu",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2025-10-15 17:30:05",
    "ori_summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act'' by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act'' by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.",
    "summary": "",
    "translation": "InternVLA-M1：一种面向通用机器人策略的空间引导视觉-语言-动作框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人控制领域的视觉-语言-动作框架，属于机器人学特定应用。虽然涉及多模态建模，但其核心是机器人策略控制，与推荐系统、搜索或广告领域没有直接关联，也不具备在这些领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13774v1": {
    "title": "UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of Robust Spatial Representations",
    "url": "https://www.alphaxiv.org/abs/2510.13774v1",
    "arxiv_id": "2510.13774v1",
    "authors": "Dominik J. Mühlematter, Lin Che, Ye Hong, Martin Raubal, Nina Wiedemann",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-15 17:26:24",
    "ori_summary": "Forecasting urban phenomena such as housing prices and public health indicators requires the effective integration of various geospatial data. Current methods primarily utilize task-specific models, while recent foundation models for spatial representations often support only limited modalities and lack multimodal fusion capabilities. To overcome these challenges, we present UrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal Fusion (SMF). The framework employs modality-specific encoders to process different types of inputs, including street view imagery, remote sensing data, cartographic maps, and points of interest (POIs) data. These multimodal inputs are integrated via a Transformer-based fusion module that learns unified representations. An extensive evaluation across 41 tasks in 56 cities worldwide demonstrates UrbanFusion's strong generalization and predictive performance compared to state-of-the-art GeoAI models. Specifically, it 1) outperforms prior foundation models on location-encoding, 2) allows multimodal input during inference, and 3) generalizes well to regions unseen during training. UrbanFusion can flexibly utilize any subset of available modalities for a given location during both pretraining and inference, enabling broad applicability across diverse data availability scenarios. All source code is available at https://github.com/DominikM198/UrbanFusion.",
    "summary": "",
    "translation": "UrbanFusion：用于鲁棒空间表示对比学习的随机多模态融合",
    "relevance_score": 3,
    "reasoning": "该论文涉及多模态融合和对比学习技术，这些属于通用表示学习方法。虽然空间表示学习在位置感知推荐系统中可能有潜在应用，但论文标题没有明确表明与推荐系统、搜索或广告的直接关联，也没有提到Transformer架构或LLM技术。多模态融合技术可能适用于处理异构用户数据，但缺乏具体的应用场景说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13768v1": {
    "title": "Scaling Vision Transformers for Functional MRI with Flat Maps",
    "url": "https://www.alphaxiv.org/abs/2510.13768v1",
    "arxiv_id": "2510.13768v1",
    "authors": "Connor Lane, Daniel Z. Kaplan, Tanishq Mathew Abraham, Paul S. Scotti",
    "categories": "cs.CV, cs.AI, q-bio.NC",
    "pub_date": "2025-10-15 17:15:00",
    "ori_summary": "A key question for adapting modern deep learning architectures to functional MRI (fMRI) is how to represent the data for model input. To bridge the modality gap between fMRI and natural images, we transform the 4D volumetric fMRI data into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K hours of fMRI flat map videos from the Human Connectome Project using the spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI modeling performance improves with dataset size according to a strict power scaling law. Downstream classification benchmarks show that our model learns rich representations supporting both fine-grained state decoding across subjects, as well as subject-specific trait decoding across changes in brain state. This work is part of an ongoing open science project to build foundation models for fMRI data. Our code and datasets are available at https://github.com/MedARC-AI/fmri-fm.",
    "summary": "",
    "translation": "使用平面映射扩展视觉变换器用于功能性磁共振成像",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉变换器在医学影像（fMRI）领域的扩展应用，属于医学/生物医学领域。虽然涉及Transformer架构扩展，但其应用场景（功能性磁共振成像）与推荐系统、搜索或广告领域没有明显关联，且没有证据表明该技术有跨领域应用的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13759v1": {
    "title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark",
    "url": "https://www.alphaxiv.org/abs/2510.13759v1",
    "arxiv_id": "2510.13759v1",
    "authors": "Kai Zou, Ziqi Huang, Yuhao Dong, Shulin Tian, Dian Zheng, Hongbo Liu, Jingwen He, Bin Liu, Yu Qiao, Ziwei Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 17:10:35",
    "ori_summary": "Unified multimodal models aim to jointly enable visual understanding and generation, yet current benchmarks rarely examine their true integration. Existing evaluations either treat the two abilities in isolation or overlook tasks that inherently couple them. To address this gap, we present Uni-MMMU, a comprehensive and discipline-aware benchmark that systematically unfolds the bidirectional synergy between generation and understanding across eight reasoning-centric domains, including science, coding, mathematics, and puzzles. Each task is bidirectionally coupled, demanding models to (i) leverage conceptual understanding to guide precise visual synthesis, or (ii) utilize generation as a cognitive scaffold for analytical reasoning. Uni-MMMU incorporates verifiable intermediate reasoning steps, unique ground truths, and a reproducible scoring protocol for both textual and visual outputs. Through extensive evaluation of state-of-the-art unified, generation-only, and understanding-only models, we reveal substantial performance disparities and cross-modal dependencies, offering new insights into when and how these abilities reinforce one another, and establishing a reliable foundation for advancing unified models.",
    "summary": "",
    "translation": "Uni-MMMU：一个大规模多学科多模态统一基准",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态基准测试，属于评估基准范畴，这在无关主题中被明确排除。虽然多模态建模在概念上与异构数据统一建模相关，但该论文专注于基准创建而非技术进展，缺乏对推荐系统、搜索或广告领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13756v1": {
    "title": "RECODE: Reasoning Through Code Generation for Visual Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.13756v1",
    "arxiv_id": "2510.13756v1",
    "authors": "Junhong Shen, Mu Cai, Bo Hu, Ameet Talwalkar, David A Ross, Cordelia Schmid, Alireza Fathi",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-15 17:05:37",
    "ori_summary": "Multimodal Large Language Models (MLLMs) struggle with precise reasoning for structured visuals like charts and diagrams, as pixel-based perception lacks a mechanism for verification. To address this, we propose to leverage derendering -- the process of reverse-engineering visuals into executable code -- as a new modality for verifiable visual reasoning. Specifically, we propose RECODE, an agentic framework that first generates multiple candidate programs to reproduce the input image. It then uses a critic to select the most faithful reconstruction and iteratively refines the code. This process not only transforms an ambiguous perceptual task into a verifiable, symbolic problem, but also enables precise calculations and logical inferences later on. On various visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K, RECODE significantly outperforms methods that do not leverage code or only use code for drawing auxiliary lines or cropping. Our work demonstrates that grounding visual perception in executable code provides a new path toward more accurate and verifiable multimodal reasoning.",
    "summary": "",
    "translation": "RECODE：通过代码生成进行视觉问答推理",
    "relevance_score": 2,
    "reasoning": "该论文专注于视觉问答(VQA)领域的代码生成方法，属于纯粹的视觉-语言交叉研究。虽然标题提到'推理'和'代码生成'，但核心应用场景是视觉问答，与推荐系统、搜索或广告领域没有直接关联。论文的技术方法也没有显示出在异构数据处理或推荐系统架构方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13747v1": {
    "title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue",
    "url": "https://www.alphaxiv.org/abs/2510.13747v1",
    "arxiv_id": "2510.13747v1",
    "authors": "Wenwen Tong, Hewei Guo, Dongchuan Ran, Jiangnan Chen, Jiefan Lu, Kaibin Wang, Keqiang Li, Xiaoxu Zhu, Jiakui Li, Kehan Li, Xueheng Li, Lumin Li, Chenxu Guo, Jiasheng Zhou, Jiandong Chen, Xianye Wu, Jiahao Wang, Silei Wu, Lei Chen, Hanming Deng, Yuxuan Song, Dinghao Zhou, Guiping Zhong, Ken Zheng, Shiyin Kang, Lewei Lu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 16:52:48",
    "ori_summary": "We introduce InteractiveOmni, a unified and open-source omni-modal large language model for audio-visual multi-turn interaction, ranging from 4B to 8B parameters, designed to lead the field of lightweight models by offering comprehensive omni-modal understanding and speech generation capabilities. To achieve this, we integrate the vision encoder, audio encoder, large language model, and speech decoder into a unified model for understanding and generation tasks. We design a multi-stage training strategy to ensure robust cross-modal capabilities, including pre-training for omni-modal understanding, followed by post-training with speech conversation and audio-visual interaction. To enable human-like long-term conversational ability, we meticulously curate a multi-turn training dataset that enhances the model's ability to handle complex and multi-turn interactions. To effectively evaluate the multi-turn memory and speech interaction capabilities, we construct the multi-modal multi-turn memory benchmark and the multi-turn speech interaction benchmark. Experiments demonstrate that InteractiveOmni significantly outperforms leading open-source models and provides a more intelligent multi-turn audio-visual experience, particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B is comparable to the much larger model like Qwen2.5-Omni-7B on general benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B while utilizing only 50% of the model size. Achieving state-of-the-art results against similarly sized models across image, audio, video understanding, and speech generation tasks, InteractiveOmni is an accessible, open-source foundation for next-generation intelligent interactive systems.",
    "summary": "",
    "translation": "InteractiveOmni：面向音视频多轮对话的统一全模态模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注音视频多模态对话，属于纯粹的VLM/多模态研究范畴，与推荐系统、搜索或广告的核心技术领域没有直接关联。虽然标题提到'统一全模态模型'，但针对的是音视频对话场景，缺乏明确的推荐/搜索/广告应用潜力，无法满足任何聚焦领域的要求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13745v1": {
    "title": "UniCalli: A Unified Diffusion Framework for Column-Level Generation and Recognition of Chinese Calligraphy",
    "url": "https://www.alphaxiv.org/abs/2510.13745v1",
    "arxiv_id": "2510.13745v1",
    "authors": "Tianshuo Xu, Kai Wang, Zhifei Chen, Leyi Wu, Tianshui Wen, Fei Chao, Ying-Cong Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 16:52:07",
    "ori_summary": "Computational replication of Chinese calligraphy remains challenging. Existing methods falter, either creating high-quality isolated characters while ignoring page-level aesthetics like ligatures and spacing, or attempting page synthesis at the expense of calligraphic correctness. We introduce \\textbf{UniCalli}, a unified diffusion framework for column-level recognition and generation. Training both tasks jointly is deliberate: recognition constrains the generator to preserve character structure, while generation provides style and layout priors. This synergy fosters concept-level abstractions that improve both tasks, especially in limited-data regimes. We curated a dataset of over 8,000 digitized pieces, with ~4,000 densely annotated. UniCalli employs asymmetric noising and a rasterized box map for spatial priors, trained on a mix of synthetic, labeled, and unlabeled data. The model achieves state-of-the-art generative quality with superior ligature continuity and layout fidelity, alongside stronger recognition. The framework successfully extends to other ancient scripts, including Oracle bone inscriptions and Egyptian hieroglyphs. Code and data can be viewed in \\href{https://github.com/EnVision-Research/UniCalli}{this URL}.",
    "summary": "",
    "translation": "UniCalli：面向中文书法列级别生成与识别的统一扩散框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于中文书法的生成与识别，属于计算机视觉领域的特定应用。虽然涉及生成模型（扩散框架），但其应用场景（书法）与推荐系统、搜索或广告的核心技术需求没有直接关联，也不涉及异构数据建模或Transformer架构的改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13740v1": {
    "title": "Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs",
    "url": "https://www.alphaxiv.org/abs/2510.13740v1",
    "arxiv_id": "2510.13740v1",
    "authors": "Mustafa Munir, Alex Zhang, Radu Marculescu",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-15 16:47:09",
    "ori_summary": "Vision graph neural networks (ViG) have demonstrated promise in vision tasks as a competitive alternative to conventional convolutional neural nets (CNN) and transformers (ViTs); however, common graph construction methods, such as k-nearest neighbor (KNN), can be expensive on larger images. While methods such as Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed step scale can lead to over-squashing and missing multiple connections to gain the same information that could be gained from a long-range link. Through this observation, we propose a new graph construction method, Logarithmic Scalable Graph Construction (LSGC) to enhance performance by limiting the number of long-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN model that utilizes LSGC. Furthermore, inspired by the successes of multi-scale and high-resolution architectures, we introduce and apply a high-resolution branch and fuse features between our high-resolution and low-resolution branches for a multi-scale high-resolution Vision GNN network. Extensive experiments show that LogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy, GMACs, and parameters on image classification and semantic segmentation tasks. Our smallest model, Ti-LogViG, achieves an average top-1 accuracy on ImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average accuracy than Vision GNN with a 24.3% reduction in parameters and 35.3% reduction in GMACs. Our work shows that leveraging long-range links in graph construction for ViGs through our proposed LSGC can exceed the performance of current state-of-the-art ViGs. Code is available at https://github.com/mmunir127/LogViG-Official.",
    "summary": "",
    "translation": "用于高效视觉图神经网络的多尺度高分辨率对数图绘制模块",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉图神经网络（GNN）的架构改进和效率优化，属于计算机视觉领域的技术进展。虽然提到了多尺度特征和高效模块设计，但这些技术主要针对视觉数据而非推荐系统、搜索或广告中常见的序列、文本或异构数据。在推荐/搜索/广告领域应用这些视觉GNN技术的路径不够明确，相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13735v1": {
    "title": "Cyclic Self-Supervised Diffusion for Ultra Low-field to High-field MRI Synthesis",
    "url": "https://www.alphaxiv.org/abs/2510.13735v1",
    "arxiv_id": "2510.13735v1",
    "authors": "Zhenxuan Zhang, Peiyuan Jing, Zi Wang, Ula Briski, Coraline Beitone, Yue Yang, Yinzhe Wu, Fanwen Wang, Liutao Yang, Jiahao Huang, Zhifan Gao, Zhaolin Chen, Kh Tohidul Islam, Guang Yang, Peter J. Lally",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 16:41:54",
    "ori_summary": "Synthesizing high-quality images from low-field MRI holds significant potential. Low-field MRI is cheaper, more accessible, and safer, but suffers from low resolution and poor signal-to-noise ratio. This synthesis process can reduce reliance on costly acquisitions and expand data availability. However, synthesizing high-field MRI still suffers from a clinical fidelity gap. There is a need to preserve anatomical fidelity, enhance fine-grained structural details, and bridge domain gaps in image contrast. To address these issues, we propose a \\emph{cyclic self-supervised diffusion (CSS-Diff)} framework for high-field MRI synthesis from real low-field MRI data. Our core idea is to reformulate diffusion-based synthesis under a cycle-consistent constraint. It enforces anatomical preservation throughout the generative process rather than just relying on paired pixel-level supervision. The CSS-Diff framework further incorporates two novel processes. The slice-wise gap perception network aligns inter-slice inconsistencies via contrastive learning. The local structure correction network enhances local feature restoration through self-reconstruction of masked and perturbed patches. Extensive experiments on cross-field synthesis tasks demonstrate the effectiveness of our method, achieving state-of-the-art performance (e.g., 31.80 $\\pm$ 2.70 dB in PSNR, 0.943 $\\pm$ 0.102 in SSIM, and 0.0864 $\\pm$ 0.0689 in LPIPS). Beyond pixel-wise fidelity, our method also preserves fine-grained anatomical structures compared with the original low-field MRI (e.g., left cerebral white matter error drops from 12.1$\\%$ to 2.1$\\%$, cortex from 4.2$\\%$ to 3.7$\\%$). To conclude, our CSS-Diff can synthesize images that are both quantitatively reliable and anatomically consistent.",
    "summary": "",
    "translation": "用于超低场到高场MRI合成的循环自监督扩散方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像领域的MRI图像合成技术，属于医疗应用范畴，与推荐系统、搜索或广告完全无关。论文涉及的自监督扩散方法虽然具有技术价值，但在当前聚焦的领域中没有直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13729v1": {
    "title": "LiFMCR: Dataset and Benchmark for Light Field Multi-Camera Registration",
    "url": "https://www.alphaxiv.org/abs/2510.13729v1",
    "arxiv_id": "2510.13729v1",
    "authors": "Aymeric Fleith, Julian Zirbel, Daniel Cremers, Niclas Zeller",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 16:32:27",
    "ori_summary": "We present LiFMCR, a novel dataset for the registration of multiple micro lens array (MLA)-based light field cameras. While existing light field datasets are limited to single-camera setups and typically lack external ground truth, LiFMCR provides synchronized image sequences from two high-resolution Raytrix R32 plenoptic cameras, together with high-precision 6-degrees of freedom (DoF) poses recorded by a Vicon motion capture system. This unique combination enables rigorous evaluation of multi-camera light field registration methods. As a baseline, we provide two complementary registration approaches: a robust 3D transformation estimation via a RANSAC-based method using cross-view point clouds, and a plenoptic PnP algorithm estimating extrinsic 6-DoF poses from single light field images. Both explicitly integrate the plenoptic camera model, enabling accurate and scalable multi-camera registration. Experiments show strong alignment with the ground truth, supporting reliable multi-view light field processing. Project page: https://lifmcr.github.io/",
    "summary": "",
    "translation": "LiFMCR：光场多相机配准的数据集与基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的光场多相机配准技术，属于纯粹的视觉处理范畴。虽然光场相机技术在某些特殊场景下可能有应用，但论文标题明确指向数据集和基准创建，没有显示出与推荐系统、搜索或广告领域的直接关联或潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13720v1": {
    "title": "Circle of Willis Centerline Graphs: A Dataset and Baseline Algorithm",
    "url": "https://www.alphaxiv.org/abs/2510.13720v1",
    "arxiv_id": "2510.13720v1",
    "authors": "Fabio Musio, Norman Juchler, Kaiyuan Yang, Suprosanna Shit, Chinmay Prabhakar, Bjoern Menze, Sven Hirsch",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 16:22:51",
    "ori_summary": "The Circle of Willis (CoW) is a critical network of arteries in the brain, often implicated in cerebrovascular pathologies. Voxel-level segmentation is an important first step toward an automated CoW assessment, but a full quantitative analysis requires centerline representations. However, conventional skeletonization techniques often struggle to extract reliable centerlines due to the CoW's complex geometry, and publicly available centerline datasets remain scarce. To address these challenges, we used a thinning-based skeletonization algorithm to extract and curate centerline graphs and morphometric features from the TopCoW dataset, which includes 200 stroke patients, each imaged with MRA and CTA. The curated graphs were used to develop a baseline algorithm for centerline and feature extraction, combining U-Net-based skeletonization with A* graph connection. Performance was evaluated on a held-out test set, focusing on anatomical accuracy and feature robustness. Further, we used the extracted features to predict the frequency of fetal PCA variants, confirm theoretical bifurcation optimality relations, and detect subtle modality differences. The baseline algorithm consistently reconstructed graph topology with high accuracy (F1 = 1), and the average Euclidean node distance between reference and predicted graphs was below one voxel. Features such as segment radius, length, and bifurcation ratios showed strong robustness, with median relative errors below 5% and Pearson correlations above 0.95. Our results demonstrate the utility of learning-based skeletonization combined with graph connection for anatomically plausible centerline extraction. We emphasize the importance of going beyond simple voxel-based measures by evaluating anatomical accuracy and feature robustness. The dataset and baseline algorithm have been released to support further method development and clinical research.",
    "summary": "",
    "translation": "Willis环中心线图：数据集与基线算法",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学影像领域，特别是脑血管解剖结构（Willis环）的数据集和算法研究。这属于明确的医学/生物学应用范畴，与推荐系统、搜索、广告或相关使能技术完全无关。根据用户指定的无关主题列表，医学领域应用应被排除。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13714v1": {
    "title": "Dedelayed: Deleting remote inference delay via on-device correction",
    "url": "https://www.alphaxiv.org/abs/2510.13714v1",
    "arxiv_id": "2510.13714v1",
    "authors": "Dan Jacobellis, Mateen Ulhaq, Fabien Racapé, Hyomin Choi, Neeraja J. Yadwadkar",
    "categories": "eess.IV, cs.AI, cs.CV, cs.LG",
    "pub_date": "2025-10-15 16:13:44",
    "ori_summary": "Remote inference allows lightweight devices to leverage powerful cloud models. However, communication network latency makes predictions stale and unsuitable for real-time tasks. To address this, we introduce Dedelayed, a delay-corrective method that mitigates arbitrary remote inference delays, allowing the local device to produce low-latency outputs in real time. Our method employs a lightweight local model that processes the current frame and fuses in features that a heavyweight remote model computes from past frames. On video from the BDD100K driving dataset, Dedelayed improves semantic segmentation accuracy over the stronger of the local-only and remote-only baselines across all realistic communication network delays beyond 33 ms. Without incurring additional delay, it improves accuracy by 6.4 mIoU compared to fully local inference and 9.8 mIoU compared to remote inference, for a round-trip delay of 100 ms. The advantage grows under longer delays and higher-motion scenes, as delay-mitigated split inference sustains accuracy more effectively, providing clear advantages for real-time tasks that must remain aligned with the current world state.",
    "summary": "",
    "translation": "Dedelayed：通过设备端校正消除远程推理延迟",
    "relevance_score": 3,
    "reasoning": "该论文关注通过设备端技术减少推理延迟，这属于边缘计算和推理效率的范畴。虽然延迟优化对实时推荐和搜索系统有潜在价值，但论文标题未明确表明与LLM、Transformer架构或推荐/搜索/广告系统的直接关联，因此相关性有限。对于'使能技术'类别，设备端延迟校正可能应用于移动推荐系统的实时推理场景，但具体应用场景不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13702v1": {
    "title": "MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering and Completion",
    "url": "https://www.alphaxiv.org/abs/2510.13702v1",
    "arxiv_id": "2510.13702v1",
    "authors": "Minjung Shin, Hyunin Cho, Sooyeon Go, Jin-Hwa Kim, Youngjung Uh",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 16:00:26",
    "ori_summary": "Multi-view generation with camera pose control and prompt-based customization are both essential elements for achieving controllable generative models. However, existing multi-view generation models do not support customization with geometric consistency, whereas customization models lack explicit viewpoint control, making them challenging to unify. Motivated by these gaps, we introduce a novel task, multi-view customization, which aims to jointly achieve multi-view camera pose control and customization. Due to the scarcity of training data in customization, existing multi-view generation models, which inherently rely on large-scale datasets, struggle to generalize to diverse prompts. To address this, we propose MVCustom, a novel diffusion-based framework explicitly designed to achieve both multi-view consistency and customization fidelity. In the training stage, MVCustom learns the subject's identity and geometry using a feature-field representation, incorporating the text-to-video diffusion backbone enhanced with dense spatio-temporal attention, which leverages temporal coherence for multi-view consistency. In the inference stage, we introduce two novel techniques: depth-aware feature rendering explicitly enforces geometric consistency, and consistent-aware latent completion ensures accurate perspective alignment of the customized subject and surrounding backgrounds. Extensive experiments demonstrate that MVCustom is the only framework that simultaneously achieves faithful multi-view generation and customization.",
    "summary": "",
    "translation": "MVCustom：通过几何潜在渲染与补全实现多视图定制化扩散",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及多视图扩散模型和几何潜在渲染，属于计算机视觉和3D生成领域。虽然标题提到'定制化'，但缺乏与推荐系统、搜索或广告的直接关联。其技术核心是视觉内容生成，而非排名、检索或用户行为建模，因此与当前关注领域相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13698v1": {
    "title": "Risk-adaptive Activation Steering for Safe Multimodal Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13698v1",
    "arxiv_id": "2510.13698v1",
    "authors": "Jonghyun Park, Minhyuk Seo, Jonghyun Choi",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 15:57:17",
    "ori_summary": "One of the key challenges of modern AI models is ensuring that they provide helpful responses to benign queries while refusing malicious ones. But often, the models are vulnerable to multimodal queries with harmful intent embedded in images. One approach for safety alignment is training with extensive safety datasets at the significant costs in both dataset curation and training. Inference-time alignment mitigates these costs, but introduces two drawbacks: excessive refusals from misclassified benign queries and slower inference speed due to iterative output adjustments. To overcome these limitations, we propose to reformulate queries to strengthen cross-modal attention to safety-critical image regions, enabling accurate risk assessment at the query level. Using the assessed risk, it adaptively steers activations to generate responses that are safe and helpful without overhead from iterative output adjustments. We call this Risk-adaptive Activation Steering (RAS). Extensive experiments across multiple benchmarks on multimodal safety and utility demonstrate that the RAS significantly reduces attack success rates, preserves general task performance, and improves inference speed over prior inference-time defenses.",
    "summary": "",
    "translation": "面向安全多模态大语言模型的风险自适应激活引导",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态大语言模型的安全性和风险控制，这属于安全伦理范畴，被明确列为不相关主题。虽然提到了多模态和激活引导技术，但核心焦点是安全控制而非推荐系统、搜索或广告的应用。该技术可能间接应用于内容安全过滤，但缺乏明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13684v1": {
    "title": "Generating healthy counterfactuals with denoising diffusion bridge models",
    "url": "https://www.alphaxiv.org/abs/2510.13684v1",
    "arxiv_id": "2510.13684v1",
    "authors": "Ana Lawry Aguila, Peirong Liu, Marina Crespo Aguirre, Juan Eugenio Iglesias",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 15:40:57",
    "ori_summary": "Generating healthy counterfactuals from pathological images holds significant promise in medical imaging, e.g., in anomaly detection or for application of analysis tools that are designed for healthy scans. These counterfactuals should represent what a patient's scan would plausibly look like in the absence of pathology, preserving individual anatomical characteristics while modifying only the pathological regions. Denoising diffusion probabilistic models (DDPMs) have become popular methods for generating healthy counterfactuals of pathology data. Typically, this involves training on solely healthy data with the assumption that a partial denoising process will be unable to model disease regions and will instead reconstruct a closely matched healthy counterpart. More recent methods have incorporated synthetic pathological images to better guide the diffusion process. However, it remains challenging to guide the generative process in a way that effectively balances the removal of anomalies with the retention of subject-specific features. To solve this problem, we propose a novel application of denoising diffusion bridge models (DDBMs) - which, unlike DDPMs, condition the diffusion process not only on the initial point (i.e., the healthy image), but also on the final point (i.e., a corresponding synthetically generated pathological image). Treating the pathological image as a structurally informative prior enables us to generate counterfactuals that closely match the patient's anatomy while selectively removing pathology. The results show that our DDBM outperforms previously proposed diffusion models and fully supervised approaches at segmentation and anomaly detection tasks.",
    "summary": "",
    "translation": "使用去噪扩散桥模型生成健康反事实",
    "relevance_score": 2,
    "reasoning": "虽然反事实生成在推荐系统中可能有解释性应用，但该论文明确聚焦于健康领域，这属于被排除的医学/生物应用范畴。扩散模型作为生成技术，其直接应用更偏向AIGC和内容生成，而非推荐系统的核心排名或检索问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13678v1": {
    "title": "FlashWorld: High-quality 3D Scene Generation within Seconds",
    "url": "https://www.alphaxiv.org/abs/2510.13678v1",
    "arxiv_id": "2510.13678v1",
    "authors": "Xinyang Li, Tengfei Wang, Zixiao Gu, Shengchuan Zhang, Chunchao Guo, Liujuan Cao",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 15:35:48",
    "ori_summary": "We propose FlashWorld, a generative model that produces 3D scenes from a single image or text prompt in seconds, 10~100$\\times$ faster than previous works while possessing superior rendering quality. Our approach shifts from the conventional multi-view-oriented (MV-oriented) paradigm, which generates multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach where the model directly produces 3D Gaussian representations during multi-view generation. While ensuring 3D consistency, 3D-oriented method typically suffers poor visual quality. FlashWorld includes a dual-mode pre-training phase followed by a cross-mode post-training phase, effectively integrating the strengths of both paradigms. Specifically, leveraging the prior from a video diffusion model, we first pre-train a dual-mode multi-view diffusion model, which jointly supports MV-oriented and 3D-oriented generation modes. To bridge the quality gap in 3D-oriented generation, we further propose a cross-mode post-training distillation by matching distribution from consistent 3D-oriented mode to high-quality MV-oriented mode. This not only enhances visual quality while maintaining 3D consistency, but also reduces the required denoising steps for inference. Also, we propose a strategy to leverage massive single-view images and text prompts during this process to enhance the model's generalization to out-of-distribution inputs. Extensive experiments demonstrate the superiority and efficiency of our method.",
    "summary": "",
    "translation": "FlashWorld：数秒内生成高质量3D场景",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D场景生成，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然3D内容可能在未来的广告展示中有潜在应用，但论文本身不涉及排名、用户建模或任何与RecSys/Search/Ads相关的核心技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13675v1": {
    "title": "Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning",
    "url": "https://www.alphaxiv.org/abs/2510.13675v1",
    "arxiv_id": "2510.13675v1",
    "authors": "Hongkuan Zhou, Lavdim Halilaj, Sebastian Monka, Stefan Schmid, Yuqicheng Zhu, Jingcheng Wu, Nadeem Nazer, Steffen Staab",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-15 15:33:36",
    "ori_summary": "Open-domain visual entity recognition aims to identify and link entities depicted in images to a vast and evolving set of real-world concepts, such as those found in Wikidata. Unlike conventional classification tasks with fixed label sets, it operates under open-set conditions, where most target entities are unseen during training and exhibit long-tail distributions. This makes the task inherently challenging due to limited supervision, high visual ambiguity, and the need for semantic disambiguation. In this work, we propose a Knowledge-guided Contrastive Learning (KnowCoL) framework that combines both images and text descriptions into a shared semantic space grounded by structured information from Wikidata. By abstracting visual and textual inputs to a conceptual level, the model leverages entity descriptions, type hierarchies, and relational context to support zero-shot entity recognition. We evaluate our approach on the OVEN benchmark, a large-scale open-domain visual recognition dataset with Wikidata IDs as the label space. Our experiments show that using visual, textual, and structured knowledge greatly improves accuracy, especially for rare and unseen entities. Our smallest model improves the accuracy on unseen entities by 10.5% compared to the state-of-the-art, despite being 35 times smaller.",
    "summary": "",
    "translation": "野外视觉与认知：基于对比学习的大规模知识图谱开放域视觉实体识别",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视觉实体识别与知识图谱的结合，属于视觉-语言多模态研究范畴。虽然涉及多模态建模，但其核心是纯粹的视觉识别任务，缺乏明确的推荐、搜索或广告应用场景。作为使能技术，其多模态融合方法可能为处理异构数据的推荐系统提供启发，但直接相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13670v1": {
    "title": "NTIRE 2025 Challenge on Low Light Image Enhancement: Methods and Results",
    "url": "https://www.alphaxiv.org/abs/2510.13670v1",
    "arxiv_id": "2510.13670v1",
    "authors": "Xiaoning Liu, Zongwei Wu, Florin-Alexandru Vasluianu, Hailong Yan, Bin Ren, Yulun Zhang, Shuhang Gu, Le Zhang, Ce Zhu, Radu Timofte, Kangbiao Shi, Yixu Feng, Tao Hu, Yu Cao, Peng Wu, Yijin Liang, Yanning Zhang, Qingsen Yan, Han Zhou, Wei Dong, Yan Min, Mohab Kishawy, Jun Chen, Pengpeng Yu, Anjin Park, Seung-Soo Lee, Young-Joon Park, Zixiao Hu, Junyv Liu, Huilin Zhang, Jun Zhang, Fei Wan, Bingxin Xu, Hongzhe Liu, Cheng Xu, Weiguo Pan, Songyin Dai, Xunpeng Yi, Qinglong Yan, Yibing Zhang, Jiayi Ma, Changhui Hu, Kerui Hu, Donghang Jing, Tiesheng Chen, Zhi Jin, Hongjun Wu, Biao Huang, Haitao Ling, Jiahao Wu, Dandan Zhan, G Gyaneshwar Rao, Vijayalaxmi Ashok Aralikatti, Nikhil Akalwadi, Ramesh Ashok Tabib, Uma Mudenagudi, Ruirui Lin, Guoxi Huang, Nantheera Anantrasirichai, Qirui Yang, Alexandru Brateanu, Ciprian Orhei, Cosmin Ancuti, Daniel Feijoo, Juan C. Benito, Álvaro García, Marcos V. Conde, Yang Qin, Raul Balmez, Anas M. Ali, Bilel Benjdira, Wadii Boulila, Tianyi Mao, Huan Zheng, Yanyan Wei, Shengeng Tang, Dan Guo, Zhao Zhang, Sabari Nathan, K Uma, A Sasithradevi, B Sathya Bama, S. Mohamed Mansoor Roomi, Ao Li, Xiangtao Zhang, Zhe Liu, Yijie Tang, Jialong Tang, Zhicheng Fu, Gong Chen, Joe Nasti, John Nicholson, Zeyu Xiao, Zhuoyuan Li, Ashutosh Kulkarni, Prashant W. Patil, Santosh Kumar Vipparthi, Subrahmanyam Murala, Duan Liu, Weile Li, Hangyuan Lu, Rixian Liu, Tengfeng Wang, Jinxing Liang, Chenxin Yu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 15:30:16",
    "ori_summary": "This paper presents a comprehensive review of the NTIRE 2025 Low-Light Image Enhancement (LLIE) Challenge, highlighting the proposed solutions and final outcomes. The objective of the challenge is to identify effective networks capable of producing brighter, clearer, and visually compelling images under diverse and challenging conditions. A remarkable total of 762 participants registered for the competition, with 28 teams ultimately submitting valid entries. This paper thoroughly evaluates the state-of-the-art advancements in LLIE, showcasing the significant progress.",
    "summary": "",
    "translation": "NTIRE 2025 低光照图像增强挑战赛：方法与结果",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于计算机视觉领域的低光照图像增强技术，属于纯粹的视觉处理任务。虽然图像质量提升在某些广告或推荐场景中可能有间接价值，但该工作本身不涉及推荐系统、搜索或广告的核心算法，也不涉及LLM、Transformer架构或异构数据建模等关键技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13669v1": {
    "title": "CanvasMAR: Improving Masked Autoregressive Video Generation With Canvas",
    "url": "https://www.alphaxiv.org/abs/2510.13669v1",
    "arxiv_id": "2510.13669v1",
    "authors": "Zian Li, Muhan Zhang",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-15 15:29:09",
    "ori_summary": "Masked autoregressive models (MAR) have recently emerged as a powerful paradigm for image and video generation, combining the flexibility of masked modeling with the potential of continuous tokenizer. However, video MAR models suffer from two major limitations: the slow-start problem, caused by the lack of a structured global prior at early sampling stages, and error accumulation across the autoregression in both spatial and temporal dimensions. In this work, we propose CanvasMAR, a novel video MAR model that mitigates these issues by introducing a canvas mechanism--a blurred, global prediction of the next frame, used as the starting point for masked generation. The canvas provides global structure early in sampling, enabling faster and more coherent frame synthesis. Furthermore, we introduce compositional classifier-free guidance that jointly enlarges spatial (canvas) and temporal conditioning, and employ noise-based canvas augmentation to enhance robustness. Experiments on the BAIR and Kinetics-600 benchmarks demonstrate that CanvasMAR produces high-quality videos with fewer autoregressive steps. Our approach achieves remarkable performance among autoregressive models on Kinetics-600 dataset and rivals diffusion-based methods.",
    "summary": "",
    "translation": "CanvasMAR：通过画布改进掩码自回归视频生成",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于视频生成技术，属于纯粹的视觉生成领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然提到了自回归模型，但这是针对视频模态的特定应用，没有展示出在异构数据处理或推荐系统架构方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13660v1": {
    "title": "OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild",
    "url": "https://www.alphaxiv.org/abs/2510.13660v1",
    "arxiv_id": "2510.13660v1",
    "authors": "Hongyu Qu, Jianan Wei, Xiangbo Shu, Yazhou Yao, Wenguan Wang, Jinhui Tang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 15:19:52",
    "ori_summary": "Current 3D gaze estimation methods struggle to generalize across diverse data domains, primarily due to i) the scarcity of annotated datasets, and ii) the insufficient diversity of labeled data. In this work, we present OmniGaze, a semi-supervised framework for 3D gaze estimation, which utilizes large-scale unlabeled data collected from diverse and unconstrained real-world environments to mitigate domain bias and generalize gaze estimation in the wild. First, we build a diverse collection of unlabeled facial images, varying in facial appearances, background environments, illumination conditions, head poses, and eye occlusions. In order to leverage unlabeled data spanning a broader distribution, OmniGaze adopts a standard pseudo-labeling strategy and devises a reward model to assess the reliability of pseudo labels. Beyond pseudo labels as 3D direction vectors, the reward model also incorporates visual embeddings extracted by an off-the-shelf visual encoder and semantic cues from gaze perspective generated by prompting a Multimodal Large Language Model to compute confidence scores. Then, these scores are utilized to select high-quality pseudo labels and weight them for loss computation. Extensive experiments demonstrate that OmniGaze achieves state-of-the-art performance on five datasets under both in-domain and cross-domain settings. Furthermore, we also evaluate the efficacy of OmniGaze as a scalable data engine for gaze estimation, which exhibits robust zero-shot generalization on four unseen datasets.",
    "summary": "",
    "translation": "OmniGaze：受奖励启发的野外可泛化视线估计",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉领域的视线估计技术，属于纯粹的视觉研究方向。虽然标题提到'奖励启发'，但这与推荐系统、搜索或广告的核心技术没有直接关联，也不涉及LLM、Transformer架构或异构数据建模等当前关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13643v1": {
    "title": "Towards Adversarial Robustness and Uncertainty Quantification in DINOv2-based Few-Shot Anomaly Detection",
    "url": "https://www.alphaxiv.org/abs/2510.13643v1",
    "arxiv_id": "2510.13643v1",
    "authors": "Akib Mohammed Khan, Bartosz Krawczyk",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 15:06:45",
    "ori_summary": "Foundation models such as DINOv2 have shown strong performance in few-shot anomaly detection, yet two key questions remain unexamined: (i) how susceptible are these detectors to adversarial perturbations; and (ii) how well do their anomaly scores reflect calibrated uncertainty? Building on AnomalyDINO, a training-free deep nearest-neighbor detector over DINOv2 features, we present one of the first systematic studies of adversarial attacks and uncertainty estimation in this setting. To enable white-box gradient attacks while preserving test-time behavior, we attach a lightweight linear head to frozen DINOv2 features only for crafting perturbations. Using this heuristic, we evaluate the impact of FGSM across the MVTec-AD and VisA datasets and observe consistent drops in F1, AUROC, AP, and G-mean, indicating that imperceptible perturbations can flip nearest-neighbor relations in feature space to induce confident misclassification. Complementing robustness, we probe reliability and find that raw anomaly scores are poorly calibrated, revealing a gap between confidence and correctness that limits safety-critical use. As a simple, strong baseline toward trustworthiness, we apply post-hoc Platt scaling to the anomaly scores for uncertainty estimation. The resulting calibrated posteriors yield significantly higher predictive entropy on adversarially perturbed inputs than on clean ones, enabling a practical flagging mechanism for attack detection while reducing calibration error (ECE). Our findings surface concrete vulnerabilities in DINOv2-based few-shot anomaly detectors and establish an evaluation protocol and baseline for robust, uncertainty-aware anomaly detection. We argue that adversarial robustness and principled uncertainty quantification are not optional add-ons but essential capabilities if anomaly detection systems are to be trustworthy and ready for real-world deployment.",
    "summary": "",
    "translation": "基于DINOv2的少样本异常检测中的对抗鲁棒性与不确定性量化研究",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的异常检测问题，涉及对抗鲁棒性和不确定性量化等通用机器学习主题。虽然DINOv2是视觉Transformer模型，但论文的应用场景（异常检测）与推荐系统、搜索或广告的核心领域没有直接关联，且未明确展示在异构数据统一建模或推荐相关应用中的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13638v1": {
    "title": "Challenges, Advances, and Evaluation Metrics in Medical Image Enhancement: A Systematic Literature Review",
    "url": "https://www.alphaxiv.org/abs/2510.13638v1",
    "arxiv_id": "2510.13638v1",
    "authors": "Chun Wai Chin, Haniza Yazid, Hoi Leong Lee",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 15:01:58",
    "ori_summary": "Medical image enhancement is crucial for improving the quality and interpretability of diagnostic images, ultimately supporting early detection, accurate diagnosis, and effective treatment planning. Despite advancements in imaging technologies such as X-ray, CT, MRI, and ultrasound, medical images often suffer from challenges like noise, artifacts, and low contrast, which limit their diagnostic potential. Addressing these challenges requires robust preprocessing, denoising algorithms, and advanced enhancement methods, with deep learning techniques playing an increasingly significant role. This systematic literature review, following the PRISMA approach, investigates the key challenges, recent advancements, and evaluation metrics in medical image enhancement. By analyzing findings from 39 peer-reviewed studies, this review provides insights into the effectiveness of various enhancement methods across different imaging modalities and the importance of evaluation metrics in assessing their impact. Key issues like low contrast and noise are identified as the most frequent, with MRI and multi-modal imaging receiving the most attention, while specialized modalities such as histopathology, endoscopy, and bone scintigraphy remain underexplored. Out of the 39 studies, 29 utilize conventional mathematical methods, 9 focus on deep learning techniques, and 1 explores a hybrid approach. In terms of image quality assessment, 18 studies employ both reference-based and non-reference-based metrics, 9 rely solely on reference-based metrics, and 12 use only non-reference-based metrics, with a total of 65 IQA metrics introduced, predominantly non-reference-based. This review highlights current limitations, research gaps, and potential future directions for advancing medical image enhancement.",
    "summary": "",
    "translation": "医学图像增强中的挑战、进展与评估指标：系统性文献综述",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像增强领域，属于明确的医学领域特定应用。虽然涉及图像处理技术，但缺乏与推荐系统、搜索或广告领域的任何直接或间接关联。论文内容完全属于被排除的医学应用范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13630v1": {
    "title": "AVAR-Net: A Lightweight Audio-Visual Anomaly Recognition Framework with a Benchmark Dataset",
    "url": "https://www.alphaxiv.org/abs/2510.13630v1",
    "arxiv_id": "2510.13630v1",
    "authors": "Amjid Ali, Zulfiqar Ahmad Khan, Altaf Hussain, Muhammad Munsif, Adnan Hussain, Sung Wook Baik",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 14:56:00",
    "ori_summary": "Anomaly recognition plays a vital role in surveillance, transportation, healthcare, and public safety. However, most existing approaches rely solely on visual data, making them unreliable under challenging conditions such as occlusion, low illumination, and adverse weather. Moreover, the absence of large-scale synchronized audio-visual datasets has hindered progress in multimodal anomaly recognition. To address these limitations, this study presents AVAR-Net, a lightweight and efficient audio-visual anomaly recognition framework designed for real-world environments. AVAR-Net consists of four main modules: an audio feature extractor, a video feature extractor, fusion strategy, and a sequential pattern learning network that models cross-modal relationships for anomaly recognition. Specifically, the Wav2Vec2 model extracts robust temporal features from raw audio, while MobileViT captures both local and global visual representations from video frames. An early fusion mechanism combines these modalities, and a Multi-Stage Temporal Convolutional Network (MTCN) model that learns long-range temporal dependencies within the fused representation, enabling robust spatiotemporal reasoning. A novel Visual-Audio Anomaly Recognition (VAAR) dataset, is also introduced, serving as a medium-scale benchmark containing 3,000 real-world videos with synchronized audio across ten diverse anomaly classes. Experimental evaluations demonstrate that AVAR-Net achieves 89.29% accuracy on VAAR and 88.56% Average Precision on the XD-Violence dataset, improving Average Precision by 2.8% over existing state-of-the-art methods. These results highlight the effectiveness, efficiency, and generalization capability of the proposed framework, as well as the utility of VAAR as a benchmark for advancing multimodal anomaly recognition research.",
    "summary": "",
    "translation": "AVAR-Net：一种轻量级音视频异常识别框架及基准数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于音视频异常识别，属于计算机视觉和音频处理的交叉领域，与推荐系统、搜索或广告的核心技术无关。虽然提到了轻量级框架，但其应用场景（异常识别）和模态（音视频）与RecSys/Search/Ads领域没有直接关联，也不涉及Transformer架构或LLM技术的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13620v1": {
    "title": "Fusion Meets Diverse Conditions: A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues",
    "url": "https://www.alphaxiv.org/abs/2510.13620v1",
    "arxiv_id": "2510.13620v1",
    "authors": "Chen Chen, Kangcheng Bin, Ting Hu, Jiahao Qi, Xingyue Liu, Tianpeng Liu, Zhen Liu, Yongxiang Liu, Ping Zhong",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 14:50:37",
    "ori_summary": "Unmanned aerial vehicles (UAV)-based object detection with visible (RGB) and infrared (IR) images facilitates robust around-the-clock detection, driven by advancements in deep learning techniques and the availability of high-quality dataset. However, the existing dataset struggles to fully capture real-world complexity for limited imaging conditions. To this end, we introduce a high-diversity dataset ATR-UMOD covering varying scenarios, spanning altitudes from 80m to 300m, angles from 0{\\deg} to 75{\\deg}, and all-day, all-year time variations in rich weather and illumination conditions. Moreover, each RGB-IR image pair is annotated with 6 condition attributes, offering valuable high-level contextual information. To meet the challenge raised by such diverse conditions, we propose a novel prompt-guided condition-aware dynamic fusion (PCDF) to adaptively reassign multimodal contributions by leveraging annotated condition cues. By encoding imaging conditions as text prompts, PCDF effectively models the relationship between conditions and multimodal contributions through a task-specific soft-gating transformation. A prompt-guided condition-decoupling module further ensures the availability in practice without condition annotations. Experiments on ATR-UMOD dataset reveal the effectiveness of PCDF.",
    "summary": "",
    "translation": "融合遇见多样条件：基于无人机的多模态目标检测的高多样性基准与基线方法（含条件线索）",
    "relevance_score": 2,
    "reasoning": "该论文主要关注无人机视觉领域的多模态目标检测，属于计算机视觉应用范畴。虽然提到了多模态融合和条件线索，但这些技术主要针对视觉传感器数据和无人机特定场景，与推荐系统、搜索或广告的核心技术栈关联性较弱。其多模态处理方法可能对异构数据处理有启发，但应用领域差异过大，实际相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13565v1": {
    "title": "XD-RCDepth: Lightweight Radar-Camera Depth Estimation with Explainability-Aligned and Distribution-Aware Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.13565v1",
    "arxiv_id": "2510.13565v1",
    "authors": "Huawei Sun, Zixu Wang, Xiangyuan Peng, Julius Ott, Georg Stettinger, Lorenzo Servadei, Robert Wille",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 14:05:33",
    "ori_summary": "Depth estimation remains central to autonomous driving, and radar-camera fusion offers robustness in adverse conditions by providing complementary geometric cues. In this paper, we present XD-RCDepth, a lightweight architecture that reduces the parameters by 29.7% relative to the state-of-the-art lightweight baseline while maintaining comparable accuracy. To preserve performance under compression and enhance interpretability, we introduce two knowledge-distillation strategies: an explainability-aligned distillation that transfers the teacher's saliency structure to the student, and a depth-distribution distillation that recasts depth regression as soft classification over discretized bins. Together, these components reduce the MAE compared with direct training with 7.97% and deliver competitive accuracy with real-time efficiency on nuScenes and ZJU-4DRadarCam datasets.",
    "summary": "",
    "translation": "XD-RCDepth：具有可解释性对齐和分布感知蒸馏的轻量级雷达-相机深度估计",
    "relevance_score": 2,
    "reasoning": "该论文专注于雷达-相机融合的深度估计技术，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术栈关联度较低。虽然轻量化和蒸馏技术可能在某些边缘场景有间接应用，但缺乏明确的RecSys/Search/Ads应用场景或技术迁移路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13562v1": {
    "title": "An efficient approach with theoretical guarantees to simultaneously reconstruct activity and attenuation sinogram for TOF-PET",
    "url": "https://www.alphaxiv.org/abs/2510.13562v1",
    "arxiv_id": "2510.13562v1",
    "authors": "Liyang Hu, Chong Chen",
    "categories": "physics.med-ph, cs.CV, cs.NA, math.NA, 65J15, 65R32, 65J22, 68U10",
    "pub_date": "2025-10-15 14:01:03",
    "ori_summary": "In positron emission tomography (PET), it is indispensable to perform attenuation correction in order to obtain the quantitatively accurate activity map (tracer distribution) in the body. Generally, this is carried out based on the estimated attenuation map obtained from computed tomography or magnetic resonance imaging. However, except for errors in the attenuation correction factors obtained, the additional scan not only brings in new radiation doses and/or increases the scanning time but also leads to severe misalignment induced by various motions during and between the two sequential scans. To address these issues, based on maximum likelihood estimation, we propose a new mathematical model for simultaneously reconstructing the activity and attenuation sinogram from the time-of-flight (TOF)-PET emission data only. Particularly, we make full use of the exclusively exponential form for the attenuation correction factors, and consider the constraint of a total amount of the activity in some mask region in the proposed model. Furthermore, we prove its well-posedness, including the existence, uniqueness and stability of the solution. We propose an alternating update algorithm to solve the model, and also analyze its convergence. Finally, numerical experiments with various TOF-PET emission data demonstrate that the proposed method is of numerical convergence and robust to noise, and outperforms some state-of-the-art methods in terms of accuracy and efficiency, and has the capability of autonomous attenuation correction.",
    "summary": "",
    "translation": "一种具有理论保证的高效方法，用于同时重建飞行时间正电子发射断层扫描的活动和衰减正弦图",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于医学成像领域的正电子发射断层扫描（PET）重建技术，属于医学/生物医学应用范畴。虽然提到了高效方法和理论保证，但这些技术是针对特定医学成像问题的，与推荐系统、搜索、广告或LLM技术没有任何直接或间接的关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13557v1": {
    "title": "Modeling Cultural Bias in Facial Expression Recognition with Adaptive Agents",
    "url": "https://www.alphaxiv.org/abs/2510.13557v1",
    "arxiv_id": "2510.13557v1",
    "authors": "David Freire-Obregón, José Salas-Cáceres, Javier Lorenzo-Navarro, Oliverio J. Santana, Daniel Hernández-Sosa, Modesto Castrillón-Santana",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 13:53:30",
    "ori_summary": "Facial expression recognition (FER) must remain robust under both cultural variation and perceptually degraded visual conditions, yet most existing evaluations assume homogeneous data and high-quality imagery. We introduce an agent-based, streaming benchmark that reveals how cross-cultural composition and progressive blurring interact to shape face recognition robustness. Each agent operates in a frozen CLIP feature space with a lightweight residual adapter trained online at sigma=0 and fixed during testing. Agents move and interact on a 5x5 lattice, while the environment provides inputs with sigma-scheduled Gaussian blur. We examine monocultural populations (Western-only, Asian-only) and mixed environments with balanced (5/5) and imbalanced (8/2, 2/8) compositions, as well as different spatial contact structures. Results show clear asymmetric degradation curves between cultural groups: JAFFE (Asian) populations maintain higher performance at low blur but exhibit sharper drops at intermediate stages, whereas KDEF (Western) populations degrade more uniformly. Mixed populations exhibit intermediate patterns, with balanced mixtures mitigating early degradation, but imbalanced settings amplify majority-group weaknesses under high blur. These findings quantify how cultural composition and interaction structure influence the robustness of FER as perceptual conditions deteriorate.",
    "summary": "",
    "translation": "基于自适应代理的面部表情识别中的文化偏见建模",
    "relevance_score": 1,
    "reasoning": "该论文专注于面部表情识别中的文化偏见问题，这属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术无关。论文内容涉及文化偏见建模和自适应代理，这些主题在用户兴趣建模、公平性评估或个性化推荐中没有任何直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13546v1": {
    "title": "Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU",
    "url": "https://www.alphaxiv.org/abs/2510.13546v1",
    "arxiv_id": "2510.13546v1",
    "authors": "Ruiqi Ye, Mikel Luján",
    "categories": "cs.CV, cs.ET, cs.PF, cs.RO, C.3; C.4; I.4.6",
    "pub_date": "2025-10-15 13:40:55",
    "ori_summary": "Feature detection is a common yet time-consuming module in Simultaneous Localization and Mapping (SLAM) implementations, which are increasingly deployed on power-constrained platforms, such as drones. Graphics Processing Units (GPUs) have been a popular accelerator for computer vision in general, and feature detection and SLAM in particular. On the other hand, System-on-Chips (SoCs) with integrated Field Programmable Gate Array (FPGA) are also widely available. This paper presents the first study of hardware-accelerated feature detectors considering a Visual SLAM (V-SLAM) pipeline. We offer new insights by comparing the best GPU-accelerated FAST, Harris, and SuperPoint implementations against the FPGA-accelerated counterparts on modern SoCs (Nvidia Jetson Orin and AMD Versal). The evaluation shows that when using a non-learning-based feature detector such as FAST and Harris, their GPU implementations, and the GPU-accelerated V-SLAM can achieve better run-time performance and energy efficiency than the FAST and Harris FPGA implementations as well as the FPGA-accelerated V-SLAM. However, when considering a learning-based detector such as SuperPoint, its FPGA implementation can achieve better run-time performance and energy efficiency (up to 3.1$\\times$ and 1.4$\\times$ improvements, respectively) than the GPU implementation. The FPGA-accelerated V-SLAM can also achieve comparable run-time performance compared to the GPU-accelerated V-SLAM, with better FPS in 2 out of 5 dataset sequences. When considering the accuracy, the results show that the GPU-accelerated V-SLAM is more accurate than the FPGA-accelerated V-SLAM in general. Last but not least, the use of hardware acceleration for feature detection could further improve the performance of the V-SLAM pipeline by having the global bundle adjustment module invoked less frequently without sacrificing accuracy.",
    "summary": "",
    "translation": "面向视觉SLAM的加速特征检测器：FPGA与GPU对比研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的SLAM（同时定位与地图构建）技术，属于纯粹的视觉领域研究。虽然提到了硬件加速（FPGA/GPU），但其应用场景是视觉SLAM而非推荐系统、搜索或广告领域。论文内容与我的关注点（推荐系统、搜索广告中的LLM应用、Transformer架构进展等）没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13540v1": {
    "title": "Learning Neural Parametric 3D Breast Shape Models for Metrical Surface Reconstruction From Monocular RGB Videos",
    "url": "https://www.alphaxiv.org/abs/2510.13540v1",
    "arxiv_id": "2510.13540v1",
    "authors": "Maximilian Weiherer, Antonia von Riedheim, Vanessa Brébant, Bernhard Egger, Christoph Palm",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 13:35:03",
    "ori_summary": "We present a neural parametric 3D breast shape model and, based on this model, introduce a low-cost and accessible 3D surface reconstruction pipeline capable of recovering accurate breast geometry from a monocular RGB video. In contrast to widely used, commercially available yet prohibitively expensive 3D breast scanning solutions and existing low-cost alternatives, our method requires neither specialized hardware nor proprietary software and can be used with any device that is able to record RGB videos. The key building blocks of our pipeline are a state-of-the-art, off-the-shelf Structure-from-motion pipeline, paired with a parametric breast model for robust and metrically correct surface reconstruction. Our model, similarly to the recently proposed implicit Regensburg Breast Shape Model (iRBSM), leverages implicit neural representations to model breast shapes. However, unlike the iRBSM, which employs a single global neural signed distance function (SDF), our approach -- inspired by recent state-of-the-art face models -- decomposes the implicit breast domain into multiple smaller regions, each represented by a local neural SDF anchored at anatomical landmark positions. When incorporated into our surface reconstruction pipeline, the proposed model, dubbed liRBSM (short for localized iRBSM), significantly outperforms the iRBSM in terms of reconstruction quality, yielding more detailed surface reconstruction than its global counterpart. Overall, we find that the introduced pipeline is able to recover high-quality 3D breast geometry within an error margin of less than 2 mm. Our method is fast (requires less than six minutes), fully transparent and open-source, and -- together with the model -- publicly available at https://rbsm.re-mic.de/local-implicit.",
    "summary": "",
    "translation": "学习神经参数化3D乳房形状模型用于从单目RGB视频进行度量表面重建",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于3D乳房形状重建的计算机视觉应用，属于医学成像和生物医学工程领域。该研究不涉及推荐系统、搜索或广告的核心技术，也没有在Transformer架构、LLM技术或异构数据建模方面提供任何可能应用于RecSys/Search/Ads的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13534v1": {
    "title": "High Semantic Features for the Continual Learning of Complex Emotions: a Lightweight Solution",
    "url": "https://www.alphaxiv.org/abs/2510.13534v1",
    "arxiv_id": "2510.13534v1",
    "authors": "Thibault Geoffroy, gauthier Gerspacher, Lionel Prevost",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 13:27:41",
    "ori_summary": "Incremental learning is a complex process due to potential catastrophic forgetting of old tasks when learning new ones. This is mainly due to transient features that do not fit from task to task. In this paper, we focus on complex emotion recognition. First, we learn basic emotions and then, incrementally, like humans, complex emotions. We show that Action Units, describing facial muscle movements, are non-transient, highly semantical features that outperform those extracted by both shallow and deep convolutional neural networks. Thanks to this ability, our approach achieves interesting results when learning incrementally complex, compound emotions with an accuracy of 0.75 on the CFEE dataset and can be favorably compared to state-of-the-art results. Moreover, it results in a lightweight model with a small memory footprint.",
    "summary": "",
    "translation": "复杂情感持续学习的高语义特征：一种轻量级解决方案",
    "relevance_score": 2,
    "reasoning": "该论文主要关注情感识别和持续学习，属于情感计算领域，与推荐系统、搜索或广告的核心技术关联较弱。虽然轻量级解决方案可能在某些边缘计算场景中有间接价值，但论文主题与我的核心关注点（推荐算法、搜索技术、广告排名、Transformer架构改进等）没有直接相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13515v1": {
    "title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
    "url": "https://www.alphaxiv.org/abs/2510.13515v1",
    "arxiv_id": "2510.13515v1",
    "authors": "Tiancheng Gu, Kaicheng Yang, Kaichen Zhang, Xiang An, Ziyong Feng, Yueyi Zhang, Weidong Cai, Jiankang Deng, Lidong Bing",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 13:07:00",
    "ori_summary": "Universal multimodal embedding models are foundational to various tasks. Existing approaches typically employ in-batch negative mining by measuring the similarity of query-candidate pairs. However, these methods often struggle to capture subtle semantic differences among candidates and lack diversity in negative samples. Moreover, the embeddings exhibit limited discriminative ability in distinguishing false and hard negatives. In this paper, we leverage the advanced understanding capabilities of MLLMs to enhance representation learning and present a novel Universal Multimodal Embedding (UniME-V2) model. Our approach first constructs a potential hard negative set through global retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes MLLMs to assess the semantic alignment of query-candidate pairs and generate soft semantic matching scores. These scores serve as a foundation for hard negative mining, mitigating the impact of false negatives and enabling the identification of diverse, high-quality hard negatives. Furthermore, the semantic matching scores are used as soft labels to mitigate the rigid one-to-one mapping constraint. By aligning the similarity matrix with the soft semantic matching score matrix, the model learns semantic distinctions among candidates, significantly enhancing its discriminative capacity. To further improve performance, we propose UniME-V2-Reranker, a reranking model trained on our mined hard negatives through a joint pairwise and listwise optimization approach. We conduct comprehensive experiments on the MMEB benchmark and multiple retrieval tasks, demonstrating that our method achieves state-of-the-art performance on average across all tasks.",
    "summary": "",
    "translation": "UniME-V2：作为评判者的多模态大语言模型用于通用多模态嵌入学习",
    "relevance_score": 3,
    "reasoning": "该论文主要关注多模态大语言模型作为评判者的应用，属于纯粹的LLM评估技术。虽然涉及多模态嵌入学习，但其核心是模型评估而非直接应用于推荐系统、搜索或广告领域。对于'赋能技术'类别，该技术可能用于评估多模态推荐模型的质量，但应用场景较为间接和有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13493v1": {
    "title": "ExpressNet-MoE: A Hybrid Deep Neural Network for Emotion Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.13493v1",
    "arxiv_id": "2510.13493v1",
    "authors": "Deeptimaan Banerjee, Prateek Gothwal, Ashis Kumer Biswas",
    "categories": "cs.CV, cs.LG, I.2.10; I.5.2; H.4.2",
    "pub_date": "2025-10-15 12:42:49",
    "ori_summary": "In many domains, including online education, healthcare, security, and human-computer interaction, facial emotion recognition (FER) is essential. Real-world FER is still difficult despite its significance because of some factors such as variable head positions, occlusions, illumination shifts, and demographic diversity. Engagement detection, which is essential for applications like virtual learning and customer services, is frequently challenging due to FER limitations by many current models. In this article, we propose ExpressNet-MoE, a novel hybrid deep learning model that blends both Convolution Neural Networks (CNNs) and Mixture of Experts (MoE) framework, to overcome the difficulties. Our model dynamically chooses the most pertinent expert networks, thus it aids in the generalization and providing flexibility to model across a wide variety of datasets. Our model improves on the accuracy of emotion recognition by utilizing multi-scale feature extraction to collect both global and local facial features. ExpressNet-MoE includes numerous CNN-based feature extractors, a MoE module for adaptive feature selection, and finally a residual network backbone for deep feature learning. To demonstrate efficacy of our proposed model we evaluated on several datasets, and compared with current state-of-the-art methods. Our model achieves accuracies of 74.77% on AffectNet (v7), 72.55% on AffectNet (v8), 84.29% on RAF-DB, and 64.66% on FER-2013. The results show how adaptive our model is and how it may be used to develop end-to-end emotion recognition systems in practical settings. Reproducible codes and results are made publicly accessible at https://github.com/DeeptimaanB/ExpressNet-MoE.",
    "summary": "",
    "translation": "ExpressNet-MoE：一种用于情感识别的混合深度神经网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于情感识别这一与推荐系统、搜索或广告无关的特定应用领域。虽然提到了混合神经网络和MoE架构，但核心应用场景（情感识别）属于语音/心理学领域，没有明确的路径可以应用于推荐系统、搜索或广告中的核心问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13464v1": {
    "title": "Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.13464v1",
    "arxiv_id": "2510.13464v1",
    "authors": "Emily Miller, Michael Milford, Muhammad Burhan Hafez, SD Ramchurn, Shoaib Ehsan",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-15 12:12:55",
    "ori_summary": "Visual Place Recognition (VPR) enables robots and autonomous vehicles to identify previously visited locations by matching current observations against a database of known places. However, VPR systems face significant challenges when deployed across varying visual environments, lighting conditions, seasonal changes, and viewpoints changes. Failure-critical VPR applications, such as loop closure detection in simultaneous localization and mapping (SLAM) pipelines, require robust estimation of place matching uncertainty. We propose three training-free uncertainty metrics that estimate prediction confidence by analyzing inherent statistical patterns in similarity scores from any existing VPR method. Similarity Distribution (SD) quantifies match distinctiveness by measuring score separation between candidates; Ratio Spread (RS) evaluates competitive ambiguity among top-scoring locations; and Statistical Uncertainty (SU) is a combination of SD and RS that provides a unified metric that generalizes across datasets and VPR methods without requiring validation data to select the optimal metric. All three metrics operate without additional model training, architectural modifications, or computationally expensive geometric verification. Comprehensive evaluation across nine state-of-the-art VPR methods and six benchmark datasets confirms that our metrics excel at discriminating between correct and incorrect VPR matches, and consistently outperform existing approaches while maintaining negligible computational overhead, making it deployable for real-time robotic applications across varied environmental conditions with improved precision-recall performance.",
    "summary": "",
    "translation": "透过怀疑的视角：视觉地点识别的鲁棒高效不确定性估计",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉中的视觉地点识别和不确定性估计，属于纯粹的视觉技术范畴。虽然不确定性估计在推荐系统中可能有潜在应用，但该论文没有明确展示与推荐系统、搜索或广告的关联，也没有涉及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13454v1": {
    "title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator",
    "url": "https://www.alphaxiv.org/abs/2510.13454v1",
    "arxiv_id": "2510.13454v1",
    "authors": "Hyojun Go, Dominik Narnhofer, Goutam Bhat, Prune Truong, Federico Tombari, Konrad Schindler",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 11:55:08",
    "ori_summary": "The rapid progress of large, pretrained models for both visual content generation and 3D reconstruction opens up new possibilities for text-to-3D generation. Intuitively, one could obtain a formidable 3D scene generator if one were able to combine the power of a modern latent text-to-video model as \"generator\" with the geometric abilities of a recent (feedforward) 3D reconstruction system as \"decoder\". We introduce VIST3A, a general framework that does just that, addressing two main challenges. First, the two components must be joined in a way that preserves the rich knowledge encoded in their weights. We revisit model stitching, i.e., we identify the layer in the 3D decoder that best matches the latent representation produced by the text-to-video generator and stitch the two parts together. That operation requires only a small dataset and no labels. Second, the text-to-video generator must be aligned with the stitched 3D decoder, to ensure that the generated latents are decodable into consistent, perceptually convincing 3D scene geometry. To that end, we adapt direct reward finetuning, a popular technique for human preference alignment. We evaluate the proposed VIST3A approach with different video generators and 3D reconstruction models. All tested pairings markedly improve over prior text-to-3D models that output Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also enables high-quality text-to-pointmap generation.",
    "summary": "",
    "translation": "VIST3A：通过将多视图重建网络拼接至视频生成器实现文本到3D生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本到3D生成技术，属于计算机视觉和3D内容生成领域。虽然涉及多模态处理，但主要关注3D视觉内容生成，与推荐系统、搜索或广告的核心技术需求没有直接关联，也不具备明显的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13452v1": {
    "title": "Near-Infrared Hyperspectral Imaging Applications in Food Analysis -- Improving Algorithms and Methodologies",
    "url": "https://www.alphaxiv.org/abs/2510.13452v1",
    "arxiv_id": "2510.13452v1",
    "authors": "Ole-Christian Galbo Engstrøm",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-15 11:53:01",
    "ori_summary": "This thesis investigates the application of near-infrared hyperspectral imaging (NIR-HSI) for food quality analysis. The investigation is conducted through four studies operating with five research hypotheses. For several analyses, the studies compare models based on convolutional neural networks (CNNs) and partial least squares (PLS). Generally, joint spatio-spectral analysis with CNNs outperforms spatial analysis with CNNs and spectral analysis with PLS when modeling parameters where chemical and physical visual information are relevant. When modeling chemical parameters with a 2-dimensional (2D) CNN, augmenting the CNN with an initial layer dedicated to performing spectral convolution enhances its predictive performance by learning a spectral preprocessing similar to that applied by domain experts. Still, PLS-based spectral modeling performs equally well for analysis of the mean content of chemical parameters in samples and is the recommended approach. Modeling the spatial distribution of chemical parameters with NIR-HSI is limited by the ability to obtain spatially resolved reference values. Therefore, a study used bulk mean references for chemical map generation of fat content in pork bellies. A PLS-based approach gave non-smooth chemical maps and pixel-wise predictions outside the range of 0-100\\%. Conversely, a 2D CNN augmented with a spectral convolution layer mitigated all issues arising with PLS. The final study attempted to model barley's germinative capacity by analyzing NIR spectra, RGB images, and NIR-HSI images. However, the results were inconclusive due to the dataset's low degree of germination. Additionally, this thesis has led to the development of two open-sourced Python packages. The first facilitates fast PLS-based modeling, while the second facilitates very fast cross-validation of PLS and other classical machine learning models with a new algorithm.",
    "summary": "",
    "translation": "近红外高光谱成像在食品分析中的应用——算法与方法的改进",
    "relevance_score": 1,
    "reasoning": "该论文专注于食品分析领域的近红外高光谱成像技术，属于特定领域应用（食品科学/农业），与推荐系统、搜索或广告的核心技术进展完全无关。论文内容涉及计算机视觉在特定行业的应用，没有展示出在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13441v1": {
    "title": "Steerable Conditional Diffusion for Domain Adaptation in PET Image Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2510.13441v1",
    "arxiv_id": "2510.13441v1",
    "authors": "George Webber, Alexander Hammers, Andrew P. King, Andrew J. Reader",
    "categories": "physics.med-ph, cs.CV, cs.LG",
    "pub_date": "2025-10-15 11:40:03",
    "ori_summary": "Diffusion models have recently enabled state-of-the-art reconstruction of positron emission tomography (PET) images while requiring only image training data. However, domain shift remains a key concern for clinical adoption: priors trained on images from one anatomy, acquisition protocol or pathology may produce artefacts on out-of-distribution data. We propose integrating steerable conditional diffusion (SCD) with our previously-introduced likelihood-scheduled diffusion (PET-LiSch) framework to improve the alignment of the diffusion model's prior to the target subject. At reconstruction time, for each diffusion step, we use low-rank adaptation (LoRA) to align the diffusion model prior with the target domain on the fly. Experiments on realistic synthetic 2D brain phantoms demonstrate that our approach suppresses hallucinated artefacts under domain shift, i.e. when our diffusion model is trained on perturbed images and tested on normal anatomy, our approach suppresses the hallucinated structure, outperforming both OSEM and diffusion model baselines qualitatively and quantitatively. These results provide a proof-of-concept that steerable priors can mitigate domain shift in diffusion-based PET reconstruction and motivate future evaluation on real data.",
    "summary": "",
    "translation": "用于PET图像重建领域自适应的可操控条件扩散模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（PET图像重建）的领域自适应问题，这属于医学/生物医学应用领域。虽然提到了扩散模型技术，但应用场景与推荐系统、搜索或广告完全无关。该研究没有展示任何在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13433v1": {
    "title": "Beyond Pixels: A Differentiable Pipeline for Probing Neuronal Selectivity in 3D",
    "url": "https://www.alphaxiv.org/abs/2510.13433v1",
    "arxiv_id": "2510.13433v1",
    "authors": "Pavithra Elumalai, Mohammad Bashiri, Goirik Chakrabarty, Suhas Shrinivasan, Fabian H. Sinz",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 11:29:21",
    "ori_summary": "Visual perception relies on inference of 3D scene properties such as shape, pose, and lighting. To understand how visual sensory neurons enable robust perception, it is crucial to characterize their selectivity to such physically interpretable factors. However, current approaches mainly operate on 2D pixels, making it difficult to isolate selectivity for physical scene properties. To address this limitation, we introduce a differentiable rendering pipeline that optimizes deformable meshes to obtain MEIs directly in 3D. The method parameterizes mesh deformations with radial basis functions and learns offsets and scales that maximize neuronal responses while enforcing geometric regularity. Applied to models of monkey area V4, our approach enables probing neuronal selectivity to interpretable 3D factors such as pose and lighting. This approach bridges inverse graphics with systems neuroscience, offering a way to probe neural selectivity with physically grounded, 3D stimuli beyond conventional pixel-based methods.",
    "summary": "",
    "translation": "超越像素：用于探测3D神经元选择性的可微分流程",
    "relevance_score": 1,
    "reasoning": "该论文专注于神经科学领域的3D神经元选择性探测，属于生物医学应用范畴。论文内容涉及神经生物学和3D视觉分析，与推荐系统、搜索、广告或相关使能技术没有任何直接或潜在的应用关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13432v1": {
    "title": "CoDS: Enhancing Collaborative Perception in Heterogeneous Scenarios via Domain Separation",
    "url": "https://www.alphaxiv.org/abs/2510.13432v1",
    "arxiv_id": "2510.13432v1",
    "authors": "Yushan Han, Hui Zhang, Honglei Zhang, Chuntao Ding, Yuanzhouhan Cao, Yidong Li",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 11:29:14",
    "ori_summary": "Collaborative perception has been proven to improve individual perception in autonomous driving through multi-agent interaction. Nevertheless, most methods often assume identical encoders for all agents, which does not hold true when these models are deployed in real-world applications. To realize collaborative perception in actual heterogeneous scenarios, existing methods usually align neighbor features to those of the ego vehicle, which is vulnerable to noise from domain gaps and thus fails to address feature discrepancies effectively. Moreover, they adopt transformer-based modules for domain adaptation, which causes the model inference inefficiency on mobile devices. To tackle these issues, we propose CoDS, a Collaborative perception method that leverages Domain Separation to address feature discrepancies in heterogeneous scenarios. The CoDS employs two feature alignment modules, i.e., Lightweight Spatial-Channel Resizer (LSCR) and Distribution Alignment via Domain Separation (DADS). Besides, it utilizes the Domain Alignment Mutual Information (DAMI) loss to ensure effective feature alignment. Specifically, the LSCR aligns the neighbor feature across spatial and channel dimensions using a lightweight convolutional layer. Subsequently, the DADS mitigates feature distribution discrepancy with encoder-specific and encoder-agnostic domain separation modules. The former removes domain-dependent information and the latter captures task-related information. During training, the DAMI loss maximizes the mutual information between aligned heterogeneous features to enhance the domain separation process. The CoDS employs a fully convolutional architecture, which ensures high inference efficiency. Extensive experiments demonstrate that the CoDS effectively mitigates feature discrepancies in heterogeneous scenarios and achieves a trade-off between detection accuracy and inference efficiency.",
    "summary": "",
    "translation": "CoDS：通过领域分离增强异构场景中的协同感知",
    "relevance_score": 2,
    "reasoning": "该论文主要关注协同感知和领域分离技术，这属于异构场景下的多智能体感知问题，与自动驾驶、机器人等领域更相关。虽然提到了异构场景处理，但其核心应用场景和问题定义与推荐系统、搜索或广告的当前关注点关联度较低，缺乏明确的潜在应用路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13419v1": {
    "title": "Ultra High-Resolution Image Inpainting with Patch-Based Content Consistency Adapter",
    "url": "https://www.alphaxiv.org/abs/2510.13419v1",
    "arxiv_id": "2510.13419v1",
    "authors": "Jianhui Zhang, Sheng Cheng, Qirui Sun, Jia Liu, Wang Luyang, Chaoyu Feng, Chen Fang, Lei Lei, Jue Wang, Shuaicheng Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 11:18:24",
    "ori_summary": "In this work, we present Patch-Adapter, an effective framework for high-resolution text-guided image inpainting. Unlike existing methods limited to lower resolutions, our approach achieves 4K+ resolution while maintaining precise content consistency and prompt alignment, two critical challenges in image inpainting that intensify with increasing resolution and texture complexity. Patch-Adapter leverages a two-stage adapter architecture to scale the diffusion model's resolution from 1K to 4K+ without requiring structural overhauls: (1) Dual Context Adapter learns coherence between masked and unmasked regions at reduced resolutions to establish global structural consistency; and (2) Reference Patch Adapter implements a patch-level attention mechanism for full-resolution inpainting, preserving local detail fidelity through adaptive feature fusion. This dual-stage architecture uniquely addresses the scalability gap in high-resolution inpainting by decoupling global semantics from localized refinement. Experiments demonstrate that Patch-Adapter not only resolves artifacts common in large-scale inpainting but also achieves state-of-the-art performance on the OpenImages and Photo-Concept-Bucket datasets, outperforming existing methods in both perceptual quality and text-prompt adherence.",
    "summary": "",
    "translation": "基于补丁内容一致性适配器的超高分辨率图像修复",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉中的图像修复技术，属于纯粹的视觉处理领域。虽然论文涉及高分辨率处理和内容一致性，但这些技术没有明确的推荐系统、搜索或广告应用场景，也不涉及LLM、Transformer架构或异质数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13418v1": {
    "title": "Reinforcement Learning Meets Masked Generative Models: Mask-GRPO for Text-to-Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.13418v1",
    "arxiv_id": "2510.13418v1",
    "authors": "Yifu Luo, Xinhao Hu, Keyu Fan, Haoyuan Sun, Zeyu Chen, Bo Xia, Tiantian Zhang, Yongzhe Chang, Xueqian Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 11:18:12",
    "ori_summary": "Reinforcement learning (RL) has garnered increasing attention in text-to-image (T2I) generation. However, most existing RL approaches are tailored to either diffusion models or autoregressive models, overlooking an important alternative: masked generative models. In this work, we propose Mask-GRPO, the first method to incorporate Group Relative Policy Optimization (GRPO)-based RL into this overlooked paradigm. Our core insight is to redefine the transition probability, which is different from current approaches, and formulate the unmasking process as a multi-step decision-making problem. To further enhance our method, we explore several useful strategies, including removing the KL constraint, applying the reduction strategy, and filtering out low-quality samples. Using Mask-GRPO, we improve a base model, Show-o, with substantial improvements on standard T2I benchmarks and preference alignment, outperforming existing state-of-the-art approaches. The code is available on https://github.com/xingzhejun/Mask-GRPO",
    "summary": "",
    "translation": "强化学习遇见掩码生成模型：用于文本到图像生成的Mask-GRPO",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文本到图像生成，属于纯粹的AIGC和内容生成领域，与我的核心关注点无关。虽然涉及强化学习，但应用场景是图像生成而非推荐系统、搜索或广告，因此相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13394v1": {
    "title": "Spatial-DISE: A Unified Benchmark for Evaluating Spatial Reasoning in Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13394v1",
    "arxiv_id": "2510.13394v1",
    "authors": "Xinmiao Huang, Qisong He, Zhenglin Huang, Boxuan Wang, Zhuoyun Li, Guangliang Cheng, Yi Dong, Xiaowei Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 10:44:01",
    "ori_summary": "Spatial reasoning ability is crucial for Vision Language Models (VLMs) to support real-world applications in diverse domains including robotics, augmented reality, and autonomous navigation. Unfortunately, existing benchmarks are inadequate in assessing spatial reasoning ability, especially the \\emph{intrinsic-dynamic} spatial reasoning which is a fundamental aspect of human spatial cognition. In this paper, we propose a unified benchmark, \\textbf{Spatial-DISE}, based on a cognitively grounded taxonomy that categorizes tasks into four fundamental quadrants: \\textbf{I}ntrinsic-\\textbf{S}tatic, Intrinsic-\\textbf{D}ynamic, \\textbf{E}xtrinsic-Static, and Extrinsic-Dynamic spatial reasoning. Moreover, to address the issue of data scarcity, we develop a scalable and automated pipeline to generate diverse and verifiable spatial reasoning questions, resulting in a new \\textbf{Spatial-DISE} dataset that includes Spatial-DISE Bench (559 evaluation VQA pairs) and Spatial-DISE-12K (12K+ training VQA pairs). Our comprehensive evaluation across 28 state-of-the-art VLMs reveals that, current VLMs have a large and consistent gap to human competence, especially on multi-step multi-view spatial reasoning. Spatial-DISE offers a robust framework, valuable dataset, and clear direction for future research toward human-like spatial intelligence. Benchmark, dataset, and code will be publicly released.",
    "summary": "",
    "translation": "Spatial-DISE：评估视觉语言模型中空间推理能力的统一基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于视觉语言模型的评估基准，属于纯粹的VLM评估范畴，与推荐系统、搜索或广告的核心技术进展无关。虽然提到了空间推理，但这是纯粹的计算机视觉评估任务，没有展示在异构数据处理或推荐/搜索应用方面的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13390v1": {
    "title": "Generalizing WiFi Gesture Recognition via Large-Model-Aware Semantic Distillation and Alignment",
    "url": "https://www.alphaxiv.org/abs/2510.13390v1",
    "arxiv_id": "2510.13390v1",
    "authors": "Feng-Qi Cui, Yu-Tong Guo, Tianyue Zheng, Jinyang Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 10:28:50",
    "ori_summary": "WiFi-based gesture recognition has emerged as a promising RF sensing paradigm for enabling non-contact and privacy-preserving human-computer interaction in AIoT environments. However, existing methods often suffer from limited generalization and semantic expressiveness due to the domain-sensitive nature of Channel State Information and the lack of high-level gesture abstraction. To address these challenges, we propose a novel generalization framework, termed Large-Model-Aware Semantic Distillation and Alignment (GLSDA), which leverages the semantic prior of pre-trained large foundation models to enhance gesture representation learning in both in-domain and cross-domain scenarios. Specifically, we first design a dual-path CSI encoding pipeline that captures geometric and dynamic gesture patterns via CSI-Ratio phase sequences and Doppler spectrograms. These representations are then fed into a Multiscale Semantic Encoder, which learns robust temporal embeddings and aligns them with gesture semantics through cross-modal attention mechanisms. To further enhance category discrimination, we introduce a Semantic-Aware Soft Supervision scheme that encodes inter-class correlations and reduces label ambiguity, especially for semantically similar gestures. Finally, we develop a Robust Dual-Distillation strategy to compress the aligned model into a lightweight student network, jointly distilling intermediate features and semantic-informed soft labels from the teacher model. Extensive experiments on the Widar3.0 benchmark show that GLSDA consistently outperforms state-of-the-art methods in both in-domain and cross-domain gesture recognition tasks, while significantly reducing model size and inference latency. Our method offers a scalable and deployable solution for generalized RF-based gesture interfaces in real-world AIoT applications.",
    "summary": "",
    "translation": "通过大模型感知语义蒸馏与对齐实现WiFi手势识别的泛化",
    "relevance_score": 2,
    "reasoning": "该论文主要关注WiFi手势识别这一特定感知任务，属于信号处理和计算机视觉的交叉领域。虽然提到了大模型和语义蒸馏技术，但其应用场景（手势识别）与推荐系统、搜索或广告的核心业务没有直接关联，也不涉及这些领域的核心架构或建模挑战。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13381v1": {
    "title": "Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering",
    "url": "https://www.alphaxiv.org/abs/2510.13381v1",
    "arxiv_id": "2510.13381v1",
    "authors": "Siddharth Tourani, Jayaram Reddy, Akash Kumbar, Satyajit Tourani, Nishant Goyal, Madhava Krishna, N. Dinesh Reddy, Muhammad Haris Khan",
    "categories": "cs.CV, cs.GR",
    "pub_date": "2025-10-15 10:21:36",
    "ori_summary": "Dynamic scene rendering and reconstruction play a crucial role in computer vision and augmented reality. Recent methods based on 3D Gaussian Splatting (3DGS), have enabled accurate modeling of dynamic urban scenes, but for urban scenes they require both camera and LiDAR data, ground-truth 3D segmentations and motion data in the form of tracklets or pre-defined object templates such as SMPL. In this work, we explore whether a combination of 2D object agnostic priors in the form of depth and point tracking coupled with a signed distance function (SDF) representation for dynamic objects can be used to relax some of these requirements. We present a novel approach that integrates Signed Distance Functions (SDFs) with 3D Gaussian Splatting (3DGS) to create a more robust object representation by harnessing the strengths of both methods. Our unified optimization framework enhances the geometric accuracy of 3D Gaussian splatting and improves deformation modeling within the SDF, resulting in a more adaptable and precise representation. We demonstrate that our method achieves state-of-the-art performance in rendering metrics even without LiDAR data on urban scenes. When incorporating LiDAR, our approach improved further in reconstructing and generating novel views across diverse object categories, without ground-truth 3D motion annotation. Additionally, our method enables various scene editing tasks, including scene decomposition, and scene composition.",
    "summary": "",
    "translation": "利用2D先验和符号距离场引导进行动态城市场景渲染",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的动态场景渲染技术，属于纯粹的视觉领域研究。虽然提到了2D先验和SDF引导等概念，但论文内容明显围绕城市场景的视觉渲染，与推荐系统、搜索或广告的核心技术领域没有直接关联，也不涉及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13375v1": {
    "title": "DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.13375v1",
    "arxiv_id": "2510.13375v1",
    "authors": "Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Zhuoguang Chen, Tao Jiang, Hang Zhao",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 10:09:00",
    "ori_summary": "Vision-Language-Action (VLA) models have recently shown impressive generalization and language-guided manipulation capabilities. However, their performance degrades on tasks requiring precise spatial reasoning due to limited spatial reasoning inherited from Vision-Language Models (VLMs). Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3D space, which reduces training efficiency and is still insufficient for accurate spatial understanding. In this work, we present DepthVLA, a simple yet effective VLA architecture that explicitly incorporates spatial awareness through a pretrained depth prediction module. DepthVLA adopts a mixture-of-transformers design that unifies a VLM, a depth transformer, and an action expert with fully shared attentions, forming an end-to-end model with enhanced spatial reasoning. Extensive evaluations in both real-world and simulated environments show that DepthVLA outperforms state-of-the-art approaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs. 93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator. Our code will be made publicly available.",
    "summary": "",
    "translation": "DepthVLA：通过深度感知空间推理增强视觉-语言-动作模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉-语言-动作模型和深度感知空间推理，属于机器人学和具身AI领域。虽然提到了多模态建模概念，但其核心应用方向（动作执行、机器人控制）与推荐系统、搜索或广告的典型应用场景关联度极低，难以看出在RecSys/Search/Ads领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13364v1": {
    "title": "Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity",
    "url": "https://www.alphaxiv.org/abs/2510.13364v1",
    "arxiv_id": "2510.13364v1",
    "authors": "MingZe Tang, Jubal Chandy Jacob",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 09:53:46",
    "ori_summary": "Recent Vision-Language Models (VLMs) enable zero-shot classification by aligning images and text in a shared space, a promising approach for data-scarce conditions. However, the influence of prompt design on recognizing visually similar categories, such as human postures, is not well understood. This study investigates how prompt specificity affects the zero-shot classification of sitting, standing, and walking/running on a small, 285-image COCO-derived dataset. A suite of modern VLMs, including OpenCLIP, MetaCLIP 2, and SigLip, were evaluated using a three-tiered prompt design that systematically increases linguistic detail. Our findings reveal a compelling, counter-intuitive trend: for the highest-performing models (MetaCLIP 2 and OpenCLIP), the simplest, most basic prompts consistently achieve the best results. Adding descriptive detail significantly degrades performance for instance, MetaCLIP 2's multi-class accuracy drops from 68.8\\% to 55.1\\% a phenomenon we term \"prompt overfitting\". Conversely, the lower-performing SigLip model shows improved classification on ambiguous classes when given more descriptive, body-cue-based prompts.",
    "summary": "",
    "translation": "语言作为标签：数据稀缺条件下日常姿态的零样本多模态分类",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态分类和姿态识别，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术关联较弱。虽然提到了零样本学习和多模态概念，但缺乏明确的RecSys/Search/Ads应用场景，且日常姿态分类的应用领域更偏向人机交互或健康监测。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13349v1": {
    "title": "No-Reference Rendered Video Quality Assessment: Dataset and Metrics",
    "url": "https://www.alphaxiv.org/abs/2510.13349v1",
    "arxiv_id": "2510.13349v1",
    "authors": "Sipeng Yang, Jiayu Ji, Qingchuan Zhu, Zhiyao Yang, Xiaogang Jin",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 09:36:52",
    "ori_summary": "Quality assessment of videos is crucial for many computer graphics applications, including video games, virtual reality, and augmented reality, where visual performance has a significant impact on user experience. When test videos cannot be perfectly aligned with references or when references are unavailable, the significance of no-reference video quality assessment (NR-VQA) methods is undeniable. However, existing NR-VQA datasets and metrics are primarily focused on camera-captured videos; applying them directly to rendered videos would result in biased predictions, as rendered videos are more prone to temporal artifacts. To address this, we present a large rendering-oriented video dataset with subjective quality annotations, as well as a designed NR-VQA metric specific to rendered videos. The proposed dataset includes a wide range of 3D scenes and rendering settings, with quality scores annotated for various display types to better reflect real-world application scenarios. Building on this dataset, we calibrate our NR-VQA metric to assess rendered video quality by looking at both image quality and temporal stability. We compare our metric to existing NR-VQA metrics, demonstrating its superior performance on rendered videos. Finally, we demonstrate that our metric can be used to benchmark supersampling methods and assess frame generation strategies in real-time rendering.",
    "summary": "",
    "translation": "无参考渲染视频质量评估：数据集与指标",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频质量评估的计算机视觉任务，属于纯粹的视觉质量度量领域。虽然视频推荐系统可能涉及视频内容，但本文的核心技术（无参考质量评估、渲染视频质量）与推荐系统、搜索或广告的排名、建模或架构创新没有直接关联，也不涉及LLM或Transformer技术的应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13331v1": {
    "title": "Group-Wise Optimization for Self-Extensible Codebooks in Vector Quantized Models",
    "url": "https://www.alphaxiv.org/abs/2510.13331v1",
    "arxiv_id": "2510.13331v1",
    "authors": "Hong-Kai Zheng, Piji Li",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 09:14:22",
    "ori_summary": "Vector Quantized Variational Autoencoders (VQ-VAEs) leverage self-supervised learning through reconstruction tasks to represent continuous vectors using the closest vectors in a codebook. However, issues such as codebook collapse persist in the VQ model. To address these issues, existing approaches employ implicit static codebooks or jointly optimize the entire codebook, but these methods constrain the codebook's learning capability, leading to reduced reconstruction quality. In this paper, we propose Group-VQ, which performs group-wise optimization on the codebook. Each group is optimized independently, with joint optimization performed within groups. This approach improves the trade-off between codebook utilization and reconstruction performance. Additionally, we introduce a training-free codebook resampling method, allowing post-training adjustment of the codebook size. In image reconstruction experiments under various settings, Group-VQ demonstrates improved performance on reconstruction metrics. And the post-training codebook sampling method achieves the desired flexibility in adjusting the codebook size.",
    "summary": "",
    "translation": "向量量化模型中自扩展码本的组优化方法",
    "relevance_score": 4,
    "reasoning": "该论文关注向量量化模型的码本优化技术，属于基础模型压缩和效率提升范畴。虽然向量量化在推荐系统中可用于embedding压缩和高效检索，但论文标题未明确指向推荐/搜索/广告应用，且未涉及LLM或Transformer架构的直接改进。其技术可能间接应用于大规模embedding存储优化，但应用路径不够直接。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.13326v1": {
    "title": "DEF-YOLO: Leveraging YOLO for Concealed Weapon Detection in Thermal Imagin",
    "url": "https://www.alphaxiv.org/abs/2510.13326v1",
    "arxiv_id": "2510.13326v1",
    "authors": "Divya Bhardwaj, Arnav Ramamoorthy, Poonam Goyal",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 09:13:35",
    "ori_summary": "Concealed weapon detection aims at detecting weapons hidden beneath a person's clothing or luggage. Various imaging modalities like Millimeter Wave, Microwave, Terahertz, Infrared, etc., are exploited for the concealed weapon detection task. These imaging modalities have their own limitations, such as poor resolution in microwave imaging, privacy concerns in millimeter wave imaging, etc. To provide a real-time, 24 x 7 surveillance, low-cost, and privacy-preserved solution, we opted for thermal imaging in spite of the lack of availability of a benchmark dataset. We propose a novel approach and a dataset for concealed weapon detection in thermal imagery. Our YOLO-based architecture, DEF-YOLO, is built with key enhancements in YOLOv8 tailored to the unique challenges of concealed weapon detection in thermal vision. We adopt deformable convolutions at the SPPF layer to exploit multi-scale features; backbone and neck layers to extract low, mid, and high-level features, enabling DEF-YOLO to adaptively focus on localization around the objects in thermal homogeneous regions, without sacrificing much of the speed and throughput. In addition to these simple yet effective key architectural changes, we introduce a new, large-scale Thermal Imaging Concealed Weapon dataset, TICW, featuring a diverse set of concealed weapons and capturing a wide range of scenarios. To the best of our knowledge, this is the first large-scale contributed dataset for this task. We also incorporate focal loss to address the significant class imbalance inherent in the concealed weapon detection task. The efficacy of the proposed work establishes a new benchmark through extensive experimentation for concealed weapon detection in thermal imagery.",
    "summary": "",
    "translation": "DEF-YOLO：利用YOLO在热成像中进行隐蔽武器检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的目标检测任务，特别是热成像中的武器检测，这属于纯粹的视觉应用领域。该技术与推荐系统、搜索或广告的核心焦点没有直接关联，也不涉及LLM、Transformer架构或异构数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13317v1": {
    "title": "Removing Cost Volumes from Optical Flow Estimators",
    "url": "https://www.alphaxiv.org/abs/2510.13317v1",
    "arxiv_id": "2510.13317v1",
    "authors": "Simon Kiefhaber, Stefan Roth, Simone Schaub-Meyer",
    "categories": "cs.CV, I.4.8",
    "pub_date": "2025-10-15 09:07:09",
    "ori_summary": "Cost volumes are used in every modern optical flow estimator, but due to their computational and space complexity, they are often a limiting factor regarding both processing speed and the resolution of input frames. Motivated by our empirical observation that cost volumes lose their importance once all other network parts of, e.g., a RAFT-based pipeline have been sufficiently trained, we introduce a training strategy that allows removing the cost volume from optical flow estimators throughout training. This leads to significantly improved inference speed and reduced memory requirements. Using our training strategy, we create three different models covering different compute budgets. Our most accurate model reaches state-of-the-art accuracy while being $1.2\\times$ faster and having a $6\\times$ lower memory footprint than comparable models; our fastest model is capable of processing Full HD frames at $20\\,\\mathrm{FPS}$ using only $500\\,\\mathrm{MB}$ of GPU memory.",
    "summary": "",
    "translation": "从光流估计器中移除代价体积",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉中的光流估计技术，属于纯粹的视觉处理领域。虽然光流在视频理解中有应用，但该论文没有展示与推荐系统、搜索或广告的明确关联，也不涉及LLM、Transformer架构或异构数据建模等核心技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13316v1": {
    "title": "Visual Interestingness Decoded: How GPT-4o Mirrors Human Interests",
    "url": "https://www.alphaxiv.org/abs/2510.13316v1",
    "arxiv_id": "2510.13316v1",
    "authors": "Fitim Abdullahu, Helmut Grabner",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 09:04:48",
    "ori_summary": "Our daily life is highly influenced by what we consume and see. Attracting and holding one's attention -- the definition of (visual) interestingness -- is essential. The rise of Large Multimodal Models (LMMs) trained on large-scale visual and textual data has demonstrated impressive capabilities. We explore these models' potential to understand to what extent the concepts of visual interestingness are captured and examine the alignment between human assessments and GPT-4o's, a leading LMM, predictions through comparative analysis. Our studies reveal partial alignment between humans and GPT-4o. It already captures the concept as best compared to state-of-the-art methods. Hence, this allows for the effective labeling of image pairs according to their (commonly) interestingness, which are used as training data to distill the knowledge into a learning-to-rank model. The insights pave the way for a deeper understanding of human interest.",
    "summary": "",
    "translation": "视觉趣味性解码：GPT-4o如何镜像人类兴趣",
    "relevance_score": 6,
    "reasoning": "该论文涉及GPT-4o在视觉趣味性理解方面的能力，这与视觉-语言模型(VLM)技术相关，属于'VLM类比异构数据'的范畴。在推荐系统和广告领域，理解视觉内容的趣味性可以用于内容推荐、广告创意优化和用户兴趣建模，通过分析图像/视频内容与用户兴趣的匹配度来提升推荐效果。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.13315v1": {
    "title": "Self-Augmented Visual Contrastive Decoding",
    "url": "https://www.alphaxiv.org/abs/2510.13315v1",
    "arxiv_id": "2510.13315v1",
    "authors": "Eun Woo Im, Muhammad Kashif Ali, Vivek Gupta",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 09:03:34",
    "ori_summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal capabilities, but they inherit the tendency to hallucinate from their underlying language models. While visual contrastive decoding has been proposed to mitigate this issue, existing methods often apply generic visual augmentations that disregard the specific context provided by the text query, limiting their effectiveness. This study introduces a novel training-free decoding strategy that addresses these limitations, featuring two key contributions. First, a self-augmentation prompting strategy that leverages the intrinsic knowledge of the model to dynamically align semantics between the query and the visual augmentation. Second, an adaptive thresholding algorithm that adaptively adjusts next token candidate size based on the output sparsity, utilizing full information from the logit distribution. Extensive experiments across four LVLMs and seven benchmarks demonstrate that the proposed decoding significantly enhances factual consistency compared to state-of-the-art decoding methods. This work highlights the importance of integrating query-dependent augmentation and entropy-aware decoding for improving effective generation of LVLMs.",
    "summary": "",
    "translation": "自增强视觉对比解码",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视觉领域的解码方法，虽然对比学习技术在推荐系统中也有应用潜力，但标题明确指向视觉模态而非推荐系统核心的异构数据处理。作为使能技术，视觉对比解码可能间接应用于广告创意分析或商品图像理解，但与当前关注的LLM在推荐/搜索中的直接应用关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13310v1": {
    "title": "InstantSfM: Fully Sparse and Parallel Structure-from-Motion",
    "url": "https://www.alphaxiv.org/abs/2510.13310v1",
    "arxiv_id": "2510.13310v1",
    "authors": "Jiankun Zhong, Zitong Zhan, Quankai Gao, Ziyu Chen, Haozhe Lou, Jiageng Mao, Ulrich Neumann, Yue Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 08:58:05",
    "ori_summary": "Structure-from-Motion (SfM), a method that recovers camera poses and scene geometry from uncalibrated images, is a central component in robotic reconstruction and simulation. Despite the state-of-the-art performance of traditional SfM methods such as COLMAP and its follow-up work, GLOMAP, naive CPU-specialized implementations of bundle adjustment (BA) or global positioning (GP) introduce significant computational overhead when handling large-scale scenarios, leading to a trade-off between accuracy and speed in SfM. Moreover, the blessing of efficient C++-based implementations in COLMAP and GLOMAP comes with the curse of limited flexibility, as they lack support for various external optimization options. On the other hand, while deep learning based SfM pipelines like VGGSfM and VGGT enable feed-forward 3D reconstruction, they are unable to scale to thousands of input views at once as GPU memory consumption increases sharply as the number of input views grows. In this paper, we unleash the full potential of GPU parallel computation to accelerate each critical stage of the standard SfM pipeline. Building upon recent advances in sparse-aware bundle adjustment optimization, our design extends these techniques to accelerate both BA and GP within a unified global SfM framework. Through extensive experiments on datasets of varying scales (e.g. 5000 images where VGGSfM and VGGT run out of memory), our method demonstrates up to about 40 times speedup over COLMAP while achieving consistently comparable or even improved reconstruction accuracy. Our project page can be found at https://cre185.github.io/InstantSfM/.",
    "summary": "",
    "translation": "InstantSfM：完全稀疏且并行的运动恢复结构",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的运动恢复结构（SfM）技术，属于纯粹的3D视觉领域。虽然提到了稀疏性和并行化效率改进，但这些优化是针对特定视觉任务，与推荐系统、搜索或广告中的Transformer架构、LLM技术或异构数据建模没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13307v1": {
    "title": "Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.13307v1",
    "arxiv_id": "2510.13307v1",
    "authors": "Yang Li, Aming Wu, Zihao Zhang, Yahong Han",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 08:54:41",
    "ori_summary": "In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation (3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classes using only the supervision from labeled (base) 3D classes. The key to this task is to setup the exact correlations between the point representations and their base class labels, as well as the representation correlations between the points from base and novel classes. A coarse or statistical correlation learning may lead to the confusion in novel class inference. lf we impose a causal relationship as a strong correlated constraint upon the learning process, the essential point cloud representations that accurately correspond to the classes should be uncovered. To this end, we introduce a structural causal model (SCM) to re-formalize the 3D-NCD problem and propose a new method, i.e., Joint Learning of Causal Representation and Reasoning. Specifically, we first analyze hidden confounders in the base class representations and the causal relationships between the base and novel classes through SCM. We devise a causal representation prototype that eliminates confounders to capture the causal representations of base classes. A graph structure is then used to model the causal relationships between the base classes' causal representation prototypes and the novel class prototypes, enabling causal reasoning from base to novel classes. Extensive experiments and visualization results on 3D and 2D NCD semantic segmentation demonstrate the superiorities of our method.",
    "summary": "",
    "translation": "通过因果表示与推理的联合学习实现点云分割的新类别发现",
    "relevance_score": 1,
    "reasoning": "该论文专注于点云分割的计算机视觉任务，属于纯粹的3D视觉领域，与推荐系统、搜索或广告没有直接关联。论文中提到的因果表示和推理技术虽然是通用方法，但在标题中明确限定于点云分割应用，缺乏明确的跨领域适用性，因此与当前关注点高度不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13303v1": {
    "title": "Automated document processing system for government agencies using DBNET++ and BART models",
    "url": "https://www.alphaxiv.org/abs/2510.13303v1",
    "arxiv_id": "2510.13303v1",
    "authors": "Aya Kaysan Bahjat",
    "categories": "cs.CV, cs.GR",
    "pub_date": "2025-10-15 08:48:02",
    "ori_summary": "An automatic document classification system is presented that detects textual content in images and classifies documents into four predefined categories (Invoice, Report, Letter, and Form). The system supports both offline images (e.g., files on flash drives, HDDs, microSD) and real-time capture via connected cameras, and is designed to mitigate practical challenges such as variable illumination, arbitrary orientation, curved or partially occluded text, low resolution, and distant text. The pipeline comprises four stages: image capture and preprocessing, text detection [1] using a DBNet++ (Differentiable Binarization Network Plus) detector, and text classification [2] using a BART (Bidirectional and Auto-Regressive Transformers) classifier, all integrated within a user interface implemented in Python with PyQt5. The achieved results by the system for text detection in images were good at about 92.88% through 10 hours on Total-Text dataset that involve high resolution images simulate a various and very difficult challenges. The results indicate the proposed approach is effective for practical, mixed-source document categorization in unconstrained imaging scenarios.",
    "summary": "",
    "translation": "基于DBNET++和BART模型的政府机构自动化文档处理系统",
    "relevance_score": 2,
    "reasoning": "该论文主要关注政府文档处理的特定应用领域，虽然使用了BART模型，但属于文档处理的垂直应用，与搜索、推荐或广告系统的核心进展关联较弱。DBNET++主要用于文档检测，BART用于文本生成，这些技术在当前形式下对RecSys/Search/Ads的直接应用潜力有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13282v1": {
    "title": "Universal Image Restoration Pre-training via Masked Degradation Classification",
    "url": "https://www.alphaxiv.org/abs/2510.13282v1",
    "arxiv_id": "2510.13282v1",
    "authors": "JiaKui Hu, Zhengjian Yao, Lujia Jin, Yinghao Chen, Yanye Lu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 08:30:15",
    "ori_summary": "This study introduces a Masked Degradation Classification Pre-Training method (MaskDCPT), designed to facilitate the classification of degradation types in input images, leading to comprehensive image restoration pre-training. Unlike conventional pre-training methods, MaskDCPT uses the degradation type of the image as an extremely weak supervision, while simultaneously leveraging the image reconstruction to enhance performance and robustness. MaskDCPT includes an encoder and two decoders: the encoder extracts features from the masked low-quality input image. The classification decoder uses these features to identify the degradation type, whereas the reconstruction decoder aims to reconstruct a corresponding high-quality image. This design allows the pre-training to benefit from both masked image modeling and contrastive learning, resulting in a generalized representation suited for restoration tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained encoder can be used to address universal image restoration and achieve outstanding performance. Implementing MaskDCPT significantly improves performance for both convolution neural networks (CNNs) and Transformers, with a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and a 34.8% reduction in PIQE compared to baseline in real-world degradation scenarios. It also emergences strong generalization to previously unseen degradation types and levels. In addition, we curate and release the UIR-2.5M dataset, which includes 2.5 million paired restoration samples across 19 degradation types and over 200 degradation levels, incorporating both synthetic and real-world data. The dataset, source code, and models are available at https://github.com/MILab-PKU/MaskDCPT.",
    "summary": "",
    "translation": "基于掩码退化分类的通用图像修复预训练",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的图像修复任务，属于纯粹的视觉处理范畴。虽然掩码预训练技术源自Transformer架构，但论文的应用场景（图像修复）与推荐系统、搜索或广告没有直接关联，也不涉及处理异构数据或多模态建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13253v1": {
    "title": "End-to-End Multi-Modal Diffusion Mamba",
    "url": "https://www.alphaxiv.org/abs/2510.13253v1",
    "arxiv_id": "2510.13253v1",
    "authors": "Chunhao Lu, Qiang Lu, Meichen Dong, Jake Luo",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 08:03:50",
    "ori_summary": "Current end-to-end multi-modal models utilize different encoders and decoders to process input and output information. This separation hinders the joint representation learning of various modalities. To unify multi-modal processing, we propose a novel architecture called MDM (Multi-modal Diffusion Mamba). MDM utilizes a Mamba-based multi-step selection diffusion model to progressively generate and refine modality-specific information through a unified variational autoencoder for both encoding and decoding. This innovative approach allows MDM to achieve superior performance when processing high-dimensional data, particularly in generating high-resolution images and extended text sequences simultaneously. Our evaluations in areas such as image generation, image captioning, visual question answering, text comprehension, and reasoning tasks demonstrate that MDM significantly outperforms existing end-to-end models (MonoFormer, LlamaGen, and Chameleon etc.) and competes effectively with SOTA models like GPT-4V, Gemini Pro, and Mistral. Our results validate MDM's effectiveness in unifying multi-modal processes while maintaining computational efficiency, establishing a new direction for end-to-end multi-modal architectures.",
    "summary": "",
    "translation": "端到端多模态扩散Mamba",
    "relevance_score": 3,
    "reasoning": "该论文结合了扩散模型（多模态生成）和Mamba架构（状态空间模型），属于多模态生成领域。虽然Mamba架构在序列建模效率方面有潜在优势，但该工作主要关注多模态内容生成，与推荐系统、搜索或广告中的排序和检索任务关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13251v1": {
    "title": "Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs",
    "url": "https://www.alphaxiv.org/abs/2510.13251v1",
    "arxiv_id": "2510.13251v1",
    "authors": "Minji Kim, Taekyung Kim, Bohyung Han",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 07:59:06",
    "ori_summary": "Video Large Language Models (VideoLLMs) extend the capabilities of vision-language models to spatiotemporal inputs, enabling tasks such as video question answering (VideoQA). Despite recent advances in VideoLLMs, their internal mechanisms on where and how they extract and propagate video and textual information remain less explored. In this study, we investigate the internal information flow of VideoLLMs using mechanistic interpretability techniques. Our analysis reveals consistent patterns across diverse VideoQA tasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame interactions in early-to-middle layers, (2) followed by progressive video-language integration in middle layers. This is facilitated by alignment between video representations and linguistic embeddings containing temporal concepts. (3) Upon completion of this integration, the model is ready to generate correct answers in middle-to-late layers. (4) Based on our analysis, we show that VideoLLMs can retain their VideoQA performance by selecting these effective information pathways while suppressing a substantial amount of attention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a blueprint on how VideoLLMs perform temporal reasoning and offer practical insights for improving model interpretability and downstream generalization. Our project page with the source code is available at https://map-the-flow.github.io",
    "summary": "",
    "translation": "映射信息流：揭示视频大语言模型中隐藏的信息通路",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频大语言模型中的信息流分析，这属于纯粹的视觉-语言模型分析范畴。虽然提到了多模态建模，但缺乏明确的推荐系统、搜索或广告应用场景的直接关联，且主要聚焦于视频理解而非异构数据的统一建模应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13250v1": {
    "title": "Real-Time Crowd Counting for Embedded Systems with Lightweight Architecture",
    "url": "https://www.alphaxiv.org/abs/2510.13250v1",
    "arxiv_id": "2510.13250v1",
    "authors": "Zhiyuan Zhao, Yubin Wen, Siyu Yang, Lichen Ning, Yuandong Liu, Junyu Gao",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 07:58:46",
    "ori_summary": "Crowd counting is a task of estimating the number of the crowd through images, which is extremely valuable in the fields of intelligent security, urban planning, public safety management, and so on. However, the existing counting methods have some problems in practical application on embedded systems for these fields, such as excessive model parameters, abundant complex calculations, etc. The practical application of embedded systems requires the model to be real-time, which means that the model is fast enough. Considering the aforementioned problems, we design a super real-time model with a stem-encoder-decoder structure for crowd counting tasks, which achieves the fastest inference compared with state-of-the-arts. Firstly, large convolution kernels in the stem network are used to enlarge the receptive field, which effectively extracts detailed head information. Then, in the encoder part, we use conditional channel weighting and multi-branch local fusion block to merge multi-scale features with low computational consumption. This part is crucial to the super real-time performance of the model. Finally, the feature pyramid networks are added to the top of the encoder to alleviate its incomplete fusion problems. Experiments on three benchmarks show that our network is suitable for super real-time crowd counting on embedded systems, ensuring competitive accuracy. At the same time, the proposed network reasoning speed is the fastest. Specifically, the proposed network achieves 381.7 FPS on NVIDIA GTX 1080Ti and 71.9 FPS on NVIDIA Jetson TX1.",
    "summary": "",
    "translation": "面向嵌入式系统的轻量级架构实时人群计数",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉领域的人群计数任务，采用轻量级架构优化嵌入式系统性能。虽然涉及实时处理，但该技术主要应用于安防监控、城市规划等视觉场景，与推荐系统、搜索或广告的核心技术领域没有直接关联，也没有展示在异构数据处理或多模态建模方面的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13245v1": {
    "title": "CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation",
    "url": "https://www.alphaxiv.org/abs/2510.13245v1",
    "arxiv_id": "2510.13245v1",
    "authors": "Li Liang, Bo Miao, Xinyu Wang, Naveed Akhtar, Jordan Vice, Ajmal Mian",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 07:47:00",
    "ori_summary": "Outdoor 3D semantic scene generation produces realistic and semantically rich environments for applications such as urban simulation and autonomous driving. However, advances in this direction are constrained by the absence of publicly available, well-annotated datasets. We introduce SketchSem3D, the first large-scale benchmark for generating 3D outdoor semantic scenes from abstract freehand sketches and pseudo-labeled annotations of satellite images. SketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-based KITTI-360 (containing LiDAR voxels along with their corresponding sketches and annotated satellite images), to enable standardized, rigorous, and diverse evaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) that significantly enhances spatial coherence in outdoor 3D scene generation. CymbaDiff imposes structured spatial ordering, explicitly captures cylindrical continuity and vertical hierarchy, and preserves both physical neighborhood relationships and global context within the generated scenes. Extensive experiments on SketchSem3D demonstrate that CymbaDiff achieves superior semantic consistency, spatial realism, and cross-dataset generalization. The code and dataset will be available at https://github.com/Lillian-research-hub/CymbaDiff",
    "summary": "",
    "translation": "CymbaDiff：基于草图的3D语义城市场景生成的结构化空间扩散方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D场景生成和计算机视觉领域，特别是基于草图的3D城市场景生成。这与推荐系统、搜索或广告的核心技术焦点没有直接关联，也不涉及LLM技术、Transformer架构改进或异构数据统一建模等相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13243v1": {
    "title": "FlyAwareV2: A Multimodal Cross-Domain UAV Dataset for Urban Scene Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.13243v1",
    "arxiv_id": "2510.13243v1",
    "authors": "Francesco Barbato, Matteo Caligiuri, Pietro Zanuttigh",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 07:44:31",
    "ori_summary": "The development of computer vision algorithms for Unmanned Aerial Vehicle (UAV) applications in urban environments heavily relies on the availability of large-scale datasets with accurate annotations. However, collecting and annotating real-world UAV data is extremely challenging and costly. To address this limitation, we present FlyAwareV2, a novel multimodal dataset encompassing both real and synthetic UAV imagery tailored for urban scene understanding tasks. Building upon the recently introduced SynDrone and FlyAware datasets, FlyAwareV2 introduces several new key contributions: 1) Multimodal data (RGB, depth, semantic labels) across diverse environmental conditions including varying weather and daytime; 2) Depth maps for real samples computed via state-of-the-art monocular depth estimation; 3) Benchmarks for RGB and multimodal semantic segmentation on standard architectures; 4) Studies on synthetic-to-real domain adaptation to assess the generalization capabilities of models trained on the synthetic data. With its rich set of annotations and environmental diversity, FlyAwareV2 provides a valuable resource for research on UAV-based 3D urban scene understanding.",
    "summary": "",
    "translation": "FlyAwareV2：面向城市场景理解的多模态跨域无人机数据集",
    "relevance_score": 1,
    "reasoning": "该论文主要关注无人机数据集和城市场景理解，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。即使作为多模态数据，其应用场景（无人机、城市理解）与RecSys/Search/Ads的业务需求和技术栈相距甚远，无法看出任何潜在的应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13237v1": {
    "title": "Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models",
    "url": "https://www.alphaxiv.org/abs/2510.13237v1",
    "arxiv_id": "2510.13237v1",
    "authors": "Haochuan Xu, Yun Sing Koh, Shuhuai Huang, Zirun Zhou, Di Wang, Jun Sakuma, Jingfeng Zhang",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-15 07:42:44",
    "ori_summary": "Vision-Language-Action (VLA) models have achieved revolutionary progress in robot learning, enabling robots to execute complex physical robot tasks from natural language instructions. Despite this progress, their adversarial robustness remains underexplored. In this work, we propose both adversarial patch attack and corresponding defense strategies for VLA models. We first introduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic adversarial attack that generates patches directly placeable within the camera's view. In comparison to prior methods, EDPA can be readily applied to different VLA models without requiring prior knowledge of the model architecture, or the controlled robotic manipulator. EDPA constructs these patches by (i) disrupting the semantic alignment between visual and textual latent representations, and (ii) maximizing the discrepancy of latent representations between adversarial and corresponding clean visual inputs. Through the optimization of these objectives, EDPA distorts the VLA's interpretation of visual information, causing the model to repeatedly generate incorrect actions and ultimately result in failure to complete the given robotic task. To counter this, we propose an adversarial fine-tuning scheme for the visual encoder, in which the encoder is optimized to produce similar latent representations for both clean and adversarially perturbed visual inputs. Extensive evaluations on the widely recognized LIBERO robotic simulation benchmark demonstrate that EDPA substantially increases the task failure rate of cutting-edge VLA models, while our proposed defense effectively mitigates this degradation. The codebase is accessible via the homepage at https://edpa-attack.github.io/.",
    "summary": "",
    "translation": "面向视觉-语言-动作模型且与模型无关的对抗攻击与防御",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉-语言-动作模型的对抗攻击与防御，属于安全领域，被明确列为不相关主题。虽然标题包含视觉和语言模态，但其核心焦点是模型安全而非推荐系统、搜索或广告的应用。在推荐系统、搜索或广告领域，对抗攻击与防御没有明确的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13235v1": {
    "title": "EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking",
    "url": "https://www.alphaxiv.org/abs/2510.13235v1",
    "arxiv_id": "2510.13235v1",
    "authors": "Yukuan Zhang, Jiarui Zhao, Shangqing Nie, Jin Kuang, Shengsheng Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 07:39:30",
    "ori_summary": "Multimodal semantic cues, such as textual descriptions, have shown strong potential in enhancing target perception for tracking. However, existing methods rely on static textual descriptions from large language models, which lack adaptability to real-time target state changes and prone to hallucinations. To address these challenges, we propose a unified multimodal vision-language tracking framework, named EPIPTrack, which leverages explicit and implicit prompts for dynamic target modeling and semantic alignment. Specifically, explicit prompts transform spatial motion information into natural language descriptions to provide spatiotemporal guidance. Implicit prompts combine pseudo-words with learnable descriptors to construct individualized knowledge representations capturing appearance attributes. Both prompts undergo dynamic adjustment via the CLIP text encoder to respond to changes in target state. Furthermore, we design a Discriminative Feature Augmentor to enhance visual and cross-modal representations. Extensive experiments on MOT17, MOT20, and DanceTrack demonstrate that EPIPTrack outperforms existing trackers in diverse scenarios, exhibiting robust adaptability and superior performance.",
    "summary": "",
    "translation": "EPIPTrack：基于显式和隐式提示的多目标跟踪中提示建模的重新思考",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的多目标跟踪任务，虽然涉及提示建模概念，但其核心应用领域是视觉跟踪而非推荐系统、搜索或广告。显式和隐式提示机制可能对Transformer架构有启发，但缺乏明确的RecSys/Search/Ads应用场景，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13234v1": {
    "title": "UniVector: Unified Vector Extraction via Instance-Geometry Interaction",
    "url": "https://www.alphaxiv.org/abs/2510.13234v1",
    "arxiv_id": "2510.13234v1",
    "authors": "Yinglong Yan, Jun Yue, Shaobo Xia, Hanmeng Sun, Tianxu Ying, Chengcheng Wu, Sifan Lan, Min He, Pedram Ghamisi, Leyuan Fang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 07:39:25",
    "ori_summary": "Vector extraction retrieves structured vector geometry from raster images, offering high-fidelity representation and broad applicability. Existing methods, however, are usually tailored to a single vector type (e.g., polygons, polylines, line segments), requiring separate models for different structures. This stems from treating instance attributes (category, structure) and geometric attributes (point coordinates, connections) independently, limiting the ability to capture complex structures. Inspired by the human brain's simultaneous use of semantic and spatial interactions in visual perception, we propose UniVector, a unified VE framework that leverages instance-geometry interaction to extract multiple vector types within a single model. UniVector encodes vectors as structured queries containing both instance- and geometry-level information, and iteratively updates them through an interaction module for cross-level context exchange. A dynamic shape constraint further refines global structures and key points. To benchmark multi-structure scenarios, we introduce the Multi-Vector dataset with diverse polygons, polylines, and line segments. Experiments show UniVector sets a new state of the art on both single- and multi-structure VE tasks. Code and dataset will be released at https://github.com/yyyyll0ss/UniVector.",
    "summary": "",
    "translation": "UniVector：通过实例-几何交互的统一向量提取",
    "relevance_score": 4,
    "reasoning": "该论文标题暗示了一种统一的向量提取方法，可能涉及多模态或多类型数据的特征提取，这与异构数据统一建模的VLM类比概念有一定相关性。然而，标题没有明确说明该方法在推荐系统、搜索或广告中的具体应用，也没有明确涉及Transformer架构或LLM技术，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.13232v1": {
    "title": "What \"Not\" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging",
    "url": "https://www.alphaxiv.org/abs/2510.13232v1",
    "arxiv_id": "2510.13232v1",
    "authors": "Inha Kang, Youngsun Lim, Seonho Lee, Jiho Choi, Junsuk Choe, Hyunjung Shim",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 07:36:38",
    "ori_summary": "State-of-the-art vision-language models (VLMs) suffer from a critical failure in understanding negation, often referred to as affirmative bias. This limitation is particularly severe in described object detection (DOD) tasks. To address this, we propose two primary contributions: (1) a new dataset pipeline and (2) a novel, lightweight adaptation recipe. First, we introduce CoVAND, a dataset constructed with a systematic chain-of-thought (CoT) and VQA-based pipeline to generate high-quality, instance-grounded negation data. Second, we propose NegToMe, a novel text token merging module that directly tackles the architectural cause of affirmative bias. NegToMe fundamentally addresses the structural loss of negation cues in tokenization, grouping them with attributes into coherent semantic phrases. It maintains correct polarity at the input level, enabling robust negation understanding even with limited data. For instance, to prevent a model from treating the fragmented tokens \"not\" and \"girl\" as simply \"girl\", NegToMe binds them into a single token whose meaning is correctly distinguished from that of \"girl\" alone. This module is integrated with a parameter-efficient and strategic LoRA fine-tuning approach. Our method significantly improves performance on challenging negation benchmarks with a lowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval and demonstrating generalization to SoTA VLMs. This work marks a crucial step forward in addressing negation understanding for real-world detection applications.",
    "summary": "",
    "translation": "“不”检测什么：通过结构化推理与令牌合并实现否定感知的视觉语言模型",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视觉语言模型(VLMs)中的否定感知能力，这属于视觉-语言交互的特定技术问题。虽然标题提到了“结构化推理”和“令牌合并”这些可能对序列建模有用的技术，但论文的核心焦点是视觉语言理解中的否定检测，与推荐系统、搜索或广告中的异构数据处理仅有微弱的间接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13226v1": {
    "title": "Sample-Centric Multi-Task Learning for Detection and Segmentation of Industrial Surface Defects",
    "url": "https://www.alphaxiv.org/abs/2510.13226v1",
    "arxiv_id": "2510.13226v1",
    "authors": "Hang-Cheng Dong, Yibo Jiao, Fupeng Wei, Guodong Liu, Dong Ye, Bingguo Liu",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-15 07:24:26",
    "ori_summary": "Industrial surface defect inspection for sample-wise quality control (QC) must simultaneously decide whether a given sample contains defects and localize those defects spatially. In real production lines, extreme foreground-background imbalance, defect sparsity with a long-tailed scale distribution, and low contrast are common. As a result, pixel-centric training and evaluation are easily dominated by large homogeneous regions, making it difficult to drive models to attend to small or low-contrast defects-one of the main bottlenecks for deployment. Empirically, existing models achieve strong pixel-overlap metrics (e.g., mIoU) but exhibit insufficient stability at the sample level, especially for sparse or slender defects. The root cause is a mismatch between the optimization objective and the granularity of QC decisions. To address this, we propose a sample-centric multi-task learning framework and evaluation suite. Built on a shared-encoder architecture, the method jointly learns sample-level defect classification and pixel-level mask localization. Sample-level supervision modulates the feature distribution and, at the gradient level, continually boosts recall for small and low-contrast defects, while the segmentation branch preserves boundary and shape details to enhance per-sample decision stability and reduce misses. For evaluation, we propose decision-linked metrics, Seg_mIoU and Seg_Recall, which remove the bias of classical mIoU caused by empty or true-negative samples and tightly couple localization quality with sample-level decisions. Experiments on two benchmark datasets demonstrate that our approach substantially improves the reliability of sample-level decisions and the completeness of defect localization.",
    "summary": "",
    "translation": "以样本为中心的多任务学习用于工业表面缺陷检测与分割",
    "relevance_score": 2,
    "reasoning": "该论文专注于工业缺陷检测这一特定领域应用，与推荐系统、搜索或广告的核心领域进展无关。虽然多任务学习是通用技术，但论文的应用场景（工业表面缺陷）与我的关注领域没有直接关联，且未涉及LLM、Transformer架构或异构数据建模等关键技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13219v1": {
    "title": "Prompt-based Adaptation in Large-scale Vision Models: A Survey",
    "url": "https://www.alphaxiv.org/abs/2510.13219v1",
    "arxiv_id": "2510.13219v1",
    "authors": "Xi Xiao, Yunbei Zhang, Lin Zhao, Yiyang Liu, Xiaoying Liao, Zheda Mai, Xingjian Li, Xiao Wang, Hao Xu, Jihun Hamm, Xue Lin, Min Xu, Qifan Wang, Tianyang Wang, Cheng Han",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 07:14:50",
    "ori_summary": "In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) have recently emerged as lightweight and effective alternatives to full fine-tuning for adapting large-scale vision models within the ``pretrain-then-finetune'' paradigm. However, despite rapid progress, their conceptual boundaries remain blurred, as VP and VPT are frequently used interchangeably in current research, reflecting a lack of systematic distinction between these techniques and their respective applications. In this survey, we revisit the designs of VP and VPT from first principles, and conceptualize them within a unified framework termed Prompt-based Adaptation (PA). We provide a taxonomy that categorizes existing methods into learnable, generative, and non-learnable prompts, and further organizes them by injection granularity -- pixel-level and token-level. Beyond the core methodologies, we examine PA's integrations across diverse domains, including medical imaging, 3D point clouds, and vision-language tasks, as well as its role in test-time adaptation and trustworthy AI. We also summarize current benchmarks and identify key challenges and future directions. To the best of our knowledge, we are the first comprehensive survey dedicated to PA's methodologies and applications in light of their distinct characteristics. Our survey aims to provide a clear roadmap for researchers and practitioners in all area to understand and explore the evolving landscape of PA-related research.",
    "summary": "",
    "translation": "基于提示的大规模视觉模型自适应方法综述",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉模型中的提示自适应技术，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术栈关联度极低。虽然提示工程在LLM中有广泛应用，但本文聚焦于视觉模型的自适应方法，缺乏明确的跨模态应用场景或向推荐/搜索领域的迁移潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13208v1": {
    "title": "MimicParts: Part-aware Style Injection for Speech-Driven 3D Motion Generation",
    "url": "https://www.alphaxiv.org/abs/2510.13208v1",
    "arxiv_id": "2510.13208v1",
    "authors": "Lianlian Liu, YongKang He, Zhaojie Chu, Xiaofen Xing, Xiangmin Xu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-15 06:53:15",
    "ori_summary": "Generating stylized 3D human motion from speech signals presents substantial challenges, primarily due to the intricate and fine-grained relationships among speech signals, individual styles, and the corresponding body movements. Current style encoding approaches either oversimplify stylistic diversity or ignore regional motion style differences (e.g., upper vs. lower body), limiting motion realism. Additionally, motion style should dynamically adapt to changes in speech rhythm and emotion, but existing methods often overlook this. To address these issues, we propose MimicParts, a novel framework designed to enhance stylized motion generation based on part-aware style injection and part-aware denoising network. It divides the body into different regions to encode localized motion styles, enabling the model to capture fine-grained regional differences. Furthermore, our part-aware attention block allows rhythm and emotion cues to guide each body region precisely, ensuring that the generated motion aligns with variations in speech rhythm and emotional state. Experimental results show that our method outperforming existing methods showcasing naturalness and expressive 3D human motion sequences.",
    "summary": "",
    "translation": "MimicParts：面向语音驱动3D运动生成的部分感知风格注入",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于语音驱动的3D运动生成，属于计算机图形学与语音处理的交叉领域，与推荐系统、搜索或广告的核心技术栈无直接关联。其研究内容涉及3D运动生成和风格注入，属于明确的视觉与图形学范畴，不符合当前关注的任何技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13201v1": {
    "title": "Paper Copilot: Tracking the Evolution of Peer Review in AI Conferences",
    "url": "https://www.alphaxiv.org/abs/2510.13201v1",
    "arxiv_id": "2510.13201v1",
    "authors": "Jing Yang, Qiyao Wei, Jiaxin Pei",
    "categories": "cs.CV, cs.AI, cs.DL, cs.LG",
    "pub_date": "2025-10-15 06:41:06",
    "ori_summary": "The rapid growth of AI conferences is straining an already fragile peer-review system, leading to heavy reviewer workloads, expertise mismatches, inconsistent evaluation standards, superficial or templated reviews, and limited accountability under compressed timelines. In response, conference organizers have introduced new policies and interventions to preserve review standards. Yet these ad-hoc changes often create further concerns and confusion about the review process, leaving how papers are ultimately accepted - and how practices evolve across years - largely opaque. We present Paper Copilot, a system that creates durable digital archives of peer reviews across a wide range of computer-science venues, an open dataset that enables researchers to study peer review at scale, and a large-scale empirical analysis of ICLR reviews spanning multiple years. By releasing both the infrastructure and the dataset, Paper Copilot supports reproducible research on the evolution of peer review. We hope these resources help the community track changes, diagnose failure modes, and inform evidence-based improvements toward a more robust, transparent, and reliable peer-review system.",
    "summary": "",
    "translation": "论文副驾驶：追踪AI会议中同行评审的演变",
    "relevance_score": 1,
    "reasoning": "该论文关注AI会议同行评审过程的演变追踪，属于学术流程和会议管理范畴。这与我的核心关注领域（推荐系统、搜索、广告中的技术进展、LLM/Transformer架构创新）完全无关，不涉及任何推荐算法、模型架构改进或搜索广告相关技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13198v1": {
    "title": "Complementary Information Guided Occupancy Prediction via Multi-Level Representation Fusion",
    "url": "https://www.alphaxiv.org/abs/2510.13198v1",
    "arxiv_id": "2510.13198v1",
    "authors": "Rongtao Xu, Jinzhou Lin, Jialei Zhou, Jiahua Dong, Changwei Wang, Ruisheng Wang, Li Guo, Shibiao Xu, Xiaodan Liang",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 06:37:33",
    "ori_summary": "Camera-based occupancy prediction is a mainstream approach for 3D perception in autonomous driving, aiming to infer complete 3D scene geometry and semantics from 2D images. Almost existing methods focus on improving performance through structural modifications, such as lightweight backbones and complex cascaded frameworks, with good yet limited performance. Few studies explore from the perspective of representation fusion, leaving the rich diversity of features in 2D images underutilized. Motivated by this, we propose \\textbf{CIGOcc, a two-stage occupancy prediction framework based on multi-level representation fusion. \\textbf{CIGOcc extracts segmentation, graphics, and depth features from an input image and introduces a deformable multi-level fusion mechanism to fuse these three multi-level features. Additionally, CIGOcc incorporates knowledge distilled from SAM to further enhance prediction accuracy. Without increasing training costs, CIGOcc achieves state-of-the-art performance on the SemanticKITTI benchmark. The code is provided in the supplementary material and will be released https://github.com/VitaLemonTea1/CIGOcc",
    "summary": "",
    "translation": "基于多层级表征融合的互补信息引导占据预测",
    "relevance_score": 2,
    "reasoning": "该论文标题表明其关注计算机视觉领域的占据预测任务，涉及多层级特征融合技术。虽然多模态融合技术在某些方面与VLM处理异构数据的理念相似，但占据预测主要应用于自动驾驶、机器人导航等视觉场景，与推荐系统、搜索或广告的核心技术关联度较低，缺乏明确的跨领域应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13186v1": {
    "title": "STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client Selection and Power Control",
    "url": "https://www.alphaxiv.org/abs/2510.13186v1",
    "arxiv_id": "2510.13186v1",
    "authors": "Zhen Li, Xibin Jin, Guoliang Li, Shuai Wang, Miaowen Wen, Huseyin Arslan, Derrick Wing Kwan Ng, Chengzhong Xu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 06:20:47",
    "ori_summary": "Edge Gaussian splatting (EGS), which aggregates data from distributed clients and trains a global GS model at the edge server, is an emerging paradigm for scene reconstruction. Unlike traditional edge resource management methods that emphasize communication throughput or general-purpose learning performance, EGS explicitly aims to maximize the GS qualities, rendering existing approaches inapplicable. To address this problem, this paper formulates a novel GS-oriented objective function that distinguishes the heterogeneous view contributions of different clients. However, evaluating this function in turn requires clients' images, leading to a causality dilemma. To this end, this paper further proposes a sample-then-transmit EGS (or STT-GS for short) strategy, which first samples a subset of images as pilot data from each client for loss prediction. Based on the first-stage evaluation, communication resources are then prioritized towards more valuable clients. To achieve efficient sampling, a feature-domain clustering (FDC) scheme is proposed to select the most representative data and pilot transmission time minimization (PTTM) is adopted to reduce the pilot overhead.Subsequently, we develop a joint client selection and power control (JCSPC) framework to maximize the GS-oriented function under communication resource constraints. Despite the nonconvexity of the problem, we propose a low-complexity efficient solution based on the penalty alternating majorization minimization (PAMM) algorithm. Experiments unveil that the proposed scheme significantly outperforms existing benchmarks on real-world datasets. It is found that the GS-oriented objective can be accurately predicted with low sampling ratios (e.g.,10%), and our method achieves an excellent tradeoff between view contributions and communication costs.",
    "summary": "",
    "translation": "STT-GS：具有联合客户端选择与功率控制的采样后传输边缘高斯泼溅",
    "relevance_score": 1,
    "reasoning": "该论文标题表明其关注边缘计算、无线传输和3D场景表示（高斯泼溅），这些主题与推荐系统、搜索或广告的核心技术领域无关。虽然提到了客户端选择和功率控制，但这些属于通信优化范畴，没有明确指向推荐系统、搜索或广告的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13160v1": {
    "title": "DP-TTA: Test-time Adaptation for Transient Electromagnetic Signal Denoising via Dictionary-driven Prior Regularization",
    "url": "https://www.alphaxiv.org/abs/2510.13160v1",
    "arxiv_id": "2510.13160v1",
    "authors": "Meng Yang, Kecheng Chen, Wei Luo, Xianjie Chen, Yong Jia, Mingyue Wang, Fanqiang Lin",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 05:22:03",
    "ori_summary": "Transient Electromagnetic (TEM) method is widely used in various geophysical applications, providing valuable insights into subsurface properties. However, time-domain TEM signals are often submerged in various types of noise. While recent deep learning-based denoising models have shown strong performance, these models are mostly trained on simulated or single real-world scenario data, overlooking the significant differences in noise characteristics from different geographical regions. Intuitively, models trained in one environment often struggle to perform well in new settings due to differences in geological conditions, equipment, and external interference, leading to reduced denoising performance. To this end, we propose the Dictionary-driven Prior Regularization Test-time Adaptation (DP-TTA). Our key insight is that TEM signals possess intrinsic physical characteristics, such as exponential decay and smoothness, which remain consistent across different regions regardless of external conditions. These intrinsic characteristics serve as ideal prior knowledge for guiding the TTA strategy, which helps the pre-trained model dynamically adjust parameters by utilizing self-supervised losses, improving denoising performance in new scenarios. To implement this, we customized a network, named DTEMDNet. Specifically, we first use dictionary learning to encode these intrinsic characteristics as a dictionary-driven prior, which is integrated into the model during training. At the testing stage, this prior guides the model to adapt dynamically to new environments by minimizing self-supervised losses derived from the dictionary-driven consistency and the signal one-order variation. Extensive experimental results demonstrate that the proposed method achieves much better performance than existing TEM denoising methods and TTA methods.",
    "summary": "",
    "translation": "DP-TTA：基于字典驱动先验正则化的瞬态电磁信号去噪测试时自适应",
    "relevance_score": 1,
    "reasoning": "该论文专注于瞬态电磁信号去噪这一特定信号处理任务，属于物理/工程领域应用。虽然提到了测试时自适应技术，但其核心是电磁信号处理而非推荐系统、搜索或广告相关的技术。论文内容与我的关注领域（RecSys/Search/Ads核心进展、LLM技术、Transformer架构等）没有明显关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13151v1": {
    "title": "Foveation Improves Payload Capacity in Steganography",
    "url": "https://www.alphaxiv.org/abs/2510.13151v1",
    "arxiv_id": "2510.13151v1",
    "authors": "Lifeng Qiu Lin, Henry Kam, Qi Sun, Kaan Akşit",
    "categories": "cs.CV, cs.GR, I.2.10; I.4",
    "pub_date": "2025-10-15 05:00:59",
    "ori_summary": "Steganography finds its use in visual medium such as providing metadata and watermarking. With support of efficient latent representations and foveated rendering, we trained models that improve existing capacity limits from 100 to 500 bits, while achieving better accuracy of up to 1 failure bit out of 2000, at 200K test bits. Finally, we achieve a comparable visual quality of 31.47 dB PSNR and 0.13 LPIPS, showing the effectiveness of novel perceptual design in creating multi-modal latent representations in steganography.",
    "summary": "",
    "translation": "注视点技术提升隐写术的有效载荷容量",
    "relevance_score": 1,
    "reasoning": "该论文专注于隐写术中的有效载荷容量问题，这是信息安全领域的技术，与推荐系统、搜索或广告的核心技术栈完全无关。论文内容涉及数据隐藏和加密技术，属于明确的无关主题范畴，没有任何潜在的应用价值或技术启发性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13137v1": {
    "title": "Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN",
    "url": "https://www.alphaxiv.org/abs/2510.13137v1",
    "arxiv_id": "2510.13137v1",
    "authors": "Madhumati Pol, Anvay Anturkar, Anushka Khot, Ayush Andure, Aniruddha Ghosh, Anvit Magadum, Anvay Bahadur",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 04:26:33",
    "ori_summary": "This study investigates the performance of 3D Convolutional Neural Networks (3D CNNs) and Long Short-Term Memory (LSTM) networks for real-time American Sign Language (ASL) recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences, LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1,200 ASL signs across 50 classes, comparing their accuracy, computational efficiency, and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2% more processing time per frame compared to LSTMs, which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNNLSTM model shows decent performance, which suggests that context-dependent architecture selection is crucial for practical implementation.This project provides professional benchmarks for developing assistive technologies, highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments.",
    "summary": "",
    "translation": "基于深度学习的实时手语到文本翻译：LSTM与3D CNN的对比研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于手语翻译这一特定领域应用，属于计算机视觉和序列建模的交叉领域。虽然涉及序列建模技术，但与搜索、推荐、广告系统没有直接关联，也不涉及LLM技术或Transformer架构的进展。该研究属于纯粹的视觉应用领域，不在当前关注范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13131v1": {
    "title": "OS-HGAdapter: Open Semantic Hypergraph Adapter for Large Language Models Assisted Entropy-Enhanced Image-Text Alignment",
    "url": "https://www.alphaxiv.org/abs/2510.13131v1",
    "arxiv_id": "2510.13131v1",
    "authors": "Rongjun Chen, Chengsi Yao, Jinchang Ren, Xianxian Zeng, Peixian Wang, Jun Yuan, Jiawen Li, Huimin Zhao, Xu Lu",
    "categories": "cs.CV, cs.MM",
    "pub_date": "2025-10-15 04:09:00",
    "ori_summary": "Text-image alignment constitutes a foundational challenge in multimedia content understanding, where effective modeling of cross-modal semantic correspondences critically enhances retrieval system performance through joint embedding space optimization. Given the inherent difference in information entropy between texts and images, conventional approaches often show an imbalance in the mutual retrieval of these two modalities. To address this particular challenge, we propose to use the open semantic knowledge of Large Language Model (LLM) to fill for the entropy gap and reproduce the alignment ability of humans in these tasks. Our entropy-enhancing alignment is achieved through a two-step process: 1) a new prompt template that does not rely on explicit knowledge in the task domain is designed to use LLM to enhance the polysemy description of the text modality. By analogy, the information entropy of the text modality relative to the visual modality is increased; 2) A hypergraph adapter is used to construct multilateral connections between the text and image modalities, which can correct the positive and negative matching errors for synonymous semantics in the same fixed embedding space, whilst reducing the noise caused by open semantic entropy by mapping the reduced dimensions back to the original dimensions. Comprehensive evaluations on the Flickr30K and MS-COCO benchmarks validate the superiority of our Open Semantic Hypergraph Adapter (OS-HGAdapter), showcasing 16.8\\% (text-to-image) and 40.1\\% (image-to-text) cross-modal retrieval gains over existing methods while establishing new state-of-the-art performance in semantic alignment tasks.",
    "summary": "",
    "translation": "OS-HGAdapter：面向大语言模型辅助熵增强图文对齐的开放语义超图适配器",
    "relevance_score": 2,
    "reasoning": "该论文主要关注图像-文本对齐任务，属于视觉-语言多模态领域，与推荐系统、搜索或广告的核心技术关联较弱。虽然提到了LLM辅助和适配器技术，但其应用场景局限于图文对齐，缺乏明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13109v1": {
    "title": "VPREG: An Optimal Control Formulation for Diffeomorphic Image Registration Based on the Variational Principle Grid Generation Method",
    "url": "https://www.alphaxiv.org/abs/2510.13109v1",
    "arxiv_id": "2510.13109v1",
    "authors": "Zicong Zhou, Baihan Zhao, Andreas Mang, Guojun Liao",
    "categories": "cs.CV, math.OC, 49J20, 49K20, 49N45",
    "pub_date": "2025-10-15 03:02:39",
    "ori_summary": "This paper introduces VPreg, a novel diffeomorphic image registration method. This work provides several improvements to our past work on mesh generation and diffeomorphic image registration. VPreg aims to achieve excellent registration accuracy while controlling the quality of the registration transformations. It ensures a positive Jacobian determinant of the spatial transformation and provides an accurate approximation of the inverse of the registration, a crucial property for many neuroimaging workflows. Unlike conventional methods, VPreg generates this inverse transformation within the group of diffeomorphisms rather than operating on the image space. The core of VPreg is a grid generation approach, referred to as \\emph{Variational Principle} (VP), which constructs non-folding grids with prescribed Jacobian determinant and curl. These VP-generated grids guarantee diffeomorphic spatial transformations essential for computational anatomy and morphometry, and provide a more accurate inverse than existing methods. To assess the potential of the proposed approach, we conduct a performance analysis for 150 registrations of brain scans from the OASIS-1 dataset. Performance evaluation based on Dice scores for 35 regions of interest, along with an empirical analysis of the properties of the computed spatial transformations, demonstrates that VPreg outperforms state-of-the-art methods in terms of Dice scores, regularity properties of the computed transformation, and accuracy and consistency of the provided inverse map. We compare our results to ANTs-SyN, Freesurfer-Easyreg, and FSL-Fnirt.",
    "summary": "",
    "translation": "VPREG：基于变分原理网格生成方法的微分同胚图像配准最优控制公式化",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像配准领域，涉及微分同胚变换和最优控制理论，属于计算机视觉中的特定应用方向。论文内容与推荐系统、搜索、广告的核心技术领域没有直接关联，也不涉及LLM、Transformer架构或异构数据建模等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13108v1": {
    "title": "DriveCritic: Towards Context-Aware, Human-Aligned Evaluation for Autonomous Driving with Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.13108v1",
    "arxiv_id": "2510.13108v1",
    "authors": "Jingyu Song, Zhenxin Li, Shiyi Lan, Xinglong Sun, Nadine Chang, Maying Shen, Joshua Chen, Katherine A. Skinner, Jose M. Alvarez",
    "categories": "cs.CV, cs.AI, cs.RO",
    "pub_date": "2025-10-15 03:00:38",
    "ori_summary": "Benchmarking autonomous driving planners to align with human judgment remains a critical challenge, as state-of-the-art metrics like the Extended Predictive Driver Model Score (EPDMS) lack context awareness in nuanced scenarios. To address this, we introduce DriveCritic, a novel framework featuring two key contributions: the DriveCritic dataset, a curated collection of challenging scenarios where context is critical for correct judgment and annotated with pairwise human preferences, and the DriveCritic model, a Vision-Language Model (VLM) based evaluator. Fine-tuned using a two-stage supervised and reinforcement learning pipeline, the DriveCritic model learns to adjudicate between trajectory pairs by integrating visual and symbolic context. Experiments show DriveCritic significantly outperforms existing metrics and baselines in matching human preferences and demonstrates strong context awareness. Overall, our work provides a more reliable, human-aligned foundation to evaluating autonomous driving systems.",
    "summary": "",
    "translation": "DriveCritic：基于视觉语言模型实现自动驾驶的上下文感知、人类对齐评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域的评估方法，属于纯粹的视觉应用场景，与推荐系统、搜索或广告领域没有直接关联。虽然提到了视觉语言模型，但其应用仅限于自动驾驶评估，没有展示在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13105v1": {
    "title": "EgoSocial: Benchmarking Proactive Intervention Ability of Omnimodal LLMs via Egocentric Social Interaction Perception",
    "url": "https://www.alphaxiv.org/abs/2510.13105v1",
    "arxiv_id": "2510.13105v1",
    "authors": "Xijun Wang, Tanay Sharma, Achin Kulshrestha, Abhimitra Meka, Aveek Purohit, Dinesh Manocha",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 02:52:19",
    "ori_summary": "As AR/VR technologies become integral to daily life, there's a growing need for AI that understands human social dynamics from an egocentric perspective. However, current LLMs often lack the social awareness to discern when to intervene as AI assistant. This leads to constant, socially unaware responses that may disrupt natural conversation and negatively impact user focus. To address these limitations, we introduce EgoSocial, a large-scale egocentric dataset with 13,500 social video-question pairs, specifically designed to benchmark intervention in social interaction perception. We also present an in-depth analysis of current omnimodal LLMs (OLLMs) to assess their effectiveness in detecting diverse social contextual cues. Experiments show that OLLMs still struggle to detect the intervention timing (14.4% for Gemini 2.5 Pro). We also propose EgoSoD (EgoSocial Detection), an end-to-end method for robustly discerning social dynamics. Informed by our OLLM analysis, EgoSoD integrates multimodal contextual cues (e.g., audio and visual cues) into a social thinking graph, dynamically modeling participants and interactions. Our method proactively detects intervention timing and social interactions, precisely determining when to intervene. Our EgoSoD improves Phi-4 by 45.6% and Gemini 2.5 Pro by 9.9% on Intervention Timing performance, and improves Phi-4 by 20.4% and Gemini 2.5 Pro by 6.9% on overall Social Interaction performance. We will release the dataset and code soon.",
    "summary": "",
    "translation": "EgoSocial：通过以自我为中心的社会交互感知基准测试全模态大语言模型的主动干预能力",
    "relevance_score": 2,
    "reasoning": "该论文主要关注社交交互感知和主动干预能力的基准测试，属于纯粹的评估基准范畴。虽然涉及多模态LLM，但其核心焦点是社交交互理解和干预能力评估，与推荐系统、搜索或广告的核心技术进展缺乏直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13084v1": {
    "title": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar Propagation",
    "url": "https://www.alphaxiv.org/abs/2510.13084v1",
    "arxiv_id": "2510.13084v1",
    "authors": "Yi Zuo, Zitao Wang, Lingling Li, Xu Liu, Fang Liu, Licheng Jiao",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 01:55:32",
    "ori_summary": "Text-to-image (T2I) diffusion models have recently demonstrated significant progress in video editing. However, existing video editing methods are severely limited by their high computational overhead and memory consumption. Furthermore, these approaches often sacrifice visual fidelity, leading to undesirable temporal inconsistencies and artifacts such as blurring and pronounced mosaic-like patterns. We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video editing method. Edit-Your-Interest introduces a spatio-temporal feature memory to cache features from previous frames, significantly reducing computational overhead compared to full-sequence spatio-temporal modeling approaches. Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM), which is designed to efficiently cache and retain the crucial image tokens processed by spatial attention. Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP propagates the most relevant tokens from previous frames to subsequent ones, preserving temporal consistency. Finally, we introduce an SFM update algorithm that continuously refreshes the cached features, ensuring their long-term relevance and effectiveness throughout the video sequence. Furthermore, we leverage cross-attention maps to automatically extract masks for the instances of interest. These masks are seamlessly integrated into the diffusion denoising process, enabling fine-grained control over target objects and allowing Edit-Your-Interest to perform highly accurate edits while robustly preserving the background integrity. Extensive experiments decisively demonstrate that the proposed Edit-Your-Interest outperforms state-of-the-art methods in both efficiency and visual fidelity, validating its superior effectiveness and practicality.",
    "summary": "",
    "translation": "编辑您的兴趣：通过特征最相似传播实现高效视频编辑",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频编辑技术，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然特征传播方法在技术上可能有一些通用性，但论文明确聚焦于视频编辑应用，没有显示出在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13080v1": {
    "title": "Counting Hallucinations in Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2510.13080v1",
    "arxiv_id": "2510.13080v1",
    "authors": "Shuai Fu, Jian Zhou, Qi Chen, Huang Jing, Huy Anh Nguyen, Xiaohan Liu, Zhixiong Zeng, Lin Ma, Quanshi Zhang, Qi Wu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 01:48:04",
    "ori_summary": "Diffusion probabilistic models (DPMs) have demonstrated remarkable progress in generative tasks, such as image and video synthesis. However, they still often produce hallucinated samples (hallucinations) that conflict with real-world knowledge, such as generating an implausible duplicate cup floating beside another cup. Despite their prevalence, the lack of feasible methodologies for systematically quantifying such hallucinations hinders progress in addressing this challenge and obscures potential pathways for designing next-generation generative models under factual constraints. In this work, we bridge this gap by focusing on a specific form of hallucination, which we term counting hallucination, referring to the generation of an incorrect number of instances or structured objects, such as a hand image with six fingers, despite such patterns being absent from the training data. To this end, we construct a dataset suite CountHalluSet, with well-defined counting criteria, comprising ToyShape, SimObject, and RealHand. Using these datasets, we develop a standardized evaluation protocol for quantifying counting hallucinations, and systematically examine how different sampling conditions in DPMs, including solver type, ODE solver order, sampling steps, and initial noise, affect counting hallucination levels. Furthermore, we analyze their correlation with common evaluation metrics such as FID, revealing that this widely used image quality metric fails to capture counting hallucinations consistently. This work aims to take the first step toward systematically quantifying hallucinations in diffusion models and offer new insights into the investigation of hallucination phenomena in image generation.",
    "summary": "",
    "translation": "扩散模型中的幻觉计数",
    "relevance_score": 1,
    "reasoning": "该论文专注于扩散模型的幻觉问题，这属于纯粹的生成模型评估范畴，与推荐系统、搜索或广告的核心技术无关。论文内容涉及AIGC和幻觉评估，这些都是明确列出的无关主题，没有任何潜在的应用于RecSys/Search/Ads的可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13075v1": {
    "title": "Unsupervised Domain Adaptation via Content Alignment for Hippocampus Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.13075v1",
    "arxiv_id": "2510.13075v1",
    "authors": "Hoda Kalabizadeh, Ludovica Griffanti, Pak-Hei Yeung, Ana I. L. Namburete, Nicola K. Dinsdale, Konstantinos Kamnitsas",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 01:34:41",
    "ori_summary": "Deep learning models for medical image segmentation often struggle when deployed across different datasets due to domain shifts - variations in both image appearance, known as style, and population-dependent anatomical characteristics, referred to as content. This paper presents a novel unsupervised domain adaptation framework that directly addresses domain shifts encountered in cross-domain hippocampus segmentation from MRI, with specific emphasis on content variations. Our approach combines efficient style harmonisation through z-normalisation with a bidirectional deformable image registration (DIR) strategy. The DIR network is jointly trained with segmentation and discriminator networks to guide the registration with respect to a region of interest and generate anatomically plausible transformations that align source images to the target domain. We validate our approach through comprehensive evaluations on both a synthetic dataset using Morpho-MNIST (for controlled validation of core principles) and three MRI hippocampus datasets representing populations with varying degrees of atrophy. Across all experiments, our method outperforms existing baselines. For hippocampus segmentation, when transferring from young, healthy populations to clinical dementia patients, our framework achieves up to 15% relative improvement in Dice score compared to standard augmentation methods, with the largest gains observed in scenarios with substantial content shift. These results highlight the efficacy of our approach for accurate hippocampus segmentation across diverse populations.",
    "summary": "",
    "translation": "基于内容对齐的无监督领域自适应用于海马体分割",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像中的海马体分割，属于医学和生物领域的特定应用，与推荐系统、搜索或广告领域完全无关。论文标题明确指向医学影像分析，没有任何技术要素可以应用于推荐系统、搜索或广告领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13067v1": {
    "title": "Direction-aware multi-scale gradient loss for infrared and visible image fusion",
    "url": "https://www.alphaxiv.org/abs/2510.13067v1",
    "arxiv_id": "2510.13067v1",
    "authors": "Kaixuan Yang, Wei Xiang, Zhenshuai Chen, Tong Jin, Yunpeng Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-15 01:26:39",
    "ori_summary": "Infrared and visible image fusion aims to integrate complementary information from co-registered source images to produce a single, informative result. Most learning-based approaches train with a combination of structural similarity loss, intensity reconstruction loss, and a gradient-magnitude term. However, collapsing gradients to their magnitude removes directional information, yielding ambiguous supervision and suboptimal edge fidelity. We introduce a direction-aware, multi-scale gradient loss that supervises horizontal and vertical components separately and preserves their sign across scales. This axis-wise, sign-preserving objective provides clear directional guidance at both fine and coarse resolutions, promoting sharper, better-aligned edges and richer texture preservation without changing model architectures or training protocols. Experiments on open-source model and multiple public benchmarks demonstrate effectiveness of our approach.",
    "summary": "",
    "translation": "用于红外与可见光图像融合的方向感知多尺度梯度损失",
    "relevance_score": 1,
    "reasoning": "该论文专注于红外与可见光图像融合的计算机视觉任务，属于纯粹的视觉处理领域。虽然提到了多尺度梯度损失等技术，但没有任何与推荐系统、搜索或广告相关的潜在应用场景或技术迁移可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.13063v1": {
    "title": "True Self-Supervised Novel View Synthesis is Transferable",
    "url": "https://www.alphaxiv.org/abs/2510.13063v1",
    "arxiv_id": "2510.13063v1",
    "authors": "Thomas W. Mitchel, Hyunwoo Ryu, Vincent Sitzmann",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-15 01:09:56",
    "ori_summary": "In this paper, we identify that the key criterion for determining whether a model is truly capable of novel view synthesis (NVS) is transferability: Whether any pose representation extracted from one video sequence can be used to re-render the same camera trajectory in another. We analyze prior work on self-supervised NVS and find that their predicted poses do not transfer: The same set of poses lead to different camera trajectories in different 3D scenes. Here, we present XFactor, the first geometry-free self-supervised model capable of true NVS. XFactor combines pair-wise pose estimation with a simple augmentation scheme of the inputs and outputs that jointly enables disentangling camera pose from scene content and facilitates geometric reasoning. Remarkably, we show that XFactor achieves transferability with unconstrained latent pose variables, without any 3D inductive biases or concepts from multi-view geometry -- such as an explicit parameterization of poses as elements of SE(3). We introduce a new metric to quantify transferability, and through large-scale experiments, we demonstrate that XFactor significantly outperforms prior pose-free NVS transformers, and show that latent poses are highly correlated with real-world poses through probing experiments.",
    "summary": "",
    "translation": "真正的自监督新视角合成具有可迁移性",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉中的新视角合成技术，属于纯粹的视觉生成任务。虽然自监督学习是重要的技术方向，但论文内容主要涉及3D场景重建和视图生成，与推荐系统、搜索或广告的核心技术缺乏直接关联。在推荐/搜索/广告领域，这种视觉生成技术的潜在应用场景非常有限且不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  }
}