{
  "2511.04541v1": {
    "title": "LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems",
    "url": "https://www.alphaxiv.org/abs/2511.04541v1",
    "arxiv_id": "2511.04541v1",
    "authors": "Baptiste Bonin, Maxime Heuillet, Audrey Durand",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-11-06 16:54:54",
    "ori_summary": "Modeling user preferences across domains remains a key challenge in slate recommendation (i.e. recommending an ordered sequence of items) research. We investigate how Large Language Models (LLM) can effectively act as world models of user preferences through pairwise reasoning over slates. We conduct an empirical study involving several LLMs on three tasks spanning different datasets. Our results reveal relationships between task performance and properties of the preference function captured by LLMs, hinting towards areas for improvement and highlighting the potential of LLMs as world models in recommender systems.",
    "summary": "论文研究slate推荐系统中跨领域用户偏好建模的核心挑战，核心思想是利用大型语言模型通过slate间的成对推理来构建用户偏好的世界模型。",
    "translation": "LLM作为评判者：面向板岩推荐系统的世界模型",
    "relevance_score": 8,
    "reasoning": "该论文直接应用LLM技术于推荐系统领域，通过将LLM作为评判者来构建世界模型，这属于'直接LLM应用'范畴。板岩推荐系统是搜索和推荐中的核心问题，该方法通过LLM模拟用户对推荐列表的整体反应，可显著提升多物品推荐场景的性能和用户体验。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接研究LLM在推荐系统中的应用，探索LLM作为世界模型进行slate推荐，完全符合直接LLM应用和推荐系统核心进展的焦点领域。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.04491v1": {
    "title": "RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables",
    "url": "https://www.alphaxiv.org/abs/2511.04491v1",
    "arxiv_id": "2511.04491v1",
    "authors": "Nikhil Abhyankar, Purvi Chaurasia, Sanchit Kabra, Ananya Srivastava, Vivek Gupta, Chandan K. Reddy",
    "categories": "cs.CL, cs.AI, cs.DB, cs.IR, cs.LG",
    "pub_date": "2025-11-06 16:10:03",
    "ori_summary": "Existing tabular reasoning benchmarks mostly test models on small, uniform tables, underrepresenting the complexity of real-world data and giving an incomplete view of Large Language Models' (LLMs) reasoning abilities. Real tables are long, heterogeneous, and domain-specific, mixing structured fields with free text and requiring multi-hop reasoning across thousands of tokens. To address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from 2031 real-world tables spanning two domains: i) RB-Science (NSF grant records) and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates LLMs jointly across scale, heterogeneity, domain specificity, and reasoning complexity. Experiments with open-source and proprietary models show that LLMs struggle with heterogeneous schemas and complex multi-hop inference, revealing persistent weaknesses in current architectures and prompting strategies. RUST-BENCH establishes a challenging new testbed for advancing tabular reasoning research.",
    "summary": "",
    "translation": "RUST-BENCH：在结构化表格中的非结构化文本上对大型语言模型推理能力进行基准测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM推理能力的基准测试，属于评估基准范畴，这在无关主题中明确排除。虽然表格数据处理在搜索和推荐系统中具有潜在应用，但论文的核心焦点是基准测试而非技术进展或直接应用，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04473v1": {
    "title": "Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs",
    "url": "https://www.alphaxiv.org/abs/2511.04473v1",
    "arxiv_id": "2511.04473v1",
    "authors": "Alberto Cattaneo, Carlo Luschi, Daniel Justus",
    "categories": "cs.LG, cs.AI, cs.CL, cs.IR",
    "pub_date": "2025-11-06 15:45:18",
    "ori_summary": "Retrieval of information from graph-structured knowledge bases represents a promising direction for improving the factuality of LLMs. While various solutions have been proposed, a comparison of methods is difficult due to the lack of challenging QA datasets with ground-truth targets for graph retrieval. We present SynthKGQA, a framework for generating high-quality synthetic Knowledge Graph Question Answering datasets from any Knowledge Graph, providing the full set of ground-truth facts in the KG to reason over each question. We show how, in addition to enabling more informative benchmarking of KG retrievers, the data produced with SynthKGQA also allows us to train better models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset designed to test zero-shot generalization abilities of KG retrievers with respect to unseen graph structures and relation types, and benchmark popular solutions for KG-augmented LLMs on it.",
    "summary": "",
    "translation": "用于知识图谱增强大语言模型更好训练和评估的真实子图",
    "relevance_score": 3,
    "reasoning": "该论文主要关注知识图谱增强的LLM训练和评估方法，这属于LLM技术本身的研究。虽然知识图谱在推荐和搜索中有应用潜力，但论文标题明确聚焦于训练和评估方法，而非直接应用于推荐系统、搜索或广告领域。该工作更偏向于NLP领域的评估基准研究，与当前关注的直接应用和架构创新关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04247v1": {
    "title": "On the Brittleness of CLIP Text Encoders",
    "url": "https://www.alphaxiv.org/abs/2511.04247v1",
    "arxiv_id": "2511.04247v1",
    "authors": "Allie Tran, Luca Rossetto",
    "categories": "cs.MM, cs.AI, cs.IR",
    "pub_date": "2025-11-06 10:33:55",
    "ori_summary": "Multimodal co-embedding models, especially CLIP, have advanced the state of the art in zero-shot classification and multimedia information retrieval in recent years by aligning images and text in a shared representation space. However, such modals trained on a contrastive alignment can lack stability towards small input perturbations. Especially when dealing with manually expressed queries, minor variations in the query can cause large differences in the ranking of the best-matching results. In this paper, we present a systematic analysis of the effect of multiple classes of non-semantic query perturbations in an multimedia information retrieval scenario. We evaluate a diverse set of lexical, syntactic, and semantic perturbations across multiple CLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 video collection. Across models, we find that syntactic and semantic perturbations drive the largest instabilities, while brittleness is concentrated in trivial surface edits such as punctuation and case. Our results highlight robustness as a critical dimension for evaluating vision-language models beyond benchmark accuracy.",
    "summary": "",
    "translation": "论CLIP文本编码器的脆弱性",
    "relevance_score": 3,
    "reasoning": "虽然CLIP是视觉-语言模型，但本文聚焦于文本编码器的脆弱性分析，这属于模型鲁棒性研究而非直接应用于推荐/搜索/广告的异构数据统一建模。该研究可能对多模态系统的可靠性有启示，但缺乏明确的RecSys/Search/Ads应用场景说明，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04237v1": {
    "title": "Denoised Recommendation Model with Collaborative Signal Decoupling",
    "url": "https://www.alphaxiv.org/abs/2511.04237v1",
    "arxiv_id": "2511.04237v1",
    "authors": "Zefeng Li, Ning Yang",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-11-06 10:18:02",
    "ori_summary": "Although the collaborative filtering (CF) algorithm has achieved remarkable performance in recommendation systems, it suffers from suboptimal recommendation performance due to noise in the user-item interaction matrix. Numerous noise-removal studies have improved recommendation models, but most existing approaches conduct denoising on a single graph. This may cause attenuation of collaborative signals: removing edges between two nodes can interrupt paths between other nodes, weakening path-dependent collaborative information. To address these limitations, this study proposes a novel GNN-based CF model called DRCSD for denoising unstable interactions. DRCSD includes two core modules: a collaborative signal decoupling module (decomposes signals into distinct orders by structural characteristics) and an order-wise denoising module (performs targeted denoising on each order). Additionally, the information aggregation mechanism of traditional GNN-based CF models is modified to avoid cross-order signal interference until the final pooling operation. Extensive experiments on three public real-world datasets show that DRCSD has superior robustness against unstable interactions and achieves statistically significant performance improvements in recommendation accuracy metrics compared to state-of-the-art baseline models.",
    "summary": "论文研究推荐系统中用户-物品交互矩阵的噪声问题，核心思想是将协同信号按结构特征分解为不同阶次，并对每个阶次进行针对性去噪，避免传统单图去噪方法导致的协同信号衰减。",
    "translation": "基于协同信号解耦的去噪推荐模型",
    "relevance_score": 9,
    "reasoning": "该论文直接聚焦推荐系统的核心领域进展，提出通过协同信号解耦来处理推荐中的噪声问题，这是推荐系统领域的关键技术挑战。去噪和信号分离技术可以显著提升推荐模型的鲁棒性和准确性，在电商、内容推荐等场景中有直接应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统中的噪声交互问题，提出基于图神经网络的协同信号解耦和分层去噪方法，属于推荐系统核心领域的重要进展。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.04221v1": {
    "title": "Coordination-Free Lane Partitioning for Convergent ANN Search",
    "url": "https://www.alphaxiv.org/abs/2511.04221v1",
    "arxiv_id": "2511.04221v1",
    "authors": "Carl Kugblenu, Petri Vuorimaa",
    "categories": "cs.IR, cs.DB",
    "pub_date": "2025-11-06 09:36:18",
    "ori_summary": "Production vector search systems often fan out each query across parallel lanes (threads, replicas, or shards) to meet latency service-level objectives (SLOs). In practice, these lanes rediscover the same candidates, so extra compute does not increase coverage. We present a coordination-free lane partitioner that turns duplication into complementary work at the same cost and deadline. For each query we (1) build a deterministic candidate pool sized to the total top-k budget, (2) apply a per-query pseudorandom permutation, and (3) assign each lane a disjoint slice of positions. Lanes then return different results by construction, with no runtime coordination. At equal cost with four lanes (total candidate budget 64), on SIFT1M (1M SIFT feature vectors) with Hierarchical Navigable Small World graphs (HNSW) recall@10 rises from 0.249 to 0.999 while lane overlap falls from nearly 100% to 0%. On MS MARCO (8.8M passages) with HNSW, hit@10 improves from 0.200 to 0.601 and Mean Reciprocal Rank at 10 (MRR@10) from 0.133 to 0.330. For inverted file (IVF) indexes we see smaller but consistent gains (for example, +11% on MS MARCO) by de-duplicating list routing. A microbenchmark shows planner overhead of ~37 microseconds per query (mean at the main setting) with linear growth in the number of merged candidates. These results yield a simple operational guideline: size the per-query pool to the total budget, deterministically partition positions across lanes, and turn redundant fan-out into complementary coverage without changing budget or deadline.",
    "summary": "",
    "translation": "面向收敛近似最近邻搜索的无协调车道划分",
    "relevance_score": 2,
    "reasoning": "该论文主要关注近似最近邻搜索的分布式系统优化，属于底层基础设施技术。虽然近似最近邻搜索在推荐和搜索系统中用于向量检索，但本文聚焦于分布式协调机制和分区策略，属于系统架构优化而非核心算法创新。对于推荐/搜索系统的直接应用潜力有限，主要价值在于大规模向量检索系统的工程效率提升。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04172v1": {
    "title": "Transforming Mentorship: An AI Powered Chatbot Approach to University Guidance",
    "url": "https://www.alphaxiv.org/abs/2511.04172v1",
    "arxiv_id": "2511.04172v1",
    "authors": "Mashrur Rahman, Mantaqa abedin, Monowar Zamil Abir, Faizul Islam Ansari, Adib Reza, Farig Yousuf Sadeque, Niloy Farhan",
    "categories": "cs.IR, cs.CL",
    "pub_date": "2025-11-06 08:24:52",
    "ori_summary": "University students face immense challenges during their undergraduate lives, often being deprived of personalized on-demand guidance that mentors fail to provide at scale. Digital tools exist, but there is a serious lack of customized coaching for newcomers. This paper presents an AI-powered chatbot that will serve as a mentor for the students of BRAC University. The main component is a data ingestion pipeline that efficiently processes and updates information from diverse sources, such as CSV files and university webpages. The chatbot retrieves information through a hybrid approach, combining BM25 lexical ranking with ChromaDB semantic retrieval, and uses a Large Language Model, LLaMA-3.3-70B, to generate conversational responses. The generated text was found to be semantically highly relevant, with a BERTScore of 0.831 and a METEOR score of 0.809. The data pipeline was also very efficient, taking 106.82 seconds for updates, compared to 368.62 seconds for new data. This chatbot will be able to help students by responding to their queries, helping them to get a better understanding of university life, and assisting them to plan better routines for their semester in the open-credit university.",
    "summary": "",
    "translation": "转变导师制：基于AI聊天机器人方法的大学指导系统",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于教育领域的AI聊天机器人应用，用于大学指导和导师制场景。这与我的关注领域（推荐系统、搜索、广告中的核心进展或LLM技术应用）完全无关，属于教育技术这一特定领域应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04087v1": {
    "title": "E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce",
    "url": "https://www.alphaxiv.org/abs/2511.04087v1",
    "arxiv_id": "2511.04087v1",
    "authors": "Ge Zhang, Rohan Deepak Ajwani, Tony Zheng, Hongjian Gu, Yaochen Hu, Wei Guo, Mark Coates, Yingxue Zhang",
    "categories": "cs.IR",
    "pub_date": "2025-11-06 05:57:48",
    "ori_summary": "Finding relevant products given a user query plays a pivotal role in an e-commerce platform, as it can spark shopping behaviors and result in revenue gains. The challenge lies in accurately predicting the correlation between queries and products. Recently, mining the cross-features between queries and products based on the commonsense reasoning capacity of Large Language Models (LLMs) has shown promising performance. However, such methods suffer from high costs due to intensive real-time LLM inference during serving, as well as human annotations and potential Supervised Fine Tuning (SFT). To boost efficiency while leveraging the commonsense reasoning capacity of LLMs for various e-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation Enhancer (E-CARE). During inference, models augmented with E-CARE can access commonsense reasoning with only a single LLM forward pass per query by utilizing a commonsense reasoning factor graph that encodes most of the reasoning schema from powerful LLMs. The experiments on 2 downstream tasks show an improvement of up to 12.1% on precision@5.",
    "summary": "论文研究电子商务搜索中查询与产品相关性预测的效率问题，核心方法是构建常识推理因子图来编码LLM的推理模式，通过单次前向传播实现高效的常识增强推荐。",
    "translation": "E-CARE：一种基于大语言模型的高效常识增强电子商务框架",
    "relevance_score": 8,
    "reasoning": "该论文直接应用LLM技术于电子商务领域，属于'直接LLM应用'范畴。通过常识增强框架，该技术可显著提升电商推荐和搜索系统的语义理解能力，帮助系统更好地理解用户意图和商品特性，从而改进推荐和搜索质量。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对电子商务搜索中的LLM应用效率问题，提出将LLM常识推理能力高效集成到推荐系统中的创新方法，完全符合直接LLM应用和核心领域进展的关注点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.04082v1": {
    "title": "Publication Trend in DESIDOC Journal of Library and Information Technology during 2013-2017: A Scientometric Approach",
    "url": "https://www.alphaxiv.org/abs/2511.04082v1",
    "arxiv_id": "2511.04082v1",
    "authors": "M Sadik Batcha, S Roselin Jahina, Muneer Ahmad",
    "categories": "cs.DL, cs.IR",
    "pub_date": "2025-11-06 05:39:27",
    "ori_summary": "DESIDOC Journal of Library & Information Technology (DJLIT) formerly known as DESIDOC Bulletin of Information Technology is a peer-reviewed, open access, bimonthly journal. This paper presents a Scientometric analysis of the DESIDOC Journal. The paper analyses the pattern of growth of the research output published in the journal, pattern of authorship, author productivity, and, subjects covered to the papers over the period (2013-2017). It is found that 227 papers were published during the period of study (2001-2012). The maximum numbers of articles were collaborative in nature. The subject concentration of the journal noted is Scientometrics. The maximum numbers of articles (65%) have ranged their thought contents between 6 and 10 pages. The study applied standard formula and statistical tools to bring out the factual result.",
    "summary": "",
    "translation": "2013-2017年DESIDOC图书馆与信息技术杂志发表趋势：科学计量学方法",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于图书馆与信息技术领域的期刊发表趋势分析，采用科学计量学方法，属于文献计量学研究范畴。论文内容与推荐系统、搜索、广告、LLM技术或Transformer架构等核心关注领域完全无关，不涉及任何技术进展或应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04080v1": {
    "title": "Caption Injection for Optimization in Generative Search Engine",
    "url": "https://www.alphaxiv.org/abs/2511.04080v1",
    "arxiv_id": "2511.04080v1",
    "authors": "Xiaolu Chen, Yong Liao",
    "categories": "cs.IR",
    "pub_date": "2025-11-06 05:37:27",
    "ori_summary": "Generative Search Engines (GSEs) leverage Retrieval-Augmented Generation (RAG) techniques and Large Language Models (LLMs) to integrate multi-source information and provide users with accurate and comprehensive responses. Unlike traditional search engines that present results in ranked lists, GSEs shift users' attention from sequential browsing to content-driven subjective perception, driving a paradigm shift in information retrieval. In this context, enhancing the subjective visibility of content through Generative Search Engine Optimization (G-SEO) methods has emerged as a new research focus. With the rapid advancement of Multimodal Retrieval-Augmented Generation (MRAG) techniques, GSEs can now efficiently integrate text, images, audio, and video, producing richer responses that better satisfy complex information needs. Existing G-SEO methods, however, remain limited to text-based optimization and fail to fully exploit multimodal data. To address this gap, we propose Caption Injection, the first multimodal G-SEO approach, which extracts captions from images and injects them into textual content, integrating visual semantics to enhance the subjective visibility of content in generative search scenarios. We systematically evaluate Caption Injection on MRAMG, a benchmark for MRAG, under both unimodal and multimodal settings. Experimental results show that Caption Injection significantly outperforms text-only G-SEO baselines under the G-Eval metric, demonstrating the necessity and effectiveness of multimodal integration in G-SEO to improve user-perceived content visibility.",
    "summary": "该论文研究生成式搜索引擎中内容主观可见性的优化问题，核心方法是提出Caption Injection技术，通过从图像提取标题并注入文本内容来整合视觉语义，实现多模态的搜索引擎优化。",
    "translation": "生成式搜索引擎优化中的标题注入技术",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及搜索引擎优化技术，属于核心搜索领域进展。标题注入技术可能指通过优化生成内容（如标题）来提升搜索效果，这与生成式搜索系统直接相关。这种优化技术可以应用于改进搜索结果的质量和相关性，对搜索系统具有明确的实用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对生成式搜索引擎优化，属于搜索领域的核心进展，并创新性地将视觉语义注入文本内容，与VLM处理异构数据的思路高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.04075v1": {
    "title": "Two Decades of Research at the University of Lagos (2004-2023): A Scientometric Analysis of Productivity, Collaboration, and Impact",
    "url": "https://www.alphaxiv.org/abs/2511.04075v1",
    "arxiv_id": "2511.04075v1",
    "authors": "Muneer Ahmad, Samuel Ibor Ubi",
    "categories": "cs.DL, cs.IR",
    "pub_date": "2025-11-06 05:26:17",
    "ori_summary": "This paper presents a scientometric analysis of research output from the University of Lagos, focusing on the two decades spanning 2004 to 2023. Using bibliometric data retrieved from the Web of Science, we examine trends in publication volume, collaboration patterns, citation impact, and the most prolific authors, departments, and research domains at the university. The study reveals a consistent increase in research productivity, with the highest publication output recorded in 2023. Health Sciences, Engineering, and Social Sciences are identified as dominant fields, reflecting the university's interdisciplinary research strengths. Collaborative efforts, both locally and internationally, show a positive correlation with higher citation impact, with the United States and the United Kingdom being the leading international collaborators. Notably, open-access publications account for a significant portion of the university's research output, enhancing visibility and citation rates. The findings offer valuable insights into the university's research performance over the past two decades, providing a foundation for strategic planning and policy formulation to foster research excellence and global impact.",
    "summary": "",
    "translation": "拉各斯大学二十年研究回顾（2004-2023）：生产力、合作与影响力的科学计量分析",
    "relevance_score": 1,
    "reasoning": "该论文是对大学整体研究产出的科学计量分析，完全不涉及推荐系统、搜索、广告或LLM相关技术。这属于学术评价和文献计量学范畴，与用户关注的LLM技术、推荐系统核心进展、Transformer架构改进等所有技术焦点均无关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04073v1": {
    "title": "Learning Filter-Aware Distance Metrics for Nearest Neighbor Search with Multiple Filters",
    "url": "https://www.alphaxiv.org/abs/2511.04073v1",
    "arxiv_id": "2511.04073v1",
    "authors": "Ananya Sutradhar, Suryansh Gupta, Ravishankar Krishnaswamy, Haiyang Xu, Aseem Rastogi, Gopal Srinivasa",
    "categories": "cs.LG, cs.DB, cs.IR",
    "pub_date": "2025-11-06 05:24:41",
    "ori_summary": "Filtered Approximate Nearest Neighbor (ANN) search retrieves the closest vectors for a query vector from a dataset. It enforces that a specified set of discrete labels $S$ for the query must be included in the labels of each retrieved vector. Existing graph-based methods typically incorporate filter awareness by assigning fixed penalties or prioritizing nodes based on filter satisfaction. However, since these methods use fixed, data in- dependent penalties, they often fail to generalize across datasets with diverse label and vector distributions. In this work, we propose a principled alternative that learns the optimal trade-off between vector distance and filter match directly from the data, rather than relying on fixed penalties. We formulate this as a constrained linear optimization problem, deriving weights that better reflect the underlying filter distribution and more effectively address the filtered ANN search problem. These learned weights guide both the search process and index construction, leading to graph structures that more effectively capture the underlying filter distribution and filter semantics. Our experiments demonstrate that adapting the distance function to the data significantly im- proves accuracy by 5-10% over fixed-penalty methods, providing a more flexible and generalizable framework for the filtered ANN search problem.",
    "summary": "该论文研究带多个离散标签过滤的近似最近邻搜索问题，核心思想是通过约束线性优化从数据中学习向量距离与过滤匹配之间的最优权衡权重，取代传统的固定惩罚方法。",
    "translation": "学习具有过滤器感知的距离度量以实现多过滤器下的最近邻搜索",
    "relevance_score": 7,
    "reasoning": "该论文涉及距离度量和最近邻搜索的改进，这是搜索和推荐系统中的核心组件。过滤器感知的距离学习可以应用于多维度过滤场景，如个性化推荐中的多约束条件搜索或广告系统中的定向过滤。这种技术能够提升在复杂过滤条件下的检索质量和效率，对搜索和推荐系统有直接的应用价值。",
    "rerank_relevance_score": 6,
    "rerank_reasoning": "该论文针对带过滤的近似最近邻搜索提出数据驱动的距离度量学习方法，与推荐系统中多维度过滤和个性化搜索需求相关，但未直接涉及LLM或Transformer架构。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.04662v1": {
    "title": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks",
    "url": "https://www.alphaxiv.org/abs/2511.04662v1",
    "arxiv_id": "2511.04662v1",
    "authors": "Yu Feng, Nathaniel Weir, Kaj Bostrom, Sam Bayless, Darion Cassel, Sapana Chaudhary, Benjamin Kiesl-Reiter, Huzefa Rangwala",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-11-06 18:50:08",
    "ori_summary": "LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but they cannot reliably verify their own logic. Even when they reach correct answers, the underlying reasoning may be flawed, undermining trust in high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a neuro-symbolic method that extracts and verifies formal logical arguments from CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order logic and identifies premises that ground the argument in source context, commonsense knowledge, or prior reasoning steps. The symbolic representation enables automated solvers to verify logical validity while the NL premises allow humans and systems to identify ungrounded or fallacious reasoning steps. Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT effectively identifies flawed reasoning, and serves as a strong predictor of final answer correctness. We also leverage VeriCoT's verification signal for (1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct preference optimization (DPO) using verification-based pairwise rewards, further improving reasoning validity and accuracy.",
    "summary": "该论文研究大语言模型思维链推理的可靠性验证问题，核心思想是通过将自然语言推理步骤形式化为一阶逻辑，利用符号推理器自动验证逻辑一致性并识别未接地或谬误的推理步骤。",
    "translation": "VeriCoT：通过逻辑一致性检查进行神经符号思维链验证",
    "relevance_score": 7,
    "reasoning": "该论文涉及思维链验证技术，属于核心LLM技术的进步领域。在搜索和推荐系统中，思维链验证可以用于提高复杂查询推理的可靠性和可解释性，确保推荐或搜索结果的逻辑一致性，这对于处理多步骤推理任务至关重要。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的神经符号验证方法可直接应用于搜索和推荐系统中的推理验证，其逻辑一致性检查机制对高风险的广告和推荐决策具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.04654v1": {
    "title": "Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning",
    "url": "https://www.alphaxiv.org/abs/2511.04654v1",
    "arxiv_id": "2511.04654v1",
    "authors": "Mohammad Atif Quamar, Mohammad Areeb",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 18:43:16",
    "ori_summary": "Chain-of-Thought (CoT) prompting is a key technique for enabling complex reasoning in large language models. However, generating full, fixed-length rationales is computationally wasteful, inflating both token usage and latency. We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free decoding algorithm that adaptively halts rationale generation. LEASH monitors two intrinsic signals: the slope of token-level entropy and the improvement in the top-logit margin. It terminates the generation once both signals plateau, indicating the model has reached a stable reasoning state. Across four instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces average token generation by 30--35% and latency by 27%, while incurring a 10 p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no additional training or supervision, offering a simple and efficient alternative to CoT decoding.",
    "summary": "",
    "translation": "用于高效思维链推理的对数熵自适应停止启发式方法",
    "relevance_score": 3,
    "reasoning": "该论文提出了一种用于思维链推理的自适应停止方法，虽然思维链推理是LLM的核心技术，但论文主要关注推理效率优化而非直接应用于推荐/搜索/广告领域。这种效率优化技术理论上可以降低LLM推理成本，可能间接应用于需要复杂推理的搜索或推荐场景，但应用路径不够直接明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04646v1": {
    "title": "DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration",
    "url": "https://www.alphaxiv.org/abs/2511.04646v1",
    "arxiv_id": "2511.04646v1",
    "authors": "Narjes Nourzad, Hanqing Yang, Shiyu Chen, Carlee Joe-Wong",
    "categories": "cs.AI, cs.CL, cs.LG, cs.MA",
    "pub_date": "2025-11-06 18:37:18",
    "ori_summary": "Cooperative multi-agent planning requires agents to make joint decisions with partial information and limited communication. Coordination at the trajectory level often fails, as small deviations in timing or movement cascade into conflicts. Symbolic planning mitigates this challenge by raising the level of abstraction and providing a minimal vocabulary of actions that enable synchronization and collective progress. We present DR. WELL, a decentralized neurosymbolic framework for cooperative multi-agent planning. Cooperation unfolds through a two-phase negotiation protocol: agents first propose candidate roles with reasoning and then commit to a joint allocation under consensus and environment constraints. After commitment, each agent independently generates and executes a symbolic plan for its role without revealing detailed trajectories. Plans are grounded in execution outcomes via a shared world model that encodes the current state and is updated as agents act. By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids brittle step-level alignment and enables higher-level operations that are reusable, synchronizable, and interpretable. Experiments on cooperative block-push tasks show that agents adapt across episodes, with the dynamic world model capturing reusable patterns and improving task completion rates and efficiency. Experiments on cooperative block-push tasks show that our dynamic world model improves task completion and efficiency through negotiation and self-refinement, trading a time overhead for evolving, more efficient collaboration strategies.",
    "summary": "",
    "translation": "DR. WELL：基于具身LLM的多智能体协作的动态推理与符号世界模型学习",
    "relevance_score": 2,
    "reasoning": "该论文主要关注具身智能和多智能体协作中的动态推理与符号世界模型，这属于机器人学和多智能体系统的特定领域。虽然提到了LLM技术，但其应用场景（具身智能、世界模型）与推荐系统、搜索或广告的核心领域没有直接关联，也没有明确展示在RecSys/Search/Ads中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04643v1": {
    "title": "When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection",
    "url": "https://www.alphaxiv.org/abs/2511.04643v1",
    "arxiv_id": "2511.04643v1",
    "authors": "Alamgir Munir Qazi, John P. McCrae, Jamal Abdul Nasir",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 18:35:45",
    "ori_summary": "The proliferation of misinformation necessitates robust yet computationally efficient fact verification systems. While current state-of-the-art approaches leverage Large Language Models (LLMs) for generating explanatory rationales, these methods face significant computational barriers and hallucination risks in real-world deployments. We present DeReC (Dense Retrieval Classification), a lightweight framework that demonstrates how general-purpose text embeddings can effectively replace autoregressive LLM-based approaches in fact verification tasks. By combining dense retrieval with specialized classification, our system achieves better accuracy while being significantly more efficient. DeReC outperforms explanation-generating LLMs in efficiency, reducing runtime by 95% on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92% on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds), showcasing its effectiveness across varying dataset sizes. On the RAWFC dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art method L-Defense (61.20%). Our results demonstrate that carefully engineered retrieval-based systems can match or exceed LLM performance in specialized tasks while being significantly more practical for real-world deployment.",
    "summary": "",
    "translation": "当检索优于生成：用于可扩展假新闻检测的密集证据检索",
    "relevance_score": 2,
    "reasoning": "该论文主要关注假新闻检测，这是一个内容安全/可信度验证问题，而非推荐系统、搜索或广告的核心排名任务。虽然提到了检索技术，但其应用场景（假新闻检测）属于内容安全领域，被明确列为不相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04584v1": {
    "title": "Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis",
    "url": "https://www.alphaxiv.org/abs/2511.04584v1",
    "arxiv_id": "2511.04584v1",
    "authors": "Daniel Gomm, Cornelius Wolff, Madelon Hulsebos",
    "categories": "cs.AI, cs.CL, cs.DB, cs.HC",
    "pub_date": "2025-11-06 17:39:18",
    "ori_summary": "Natural language interfaces to tabular data must handle ambiguities inherent to queries. Instead of treating ambiguity as a deficiency, we reframe it as a feature of cooperative interaction, where the responsibility of query specification is shared among the user and the system. We develop a principled framework distinguishing cooperative queries, i.e., queries that yield a resolvable interpretation, from uncooperative queries that cannot be resolved. Applying the framework to evaluations for tabular question answering and analysis, we analyze the queries in 15 popular datasets, and observe an uncontrolled mixing of query types neither adequate for evaluating a system's execution accuracy nor for evaluating interpretation capabilities. Our framework and analysis of queries shifts the perspective from fixing ambiguity to embracing cooperation in resolving queries. This reflection enables more informed design and evaluation for natural language interfaces for tabular data, for which we outline implications and directions for future research.",
    "summary": "论文研究自然语言查询在表格数据分析中的歧义性问题，核心思想是将歧义重构为用户与系统协作交互的特征，提出区分可解析协作查询与不可解析非协作查询的框架，以改进自然语言接口的设计与评估。",
    "translation": "我们是否在提出正确的问题？论表格数据分析中自然语言查询的模糊性",
    "relevance_score": 4,
    "reasoning": "该论文探讨自然语言查询在表格数据分析中的模糊性问题，这与搜索系统中查询理解和语义解析相关。虽然查询理解是搜索领域的重要问题，但论文聚焦于表格数据分析这一特定场景，而非通用的搜索或推荐系统应用，因此相关性有限。",
    "rerank_relevance_score": 6,
    "rerank_reasoning": "该论文提出将查询歧义视为协作特征而非缺陷的框架，直接适用于搜索系统的查询理解与交互设计，但主要聚焦表格数据分析而非推荐或广告场景。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.04583v1": {
    "title": "Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper",
    "url": "https://www.alphaxiv.org/abs/2511.04583v1",
    "arxiv_id": "2511.04583v1",
    "authors": "Atsuyuki Miyai, Mashiro Toyooka, Takashi Otonari, Zaiying Zhao, Kiyoharu Aizawa",
    "categories": "cs.AI, cs.CL, cs.CV, cs.LG",
    "pub_date": "2025-11-06 17:37:49",
    "ori_summary": "Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.",
    "summary": "",
    "translation": "初级AI科学家及其风险报告：基于基线论文的自主科学探索",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于AI驱动的自主科学探索和风险评估，属于AI在科学研究领域的应用。这与推荐系统、搜索或广告的核心技术进展、LLM基础技术、Transformer架构改进或直接应用均无明确关联。标题中提到的'基线论文'和'风险报告'表明其关注科学方法论和风险评估，而非推荐/搜索/广告领域的技术问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04570v1": {
    "title": "Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm",
    "url": "https://www.alphaxiv.org/abs/2511.04570v1",
    "arxiv_id": "2511.04570v1",
    "authors": "Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-11-06 17:25:23",
    "ori_summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce \"Thinking with Video\", a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2's performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions \"thinking with video\" as a unified multimodal reasoning paradigm.",
    "summary": "",
    "translation": "视频思维：视频生成作为一种有前景的多模态推理范式",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频生成作为多模态推理范式，这属于纯粹的生成式AI领域，与推荐系统、搜索或广告的核心排名任务没有直接关联。虽然多模态推理在概念上可能与搜索相关，但视频生成本身更偏向内容创作而非信息检索或用户行为建模，在当前聚焦范围内应用潜力有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04560v1": {
    "title": "BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering",
    "url": "https://www.alphaxiv.org/abs/2511.04560v1",
    "arxiv_id": "2511.04560v1",
    "authors": "Sadia Sultana, Saiyma Sittul Muna, Mosammat Zannatul Samarukh, Ajwad Abrar, Tareque Mohmud Chowdhury",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 17:15:33",
    "ori_summary": "Developing accurate biomedical Question Answering (QA) systems in low-resource languages remains a major challenge, limiting equitable access to reliable medical knowledge. This paper introduces BanglaMedQA and BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical artificial intelligence (AI). The study applies and benchmarks several Retrieval-Augmented Generation (RAG) strategies, including Traditional, Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining textbook-based and web retrieval with generative reasoning to improve factual accuracy. A key novelty lies in integrating a Bangla medical textbook corpus through Optical Character Recognition (OCR) and implementing an Agentic RAG pipeline that dynamically selects between retrieval and reasoning strategies. Experimental results show that the Agentic RAG achieved the highest accuracy 89.54% with openai/gpt-oss-120b, outperforming other configurations and demonstrating superior rationale quality. These findings highlight the potential of RAG-based methods to enhance the reliability and accessibility of Bangla medical QA, establishing a foundation for future research in multilingual medical artificial intelligence.",
    "summary": "",
    "translation": "BanglaMedQA与BanglaMMedBench：针对孟加拉语生物医学问答的检索增强生成策略评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于生物医学领域的孟加拉语问答评估，属于明确的医学领域特定应用。虽然涉及检索增强生成技术，但其应用场景（生物医学、孟加拉语）与推荐系统、搜索或广告的核心技术领域完全无关，且医学应用属于明确排除的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04538v1": {
    "title": "From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting",
    "url": "https://www.alphaxiv.org/abs/2511.04538v1",
    "arxiv_id": "2511.04538v1",
    "authors": "Cyril Vallez, Alexander Sternfeld, Andrei Kucharavy, Ljiljana Dolamic",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 16:52:27",
    "ori_summary": "As the role of Large Language Models (LLM)-based coding assistants in software development becomes more critical, so does the role of the bugs they generate in the overall cybersecurity landscape. While a number of LLM code security benchmarks have been proposed alongside approaches to improve the security of generated code, it remains unclear to what extent they have impacted widely used coding LLMs. Here, we show that even the latest open-weight models are vulnerable in the earliest reported vulnerability scenarios in a realistic use setting, suggesting that the safety-functionality trade-off has until now prevented effective patching of vulnerabilities. To help address this issue, we introduce a new severity metric that reflects the risk posed by an LLM-generated vulnerability, accounting for vulnerability severity, generation chance, and the formulation of the prompt that induces vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation of the most serious and prevalent vulnerabilities, we use PE to define the Model Exposure (ME) score, which indicates the severity and prevalence of vulnerabilities a model generates.",
    "summary": "",
    "translation": "从模型到漏洞：迈向可操作的LLM生成漏洞报告",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于LLM生成的漏洞报告和网络安全领域，属于安全范畴，这明确列在无关主题中。论文内容涉及漏洞发现和报告，与推荐系统、搜索或广告的核心技术进展、LLM使能技术或Transformer架构改进没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04528v1": {
    "title": "IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection",
    "url": "https://www.alphaxiv.org/abs/2511.04528v1",
    "arxiv_id": "2511.04528v1",
    "authors": "Kaveh Eskandari Miandoab, Katharine Kowalyshyn, Kabir Pamnani, Anesu Gavhera, Vasanth Sarathy, Matthias Scheutz",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 16:43:37",
    "ori_summary": "We present IntelliProof, an interactive system for analyzing argumentative essays through LLMs. IntelliProof structures an essay as an argumentation graph, where claims are represented as nodes, supporting evidence is attached as node properties, and edges encode supporting or attacking relations. Unlike existing automated essay scoring systems, IntelliProof emphasizes the user experience: each relation is initially classified and scored by an LLM, then visualized for enhanced understanding. The system provides justifications for classifications and produces quantitative measures for essay coherence. It enables rapid exploration of argumentative quality while retaining human oversight. In addition, IntelliProof provides a set of tools for a better understanding of an argumentative essay and its corresponding graph in natural language, bridging the gap between the structural semantics of argumentative essays and the user's understanding of a given text. A live demo and the system are available here to try: \\textbf{https://intelliproof.vercel.app}",
    "summary": "",
    "translation": "IntelliProof：一种基于论证网络的对话式助手，用于结构化反思",
    "relevance_score": 2,
    "reasoning": "该论文主要关注对话式助手和论证网络，用于结构化反思任务，这与推荐系统、搜索或广告的核心技术领域没有直接关联。虽然对话系统技术可能间接应用于搜索对话界面，但论文的焦点是反思辅助而非排名、检索或用户建模等核心RecSys/Search/Ads问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04527v1": {
    "title": "Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics",
    "url": "https://www.alphaxiv.org/abs/2511.04527v1",
    "arxiv_id": "2511.04527v1",
    "authors": "Amir Zur, Atticus Geiger, Ekdeep Singh Lubana, Eric Bigelow",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-11-06 16:43:25",
    "ori_summary": "When a language model generates text, the selection of individual tokens might lead it down very different reasoning paths, making uncertainty difficult to quantify. In this work, we consider whether reasoning language models represent the alternate paths that they could take during generation. To test this hypothesis, we use hidden activations to control and predict a language model's uncertainty during chain-of-thought reasoning. In our experiments, we find a clear correlation between how uncertain a model is at different tokens, and how easily the model can be steered by controlling its activations. This suggests that activation interventions are most effective when there are alternate paths available to the model -- in other words, when it has not yet committed to a particular final answer. We also find that hidden activations can predict a model's future outcome distribution, demonstrating that models implicitly represent the space of possible paths.",
    "summary": "",
    "translation": "语言模型是否意识到未选择的路径？基于词元级别不确定性和隐藏状态动态的分析",
    "relevance_score": 3,
    "reasoning": "该论文主要研究语言模型内部的不确定性机制和隐藏状态动态，属于LLM基础技术范畴。虽然涉及语言模型内部工作机制，但其焦点是模型意识和路径选择的理论分析，与推荐系统、搜索或广告的直接应用关联较弱。这种不确定性分析可能为模型可靠性提供见解，但在当前焦点领域缺乏明确的应用路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04506v1": {
    "title": "Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways",
    "url": "https://www.alphaxiv.org/abs/2511.04506v1",
    "arxiv_id": "2511.04506v1",
    "authors": "Paloma Rabaey, Jong Hak Moon, Jung-Oh Lee, Min Gwan Kim, Hangyul Yoon, Thomas Demeester, Edward Choi",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 16:24:53",
    "ori_summary": "Radiology reports are invaluable for clinical decision-making and hold great potential for automated analysis when structured into machine-readable formats. These reports often contain uncertainty, which we categorize into two distinct types: (i) Explicit uncertainty reflects doubt about the presence or absence of findings, conveyed through hedging phrases. These vary in meaning depending on the context, making rule-based systems insufficient to quantify the level of uncertainty for specific findings; (ii) Implicit uncertainty arises when radiologists omit parts of their reasoning, recording only key findings or diagnoses. Here, it is often unclear whether omitted findings are truly absent or simply unmentioned for brevity. We address these challenges with a two-part framework. We quantify explicit uncertainty by creating an expert-validated, LLM-based reference ranking of common hedging phrases, and mapping each finding to a probability value based on this reference. In addition, we model implicit uncertainty through an expansion framework that systematically adds characteristic sub-findings derived from expert-defined diagnostic pathways for 14 common diagnoses. Using these methods, we release Lunguage++, an expanded, uncertainty-aware version of the Lunguage benchmark of fine-grained structured radiology reports. This enriched resource enables uncertainty-aware image classification, faithful diagnostic reasoning, and new investigations into the clinical impact of diagnostic uncertainty.",
    "summary": "",
    "translation": "放射学报告中的临床不确定性建模：从显式不确定性标记到隐式推理路径",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学放射学领域的临床不确定性建模，属于明确的医学领域应用。论文内容涉及医学报告分析和不确定性处理，与推荐系统、搜索或广告的核心技术领域完全无关，也不涉及LLM、Transformer架构或异构数据建模的相关技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04502v1": {
    "title": "RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG",
    "url": "https://www.alphaxiv.org/abs/2511.04502v1",
    "arxiv_id": "2511.04502v1",
    "authors": "Joshua Gao, Quoc Huy Pham, Subin Varghese, Silwal Saurav, Vedhus Hoskere",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-11-06 16:22:52",
    "ori_summary": "Retrieval-Augmented Generation (RAG) is a critical technique for grounding Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in specialized, safety-critical domains remains a significant challenge. Existing evaluation frameworks often rely on heuristic-based metrics that fail to capture domain-specific nuances and other works utilize LLM-as-a-Judge approaches that lack validated alignment with human judgment. This paper introduces RAGalyst, an automated, human-aligned agentic framework designed for the rigorous evaluation of domain-specific RAG systems. RAGalyst features an agentic pipeline that generates high-quality, synthetic question-answering (QA) datasets from source documents, incorporating an agentic filtering step to ensure data fidelity. The framework refines two key LLM-as-a-Judge metrics-Answer Correctness and Answerability-using prompt optimization to achieve a strong correlation with human annotations. Applying this framework to evaluate various RAG components across three distinct domains (military operations, cybersecurity, and bridge engineering), we find that performance is highly context-dependent. No single embedding model, LLM, or hyperparameter configuration proves universally optimal. Additionally, we provide an analysis on the most common low Answer Correctness reasons in RAG. These findings highlight the necessity of a systematic evaluation framework like RAGalyst, which empowers practitioners to uncover domain-specific trade-offs and make informed design choices for building reliable and effective RAG systems. RAGalyst is available on our Github.",
    "summary": "",
    "translation": "RAGalyst：面向领域特定检索增强生成的自动化人机对齐智能体评估",
    "relevance_score": 3,
    "reasoning": "该论文主要关注RAG系统的自动化评估方法，属于评估基准范畴，这在无关主题中被明确排除。虽然RAG技术本身在搜索系统中有关联，但论文聚焦于评估而非核心算法进步或直接应用，对推荐系统、搜索或广告的核心技术发展贡献有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04500v1": {
    "title": "Large language models replicate and predict human cooperation across experiments in game theory",
    "url": "https://www.alphaxiv.org/abs/2511.04500v1",
    "arxiv_id": "2511.04500v1",
    "authors": "Andrea Cera Palatsi, Samuel Martin-Gutierrez, Ana S. Cardenal, Max Pellert",
    "categories": "cs.AI, cs.CL, cs.GT, cs.MA",
    "pub_date": "2025-11-06 16:21:27",
    "ori_summary": "Large language models (LLMs) are increasingly used both to make decisions in domains such as health, education and law, and to simulate human behavior. Yet how closely LLMs mirror actual human decision-making remains poorly understood. This gap is critical: misalignment could produce harmful outcomes in practical applications, while failure to replicate human behavior renders LLMs ineffective for social simulations. Here, we address this gap by developing a digital twin of game-theoretic experiments and introducing a systematic prompting and probing framework for machine-behavioral evaluation. Testing three open-source models (Llama, Mistral and Qwen), we find that Llama reproduces human cooperation patterns with high fidelity, capturing human deviations from rational choice theory, while Qwen aligns closely with Nash equilibrium predictions. Notably, we achieved population-level behavioral replication without persona-based prompting, simplifying the simulation process. Extending beyond the original human-tested games, we generate and preregister testable hypotheses for novel game configurations outside the original parameter grid. Our findings demonstrate that appropriately calibrated LLMs can replicate aggregate human behavioral patterns and enable systematic exploration of unexplored experimental spaces, offering a complementary approach to traditional research in the social and behavioral sciences that generates new empirical predictions about human social decision-making.",
    "summary": "",
    "translation": "大型语言模型在博弈论实验中复现并预测人类合作行为",
    "relevance_score": 2,
    "reasoning": "该论文研究LLM在博弈论中的人类合作行为预测，属于基础行为科学领域。虽然涉及LLM能力评估，但缺乏与推荐系统、搜索或广告的直接技术关联，且更偏向社会科学应用而非实际系统改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04499v1": {
    "title": "Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering",
    "url": "https://www.alphaxiv.org/abs/2511.04499v1",
    "arxiv_id": "2511.04499v1",
    "authors": "Christos-Nikolaos Zacharopoulos, Revekka Kyriakoglou",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-11-06 16:20:52",
    "ori_summary": "As Large Language Models (LLMs) become integral to human-centered applications, understanding their personality-like behaviors is increasingly important for responsible development and deployment. This paper systematically evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to assess trait expressions under varying sampling temperatures. We find significant differences across four of the five personality dimensions, with Neuroticism and Extraversion susceptible to temperature adjustments. Further, hierarchical clustering reveals distinct model clusters, suggesting that architectural features may predispose certain models toward stable trait profiles. Taken together, these results offer new insights into the emergence of personality-like patterns in LLMs and provide a new perspective on model tuning, selection, and the ethical governance of AI systems. We share the data and code for this analysis here: https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1",
    "summary": "",
    "translation": "解码大型语言模型中涌现的大五人格特质：温度依赖性表达与架构聚类",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLM中的人格特质涌现现象，属于LLM行为分析范畴，与推荐系统、搜索或广告的核心技术进展没有直接关联。虽然涉及LLM架构分析，但缺乏明确的RecSys/Search/Ads应用场景，更多属于基础LLM行为研究而非实际应用技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04495v1": {
    "title": "OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation",
    "url": "https://www.alphaxiv.org/abs/2511.04495v1",
    "arxiv_id": "2511.04495v1",
    "authors": "Cuong Huynh, Jie Cao",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-11-06 16:16:32",
    "ori_summary": "This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task (Alva-Manchego et al., 2025), designed for readability-controlled text simplification using LLM-prompting-based generation. Based on the analysis of prompt-based text simplification methods, we discovered an interesting finding that text simplification performance is highly related to the gap between the source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by this finding, we propose two multi-round simplification methods and generate them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams. Later improvements with MRS-Joint show that taking the LLM simplified candidates as the starting point could further boost the multi-round simplification performance.",
    "summary": "",
    "translation": "OUNLP 在 TSAR 2025 共享任务中的方法：通过代码生成实现多轮文本简化器",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本简化任务，属于纯粹的NLP应用领域，与推荐系统、搜索或广告的核心技术无关。论文涉及代码生成和多轮简化，这些内容在指定的无关主题列表中明确排除（如纯粹NLP中心主题、内容生成等），没有任何潜在的应用于RecSys/Search/Ads的关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04479v1": {
    "title": "ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai",
    "url": "https://www.alphaxiv.org/abs/2511.04479v1",
    "arxiv_id": "2511.04479v1",
    "authors": "Surapon Nonesung, Teetouch Jaknamon, Sirinya Chaiophat, Natapong Nitarach, Chanakan Wittayasakpan, Warit Sirichotedumrong, Adisai Na-Thalang, Kunat Pipatanakul",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 15:57:39",
    "ori_summary": "We present ThaiOCRBench, the first comprehensive benchmark for evaluating vision-language models (VLMs) on Thai text-rich visual understanding tasks. Despite recent progress in multimodal modeling, existing benchmarks predominantly focus on high-resource languages, leaving Thai underrepresented, especially in tasks requiring document structure understanding. ThaiOCRBench addresses this gap by offering a diverse, human-annotated dataset comprising 2,808 samples across 13 task categories. We evaluate a wide range of state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and open-source systems. Results show a significant performance gap, with proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source counterparts. Notably, fine-grained text recognition and handwritten content extraction exhibit the steepest performance drops among open-source models. Through detailed error analysis, we identify key challenges such as language bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a standardized framework for assessing VLMs in low-resource, script-complex settings, and provides actionable insights for improving Thai-language document understanding.",
    "summary": "",
    "translation": "ThaiOCRBench：一个面向泰语视觉语言理解的任务多样化基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于泰语OCR基准测试，属于特定语言的视觉文本识别评估，与推荐系统、搜索或广告的核心技术进展无关。虽然涉及视觉语言理解，但仅限于泰语这一特定语言场景，没有展示出在异构数据处理或跨模态建模方面的通用技术创新，无法应用于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04476v1": {
    "title": "Probabilistic Textual Time Series Depression Detection",
    "url": "https://www.alphaxiv.org/abs/2511.04476v1",
    "arxiv_id": "2511.04476v1",
    "authors": "Fabian Schmidt, Seyedehmoniba Ravan, Vladimir Vlassov",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 15:50:33",
    "ori_summary": "Accurate and interpretable predictions of depression severity are essential for clinical decision support, yet existing models often lack uncertainty estimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time Series Depression Detection framework that predicts PHQ-8 scores from utterance-level clinical interviews while modeling uncertainty over time. PTTSD includes sequence-to-sequence and sequence-to-one variants, both combining bidirectional LSTMs, self-attention, and residual connections with Gaussian or Student-t output heads trained via negative log-likelihood. Evaluated on E-DAIC and DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only systems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated prediction intervals. Ablations confirm the value of attention and probabilistic modeling, while comparisons with MentalBERT establish generality. A three-part calibration analysis and qualitative case studies further highlight the interpretability and clinical relevance of uncertainty-aware forecasting.",
    "summary": "",
    "translation": "基于概率文本时间序列的抑郁症检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域的抑郁症检测，属于明确的医学应用范畴，与推荐系统、搜索或广告技术完全无关。论文涉及文本时间序列分析，但应用场景是心理健康诊断，不在当前关注的技术领域范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04432v1": {
    "title": "If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs",
    "url": "https://www.alphaxiv.org/abs/2511.04432v1",
    "arxiv_id": "2511.04432v1",
    "authors": "Lars Bungum, Charles Yijia Huang, Abeer Kashar",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 15:06:22",
    "ori_summary": "In this study, we experiment with the ability of LLMs to do temporal reasoning. Using a Norwegian book from 1940 containing trivia questions, we prompt the LLMs to answer the questions as if it were 1940. We also pose the questions in both English and Norwegian. Correct answers are often presented as sentences, and grading is done by means of LLM-as-judge, with sampled checks by a native speaker. Prompting in English consistently gave better results than in Norwegian, an unexpected result. In contrast, using larger LLMs improved results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families, and also the largest available LLM especially crafted for Norwegian.",
    "summary": "",
    "translation": "如果我能让时光倒流：时间重构作为大语言模型的历史推理任务",
    "relevance_score": 2,
    "reasoning": "该论文主要关注大语言模型在历史推理任务中的表现，属于纯粹的NLP应用研究。虽然涉及时间序列推理，但缺乏与推荐系统、搜索或广告领域的明确连接点，也没有展示在异构数据处理或Transformer架构改进方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04418v1": {
    "title": "The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity",
    "url": "https://www.alphaxiv.org/abs/2511.04418v1",
    "arxiv_id": "2511.04418v1",
    "authors": "Tim Tomov, Dominik Fuchsgruber, Tom Wollschläger, Stephan Günnemann",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-11-06 14:46:35",
    "ori_summary": "Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is critical for trustworthy deployment. While real-world language is inherently ambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically benchmarked against tasks with no ambiguity. In this work, we demonstrate that while current uncertainty estimators perform well under the restrictive assumption of no ambiguity, they degrade to close-to-random performance on ambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first ambiguous question-answering (QA) datasets equipped with ground-truth answer distributions estimated from factual co-occurrence. We find this performance deterioration to be consistent across different estimation paradigms: using the predictive distribution itself, internal representations throughout the model, and an ensemble of models. We show that this phenomenon can be theoretically explained, revealing that predictive-distribution and ensemble-based estimators are fundamentally limited under ambiguity. Overall, our study reveals a key shortcoming of current UQ methods for LLMs and motivates a rethinking of current modeling paradigms.",
    "summary": "",
    "translation": "确定性的幻觉：在模糊性下LLM的不确定性量化失效",
    "relevance_score": 2,
    "reasoning": "该论文聚焦于LLM不确定性量化的技术局限，属于LLM评估范畴，与用户关注的RecSys/Search/Ads核心进展或LLM/Transformer使能技术关联较弱。虽然不确定性量化在推荐系统中对置信度校准有潜在价值，但论文标题强调'失效'问题，更偏向纯NLP评估而非面向应用的使能技术突破。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04406v1": {
    "title": "Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning",
    "url": "https://www.alphaxiv.org/abs/2511.04406v1",
    "arxiv_id": "2511.04406v1",
    "authors": "Mohammad Amin Ghanizadeh, Mohammad Javad Dousti",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 14:33:29",
    "ori_summary": "Data quality and its effective selection are fundamental to improving the performance of machine translation models, serving as cornerstones for achieving robust and reliable translation systems. This paper presents a data selection methodology specifically designed for fine-tuning machine translation systems, which leverages the synergy between a learner model and a pre-trained reference model to enhance overall training effectiveness. By defining a learnability score, our approach systematically evaluates the utility of data points for training, ensuring that only the most relevant and impactful examples contribute to the fine-tuning process. Furthermore, our method employs a batch selection strategy which considers interdependencies among data points, optimizing the efficiency of the training process while maintaining a focus on data relevance. Experiments on English to Persian and several other language pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that our method can achieve up to a fivefold improvement in data efficiency compared to an iid baseline. Experimental results indicate that our approach improves computational efficiency by 24 when utilizing cached embeddings, as it requires fewer training data points. Additionally, it enhances generalization, resulting in superior translation performance compared to random selection method.",
    "summary": "",
    "translation": "面向数据高效机器翻译微调的动态联合批次选择",
    "relevance_score": 2,
    "reasoning": "该论文主要关注机器翻译领域的批次选择和数据效率优化，属于特定NLP任务的技术改进。虽然批次选择技术可能间接影响训练效率，但其核心应用场景是机器翻译而非推荐系统、搜索或广告领域，且未涉及Transformer架构创新或LLM技术在RecSys/Search/Ads中的直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04256v1": {
    "title": "SSPO: Subsentence-level Policy Optimization",
    "url": "https://www.alphaxiv.org/abs/2511.04256v1",
    "arxiv_id": "2511.04256v1",
    "authors": "Kun Yang, Zikang chen, Yanmeng Wang, Zhigen Li",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 10:48:31",
    "ori_summary": "As a significant part of post-training of the Large Language Models (LLMs), Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs' reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative Policy Optimization) and GSPO (Group Sequence Policy Optimization), are observed to suffer from unstable policy updates and low usage of sampling data, respectively. The importance ratio of GRPO is calculated at the token level, which focuses more on optimizing a single token. This will be easily affected by outliers, leading to model training collapse. GSPO proposed the calculation of the response level importance ratio, which solves the problem of high variance and training noise accumulation in the calculation of the GRPO importance ratio. However, since all the response tokens share a common importance ratio, extreme values can easily raise or lower the overall mean, leading to the entire response being mistakenly discarded, resulting in a decrease in the utilization of sampled data. This paper introduces SSPO, which applies sentence-level importance ratio, taking the balance between GRPO and GSPO. SSPO not only avoids training collapse and high variance, but also prevents the whole response tokens from being abandoned by the clipping mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily adjust the clipping bounds, encouraging high-entropy tokens to explore and narrow the clipping range of low-entropy tokens. In particular, SSPO achieves an average score of 46.57 across five datasets, surpassing GRPO (43.01) and GSPO (44.42), and wins state-of-the-art performance on three datasets. These results highlight SSPO's effectiveness in leveraging generated data by taking the essence of GSPO but rejecting its shortcomings.",
    "summary": "",
    "translation": "SSPO：子句级策略优化",
    "relevance_score": 2,
    "reasoning": "该论文标题表明其专注于强化学习中的策略优化技术，特别是子句级别的优化方法。虽然策略优化在推荐系统和广告中可能有间接应用，但该标题没有明确指向推荐、搜索或广告领域，也没有涉及LLM、Transformer或异构数据建模等核心技术。其潜在应用过于间接和推测性，无法满足当前关注点的具体要求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04248v1": {
    "title": "Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models",
    "url": "https://www.alphaxiv.org/abs/2511.04248v1",
    "arxiv_id": "2511.04248v1",
    "authors": "Salma Mekaooui, Hiba Sofyan, Imane Amaaz, Imane Benchrif, Arsalane Zarghili, Ilham Chaker, Nikola S. Nikolov",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 10:34:35",
    "ori_summary": "Extracting topics from text has become an essential task, especially with the rapid growth of unstructured textual data. Most existing works rely on highly computational methods to address this challenge. In this paper, we argue that probabilistic and statistical approaches, such as topic modeling (TM), can offer effective alternatives that require fewer computational resources. TM is a statistical method that automatically discovers topics in large collections of unlabeled text; however, it produces topics as distributions of representative words, which often lack clear interpretability. Our objective is to perform topic labeling by assigning meaningful labels to these sets of words. To achieve this without relying on computationally expensive models, we propose a graph-based approach that not only enriches topic words with semantically related terms but also explores the relationships among them. By analyzing these connections within the graph, we derive suitable labels that accurately capture each topic's meaning. We present a comparative study between our proposed method and several benchmarks, including ChatGPT-3.5, across two different datasets. Our method achieved consistently better results than traditional benchmarks in terms of BERTScore and cosine similarity and produced results comparable to ChatGPT-3.5, while remaining computationally efficient. Finally, we discuss future directions for topic labeling and highlight potential research avenues for enhancing interpretability and automation.",
    "summary": "该论文研究主题建模中主题标签分配问题，核心思想是利用图结构分析主题词之间的关系，通过语义关联和连接分析来生成准确的主题标签，避免依赖计算密集型模型。",
    "translation": "基于图标注的高效主题提取：深度模型的轻量级替代方案",
    "relevance_score": 4,
    "reasoning": "该论文提出了一种轻量级的图基标注方法用于主题提取，可作为深度模型的替代方案。虽然主题提取在搜索和推荐系统中具有潜在应用价值（如内容理解和分类），但该方法主要聚焦于传统图算法而非Transformer架构或LLM技术，与当前关注的核心领域进展关联度有限。",
    "rerank_relevance_score": 4,
    "rerank_reasoning": "该论文提出了一种基于图的高效主题标签方法，虽然不直接涉及推荐系统或广告，但其轻量级图处理方法对需要高效文本理解的应用具有潜在参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.04234v1": {
    "title": "Reusing Pre-Training Data at Test Time is a Compute Multiplier",
    "url": "https://www.alphaxiv.org/abs/2511.04234v1",
    "arxiv_id": "2511.04234v1",
    "authors": "Alex Fang, Thomas Voice, Ruoming Pang, Ludwig Schmidt, Tom Gunter",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 10:10:43",
    "ori_summary": "Large language models learn from their vast pre-training corpora, gaining the ability to solve an ever increasing variety of tasks; yet although researchers work to improve these datasets, there is little effort to understand how efficient the pre-training apparatus is at extracting ideas and knowledge from the data. In this work, we use retrieval augmented generation along with test-time compute as a way to quantify how much dataset value was left behind by the process of pre-training, and how this changes across scale. We demonstrate that pre-training then retrieving from standard and largely open-sourced datasets results in significant accuracy gains in MMLU, Math-500, and SimpleQA, which persist through decontamination. For MMLU we observe that retrieval acts as a ~5x compute multiplier versus pre-training alone. We show that these results can be further improved by leveraging additional compute at test time to parse the retrieved context, demonstrating a 10 percentage point improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results suggest that today's pre-training methods do not make full use of the information in existing pre-training datasets, leaving significant room for progress.",
    "summary": "论文研究预训练方法从数据中提取知识的效率问题，核心发现是通过检索增强生成和测试时计算可以量化预训练过程中遗留的数据价值，表明现有预训练方法未能充分利用预训练数据集中的信息。",
    "translation": "在测试时复用预训练数据是一种计算倍增器",
    "relevance_score": 8,
    "reasoning": "这篇论文探讨预训练数据在推理阶段的有效复用，这直接属于'Enabling LLM Tech'范畴。通过减少对大规模新数据的依赖，该方法可以显著降低推荐系统和搜索系统中LLM部署的计算成本，同时保持模型性能，对于大规模工业应用具有重要价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接研究预训练数据利用效率问题，揭示了当前预训练方法未能充分利用数据价值，这对LLM在搜索推荐系统的应用效率有重要启示。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.04228v1": {
    "title": "REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs",
    "url": "https://www.alphaxiv.org/abs/2511.04228v1",
    "arxiv_id": "2511.04228v1",
    "authors": "Liran Cohen, Yaniv Nemcovesky, Avi Mendelson",
    "categories": "cs.CL, cs.LG, I.2.7; I.2.6; K.4.1",
    "pub_date": "2025-11-06 09:58:19",
    "ori_summary": "Machine unlearning aims to remove the influence of specific training data from a model without requiring full retraining. This capability is crucial for ensuring privacy, safety, and regulatory compliance. Therefore, verifying whether a model has truly forgotten target data is essential for maintaining reliability and trustworthiness. However, existing evaluation methods often assess forgetting at the level of individual inputs. This approach may overlook residual influence present in semantically similar examples. Such influence can compromise privacy and lead to indirect information leakage. We propose REMIND (Residual Memorization In Neighborhood Dynamics), a novel evaluation method aiming to detect the subtle remaining influence of unlearned data and classify whether the data has been effectively forgotten. REMIND analyzes the model's loss over small input variations and reveals patterns unnoticed by single-point evaluations. We show that unlearned data yield flatter, less steep loss landscapes, while retained or unrelated data exhibit sharper, more volatile patterns. REMIND requires only query-based access, outperforms existing methods under similar constraints, and demonstrates robustness across different models, datasets, and paraphrased inputs, making it practical for real-world deployment. By providing a more sensitive and interpretable measure of unlearning effectiveness, REMIND provides a reliable framework to assess unlearning in language models. As a result, REMIND offers a novel perspective on memorization and unlearning.",
    "summary": "",
    "translation": "REMIND：输入损失景观揭示后遗忘大语言模型中的残余记忆",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM遗忘后的残余记忆问题，这属于隐私和安全范畴，属于明确排除的无关主题。虽然涉及LLM技术，但其核心关注点（记忆检测和遗忘验证）与推荐系统、搜索或广告的核心技术进展没有直接关联，也没有展示在这些领域的具体应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04215v1": {
    "title": "Black-Box Guardrail Reverse-engineering Attack",
    "url": "https://www.alphaxiv.org/abs/2511.04215v1",
    "arxiv_id": "2511.04215v1",
    "authors": "Hongwei Yao, Yun Xia, Shuo Shao, Haoran Shi, Tong Qiao, Cong Wang",
    "categories": "cs.CR, cs.CL",
    "pub_date": "2025-11-06 09:24:49",
    "ori_summary": "Large language models (LLMs) increasingly employ guardrails to enforce ethical, legal, and application-specific constraints on their outputs. While effective at mitigating harmful responses, these guardrails introduce a new class of vulnerabilities by exposing observable decision patterns. In this work, we present the first study of black-box LLM guardrail reverse-engineering attacks. We propose Guardrail Reverse-engineering Attack (GRA), a reinforcement learning-based framework that leverages genetic algorithm-driven data augmentation to approximate the decision-making policy of victim guardrails. By iteratively collecting input-output pairs, prioritizing divergence cases, and applying targeted mutations and crossovers, our method incrementally converges toward a high-fidelity surrogate of the victim guardrail. We evaluate GRA on three widely deployed commercial systems, namely ChatGPT, DeepSeek, and Qwen3, and demonstrate that it achieves an rule matching rate exceeding 0.92 while requiring less than $85 in API costs. These findings underscore the practical feasibility of guardrail extraction and highlight significant security risks for current LLM safety mechanisms. Our findings expose critical vulnerabilities in current guardrail designs and highlight the urgent need for more robust defense mechanisms in LLM deployment.",
    "summary": "",
    "translation": "黑盒护栏逆向工程攻击",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及安全攻击和逆向工程，属于明确排除的隐私和安全相关主题。虽然护栏技术可能与LLM部署相关，但逆向工程攻击的焦点完全落在安全领域，与推荐系统、搜索或广告的核心技术进展无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04214v1": {
    "title": "Block Rotation is All You Need for MXFP4 Quantization",
    "url": "https://www.alphaxiv.org/abs/2511.04214v1",
    "arxiv_id": "2511.04214v1",
    "authors": "Yuantian Shao, Peisong Wang, Yuanteng Chen, Chang Xu, Zhihui Wei, Jian Cheng",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-11-06 09:22:31",
    "ori_summary": "Large language models (LLMs) have achieved remarkable success, but their rapidly growing scale imposes prohibitive costs in memory, computation, and energy. Post-training quantization (PTQ) is a promising solution for efficient deployment, yet achieving accurate W4A4 quantization remains an open challenge. While most existing methods are designed for INT4 formats, the emergence of MXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)-- raises questions about the applicability of current techniques. In this work, we establish a comprehensive benchmark of PTQ methods under the MXFP4 format. Through systematic evaluation, we find that methods like GPTQ consistently deliver strong performance, whereas rotation-based approaches, which are almost used by all state-of-the-art approaches, suffer from severe incompatibility with MXFP4. We further provide the first in-depth analysis of this conflict, tracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two) block scaling and the redistribution of outlier energy via global rotation. Building on this insight, we propose a simple yet effective block rotation strategy that adapts rotation-based methods to MXFP4, leading to substantial accuracy improvements across diverse LLMs. Our findings not only offer clear guidance for practitioners but also set a foundation for advancing PTQ research under emerging low-precision formats.",
    "summary": "论文研究MXFP4格式下LLM后训练量化的准确性问题，核心发现是全局旋转方法与MXFP4的幂次块缩放存在根本冲突，并提出块旋转策略来适配旋转基方法到MXFP4格式。",
    "translation": "块旋转是MXFP4量化的全部所需",
    "relevance_score": 8,
    "reasoning": "该论文涉及MXFP4量化技术，这是LLM推理效率的核心推动因素，属于'使能LLM技术'类别。4位量化可以显著降低LLM的内存占用和计算成本，这对于在推荐和搜索系统中部署大型语言模型至关重要，能够实现更高效的实时推理和更大规模的模型部署。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对LLM量化部署的核心挑战，提出的块旋转策略属于Transformer架构效率优化的重要进展，对搜索推荐系统的模型压缩有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.04205v1": {
    "title": "LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal",
    "url": "https://www.alphaxiv.org/abs/2511.04205v1",
    "arxiv_id": "2511.04205v1",
    "authors": "Michał Karp, Anna Kubaszewska, Magdalena Król, Robert Król, Aleksander Smywiński-Pohl, Mateusz Szymański, Witold Wydmański",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 09:11:20",
    "ori_summary": "This study provides an empirical assessment of whether current large language models (LLMs) can pass the official qualifying examination for membership in Poland's National Appeal Chamber (Krajowa Izba Odwo{\\l}awcza). The authors examine two related ideas: using LLM as actual exam candidates and applying the 'LLM-as-a-judge' approach, in which model-generated answers are automatically evaluated by other models. The paper describes the structure of the exam, which includes a multiple-choice knowledge test on public procurement law and a written judgment, and presents the hybrid information recovery and extraction pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4 Sonnet and Bielik-11B-v2.6) were tested in closed-book and various Retrieval-Augmented Generation settings. The results show that although the models achieved satisfactory scores in the knowledge test, none met the passing threshold in the practical written part, and the evaluations of the 'LLM-as-a-judge' often diverged from the judgments of the official examining committee. The authors highlight key limitations: susceptibility to hallucinations, incorrect citation of legal provisions, weaknesses in logical argumentation, and the need for close collaboration between legal experts and technical teams. The findings indicate that, despite rapid technological progress, current LLMs cannot yet replace human judges or independent examiners in Polish public procurement adjudication.",
    "summary": "",
    "translation": "基于AI尝试波兰国家上诉委员会成员资格考试的分析：LLM作为评判者表现不佳",
    "relevance_score": 1,
    "reasoning": "该论文主要关注LLM在特定资格考试中的评判能力评估，这属于纯粹的LLM评估和基准测试范畴。论文内容与推荐系统、搜索或广告的核心技术进展、架构改进或实际应用没有直接关联，也不涉及Transformer架构效率、多模态建模或其他相关技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04195v1": {
    "title": "Computational Turing Test Reveals Systematic Differences Between Human and AI Language",
    "url": "https://www.alphaxiv.org/abs/2511.04195v1",
    "arxiv_id": "2511.04195v1",
    "authors": "Nicolò Pagan, Petter Törnberg, Christopher A. Bail, Anikó Hannák, Christopher Barrie",
    "categories": "cs.CL, cs.MA, cs.SI",
    "pub_date": "2025-11-06 08:56:37",
    "ori_summary": "Large language models (LLMs) are increasingly used in the social sciences to simulate human behavior, based on the assumption that they can generate realistic, human-like text. Yet this assumption remains largely untested. Existing validation efforts rely heavily on human-judgment-based evaluations -- testing whether humans can distinguish AI from human output -- despite evidence that such judgments are blunt and unreliable. As a result, the field lacks robust tools for assessing the realism of LLM-generated text or for calibrating models to real-world data. This paper makes two contributions. First, we introduce a computational Turing test: a validation framework that integrates aggregate metrics (BERT-based detectability and semantic similarity) with interpretable linguistic features (stylistic markers and topical patterns) to assess how closely LLMs approximate human language within a given dataset. Second, we systematically compare nine open-weight LLMs across five calibration strategies -- including fine-tuning, stylistic prompting, and context retrieval -- benchmarking their ability to reproduce user interactions on X (formerly Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the literature. Even after calibration, LLM outputs remain clearly distinguishable from human text, particularly in affective tone and emotional expression. Instruction-tuned models underperform their base counterparts, and scaling up model size does not enhance human-likeness. Crucially, we identify a trade-off: optimizing for human-likeness often comes at the cost of semantic fidelity, and vice versa. These results provide a much-needed scalable framework for validation and calibration in LLM simulations -- and offer a cautionary note about their current limitations in capturing human communication.",
    "summary": "",
    "translation": "计算图灵测试揭示人类与AI语言的系统性差异",
    "relevance_score": 2,
    "reasoning": "该论文主要关注AI语言与人类语言的差异检测和评估，这属于纯粹的NLP评估基准范畴。虽然可能涉及LLM分析，但论文焦点是语言差异检测而非在推荐系统、搜索或广告中的实际应用，与当前关注的技术进展和应用方向关联度很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04184v1": {
    "title": "Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains",
    "url": "https://www.alphaxiv.org/abs/2511.04184v1",
    "arxiv_id": "2511.04184v1",
    "authors": "Mohammed Musthafa Rafi, Adarsh Krishnamurthy, Aditya Balu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-11-06 08:36:42",
    "ori_summary": "The proliferation of AI-generated content has created an absurd communication theater where senders use LLMs to inflate simple ideas into verbose content, recipients use LLMs to compress them back into summaries, and as a consequence neither party engage with authentic content. LAAC (LLM as a Communicator) proposes a paradigm shift - positioning LLMs as intelligent communication intermediaries that capture the sender's intent through structured dialogue and facilitate genuine knowledge exchange with recipients. Rather than perpetuating cycles of AI-generated inflation and compression, LAAC enables authentic communication across diverse contexts including academic papers, proposals, professional emails, and cross-platform content generation. However, deploying LLMs as trusted communication intermediaries raises critical questions about information fidelity, consistency, and reliability. This position paper systematically evaluates the trustworthiness requirements for LAAC's deployment across multiple communication domains. We investigate three fundamental dimensions: (1) Information Capture Fidelity - accuracy of intent extraction during sender interviews across different communication types, (2) Reproducibility - consistency of structured knowledge across multiple interaction instances, and (3) Query Response Integrity - reliability of recipient-facing responses without hallucination, source conflation, or fabrication. Through controlled experiments spanning multiple LAAC use cases, we assess these trust dimensions using LAAC's multi-agent architecture. Preliminary findings reveal measurable trust gaps that must be addressed before LAAC can be reliably deployed in high-stakes communication scenarios.",
    "summary": "",
    "translation": "可信赖的LLM中介通信：在多应用领域中评估LLM作为通信者（LAAC）框架中的信息保真度",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM作为通信中介时的信息保真度和可信赖性评估，这属于LLM可靠性和通信质量的研究范畴。虽然标题提到多应用领域，但核心焦点是通信框架的信息保真度评估，而非直接应用于推荐系统、搜索或广告的排名、匹配或建模技术。该研究更偏向于LLM可靠性评估，而非能够直接应用于RecSys/Search/Ads的核心技术进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04161v1": {
    "title": "Seeing Straight: Document Orientation Detection for Efficient OCR",
    "url": "https://www.alphaxiv.org/abs/2511.04161v1",
    "arxiv_id": "2511.04161v1",
    "authors": "Suranjan Goswami, Abhinav Ravi, Raja Kolla, Ali Faraz, Shaharukh Khan, Akash, Chandra Khatri, Shubham Agarwal",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-11-06 08:04:57",
    "ori_summary": "Despite significant advances in document understanding, determining the correct orientation of scanned or photographed documents remains a critical pre-processing step in the real world settings. Accurate rotation correction is essential for enhancing the performance of downstream tasks such as Optical Character Recognition (OCR) where misalignment commonly arises due to user errors, particularly incorrect base orientations of the camera during capture. In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from rotation-transformed structured and free-form English OCR datasets, and (ii) ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource languages. We also present a fast, robust and lightweight rotation classification pipeline built on the vision encoder of Phi-3.5-Vision model with dynamic image cropping, fine-tuned specifically for 4-class rotation task in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy on identifying the rotations respectively on both the datasets. Beyond classification, we demonstrate the critical role of our module in boosting OCR performance: closed-source (up to 14%) and open-weights models (up to 4x) in the simulated real-world setting.",
    "summary": "",
    "translation": "清晰识别：面向高效OCR的文档方向检测",
    "relevance_score": 2,
    "reasoning": "该论文专注于文档方向检测以优化OCR效率，属于计算机视觉预处理技术。虽然OCR在搜索系统中可能用于文档内容提取，但这属于底层视觉处理而非核心推荐/搜索/广告系统的算法创新。该技术缺乏与LLM、Transformer架构或异构数据建模的直接关联，应用场景较为局限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04153v1": {
    "title": "BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation",
    "url": "https://www.alphaxiv.org/abs/2511.04153v1",
    "arxiv_id": "2511.04153v1",
    "authors": "Fahim Ahmed, Md Mubtasim Ahasan, Jahir Sadik Monon, Muntasir Wahed, M Ashraful Amin, A K M Mahbubur Rahman, Amin Ahsan Ali",
    "categories": "cs.CL, cs.AI, cs.DB, cs.MA",
    "pub_date": "2025-11-06 08:00:15",
    "ori_summary": "Text-to-SQL systems provide a natural language interface that can enable even laymen to access information stored in databases. However, existing Large Language Models (LLM) struggle with SQL generation from natural instructions due to large schema sizes and complex reasoning. Prior work often focuses on complex, somewhat impractical pipelines using flagship models, while smaller, efficient models remain overlooked. In this work, we explore three multi-agent LLM pipelines, with systematic performance benchmarking across a range of small to large open-source models: (1) Multi-agent discussion pipeline, where agents iteratively critique and refine SQL queries, and a judge synthesizes the final answer; (2) Planner-Coder pipeline, where a thinking model planner generates stepwise SQL generation plans and a coder synthesizes queries; and (3) Coder-Aggregator pipeline, where multiple coders independently generate SQL queries, and a reasoning agent selects the best query. Experiments on the Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small model performance, with up to 10.6% increase in Execution Accuracy for Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines, the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest score of 56.4%. Codes are available at https://github.com/treeDweller98/bappa-sql.",
    "summary": "",
    "translation": "BAPPA：用于自动化文本到SQL生成的智能体、规划与流水线基准测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文本到SQL的基准测试和自动化流程，属于数据库查询领域的特定应用。虽然文本到SQL技术理论上可以应用于搜索系统的查询理解，但论文焦点是基准测试和自动化流程评估，而非核心推荐系统、搜索或广告技术的直接进展。缺乏明确的Transformer架构创新或LLM技术在推荐/搜索领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04139v1": {
    "title": "CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese",
    "url": "https://www.alphaxiv.org/abs/2511.04139v1",
    "arxiv_id": "2511.04139v1",
    "authors": "Dazhong Chen, Yi-Cheng Lin, Yuchen Huang, Ziwei Gong, Di Jiang, Zeying Xie, Yi R., Fung",
    "categories": "cs.CL, cs.SD",
    "pub_date": "2025-11-06 07:36:24",
    "ori_summary": "Automatic speech recognition (ASR) is critical for language accessibility, yet low-resource Cantonese remains challenging due to limited annotated data, six lexical tones, tone sandhi, and accent variation. Existing ASR models, such as Whisper, often suffer from high word error rates. Large audio-language models (LALMs), in contrast, can leverage broader contextual reasoning but still require explicit tonal and prosodic acoustic cues. We introduce CantoASR, a collaborative ASR-LALM error correction framework that integrates forced alignment for acoustic feature extraction, a LoRA-finetuned Whisper for improved tone discrimination, and an instruction-tuned Qwen-Audio for prosody-aware correction. Evaluations on spontaneous Cantonese data show substantial CER gains over Whisper-Large-V3. These findings suggest that integrating acoustic cues with LALM reasoning provides a scalable strategy for low-resource tonal and dialectal ASR.",
    "summary": "",
    "translation": "CantoASR：面向低资源粤语的韵律感知ASR-LALM协同方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于粤语语音识别（ASR）和语言模型（LALM）的协同，属于语音处理领域。虽然提到了低资源场景，但其核心是语音识别技术，与推荐系统、搜索或广告的排名任务没有直接关联，也不涉及Transformer架构改进或LLM在推荐/搜索中的直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04120v1": {
    "title": "RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning",
    "url": "https://www.alphaxiv.org/abs/2511.04120v1",
    "arxiv_id": "2511.04120v1",
    "authors": "Xinyuan Li, Murong Xu, Wenbiao Tao, Hanlun Zhu, Yike Zhao, Jipeng Zhang, Yunshi Lan",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 07:10:17",
    "ori_summary": "Large language models (LLMs) achieve high performance on mathematical reasoning, but these results can be inflated by training data leakage or superficial pattern matching rather than genuine reasoning. To this end, an adversarial perturbation-based evaluation is needed to measure true mathematical reasoning ability. Current rule-based perturbation methods often generate ill-posed questions and impede the systematic evaluation of question difficulty and the evolution of benchmarks. To bridge this gap, we propose RIDE, a novel adversarial question-rewriting framework that leverages Item Response Theory (IRT) to rigorously measure question difficulty and to generate intrinsically more challenging, well-posed variations of mathematical problems. We employ 35 LLMs to simulate students and build a difficulty ranker from their responses. This ranker provides a reward signal during reinforcement learning and guides a question-rewriting model to reformulate existing questions across difficulty levels. Applying RIDE to competition-level mathematical benchmarks yields perturbed versions that degrade advanced LLM performance, with experiments showing an average 21.73% drop across 26 models, thereby exposing limited robustness in mathematical reasoning and confirming the validity of our evaluation approach.",
    "summary": "",
    "translation": "RIDE：基于项目反应理论的难度演化扰动用于数学推理",
    "relevance_score": 2,
    "reasoning": "该论文专注于数学推理领域的难度建模和扰动技术，属于特定领域的NLP应用。虽然项目反应理论在传统教育评估中有应用，但该论文没有展示与推荐系统、搜索或广告的明确联系。核心方法主要针对数学问题求解，缺乏在异构数据建模或Transformer架构改进方面的通用性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04108v1": {
    "title": "Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models",
    "url": "https://www.alphaxiv.org/abs/2511.04108v1",
    "arxiv_id": "2511.04108v1",
    "authors": "Wenmo Qiu, Saurabh Srivastava",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 06:47:39",
    "ori_summary": "Recent work has explored batch prompting as a strategy to amortize inference cost in large language models (LLMs). In this paper, we show that batching offers an additional, underappreciated benefit: it regularizes model behavior during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a comprehensive study across 13 diverse benchmarks and observe that batching improves accuracy while substantially reducing reasoning token usage, often by 3x-5x. Through detailed behavioral analysis, we find that batching suppresses overthinking, reduces hedging language (e.g., repetitive self-corrections), and encourages more decisive answers. Surprisingly, we also observe emergent collective effects in batched inference: models often generalize patterns from earlier examples to solve harder ones in the same batch. These findings position batching not just as a throughput optimization, but as a powerful inference-time regularizer for more efficient and reliable LLM reasoning.",
    "summary": "",
    "translation": "批量提示抑制过度思考：约束条件下批量提示如何抑制推理模型中的过度思考",
    "relevance_score": 2,
    "reasoning": "该论文主要关注推理模型中的过度思考问题，这属于LLM推理优化的范畴。虽然批量提示技术可能对推理效率有所提升，但论文没有明确展示其在推荐系统、搜索或广告领域的直接应用潜力。该工作更偏向于纯粹的推理优化而非针对特定应用领域的创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04106v1": {
    "title": "Sub-exponential Growth in Online Word Usage: A Piecewise Power-Law Model",
    "url": "https://www.alphaxiv.org/abs/2511.04106v1",
    "arxiv_id": "2511.04106v1",
    "authors": "Hayafumi Watanabe",
    "categories": "physics.soc-ph, cs.CL, cs.CY, stat.AP",
    "pub_date": "2025-11-06 06:44:45",
    "ori_summary": "The diffusion of ideas and language in society has conventionally been described by S-shaped models, such as the logistic curve. However, the role of sub-exponential growth -a slower than exponential pattern known in epidemiology- has been largely overlooked in broader social phenomena. Here, we present a piecewise power-law model to characterize complex growth curves with a few parameters. We systematically analyzed a large-scale dataset of approximately one billion Japanese blog articles linked to Wikipedia vocabulary, and observed consistent patterns in web search trend data (English, Spanish, and Japanese). Our analysis of the 2,965 selected items reveals that about 55% (1,625 items) were found to have no abrupt jumps and were well captured by one or two segments. For single-segment curves, we found that (i) the mode of the shape parameter alpha was near 0.5, indicating prevalent sub-exponential growth; (ii) the ultimate diffusion scale is primarily determined by the growth rate R, with minor contributions from alpha or the duration T; and (iii) alpha showed a tendency to vary with the nature of the topic, being smaller for niche/local topics and larger for widely shared ones. Furthermore, a micro-behavioral model distinguishing outward contact with strangers from inward interaction within their community suggests that alpha can be interpreted as an index of the preference for outward-oriented communication. These findings suggest that sub-exponential growth is a common pattern of social diffusion, and our model provides a practical framework for consistently describing, comparing, and interpreting complex and diverse growth curves.",
    "summary": "",
    "translation": "在线词汇使用中的亚指数增长：分段幂律模型",
    "relevance_score": 1,
    "reasoning": "该论文研究在线词汇使用的增长模式，属于语言学和统计建模领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然涉及在线数据，但焦点是词汇增长规律而非用户行为建模、内容理解或排序算法，无法应用于推荐/搜索/广告系统的任何技术组件。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04103v1": {
    "title": "A Characterization of List Language Identification in the Limit",
    "url": "https://www.alphaxiv.org/abs/2511.04103v1",
    "arxiv_id": "2511.04103v1",
    "authors": "Moses Charikar, Chirag Pabbaraju, Ambuj Tewari",
    "categories": "cs.CL, cs.AI, cs.DS, cs.LG",
    "pub_date": "2025-11-06 06:39:20",
    "ori_summary": "We study the problem of language identification in the limit, where given a sequence of examples from a target language, the goal of the learner is to output a sequence of guesses for the target language such that all the guesses beyond some finite time are correct. Classical results of Gold showed that language identification in the limit is impossible for essentially any interesting collection of languages. Later, Angluin gave a precise characterization of language collections for which this task is possible. Motivated by recent positive results for the related problem of language generation, we revisit the classic language identification problem in the setting where the learner is given the additional power of producing a list of $k$ guesses at each time step. The goal is to ensure that beyond some finite time, one of the guesses is correct at each time step. We give an exact characterization of collections of languages that can be $k$-list identified in the limit, based on a recursive version of Angluin's characterization (for language identification with a list of size $1$). This further leads to a conceptually appealing characterization: A language collection can be $k$-list identified in the limit if and only if the collection can be decomposed into $k$ collections of languages, each of which can be identified in the limit (with a list of size $1$). We also use our characterization to establish rates for list identification in the statistical setting where the input is drawn as an i.i.d. stream from a distribution supported on some language in the collection. Our results show that if a collection is $k$-list identifiable in the limit, then the collection can be $k$-list identified at an exponential rate, and this is best possible. On the other hand, if a collection is not $k$-list identifiable in the limit, then it cannot be $k$-list identified at any rate that goes to zero.",
    "summary": "",
    "translation": "极限条件下列表语言识别的特征刻画",
    "relevance_score": 1,
    "reasoning": "该论文标题表明这是关于形式语言理论和计算学习理论的研究，专注于语言识别的理论极限分析。这与我的关注领域（推荐系统、搜索、广告及其相关技术）完全无关，因为它不涉及任何实际应用或与推荐系统/搜索/广告相关的技术组件。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04079v1": {
    "title": "Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods",
    "url": "https://www.alphaxiv.org/abs/2511.04079v1",
    "arxiv_id": "2511.04079v1",
    "authors": "Eva Prakash, Maayane Attias, Pierre Chambon, Justin Xu, Steven Truong, Jean-Benoit Delbrouck, Tessa Cook, Curtis Langlotz",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 05:37:26",
    "ori_summary": "Objective: To enhance automated de-identification of radiology reports by scaling transformer-based models through extensive training datasets and benchmarking performance against commercial cloud vendor systems for protected health information (PHI) detection. Materials and Methods: In this retrospective study, we built upon a state-of-the-art, transformer-based, PHI de-identification pipeline by fine-tuning on two large annotated radiology corpora from Stanford University, encompassing chest X-ray, chest CT, abdomen/pelvis CT, and brain MR reports and introducing an additional PHI category (AGE) into the architecture. Model performance was evaluated on test sets from Stanford and the University of Pennsylvania (Penn) for token-level PHI detection. We further assessed (1) the stability of synthetic PHI generation using a \"hide-in-plain-sight\" method and (2) performance against commercial systems. Precision, recall, and F1 scores were computed across all PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining the previous state-of-the-art model performance. Synthetic PHI evaluation showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50 independently de-identified Penn datasets. Our model outperformed all vendor systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754). Discussion: Large-scale, multimodal training improved cross-institutional generalization and robustness. Synthetic PHI generation preserved data utility while ensuring privacy. Conclusion: A transformer-based de-identification model trained on diverse radiology datasets outperforms prior academic and commercial systems in PHI detection and establishes a new benchmark for secure clinical text processing.",
    "summary": "",
    "translation": "通过大规模训练和与云供应商方法的基准测试提升放射学报告去标识化性能",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域的放射学报告去标识化，这属于医疗隐私保护范畴，与我的关注领域（推荐系统、搜索、广告）完全无关。论文内容涉及医疗数据处理和隐私技术，这些都属于明确排除的无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04077v1": {
    "title": "The truth is no diaper: Human and AI-generated associations to emotional words",
    "url": "https://www.alphaxiv.org/abs/2511.04077v1",
    "arxiv_id": "2511.04077v1",
    "authors": "Špela Vintar, Jan Jona Javoršek",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 05:32:04",
    "ori_summary": "Human word associations are a well-known method of gaining insight into the internal mental lexicon, but the responses spontaneously offered by human participants to word cues are not always predictable as they may be influenced by personal experience, emotions or individual cognitive styles. The ability to form associative links between seemingly unrelated concepts can be the driving mechanisms of creativity. We perform a comparison of the associative behaviour of humans compared to large language models. More specifically, we explore associations to emotionally loaded words and try to determine whether large language models generate associations in a similar way to humans. We find that the overlap between humans and LLMs is moderate, but also that the associations of LLMs tend to amplify the underlying emotional load of the stimulus, and that they tend to be more predictable and less creative than human ones.",
    "summary": "",
    "translation": "真相并非尿布：人类与AI生成对情感词汇的联想",
    "relevance_score": 1,
    "reasoning": "该论文研究人类与AI对情感词汇的联想差异，属于纯粹的心理学和NLP基础研究。论文内容聚焦于词汇联想和情感分析，与推荐系统、搜索或广告的核心技术进展、LLM架构改进或直接应用完全无关，也没有任何明显的技术迁移潜力到这些领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04072v1": {
    "title": "Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering",
    "url": "https://www.alphaxiv.org/abs/2511.04072v1",
    "arxiv_id": "2511.04072v1",
    "authors": "Xinying Qian, Ying Zhang, Yu Zhao, Baohang Zhou, Xuhui Sui, Xiaojie Yuan",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 05:24:14",
    "ori_summary": "Temporal Knowledge Graph Question Answering (TKGQA) aims to answer time-sensitive questions by leveraging factual information from Temporal Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG embeddings or graph neural networks to inject temporal knowledge, they fail to fully understand the complex semantic information of time constraints. Recently, Large Language Models (LLMs) have shown remarkable progress, benefiting from their strong semantic understanding and reasoning generalization capabilities. However, their temporal reasoning ability remains limited. LLMs frequently suffer from hallucination and a lack of knowledge. To address these limitations, we propose the Plan of Knowledge framework with a contrastive temporal retriever, which is named PoK. Specifically, the proposed Plan of Knowledge module decomposes a complex temporal question into a sequence of sub-objectives from the pre-defined tools, serving as intermediate guidance for reasoning exploration. In parallel, we construct a Temporal Knowledge Store (TKS) with a contrastive retrieval framework, enabling the model to selectively retrieve semantically and temporally aligned facts from TKGs. By combining structured planning with temporal knowledge retrieval, PoK effectively enhances the interpretability and factual consistency of temporal reasoning. Extensive experiments on four benchmark TKGQA datasets demonstrate that PoK significantly improves the retrieval precision and reasoning accuracy of LLMs, surpassing the performance of the state-of-the-art TKGQA methods by 56.0% at most.",
    "summary": "",
    "translation": "知识规划：用于时序知识图谱问答的检索增强大语言模型",
    "relevance_score": 3,
    "reasoning": "该论文主要关注时序知识图谱问答这一特定NLP任务，属于纯粹的LLM应用研究。虽然涉及检索增强技术，但其应用场景（知识图谱问答）与推荐系统、搜索或广告的核心业务需求关联度较低。检索增强技术本身在搜索领域有潜在应用价值，但论文的具体应用方向限制了其直接相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04070v1": {
    "title": "T-FIX: Text-Based Explanations with Features Interpretable to eXperts",
    "url": "https://www.alphaxiv.org/abs/2511.04070v1",
    "arxiv_id": "2511.04070v1",
    "authors": "Shreya Havaldar, Helen Jin, Chaehyeon Kim, Anton Xue, Weiqiu You, Marco Gatti, Bhuvnesh Jain, Helen Qu, Daniel A Hashimoto, Amin Madani, Rajat Deo, Sameed Ahmed M. Khatana, Gary E. Weissman, Lyle Ungar, Eric Wong",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 05:19:54",
    "ori_summary": "As LLMs are deployed in knowledge-intensive settings (e.g., surgery, astronomy, therapy), users expect not just answers, but also meaningful explanations for those answers. In these settings, users are often domain experts (e.g., doctors, astrophysicists, psychologists) who require explanations that reflect expert-level reasoning. However, current evaluation schemes primarily emphasize plausibility or internal faithfulness of the explanation, which fail to capture whether the content of the explanation truly aligns with expert intuition. We formalize expert alignment as a criterion for evaluating explanations with T-FIX, a benchmark spanning seven knowledge-intensive domains. In collaboration with domain experts, we develop novel metrics to measure the alignment of LLM explanations with expert judgment.",
    "summary": "",
    "translation": "T-FIX：基于文本的专家可解释特征解释方法",
    "relevance_score": 3,
    "reasoning": "该论文关注模型可解释性，这在推荐系统和搜索领域有一定价值，可以帮助理解模型决策过程。然而，论文标题明确强调专家可解释性，这可能更偏向通用AI安全或模型诊断，而非直接针对推荐/搜索/广告的核心算法改进或LLM技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04063v1": {
    "title": "DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization",
    "url": "https://www.alphaxiv.org/abs/2511.04063v1",
    "arxiv_id": "2511.04063v1",
    "authors": "Yuantian Shao, Yuanteng Chen, Peisong Wang, Jianlin Yu, Jing Lin, Yiwu Yao, Zhihui Wei, Jian Cheng",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-11-06 05:05:24",
    "ori_summary": "Quantization plays a crucial role in accelerating the inference of large-scale models, and rotational matrices have been shown to effectively improve quantization performance by smoothing outliers. However, end-to-end fine-tuning of rotational optimization algorithms incurs high computational costs and is prone to overfitting. To address this challenge, we propose an efficient distribution-aware rotational calibration method, DartQuant, which reduces the complexity of rotational optimization by constraining the distribution of the activations after rotation. This approach also effectively reduces reliance on task-specific losses, thereby mitigating the risk of overfitting. Additionally, we introduce the QR-Orth optimization scheme, which replaces expensive alternating optimization with a more efficient solution. In a variety of model quantization experiments, DartQuant demonstrates superior performance. Compared to existing methods, it achieves 47$\\times$ acceleration and 10$\\times$ memory savings for rotational optimization on a 70B model. Furthermore, it is the first to successfully complete rotational calibration for a 70B model on a single 3090 GPU, making quantization of large language models feasible in resource-constrained environments. Code is available at https://github.com/CAS-CLab/DartQuant.git.",
    "summary": "该论文研究LLM量化中旋转优化计算成本高且易过拟合的问题，核心方法是提出分布感知的旋转校准技术，通过约束旋转后激活值的分布来降低优化复杂度，并引入QR-Orth方案替代昂贵的交替优化。",
    "translation": "DartQuant：面向大语言模型量化的高效旋转分布校准",
    "relevance_score": 6,
    "reasoning": "该论文专注于LLM量化的效率优化，属于'使能LLM技术'范畴，因为高效的量化方法可以显著降低LLM在推荐/搜索系统中的部署成本和推理延迟。虽然不直接涉及推荐/搜索算法本身，但量化技术的进步对于实际工业应用至关重要，能够使更大规模的LLM在资源受限的生产环境中部署。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文专注于LLM量化效率的核心技术，通过旋转分布校准和QR-Orth优化方案显著降低计算成本，直接服务于LLM在资源受限环境中的部署应用。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.04048v1": {
    "title": "Explorability in Pushdown Automata",
    "url": "https://www.alphaxiv.org/abs/2511.04048v1",
    "arxiv_id": "2511.04048v1",
    "authors": "Ayaan Bedi, Karoliina Lehtinen",
    "categories": "cs.FL, cs.CL",
    "pub_date": "2025-11-06 04:35:22",
    "ori_summary": "We study explorability, a measure of nondeterminism in pushdown automata, which generalises history-determinism. An automaton is k-explorable if, while reading the input, it suffices to follow k concurrent runs, built step-by-step based only on the input seen so far, to construct an accepting one, if it exists. We show that the class of explorable PDAs lies strictly between history-deterministic and fully nondeterministic PDAs in terms of both expressiveness and succinctness. In fact increasing explorability induces an infinite hierarchy: each level k defines a strictly more expressive class than level k-1, yet the entire class remains less expressive than general nondeterministic PDAs. We then introduce a parameterized notion of explorability, where the number of runs may depend on input length, and show that exponential explorability precisely captures the context-free languages. Finally, we prove that explorable PDAs can be doubly exponentially more succinct than history-deterministic ones, and that the succinctness gap between deterministic and 2-explorable PDAs is not recursively enumerable. These results position explorability as a robust and operationally meaningful measure of nondeterminism for pushdown systems.",
    "summary": "",
    "translation": "下推自动机中的可探索性",
    "relevance_score": 1,
    "reasoning": "该论文研究下推自动机的理论性质，属于形式语言和自动机理论领域，与推荐系统、搜索或广告的核心技术无直接关联。该主题纯粹是理论计算机科学，没有展示出在推荐、搜索或广告系统中的实际应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04035v1": {
    "title": "WST: Weakly Supervised Transducer for Automatic Speech Recognition",
    "url": "https://www.alphaxiv.org/abs/2511.04035v1",
    "arxiv_id": "2511.04035v1",
    "authors": "Dongji Gao, Chenda Liao, Changliang Liu, Matthew Wiesner, Leibny Paola Garcia, Daniel Povey, Sanjeev Khudanpur, Jian Wu",
    "categories": "cs.CL",
    "pub_date": "2025-11-06 04:14:07",
    "ori_summary": "The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily on large-scale, high-quality annotated data, which are often costly and difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised Transducer (WST), which integrates a flexible training graph designed to robustly handle errors in the transcripts without requiring additional confidence estimation or auxiliary pre-trained models. Empirical evaluations on synthetic and industrial datasets reveal that WST effectively maintains performance even with transcription error rates of up to 70%, consistently outperforming existing Connectionist Temporal Classification (CTC)-based weakly supervised approaches, such as Bypass Temporal Classification (BTC) and Omni-Temporal Classification (OTC). These results demonstrate the practical utility and robustness of WST in realistic ASR settings. The implementation will be publicly available.",
    "summary": "",
    "translation": "WST：用于自动语音识别的弱监督传感器",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动语音识别技术，属于纯粹的语音处理领域，与搜索、推荐或广告系统没有直接关联。弱监督传感器架构在语音识别中的改进，在当前技术路径下难以看出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04020v1": {
    "title": "Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises",
    "url": "https://www.alphaxiv.org/abs/2511.04020v1",
    "arxiv_id": "2511.04020v1",
    "authors": "Shiyin Lin",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-11-06 03:37:24",
    "ori_summary": "Large Language Models (LLMs) enhanced with retrieval -- commonly referred to as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved evidence is incomplete, leaving gaps in the reasoning process. In such cases, \\emph{abductive inference} -- the process of generating plausible missing premises to explain observations -- offers a principled approach to bridge these gaps. In this paper, we propose a framework that integrates abductive inference into retrieval-augmented LLMs. Our method detects insufficient evidence, generates candidate missing premises, and validates them through consistency and plausibility checks. Experimental results on abductive reasoning and multi-hop QA benchmarks show that our approach improves both answer accuracy and reasoning faithfulness. This work highlights abductive inference as a promising direction for enhancing the robustness and explainability of RAG systems.",
    "summary": "论文研究检索增强语言模型在证据不完整时的推理失败问题，核心思想是引入溯因推理框架，通过检测证据不足、生成候选缺失前提并进行一致性验证来填补推理空白。",
    "translation": "检索增强语言模型中的溯因推理：生成并验证缺失前提",
    "relevance_score": 8,
    "reasoning": "该论文涉及检索增强语言模型(RAG)中的推理能力改进，这直接属于'直接LLM应用'范畴，对搜索系统具有重要价值。溯因推理能够帮助搜索系统生成合理的假设来填补信息空白，提升复杂查询的理解和回答质量。检索增强架构本身就是搜索领域的关键技术，推理能力的增强可显著改善搜索结果的准确性和完整性。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的溯因推理框架直接针对检索增强生成(RAG)系统的核心缺陷，通过生成和验证缺失前提来增强推理鲁棒性，这对搜索和推荐系统中的证据不完整问题具有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.04000v1": {
    "title": "Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations",
    "url": "https://www.alphaxiv.org/abs/2511.04000v1",
    "arxiv_id": "2511.04000v1",
    "authors": "Kyaw Hpone Myint, Zhe Wu, Alexandre G. R. Day, Giri Iyengar",
    "categories": "cs.LG, cs.AI, cs.CL, stat.ML",
    "pub_date": "2025-11-06 02:50:23",
    "ori_summary": "Decision trees are widely used in high-stakes fields like finance and healthcare due to their interpretability. This work introduces an efficient, scalable method for generating synthetic pre-training data to enable meta-learning of decision trees. Our approach samples near-optimal decision trees synthetically, creating large-scale, realistic datasets. Using the MetaTree transformer architecture, we demonstrate that this method achieves performance comparable to pre-training on real-world data or with computationally expensive optimal decision trees. This strategy significantly reduces computational costs, enhances data generation flexibility, and paves the way for scalable and efficient meta-learning of interpretable decision tree models.",
    "summary": "",
    "translation": "通过合成模型生成实现可扩展元学习以获取接近最优可解释模型",
    "relevance_score": 2,
    "reasoning": "该论文关注元学习和可解释模型生成，属于通用机器学习方法，与推荐系统、搜索或广告的核心技术没有直接关联。虽然元学习在理论上可以应用于模型初始化，但论文标题未表明在推荐、搜索或广告领域的具体应用潜力，更偏向通用机器学习优化。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.03980v1": {
    "title": "LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing",
    "url": "https://www.alphaxiv.org/abs/2511.03980v1",
    "arxiv_id": "2511.03980v1",
    "authors": "Bram Bulté, Ayla Rigouts Terryn",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-11-06 02:09:29",
    "ori_summary": "Large Language Models (LLMs) are rapidly being adopted by users across the globe, who interact with them in a diverse range of languages. At the same time, there are well-documented imbalances in the training data and optimisation objectives of this technology, raising doubts as to whether LLMs can represent the cultural diversity of their broad user base. In this study, we look at LLMs and cultural values and examine how prompt language and cultural framing influence model responses and their alignment with human values in different countries. We probe 10 LLMs with 63 items from the Hofstede Values Survey Module and World Values Survey, translated into 11 languages, and formulated as prompts with and without different explicit cultural perspectives. Our study confirms that both prompt language and cultural perspective produce variation in LLM outputs, but with an important caveat: While targeted prompting can, to a certain extent, steer LLM responses in the direction of the predominant values of the corresponding countries, it does not overcome the models' systematic bias toward the values associated with a restricted set of countries in our dataset: the Netherlands, Germany, the US, and Japan. All tested models, regardless of their origin, exhibit remarkably similar patterns: They produce fairly neutral responses on most topics, with selective progressive stances on issues such as social tolerance. Alignment with cultural values of human respondents is improved more with an explicit cultural perspective than with a targeted prompt language. Unexpectedly, combining both approaches is no more effective than cultural framing with an English prompt. These findings reveal that LLMs occupy an uncomfortable middle ground: They are responsive enough to changes in prompts to produce variation, but too firmly anchored to specific cultural defaults to adequately represent cultural diversity.",
    "summary": "",
    "translation": "大语言模型与文化价值观：提示语言与显性文化框架的影响",
    "relevance_score": 1,
    "reasoning": "该论文主要探讨LLMs与文化价值观的关系，属于伦理、公平性等非技术性话题范畴。论文焦点在于文化框架对模型输出的影响，这与当前关注的推荐系统、搜索广告技术进展、Transformer架构改进或LLM直接应用等核心技术领域无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.03958v1": {
    "title": "Multi-Agent Collaborative Framework For Math Problem Generation",
    "url": "https://www.alphaxiv.org/abs/2511.03958v1",
    "arxiv_id": "2511.03958v1",
    "authors": "Kia Karbasi, Kevin Hong, Mohammad Amin Samadi, Gregory Pottie",
    "categories": "cs.MA, cs.CL, cs.HC, I.2.11; I.2.6; K.3.1",
    "pub_date": "2025-11-06 01:24:07",
    "ori_summary": "Automatic question generation (AQG) for mathematics education remains an elusive goal for Intelligent Tutoring Systems and educators. While pre-trained transformer-based language models have significantly advanced natural language generation, they often struggle to precisely control problem complexity and cognitive demands. In this paper, we introduce a collaborative multi-agent framework as a novel method of incorporating inference-time computation into AQG. This approach leverages multiple agents that iteratively refine generated question-answer pairs to better balance complexity and cognitive demand. We evaluate the generated questions on five meta-evaluation criteria: relevance, importance, clarity, difficulty matching, answerability, to assess the system's ability to control the required complexity and quality of the questions. Preliminary evaluations show that this collaborative multi-agent framework elevates the quality of generated educational content by fostering a more nuanced balance between cognitive challenge and clarity. These promising outcomes suggest that integrating collaborative multi-agent workflows can yield more controlled, pedagogically valuable content that can help advance automated educational content generation and adaptive learning environments.",
    "summary": "",
    "translation": "用于数学问题生成的多智能体协作框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于数学问题生成，这是一个与教育技术相关的特定领域应用，与推荐系统、搜索或广告的核心技术没有直接关联。多智能体协作框架在此上下文中主要解决数学内容生成问题，没有明显的潜力应用于RecSys/Search/Ads领域的技术或架构。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.03945v1": {
    "title": "Direct Semantic Communication Between Large Language Models via Vector Translation",
    "url": "https://www.alphaxiv.org/abs/2511.03945v1",
    "arxiv_id": "2511.03945v1",
    "authors": "Fu-Chun Yang, Jason Eshraghian",
    "categories": "cs.CL, cs.AI, I.2.7",
    "pub_date": "2025-11-06 00:43:29",
    "ori_summary": "In multi-agent settings, such as debate, reflection, or tool-calling, large language models (LLMs) pass messages as plain tokens, discarding most latent semantics. This constrains information transfer and adds unnecessary computational overhead. We form a latent bridge via vector translations, which use learned mappings that enable direct semantic exchange between representation spaces. A dual-encoder translator trained between Llama-2-7B and Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the translated vectors at 30 percent blending strength steers the target model's generation without destabilizing logits. Bidirectional evaluation shows a 2.01:1 transfer asymmetry, indicating that general-purpose models yield more transferable representations than instruction-tuned variants. This conservative injection preserves computational stability while demonstrating that cross-model latent communication is feasible, enabling collaborative AI systems that share meaning rather than tokens.",
    "summary": "该论文研究多智能体系统中LLM间低效的token通信问题，核心思想是通过学习向量翻译映射实现跨模型表示空间的直接语义交换，避免token转换的计算开销。",
    "translation": "通过向量翻译实现大型语言模型之间的直接语义通信",
    "relevance_score": 7,
    "reasoning": "该论文涉及LLM之间的语义通信和向量翻译技术，这属于'使能LLM技术'范畴。在推荐系统或搜索场景中，这种技术可以用于实现多智能体协同推荐、跨域知识迁移或用户-系统间的语义对齐，从而提升个性化服务的质量和效率。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的向量翻译方法实现了跨LLM的语义通信，直接提升了多智能体协作效率，对推荐系统中的模型协同和语义理解有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.03942v1": {
    "title": "MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation",
    "url": "https://www.alphaxiv.org/abs/2511.03942v1",
    "arxiv_id": "2511.03942v1",
    "authors": "Shih-Lun Wu, Yoon Kim, Cheng-Zhi Anna Huang",
    "categories": "cs.SD, cs.CL, cs.MM",
    "pub_date": "2025-11-06 00:40:07",
    "ori_summary": "We present MIDI-LLM, an LLM for generating multitrack MIDI music from free-form text prompts. Our approach expands a text LLM's vocabulary to include MIDI tokens, and uses a two-stage training recipe to endow text-to-MIDI abilities. By preserving the original LLM's parameter structure, we can directly leverage the vLLM library for accelerated inference. Experiments show that MIDI-LLM achieves higher quality, better text control, and faster inference compared to the recent Text2midi model. Live demo at https://midi-llm-demo.vercel.app.",
    "summary": "",
    "translation": "MIDI-LLM：面向文本到MIDI音乐生成的大语言模型适配",
    "relevance_score": 1,
    "reasoning": "该论文专注于音乐生成领域，属于纯粹的AIGC内容生成应用，与推荐系统、搜索或广告的核心技术无关。虽然涉及大语言模型适配，但应用场景（音乐生成）完全超出当前关注的技术领域范围，没有任何潜在的应用相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.03939v1": {
    "title": "RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods",
    "url": "https://www.alphaxiv.org/abs/2511.03939v1",
    "arxiv_id": "2511.03939v1",
    "authors": "Raghav Sharma, Manan Mehta, Sai Tiger Raina",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-11-06 00:35:17",
    "ori_summary": "Reinforcement Learning from Human Feedback (RLHF) is the standard for aligning Large Language Models (LLMs), yet recent progress has moved beyond canonical text-based methods. This survey synthesizes the new frontier of alignment research by addressing critical gaps in multi-modal alignment, cultural fairness, and low-latency optimization. To systematically explore these domains, we first review foundational algo- rithms, including PPO, DPO, and GRPO, before presenting a detailed analysis of the latest innovations. By providing a comparative synthesis of these techniques and outlining open challenges, this work serves as an essential roadmap for researchers building more robust, efficient, and equitable AI systems.",
    "summary": "",
    "translation": "RLHF：针对文化、多模态和低延迟对齐方法的全面综述",
    "relevance_score": 2,
    "reasoning": "虽然RLHF（人类反馈强化学习）是LLM对齐的核心技术，但该论文主要关注文化对齐、多模态对齐和低延迟对齐等特定方面，这些与推荐系统、搜索或广告的核心技术需求关联较弱。论文标题表明其重点在于对齐方法的综述，而非直接应用于推荐/搜索/广告系统的技术创新或架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04680v1": {
    "title": "Carousel: A High-Resolution Dataset for Multi-Target Automatic Image Cropping",
    "url": "https://www.alphaxiv.org/abs/2511.04680v1",
    "arxiv_id": "2511.04680v1",
    "authors": "Rafe Loya, Andrew Hamara, Benjamin Estell, Benjamin Kilpatrick, Andrew C. Freeman",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 18:59:52",
    "ori_summary": "Automatic image cropping is a method for maximizing the human-perceived quality of cropped regions in photographs. Although several works have proposed techniques for producing singular crops, little work has addressed the problem of producing multiple, distinct crops with aesthetic appeal. In this paper, we motivate the problem with a discussion on modern social media applications, introduce a dataset of 277 relevant images and human labels, and evaluate the efficacy of several single-crop models with an image partitioning algorithm as a pre-processing step. The dataset is available at https://github.com/RafeLoya/carousel.",
    "summary": "",
    "translation": "Carousel：用于多目标自动图像裁剪的高分辨率数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的图像裁剪任务和数据集构建，与推荐系统、搜索或广告的核心技术焦点无关。虽然图像在推荐和广告中可能作为内容出现，但自动图像裁剪本身属于纯粹的视觉处理任务，没有明确的推荐/搜索/广告应用路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04679v1": {
    "title": "GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction",
    "url": "https://www.alphaxiv.org/abs/2511.04679v1",
    "arxiv_id": "2511.04679v1",
    "authors": "Qingzhou Lu, Yao Feng, Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu",
    "categories": "cs.RO, cs.CV, cs.HC",
    "pub_date": "2025-11-06 18:59:33",
    "ori_summary": "Humanoid robots are expected to operate in human-centered environments where safe and natural physical interaction is essential. However, most recent reinforcement learning (RL) policies emphasize rigid tracking and suppress external forces. Existing impedance-augmented approaches are typically restricted to base or end-effector control and focus on resisting extreme forces rather than enabling compliance. We introduce GentleHumanoid, a framework that integrates impedance control into a whole-body motion tracking policy to achieve upper-body compliance. At its core is a unified spring-based formulation that models both resistive contacts (restoring forces when pressing against surfaces) and guiding contacts (pushes or pulls sampled from human motion data). This formulation ensures kinematically consistent forces across the shoulder, elbow, and wrist, while exposing the policy to diverse interaction scenarios. Safety is further supported through task-adjustable force thresholds. We evaluate our approach in both simulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging, sit-to-stand assistance, and safe object manipulation. Compared to baselines, our policy consistently reduces peak contact forces while maintaining task success, resulting in smoother and more natural interactions. These results highlight a step toward humanoid robots that can safely and effectively collaborate with humans and handle objects in real-world environments.",
    "summary": "",
    "translation": "GentleHumanoid：学习用于接触丰富的人与物体交互的上半身柔顺性",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人控制和人机交互中的柔顺性学习，属于机器人学领域。虽然涉及交互概念，但与搜索、推荐、广告系统或LLM技术没有直接关联，也不涉及Transformer架构或异构数据处理。该研究主要解决物理机器人控制问题，而非信息检索或用户行为建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04678v1": {
    "title": "Tracking and Understanding Object Transformations",
    "url": "https://www.alphaxiv.org/abs/2511.04678v1",
    "arxiv_id": "2511.04678v1",
    "authors": "Yihong Sun, Xinyu Yang, Jennifer J. Sun, Bharath Hariharan",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 18:59:30",
    "ori_summary": "Real-world objects frequently undergo state transformations. From an apple being cut into pieces to a butterfly emerging from its cocoon, tracking through these changes is important for understanding real-world objects and dynamics. However, existing methods often lose track of the target object after transformation, due to significant changes in object appearance. To address this limitation, we introduce the task of Track Any State: tracking objects through transformations while detecting and describing state changes, accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we present TubeletGraph, a zero-shot system that recovers missing objects after transformation and maps out how object states are evolving over time. TubeletGraph first identifies potentially overlooked tracks, and determines whether they should be integrated based on semantic and proximity priors. Then, it reasons about the added tracks and generates a state graph describing each observed transformation. TubeletGraph achieves state-of-the-art tracking performance under transformations, while demonstrating deeper understanding of object transformations and promising capabilities in temporal grounding and semantic reasoning for complex object transformations. Code, additional results, and the benchmark dataset are available at https://tubelet-graph.github.io.",
    "summary": "",
    "translation": "追踪与理解物体变换",
    "relevance_score": 2,
    "reasoning": "该论文标题主要关注计算机视觉领域的物体变换追踪与理解，属于纯粹的视觉技术范畴。虽然物体变换理解在概念上可能与推荐系统中的用户行为模式变化分析有微弱关联，但缺乏明确的跨模态应用连接点，且未提及任何与推荐、搜索或广告相关的具体技术要素。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04675v1": {
    "title": "InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation",
    "url": "https://www.alphaxiv.org/abs/2511.04675v1",
    "arxiv_id": "2511.04675v1",
    "authors": "Jinlai Liu, Jian Han, Bin Yan, Hui Wu, Fengda Zhu, Xing Wang, Yi Jiang, Bingyue Peng, Zehuan Yuan",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 18:58:03",
    "ori_summary": "We introduce InfinityStar, a unified spacetime autoregressive framework for high-resolution image and dynamic video synthesis. Building on the recent success of autoregressive modeling in both vision and language, our purely discrete approach jointly captures spatial and temporal dependencies within a single architecture. This unified design naturally supports a variety of generation tasks such as text-to-image, text-to-video, image-to-video, and long interactive video synthesis via straightforward temporal autoregression. Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench, outperforming all autoregressive models by large margins, even surpassing some diffusion competitors like HunyuanVideo. Without extra optimizations, our model generates a 5s, 720p video approximately 10x faster than leading diffusion-based methods. To our knowledge, InfinityStar is the first discrete autoregressive video generator capable of producing industrial level 720p videos. We release all code and models to foster further research in efficient, high-quality video generation.",
    "summary": "",
    "translation": "InfinityStar：视觉生成的统一时空自回归建模",
    "relevance_score": 2,
    "reasoning": "该论文专注于视觉生成领域的自回归建模技术，属于纯粹的视觉内容生成范畴。虽然标题中提到“统一时空建模”可能涉及序列处理技术，但核心应用场景是视觉生成而非推荐系统、搜索或广告中的排序任务，与当前关注的技术方向关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04671v1": {
    "title": "X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations",
    "url": "https://www.alphaxiv.org/abs/2511.04671v1",
    "arxiv_id": "2511.04671v1",
    "authors": "Maximus A. Pace, Prithwish Dan, Chuanruo Ning, Atiksh Bhardwaj, Audrey Du, Edward W. Duan, Wei-Chiu Ma, Kushal Kedia",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2025-11-06 18:56:30",
    "ori_summary": "Human videos can be recorded quickly and at scale, making them an appealing source of training data for robot learning. However, humans and robots differ fundamentally in embodiment, resulting in mismatched action execution. Direct kinematic retargeting of human hand motion can therefore produce actions that are physically infeasible for robots. Despite these low-level differences, human demonstrations provide valuable motion cues about how to manipulate and interact with objects. Our key idea is to exploit the forward diffusion process: as noise is added to actions, low-level execution differences fade while high-level task guidance is preserved. We present X-Diffusion, a principled framework for training diffusion policies that maximally leverages human data without learning dynamically infeasible motions. X-Diffusion first trains a classifier to predict whether a noisy action is executed by a human or robot. Then, a human action is incorporated into policy training only after adding sufficient noise such that the classifier cannot discern its embodiment. Actions consistent with robot execution supervise fine-grained denoising at low noise levels, while mismatched human actions provide only coarse guidance at higher noise levels. Our experiments show that naive co-training under execution mismatches degrades policy performance, while X-Diffusion consistently improves it. Across five manipulation tasks, X-Diffusion achieves a 16% higher average success rate than the best baseline. The project website is available at https://portal-cornell.github.io/X-Diffusion/.",
    "summary": "",
    "translation": "X-扩散：在跨具身人类演示数据上训练扩散策略",
    "relevance_score": 2,
    "reasoning": "该论文主要关注扩散模型在机器人策略学习中的应用，属于具身智能和机器人控制领域。虽然扩散模型本身是生成模型的重要进展，但论文的应用场景（跨具身人类演示）与推荐系统、搜索或广告领域没有直接关联，且未涉及Transformer架构或LLM技术的核心进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04670v1": {
    "title": "Cambrian-S: Towards Spatial Supersensing in Video",
    "url": "https://www.alphaxiv.org/abs/2511.04670v1",
    "arxiv_id": "2511.04670v1",
    "authors": "Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, Daohan Lu, Rob Fergus, Yann LeCun, Li Fei-Fei, Saining Xie",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 18:55:17",
    "ori_summary": "We argue that progress in true multimodal intelligence calls for a shift from reactive, task-driven systems and brute-force long context towards a broader paradigm of supersensing. We frame spatial supersensing as four stages beyond linguistic-only understanding: semantic perception (naming what is seen), streaming event cognition (maintaining memory across continuous experiences), implicit 3D spatial cognition (inferring the world behind pixels), and predictive world modeling (creating internal models that filter and organize information). Current benchmarks largely test only the early stages, offering narrow coverage of spatial cognition and rarely challenging models in ways that require true world modeling. To drive progress in spatial supersensing, we present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial recall) and VSC (continual visual spatial counting). These tasks require arbitrarily long video inputs yet are resistant to brute-force context expansion. We then test data scaling limits by curating VSI-590K and training Cambrian-S, achieving +30% absolute improvement on VSI-Bench without sacrificing general capabilities. Yet performance on VSI-SUPER remains limited, indicating that scale alone is insufficient for spatial supersensing. We propose predictive sensing as a path forward, presenting a proof-of-concept in which a self-supervised next-latent-frame predictor leverages surprise (prediction error) to drive memory and event segmentation. On VSI-SUPER, this approach substantially outperforms leading proprietary baselines, showing that spatial supersensing requires models that not only see but also anticipate, select, and organize experience.",
    "summary": "",
    "translation": "寒武纪-S：迈向视频中的空间超感知",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于视频处理中的空间感知技术，属于计算机视觉领域。虽然标题提及“超感知”概念，但没有明确连接推荐系统、搜索或广告应用。该研究似乎更偏向纯粹的视觉感知增强，缺乏与异质数据处理、Transformer架构或LLM技术的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04668v1": {
    "title": "SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding",
    "url": "https://www.alphaxiv.org/abs/2511.04668v1",
    "arxiv_id": "2511.04668v1",
    "authors": "Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, Saining Xie",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 18:53:31",
    "ori_summary": "Despite impressive high-level video comprehension, multimodal language models struggle with spatial reasoning across time and space. While current spatial training approaches rely on real-world video data, obtaining diverse footage with precise spatial annotations remains a bottleneck. To alleviate this bottleneck, we present SIMS-V -- a systematic data-generation framework that leverages the privileged information of 3D simulators to create spatially-rich video training data for multimodal language models. Using this framework, we investigate which properties of simulated data drive effective real-world transfer through systematic ablations of question types, mixes, and scales. We identify a minimal set of three question categories (metric measurement, perspective-dependent reasoning, and temporal tracking) that prove most effective for developing transferable spatial intelligence, outperforming comprehensive coverage despite using fewer question types. These insights enable highly efficient training: our 7B-parameter video LLM fine-tuned on just 25K simulated examples outperforms the larger 72B baseline and achieves competitive performance with proprietary models on rigorous real-world spatial reasoning benchmarks. Our approach demonstrates robust generalization, maintaining performance on general video understanding while showing substantial improvements on embodied and real-world spatial tasks.",
    "summary": "",
    "translation": "SIMS-V：面向空间视频理解的模拟指令调优",
    "relevance_score": 2,
    "reasoning": "该论文聚焦于空间视频理解，这属于纯粹的视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然标题中提到'指令调优'技术，但其应用场景限定在视频理解，没有展示出在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04665v1": {
    "title": "Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions",
    "url": "https://www.alphaxiv.org/abs/2511.04665v1",
    "arxiv_id": "2511.04665v1",
    "authors": "Kaifeng Zhang, Shuo Sha, Hanxiao Jiang, Matthew Loper, Hyunjong Song, Guangyan Cai, Zhuo Xu, Xiaochen Hu, Changxi Zheng, Yunzhu Li",
    "categories": "cs.RO, cs.CV, cs.LG",
    "pub_date": "2025-11-06 18:52:08",
    "ori_summary": "Robotic manipulation policies are advancing rapidly, but their direct evaluation in the real world remains costly, time-consuming, and difficult to reproduce, particularly for tasks involving deformable objects. Simulation provides a scalable and systematic alternative, yet existing simulators often fail to capture the coupled visual and physical complexity of soft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from real-world videos and renders robots, objects, and environments with photorealistic fidelity using 3D Gaussian Splatting. We validate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and T-block pushing, demonstrating that simulated rollouts correlate strongly with real-world execution performance and reveal key behavioral patterns of learned policies. Our results suggest that combining physics-informed reconstruction with high-quality rendering enables reproducible, scalable, and accurate evaluation of robotic manipulation policies. Website: https://real2sim-eval.github.io/",
    "summary": "",
    "translation": "基于高斯溅射软体交互仿真的实到虚机器人策略评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人领域的仿真技术，涉及物理交互模拟和机器人策略评估。虽然提到了高斯溅射这一渲染技术，但其应用场景完全局限于机器人学领域，与推荐系统、搜索或广告没有任何直接或间接的潜在应用关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04655v1": {
    "title": "Benchmark Designers Should \"Train on the Test Set\" to Expose Exploitable Non-Visual Shortcuts",
    "url": "https://www.alphaxiv.org/abs/2511.04655v1",
    "arxiv_id": "2511.04655v1",
    "authors": "Ellis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, Saining Xie",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 18:43:21",
    "ori_summary": "Robust benchmarks are crucial for evaluating Multimodal Large Language Models (MLLMs). Yet we find that models can ace many multimodal benchmarks without strong visual understanding, instead exploiting biases, linguistic priors, and superficial patterns. This is especially problematic for vision-centric benchmarks that are meant to require visual inputs. We adopt a diagnostic principle for benchmark design: if a benchmark can be gamed, it will be. Designers should therefore try to ``game'' their own benchmarks first, using diagnostic and debiasing procedures to systematically identify and mitigate non-visual biases. Effective diagnosis requires directly ``training on the test set'' -- probing the released test set for its intrinsic, exploitable patterns. We operationalize this standard with two components. First, we diagnose benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology. Our primary diagnostic tool involves fine-tuning a powerful Large Language Model via $k$-fold cross-validation on exclusively the non-visual, textual inputs of the test set to reveal shortcut performance and assign each sample a bias score $s(x)$. We complement this with a lightweight Random Forest-based diagnostic operating on hand-crafted features for fast, interpretable auditing. Second, we debias benchmarks by filtering high-bias samples using an ``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive non-visual biases. As a case study, we apply our full framework to create VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider vision-blind performance gap than the original.",
    "summary": "",
    "translation": "基准设计者应“在测试集上训练”以暴露可被利用的非视觉捷径",
    "relevance_score": 1,
    "reasoning": "该论文关注基准设计和评估方法，属于评估基准范畴，这在无关主题中明确排除。虽然可能涉及模型利用数据模式的问题，但缺乏与推荐系统、搜索或广告的直接技术关联，且未提及任何可能应用于这些领域的使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04652v1": {
    "title": "Polarization-resolved imaging improves eye tracking",
    "url": "https://www.alphaxiv.org/abs/2511.04652v1",
    "arxiv_id": "2511.04652v1",
    "authors": "Mantas Žurauskas, Tom Bu, Sanaz Alali, Beyza Kalkanli, Derek Shi, Fernando Alamos, Gauresh Pandit, Christopher Mei, Ali Behrooz, Ramin Mirjalili, Dave Stronks, Alexander Fix, Dmitri Model",
    "categories": "cs.CV, physics.optics",
    "pub_date": "2025-11-06 18:42:09",
    "ori_summary": "Polarization-resolved near-infrared imaging adds a useful optical contrast mechanism to eye tracking by measuring the polarization state of light reflected by ocular tissues in addition to its intensity. In this paper we demonstrate how this contrast can be used to enable eye tracking. Specifically, we demonstrate that a polarization-enabled eye tracking (PET) system composed of a polarization--filter--array camera paired with a linearly polarized near-infrared illuminator can reveal trackable features across the sclera and gaze-informative patterns on the cornea, largely absent in intensity-only images. Across a cohort of 346 participants, convolutional neural network based machine learning models trained on data from PET reduced the median 95th-percentile absolute gaze error by 10--16\\% relative to capacity-matched intensity baselines under nominal conditions and in the presence of eyelid occlusions, eye-relief changes, and pupil-size variation. These results link light--tissue polarization effects to practical gains in human--computer interaction and position PET as a simple, robust sensing modality for future wearable devices.",
    "summary": "",
    "translation": "偏振分辨成像改进眼动追踪",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的眼动追踪技术改进，属于纯粹的视觉应用领域。虽然眼动追踪在用户行为分析中有潜在用途，但论文标题明确聚焦于成像技术改进，没有显示与推荐系统、搜索或广告排名的直接关联。这属于纯粹的视觉技术论文，不符合当前关注的任何技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04628v1": {
    "title": "NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment",
    "url": "https://www.alphaxiv.org/abs/2511.04628v1",
    "arxiv_id": "2511.04628v1",
    "authors": "Kylie Cancilla, Alexander Moore, Amar Saini, Carmen Carrano",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 18:23:55",
    "ori_summary": "Video quality assessment (VQA) is vital for computer vision tasks, but existing approaches face major limitations: full-reference (FR) metrics require clean reference videos, and most no-reference (NR) models depend on training on costly human opinion labels. Moreover, most opinion-unaware NR methods are image-based, ignoring temporal context critical for video object detection. In this work, we present a scalable, streaming-based VQA model that is both no-reference and opinion-unaware. Our model leverages synthetic degradations of the DAVIS dataset, training a temporal-aware convolutional architecture to predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without references at inference. We show that our streaming approach outperforms our own image-based baseline by generalizing across diverse degradations, underscoring the value of temporal modeling for scalable VQA in real-world vision systems. Additionally, we demonstrate that our model achieves higher correlation with full-reference metrics compared to BRISQUE, a widely-used opinion-aware image quality assessment baseline, validating the effectiveness of our temporal, opinion-unaware approach.",
    "summary": "",
    "translation": "NovisVQ：一种用于无参考意见无关帧质量评估的流式卷积神经网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频帧质量评估的计算机视觉任务，属于纯粹的视觉质量分析领域。论文内容涉及无参考质量评估和流式处理，没有明确的技术路径或应用场景与推荐系统、搜索或广告相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04615v1": {
    "title": "Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality",
    "url": "https://www.alphaxiv.org/abs/2511.04615v1",
    "arxiv_id": "2511.04615v1",
    "authors": "Tushar Kataria, Shikha Dubey, Mary Bronner, Jolanta Jedrzkiewicz, Ben J. Brintz, Shireen Y. Elhabian, Beatrice S. Knudsen",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 18:09:09",
    "ori_summary": "Deep learning models can generate virtual immunohistochemistry (IHC) stains from hematoxylin and eosin (H&E) images, offering a scalable and low-cost alternative to laboratory IHC. However, reliable evaluation of image quality remains a challenge as current texture- and distribution-based metrics quantify image fidelity rather than the accuracy of IHC staining. Here, we introduce an automated and accuracy grounded framework to determine image quality across sixteen paired or unpaired image translation models. Using color deconvolution, we generate masks of pixels stained brown (i.e., IHC-positive) as predicted by each virtual IHC model. We use the segmented masks of real and virtual IHC to compute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directly quantify correct pixel - level labeling without needing expert manual annotations. Our results demonstrate that conventional image fidelity metrics, including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR), and structural similarity (SSIM), correlate poorly with stain accuracy and pathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCE achieve the highest stain accuracy, whereas unpaired diffusion- and GAN-based models are less reliable in providing accurate IHC positive pixel labels. Moreover, whole-slide images (WSI) reveal performance declines that are invisible in patch-based evaluations, emphasizing the need for WSI-level benchmarks. Together, this framework defines a reproducible approach for assessing the quality of virtual IHC models, a critical step to accelerate translation towards routine use by pathologists.",
    "summary": "",
    "translation": "构建虚拟免疫组织化学中的信任：图像质量的自动化评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分析领域的虚拟免疫组织化学，这属于明确的无关主题（医学/生物学应用）。论文内容涉及图像质量评估，但缺乏与推荐系统、搜索或广告领域的任何潜在联系。没有证据表明该技术可以应用于异构数据处理或推荐系统架构。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04601v1": {
    "title": "PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning",
    "url": "https://www.alphaxiv.org/abs/2511.04601v1",
    "arxiv_id": "2511.04601v1",
    "authors": "Yicheng Xiao, Yu Chen, Haoxuan Ma, Jiale Hong, Caorui Li, Lingxiang Wu, Haiyun Guo, Jinqiao Wang",
    "categories": "cs.CV, cs.MM",
    "pub_date": "2025-11-06 17:54:12",
    "ori_summary": "While the Contrastive Language-Image Pretraining(CLIP) model has achieved remarkable success in a variety of downstream vison language understanding tasks, enhancing its capability for fine-grained image-text alignment remains an active research focus. To this end, most existing works adopt the strategy of explicitly increasing the granularity of visual information processing, e.g., incorporating visual prompts to guide the model focus on specific local regions within the image. Meanwhile, researches on Multimodal Large Language Models(MLLMs) have demonstrated that training with long and detailed textual descriptions can effectively improve the model's fine-grained vision-language alignment. However, the inherent token length limitation of CLIP's text encoder fundamentally limits CLIP to process more granular textual information embedded in long text sequences. To synergistically leverage the advantages of enhancing both visual and textual content processing granularity, we propose PixCLIP, a novel framework designed to concurrently accommodate visual prompt inputs and process lengthy textual descriptions. Specifically, we first establish an automated annotation pipeline capable of generating pixel-level localized, long-form textual descriptions for images. Utilizing this pipeline, we construct LongGRIT, a high-quality dataset comprising nearly 1.5 million samples. Secondly, we replace CLIP's original text encoder with the LLM and propose a three-branch pixel-text alignment learning framework, facilitating fine-grained alignment between image regions and corresponding textual descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP showcases breakthroughs in pixel-level interaction and handling long-form texts, achieving state-of-the-art performance.",
    "summary": "该论文研究如何提升CLIP模型在细粒度视觉语言理解任务中的性能，核心思想是通过构建像素级定位的长文本描述数据集，并设计三分支像素-文本对齐学习框架，实现任意粒度下图像区域与对应文本描述的细粒度对齐。",
    "translation": "PixCLIP：通过任意粒度像素-文本对齐学习实现细粒度视觉语言理解",
    "relevance_score": 7,
    "reasoning": "该论文属于VLM（视觉语言模型）技术范畴，与'VLM类比处理异构数据'这一关注点高度相关。通过像素级文本对齐实现细粒度视觉语言理解的技术，可以类比应用于推荐系统中处理用户行为序列与上下文特征的异构数据融合，实现更精准的跨模态匹配和个性化推荐。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出的像素级文本对齐框架和多模态统一建模方法，与VLM类比异构数据的思想高度相关，为推荐系统中的细粒度特征对齐提供了新思路。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.04595v1": {
    "title": "UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2511.04595v1",
    "arxiv_id": "2511.04595v1",
    "authors": "Chen Shi, Shaoshuai Shi, Xiaoyang Lyu, Chunyang Liu, Kehua Sheng, Bo Zhang, Li Jiang",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 17:49:39",
    "ori_summary": "Feed-forward 3D reconstruction for autonomous driving has advanced rapidly, yet existing methods struggle with the joint challenges of sparse, non-overlapping camera views and complex scene dynamics. We present UniSplat, a general feed-forward framework that learns robust dynamic scene reconstruction through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent scaffold, a structured representation that captures geometric and semantic scene context by leveraging pretrained foundation models. To effectively integrate information across spatial views and temporal frames, we introduce an efficient fusion mechanism that operates directly within the 3D scaffold, enabling consistent spatio-temporal alignment. To ensure complete and detailed reconstructions, we design a dual-branch decoder that generates dynamic-aware Gaussians from the fused scaffold by combining point-anchored refinement with voxel-based generation, and maintain a persistent memory of static Gaussians to enable streaming scene completion beyond current camera coverage. Extensive experiments on real-world datasets demonstrate that UniSplat achieves state-of-the-art performance in novel view synthesis, while providing robust and high-quality renderings even for viewpoints outside the original camera coverage.",
    "summary": "",
    "translation": "UniSplat：基于3D潜在支架的统一时空融合用于动态驾驶场景重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于动态驾驶场景的3D重建和时空融合，属于计算机视觉和3D视觉领域。虽然标题提到'统一'和'融合'概念，但核心应用是驾驶场景重建，与推荐系统、搜索或广告没有直接关联。该技术缺乏在RecSys/Search/Ads领域的明显应用潜力，属于纯粹的视觉任务。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04555v1": {
    "title": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment",
    "url": "https://www.alphaxiv.org/abs/2511.04555v1",
    "arxiv_id": "2511.04555v1",
    "authors": "Tao Lin, Yilei Zhong, Yuxin Du, Jingjing Zhang, Jiting Liu, Yinxinyu Chen, Encheng Gu, Ziyan Liu, Hongyi Cai, Yanwen Zou, Lixing Zou, Zhaoye Zhou, Gen Li, Bo Zhao",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-11-06 17:07:49",
    "ori_summary": "Vision-Language-Action (VLA) models have emerged as a powerful framework that unifies perception, language, and control, enabling robots to perform diverse tasks through multimodal understanding. However, current VLA models typically contain massive parameters and rely heavily on large-scale robot data pretraining, leading to high computational costs during training, as well as limited deployability for real-time inference. Moreover, most training paradigms often degrade the perceptual representations of the vision-language backbone, resulting in overfitting and poor generalization to downstream tasks. In this work, we present Evo-1, a lightweight VLA model that reduces computation and improves deployment efficiency, while maintaining strong performance without pretraining on robot data. Evo-1 builds on a native multimodal Vision-Language model (VLM), incorporating a novel cross-modulated diffusion transformer along with an optimized integration module, together forming an effective architecture. We further introduce a two-stage training paradigm that progressively aligns action with perception, preserving the representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1 achieves state-of-the-art results on the Meta-World and RoboTwin suite, surpassing the previous best models by 12.4% and 6.9%, respectively, and also attains a competitive result of 94.8% on LIBERO. In real-world evaluations, Evo-1 attains a 78% success rate with high inference frequency and low memory overhead, outperforming all baseline methods. We release code, data, and model weights to facilitate future research on lightweight and efficient VLA models.",
    "summary": "",
    "translation": "Evo-1：具有保持语义对齐能力的轻量级视觉-语言-动作模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉-语言-动作模型，属于机器人控制和具身智能领域。虽然涉及多模态建模概念，但动作模态与推荐系统、搜索或广告的核心需求关联度极低。视觉-语言对齐技术虽有潜在启发价值，但论文的机器人应用焦点使其与当前关注领域相距甚远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04525v1": {
    "title": "Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy",
    "url": "https://www.alphaxiv.org/abs/2511.04525v1",
    "arxiv_id": "2511.04525v1",
    "authors": "Dimitrios Anastasiou, Santiago Barbarisi, Lucy Culshaw, Jayna Patel, Evangelos B. Mazomenos, Imanol Luengo, Danail Stoyanov",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 16:39:55",
    "ori_summary": "Purpose: Accurate assessment of surgical complexity is essential in Laparoscopic Cholecystectomy (LC), where severe inflammation is associated with longer operative times and increased risk of postoperative complications. The Parkland Grading Scale (PGS) provides a clinically validated framework for stratifying inflammation severity; however, its automation in surgical videos remains largely unexplored, particularly in realistic scenarios where complete videos must be analyzed without prior manual curation. Methods: In this work, we introduce STC-Net, a novel framework for SingleTimestamp-based Complexity estimation in LC via the PGS, designed to operate under weak temporal supervision. Unlike prior methods limited to static images or manually trimmed clips, STC-Net operates directly on full videos. It jointly performs temporal localization and grading through a localization, window proposal, and grading module. We introduce a novel loss formulation combining hard and soft localization objectives and background-aware grading supervision. Results: Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy of 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by over 10% in both metrics and highlighting the effectiveness of weak supervision for surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable and effective approach for automated PGS-based surgical complexity estimation from full LC videos, making it promising for post-operative analysis and surgical training.",
    "summary": "",
    "translation": "从单一时间戳学习：腹腔镜胆囊切除术中的复杂度估计",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域（腹腔镜胆囊切除术）的手术复杂度估计，这属于明确的无关主题（医学、生物学等）。论文内容与推荐系统、搜索、广告或相关使能技术没有任何关联，也没有潜在的跨领域应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04520v1": {
    "title": "THEval. Evaluation Framework for Talking Head Video Generation",
    "url": "https://www.alphaxiv.org/abs/2511.04520v1",
    "arxiv_id": "2511.04520v1",
    "authors": "Nabyl Quignon, Baptiste Chopin, Yaohui Wang, Antitza Dantcheva",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 16:34:10",
    "ori_summary": "Video generation has achieved remarkable progress, with generated videos increasingly resembling real ones. However, the rapid advance in generation has outpaced the development of adequate evaluation metrics. Currently, the assessment of talking head generation primarily relies on limited metrics, evaluating general video quality, lip synchronization, and on conducting user studies. Motivated by this, we propose a new evaluation framework comprising 8 metrics related to three dimensions (i) quality, (ii) naturalness, and (iii) synchronization. In selecting the metrics, we place emphasis on efficiency, as well as alignment with human preferences. Based on this considerations, we streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as well as face quality. Our extensive experiments on 85,000 videos generated by 17 state-of-the-art models suggest that while many algorithms excel in lip synchronization, they face challenges with generating expressiveness and artifact-free details. These videos were generated based on a novel real dataset, that we have curated, in order to mitigate bias of training data. Our proposed benchmark framework is aimed at evaluating the improvement of generative methods. Original code, dataset and leaderboards will be publicly released and regularly updated with new methods, in order to reflect progress in the field.",
    "summary": "",
    "translation": "THEval：说话头部视频生成的评估框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于说话头部视频生成的评估框架，属于纯粹的视觉生成和评估领域，与推荐系统、搜索或广告的核心技术无关。论文内容涉及视频生成质量评估，没有展示在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04510v1": {
    "title": "$μ$NeuFMT: Optical-Property-Adaptive Fluorescence Molecular Tomography via Implicit Neural Representation",
    "url": "https://www.alphaxiv.org/abs/2511.04510v1",
    "arxiv_id": "2511.04510v1",
    "authors": "Shihan Zhao, Jianru Zhang, Yanan Wu, Linlin Li, Siyuan Shen, Xingjun Zhu, Guoyan Zheng, Jiahua Jiang, Wuwei Ren",
    "categories": "eess.IV, cs.CV, physics.optics, 68T07, 78A46, 78A70, 92C55, I.2.10; I.4.5",
    "pub_date": "2025-11-06 16:28:30",
    "ori_summary": "Fluorescence Molecular Tomography (FMT) is a promising technique for non-invasive 3D visualization of fluorescent probes, but its reconstruction remains challenging due to the inherent ill-posedness and reliance on inaccurate or often-unknown tissue optical properties. While deep learning methods have shown promise, their supervised nature limits generalization beyond training data. To address these problems, we propose $\\mu$NeuFMT, a self-supervised FMT reconstruction framework that integrates implicit neural-based scene representation with explicit physical modeling of photon propagation. Its key innovation lies in jointly optimize both the fluorescence distribution and the optical properties ($\\mu$) during reconstruction, eliminating the need for precise prior knowledge of tissue optics or pre-conditioned training data. We demonstrate that $\\mu$NeuFMT robustly recovers accurate fluorophore distributions and optical coefficients even with severely erroneous initial values (0.5$\\times$ to 2$\\times$ of ground truth). Extensive numerical, phantom, and in vivo validations show that $\\mu$NeuFMT outperforms conventional and supervised deep learning approaches across diverse heterogeneous scenarios. Our work establishes a new paradigm for robust and accurate FMT reconstruction, paving the way for more reliable molecular imaging in complex clinically related scenarios, such as fluorescence guided surgery.",
    "summary": "",
    "translation": "μNeuFMT：基于隐式神经表示的光学特性自适应荧光分子断层成像",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学成像领域的荧光分子断层成像技术，属于生物医学应用范畴。论文涉及的光学特性自适应方法和隐式神经表示技术是针对医学成像的特定优化，与推荐系统、搜索或广告领域没有任何直接或间接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04494v1": {
    "title": "Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks",
    "url": "https://www.alphaxiv.org/abs/2511.04494v1",
    "arxiv_id": "2511.04494v1",
    "authors": "Alper Kalle, Theo Rudkiewicz, Mohamed-Oumar Ouerfelli, Mohamed Tamaazousti",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-11-06 16:15:15",
    "ori_summary": "Neural networks are widely used for image-related tasks but typically demand considerable computing power. Once a network has been trained, however, its memory- and compute-footprint can be reduced by compression. In this work, we focus on compression through tensorization and low-rank representations. Whereas classical approaches search for a low-rank approximation by minimizing an isotropic norm such as the Frobenius norm in weight-space, we use data-informed norms that measure the error in function space. Concretely, we minimize the change in the layer's output distribution, which can be expressed as $\\lVert (W - \\widetilde{W}) \\Sigma^{1/2}\\rVert_F$ where $\\Sigma^{1/2}$ is the square root of the covariance matrix of the layer's input and $W$, $\\widetilde{W}$ are the original and compressed weights. We propose new alternating least square algorithms for the two most common tensor decompositions (Tucker-2 and CPD) that directly optimize the new norm. Unlike conventional compression pipelines, which almost always require post-compression fine-tuning, our data-informed approach often achieves competitive accuracy without any fine-tuning. We further show that the same covariance-based norm can be transferred from one dataset to another with only a minor accuracy drop, enabling compression even when the original training dataset is unavailable. Experiments on several CNN architectures (ResNet-18/50, and GoogLeNet) and datasets (ImageNet, FGVC-Aircraft, Cifar10, and Cifar100) confirm the advantages of the proposed method.",
    "summary": "",
    "translation": "面向卷积神经网络压缩的分布感知张量分解",
    "relevance_score": 2,
    "reasoning": "该论文主要关注CNN模型压缩技术，属于通用的神经网络优化领域。虽然模型压缩技术可以间接应用于推荐系统中的深度模型，但论文本身没有明确展示与RecSys/Search/Ads的直接关联，且不属于核心LLM技术或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04474v1": {
    "title": "Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability",
    "url": "https://www.alphaxiv.org/abs/2511.04474v1",
    "arxiv_id": "2511.04474v1",
    "authors": "Wenwen Li, Sizhe Wang, Hyunho Lee, Chenyan Lu, Sujit Roy, Rahul Ramachandran, Chia-Yu Hsu",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 15:47:37",
    "ori_summary": "Landslides cause severe damage to lives, infrastructure, and the environment, making accurate and timely mapping essential for disaster preparedness and response. However, conventional deep learning models often struggle when applied across different sensors, regions, or under conditions of limited training data. To address these challenges, we present a three-axis analytical framework of sensor, label, and domain for adapting geospatial foundation models (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a series of experiments, we show that it consistently outperforms task-specific CNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other GeoFMs (TerraMind, SatMAE). The model, built on global pretraining, self-supervision, and adaptable fine-tuning, proved resilient to spectral variation, maintained accuracy under label scarcity, and generalized more reliably across diverse datasets and geographic settings. Alongside these strengths, we also highlight remaining challenges such as computational cost and the limited availability of reusable AI-ready training data for landslide research. Overall, our study positions GeoFMs as a step toward more robust and scalable approaches for landslide risk reduction and environmental monitoring.",
    "summary": "",
    "translation": "基于地理空间基础模型的滑坡灾害制图：地理泛化性、数据稀缺性与波段适应性",
    "relevance_score": 1,
    "reasoning": "该论文专注于地理空间分析和地质灾害预测领域，与推荐系统、搜索或广告的核心技术完全无关。虽然提到了基础模型概念，但其应用场景是地理科学而非用户行为建模或内容排序，因此没有任何潜在的应用相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04460v1": {
    "title": "V-Thinker: Interactive Thinking with Images",
    "url": "https://www.alphaxiv.org/abs/2511.04460v1",
    "arxiv_id": "2511.04460v1",
    "authors": "Runqi Qiao, Qiuna Tan, Minghan Yang, Guanting Dong, Peiqing Yang, Shiqiang Lang, Enhui Wan, Xiaowan Wang, Yida Xu, Lan Yang, Chong Sun, Chen Li, Honggang Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 15:32:29",
    "ori_summary": "Empowering Large Multimodal Models (LMMs) to deeply integrate image interaction with long-horizon reasoning capabilities remains a long-standing challenge in this field. Recent advances in vision-centric reasoning explore a promising \"Thinking with Images\" paradigm for LMMs, marking a shift from image-assisted reasoning to image-interactive thinking. While this milestone enables models to focus on fine-grained image regions, progress remains constrained by limited visual tool spaces and task-specific workflow designs. To bridge this gap, we present V-Thinker, a general-purpose multimodal reasoning assistant that enables interactive, vision-centric thinking through end-to-end reinforcement learning. V-Thinker comprises two key components: (1) a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies interactive reasoning datasets across three dimensions-diversity, quality, and difficulty; and (2) a Visual Progressive Training Curriculum that first aligns perception via point-level supervision, then integrates interactive reasoning through a two-stage reinforcement learning framework. Furthermore, we introduce VTBench, an expert-verified benchmark targeting vision-centric interactive reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently outperforms strong LMM-based baselines in both general and interactive reasoning scenarios, providing valuable insights for advancing image-interactive reasoning applications.",
    "summary": "",
    "translation": "V-Thinker：基于图像的交互式思维",
    "relevance_score": 2,
    "reasoning": "该论文标题暗示了图像交互式思维处理，可能涉及视觉推理或多模态交互。虽然标题包含'交互式思维'概念，但缺乏与推荐系统、搜索或广告的具体联系。没有明确证据表明该技术能够处理推荐系统中的异构数据或应用于搜索广告场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04450v1": {
    "title": "Solving Convex Partition Visual Jigsaw Puzzles",
    "url": "https://www.alphaxiv.org/abs/2511.04450v1",
    "arxiv_id": "2511.04450v1",
    "authors": "Yaniv Ohayon, Ofir Itzhak Shahar, Ohad Ben-Shahar",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 15:22:46",
    "ori_summary": "Jigsaw puzzle solving requires the rearrangement of unordered pieces into their original pose in order to reconstruct a coherent whole, often an image, and is known to be an intractable problem. While the possible impact of automatic puzzle solvers can be disruptive in various application domains, most of the literature has focused on developing solvers for square jigsaw puzzles, severely limiting their practical use. In this work, we significantly expand the types of puzzles handled computationally, focusing on what is known as Convex Partitions, a major subset of polygonal puzzles whose pieces are convex. We utilize both geometrical and pictorial compatibilities, introduce a greedy solver, and report several performance measures next to the first benchmark dataset of such puzzles.",
    "summary": "",
    "translation": "求解凸分割视觉拼图游戏",
    "relevance_score": 1,
    "reasoning": "该论文专注于视觉拼图游戏这一纯粹的计算机视觉任务，与推荐系统、搜索或广告的核心技术领域无关。论文标题表明其研究视觉模式识别和几何分割问题，这些技术没有明显的潜在应用场景可以迁移到推荐系统、搜索或广告领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04426v1": {
    "title": "HideAndSeg: an AI-based tool with automated prompting for octopus segmentation in natural habitats",
    "url": "https://www.alphaxiv.org/abs/2511.04426v1",
    "arxiv_id": "2511.04426v1",
    "authors": "Alan de Aguiar, Michaella Pereira Andrade, Charles Morphy D. Santos, João Paulo Gois",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 14:59:27",
    "ori_summary": "Analyzing octopuses in their natural habitats is challenging due to their camouflage capability, rapid changes in skin texture and color, non-rigid body deformations, and frequent occlusions, all of which are compounded by variable underwater lighting and turbidity. Addressing the lack of large-scale annotated datasets, this paper introduces HideAndSeg, a novel, minimally supervised AI-based tool for segmenting videos of octopuses. It establishes a quantitative baseline for this task. HideAndSeg integrates SAM2 with a custom-trained YOLOv11 object detector. First, the user provides point coordinates to generate the initial segmentation masks with SAM2. These masks serve as training data for the YOLO model. After that, our approach fully automates the pipeline by providing a bounding box prompt to SAM2, eliminating the need for further manual intervention. We introduce two unsupervised metrics - temporal consistency $DICE_t$ and new component count $NC_t$ - to quantitatively evaluate segmentation quality and guide mask refinement in the absence of ground-truth data, i.e., real-world information that serves to train, validate, and test AI models. Results show that HideAndSeg achieves satisfactory performance, reducing segmentation noise compared to the manually prompted approach. Our method can re-identify and segment the octopus even after periods of complete occlusion in natural environments, a scenario in which the manually prompted model fails. By reducing the need for manual analysis in real-world scenarios, this work provides a practical tool that paves the way for more efficient behavioral studies of wild cephalopods.",
    "summary": "",
    "translation": "HideAndSeg：一种基于人工智能的工具，具有自动提示功能，用于自然栖息地中章鱼的图像分割",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的图像分割任务，特别是针对海洋生物学领域的章鱼识别。这属于纯粹的视觉应用，与推荐系统、搜索或广告领域没有任何关联。论文标题明确指向生物识别和自然栖息地分析，完全落在无关主题范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04422v1": {
    "title": "On the Equivalence of Regression and Classification",
    "url": "https://www.alphaxiv.org/abs/2511.04422v1",
    "arxiv_id": "2511.04422v1",
    "authors": "Jayadeva, Naman Dwivedi, Hari Krishnan, N. M. Anoop Krishnan",
    "categories": "cs.LG, cs.AI, cs.CV, 68T05, 68T10, 68Q32, I.2.6; I.5.1; I.5.2",
    "pub_date": "2025-11-06 14:54:25",
    "ori_summary": "A formal link between regression and classification has been tenuous. Even though the margin maximization term $\\|w\\|$ is used in support vector regression, it has at best been justified as a regularizer. We show that a regression problem with $M$ samples lying on a hyperplane has a one-to-one equivalence with a linearly separable classification task with $2M$ samples. We show that margin maximization on the equivalent classification task leads to a different regression formulation than traditionally used. Using the equivalence, we demonstrate a ``regressability'' measure, that can be used to estimate the difficulty of regressing a dataset, without needing to first learn a model for it. We use the equivalence to train neural networks to learn a linearizing map, that transforms input variables into a space where a linear regressor is adequate.",
    "summary": "",
    "translation": "论回归与分类的等价性",
    "relevance_score": 2,
    "reasoning": "该论文探讨回归与分类的数学等价性，属于机器学习理论基础研究。虽然回归和分类是推荐系统和搜索中的基础技术，但论文本身聚焦于理论等价性证明，缺乏明确的实际应用场景或技术改进，与当前关注的领域进展和实际应用相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04394v1": {
    "title": "DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale",
    "url": "https://www.alphaxiv.org/abs/2511.04394v1",
    "arxiv_id": "2511.04394v1",
    "authors": "Ke Du, Yimin Peng, Chao Gao, Fan Zhou, Siqiao Xue",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 14:22:51",
    "ori_summary": "DORAEMON is an open-source PyTorch library that unifies visual object modeling and representation learning across diverse scales. A single YAML-driven workflow covers classification, retrieval and metric learning; more than 1000 pretrained backbones are exposed through a timm-compatible interface, together with modular losses, augmentations and distributed-training utilities. Reproducible recipes match or exceed reference results on ImageNet-1K, MS-Celeb-1M and Stanford online products, while one-command export to ONNX or HuggingFace bridges research and deployment. By consolidating datasets, models, and training techniques into one platform, DORAEMON offers a scalable foundation for rapid experimentation in visual recognition and representation learning, enabling efficient transfer of research advances to real-world applications. The repository is available at https://github.com/wuji3/DORAEMON.",
    "summary": "",
    "translation": "DORAEMON：用于大规模视觉对象建模与表示学习的统一库",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉对象建模和表示学习的库开发，属于纯粹的计算机视觉领域。虽然表示学习有通用价值，但论文标题明确限定于视觉对象，没有表明与推荐系统、搜索或广告的潜在应用关联。缺乏对异构数据处理或多模态学习的明确提及，使其与当前关注点相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04388v1": {
    "title": "BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems",
    "url": "https://www.alphaxiv.org/abs/2511.04388v1",
    "arxiv_id": "2511.04388v1",
    "authors": "Chang Liu, Juan Li, Sheng Zhang, Chang Liu, Jie Li, Xu Zhang",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-11-06 14:17:33",
    "ori_summary": "Depth estimation is one of the key technologies for realizing 3D perception in unmanned systems. Monocular depth estimation has been widely researched because of its low-cost advantage, but the existing methods face the challenges of poor depth estimation performance and blurred object boundaries on embedded systems. In this paper, we propose a novel monocular depth estimation model, BoRe-Depth, which contains only 8.7M parameters. It can accurately estimate depth maps on embedded systems and significantly improves boundary quality. Firstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which adaptively fuses depth features to enhance boundary detail representation. Secondly, we integrate semantic knowledge into the encoder to improve the object recognition and boundary perception capabilities. Finally, BoRe-Depth is deployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We demonstrate that the proposed model significantly outperforms previous lightweight models on multiple challenging datasets, and we provide detailed ablation studies for the proposed methods. The code is available at https://github.com/liangxiansheng093/BoRe-Depth.",
    "summary": "",
    "translation": "BoRe-Depth：面向嵌入式系统的边界细化自监督单目深度估计",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的单目深度估计技术，主要解决嵌入式系统的部署问题。虽然深度感知在机器人导航等场景中有应用，但该工作没有展示与推荐系统、搜索或广告领域的直接关联，也不涉及LLM、Transformer架构或异构数据建模等核心技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04384v1": {
    "title": "Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA",
    "url": "https://www.alphaxiv.org/abs/2511.04384v1",
    "arxiv_id": "2511.04384v1",
    "authors": "Itbaan Safwan, Muhammad Annas Shaikh, Muhammad Haaris, Ramail Khan, Muhammad Atif Tahir",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-11-06 14:09:56",
    "ori_summary": "We present a multi-task framework for the MediaEval Medico 2025 challenge, leveraging a LoRA-tuned Florence-2 model for simultaneous visual question answering (VQA), explanation generation, and visual grounding. The proposed system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer learning, (2) a synthetically enriched explanation dataset offering structured medical reasoning, and (3) text-to-region pairs linking visual features with segmentation masks. This multi-task setup enables the model to jointly learn visual grounding, reasoning, and interpretation, producing responses that are both accurate and interpretable. Extensive evaluation demonstrates that our approach substantially improves over single-task baselines in both answer accuracy and visual localization, highlighting the effectiveness of grounded multi-task learning for medical VQA applications.",
    "summary": "",
    "translation": "面向胃肠道视觉问答中视觉基础推理的多任务学习",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的视觉问答应用，特别是胃肠道图像分析，这属于明确的无关主题范畴。虽然涉及多任务学习和视觉推理，但其应用场景与推荐系统、搜索或广告完全无关，且没有显示出任何在这些领域应用的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04357v1": {
    "title": "GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies",
    "url": "https://www.alphaxiv.org/abs/2511.04357v1",
    "arxiv_id": "2511.04357v1",
    "authors": "Maëlic Neau, Zoe Falomir, Paulo E. Santos, Anne-Gwenn Bosser, Cédric Buche",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-11-06 13:39:38",
    "ori_summary": "Deploying autonomous robots that can learn new skills from demonstrations is an important challenge of modern robotics. Existing solutions often apply end-to-end imitation learning with Vision-Language Action (VLA) models or symbolic approaches with Action Model Learning (AML). On the one hand, current VLA models are limited by the lack of high-level symbolic planning, which hinders their abilities in long-horizon tasks. On the other hand, symbolic approaches in AML lack generalization and scalability perspectives. In this paper we present a new neuro-symbolic approach, GraSP-VLA, a framework that uses a Continuous Scene Graph representation to generate a symbolic representation of human demonstrations. This representation is used to generate new planning domains during inference and serves as an orchestrator for low-level VLA policies, scaling up the number of actions that can be reproduced in a row. Our results show that GraSP-VLA is effective for modeling symbolic representations on the task of automatic planning domain generation from observations. In addition, results on real-world experiments show the potential of our Continuous Scene Graph representation to orchestrate low-level VLA policies in long-horizon tasks.",
    "summary": "",
    "translation": "GraSP-VLA：基于图的符号化动作表示，用于VLA策略的长程规划",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言动作（VLA）策略的长程规划，属于机器人学和具身AI领域。虽然涉及多模态建模（视觉+语言），但其核心是机器人动作规划和符号推理，与推荐系统、搜索或广告中的异构数据处理没有直接关联。论文的技术重点（图基符号表示、长程规划）在RecSys/Search/Ads领域缺乏明确的应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04349v1": {
    "title": "A MATLAB tutorial on deep feature extraction combined with chemometrics for analytical applications",
    "url": "https://www.alphaxiv.org/abs/2511.04349v1",
    "arxiv_id": "2511.04349v1",
    "authors": "Puneet Mishra, Martijntje Vollebregt, Yizhou Ma, Maria Font-i-Furnols",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 13:29:55",
    "ori_summary": "Background In analytical chemistry, spatial information about materials is commonly captured through imaging techniques, such as traditional color cameras or with advanced hyperspectral cameras and microscopes. However, efficiently extracting and analyzing this spatial information for exploratory and predictive purposes remains a challenge, especially when using traditional chemometric methods. Recent advances in deep learning and artificial intelligence have significantly enhanced image processing capabilities, enabling the extraction of multiscale deep features that are otherwise challenging to capture with conventional image processing techniques. Despite the wide availability of open-source deep learning models, adoption in analytical chemistry remains limited because of the absence of structured, step-by-step guidance for implementing these models. Results This tutorial aims to bridge this gap by providing a step-by-step guide for applying deep learning approaches to extract spatial information from imaging data and integrating it with other data sources, such as spectral information. Importantly, the focus of this work is not on training deep learning models for image processing but on using existing open source models to extract deep features from imaging data. Significance The tutorial provides MATLAB code tutorial demonstrations, showcasing the processing of imaging data from various imaging modalities commonly encountered in analytical chemistry. Readers must run the tutorial steps on their own datasets using the codes presented in this tutorial.",
    "summary": "",
    "translation": "结合化学计量学用于分析应用的深度特征提取MATLAB教程",
    "relevance_score": 1,
    "reasoning": "该论文专注于化学计量学和分析应用的深度特征提取，属于化学/分析化学领域特定应用，与推荐系统、搜索或广告的核心技术无关。论文内容涉及MATLAB教程和化学分析，完全落在指定的无关主题范围内，没有任何潜在的应用于RecSys/Search/Ads的可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04347v1": {
    "title": "Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection",
    "url": "https://www.alphaxiv.org/abs/2511.04347v1",
    "arxiv_id": "2511.04347v1",
    "authors": "Sanjay Kumar, Tim Brophy, Eoin Martino Grua, Ganesh Sistu, Valentina Donzella, Ciaran Eising",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 13:25:18",
    "ori_summary": "Accurate 3D object detection is essential for automated vehicles to navigate safely in complex real-world environments. Bird's Eye View (BEV) representations, which project multi-sensor data into a top-down spatial format, have emerged as a powerful approach for robust perception. Although BEV-based fusion architectures have demonstrated strong performance through multimodal integration, the effects of sensor occlusions, caused by environmental conditions such as fog, haze, or physical obstructions, on 3D detection accuracy remain underexplored. In this work, we investigate the impact of occlusions on both camera and Light Detection and Ranging (LiDAR) outputs using the BEVFusion architecture, evaluated on the nuScenes dataset. Detection performance is measured using mean Average Precision (mAP) and the nuScenes Detection Score (NDS). Our results show that moderate camera occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is based only on the camera. On the other hand, LiDAR sharply drops in performance only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%), with a severe impact on long-range detection. In fused settings, the effect depends on which sensor is occluded: occluding the camera leads to a minor 4.1% drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8% drop (to 50.1%), revealing the model's stronger reliance on LiDAR for the task of 3D object detection. Our results highlight the need for future research into occlusion-aware evaluation methods and improved sensor fusion techniques that can maintain detection accuracy in the presence of partial sensor failure or degradation due to adverse environmental conditions.",
    "summary": "",
    "translation": "评估天气引起的传感器遮挡对BEVFusion在3D目标检测中的影响",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于3D视觉和自动驾驶领域的传感器融合技术，属于纯粹的计算机视觉研究。虽然BEVFusion是先进的感知架构，但论文关注的是天气条件对传感器性能的影响，与推荐系统、搜索或广告领域没有任何直接或间接的应用关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04344v1": {
    "title": "Comparative Study of CNN Architectures for Binary Classification of Horses and Motorcycles in the VOC 2008 Dataset",
    "url": "https://www.alphaxiv.org/abs/2511.04344v1",
    "arxiv_id": "2511.04344v1",
    "authors": "Muhammad Annas Shaikh, Hamza Zaman, Arbaz Asif",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 13:24:07",
    "ori_summary": "This paper presents a comprehensive evaluation of nine convolutional neural network architectures for binary classification of horses and motorcycles in the VOC 2008 dataset. We address the significant class imbalance problem by implementing minority-class augmentation techniques. Our experiments compare modern architectures including ResNet-50, ConvNeXt-Tiny, DenseNet-121, and Vision Transformer across multiple performance metrics. Results demonstrate substantial performance variations, with ConvNeXt-Tiny achieving the highest Average Precision (AP) of 95.53% for horse detection and 89.12% for motorcycle detection. We observe that data augmentation significantly improves minority class detection, particularly benefiting deeper architectures. This study provides insights into architecture selection for imbalanced binary classification tasks and quantifies the impact of data augmentation strategies in mitigating class imbalance issues in object detection.",
    "summary": "",
    "translation": "基于VOC 2008数据集的马与摩托车二分类CNN架构对比研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于纯粹的计算机视觉任务（马与摩托车分类），使用特定领域数据集VOC 2008。虽然涉及CNN架构比较，但这属于纯粹的视觉分类研究，与推荐系统、搜索或广告的核心技术没有明显关联。论文没有展示任何在异构数据处理、序列建模或推荐相关应用方面的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04334v1": {
    "title": "Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography",
    "url": "https://www.alphaxiv.org/abs/2511.04334v1",
    "arxiv_id": "2511.04334v1",
    "authors": "Saúl Alonso-Monsalve, Leigh H. Whitehead, Adam Aurisano, Lorena Escudero Sanchez",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-11-06 13:17:16",
    "ori_summary": "The accurate delineation of tumours in radiological images like Computed Tomography is a very specialised and time-consuming task, and currently a bottleneck preventing quantitative analyses to be performed routinely in the clinical setting. For this reason, developing methods for the automated segmentation of tumours in medical imaging is of the utmost importance and has driven significant efforts in recent years. However, challenges regarding the impracticality of 3D scans, given the large amount of voxels to be analysed, usually requires the downsampling of such images or using patches thereof when applying traditional convolutional neural networks. To overcome this problem, in this paper we propose a new methodology that uses, divided into two stages, voxel sparsification and submanifold sparse convolutional networks. This method allows segmentations to be performed with high-resolution inputs and a native 3D model architecture, obtaining state-of-the-art accuracies while significantly reducing the computational resources needed in terms of GPU memory and time. We studied the deployment of this methodology in the context of Computed Tomography images of renal cancer patients from the KiTS23 challenge, and our method achieved results competitive with the challenge winners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7% for tumours + cysts, and 80.3% for tumours alone. Crucially, our method also offers significant computational improvements, achieving up to a 60% reduction in inference time and up to a 75\\% reduction in VRAM usage compared to an equivalent dense architecture, across both CPU and various GPU cards tested.",
    "summary": "",
    "translation": "用于计算机断层扫描中肾脏和肾脏肿瘤自动3D分割的子流形稀疏卷积网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像分割，属于医学/生物学领域的特定应用，与推荐系统、搜索或广告完全无关。论文涉及3D分割和CT扫描分析，这些技术没有明显的推荐系统、搜索或广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04317v1": {
    "title": "RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation",
    "url": "https://www.alphaxiv.org/abs/2511.04317v1",
    "arxiv_id": "2511.04317v1",
    "authors": "Xiangjun Zhang, Litong Gong, Yinglin Zheng, Yansong Liu, Wentao Jiang, Mingyi Xu, Biao Wang, Tiezheng Ge, Ming Zeng",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 12:42:03",
    "ori_summary": "Most text-to-video(T2V) diffusion models depend on pre-trained text encoders for semantic alignment, yet they often fail to maintain video quality when provided with concise prompts rather than well-designed ones. The primary issue lies in their limited textual semantics understanding. Moreover, these text encoders cannot rephrase prompts online to better align with user intentions, which limits both the scalability and usability of the models, To address these challenges, we introduce RISE-T2V, which uniquely integrates the processes of prompt rephrasing and semantic feature extraction into a single and seamless step instead of two separate steps. RISE-T2V is universal and can be applied to various pre-trained LLMs and video diffusion models(VDMs), significantly enhancing their capabilities for T2V tasks. We propose an innovative module called the Rephrasing Adapter, enabling diffusion models to utilize text hidden states during the next token prediction of the LLM as a condition for video generation. By employing a Rephrasing Adapter, the video generation model can implicitly rephrase basic prompts into more comprehensive representations that better match the user's intent. Furthermore, we leverage the powerful capabilities of LLMs to enable video generation models to accomplish a broader range of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a versatile framework applicable to different video diffusion model architectures, significantly enhancing the ability of T2V models to generate high-quality videos that align with user intent. Visual results are available on the webpage at https://rise-t2v.github.io.",
    "summary": "",
    "translation": "RISE-T2V：利用大语言模型进行语义重述与注入以实现扩展性文本到视频生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本到视频生成，属于纯粹的视觉内容生成领域。虽然使用了LLM技术，但其应用方向是视频生成而非推荐系统、搜索或广告中的排名任务。该研究不涉及用户行为建模、项目推荐或查询理解等核心RecSys/Search/Ads问题，也没有展示在异构数据统一建模方面的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04304v1": {
    "title": "Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data",
    "url": "https://www.alphaxiv.org/abs/2511.04304v1",
    "arxiv_id": "2511.04304v1",
    "authors": "Robin Spanier, Thorsten Hoeser, Claudia Kuenzer",
    "categories": "cs.CV, cs.AI, eess.IV",
    "pub_date": "2025-11-06 12:13:53",
    "ori_summary": "The recent and ongoing expansion of marine infrastructure, including offshore wind farms, oil and gas platforms, artificial islands, and aquaculture facilities, highlights the need for effective monitoring systems. The development of robust models for offshore infrastructure detection relies on comprehensive, balanced datasets, but falls short when samples are scarce, particularly for underrepresented object classes, shapes, and sizes. By training deep learning-based YOLOv10 object detection models with a combination of synthetic and real Sentinel-1 satellite imagery acquired in the fourth quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of Guinea, and Coast of Brazil), this study investigates the use of synthetic training data to enhance model performance. We evaluated this approach by applying the model to detect offshore platforms in three unseen regions (Gulf of Mexico, North Sea, Persian Gulf) and thereby assess geographic transferability. This region-holdout evaluation demonstrated that the model generalises beyond the training areas. In total, 3,529 offshore platforms were detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and 1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which improved to 0.90 upon incorporating synthetic data. We analysed how synthetic data enhances the representation of unbalanced classes and overall model performance, taking a first step toward globally transferable detection of offshore infrastructure. This study underscores the importance of balanced datasets and highlights synthetic data generation as an effective strategy to address common challenges in remote sensing, demonstrating the potential of deep learning for scalable, global offshore infrastructure monitoring.",
    "summary": "",
    "translation": "基于深度学习的Sentinel-1影像海上平台目标检测及合成训练数据影响研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于遥感图像中的目标检测，属于纯粹的计算机视觉应用领域。虽然涉及深度学习技术，但其应用场景（海上平台检测、卫星图像）与推荐系统、搜索或广告领域没有任何直接关联，也不涉及Transformer架构、LLM技术或异构数据统一建模等核心关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04288v1": {
    "title": "Vision Foundation Models in Agriculture: Toward Domain-Specific Adaptation for Weed Herbicide Trials Assessment",
    "url": "https://www.alphaxiv.org/abs/2511.04288v1",
    "arxiv_id": "2511.04288v1",
    "authors": "Leire Benito-Del-Valle, Artzai Picón, Daniel Mugica, Manuel Ramos, Eva Portillo, Javier Romero, Carlos Javier Jimenez, Ramón Navarra-Mestre",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 11:30:32",
    "ori_summary": "Herbicide field trials require accurate identification of plant species and assessment of herbicide-induced damage across diverse environments. While general-purpose vision foundation models have shown promising results in complex visual domains, their performance can be limited in agriculture, where fine-grained distinctions between species and damage types are critical. In this work, we adapt a general-purpose vision foundation model to herbicide trial characterization. Trained using a self-supervised learning approach on a large, curated agricultural dataset, the model learns rich and transferable representations optimized for herbicide trials images. Our domain-specific model significantly outperforms the best general-purpose foundation model in both species identification (F1 score improvement from 0.91 to 0.94) and damage classification (from 0.26 to 0.33). Under unseen conditions (new locations and other time), it achieves even greater gains (species identification from 0.56 to 0.66; damage classification from 0.17 to 0.27). In domain-shift scenarios, such as drone imagery, it maintains strong performance (species classification from 0.49 to 0.60). Additionally, we show that domain-specific pretraining enhances segmentation accuracy, particularly in low-annotation regimes. An annotation-efficiency analysis reveals that, under unseen conditions, the domain-specific model achieves 5.4% higher F1 score than the general-purpose model, while using 80% fewer labeled samples. These results demonstrate the generalization capabilities of domain-specific foundation models and their potential to significantly reduce manual annotation efforts, offering a scalable and automated solution for herbicide trial analysis.",
    "summary": "",
    "translation": "视觉基础模型在农业领域的应用：面向杂草除草剂试验评估的领域特定适应",
    "relevance_score": 1,
    "reasoning": "该论文专注于农业领域的视觉基础模型应用，特别是杂草除草剂试验评估，这属于明确的领域特定应用。虽然涉及基础模型，但其应用场景与搜索、推荐、广告系统完全无关，且农业领域被明确列为无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04283v1": {
    "title": "FastGS: Training 3D Gaussian Splatting in 100 Seconds",
    "url": "https://www.alphaxiv.org/abs/2511.04283v1",
    "arxiv_id": "2511.04283v1",
    "authors": "Shiwei Ren, Tianci Wen, Yongchun Fang, Biao Lu",
    "categories": "cs.CV, 68T40(Primary)68T45, 68U99 (Secondary), I.4.8; I.3.7",
    "pub_date": "2025-11-06 11:21:16",
    "ori_summary": "The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to properly regulate the number of Gaussians during training, causing redundant computational time overhead. In this paper, we propose FastGS, a novel, simple, and general acceleration framework that fully considers the importance of each Gaussian based on multi-view consistency, efficiently solving the trade-off between training time and rendering quality. We innovatively design a densification and pruning strategy based on multi-view consistency, dispensing with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets demonstrate that our method significantly outperforms the state-of-the-art methods in training speed, achieving a 3.32$\\times$ training acceleration and comparable rendering quality compared with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\\times$ acceleration compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that FastGS exhibits strong generality, delivering 2-7$\\times$ training acceleration across various tasks, including dynamic scene reconstruction, surface reconstruction, sparse-view reconstruction, large-scale reconstruction, and simultaneous localization and mapping. The project page is available at https://fastgs.github.io/",
    "summary": "",
    "translation": "FastGS：在100秒内训练3D高斯泼溅",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于3D计算机视觉中的高斯泼溅技术训练加速，属于纯粹的视觉领域研究。该技术没有明确的推荐系统、搜索或广告应用潜力，与当前关注的LLM技术、推荐系统核心进展或Transformer架构改进完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04281v1": {
    "title": "DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification",
    "url": "https://www.alphaxiv.org/abs/2511.04281v1",
    "arxiv_id": "2511.04281v1",
    "authors": "Yujie Yang, Shuang Li, Jun Ye, Neng Dong, Fan Li, Huafeng Li",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 11:21:13",
    "ori_summary": "Video-based Visible-Infrared person re-identification (VVI-ReID) aims to retrieve the same pedestrian across visible and infrared modalities from video sequences. Existing methods tend to exploit modality-invariant visual features but largely overlook gait features, which are not only modality-invariant but also rich in temporal dynamics, thus limiting their ability to model the spatiotemporal consistency essential for cross-modal video matching. To address these challenges, we propose a DINOv2-Driven Gait Representation Learning (DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn gait features complementary to appearance cues, facilitating robust sequence-level representations for cross-modal retrieval. Specifically, we introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which generates and enhances silhouette representations with general-purpose semantic priors from DINOv2 and jointly optimizes them with the ReID objective to achieve semantically enriched and task-adaptive gait feature learning. Furthermore, we develop a Progressive Bidirectional Multi-Granularity Enhancement (PBMGE) module, which progressively refines feature representations by enabling bidirectional interactions between gait and appearance streams across multiple spatial granularities, fully leveraging their complementarity to enhance global representations with rich local details and produce highly discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets demonstrate the superiority of our approach, significantly outperforming existing state-of-the-art methods.",
    "summary": "",
    "translation": "基于DINOv2驱动的步态表征学习用于视频可见光-红外行人重识别",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域中的行人重识别任务，使用DINOv2进行步态表征学习。虽然涉及表征学习技术，但其应用场景（视频可见光-红外行人重识别）与推荐系统、搜索或广告领域没有直接关联，且不涉及Transformer架构的效率改进或异构数据统一建模等核心关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04260v1": {
    "title": "Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery",
    "url": "https://www.alphaxiv.org/abs/2511.04260v1",
    "arxiv_id": "2511.04260v1",
    "authors": "Claudio Giusti, Luca Guarnera, Sebastiano Battiato",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-06 10:51:11",
    "ori_summary": "The growing sophistication of synthetic image and deepfake generation models has turned source attribution and authenticity verification into a critical challenge for modern computer vision systems. Recent studies suggest that diffusion pipelines unintentionally imprint persistent statistical traces, known as signal leaks, within their outputs, particularly in latent representations. Building on this observation, we propose Proto-LeakNet, a signal-leak-aware and interpretable attribution framework that integrates closed-set classification with a density-based open-set evaluation on the learned embeddings, enabling analysis of unseen generators without retraining. Operating in the latent domain of diffusion models, our method re-simulates partial forward diffusion to expose residual generator-specific cues. A temporal attention encoder aggregates multi-step latent features, while a feature-weighted prototype head structures the embedding space and enables transparent attribution. Trained solely on closed data and achieving a Macro AUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under post-processing, surpassing state-of-the-art methods, and achieves strong separability between known and unseen generators. These results demonstrate that modeling signal-leak bias in latent space enables reliable and interpretable AI-image and deepfake forensics. The code for the whole work will be available upon submission.",
    "summary": "",
    "translation": "Proto-LeakNet：面向合成人脸图像中信号泄漏感知的归因方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于合成人脸图像中的信号泄漏检测和归因问题，这属于计算机视觉和图像取证领域。虽然提到了合成数据，但核心关注点是图像真实性检测和信号泄漏分析，与推荐系统、搜索或广告中的核心进展、LLM技术或Transformer架构改进没有直接关联。该技术主要面向图像安全检测，属于被排除的视觉相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04255v1": {
    "title": "MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection",
    "url": "https://www.alphaxiv.org/abs/2511.04255v1",
    "arxiv_id": "2511.04255v1",
    "authors": "Marawan Elbatel, Anbang Wang, Keyuan Liu, Kaouther Mouheb, Enrique Almar-Munoz, Lizhuo Lin, Yanqi Yang, Karim Lekadir, Xiaomeng Li",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-11-06 10:45:49",
    "ori_summary": "This paper does not introduce a novel architecture; instead, it revisits a fundamental yet overlooked baseline: adapting human-centric foundation models for anatomical landmark detection in medical imaging. While landmark detection has traditionally relied on domain-specific models, the emergence of large-scale pre-trained vision models presents new opportunities. In this study, we investigate the adaptation of Sapiens, a human-centric foundation model designed for pose estimation, to medical imaging through multi-dataset pretraining, establishing a new state of the art across multiple datasets. Our proposed model, MedSapiens, demonstrates that human-centric foundation models, inherently optimized for spatial pose localization, provide strong priors for anatomical landmark detection, yet this potential has remained largely untapped. We benchmark MedSapiens against existing state-of-the-art models, achieving up to 5.26% improvement over generalist models and up to 21.81% improvement over specialist models in the average success detection rate (SDR). To further assess MedSapiens adaptability to novel downstream tasks with few annotations, we evaluate its performance in limited-data settings, achieving 2.69% improvement over the few-shot state of the art in SDR. Code and model weights are available at https://github.com/xmed-lab/MedSapiens .",
    "summary": "",
    "translation": "MedSapiens：采用姿态重新思考医学影像标志点检测",
    "relevance_score": 1,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false,
    "relevance_reasoning": "该论文标题明确聚焦于医学影像领域的具体应用（医学标志点检测），这属于明确的无关主题范畴。尽管提到了'姿态'这一技术概念，但整个研究背景和应用场景完全限定在医疗领域，与推荐系统、搜索或广告没有任何技术关联或潜在应用可能性。"
  },
  "2511.04192v1": {
    "title": "AStF: Motion Style Transfer via Adaptive Statistics Fusor",
    "url": "https://www.alphaxiv.org/abs/2511.04192v1",
    "arxiv_id": "2511.04192v1",
    "authors": "Hanmo Chen, Chenghao Xu, Jiexi Yan, Cheng Deng",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-06 08:51:24",
    "ori_summary": "Human motion style transfer allows characters to appear less rigidity and more realism with specific style. Traditional arbitrary image style transfer typically process mean and variance which is proved effective. Meanwhile, similar methods have been adapted for motion style transfer. However, due to the fundamental differences between images and motion, relying on mean and variance is insufficient to fully capture the complex dynamic patterns and spatiotemporal coherence properties of motion data. Building upon this, our key insight is to bring two more coefficient, skewness and kurtosis, into the analysis of motion style. Specifically, we propose a novel Adaptive Statistics Fusor (AStF) which consists of Style Disentanglement Module (SDM) and High-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in conjunction with a Motion Consistency Regularization (MCR) discriminator. Experimental results show that, by providing a more comprehensive model of the spatiotemporal statistical patterns inherent in dynamic styles, our proposed AStF shows proficiency superiority in motion style transfers over state-of-the-arts. Our code and model are available at https://github.com/CHMimilanlan/AStF.",
    "summary": "",
    "translation": "AStF：通过自适应统计融合器实现运动风格迁移",
    "relevance_score": 1,
    "reasoning": "该论文涉及运动风格迁移，属于计算机视觉和图形学领域，与推荐系统、搜索或广告的核心技术无关。论文标题表明其专注于动作/运动数据的风格转换，没有显示出在异构数据处理、Transformer架构或LLM技术方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04190v1": {
    "title": "Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification",
    "url": "https://www.alphaxiv.org/abs/2511.04190v1",
    "arxiv_id": "2511.04190v1",
    "authors": "Josef Mayr, Anna Reithmeir, Maxime Di Folco, Julia A. Schnabel",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 08:49:25",
    "ori_summary": "Covariance descriptors capture second-order statistics of image features. They have shown strong performance in general computer vision tasks, but remain underexplored in medical imaging. We investigate their effectiveness for both conventional and learning-based medical image classification, with a particular focus on SPDNet, a classification network specifically designed for symmetric positive definite (SPD) matrices. We propose constructing covariance descriptors from features extracted by pre-trained general vision encoders (GVEs) and comparing them with handcrafted descriptors. Two GVEs - DINOv2 and MedSAM - are evaluated across eleven binary and multi-class datasets from the MedMNSIT benchmark. Our results show that covariance descriptors derived from GVE features consistently outperform those derived from handcrafted features. Moreover, SPDNet yields superior performance to state-of-the-art methods when combined with DINOv2 features. Our findings highlight the potential of combining covariance descriptors with powerful pretrained vision encoders for medical image analysis.",
    "summary": "",
    "translation": "协方差描述符遇见通用视觉编码器：面向医学图像分类的黎曼深度学习",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分类这一明确被排除的领域，与推荐系统、搜索或广告无关。虽然提到了通用视觉编码器，但应用场景仅限于医疗领域，没有任何潜在的应用于推荐系统、搜索或广告的路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04171v1": {
    "title": "Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology",
    "url": "https://www.alphaxiv.org/abs/2511.04171v1",
    "arxiv_id": "2511.04171v1",
    "authors": "Fatemehzahra Darzi, Rodrigo Escobar Diaz Guerrero, Thomas Bocklitz",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-06 08:22:44",
    "ori_summary": "Image registration refers to the process of spatially aligning two or more images by mapping them into a common coordinate system, so that corresponding anatomical or tissue structures are matched across images. In digital pathology, registration enables direct comparison and integration of information from different stains or imaging modalities, sup-porting applications such as biomarker analysis and tissue reconstruction. Accurate registration of images from different modalities is an essential step in digital pathology. In this study, we investigated how various color transformation techniques affect image registration between hematoxylin and eosin (H&E) stained images and non-linear multimodal images. We used a dataset of 20 tissue sample pairs, with each pair undergoing several preprocessing steps, including different color transformation (CycleGAN, Macenko, Reinhard, Vahadane), inversion, contrast adjustment, intensity normalization, and denoising. All images were registered using the VALIS registration method, which first applies rigid registration and then performs non-rigid registration in two steps on both low and high-resolution images. Registration performance was evaluated using the relative Target Registration Error (rTRE). We reported the median of median rTRE values (MMrTRE) and the average of median rTRE values (AMrTRE) for each method. In addition, we performed a custom point-based evaluation using ten manually selected key points. Registration was done separately for two scenarios, using either the original or inverted multimodal images. In both scenarios, CycleGAN color transformation achieved the lowest registration errors, while the other methods showed higher errors. These findings show that applying color transformation before registration improves alignment between images from different modalities and supports more reliable analysis in digital pathology.",
    "summary": "",
    "translation": "数字病理学中精确图像配准的预处理技术系统评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于数字病理学中的图像配准预处理技术，这是一个纯粹的计算机视觉/医学影像处理主题。虽然图像配准技术本身有广泛应用，但论文明确限定在数字病理学领域，这与医疗应用直接相关，属于明确排除的领域。没有证据表明该技术有直接的推荐系统、搜索或广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04137v1": {
    "title": "Learning from Online Videos at Inference Time for Computer-Use Agents",
    "url": "https://www.alphaxiv.org/abs/2511.04137v1",
    "arxiv_id": "2511.04137v1",
    "authors": "Yujian Liu, Ze Wang, Hao Chen, Ximeng Sun, Xiaodong Yu, Jialian Wu, Jiang Liu, Emad Barsoum, Zicheng Liu, Shiyu Chang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-06 07:29:02",
    "ori_summary": "Computer-use agents can operate computers and automate laborious tasks, but despite recent rapid progress, they still lag behind human users, especially when tasks require domain-specific procedural knowledge about particular applications, platforms, and multi-step workflows. Humans can bridge this gap by watching video tutorials: we search, skim, and selectively imitate short segments that match our current subgoal. In this paper, we study how to enable computer-use agents to learn from online videos at inference time effectively. We propose a framework that retrieves and filters tutorial videos, converts them into structured demonstration trajectories, and dynamically selects trajectories as in-context guidance during execution. Particularly, using a VLM, we infer UI actions, segment videos into short subsequences of actions, and assign each subsequence a textual objective. At inference time, a two-stage selection mechanism dynamically chooses a single trajectory to add in context at each step, focusing the agent on the most helpful local guidance for its next decision. Experiments on two widely used benchmarks show that our framework consistently outperforms strong base agents and variants that use only textual tutorials or transcripts. Analyses highlight the importance of trajectory segmentation and selection, action filtering, and visual information, suggesting that abundant online videos can be systematically distilled into actionable guidance that improves computer-use agents at inference time. Our code is available at https://github.com/UCSB-NLP-Chang/video_demo.",
    "summary": "",
    "translation": "在推理时从在线视频中学习用于计算机使用智能体",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机使用智能体从在线视频中学习，这属于特定领域的人机交互应用，与推荐系统、搜索或广告的核心技术没有直接关联。虽然提到了推理时学习的概念，但应用场景过于特定（计算机使用智能体），缺乏在推荐、搜索或广告领域的明确应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04128v1": {
    "title": "DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms",
    "url": "https://www.alphaxiv.org/abs/2511.04128v1",
    "arxiv_id": "2511.04128v1",
    "authors": "Shengyu Tang, Zeyuan Lu, Jiazhi Dong, Changdong Yu, Xiaoyu Wang, Yaohui Lyu, Weihao Xia",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-06 07:20:36",
    "ori_summary": "Accurate perception of the marine environment through robust multi-object tracking (MOT) is essential for ensuring safe vessel navigation and effective maritime surveillance. However, the complicated maritime environment often causes camera motion and subsequent visual degradation, posing significant challenges to MOT. To address this challenge, we propose an efficient Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the framework is a parallel tracker with affine compensation, which incorporates an object detection and re-identification (ReID) branch, along with a dedicated branch for dynamic camera motion estimation. Specifically, a Reversible Columnar Detection Network (RCDN) is integrated into the detection module to leverage multi-level visual features for robust object detection. Furthermore, a lightweight Transformer-based appearance extractor (Li-TAE) is designed to capture global contextual information and generate robust appearance features. Another branch decouples platform-induced and target-intrinsic motion by constructing a projective transformation, applying platform-motion compensation within the Kalman filter, and thereby stabilizing true object trajectories. Finally, a clustering-optimized feature fusion module effectively combines motion and appearance cues to ensure identity consistency under noise, occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT attains the fastest runtime among existing ReID-based MOT frameworks while maintaining high identity consistency and robustness to jitter and occlusion. Code is available at: https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-.",
    "summary": "",
    "translation": "DMSORT：一种用于无人船平台的高效并行海上多目标跟踪架构",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的多目标跟踪技术，应用于无人船平台的海洋场景。这与推荐系统、搜索或广告的核心技术领域完全无关，也不涉及LLM、Transformer架构或异构数据建模等使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04126v1": {
    "title": "Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)",
    "url": "https://www.alphaxiv.org/abs/2511.04126v1",
    "arxiv_id": "2511.04126v1",
    "authors": "Venkata Manikanta Desu, Syed Fawaz Ali",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-06 07:18:54",
    "ori_summary": "This study presents a complete pipeline for automated tennis match analysis. Our framework integrates multiple deep learning models to detect and track players and the tennis ball in real time, while also identifying court keypoints for spatial reference. Using YOLOv8 for player detection, a custom-trained YOLOv5 model for ball tracking, and a ResNet50-based architecture for court keypoint detection, our system provides detailed analytics including player movement patterns, ball speed, shot accuracy, and player reaction times. The experimental results demonstrate robust performance in varying court conditions and match scenarios. The model outputs an annotated video along with detailed performance metrics, enabling coaches, broadcasters, and players to gain actionable insights into the dynamics of the game.",
    "summary": "",
    "translation": "基于球场关键点检测的自动化网球运动员与球体追踪（鹰眼系统）",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的体育视频分析，涉及运动员和球体追踪技术。虽然标题提到追踪系统，但这属于纯粹的视觉应用领域，与推荐系统、搜索或广告的核心技术没有直接关联。该技术缺乏在RecSys/Search/Ads领域的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04123v1": {
    "title": "Text to Sketch Generation with Multi-Styles",
    "url": "https://www.alphaxiv.org/abs/2511.04123v1",
    "arxiv_id": "2511.04123v1",
    "authors": "Tengjie Li, Shikui Tu, Lei Xu",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 07:13:56",
    "ori_summary": "Recent advances in vision-language models have facilitated progress in sketch generation. However, existing specialized methods primarily focus on generic synthesis and lack mechanisms for precise control over sketch styles. In this work, we propose a training-free framework based on diffusion models that enables explicit style guidance via textual prompts and referenced style sketches. Unlike previous style transfer methods that overwrite key and value matrices in self-attention, we incorporate the reference features as auxiliary information with linear smoothing and leverage a style-content guidance mechanism. This design effectively reduces content leakage from reference sketches and enhances synthesis quality, especially in cases with low structural similarity between reference and target sketches. Furthermore, we extend our framework to support controllable multi-style generation by integrating features from multiple reference sketches, coordinated via a joint AdaIN module. Extensive experiments demonstrate that our approach achieves high-quality sketch generation with accurate style alignment and improved flexibility in style control. The official implementation of M3S is available at https://github.com/CMACH508/M3S.",
    "summary": "",
    "translation": "多风格文本到草图生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本到草图的生成任务，属于纯粹的视觉内容生成领域。虽然标题提到多风格生成，但这与推荐系统、搜索或广告中的核心排名、检索或用户建模问题没有直接关联。该技术缺乏在RecSys/Search/Ads领域的明显应用场景，属于无关的视觉生成主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04117v1": {
    "title": "Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration",
    "url": "https://www.alphaxiv.org/abs/2511.04117v1",
    "arxiv_id": "2511.04117v1",
    "authors": "Yunghee Lee, Byeonghyun Pak, Junwha Hong, Hoseong Kim",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 07:08:58",
    "ori_summary": "In this paper, we propose Tortoise and Hare Guidance (THG), a training-free strategy that accelerates diffusion sampling while maintaining high-fidelity generation. We demonstrate that the noise estimate and the additional guidance term exhibit markedly different sensitivity to numerical error by reformulating the classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our error-bound analysis shows that the additional guidance branch is more robust to approximation, revealing substantial redundancy that conventional solvers fail to exploit. Building on this insight, THG significantly reduces the computation of the additional guidance: the noise estimate is integrated with the tortoise equation on the original, fine-grained timestep grid, while the additional guidance is integrated with the hare equation only on a coarse grid. We also introduce (i) an error-bound-aware timestep sampler that adaptively selects step sizes and (ii) a guidance-scale scheduler that stabilizes large extrapolation spans. THG reduces the number of function evaluations (NFE) by up to 30% with virtually no loss in generation fidelity ($\\Delta$ImageReward $\\leq$ 0.032) and outperforms state-of-the-art CFG-based training-free accelerators under identical computation budgets. Our findings highlight the potential of multirate formulations for diffusion solvers, paving the way for real-time high-quality image synthesis without any model retraining. The source code is available at https://github.com/yhlee-add/THG.",
    "summary": "",
    "translation": "龟兔引导：通过多速率积分加速扩散模型推理",
    "relevance_score": 2,
    "reasoning": "该论文专注于扩散模型的推理加速技术，属于生成模型优化领域。虽然扩散模型在AIGC和内容生成中有广泛应用，但论文标题没有显示与推荐系统、搜索或广告的直接关联，也没有提到处理异构数据或多模态建模。因此，该技术对当前关注领域的潜在应用不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04112v1": {
    "title": "SpatialLock: Precise Spatial Control in Text-to-Image Synthesis",
    "url": "https://www.alphaxiv.org/abs/2511.04112v1",
    "arxiv_id": "2511.04112v1",
    "authors": "Biao Liu, Yuanzhi Liang",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 06:51:55",
    "ori_summary": "Text-to-Image (T2I) synthesis has made significant advancements in recent years, driving applications such as generating datasets automatically. However, precise control over object localization in generated images remains a challenge. Existing methods fail to fully utilize positional information, leading to an inadequate understanding of object spatial layouts. To address this issue, we propose SpatialLock, a novel framework that leverages perception signals and grounding information to jointly control the generation of spatial locations. SpatialLock incorporates two components: Position-Engaged Injection (PoI) and Position-Guided Learning (PoG). PoI directly integrates spatial information through an attention layer, encouraging the model to learn the grounding information effectively. PoG employs perception-based supervision to further refine object localization. Together, these components enable the model to generate objects with precise spatial arrangements and improve the visual quality of the generated images. Experiments show that SpatialLock sets a new state-of-the-art for precise object positioning, achieving IOU scores above 0.9 across multiple datasets.",
    "summary": "",
    "translation": "SpatialLock：文本到图像合成中的精确空间控制",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本到图像合成的空间控制技术，属于纯粹的视觉内容生成领域。虽然标题涉及合成技术，但没有任何迹象表明该工作与推荐系统、搜索或广告的排名任务相关，也不涉及LLM在RecSys/Search/Ads中的直接应用或Transformer架构的改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04084v1": {
    "title": "When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation",
    "url": "https://www.alphaxiv.org/abs/2511.04084v1",
    "arxiv_id": "2511.04084v1",
    "authors": "Nishchal Sapkota, Haoyan Shi, Yejia Zhang, Xianshi Ma, Bofang Zheng, Danny Z. Chen",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 05:44:57",
    "ori_summary": "Medical image segmentation is critical for accurate diagnostics and treatment planning, but remains challenging due to complex anatomical structures and limited annotated training data. CNN-based segmentation methods excel at local feature extraction, but struggle with modeling long-range dependencies. Transformers, on the other hand, capture global context more effectively, but are inherently data-hungry and computationally expensive. In this work, we introduce UKAST, a U-Net like architecture that integrates rational-function based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By leveraging rational base functions and Group Rational KANs (GR-KANs) from the Kolmogorov-Arnold Transformer (KAT), our architecture addresses the inefficiencies of vanilla spline-based KANs, yielding a more expressive and data-efficient framework with reduced FLOPs and only a very small increase in parameter count compared to SwinUNETR. UKAST achieves state-of-the-art performance on four diverse 2D and 3D medical image segmentation benchmarks, consistently surpassing both CNN- and Transformer-based baselines. Notably, it attains superior accuracy in data-scarce settings, alleviating the data-hungry limitations of standard Vision Transformers. These results show the potential of KAN-enhanced Transformers to advance data-efficient medical image segmentation. Code is available at: https://github.com/nsapkota417/UKAST",
    "summary": "",
    "translation": "当Swin Transformer遇见KAN：一种改进的用于医学图像分割的Transformer架构",
    "relevance_score": 2,
    "reasoning": "该论文专注于医学图像分割这一特定领域应用，属于明确的无关主题范畴。虽然提到了Transformer架构改进，但其应用场景（医学图像）与推荐系统、搜索或广告领域没有直接关联，且没有证据表明该架构改进具有跨领域应用的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04083v1": {
    "title": "Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score",
    "url": "https://www.alphaxiv.org/abs/2511.04083v1",
    "arxiv_id": "2511.04083v1",
    "authors": "Abu Hanif Muhammad Syarubany",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 05:42:26",
    "ori_summary": "We study CT image denoising in the unpaired and self-supervised regimes by evaluating two strong, training-data-efficient paradigms: a CycleGAN-based residual translator and a Noise2Score (N2S) score-matching denoiser. Under a common evaluation protocol, a configuration sweep identifies a simple standard U-Net backbone within CycleGAN (lambda_cycle = 30, lambda_iden = 2, ngf = ndf = 64) as the most reliable setting; we then train it to convergence with a longer schedule. The selected CycleGAN improves the noisy input from 34.66 dB / 0.9234 SSIM to 38.913 dB / 0.971 SSIM and attains an estimated score of 1.9441 and an unseen-set (Kaggle leaderboard) score of 1.9343. Noise2Score, while slightly behind in absolute PSNR / SSIM, achieves large gains over very noisy inputs, highlighting its utility when clean pairs are unavailable. Overall, CycleGAN offers the strongest final image quality, whereas Noise2Score provides a robust pair-free alternative with competitive performance. Source code is available at https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score.",
    "summary": "",
    "translation": "对抗性与基于分数的CT去噪：CycleGAN对比Noise2Score",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学CT图像去噪技术，属于计算机视觉在医疗领域的特定应用。虽然涉及对抗性学习和基于分数的生成模型，但这些技术在当前论文中仅应用于医学影像处理，与推荐系统、搜索或广告领域没有直接关联，也不符合任何当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04078v1": {
    "title": "Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment",
    "url": "https://www.alphaxiv.org/abs/2511.04078v1",
    "arxiv_id": "2511.04078v1",
    "authors": "Zehui Feng, Chenqi Zhang, Mingru Wang, Minuo Wei, Shiwei Cheng, Cuntai Guan, Ting Han",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 05:36:39",
    "ori_summary": "Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI remains a fundamental challenge due to subject variability and the entangled nature of visual features. Existing approaches primarily align neural activity directly with visual embeddings, but visual-only representations often fail to capture latent semantic dimensions, limiting interpretability and deep robustness. To address these limitations, we propose Bratrix, the first end-to-end framework to achieve multimodal Language-Anchored Vision-Brain alignment. Bratrix decouples visual stimuli into hierarchical visual and linguistic semantic components, and projects both visual and brain representations into a shared latent space, enabling the formation of aligned visual-language and brain-language embeddings. To emulate human-like perceptual reliability and handle noisy neural signals, Bratrix incorporates a novel uncertainty perception module that applies uncertainty-aware weighting during alignment. By leveraging learnable language-anchored semantic matrices to enhance cross-modal correlations and employing a two-stage training strategy of single-modality pretraining followed by multimodal fine-tuning, Bratrix-M improves alignment precision. Extensive experiments on EEG, MEG, and fMRI benchmarks demonstrate that Bratrix improves retrieval, reconstruction, and captioning performance compared to state-of-the-art methods, specifically surpassing 14.3% in 200-way EEG retrieval task. Code and model are available.",
    "summary": "",
    "translation": "揭示语言锚定的多模态视觉-大脑对齐中的深度语义不确定性感知",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉-大脑对齐和不确定性感知，属于视觉和神经科学交叉领域。虽然提到了多模态和语言锚定，但其核心焦点是视觉-大脑对齐，这与推荐系统、搜索或广告的直接应用关联较弱。潜在的间接应用可能包括理解用户对视觉内容的语义不确定性，但这在当前标题中不够明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04037v1": {
    "title": "A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals",
    "url": "https://www.alphaxiv.org/abs/2511.04037v1",
    "arxiv_id": "2511.04037v1",
    "authors": "Arfina Rahman, Mahesh Banavar",
    "categories": "cs.CV, eess.SP",
    "pub_date": "2025-11-06 04:16:13",
    "ori_summary": "Photoplethysmography (PPG) signals, which measure changes in blood volume in the skin using light, have recently gained attention in biometric authentication because of their non-invasive acquisition, inherent liveness detection, and suitability for low-cost wearable devices. However, PPG signal quality is challenged by motion artifacts, illumination changes, and inter-subject physiological variability, making robust feature extraction and classification crucial. This study proposes a lightweight and cost-effective biometric authentication framework based on PPG signals extracted from low-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings from 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The raw PPG signals undergo a standard preprocessing pipeline involving baseline drift removal, motion artifact suppression using Principal Component Analysis (PCA), bandpass filtering, Fourier-based resampling, and amplitude normalization. To generate robust representations, each one-dimensional PPG segment is converted into a two-dimensional time-frequency scalogram via the Continuous Wavelet Transform (CWT), effectively capturing transient cardiovascular dynamics. We developed a hybrid deep learning model, termed CVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision Transformer (CVT) and ConvMixer branches with temporal features from a Long Short-Term Memory network (LSTM). The experimental results on 46 subjects demonstrate an authentication accuracy of 98%, validating the robustness of the model to noise and variability between subjects. Due to its efficiency, scalability, and inherent liveness detection capability, the proposed system is well-suited for real-world mobile and embedded biometric security applications.",
    "summary": "",
    "translation": "基于低帧率光电容积脉搏波信号的鲁棒生物特征认证混合深度学习模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于生物特征认证技术，使用PPG信号进行身份验证，这属于安全认证领域而非推荐系统、搜索或广告的核心技术。虽然涉及深度学习模型，但其应用场景（生物认证）与当前关注的RecSys/Search/Ads领域没有直接关联，也不涉及LLM技术、Transformer架构或异构数据统一建模等核心研究方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04029v1": {
    "title": "Near-Lossless 3D Voxel Representation Free from Iso-surface",
    "url": "https://www.alphaxiv.org/abs/2511.04029v1",
    "arxiv_id": "2511.04029v1",
    "authors": "Yihao Luo, Xianglong He, Chuanyu Pan, Yiwen Chen, Jiaqi Wu, Yangguang Li, Wanli Ouyang, Yuanming Hu, Guang Yang, ChoonHwai Yap",
    "categories": "cs.CV, cs.GR",
    "pub_date": "2025-11-06 03:56:12",
    "ori_summary": "Accurate and efficient voxelized representations of 3D meshes are the foundation of 3D reconstruction and generation. However, existing representations based on iso-surface heavily rely on water-tightening or rendering optimization, which inevitably compromise geometric fidelity. We propose Faithful Contouring, a sparse voxelized representation that supports 2048+ resolutions for arbitrary meshes, requiring neither converting meshes to field functions nor extracting the isosurface during remeshing. It achieves near-lossless fidelity by preserving sharpness and internal structures, even for challenging cases with complex geometry and topology. The proposed method also shows flexibility for texturing, manipulation, and editing. Beyond representation, we design a dual-mode autoencoder for Faithful Contouring, enabling scalable and detail-preserving shape reconstruction. Extensive experiments show that Faithful Contouring surpasses existing methods in accuracy and efficiency for both representation and reconstruction. For direct representation, it achieves distance errors at the $10^{-5}$ level; for mesh reconstruction, it yields a 93\\% reduction in Chamfer Distance and a 35\\% improvement in F-score over strong baselines, confirming superior fidelity as a representation for 3D learning tasks.",
    "summary": "",
    "translation": "无需等值面的近无损三维体素表示",
    "relevance_score": 1,
    "reasoning": "该论文涉及3D体素表示和等值面技术，属于计算机图形学和3D视觉领域。根据用户关注点排除标准，纯粹的3D视觉论文与搜索、推荐或广告系统没有明确相关性，且不涉及LLM、Transformer架构或异构数据建模等核心技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04016v1": {
    "title": "MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging",
    "url": "https://www.alphaxiv.org/abs/2511.04016v1",
    "arxiv_id": "2511.04016v1",
    "authors": "Mahmoud Soliman, Islam Osman, Mohamed S. Shehata, Rasika Rajapakshe",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 03:28:56",
    "ori_summary": "The performance of vision models in medical imaging is often hindered by the prevailing paradigm of fine-tuning backbones pre-trained on out-of-domain natural images. To address this fundamental domain gap, we propose MedDChest, a new foundational Vision Transformer (ViT) model optimized specifically for thoracic imaging. We pre-trained MedDChest from scratch on a massive, curated, multimodal dataset of over 1.2 million images, encompassing different modalities including Chest X-ray and Computed Tomography (CT) compiled from 10 public sources. A core technical contribution of our work is Guided Random Resized Crops, a novel content-aware data augmentation strategy that biases sampling towards anatomically relevant regions, overcoming the inefficiency of standard cropping techniques on medical scans. We validate our model's effectiveness by fine-tuning it on a diverse set of downstream diagnostic tasks. Comprehensive experiments empirically demonstrate that MedDChest significantly outperforms strong, publicly available ImageNet-pretrained models. By establishing the superiority of large-scale, in-domain pre-training combined with domain-specific data augmentation, MedDChest provides a powerful and robust feature extractor that serves as a significantly better starting point for a wide array of thoracic diagnostic tasks. The model weights will be made publicly available to foster future research and applications.",
    "summary": "",
    "translation": "MedDChest：一种面向胸部影像的内容感知多模态基础视觉模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像领域的视觉模型开发，属于医疗生物医学应用范畴。尽管标题提到多模态和基础模型，但其特定应用于胸部影像诊断，与推荐系统、搜索或广告领域没有任何直接或潜在关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.04008v1": {
    "title": "GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization",
    "url": "https://www.alphaxiv.org/abs/2511.04008v1",
    "arxiv_id": "2511.04008v1",
    "authors": "Mahmoud Soliman, Omar Abdelaziz, Ahmed Radwan, Anand, Mohamed Shehata",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 03:16:08",
    "ori_summary": "Domain generalization (DG) seeks robust Vision Transformer (ViT) performance on unseen domains. Efficiently adapting pretrained ViTs for DG is challenging; standard fine-tuning is costly and can impair generalization. We propose GNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a Mixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead of token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT, SAGE) operates on inter-patch graphs to dynamically assign patches to specialized experts. This context-aware GNN routing leverages inter-patch relationships for better adaptation to domain shifts. GNN-MoE achieves state-of-the-art or competitive DG benchmark performance with high parameter efficiency, highlighting the utility of graph-based contextual routing for robust, lightweight DG.",
    "summary": "论文研究视觉Transformer在领域泛化中的参数高效微调问题，核心思想是利用图神经网络对图像块间关系进行建模，实现基于上下文的动态专家路由，替代传统的基于token的路由机制。",
    "translation": "GNN-MoE：使用图神经网络进行上下文感知的补丁路由，以实现参数高效的领域泛化",
    "relevance_score": 8,
    "reasoning": "该论文结合了Transformer架构中的MoE（专家混合）技术和图神经网络，专注于参数效率和领域泛化，这直接属于'Enabling Transformer Tech'范畴。在推荐系统和搜索领域，MoE架构可以显著提升模型容量和效率，而GNN能够有效处理用户-物品交互图结构，两者的结合有望为大规模推荐系统提供更高效、更泛化的建模能力。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文在Transformer架构效率优化和MoE专家路由机制方面有重要创新，其上下文感知的图神经网络路由方法对推荐系统中的序列建模和异构数据处理具有直接借鉴价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2511.03997v1": {
    "title": "PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection",
    "url": "https://www.alphaxiv.org/abs/2511.03997v1",
    "arxiv_id": "2511.03997v1",
    "authors": "Peiyao Wang, Weining Wang, Qi Li",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 02:40:57",
    "ori_summary": "Recent advances in text-to-video generation have achieved impressive perceptual quality, yet generated content often violates fundamental principles of physical plausibility - manifesting as implausible object dynamics, incoherent interactions, and unrealistic motion patterns. Such failures hinder the deployment of video generation models in embodied AI, robotics, and simulation-intensive domains. To bridge this gap, we propose PhysCorr, a unified framework for modeling, evaluating, and optimizing physical consistency in video generation. Specifically, we introduce PhysicsRM, the first dual-dimensional reward model that quantifies both intra-object stability and inter-object interactions. On this foundation, we develop PhyDPO, a novel direct preference optimization pipeline that leverages contrastive feedback and physics-aware reweighting to guide generation toward physically coherent outputs. Our approach is model-agnostic and scalable, enabling seamless integration into a wide range of video diffusion and transformer-based backbones. Extensive experiments across multiple benchmarks demonstrate that PhysCorr achieves significant improvements in physical realism while preserving visual fidelity and semantic alignment. This work takes a critical step toward physically grounded and trustworthy video generation.",
    "summary": "",
    "translation": "PhysCorr：基于双奖励DPO的物理约束文本到视频生成与自动偏好选择",
    "relevance_score": 1,
    "reasoning": "该论文专注于物理约束的文本到视频生成，属于纯粹的AIGC和内容生成领域。虽然涉及DPO（直接偏好优化）技术，但其应用场景仅限于视频生成，与推荐系统、搜索或广告的排名和建模需求没有直接关联。论文的核心贡献在于视频生成的物理约束和偏好选择，属于被明确排除的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.03992v1": {
    "title": "CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation",
    "url": "https://www.alphaxiv.org/abs/2511.03992v1",
    "arxiv_id": "2511.03992v1",
    "authors": "Yuwen Tao, Kanglei Zhou, Xin Tan, Yuan Xie",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 02:24:04",
    "ori_summary": "Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret free-form language expressions and localize the corresponding 3D regions in Gaussian fields. While recent advances have introduced cross-modal alignment between language and 3D geometry, existing pipelines still struggle with cross-view consistency due to their reliance on 2D rendered pseudo supervision and view specific feature learning. In this work, we present Camera Aware Referring Field (CaRF), a fully differentiable framework that operates directly in the 3D Gaussian space and achieves multi view consistency. Specifically, CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates camera geometry into Gaussian text interactions to explicitly model view dependent variations and enhance geometric reasoning. Building on this, In Training Paired View Supervision (ITPVS) is proposed to align per Gaussian logits across calibrated views during training, effectively mitigating single view overfitting and exposing inter view discrepancies for optimization. Extensive experiments on three representative benchmarks demonstrate that CaRF achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively. Moreover, this work promotes more reliable and view consistent 3D scene understanding, with potential benefits for embodied AI, AR/VR interaction, and autonomous perception.",
    "summary": "",
    "translation": "CaRF：增强基于参考的3D高斯泼溅分割中的多视角一致性",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及3D视觉分割和多视角一致性技术，属于纯粹的3D视觉领域研究。虽然3D高斯泼溅是计算机视觉中的新兴技术，但论文没有展示与推荐系统、搜索或广告的明确关联，也不涉及Transformer架构改进或LLM技术的应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.03988v1": {
    "title": "Simple 3D Pose Features Support Human and Machine Social Scene Understanding",
    "url": "https://www.alphaxiv.org/abs/2511.03988v1",
    "arxiv_id": "2511.03988v1",
    "authors": "Wenshuo Qin, Leyla Isik",
    "categories": "cs.CV, q-bio.NC",
    "pub_date": "2025-11-06 02:19:26",
    "ori_summary": "Humans can quickly and effortlessly extract a variety of information about others' social interactions from visual input, ranging from visuospatial cues like whether two people are facing each other to higher-level information. Yet, the computations supporting these abilities remain poorly understood, and social interaction recognition continues to challenge even the most advanced AI vision systems. Here, we hypothesized that humans rely on 3D visuospatial pose information to make social interaction judgments, which is absent in most AI vision models. To test this, we combined state-of-the-art pose and depth estimation algorithms to extract 3D joint positions of people in short video clips depicting everyday human actions and compared their ability to predict human social interaction judgments with current AI vision models. Strikingly, 3D joint positions outperformed most current AI vision models, revealing that key social information is available in explicit body position but not in the learned features of most vision models, including even the layer-wise embeddings of the pose models used to extract joint positions. To uncover the critical pose features humans use to make social judgments, we derived a compact set of 3D social pose features describing only the 3D position and direction of faces in the videos. We found that these minimal descriptors matched the predictive strength of the full set of 3D joints and significantly improved the performance of off-the-shelf AI vision models when combined with their embeddings. Moreover, the degree to which 3D social pose features were represented in each off-the-shelf AI vision model predicted the model's ability to match human social judgments. Together, our findings provide strong evidence that human social scene understanding relies on explicit representations of 3D pose and can be supported by simple, structured visuospatial primitives.",
    "summary": "",
    "translation": "简单的3D姿态特征支持人类和机器对社交场景的理解",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于3D姿态特征和社交场景理解，属于计算机视觉领域。虽然涉及场景理解，但主要针对人类姿态分析和社交交互识别，与推荐系统、搜索或广告的核心技术没有直接关联。论文内容更偏向纯粹的视觉理解，缺乏在RecSys/Search/Ads领域的潜在应用前景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.03970v1": {
    "title": "Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images",
    "url": "https://www.alphaxiv.org/abs/2511.03970v1",
    "arxiv_id": "2511.03970v1",
    "authors": "Sam Bahrami, Dylan Campbell",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 01:46:36",
    "ori_summary": "Modern scene reconstruction methods are able to accurately recover 3D surfaces that are visible in one or more images. However, this leads to incomplete reconstructions, missing all occluded surfaces. While much progress has been made on reconstructing entire objects given partial observations using generative models, the structural elements of a scene, like the walls, floors and ceilings, have received less attention. We argue that these scene elements should be relatively easy to predict, since they are typically planar, repetitive and simple, and so less costly approaches may be suitable. In this work, we present a synthetic dataset -- Room Envelopes -- that facilitates progress on this task by providing a set of RGB images and two associated pointmaps for each image: one capturing the visible surface and one capturing the first surface once fittings and fixtures are removed, that is, the structural layout. As we show, this enables direct supervision for feed-forward monocular geometry estimators that predict both the first visible surface and the first layout surface. This confers an understanding of the scene's extent, as well as the shape and location of its objects.",
    "summary": "",
    "translation": "房间包络：从图像重建室内布局的合成数据集",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉中的室内布局重建，属于纯粹的视觉任务，与推荐系统、搜索或广告没有直接关联。虽然室内布局数据可能在某些特定场景下作为上下文特征，但论文本身没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.03962v1": {
    "title": "A Linear Fractional Transformation Model and Calibration Method for Light Field Camera",
    "url": "https://www.alphaxiv.org/abs/2511.03962v1",
    "arxiv_id": "2511.03962v1",
    "authors": "Zhong Chen, Changfeng Chen",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 01:32:04",
    "ori_summary": "Accurate calibration of internal parameters is a crucial yet challenging prerequisite for 3D reconstruction using light field cameras. In this paper, we propose a linear fractional transformation(LFT) parameter $\\alpha$ to decoupled the main lens and micro lens array (MLA). The proposed method includes an analytical solution based on least squares, followed by nonlinear refinement. The method for detecting features from the raw images is also introduced. Experimental results on both physical and simulated data have verified the performance of proposed method. Based on proposed model, the simulation of raw light field images becomes faster, which is crucial for data-driven deep learning methods. The corresponding code can be obtained from the author's website.",
    "summary": "",
    "translation": "光场相机的线性分式变换模型与标定方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于光场相机的特定硬件模型和标定方法，属于计算机视觉中的相机标定领域。光场相机主要用于3D视觉和图像处理应用，与推荐系统、搜索或广告的核心技术栈没有直接关联。论文内容不涉及Transformer架构、LLM技术或任何可在RecSys/Search/Ads中应用的算法框架。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.03950v1": {
    "title": "Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization",
    "url": "https://www.alphaxiv.org/abs/2511.03950v1",
    "arxiv_id": "2511.03950v1",
    "authors": "Zhejia Cai, Puhua Jiang, Shiwei Mao, Hongkun Cao, Ruqi Huang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-11-06 01:05:08",
    "ori_summary": "Reconstructing real-world objects from multi-view images is essential for applications in 3D editing, AR/VR, and digital content creation. Existing methods typically prioritize either geometric accuracy (Multi-View Stereo) or photorealistic rendering (Novel View Synthesis), often decoupling geometry and appearance optimization, which hinders downstream editing tasks. This paper advocates an unified treatment on geometry and appearance optimization for seamless Gaussian-mesh joint optimization. More specifically, we propose a novel framework that simultaneously optimizes mesh geometry (vertex positions and faces) and vertex colors via Gaussian-guided mesh differentiable rendering, leveraging photometric consistency from input images and geometric regularization from normal and depth maps. The obtained high-quality 3D reconstruction can be further exploit in down-stream editing tasks, such as relighting and shape deformation. The code will be publicly available upon acceptance.",
    "summary": "",
    "translation": "通过纹理引导的高斯-网格联合优化改进多视图重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的多视图3D重建技术，使用高斯和网格联合优化方法。虽然技术上先进，但该工作纯粹属于3D视觉领域，没有展示与推荐系统、搜索或广告的潜在应用连接。重建3D对象与排名、用户建模或内容理解等核心RecSys/Search/Ads任务没有明显关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.03943v1": {
    "title": "Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization",
    "url": "https://www.alphaxiv.org/abs/2511.03943v1",
    "arxiv_id": "2511.03943v1",
    "authors": "Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma",
    "categories": "cs.CV",
    "pub_date": "2025-11-06 00:41:54",
    "ori_summary": "Temporal action localization requires precise boundary detection; however, current methods apply uniform computation despite significant variations in difficulty across boundaries. We present two complementary contributions. First, Boundary Distance Regression (BDR) provides information-theoretically optimal localization through signed-distance regression rather than classification, achieving 43\\% sharper boundary peaks. BDR retrofits to existing methods with approximately 50 lines of code, yielding consistent 1.8 to 3.1\\% mAP@0.7 improvements across diverse architectures. Second, Adaptive Temporal Refinement (ATR) allocates computation via continuous depth selection $\\tau \\in [0,1]$, enabling end-to-end differentiable optimization without reinforcement learning. On THUMOS14, ATR achieves 56.5\\% mAP@0.7 at 162G FLOPs, compared to 53.6\\% at 198G for uniform processing, providing a 2.9\\% improvement with 18\\% less compute. Gains scale with boundary heterogeneity, showing 4.2\\% improvement on short actions. Training cost is mitigated via knowledge distillation, with lightweight students retaining 99\\% performance at baseline cost. Results are validated across four benchmarks with rigorous statistical testing.",
    "summary": "",
    "translation": "自适应时序精炼：面向高效动作定位的连续深度分配与距离回归",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉领域的动作定位任务，主要涉及时序建模和效率优化。虽然提到了效率改进，但其核心应用场景是视频理解而非推荐系统、搜索或广告领域。论文中提到的技术（如连续深度分配、距离回归）缺乏明确的跨模态应用潜力，无法直接应用于处理用户序列或上下文特征等推荐系统相关的异构数据。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2511.03929v1": {
    "title": "NVIDIA Nemotron Nano V2 VL",
    "url": "https://www.alphaxiv.org/abs/2511.03929v1",
    "arxiv_id": "2511.03929v1",
    "authors": "NVIDIA, :, Amala Sanjay Deshmukh, Kateryna Chumachenko, Tuomas Rintamaki, Matthieu Le, Tyler Poon, Danial Mohseni Taheri, Ilia Karmanov, Guilin Liu, Jarno Seppanen, Guo Chen, Karan Sapra, Zhiding Yu, Adi Renduchintala, Charles Wang, Peter Jin, Arushi Goel, Mike Ranzinger, Lukas Voegtle, Philipp Fischer, Timo Roman, Wei Ping, Boxin Wang, Zhuolin Yang, Nayeon Lee, Shaokun Zhang, Fuxiao Liu, Zhiqi Li, Di Zhang, Greg Heinrich, Hongxu, Yin, Song Han, Pavlo Molchanov, Parth Mannan, Yao Xu, Jane Polak Scowcroft, Tom Balough, Subhashree Radhakrishnan, Paris Zhang, Sean Cha, Ratnesh Kumar, Zaid Pervaiz Bhat, Jian Zhang, Darragh Hanley, Pritam Biswas, Jesse Oliver, Kevin Vasques, Roger Waleffe, Duncan Riach, Oluwatobi Olabiyi, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Pritam Gundecha, Khanh Nguyen, Alexandre Milesi, Eugene Khvedchenia, Ran Zilberstein, Ofri Masad, Natan Bagrov, Nave Assaf, Tomer Asida, Daniel Afrimi, Amit Zuker, Netanel Haber, Zhiyu Cheng, Jingyu, Xin, Di, Wu, Nik Spirin, Maryam Moosaei, Roman Ageev, Vanshil Atul Shah, Yuting Wu, Daniel Korzekwa, Unnikrishnan Kizhakkemadam Sreekumar, Wanli Jiang, Padmavathy Subramanian, Alejandra Rico, Sandip Bhaskar, Saeid Motiian, Kedi Wu, Annie Surla, Chia-Chih Chen, Hayden Wolff, Matthew Feinberg, Melissa Corpuz, Marek Wawrzos, Eileen Long, Aastha Jhunjhunwala, Paul Hendricks, Farzan Memarian, Benika Hall, Xin-Yu Wang, David Mosallanezhad, Soumye Singhal, Luis Vega, Katherine Cheung, Krzysztof Pawelec, Michael Evans, Katherine Luna, Jie Lou, Erick Galinkin, Akshay Hazare, Kaustubh Purandare, Ann Guan, Anna Warno, Chen Cui, Yoshi Suhara, Shibani Likhite, Seph Mard, Meredith Price, Laya Sleiman, Saori Kaji, Udi Karpas, Kari Briski, Joey Conway, Michael Lightstone, Jan Kautz, Mohammad Shoeybi, Mostofa Patwary, Jonathen Cohen, Oleksii Kuchaiev, Andrew Tao, Bryan Catanzaro",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-11-06 00:10:19",
    "ori_summary": "We introduce Nemotron Nano V2 VL, the latest model of the Nemotron vision-language series designed for strong real-world document understanding, long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers significant improvements over our previous model, Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major enhancements in model architecture, datasets, and training recipes. Nemotron Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and innovative token reduction techniques to achieve higher inference throughput in long document and video scenarios. We are releasing model checkpoints in BF16, FP8, and FP4 formats and sharing large parts of our datasets, recipes and training code.",
    "summary": "",
    "translation": "英伟达Nemotron Nano V2视觉语言模型",
    "relevance_score": 3,
    "reasoning": "该论文涉及视觉语言模型(VLM)，属于VLM类比异构数据范畴。虽然VLM架构可以启发异构特征建模，但该标题未明确表明在推荐系统、搜索或广告中的具体应用，仅停留在基础模型层面。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  }
}