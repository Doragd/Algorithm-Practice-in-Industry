{
  "2601.02306v1": {
    "title": "Cold-Starting Podcast Ads and Promotions with Multi-Task Learning on Spotify",
    "url": "https://www.alphaxiv.org/abs/2601.02306v1",
    "arxiv_id": "2601.02306v1",
    "authors": "Shivam Verma, Hannes Karlbom, Yu Zhao, Nick Topping, Vivian Chen, Kieran Stanley, Bharath Rengarajan",
    "categories": "cs.IR",
    "pub_date": "2026-01-05 17:48:15",
    "ori_summary": "We present a unified multi-objective model for targeting both advertisements and promotions within the Spotify podcast ecosystem. Our approach addresses key challenges in personalization and cold-start initialization, particularly for new advertising objectives. By leveraging transfer learning from large-scale ad and content interactions within a multi-task learning (MTL) framework, a single joint model can be fine-tuned or directly applied to new or low-data targeting tasks, including in-app promotions. This multi-objective design jointly optimizes podcast outcomes such as streams, clicks, and follows for both ads and promotions using a shared representation over user, content, context, and creative features, effectively supporting diverse business goals while improving user experience. Online A/B tests show up to a 22% reduction in effective Cost-Per-Stream (eCPS), particularly for less-streamed podcasts, and an 18-24% increase in podcast stream rates. Offline experiments and ablations highlight the contribution of ancillary objectives and feature groups to cold-start performance. Our experience shows that a unified modeling strategy improves maintainability, cold-start performance, and coverage, while breaking down historically siloed targeting pipelines. We discuss practical trade-offs of such joint models in a real-world advertising system.",
    "summary": "",
    "translation": "基于多任务学习的Spotify播客广告与推广冷启动",
    "relevance_score": 3,
    "reasoning": "该论文标题涉及广告冷启动和多任务学习，属于广告领域的排名/推荐技术，符合核心领域进展。然而，它明确聚焦于播客广告这一特定广告类型，而非通用的推荐/搜索系统技术，应用范围较窄。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02002v1": {
    "title": "Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2601.02002v1",
    "arxiv_id": "2601.02002v1",
    "authors": "Antonio Colacicco, Vito Guida, Dario Di Palma, Fedelucio Narducci, Tommaso Di Noia",
    "categories": "cs.IR, cs.AI, cs.CL",
    "pub_date": "2026-01-05 11:03:56",
    "ori_summary": "Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples.",
    "summary": "",
    "translation": "探索检测大型语言模型中推荐系统数据记忆化的方法",
    "relevance_score": 3,
    "reasoning": "该论文涉及LLM记忆化检测，属于LLM评估范畴，与用户关注的RecSys核心进展、LLM技术应用或Transformer架构改进等焦点领域关联较弱。虽然提及推荐系统数据，但重点在于记忆化检测方法而非RecSys技术本身，且记忆化检测属于LLM评估的子领域，与用户明确排除的'评估基准'等NLP中心话题相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01997v1": {
    "title": "Exploring Diversity, Novelty, and Popularity Bias in ChatGPT's Recommendations",
    "url": "https://www.alphaxiv.org/abs/2601.01997v1",
    "arxiv_id": "2601.01997v1",
    "authors": "Dario Di Palma, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia",
    "categories": "cs.IR, cs.AI, cs.CL",
    "pub_date": "2026-01-05 10:56:01",
    "ori_summary": "ChatGPT has emerged as a versatile tool, demonstrating capabilities across diverse domains. Given these successes, the Recommender Systems (RSs) community has begun investigating its applications within recommendation scenarios primarily focusing on accuracy. While the integration of ChatGPT into RSs has garnered significant attention, a comprehensive analysis of its performance across various dimensions remains largely unexplored. Specifically, the capabilities of providing diverse and novel recommendations or exploring potential biases such as popularity bias have not been thoroughly examined. As the use of these models continues to expand, understanding these aspects is crucial for enhancing user satisfaction and achieving long-term personalization. This study investigates the recommendations provided by ChatGPT-3.5 and ChatGPT-4 by assessing ChatGPT's capabilities in terms of diversity, novelty, and popularity bias. We evaluate these models on three distinct datasets and assess their performance in Top-N recommendation and cold-start scenarios. The findings reveal that ChatGPT-4 matches or surpasses traditional recommenders, demonstrating the ability to balance novelty and diversity in recommendations. Furthermore, in the cold-start scenario, ChatGPT models exhibit superior performance in both accuracy and novelty, suggesting they can be particularly beneficial for new users. This research highlights the strengths and limitations of ChatGPT's recommendations, offering new perspectives on the capacity of these models to provide recommendations beyond accuracy-focused metrics.",
    "summary": "该研究探讨ChatGPT在推荐场景中，除了准确性外，提供多样、新颖推荐及避免流行度偏差的能力。核心方法是系统评估ChatGPT-3.5和ChatGPT-4在Top-N和冷启动场景下，于多样性、新颖性和流行度偏差三个维度的表现。",
    "translation": "探索ChatGPT推荐中的多样性、新颖性与流行度偏差",
    "relevance_score": 8,
    "reasoning": "该论文直接研究LLM在推荐系统中的应用（ChatGPT作为推荐器），属于'直接LLM应用'范畴。论文标题表明将分析推荐系统中的关键偏差问题（多样性、新颖性、流行度），这些都是推荐系统核心领域的重要研究课题，与'核心领域进展'高度相关。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接评估ChatGPT在推荐系统中的表现，关注多样性、新颖性和流行度偏差等关键维度，与“直接LLM应用”和“核心领域进展”高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.01944v1": {
    "title": "The Invisible Hand of AI Libraries Shaping Open Source Projects and Communities",
    "url": "https://www.alphaxiv.org/abs/2601.01944v1",
    "arxiv_id": "2601.01944v1",
    "authors": "Matteo Esposito, Andrea Janes, Valentina Lenarduzzi, Davide Taibi",
    "categories": "cs.SE, cs.AI, cs.CL, cs.IR, cs.PL",
    "pub_date": "2026-01-05 09:50:37",
    "ori_summary": "In the early 1980s, Open Source Software emerged as a revolutionary concept amidst the dominance of proprietary software. What began as a revolutionary idea has now become the cornerstone of computer science. Amidst OSS projects, AI is increasing its presence and relevance. However, despite the growing popularity of AI, its adoption and impacts on OSS projects remain underexplored. We aim to assess the adoption of AI libraries in Python and Java OSS projects and examine how they shape development, including the technical ecosystem and community engagement. To this end, we will perform a large-scale analysis on 157.7k potential OSS repositories, employing repository metrics and software metrics to compare projects adopting AI libraries against those that do not. We expect to identify measurable differences in development activity, community engagement, and code complexity between OSS projects that adopt AI libraries and those that do not, offering evidence-based insights into how AI integration reshapes software development practices.",
    "summary": "",
    "translation": "AI库的无形之手：塑造开源项目与社区",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于AI库对开源项目和社区的社会学或生态学影响，属于非技术性话题。它不涉及推荐系统、搜索或广告领域的核心进展、LLM/Transformer技术发展、直接应用，也不涉及异构数据统一建模等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01930v1": {
    "title": "MCGI: Manifold-Consistent Graph Indexing for Billion-Scale Disk-Resident Vector Search",
    "url": "https://www.alphaxiv.org/abs/2601.01930v1",
    "arxiv_id": "2601.01930v1",
    "authors": "Dongfang Zhao",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2026-01-05 09:23:48",
    "ori_summary": "Graph-based Approximate Nearest Neighbor (ANN) search often suffers from performance degradation in high-dimensional spaces due to the ``Euclidean-Geodesic mismatch,'' where greedy routing diverges from the underlying data manifold. To address this, we propose Manifold-Consistent Graph Indexing (MCGI), a geometry-aware and disk-resident indexing method that leverages Local Intrinsic Dimensionality (LID) to dynamically adapt search strategies to the data's intrinsic geometry. Unlike standard algorithms that treat dimensions uniformly, MCGI modulates its beam search budget based on in situ geometric analysis, eliminating dependency on static hyperparameters. Theoretical analysis confirms that MCGI enables improved approximation guarantees by preserving manifold-consistent topological connectivity. Empirically, MCGI achieves 5.8$\\times$ higher throughput at 95\\% recall on high-dimensional GIST1M compared to state-of-the-art DiskANN. On the billion-scale SIFT1B dataset, MCGI further validates its scalability by reducing high-recall query latency by 3$\\times$, while maintaining performance parity on standard lower-dimensional datasets.",
    "summary": "该论文研究高维空间中基于图的近似最近邻搜索因欧几里得距离与流形测地线不匹配导致的性能下降问题。核心方法是提出一种基于局部本征维度的流形一致图索引，通过动态调整搜索策略来适应数据的内在几何结构。",
    "translation": "MCGI：面向十亿级磁盘驻留向量搜索的流形一致图索引",
    "relevance_score": 8,
    "reasoning": "该论文涉及大规模向量搜索的索引技术，这是推荐系统、搜索和广告中向量检索的核心基础。MCGI的流形一致图索引方法通过改进图索引结构，能显著提升十亿级向量在磁盘存储下的搜索效率，直接适用于大规模推荐和搜索系统中的近似最近邻搜索场景。",
    "rerank_relevance_score": 4,
    "rerank_reasoning": "该论文专注于大规模向量检索的索引优化，属于搜索领域的基础设施技术，但未直接涉及LLM、推荐系统或广告应用。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.01921v1": {
    "title": "A Defect is Being Born: How Close Are We? A Time Sensitive Forecasting Approach",
    "url": "https://www.alphaxiv.org/abs/2601.01921v1",
    "arxiv_id": "2601.01921v1",
    "authors": "Mikel Robredo, Matteo Esposito, Fabio Palomba, Rafael Peñaloza, Valentina Lenarduzzi",
    "categories": "cs.SE, cs.AI, cs.IR, cs.LG",
    "pub_date": "2026-01-05 09:11:29",
    "ori_summary": "Background. Defect prediction has been a highly active topic among researchers in the Empirical Software Engineering field. Previous literature has successfully achieved the most accurate prediction of an incoming fault and identified the features and anomalies that precede it through just-in-time prediction. As software systems evolve continuously, there is a growing need for time-sensitive methods capable of forecasting defects before they manifest. Aim. Our study seeks to explore the effectiveness of time-sensitive techniques for defect forecasting. Moreover, we aim to investigate the early indicators that precede the occurrence of a defect. Method. We will train multiple time-sensitive forecasting techniques to forecast the future bug density of a software project, as well as identify the early symptoms preceding the occurrence of a defect. Expected results. Our expected results are translated into empirical evidence on the effectiveness of our approach for early estimation of bug proneness.",
    "summary": "",
    "translation": "缺陷正在形成：我们距离有多近？一种时间敏感的预测方法",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及缺陷预测和时间敏感预测，这属于特定领域（如软件工程或制造）的质量控制问题，与推荐系统、搜索或广告的核心领域进展、LLM技术、Transformer架构或异构数据建模没有直接关联。标题中未提及任何与推荐、搜索、广告相关的技术或应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01897v1": {
    "title": "A Hybrid Architecture for Multi-Stage Claim Document Understanding: Combining Vision-Language Models and Machine Learning for Real-Time Processing",
    "url": "https://www.alphaxiv.org/abs/2601.01897v1",
    "arxiv_id": "2601.01897v1",
    "authors": "Lilu Cheng, Jingjun Lu, Yi Xuan Chan, Quoc Khai Nguyen, John Bi, Sean Ho",
    "categories": "cs.IR",
    "pub_date": "2026-01-05 08:40:44",
    "ori_summary": "Claims documents are fundamental to healthcare and insurance operations, serving as the basis for reimbursement, auditing, and compliance. However, these documents are typically not born digital; they often exist as scanned PDFs or photographs captured under uncontrolled conditions. Consequently, they exhibit significant content heterogeneity, ranging from typed invoices to handwritten medical reports, as well as linguistic diversity. This challenge is exemplified by operations at Fullerton Health, which handles tens of millions of claims annually across nine markets, including Singapore, the Philippines, Indonesia, Malaysia, Mainland China, Hong Kong, Vietnam, Papua New Guinea, and Cambodia. Such variability, coupled with inconsistent image quality and diverse layouts, poses a significant obstacle to automated parsing and structured information extraction. This paper presents a robust multi-stage pipeline that integrates the multilingual optical character recognition (OCR) engine PaddleOCR, a traditional Logistic Regression classifier, and a compact Vision-Language Model (VLM), Qwen 2.5-VL-7B, to achieve efficient and accurate field extraction from large-scale claims data. The proposed system achieves a document-type classification accuracy of over 95 percent and a field-level extraction accuracy of approximately 87 percent, while maintaining an average processing latency of under 2 seconds per document. Compared to manual processing, which typically requires around 10 minutes per claim, our system delivers a 300x improvement in efficiency. These results demonstrate that combining traditional machine learning models with modern VLMs enables production-grade accuracy and speed for real-world automation. The solution has been successfully deployed in our mobile application and is currently processing tens of thousands of claims weekly from Vietnam and Singapore.",
    "summary": "",
    "translation": "一种用于多阶段索赔文档理解的混合架构：结合视觉语言模型与机器学习实现实时处理",
    "relevance_score": 3,
    "reasoning": "该论文涉及视觉语言模型（VLM）和实时处理，与'VLM Analogy for Heterogeneous Data'有一定相关性，但主要针对特定文档处理任务而非推荐/搜索/广告场景。标题未明确提及推荐系统、搜索或广告应用，且文档理解可能偏向特定领域而非通用异构数据处理。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01862v1": {
    "title": "Judging with Personality and Confidence: A Study on Personality-Conditioned LLM Relevance Assessment",
    "url": "https://www.alphaxiv.org/abs/2601.01862v1",
    "arxiv_id": "2601.01862v1",
    "authors": "Nuo Chen, Hanpei Fang, Piaohong Wang, Jiqun Liu, Tetsuya Sakai, Xiao-Ming Wu",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2026-01-05 07:46:29",
    "ori_summary": "Recent studies have shown that prompting can enable large language models (LLMs) to simulate specific personality traits and produce behaviors that align with those traits. However, there is limited understanding of how these simulated personalities influence critical web search decisions, specifically relevance assessment. Moreover, few studies have examined how simulated personalities impact confidence calibration, specifically the tendencies toward overconfidence or underconfidence. This gap exists even though psychological literature suggests these biases are trait-specific, often linking high extraversion to overconfidence and high neuroticism to underconfidence. To address this gap, we conducted a comprehensive study evaluating multiple LLMs, including commercial models and open-source models, prompted to simulate Big Five personality traits. We tested these models across three test collections (TREC DL 2019, TREC DL 2020, and LLMJudge), collecting two key outputs for each query-document pair: a relevance judgment and a self-reported confidence score. The findings show that personalities such as low agreeableness consistently align more closely with human labels than the unprompted condition. Additionally, low conscientiousness performs well in balancing the suppression of both overconfidence and underconfidence. We also observe that relevance scores and confidence distributions vary systematically across different personalities. Based on the above findings, we incorporate personality-conditioned scores and confidence as features in a random forest classifier. This approach achieves performance that surpasses the best single-personality condition on a new dataset (TREC DL 2021), even with limited training data. These findings highlight that personality-derived confidence offers a complementary predictive signal, paving the way for more reliable and human-aligned LLM evaluators.",
    "summary": "",
    "translation": "基于个性与置信度的判断：个性条件化大语言模型相关性评估研究",
    "relevance_score": 2,
    "reasoning": "该论文虽然涉及LLM评估，但核心关注个性条件化和相关性评估，这属于纯粹的NLP评估基准研究，与RecSys/Search/Ads的核心技术进展无关。论文未展示LLM架构创新或直接应用于推荐/搜索/广告系统的潜力，主要停留在评估方法论层面。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01831v1": {
    "title": "ARIES: A Scalable Multi-Agent Orchestration Framework for Real-Time Epidemiological Surveillance and Outbreak Monitoring",
    "url": "https://www.alphaxiv.org/abs/2601.01831v1",
    "arxiv_id": "2601.01831v1",
    "authors": "Aniket Wattamwar, Sampson Akwafuo",
    "categories": "cs.MA, cs.AI, cs.IR, cs.SE",
    "pub_date": "2026-01-05 06:50:40",
    "ori_summary": "Global health surveillance is currently facing a challenge of Knowledge Gaps. While general-purpose AI has proliferated, it remains fundamentally unsuited for the high-stakes epidemiological domain due to chronic hallucinations and an inability to navigate specialized data silos. This paper introduces ARIES (Agentic Retrieval Intelligence for Epidemiological Surveillance), a specialized, autonomous multi-agent framework designed to move beyond static, disease-specific dashboards toward a dynamic intelligence ecosystem. Built on a hierarchical command structure, ARIES utilizes GPTs to orchestrate a scalable swarm of sub-agents capable of autonomously querying World Health Organization (WHO), Center for Disease Control and Prevention (CDC), and peer-reviewed research papers. By automating the extraction and logical synthesis of surveillance data, ARIES provides a specialized reasoning that identifies emergent threats and signal divergence in near real-time. This modular architecture proves that a task-specific agentic swarm can outperform generic models, offering a robust, extensible for next-generation outbreak response and global health intelligence.",
    "summary": "",
    "translation": "ARIES：一种用于实时流行病学监测和疫情监控的可扩展多智能体编排框架",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于流行病学监测和疫情监控，这属于医疗/公共卫生领域的具体应用。虽然提到了多智能体框架，但该主题与推荐系统、搜索、广告、LLM技术或Transformer架构等当前关注领域完全无关，且明确属于应排除的医学领域应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01785v1": {
    "title": "SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines",
    "url": "https://www.alphaxiv.org/abs/2601.01785v1",
    "arxiv_id": "2601.01785v1",
    "authors": "Rajiv Chaitanya Muttur",
    "categories": "cs.IR, cs.LG",
    "pub_date": "2026-01-05 04:39:31",
    "ori_summary": "Retrieval-Augmented Generation (RAG) systems often rely on fixed top-k document selection mechanisms that ignore downstream generation quality and impose computational overheads. We propose SRAS (Sparse Reward-Aware Selector), a lightweight document selector trained via reinforcement learning (RL) for edge-native RAG deployment. Unlike prior RL-based retrievers that assume large memory and latency budgets, SRAS learns a compact (~0.76MB) policy using Proximal Policy Optimization (PPO), guided by a hybrid reward signal combining Relaxed F1 and BERTScore. Our method operates under tight token and compute constraints, maintaining <1s latency on CPU. SRAS outperforms supervised and random selectors on a synthetic QA benchmark, and generalizes to real-world data, achieving BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning. This work is the first to demonstrate that RL-based document selection can be made ultra-lightweight, latency-aware, and effective for on-device RAG pipelines.",
    "summary": "",
    "translation": "SRAS：一种基于轻量级强化学习的文档选择器，用于边缘原生RAG管道",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及强化学习在RAG（检索增强生成）管道中的应用，属于边缘计算与检索系统的交叉领域。虽然文档选择与搜索系统相关，但论文的核心是强化学习技术而非直接应用于推荐/搜索/广告的架构或方法，且未明确涉及Transformer或LLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01753v1": {
    "title": "MergeRec: Model Merging for Data-Isolated Cross-Domain Sequential Recommendation",
    "url": "https://www.alphaxiv.org/abs/2601.01753v1",
    "arxiv_id": "2601.01753v1",
    "authors": "Hyunsoo Kim, Jaewan Moon, Seongmin Park, Jongwuk Lee",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2026-01-05 03:14:23",
    "ori_summary": "Modern recommender systems trained on domain-specific data often struggle to generalize across multiple domains. Cross-domain sequential recommendation has emerged as a promising research direction to address this challenge; however, existing approaches face fundamental limitations, such as reliance on overlapping users or items across domains, or unrealistic assumptions that ignore privacy constraints. In this work, we propose a new framework, MergeRec, based on model merging under a new and realistic problem setting termed data-isolated cross-domain sequential recommendation, where raw user interaction data cannot be shared across domains. MergeRec consists of three key components: (1) merging initialization, (2) pseudo-user data construction, and (3) collaborative merging optimization. First, we initialize a merged model using training-free merging techniques. Next, we construct pseudo-user data by treating each item as a virtual sequence in each domain, enabling the synthesis of meaningful training samples without relying on real user interactions. Finally, we optimize domain-specific merging weights through a joint objective that combines a recommendation loss, which encourages the merged model to identify relevant items, and a distillation loss, which transfers collaborative filtering signals from the fine-tuned source models. Extensive experiments demonstrate that MergeRec not only preserves the strengths of the original models but also significantly enhances generalizability to unseen domains. Compared to conventional model merging methods, MergeRec consistently achieves superior performance, with average improvements of up to 17.21% in Recall@10, highlighting the potential of model merging as a scalable and effective approach for building universal recommender systems. The source code is available at https://github.com/DIALLab-SKKU/MergeRec.",
    "summary": "该论文研究数据隔离的跨域序列推荐问题，核心思想是通过训练无关的模型合并初始化、基于物品虚拟序列构建伪用户数据、以及结合推荐与蒸馏损失的协同优化权重，实现无需共享原始用户数据的跨域知识迁移。",
    "translation": "MergeRec：面向数据隔离跨域序列推荐的模型融合方法",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及跨域序列推荐这一核心推荐系统领域，属于'Core Domain Advances'范畴。模型融合技术可能借鉴了Transformer架构中的模型合并思想，具有'Enabling Transformer Tech'的潜力，可用于提升推荐系统的效率和性能。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统核心挑战——跨域推荐，提出创新的模型合并框架，在数据隔离的隐私约束下提升泛化能力，与LLM/Transformer技术趋势高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.01751v1": {
    "title": "Query-Document Dense Vectors for LLM Relevance Judgment Bias Analysis",
    "url": "https://www.alphaxiv.org/abs/2601.01751v1",
    "arxiv_id": "2601.01751v1",
    "authors": "Samaneh Mohtadi, Gianluca Demartini",
    "categories": "cs.IR, cs.AI, cs.CL",
    "pub_date": "2026-01-05 03:02:33",
    "ori_summary": "Large Language Models (LLMs) have been used as relevance assessors for Information Retrieval (IR) evaluation collection creation due to reduced cost and increased scalability as compared to human assessors. While previous research has looked at the reliability of LLMs as compared to human assessors, in this work, we aim to understand if LLMs make systematic mistakes when judging relevance, rather than just understanding how good they are on average. To this aim, we propose a novel representational method for queries and documents that allows us to analyze relevance label distributions and compare LLM and human labels to identify patterns of disagreement and localize systematic areas of disagreement. We introduce a clustering-based framework that embeds query-document (Q-D) pairs into a joint semantic space, treating relevance as a relational property. Experiments on TREC Deep Learning 2019 and 2020 show that systematic disagreement between humans and LLMs is concentrated in specific semantic clusters rather than distributed randomly. Query-level analyses reveal recurring failures, most often in definition-seeking, policy-related, or ambiguous contexts. Queries with large variation in agreement across their clusters emerge as disagreement hotspots, where LLMs tend to under-recall relevant content or over-include irrelevant material. This framework links global diagnostics with localized clustering to uncover hidden weaknesses in LLM judgments, enabling bias-aware and more reliable IR evaluation.",
    "summary": "该论文研究LLM作为相关性评估者时存在的系统性偏差问题。其核心方法是提出一种将查询-文档对嵌入联合语义空间的表示方法，通过聚类分析来定位LLM与人类评估者产生分歧的特定语义区域。",
    "translation": "基于查询-文档稠密向量的LLM相关性判断偏差分析",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及LLM在搜索相关性判断中的应用，属于'Direct LLM Applications'范畴。稠密向量检索是搜索系统的核心技术，分析LLM在相关性判断中的偏差对改进搜索质量有直接价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接研究LLM在信息检索评估中的系统性偏差，属于LLM在搜索领域的直接应用分析，与用户关注的LLM应用和偏差分析高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.01750v1": {
    "title": "When Attention Becomes Exposure in Generative Search",
    "url": "https://www.alphaxiv.org/abs/2601.01750v1",
    "arxiv_id": "2601.01750v1",
    "authors": "Shayan Alipour, Mehdi Kargar, Morteza Zihayat",
    "categories": "cs.IR, cs.CY",
    "pub_date": "2026-01-05 03:01:46",
    "ori_summary": "Generative search engines are reshaping information access by replacing traditional ranked lists with synthesized answers and references. In parallel, with the growth of Web3 platforms, incentive-driven creator ecosystems have become an essential part of how enterprises build visibility and community by rewarding creators for contributing to shared narratives. However, the extent to which exposure in generative search engine citations is shaped by external attention markets remains uncertain. In this study, we audit the exposure for 44 Web3 enterprises. First, we show that the creator community around each enterprise is persistent over time. Second, enterprise-specific queries reveal that more popular voices systematically receive greater citation exposure than others. Third, we find that larger follower bases and enterprises with more concentrated creator cores are associated with higher-ranked exposure. Together, these results show that generative search engine citations exhibit exposure bias toward already prominent voices, which risks entrenching incumbents and narrowing viewpoint diversity.",
    "summary": "该论文研究生成式搜索引擎引用中的曝光偏差问题，核心发现是引用曝光倾向于已有影响力的声音，存在固化现有优势、削弱观点多样性的风险。",
    "translation": "当注意力机制在生成式搜索中成为曝光机制时",
    "relevance_score": 9,
    "reasoning": "该论文标题直接涉及生成式搜索中的注意力机制，这属于'直接LLM应用'和'核心领域进展'范畴，因为生成式搜索是搜索领域的前沿方向。注意力机制作为Transformer的核心组件，其演变为曝光机制的研究对搜索排名、结果生成和用户交互建模具有直接应用价值。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文研究生成式搜索引擎中的曝光偏差问题，直接关联搜索领域的公平性和多样性，属于核心领域进展。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.01703v1": {
    "title": "Beyond Homophily: Community Search on Heterophilic Graphs",
    "url": "https://www.alphaxiv.org/abs/2601.01703v1",
    "arxiv_id": "2601.01703v1",
    "authors": "Qing Sima, Xiaoyang Wang, Wenjie Zhang",
    "categories": "cs.SI, cs.AI, cs.DB, cs.IR",
    "pub_date": "2026-01-05 00:44:17",
    "ori_summary": "Community search aims to identify a refined set of nodes that are most relevant to a given query, supporting tasks ranging from fraud detection to recommendation. Unlike homophilic graphs, many real-world networks are heterophilic, where edges predominantly connect dissimilar nodes. Therefore, structural signals that once reflected smooth, low-frequency similarity now appear as sharp, high-frequency contrasts. However, both classical algorithms (e.g., k-core, k-truss) and recent ML-based models struggle to achieve effective community search on heterophilic graphs, where edge signs or semantics are generally unknown. Algorithm-based methods often return communities with mixed class labels, while GNNs, built on homophily, smooth away meaningful signals and blur community boundaries. Therefore, we propose Adaptive Community Search (AdaptCS), a unified framework featuring three key designs: (i) an AdaptCS Encoder that disentangles multi-hop and multi-frequency signals, enabling the model to capture both smooth (homophilic) and contrastive (heterophilic) relations; (ii) a memory-efficient low-rank optimization that removes the main computational bottleneck and ensures model scalability; and (iii) an Adaptive Community Score (ACS) that guides online search by balancing embedding similarity and topological relations. Extensive experiments on both heterophilic and homophilic benchmarks demonstrate that AdaptCS outperforms the best-performing baseline by an average of 11% in F1-score, retains robustness across heterophily levels, and achieves up to 2 orders of magnitude speedup.",
    "summary": "",
    "translation": "超越同质性：异质性图上的社区搜索",
    "relevance_score": 3,
    "reasoning": "该论文关注异质性图上的社区搜索，这属于图算法领域，与推荐系统、搜索或广告中的用户-物品交互图建模有一定潜在关联。然而，论文标题未明确提及LLM、Transformer架构或具体应用场景，且社区搜索算法本身属于通用图技术，而非直接针对推荐/搜索/广告领域的核心问题，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02337v1": {
    "title": "Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling",
    "url": "https://www.alphaxiv.org/abs/2601.02337v1",
    "arxiv_id": "2601.02337v1",
    "authors": "Berk Atil, Rebecca J. Passonneau, Ninareh Mehrabi",
    "categories": "cs.CL",
    "pub_date": "2026-01-05 18:32:45",
    "ori_summary": "Toxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ``pluralistic'' modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, we conduct a systematic evaluation of persona-aware toxicity detection, showing that no single prompting method, including our proposed automated prompt optimization strategy, uniformly dominates across all model-persona pairs. To exploit complementary errors, we explore ensembling four prompting variants and propose a lightweight meta-ensemble: an SVM over the 4-bit vector of prompt predictions. Our results demonstrate that the proposed SVM ensemble consistently outperforms individual prompting methods and traditional majority-voting techniques, achieving the strongest overall performance across diverse personas. This work provides one of the first systematic comparisons of persona-conditioned prompting for toxicity detection and offers a robust method for pluralistic evaluation in subjective NLP tasks.",
    "summary": "",
    "translation": "基于提示优化与学习式集成方法的鲁棒性人物角色感知毒性检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注毒性检测这一内容安全/审核任务，属于自然语言处理中的特定应用领域。虽然涉及提示优化和集成学习等技术，但这些技术本身是通用方法，论文并未明确展示其在推荐系统、搜索或广告领域的潜在应用价值。毒性检测与用户建模、内容排序、广告投放等核心业务场景的直接关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02320v1": {
    "title": "Estimating Text Temperature",
    "url": "https://www.alphaxiv.org/abs/2601.02320v1",
    "arxiv_id": "2601.02320v1",
    "authors": "Nikolay Mikhaylovskiy",
    "categories": "cs.CL",
    "pub_date": "2026-01-05 18:09:41",
    "ori_summary": "Autoregressive language models typically use temperature parameter at inference to shape the probability distribution and control the randomness of the text generated. After the text was generated, this parameter can be estimated using maximum likelihood approach. Following it, we propose a procedure to estimate the temperature of any text, including ones written by humans, with respect to a given language model. We evaluate the temperature estimation capability of a wide selection of small-to-medium LLMs. We then use the best-performing Qwen3 14B to estimate temperatures of popular corpora.",
    "summary": "",
    "translation": "估计文本温度",
    "relevance_score": 1,
    "reasoning": "该标题涉及文本温度估计，这通常与LLM生成控制、多样性调节或采样策略相关，属于纯粹的LLM中心话题。没有明确证据表明该技术直接应用于推荐系统、搜索或广告领域，也不属于核心架构进展或异构数据建模范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02303v1": {
    "title": "Classifying several dialectal Nawatl varieties",
    "url": "https://www.alphaxiv.org/abs/2601.02303v1",
    "arxiv_id": "2601.02303v1",
    "authors": "Juan-José Guzmán-Landa, Juan-Manuel Torres-Moreno, Miguel Figueroa-Saavedra, Carlos-Emiliano González-Gallardo, Graham Ranger, Martha Lorena-Avendaño-Garrido",
    "categories": "cs.CL",
    "pub_date": "2026-01-05 17:38:55",
    "ori_summary": "Mexico is a country with a large number of indigenous languages, among which the most widely spoken is Nawatl, with more than two million people currently speaking it (mainly in North and Central America). Despite its rich cultural heritage, which dates back to the 15th century, Nawatl is a language with few computer resources. The problem is compounded when it comes to its dialectal varieties, with approximately 30 varieties recognised, not counting the different spellings in the written forms of the language. In this research work, we addressed the problem of classifying Nawatl varieties using Machine Learning and Neural Networks.",
    "summary": "",
    "translation": "多种纳瓦特尔方言变体的分类研究",
    "relevance_score": 1,
    "reasoning": "该论文标题表明研究焦点是纳瓦特尔方言的语言学分类，这属于特定语言领域的语言学分析。该主题与推荐系统、搜索、广告、LLM技术或Transformer架构等当前关注领域均无直接关联，也不涉及异构数据处理或VLM类比方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02298v1": {
    "title": "Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)",
    "url": "https://www.alphaxiv.org/abs/2601.02298v1",
    "arxiv_id": "2601.02298v1",
    "authors": "Mahmoud Elgenedy",
    "categories": "cs.CL, eess.SP",
    "pub_date": "2026-01-05 17:33:16",
    "ori_summary": "In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.",
    "summary": "该论文研究如何解决大型语言模型在边缘设备部署时面临的内存和计算资源限制问题。其核心方法是提出一种仅使用2的幂次方数值的量化方案，结合量化感知训练来保持模型性能，通过用低成本位移操作替代昂贵乘法来显著提升推理效率。",
    "translation": "大语言模型中的二幂量化感知训练",
    "relevance_score": 7,
    "reasoning": "该论文属于'使能LLM技术'范畴，专注于LLM量化效率提升。量化技术可显著降低LLM推理成本，这对需要实时响应的推荐系统、搜索和广告应用至关重要，能实现更高效的模型部署和更快的用户交互。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文研究LLM的量化压缩技术，直接属于核心LLM技术进展，对边缘设备部署有重要应用价值，与推荐/搜索/广告系统的效率优化高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.02285v1": {
    "title": "pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs",
    "url": "https://www.alphaxiv.org/abs/2601.02285v1",
    "arxiv_id": "2601.02285v1",
    "authors": "Tobias Schimanski, Imene Kolli, Jingwei Ni, Yu Fan, Ario Saeid Vaghefi, Elliott Ash, Markus Leippold",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-05 17:15:26",
    "ori_summary": "PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing).",
    "summary": "",
    "translation": "pdfQA：基于PDF文档的多样化、挑战性及现实性问答",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于PDF文档的问答任务，属于纯粹的NLP应用领域，与推荐系统、搜索或广告的核心技术无关。虽然问答技术可能在某些边缘场景中涉及文档理解，但论文标题明确限定于PDF文档处理，没有展示任何与推荐/搜索/广告相关的潜在应用或技术交叉点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02236v1": {
    "title": "CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models",
    "url": "https://www.alphaxiv.org/abs/2601.02236v1",
    "arxiv_id": "2601.02236v1",
    "authors": "Yihao Liang, Ze Wang, Hao Chen, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Emad Barsoum, Zicheng Liu, Niraj K. Jha",
    "categories": "cs.CL",
    "pub_date": "2026-01-05 16:09:22",
    "ori_summary": "Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive \"long-jump\" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM",
    "summary": "",
    "translation": "CD4LM：扩散语言模型的一致性蒸馏与自适应解码",
    "relevance_score": 2,
    "reasoning": "该论文主要关注扩散语言模型的训练和解码技术改进，属于纯粹的LLM技术优化范畴。虽然扩散模型在生成质量方面有优势，但论文标题未表明其在推荐系统、搜索或广告领域的潜在应用，也未涉及异构数据处理或多模态建模等与当前关注点直接相关的方面。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02224v1": {
    "title": "From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality",
    "url": "https://www.alphaxiv.org/abs/2601.02224v1",
    "arxiv_id": "2601.02224v1",
    "authors": "Fabian Lukassen, Jan Herrmann, Christoph Weisser, Benjamin Saefken, Thomas Kneib",
    "categories": "cs.CL",
    "pub_date": "2026-01-05 15:52:20",
    "ori_summary": "Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps.",
    "summary": "",
    "translation": "从可解释人工智能到故事：LLM生成解释质量的因子研究",
    "relevance_score": 1,
    "reasoning": "该论文标题表明其核心关注LLM生成解释的质量评估，这属于纯粹的LLM评估和可解释性研究范畴。虽然可解释性在推荐/搜索系统中很重要，但该论文明显聚焦于LLM生成解释的评估方法（因子研究），而非直接应用于推荐/搜索/广告系统，也不涉及核心架构进步或跨模态建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02209v1": {
    "title": "ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging",
    "url": "https://www.alphaxiv.org/abs/2601.02209v1",
    "arxiv_id": "2601.02209v1",
    "authors": "Omer Nacar, Serry Sibaee, Adel Ammar, Yasser Alhabashi, Nadia Samer Sibai, Yara Farouk Ahmed, Ahmed Saud Alqusaiyer, Sulieman Mahmoud AlMahmoud, Abdulrhman Mamdoh Mukhaniq, Lubaba Raed, Sulaiman Mohammed Alatwah, Waad Nasser Alqahtani, Yousif Abdulmajeed Alnasser, Mohamed Aziz Khadraoui, Wadii Boulila",
    "categories": "cs.CL, cs.CY, cs.SD",
    "pub_date": "2026-01-05 15:32:17",
    "ori_summary": "The Arabic language is characterized by a rich tapestry of regional dialects that differ substantially in phonetics and lexicon, reflecting the geographic and cultural diversity of its speakers. Despite the availability of many multi-dialect datasets, mapping speech to fine-grained dialect sources, such as cities, remains underexplored. We present ARCADE (Arabic Radio Corpus for Audio Dialect Evaluation), the first Arabic speech dataset designed explicitly with city-level dialect granularity. The corpus comprises Arabic radio speech collected from streaming services across the Arab world. Our data pipeline captures 30-second segments from verified radio streams, encompassing both Modern Standard Arabic (MSA) and diverse dialectal speech. To ensure reliability, each clip was annotated by one to three native Arabic reviewers who assigned rich metadata, including emotion, speech type, dialect category, and a validity flag for dialect identification tasks. The resulting corpus comprises 6,907 annotations and 3,790 unique audio segments spanning 58 cities across 19 countries. These fine-grained annotations enable robust multi-task learning, serving as a benchmark for city-level dialect tagging. We detail the data collection methodology, assess audio quality, and provide a comprehensive analysis of label distributions. The dataset is available on: https://huggingface.co/datasets/riotu-lab/ARCADE-full",
    "summary": "",
    "translation": "ARCADE：用于细粒度阿拉伯语方言标注的城市规模语料库",
    "relevance_score": 1,
    "reasoning": "该论文标题表明这是一个特定于阿拉伯语方言标注的语言数据集，属于纯粹的NLP数据资源建设。它不涉及RecSys/Search/Ads的核心算法、架构创新、LLM应用或异构数据建模，也没有展示任何在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02186v1": {
    "title": "Toward Global Large Language Models in Medicine",
    "url": "https://www.alphaxiv.org/abs/2601.02186v1",
    "arxiv_id": "2601.02186v1",
    "authors": "Rui Yang, Huitao Li, Weihao Xuan, Heli Qi, Xin Li, Kunyu Yu, Yingjian Chen, Rongrong Wang, Jacques Behmoaras, Tianxi Cai, Bibhas Chakraborty, Qingyu Chen, Lionel Tim-Ee Cheng, Marie-Louise Damwanza, Chido Dzinotyiwei, Aosong Feng, Chuan Hong, Yusuke Iwasawa, Yuhe Ke, Linah Kitala, Taehoon Ko, Jisan Lee, Irene Li, Jonathan Chong Kai Liew, Hongfang Liu, Lian Leng Low, Edison Marrese-Taylor, Yutaka Matsuo, Isheanesu Misi, Yilin Ning, Jasmine Chiat Ling Ong, Marcus Eng Hock Ong, Enrico Petretto, Hossein Rouhizadeh, Abiram Sandralegar, Oren Schreier, Iain Bee Huat Tan, Patrick Tan, Daniel Shu Wei Ting, Junjue Wang, Chunhua Weng, Matthew Yu Heng Wong, Fang Wu, Yunze Xiao, Xuhai Xu, Qingcheng Zeng, Zhuo Zheng, Yifan Peng, Douglas Teodoro, Nan Liu",
    "categories": "cs.CL",
    "pub_date": "2026-01-05 15:05:49",
    "ori_summary": "Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.",
    "summary": "",
    "translation": "迈向医学领域的全球性大规模语言模型",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医学领域的LLM应用，属于明确的医学领域特定应用，与用户关注的RecSys/Search/Ads技术领域完全无关。根据用户列出的无关主题清单，医学、生物学、化学等特定领域应用应被排除。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02179v1": {
    "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
    "url": "https://www.alphaxiv.org/abs/2601.02179v1",
    "arxiv_id": "2601.02179v1",
    "authors": "Caiqi Zhang, Ruihan Yang, Xiaochen Zhu, Chengzu Li, Tiancheng Hu, Yijiang River Dong, Deqing Yang, Nigel Collier",
    "categories": "cs.CL",
    "pub_date": "2026-01-05 14:58:04",
    "ori_summary": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.",
    "summary": "",
    "translation": "大语言模型在多轮交互中的置信度估计",
    "relevance_score": 3,
    "reasoning": "该论文涉及LLM置信度估计，属于LLM评估范畴，与您关注的RecSys/Search/Ads核心进展或直接应用关联较弱。虽然置信度估计在推荐系统中可能有潜在应用（如不确定性感知推荐），但论文标题未明确指向这些领域，且更偏向NLP评估主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02163v1": {
    "title": "EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning",
    "url": "https://www.alphaxiv.org/abs/2601.02163v1",
    "arxiv_id": "2601.02163v1",
    "authors": "Chuanrui Hu, Xingze Gao, Zuyi Zhou, Dannong Xu, Yi Bai, Xintong Li, Hui Zhang, Tong Li, Chong Zhang, Lidong Bing, Yafeng Deng",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2026-01-05 14:39:43",
    "ori_summary": "Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.",
    "summary": "",
    "translation": "EverMemOS：一种用于结构化长程推理的自组织内存操作系统",
    "relevance_score": 3,
    "reasoning": "该论文涉及内存管理和长程推理系统，属于底层系统优化技术。虽然长程推理能力对推荐/搜索系统中的用户行为序列建模有潜在应用价值（如处理长用户历史），但论文标题明确聚焦于操作系统层面的内存管理，而非直接针对推荐/搜索/广告领域的算法或架构创新，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02158v1": {
    "title": "FormationEval, an open multiple-choice benchmark for petroleum geoscience",
    "url": "https://www.alphaxiv.org/abs/2601.02158v1",
    "arxiv_id": "2601.02158v1",
    "authors": "Almaz Ermilov",
    "categories": "cs.CL, cs.AI, cs.LG, physics.geo-ph",
    "pub_date": "2026-01-05 14:36:02",
    "ori_summary": "This paper presents FormationEval, an open multiple-choice question benchmark for evaluating language models on petroleum geoscience and subsurface disciplines. The dataset contains 505 questions across seven domains including petrophysics, petroleum geology and reservoir engineering, derived from three authoritative sources using a reasoning model with detailed instructions and a concept-based approach that avoids verbatim copying of copyrighted text. Each question includes source metadata to support traceability and audit. The evaluation covers 72 models from major providers including OpenAI, Anthropic, Google, Meta and open-weight alternatives. The top performers achieve over 97\\% accuracy, with Gemini 3 Pro Preview reaching 99.8\\%, while tier and domain gaps persist. Among open-weight models, GLM-4.7 leads at 98.6\\%, with several DeepSeek, Llama, Qwen and Mistral models also exceeding 93\\%. The performance gap between open-weight and closed models is narrower than expected, with several lower-cost open-weight models exceeding 90\\% accuracy. Petrophysics emerges as the most challenging domain across all models, while smaller models show wider performance variance. Residual length bias in the dataset (correct answers tend to be longer) is documented along with bias mitigation strategies applied during construction. The benchmark, evaluation code and results are publicly available.",
    "summary": "",
    "translation": "FormationEval：一个用于石油地质科学的开放式多项选择题基准",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于石油地质科学领域的基准测试，这属于地质学/地球科学这一特定领域应用，与您关注的推荐系统、搜索、广告、LLM技术或Transformer架构等核心领域完全无关。该论文没有展示任何与您当前研究方向相关的技术内容或潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02151v1": {
    "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting",
    "url": "https://www.alphaxiv.org/abs/2601.02151v1",
    "arxiv_id": "2601.02151v1",
    "authors": "Muxi Diao, Lele Yang, Wuxuan Gong, Yutong Zhang, Zhonghao Yan, Yufei Han, Kongming Liang, Weiran Xu, Zhanyu Ma",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2026-01-05 14:28:17",
    "ori_summary": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.",
    "summary": "",
    "translation": "熵自适应微调：解决置信冲突以缓解遗忘",
    "relevance_score": 3,
    "reasoning": "该论文涉及LLM微调技术，属于Enabling LLM Tech范畴，可能通过改进模型稳定性间接应用于推荐/搜索系统。但标题未明确说明与RecSys/Search/Ads的直接关联，且主要针对通用LLM遗忘问题，应用潜力有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02144v1": {
    "title": "Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts",
    "url": "https://www.alphaxiv.org/abs/2601.02144v1",
    "arxiv_id": "2601.02144v1",
    "authors": "Boxuan Lyu, Soichiro Murakami, Hidetaka Kamigaito, Peinan Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-05 14:16:11",
    "ori_summary": "Mixture-of-Experts (MoE) architectures scale large language models efficiently by employing a parametric \"router\" to dispatch tokens to a sparse subset of experts. Typically, this router is trained once and then frozen, rendering routing decisions brittle under distribution shifts. We address this limitation by introducing kNN-MoE, a retrieval-augmented routing framework that reuses optimal expert assignments from a memory of similar past cases. This memory is constructed offline by directly optimizing token-wise routing logits to maximize the likelihood on a reference set. Crucially, we use the aggregate similarity of retrieved neighbors as a confidence-driven mixing coefficient, thus allowing the method to fall back to the frozen router when no relevant cases are found. Experiments show kNN-MoE outperforms zero-shot baselines and rivals computationally expensive supervised fine-tuning.",
    "summary": "该论文研究MoE架构中固定参数路由器在数据分布变化时决策脆性的问题，核心方法是构建离线记忆库存储历史最优专家分配，通过kNN检索相似案例并动态融合检索结果与原始路由器的决策，实现自适应路由。",
    "translation": "基于类比的专家路由：用于混合专家的k近邻增强专家分配",
    "relevance_score": 8,
    "reasoning": "该论文涉及混合专家（MoE）架构中的专家分配机制，属于'Enabling Transformer Tech'范畴。kNN增强的路由方法可以显著提高MoE模型的效率和性能，这对于构建大规模推荐系统和广告系统中的高效Transformer模型具有直接应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对MoE架构的路由机制进行创新，通过检索增强方法提升路由鲁棒性，属于Transformer架构效率与专家系统优化的核心前沿，对推荐系统的大规模模型部署有重要参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.02128v1": {
    "title": "Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents Generation",
    "url": "https://www.alphaxiv.org/abs/2601.02128v1",
    "arxiv_id": "2601.02128v1",
    "authors": "Steffen Freisinger, Philipp Seeberger, Thomas Ranzenberger, Tobias Bocklet, Korbinian Riedhammer",
    "categories": "cs.CL, eess.AS",
    "pub_date": "2026-01-05 14:00:48",
    "ori_summary": "Segmenting speech transcripts into thematic sections benefits both downstream processing and users who depend on written text for accessibility. We introduce a novel approach to hierarchical topic segmentation in transcripts, generating multi-level tables of contents that capture both topic and subtopic boundaries. We compare zero-shot prompting and LoRA fine-tuning on large language models, while also exploring the integration of high-level speech pause features. Evaluations on English meeting recordings and multilingual lecture transcripts (Portuguese, German) show significant improvements over established topic segmentation baselines. Additionally, we adapt a common evaluation measure for multi-level segmentation, taking into account all hierarchical levels within one metric.",
    "summary": "",
    "translation": "迈向多层次转录文本分割：基于LoRA微调技术的目录生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注转录文本分割和目录生成，这属于文档处理的具体应用场景。虽然涉及LoRA微调技术（属于Transformer效率优化），但其应用领域（转录文本处理）与推荐系统、搜索或广告的核心技术需求关联度较低，缺乏明确的跨领域应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02123v1": {
    "title": "DeCode: Decoupling Content and Delivery for Medical QA",
    "url": "https://www.alphaxiv.org/abs/2601.02123v1",
    "arxiv_id": "2601.02123v1",
    "authors": "Po-Jen Ko, Chen-Han Tsai, Yu-Shao Peng",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-05 13:54:38",
    "ori_summary": "Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\\%$ to $49.8\\%$, corresponding to a $75\\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs.",
    "summary": "",
    "translation": "DeCode：用于医学问答的内容与传递解耦",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医学问答（Medical QA），这属于明确的无关主题（Medical domain-specific applications）。虽然提到了内容与传递的解耦，但核心应用领域是医学，与推荐系统、搜索或广告没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02076v1": {
    "title": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows",
    "url": "https://www.alphaxiv.org/abs/2601.02076v1",
    "arxiv_id": "2601.02076v1",
    "authors": "Yingte Shu, Yuchuan Tian, Chao Xu, Yunhe Wang, Hanting Chen",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-05 12:57:33",
    "ori_summary": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.",
    "summary": "",
    "translation": "基于置信度感知滑动窗口的扩散语言模型延迟承诺解码",
    "relevance_score": 3,
    "reasoning": "该论文主要关注扩散语言模型的解码方法改进，属于LLM效率优化技术，可能通过提升推理速度间接应用于推荐/搜索系统的实时性需求。然而，论文标题未明确展示与异构数据建模、推荐算法或广告排名的直接关联，应用潜力有限且不直接属于核心领域进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02065v1": {
    "title": "Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory",
    "url": "https://www.alphaxiv.org/abs/2601.02065v1",
    "arxiv_id": "2601.02065v1",
    "authors": "Md. Asif Hossain, Nabil Subhan, Mantasha Rahman Mahi, Jannatul Ferdous Nabila",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-05 12:41:44",
    "ori_summary": "Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings",
    "summary": "",
    "translation": "面向低资源语言的成本高效跨语言检索增强生成：以孟加拉语农业咨询为例",
    "relevance_score": 2,
    "reasoning": "该论文主要关注跨语言检索增强生成在低资源语言（孟加拉语）和特定领域（农业咨询）的应用，这属于纯粹的NLP应用研究。虽然检索增强生成技术本身可能对搜索系统有间接启发，但论文的焦点是语言处理而非推荐/搜索/广告的核心算法或架构创新，与当前关注的四大方向（核心领域进展、使能LLM技术、使能Transformer技术、直接LLM应用）关联度很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02043v1": {
    "title": "Simulated Reasoning is Reasoning",
    "url": "https://www.alphaxiv.org/abs/2601.02043v1",
    "arxiv_id": "2601.02043v1",
    "authors": "Hendrik Kempt, Alon Lavie",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2026-01-05 12:00:04",
    "ori_summary": "Reasoning has long been understood as a pathway between stages of understanding. Proper reasoning leads to understanding of a given subject. This reasoning was conceptualized as a process of understanding in a particular way, i.e., \"symbolic reasoning\". Foundational Models (FM) demonstrate that this is not a necessary condition for many reasoning tasks: they can \"reason\" by way of imitating the process of \"thinking out loud\", testing the produced pathways, and iterating on these pathways on their own. This leads to some form of reasoning that can solve problems on its own or with few-shot learning, but appears fundamentally different from human reasoning due to its lack of grounding and common sense, leading to brittleness of the reasoning process. These insights promise to substantially alter our assessment of reasoning and its necessary conditions, but also inform the approaches to safety and robust defences against this brittleness of FMs. This paper offers and discusses several philosophical interpretations of this phenomenon, argues that the previously apt metaphor of the \"stochastic parrot\" has lost its relevance and thus should be abandoned, and reflects on different normative elements in the safety- and appropriateness-considerations emerging from these reasoning models and their growing capacity.",
    "summary": "",
    "translation": "模拟推理即推理",
    "relevance_score": 2,
    "reasoning": "该标题暗示了关于推理过程模拟或形式化的理论讨论，可能涉及LLM的推理能力。虽然与'Enabling LLM Tech'有一定关联（如提升LLM的推理能力），但缺乏明确的RecSys/Search/Ads应用指向，更偏向于LLM的基础能力研究，与直接应用或异构数据建模等焦点领域关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02031v1": {
    "title": "Output Embedding Centering for Stable LLM Pretraining",
    "url": "https://www.alphaxiv.org/abs/2601.02031v1",
    "arxiv_id": "2601.02031v1",
    "authors": "Felix Stollenwerk, Anna Lokrantz, Niclas Hertzberg",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2026-01-05 11:44:05",
    "ori_summary": "Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings' geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called μ-centering, or a regularization method called μ-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that μ-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss.",
    "summary": "该论文研究大型语言模型预训练末期因大学习率导致的输出logit发散不稳定问题。其核心方法是分析输出嵌入的几何特性，提出输出嵌入中心化策略来抑制发散，包括确定性操作μ-centering和正则化方法μ-loss两种实现方式。",
    "translation": "面向稳定大语言模型预训练的输出嵌入中心化",
    "relevance_score": 8,
    "reasoning": "该论文专注于提升LLM预训练的稳定性，属于'Enabling LLM Tech'范畴。输出嵌入中心化技术可改善模型收敛性和训练稳定性，这对于构建更可靠的推荐、搜索和广告系统的基础LLM至关重要，能直接提升下游任务的性能表现。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出一种稳定LLM预训练的新方法，直接针对Transformer架构的核心训练稳定性问题，对大规模LLM开发具有基础性价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.02023v1": {
    "title": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs",
    "url": "https://www.alphaxiv.org/abs/2601.02023v1",
    "arxiv_id": "2601.02023v1",
    "authors": "Amirali Ebrahimzadeh, Seyyed M. Salili",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-05 11:30:56",
    "ori_summary": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.",
    "summary": "",
    "translation": "并非所有针都能被找到：事实分布与'勿虚构'提示如何影响长上下文大语言模型中的字面提取、逻辑推理与幻觉风险",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及'幻觉风险'和'长上下文LLMs'，这属于'幻觉、评估基准或其他纯NLP中心主题'的无关主题范畴。虽然提到了长上下文处理，但核心焦点是幻觉问题而非推荐/搜索/广告应用，且未提及任何潜在的应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02015v1": {
    "title": "Surprisal and Metaphor Novelty: Moderate Correlations and Divergent Scaling Effects",
    "url": "https://www.alphaxiv.org/abs/2601.02015v1",
    "arxiv_id": "2601.02015v1",
    "authors": "Omar Momen, Emilie Sitter, Berenike Herrmann, Sina Zarrieß",
    "categories": "cs.CL, cs.AI, cs.IT",
    "pub_date": "2026-01-05 11:24:33",
    "ori_summary": "Novel metaphor comprehension involves complex semantic processes and linguistic creativity, making it an interesting task for studying language models (LMs). This study investigates whether surprisal, a probabilistic measure of predictability in LMs, correlates with different metaphor novelty datasets. We analyse surprisal from 16 LM variants on corpus-based and synthetic metaphor novelty datasets. We explore a cloze-style surprisal method that conditions on full-sentence context. Results show that LMs yield significant moderate correlations with scores/labels of metaphor novelty. We further identify divergent scaling patterns: on corpus-based data, correlation strength decreases with model size (inverse scaling effect), whereas on synthetic data it increases (Quality-Power Hypothesis). We conclude that while surprisal can partially account for annotations of metaphor novelty, it remains a limited metric of linguistic creativity.",
    "summary": "",
    "translation": "惊奇度与隐喻新颖性：中等相关性及发散性缩放效应",
    "relevance_score": 1,
    "reasoning": "该论文标题表明研究焦点是语言处理中的惊奇度（surprisal）与隐喻新颖性之间的相关性，这属于纯粹的语言学或心理语言学范畴。虽然惊奇度概念在语言模型中有所应用，但论文标题明确指向隐喻分析这一特定语言现象，与推荐系统、搜索或广告的技术进展无直接关联，也不涉及Transformer架构改进或跨模态建模等当前关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02010v1": {
    "title": "A neural network for modeling human concept formation, understanding and communication",
    "url": "https://www.alphaxiv.org/abs/2601.02010v1",
    "arxiv_id": "2601.02010v1",
    "authors": "Liangxuan Guo, Haoyang Chen, Yang Chen, Yanchao Bi, Shan Yu",
    "categories": "q-bio.NC, cs.AI, cs.CL",
    "pub_date": "2026-01-05 11:19:07",
    "ori_summary": "A remarkable capability of the human brain is to form more abstract conceptual representations from sensorimotor experiences and flexibly apply them independent of direct sensory inputs. However, the computational mechanism underlying this ability remains poorly understood. Here, we present a dual-module neural network framework, the CATS Net, to bridge this gap. Our model consists of a concept-abstraction module that extracts low-dimensional conceptual representations, and a task-solving module that performs visual judgement tasks under the hierarchical gating control of the formed concepts. The system develops transferable semantic structure based on concept representations that enable cross-network knowledge transfer through conceptual communication. Model-brain fitting analyses reveal that these emergent concept spaces align with both neurocognitive semantic model and brain response structures in the human ventral occipitotemporal cortex, while the gating mechanisms mirror that in the semantic control brain network. This work establishes a unified computational framework that can offer mechanistic insights for understanding human conceptual cognition and engineering artificial systems with human-like conceptual intelligence.",
    "summary": "",
    "translation": "一种用于建模人类概念形成、理解与交流的神经网络",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于人类认知过程的建模，属于认知科学和基础AI研究的范畴，与推荐系统、搜索或广告的核心技术进展没有直接关联。虽然概念形成和理解可能间接影响用户建模，但论文没有明确指向推荐/搜索/广告领域的应用或技术改进，因此相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01972v1": {
    "title": "Hidden State Poisoning Attacks against Mamba-based Language Models",
    "url": "https://www.alphaxiv.org/abs/2601.01972v1",
    "arxiv_id": "2601.01972v1",
    "authors": "Alexandre Le Mercier, Chris Develder, Thomas Demeester",
    "categories": "cs.CL",
    "pub_date": "2026-01-05 10:27:19",
    "ori_summary": "State space models (SSMs) like Mamba offer efficient alternatives to Transformer-based language models, with linear time complexity. Yet, their adversarial robustness remains critically unexplored. This paper studies the phenomenon whereby specific short input phrases induce a partial amnesia effect in such models, by irreversibly overwriting information in their hidden states, referred to as a Hidden State Poisoning Attack (HiSPA). Our benchmark RoBench25 allows evaluating a model's information retrieval capabilities when subject to HiSPAs, and confirms the vulnerability of SSMs against such attacks. Even a recent 52B hybrid SSM-Transformer model from the Jamba family collapses on RoBench25 under optimized HiSPA triggers, whereas pure Transformers do not. We also observe that HiSPA triggers significantly weaken the Jamba model on the popular Open-Prompt-Injections benchmark, unlike pure Transformers. Finally, our interpretability study reveals patterns in Mamba's hidden layers during HiSPAs that could be used to build a HiSPA mitigation system. The full code and data to reproduce the experiments can be found at https://anonymous.4open.science/r/hispa_anonymous-5DB0.",
    "summary": "",
    "translation": "针对基于Mamba架构的语言模型的隐藏状态投毒攻击",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及安全攻击（投毒攻击），这属于明确的无关主题范畴。虽然Mamba架构是Transformer的替代方案，但论文焦点是安全漏洞而非架构本身的技术进展或其在推荐/搜索/广告领域的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01964v1": {
    "title": "CSF: Contrastive Semantic Features for Direct Multilingual Sign Language Generation",
    "url": "https://www.alphaxiv.org/abs/2601.01964v1",
    "arxiv_id": "2601.01964v1",
    "authors": "Tran Sy Bao",
    "categories": "cs.CL",
    "pub_date": "2026-01-05 10:15:35",
    "ori_summary": "Sign language translation systems typically require English as an intermediary language, creating barriers for non-English speakers in the global deaf community. We present Canonical Semantic Form (CSF), a language-agnostic semantic representation framework that enables direct translation from any source language to sign language without English mediation. CSF decomposes utterances into nine universal semantic slots: event, intent, time, condition, agent, object, location, purpose, and modifier. A key contribution is our comprehensive condition taxonomy comprising 35 condition types across eight semantic categories, enabling nuanced representation of conditional expressions common in everyday communication. We train a lightweight transformer-based extractor (0.74 MB) that achieves 99.03% average slot extraction accuracy across four typologically diverse languages: English, Vietnamese, Japanese, and French. The model demonstrates particularly strong performance on condition classification (99.4% accuracy) despite the 35-class complexity. With inference latency of 3.02ms on CPU, our approach enables real-time sign language generation in browser-based applications. We release our code, trained models, and multilingual dataset to support further research in accessible sign language technology.",
    "summary": "",
    "translation": "CSF：基于对比语义特征的多语言手语直接生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于手语生成这一特定领域，属于视觉模态的生成任务。虽然涉及多语言处理，但其核心是手语视频生成而非文本处理，与推荐系统、搜索或广告的异构数据建模没有直接关联。论文的技术方法（对比语义特征）可能具有通用性，但未明确展示在推荐/搜索/广告领域的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01896v1": {
    "title": "Tackling the Inherent Difficulty of Noise Filtering in RAG",
    "url": "https://www.alphaxiv.org/abs/2601.01896v1",
    "arxiv_id": "2601.01896v1",
    "authors": "Jingyu Liu, Jiaen Lin, Yong Liu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-05 08:40:37",
    "ori_summary": "Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.",
    "summary": "",
    "translation": "解决RAG中噪声过滤的固有难题",
    "relevance_score": 2,
    "reasoning": "该论文标题关注检索增强生成(RAG)中的噪声过滤问题，这主要属于纯粹的LLM技术优化范畴，与检索系统本身的核心进展或LLM在推荐/搜索/广告中的直接应用关联较弱。虽然RAG技术可能间接影响信息检索质量，但论文焦点更偏向NLP领域的检索-生成集成优化，而非推荐/搜索/广告领域的核心算法或架构创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01885v1": {
    "title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents",
    "url": "https://www.alphaxiv.org/abs/2601.01885v1",
    "arxiv_id": "2601.01885v1",
    "authors": "Yi Yu, Liuyi Yao, Yuexiang Xie, Qingquan Tan, Jiaqi Feng, Yaliang Li, Libing Wu",
    "categories": "cs.CL",
    "pub_date": "2026-01-05 08:24:16",
    "ori_summary": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.",
    "summary": "该论文研究LLM智能体在有限上下文窗口下进行长视野推理时的记忆管理问题。其核心方法是提出一个统一框架，将长短期记忆管理直接集成到智能体策略中，通过将记忆操作（存储、检索、更新等）暴露为工具动作，使LLM能自主决策信息管理。",
    "translation": "智能体记忆：为大语言模型智能体学习统一的长短期记忆管理",
    "relevance_score": 7,
    "reasoning": "该论文属于'赋能LLM技术'范畴，专注于LLM智能体的记忆管理改进。虽然不直接针对推荐系统、搜索或广告，但改进的长期记忆管理能力可以应用于个性化推荐（通过记忆用户长期偏好）、会话式搜索（通过记忆多轮对话上下文）和广告定向（通过记忆用户历史行为模式）。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出的统一长短期记忆管理框架直接针对LLM智能体的核心瓶颈，其将记忆操作作为工具动作集成到策略中的思想对搜索、推荐等长序列建模任务具有直接启发性。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.01868v1": {
    "title": "DermoGPT: Open Weights and Open Data for Morphology-Grounded Dermatological Reasoning MLLMs",
    "url": "https://www.alphaxiv.org/abs/2601.01868v1",
    "arxiv_id": "2601.01868v1",
    "authors": "Jinghan Ru, Siyuan Yan, Yuguo Yin, Yuexian Zou, Zongyuan Ge",
    "categories": "cs.CL",
    "pub_date": "2026-01-05 07:55:36",
    "ori_summary": "Multimodal Large Language Models (MLLMs) show promise for medical applications, yet progress in dermatology lags due to limited training data, narrow task coverage, and lack of clinically-grounded supervision that mirrors expert diagnostic workflows. We present a comprehensive framework to address these gaps. First, we introduce DermoInstruct, a large-scale morphology-anchored instruction corpus comprising 211,243 images and 772,675 trajectories across five task formats, capturing the complete diagnostic pipeline from morphological observation and clinical reasoning to final diagnosis. Second, we establish DermoBench, a rigorous benchmark evaluating 11 tasks across four clinical axes: Morphology, Diagnosis, Reasoning, and Fairness, including a challenging subset of 3,600 expert-verified open-ended instances and human performance baselines. Third, we develop DermoGPT, a dermatology reasoning MLLM trained via supervised fine-tuning followed by our Morphologically-Anchored Visual-Inference-Consistent (MAVIC) reinforcement learning objective, which enforces consistency between visual observations and diagnostic conclusions. At inference, we deploy Confidence-Consistency Test-time adaptation (CCT) for robust predictions. Experiments show DermoGPT significantly outperforms 16 representative baselines across all axes, achieving state-of-the-art performance while substantially narrowing the human-AI gap. DermoInstruct, DermoBench and DermoGPT will be made publicly available at https://github.com/mendicant04/DermoGPT upon acceptance.",
    "summary": "",
    "translation": "DermoGPT：用于形态学基础皮肤病学推理的多模态大语言模型的开源权重与数据",
    "relevance_score": 1,
    "reasoning": "该论文专注于皮肤病学领域的医学应用，属于明确的医学/生物学特定领域，与RecSys/Search/Ads的核心技术进展、LLM基础技术、Transformer架构改进或直接应用完全无关。其标题中提到的“形态学基础皮肤病学推理”表明这是一个纯粹的医学诊断应用，不涉及推荐、搜索或广告系统的任何技术要素。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01842v1": {
    "title": "Towards Automated Lexicography: Generating and Evaluating Definitions for Learner's Dictionaries",
    "url": "https://www.alphaxiv.org/abs/2601.01842v1",
    "arxiv_id": "2601.01842v1",
    "authors": "Yusuke Ide, Adam Nohejl, Joshua Tanner, Hitomi Yanaka, Christopher Lindsay, Taro Watanabe",
    "categories": "cs.CL",
    "pub_date": "2026-01-05 07:11:24",
    "ori_summary": "We study dictionary definition generation (DDG), i.e., the generation of non-contextualized definitions for given headwords. Dictionary definitions are an essential resource for learning word senses, but manually creating them is costly, which motivates us to automate the process. Specifically, we address learner's dictionary definition generation (LDDG), where definitions should consist of simple words. First, we introduce a reliable evaluation approach for DDG, based on our new evaluation criteria and powered by an LLM-as-a-judge. To provide reference definitions for the evaluation, we also construct a Japanese dataset in collaboration with a professional lexicographer. Validation results demonstrate that our evaluation approach agrees reasonably well with human annotators. Second, we propose an LDDG approach via iterative simplification with an LLM. Experimental results indicate that definitions generated by our approach achieve high scores on our criteria while maintaining lexical simplicity.",
    "summary": "",
    "translation": "迈向自动化词典编纂：为学习者词典生成与评估定义",
    "relevance_score": 1,
    "reasoning": "该论文专注于词典编纂自动化和定义生成，属于纯粹的NLP/语言学研究领域。虽然涉及文本生成，但未展示与推荐系统、搜索或广告领域的直接关联或潜在应用。该主题更接近语言教育工具开发，而非当前关注的LLM在推荐/搜索/广告中的技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01828v1": {
    "title": "Emergent Introspective Awareness in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2601.01828v1",
    "arxiv_id": "2601.01828v1",
    "authors": "Jack Lindsey",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-05 06:47:41",
    "ori_summary": "We investigate whether large language models can introspect on their internal states. It is difficult to answer this question through conversation alone, as genuine introspection cannot be distinguished from confabulations. Here, we address this challenge by injecting representations of known concepts into a model's activations, and measuring the influence of these manipulations on the model's self-reported states. We find that models can, in certain scenarios, notice the presence of injected concepts and accurately identify them. Models demonstrate some ability to recall prior internal representations and distinguish them from raw text inputs. Strikingly, we find that some models can use their ability to recall prior intentions in order to distinguish their own outputs from artificial prefills. In all these experiments, Claude Opus 4 and 4.1, the most capable models we tested, generally demonstrate the greatest introspective awareness; however, trends across models are complex and sensitive to post-training strategies. Finally, we explore whether models can explicitly control their internal representations, finding that models can modulate their activations when instructed or incentivized to \"think about\" a concept. Overall, our results indicate that current language models possess some functional introspective awareness of their own internal states. We stress that in today's models, this capacity is highly unreliable and context-dependent; however, it may continue to develop with further improvements to model capabilities.",
    "summary": "",
    "translation": "大型语言模型中的涌现自省意识",
    "relevance_score": 2,
    "reasoning": "该论文标题聚焦于LLM的自省意识这一内在认知特性，属于纯粹的LLM中心化主题，与推荐系统、搜索或广告的直接应用或使能技术关联性较弱。虽然自省能力可能间接影响模型可靠性，但论文未明确涉及异构数据处理、架构效率或具体应用场景，因此与当前关注点的相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01827v1": {
    "title": "Aspect Extraction from E-Commerce Product and Service Reviews",
    "url": "https://www.alphaxiv.org/abs/2601.01827v1",
    "arxiv_id": "2601.01827v1",
    "authors": "Valiant Lance D. Dionela, Fatima Kriselle S. Dy, Robin James M. Hombrebueno, Aaron Rae M. Nicolas, Charibeth K. Cheng, Raphael W. Gonda",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2026-01-05 06:45:51",
    "ori_summary": "Aspect Extraction (AE) is a key task in Aspect-Based Sentiment Analysis (ABSA), yet it remains difficult to apply in low-resource and code-switched contexts like Taglish, a mix of Tagalog and English commonly used in Filipino e-commerce reviews. This paper introduces a comprehensive AE pipeline designed for Taglish, combining rule-based, large language model (LLM)-based, and fine-tuning techniques to address both aspect identification and extraction. A Hierarchical Aspect Framework (HAF) is developed through multi-method topic modeling, along with a dual-mode tagging scheme for explicit and implicit aspects. For aspect identification, four distinct models are evaluated: a Rule-Based system, a Generative LLM (Gemini 2.0 Flash), and two Fine-Tuned Gemma-3 1B models trained on different datasets (Rule-Based vs. LLM-Annotated). Results indicate that the Generative LLM achieved the highest performance across all tasks (Macro F1 0.91), demonstrating superior capability in handling implicit aspects. In contrast, the fine-tuned models exhibited limited performance due to dataset imbalance and architectural capacity constraints. This work contributes a scalable and linguistically adaptive framework for enhancing ABSA in diverse, code-switched environments.",
    "summary": "论文研究低资源混合语言（Taglish）电商评论中的方面提取问题。核心方法是构建一个结合规则、LLM生成和微调技术的综合流程，并引入分层方面框架和双模式标注方案来处理显式和隐式方面。",
    "translation": "电商产品与服务评论中的方面提取",
    "relevance_score": 4,
    "reasoning": "该论文涉及从用户生成内容中提取结构化信息，这在推荐系统和搜索中可用于改进商品理解和用户意图建模。然而，它更侧重于传统NLP任务而非LLM/Transformer架构的核心进展，且未明确涉及多模态或异构数据处理，因此相关性有限。",
    "rerank_relevance_score": 6,
    "rerank_reasoning": "论文涉及LLM在特定领域（电商评论）的应用，属于直接LLM应用范畴，但方法较为传统，创新性有限。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.01825v1": {
    "title": "CSCBench: A PVC Diagnostic Benchmark for Commodity Supply Chain Reasoning",
    "url": "https://www.alphaxiv.org/abs/2601.01825v1",
    "arxiv_id": "2601.01825v1",
    "authors": "Yaxin Cui, Yuanqiang Zeng, Jiapeng Yan, Keling Lin, Kai Ji, Jianhui Zeng, Sheng Zhang, Xin Luo, Binzhu Su, Chaolai Shen, Jiahao Yu",
    "categories": "cs.CL",
    "pub_date": "2026-01-05 06:44:29",
    "ori_summary": "Large Language Models (LLMs) have achieved remarkable success in general benchmarks, yet their competence in commodity supply chains (CSCs) -- a domain governed by institutional rule systems and feasibility constraints -- remains under-explored. CSC decisions are shaped jointly by process stages (e.g., planning, procurement, delivery), variety-specific rules (e.g., contract specifications and delivery grades), and reasoning depth (from retrieval to multi-step analysis and decision selection). We introduce CSCBench, a 2.3K+ single-choice benchmark for CSC reasoning, instantiated through our PVC 3D Evaluation Framework (Process, Variety, and Cognition). The Process axis aligns tasks with SCOR+Enable; the Variety axis operationalizes commodity-specific rule systems under coupled material-information-financial constraints, grounded in authoritative exchange guidebooks/rulebooks and industry reports; and the Cognition axis follows Bloom's revised taxonomy. Evaluating representative LLMs under a direct prompting setting, we observe strong performance on the Process and Cognition axes but substantial degradation on the Variety axis, especially on Freight Agreements. CSCBench provides a diagnostic yardstick for measuring and improving LLM capabilities in this high-stakes domain.",
    "summary": "",
    "translation": "CSCBench：一个用于商品供应链推理的PVC诊断基准",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向特定领域（商品供应链）的诊断基准，属于供应链管理或物流优化领域，与推荐系统、搜索或广告的核心技术进展完全无关。标题中提到的\"PVC诊断\"和\"商品供应链推理\"表明这是针对特定行业应用的专业基准测试，不属于LLM技术、Transformer架构进展或推荐/搜索/广告系统的任何相关技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01792v1": {
    "title": "HyperCLOVA X 8B Omni",
    "url": "https://www.alphaxiv.org/abs/2601.01792v1",
    "arxiv_id": "2601.01792v1",
    "authors": "NAVER Cloud HyperCLOVA X Team",
    "categories": "cs.LG, cs.AI, cs.CL, cs.SD",
    "pub_date": "2026-01-05 05:06:11",
    "ori_summary": "In this report, we present HyperCLOVA X 8B Omni, the first any-to-any omnimodal model in the HyperCLOVA X family that supports text, audio, and vision as both inputs and outputs. By consolidating multimodal understanding and generation into a single model rather than separate modality-specific pipelines, HyperCLOVA X 8B Omni serves as an 8B-scale omni-pathfinding point toward practical any-to-any omni assistants. At a high level, the model unifies modalities through a shared next-token prediction interface over an interleaved multimodal sequence, while vision and audio encoders inject continuous embeddings for fine-grained understanding and grounding. Empirical evaluations demonstrate competitive performance against comparably sized models across diverse input-output combinations spanning text, audio, and vision, in both Korean and English. We anticipate that the open-weight release of HyperCLOVA X 8B Omni will support a wide range of research and deployment scenarios.",
    "summary": "",
    "translation": "超大规模语言模型CLOVA X 8B全模态",
    "relevance_score": 3,
    "reasoning": "该标题表明这是一个8B参数规模的语言模型，可能属于'Enabling LLM Tech'范畴，但缺乏具体技术细节。仅从标题无法判断其架构创新或与推荐/搜索/广告系统的具体应用关联，因此相关性较低。需要更多信息才能评估其作为基础模型在目标领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01778v1": {
    "title": "BanglaIPA: Towards Robust Text-to-IPA Transcription with Contextual Rewriting in Bengali",
    "url": "https://www.alphaxiv.org/abs/2601.01778v1",
    "arxiv_id": "2601.01778v1",
    "authors": "Jakir Hasan, Shrestha Datta, Md Saiful Islam, Shubhashis Roy Dipta, Ameya Debnath",
    "categories": "cs.CL",
    "pub_date": "2026-01-05 04:17:31",
    "ori_summary": "Despite its widespread use, Bengali lacks a robust automated International Phonetic Alphabet (IPA) transcription system that effectively supports both standard language and regional dialectal texts. Existing approaches struggle to handle regional variations, numerical expressions, and generalize poorly to previously unseen words. To address these limitations, we propose BanglaIPA, a novel IPA generation system that integrates a character-based vocabulary with word-level alignment. The proposed system accurately handles Bengali numerals and demonstrates strong performance across regional dialects. BanglaIPA improves inference efficiency by leveraging a precomputed word-to-IPA mapping dictionary for previously observed words. The system is evaluated on the standard Bengali and six regional variations of the DUAL-IPA dataset. Experimental results show that BanglaIPA outperforms baseline IPA transcription models by 58.4-78.7% and achieves an overall mean word error rate of 11.4%, highlighting its robustness in phonetic transcription generation for the Bengali language.",
    "summary": "",
    "translation": "BanglaIPA：通过上下文重写在孟加拉语中实现鲁棒的文本到国际音标转写",
    "relevance_score": 1,
    "reasoning": "该论文专注于特定语言（孟加拉语）的文本到国际音标转写任务，属于语音处理/语言学的技术范畴。虽然涉及文本处理，但缺乏与推荐系统、搜索或广告领域的直接联系，也不属于核心LLM技术、Transformer架构进展或异构数据统一建模的范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01768v1": {
    "title": "Can LLMs Track Their Output Length? A Dynamic Feedback Mechanism for Precise Length Regulation",
    "url": "https://www.alphaxiv.org/abs/2601.01768v1",
    "arxiv_id": "2601.01768v1",
    "authors": "Meiman Xiao, Ante Wang, Qingguo Hu, Zhongjian Miao, Huangjun Shen, Longyue Wang, Weihua Luo, Jinsong Su",
    "categories": "cs.CL",
    "pub_date": "2026-01-05 03:49:14",
    "ori_summary": "Precisely controlling the length of generated text is a common requirement in real-world applications. However, despite significant advancements in following human instructions, Large Language Models (LLMs) still struggle with this task. In this work, we demonstrate that LLMs often fail to accurately measure input text length, leading to poor adherence to length constraints. To address this issue, we propose a novel length regulation approach that incorporates dynamic length feedback during generation, enabling adaptive adjustments to meet target lengths. Experiments on summarization and biography tasks show our training-free approach significantly improves precision in achieving target token, word, or sentence counts without compromising quality. Additionally, we demonstrate that further supervised fine-tuning allows our method to generalize effectively to broader text-generation tasks.",
    "summary": "该论文研究LLM难以精确控制生成文本长度的问题，核心方法是引入动态长度反馈机制，在生成过程中自适应调整以满足目标长度要求。",
    "translation": "大型语言模型能否追踪其输出长度？一种用于精确长度调控的动态反馈机制",
    "relevance_score": 4,
    "reasoning": "该论文探讨LLM输出长度控制机制，属于核心LLM技术范畴，可能应用于搜索摘要生成或广告文案长度优化。但论文焦点较为狭窄，主要解决长度调控这一具体技术问题，而非更广泛的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 5,
    "rerank_reasoning": "该论文聚焦LLM输出长度控制，属于LLM基础能力改进，对搜索/推荐中的摘要生成、内容裁剪等应用有间接价值，但非核心领域突破或直接应用创新。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.01754v1": {
    "title": "Context-Free Recognition with Transformers",
    "url": "https://www.alphaxiv.org/abs/2601.01754v1",
    "arxiv_id": "2601.01754v1",
    "authors": "Selim Jerad, Anej Svete, Sophie Hao, Ryan Cotterell, William Merrill",
    "categories": "cs.LG, cs.CC, cs.CL, cs.FL",
    "pub_date": "2026-01-05 03:14:23",
    "ori_summary": "Transformers excel on tasks that process well-formed inputs according to some grammar, such as natural language and code. However, it remains unclear how they can process grammatical syntax. In fact, under standard complexity conjectures, standard transformers cannot recognize context-free languages (CFLs), a canonical formalism to describe syntax, or even regular languages, a subclass of CFLs (Merrill et al., 2022). Merrill & Sabharwal (2024) show that $\\mathcal{O}(\\log n)$ looping layers (w.r.t. input length $n$) allows transformers to recognize regular languages, but the question of context-free recognition remained open. In this work, we show that looped transformers with $\\mathcal{O}(\\log n)$ looping layers and $\\mathcal{O}(n^6)$ padding tokens can recognize all CFLs. However, training and inference with $\\mathcal{O}(n^6)$ padding tokens is potentially impractical. Fortunately, we show that, for natural subclasses such as unambiguous CFLs, the recognition problem on transformers becomes more tractable, requiring $\\mathcal{O}(n^3)$ padding. We empirically validate our results and show that looping helps on a language that provably requires logarithmic depth. Overall, our results shed light on the intricacy of CFL recognition by transformers: While general recognition may require an intractable amount of padding, natural constraints such as unambiguity yield efficient recognition algorithms.",
    "summary": "",
    "translation": "基于Transformer的无上下文识别",
    "relevance_score": 3,
    "reasoning": "该标题涉及Transformer架构，属于'Enabling Transformer Tech'范畴，可能探讨注意力机制或效率改进。然而，'Context-Free Recognition'暗示可能专注于形式语言理论或句法分析等基础NLP任务，与推荐/搜索/广告系统的实际应用关联较弱，除非能明确展示如何提升序列建模效率或处理异构数据。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01745v1": {
    "title": "Multi-granularity Interactive Attention Framework for Residual Hierarchical Pronunciation Assessment",
    "url": "https://www.alphaxiv.org/abs/2601.01745v1",
    "arxiv_id": "2601.01745v1",
    "authors": "Hong Han, Hao-Chen Pei, Zhao-Zheng Nie, Xin Luo, Xin-Shun Xu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-05 02:43:04",
    "ori_summary": "Automatic pronunciation assessment plays a crucial role in computer-assisted pronunciation training systems. Due to the ability to perform multiple pronunciation tasks simultaneously, multi-aspect multi-granularity pronunciation assessment methods are gradually receiving more attention and achieving better performance than single-level modeling tasks. However, existing methods only consider unidirectional dependencies between adjacent granularity levels, lacking bidirectional interaction among phoneme, word, and utterance levels and thus insufficiently capturing the acoustic structural correlations. To address this issue, we propose a novel residual hierarchical interactive method, HIA for short, that enables bidirectional modeling across granularities. As the core of HIA, the Interactive Attention Module leverages an attention mechanism to achieve dynamic bidirectional interaction, effectively capturing linguistic features at each granularity while integrating correlations between different granularity levels. We also propose a residual hierarchical structure to alleviate the feature forgetting problem when modeling acoustic hierarchies. In addition, we use 1-D convolutional layers to enhance the extraction of local contextual cues at each granularity. Extensive experiments on the speechocean762 dataset show that our model is comprehensively ahead of the existing state-of-the-art methods.",
    "summary": "",
    "translation": "用于残差层次化发音评估的多粒度交互注意力框架",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向语音发音评估领域，属于语音处理/语音识别的技术范畴。虽然提到了注意力机制，但其应用场景（发音评估）与推荐系统、搜索或广告的核心技术需求没有直接关联，也不涉及异构数据处理或多模态建模的相关概念。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01739v1": {
    "title": "K-EXAONE Technical Report",
    "url": "https://www.alphaxiv.org/abs/2601.01739v1",
    "arxiv_id": "2601.01739v1",
    "authors": "Eunbi Choi, Kibong Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Hyunjik Jo, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Haeju Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Heuiyeen Yeen, Hwan Chang, Stanley Jungkyu Choi, Yejin Choi, Jiwon Ham, Kijeong Jeon, Geunyeong Jeong, Gerrard Jeongwon Jo, Yonghwan Jo, Jiyeon Jung, Naeun Kang, Dohoon Kim, Euisoon Kim, Hayeon Kim, Hyosang Kim, Hyunseo Kim, Jieun Kim, Minu Kim, Myoungshin Kim, Unsol Kim, Youchul Kim, YoungJin Kim, Chaeeun Lee, Chaeyoon Lee, Changhun Lee, Dahm Lee, Edward Hwayoung Lee, Honglak Lee, Jinsang Lee, Jiyoung Lee, Sangeun Lee, Seungwon Lim, Solji Lim, Woohyung Lim, Chanwoo Moon, Jaewoo Park, Jinho Park, Yongmin Park, Hyerin Seo, Wooseok Seo, Yongwoo Song, Sejong Yang, Sihoon Yang, Chang En Yea, Sihyuk Yi, Chansik Yoon, Dongkeun Yoon, Sangyeon Yoon, Hyeongu Yun",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-01-05 02:30:59",
    "ori_summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.",
    "summary": "",
    "translation": "K-EXAONE技术报告",
    "relevance_score": 1,
    "reasoning": "标题仅包含项目名称和技术报告标识，未提供任何技术内容、方法或领域信息。无法判断是否涉及推荐系统、搜索、广告、LLM技术、Transformer架构或异构数据建模等关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01714v1": {
    "title": "Entropy-Aligned Decoding of LMs for Better Writing and Reasoning",
    "url": "https://www.alphaxiv.org/abs/2601.01714v1",
    "arxiv_id": "2601.01714v1",
    "authors": "Kareem Ahmed, Sameer Singh",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2026-01-05 01:37:10",
    "ori_summary": "Language models (LMs) are trained on billions of tokens in an attempt to recover the true language distribution. Still, vanilla random sampling from LMs yields low quality generations. Decoding algorithms attempt to restrict the LM distribution to a set of high-probability continuations, but rely on greedy heuristics that introduce myopic distortions, yielding sentences that are homogeneous, repetitive and incoherent. In this paper, we introduce EPIC, a hyperparameter-free decoding approach that incorporates the entropy of future trajectories into LM decoding. EPIC explicitly regulates the amount of uncertainty expressed at every step of generation, aligning the sampling distribution's entropy to the aleatoric (data) uncertainty. Through Entropy-Aware Lazy Gumbel-Max sampling, EPIC manages to be exact, while also being efficient, requiring only a sublinear number of entropy evaluations per step. Unlike current baselines, EPIC yields sampling distributions that are empirically well-aligned with the entropy of the underlying data distribution. Across creative writing and summarization tasks, EPIC consistently improves LM-as-judge preference win-rates over widely used decoding strategies. These preference gains are complemented by automatic metrics, showing that EPIC produces more diverse generations and more faithful summaries. We also evaluate EPIC on mathematical reasoning, where it outperforms all baselines.",
    "summary": "该论文研究如何改进语言模型的解码过程以提升生成质量。其核心思想是提出EPIC方法，通过将采样分布的熵与数据不确定性对齐，在解码时显式调控每一步生成的不确定性程度。",
    "translation": "基于熵对齐解码的语言模型用于提升写作与推理能力",
    "relevance_score": 4,
    "reasoning": "该论文提出了一种语言模型解码方法，属于'Enabling LLM Tech'范畴，因为改进的解码策略可能提升LLM在生成任务中的性能。虽然写作增强可能间接有益于搜索或推荐中的内容生成，但论文标题明确聚焦于写作和推理这两个通用NLP任务，而非专门针对RecSys/Search/Ads领域的应用或优化，因此相关性有限。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出了一种新的解码方法EPIC，通过熵对齐改进语言模型生成质量，属于核心LLM技术进步，可直接应用于搜索和推荐系统的内容生成环节。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.01708v1": {
    "title": "A Training-Free Large Reasoning Model-based Knowledge Tracing Framework for Unified Prediction and Prescription",
    "url": "https://www.alphaxiv.org/abs/2601.01708v1",
    "arxiv_id": "2601.01708v1",
    "authors": "Unggi Lee, Joo Young Kim, Ran Ju, Minyoung Jung, Jeyeon Eo",
    "categories": "cs.CL",
    "pub_date": "2026-01-05 01:02:21",
    "ori_summary": "Knowledge Tracing (KT) aims to estimate a learner's evolving mastery based on interaction histories. Recent studies have explored Large Language Models (LLMs) for KT via autoregressive nature, but such approaches typically require fine-tuning and exhibit unstable or near-random performance. Moreover, prior KT systems primarily focus on prediction and rely on multi-stage pipelines for feedback and recommendation, resulting in increased system complexity and resources. To address this gap, we propose Thinking-KT, a training-free KT framework that incorporates Test-Time Scaling (TTS), enabling even small LLMs to achieve competitive KT performance. Moreover, in this framework, a small LLM can jointly perform KT prediction, personalized feedback generation, and learning recommendation in a unified output without degrading prediction accuracy. Beyond performance, we present the systematic analysis of reasoning traces in KT. Our results demonstrate that TTS is a critical yet underexplored factor in LLM-based KT, and that small LLMs can serve as unified ITS engines.",
    "summary": "该论文研究知识追踪中传统方法需要微调、系统复杂的问题，核心思想是提出免训练的Thinking-KT框架，通过测试时缩放技术让小型LLM统一完成预测、反馈生成和学习推荐。",
    "translation": "一种基于免训练大型推理模型的知识追踪框架，用于统一预测与处方",
    "relevance_score": 4,
    "reasoning": "该论文涉及知识追踪和大型推理模型，可能属于教育技术领域，与推荐系统、搜索或广告的直接关联较弱。虽然提到了大型模型和预测任务，但未明确指向推荐系统、搜索或广告的具体应用场景。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出免训练框架，直接应用LLM进行知识追踪的统一预测与处方，属于LLM在推荐/教育领域的直接应用创新，并涉及推理机制分析。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.02359v1": {
    "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
    "url": "https://www.alphaxiv.org/abs/2601.02359v1",
    "arxiv_id": "2601.02359v1",
    "authors": "Kaede Shiohara, Toshihiko Yamasaki, Vladislav Golyanik",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 18:59:54",
    "ori_summary": "Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.",
    "summary": "",
    "translation": "ExposeAnyone：个性化音频到表情扩散模型作为鲁棒的零样本人脸伪造检测器",
    "relevance_score": 1,
    "reasoning": "该论文涉及人脸伪造检测和音频到表情生成，属于计算机视觉和多媒体安全领域。虽然提到了扩散模型，但其核心应用是伪造检测而非推荐系统、搜索或广告中的排名、检索或用户建模。没有明显的潜在应用连接到我关注的领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02358v1": {
    "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
    "url": "https://www.alphaxiv.org/abs/2601.02358v1",
    "arxiv_id": "2601.02358v1",
    "authors": "Junyi Chen, Tong He, Zhoujie Fu, Pengfei Wan, Kun Gai, Weicai Ye",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 18:56:34",
    "ori_summary": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.",
    "summary": "",
    "translation": "VINO：一种具有交错全模态上下文的统一视觉生成器",
    "relevance_score": 2,
    "reasoning": "该论文标题表明其核心是视觉生成技术，属于AIGC/内容生成领域，这被明确列为无关主题。虽然提到了“全模态上下文”可能涉及多模态处理，但主要关注视觉生成而非推荐/搜索/广告中的排名或建模应用，因此相关性很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02356v1": {
    "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
    "url": "https://www.alphaxiv.org/abs/2601.02356v1",
    "arxiv_id": "2601.02356v1",
    "authors": "Jing Tan, Zhaoyang Zhang, Yantao Shen, Jiarui Cai, Shuo Yang, Jiajun Wu, Wei Xia, Zhuowen Tu, Stefano Soatto",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 18:55:32",
    "ori_summary": "We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.",
    "summary": "",
    "translation": "Talk2Move：基于强化学习的文本指令场景物体级几何变换",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及强化学习在视觉场景物体几何变换中的应用，属于计算机视觉与机器人交互的交叉领域。虽然使用了强化学习和文本指令，但其核心是物体级几何操作而非推荐/搜索/广告相关的用户行为建模或内容理解，与当前关注的LLM在推荐系统应用、Transformer架构进展等方向关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02353v1": {
    "title": "Meta-Learning Guided Pruning for Few-Shot Plant Pathology on Edge Devices",
    "url": "https://www.alphaxiv.org/abs/2601.02353v1",
    "arxiv_id": "2601.02353v1",
    "authors": "Shahnawaz Alam, Mohammed Mudassir Uddin, Mohammed Kaif Pasha",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-01-05 18:55:05",
    "ori_summary": "Farmers in remote areas need quick and reliable methods for identifying plant diseases, yet they often lack access to laboratories or high-performance computing resources. Deep learning models can detect diseases from leaf images with high accuracy, but these models are typically too large and computationally expensive to run on low-cost edge devices such as Raspberry Pi. Furthermore, collecting thousands of labeled disease images for training is both expensive and time-consuming. This paper addresses both challenges by combining neural network pruning -- removing unnecessary parts of the model -- with few-shot learning, which enables the model to learn from limited examples. This paper proposes Disease-Aware Channel Importance Scoring (DACIS), a method that identifies which parts of the neural network are most important for distinguishing between different plant diseases, integrated into a three-stage Prune-then-Meta-Learn-then-Prune (PMP) pipeline. Experiments on PlantVillage and PlantDoc datasets demonstrate that the proposed approach reduces model size by 78\\% while maintaining 92.3\\% of the original accuracy, with the compressed model running at 7 frames per second on a Raspberry Pi 4, making real-time field diagnosis practical for smallholder farmers.",
    "summary": "",
    "translation": "面向边缘设备上少样本植物病理学的元学习引导剪枝",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及植物病理学这一特定领域应用，属于明确的无关主题范畴。虽然提到了元学习和模型剪枝技术，但这些技术被应用于与推荐系统、搜索或广告无关的农业/生物领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02339v1": {
    "title": "Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding",
    "url": "https://www.alphaxiv.org/abs/2601.02339v1",
    "arxiv_id": "2601.02339v1",
    "authors": "Jingming He, Chongyi Li, Shiqi Wang, Sam Kwong",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 18:33:50",
    "ori_summary": "Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.",
    "summary": "",
    "translation": "基于各向异性局部编码的3D高斯建模中联合语义与渲染增强",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D视觉和渲染技术，属于明确的无关主题。虽然提到了语义增强，但核心是3D高斯建模和渲染改进，没有展示与推荐系统、搜索或广告的潜在应用联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02329v1": {
    "title": "BEDS: Bayesian Emergent Dissipative Structures",
    "url": "https://www.alphaxiv.org/abs/2601.02329v1",
    "arxiv_id": "2601.02329v1",
    "authors": "Laurent Caraffa",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 18:21:02",
    "ori_summary": "We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning. The central thesis proposes that learning, across physical, biological, and computational systems, fundamentally constitutes the conversion of flux into structure through entropy export. Building on Prigogine's theory of dissipative structures, we establish a formal isomorphism between thermodynamic processes and Bayesian updating, demonstrating that sustainable learning systems must follow dissipative patterns where crystallized posteriors become priors for subsequent levels of emergence. We derive fundamental mathematical constants (e, π, φ) as fixed points of Bayesian inference under minimal axioms, suggesting these constants emerge necessarily from any system capable of representing and updating uncertainty. Furthermore, we propose a conjecture linking Gödel's incompleteness theorems to thermodynamic constraints, hypothesizing that pathologies of formal systems (incompleteness, undecidability) are structurally analogous to dissipation deficits in physical systems. As practical validation, we present a peer-to-peer network architecture implementing BEDS principles, achieving six orders of magnitude improvement in energy efficiency compared to existing distributed consensus systems while enabling continuous learning. This work bridges fundamental physics, mathematical logic, and practical system design, offering both theoretical insights into the nature of learning and computation, and a concrete pathway toward sustainable artificial intelligence.",
    "summary": "",
    "translation": "BEDS：贝叶斯涌现耗散结构",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及贝叶斯方法和涌现耗散结构，属于物理学或复杂系统理论领域，与推荐系统、搜索、广告或LLM技术没有直接关联。标题中未提及任何与Transformer架构、多模态建模、推荐算法或广告排名相关的关键词，因此判断为完全不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02318v1": {
    "title": "Fusion2Print: Deep Flash-Non-Flash Fusion for Contactless Fingerprint Matching",
    "url": "https://www.alphaxiv.org/abs/2601.02318v1",
    "arxiv_id": "2601.02318v1",
    "authors": "Roja Sahoo, Anoop Namboodiri",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 18:09:27",
    "ori_summary": "Contactless fingerprint recognition offers a hygienic and convenient alternative to contact-based systems, enabling rapid acquisition without latent prints, pressure artifacts, or hygiene risks. However, contactless images often show degraded ridge clarity due to illumination variation, subcutaneous skin discoloration, and specular reflections. Flash captures preserve ridge detail but introduce noise, whereas non-flash captures reduce noise but lower ridge contrast. We propose Fusion2Print (F2P), the first framework to systematically capture and fuse paired flash-non-flash contactless fingerprints. We construct a custom paired dataset, FNF Database, and perform manual flash-non-flash subtraction to isolate ridge-preserving signals. A lightweight attention-based fusion network also integrates both modalities, emphasizing informative channels and suppressing noise, and then a U-Net enhancement module produces an optimally weighted grayscale image. Finally, a deep embedding model with cross-domain compatibility, generates discriminative and robust representations in a unified embedding space compatible with both contactless and contact-based fingerprints for verification. F2P enhances ridge clarity and achieves superior recognition performance (AUC=0.999, EER=1.12%) over single-capture baselines (Verifinger, DeepPrint).",
    "summary": "",
    "translation": "Fusion2Print：用于非接触式指纹匹配的深度闪光-非闪光融合技术",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及指纹识别技术，这属于明确列出的不相关主题范畴。虽然提到了深度学习和融合技术，但核心应用（指纹匹配）与推荐系统、搜索或广告领域没有直接关联，也不属于任何当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02315v1": {
    "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping",
    "url": "https://www.alphaxiv.org/abs/2601.02315v1",
    "arxiv_id": "2601.02315v1",
    "authors": "Saurabh Kaushik, Lalit Maurya, Beth Tellman",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 18:07:21",
    "ori_summary": "Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on \\href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}",
    "summary": "",
    "translation": "Prithvi-互补自适应融合编码器（CAFE）：释放洪水淹没测绘的全部潜力",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于洪水淹没测绘这一地球科学/遥感应用领域，属于明确的“Irrelevant Topics”中提到的“Medical, Biology, Chemistry, Physics or other domain-specific applications”。其技术（如融合编码器）可能具有通用性，但标题未暗示与推荐系统、搜索或广告的任何潜在联系，也未涉及LLM、Transformer架构或异构数据统一建模等当前关注的核心技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02309v1": {
    "title": "360DVO: Deep Visual Odometry for Monocular 360-Degree Camera",
    "url": "https://www.alphaxiv.org/abs/2601.02309v1",
    "arxiv_id": "2601.02309v1",
    "authors": "Xiaopeng Guo, Yinzhe Xu, Huajian Huang, Sai-Kit Yeung",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 17:52:50",
    "ori_summary": "Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems. However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination. To address this, we present 360DVO, the first deep learning-based OVO framework. Our approach introduces a distortion-aware spherical feature extractor (DAS-Feat) that adaptively learns distortion-resistant features from 360-degree images. These sparse feature patches are then used to establish constraints for effective pose estimation within a novel omnidirectional differentiable bundle adjustment (ODBA) module. To facilitate evaluation in realistic settings, we also contribute a new real-world OVO benchmark. Extensive experiments on this benchmark and public synthetic datasets (TartanAir V2 and 360VO) demonstrate that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%. Homepage: https://chris1004336379.github.io/360DVO-homepage",
    "summary": "",
    "translation": "360DVO：用于单目360度相机的深度视觉里程计",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的视觉里程计技术，属于纯粹的视觉领域研究。虽然涉及深度学习方法，但其应用场景（360度相机定位）与推荐系统、搜索或广告的核心技术（如排序、召回、用户建模、内容理解等）没有直接关联。论文没有展示任何在推荐/搜索/广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02299v1": {
    "title": "SortWaste: A Densely Annotated Dataset for Object Detection in Industrial Waste Sorting",
    "url": "https://www.alphaxiv.org/abs/2601.02299v1",
    "arxiv_id": "2601.02299v1",
    "authors": "Sara Inácio, Hugo Proença, João C. Neves",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 17:34:50",
    "ori_summary": "The increasing production of waste, driven by population growth, has created challenges in managing and recycling materials effectively. Manual waste sorting is a common practice; however, it remains inefficient for handling large-scale waste streams and presents health risks for workers. On the other hand, existing automated sorting approaches still struggle with the high variability, clutter, and visual complexity of real-world waste streams. The lack of real-world datasets for waste sorting is a major reason automated systems for this problem are underdeveloped. Accordingly, we introduce SortWaste, a densely annotated object detection dataset collected from a Material Recovery Facility. Additionally, we contribute to standardizing waste detection in sorting lines by proposing ClutterScore, an objective metric that gauges the scene's hardness level using a set of proxies that affect visual complexity (e.g., object count, class and size entropy, and spatial overlap). In addition to these contributions, we provide an extensive benchmark of state-of-the-art object detection models, detailing their results with respect to the hardness level assessed by the proposed metric. Despite achieving promising results (mAP of 59.7% in the plastic-only detection task), performance significantly decreases in highly cluttered scenes. This highlights the need for novel and more challenging datasets on the topic.",
    "summary": "",
    "translation": "SortWaste：一个用于工业废物分拣中目标检测的密集标注数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于工业废物分拣中的目标检测数据集，属于计算机视觉的特定应用领域。它不涉及推荐系统、搜索、广告、LLM技术或Transformer架构的任何方面，也没有展示与异构数据统一建模相关的概念。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02289v1": {
    "title": "Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery",
    "url": "https://www.alphaxiv.org/abs/2601.02289v1",
    "arxiv_id": "2601.02289v1",
    "authors": "Tom Burgert, Leonard Hackel, Paolo Rota, Begüm Demir",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 17:24:50",
    "ori_summary": "Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.",
    "summary": "",
    "translation": "基于排序的地理正则化：重新审视多光谱遥感影像的对比自监督学习",
    "relevance_score": 2,
    "reasoning": "该论文主要关注遥感影像处理，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然涉及对比学习和自监督学习技术，但这些方法在RecSys/Search/Ads中的应用潜力非常有限且不明确，不符合当前关注的任何技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02281v1": {
    "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
    "url": "https://www.alphaxiv.org/abs/2601.02281v1",
    "arxiv_id": "2601.02281v1",
    "authors": "Shuai Yuan, Yantai Yang, Xiaotian Yang, Xupeng Zhang, Zhonghao Zhao, Lingming Zhang, Zhipeng Zhang",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 17:11:00",
    "ori_summary": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
    "summary": "",
    "translation": "InfiniteVGGT：面向无限数据流的视觉几何基础Transformer",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及视觉几何基础Transformer，主要属于视觉领域（Visual Geometry）而非推荐系统、搜索或广告的核心技术。虽然提到了Transformer架构，但未明确展示其在推荐/搜索/广告中的潜在应用，且“无限数据流”的表述过于宽泛，缺乏具体的技术连接点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02273v1": {
    "title": "TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation",
    "url": "https://www.alphaxiv.org/abs/2601.02273v1",
    "arxiv_id": "2601.02273v1",
    "authors": "Salim Khazem",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2026-01-05 17:03:45",
    "ori_summary": "Foundation segmentation models such as the Segment Anything Model (SAM) exhibit strong zero-shot generalization through large-scale pretraining, but adapting them to domain-specific semantic segmentation remains challenging, particularly for thin structures (e.g., retinal vessels) and noisy modalities (e.g., SAR imagery). Full fine-tuning is computationally expensive and risks catastrophic forgetting. We propose \\textbf{TopoLoRA-SAM}, a topology-aware and parameter-efficient adaptation framework for binary semantic segmentation. TopoLoRA-SAM injects Low-Rank Adaptation (LoRA) into the frozen ViT encoder, augmented with a lightweight spatial convolutional adapter and optional topology-aware supervision via differentiable clDice. We evaluate our approach on five benchmarks spanning retinal vessel segmentation (DRIVE, STARE, CHASE\\_DB1), polyp segmentation (Kvasir-SEG), and SAR sea/land segmentation (SL-SSDD), comparing against U-Net, DeepLabV3+, SegFormer, and Mask2Former. TopoLoRA-SAM achieves the best retina-average Dice and the best overall average Dice across datasets, while training only \\textbf{5.2\\%} of model parameters ($\\sim$4.9M). On the challenging CHASE\\_DB1 dataset, our method substantially improves segmentation accuracy and robustness, demonstrating that topology-aware parameter-efficient adaptation can match or exceed fully fine-tuned specialist models. Code is available at : https://github.com/salimkhazem/Seglab.git",
    "summary": "",
    "translation": "TopoLoRA-SAM：面向细长结构与跨域二值语义分割的拓扑感知参数高效基础分割器适配方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的语义分割任务，特别是细长结构和跨域场景，属于纯粹的视觉领域研究。虽然提到了参数高效微调技术（LoRA），但其核心应用场景（医学图像、遥感等）与推荐系统、搜索或广告的异构数据处理需求没有直接关联。论文未涉及序列建模、多模态融合或推荐相关架构创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02267v1": {
    "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
    "url": "https://www.alphaxiv.org/abs/2601.02267v1",
    "arxiv_id": "2601.02267v1",
    "authors": "Renke Wang, Zhenyu Zhang, Ying Tai, Jian Yang",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 16:51:45",
    "ori_summary": "Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html",
    "summary": "",
    "translation": "DiffProxy：通过扩散生成密集代理实现多视角人体网格重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的人体网格重建，属于纯粹的视觉任务。虽然涉及扩散模型，但应用场景（人体网格恢复）与推荐系统、搜索或广告没有直接关联，也不涉及异构数据处理或多模态建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02256v1": {
    "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
    "url": "https://www.alphaxiv.org/abs/2601.02256v1",
    "arxiv_id": "2601.02256v1",
    "authors": "Shikun Sun, Liao Qu, Huichao Zhang, Yiheng Liu, Yangyang Song, Xian Li, Xu Wang, Yi Jiang, Daniel K. Du, Xinglong Wu, Jia Jia",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-01-05 16:36:40",
    "ori_summary": "Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.",
    "summary": "",
    "translation": "VAR RL的正确实践：解决视觉自回归生成中的异步策略冲突",
    "relevance_score": 1,
    "reasoning": "这篇论文标题明确涉及视觉自回归生成和强化学习（RL），属于视觉生成领域，与您的关注点无关。您的关注点明确排除了纯粹视觉论文、强化学习论文（除非明确与推荐系统/搜索/广告相关），以及纯粹LLM中心化主题。该论文没有显示出与推荐系统、搜索或广告领域的任何潜在应用联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02253v1": {
    "title": "Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission",
    "url": "https://www.alphaxiv.org/abs/2601.02253v1",
    "arxiv_id": "2601.02253v1",
    "authors": "Emrah Mete, Emin Erkan Korkmaz",
    "categories": "cs.LG, cs.AR, cs.CV",
    "pub_date": "2026-01-05 16:33:13",
    "ori_summary": "The rapid proliferation of Deep Learning is increasingly constrained by its heavy reliance on high-performance hardware, particularly Graphics Processing Units (GPUs). These specialized accelerators are not only prohibitively expensive and energy-intensive but also suffer from significant supply scarcity, limiting the ubiquity of Artificial Intelligence (AI) deployment on edge devices. The core of this inefficiency stems from the standard artificial perceptron's dependence on intensive matrix multiplications. However, biological nervous systems achieve unparalleled efficiency without such arithmetic intensity; synaptic signal transmission is regulated by physical ion channel limits and chemical neurotransmitter levels rather than a process that can be analogous to arithmetic multiplication. Inspired by this biological mechanism, we propose Neuro-Channel Networks (NCN), a novel multiplication-free architecture designed to decouple AI from expensive hardware dependencies. In our model, weights are replaced with Channel Widths that physically limit the signal magnitude, while a secondary parameter acts as a Neurotransmitter to regulate Signal Transmission based on sign logic. The forward pass relies exclusively on addition, subtraction, and bitwise operations (minimum, sign), eliminating floating-point multiplication entirely. In this proof-of-concept study, we demonstrate that NCNs can solve non-linearly separable problems like XOR and the Majority function with 100% accuracy using standard backpropagation, proving their capability to form complex decision boundaries without multiplicative weights. This architecture offers a highly efficient alternative for next-generation neuromorphic hardware, paving the way for running complex models on commodity CPUs or ultra-low-power chips without relying on costly GPU clusters.",
    "summary": "",
    "translation": "神经通道网络：一种基于生物信号传输的无乘法架构",
    "relevance_score": 2,
    "reasoning": "该论文提出了一种无乘法的神经网络架构，这属于Transformer架构效率改进的范畴（Enabling Transformer Tech），可能通过降低计算成本间接应用于大规模推荐系统。然而，标题中强调的“生物信号传输”表明其灵感主要来自生物学领域，这超出了您指定的核心关注范围，且没有明确指向推荐、搜索或广告的具体应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02249v1": {
    "title": "SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection",
    "url": "https://www.alphaxiv.org/abs/2601.02249v1",
    "arxiv_id": "2601.02249v1",
    "authors": "Xiantai Xiang, Guangyao Zhou, Zixiao Wen, Wenshuai Li, Ben Niu, Feng Wang, Lijia Huang, Qiantong Wang, Yuhan Liu, Zongxu Pan, Yuxin Hu",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 16:31:41",
    "ori_summary": "Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.",
    "summary": "",
    "translation": "SLGNet：融合结构先验与语言引导调制的多模态目标检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的多模态目标检测，虽然涉及语言引导调制，但其核心是视觉目标检测任务，与推荐系统、搜索或广告的排序/召回核心问题没有直接关联。论文中的语言引导可能涉及文本-视觉对齐，但缺乏明确的RecSys/Search/Ads应用场景说明，属于视觉领域论文。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02246v1": {
    "title": "A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets",
    "url": "https://www.alphaxiv.org/abs/2601.02246v1",
    "arxiv_id": "2601.02246v1",
    "authors": "Annoor Sharara Akhand",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2026-01-05 16:26:32",
    "ori_summary": "Convolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-trained backbone. This report presents a controlled comparison of these three paradigms across five real-world image classification datasets spanning road-surface defect recognition, agricultural variety identification, fruit/leaf disease recognition, pedestrian walkway encroachment recognition, and unauthorized vehicle recognition. Models are evaluated using accuracy and macro F1-score, complemented by efficiency metrics including training time per epoch and parameter counts. The results show that transfer learning consistently yields the strongest predictive performance, while the custom CNN provides an attractive efficiency--accuracy trade-off, especially when compute and memory budgets are constrained.",
    "summary": "",
    "translation": "跨多个视觉数据集的自定义CNN、预训练模型与迁移学习的比较研究",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的模型比较，特别是CNN架构和迁移学习在视觉数据集上的应用。虽然预训练模型和迁移学习是重要的技术，但论文明确限定在视觉数据上，没有探讨这些技术如何应用于推荐系统、搜索或广告中的异构数据处理，也没有涉及Transformer架构或LLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02242v1": {
    "title": "VIBE: Visual Instruction Based Editor",
    "url": "https://www.alphaxiv.org/abs/2601.02242v1",
    "arxiv_id": "2601.02242v1",
    "authors": "Grigorii Alekseenko, Aleksandr Gordeev, Irina Tolstykh, Bulat Suleimanov, Vladimir Dokholyan, Georgii Fedorov, Sergey Yakubson, Aleksandra Tsybina, Mikhail Chernyshov, Maksim Kuprashevich",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2026-01-05 16:17:20",
    "ori_summary": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.",
    "summary": "",
    "translation": "VIBE：基于视觉指令的编辑器",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向视觉指令编辑，属于纯粹的视觉或多模态内容生成领域，与推荐系统、搜索或广告的核心排序任务无关。虽然标题包含“指令”可能暗示LLM交互，但整体焦点是视觉编辑而非RecSys/Search/Ads应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02228v1": {
    "title": "FMVP: Masked Flow Matching for Adversarial Video Purification",
    "url": "https://www.alphaxiv.org/abs/2601.02228v1",
    "arxiv_id": "2601.02228v1",
    "authors": "Duoxun Tang, Xueyi Zhang, Chak Hin Wang, Xi Xiao, Dasen Dai, Xinhang Jiang, Wentao Shi, Rui Li, Qing Li",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 15:55:46",
    "ori_summary": "Video recognition models remain vulnerable to adversarial attacks, while existing diffusion-based purification methods suffer from inefficient sampling and curved trajectories. Directly regressing clean videos from adversarial inputs often fails to recover faithful content due to the subtle nature of perturbations; this necessitates physically shattering the adversarial structure. Therefore, we propose Flow Matching for Adversarial Video Purification FMVP. FMVP physically shatters global adversarial structures via a masking strategy and reconstructs clean video dynamics using Conditional Flow Matching (CFM) with an inpainting objective. To further decouple semantic content from adversarial noise, we design a Frequency-Gated Loss (FGL) that explicitly suppresses high-frequency adversarial residuals while preserving low-frequency fidelity. We design Attack-Aware and Generalist training paradigms to handle known and unknown threats, respectively. Extensive experiments on UCF-101 and HMDB-51 demonstrate that FMVP outperforms state-of-the-art methods (DiffPure, Defense Patterns (DP), Temporal Shuffling (TS) and FlowPure), achieving robust accuracy exceeding 87% against PGD and 89% against CW attacks. Furthermore, FMVP demonstrates superior robustness against adaptive attacks (DiffHammer) and functions as a zero-shot adversarial detector, attaining detection accuracies of 98% for PGD and 79% for highly imperceptible CW attacks.",
    "summary": "",
    "translation": "FMVP：基于掩码流匹配的对抗性视频净化",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及对抗性视频净化技术，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然视频内容可能出现在某些推荐场景中，但论文聚焦于对抗攻击防御而非推荐/搜索/广告的排名、匹配或建模问题，因此相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02212v1": {
    "title": "Prior-Guided DETR for Ultrasound Nodule Detection",
    "url": "https://www.alphaxiv.org/abs/2601.02212v1",
    "arxiv_id": "2601.02212v1",
    "authors": "Jingjing Wang, Zhuo Xiao, Xinning Yao, Bo Liu, Lijuan Niu, Xiangzhi Bai, Fugen Zhou",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 15:32:58",
    "ori_summary": "Accurate detection of ultrasound nodules is essential for the early diagnosis and treatment of thyroid and breast cancers. However, this task remains challenging due to irregular nodule shapes, indistinct boundaries, substantial scale variations, and the presence of speckle noise that degrades structural visibility. To address these challenges, we propose a prior-guided DETR framework specifically designed for ultrasound nodule detection. Instead of relying on purely data-driven feature learning, the proposed framework progressively incorporates different prior knowledge at multiple stages of the network. First, a Spatially-adaptive Deformable FFN with Prior Regularization (SDFPR) is embedded into the CNN backbone to inject geometric priors into deformable sampling, stabilizing feature extraction for irregular and blurred nodules. Second, a Multi-scale Spatial-Frequency Feature Mixer (MSFFM) is designed to extract multi-scale structural priors, where spatial-domain processing emphasizes contour continuity and boundary cues, while frequency-domain modeling captures global morphology and suppresses speckle noise. Furthermore, a Dense Feature Interaction (DFI) mechanism propagates and exploits these prior-modulated features across all encoder layers, enabling the decoder to enhance query refinement under consistent geometric and structural guidance. Experiments conducted on two clinically collected thyroid ultrasound datasets (Thyroid I and Thyroid II) and two public benchmarks (TN3K and BUSI) for thyroid and breast nodules demonstrate that the proposed method achieves superior accuracy compared with 18 detection methods, particularly in detecting morphologically complex nodules.The source code is publicly available at https://github.com/wjj1wjj/Ultrasound-DETR.",
    "summary": "",
    "translation": "用于超声结节检测的先验引导DETR",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学影像（超声结节检测）的特定应用，这属于明确的无关主题（医学/生物领域）。虽然DETR是Transformer架构的一种变体，但论文专注于医学诊断任务，没有表明对推荐系统、搜索或广告有任何潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02211v1": {
    "title": "Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion",
    "url": "https://www.alphaxiv.org/abs/2601.02211v1",
    "arxiv_id": "2601.02211v1",
    "authors": "Binglei Li, Mengping Yang, Zhiyu Tan, Junping Zhang, Hao Li",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 15:32:53",
    "ori_summary": "Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block's functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.",
    "summary": "",
    "translation": "解构MMDiT模块：基于文本条件的扩散模型的无训练分析与增强",
    "relevance_score": 2,
    "reasoning": "该论文主要研究文本条件扩散模型的架构分析，属于生成式AI领域。虽然涉及Transformer架构，但其核心应用方向是图像生成而非推荐系统、搜索或广告。论文缺乏与推荐/搜索/广告领域的直接联系或潜在应用场景说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02206v1": {
    "title": "Seeing the Unseen: Zooming in the Dark with Event Cameras",
    "url": "https://www.alphaxiv.org/abs/2601.02206v1",
    "arxiv_id": "2601.02206v1",
    "authors": "Dachun Kai, Zeyu Xiao, Huyue Zhu, Jiaxiao Wang, Yueyi Zhang, Xiaoyan Sun",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-05 15:31:07",
    "ori_summary": "This paper addresses low-light video super-resolution (LVSR), aiming to restore high-resolution videos from low-light, low-resolution (LR) inputs. Existing LVSR methods often struggle to recover fine details due to limited contrast and insufficient high-frequency information. To overcome these challenges, we present RetinexEVSR, the first event-driven LVSR framework that leverages high-contrast event signals and Retinex-inspired priors to enhance video quality under low-light scenarios. Unlike previous approaches that directly fuse degraded signals, RetinexEVSR introduces a novel bidirectional cross-modal fusion strategy to extract and integrate meaningful cues from noisy event data and degraded RGB frames. Specifically, an illumination-guided event enhancement module is designed to progressively refine event features using illumination maps derived from the Retinex model, thereby suppressing low-light artifacts while preserving high-contrast details. Furthermore, we propose an event-guided reflectance enhancement module that utilizes the enhanced event features to dynamically recover reflectance details via a multi-scale fusion mechanism. Experimental results show that our RetinexEVSR achieves state-of-the-art performance on three datasets. Notably, on the SDSD benchmark, our method can get up to 2.95 dB gain while reducing runtime by 65% compared to prior event-based methods. Code: https://github.com/DachunKai/RetinexEVSR.",
    "summary": "",
    "translation": "洞察未见：利用事件相机在黑暗中变焦",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向事件相机（一种视觉传感器）在低光条件下的应用，属于纯粹的计算机视觉领域。虽然事件相机处理动态视觉信息，但标题没有暗示任何与推荐系统、搜索或广告相关的应用，也没有涉及LLM、Transformer架构或多模态建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02204v1": {
    "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
    "url": "https://www.alphaxiv.org/abs/2601.02204v1",
    "arxiv_id": "2601.02204v1",
    "authors": "Huichao Zhang, Liao Qu, Yiheng Liu, Hang Chen, Yangyang Song, Yongsheng Dong, Shikun Sun, Xian Li, Xu Wang, Yi Jiang, Hu Ye, Bo Chen, Yiming Gao, Peng Liu, Akide Liu, Zhipeng Yang, Qili Deng, Linjie Xing, Jiyang Liu, Zhao Wang, Yang Zhou, Mingcong Liu, Yi Zhang, Qian He, Xiwei Hu, Zhongqi Qi, Jie Shao, Zhiye Fu, Shuai Wang, Fangmin Chen, Xuezhi Chai, Zhihua Wu, Yitong Wang, Zehuan Yuan, Daniel K. Du, Xinglong Wu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-05 15:27:04",
    "ori_summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.",
    "summary": "论文研究多模态统一建模问题，核心思想是通过文本的下一词预测与图像的下尺度预测相结合的自回归架构，实现跨模态理解与生成能力的原生激活。",
    "translation": "NextFlow：统一序列建模激活多模态理解与生成",
    "relevance_score": 8,
    "reasoning": "该论文提出统一序列建模方法，直接对应'VLM类比处理异构数据'的关注点，将多模态数据（如用户序列和上下文特征）作为不同模态进行统一建模。这种架构创新在推荐/搜索系统中具有直接应用潜力，可用于处理用户行为序列、物品特征等多源异构数据，实现更精准的个性化建模。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出统一序列建模框架，通过创新的多尺度视觉生成机制处理异构模态数据，与VLM类比和Transformer架构优化高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2601.02203v1": {
    "title": "Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules",
    "url": "https://www.alphaxiv.org/abs/2601.02203v1",
    "arxiv_id": "2601.02203v1",
    "authors": "Oliver Custance, Saad Khan, Simon Parkinson, Quan Z. Sheng",
    "categories": "cs.CV, cs.CR",
    "pub_date": "2026-01-05 15:27:04",
    "ori_summary": "Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\\% of a full fine-tune (98.84\\% vs. 99.67\\%) while training 97.2\\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments.",
    "summary": "",
    "translation": "基于适配器模块自监督学习的参数高效CSI人群计数领域自适应",
    "relevance_score": 1,
    "reasoning": "该论文关注无线信道状态信息（CSI）的人群计数，这属于计算机视觉中的特定应用领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然提到了参数高效和自监督学习，但这些技术在该论文中的应用是针对特定传感器数据（CSI）的领域自适应，没有明确展示在推荐/搜索/广告系统中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02201v1": {
    "title": "CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents",
    "url": "https://www.alphaxiv.org/abs/2601.02201v1",
    "arxiv_id": "2601.02201v1",
    "authors": "Keyu Wang, Bingchen Miao, Wendong Bu, Yu Wu, Juncheng Li, Shengyu Zhang, Wenqiao Zhang, Siliang Tang, Jun Xiao, Yueting Zhuang",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2026-01-05 15:24:05",
    "ori_summary": "The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.",
    "summary": "",
    "translation": "CORE：基于代码的逆自训练框架与图扩展用于虚拟代理",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于虚拟代理的代码生成和自训练框架，属于AIGC和内容生成领域。虽然涉及图扩展技术，但核心应用场景（虚拟代理）与推荐系统、搜索或广告的排名任务没有直接关联，也不属于核心LLM技术、Transformer架构进展或异构数据统一建模的范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02198v1": {
    "title": "Mind the Gap: Continuous Magnification Sampling for Pathology Foundation Models",
    "url": "https://www.alphaxiv.org/abs/2601.02198v1",
    "arxiv_id": "2601.02198v1",
    "authors": "Alexander Möllers, Julius Hense, Florian Schulz, Timo Milbich, Maximilian Alber, Lukas Ruff",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-01-05 15:19:59",
    "ori_summary": "In histopathology, pathologists examine both tissue architecture at low magnification and fine-grained morphology at high magnification. Yet, the performance of pathology foundation models across magnifications and the effect of magnification sampling during training remain poorly understood. We model magnification sampling as a multi-source domain adaptation problem and develop a simple theoretical framework that reveals systematic trade-offs between sampling strategies. We show that the widely used discrete uniform sampling of magnifications (0.25, 0.5, 1.0, 2.0 mpp) leads to degradation at intermediate magnifications. We introduce continuous magnification sampling, which removes gaps in magnification coverage while preserving performance at standard scales. Further, we derive sampling distributions that optimize representation quality across magnification scales. To evaluate these strategies, we introduce two new benchmarks (TCGA-MS, BRACS-MS) with appropriate metrics. Our experiments show that continuous sampling substantially improves over discrete sampling at intermediate magnifications, with gains of up to 4 percentage points in balanced classification accuracy, and that optimized distributions can further improve performance. Finally, we evaluate current histopathology foundation models, finding that magnification is a primary driver of performance variation across models. Our work paves the way towards future pathology foundation models that perform reliably across magnifications.",
    "summary": "",
    "translation": "弥合鸿沟：面向病理学基础模型的连续放大采样方法",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向病理学基础模型和医学图像分析领域，这属于明确的无关主题（医学/生物学领域特定应用）。标题中的'病理学基础模型'和'连续放大采样'都是医学图像分析的技术术语，与推荐系统、搜索或广告的核心技术进展、LLM应用或Transformer架构改进没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02189v1": {
    "title": "QuIC: A Quantum-Inspired Interaction Classifier for Revitalizing Shallow CNNs in Fine-Grained Recognition",
    "url": "https://www.alphaxiv.org/abs/2601.02189v1",
    "arxiv_id": "2601.02189v1",
    "authors": "Cheng Ying Wu, Yen Jui Chang",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-01-05 15:09:18",
    "ori_summary": "Deploying deep learning models for Fine-Grained Visual Classification (FGVC) on resource-constrained edge devices remains a significant challenge. While deep architectures achieve high accuracy on benchmarks like CUB-200-2011, their computational cost is often prohibitive. Conversely, shallow networks (e.g., AlexNet, VGG) offer efficiency but fail to distinguish visually similar sub-categories. This is because standard Global Average Pooling (GAP) heads capture only first-order statistics, missing the subtle high-order feature interactions required for FGVC. While Bilinear CNNs address this, they suffer from high feature dimensionality and instability during training. To bridge this gap, we propose the Quantum-inspired Interaction Classifier (QuIC). Drawing inspiration from quantum mechanics, QuIC models feature channels as interacting quantum states and captures second-order feature covariance via a learnable observable operator. Designed as a lightweight, plug-and-play module, QuIC supports stable, single-stage end-to-end training without exploding feature dimensions. Experimental results demonstrate that QuIC significantly revitalizes shallow backbones: it boosts the Top-1 accuracy of VGG16 by nearly 20% and outperforms state-of-the-art attention mechanisms (SE-Block) on ResNet18. Qualitative analysis, including t-SNE visualization, further confirms that QuIC resolves ambiguous cases by explicitly attending to fine-grained discriminative features and enforcing compact intra-class clustering.",
    "summary": "",
    "translation": "QuIC：一种量子启发的交互分类器，用于在细粒度识别中复兴浅层CNN",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于计算机视觉领域的细粒度识别任务，使用量子启发的CNN架构改进。这属于纯粹的视觉研究，没有提及任何与推荐系统、搜索或广告相关的应用或潜在联系。根据用户指定的无关主题，'Purely Vision、3D Vision, Graphic or Speech papers without clear relevance to RecSys/Search/Ads' 应被排除。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02177v1": {
    "title": "Why Commodity WiFi Sensors Fail at Multi-Person Gait Identification: A Systematic Analysis Using ESP32",
    "url": "https://www.alphaxiv.org/abs/2601.02177v1",
    "arxiv_id": "2601.02177v1",
    "authors": "Oliver Custance, Saad Khan, Simon Parkinson",
    "categories": "cs.CV, cs.CR",
    "pub_date": "2026-01-05 14:55:38",
    "ori_summary": "WiFi Channel State Information (CSI) has shown promise for single-person gait identification, with numerous studies reporting high accuracy. However, multi-person identification remains largely unexplored, with the limited existing work relying on complex, expensive setups requiring modified firmware. A critical question remains unanswered: is poor multi-person performance an algorithmic limitation or a fundamental hardware constraint? We systematically evaluate six diverse signal separation methods (FastICA, SOBI, PCA, NMF, Wavelet, Tensor Decomposition) across seven scenarios with 1-10 people using commodity ESP32 WiFi sensors--a simple, low-cost, off-the-shelf solution. Through novel diagnostic metrics (intra-subject variability, inter-subject distinguishability, performance degradation rate), we reveal that all methods achieve similarly low accuracy (45-56\\%, $σ$=3.74\\%) with statistically insignificant differences (p $>$ 0.05). Even the best-performing method, NMF, achieves only 56\\% accuracy. Our analysis reveals high intra-subject variability, low inter-subject distinguishability, and severe performance degradation as person count increases, indicating that commodity ESP32 sensors cannot provide sufficient signal quality for reliable multi-person separation.",
    "summary": "",
    "translation": "为何商用WiFi传感器在多人员步态识别中失效：基于ESP32的系统性分析",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于WiFi传感器在步态识别中的技术局限分析，这属于物联网传感技术领域，与推荐系统、搜索或广告的核心技术栈无直接关联。论文内容可能涉及信号处理或生物识别，但未表明与Transformer架构、LLM技术或推荐/搜索/广告系统的任何潜在应用连接。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02147v1": {
    "title": "BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2601.02147v1",
    "arxiv_id": "2601.02147v1",
    "authors": "Sunny Gupta, Shounak Das, Amit Sethi",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2026-01-05 14:22:20",
    "ori_summary": "Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.",
    "summary": "",
    "translation": "BiPrompt：面向视觉语言模型中视觉与文本去偏的双边提示优化",
    "relevance_score": 3,
    "reasoning": "该论文主要研究视觉语言模型中的去偏问题，属于VLM技术范畴，与“VLM Analogy for Heterogeneous Data”有一定概念关联，但未明确涉及推荐系统、搜索或广告中的异构数据处理。虽然提示优化技术可能间接应用于多模态推荐，但论文标题未体现对RecSys/Search/Ads的直接应用潜力，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02141v1": {
    "title": "Efficient Unrolled Networks for Large-Scale 3D Inverse Problems",
    "url": "https://www.alphaxiv.org/abs/2601.02141v1",
    "arxiv_id": "2601.02141v1",
    "authors": "Romain Vo, Julián Tachella",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 14:12:43",
    "ori_summary": "Deep learning-based methods have revolutionized the field of imaging inverse problems, yielding state-of-the-art performance across various imaging domains. The best performing networks incorporate the imaging operator within the network architecture, typically in the form of deep unrolling. However, in large-scale problems, such as 3D imaging, most existing methods fail to incorporate the operator in the architecture due to the prohibitive amount of memory required by global forward operators, which hinder typical patching strategies. In this work, we present a domain partitioning strategy and normal operator approximations that enable the training of end-to-end reconstruction models incorporating forward operators of arbitrarily large problems into their architecture. The proposed method achieves state-of-the-art performance on 3D X-ray cone-beam tomography and 3D multi-coil accelerated MRI, while requiring only a single GPU for both training and inference.",
    "summary": "",
    "translation": "用于大规模三维逆问题的高效展开网络",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及3D逆问题，属于计算机视觉/图像处理领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然提到了高效网络架构，但未表明与Transformer、LLM技术或推荐/搜索/广告应用有任何联系，因此完全不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02139v1": {
    "title": "Beyond Segmentation: An Oil Spill Change Detection Framework Using Synthetic SAR Imagery",
    "url": "https://www.alphaxiv.org/abs/2601.02139v1",
    "arxiv_id": "2601.02139v1",
    "authors": "Chenyang Lai, Shuaiyu Chen, Tianjin Huang, Siyang Song, Guangliang Cheng, Chunbo Luo, Zeyu Fu",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 14:10:13",
    "ori_summary": "Marine oil spills are urgent environmental hazards that demand rapid and reliable detection to minimise ecological and economic damage. While Synthetic Aperture Radar (SAR) imagery has become a key tool for large-scale oil spill monitoring, most existing detection methods rely on deep learning-based segmentation applied to single SAR images. These static approaches struggle to distinguish true oil spills from visually similar oceanic features (e.g., biogenic slicks or low-wind zones), leading to high false positive rates and limited generalizability, especially under data-scarce conditions. To overcome these limitations, we introduce Oil Spill Change Detection (OSCD), a new bi-temporal task that focuses on identifying changes between pre- and post-spill SAR images. As real co-registered pre-spill imagery is not always available, we propose the Temporal-Aware Hybrid Inpainting (TAHI) framework, which generates synthetic pre-spill images from post-spill SAR data. TAHI integrates two key components: High-Fidelity Hybrid Inpainting for oil-free reconstruction, and Temporal Realism Enhancement for radiometric and sea-state consistency. Using TAHI, we construct the first OSCD dataset and benchmark several state-of-the-art change detection models. Results show that OSCD significantly reduces false positives and improves detection accuracy compared to conventional segmentation, demonstrating the value of temporally-aware methods for reliable, scalable oil spill monitoring in real-world scenarios.",
    "summary": "",
    "translation": "超越分割：基于合成SAR图像的溢油变化检测框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于遥感图像处理中的溢油检测，属于环境监测领域。虽然涉及合成数据和变化检测技术，但缺乏与推荐系统、搜索或广告领域的明确联系，且不属于核心LLM技术、Transformer架构进展或异构数据统一建模的范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02126v1": {
    "title": "Remote Sensing Change Detection via Weak Temporal Supervision",
    "url": "https://www.alphaxiv.org/abs/2601.02126v1",
    "arxiv_id": "2601.02126v1",
    "authors": "Xavier Bou, Elliot Vincent, Gabriele Facciolo, Rafael Grompone von Gioi, Jean-Michel Morel, Thibaud Ehret",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-05 13:57:02",
    "ori_summary": "Semantic change detection in remote sensing aims to identify land cover changes between bi-temporal image pairs. Progress in this area has been limited by the scarcity of annotated datasets, as pixel-level annotation is costly and time-consuming. To address this, recent methods leverage synthetic data or generate artificial change pairs, but out-of-domain generalization remains limited. In this work, we introduce a weak temporal supervision strategy that leverages additional temporal observations of existing single-temporal datasets, without requiring any new annotations. Specifically, we extend single-date remote sensing datasets with new observations acquired at different times and train a change detection model by assuming that real bi-temporal pairs mostly contain no change, while pairing images from different locations to generate change examples. To handle the inherent noise in these weak labels, we employ an object-aware change map generation and an iterative refinement process. We validate our approach on extended versions of the FLAIR and IAILD aerial datasets, achieving strong zero-shot and low-data regime performance across different benchmarks. Lastly, we showcase results over large areas in France, highlighting the scalability potential of our method.",
    "summary": "",
    "translation": "基于弱时序监督的遥感变化检测",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向遥感领域的变化检测，属于计算机视觉中的特定应用方向。虽然涉及时序监督技术，但遥感变化检测与推荐系统、搜索或广告的核心领域没有直接关联，也不属于能够应用于这些领域的LLM或Transformer基础技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02112v1": {
    "title": "Car Drag Coefficient Prediction from 3D Point Clouds Using a Slice-Based Surrogate Model",
    "url": "https://www.alphaxiv.org/abs/2601.02112v1",
    "arxiv_id": "2601.02112v1",
    "authors": "Utkarsh Singh, Absaar Ali, Adarsh Roy",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-01-05 13:41:20",
    "ori_summary": "The automotive industry's pursuit of enhanced fuel economy and performance necessitates efficient aerodynamic design. However, traditional evaluation methods such as computational fluid dynamics (CFD) and wind tunnel testing are resource intensive, hindering rapid iteration in the early design stages. Machine learning-based surrogate models offer a promising alternative, yet many existing approaches suffer from high computational complexity, limited interpretability, or insufficient accuracy for detailed geometric inputs. This paper introduces a novel lightweight surrogate model for the prediction of the aerodynamic drag coefficient (Cd) based on a sequential slice-wise processing of the geometry of the 3D vehicle. Inspired by medical imaging, 3D point clouds of vehicles are decomposed into an ordered sequence of 2D cross-sectional slices along the stream-wise axis. Each slice is encoded by a lightweight PointNet2D module, and the sequence of slice embeddings is processed by a bidirectional LSTM to capture longitudinal geometric evolution. The model, trained and evaluated on the DrivAerNet++ dataset, achieves a high coefficient of determination (R^2 > 0.9528) and a low mean absolute error (MAE approx 6.046 x 10^{-3}) in Cd prediction. With an inference time of approximately 0.025 seconds per sample on a consumer-grade GPU, our approach provides fast, accurate, and interpretable aerodynamic feedback, facilitating more agile and informed automotive design exploration.",
    "summary": "",
    "translation": "基于切片代理模型从三维点云预测汽车风阻系数",
    "relevance_score": 1,
    "reasoning": "该论文专注于汽车工程领域的风阻系数预测，使用三维点云和代理模型方法，这属于物理模拟和计算流体动力学范畴。虽然涉及3D数据处理，但论文内容与推荐系统、搜索、广告、LLM技术或Transformer架构无直接关联，也不符合将异构数据作为不同模态进行统一建模的VLM类比思路。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02107v1": {
    "title": "MagicFight: Personalized Martial Arts Combat Video Generation",
    "url": "https://www.alphaxiv.org/abs/2601.02107v1",
    "arxiv_id": "2601.02107v1",
    "authors": "Jiancheng Huang, Mingfu Yan, Songyan Chen, Yi Huang, Shifeng Chen",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 13:34:17",
    "ori_summary": "Amid the surge in generic text-to-video generation, the field of personalized human video generation has witnessed notable advancements, primarily concentrated on single-person scenarios. However, to our knowledge, the domain of two-person interactions, particularly in the context of martial arts combat, remains uncharted. We identify a significant gap: existing models for single-person dancing generation prove insufficient for capturing the subtleties and complexities of two engaged fighters, resulting in challenges such as identity confusion, anomalous limbs, and action mismatches. To address this, we introduce a pioneering new task, Personalized Martial Arts Combat Video Generation. Our approach, MagicFight, is specifically crafted to overcome these hurdles. Given this pioneering task, we face a lack of appropriate datasets. Thus, we generate a bespoke dataset using the game physics engine Unity, meticulously crafting a multitude of 3D characters, martial arts moves, and scenes designed to represent the diversity of combat. MagicFight refines and adapts existing models and strategies to generate high-fidelity two-person combat videos that maintain individual identities and ensure seamless, coherent action sequences, thereby laying the groundwork for future innovations in the realm of interactive video content creation. Website: https://MingfuYAN.github.io/MagicFight/ Dataset: https://huggingface.co/datasets/MingfuYAN/KungFu-Fiesta",
    "summary": "",
    "translation": "MagicFight：个性化武术格斗视频生成",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于视频生成（Video Generation），这属于纯粹的视觉内容生成领域，与推荐系统、搜索或广告的核心技术无关。虽然标题包含“个性化”一词，但上下文表明这是针对武术格斗视频的生成，属于AIGC/内容生成范畴，属于明确列出的无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02103v1": {
    "title": "HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures",
    "url": "https://www.alphaxiv.org/abs/2601.02103v1",
    "arxiv_id": "2601.02103v1",
    "authors": "Yating Wang, Yuan Sun, Xuan Wang, Ran Yi, Boyao Zhou, Yipengjing Sun, Hongyu Liu, Yinuo Wang, Lizhuang Ma",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 13:32:37",
    "ori_summary": "Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.",
    "summary": "",
    "translation": "HeadLighter：通过光阶捕捉在生成式3D高斯头部模型中解耦光照",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机图形学中的3D生成和光照解耦技术，属于纯粹的视觉/图形学领域。虽然标题提及'生成式'，但其核心是3D头部模型的渲染和光照处理，与推荐系统、搜索或广告中的排序、用户建模、内容理解等核心技术无直接关联。该技术缺乏在RecSys/Search/Ads领域的明确应用路径，因此相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02102v1": {
    "title": "360-GeoGS: Geometrically Consistent Feed-Forward 3D Gaussian Splatting Reconstruction for 360 Images",
    "url": "https://www.alphaxiv.org/abs/2601.02102v1",
    "arxiv_id": "2601.02102v1",
    "authors": "Jiaqi Yao, Zhongmiao Yan, Jingyi Xu, Songpengcheng Xia, Yan Xiang, Ling Pei",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 13:28:28",
    "ori_summary": "3D scene reconstruction is fundamental for spatial intelligence applications such as AR, robotics, and digital twins. Traditional multi-view stereo struggles with sparse viewpoints or low-texture regions, while neural rendering approaches, though capable of producing high-quality results, require per-scene optimization and lack real-time efficiency. Explicit 3D Gaussian Splatting (3DGS) enables efficient rendering, but most feed-forward variants focus on visual quality rather than geometric consistency, limiting accurate surface reconstruction and overall reliability in spatial perception tasks. This paper presents a novel feed-forward 3DGS framework for 360 images, capable of generating geometrically consistent Gaussian primitives while maintaining high rendering quality. A Depth-Normal geometric regularization is introduced to couple rendered depth gradients with normal information, supervising Gaussian rotation, scale, and position to improve point cloud and surface accuracy. Experimental results show that the proposed method maintains high rendering quality while significantly improving geometric consistency, providing an effective solution for 3D reconstruction in spatial perception tasks.",
    "summary": "",
    "translation": "360-GeoGS：面向360度图像的几何一致前馈式3D高斯泼溅重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D视觉重建技术（360度图像到3D高斯泼溅表示），属于纯粹的计算机视觉领域。虽然标题提及“重建”，但未涉及推荐系统、搜索或广告中的异构数据处理、序列建模或排序等核心问题，也没有展示在推荐/搜索场景中处理多模态数据的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02098v1": {
    "title": "InpaintHuman: Reconstructing Occluded Humans with Multi-Scale UV Mapping and Identity-Preserving Diffusion Inpainting",
    "url": "https://www.alphaxiv.org/abs/2601.02098v1",
    "arxiv_id": "2601.02098v1",
    "authors": "Jinlong Fan, Shanshan Zhao, Liang Zheng, Jing Zhang, Yuxiang Yang, Mingming Gong",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 13:26:02",
    "ori_summary": "Reconstructing complete and animatable 3D human avatars from monocular videos remains challenging, particularly under severe occlusions. While 3D Gaussian Splatting has enabled photorealistic human rendering, existing methods struggle with incomplete observations, often producing corrupted geometry and temporal inconsistencies. We present InpaintHuman, a novel method for generating high-fidelity, complete, and animatable avatars from occluded monocular videos. Our approach introduces two key innovations: (i) a multi-scale UV-parameterized representation with hierarchical coarse-to-fine feature interpolation, enabling robust reconstruction of occluded regions while preserving geometric details; and (ii) an identity-preserving diffusion inpainting module that integrates textual inversion with semantic-conditioned guidance for subject-specific, temporally coherent completion. Unlike SDS-based methods, our approach employs direct pixel-level supervision to ensure identity fidelity. Experiments on synthetic benchmarks (PeopleSnapshot, ZJU-MoCap) and real-world scenarios (OcMotion) demonstrate competitive performance with consistent improvements in reconstruction quality across diverse poses and viewpoints.",
    "summary": "",
    "translation": "InpaintHuman：基于多尺度UV映射与身份保持扩散修复的遮挡人体重建",
    "relevance_score": 2,
    "reasoning": "该论文专注于计算机视觉中的人体重建与修复技术，属于纯粹的视觉处理领域。虽然扩散模型技术本身可能具有通用性，但论文标题明确聚焦于人体遮挡修复这一具体视觉任务，没有显示出与推荐系统、搜索或广告领域的直接关联或潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02096v1": {
    "title": "Dancing Points: Synthesizing Ballroom Dancing with Three-Point Inputs",
    "url": "https://www.alphaxiv.org/abs/2601.02096v1",
    "arxiv_id": "2601.02096v1",
    "authors": "Peizhuo Li, Sebastian Starke, Yuting Ye, Olga Sorkine-Hornung",
    "categories": "cs.GR, cs.CV",
    "pub_date": "2026-01-05 13:24:12",
    "ori_summary": "Ballroom dancing is a structured yet expressive motion category. Its highly diverse movement and complex interactions between leader and follower dancers make the understanding and synthesis challenging. We demonstrate that the three-point trajectory available from a virtual reality (VR) device can effectively serve as a dancer's motion descriptor, simplifying the modeling and synthesis of interplay between dancers' full-body motions down to sparse trajectories. Thanks to the low dimensionality, we can employ an efficient MLP network to predict the follower's three-point trajectory directly from the leader's three-point input for certain types of ballroom dancing, addressing the challenge of modeling high-dimensional full-body interaction. It also prevents our method from overfitting thanks to its compact yet explicit representation. By leveraging the inherent structure of the movements and carefully planning the autoregressive procedure, we show a deterministic neural network is able to translate three-point trajectories into a virtual embodied avatar, which is typically considered under-constrained and requires generative models for common motions. In addition, we demonstrate this deterministic approach generalizes beyond small, structured datasets like ballroom dancing, and performs robustly on larger, more diverse datasets such as LaFAN. Our method provides a computationally- and data-efficient solution, opening new possibilities for immersive paired dancing applications. Code and pre-trained models for this paper are available at https://peizhuoli.github.io/dancing-points.",
    "summary": "",
    "translation": "舞动点：基于三点输入的舞厅舞蹈合成",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及舞蹈动作合成，属于计算机图形学或运动生成领域，与推荐系统、搜索或广告的核心技术无关。标题中提到的三点输入和舞蹈合成技术，在推荐、搜索或广告领域没有明显的应用潜力，也不属于LLM、Transformer架构或异构数据处理等关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02091v1": {
    "title": "MCD-Net: A Lightweight Deep Learning Baseline for Optical-Only Moraine Segmentation",
    "url": "https://www.alphaxiv.org/abs/2601.02091v1",
    "arxiv_id": "2601.02091v1",
    "authors": "Zhehuan Cao, Fiseha Berhanu Tesema, Ping Fu, Jianfeng Ren, Ahmed Nasr",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 13:18:11",
    "ori_summary": "Glacial segmentation is essential for reconstructing past glacier dynamics and evaluating climate-driven landscape change. However, weak optical contrast and the limited availability of high-resolution DEMs hinder automated mapping. This study introduces the first large-scale optical-only moraine segmentation dataset, comprising 3,340 manually annotated high-resolution images from Google Earth covering glaciated regions of Sichuan and Yunnan, China. We develop MCD-Net, a lightweight baseline that integrates a MobileNetV2 encoder, a Convolutional Block Attention Module (CBAM), and a DeepLabV3+ decoder. Benchmarking against deeper backbones (ResNet152, Xception) shows that MCD-Net achieves 62.3\\% mean Intersection over Union (mIoU) and 72.8\\% Dice coefficient while reducing computational cost by more than 60\\%. Although ridge delineation remains constrained by sub-pixel width and spectral ambiguity, the results demonstrate that optical imagery alone can provide reliable moraine-body segmentation. The dataset and code are publicly available at https://github.com/Lyra-alpha/MCD-Net, establishing a reproducible benchmark for moraine-specific segmentation and offering a deployable baseline for high-altitude glacial monitoring.",
    "summary": "",
    "translation": "MCD-Net：一种用于纯光学冰碛分割的轻量级深度学习基准模型",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及计算机视觉中的图像分割任务，专注于地质学领域的冰碛识别，属于纯粹的视觉应用。标题中提到的“轻量级深度学习”可能涉及模型效率技术，但该论文专注于特定领域（地质/遥感）的视觉任务，与推荐系统、搜索或广告的核心技术焦点没有直接关联，也没有展示出将这些技术应用于推荐/搜索/广告领域的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02088v1": {
    "title": "PhysSFI-Net: Physics-informed Geometric Learning of Skeletal and Facial Interactions for Orthognathic Surgical Outcome Prediction",
    "url": "https://www.alphaxiv.org/abs/2601.02088v1",
    "arxiv_id": "2601.02088v1",
    "authors": "Jiahao Bao, Huazhen Liu, Yu Zhuang, Leran Tao, Xinyu Xu, Yongtao Shi, Mengjia Cheng, Yiming Wang, Congshuang Ku, Ting Zeng, Yilang Du, Siyi Chen, Shunyao Shen, Suncheng Xiang, Hongbo Yu",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 13:14:19",
    "ori_summary": "Orthognathic surgery repositions jaw bones to restore occlusion and enhance facial aesthetics. Accurate simulation of postoperative facial morphology is essential for preoperative planning. However, traditional biomechanical models are computationally expensive, while geometric deep learning approaches often lack interpretability. In this study, we develop and validate a physics-informed geometric deep learning framework named PhysSFI-Net for precise prediction of soft tissue deformation following orthognathic surgery. PhysSFI-Net consists of three components: a hierarchical graph module with craniofacial and surgical plan encoders combined with attention mechanisms to extract skeletal-facial interaction features; a Long Short-Term Memory (LSTM)-based sequential predictor for incremental soft tissue deformation; and a biomechanics-inspired module for high-resolution facial surface reconstruction. Model performance was assessed using point cloud shape error (Hausdorff distance), surface deviation error, and landmark localization error (Euclidean distances of craniomaxillofacial landmarks) between predicted facial shapes and corresponding ground truths. A total of 135 patients who underwent combined orthodontic and orthognathic treatment were included for model training and validation. Quantitative analysis demonstrated that PhysSFI-Net achieved a point cloud shape error of 1.070 +/- 0.088 mm, a surface deviation error of 1.296 +/- 0.349 mm, and a landmark localization error of 2.445 +/- 1.326 mm. Comparative experiments indicated that PhysSFI-Net outperformed the state-of-the-art method ACMT-Net in prediction accuracy. In conclusion, PhysSFI-Net enables interpretable, high-resolution prediction of postoperative facial morphology with superior accuracy, showing strong potential for clinical application in orthognathic surgical planning and simulation.",
    "summary": "",
    "translation": "PhysSFI-Net：基于物理信息的骨骼与面部交互几何学习用于正颌外科手术结果预测",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医学领域（正颌外科手术预测），属于明确的无关主题。标题中提到的物理信息几何学习虽然涉及技术方法，但完全应用于特定医疗场景，与推荐系统、搜索、广告等当前关注领域无任何直接或潜在关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02072v1": {
    "title": "SketchRodGS: Sketch-based Extraction of Slender Geometries for Animating Gaussian Splatting Scenes",
    "url": "https://www.alphaxiv.org/abs/2601.02072v1",
    "arxiv_id": "2601.02072v1",
    "authors": "Haato Watanabe, Nobuyuki Umetani",
    "categories": "cs.GR, cs.CV",
    "pub_date": "2026-01-05 12:51:12",
    "ori_summary": "Physics simulation of slender elastic objects often requires discretization as a polyline. However, constructing a polyline from Gaussian splatting is challenging as Gaussian splatting lacks connectivity information and the configuration of Gaussian primitives contains much noise. This paper presents a method to extract a polyline representation of the slender part of the objects in a Gaussian splatting scene from the user's sketching input. Our method robustly constructs a polyline mesh that represents the slender parts using the screen-space shortest path analysis that can be efficiently solved using dynamic programming. We demonstrate the effectiveness of our approach in several in-the-wild examples.",
    "summary": "",
    "translation": "SketchRodGS：基于草图的细长几何结构提取用于动画化高斯溅射场景",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及计算机图形学中的3D场景动画和几何提取技术，专注于高斯溅射渲染方法。这属于纯粹的计算机视觉/图形学领域，与推荐系统、搜索、广告或相关LLM技术无直接关联，也不符合您关注的异构数据建模或Transformer架构进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02046v1": {
    "title": "Agentic Retoucher for Text-To-Image Generation",
    "url": "https://www.alphaxiv.org/abs/2601.02046v1",
    "arxiv_id": "2601.02046v1",
    "authors": "Shaocheng Shen, Jianfeng Liang. Chunlei Cai, Cong Geng, Huiyu Duan, Xiaoyun Zhang, Qiang Hu, Guangtao Zhai",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-05 12:06:43",
    "ori_summary": "Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.",
    "summary": "",
    "translation": "面向文本到图像生成的智能修图代理",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本到图像生成中的图像编辑和修图技术，属于纯粹的AIGC和内容生成领域。虽然涉及智能代理，但核心应用是图像创作而非推荐、搜索或广告系统，与当前关注的推荐系统、搜索广告核心进展、Transformer架构效率、LLM直接应用或异构数据统一建模等方向均无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02038v1": {
    "title": "AlignVTOFF: Texture-Spatial Feature Alignment for High-Fidelity Virtual Try-Off",
    "url": "https://www.alphaxiv.org/abs/2601.02038v1",
    "arxiv_id": "2601.02038v1",
    "authors": "Yihan Zhu, Mengying Ge",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 11:50:02",
    "ori_summary": "Virtual Try-Off (VTOFF) is a challenging multimodal image generation task that aims to synthesize high-fidelity flat-lay garments under complex geometric deformation and rich high-frequency textures. Existing methods often rely on lightweight modules for fast feature extraction, which struggles to preserve structured patterns and fine-grained details, leading to texture attenuation during generation.To address these issues, we propose AlignVTOFF, a novel parallel U-Net framework built upon a Reference U-Net and Texture-Spatial Feature Alignment (TSFA). The Reference U-Net performs multi-scale feature extraction and enhances geometric fidelity, enabling robust modeling of deformation while retaining complex structured patterns. TSFA then injects the reference garment features into a frozen denoising U-Net via a hybrid attention design, consisting of a trainable cross-attention module and a frozen self-attention module. This design explicitly aligns texture and spatial cues and alleviates the loss of high-frequency information during the denoising process.Extensive experiments across multiple settings demonstrate that AlignVTOFF consistently outperforms state-of-the-art methods, producing flat-lay garment results with improved structural realism and high-frequency detail fidelity.",
    "summary": "",
    "translation": "AlignVTOFF：面向高保真虚拟试穿系统的纹理-空间特征对齐方法",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向虚拟试穿技术，属于计算机视觉中的特定应用领域。虽然涉及特征对齐技术，但核心是服装纹理和空间特征的视觉处理，与推荐系统、搜索或广告的排序、检索、个性化等核心技术无直接关联。该研究属于纯粹的视觉应用，不符合当前关注的任何技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02036v1": {
    "title": "GDRO: Group-level Reward Post-training Suitable for Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2601.02036v1",
    "arxiv_id": "2601.02036v1",
    "authors": "Yiyang Wang, Xi Chen, Xiaogang Xu, Yu Liu, Hengshuang Zhao",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2026-01-05 11:47:18",
    "ori_summary": "Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.",
    "summary": "",
    "translation": "GDRO：适用于扩散模型的群体级奖励后训练方法",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及扩散模型的训练方法，属于AIGC/内容生成领域，属于明确的无关主题。虽然提到了奖励机制，但没有明确指向推荐系统、搜索或广告中的排序应用，因此相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02029v1": {
    "title": "Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding",
    "url": "https://www.alphaxiv.org/abs/2601.02029v1",
    "arxiv_id": "2601.02029v1",
    "authors": "Toshihiko Nishimura, Hirofumi Abe, Kazuhiko Murasaki, Taiga Yoshida, Ryuichi Tanida",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 11:42:49",
    "ori_summary": "This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.",
    "summary": "",
    "translation": "利用二维视觉语言模型实现大规模户外场景理解中的无标签三维分割",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D视觉和户外场景分割，属于纯粹的计算机视觉领域研究。虽然提到了视觉语言模型（VLM），但其应用仅限于3D分割任务，没有展示与推荐系统、搜索或广告相关的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02020v1": {
    "title": "Adapting Depth Anything to Adverse Imaging Conditions with Events",
    "url": "https://www.alphaxiv.org/abs/2601.02020v1",
    "arxiv_id": "2601.02020v1",
    "authors": "Shihan Peng, Yuyang Xiong, Hanyu Zhou, Zhiwei Shi, Haoyue Liu, Gang Chen, Luxin Yan, Yi Chang",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 11:29:49",
    "ori_summary": "Robust depth estimation under dynamic and adverse lighting conditions is essential for robotic systems. Currently, depth foundation models, such as Depth Anything, achieve great success in ideal scenes but remain challenging under adverse imaging conditions such as extreme illumination and motion blur. These degradations corrupt the visual signals of frame cameras, weakening the discriminative features of frame-based depths across the spatial and temporal dimensions. Typically, existing approaches incorporate event cameras to leverage their high dynamic range and temporal resolution, aiming to compensate for corrupted frame features. However, such specialized fusion models are predominantly trained from scratch on domain-specific datasets, thereby failing to inherit the open-world knowledge and robust generalization inherent to foundation models. In this work, we propose ADAE, an event-guided spatiotemporal fusion framework for Depth Anything in degraded scenes. Our design is guided by two key insights: 1) Entropy-Aware Spatial Fusion. We adaptively merge frame-based and event-based features using an information entropy strategy to indicate illumination-induced degradation. 2) Motion-Guided Temporal Correction. We resort to the event-based motion cue to recalibrate ambiguous features in blurred regions. Under our unified framework, the two components are complementary to each other and jointly enhance Depth Anything under adverse imaging conditions. Extensive experiments have been performed to verify the superiority of the proposed method. Our code will be released upon acceptance.",
    "summary": "",
    "translation": "利用事件数据将Depth Anything模型适配至恶劣成像条件",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及计算机视觉中的深度估计和事件相机技术，属于纯粹的视觉领域研究。虽然提到了模型适配技术，但核心内容围绕视觉感知在恶劣条件下的改进，没有明确展示与推荐系统、搜索或广告领域的潜在应用关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02018v1": {
    "title": "Towards Any-Quality Image Segmentation via Generative and Adaptive Latent Space Enhancement",
    "url": "https://www.alphaxiv.org/abs/2601.02018v1",
    "arxiv_id": "2601.02018v1",
    "authors": "Guangqian Guo, Aixi Ren, Yong Guo, Xuehui Yu, Jiacheng Tian, Wenli Li, Yaoxing Wang, Shan Gao",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 11:28:58",
    "ori_summary": "Segment Anything Models (SAMs), known for their exceptional zero-shot segmentation performance, have garnered significant attention in the research community. Nevertheless, their performance drops significantly on severely degraded, low-quality images, limiting their effectiveness in real-world scenarios. To address this, we propose GleSAM++, which utilizes Generative Latent space Enhancement to boost robustness on low-quality images, thus enabling generalization across various image qualities. Additionally, to improve compatibility between the pre-trained diffusion model and the segmentation framework, we introduce two techniques, i.e., Feature Distribution Alignment (FDA) and Channel Replication and Expansion (CRE). However, the above components lack explicit guidance regarding the degree of degradation. The model is forced to implicitly fit a complex noise distribution that spans conditions from mild noise to severe artifacts, which substantially increases the learning burden and leads to suboptimal reconstructions. To address this issue, we further introduce a Degradation-aware Adaptive Enhancement (DAE) mechanism. The key principle of DAE is to decouple the reconstruction process for arbitrary-quality features into two stages: degradation-level prediction and degradation-aware reconstruction. Our method can be applied to pre-trained SAM and SAM2 with only minimal additional learnable parameters, allowing for efficient optimization. Extensive experiments demonstrate that GleSAM++ significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Furthermore, GleSAM++ also performs well on unseen degradations, underscoring the versatility of our approach and dataset.",
    "summary": "",
    "translation": "通过生成式与自适应潜在空间增强实现任意质量图像分割",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的图像分割技术，属于纯粹的视觉研究范畴。虽然提到了生成式方法和潜在空间增强，但论文标题明确聚焦于图像分割这一具体视觉任务，没有表明与推荐系统、搜索或广告领域的直接关联或潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02016v1": {
    "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
    "url": "https://www.alphaxiv.org/abs/2601.02016v1",
    "arxiv_id": "2601.02016v1",
    "authors": "Matthias Bartolo, Dylan Seychell, Gabriel Hili, Matthew Montebello, Carl James Debono, Saviour Formosa, Konstantinos Makantasis",
    "categories": "cs.CV, cs.AI, cs.ET, cs.LG",
    "pub_date": "2026-01-05 11:24:34",
    "ori_summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
    "summary": "",
    "translation": "利用特权信息增强目标检测：一种模型无关的师生方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的目标检测任务，虽然采用了师生框架这一通用技术，但其核心应用场景（目标检测）与推荐系统、搜索或广告的排序任务关联较弱。特权信息的使用可能启发多模态学习，但论文未明确展示在异构数据（如用户序列和上下文特征）统一建模方面的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.02008v1": {
    "title": "XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging",
    "url": "https://www.alphaxiv.org/abs/2601.02008v1",
    "arxiv_id": "2601.02008v1",
    "authors": "Midhat Urooj, Ayan Banerjee, Sandeep Gupta",
    "categories": "cs.AI, cs.CV",
    "pub_date": "2026-01-05 11:17:33",
    "ori_summary": "Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.",
    "summary": "",
    "translation": "XAI-MeD：可解释知识引导的神经符号框架，用于医学影像中的领域泛化与罕见类别检测",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医学影像领域（Medical Imaging），属于指定的不相关主题“Medical, Biology, Chemistry, Physics or other domain-specific applications”。虽然提到了可解释AI（XAI）和神经符号框架等通用技术概念，但其核心应用场景与推荐系统、搜索或广告完全无关，且未表明这些技术有向这些领域迁移的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01998v1": {
    "title": "Nighttime Hazy Image Enhancement via Progressively and Mutually Reinforcing Night-Haze Priors",
    "url": "https://www.alphaxiv.org/abs/2601.01998v1",
    "arxiv_id": "2601.01998v1",
    "authors": "Chen Zhu, Huiwen Zhang, Mu He, Yujie Li, Xiaotian Qiao",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 10:58:02",
    "ori_summary": "Enhancing the visibility of nighttime hazy images is challenging due to the complex degradation distributions. Existing methods mainly address a single type of degradation (e.g., haze or low-light) at a time, ignoring the interplay of different degradation types and resulting in limited visibility improvement. We observe that the domain knowledge shared between low-light and haze priors can be reinforced mutually for better visibility. Based on this key insight, in this paper, we propose a novel framework that enhances visibility in nighttime hazy images by reinforcing the intrinsic consistency between haze and low-light priors mutually and progressively. In particular, our model utilizes image-, patch-, and pixel-level experts that operate across visual and frequency domains to recover global scene structure, regional patterns, and fine-grained details progressively. A frequency-aware router is further introduced to adaptively guide the contribution of each expert, ensuring robust image restoration. Extensive experiments demonstrate the superior performance of our model on nighttime dehazing benchmarks both quantitatively and qualitatively. Moreover, we showcase the generalizability of our model in daytime dehazing and low-light enhancement tasks.",
    "summary": "",
    "translation": "通过渐进式与互增强的夜间雾霾先验实现夜间有雾图像增强",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于计算机视觉中的夜间有雾图像增强问题，属于纯粹的视觉处理任务。虽然提到了'先验'这一概念，但该技术专门针对夜间雾霾这一特定视觉场景，与推荐系统、搜索或广告中的异构数据处理、Transformer架构或LLM应用没有直接关联。该研究属于纯粹的视觉领域，不符合您关注的任何技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01992v1": {
    "title": "API: Empowering Generalizable Real-World Image Dehazing via Adaptive Patch Importance Learning",
    "url": "https://www.alphaxiv.org/abs/2601.01992v1",
    "arxiv_id": "2601.01992v1",
    "authors": "Chen Zhu, Huiwen Zhang, Yujie Li, Mu He, Xiaotian Qiao",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 10:53:41",
    "ori_summary": "Real-world image dehazing is a fundamental yet challenging task in low-level vision. Existing learning-based methods often suffer from significant performance degradation when applied to complex real-world hazy scenes, primarily due to limited training data and the intrinsic complexity of haze density distributions.To address these challenges, we introduce a novel Adaptive Patch Importance-aware (API) framework for generalizable real-world image dehazing. Specifically, our framework consists of an Automatic Haze Generation (AHG) module and a Density-aware Haze Removal (DHR) module. AHG provides a hybrid data augmentation strategy by generating realistic and diverse hazy images as additional high-quality training data. DHR considers hazy regions with varying haze density distributions for generalizable real-world image dehazing in an adaptive patch importance-aware manner. To alleviate the ambiguity of the dehazed image details, we further introduce a new Multi-Negative Contrastive Dehazing (MNCD) loss, which fully utilizes information from multiple negative samples across both spatial and frequency domains. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across multiple real-world benchmarks, delivering strong results in both quantitative metrics and qualitative visual quality, and exhibiting robust generalization across diverse haze distributions.",
    "summary": "",
    "translation": "API：通过自适应补丁重要性学习赋能可泛化的真实世界图像去雾",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的图像去雾任务，属于纯粹的视觉处理领域。虽然标题中提到“自适应学习”和“可泛化”，但这些概念并未与推荐系统、搜索或广告的异构数据处理建立明确联系。该研究缺乏对推荐/搜索/广告领域的具体应用潜力说明，因此与当前关注点高度不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01989v1": {
    "title": "VIT-Ped: Visionary Intention Transformer for Pedestrian Behavior Analysis",
    "url": "https://www.alphaxiv.org/abs/2601.01989v1",
    "arxiv_id": "2601.01989v1",
    "authors": "Aly R. Elkammar, Karim M. Gamaleldin, Catherine M. Elias",
    "categories": "cs.CV, cs.AI, cs.RO",
    "pub_date": "2026-01-05 10:48:12",
    "ori_summary": "Pedestrian Intention prediction is one of the key technologies in the transition from level 3 to level 4 autonomous driving. To understand pedestrian crossing behaviour, several elements and features should be taken into consideration to make the roads of tomorrow safer for everybody. We introduce a transformer / video vision transformer based algorithm of different sizes which uses different data modalities .We evaluated our algorithms on popular pedestrian behaviour dataset, JAAD, and have reached SOTA performance and passed the SOTA in metrics like Accuracy, AUC and F1-score. The advantages brought by different model design choices are investigated via extensive ablation studies.",
    "summary": "",
    "translation": "VIT-Ped：用于行人行为分析的视觉意图Transformer模型",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于计算机视觉领域中的行人行为分析，属于纯粹的视觉应用。虽然使用了Transformer架构，但内容完全围绕视觉模态（行人行为）展开，没有涉及推荐系统、搜索或广告领域所需的异构数据建模、用户序列处理或文本-特征交互等关键要素。标题中没有任何迹象表明该技术会被应用于或启发布局、搜索或广告系统。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01984v1": {
    "title": "Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation",
    "url": "https://www.alphaxiv.org/abs/2601.01984v1",
    "arxiv_id": "2601.01984v1",
    "authors": "Weijian Ma, Shizhao Sun, Tianyu Yu, Ruiyu Wang, Tat-Seng Chua, Jiang Bian",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 10:38:26",
    "ori_summary": "Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.",
    "summary": "",
    "translation": "蓝图思维：通过结构化对象表示辅助视觉语言模型进行空间推理",
    "relevance_score": 3,
    "reasoning": "该论文涉及视觉语言模型（VLM）和空间推理，与'VLM类比异构数据'焦点有一定关联，但主要针对视觉空间任务而非推荐/搜索/广告中的异构数据建模。虽然结构化表示思想可能启发推荐系统中的特征处理，但论文标题明确指向视觉空间推理，与当前聚焦领域的直接应用距离较远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01963v1": {
    "title": "Forget Less by Learning Together through Concept Consolidation",
    "url": "https://www.alphaxiv.org/abs/2601.01963v1",
    "arxiv_id": "2601.01963v1",
    "authors": "Arjun Ramesh Kaushik, Naresh Kumar Devulapally, Vishnu Suresh Lokhande, Nalini Ratha, Venu Govindaraju",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-01-05 10:14:16",
    "ori_summary": "Custom Diffusion Models (CDMs) have gained significant attention due to their remarkable ability to personalize generative processes. However, existing CDMs suffer from catastrophic forgetting when continuously learning new concepts. Most prior works attempt to mitigate this issue under the sequential learning setting with a fixed order of concept inflow and neglect inter-concept interactions. In this paper, we propose a novel framework - Forget Less by Learning Together (FL2T) - that enables concurrent and order-agnostic concept learning while addressing catastrophic forgetting. Specifically, we introduce a set-invariant inter-concept learning module where proxies guide feature selection across concepts, facilitating improved knowledge retention and transfer. By leveraging inter-concept guidance, our approach preserves old concepts while efficiently incorporating new ones. Extensive experiments, across three datasets, demonstrates that our method significantly improves concept retention and mitigates catastrophic forgetting, highlighting the effectiveness of inter-concept catalytic behavior in incremental concept learning of ten tasks with at least 2% gain on average CLIP Image Alignment scores.",
    "summary": "",
    "translation": "通过概念整合共同学习以减少遗忘",
    "relevance_score": 2,
    "reasoning": "该论文标题暗示了持续学习或灾难性遗忘的主题，这属于通用机器学习领域而非特定于推荐系统、搜索或广告。虽然持续学习在理论上可能应用于用户兴趣演化建模，但标题未明确指向Transformer架构、LLM技术或推荐/搜索/广告的具体应用，因此与当前关注点的直接相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01957v1": {
    "title": "AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing",
    "url": "https://www.alphaxiv.org/abs/2601.01957v1",
    "arxiv_id": "2601.01957v1",
    "authors": "Tianbo Wang, Yuqing Ma, Kewei Liao, Zhange Zhang, Simin Li, Jinyang Guo, Xianglong Liu",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 10:02:22",
    "ori_summary": "Large Vision-Language Models (LVLMs) have achieved substantial progress in cross-modal tasks. However, due to language bias, LVLMs are susceptible to object hallucination, which can be primarily divided into category, attribute, and relation hallucination, significantly impeding the trustworthy AI applications. Editing the internal activations of LVLMs has shown promising effectiveness in mitigating hallucinations with minimal cost. However, previous editing approaches neglect the effective guidance offered by factual textual semantics, thereby struggling to explicitly mitigate language bias. To address these issues, we propose Adaptive Factual-guided Visual-Textual Editing for hallucination mitigation (AFTER), which comprises Factual-Augmented Activation Steering (FAS) and Query-Adaptive Offset Optimization (QAO), to adaptively guides the original biased activations towards factual semantics. Specifically, FAS is proposed to provide factual and general guidance for activation editing, thereby explicitly modeling the precise visual-textual associations. Subsequently, QAO introduces a query-aware offset estimator to establish query-specific editing from the general steering vector, enhancing the diversity and granularity of editing. Extensive experiments on standard hallucination benchmarks across three widely adopted LVLMs validate the efficacy of the proposed AFTER, notably achieving up to a 16.3% reduction of hallucination over baseline on the AMBER benchmark. Our code and data will be released for reproducibility.",
    "summary": "",
    "translation": "AFTER：通过自适应事实引导的激活编辑缓解大型视觉语言模型的对象幻觉",
    "relevance_score": 2,
    "reasoning": "该论文主要关注大型视觉语言模型的对象幻觉问题，这属于纯粹的VLM/NLP评估和幻觉缓解范畴。虽然提到了视觉语言模型，但焦点是解决幻觉这一特定NLP问题，而非探索异构数据统一建模或VLM启发的推荐/搜索应用。该技术缺乏明确的推荐系统、搜索或广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01955v1": {
    "title": "MotionAdapter: Video Motion Transfer via Content-Aware Attention Customization",
    "url": "https://www.alphaxiv.org/abs/2601.01955v1",
    "arxiv_id": "2601.01955v1",
    "authors": "Zhexin Zhang, Yifeng Zhu, Yangyang Xu, Long Chen, Yong Du, Shengfeng He, Jun Yu",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 10:01:27",
    "ori_summary": "Recent advances in diffusion-based text-to-video models, particularly those built on the diffusion transformer architecture, have achieved remarkable progress in generating high-quality and temporally coherent videos. However, transferring complex motions between videos remains challenging. In this work, we present MotionAdapter, a content-aware motion transfer framework that enables robust and semantically aligned motion transfer within DiT-based T2V models. Our key insight is that effective motion transfer requires \\romannumeral1) explicit disentanglement of motion from appearance and \\romannumeral 2) adaptive customization of motion to target content. MotionAdapter first isolates motion by analyzing cross-frame attention within 3D full-attention modules to extract attention-derived motion fields. To bridge the semantic gap between reference and target videos, we further introduce a DINO-guided motion customization module that rearranges and refines motion fields based on content correspondences. The customized motion field is then used to guide the DiT denoising process, ensuring that the synthesized video inherits the reference motion while preserving target appearance and semantics. Extensive experiments demonstrate that MotionAdapter outperforms state-of-the-art methods in both qualitative and quantitative evaluations. Moreover, MotionAdapter naturally supports complex motion transfer and motion editing tasks such as zooming.",
    "summary": "",
    "translation": "MotionAdapter：基于内容感知注意力定制的视频运动迁移",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及视频运动迁移，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术关联较弱。虽然注意力机制是Transformer架构的关键组件，但论文专注于视频内容处理，未明确展示其在推荐、搜索或广告中的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01950v1": {
    "title": "Face Normal Estimation from Rags to Riches",
    "url": "https://www.alphaxiv.org/abs/2601.01950v1",
    "arxiv_id": "2601.01950v1",
    "authors": "Meng Wang, Wenjing Dai, Jiawan Zhang, Xiaojie Guo",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 09:57:24",
    "ori_summary": "Although recent approaches to face normal estimation have achieved promising results, their effectiveness heavily depends on large-scale paired data for training. This paper concentrates on relieving this requirement via developing a coarse-to-fine normal estimator. Concretely, our method first trains a neat model from a small dataset to produce coarse face normals that perform as guidance (called exemplars) for the following refinement. A self-attention mechanism is employed to capture long-range dependencies, thus remedying severe local artifacts left in estimated coarse facial normals. Then, a refinement network is customized for the sake of mapping input face images together with corresponding exemplars to fine-grained high-quality facial normals. Such a logical function split can significantly cut the requirement of massive paired data and computational resource. Extensive experiments and ablation studies are conducted to demonstrate the efficacy of our design and reveal its superiority over state-of-the-art methods in terms of both training expense as well as estimation quality. Our code and models are open-sourced at: https://github.com/AutoHDR/FNR2R.git.",
    "summary": "",
    "translation": "从粗到精的面部法线估计",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及计算机视觉中的面部法线估计，属于纯粹的视觉处理任务。根据用户列出的不相关主题，明确排除了'纯粹视觉、3D视觉、图形或语音论文，除非与推荐系统/搜索/广告有明确相关性'。该论文没有显示出与推荐系统、搜索或广告领域的任何直接或间接联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01926v1": {
    "title": "MacVQA: Adaptive Memory Allocation and Global Noise Filtering for Continual Visual Question Answering",
    "url": "https://www.alphaxiv.org/abs/2601.01926v1",
    "arxiv_id": "2601.01926v1",
    "authors": "Zhifei Li, Yiran Wang, Chenyi Xiong, Yujing Xia, Xiaoju Hou, Yue Zhao, Miao Zhang, Kui Xiao, Bing Yang",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 09:18:09",
    "ori_summary": "Visual Question Answering (VQA) requires models to reason over multimodal information, combining visual and textual data. With the development of continual learning, significant progress has been made in retaining knowledge and adapting to new information in the VQA domain. However, current methods often struggle with balancing knowledge retention, adaptation, and robust feature representation. To address these challenges, we propose a novel framework with adaptive memory allocation and global noise filtering called MacVQA for visual question answering. MacVQA fuses visual and question information while filtering noise to ensure robust representations, and employs prototype-based memory allocation to optimize feature quality and memory usage. These designs enable MacVQA to balance knowledge acquisition, retention, and compositional generalization in continual VQA learning. Experiments on ten continual VQA tasks show that MacVQA outperforms existing baselines, achieving 43.38% average accuracy and 2.32% average forgetting on standard tasks, and 42.53% average accuracy and 3.60% average forgetting on novel composition tasks.",
    "summary": "",
    "translation": "MacVQA：持续视觉问答中的自适应内存分配与全局噪声过滤",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉问答（VQA）领域的持续学习问题，属于纯粹的视觉-语言交叉领域研究。虽然涉及内存管理和噪声过滤技术，但这些技术主要针对视觉模态的持续学习场景，与推荐系统、搜索或广告领域的异构数据处理、Transformer架构改进或LLM直接应用没有明确关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01925v1": {
    "title": "AR-MOT: Autoregressive Multi-object Tracking",
    "url": "https://www.alphaxiv.org/abs/2601.01925v1",
    "arxiv_id": "2601.01925v1",
    "authors": "Lianjie Jia, Yuhan Wu, Binghao Ran, Yifan Wang, Lijun Wang, Huchuan Lu",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 09:17:28",
    "ori_summary": "As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.",
    "summary": "",
    "translation": "AR-MOT：自回归多目标跟踪",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及计算机视觉中的多目标跟踪技术，属于纯粹的视觉领域研究。虽然自回归（AR）建模在序列处理中具有通用性，但论文标题明确指向视觉跟踪任务，没有表明与推荐系统、搜索或广告领域的潜在应用联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01915v1": {
    "title": "TalkPhoto: A Versatile Training-Free Conversational Assistant for Intelligent Image Editing",
    "url": "https://www.alphaxiv.org/abs/2601.01915v1",
    "arxiv_id": "2601.01915v1",
    "authors": "Yujie Hu, Zecheng Tang, Xu Jiang, Weiqi Li, Jian Zhang",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 09:00:32",
    "ori_summary": "Thanks to the powerful language comprehension capabilities of Large Language Models (LLMs), existing instruction-based image editing methods have introduced Multimodal Large Language Models (MLLMs) to promote information exchange between instructions and images, ensuring the controllability and flexibility of image editing. However, these frameworks often build a multi-instruction dataset to train the model to handle multiple editing tasks, which is not only time-consuming and labor-intensive but also fails to achieve satisfactory results. In this paper, we present TalkPhoto, a versatile training-free image editing framework that facilitates precise image manipulation through conversational interaction. We instruct the open-source LLM with a specially designed prompt template to analyze user needs after receiving instructions and hierarchically invoke existing advanced editing methods, all without additional training. Moreover, we implement a plug-and-play and efficient invocation of image editing methods, allowing complex and unseen editing tasks to be integrated into the current framework, achieving stable and high-quality editing results. Extensive experiments demonstrate that our method not only provides more accurate invocation with fewer token consumption but also achieves higher editing quality across various image editing tasks.",
    "summary": "",
    "translation": "TalkPhoto：一种用于智能图像编辑的多功能免训练对话助手",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于图像编辑领域的对话助手，属于纯粹的视觉内容生成应用。虽然涉及对话交互，但核心是图像编辑而非推荐系统、搜索或广告中的排名、检索或建模任务。该技术没有明显的应用潜力于推荐系统、搜索或广告领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01914v1": {
    "title": "Learning Action Hierarchies via Hybrid Geometric Diffusion",
    "url": "https://www.alphaxiv.org/abs/2601.01914v1",
    "arxiv_id": "2601.01914v1",
    "authors": "Arjun Ramesh Kaushik, Nalini K. Ratha, Venu Govindaraju",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 08:59:07",
    "ori_summary": "Temporal action segmentation is a critical task in video understanding, where the goal is to assign action labels to each frame in a video. While recent advances leverage iterative refinement-based strategies, they fail to explicitly utilize the hierarchical nature of human actions. In this work, we propose HybridTAS - a novel framework that incorporates a hybrid of Euclidean and hyperbolic geometries into the denoising process of diffusion models to exploit the hierarchical structure of actions. Hyperbolic geometry naturally provides tree-like relationships between embeddings, enabling us to guide the action label denoising process in a coarse-to-fine manner: higher diffusion timesteps are influenced by abstract, high-level action categories (root nodes), while lower timesteps are refined using fine-grained action classes (leaf nodes). Extensive experiments on three benchmark datasets, GTEA, 50Salads, and Breakfast, demonstrate that our method achieves state-of-the-art performance, validating the effectiveness of hyperbolic-guided denoising for the temporal action segmentation task.",
    "summary": "",
    "translation": "通过混合几何扩散学习动作层次结构",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及动作层次结构学习和扩散方法，可能属于强化学习或机器人控制领域。虽然扩散模型在生成建模方面有应用，但标题未明确表明与推荐系统、搜索或广告的直接关联，也没有提到Transformer架构、多模态学习或LLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01908v1": {
    "title": "Nodule-DETR: A Novel DETR Architecture with Frequency-Channel Attention for Ultrasound Thyroid Nodule Detection",
    "url": "https://www.alphaxiv.org/abs/2601.01908v1",
    "arxiv_id": "2601.01908v1",
    "authors": "Jingjing Wang, Qianglin Liu, Zhuo Xiao, Xinning Yao, Bo Liu, Lu Li, Lijuan Niu, Fugen Zhou",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-05 08:53:04",
    "ori_summary": "Thyroid cancer is the most common endocrine malignancy, and its incidence is rising globally. While ultrasound is the preferred imaging modality for detecting thyroid nodules, its diagnostic accuracy is often limited by challenges such as low image contrast and blurred nodule boundaries. To address these issues, we propose Nodule-DETR, a novel detection transformer (DETR) architecture designed for robust thyroid nodule detection in ultrasound images. Nodule-DETR introduces three key innovations: a Multi-Spectral Frequency-domain Channel Attention (MSFCA) module that leverages frequency analysis to enhance features of low-contrast nodules; a Hierarchical Feature Fusion (HFF) module for efficient multi-scale integration; and Multi-Scale Deformable Attention (MSDA) to flexibly capture small and irregularly shaped nodules. We conducted extensive experiments on a clinical dataset of real-world thyroid ultrasound images. The results demonstrate that Nodule-DETR achieves state-of-the-art performance, outperforming the baseline model by a significant margin of 0.149 in mAP@0.5:0.95. The superior accuracy of Nodule-DETR highlights its significant potential for clinical application as an effective tool in computer-aided thyroid diagnosis. The code of work is available at https://github.com/wjj1wjj/Nodule-DETR.",
    "summary": "",
    "translation": "Nodule-DETR：一种具有频域通道注意力机制的新型DETR架构，用于超声甲状腺结节检测",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医学影像（超声甲状腺结节检测）这一领域特定应用，属于“Irrelevant Topics”中明确排除的“Medical, Biology, Chemistry, Physics or other domain-specific applications”。虽然提及了DETR架构（一种基于Transformer的检测模型），但其应用场景与推荐系统、搜索或广告的核心技术领域无直接关联，且未暗示任何潜在的应用迁移可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01892v1": {
    "title": "Forget Less by Learning from Parents Through Hierarchical Relationships",
    "url": "https://www.alphaxiv.org/abs/2601.01892v1",
    "arxiv_id": "2601.01892v1",
    "authors": "Arjun Ramesh Kaushik, Naresh Kumar Devulapally, Vishnu Suresh Lokhande, Nalini K. Ratha, Venu Govindaraju",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-01-05 08:35:36",
    "ori_summary": "Custom Diffusion Models (CDMs) offer impressive capabilities for personalization in generative modeling, yet they remain vulnerable to catastrophic forgetting when learning new concepts sequentially. Existing approaches primarily focus on minimizing interference between concepts, often neglecting the potential for positive inter-concept interactions. In this work, we present Forget Less by Learning from Parents (FLLP), a novel framework that introduces a parent-child inter-concept learning mechanism in hyperbolic space to mitigate forgetting. By embedding concept representations within a Lorentzian manifold, naturally suited to modeling tree-like hierarchies, we define parent-child relationships in which previously learned concepts serve as guidance for adapting to new ones. Our method not only preserves prior knowledge but also supports continual integration of new concepts. We validate FLLP on three public datasets and one synthetic benchmark, showing consistent improvements in both robustness and generalization.",
    "summary": "",
    "translation": "通过层级关系向父节点学习以减少遗忘",
    "relevance_score": 3,
    "reasoning": "该标题涉及机器学习中的持续学习或知识保留问题，可能属于模型效率或架构改进的范畴。然而，标题未明确说明其与推荐系统、搜索或广告的具体关联，也未提及Transformer、LLM或异构数据处理等关键技术，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01891v1": {
    "title": "Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems",
    "url": "https://www.alphaxiv.org/abs/2601.01891v1",
    "arxiv_id": "2601.01891v1",
    "authors": "Niloufar Alipour Talemi, Julia Boone, Fatemeh Afghah",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 08:34:17",
    "ori_summary": "The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.",
    "summary": "",
    "translation": "遥感领域中的智能体人工智能：基础、分类与新兴系统",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于遥感（Remote Sensing）这一特定领域应用，属于明确的“Irrelevant Topics”中提到的“Medical, Biology, Chemistry, Physics or other domain-specific applications”。虽然涉及“Agentic AI”这一概念，但其核心应用场景（遥感）与用户关注的推荐系统、搜索、广告等核心领域无直接关联，且标题未暗示任何潜在的跨领域应用可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01874v1": {
    "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
    "url": "https://www.alphaxiv.org/abs/2601.01874v1",
    "arxiv_id": "2601.01874v1",
    "authors": "Shuhang Chen, Yunqiu Xu, Junjie Xie, Aojun Lu, Tao Feng, Zeying Huang, Ning Zhang, Yi Sun, Yi Yang, Hangjie Yuan",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-05 08:02:18",
    "ori_summary": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\\Rightarrow$internalization$\\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.",
    "summary": "",
    "translation": "CogFlow：通过知识内化连接感知与推理，用于视觉数学问题求解",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉数学问题求解，属于视觉推理的特定领域应用，与推荐系统、搜索或广告的核心技术没有直接关联。虽然涉及感知与推理的桥接，但其应用场景过于具体，缺乏在RecSys/Search/Ads领域的明确应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01870v1": {
    "title": "Entity-Guided Multi-Task Learning for Infrared and Visible Image Fusion",
    "url": "https://www.alphaxiv.org/abs/2601.01870v1",
    "arxiv_id": "2601.01870v1",
    "authors": "Wenyu Shao, Hongbo Liu, Yunchuan Ma, Ruili Wang",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 08:00:03",
    "ori_summary": "Existing text-driven infrared and visible image fusion approaches often rely on textual information at the sentence level, which can lead to semantic noise from redundant text and fail to fully exploit the deeper semantic value of textual information. To address these issues, we propose a novel fusion approach named Entity-Guided Multi-Task learning for infrared and visible image fusion (EGMT). Our approach includes three key innovative components: (i) A principled method is proposed to extract entity-level textual information from image captions generated by large vision-language models, eliminating semantic noise from raw text while preserving critical semantic information; (ii) A parallel multi-task learning architecture is constructed, which integrates image fusion with a multi-label classification task. By using entities as pseudo-labels, the multi-label classification task provides semantic supervision, enabling the model to achieve a deeper understanding of image content and significantly improving the quality and semantic density of the fused image; (iii) An entity-guided cross-modal interactive module is also developed to facilitate the fine-grained interaction between visual and entity-level textual features, which enhances feature representation by capturing cross-modal dependencies at both inter-visual and visual-entity levels. To promote the wide application of the entity-guided image fusion framework, we release the entity-annotated version of four public datasets (i.e., TNO, RoadScene, M3FD, and MSRS). Extensive experiments demonstrate that EGMT achieves superior performance in preserving salient targets, texture details, and semantic consistency, compared to the state-of-the-art methods. The code and dataset will be publicly available at https://github.com/wyshao-01/EGMT.",
    "summary": "",
    "translation": "实体引导的红外与可见光图像融合多任务学习",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及红外与可见光图像融合，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术无关。虽然提到了多任务学习，但应用场景（图像融合）与当前关注的文本/序列数据建模、Transformer架构或LLM应用没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01865v1": {
    "title": "RRNet: Configurable Real-Time Video Enhancement with Arbitrary Local Lighting Variations",
    "url": "https://www.alphaxiv.org/abs/2601.01865v1",
    "arxiv_id": "2601.01865v1",
    "authors": "Wenlong Yang, Canran Jin, Weihang Yuan, Chao Wang, Lifeng Sun",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 07:50:59",
    "ori_summary": "With the growing demand for real-time video enhancement in live applications, existing methods often struggle to balance speed and effective exposure control, particularly under uneven lighting. We introduce RRNet (Rendering Relighting Network), a lightweight and configurable framework that achieves a state-of-the-art tradeoff between visual quality and efficiency. By estimating parameters for a minimal set of virtual light sources, RRNet enables localized relighting through a depth-aware rendering module without requiring pixel-aligned training data. This object-aware formulation preserves facial identity and supports real-time, high-resolution performance using a streamlined encoder and lightweight prediction head. To facilitate training, we propose a generative AI-based dataset creation pipeline that synthesizes diverse lighting conditions at low cost. With its interpretable lighting control and efficient architecture, RRNet is well suited for practical applications such as video conferencing, AR-based portrait enhancement, and mobile photography. Experiments show that RRNet consistently outperforms prior methods in low-light enhancement, localized illumination adjustment, and glare removal.",
    "summary": "",
    "translation": "RRNet：可配置的实时视频增强技术，适用于任意局部光照变化",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的视频增强技术，特别是处理局部光照变化，这属于纯粹的视觉处理领域。虽然标题提到“实时”和“可配置”，但核心内容与推荐系统、搜索或广告的排名、建模或架构创新没有直接关联，也没有涉及LLM、Transformer或异构数据统一建模等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01856v1": {
    "title": "GCR: Geometry-Consistent Routing for Task-Agnostic Continual Anomaly Detection",
    "url": "https://www.alphaxiv.org/abs/2601.01856v1",
    "arxiv_id": "2601.01856v1",
    "authors": "Joongwon Chae, Lihui Luo, Yang Liu, Runming Wang, Dongmei Yu, Zeming Liang, Xi Yuan, Dayan Zhang, Zhenglin Chen, Peiwu Qin, Ilmoon Chae",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 07:33:50",
    "ori_summary": "Feature-based anomaly detection is widely adopted in industrial inspection due to the strong representational power of large pre-trained vision encoders. While most existing methods focus on improving within-category anomaly scoring, practical deployments increasingly require task-agnostic operation under continual category expansion, where the category identity is unknown at test time. In this setting, overall performance is often dominated by expert selection, namely routing an input to an appropriate normality model before any head-specific scoring is applied. However, routing rules that compare head-specific anomaly scores across independently constructed heads are unreliable in practice, as score distributions can differ substantially across categories in scale and tail behavior. We propose GCR, a lightweight mixture-of-experts framework for stabilizing task-agnostic continual anomaly detection through geometry-consistent routing. GCR routes each test image directly in a shared frozen patch-embedding space by minimizing an accumulated nearest-prototype distance to category-specific prototype banks, and then computes anomaly maps only within the routed expert using a standard prototype-based scoring rule. By separating cross-head decision making from within-head anomaly scoring, GCR avoids cross-head score comparability issues without requiring end-to-end representation learning. Experiments on MVTec AD and VisA show that geometry-consistent routing substantially improves routing stability and mitigates continual performance collapse, achieving near-zero forgetting while maintaining competitive detection and localization performance. These results indicate that many failures previously attributed to representation forgetting can instead be explained by decision-rule instability in cross-head routing. Code is available at https://github.com/jw-chae/GCR",
    "summary": "",
    "translation": "GCR：面向任务无关持续异常检测的几何一致性路由",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及异常检测和持续学习，属于计算机视觉或时间序列分析领域，与推荐系统、搜索或广告的核心技术无直接关联。标题中提到的几何一致性路由可能指神经网络架构设计，但未明确指向Transformer、LLM技术或其在推荐/搜索/广告中的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01847v1": {
    "title": "ESGaussianFace: Emotional and Stylized Audio-Driven Facial Animation via 3D Gaussian Splatting",
    "url": "https://www.alphaxiv.org/abs/2601.01847v1",
    "arxiv_id": "2601.01847v1",
    "authors": "Chuhang Ma, Shuai Tan, Ye Pan, Jiaolong Yang, Xin Tong",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 07:19:38",
    "ori_summary": "Most current audio-driven facial animation research primarily focuses on generating videos with neutral emotions. While some studies have addressed the generation of facial videos driven by emotional audio, efficiently generating high-quality talking head videos that integrate both emotional expressions and style features remains a significant challenge. In this paper, we propose ESGaussianFace, an innovative framework for emotional and stylized audio-driven facial animation. Our approach leverages 3D Gaussian Splatting to reconstruct 3D scenes and render videos, ensuring efficient generation of 3D consistent results. We propose an emotion-audio-guided spatial attention method that effectively integrates emotion features with audio content features. Through emotion-guided attention, the model is able to reconstruct facial details across different emotional states more accurately. To achieve emotional and stylized deformations of the 3D Gaussian points through emotion and style features, we introduce two 3D Gaussian deformation predictors. Futhermore, we propose a multi-stage training strategy, enabling the step-by-step learning of the character's lip movements, emotional variations, and style features. Our generated results exhibit high efficiency, high quality, and 3D consistency. Extensive experimental results demonstrate that our method outperforms existing state-of-the-art techniques in terms of lip movement accuracy, expression variation, and style feature expressiveness.",
    "summary": "",
    "translation": "ESGaussianFace：基于3D高斯泼溅的情感与风格化音频驱动面部动画",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D面部动画生成技术，属于计算机图形学/视觉领域，与推荐系统、搜索或广告的核心技术栈无直接关联。论文内容涉及音频驱动、情感风格化等AIGC/内容生成方向，属于明确排除的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01835v1": {
    "title": "RSwinV2-MD: An Enhanced Residual SwinV2 Transformer for Monkeypox Detection from Skin Images",
    "url": "https://www.alphaxiv.org/abs/2601.01835v1",
    "arxiv_id": "2601.01835v1",
    "authors": "Rashid Iqbal, Saddam Hussain Khan",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-05 06:57:26",
    "ori_summary": "In this paper, a deep learning approach for Mpox diagnosis named Customized Residual SwinTransformerV2 (RSwinV2) has been proposed, trying to enhance the capability of lesion classification by employing the RSwinV2 tool-assisted vision approach. In the RSwinV2 method, a hierarchical structure of the transformer has been customized based on the input dimensionality, embedding structure, and output targeted by the method. In this RSwinV2 approach, the input image has been split into non-overlapping patches and processed using shifted windows and attention in these patches. This process has helped the method link all the windows efficiently by avoiding the locality issues of non-overlapping regions in attention, while being computationally efficient. RSwinV2 has further developed based on SwinTransformer and has included patch and position embeddings to take advantage of the transformer global-linking capability by employing multi-head attention in these embeddings. Furthermore, RSwinV2 has developed and incorporated the Inverse Residual Block (IRB) into this method, which utilizes convolutional skip connections with these inclusive designs to address the vanishing gradient issues during processing. RSwinV2 inclusion of IRB has therefore facilitated this method to link global patterns as well as local patterns; hence, its integrity has helped improve lesion classification capability by minimizing variability of Mpox and increasing differences of Mpox, chickenpox, measles, and cowpox. In testing SwinV2, its accuracy of 96.21 and an F1score of 95.62 have been achieved on the Kaggle public dataset, which has outperformed standard CNN models and SwinTransformers; RSwinV2 vector has thus proved its valiance as a computer-assisted tool for Mpox lesion observation interpretation.",
    "summary": "",
    "translation": "RSwinV2-MD：一种用于皮肤图像中猴痘检测的增强型残差SwinV2 Transformer",
    "relevance_score": 1,
    "reasoning": "该论文专注于猴痘检测这一医学应用领域，属于明确的无关主题。虽然提到了SwinV2 Transformer架构，但其应用场景（医学图像分析）与推荐系统、搜索或广告领域没有直接关联，也不符合任何当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01822v1": {
    "title": "DisCo-FLoc: Using Dual-Level Visual-Geometric Contrasts to Disambiguate Depth-Aware Visual Floorplan Localization",
    "url": "https://www.alphaxiv.org/abs/2601.01822v1",
    "arxiv_id": "2601.01822v1",
    "authors": "Shiyong Meng, Tao Zou, Bolei Chen, Chaoxu Mu, Jianxin Wang",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2026-01-05 06:31:24",
    "ori_summary": "Since floorplan data is readily available, long-term persistent, and robust to changes in visual appearance, visual Floorplan Localization (FLoc) has garnered significant attention. Existing methods either ingeniously match geometric priors or utilize sparse semantics to reduce FLoc uncertainty. However, they still suffer from ambiguous FLoc caused by repetitive structures within minimalist floorplans. Moreover, expensive but limited semantic annotations restrict their applicability. To address these issues, we propose DisCo-FLoc, which utilizes dual-level visual-geometric Contrasts to Disambiguate depth-aware visual Floc, without requiring additional semantic labels. Our solution begins with a ray regression predictor tailored for ray-casting-based FLoc, predicting a series of FLoc candidates using depth estimation expertise. In addition, a novel contrastive learning method with position-level and orientation-level constraints is proposed to strictly match depth-aware visual features with the corresponding geometric structures in the floorplan. Such matches can effectively eliminate FLoc ambiguity and select the optimal imaging pose from FLoc candidates. Exhaustive comparative studies on two standard visual Floc benchmarks demonstrate that our method outperforms the state-of-the-art semantic-based method, achieving significant improvements in both robustness and accuracy.",
    "summary": "",
    "translation": "DisCo-FLoc：利用双层级视觉-几何对比进行深度感知视觉平面图定位消歧",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及计算机视觉（视觉-几何对比、深度感知、平面图定位）和3D视觉领域，属于您列出的无关主题范畴。虽然标题包含“定位”概念，但这与推荐系统、搜索或广告中的用户/物品定位无关，而是纯粹的视觉定位任务，没有显示出与您关注领域的潜在应用联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01818v1": {
    "title": "Robust Egocentric Visual Attention Prediction Through Language-guided Scene Context-aware Learning",
    "url": "https://www.alphaxiv.org/abs/2601.01818v1",
    "arxiv_id": "2601.01818v1",
    "authors": "Sungjune Park, Hongda Mao, Qingshuang Chen, Yong Man Ro, Yelin Kim",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 06:14:41",
    "ori_summary": "As the demand for analyzing egocentric videos grows, egocentric visual attention prediction, anticipating where a camera wearer will attend, has garnered increasing attention. However, it remains challenging due to the inherent complexity and ambiguity of dynamic egocentric scenes. Motivated by evidence that scene contextual information plays a crucial role in modulating human attention, in this paper, we present a language-guided scene context-aware learning framework for robust egocentric visual attention prediction. We first design a context perceiver which is guided to summarize the egocentric video based on a language-based scene description, generating context-aware video representations. We then introduce two training objectives that: 1) encourage the framework to focus on the target point-of-interest regions and 2) suppress distractions from irrelevant regions which are less likely to attract first-person attention. Extensive experiments on Ego4D and Aria Everyday Activities (AEA) datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance and enhanced robustness across diverse, dynamic egocentric scenarios.",
    "summary": "",
    "translation": "通过语言引导的场景上下文感知学习实现鲁棒的第一人称视觉注意力预测",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及视觉注意力预测和第一人称视角，这属于纯粹的计算机视觉研究领域，与推荐系统、搜索或广告的核心技术没有直接关联。尽管提到了语言引导，但整体焦点是视觉处理而非文本或序列建模，因此不符合任何指定的关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01807v1": {
    "title": "Adaptive Hybrid Optimizer based Framework for Lumpy Skin Disease Identification",
    "url": "https://www.alphaxiv.org/abs/2601.01807v1",
    "arxiv_id": "2601.01807v1",
    "authors": "Ubaidullah, Muhammad Abid Hussain, Mohsin Raza Jafri, Rozi Khan, Moid Sandhu, Abd Ullah Khan, Hyundong Shin",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-05 05:35:45",
    "ori_summary": "Lumpy Skin Disease (LSD) is a contagious viral infection that significantly deteriorates livestock health, thereby posing a serious threat to the global economy and food security. Owing to its rapid spread characteristics, early and precise identification is crucial to prevent outbreaks and ensure timely intervention. In this paper, we propose a hybrid deep learning-based approach called LUMPNet for the early detection of LSD. LUMPNet utilizes image data to detect and classify skin nodules -- the primary indicator of LSD. To this end, LUMPNet uses YOLOv11, EfficientNet-based CNN classifier with compound scaling, and a novel adaptive hybrid optimizer. More precisely, LUMPNet detects and localizes LSD skin nodules and lesions on cattle images. It exploits EfficientNet to classify the localized cattle images into LSD-affected or healthy categories. To stabilize and accelerate the training of YOLOv11 and EfficientNet hybrid model, a novel adaptive hybrid optimizer is proposed and utilized. We evaluate LUMPNet at various stages of LSD using a publicly available dataset. Results indicate that the proposed scheme achieves 99% LSD detection training accuracy, and outperforms existing schemes. The model also achieves validation accuracy of 98%. Moreover, for further evaluation, we conduct a case study using an optimized EfficientNet-B0 model trained with the AdamW optimizer, and compare its performance with LUMPNet. The results show that LUMPNet achieves superior performance.",
    "summary": "",
    "translation": "基于自适应混合优化器框架的牛结节性皮肤病识别",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学/生物学领域的特定疾病识别（牛结节性皮肤病），这属于明确列出的无关主题。标题中提到的'自适应混合优化器框架'可能涉及优化技术，但整体应用场景与推荐系统、搜索、广告等核心关注领域完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01804v1": {
    "title": "Causality-Aware Temporal Projection for Video Understanding in Video-LLMs",
    "url": "https://www.alphaxiv.org/abs/2601.01804v1",
    "arxiv_id": "2601.01804v1",
    "authors": "Zhengjian Kang, Qi Chen, Rui Liu, Kangtong Mo, Xingyu Zhang, Xiaoyu Deng, Ye Zhang",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 05:30:13",
    "ori_summary": "Recent Video Large Language Models (Video-LLMs) have shown strong multimodal reasoning capabilities, yet remain challenged by video understanding tasks that require consistent temporal ordering and causal coherence. Many parameter-efficient Video-LLMs rely on unconstrained bidirectional projectors to model inter-frame interactions, which can blur temporal ordering by allowing later frames to influence earlier representations, without explicit architectural mechanisms to respect the directional nature of video reasoning. To address this limitation, we propose V-CORE, a parameter-efficient framework that introduces explicit temporal ordering constraints for video understanding. V-CORE consists of two key components: (1) Learnable Spatial Aggregation (LSA), which adaptively selects salient spatial tokens to reduce redundancy, and (2) a Causality-Aware Temporal Projector (CATP), which enforces structured unidirectional information flow via block-causal attention and a terminal dynamic summary token acting as a causal sink. This design preserves intra-frame spatial interactions while ensuring that temporal information is aggregated in a strictly ordered manner. With 4-bit QLoRA and a frozen LLM backbone, V-CORE can be trained efficiently on a single consumer GPU. Experiments show that V-CORE achieves strong performance on the challenging NExT-QA benchmark, reaching 61.2% accuracy, and remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with gains concentrated in temporal and causal reasoning subcategories (+3.5% and +5.2% respectively), directly validating the importance of explicit temporal ordering constraints.",
    "summary": "",
    "translation": "面向视频大语言模型的因果感知时序投影视频理解方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频理解和视频大语言模型，属于纯粹的视觉领域研究，与RecSys/Search/Ads的核心关注点（如用户行为建模、内容排序、广告投放等）没有直接关联。虽然标题中提到'因果感知'和'时序投影'等概念，但这些技术并未明确指向推荐、搜索或广告领域的应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01798v1": {
    "title": "VerLM: Explaining Face Verification Using Natural Language",
    "url": "https://www.alphaxiv.org/abs/2601.01798v1",
    "arxiv_id": "2601.01798v1",
    "authors": "Syed Abdul Hannan, Hazim Bukhari, Thomas Cantalapiedra, Eman Ansar, Massa Baali, Rita Singh, Bhiksha Raj",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-01-05 05:16:07",
    "ori_summary": "Face verification systems have seen substantial advancements; however, they often lack transparency in their decision-making processes. In this paper, we introduce an innovative Vision-Language Model (VLM) for Face Verification, which not only accurately determines if two face images depict the same individual but also explicitly explains the rationale behind its decisions. Our model is uniquely trained using two complementary explanation styles: (1) concise explanations that summarize the key factors influencing its decision, and (2) comprehensive explanations detailing the specific differences observed between the images. We adapt and enhance a state-of-the-art modeling approach originally designed for audio-based differentiation to suit visual inputs effectively. This cross-modal transfer significantly improves our model's accuracy and interpretability. The proposed VLM integrates sophisticated feature extraction techniques with advanced reasoning capabilities, enabling clear articulation of its verification process. Our approach demonstrates superior performance, surpassing baseline methods and existing models. These findings highlight the immense potential of vision language models in face verification set up, contributing to more transparent, reliable, and explainable face verification systems.",
    "summary": "",
    "translation": "VerLM：使用自然语言解释人脸验证",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的人脸验证任务，并利用自然语言进行解释，这属于纯粹的视觉领域应用。虽然涉及自然语言处理，但其核心是视觉任务（人脸验证），没有展示与推荐系统、搜索或广告的明确关联。该研究属于纯粹的视觉-语言交叉领域，而非推荐/搜索/广告相关的异构数据统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01784v1": {
    "title": "DDNet: A Dual-Stream Graph Learning and Disentanglement Framework for Temporal Forgery Localization",
    "url": "https://www.alphaxiv.org/abs/2601.01784v1",
    "arxiv_id": "2601.01784v1",
    "authors": "Boyang Zhao, Xin Liao, Jiaxin Chen, Xiaoshuai Wu, Yufeng Wu",
    "categories": "cs.CV, cs.MM, eess.IV",
    "pub_date": "2026-01-05 04:35:39",
    "ori_summary": "The rapid evolution of AIGC technology enables misleading viewers by tampering mere small segments within a video, rendering video-level detection inaccurate and unpersuasive. Consequently, temporal forgery localization (TFL), which aims to precisely pinpoint tampered segments, becomes critical. However, existing methods are often constrained by \\emph{local view}, failing to capture global anomalies. To address this, we propose a \\underline{d}ual-stream graph learning and \\underline{d}isentanglement framework for temporal forgery localization (DDNet). By coordinating a \\emph{Temporal Distance Stream} for local artifacts and a \\emph{Semantic Content Stream} for long-range connections, DDNet prevents global cues from being drowned out by local smoothness. Furthermore, we introduce Trace Disentanglement and Adaptation (TDA) to isolate generic forgery fingerprints, alongside Cross-Level Feature Embedding (CLFE) to construct a robust feature foundation via deep fusion of hierarchical features. Experiments on ForgeryNet and TVIL benchmarks demonstrate that our method outperforms state-of-the-art approaches by approximately 9\\% in AP@0.95, with significant improvements in cross-domain robustness.",
    "summary": "",
    "translation": "DDNet：一种用于时序伪造定位的双流图学习与解耦框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注时序伪造定位，属于计算机视觉中的视频取证领域，与推荐系统、搜索或广告的核心技术焦点没有直接关联。虽然涉及图学习和解耦技术，但这些方法在推荐/搜索/广告中的潜在应用不明确，且论文标题未表明任何跨模态建模或与异构数据处理的联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01781v1": {
    "title": "Subimage Overlap Prediction: Task-Aligned Self-Supervised Pretraining For Semantic Segmentation In Remote Sensing Imagery",
    "url": "https://www.alphaxiv.org/abs/2601.01781v1",
    "arxiv_id": "2601.01781v1",
    "authors": "Lakshay Sharma, Alex Marin",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2026-01-05 04:28:49",
    "ori_summary": "Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \\href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.",
    "summary": "",
    "translation": "子图像重叠预测：面向遥感影像语义分割的任务对齐自监督预训练",
    "relevance_score": 1,
    "reasoning": "该论文专注于遥感影像的语义分割，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术无关。虽然涉及自监督预训练技术，但该技术被专门应用于遥感这一特定领域，没有显示出向推荐/搜索/广告领域迁移的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01769v1": {
    "title": "CTIS-QA: Clinical Template-Informed Slide-level Question Answering for Pathology",
    "url": "https://www.alphaxiv.org/abs/2601.01769v1",
    "arxiv_id": "2601.01769v1",
    "authors": "Hao Lu, Ziniu Qian, Yifu Li, Yang Zhou, Bingzheng Wei, Yan Xu",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 03:54:02",
    "ori_summary": "In this paper, we introduce a clinical diagnosis template-based pipeline to systematically collect and structure pathological information. In collaboration with pathologists and guided by the the College of American Pathologists (CAP) Cancer Protocols, we design a Clinical Pathology Report Template (CPRT) that ensures comprehensive and standardized extraction of diagnostic elements from pathology reports. We validate the effectiveness of our pipeline on TCGA-BRCA. First, we extract pathological features from reports using CPRT. These features are then used to build CTIS-Align, a dataset of 80k slide-description pairs from 804 WSIs for vision-language alignment training, and CTIS-Bench, a rigorously curated VQA benchmark comprising 977 WSIs and 14,879 question-answer pairs. CTIS-Bench emphasizes clinically grounded, closed-ended questions (e.g., tumor grade, receptor status) that reflect real diagnostic workflows, minimize non-visual reasoning, and require genuine slide understanding. We further propose CTIS-QA, a Slide-level Question Answering model, featuring a dual-stream architecture that mimics pathologists' diagnostic approach. One stream captures global slide-level context via clustering-based feature aggregation, while the other focuses on salient local regions through attention-guided patch perception module. Extensive experiments on WSI-VQA, CTIS-Bench, and slide-level diagnostic tasks show that CTIS-QA consistently outperforms existing state-of-the-art models across multiple metrics. Code and data are available at https://github.com/HLSvois/CTIS-QA.",
    "summary": "",
    "translation": "CTIS-QA：基于临床模板的病理学切片级问答系统",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学病理学领域的特定应用（临床模板、病理切片、问答系统），这属于明确的医学领域应用。根据用户列出的无关主题，明确排除了'Medical, Biology, Chemistry, Physics or other domain-specific applications'，因此该论文与用户关注的推荐系统、搜索、广告等核心领域完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01762v1": {
    "title": "AlignDrive: Aligned Lateral-Longitudinal Planning for End-to-End Autonomous Driving",
    "url": "https://www.alphaxiv.org/abs/2601.01762v1",
    "arxiv_id": "2601.01762v1",
    "authors": "Yanhao Wu, Haoyang Zhang, Fei He, Rui Wu, Congpei Qiu, Liang Gao, Wei Ke, Tong Zhang",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2026-01-05 03:41:20",
    "ori_summary": "End-to-end autonomous driving has rapidly progressed, enabling joint perception and planning in complex environments. In the planning stage, state-of-the-art (SOTA) end-to-end autonomous driving models decouple planning into parallel lateral and longitudinal predictions. While effective, this parallel design can lead to i) coordination failures between the planned path and speed, and ii) underutilization of the drive path as a prior for longitudinal planning, thus redundantly encoding static information. To address this, we propose a novel cascaded framework that explicitly conditions longitudinal planning on the drive path, enabling coordinated and collision-aware lateral and longitudinal planning. Specifically, we introduce a path-conditioned formulation that explicitly incorporates the drive path into longitudinal planning. Building on this, the model predicts longitudinal displacements along the drive path rather than full 2D trajectory waypoints. This design simplifies longitudinal reasoning and more tightly couples it with lateral planning. Additionally, we introduce a planning-oriented data augmentation strategy that simulates rare safety-critical events, such as vehicle cut-ins, by adding agents and relabeling longitudinal targets to avoid collision. Evaluated on the challenging Bench2Drive benchmark, our method sets a new SOTA, achieving a driving score of 89.07 and a success rate of 73.18%, demonstrating significantly improved coordination and safety",
    "summary": "",
    "translation": "AlignDrive：面向端到端自动驾驶的对齐式横向-纵向规划",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域，涉及车辆控制、路径规划等具体应用，与搜索、推荐、广告等核心业务场景无直接关联。论文标题中提到的“对齐”概念虽在LLM中有应用，但此处特指车辆运动规划中的横向与纵向控制对齐，无法迁移至推荐系统或广告领域的异构数据对齐问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01749v1": {
    "title": "MANGO:Natural Multi-speaker 3D Talking Head Generation via 2D-Lifted Enhancement",
    "url": "https://www.alphaxiv.org/abs/2601.01749v1",
    "arxiv_id": "2601.01749v1",
    "authors": "Lei Zhu, Lijian Lin, Ye Zhu, Jiahao Wu, Xuehan Hou, Yu Li, Yunfei Liu, Jie Chen",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 02:59:49",
    "ori_summary": "Current audio-driven 3D head generation methods mainly focus on single-speaker scenarios, lacking natural, bidirectional listen-and-speak interaction. Achieving seamless conversational behavior, where speaking and listening states transition fluidly remains a key challenge. Existing 3D conversational avatar approaches rely on error-prone pseudo-3D labels that fail to capture fine-grained facial dynamics. To address these limitations, we introduce a novel two-stage framework MANGO, which leveraging pure image-level supervision by alternately training to mitigate the noise introduced by pseudo-3D labels, thereby achieving better alignment with real-world conversational behaviors. Specifically, in the first stage, a diffusion-based transformer with a dual-audio interaction module models natural 3D motion from multi-speaker audio. In the second stage, we use a fast 3D Gaussian Renderer to generate high-fidelity images and provide 2D-level photometric supervision for the 3D motions through alternate training. Additionally, we introduce MANGO-Dialog, a high-quality dataset with over 50 hours of aligned 2D-3D conversational data across 500+ identities. Extensive experiments demonstrate that our method achieves exceptional accuracy and realism in modeling two-person 3D dialogue motion, significantly advancing the fidelity and controllability of audio-driven talking heads.",
    "summary": "",
    "translation": "MANGO：通过2D提升增强实现自然多说话者3D说话头生成",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于3D说话头生成，属于计算机视觉和图形学领域，与推荐系统、搜索或广告的核心技术没有直接关联。它涉及AIGC和内容生成，这些都属于被明确排除的无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01747v1": {
    "title": "Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization",
    "url": "https://www.alphaxiv.org/abs/2601.01747v1",
    "arxiv_id": "2601.01747v1",
    "authors": "Jiwei Guan, Haibo Jin, Haohan Wang",
    "categories": "cs.CR, cs.AI, cs.CV, cs.LG",
    "pub_date": "2026-01-05 02:49:33",
    "ori_summary": "Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs",
    "summary": "",
    "translation": "利用黑盒优化为大型视觉语言模型构建对抗性输入",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型的对抗性攻击，属于安全/鲁棒性领域，这在您的无关主题列表中明确排除。虽然涉及大型模型，但其核心是攻击方法而非在推荐/搜索/广告中的应用，也没有讨论如何将这种技术转化为推荐系统的实际应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01746v1": {
    "title": "Point-SRA: Self-Representation Alignment for 3D Representation Learning",
    "url": "https://www.alphaxiv.org/abs/2601.01746v1",
    "arxiv_id": "2601.01746v1",
    "authors": "Lintong Wei, Jian Lu, Haozhe Cheng, Jihua Zhu, Kaibing Zhang",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 02:44:21",
    "ori_summary": "Masked autoencoders (MAE) have become a dominant paradigm in 3D representation learning, setting new performance benchmarks across various downstream tasks. Existing methods with fixed mask ratio neglect multi-level representational correlations and intrinsic geometric structures, while relying on point-wise reconstruction assumptions that conflict with the diversity of point cloud. To address these issues, we propose a 3D representation learning method, termed Point-SRA, which aligns representations through self-distillation and probabilistic modeling. Specifically, we assign different masking ratios to the MAE to capture complementary geometric and semantic information, while the MeanFlow Transformer (MFT) leverages cross-modal conditional embeddings to enable diverse probabilistic reconstruction. Our analysis further reveals that representations at different time steps in MFT also exhibit complementarity. Therefore, a Dual Self-Representation Alignment mechanism is proposed at both the MAE and MFT levels. Finally, we design a Flow-Conditioned Fine-Tuning Architecture to fully exploit the point cloud distribution learned via MeanFlow. Point-SRA outperforms Point-MAE by 5.37% on ScanObjectNN. On intracranial aneurysm segmentation, it reaches 96.07% mean IoU for arteries and 86.87% for aneurysms. For 3D object detection, Point-SRA achieves 47.3% AP@50, surpassing MaskPoint by 5.12%.",
    "summary": "",
    "translation": "Point-SRA：用于三维表征学习的自表示对齐",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D表征学习，属于纯粹的3D视觉领域，与推荐系统、搜索或广告没有明确关联。标题中提到的自表示对齐技术是针对3D点云数据的特定方法，没有显示出在异构数据处理或推荐系统应用方面的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01720v1": {
    "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
    "url": "https://www.alphaxiv.org/abs/2601.01720v1",
    "arxiv_id": "2601.01720v1",
    "authors": "Xijie Huang, Chengming Xu, Donghao Luo, Xiaobin Hu, Peng Tang, Xu Peng, Jiangning Zhang, Chengjie Wang, Yanwei Fu",
    "categories": "cs.CV",
    "pub_date": "2026-01-05 01:46:22",
    "ori_summary": "First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.",
    "summary": "",
    "translation": "FFP-300K：扩展首帧传播以实现可泛化的视频编辑",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于视频编辑技术，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术无直接关联。尽管涉及“可泛化”概念，但主要应用于视频内容生成，属于明确的无关主题（AIGC、内容生成、纯视觉论文）。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2601.01696v1": {
    "title": "Real-Time Lane Detection via Efficient Feature Alignment and Covariance Optimization for Low-Power Embedded Systems",
    "url": "https://www.alphaxiv.org/abs/2601.01696v1",
    "arxiv_id": "2601.01696v1",
    "authors": "Yian Liu, Xiong Wang, Ping Xu, Lei Zhu, Ming Yan, Linyun Xue",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2026-01-05 00:06:06",
    "ori_summary": "Real-time lane detection in embedded systems encounters significant challenges due to subtle and sparse visual signals in RGB images, often constrained by limited computational resources and power consumption. Although deep learning models for lane detection categorized into segmentation-based, anchor-based, and curve-based methods there remains a scarcity of universally applicable optimization techniques tailored for low-power embedded environments. To overcome this, we propose an innovative Covariance Distribution Optimization (CDO) module specifically designed for efficient, real-time applications. The CDO module aligns lane feature distributions closely with ground-truth labels, significantly enhancing detection accuracy without increasing computational complexity. Evaluations were conducted on six diverse models across all three method categories, including two optimized for real-time applications and four state-of-the-art (SOTA) models, tested comprehensively on three major datasets: CULane, TuSimple, and LLAMAS. Experimental results demonstrate accuracy improvements ranging from 0.01% to 1.5%. The proposed CDO module is characterized by ease of integration into existing systems without structural modifications and utilizes existing model parameters to facilitate ongoing training, thus offering substantial benefits in performance, power efficiency, and operational flexibility in embedded systems.",
    "summary": "",
    "translation": "面向低功耗嵌入式系统的实时车道检测：基于高效特征对齐与协方差优化",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的车道检测任务，属于纯粹的视觉感知领域，与推荐系统、搜索或广告的核心技术无直接关联。论文讨论的低功耗嵌入式系统优化和特征对齐技术，在当前关注点中缺乏明确的跨模态应用潜力或对Transformer架构的贡献。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  }
}