{
  "2510.19791v1": {
    "title": "ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers",
    "url": "https://www.alphaxiv.org/abs/2510.19791v1",
    "arxiv_id": "2510.19791v1",
    "authors": "Saptarshi Sengupta, Zhengyu Zhou, Jun Araki, Xingbo Wang, Bingqing Wang, Suhang Wang, Zhe Feng",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2025-10-22 17:26:05",
    "ori_summary": "Tool calling has become increasingly popular for Large Language Models (LLMs). However, for large tool sets, the resulting tokens would exceed the LLM's context window limit, making it impossible to include every tool. Hence, an external retriever is used to provide LLMs with the most relevant tools for a query. Existing retrieval models rank tools based on the similarity between a user query and a tool description (TD). This leads to suboptimal retrieval as user requests are often poorly aligned with the language of TD. To remedy the issue, we propose ToolDreamer, a framework to condition retriever models to fetch tools based on hypothetical (synthetic) TD generated using an LLM, i.e., description of tools that the LLM feels will be potentially useful for the query. The framework enables a more natural alignment between queries and tools within the language space of TD's. We apply ToolDreamer on the ToolRet dataset and show that our method improves the performance of sparse and dense retrievers with and without training, thus showcasing its flexibility. Through our proposed framework, our aim is to offload a portion of the reasoning burden to the retriever so that the LLM may effectively handle a large collection of tools without inundating its context window.",
    "summary": "该论文研究大规模工具集检索中的查询与工具描述语言不匹配问题。核心方法是利用LLM生成假设性工具描述来训练检索器，使检索器能够基于LLM的推理能力而非单纯语义相似度来检索工具。",
    "translation": "ToolDreamer：将大型语言模型推理能力注入工具检索器",
    "relevance_score": 8,
    "reasoning": "该论文涉及将LLM推理能力与检索系统相结合，这直接适用于搜索和推荐系统中的工具检索场景。在搜索和推荐系统中，增强的检索器可以更好地理解用户意图并检索相关工具或内容，提高系统性能和用户体验。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文通过LLM生成假设性工具描述来改进检索器，直接涉及LLM推理能力向检索系统的迁移，与LLM在搜索推荐中的直接应用高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19758v1": {
    "title": "Top-P Masking for Cross Language Information Retrieval",
    "url": "https://www.alphaxiv.org/abs/2510.19758v1",
    "arxiv_id": "2510.19758v1",
    "authors": "Joseph Casale, Andrew Silverschotz, Joseph DeSimone",
    "categories": "cs.IR",
    "pub_date": "2025-10-22 16:47:42",
    "ori_summary": "Top-K masking schemes have been proposed as a method to promote sparse representations in Information Retrieval (IR) tasks, as a simple alternative to Floating Point Operations per Second (FLOPS) regularization. Algorithms such as Bilingual Lexical and Document Expansion Model (BLADE), adopt this approach as a post-processing stage. We propose using Top-P Dynamic Masking similar to Nucleus Sampling in Large Language Models, and demonstrate better performance than Top-K masking. Specifically, we evaluate our methods in the domain of Cross Language Information Retrieval (CLIR)",
    "summary": "研究跨语言信息检索中稀疏表示优化问题，核心思想是将LLM中的核采样Top-P方法应用于掩码机制，替代传统的Top-K掩码来改进检索效果。",
    "translation": "用于跨语言信息检索的Top-P掩码方法",
    "relevance_score": 6,
    "reasoning": "该论文提出Top-P掩码方法用于跨语言信息检索，这属于搜索领域的核心进展。虽然不直接涉及LLM技术，但跨语言检索是搜索系统的重要能力，该方法可能提升多语言搜索的准确性和效率，对搜索系统有直接应用价值。",
    "rerank_relevance_score": 4,
    "rerank_reasoning": "该论文提出Top-P动态掩码方法改进信息检索稀疏表示，属于检索系统核心领域进展，但与LLM技术关联较弱，仅借鉴了采样思想。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19559v1": {
    "title": "A Matter of Time: Revealing the Structure of Time in Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.19559v1",
    "arxiv_id": "2510.19559v1",
    "authors": "Nidham Tekaya, Manuela Waldner, Matthias Zeppelzauer",
    "categories": "cs.CV, cs.AI, cs.IR, cs.MM",
    "pub_date": "2025-10-22 13:14:02",
    "ori_summary": "Large-scale vision-language models (VLMs) such as CLIP have gained popularity for their generalizable and expressive multimodal representations. By leveraging large-scale training data with diverse textual metadata, VLMs acquire open-vocabulary capabilities, solving tasks beyond their training scope. This paper investigates the temporal awareness of VLMs, assessing their ability to position visual content in time. We introduce TIME10k, a benchmark dataset of over 10,000 images with temporal ground truth, and evaluate the time-awareness of 37 VLMs by a novel methodology. Our investigation reveals that temporal information is structured along a low-dimensional, non-linear manifold in the VLM embedding space. Based on this insight, we propose methods to derive an explicit ``timeline'' representation from the embedding space. These representations model time and its chronological progression and thereby facilitate temporal reasoning tasks. Our timeline approaches achieve competitive to superior accuracy compared to a prompt-based baseline while being computationally efficient. All code and data are available at https://tekayanidham.github.io/timeline-page/.",
    "summary": "",
    "translation": "时间问题：揭示视觉语言模型中时间结构的研究",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视觉语言模型中的时间结构理解，属于VLM领域。虽然VLM技术可以类比到异构数据处理，但论文标题没有明确指向推荐系统、搜索或广告领域的应用场景，也没有表明会探索时间序列建模在推荐系统等领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19365v1": {
    "title": "The Massive Legal Embedding Benchmark (MLEB)",
    "url": "https://www.alphaxiv.org/abs/2510.19365v1",
    "arxiv_id": "2510.19365v1",
    "authors": "Umar Butler, Abdur-Rahman Butler, Adrian Lucas Malec",
    "categories": "cs.CL, cs.AI, cs.IR",
    "pub_date": "2025-10-22 08:38:44",
    "ori_summary": "We present the Massive Legal Embedding Benchmark (MLEB), the largest, most diverse, and most comprehensive open-source benchmark for legal information retrieval to date. MLEB consists of ten expert-annotated datasets spanning multiple jurisdictions (the US, UK, EU, Australia, Ireland, and Singapore), document types (cases, legislation, regulatory guidance, contracts, and literature), and task types (search, zero-shot classification, and question answering). Seven of the datasets in MLEB were newly constructed in order to fill domain and jurisdictional gaps in the open-source legal information retrieval landscape. We document our methodology in building MLEB and creating the new constituent datasets, and release our code, results, and data openly to assist with reproducible evaluations.",
    "summary": "",
    "translation": "大规模法律嵌入基准（MLEB）",
    "relevance_score": 2,
    "reasoning": "该论文提出了一个法律领域的嵌入基准，虽然嵌入技术本身与推荐系统相关，但该工作专注于法律这一特定领域应用，与搜索、推荐或广告的核心技术进展关联度较低。法律领域的专业性和特殊性限制了其在通用推荐系统中的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19340v1": {
    "title": "CoRECT: A Framework for Evaluating Embedding Compression Techniques at Scale",
    "url": "https://www.alphaxiv.org/abs/2510.19340v1",
    "arxiv_id": "2510.19340v1",
    "authors": "L. Caspari, M. Dinzinger, K. Gosh Dastidar, C. Fellicious, J. Mitrović, M. Granitzer",
    "categories": "cs.IR",
    "pub_date": "2025-10-22 08:03:31",
    "ori_summary": "Dense retrieval systems have proven to be effective across various benchmarks, but require substantial memory to store large search indices. Recent advances in embedding compression show that index sizes can be greatly reduced with minimal loss in ranking quality. However, existing studies often overlook the role of corpus complexity -- a critical factor, as recent work shows that both corpus size and document length strongly affect dense retrieval performance. In this paper, we introduce CoRECT (Controlled Retrieval Evaluation of Compression Techniques), a framework for large-scale evaluation of embedding compression methods, supported by a newly curated dataset collection. To demonstrate its utility, we benchmark eight representative types of compression methods. Notably, we show that non-learned compression achieves substantial index size reduction, even on up to 100M passages, with statistically insignificant performance loss. However, selecting the optimal compression method remains challenging, as performance varies across models. Such variability highlights the necessity of CoRECT to enable consistent comparison and informed selection of compression methods. All code, data, and results are available on GitHub and HuggingFace.",
    "summary": "研究如何系统评估嵌入压缩技术以解决密集检索系统索引存储问题；核心是提出CoRECT框架，通过控制语料复杂度变量实现大规模压缩方法对比，揭示非学习压缩在保持性能同时显著减小索引。",
    "translation": "CoRECT：大规模评估嵌入压缩技术的框架",
    "relevance_score": 8,
    "reasoning": "嵌入压缩技术是推荐系统和搜索系统的核心基础设施技术，直接影响模型部署效率和推理性能。该框架的大规模评估能力对于实际工业应用中的嵌入压缩方案选择具有直接指导意义，能够帮助优化推荐/搜索系统的存储和计算效率。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文专注于嵌入压缩技术的系统评估框架，直接关联推荐系统和搜索中的索引存储效率问题，但未涉及LLM或Transformer架构创新。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19334v1": {
    "title": "Metadata Extraction Leveraging Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.19334v1",
    "arxiv_id": "2510.19334v1",
    "authors": "Cuize Han, Sesh Jalagam",
    "categories": "stat.ML, cs.AI, cs.IR, cs.LG",
    "pub_date": "2025-10-22 07:56:36",
    "ori_summary": "The advent of Large Language Models has revolutionized tasks across domains, including the automation of legal document analysis, a critical component of modern contract management systems. This paper presents a comprehensive implementation of LLM-enhanced metadata extraction for contract review, focusing on the automatic detection and annotation of salient legal clauses. Leveraging both the publicly available Contract Understanding Atticus Dataset (CUAD) and proprietary contract datasets, our work demonstrates the integration of advanced LLM methodologies with practical applications. We identify three pivotal elements for optimizing metadata extraction: robust text conversion, strategic chunk selection, and advanced LLM-specific techniques, including Chain of Thought (CoT) prompting and structured tool calling. The results from our experiments highlight the substantial improvements in clause identification accuracy and efficiency. Our approach shows promise in reducing the time and cost associated with contract review while maintaining high accuracy in legal clause identification. The results suggest that carefully optimized LLM systems could serve as valuable tools for legal professionals, potentially increasing access to efficient contract review services for organizations of all sizes.",
    "summary": "该论文研究如何从法律合同中自动提取关键条款元数据的问题，其核心方法是结合文本转换、分块策略以及CoT提示和结构化工具调用等LLM特定技术来优化元数据提取流程。",
    "translation": "基于大型语言模型的元数据提取",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM在信息提取方面的直接应用，这在搜索和推荐系统中具有重要价值。元数据提取可以显著提升内容理解、特征工程和用户意图识别的能力，为个性化推荐和精准搜索提供更丰富的数据基础。",
    "rerank_relevance_score": 6,
    "rerank_reasoning": "该论文专注于LLM在文档元数据提取中的应用，与搜索和推荐系统中的信息提取技术高度相关，但未涉及核心推荐算法或Transformer架构创新。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19221v1": {
    "title": "C2T-ID: Converting Semantic Codebooks to Textual Document Identifiers for Generative Search",
    "url": "https://www.alphaxiv.org/abs/2510.19221v1",
    "arxiv_id": "2510.19221v1",
    "authors": "Yingchen Zhang, Ruqing Zhang, Jiafeng Guo, Wenjun Peng, Sen Li, Fuyu Lv, Xueqi Cheng",
    "categories": "cs.IR",
    "pub_date": "2025-10-22 04:05:38",
    "ori_summary": "Designing document identifiers (docids) that carry rich semantic information while maintaining tractable search spaces is a important challenge in generative retrieval (GR). Popular codebook methods address this by building a hierarchical semantic tree and constraining generation to its child nodes, yet their numeric identifiers cannot leverage the large language model's pretrained natural language understanding. Conversely, using text as docid provides more semantic expressivity but inflates the decoding space, making the system brittle to early-step errors. To resolve this trade-off, we propose C2T-ID: (i) first construct semantic numerical docid via hierarchical clustering; (ii) then extract high-frequency metadata keywords and iteratively replace each numeric label with its cluster's top-K keywords; and (iii) an optional two-level semantic smoothing step further enhances the fluency of C2T-ID. Experiments on Natural Questions and Taobao's product search demonstrate that C2T-ID significantly outperforms atomic, semantic codebook, and pure-text docid baselines, demonstrating its effectiveness in balancing semantic expressiveness with search space constraints.",
    "summary": "该论文研究生成式检索中语义文档标识符的设计问题，核心思想是通过层次聚类构建数值语义码本，然后将其转换为高频关键词组成的文本标识符，在保持搜索空间可控性的同时增强语义表达能力。",
    "translation": "C2T-ID：将语义码本转换为文本文档标识符用于生成式搜索",
    "relevance_score": 8,
    "reasoning": "该论文涉及生成式搜索技术，直接属于搜索领域的核心进展。将语义码本转换为文本标识符的方法在文档检索和语义搜索中有直接应用，能够提升搜索系统的生成能力和语义理解能力。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接解决生成式检索中语义标识符设计的核心挑战，通过将数值语义码本转换为文本标识符，在保持搜索空间可控的同时增强语义表达能力，对搜索和推荐系统具有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19817v1": {
    "title": "olmOCR 2: Unit Test Rewards for Document OCR",
    "url": "https://www.alphaxiv.org/abs/2510.19817v1",
    "arxiv_id": "2510.19817v1",
    "authors": "Jake Poznanski, Luca Soldaini, Kyle Lo",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-22 17:53:02",
    "ori_summary": "We present olmOCR 2, the latest in our family of powerful OCR systems for converting digitized print documents, like PDFs, into clean, naturally ordered plain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision language model (VLM) trained using reinforcement learning with verifiable rewards (RLVR), where our rewards are a diverse set of binary unit tests. To scale unit test creation, we develop a pipeline for generating synthetic documents with diverse and challenging layouts, known ground-truth HTML source code, and extracted test cases. We show that RL training on these test cases results in state-of-the-art performance on olmOCR-Bench, our English-language OCR benchmark, with the largest improvements in math formula conversion, table parsing, and multi-column layouts compared to previous versions. We release our model, data and code under permissive open licenses.",
    "summary": "",
    "translation": "olmOCR 2：为文档OCR设计的单元测试奖励机制",
    "relevance_score": 1,
    "reasoning": "该论文专注于文档OCR（光学字符识别）中的单元测试奖励机制，属于计算机视觉中的文档处理领域。这与我的关注点（推荐系统、搜索、广告中的核心进展、LLM技术、Transformer架构或异构数据建模）没有直接关联，也没有明确的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19811v1": {
    "title": "Hubble: a Model Suite to Advance the Study of LLM Memorization",
    "url": "https://www.alphaxiv.org/abs/2510.19811v1",
    "arxiv_id": "2510.19811v1",
    "authors": "Johnny Tian-Zheng Wei, Ameya Godbole, Mohammad Aflah Khan, Ryan Wang, Xiaoyuan Zhu, James Flemings, Nitya Kashyap, Krishna P. Gummadi, Willie Neiswanger, Robin Jia",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-22 17:48:23",
    "ori_summary": "We present Hubble, a suite of fully open-source large language models (LLMs) for the scientific study of LLM memorization. Hubble models come in standard and perturbed variants: standard models are pretrained on a large English corpus, and perturbed models are trained in the same way but with controlled insertion of text (e.g., book passages, biographies, and test sets) designed to emulate key memorization risks. Our core release includes 8 models -- standard and perturbed models with 1B or 8B parameters, pretrained on 100B or 500B tokens -- establishing that memorization risks are determined by the frequency of sensitive data relative to size of the training corpus (i.e., a password appearing once in a smaller corpus is memorized better than the same password in a larger corpus). Our release also includes 6 perturbed models with text inserted at different pretraining phases, showing that sensitive data without continued exposure can be forgotten. These findings suggest two best practices for addressing memorization risks: to dilute sensitive data by increasing the size of the training corpus, and to order sensitive data to appear earlier in training. Beyond these general empirical findings, Hubble enables a broad range of memorization research; for example, analyzing the biographies reveals how readily different types of private information are memorized. We also demonstrate that the randomized insertions in Hubble make it an ideal testbed for membership inference and machine unlearning, and invite the community to further explore, benchmark, and build upon our work.",
    "summary": "",
    "translation": "Hubble：一个用于推进LLM记忆化研究的模型套件",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM记忆化研究，这属于纯粹的LLM评估和基准测试范畴，与我的核心关注点无关。虽然记忆化研究可能对模型安全有影响，但这属于隐私和伦理领域，已被明确列为不相关主题，且没有显示出在推荐系统、搜索或广告中的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19808v1": {
    "title": "Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing",
    "url": "https://www.alphaxiv.org/abs/2510.19808v1",
    "arxiv_id": "2510.19808v1",
    "authors": "Yusu Qian, Eli Bocek-Rivele, Liangchen Song, Jialing Tong, Yinfei Yang, Jiasen Lu, Wenze Hu, Zhe Gan",
    "categories": "cs.CV, cs.CL, cs.LG",
    "pub_date": "2025-10-22 17:43:15",
    "ori_summary": "Recent advances in multimodal models have demonstrated remarkable text-guided image editing capabilities, with systems like GPT-4o and Nano-Banana setting new benchmarks. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built from real images. We introduce Pico-Banana-400K, a comprehensive 400K-image dataset for instruction-based image editing. Our dataset is constructed by leveraging Nano-Banana to generate diverse edit pairs from real photographs in the OpenImages collection. What distinguishes Pico-Banana-400K from previous synthetic datasets is our systematic approach to quality and diversity. We employ a fine-grained image editing taxonomy to ensure comprehensive coverage of edit types while maintaining precise content preservation and instruction faithfulness through MLLM-based quality scoring and careful curation. Beyond single turn editing, Pico-Banana-400K enables research into complex editing scenarios. The dataset includes three specialized subsets: (1) a 72K-example multi-turn collection for studying sequential editing, reasoning, and planning across consecutive modifications; (2) a 56K-example preference subset for alignment research and reward model training; and (3) paired long-short editing instructions for developing instruction rewriting and summarization capabilities. By providing this large-scale, high-quality, and task-rich resource, Pico-Banana-400K establishes a robust foundation for training and benchmarking the next generation of text-guided image editing models.",
    "summary": "",
    "translation": "Pico-Banana-400K：用于文本引导图像编辑的大规模数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本引导的图像编辑数据集，属于纯粹的视觉内容生成领域。虽然涉及文本-图像多模态，但核心是图像编辑而非推荐/搜索/广告中的排序或理解任务，与当前关注的LLM在推荐系统、搜索广告中的应用以及Transformer架构进展等焦点无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19807v1": {
    "title": "Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.19807v1",
    "arxiv_id": "2510.19807v1",
    "authors": "Xichen Zhang, Sitong Wu, Yinghao Zhu, Haoru Tan, Shaozuo Yu, Ziyi He, Jiaya Jia",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-22 17:41:30",
    "ori_summary": "Reinforcement learning from verifiable rewards has emerged as a powerful technique for enhancing the complex reasoning abilities of Large Language Models (LLMs). However, these methods are fundamentally constrained by the ''learning cliff'' phenomenon: when faced with problems far beyond their current capabilities, models consistently fail, yielding a persistent zero-reward signal. In policy optimization algorithms like GRPO, this collapses the advantage calculation to zero, rendering these difficult problems invisible to the learning gradient and stalling progress. To overcome this, we introduce Scaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive training framework that strategically provides minimal guidance only when a model's independent learning has plateaued. The framework first diagnoses learning stagnation and then intervenes by injecting tiered in-prompt hints, ranging from abstract concepts to concrete steps, enabling the model to construct a valid solution by itself. Extensive experiments on challenging mathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the pass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative 44.3% over a vanilla GRPO baseline. This result demonstrates our framework provides a robust and effective methodology for unlocking a model's ability to solve problems previously beyond its reach, a critical step towards extending the frontier of autonomous reasoning in LLM.",
    "summary": "",
    "translation": "Scaf-GRPO：用于增强大型语言模型推理能力的支架式组相对策略优化",
    "relevance_score": 3,
    "reasoning": "该论文主要关注LLM推理能力的优化方法，属于Enabling LLM Tech范畴。虽然推理能力增强可能间接提升RecSys/Search中的复杂决策任务，但论文标题未明确显示与推荐、搜索或广告系统的直接应用关联，应用潜力较为间接。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19806v1": {
    "title": "The Art of Asking: Multilingual Prompt Optimization for Synthetic Data",
    "url": "https://www.alphaxiv.org/abs/2510.19806v1",
    "arxiv_id": "2510.19806v1",
    "authors": "David Mora, Viraat Aryabumi, Wei-Yin Ko, Sara Hooker, Julia Kreutzer, Marzieh Fadaee",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 17:41:20",
    "ori_summary": "Synthetic data has become a cornerstone for scaling large language models, yet its multilingual use remains bottlenecked by translation-based prompts. This strategy inherits English-centric framing and style and neglects cultural dimensions, ultimately constraining model generalization. We argue that the overlooked prompt space-the very inputs that define training distributions-offers a more powerful lever for improving multilingual performance. We introduce a lightweight framework for prompt-space optimization, where translated prompts are systematically transformed for Naturalness, Cultural Adaptation, and Difficulty Enhancement. Using an off-the-shelf multilingual LLM, we apply these transformations to prompts for 12 languages spanning 7 families. Under identical data conditions, our approaches achieve substantial and consistent downstream improvements over the translation-only baseline: +4.7% on Global-MMLU accuracy, +2.4% on Flores XCometXL and +35.3% wins in preferences on mArenaHard. We establish prompt-space optimization as a simple yet powerful paradigm for building multilingual LLMs that are more robust, culturally grounded, and globally capable.",
    "summary": "",
    "translation": "提问的艺术：面向合成数据的多语言提示优化",
    "relevance_score": 6,
    "reasoning": "该论文涉及提示优化技术，这是LLM领域的核心进展，具有在推荐系统和搜索中应用的潜力，例如通过优化提示来生成更高质量的合成训练数据或改进查询理解。然而，论文标题没有明确说明与推荐/搜索/广告的具体联系，因此相关性中等。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.19796v1": {
    "title": "Blackbox Model Provenance via Palimpsestic Membership Inference",
    "url": "https://www.alphaxiv.org/abs/2510.19796v1",
    "arxiv_id": "2510.19796v1",
    "authors": "Rohith Kuditipudi, Jing Huang, Sally Zhu, Diyi Yang, Christopher Potts, Percy Liang",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-22 17:30:39",
    "ori_summary": "Suppose Alice trains an open-weight language model and Bob uses a blackbox derivative of Alice's model to produce text. Can Alice prove that Bob is using her model, either by querying Bob's derivative model (query setting) or from the text alone (observational setting)? We formulate this question as an independence testing problem--in which the null hypothesis is that Bob's model or text is independent of Alice's randomized training run--and investigate it through the lens of palimpsestic memorization in language models: models are more likely to memorize data seen later in training, so we can test whether Bob is using Alice's model using test statistics that capture correlation between Bob's model or text and the ordering of training examples in Alice's training run. If Alice has randomly shuffled her training data, then any significant correlation amounts to exactly quantifiable statistical evidence against the null hypothesis, regardless of the composition of Alice's training data. In the query setting, we directly estimate (via prompting) the likelihood Bob's model gives to Alice's training examples and order; we correlate the likelihoods of over 40 fine-tunes of various Pythia and OLMo base models ranging from 1B to 12B parameters with the base model's training data order, achieving a p-value on the order of at most 1e-8 in all but six cases. In the observational setting, we try two approaches based on estimating 1) the likelihood of Bob's text overlapping with spans of Alice's training examples and 2) the likelihood of Bob's text with respect to different versions of Alice's model we obtain by repeating the last phase (e.g., 1%) of her training run on reshuffled data. The second approach can reliably distinguish Bob's text from as little as a few hundred tokens; the first does not involve any retraining but requires many more tokens (several hundred thousand) to achieve high power.",
    "summary": "",
    "translation": "基于重写痕迹成员推理的黑盒模型溯源",
    "relevance_score": 1,
    "reasoning": "该论文关注模型溯源和成员推理，属于模型安全和隐私保护范畴，这些主题被明确列为不相关内容。论文标题表明其核心是识别模型训练数据中的特定样本，这属于隐私保护技术，与推荐系统、搜索或广告中的核心算法进展、LLM技术应用或Transformer架构改进无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19782v1": {
    "title": "Adapting Multilingual Models to Code-Mixed Tasks via Model Merging",
    "url": "https://www.alphaxiv.org/abs/2510.19782v1",
    "arxiv_id": "2510.19782v1",
    "authors": "Prashant Kodali, Vaishnavi Shivkumar, Swarang Joshi, Monojit Choudhary, Ponnurangam Kumaraguru, Manish Shrivastava",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 17:16:23",
    "ori_summary": "We study model merging as a practical alternative to conventional adaptation strategies for code-mixed NLP. Starting from a multilingual base model, we: (i) perform continued pre-training (CPT) on unlabeled code-mixed text to obtain an adapted checkpoint, (ii) merge checkpoint with the base model, and (iii) fine-tune (FT) on the downstream task data. We evaluate our approach for sentence classification (sentiment and hate speech) task in English-Hindi (En-Hi) and English-Spanish (En-Es) using XLM-R and Llama-3.2-1B models. Our results show that merged models consistently outperform full fine-tuning and CPT->FT. We observe gains of 2--5 points in F1 over full fine-tuning and ~1-2 points over CPT->FT, indicating that unlabeled data is leveraged more effectively via merging than via CPT alone. Zero-/few-shot prompting with larger LLMs (e.g., Llama-3.3-70B) lags behind fine-tuned and merged checkpoints, underscoring limits of in-context learning for code-mixed inputs. We further test cross-pair transfer by training on En-Hi and evaluating on En-Ta and En-Ml: merged checkpoints transfer more strongly than monolingual-English baselines (e.g., TV/TIES variants reaching 0.65-0.68 F1 vs 0.61-0.63 for full fine-tuning), suggesting that code-mixed knowledge is a more reliable substrate for low-resource pairs. We conclude with adaptation recipes matched to common data regimes (labeled only; labeled+unlabeled; transfer-only) and discuss limitations and scaling considerations for broader tasks and larger models.",
    "summary": "",
    "translation": "通过模型融合将多语言模型适配到代码混合任务",
    "relevance_score": 3,
    "reasoning": "该论文主要关注多语言模型在代码混合任务上的适配，这属于多语言NLP领域。虽然模型融合技术可能对某些推荐/搜索系统有间接价值，但论文核心焦点是语言处理而非推荐系统、搜索或广告的直接应用。没有明确证据表明该技术会应用于异构数据建模或Transformer架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19779v1": {
    "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders",
    "url": "https://www.alphaxiv.org/abs/2510.19779v1",
    "arxiv_id": "2510.19779v1",
    "authors": "Yuezhou Hu, Jiaxin Guo, Xinyu Feng, Tuo Zhao",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-22 17:13:00",
    "ori_summary": "Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\\%). The code is publicly available at https://github.com/yuezhouhu/adaspec.",
    "summary": "论文研究推测解码中知识蒸馏与目标不匹配的问题，核心思想是通过参考模型过滤难拟合token，使小模型专注于学习目标模型在简单token上的分布，从而提高token接受率。",
    "translation": "AdaSPEC：面向高效推测式解码器的选择性知识蒸馏",
    "relevance_score": 7,
    "reasoning": "该论文关注推测式解码器的效率优化，属于'Enabling LLM Tech'范畴。推测式解码技术可显著加速LLM推理，在搜索和推荐系统中具有直接应用价值，能够降低大规模部署的延迟和计算成本。选择性知识蒸馏方法进一步提升了效率优化的精细度。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文针对LLM推理加速的核心问题，提出了选择性知识蒸馏方法，直接优化推测解码的接受率目标，对LLM效率提升有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19778v1": {
    "title": "GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters",
    "url": "https://www.alphaxiv.org/abs/2510.19778v1",
    "arxiv_id": "2510.19778v1",
    "authors": "Anand Choudhary, Yasser Sulaıman, Lukas Mauch, Ghouthi Boukli Hacene, Fabien Cardinaux, Antoine Bosselut",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-22 17:11:49",
    "ori_summary": "Sparse fine-tuning techniques adapt LLMs to downstream tasks by only tuning a sparse subset of model parameters. However, the effectiveness of sparse adaptation depends on optimally selecting the model parameters to be fine-tuned. In this work, we introduce a novel sparse fine-tuning technique named GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters, which fine-tunes only those model parameters which have the largest gradient magnitudes on downstream tasks and the smallest pre-trained magnitudes, intuitively prioritizing parameters that are highly task-relevant, but minimally disruptive to pre-trained knowledge. Our experimentation with LLaMA3 8B and Gemma 2B as base models shows that GaLLoP consistently improves or matches the in-distribution as well as out-of-distribution performance obtained via the usage of other leading parameter-efficient fine-tuning techniques, including LoRA, DoRA, and SAFT. Our analysis demonstrates that GaLLoP mitigates catastrophic forgetting and memorization of task data, as important pre-trained parameters remain unchanged, and stabilizes performance relative to other fine-tuning techniques, robustly generalizing across most random seeds.",
    "summary": "该论文研究LLM稀疏微调中的参数选择优化问题，核心思想是选择梯度大但预训练权重小的参数进行微调，以平衡任务相关性和预训练知识保护。",
    "translation": "GaLLoP：基于梯度的低幅值参数稀疏学习",
    "relevance_score": 8,
    "reasoning": "该论文聚焦于梯度驱动的稀疏学习技术，专门针对低幅值参数进行优化，这属于Transformer架构效率提升的核心方向。在推荐系统和搜索广告领域，这种稀疏化方法可直接应用于大规模模型压缩、推理加速和内存优化，显著降低工业级部署成本。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出基于梯度的稀疏微调方法，直接属于LLM高效微调技术领域，对推荐系统和搜索中的模型适配具有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19767v1": {
    "title": "SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via Promoting Deeper Thought Exploration",
    "url": "https://www.alphaxiv.org/abs/2510.19767v1",
    "arxiv_id": "2510.19767v1",
    "authors": "Xichen Zhang, Sitong Wu, Haoru Tan, Shaozuo Yu, Yinghao Zhu, Ziyi He, Jiaya Jia",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-22 16:56:01",
    "ori_summary": "The long chain-of-thought (LongCoT) capability is central to the recent breakthroughs achieved by large language models in complex reasoning tasks. However, the accompanying issue of ''underthinking'', where models exhibit shallow reasoning by frequently switching thoughts without sufficient exploration, limits both performance and token efficiency. To address this problem, we propose a simple yet effective reasoning strategy: the SmartSwitch inference framework. This framework can be easily integrated into any large language model as a plug-and-play solution, continuously monitoring the model's reasoning process to detect underthinking and guide it toward deeper exploration of promising but overlooked thoughts. Specifically, the perception module identifies points where thoughts switch and evaluates the potential of the preceding thought using an off-the-shelf process reward model (PRM). If a high-potential thought is found to be prematurely abandoned, the intervention module interrupts the ongoing inference, backtracks to the point before the switch, and inserts a \"deepening prompt\" to encourage further exploration along that promising path. Extensive experiments on challenging mathematical reasoning benchmarks demonstrate that our method significantly enhances the performance of various large language models of different sizes.",
    "summary": "",
    "translation": "SmartSwitch：通过促进更深层次思维探索克服思考不足来推进大语言模型推理",
    "relevance_score": 3,
    "reasoning": "该论文主要关注LLM推理能力的提升，属于核心LLM技术进展，但与应用场景的连接较弱。虽然推理改进可能间接提升搜索中的复杂查询处理或推荐中的多步骤决策，但论文标题未明确指向RecSys/Search/Ads的具体应用，潜在相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19733v1": {
    "title": "Zhyper: Factorized Hypernetworks for Conditioned LLM Fine-Tuning",
    "url": "https://www.alphaxiv.org/abs/2510.19733v1",
    "arxiv_id": "2510.19733v1",
    "authors": "M. H. I. Abdalla, Zhipin Wang, Christian Frey, Steffen Eger, Josif Grabocka",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-22 16:25:43",
    "ori_summary": "Large Language Model (LLM) conditioning refers to instructing an LLM to generate content in accordance with the norms and values of a specific culture, beliefs of a particular political orientation, or any desired text-specified semantic conditioning. Unfortunately, prompt engineering does not ensure that LLMs behave in accordance with a desired conditioning due to the inductive bias of the pre-training and alignment datasets. Prior works have focused on fine-tuning LLMs by directly conditioning the LoRA weights; however, such methods introduce a large number of parameters. As a remedy, we propose Zhyper, a parameter-efficient factorized hypernetwork framework that generates context-aware LoRA adapters from textual descriptions. Experiments on multiple benchmarks show that Zhyper achieves competitive performance with up to 26x fewer parameters than the state-of-the-art baselines. Furthermore, we extend Zhyper to cultural alignment, demonstrating improved generalization to out-of-domain settings and a better capturing of fine-grained contextual values.",
    "summary": "论文研究LLM的条件化微调问题，核心思想是通过因子化超网络从文本描述生成上下文感知的LoRA适配器，实现参数高效的条件化模型控制。",
    "translation": "Zhyper：用于条件化LLM微调的因子化超网络",
    "relevance_score": 8,
    "reasoning": "该论文涉及超网络技术，这是Transformer架构中参数高效微调的重要进展，属于'Enabling Transformer Tech'范畴。因子化超网络可以显著降低计算成本，在推荐系统和搜索场景中实现更高效的多任务、多用户个性化LLM微调，对于大规模部署至关重要。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出参数高效的因子化超网络框架，直接针对LLM条件微调这一核心问题，在推荐系统、搜索和广告的个性化建模中具有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19723v1": {
    "title": "From Answers to Guidance: A Proactive Dialogue System for Legal Documents",
    "url": "https://www.alphaxiv.org/abs/2510.19723v1",
    "arxiv_id": "2510.19723v1",
    "authors": "Ashish Chouhan, Michael Gertz",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 16:08:05",
    "ori_summary": "The accessibility of legal information remains a constant challenge, particularly for laypersons seeking to understand and apply complex institutional texts. While the European Union provides open access to legislation, parliamentary responses, and regulatory documents, these resources can be challenging for laypeople to explore. In this paper, we introduce EUDial, a proactive multi-turn dialogue dataset constructed from 204 blogs curated by the Citizens' Enquiries Unit (AskEP) of the European Parliamentary Research Service. EUDial contains 880 dialogue turns (averaging 4.3 turns per dialogue), where each dialogue includes initial questions, structured answers, and follow-up questions. Beyond dataset construction, we propose the LexGuide framework that leverages retrieval-augmented generation with hierarchical topic organization to structure dialogue progression, ensuring both comprehensive coverage of legal aspects and coherence across conversational turns. The results demonstrate that proactive, structured navigation closes the gap between the availability of legal information and citizen comprehension, establishing EUDial and LexGuide as practical resources for advancing proactive legal dialogue systems.",
    "summary": "",
    "translation": "从答案到引导：面向法律文档的主动式对话系统",
    "relevance_score": 2,
    "reasoning": "该论文主要关注法律领域的专业对话系统，属于特定领域应用而非通用的搜索或推荐技术。虽然对话系统技术可能间接启发搜索中的对话式交互，但法律文档处理与RecSys/Search/Ads的核心技术栈关联度较低，且未涉及LLM在推荐/搜索中的核心应用或Transformer架构创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19694v1": {
    "title": "Do Prompts Reshape Representations? An Empirical Study of Prompting Effects on Embeddings",
    "url": "https://www.alphaxiv.org/abs/2510.19694v1",
    "arxiv_id": "2510.19694v1",
    "authors": "Cesar Gonzalez-Gutierrez, Dirk Hovy",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-22 15:43:40",
    "ori_summary": "Prompting is a common approach for leveraging LMs in zero-shot settings. However, the underlying mechanisms that enable LMs to perform diverse tasks without task-specific supervision remain poorly understood. Studying the relationship between prompting and the quality of internal representations can shed light on how pre-trained embeddings may support in-context task solving. In this empirical study, we conduct a series of probing experiments on prompt embeddings, analyzing various combinations of prompt templates for zero-shot classification. Our findings show that while prompting affects the quality of representations, these changes do not consistently correlate with the relevance of the prompts to the target task. This result challenges the assumption that more relevant prompts necessarily lead to better representations. We further analyze potential factors that may contribute to this unexpected behavior.",
    "summary": "研究提示如何影响语言模型的内部表征质量，核心发现是提示相关性与其产生的表征质量之间不存在必然正相关关系。",
    "translation": "提示词是否重塑表征？关于提示对嵌入影响的实证研究",
    "relevance_score": 8,
    "reasoning": "该论文研究提示工程对LLM嵌入表征的影响，属于核心LLM技术进展（Enabling LLM Tech）。在搜索和推荐系统中，提示工程直接影响查询理解、用户意图建模和内容表征质量，对提升系统性能具有直接应用价值。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文直接研究提示对LLM内部表征的影响，属于核心LLM技术的基础研究，对搜索推荐系统中提示工程优化有重要指导意义。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19687v1": {
    "title": "Are Large Language Models Sensitive to the Motives Behind Communication?",
    "url": "https://www.alphaxiv.org/abs/2510.19687v1",
    "arxiv_id": "2510.19687v1",
    "authors": "Addison J. Wu, Ryan Liu, Kerem Oktar, Theodore R. Sumers, Thomas L. Griffiths",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-22 15:35:00",
    "ori_summary": "Human communication is motivated: people speak, write, and create content with a particular communicative intent in mind. As a result, information that large language models (LLMs) and AI agents process is inherently framed by humans' intentions and incentives. People are adept at navigating such nuanced information: we routinely identify benevolent or self-serving motives in order to decide what statements to trust. For LLMs to be effective in the real world, they too must critically evaluate content by factoring in the motivations of the source -- for instance, weighing the credibility of claims made in a sales pitch. In this paper, we undertake a comprehensive study of whether LLMs have this capacity for motivational vigilance. We first employ controlled experiments from cognitive science to verify that LLMs' behavior is consistent with rational models of learning from motivated testimony, and find they successfully discount information from biased sources in a human-like manner. We then extend our evaluation to sponsored online adverts, a more naturalistic reflection of LLM agents' information ecosystems. In these settings, we find that LLMs' inferences do not track the rational models' predictions nearly as closely -- partly due to additional information that distracts them from vigilance-relevant considerations. However, a simple steering intervention that boosts the salience of intentions and incentives substantially increases the correspondence between LLMs and the rational model. These results suggest that LLMs possess a basic sensitivity to the motivations of others, but generalizing to novel real-world settings will require further improvements to these models.",
    "summary": "",
    "translation": "大型语言模型是否对沟通背后的动机敏感？",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLM对沟通动机的敏感性，这属于LLM基础行为分析范畴，与推荐系统、搜索或广告的核心技术进展关联较弱。虽然理解用户意图在搜索和推荐中有一定价值，但该研究更偏向于LLM的心理学特性分析，而非直接的技术应用或架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19670v1": {
    "title": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation",
    "url": "https://www.alphaxiv.org/abs/2510.19670v1",
    "arxiv_id": "2510.19670v1",
    "authors": "Hasan Akgul, Mari Eplik, Javier Rojas, Aina Binti Abdullah, Pieter van der Merwe",
    "categories": "cs.CL, I.2.6; C.2.4; C.3",
    "pub_date": "2025-10-22 15:16:56",
    "ori_summary": "We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer that grounds generation in site specific policies and notes; (iii) PromptRouter, a cost and uncertainty aware policy that selects edge only generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system works with modern serving optimizations, including paged or streaming KV caches, FlashAttention style kernels, speculative decoding, and quantized LoRA adapters, and supports on device personalization and federated updates under non IID drift. Across home, office, and clinic deployments, CoSense-LLM delivers grounded explanations while meeting tight service level objectives: it sustains sub second (p95) end to end latency on edge dominant paths, reduces inter tier token and bandwidth costs by preferring local retrieval grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge-RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV plus decoding accelerators lower energy per decision. The results support an edge first design that treats semantics, privacy, and predictable latency as co equal goals for large model deployments in interference prone environments.",
    "summary": "该论文研究在资源受限的边缘环境中部署大型语言模型的问题，核心方法是通过轻量级编码器将传感器数据转换为语义令牌，并结合成本感知策略实现云边协同决策，确保隐私和低延迟。",
    "translation": "CoSense-LLM：基于成本和不确定性感知的云边协同边缘语义计算",
    "relevance_score": 8,
    "reasoning": "该论文涉及云边协同架构和不确定性感知机制，这是推荐系统和搜索领域的关键使能技术。成本感知的云边协同可直接应用于移动推荐系统的实时推理优化，而不确定性感知可提升搜索和广告系统中的结果可靠性评估。",
    "rerank_relevance_score": 6,
    "rerank_reasoning": "该论文主要关注边缘计算与LLM的协同部署，虽然涉及LLM技术应用，但核心焦点是系统架构和边缘优化，与推荐/搜索/广告的直接关联较弱。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19669v1": {
    "title": "DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference",
    "url": "https://www.alphaxiv.org/abs/2510.19669v1",
    "arxiv_id": "2510.19669v1",
    "authors": "Xiang Liu, Xuming Hu, Xiaowen Chu, Eunsol Choi",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 15:16:06",
    "ori_summary": "Recent reasoning Large Language Models (LLMs) demonstrate remarkable problem-solving abilities but often generate long thinking traces whose utility is unclear. Our work aims to improve their efficiency, enabling them to reach high performance without overthinking. First, we analyze the entropy of token probabilities in reasoning traces. Across three models, we observe a consistent U-shaped entropy pattern: high entropy on easy problems despite high accuracy, low entropy on problems with medium difficulty, and high entropy on hard problems reflecting uncertainty. Specifically, we notice 22--25\\% entropy reduction from easy to medium difficulty regions, suggesting an {overthinking} phenomenon on easy instances. Building on these insights, we introduce \\textbf{DiffAdapt}, a lightweight framework that selects Easy/Normal/Hard inference strategies per question based on their difficulty and reasoning trace entropy. Each inference strategy consists of a fixed prompt, temperature and maximum token length. In contrast to existing efficiency optimization methods, our approach does not fine-tune base LLM but a small probe that classifies LLM's final hidden state, allowing inexpensive adaptation. We comprehensively evaluate our method on five models and eight benchmarks. Our method achieves comparable or improved accuracy while reducing token usage by up to 22.4\\%, establishing a practical path toward compute-efficient reasoning.",
    "summary": "该论文研究LLM推理过程中的效率优化问题，核心思想是通过分析推理轨迹的熵模式识别问题难度，并基于难度自适应选择不同的推理策略来减少不必要的计算开销。",
    "translation": "DiffAdapt：面向令牌高效大语言模型推理的难度自适应推理",
    "relevance_score": 8,
    "reasoning": "该论文专注于LLM推理效率优化，属于'Enabling LLM Tech'范畴。难度自适应推理技术可显著降低RecSys/Search/Ads系统中LLM推理的计算成本，特别是在处理复杂用户查询或个性化推荐生成时，能够根据任务复杂度动态调整计算资源，实现成本与性能的平衡。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文针对LLM推理效率问题提出难度自适应方法，直接应用于推理优化，与搜索推荐系统的性能优化高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19668v1": {
    "title": "Unraveling Emotions with Pre-Trained Models",
    "url": "https://www.alphaxiv.org/abs/2510.19668v1",
    "arxiv_id": "2510.19668v1",
    "authors": "Alejandro Pajón-Sanmartín, Francisco De Arriba-Pérez, Silvia García-Méndez, Fátima Leal, Benedita Malheiro, Juan Carlos Burguillo-Rial",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-22 15:13:52",
    "ori_summary": "Transformer models have significantly advanced the field of emotion recognition. However, there are still open challenges when exploring open-ended queries for Large Language Models (LLMs). Although current models offer good results, automatic emotion analysis in open texts presents significant challenges, such as contextual ambiguity, linguistic variability, and difficulty interpreting complex emotional expressions. These limitations make the direct application of generalist models difficult. Accordingly, this work compares the effectiveness of fine-tuning and prompt engineering in emotion detection in three distinct scenarios: (i) performance of fine-tuned pre-trained models and general-purpose LLMs using simple prompts; (ii) effectiveness of different emotion prompt designs with LLMs; and (iii) impact of emotion grouping techniques on these models. Experimental tests attain metrics above 70% with a fine-tuned pre-trained model for emotion recognition. Moreover, the findings highlight that LLMs require structured prompt engineering and emotion grouping to enhance their performance. These advancements improve sentiment analysis, human-computer interaction, and understanding of user behavior across various domains.",
    "summary": "",
    "translation": "基于预训练模型的情感解析",
    "relevance_score": 2,
    "reasoning": "该论文标题聚焦于情感分析，这属于通用NLP任务，与推荐系统、搜索或广告的核心技术关联较弱。虽然情感分析在用户评论理解等边缘场景有潜在应用，但缺乏明确的RecSys/Search/Ads技术相关性，且未涉及Transformer架构改进或跨模态建模等关键方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19654v1": {
    "title": "From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction",
    "url": "https://www.alphaxiv.org/abs/2510.19654v1",
    "arxiv_id": "2510.19654v1",
    "authors": "Zhida Zhao, Talas Fu, Yifan Wang, Lijun Wang, Huchuan Lu",
    "categories": "cs.CV, cs.AI, cs.CL, cs.RO",
    "pub_date": "2025-10-22 14:57:51",
    "ori_summary": "Despite remarkable progress in driving world models, their potential for autonomous systems remains largely untapped: the world models are mostly learned for world simulation and decoupled from trajectory planning. While recent efforts aim to unify world modeling and planning in a single framework, the synergistic facilitation mechanism of world modeling for planning still requires further exploration. In this work, we introduce a new driving paradigm named Policy World Model (PWM), which not only integrates world modeling and trajectory planning within a unified architecture, but is also able to benefit planning using the learned world knowledge through the proposed action-free future state forecasting scheme. Through collaborative state-action prediction, PWM can mimic the human-like anticipatory perception, yielding more reliable planning performance. To facilitate the efficiency of video forecasting, we further introduce a dynamically enhanced parallel token generation mechanism, equipped with a context-guided tokenizer and an adaptive dynamic focal loss. Despite utilizing only front camera input, our method matches or exceeds state-of-the-art approaches that rely on multi-view and multi-modal inputs. Code and model weights will be released at https://github.com/6550Zhao/Policy-World-Model.",
    "summary": "",
    "translation": "从预测到规划：用于协作状态-动作预测的策略世界模型",
    "relevance_score": 3,
    "reasoning": "该论文主要关注强化学习中的策略规划和状态-动作预测，属于预测到规划的转换框架。虽然预测模型在推荐和搜索中有应用潜力，但论文标题明确强调'Policy World Model'和'Collaborative State-Action Prediction'，更偏向通用强化学习领域，与推荐系统、搜索或广告的核心技术关联较弱，且没有明确指向这些特定应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19644v1": {
    "title": "LLavaCode: Compressed Code Representations for Retrieval-Augmented Code Generation",
    "url": "https://www.alphaxiv.org/abs/2510.19644v1",
    "arxiv_id": "2510.19644v1",
    "authors": "Daria Cherniuk, Nikita Sukhorukov, Nikita Sushko, Daniil Gusak, Danil Sivtsov, Elena Tutubalina, Evgeny Frolov",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 14:49:21",
    "ori_summary": "Retrieval-augmented generation has emerged as one of the most effective approaches for code completion, particularly when context from a surrounding repository is essential. However, incorporating context significantly extends sequence length, leading to slower inference - a critical limitation for interactive settings such as IDEs. In this work, we introduce LlavaCode, a framework that compresses code into compact, semantically rich representations interpretable by code LLM, enhancing generation quality while reducing the retrieved context to only a few compressed single-token vectors. Using a small projector module we can significantly increase the EM and ES metrics of coding model with negligible latency increase. Our experiments demonstrate that compressed context enables 20-38% reduction in Time-to-First-Token (TTFT) on line completion tasks compared to full-RAG pipelines.",
    "summary": "",
    "translation": "LLavaCode：用于检索增强代码生成的压缩代码表示",
    "relevance_score": 2,
    "reasoning": "该论文主要关注代码生成领域的检索增强技术，这与推荐系统、搜索或广告的核心关注点相距甚远。虽然检索增强生成（RAG）技术本身有潜力应用于搜索系统，但论文专门针对代码表示和代码生成，缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19641v1": {
    "title": "Style Attack Disguise: When Fonts Become a Camouflage for Adversarial Intent",
    "url": "https://www.alphaxiv.org/abs/2510.19641v1",
    "arxiv_id": "2510.19641v1",
    "authors": "Yangshijie Zhang, Xinda Wang, Jialin Liu, Wenqiang Wang, Zhicong Ma, Xingxing Jia",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-22 14:40:24",
    "ori_summary": "With social media growth, users employ stylistic fonts and font-like emoji to express individuality, creating visually appealing text that remains human-readable. However, these fonts introduce hidden vulnerabilities in NLP models: while humans easily read stylistic text, models process these characters as distinct tokens, causing interference. We identify this human-model perception gap and propose a style-based attack, Style Attack Disguise (SAD). We design two sizes: light for query efficiency and strong for superior attack performance. Experiments on sentiment classification and machine translation across traditional models, LLMs, and commercial services demonstrate SAD's strong attack performance. We also show SAD's potential threats to multimodal tasks including text-to-image and text-to-speech generation.",
    "summary": "",
    "translation": "风格攻击伪装：当字体成为对抗意图的伪装",
    "relevance_score": 1,
    "reasoning": "该论文涉及对抗攻击和字体伪装，属于安全领域的安全对抗主题，与我的核心关注点（推荐系统、搜索、广告中的技术进展、LLM应用等）完全无关。论文标题明确指向安全攻击和对抗意图，这属于明确排除的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19631v1": {
    "title": "HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application",
    "url": "https://www.alphaxiv.org/abs/2510.19631v1",
    "arxiv_id": "2510.19631v1",
    "authors": "Yiqian Yang, Tian Lan, Qianghuai Jia, Li Zhu, Hui Jiang, Hang Zhu, Longyue Wang, Weihua Luo, Kaifu Zhang",
    "categories": "cs.AI, cs.CL, cs.MA",
    "pub_date": "2025-10-22 14:28:33",
    "ori_summary": "Effective deep search agents must not only access open-domain and domain-specific knowledge but also apply complex rules-such as legal clauses, medical manuals and tariff rules. These rules often feature vague boundaries and implicit logic relationships, making precise application challenging for agents. However, this critical capability is largely overlooked by current agent benchmarks. To fill this gap, we introduce HSCodeComp, the first realistic, expert-level e-commerce benchmark designed to evaluate deep search agents in hierarchical rule application. In this task, the deep reasoning process of agents is guided by these rules to predict 10-digit Harmonized System Code (HSCode) of products with noisy but realistic descriptions. These codes, established by the World Customs Organization, are vital for global supply chain efficiency. Built from real-world data collected from large-scale e-commerce platforms, our proposed HSCodeComp comprises 632 product entries spanning diverse product categories, with these HSCodes annotated by several human experts. Extensive experimental results on several state-of-the-art LLMs, open-source, and closed-source agents reveal a huge performance gap: best agent achieves only 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides, detailed analysis demonstrates the challenges of hierarchical rule application, and test-time scaling fails to improve performance further.",
    "summary": "",
    "translation": "HSCodeComp：一个用于层次规则应用中深度搜索代理的现实且专家级基准",
    "relevance_score": 3,
    "reasoning": "该论文主要关注搜索代理的基准测试，与搜索领域有一定相关性。然而，它专注于特定的层次规则应用场景（如海关编码），这种特定领域的基准测试缺乏对通用搜索、推荐或广告系统的直接应用价值，且未涉及LLM或Transformer架构的核心进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19628v1": {
    "title": "CrossNews-UA: A Cross-lingual News Semantic Similarity Benchmark for Ukrainian, Polish, Russian, and English",
    "url": "https://www.alphaxiv.org/abs/2510.19628v1",
    "arxiv_id": "2510.19628v1",
    "authors": "Daryna Dementieva, Evgeniya Sukhodolskaya, Alexander Fraser",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 14:23:50",
    "ori_summary": "In the era of social networks and rapid misinformation spread, news analysis remains a critical task. Detecting fake news across multiple languages, particularly beyond English, poses significant challenges. Cross-lingual news comparison offers a promising approach to verify information by leveraging external sources in different languages (Chen and Shu, 2024). However, existing datasets for cross-lingual news analysis (Chen et al., 2022a) were manually curated by journalists and experts, limiting their scalability and adaptability to new languages. In this work, we address this gap by introducing a scalable, explainable crowdsourcing pipeline for cross-lingual news similarity assessment. Using this pipeline, we collected a novel dataset CrossNews-UA of news pairs in Ukrainian as a central language with linguistically and contextually relevant languages-Polish, Russian, and English. Each news pair is annotated for semantic similarity with detailed justifications based on the 4W criteria (Who, What, Where, When). We further tested a range of models, from traditional bag-of-words, Transformer-based architectures to large language models (LLMs). Our results highlight the challenges in multilingual news analysis and offer insights into models performance.",
    "summary": "",
    "translation": "CrossNews-UA：一个面向乌克兰语、波兰语、俄语和英语的跨语言新闻语义相似度基准",
    "relevance_score": 3,
    "reasoning": "该论文主要关注跨语言语义相似度基准构建，属于NLP评估基准范畴，与推荐系统、搜索或广告的核心技术进展关联较弱。虽然跨语言语义理解在理论上可能应用于多语言搜索场景，但论文本身更侧重于基准评估而非实际系统应用，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19616v1": {
    "title": "PBBQ: A Persian Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.19616v1",
    "arxiv_id": "2510.19616v1",
    "authors": "Farhan Farsi, Shayan Bali, Fatemeh Valeh, Parsa Ghofrani, Alireza Pakniat, Kian Kashfipour, Amir H. Payberah",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 14:12:00",
    "ori_summary": "With the increasing adoption of large language models (LLMs), ensuring their alignment with social norms has become a critical concern. While prior research has examined bias detection in various languages, there remains a significant gap in resources addressing social biases within Persian cultural contexts. In this work, we introduce PBBQ, a comprehensive benchmark dataset designed to evaluate social biases in Persian LLMs. Our benchmark, which encompasses 16 cultural categories, was developed through questionnaires completed by 250 diverse individuals across multiple demographics, in close collaboration with social science experts to ensure its validity. The resulting PBBQ dataset contains over 37,000 carefully curated questions, providing a foundation for the evaluation and mitigation of bias in Persian language models. We benchmark several open-source LLMs, a closed-source model, and Persian-specific fine-tuned models on PBBQ. Our findings reveal that current LLMs exhibit significant social biases across Persian culture. Additionally, by comparing model outputs to human responses, we observe that LLMs often replicate human bias patterns, highlighting the complex interplay between learned representations and cultural stereotypes.Upon acceptance of the paper, our PBBQ dataset will be publicly available for use in future work. Content warning: This paper contains unsafe content.",
    "summary": "",
    "translation": "PBBQ：一个通过人机协作为大型语言模型构建的波斯语偏见基准数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于波斯语偏见基准数据集构建，属于偏见、公平性等非技术性伦理话题，与您关注的推荐系统、搜索广告核心进展、Transformer架构效率、LLM直接应用等焦点完全无关。此类偏见评估基准属于您明确排除的伦理和非技术性范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19600v1": {
    "title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1",
    "url": "https://www.alphaxiv.org/abs/2510.19600v1",
    "arxiv_id": "2510.19600v1",
    "authors": "Qianli Ma, Siyu Wang, Yilin Chen, Yinhao Tang, Yixiang Yang, Chang Guo, Bingjie Gao, Zhening Xing, Yanan Sun, Zhipeng Zhang",
    "categories": "cs.SE, cs.AI, cs.CL",
    "pub_date": "2025-10-22 13:53:57",
    "ori_summary": "In the quest for scientific progress, communicating research is as vital as the discovery itself. Yet, researchers are often sidetracked by the manual, repetitive chore of building project webpages to make their dense papers accessible. While automation has tackled static slides and posters, the dynamic, interactive nature of webpages has remained an unaddressed challenge. To bridge this gap, we reframe the problem, arguing that the solution lies not in a single command, but in a collaborative, hierarchical process. We introduce $\\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy. AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline from narrative planning to multimodal content generation and interactive rendering. To combat AI hallucination, dedicated \"Checker\" agents verify each step against the source paper, while optional human checkpoints ensure the final product aligns perfectly with the author's vision, transforming the system from a mere tool into a powerful collaborative assistant. To rigorously validate our approach, we also construct $\\textbf{PageBench}$, the first benchmark for this new task. Experiments show AutoPage not only generates high-quality, visually appealing pages but does so with remarkable efficiency in under 15 minutes for less than \\$0.1. Code and dataset will be released at $\\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.",
    "summary": "",
    "translation": "人机协作的论文到页面制作，成本低于0.1美元",
    "relevance_score": 2,
    "reasoning": "该论文标题主要涉及人机协作的内容生成流程优化，属于AIGC和内容生成领域。虽然提到了低成本制作，但没有明确指向推荐系统、搜索或广告中的核心问题，也没有涉及Transformer架构改进或异构数据建模等关键技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19585v1": {
    "title": "Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark",
    "url": "https://www.alphaxiv.org/abs/2510.19585v1",
    "arxiv_id": "2510.19585v1",
    "authors": "Yu Wu, Ke Shu, Jonas Fischer, Lidia Pivovarova, David Rosson, Eetu Mäkelä, Mikko Tolonen",
    "categories": "cs.CL, cs.AI, cs.CV, cs.DL",
    "pub_date": "2025-10-22 13:37:52",
    "ori_summary": "This paper presents a novel task of extracting Latin fragments from mixed-language historical documents with varied layouts. We benchmark and evaluate the performance of large foundation models against a multimodal dataset of 724 annotated pages. The results demonstrate that reliable Latin detection with contemporary models is achievable. Our study provides the first comprehensive analysis of these models' capabilities and limits for this task.",
    "summary": "",
    "translation": "使用大型语言模型检测历史书籍中的拉丁语：一个多模态基准",
    "relevance_score": 2,
    "reasoning": "该论文主要关注历史文档中的拉丁语检测，这属于特定领域（历史语言学）的文本分析应用。虽然涉及LLM和多模态概念，但与推荐系统、搜索或广告的核心技术进展没有直接关联。多模态基准的构建方法可能对处理异构数据有启发，但应用场景过于特定且远离商业推荐/搜索领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19555v1": {
    "title": "[De|Re]constructing VLMs' Reasoning in Counting",
    "url": "https://www.alphaxiv.org/abs/2510.19555v1",
    "arxiv_id": "2510.19555v1",
    "authors": "Simone Alghisi, Gabriel Roccabruna, Massimo Rizzoli, Seyed Mahed Mousavi, Giuseppe Riccardi",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-22 13:08:47",
    "ori_summary": "Vision-Language Models (VLMs) have recently gained attention due to their competitive performance on multiple downstream tasks, achieved by following user-input instructions. However, VLMs still exhibit several limitations in visual reasoning, such as difficulties in identifying relations (e.g., spatial, temporal, and among objects), understanding temporal sequences (e.g., frames), and counting objects. In this work, we go beyond score-level benchmark evaluations of VLMs by investigating the underlying causes of their failures and proposing a targeted approach to improve their reasoning capabilities. We study the reasoning skills of seven state-of-the-art VLMs in the counting task under controlled experimental conditions. Our experiments show that VLMs are highly sensitive to the number and type of objects, their spatial arrangement, and the co-occurrence of distractors. A layer-wise analysis reveals that errors are due to incorrect mapping of the last-layer representation into the output space. Our targeted training shows that fine-tuning just the output layer improves accuracy by up to 21%. We corroborate these findings by achieving consistent improvements on real-world datasets.",
    "summary": "",
    "translation": "解构与重构视觉语言模型在计数任务中的推理过程",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型在计数任务中的推理机制分析，属于纯粹的视觉语言模型研究范畴。虽然标题提到'解构与重构推理过程'，但核心焦点是计数这一特定视觉任务，与推荐系统、搜索或广告的异构数据建模缺乏直接关联，且未明确展示在推荐/搜索领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19546v1": {
    "title": "Conditions for Catastrophic Forgetting in Multilingual Translation",
    "url": "https://www.alphaxiv.org/abs/2510.19546v1",
    "arxiv_id": "2510.19546v1",
    "authors": "Danni Liu, Jan Niehues",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 12:54:00",
    "ori_summary": "Fine-tuning multilingual foundation models on specific languages often induces catastrophic forgetting, degrading performance on languages unseen in fine-tuning. While this phenomenon is widely-documented, the literature presents fragmented results about when forgetting occurs. To address this ambiguity, we conduct a systematic empirical study using machine translation as a testbed to identify the conditions that trigger catastrophic forgetting in multilingual fine-tuning. Through controlled experiments across different model architectures, data scales, and fine-tuning approaches, we reveal that the relative scale between model and data size is a primary determinant of forgetting. Moreover, we demonstrate that a model's instruction-following ability is more critical for retaining multilingual knowledge than its architecture. Contrary to assumptions, parameter-efficient fine-tuning offers no clear advantage over full fine-tuning in mitigating forgetting. Lastly, we show that cross-lingual alignment can mitigate forgetting while also facilitating positive transfer to unseen target languages.",
    "summary": "",
    "translation": "多语言翻译中灾难性遗忘的条件",
    "relevance_score": 2,
    "reasoning": "该论文研究多语言翻译中的灾难性遗忘问题，属于NLP领域的模型稳定性研究。虽然涉及多语言处理，但主要关注训练动态和遗忘机制，与推荐系统、搜索或广告的核心技术进展缺乏直接关联，且未明确展示在RecSys/Search/Ads领域的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19509v1": {
    "title": "Which Evaluation for Which Model? A Taxonomy for Speech Model Assessment",
    "url": "https://www.alphaxiv.org/abs/2510.19509v1",
    "arxiv_id": "2510.19509v1",
    "authors": "Maureen de Seyssel, Eeshan Gunesh Dhekane",
    "categories": "cs.CL, eess.AS",
    "pub_date": "2025-10-22 12:04:32",
    "ori_summary": "Speech foundation models have recently achieved remarkable capabilities across a wide range of tasks. However, their evaluation remains disjointed across tasks and model types. Different models excel at distinct aspects of speech processing and thus require different evaluation protocols. This paper proposes a unified taxonomy that addresses the question: Which evaluation is appropriate for which model? The taxonomy defines three orthogonal axes: the \\textbf{evaluation aspect} being measured, the model capabilities required to attempt the task, and the task or protocol requirements needed to perform it. We classify a broad set of existing evaluations and benchmarks along these axes, spanning areas such as representation learning, speech generation, and interactive dialogue. By mapping each evaluation to the capabilities a model exposes (e.g., speech generation, real-time processing) and to its methodological demands (e.g., fine-tuning data, human judgment), the taxonomy provides a principled framework for aligning models with suitable evaluation methods. It also reveals systematic gaps, such as limited coverage of prosody, interaction, or reasoning, that highlight priorities for future benchmark design. Overall, this work offers a conceptual foundation and practical guide for selecting, interpreting, and extending evaluations of speech models.",
    "summary": "",
    "translation": "哪种评估适用于哪种模型？语音模型评估的分类学",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音模型的评估方法学和分类体系，这属于纯粹的语音领域研究。虽然评估方法学在技术上有一定通用性，但论文明确限定于语音模型，没有提及任何在推荐系统、搜索或广告领域的潜在应用，与用户关注的LLM、Transformer架构或异构数据建模等核心焦点完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19506v1": {
    "title": "Lookahead Routing for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.19506v1",
    "arxiv_id": "2510.19506v1",
    "authors": "Canbin Huang, Tianyuan Shi, Yuhua Zhu, Ruijun Chen, Xiaojun Quan",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 12:00:21",
    "ori_summary": "Large language model (LLM) routers improve the efficiency of multi-model systems by directing each query to the most appropriate model while leveraging the diverse strengths of heterogeneous LLMs. Most existing approaches frame routing as a classification problem based solely on the input query. While this reduces overhead by avoiding inference across all models, it overlooks valuable information that could be gleaned from potential outputs and fails to capture implicit intent or contextual nuances that often emerge only during response generation. These limitations can result in suboptimal routing decisions, particularly for complex or ambiguous queries that require deeper semantic understanding. To address this challenge, we propose Lookahead, a routing framework that \"foresees\" potential model outputs by predicting their latent representations and uses these predictions to guide model selection, thus enabling more informed routing without full inference. Within this framework, we implement two approaches based on causal and masked language models. Empirical evaluations across seven public benchmarks - spanning instruction following, mathematical reasoning, and code generation - show that Lookahead consistently outperforms existing routing baselines, achieving an average performance gain of 7.7% over the state-of-the-art. Our code is available at https://github.com/huangcb01/lookahead-routing.",
    "summary": "该论文研究如何在大规模语言模型系统中优化模型路由选择问题，其核心思想是通过预测候选模型的潜在输出表示来指导路由决策，从而在避免完整推理开销的同时实现更智能的模型选择。",
    "translation": "面向大语言模型的超前路由机制",
    "relevance_score": 8,
    "reasoning": "该论文提出的大语言模型路由机制属于'赋能LLM技术'范畴，通过优化模型推理路径选择来提升效率。这种路由技术可直接应用于推荐系统和搜索中的LLM部署，通过动态选择最优模型路径来平衡计算开销与性能，对于大规模在线服务至关重要。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出通过预测模型潜在输出来改进LLM路由决策，直接解决推荐系统中模型选择效率与准确性的核心问题，与LLM在搜索推荐领域的应用高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19493v1": {
    "title": "What is the Best Sequence Length for BABYLM?",
    "url": "https://www.alphaxiv.org/abs/2510.19493v1",
    "arxiv_id": "2510.19493v1",
    "authors": "Suchir Salhan, Richard Diehl Martinez, Zébulon Goriely, Paula Buttery",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 11:42:33",
    "ori_summary": "Transformer language models typically operate with a fixed-length context window, which has grown in step with large-scale pretraining datasets. In the BabyLM Challenge, however, many past submissions have defaulted to using much shorter sequence lengths. We examine the impact of sequence length on BabyLM pretraining, to answer the simple question: what sequence length should we be using when training Baby LMs? Using 100M-word training data and fixed compute budgets, we compare 125M-parameter Mamba and OPT models, finding that although longer is often better, the optimal length depends on both task and architecture. Shorter sequences are sufficient for grammatical generalization tasks whereas longer contexts benefit morphological analogical reasoning tasks.",
    "summary": "",
    "translation": "BABYLM的最佳序列长度是多少？",
    "relevance_score": 3,
    "reasoning": "BABYLM是关于语言模型预训练的研究，属于核心LLM技术范畴。虽然序列长度优化是Transformer架构效率的重要方面，但该论文标题没有明确指向推荐系统、搜索或广告的具体应用场景，只是泛泛讨论语言模型的序列长度问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19492v1": {
    "title": "Machine Text Detectors are Membership Inference Attacks",
    "url": "https://www.alphaxiv.org/abs/2510.19492v1",
    "arxiv_id": "2510.19492v1",
    "authors": "Ryuto Koike, Liam Dugan, Masahiro Kaneko, Chris Callison-Burch, Naoaki Okazaki",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 11:39:01",
    "ori_summary": "Although membership inference attacks (MIAs) and machine-generated text detection target different goals, identifying training samples and synthetic texts, their methods often exploit similar signals based on a language model's probability distribution. Despite this shared methodological foundation, the two tasks have been independently studied, which may lead to conclusions that overlook stronger methods and valuable insights developed in the other task. In this work, we theoretically and empirically investigate the transferability, i.e., how well a method originally developed for one task performs on the other, between MIAs and machine text detection. For our theoretical contribution, we prove that the metric that achieves the asymptotically highest performance on both tasks is the same. We unify a large proportion of the existing literature in the context of this optimal metric and hypothesize that the accuracy with which a given method approximates this metric is directly correlated with its transferability. Our large-scale empirical experiments, including 7 state-of-the-art MIA methods and 5 state-of-the-art machine text detectors across 13 domains and 10 generators, demonstrate very strong rank correlation (rho > 0.6) in cross-task performance. We notably find that Binoculars, originally designed for machine text detection, achieves state-of-the-art performance on MIA benchmarks as well, demonstrating the practical impact of the transferability. Our findings highlight the need for greater cross-task awareness and collaboration between the two research communities. To facilitate cross-task developments and fair evaluations, we introduce MINT, a unified evaluation suite for MIAs and machine-generated text detection, with implementation of 15 recent methods from both tasks.",
    "summary": "",
    "translation": "机器文本检测器即成员推理攻击",
    "relevance_score": 1,
    "reasoning": "该论文主要关注机器生成文本的检测和成员推理攻击，这属于安全、隐私和检测相关的主题，与我的核心关注点（推荐系统、搜索、广告中的核心进展、LLM技术应用、Transformer架构改进等）无关。成员推理攻击和安全检测属于明确的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19488v1": {
    "title": "VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos",
    "url": "https://www.alphaxiv.org/abs/2510.19488v1",
    "arxiv_id": "2510.19488v1",
    "authors": "Dunjie Lu, Yiheng Xu, Junli Wang, Haoyuan Wu, Xinyuan Wang, Zekun Wang, Junlin Yang, Hongjin Su, Jixuan Chen, Junda Chen, Yuchen Mao, Jingren Zhou, Junyang Lin, Binyuan Hui, Tao Yu",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-22 11:25:48",
    "ori_summary": "Training computer-use agents requires massive amounts of GUI interaction data, but manually annotating action trajectories at scale is prohibitively expensive. We present VideoAgentTrek, a scalable pipeline that automatically mines training data from publicly available screen-recorded videos at web scale, eliminating the need for manual annotation. Our approach addresses a key challenge: raw videos contain implicit demonstrations but lack explicit action labels. To solve this, we develop Video2Action, an inverse dynamics module (IDM) with two components: (1) a video grounding model that detects and localizes GUI actions with precise temporal boundaries and context, and (2) an action-content recognizer that extracts structured parameters like click coordinates and typed text with high fidelity. Applied to 39,000 YouTube tutorial videos, our pipeline generates 1.52 million interaction steps automatically. We leverage this data through continued pretraining followed by supervised fine-tuning. On OSWorld-Verified, our approach improves task success rates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On AgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results demonstrate that passive internet videos can be transformed into high-quality supervision for computer-use agents, providing a scalable alternative to expensive manual annotation.",
    "summary": "",
    "translation": "VideoAgentTrek：从未标注视频中进行计算机使用预训练",
    "relevance_score": 2,
    "reasoning": "该论文主要关注从视频中进行计算机使用的预训练，这属于计算机视觉和具身智能领域，与推荐系统、搜索或广告的核心技术栈关联较弱。虽然视频理解技术理论上可以应用于多媒体内容推荐，但论文标题明确聚焦于计算机使用场景的具身学习，缺乏明确的RecSys/Search/Ads应用路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19471v1": {
    "title": "Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.19471v1",
    "arxiv_id": "2510.19471v1",
    "authors": "Yuu Jinnai",
    "categories": "cs.CL, cs.LG, eess.AS",
    "pub_date": "2025-10-22 11:06:20",
    "ori_summary": "Recent work has shown that sample-based Minimum Bayes Risk (MBR) decoding outperforms beam search in text-to-text generation tasks, such as machine translation, text summarization, and image captioning. On the other hand, beam search is the current practice for speech-to-text tasks such as automatic speech recognition (ASR) and Speech Translation (ST). Given that MBR decoding is effective in text-to-text generation tasks, it is reasonable to expect it to also be effective for speech-to-text tasks. In this paper, we evaluate MBR decoding for ASR and ST tasks on English and Japanese using Whisper and its derivative models. We observe that the accuracy of MBR decoding outperforms that of beam search in most of the experimental settings we have evaluated. The results show that MBR decoding is a promising method for offline ASR and ST tasks that require high accuracy. The code is available at https://github.com/CyberAgentAILab/mbr-for-asr",
    "summary": "",
    "translation": "重新评估自动语音识别中的最小贝叶斯风险解码",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于自动语音识别中的解码方法评估，属于纯粹的语音处理领域。该技术没有明确的推荐系统、搜索或广告应用潜力，也不涉及LLM、Transformer架构或异构数据建模等关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19457v1": {
    "title": "MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models",
    "url": "https://www.alphaxiv.org/abs/2510.19457v1",
    "arxiv_id": "2510.19457v1",
    "authors": "Kailin Jiang, Ning Jiang, Yuchen Ren, Yuchen Li, Yifan Gao, Jinhe Bi, Yunpu Ma, Qingqing Liu, Xianhao Wang, Yifan Jia, Hongbo Jiang, Yaocong Hu, Bin Li, Lei Liu, Yuntao Du",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 10:41:57",
    "ori_summary": "Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal pre-training, yet their static representations struggle to maintain an accurate understanding of time-sensitive factual knowledge. Existing benchmarks remain constrained by static designs, inadequately evaluating LMMs' ability to understand time-sensitive knowledge. To address this gap, we propose MINED, a comprehensive benchmark that evaluates temporal awareness along 6 key dimensions and 11 challenging tasks: cognition, awareness, trustworthiness, understanding, reasoning, and robustness. MINED is constructed from Wikipedia by two professional annotators, containing 2,104 time-sensitive knowledge samples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED shows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07, while most open-source LMMs still lack time understanding ability. Meanwhile, LMMs perform best on organization knowledge, whereas their performance is weakest on sport. To address these challenges, we investigate the feasibility of updating time-sensitive knowledge in LMMs through knowledge editing methods and observe that LMMs can effectively update knowledge via knowledge editing methods in single editing scenarios.",
    "summary": "",
    "translation": "MINED：基于多模态时间敏感知识对大型多模态模型进行探测与更新",
    "relevance_score": 4,
    "reasoning": "该论文涉及多模态模型和时间敏感知识更新，与VLM类比异质数据建模有一定相关性，可用于处理搜索推荐中的动态用户行为和上下文特征。但论文焦点更偏向多模态模型维护而非直接应用于推荐系统，且可能包含较多NLP相关内容，因此相关性中等。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.19422v1": {
    "title": "LLM Unlearning with LLM Beliefs",
    "url": "https://www.alphaxiv.org/abs/2510.19422v1",
    "arxiv_id": "2510.19422v1",
    "authors": "Kemou Li, Qizhou Wang, Yue Wang, Fengpeng Li, Jun Liu, Bo Han, Jiantao Zhou",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-22 09:44:36",
    "ori_summary": "Large language models trained on vast corpora inherently risk memorizing sensitive or harmful content, which may later resurface in their outputs. Prevailing unlearning methods generally rely on gradient ascent and its variants to lower the probability of specific target responses. However, we find that this strategy induces a critical side effect: probability mass is redistributed into high-likelihood regions, often corresponding to semantically related rephrasings of the targets. We refer to this as the squeezing effect, which explains why many methods yield merely spurious unlearning, a problem further obscured by automated metrics (e.g., ROUGE, truth ratio) that misreport actual success. To address this, we propose a bootstrapping (BS) framework that explicitly links the squeezing effect with the model's own high-confidence generations, namely its model beliefs. Since model beliefs inherently capture the very high-likelihood regions where probability mass is squeezed, incorporating them into the unlearning objective directly counters the squeezing effect. By jointly suppressing both target responses and model beliefs, BS-T (token) attenuates high-probability tokens, whereas BS-S (sequence) removes entire high-confidence generations, together achieving more thorough forgetting while preserving utility. Extensive experiments across diverse benchmarks with various model families confirm the effectiveness of our approach.",
    "summary": "",
    "translation": "基于大语言模型信念的大语言模型遗忘",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM遗忘技术，这属于模型安全和隐私保护范畴，与我的关注点中的非技术性主题（如隐私）重叠。虽然LLM技术本身是关注点，但遗忘技术主要解决模型移除特定知识的问题，在推荐系统、搜索或广告中的潜在应用有限，更多涉及合规性而非核心算法改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19419v1": {
    "title": "BLiSS 1.0: Evaluating Bilingual Learner Competence in Second Language Small Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.19419v1",
    "arxiv_id": "2510.19419v1",
    "authors": "Yuan Gao, Suchir Salhan, Andrew Caines, Paula Buttery, Weiwei Sun",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 09:42:01",
    "ori_summary": "To bridge the gap between performance-oriented benchmarks and the evaluation of cognitively inspired models, we introduce BLiSS 1.0, a Benchmark of Learner Interlingual Syntactic Structure. Our benchmark operationalizes a new paradigm of selective tolerance, testing whether a model finds a naturalistic learner error more plausible than a matched, artificial error within the same sentence. Constructed from over 2.8 million naturalistic learner sentences, BLiSS provides 136,867 controlled triplets (corrected, learner, artificial) for this purpose. Experiments on a diverse suite of models demonstrate that selective tolerance is a distinct capability from standard grammaticality, with performance clustering strongly by training paradigm. This validates BLiSS as a robust tool for measuring how different training objectives impact a model's alignment with the systematic patterns of human language acquisition.",
    "summary": "",
    "translation": "BLiSS 1.0：评估第二语言小语言模型中的双语学习者能力",
    "relevance_score": 1,
    "reasoning": "该论文专注于小语言模型的双语学习者能力评估，属于教育技术领域的特定应用。这与我的核心关注点（推荐系统、搜索、广告中的LLM应用或基础架构进展）没有直接关联，也没有展示出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19413v1": {
    "title": "Spatio-temporal Sign Language Representation and Translation",
    "url": "https://www.alphaxiv.org/abs/2510.19413v1",
    "arxiv_id": "2510.19413v1",
    "authors": "Yasser Hamidullah, Josef van Genabith, Cristina España-Bonet",
    "categories": "cs.CL, cs.CV",
    "pub_date": "2025-10-22 09:34:01",
    "ori_summary": "This paper describes the DFKI-MLT submission to the WMT-SLT 2022 sign language translation (SLT) task from Swiss German Sign Language (video) into German (text). State-of-the-art techniques for SLT use a generic seq2seq architecture with customized input embeddings. Instead of word embeddings as used in textual machine translation, SLT systems use features extracted from video frames. Standard approaches often do not benefit from temporal features. In our participation, we present a system that learns spatio-temporal feature representations and translation in a single model, resulting in a real end-to-end architecture expected to better generalize to new data sets. Our best system achieved $5\\pm1$ BLEU points on the development set, but the performance on the test dropped to $0.11\\pm0.06$ BLEU points.",
    "summary": "",
    "translation": "时空手语表示与翻译",
    "relevance_score": 1,
    "reasoning": "该论文专注于手语识别和翻译这一特定领域，属于计算机视觉和多模态处理的范畴。虽然涉及表示学习，但其应用场景（手语）与推荐系统、搜索或广告领域没有直接关联，也不涉及LLM或Transformer架构在推荐/搜索/广告中的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19410v1": {
    "title": "ToMMeR -- Efficient Entity Mention Detection from Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.19410v1",
    "arxiv_id": "2510.19410v1",
    "authors": "Victor Morand, Nadi Tomeh, Josiane Mothe, Benjamin Piwowarski",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-22 09:28:18",
    "ori_summary": "Identifying which text spans refer to entities -- mention detection -- is both foundational for information extraction and a known performance bottleneck. We introduce ToMMeR, a lightweight model (<300K parameters) probing mention detection capabilities from early LLM layers. Across 13 NER benchmarks, ToMMeR achieves 93\\% recall zero-shot, with over 90\\% precision using an LLM as a judge showing that ToMMeR rarely produces spurious predictions despite high recall. Cross-model analysis reveals that diverse architectures (14M-15B parameters) converge on similar mention boundaries (DICE >75\\%), confirming that mention detection emerges naturally from language modeling. When extended with span classification heads, ToMMeR achieves near SOTA NER performance (80-87\\% F1 on standard benchmarks). Our work provides evidence that structured entity representations exist in early transformer layers and can be efficiently recovered with minimal parameters.",
    "summary": "研究如何从大型语言模型中高效检测实体提及；核心方法是利用极轻量级模型（<300K参数）从LLM早期层中提取已存在的结构化实体表示，证明实体检测能力自然内嵌于语言建模过程中。",
    "translation": "ToMMeR——基于大语言模型的高效实体提及检测",
    "relevance_score": 7,
    "reasoning": "该论文涉及从大语言模型中高效提取实体提及，这属于'使能LLM技术'范畴，在搜索和推荐系统中具有直接应用潜力，可用于改进查询理解、文档索引和用户意图识别。实体检测是构建知识增强推荐和搜索系统的关键技术，能够提升语义匹配和上下文理解能力。",
    "rerank_relevance_score": 6,
    "rerank_reasoning": "该论文揭示了LLM早期层已具备结构化实体表示能力，这对搜索和推荐系统的实体识别效率提升有直接价值，但主要聚焦信息抽取而非核心推荐算法。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19398v1": {
    "title": "SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision",
    "url": "https://www.alphaxiv.org/abs/2510.19398v1",
    "arxiv_id": "2510.19398v1",
    "authors": "Yasser Hamidullah, Shakib Yazdani, Cennet Oguz, Josef van Genabith, Cristina España-Bonet",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 09:17:31",
    "ori_summary": "Sign language translation (SLT) is typically trained with text in a single spoken language, which limits scalability and cross-language generalization. Earlier approaches have replaced gloss supervision with text-based sentence embeddings, but up to now, these remain tied to a specific language and modality. In contrast, here we employ language-agnostic, multimodal embeddings trained on text and speech from multiple languages to supervise SLT, enabling direct multilingual translation. To address data scarcity, we propose a coupled augmentation method that combines multilingual target augmentations (i.e. translations into many languages) with video-level perturbations, improving model robustness. Experiments show consistent BLEURT gains over text-only sentence embedding supervision, with larger improvements in low-resource settings. Our results demonstrate that language-agnostic embedding supervision, combined with coupled augmentation, provides a scalable and semantically robust alternative to traditional SLT training.",
    "summary": "",
    "translation": "SONAR-SLT：通过语言无关句子嵌入监督实现多语言手语翻译",
    "relevance_score": 1,
    "reasoning": "该论文专注于手语翻译这一特定领域，属于多模态处理的边缘应用。虽然涉及多语言和嵌入技术，但手语翻译与推荐系统、搜索或广告的核心技术栈没有直接关联，也不符合VLM类比中处理异构数据（如上下文特征和用户序列）的应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19386v1": {
    "title": "ColorAgent: Building A Robust, Personalized, and Interactive OS Agent",
    "url": "https://www.alphaxiv.org/abs/2510.19386v1",
    "arxiv_id": "2510.19386v1",
    "authors": "Ning Li, Qiqiang Lin, Zheng Wu, Xiaoyun Mo, Weiming Zhang, Yin Zhao, Xiangmou Qu, Jiamu Zhou, Jun Wang, Congmin Zheng, Yuanyi Song, Hongjiang Chen, Heyuan Huang, Jihong Wang, Jiaxin Yin, Jingwei Yu, Junwei Liao, Qiuying Peng, Xingyu Lou, Jun Wang, Weiwen Liu, Zhuosheng Zhang, Weinan Zhang",
    "categories": "cs.MA, cs.AI, cs.CL",
    "pub_date": "2025-10-22 09:02:48",
    "ori_summary": "With the advancements in hardware, software, and large language model technologies, the interaction between humans and operating systems has evolved from the command-line interface to the rapidly emerging AI agent interactions. Building an operating system (OS) agent capable of executing user instructions and faithfully following user desires is becoming a reality. In this technical report, we present ColorAgent, an OS agent designed to engage in long-horizon, robust interactions with the environment while also enabling personalized and proactive user interaction. To enable long-horizon interactions with the environment, we enhance the model's capabilities through step-wise reinforcement learning and self-evolving training, while also developing a tailored multi-agent framework that ensures generality, consistency, and robustness. In terms of user interaction, we explore personalized user intent recognition and proactive engagement, positioning the OS agent not merely as an automation tool but as a warm, collaborative partner. We evaluate ColorAgent on the AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2% and 50.7%, respectively, establishing a new state of the art. Nonetheless, we note that current benchmarks are insufficient for a comprehensive evaluation of OS agents and propose further exploring directions in future work, particularly in the areas of evaluation paradigms, agent collaboration, and security. Our code is available at https://github.com/MadeAgents/mobile-use.",
    "summary": "",
    "translation": "ColorAgent：构建一个鲁棒、个性化且交互式的操作系统智能体",
    "relevance_score": 2,
    "reasoning": "该论文主要关注操作系统智能体的构建，属于通用AI智能体领域，与推荐系统、搜索或广告的核心技术关联度较低。虽然个性化是推荐系统的关键特征，但论文聚焦于操作系统层面的交互智能体，缺乏明确的RecSys/Search/Ads应用场景或技术迁移路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19367v1": {
    "title": "Sign Language Translation with Sentence Embedding Supervision",
    "url": "https://www.alphaxiv.org/abs/2510.19367v1",
    "arxiv_id": "2510.19367v1",
    "authors": "Yasser Hamidullah, Josef van Genabith, Cristina España-Bonet",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 08:40:41",
    "ori_summary": "State-of-the-art sign language translation (SLT) systems facilitate the learning process through gloss annotations, either in an end2end manner or by involving an intermediate step. Unfortunately, gloss labelled sign language data is usually not available at scale and, when available, gloss annotations widely differ from dataset to dataset. We present a novel approach using sentence embeddings of the target sentences at training time that take the role of glosses. The new kind of supervision does not need any manual annotation but it is learned on raw textual data. As our approach easily facilitates multilinguality, we evaluate it on datasets covering German (PHOENIX-2014T) and American (How2Sign) sign languages and experiment with mono- and multilingual sentence embeddings and translation systems. Our approach significantly outperforms other gloss-free approaches, setting the new state-of-the-art for data sets where glosses are not available and when no additional SLT datasets are used for pretraining, diminishing the gap between gloss-free and gloss-dependent systems.",
    "summary": "",
    "translation": "基于句子嵌入监督的手语翻译",
    "relevance_score": 1,
    "reasoning": "该论文专注于手语翻译这一特定领域，属于计算机视觉和多模态处理的交叉应用。虽然涉及嵌入监督技术，但其应用场景（手语翻译）与推荐系统、搜索或广告领域没有直接关联，也不符合任何当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19366v1": {
    "title": "MoE-Prism: Disentangling Monolithic Experts for Elastic MoE Services via Model-System Co-Designs",
    "url": "https://www.alphaxiv.org/abs/2510.19366v1",
    "arxiv_id": "2510.19366v1",
    "authors": "Xinfeng Xia, Jiacheng Liu, Xiaofeng Hou, Peng Tang, Mingxuan Zhang, Wenfeng Wang, Chao Li",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-22 08:40:01",
    "ori_summary": "Mixture-of-Experts (MoE) models, the state-of-the-art in large-scale AI, achieve high quality by sparsely activating parameters. However, their reliance on routing between a few monolithic experts via a top-k mechanism creates a \"quality cliff\", offering only a few coarse-grained operating points. This inflexibility forces a difficult trade-off between cost and quality, preventing adaptation to diverse Service Level Objectives (SLOs) and leading to significant resource over-provisioning. This paper introduces MoE-Prism, a model-system co-design that transforms rigid MoE models into elastic services. Our methodology is divided into two phases. First, an \\emph{Offline Refactoring Engine} systematically deconstructs monolithic experts into fine-grained \"sub-experts.\" This engine employs a partitioning optimization solver that uses a metaheuristic-based approach to group neurons, preserving functional locality without requiring retraining. Second, an \\emph{Online Scheduling Engine} leverages this new elasticity through QoS-aware scheduling. It implements specialized policies to solve complex system problems, including maximizing throughput in cloud deployments and managing latency-optimized offloading for memory-constrained devices. Our evaluation across three different MoE models shows that MoE-Prismprovides over 4 times more distinct, stable operating points than the baseline. This allows an AI service to dynamically improve throughput by up to 19.9\\% under a strict latency budget or reduce latency by up to 10.36\\% under limited resources. MoE-Prism provides the critical \"control knob\" to bridge the model-system gap, enabling the next generation of adaptive, efficient, and QoS-aware AI services.",
    "summary": "该论文研究MoE模型在服务部署中因专家模块刚性导致的成本-质量权衡问题，核心思想是通过离线重构引擎将单体专家分解为细粒度子专家，并结合在线调度引擎实现QoS感知的弹性服务调度。",
    "translation": "MoE-Prism：通过模型-系统协同设计解耦单体专家以实现弹性MoE服务",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及Transformer架构中的混合专家（MoE）技术，属于'Enabling Transformer Tech'范畴。MoE-Prism通过解耦单体专家和弹性服务设计，可以显著提升大规模推荐系统中MoE模型的部署效率和资源利用率，这对于需要处理海量用户和物品特征的推荐系统具有重要应用价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文通过模型-系统协同设计改进MoE架构的弹性服务能力，直接涉及Transformer架构效率优化和资源自适应技术，对推荐系统的大规模部署具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19363v1": {
    "title": "LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts",
    "url": "https://www.alphaxiv.org/abs/2510.19363v1",
    "arxiv_id": "2510.19363v1",
    "authors": "Siyuan Wang, Gaokai Zhang, Li Lyna Zhang, Ning Shang, Fan Yang, Dongyao Chen, Mao Yang",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 08:35:28",
    "ori_summary": "Reasoning over long contexts is essential for large language models. While reinforcement learning (RL) enhances short-context reasoning by inducing \"Aha\" moments in chain-of-thought, the advanced thinking patterns required for long-context reasoning remain largely unexplored, and high-difficulty RL data are scarce. In this paper, we introduce LoongRL, a data-driven RL method for advanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis approach that transforms short multi-hop QA into high-difficulty long-context tasks by inserting UUID chains that hide the true question among large collections of distracting documents. Solving these tasks requires the model to trace the correct chain step-by-step, identify the true question, retrieve relevant facts and reason over them to answer correctly. RL training on KeyChain data induces an emergent plan-retrieve-reason-recheck reasoning pattern that generalizes far beyond training length. Models trained at 16K effectively solve 128K tasks without prohibitive full-length RL rollout costs. On Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA accuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches a score of 74.2, rivaling much larger frontier models such as o3-mini (74.5) and DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all 128K needle-in-a-haystack stress tests, and preserves short-context reasoning capabilities.",
    "summary": "",
    "translation": "LoongRL：面向长上下文高级推理的强化学习",
    "relevance_score": 2,
    "reasoning": "虽然该论文涉及长上下文处理，但主要聚焦于强化学习在推理任务中的应用。根据排除标准，纯粹的强化学习论文如果没有明确展示在推荐系统、搜索或广告中的相关性，则被视为不相关。论文标题没有表明在推荐系统、搜索或广告领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19361v1": {
    "title": "AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation",
    "url": "https://www.alphaxiv.org/abs/2510.19361v1",
    "arxiv_id": "2510.19361v1",
    "authors": "Xianyang Liu, Yilin Liu, Shuai Wang, Hao Cheng, Andrew Estornell, Yuzhi Zhao, Jiaheng Wei",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-22 08:34:13",
    "ori_summary": "The creation of high-quality datasets to improve Large Language Model (LLM) reasoning remains a significant challenge, as current methods often suffer from generating low-quality/incorrect answers and limited information richness from available data sources. To address this, we propose AgenticMath, a novel agentic pipeline for generating high-quality mathematical question-answer pairs to enhance the supervised fine-tuning of LLMs. Our method operates through four stages: (1) Seed Question Filter that selects questions with high information richness, complexity, and clarity; (2) an Agentic Question Rephrase step that employs a multi-agent system to generate diverse, logically consistent paraphrases; (3) an Answer Augment step where rewrite answers using chain-of-thought reasoning to enhance numerical and logical correctness, without reliance on human-provided labels; and (4) a final Question and Answer Evaluation that retains only the most superior pairs. Extensive experiments demonstrate that, fine-tuning 3B-8B parameter LLMs on AgenticMath generated datasets (comprising only 30-60K math samples) achieves competitive or superior performance on diverse in domain and out-of-domain mathematical reasoning benchmarks compared to baselines trained on much more data (e.g., 400K or 2.3M samples). Our work demonstrates that targeted, high-quality data generation is a more efficient path to improving mathematical reasoning in LLMs than large-scale, low-quality alternatives.",
    "summary": "",
    "translation": "AgenticMath：通过基于智能体的数学数据生成增强大语言模型推理能力",
    "relevance_score": 3,
    "reasoning": "该论文主要关注通过智能体方法生成数学数据来增强LLM推理能力，这属于纯粹的LLM推理改进范畴。虽然推理能力是LLM的核心技术，但论文标题未表明其在推荐系统、搜索或广告领域的直接应用潜力，更侧重于通用数学推理能力的提升。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19358v1": {
    "title": "M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.19358v1",
    "arxiv_id": "2510.19358v1",
    "authors": "Yejin Kwon, Taewoo Kang, Hyunsoo Yoon, Changouk Kim",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-22 08:28:43",
    "ori_summary": "We present M3-SLU, a new multimodal large language model (MLLM) benchmark for evaluating multi-speaker, multi-turn spoken language understanding. While recent models show strong performance in speech and text comprehension, they still struggle with speaker-attributed reasoning, the ability to understand who said what and when in natural conversations. M3-SLU is built from four open corpora (CHiME-6, MELD, MultiDialog, and AMI) and comprises over 12,000 validated instances with paired audio, transcripts, and metadata. It includes two tasks: (1) Speaker-Attributed Question Answering and (2) Speaker Attribution via Utterance Matching. We provide baseline results for both cascaded pipelines and end-to-end MLLMs, evaluated using an LLM-as-Judge and accuracy metrics. Results show that while models can capture what was said, they often fail to identify who said it, revealing a key gap in speaker-aware dialogue understanding. M3-SLU offers as a challenging benchmark to advance research in speaker-aware multimodal understanding.",
    "summary": "",
    "translation": "M3-SLU：评估多模态大语言模型中的说话人归属推理能力",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态大语言模型在说话人归属推理方面的评估，这属于纯评估基准研究。虽然涉及多模态和LLM技术，但论文焦点是评估而非技术进展，且没有明确展示在推荐系统、搜索或广告领域的应用潜力。根据用户要求，评估基准和纯NLP中心话题属于不相关范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19350v1": {
    "title": "Modeling Turn-Taking with Semantically Informed Gestures",
    "url": "https://www.alphaxiv.org/abs/2510.19350v1",
    "arxiv_id": "2510.19350v1",
    "authors": "Varsha Suresh, M. Hamza Mughal, Christian Theobalt, Vera Demberg",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 08:17:54",
    "ori_summary": "In conversation, humans use multimodal cues, such as speech, gestures, and gaze, to manage turn-taking. While linguistic and acoustic features are informative, gestures provide complementary cues for modeling these transitions. To study this, we introduce DnD Gesture++, an extension of the multi-party DnD Gesture corpus enriched with 2,663 semantic gesture annotations spanning iconic, metaphoric, deictic, and discourse types. Using this dataset, we model turn-taking prediction through a Mixture-of-Experts framework integrating text, audio, and gestures. Experiments show that incorporating semantically guided gestures yields consistent performance gains over baselines, demonstrating their complementary role in multimodal turn-taking.",
    "summary": "",
    "translation": "基于语义信息手势的对话轮次转换建模",
    "relevance_score": 1,
    "reasoning": "该论文专注于对话系统中的轮次转换建模和手势识别，属于人机交互和对话系统的特定领域。虽然涉及序列建模，但主要关注物理手势和对话轮次转换机制，与推荐系统、搜索或广告的核心技术领域没有直接关联，也不涉及LLM、Transformer架构或异构数据统一建模等关键技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19346v1": {
    "title": "Local Obfuscation by GLINER for Impartial Context Aware Lineage: Development and evaluation of PII Removal system",
    "url": "https://www.alphaxiv.org/abs/2510.19346v1",
    "arxiv_id": "2510.19346v1",
    "authors": "Prakrithi Shivaprakash, Lekhansh Shukla, Animesh Mukherjee, Prabhat Chand, Pratima Murthy",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 08:12:07",
    "ori_summary": "Removing Personally Identifiable Information (PII) from clinical notes in Electronic Health Records (EHRs) is essential for research and AI development. While Large Language Models (LLMs) are powerful, their high computational costs and the data privacy risks of API-based services limit their use, especially in low-resource settings. To address this, we developed LOGICAL (Local Obfuscation by GLINER for Impartial Context-Aware Lineage), an efficient, locally deployable PII removal system built on a fine-tuned Generalist and Lightweight Named Entity Recognition (GLiNER) model. We used 1515 clinical documents from a psychiatric hospital's EHR system. We defined nine PII categories for removal. A modern-gliner-bi-large-v1.0 model was fine-tuned on 2849 text instances and evaluated on a test set of 376 instances using character-level precision, recall, and F1-score. We compared its performance against Microsoft Azure NER, Microsoft Presidio, and zero-shot prompting with Gemini-Pro-2.5 and Llama-3.3-70B-Instruct. The fine-tuned GLiNER model achieved superior performance, with an overall micro-average F1-score of 0.980, significantly outperforming Gemini-Pro-2.5 (F1-score: 0.845). LOGICAL correctly sanitised 95% of documents completely, compared to 64% for the next-best solution. The model operated efficiently on a standard laptop without a dedicated GPU. However, a 2% entity-level false negative rate underscores the need for human-in-the-loop validation across all tested systems. Fine-tuned, specialised transformer models like GLiNER offer an accurate, computationally efficient, and secure solution for PII removal from clinical notes. This \"sanitisation at the source\" approach is a practical alternative to resource-intensive LLMs, enabling the creation of de-identified datasets for research and AI development while preserving data privacy, particularly in resource-constrained environments.",
    "summary": "",
    "translation": "基于GLINER的局部混淆用于公正上下文感知溯源：PII移除系统的开发与评估",
    "relevance_score": 1,
    "reasoning": "该论文主要关注个人身份信息（PII）移除系统，这属于隐私保护技术范畴。根据用户指定的无关主题，隐私、安全等非技术性主题应被排除。该论文没有涉及推荐系统、搜索、广告或相关的LLM/Transformer技术，也没有展示在相关领域的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19338v1": {
    "title": "Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.19338v1",
    "arxiv_id": "2510.19338v1",
    "authors": "Ling Team, Bin Han, Caizhi Tang, Chen Liang, Donghao Zhang, Fan Yuan, Feng Zhu, Jie Gao, Jingyu Hu, Longfei Li, Meng Li, Mingyang Zhang, Peijie Jiang, Peng Jiao, Qian Zhao, Qingyuan Yang, Wenbo Shen, Xinxing Yang, Yalin Zhang, Yankun Ren, Yao Zhao, Yibo Cao, Yixuan Sun, Yue Zhang, Yuchen Fang, Zibin Lin, Zixuan Cheng, Jun Zhou",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-22 07:59:38",
    "ori_summary": "In this technical report, we present the Ring-linear model series, specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0. Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both models adopt a hybrid architecture that effectively integrates linear attention and softmax attention, significantly reducing I/O and computational overhead in long-context inference scenarios. Compared to a 32 billion parameter dense model, this series reduces inference cost to 1/10, and compared to the original Ring series, the cost is also reduced by over 50%. Furthermore, through systematic exploration of the ratio between different attention mechanisms in the hybrid architecture, we have identified the currently optimal model structure. Additionally, by leveraging our self-developed high-performance FP8 operator library-linghe, overall training efficiency has been improved by 50%. Benefiting from the high alignment between the training and inference engine operators, the models can undergo long-term, stable, and highly efficient optimization during the reinforcement learning phase, consistently maintaining SOTA performance across multiple challenging complex reasoning benchmarks.",
    "summary": "该论文研究长上下文推理场景下的计算效率问题，核心思想是通过混合线性注意力和softmax注意力的架构设计来显著降低I/O和计算开销。",
    "translation": "每个注意力都重要：一种用于长上下文推理的高效混合架构",
    "relevance_score": 9,
    "reasoning": "该论文专注于Transformer架构效率和新注意力机制的进展，属于'Enabling Transformer Tech'范畴。长上下文推理能力对于搜索和推荐系统至关重要，可以处理更长的用户历史序列和上下文信息，提升个性化推荐的准确性和搜索结果的深度理解。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的混合注意力架构和高效推理技术直接适用于推荐系统的长序列建模，Transformer架构效率优化与我的研究方向高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19331v1": {
    "title": "Algorithmic Fairness in NLP: Persona-Infused LLMs for Human-Centric Hate Speech Detection",
    "url": "https://www.alphaxiv.org/abs/2510.19331v1",
    "arxiv_id": "2510.19331v1",
    "authors": "Ewelina Gajewska, Arda Derbent, Jaroslaw A Chudziak, Katarzyna Budzynska",
    "categories": "cs.CL, cs.CY",
    "pub_date": "2025-10-22 07:48:57",
    "ori_summary": "In this paper, we investigate how personalising Large Language Models (Persona-LLMs) with annotator personas affects their sensitivity to hate speech, particularly regarding biases linked to shared or differing identities between annotators and targets. To this end, we employ Google's Gemini and OpenAI's GPT-4.1-mini models and two persona-prompting methods: shallow persona prompting and a deeply contextualised persona development based on Retrieval-Augmented Generation (RAG) to incorporate richer persona profiles. We analyse the impact of using in-group and out-group annotator personas on the models' detection performance and fairness across diverse social groups. This work bridges psychological insights on group identity with advanced NLP techniques, demonstrating that incorporating socio-demographic attributes into LLMs can address bias in automated hate speech detection. Our results highlight both the potential and limitations of persona-based approaches in reducing bias, offering valuable insights for developing more equitable hate speech detection systems.",
    "summary": "",
    "translation": "NLP中的算法公平性：融合人物角色的LLM用于以人为本的仇恨言论检测",
    "relevance_score": 1,
    "reasoning": "该论文主要关注算法公平性和仇恨言论检测，这些属于明确的无关主题（公平性、伦理、非技术性话题）。虽然涉及LLM技术，但应用领域是内容审核而非推荐系统、搜索或广告的核心排名问题，且没有展示出在推荐/搜索/广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19326v1": {
    "title": "Slot Filling as a Reasoning Task for SpeechLLMs",
    "url": "https://www.alphaxiv.org/abs/2510.19326v1",
    "arxiv_id": "2510.19326v1",
    "authors": "Kadri Hacioglu, Manjunath K E, Andreas Stolcke",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 07:39:56",
    "ori_summary": "We propose integration of reasoning into speech large language models (speechLLMs) for the end-to-end slot-filling task. Inspired by the recent development of reasoning LLMs, we use a chain-of-thought framework to decompose the slot-filling task into multiple reasoning steps, create a reasoning dataset and apply the supervised fine-tuning strategy to a speechLLM. We distinguish between regular and reasoning speechLLMs and experiment with different types and sizes of LLMs as their text foundation models. We demonstrate performance improvements by introducing reasoning (intermediate) steps. However, we show that a reasoning textual LLM developed mainly for math, logic and coding domains might be inferior as a foundation model for a reasoning speechLLM. We further show that hybrid speechLLMs, built on a hybrid text foundation LLM and fine-tuned to preserve both direct and reasoning modes of operation, have better performance than those fine-tuned employing only one mode of operation.",
    "summary": "",
    "translation": "槽位填充作为语音大语言模型的推理任务",
    "relevance_score": 2,
    "reasoning": "该论文主要关注语音大语言模型（SpeechLLMs）中的槽位填充任务，这属于语音处理与NLP的交叉领域。虽然涉及大语言模型技术，但其核心应用场景是语音交互系统而非推荐系统、搜索或广告领域，与当前关注的RecSys/Search/Ads核心问题关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19325v1": {
    "title": "Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.19325v1",
    "arxiv_id": "2510.19325v1",
    "authors": "Junjie Song, Yiwen Liu, Dapeng Li, Yin Sun, Shukun Fu, Siqi Chen, Yuji Cao",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-22 07:39:04",
    "ori_summary": "Text summarization is a crucial task that requires the simultaneous optimization of multiple objectives, including consistency, coherence, relevance, and fluency, which presents considerable challenges. Although large language models (LLMs) have demonstrated remarkable performance, enhanced by reinforcement learning (RL), few studies have focused on optimizing the multi-objective problem of summarization through RL based on LLMs. In this paper, we introduce hypervolume optimization (HVO), a novel optimization strategy that dynamically adjusts the scores between groups during the reward process in RL by using the hypervolume method. This method guides the model's optimization to progressively approximate the pareto front, thereby generating balanced summaries across multiple objectives. Experimental results on several representative summarization datasets demonstrate that our method outperforms group relative policy optimization (GRPO) in overall scores and shows more balanced performance across different dimensions. Moreover, a 7B foundation model enhanced by HVO performs comparably to GPT-4 in the summarization task, while maintaining a shorter generation length. Our code is publicly available at https://github.com/ai4business-LiAuto/HVO.git",
    "summary": "",
    "translation": "文本摘要中的奖励平衡：通过超体积优化的多目标强化学习",
    "relevance_score": 2,
    "reasoning": "该论文专注于文本摘要中的多目标强化学习，这属于纯粹的NLP应用领域。虽然提到了强化学习技术，但没有展示与推荐系统、搜索或广告排名的明确关联，因此与当前关注点相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19318v1": {
    "title": "HAD: HAllucination Detection Language Models Based on a Comprehensive Hallucination Taxonomy",
    "url": "https://www.alphaxiv.org/abs/2510.19318v1",
    "arxiv_id": "2510.19318v1",
    "authors": "Fan Xu, Xinyu Hu, Zhenghan Yu, Li Lin, Xu Zhang, Yang Zhang, Wei Zhou, Jinjie Gu, Xiaojun Wan",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 07:28:37",
    "ori_summary": "The increasing reliance on natural language generation (NLG) models, particularly large language models, has raised concerns about the reliability and accuracy of their outputs. A key challenge is hallucination, where models produce plausible but incorrect information. As a result, hallucination detection has become a critical task. In this work, we introduce a comprehensive hallucination taxonomy with 11 categories across various NLG tasks and propose the HAllucination Detection (HAD) models https://github.com/pku0xff/HAD, which integrate hallucination detection, span-level identification, and correction into a single inference process. Trained on an elaborate synthetic dataset of about 90K samples, our HAD models are versatile and can be applied to various NLG tasks. We also carefully annotate a test set for hallucination detection, called HADTest, which contains 2,248 samples. Evaluations on in-domain and out-of-domain test sets show that our HAD models generally outperform the existing baselines, achieving state-of-the-art results on HaluEval, FactCHD, and FaithBench, confirming their robustness and versatility.",
    "summary": "",
    "translation": "HAD：基于全面幻觉分类学的幻觉检测语言模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于幻觉检测和分类学，这属于纯粹的NLP中心话题，与我的关注点无关。幻觉检测在推荐系统、搜索或广告中没有直接应用潜力，因为它主要解决LLM生成内容的准确性问题，而非排名或建模任务。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19316v1": {
    "title": "KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints",
    "url": "https://www.alphaxiv.org/abs/2510.19316v1",
    "arxiv_id": "2510.19316v1",
    "authors": "Kailin Jiang, Hongbo Jiang, Ning Jiang, Zhi Gao, Jinhe Bi, Yuchen Ren, Bin Li, Yuntao Du, Lei Liu, Qing Li",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 07:26:55",
    "ori_summary": "Large Multimodal Models encode extensive factual knowledge in their pre-trained weights. However, its knowledge remains static and limited, unable to keep pace with real-world developments, which hinders continuous knowledge acquisition. Effective knowledge injection thus becomes critical, involving two goals: knowledge adaptation (injecting new knowledge) and knowledge retention (preserving old knowledge). Existing methods often struggle to learn new knowledge and suffer from catastrophic forgetting. To address this, we propose KORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints for injecting new knowledge into large multimodal models while preserving old knowledge. Unlike general text or image data augmentation, KORE automatically converts individual knowledge items into structured and comprehensive knowledge to ensure that the model accurately learns new knowledge, enabling accurate adaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix of LMM's linear layer activations and initializes the adapter by projecting the original weights into the matrix's null space, defining a fine-tuning direction that minimizes interference with previous knowledge, enabling powerful retention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B, LLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new knowledge injection performance and effectively mitigates catastrophic forgetting.",
    "summary": "论文研究大型多模态模型的知识更新问题，核心方法是通过知识导向的数据增强将知识项转换为结构化表示确保准确学习，同时利用协方差矩阵空空间投影定义微调方向以最小化对旧知识的干扰。",
    "translation": "KORE：通过知识导向的数据增强与约束增强大型多模态模型的知识注入能力",
    "relevance_score": 8,
    "reasoning": "该论文聚焦于增强大型多模态模型的知识注入能力，这属于'Enabling LLM Tech'范畴，对搜索和推荐系统具有重要应用价值。在搜索系统中，更好的知识注入可以提升事实准确性、理解复杂查询意图；在推荐系统中，可以更准确地建模用户兴趣和商品知识，实现更精准的个性化推荐。知识导向的数据增强和约束方法可以直接应用于处理搜索和推荐中的异构数据融合问题。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出知识注入与保留的协同方法，直接适用于推荐系统中用户偏好动态更新的核心挑战，其结构化知识转换和干扰最小化技术具有通用迁移价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19310v1": {
    "title": "JointCQ: Improving Factual Hallucination Detection with Joint Claim and Query Generation",
    "url": "https://www.alphaxiv.org/abs/2510.19310v1",
    "arxiv_id": "2510.19310v1",
    "authors": "Fan Xu, Huixuan Zhang, Zhenliang Zhang, Jiahao Wang, Xiaojun Wan",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 07:15:37",
    "ori_summary": "Current large language models (LLMs) often suffer from hallucination issues, i,e, generating content that appears factual but is actually unreliable. A typical hallucination detection pipeline involves response decomposition (i.e., claim extraction), query generation, evidence collection (i.e., search or retrieval), and claim verification. However, existing methods exhibit limitations in the first two stages, such as context loss during claim extraction and low specificity in query generation, resulting in degraded performance across the hallucination detection pipeline. In this work, we introduce JointCQ https://github.com/pku0xff/JointCQ, a joint claim-and-query generation framework designed to construct an effective and efficient claim-query generator. Our framework leverages elaborately designed evaluation criteria to filter synthesized training data, and finetunes a language model for joint claim extraction and query generation, providing reliable and informative inputs for downstream search and verification. Experimental results demonstrate that our method outperforms previous methods on multiple open-domain QA hallucination detection benchmarks, advancing the goal of more trustworthy and transparent language model systems.",
    "summary": "",
    "translation": "JointCQ：通过联合声明和查询生成改进事实幻觉检测",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于事实幻觉检测，这属于纯粹的NLP中心主题，与我的关注点无关。论文标题表明其核心是检测LLM中的事实错误，没有显示出在推荐系统、搜索或广告中的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19286v1": {
    "title": "TheMCPCompany: Creating General-purpose Agents with Task-specific Tools",
    "url": "https://www.alphaxiv.org/abs/2510.19286v1",
    "arxiv_id": "2510.19286v1",
    "authors": "Reza Esfandiarpoor, Vishwas Suryanarayanan, Stephen H. Bach, Vishal Chowdhary, Anthony Aue",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 06:42:01",
    "ori_summary": "Since the introduction of the Model Context Protocol (MCP), the number of available tools for Large Language Models (LLMs) has increased significantly. These task-specific tool sets offer an alternative to general-purpose tools such as web browsers, while being easier to develop and maintain than GUIs. However, current general-purpose agents predominantly rely on web browsers for interacting with the environment. Here, we introduce TheMCPCompany, a benchmark for evaluating tool-calling agents on tasks that involve interacting with various real-world services. We use the REST APIs of these services to create MCP servers, which include over 18,000 tools. We also provide manually annotated ground-truth tools for each task. In our experiments, we use the ground truth tools to show the potential of tool-calling agents for both improving performance and reducing costs assuming perfect tool retrieval. Next, we explore agent performance using tool retrieval to study the real-world practicality of tool-based agents. While all models with tool retrieval perform similarly or better than browser-based agents, smaller models cannot take full advantage of the available tools through retrieval. On the other hand, GPT-5's performance with tool retrieval is very close to its performance with ground-truth tools. Overall, our work shows that the most advanced reasoning models are effective at discovering tools in simpler environments, but seriously struggle with navigating complex enterprise environments. TheMCPCompany reveals that navigating tens of thousands of tools and combining them in non-trivial ways to solve complex problems is still a challenging task for current models and requires both better reasoning and better retrieval models.",
    "summary": "",
    "translation": "TheMCP公司：使用特定任务工具创建通用智能体",
    "relevance_score": 3,
    "reasoning": "该论文关注通用智能体与任务特定工具的创建，这与推荐系统、搜索或广告中的核心进展或直接LLM应用关联较弱。虽然智能体技术可能间接应用于个性化推荐或搜索任务，但论文标题未明确表明其在RecSys/Search/Ads领域的直接应用潜力，且未涉及Transformer架构效率、多模态建模等关键焦点领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19265v1": {
    "title": "Difficulty-Controllable Multiple-Choice Question Generation Using Large Language Models and Direct Preference Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.19265v1",
    "arxiv_id": "2510.19265v1",
    "authors": "Yuto Tomikawa, Masaki Uto",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 05:49:31",
    "ori_summary": "Difficulty-controllable question generation for reading comprehension has gained significant attention in the field of education as a fundamental tool for adaptive learning support. Although several neural question generation methods have recently succeeded in controlling difficulty, conventional approaches still face two major limitations. First, they cannot directly generate multiple-choice questions, which are the most widely used question type in educational contexts. Second, they are not explicitly trained to optimize the accuracy of difficulty control, leaving room for further improvement in difficulty controllability. To address these limitations, this study proposes a novel difficulty-controllable multiple-choice question generation method for reading comprehension which leverages a large language model trained using a direct preference optimization technique to improve the accuracy of difficulty control.",
    "summary": "",
    "translation": "使用大型语言模型和直接偏好优化的难度可控多选题生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多选题生成这一特定NLP任务，属于纯粹的LLM应用场景。虽然使用了LLM和DPO技术，但缺乏明确的推荐系统、搜索或广告应用场景，更偏向于教育技术领域而非核心的RecSys/Search/Ads进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19247v1": {
    "title": "SheetBrain: A Neuro-Symbolic Agent for Accurate Reasoning over Complex and Large Spreadsheets",
    "url": "https://www.alphaxiv.org/abs/2510.19247v1",
    "arxiv_id": "2510.19247v1",
    "authors": "Ziwei Wang, Jiayuan Su, Mengyu Zhou, Huaxing Zeng, Mengni Jia, Xiao Lv, Haoyu Dong, Xiaojun Ma, Shi Han, Dongmei Zhang",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 05:09:44",
    "ori_summary": "Understanding and reasoning over complex spreadsheets remain fundamental challenges for large language models (LLMs), which often struggle with accurately capturing the complex structure of tables and ensuring reasoning correctness. In this work, we propose SheetBrain, a neuro-symbolic dual workflow agent framework designed for accurate reasoning over tabular data, supporting both spreadsheet question answering and manipulation tasks. SheetBrain comprises three core modules: an understanding module, which produces a comprehensive overview of the spreadsheet - including sheet summary and query-based problem insight to guide reasoning; an execution module, which integrates a Python sandbox with preloaded table-processing libraries and an Excel helper toolkit for effective multi-turn reasoning; and a validation module, which verifies the correctness of reasoning and answers, triggering re-execution when necessary. We evaluate SheetBrain on multiple public tabular QA and manipulation benchmarks, and introduce SheetBench, a new benchmark targeting large, multi-table, and structurally complex spreadsheets. Experimental results show that SheetBrain significantly improves accuracy on both existing benchmarks and the more challenging scenarios presented in SheetBench. Our code is publicly available at https://github.com/microsoft/SheetBrain.",
    "summary": "",
    "translation": "SheetBrain：一种用于在复杂大型电子表格上进行精确推理的神经符号智能体",
    "relevance_score": 2,
    "reasoning": "该论文主要关注电子表格的神经符号推理，这属于特定领域的应用，与推荐系统、搜索或广告的核心领域进展没有直接关联。虽然神经符号方法在技术上可能有趣，但论文没有表现出在推荐系统、搜索或广告中的潜在应用，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19217v1": {
    "title": "Modality Matching Matters: Calibrating Language Distances for Cross-Lingual Transfer in URIEL+",
    "url": "https://www.alphaxiv.org/abs/2510.19217v1",
    "arxiv_id": "2510.19217v1",
    "authors": "York Hay Ng, Aditya Khan, Xiang Lu, Matteo Salloum, Michael Zhou, Phuong H. Hoang, A. Seza Doğruöz, En-Shiun Annie Lee",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 03:59:19",
    "ori_summary": "Existing linguistic knowledge bases such as URIEL+ provide valuable geographic, genetic and typological distances for cross-lingual transfer but suffer from two key limitations. One, their one-size-fits-all vector representations are ill-suited to the diverse structures of linguistic data, and two, they lack a principled method for aggregating these signals into a single, comprehensive score. In this paper, we address these gaps by introducing a framework for type-matched language distances. We propose novel, structure-aware representations for each distance type: speaker-weighted distributions for geography, hyperbolic embeddings for genealogy, and a latent variables model for typology. We unify these signals into a robust, task-agnostic composite distance. In selecting transfer languages, our representations and composite distances consistently improve performance across a wide range of NLP tasks, providing a more principled and effective toolkit for multilingual research.",
    "summary": "",
    "translation": "模态匹配至关重要：在URIEL+中校准语言距离以实现跨语言迁移",
    "relevance_score": 3,
    "reasoning": "该论文主要关注跨语言迁移中的语言距离校准，属于多语言NLP领域。虽然多语言能力对搜索系统有一定价值，但论文核心聚焦于语言模态匹配而非推荐/搜索/广告的核心技术。在推荐/搜索/广告中，多语言应用相对边缘，且论文未明确涉及Transformer架构改进或直接应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19208v1": {
    "title": "DiSRouter: Distributed Self-Routing for LLM Selections",
    "url": "https://www.alphaxiv.org/abs/2510.19208v1",
    "arxiv_id": "2510.19208v1",
    "authors": "Hang Zheng, Hongshen Xu, Yongkai Lin, Shuai Fan, Lu Chen, Kai Yu",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 03:36:40",
    "ori_summary": "The proliferation of Large Language Models (LLMs) has created a diverse ecosystem of models with highly varying performance and costs, necessitating effective query routing to balance performance and expense. Current routing systems often rely on a centralized external router trained on a fixed set of LLMs, making them inflexible and prone to poor performance since the small router can not fully understand the knowledge boundaries of different LLMs. We introduce DiSRouter (Distributed Self-Router), a novel paradigm that shifts from centralized control to distributed routing. In DiSRouter, a query traverses a network of LLM agents, each independently deciding whether to answer or route to other agents based on its own self-awareness, its ability to judge its competence. This distributed design offers superior flexibility, scalability, and generalizability. To enable this, we propose a two-stage Self-Awareness Training pipeline that enhances each LLM's self-awareness. Extensive experiments demonstrate that DiSRouter significantly outperforms existing routing methods in utility across various scenarios, effectively distinguishes between easy and hard queries, and shows strong generalization to out-of-domain tasks. Our work validates that leveraging an LLM's intrinsic self-awareness is more effective than external assessment, paving the way for more modular and efficient multi-agent systems.",
    "summary": "论文研究如何在大规模LLM生态系统中实现高效查询路由，核心思想是采用分布式自路由范式，让每个LLM基于自身能力判断独立决定是否回答或转发查询，无需集中式路由器。",
    "translation": "DiSRouter：面向大语言模型选择的分布式自路由",
    "relevance_score": 8,
    "reasoning": "该论文专注于LLM选择的高效分布式路由机制，属于'直接LLM应用'范畴，在推荐系统、搜索和广告中具有明确的实用性。分布式自路由技术可以优化多LLM部署场景下的模型选择和负载均衡，直接提升大规模推荐和搜索系统的服务效率与质量。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出分布式自路由范式，直接解决LLM在推荐搜索系统中的路由选择问题，核心方法利用LLM内在自我意识进行路由决策，与多智能体系统和高效LLM应用高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19203v1": {
    "title": "Aligning Multilingual News for Stock Return Prediction",
    "url": "https://www.alphaxiv.org/abs/2510.19203v1",
    "arxiv_id": "2510.19203v1",
    "authors": "Yuntao Wu, Lynn Tao, Ing-Haw Cheng, Charles Martineau, Yoshio Nozawa, John Hull, Andreas Veneris",
    "categories": "q-fin.CP, cs.CL, J.4; I.2.7",
    "pub_date": "2025-10-22 03:23:24",
    "ori_summary": "News spreads rapidly across languages and regions, but translations may lose subtle nuances. We propose a method to align sentences in multilingual news articles using optimal transport, identifying semantically similar content across languages. We apply this method to align more than 140,000 pairs of Bloomberg English and Japanese news articles covering around 3500 stocks in Tokyo exchange over 2012-2024. Aligned sentences are sparser, more interpretable, and exhibit higher semantic similarity. Return scores constructed from aligned sentences show stronger correlations with realized stock returns, and long-short trading strategies based on these alignments achieve 10\\% higher Sharpe ratios than analyzing the full text sample.",
    "summary": "",
    "translation": "基于多语言新闻对齐的股票收益预测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注金融领域的股票预测应用，属于特定领域应用而非核心推荐系统、搜索或广告技术。虽然可能涉及多语言文本处理，但缺乏明确的与推荐、搜索或广告系统的直接关联，且未提及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19186v1": {
    "title": "Multi-Faceted Evaluation of Tool-Augmented Dialogue Systems",
    "url": "https://www.alphaxiv.org/abs/2510.19186v1",
    "arxiv_id": "2510.19186v1",
    "authors": "Zhaoyi Joey Hou, Tanya Shourya, Yingfan Wang, Shamik Roy, Vinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 02:44:11",
    "ori_summary": "Evaluating conversational AI systems that use external tools is challenging, as errors can arise from complex interactions among user, agent, and tools. While existing evaluation methods assess either user satisfaction or agents' tool-calling capabilities, they fail to capture critical errors in multi-turn tool-augmented dialogues-such as when agents misinterpret tool results yet appear satisfactory to users. We introduce TRACE, a benchmark of systematically synthesized tool-augmented conversations covering diverse error cases, and SCOPE, an evaluation framework that automatically discovers diverse error patterns and evaluation rubrics in tool-augmented dialogues. Experiments show SCOPE significantly outperforms the baseline, particularly on challenging cases where user satisfaction signals are misleading.",
    "summary": "",
    "translation": "工具增强对话系统的多维度评估",
    "relevance_score": 2,
    "reasoning": "该论文主要关注对话系统的工具增强和评估方法，这与纯粹的LLM应用或推荐系统/搜索/广告的核心进展关联较弱。虽然工具增强技术可能间接应用于搜索查询理解，但论文焦点在于对话系统评估而非直接的技术创新，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19181v1": {
    "title": "Interpretable Question Answering with Knowledge Graphs",
    "url": "https://www.alphaxiv.org/abs/2510.19181v1",
    "arxiv_id": "2510.19181v1",
    "authors": "Kartikeya Aneja, Manasvi Srivastava, Subhayan Das, Nagender Aneja",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-22 02:36:35",
    "ori_summary": "This paper presents a question answering system that operates exclusively on a knowledge graph retrieval without relying on retrieval augmented generation (RAG) with large language models (LLMs). Instead, a small paraphraser model is used to paraphrase the entity relationship edges retrieved from querying the knowledge graph. The proposed pipeline is divided into two main stages. The first stage involves pre-processing a document to generate sets of question-answer (QA) pairs. The second stage converts these QAs into a knowledge graph from which graph-based retrieval is performed using embeddings and fuzzy techniques. The graph is queried, re-ranked, and paraphrased to generate a final answer. This work includes an evaluation using LLM-as-a-judge on the CRAG benchmark, which resulted in accuracies of 71.9% and 54.4% using LLAMA-3.2 and GPT-3.5-Turbo, respectively.",
    "summary": "",
    "translation": "基于知识图谱的可解释问答",
    "relevance_score": 3,
    "reasoning": "该论文主要关注知识图谱上的可解释问答，这是一个典型的NLP任务，与推荐系统、搜索或广告的核心技术关联较弱。虽然知识图谱在搜索中有应用，但该论文更侧重于问答系统的可解释性，而非直接提升搜索或推荐的性能。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19176v1": {
    "title": "The Zero-Step Thinking: An Empirical Study of Mode Selection as Harder Early Exit in Reasoning Models",
    "url": "https://www.alphaxiv.org/abs/2510.19176v1",
    "arxiv_id": "2510.19176v1",
    "authors": "Yuqiao Tan, Shizhu He, Kang Liu, Jun Zhao",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-22 02:28:10",
    "ori_summary": "Reasoning models have demonstrated exceptional performance in tasks such as mathematics and logical reasoning, primarily due to their ability to engage in step-by-step thinking during the reasoning process. However, this often leads to overthinking, resulting in unnecessary computational overhead. To address this issue, Mode Selection aims to automatically decide between Long-CoT (Chain-of-Thought) or Short-CoT by utilizing either a Thinking or NoThinking mode. Simultaneously, Early Exit determines the optimal stopping point during the iterative reasoning process. Both methods seek to reduce the computational burden. In this paper, we first identify Mode Selection as a more challenging variant of the Early Exit problem, as they share similar objectives but differ in decision timing. While Early Exit focuses on determining the best stopping point for concise reasoning at inference time, Mode Selection must make this decision at the beginning of the reasoning process, relying on pre-defined fake thoughts without engaging in an explicit reasoning process, referred to as zero-step thinking. Through empirical studies on nine baselines, we observe that prompt-based approaches often fail due to their limited classification capabilities when provided with minimal hand-crafted information. In contrast, approaches that leverage internal information generally perform better across most scenarios but still exhibit issues with stability. Our findings indicate that existing methods relying solely on the information provided by models are insufficient for effectively addressing Mode Selection in scenarios with limited information, highlighting the ongoing challenges of this task. Our code is available at https://github.com/Trae1ounG/Zero_Step_Thinking.",
    "summary": "",
    "translation": "零步思考：推理模型中模式选择作为更难的早期退出的实证研究",
    "relevance_score": 3,
    "reasoning": "该论文研究推理模型中的早期退出机制和模式选择，这属于Transformer架构效率优化范畴。虽然早期退出技术可以提升推理效率，但论文聚焦于推理任务而非推荐/搜索/广告场景，且模式选择在推荐系统中的直接应用潜力有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19172v1": {
    "title": "When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA",
    "url": "https://www.alphaxiv.org/abs/2510.19172v1",
    "arxiv_id": "2510.19172v1",
    "authors": "Nishanth Sridhar Nakshatri, Shamik Roy, Manoj Ghuhan Arivazhagan, Hanhan Zhou, Vinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-22 02:12:32",
    "ori_summary": "LLMs often fail to handle temporal knowledge conflicts--contradictions arising when facts evolve over time within their training data. Existing studies evaluate this phenomenon through benchmarks built on structured knowledge bases like Wikidata, but they focus on widely-covered, easily-memorized popular entities and lack the dynamic structure needed to fairly evaluate LLMs with different knowledge cut-off dates. We introduce evolveQA, a benchmark specifically designed to evaluate LLMs on temporally evolving knowledge, constructed from 3 real-world, time-stamped corpora: AWS updates, Azure changes, and WHO disease outbreak reports. Our framework identifies naturally occurring knowledge evolution and generates questions with gold answers tailored to different LLM knowledge cut-off dates. Through extensive evaluation of 12 open and closed-source LLMs across 3 knowledge probing formats, we demonstrate significant performance drops of up to 31% on evolveQA compared to static knowledge questions.",
    "summary": "",
    "translation": "当事实改变时：使用evolveQA探究LLM在演化知识上的表现",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM在演化知识上的评估和测试，这属于纯粹的NLP评估基准范畴，与我的关注点无关。虽然涉及LLM技术，但焦点是知识演化的测试方法论，而非在推荐系统、搜索或广告中的实际应用或架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19171v1": {
    "title": "Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop RAG",
    "url": "https://www.alphaxiv.org/abs/2510.19171v1",
    "arxiv_id": "2510.19171v1",
    "authors": "Jihwan Bang, Juntae Lee, Seunghan Yang, Sungha Choi",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 02:09:23",
    "ori_summary": "Multi-hop retrieval-augmented generation (RAG) is a promising strategy for complex reasoning, yet existing iterative prompting approaches remain inefficient. They often regenerate predictable token sequences at every step and rely on stochastic stopping, leading to excessive token usage and unstable termination. We propose TSSS (Think Straight, Stop Smart), a structured multi-hop RAG framework designed for efficiency. TSSS introduces (i) a template-based reasoning that caches recurring prefixes and anchors sub-queries to the main question, reducing token generation cost while promoting stable reasoning, and (ii) a retriever-based terminator, which deterministically halts reasoning once additional sub-queries collapse into repetition. This separation of structured reasoning and termination control enables both faster inference and more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS achieves state-of-the-art accuracy and competitive efficiency among RAG-CoT approaches, highlighting its effectiveness in efficiency-constrained scenarios such as on-device inference.",
    "summary": "论文研究多跳检索增强生成中的推理效率问题，核心思想是通过模板化推理缓存重复前缀并锚定子问题，结合基于检索器的确定性终止机制来分离推理与终止控制。",
    "translation": "直截了当思考，停止耍小聪明：面向高效多跳检索增强生成的结构化推理",
    "relevance_score": 8,
    "reasoning": "该论文聚焦多跳检索增强生成(RAG)的结构化推理效率问题，属于核心LLM技术进展。在搜索和推荐系统中，多跳推理对于理解复杂用户查询、处理多模态上下文特征至关重要，结构化推理方法可直接提升搜索相关性排序和个性化推荐的准确性与效率。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出的结构化推理框架和高效终止机制可直接应用于推荐系统的多跳推理场景，其模板化方法和确定性终止策略对搜索和广告的效率优化具有参考价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19169v1": {
    "title": "OpenGuardrails: An Open-Source Context-Aware AI Guardrails Platform",
    "url": "https://www.alphaxiv.org/abs/2510.19169v1",
    "arxiv_id": "2510.19169v1",
    "authors": "Thomas Wang, Haowen Li",
    "categories": "cs.CR, cs.CL",
    "pub_date": "2025-10-22 02:02:27",
    "ori_summary": "As large language models (LLMs) become increasingly integrated into real-world applications, safeguarding them against unsafe, malicious, or privacy-violating content is critically important. We present OpenGuardrails, the first open-source project to provide both a context-aware safety and manipulation detection model and a deployable platform for comprehensive AI guardrails. OpenGuardrails protects against content-safety risks, model-manipulation attacks (e.g., prompt injection, jailbreaking, code-interpreter abuse, and the generation/execution of malicious code), and data leakage. Content-safety and model-manipulation detection are implemented by a unified large model, while data-leakage identification and redaction are performed by a separate lightweight NER pipeline (e.g., Presidio-style models or regex-based detectors). The system can be deployed as a security gateway or an API-based service, with enterprise-grade, fully private deployment options. OpenGuardrails achieves state-of-the-art (SOTA) performance on safety benchmarks, excelling in both prompt and response classification across English, Chinese, and multilingual tasks. All models are released under the Apache 2.0 license for public use.",
    "summary": "",
    "translation": "OpenGuardrails：一个开源的情境感知AI护栏平台",
    "relevance_score": 1,
    "reasoning": "该论文关注AI安全护栏技术，属于安全、伦理相关范畴，明确属于用户指定的无关主题。虽然提到了'情境感知'，但核心焦点是护栏平台而非推荐系统、搜索或广告的技术进步，与当前关注的四大方向均无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19167v1": {
    "title": "\"You Are Rejected!\": An Empirical Study of Large Language Models Taking Hiring Evaluations",
    "url": "https://www.alphaxiv.org/abs/2510.19167v1",
    "arxiv_id": "2510.19167v1",
    "authors": "Dingjie Fu, Dianxing Shi",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 01:59:30",
    "ori_summary": "With the proliferation of the internet and the rapid advancement of Artificial Intelligence, leading technology companies face an urgent annual demand for a considerable number of software and algorithm engineers. To efficiently and effectively identify high-potential candidates from thousands of applicants, these firms have established a multi-stage selection process, which crucially includes a standardized hiring evaluation designed to assess job-specific competencies. Motivated by the demonstrated prowess of Large Language Models (LLMs) in coding and reasoning tasks, this paper investigates a critical question: Can LLMs successfully pass these hiring evaluations? To this end, we conduct a comprehensive examination of a widely used professional assessment questionnaire. We employ state-of-the-art LLMs to generate responses and subsequently evaluate their performance. Contrary to any prior expectation of LLMs being ideal engineers, our analysis reveals a significant inconsistency between the model-generated answers and the company-referenced solutions. Our empirical findings lead to a striking conclusion: All evaluated LLMs fails to pass the hiring evaluation.",
    "summary": "",
    "translation": "“你被拒绝了！”：大型语言模型参与招聘评估的实证研究",
    "relevance_score": 1,
    "reasoning": "该论文研究LLMs在招聘评估中的应用，这属于人力资源领域的特定应用场景，与推荐系统、搜索或广告的核心技术进展无关。虽然涉及LLMs，但应用领域超出了指定的关注范围，且没有展示在RecSys/Search/Ads中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19144v1": {
    "title": "Tibetan Language and AI: A Comprehensive Survey of Resources, Methods and Challenges",
    "url": "https://www.alphaxiv.org/abs/2510.19144v1",
    "arxiv_id": "2510.19144v1",
    "authors": "Cheng Huang, Nyima Tashi, Fan Gao, Yutong Liu, Jiahao Li, Hao Tian, Siyang Jiang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Jin Zhang, Xiao Feng, Hao Wang, Jie Tang, Guojie Tang, Xiangxiang Wang, Jia Zhang, Tsengdar Lee, Yongbin Yu",
    "categories": "cs.CL",
    "pub_date": "2025-10-22 00:29:35",
    "ori_summary": "Tibetan, one of the major low-resource languages in Asia, presents unique linguistic and sociocultural characteristics that pose both challenges and opportunities for AI research. Despite increasing interest in developing AI systems for underrepresented languages, Tibetan has received limited attention due to a lack of accessible data resources, standardized benchmarks, and dedicated tools. This paper provides a comprehensive survey of the current state of Tibetan AI in the AI domain, covering textual and speech data resources, NLP tasks, machine translation, speech recognition, and recent developments in LLMs. We systematically categorize existing datasets and tools, evaluate methods used across different tasks, and compare performance where possible. We also identify persistent bottlenecks such as data sparsity, orthographic variation, and the lack of unified evaluation metrics. Additionally, we discuss the potential of cross-lingual transfer, multi-modal learning, and community-driven resource creation. This survey aims to serve as a foundational reference for future work on Tibetan AI research and encourages collaborative efforts to build an inclusive and sustainable AI ecosystem for low-resource languages.",
    "summary": "",
    "translation": "藏语与人工智能：资源、方法与挑战的全面综述",
    "relevance_score": 1,
    "reasoning": "该论文专注于藏语这一特定语言的AI研究，属于语言特定的NLP领域，与推荐系统、搜索或广告的核心技术进展没有直接关联。论文内容涉及语言资源和方法论，但缺乏在RecSys/Search/Ads领域的潜在应用价值或技术启发性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19139v1": {
    "title": "A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist",
    "url": "https://www.alphaxiv.org/abs/2510.19139v1",
    "arxiv_id": "2510.19139v1",
    "authors": "Sohyeon Jeon, Hyung-Chul Lee",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-22 00:15:02",
    "ori_summary": "Despite the rapid expansion of Large Language Models (LLMs) in healthcare, the ability of these systems to assess clinical trial reporting according to CONSORT standards remains unclear, particularly with respect to their cognitive and reasoning strategies. This study applies a behavioral and metacognitive analytic approach with expert-validated data, systematically comparing two representative LLMs under three prompt conditions. Clear differences emerged in how the models approached various CONSORT items, and prompt types, including shifts in reasoning style, explicit uncertainty, and alternative interpretations shaped response patterns. Our results highlight the current limitations of these systems in clinical compliance automation and underscore the importance of understanding their cognitive adaptations and strategic behavior in developing more explainable and reliable medical AI.",
    "summary": "",
    "translation": "认知能力的多维度分析：基于CONSORT清单使用大型语言模型评估提示方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM在医学研究清单(CONSORT)上的认知能力评估，属于医学领域应用和LLM评估基准研究。虽然涉及提示方法，但其应用场景是医学研究而非推荐系统、搜索或广告领域，与当前关注的技术方向相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19819v1": {
    "title": "Is This Tracker On? A Benchmark Protocol for Dynamic Tracking",
    "url": "https://www.alphaxiv.org/abs/2510.19819v1",
    "arxiv_id": "2510.19819v1",
    "authors": "Ilona Demler, Saumya Chauhan, Georgia Gkioxari",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 17:53:56",
    "ori_summary": "We introduce ITTO, a challenging new benchmark suite for evaluating and diagnosing the capabilities and limitations of point tracking methods. Our videos are sourced from existing datasets and egocentric real-world recordings, with high-quality human annotations collected through a multi-stage pipeline. ITTO captures the motion complexity, occlusion patterns, and object diversity characteristic of real-world scenes -- factors that are largely absent in current benchmarks. We conduct a rigorous analysis of state-of-the-art tracking methods on ITTO, breaking down performance along key axes of motion complexity. Our findings reveal that existing trackers struggle with these challenges, particularly in re-identifying points after occlusion, highlighting critical failure modes. These results point to the need for new modeling approaches tailored to real-world dynamics. We envision ITTO as a foundation testbed for advancing point tracking and guiding the development of more robust tracking algorithms.",
    "summary": "",
    "translation": "这个追踪器开启了吗？动态追踪的基准协议",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及动态追踪和基准协议，这属于计算机视觉或系统监控领域，与推荐系统、搜索或广告的核心技术无关。没有证据表明该追踪技术可以应用于用户行为建模、特征工程或任何RecSys/Search/Ads相关的场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19814v1": {
    "title": "How to Evaluate Monocular Depth Estimation?",
    "url": "https://www.alphaxiv.org/abs/2510.19814v1",
    "arxiv_id": "2510.19814v1",
    "authors": "Siyang Wu, Jack Nugent, Willow Yang, Jia Deng",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 17:51:24",
    "ori_summary": "Monocular depth estimation is an important task with rapid progress, but how to evaluate it remains an open question, as evidenced by a lack of standardization in existing literature and a large selection of evaluation metrics whose trade-offs and behaviors are not well understood. This paper contributes a novel, quantitative analysis of existing metrics in terms of their sensitivity to various types of perturbations of ground truth, emphasizing comparison to human judgment. Our analysis reveals that existing metrics are severely under-sensitive to curvature perturbation such as making flat surfaces wavy. To remedy this, we introduce a new metric based on relative surface normals, along with new depth visualization tools and a principled method to create composite metrics with better human alignment. Code and data are available at: https://github.com/princeton-vl/evalmde.",
    "summary": "",
    "translation": "如何评估单目深度估计？",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的深度估计评估方法，属于纯粹的视觉技术范畴。虽然深度估计在自动驾驶和机器人领域有应用，但论文标题没有显示出与推荐系统、搜索或广告的直接关联，也没有涉及LLM技术或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19802v1": {
    "title": "Class-Aware Prototype Learning with Negative Contrast for Test-Time Adaptation of Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.19802v1",
    "arxiv_id": "2510.19802v1",
    "authors": "Xiaozhen Qiao, Jingkai Zhao, Yuqiu Jiang, Xianda Guo, Zhe Sun, Hongyuan Zhang, Xuelong Li",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 17:38:35",
    "ori_summary": "Vision-Language Models (VLMs) demonstrate impressive zero-shot generalization through large-scale image-text pretraining, yet their performance can drop once the deployment distribution diverges from the training distribution. To address this, Test-Time Adaptation (TTA) methods update models using unlabeled target data. However, existing approaches often ignore two key challenges: prototype degradation in long-tailed distributions and confusion between semantically similar classes. To tackle these issues, we propose \\textbf{C}lass-Aware \\textbf{P}rototype \\textbf{L}earning with \\textbf{N}egative \\textbf{C}ontrast(\\textbf{CPL-NC}), a lightweight TTA framework designed specifically for VLMs to enhance generalization under distribution shifts. CPL-NC introduces a \\textit{Class-Aware Prototype Cache} Module that dynamically adjusts per-class capacity based on test-time frequency and activation history, with a rejuvenation mechanism for inactive classes to retain rare-category knowledge. Additionally, a \\textit{Negative Contrastive Learning} Mechanism identifies and constrains hard visual-textual negatives to improve class separability. The framework employs asymmetric optimization, refining only textual prototypes while anchoring on stable visual features. Experiments on 15 benchmarks show that CPL-NC consistently outperforms prior TTA methods across both ResNet-50 and ViT-B/16 backbones.",
    "summary": "",
    "translation": "基于负对比的类感知原型学习用于视觉语言模型的测试时自适应",
    "relevance_score": 3,
    "reasoning": "虽然该论文涉及视觉语言模型（VLM）和测试时自适应技术，但其核心关注点是计算机视觉领域的模型适应问题，而非推荐系统、搜索或广告中的异构数据处理。论文提出的原型学习和对比学习技术主要针对视觉分类任务，在推荐/搜索领域缺乏明确的直接应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19789v1": {
    "title": "OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation",
    "url": "https://www.alphaxiv.org/abs/2510.19789v1",
    "arxiv_id": "2510.19789v1",
    "authors": "Guowei Xu, Yuxuan Bian, Ailing Zeng, Mingyi Shi, Shaoli Huang, Wen Li, Lixin Duan, Qiang Xu",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 17:25:33",
    "ori_summary": "This paper introduces OmniMotion-X, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks, including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis), as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct OmniMoCap-X, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.",
    "summary": "",
    "translation": "OmniMotion-X：多功能多模态全身运动生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于人体运动生成这一计算机视觉和图形学领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然标题提及'多模态'，但内容明显偏向于人体动作生成，而非处理推荐/搜索/广告中常见的异构数据（如用户序列、上下文特征），因此不符合任何关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19760v1": {
    "title": "Adaptive Distribution-aware Quantization for Mixed-Precision Neural Networks",
    "url": "https://www.alphaxiv.org/abs/2510.19760v1",
    "arxiv_id": "2510.19760v1",
    "authors": "Shaohang Jia, Zhiyong Huang, Zhi Yu, Mingyang Hou, Shuai Miao, Han Yang",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 16:48:29",
    "ori_summary": "Quantization-Aware Training (QAT) is a critical technique for deploying deep neural networks on resource-constrained devices. However, existing methods often face two major challenges: the highly non-uniform distribution of activations and the static, mismatched codebooks used in weight quantization. To address these challenges, we propose Adaptive Distribution-aware Quantization (ADQ), a mixed-precision quantization framework that employs a differentiated strategy. The core of ADQ is a novel adaptive weight quantization scheme comprising three key innovations: (1) a quantile-based initialization method that constructs a codebook closely aligned with the initial weight distribution; (2) an online codebook adaptation mechanism based on Exponential Moving Average (EMA) to dynamically track distributional shifts; and (3) a sensitivity-informed strategy for mixed-precision allocation. For activations, we integrate a hardware-friendly non-uniform-to-uniform mapping scheme. Comprehensive experiments validate the effectiveness of our method. On ImageNet, ADQ enables a ResNet-18 to achieve 71.512% Top-1 accuracy with an average bit-width of only 2.81 bits, outperforming state-of-the-art methods under comparable conditions. Furthermore, detailed ablation studies on CIFAR-10 systematically demonstrate the individual contributions of each innovative component, validating the rationale and effectiveness of our design.",
    "summary": "该论文研究神经网络在资源受限设备上的高效部署问题，核心思想是通过自适应分布感知的混合精度量化方法，包括基于分位数的初始化、在线码本适应和敏感度指导的精度分配策略来解决权重分布不均匀和静态码本不匹配的挑战。",
    "translation": "面向混合精度神经网络的自适应分布感知量化",
    "relevance_score": 8,
    "reasoning": "该论文属于'Enabling Transformer Tech'类别，专注于神经网络效率优化。自适应分布感知量化技术可以直接应用于推荐系统和搜索中的大规模Transformer模型，通过混合精度量化显著降低模型推理延迟和存储成本，这对于工业级推荐和搜索系统的实时服务至关重要。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文专注于神经网络量化技术，虽然不直接涉及推荐系统或广告，但其混合精度量化和自适应分布感知方法对边缘设备部署LLM和Transformer模型具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19755v1": {
    "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation",
    "url": "https://www.alphaxiv.org/abs/2510.19755v1",
    "arxiv_id": "2510.19755v1",
    "authors": "Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-22 16:46:05",
    "ori_summary": "Diffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \\textit{multi-step iterations} and \\textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation. Against this backdrop, \\textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis. Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \\textit{Efficient Generative Intelligence}.",
    "summary": "",
    "translation": "扩散模型中缓存方法综述：面向高效多模态生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注扩散模型中的缓存方法优化，属于多模态生成领域。虽然提到了效率优化，但其核心应用场景是内容生成而非推荐/搜索/广告系统的核心排序任务。缓存技术本身在推荐系统中有应用，但本文聚焦于生成模型的特定优化，与当前关注点的直接关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19732v1": {
    "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.19732v1",
    "arxiv_id": "2510.19732v1",
    "authors": "Gunshi Gupta, Karmesh Yadav, Zsolt Kira, Yarin Gal, Rahaf Aljundi",
    "categories": "cs.AI, cs.CV, cs.RO",
    "pub_date": "2025-10-22 16:24:47",
    "ori_summary": "To enable embodied agents to operate effectively over extended timeframes, it is crucial to develop models that form and access memories to stay contextualized in their environment. In the current paradigm of training transformer-based policies for embodied sequential decision-making tasks, visual inputs often overwhelm the context limits of transformers, while humans can maintain and utilize a lifetime of experience compressed as memories. Significant compression is possible in principle, as much of the input is irrelevant and can be abstracted. However, existing approaches predominantly focus on either recurrent models with fixed-size memory or transformers with full-context reliance. In this work, we propose Memo, a transformer-based architecture and training recipe for reinforcement learning (RL) on memory-intensive, long-horizon tasks. Memo incorporates the creation and retrieval of memory by interleaving periodic summarization tokens with the inputs of a model during training. We demonstrate Memo's effectiveness on a gridworld meta-RL benchmark and a multi-object navigation task in photo-realistic indoor settings. Memo outperforms naive long-context transformer baselines while being more compute and storage efficient. Additionally, Memo generalizes better to longer contexts at inference time and remains robust in streaming settings, where historical context must be truncated to fit inference constraints.",
    "summary": "",
    "translation": "Memo：使用强化学习训练内存高效的具身智能体",
    "relevance_score": 2,
    "reasoning": "该论文主要关注具身智能体和强化学习中的内存效率优化，这与推荐系统、搜索或广告的核心领域没有直接关联。虽然内存效率技术可能间接应用于大规模推荐系统的资源优化，但论文的具身智能体焦点使其与当前关注点相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19716v1": {
    "title": "LyTimeT: Towards Robust and Interpretable State-Variable Discovery",
    "url": "https://www.alphaxiv.org/abs/2510.19716v1",
    "arxiv_id": "2510.19716v1",
    "authors": "Kuai Yu, Crystal Su, Xiang Liu, Judah Goldfeder, Mingyuan Shao, Hod Lipson",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 16:03:10",
    "ori_summary": "Extracting the true dynamical variables of a system from high-dimensional video is challenging due to distracting visual factors such as background motion, occlusions, and texture changes. We propose LyTimeT, a two-phase framework for interpretable variable extraction that learns robust and stable latent representations of dynamical systems. In Phase 1, LyTimeT employs a spatio-temporal TimeSformer-based autoencoder that uses global attention to focus on dynamically relevant regions while suppressing nuisance variation, enabling distraction-robust latent state learning and accurate long-horizon video prediction. In Phase 2, we probe the learned latent space, select the most physically meaningful dimensions using linear correlation analysis, and refine the transition dynamics with a Lyapunov-based stability regularizer to enforce contraction and reduce error accumulation during roll-outs. Experiments on five synthetic benchmarks and four real-world dynamical systems, including chaotic phenomena, show that LyTimeT achieves mutual information and intrinsic dimension estimates closest to ground truth, remains invariant under background perturbations, and delivers the lowest analytical mean squared error among CNN-based (TIDE) and transformer-only baselines. Our results demonstrate that combining spatio-temporal attention with stability constraints yields predictive models that are not only accurate but also physically interpretable.",
    "summary": "",
    "translation": "LyTimeT：迈向鲁棒且可解释的状态变量发现",
    "relevance_score": 3,
    "reasoning": "该论文标题暗示了时间序列分析和状态变量发现的研究方向，这可能与用户行为序列建模有一定关联。然而，没有明确证据表明该工作直接应用于推荐系统、搜索或广告领域，且缺乏与Transformer架构、LLM技术或异构数据统一建模的具体联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19695v1": {
    "title": "Explainable Face Presentation Attack Detection via Ensemble-CAM",
    "url": "https://www.alphaxiv.org/abs/2510.19695v1",
    "arxiv_id": "2510.19695v1",
    "authors": "Rashik Shadman, M G Sarwar Murshed, Faraz Hussain",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 15:45:28",
    "ori_summary": "Presentation attacks represent a critical security threat where adversaries use fake biometric data, such as face, fingerprint, or iris images, to gain unauthorized access to protected systems. Various presentation attack detection (PAD) systems have been designed leveraging deep learning (DL) models to mitigate this type of threat. Despite their effectiveness, most of the DL models function as black boxes - their decisions are opaque to their users. The purpose of explainability techniques is to provide detailed information about the reason behind the behavior or decision of DL models. In particular, visual explanation is necessary to better understand the decisions or predictions of DL-based PAD systems and determine the key regions due to which a biometric image is considered real or fake by the system. In this work, a novel technique, Ensemble-CAM, is proposed for providing visual explanations for the decisions made by deep learning-based face PAD systems. Our goal is to improve DL-based face PAD systems by providing a better understanding of their behavior. Our provided visual explanations will enhance the transparency and trustworthiness of DL-based face PAD systems.",
    "summary": "",
    "translation": "基于集成类激活映射的可解释人脸呈现攻击检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的人脸呈现攻击检测，属于生物识别安全范畴，与推荐系统、搜索或广告的核心技术领域无直接关联。论文内容涉及人脸识别安全性和可解释性，属于纯粹的视觉应用，不符合当前关注的任何技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19679v1": {
    "title": "Curvilinear Structure-preserving Unpaired Cross-domain Medical Image Translation",
    "url": "https://www.alphaxiv.org/abs/2510.19679v1",
    "arxiv_id": "2510.19679v1",
    "authors": "Zihao Chen, Yi Zhou, Xudong Jiang, Li Chen, Leopold Schmetterer, Bingyao Tan, Jun Cheng",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 15:24:32",
    "ori_summary": "Unpaired image-to-image translation has emerged as a crucial technique in medical imaging, enabling cross-modality synthesis, domain adaptation, and data augmentation without costly paired datasets. Yet, existing approaches often distort fine curvilinear structures, such as microvasculature, undermining both diagnostic reliability and quantitative analysis. This limitation is consequential in ophthalmic and vascular imaging, where subtle morphological changes carry significant clinical meaning. We propose Curvilinear Structure-preserving Translation (CST), a general framework that explicitly preserves fine curvilinear structures during unpaired translation by integrating structure consistency into the training. Specifically, CST augments baseline models with a curvilinear extraction module for topological supervision. It can be seamlessly incorporated into existing methods. We integrate it into CycleGAN and UNSB as two representative backbones. Comprehensive evaluation across three imaging modalities: optical coherence tomography angiography, color fundus and X-ray coronary angiography demonstrates that CST improves translation fidelity and achieves state-of-the-art performance. By reinforcing geometric integrity in learned mappings, CST establishes a principled pathway toward curvilinear structure-aware cross-domain translation in medical imaging.",
    "summary": "",
    "translation": "保留曲线结构特征的无配对跨域医学图像转换",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像领域的跨域转换技术，属于明确的医学领域应用，与RecSys、搜索或广告系统完全无关。论文内容涉及图像处理和医学影像分析，这些技术没有明显的推荐、搜索或广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19678v1": {
    "title": "I Spy With My Model's Eye: Visual Search as a Behavioural Test for MLLMs",
    "url": "https://www.alphaxiv.org/abs/2510.19678v1",
    "arxiv_id": "2510.19678v1",
    "authors": "John Burden, Jonathan Prunty, Ben Slater, Matthieu Tehenan, Greg Davis, Lucy Cheke",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-22 15:24:07",
    "ori_summary": "Multimodal large language models (MLLMs) achieve strong performance on vision-language tasks, yet their visual processing is opaque. Most black-box evaluations measure task accuracy, but reveal little about underlying mechanisms. Drawing on cognitive psychology, we adapt classic visual search paradigms -- originally developed to study human perception -- to test whether MLLMs exhibit the ``pop-out'' effect, where salient visual features are detected independently of distractor set size. Using controlled experiments targeting colour, size and lighting features, we find that advanced MLLMs exhibit human-like pop-out effects in colour or size-based disjunctive (single feature) search, as well as capacity limits for conjunctive (multiple feature) search. We also find evidence to suggest that MLLMs, like humans, incorporate natural scene priors such as lighting direction into object representations. We reinforce our findings using targeted fine-tuning and mechanistic interpretability analyses. Our work shows how visual search can serve as a cognitively grounded diagnostic tool for evaluating perceptual capabilities in MLLMs.",
    "summary": "",
    "translation": "我用模型之眼窥探：视觉搜索作为多模态大语言模型的行为测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态大语言模型（MLLMs）的视觉搜索行为测试，属于纯粹的视觉和多模态评估范畴。虽然视觉搜索在概念上与搜索系统相关，但论文重点在于模型行为测试而非搜索系统本身的技术进步，且没有明确说明在推荐系统或广告领域的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19626v1": {
    "title": "MedReason-R1: Learning to Reason for CT Diagnosis with Reinforcement Learning and Local Zoom",
    "url": "https://www.alphaxiv.org/abs/2510.19626v1",
    "arxiv_id": "2510.19626v1",
    "authors": "Yifan Li, Fenghe Tang, Yingtai Li, Shaohua Kevin Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 14:21:59",
    "ori_summary": "General-purpose large Vision-Language Models (VLMs) demonstrate strong capabilities in generating detailed descriptions for natural images. However, their performance in the medical domain remains suboptimal, even for relatively straightforward tasks, primarily due to the lack of large-scale, high-quality, specialized medical imaging datasets and the neglect of the diagnostic process that progresses from coarse to fine-grained. To address the first issue, we construct the CT-RATE-VQA dataset, which has 84K QA pairs. For the second issue, we propose MedReason-R1, a medical VLM with explicit reasoning process for disease diagnosis. MedReason-R1 incorporates a novel strategy that embeds zoom-in disease region-of-interest areas into the image, highlighting the crucial role of both global localization and disease-specific details in enhancing the model's diagnostic performance. Furthermore, we introduce the GRPO reinforcement learning framework to MedReason-R1, which enables effective reasoning without relying on costly manual annotations. Compared to recent general-purpose and medical VLMs, MedReason-R1 achieves state-of-the-art performance in CT disease diagnosis while retaining generalization. The code, checkpoints, and dataset are available at: https://github.com/Leevan001/MedReason-R1",
    "summary": "",
    "translation": "MedReason-R1：通过强化学习与局部缩放学习CT诊断推理",
    "relevance_score": 1,
    "reasoning": "这篇论文明确聚焦于医学CT诊断领域，属于明确的无关主题（医学应用）。虽然使用了强化学习技术，但论文标题明确表明这是针对特定医疗领域的诊断应用，与推荐系统、搜索或广告没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19622v1": {
    "title": "Augmenting Moment Retrieval: Zero-Dependency Two-Stage Learning",
    "url": "https://www.alphaxiv.org/abs/2510.19622v1",
    "arxiv_id": "2510.19622v1",
    "authors": "Zhengxuan Wei, Jiajin Tang, Sibei Yang",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 14:19:38",
    "ori_summary": "Existing Moment Retrieval methods face three critical bottlenecks: (1) data scarcity forces models into shallow keyword-feature associations; (2) boundary ambiguity in transition regions between adjacent events; (3) insufficient discrimination of fine-grained semantics (e.g., distinguishing ``kicking\" vs. ``throwing\" a ball). In this paper, we propose a zero-external-dependency Augmented Moment Retrieval framework, AMR, designed to overcome local optima caused by insufficient data annotations and the lack of robust boundary and semantic discrimination capabilities. AMR is built upon two key insights: (1) it resolves ambiguous boundary information and semantic confusion in existing annotations without additional data (avoiding costly manual labeling), and (2) it preserves boundary and semantic discriminative capabilities enhanced by training while generalizing to real-world scenarios, significantly improving performance. Furthermore, we propose a two-stage training framework with cold-start and distillation adaptation. The cold-start stage employs curriculum learning on augmented data to build foundational boundary/semantic awareness. The distillation stage introduces dual query sets: Original Queries maintain DETR-based localization using frozen Base Queries from the cold-start model, while Active Queries dynamically adapt to real-data distributions. A cross-stage distillation loss enforces consistency between Original and Base Queries, preventing knowledge forgetting while enabling real-world generalization. Experiments on multiple benchmarks show that AMR achieves improved performance over prior state-of-the-art approaches.",
    "summary": "",
    "translation": "增强时刻检索：零依赖两阶段学习",
    "relevance_score": 2,
    "reasoning": "该论文涉及视频时刻检索任务，这属于计算机视觉领域，与推荐系统、搜索或广告的核心关注点没有直接关联。虽然两阶段学习架构可能具有通用性，但论文标题没有表明在Transformer效率、多模态建模或推荐系统应用方面的潜在贡献。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19618v1": {
    "title": "Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism",
    "url": "https://www.alphaxiv.org/abs/2510.19618v1",
    "arxiv_id": "2510.19618v1",
    "authors": "Junfei Zhou, Penglin Dai, Quanmin Wei, Bingyi Liu, Xiao Wu, Jianping Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 14:15:20",
    "ori_summary": "Multi-agent collaboration enhances the perception capabilities of individual agents through information sharing. However, in real-world applications, differences in sensors and models across heterogeneous agents inevitably lead to domain gaps during collaboration. Existing approaches based on adaptation and reconstruction fail to support pragmatic heterogeneous collaboration due to two key limitations: (1) Intrusive retraining of the encoder or core modules disrupts the established semantic consistency among agents; and (2) accommodating new agents incurs high computational costs, limiting scalability. To address these challenges, we present a novel Generative Communication mechanism (GenComm) that facilitates seamless perception across heterogeneous multi-agent systems through feature generation, without altering the original network, and employs lightweight numerical alignment of spatial information to efficiently integrate new agents at minimal cost. Specifically, a tailored Deformable Message Extractor is designed to extract spatial message for each collaborator, which is then transmitted in place of intermediate features. The Spatial-Aware Feature Generator, utilizing a conditional diffusion model, generates features aligned with the ego agent's semantic space while preserving the spatial information of the collaborators. These generated features are further refined by a Channel Enhancer before fusion. Experiments conducted on the OPV2V-H, DAIR-V2X and V2X-Real datasets demonstrate that GenComm outperforms existing state-of-the-art methods, achieving an 81\\% reduction in both computational cost and parameter count when incorporating new agents. Our code is available at https://github.com/jeffreychou777/GenComm.",
    "summary": "",
    "translation": "基于生成式通信机制的实用异构协同感知",
    "relevance_score": 3,
    "reasoning": "该论文主要关注异构系统中的协同感知问题，虽然涉及多模态数据融合和通信机制，但其核心应用场景更偏向自动驾驶、机器人等感知系统，而非推荐系统、搜索或广告领域。生成式通信机制在理论上可能启发多源数据融合方法，但与当前关注的核心技术方向关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19612v1": {
    "title": "Beyond sparse denoising in frames: minimax estimation with a scattering transform",
    "url": "https://www.alphaxiv.org/abs/2510.19612v1",
    "arxiv_id": "2510.19612v1",
    "authors": "Nathanaël Cuvelle--Magar, Stéphane Mallat",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 14:05:25",
    "ori_summary": "A considerable amount of research in harmonic analysis has been devoted to non-linear estimators of signals contaminated by additive Gaussian noise. They are implemented by thresholding coefficients in a frame, which provide a sparse signal representation, or by minimising their $\\ell^1$ norm. However, sparse estimators in frames are not sufficiently rich to adapt to complex signal regularities. For cartoon images whose edges are piecewise $\\bf C^\\alpha$ curves, wavelet, curvelet and Xlet frames are suboptimal if the Lipschitz exponent $\\alpha \\leq 2$ is an unknown parameter. Deep convolutional neural networks have recently obtained much better numerical results, which reach the minimax asymptotic bounds for all $\\alpha$. Wavelet scattering coefficients have been introduced as simplified convolutional neural network models. They are computed by transforming the modulus of wavelet coefficients with a second wavelet transform. We introduce a denoising estimator by jointly minimising and maximising the $\\ell^1$ norms of different subsets of scattering coefficients. We prove that these $\\ell^1$ norms capture different types of geometric image regularity. Numerical experiments show that this denoising estimator reaches the minimax asymptotic bound for cartoon images for all Lipschitz exponents $\\alpha \\leq 2$. We state this numerical result as a mathematical conjecture. It provides a different harmonic analysis approach to suppress noise from signals, and to specify the geometric regularity of functions. It also opens a mathematical bridge between harmonic analysis and denoising estimators with deep convolutional network.",
    "summary": "",
    "translation": "超越帧稀疏去噪：基于散射变换的极小极大估计",
    "relevance_score": 2,
    "reasoning": "该论文主要关注信号处理中的散射变换和极小极大估计理论，属于基础数学和信号处理领域。虽然散射变换在特征提取方面有一定潜力，但论文标题明确聚焦于稀疏去噪和估计理论，与推荐系统、搜索或广告的核心技术栈距离较远，缺乏明确的跨模态建模或Transformer架构的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19599v1": {
    "title": "XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography",
    "url": "https://www.alphaxiv.org/abs/2510.19599v1",
    "arxiv_id": "2510.19599v1",
    "authors": "Haozhe Luo, Shelley Zixin Shu, Ziyu Zhou, Sebastian Otalora, Mauricio Reyes",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-22 13:52:19",
    "ori_summary": "Vision-language models (VLMs) have recently shown remarkable zero-shot performance in medical image understanding, yet their grounding ability, the extent to which textual concepts align with visual evidence, remains underexplored. In the medical domain, however, reliable grounding is essential for interpretability and clinical adoption. In this work, we present the first systematic benchmark for evaluating cross-modal interpretability in chest X-rays across seven CLIP-style VLM variants. We generate visual explanations using cross-attention and similarity-based localization maps, and quantitatively assess their alignment with radiologist-annotated regions across multiple pathologies. Our analysis reveals that: (1) while all VLM variants demonstrate reasonable localization for large and well-defined pathologies, their performance substantially degrades for small or diffuse lesions; (2) models that are pretrained on chest X-ray-specific datasets exhibit improved alignment compared to those trained on general-domain data. (3) The overall recognition ability and grounding ability of the model are strongly correlated. These findings underscore that current VLMs, despite their strong recognition ability, still fall short in clinically reliable grounding, highlighting the need for targeted interpretability benchmarks before deployment in medical practice. XBench code is available at https://github.com/Roypic/Benchmarkingattention",
    "summary": "",
    "translation": "XBench：胸部放射学中视觉语言解释的综合基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像领域的视觉语言基准测试，属于明确的医学领域应用。虽然涉及视觉语言模型概念，但专门针对胸部放射学这一医疗场景，与推荐系统、搜索或广告领域没有任何直接或间接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19597v1": {
    "title": "CBDiff:Conditional Bernoulli Diffusion Models for Image Forgery Localization",
    "url": "https://www.alphaxiv.org/abs/2510.19597v1",
    "arxiv_id": "2510.19597v1",
    "authors": "Zhou Lei, Pan Gang, Wang Jiahao, Sun Di",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 13:48:36",
    "ori_summary": "Image Forgery Localization (IFL) is a crucial task in image forensics, aimed at accurately identifying manipulated or tampered regions within an image at the pixel level. Existing methods typically generate a single deterministic localization map, which often lacks the precision and reliability required for high-stakes applications such as forensic analysis and security surveillance. To enhance the credibility of predictions and mitigate the risk of errors, we introduce an advanced Conditional Bernoulli Diffusion Model (CBDiff). Given a forged image, CBDiff generates multiple diverse and plausible localization maps, thereby offering a richer and more comprehensive representation of the forgery distribution. This approach addresses the uncertainty and variability inherent in tampered regions. Furthermore, CBDiff innovatively incorporates Bernoulli noise into the diffusion process to more faithfully reflect the inherent binary and sparse properties of forgery masks. Additionally, CBDiff introduces a Time-Step Cross-Attention (TSCAttention), which is specifically designed to leverage semantic feature guidance with temporal steps to improve manipulation detection. Extensive experiments on eight publicly benchmark datasets demonstrate that CBDiff significantly outperforms existing state-of-the-art methods, highlighting its strong potential for real-world deployment.",
    "summary": "",
    "translation": "CBDiff：用于图像伪造定位的条件伯努利扩散模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于图像伪造检测和定位，属于计算机视觉安全领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然扩散模型是生成式AI的重要技术，但该应用场景（图像伪造定位）在RecSys/Search/Ads领域缺乏明确的实用价值和应用路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19592v1": {
    "title": "Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.19592v1",
    "arxiv_id": "2510.19592v1",
    "authors": "Su Ho Han, Jeongseok Hyun, Pilhyeon Lee, Minho Shim, Dongyoon Wee, Seon Joo Kim",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 13:42:59",
    "ori_summary": "Multimodal large language models (MLLMs) demonstrate strong video understanding by attending to visual tokens relevant to textual queries. To directly adapt this for localization in a training-free manner, we cast video reasoning segmentation as a video QA task and extract attention maps via rollout mechanism. However, raw attention maps are noisy and poorly aligned with object regions. We propose Decomposed Attention Fusion (DecAF), which refines these maps through two mechanisms: (1) contrastive object-background fusion and (2) complementary video-frame fusion. This method suppresses irrelevant activations and enhances object-focused cues, enabling direct conversion of attention maps into coarse segmentation masks. In addition, we introduce attention-guided SAM2 prompting for obtaining fine-grained masks. Unlike existing methods that jointly train MLLMs with SAM, our method operates entirely without retraining. DecAF outperforms training-free methods and achieves performance comparable to training-based methods on both referring and reasoning VOS benchmarks. The code will be available at https://github.com/HYUNJS/DecAF.",
    "summary": "",
    "translation": "多模态大语言模型中分解注意力融合用于免训练视频推理分割",
    "relevance_score": 3,
    "reasoning": "该论文虽然涉及多模态建模和注意力机制，但其核心聚焦于视频分割这一计算机视觉任务，与推荐系统、搜索或广告的排序和建模需求关联较弱。分解注意力融合技术虽有潜力应用于异构数据处理，但论文的免训练视频推理分割应用场景与当前关注领域差距较大。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19590v1": {
    "title": "Digitizing Paper ECGs at Scale: An Open-Source Algorithm for Clinical Research",
    "url": "https://www.alphaxiv.org/abs/2510.19590v1",
    "arxiv_id": "2510.19590v1",
    "authors": "Elias Stenhede, Agnar Martin Bjørnstad, Arian Ranjbar",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 13:41:21",
    "ori_summary": "Millions of clinical ECGs exist only as paper scans, making them unusable for modern automated diagnostics. We introduce a fully automated, modular framework that converts scanned or photographed ECGs into digital signals, suitable for both clinical and research applications. The framework is validated on 37,191 ECG images with 1,596 collected at Akershus University Hospital, where the algorithm obtains a mean signal-to-noise ratio of 19.65 dB on scanned papers with common artifacts. It is further evaluated on the Emory Paper Digitization ECG Dataset, comprising 35,595 images, including images with perspective distortion, wrinkles, and stains. The model improves on the state-of-the-art in all subcategories. The full software is released as open-source, promoting reproducibility and further development. We hope the software will contribute to unlocking retrospective ECG archives and democratize access to AI-driven diagnostics.",
    "summary": "",
    "translation": "大规模数字化纸质心电图：一种用于临床研究的开源算法",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域的ECG数字化处理，属于医学图像处理范畴，与推荐系统、搜索或广告领域完全无关。论文内容涉及临床研究和医疗数据转换，没有任何技术要素可以应用于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19586v1": {
    "title": "Uncertainty evaluation of segmentation models for Earth observation",
    "url": "https://www.alphaxiv.org/abs/2510.19586v1",
    "arxiv_id": "2510.19586v1",
    "authors": "Melanie Rey, Andriy Mnih, Maxim Neumann, Matt Overlan, Drew Purves",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-22 13:39:28",
    "ori_summary": "This paper investigates methods for estimating uncertainty in semantic segmentation predictions derived from satellite imagery. Estimating uncertainty for segmentation presents unique challenges compared to standard image classification, requiring scalable methods producing per-pixel estimates. While most research on this topic has focused on scene understanding or medical imaging, this work benchmarks existing methods specifically for remote sensing and Earth observation applications. Our evaluation focuses on the practical utility of uncertainty measures, testing their ability to identify prediction errors and noise-corrupted input image regions. Experiments are conducted on two remote sensing datasets, PASTIS and ForTy, selected for their differences in scale, geographic coverage, and label confidence. We perform an extensive evaluation featuring several models, such as Stochastic Segmentation Networks and ensembles, in combination with a number of neural architectures and uncertainty metrics. We make a number of practical recommendations based on our findings.",
    "summary": "",
    "translation": "地球观测分割模型的不确定性评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于地球观测领域的计算机视觉分割任务，与推荐系统、搜索或广告的核心领域完全无关。不确定性评估方法虽然技术上有价值，但在地球观测这一特定领域应用，没有明显的路径可以转化到推荐系统或广告技术中。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19581v1": {
    "title": "Addressing the Depth-of-Field Constraint: A New Paradigm for High Resolution Multi-Focus Image Fusion",
    "url": "https://www.alphaxiv.org/abs/2510.19581v1",
    "arxiv_id": "2510.19581v1",
    "authors": "Luca Piano, Peng Huanwen, Radu Ciprian Bilcu",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 13:32:04",
    "ori_summary": "Multi-focus image fusion (MFIF) addresses the depth-of-field (DOF) limitations of optical lenses, where only objects within a specific range appear sharp. Although traditional and deep learning methods have advanced the field, challenges persist, including limited training data, domain gaps from synthetic datasets, and difficulties with regions lacking information. We propose VAEEDOF, a novel MFIF method that uses a distilled variational autoencoder for high-fidelity, efficient image reconstruction. Our fusion module processes up to seven images simultaneously, enabling robust fusion across diverse focus points. To address data scarcity, we introduce MattingMFIF, a new syntetic 4K dataset, simulating realistic DOF effects from real photographs. Our method achieves state-of-the-art results, generating seamless artifact-free fused images and bridging the gap between synthetic and real-world scenarios, offering a significant step forward in addressing complex MFIF challenges. The code, and weights are available here:",
    "summary": "",
    "translation": "解决景深限制：高分辨率多焦点图像融合的新范式",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的多焦点图像融合技术，旨在解决摄影和成像领域的景深限制问题。这与推荐系统、搜索或广告的核心技术领域完全无关，也不涉及LLM、Transformer架构或异构数据处理等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19579v1": {
    "title": "Multi-modal Co-learning for Earth Observation: Enhancing single-modality models via modality collaboration",
    "url": "https://www.alphaxiv.org/abs/2510.19579v1",
    "arxiv_id": "2510.19579v1",
    "authors": "Francisco Mena, Dino Ienco, Cassio F. Dantas, Roberto Interdonato, Andreas Dengel",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-22 13:29:32",
    "ori_summary": "Multi-modal co-learning is emerging as an effective paradigm in machine learning, enabling models to collaboratively learn from different modalities to enhance single-modality predictions. Earth Observation (EO) represents a quintessential domain for multi-modal data analysis, wherein diverse remote sensors collect data to sense our planet. This unprecedented volume of data introduces novel challenges. Specifically, the access to the same sensor modalities at both training and inference stages becomes increasingly complex based on real-world constraints affecting remote sensing platforms. In this context, multi-modal co-learning presents a promising strategy to leverage the vast amount of sensor-derived data available at the training stage to improve single-modality models for inference-time deployment. Most current research efforts focus on designing customized solutions for either particular downstream tasks or specific modalities available at the inference stage. To address this, we propose a novel multi-modal co-learning framework capable of generalizing across various tasks without targeting a specific modality for inference. Our approach combines contrastive and modality discriminative learning together to guide single-modality models to structure the internal model manifold into modality-shared and modality-specific information. We evaluate our framework on four EO benchmarks spanning classification and regression tasks across different sensor modalities, where only one of the modalities available during training is accessible at inference time. Our results demonstrate consistent predictive improvements over state-of-the-art approaches from the recent machine learning and computer vision literature, as well as EO-specific methods. The obtained findings validate our framework in the single-modality inference scenarios across a diverse range of EO applications.",
    "summary": "",
    "translation": "地球观测中的多模态协同学习：通过模态协作增强单模态模型",
    "relevance_score": 3,
    "reasoning": "该论文虽然涉及多模态学习技术，但其应用领域是地球观测，与推荐系统、搜索或广告没有直接关联。多模态协同学习技术本身具有潜在价值，但论文没有表明这些技术会应用于相关领域，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19578v1": {
    "title": "VGD: Visual Geometry Gaussian Splatting for Feed-Forward Surround-view Driving Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2510.19578v1",
    "arxiv_id": "2510.19578v1",
    "authors": "Junhong Lin, Kangli Wang, Shunzhou Wang, Songlin Fan, Ge Li, Wei Gao",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 13:28:49",
    "ori_summary": "Feed-forward surround-view autonomous driving scene reconstruction offers fast, generalizable inference ability, which faces the core challenge of ensuring generalization while elevating novel view quality. Due to the surround-view with minimal overlap regions, existing methods typically fail to ensure geometric consistency and reconstruction quality for novel views. To tackle this tension, we claim that geometric information must be learned explicitly, and the resulting features should be leveraged to guide the elevating of semantic quality in novel views. In this paper, we introduce \\textbf{Visual Gaussian Driving (VGD)}, a novel feed-forward end-to-end learning framework designed to address this challenge. To achieve generalizable geometric estimation, we design a lightweight variant of the VGGT architecture to efficiently distill its geometric priors from the pre-trained VGGT to the geometry branch. Furthermore, we design a Gaussian Head that fuses multi-scale geometry tokens to predict Gaussian parameters for novel view rendering, which shares the same patch backbone as the geometry branch. Finally, we integrate multi-scale features from both geometry and Gaussian head branches to jointly supervise a semantic refinement model, optimizing rendering quality through feature-consistent learning. Experiments on nuScenes demonstrate that our approach significantly outperforms state-of-the-art methods in both objective metrics and subjective quality under various settings, which validates VGD's scalability and high-fidelity surround-view reconstruction.",
    "summary": "",
    "translation": "VGD：用于前馈环视驾驶重建的视觉几何高斯泼溅技术",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的3D场景重建技术，特别是针对自动驾驶的环视重建应用。虽然标题提到'视觉几何'和'重建'，但这属于纯粹的3D视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。高斯泼溅技术主要用于3D表示和渲染，在当前技术路径下难以看到在RecSys/Search/Ads领域的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19574v1": {
    "title": "Can You Trust What You See? Alpha Channel No-Box Attacks on Video Object Detection",
    "url": "https://www.alphaxiv.org/abs/2510.19574v1",
    "arxiv_id": "2510.19574v1",
    "authors": "Ariana Yi, Ce Zhou, Liyang Xiao, Qiben Yan",
    "categories": "cs.CV, cs.CR",
    "pub_date": "2025-10-22 13:27:02",
    "ori_summary": "As object detection models are increasingly deployed in cyber-physical systems such as autonomous vehicles (AVs) and surveillance platforms, ensuring their security against adversarial threats is essential. While prior work has explored adversarial attacks in the image domain, those attacks in the video domain remain largely unexamined, especially in the no-box setting. In this paper, we present {\\alpha}-Cloak, the first no-box adversarial attack on object detectors that operates entirely through the alpha channel of RGBA videos. {\\alpha}-Cloak exploits the alpha channel to fuse a malicious target video with a benign video, resulting in a fused video that appears innocuous to human viewers but consistently fools object detectors. Our attack requires no access to model architecture, parameters, or outputs, and introduces no perceptible artifacts. We systematically study the support for alpha channels across common video formats and playback applications, and design a fusion algorithm that ensures visual stealth and compatibility. We evaluate {\\alpha}-Cloak on five state-of-the-art object detectors, a vision-language model, and a multi-modal large language model (Gemini-2.0-Flash), demonstrating a 100% attack success rate across all scenarios. Our findings reveal a previously unexplored vulnerability in video-based perception systems, highlighting the urgent need for defenses that account for the alpha channel in adversarial settings.",
    "summary": "",
    "translation": "你能相信你所看到的吗？针对视频目标检测的Alpha通道无盒攻击",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于视频目标检测领域的对抗性攻击和安全性问题，属于计算机视觉安全范畴。这与我的关注领域（推荐系统、搜索、广告中的核心进展、LLM技术及其应用）完全无关，且明确排除了安全、隐私等非技术主题以及纯粹的视觉论文。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19560v1": {
    "title": "HAD: Hierarchical Asymmetric Distillation to Bridge Spatio-Temporal Gaps in Event-Based Object Tracking",
    "url": "https://www.alphaxiv.org/abs/2510.19560v1",
    "arxiv_id": "2510.19560v1",
    "authors": "Yao Deng, Xian Zhong, Wenxuan Liu, Zhaofei Yu, Jingling Yuan, Tiejun Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 13:15:13",
    "ori_summary": "RGB cameras excel at capturing rich texture details with high spatial resolution, whereas event cameras offer exceptional temporal resolution and a high dynamic range (HDR). Leveraging their complementary strengths can substantially enhance object tracking under challenging conditions, such as high-speed motion, HDR environments, and dynamic background interference. However, a significant spatio-temporal asymmetry exists between these two modalities due to their fundamentally different imaging mechanisms, hindering effective multi-modal integration. To address this issue, we propose {Hierarchical Asymmetric Distillation} (HAD), a multi-modal knowledge distillation framework that explicitly models and mitigates spatio-temporal asymmetries. Specifically, HAD proposes a hierarchical alignment strategy that minimizes information loss while maintaining the student network's computational efficiency and parameter compactness. Extensive experiments demonstrate that HAD consistently outperforms state-of-the-art methods, and comprehensive ablation studies further validate the effectiveness and necessity of each designed component. The code will be released soon.",
    "summary": "",
    "translation": "HAD：基于分层非对称蒸馏的事件驱动目标跟踪时空差距桥接方法",
    "relevance_score": 2,
    "reasoning": "该论文专注于事件驱动视觉中的目标跟踪技术，属于计算机视觉领域而非推荐系统、搜索或广告的核心范畴。虽然事件相机数据可视为一种异构模态，但论文未明确展示其在推荐系统、搜索或广告中的潜在应用价值，与当前关注点关联度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19557v1": {
    "title": "The Intricate Dance of Prompt Complexity, Quality, Diversity, and Consistency in T2I Models",
    "url": "https://www.alphaxiv.org/abs/2510.19557v1",
    "arxiv_id": "2510.19557v1",
    "authors": "Xiaofeng Zhang, Aaron Courville, Michal Drozdzal, Adriana Romero-Soriano",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 13:13:27",
    "ori_summary": "Text-to-image (T2I) models offer great potential for creating virtually limitless synthetic data, a valuable resource compared to fixed and finite real datasets. Previous works evaluate the utility of synthetic data from T2I models on three key desiderata: quality, diversity, and consistency. While prompt engineering is the primary means of interacting with T2I models, the systematic impact of prompt complexity on these critical utility axes remains underexplored. In this paper, we first conduct synthetic experiments to motivate the difficulty of generalization w.r.t. prompt complexity and explain the observed difficulty with theoretical derivations. Then, we introduce a new evaluation framework that can compare the utility of real data and synthetic data, and present a comprehensive analysis of how prompt complexity influences the utility of synthetic data generated by commonly used T2I models. We conduct our study across diverse datasets, including CC12M, ImageNet-1k, and DCI, and evaluate different inference-time intervention methods. Our synthetic experiments show that generalizing to more general conditions is harder than the other way round, since the former needs an estimated likelihood that is not learned by diffusion models. Our large-scale empirical experiments reveal that increasing prompt complexity results in lower conditional diversity and prompt consistency, while reducing the synthetic-to-real distribution shift, which aligns with the synthetic experiments. Moreover, current inference-time interventions can augment the diversity of the generations at the expense of moving outside the support of real data. Among those interventions, prompt expansion, by deliberately using a pre-trained language model as a likelihood estimator, consistently achieves the highest performance in both image diversity and aesthetics, even higher than that of real data.",
    "summary": "",
    "translation": "文本到图像模型中提示词复杂性、质量、多样性与一致性的复杂交互关系",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本到图像(T2I)模型的提示工程研究，属于纯粹的图像生成领域。虽然提示工程是LLM相关技术，但该研究聚焦于图像生成质量评估，与推荐系统、搜索或广告的核心技术需求没有直接关联，也不涉及Transformer架构改进或异构数据处理。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19527v1": {
    "title": "PoseCrafter: Extreme Pose Estimation with Hybrid Video Synthesis",
    "url": "https://www.alphaxiv.org/abs/2510.19527v1",
    "arxiv_id": "2510.19527v1",
    "authors": "Qing Mao, Tianxin Huang, Yu Zhu, Jinqiu Sun, Yanning Zhang, Gim Hee Lee",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 12:32:37",
    "ori_summary": "Pairwise camera pose estimation from sparsely overlapping image pairs remains a critical and unsolved challenge in 3D vision. Most existing methods struggle with image pairs that have small or no overlap. Recent approaches attempt to address this by synthesizing intermediate frames using video interpolation and selecting key frames via a self-consistency score. However, the generated frames are often blurry due to small overlap inputs, and the selection strategies are slow and not explicitly aligned with pose estimation. To solve these cases, we propose Hybrid Video Generation (HVG) to synthesize clearer intermediate frames by coupling a video interpolation model with a pose-conditioned novel view synthesis model, where we also propose a Feature Matching Selector (FMS) based on feature correspondence to select intermediate frames appropriate for pose estimation from the synthesized results. Extensive experiments on Cambridge Landmarks, ScanNet, DL3DV-10K, and NAVI demonstrate that, compared to existing SOTA methods, PoseCrafter can obviously enhance the pose estimation performances, especially on examples with small or no overlap.",
    "summary": "",
    "translation": "PoseCrafter：基于混合视频合成的极端姿态估计",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的姿态估计和视频合成技术，属于纯粹的视觉领域研究。虽然姿态估计在理论上可能用于用户行为分析，但论文标题明确聚焦于极端姿态估计和视频合成，与推荐系统、搜索或广告的核心技术栈没有直接关联，也没有显示出向这些领域的潜在应用迁移路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19496v1": {
    "title": "CARES: Context-Aware Resolution Selector for VLMs",
    "url": "https://www.alphaxiv.org/abs/2510.19496v1",
    "arxiv_id": "2510.19496v1",
    "authors": "Moshe Kimhi, Nimrod Shabtay, Raja Giryes, Chaim Baskin, Eli Schwartz",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-22 11:44:31",
    "ori_summary": "Large vision-language models (VLMs) commonly process images at native or high resolution to remain effective across tasks. This inflates visual tokens ofter to 97-99% of total tokens, resulting in high compute and latency, even when low-resolution images would suffice. We introduce \\emph{CARES}-a \\textbf{C}ontext-\\textbf{A}ware \\textbf{R}esolution \\textbf{S}elector, a lightweight preprocessing module that, given an image-query pair, predicts the \\emph{minimal} sufficient input resolution. CARES uses a compact VLM (350M) to extract features and predict when a target pretrained VLM's response converges to its peak ability to answer correctly. Though trained as a discrete classifier over a set of optional resolutions, CARES interpolates continuous resolutions at inference for fine-grained control. Across five multimodal benchmarks spanning documents and natural images, as well as diverse target VLMs, CARES preserves task performance while reducing compute by up to 80%.",
    "summary": "该论文研究如何降低视觉语言模型的计算开销问题，核心思想是设计一个轻量级预处理模块，通过分析图像-查询对的上下文特征来预测完成任务所需的最小输入分辨率，实现计算资源的动态优化分配。",
    "translation": "CARES：面向视觉语言模型的情境感知分辨率选择器",
    "relevance_score": 7,
    "reasoning": "该论文提出了一种面向视觉语言模型的分辨率选择机制，属于视觉语言模型（VLM）的效率优化技术。虽然论文本身聚焦视觉模态，但这种情境感知的输入处理机制可以类比应用于推荐/搜索系统中的异构数据处理，例如根据用户上下文动态选择特征分辨率或序列长度，从而提高系统效率。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出了一种基于上下文感知的输入分辨率选择方法，虽然针对VLM设计，但其核心思想——根据输入特征动态选择计算资源分配——可直接迁移到推荐系统中处理用户序列和上下文特征的资源优化问题。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.19487v1": {
    "title": "Towards Single-Source Domain Generalized Object Detection via Causal Visual Prompts",
    "url": "https://www.alphaxiv.org/abs/2510.19487v1",
    "arxiv_id": "2510.19487v1",
    "authors": "Chen Li, Huiying Xu, Changxin Gao, Zeyu Wang, Yun Liu, Xinzhong Zhu",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 11:24:52",
    "ori_summary": "Single-source Domain Generalized Object Detection (SDGOD), as a cutting-edge research topic in computer vision, aims to enhance model generalization capability in unseen target domains through single-source domain training. Current mainstream approaches attempt to mitigate domain discrepancies via data augmentation techniques. However, due to domain shift and limited domain-specific knowledge, models tend to fall into the pitfall of spurious correlations. This manifests as the model's over-reliance on simplistic classification features (e.g., color) rather than essential domain-invariant representations like object contours. To address this critical challenge, we propose the Cauvis (Causal Visual Prompts) method. First, we introduce a Cross-Attention Prompts module that mitigates bias from spurious features by integrating visual prompts with cross-attention. To address the inadequate domain knowledge coverage and spurious feature entanglement in visual prompts for single-domain generalization, we propose a dual-branch adapter that disentangles causal-spurious features while achieving domain adaptation via high-frequency feature extraction. Cauvis achieves state-of-the-art performance with 15.9-31.4% gains over existing domain generalization methods on SDGOD datasets, while exhibiting significant robustness advantages in complex interference environments.",
    "summary": "",
    "translation": "基于因果视觉提示实现单源域泛化目标检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的目标检测和域泛化问题，属于纯粹的视觉领域研究。虽然提到了因果推理和提示学习等概念，但这些技术在当前标题描述中主要应用于视觉域适应，没有明确展示在推荐系统、搜索或广告中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19478v1": {
    "title": "Mitigating representation bias caused by missing pixels in methane plume detection",
    "url": "https://www.alphaxiv.org/abs/2510.19478v1",
    "arxiv_id": "2510.19478v1",
    "authors": "Julia Wąsala, Joannes D. Maasakkers, Ilse Aben, Rochelle Schneider, Holger Hoos, Mitra Baratchi",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 11:15:31",
    "ori_summary": "Most satellite images have systematically missing pixels (i.e., missing data not at random (MNAR)) due to factors such as clouds. If not addressed, these missing pixels can lead to representation bias in automated feature extraction models. In this work, we show that spurious association between the label and the number of missing values in methane plume detection can cause the model to associate the coverage (i.e., the percentage of valid pixels in an image) with the label, subsequently under-detecting plumes in low-coverage images. We evaluate multiple imputation approaches to remove the dependence between the coverage and a label. Additionally, we propose a weighted resampling scheme during training that removes the association between the label and the coverage by enforcing class balance in each coverage bin. Our results show that both resampling and imputation can significantly reduce the representation bias without hurting balanced accuracy, precision, or recall. Finally, we evaluate the capability of the debiased models using these techniques in an operational scenario and demonstrate that the debiased models have a higher chance of detecting plumes in low-coverage images.",
    "summary": "",
    "translation": "缓解甲烷羽流检测中因缺失像素导致的表征偏差",
    "relevance_score": 1,
    "reasoning": "该论文专注于甲烷羽流检测这一特定环境科学应用，涉及计算机视觉中的像素缺失问题。这与推荐系统、搜索或广告的核心领域进展、使能技术或直接应用完全无关，属于明确的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19475v1": {
    "title": "PRGCN: A Graph Memory Network for Cross-Sequence Pattern Reuse in 3D Human Pose Estimation",
    "url": "https://www.alphaxiv.org/abs/2510.19475v1",
    "arxiv_id": "2510.19475v1",
    "authors": "Zhuoyang Xie, Yibo Zhao, Hui Huang, Riwei Wang, Zan Gao",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 11:12:07",
    "ori_summary": "Monocular 3D human pose estimation remains a fundamentally ill-posed inverse problem due to the inherent depth ambiguity in 2D-to-3D lifting. While contemporary video-based methods leverage temporal context to enhance spatial reasoning, they operate under a critical paradigm limitation: processing each sequence in isolation, thereby failing to exploit the strong structural regularities and repetitive motion patterns that pervade human movement across sequences. This work introduces the Pattern Reuse Graph Convolutional Network (PRGCN), a novel framework that formalizes pose estimation as a problem of pattern retrieval and adaptation. At its core, PRGCN features a graph memory bank that learns and stores a compact set of pose prototypes, encoded as relational graphs, which are dynamically retrieved via an attention mechanism to provide structured priors. These priors are adaptively fused with hard-coded anatomical constraints through a memory-driven graph convolution, ensuring geometrical plausibility. To underpin this retrieval process with robust spatiotemporal features, we design a dual-stream hybrid architecture that synergistically combines the linear-complexity, local temporal modeling of Mamba-based state-space models with the global relational capacity of self-attention. Extensive evaluations on Human3.6M and MPI-INF-3DHP benchmarks demonstrate that PRGCN establishes a new state-of-the-art, achieving an MPJPE of 37.1mm and 13.4mm, respectively, while exhibiting enhanced cross-domain generalization capability. Our work posits that the long-overlooked mechanism of cross-sequence pattern reuse is pivotal to advancing the field, shifting the paradigm from per-sequence optimization towards cumulative knowledge learning.",
    "summary": "",
    "translation": "PRGCN：一种用于3D人体姿态估计中跨序列模式重用的图记忆网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D人体姿态估计这一计算机视觉任务，属于纯粹的视觉领域研究。虽然涉及图神经网络和模式重用技术，但论文明确限定在3D视觉应用场景，与推荐系统、搜索或广告领域没有任何直接或潜在的关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19472v1": {
    "title": "Predicting before Reconstruction: A generative prior framework for MRI acceleration",
    "url": "https://www.alphaxiv.org/abs/2510.19472v1",
    "arxiv_id": "2510.19472v1",
    "authors": "Juhyung Park, Rokgi Hong, Roh-Eul Yoo, Jaehyeon Koo, Se Young Chun, Seung Hong Choi, Jongho Lee",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 11:07:57",
    "ori_summary": "Recent advancements in artificial intelligence have created transformative capabilities in image synthesis and generation, enabling diverse research fields to innovate at revolutionary speed and spectrum. In this study, we leverage this generative power to introduce a new paradigm for accelerating Magnetic Resonance Imaging (MRI), introducing a shift from image reconstruction to proactive predictive imaging. Despite being a cornerstone of modern patient care, MRI's lengthy acquisition times limit clinical throughput. Our novel framework addresses this challenge by first predicting a target contrast image, which then serves as a data-driven prior for reconstructing highly under-sampled data. This informative prior is predicted by a generative model conditioned on diverse data sources, such as other contrast images, previously scanned images, acquisition parameters, patient information. We demonstrate this approach with two key applications: (1) reconstructing FLAIR images using predictions from T1w and/or T2w scans, and (2) reconstructing T1w images using predictions from previously acquired T1w scans. The framework was evaluated on internal and multiple public datasets (total 14,921 scans; 1,051,904 slices), including multi-channel k-space data, for a range of high acceleration factors (x4, x8 and x12). The results demonstrate that our prediction-prior reconstruction method significantly outperforms other approaches, including those with alternative or no prior information. Through this framework we introduce a fundamental shift from image reconstruction towards a new paradigm of predictive imaging.",
    "summary": "",
    "translation": "重建前预测：用于MRI加速的生成先验框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（MRI）加速技术，属于医疗领域的特定应用，与推荐系统、搜索或广告领域完全无关。论文内容涉及图像重建和生成模型在医学影像中的应用，不在当前关注的任何技术范畴内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19465v1": {
    "title": "PCP-GAN: Property-Constrained Pore-scale image reconstruction via conditional Generative Adversarial Networks",
    "url": "https://www.alphaxiv.org/abs/2510.19465v1",
    "arxiv_id": "2510.19465v1",
    "authors": "Ali Sadeghkhani, Brandon Bennett, Masoud Babaei, Arash Rabbani",
    "categories": "cs.CV, cs.LG, physics.geo-ph",
    "pub_date": "2025-10-22 10:54:51",
    "ori_summary": "Obtaining truly representative pore-scale images that match bulk formation properties remains a fundamental challenge in subsurface characterization, as natural spatial heterogeneity causes extracted sub-images to deviate significantly from core-measured values. This challenge is compounded by data scarcity, where physical samples are only available at sparse well locations. This study presents a multi-conditional Generative Adversarial Network (cGAN) framework that generates representative pore-scale images with precisely controlled properties, addressing both the representativeness challenge and data availability constraints. The framework was trained on thin section samples from four depths (1879.50-1943.50 m) of a carbonate formation, simultaneously conditioning on porosity values and depth parameters within a single unified model. This approach captures both universal pore network principles and depth-specific geological characteristics, from grainstone fabrics with interparticle-intercrystalline porosity to crystalline textures with anhydrite inclusions. The model achieved exceptional porosity control (R^2=0.95) across all formations with mean absolute errors of 0.0099-0.0197. Morphological validation confirmed preservation of critical pore network characteristics including average pore radius, specific surface area, and tortuosity, with statistical differences remaining within acceptable geological tolerances. Most significantly, generated images demonstrated superior representativeness with dual-constraint errors of 1.9-11.3% compared to 36.4-578% for randomly extracted real sub-images. This capability provides transformative tools for subsurface characterization, particularly valuable for carbon storage, geothermal energy, and groundwater management applications where knowing the representative morphology of the pore space is critical for implementing digital rock physics.",
    "summary": "",
    "translation": "PCP-GAN：基于条件生成对抗网络的属性约束孔隙尺度图像重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于地质/材料科学领域的孔隙尺度图像重建，使用条件GAN技术解决特定领域问题。这与推荐系统、搜索或广告的核心技术发展没有任何关联，也不涉及LLM、Transformer架构或异构数据统一建模等关键技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19463v1": {
    "title": "Exploring \"Many in Few\" and \"Few in Many\" Properties in Long-Tailed, Highly-Imbalanced IC Defect Classification",
    "url": "https://www.alphaxiv.org/abs/2510.19463v1",
    "arxiv_id": "2510.19463v1",
    "authors": "Hao-Chiang Shao, Chun-Hao Chang, Yu-Hsien Lin, Chia-Wen Lin, Shao-Yun Fang, Yan-Hsiu Liu",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-22 10:50:27",
    "ori_summary": "Despite significant advancements in deep classification techniques and in-lab automatic optical inspection models for long-tailed or highly imbalanced data, applying these approaches to real-world IC defect classification tasks remains challenging. This difficulty stems from two primary factors. First, real-world conditions, such as the high yield-rate requirements in the IC industry, result in data distributions that are far more skewed than those found in general public imbalanced datasets. Consequently, classifiers designed for open imbalanced datasets often fail to perform effectively in real-world scenarios. Second, real-world samples exhibit a mix of class-specific attributes and class-agnostic, domain-related features. This complexity adds significant difficulty to the classification process, particularly for highly imbalanced datasets. To address these challenges, this paper introduces the IC-Defect-14 dataset, a large, highly imbalanced IC defect image dataset sourced from AOI systems deployed in real-world IC production lines. This dataset is characterized by its unique \"intra-class clusters\" property, which presents two major challenges: large intra-class diversity and high inter-class similarity. These characteristics, rarely found simultaneously in existing public datasets, significantly degrade the performance of current state-of-the-art classifiers for highly imbalanced data. To tackle this challenge, we propose ReCAME-Net, which follows a multi-expert classifier framework and integrates a regional channel attention module, metric learning losses, a hard category mining strategy, and a knowledge distillation procedure. Extensive experimental evaluations demonstrate that ReCAME-Net outperforms previous state-of-the-art models on the IC-Defect-14 dataset while maintaining comparable performance and competitiveness on general public datasets.",
    "summary": "",
    "translation": "探索长尾、高度不平衡集成电路缺陷分类中的“多中少”与“少中多”特性",
    "relevance_score": 1,
    "reasoning": "该论文专注于集成电路缺陷分类，属于特定硬件领域的计算机视觉应用。虽然涉及长尾和高度不平衡数据问题，但这些概念在推荐系统或搜索中的通用性不足以建立直接关联。论文的核心应用场景（IC缺陷检测）与我的关注领域（推荐系统、搜索、广告）没有明确的联系或潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19455v1": {
    "title": "Automated Morphological Analysis of Neurons in Fluorescence Microscopy Using YOLOv8",
    "url": "https://www.alphaxiv.org/abs/2510.19455v1",
    "arxiv_id": "2510.19455v1",
    "authors": "Banan Alnemri, Arwa Basbrain",
    "categories": "eess.IV, cs.CV, q-bio.QM",
    "pub_date": "2025-10-22 10:35:08",
    "ori_summary": "Accurate segmentation and precise morphological analysis of neuronal cells in fluorescence microscopy images are crucial steps in neuroscience and biomedical imaging applications. However, this process is labor-intensive and time-consuming, requiring significant manual effort and expertise to ensure reliable outcomes. This work presents a pipeline for neuron instance segmentation and measurement based on a high-resolution dataset of stem-cell-derived neurons. The proposed method uses YOLOv8, trained on manually annotated microscopy images. The model achieved high segmentation accuracy, exceeding 97%. In addition, the pipeline utilized both ground truth and predicted masks to extract biologically significant features, including cell length, width, area, and grayscale intensity values. The overall accuracy of the extracted morphological measurements reached 75.32%, further supporting the effectiveness of the proposed approach. This integrated framework offers a valuable tool for automated analysis in cell imaging and neuroscience research, reducing the need for manual annotation and enabling scalable, precise quantification of neuron morphology.",
    "summary": "",
    "translation": "基于YOLOv8的荧光显微镜神经元形态自动分析",
    "relevance_score": 1,
    "reasoning": "该论文专注于生物医学领域的神经元形态分析，使用计算机视觉技术进行细胞识别。这属于明确的无关主题范畴，与推荐系统、搜索、广告或相关LLM技术没有任何关联。论文内容纯粹是医学/生物学应用，没有任何潜在的应用于RecSys/Search/Ads的可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19451v1": {
    "title": "Reasoning Like Experts: Leveraging Multimodal Large Language Models for Drawing-based Psychoanalysis",
    "url": "https://www.alphaxiv.org/abs/2510.19451v1",
    "arxiv_id": "2510.19451v1",
    "authors": "Xueqi Ma, Yanbei Jiang, Sarah Erfani, James Bailey, Weifeng Liu, Krista A. Ehinger, Jey Han Lau",
    "categories": "cs.CV, cs.MM",
    "pub_date": "2025-10-22 10:29:14",
    "ori_summary": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional performance across various objective multimodal perception tasks, yet their application to subjective, emotionally nuanced domains, such as psychological analysis, remains largely unexplored. In this paper, we introduce PICK, a multi-step framework designed for Psychoanalytical Image Comprehension through hierarchical analysis and Knowledge injection with MLLMs, specifically focusing on the House-Tree-Person (HTP) Test, a widely used psychological assessment in clinical practice. First, we decompose drawings containing multiple instances into semantically meaningful sub-drawings, constructing a hierarchical representation that captures spatial structure and content across three levels: single-object level, multi-object level, and whole level. Next, we analyze these sub-drawings at each level with a targeted focus, extracting psychological or emotional insights from their visual cues. We also introduce an HTP knowledge base and design a feature extraction module, trained with reinforcement learning, to generate a psychological profile for single-object level analysis. This profile captures both holistic stylistic features and dynamic object-specific features (such as those of the house, tree, or person), correlating them with psychological states. Finally, we integrate these multi-faceted information to produce a well-informed assessment that aligns with expert-level reasoning. Our approach bridges the gap between MLLMs and specialized expert domains, offering a structured and interpretable framework for understanding human mental states through visual expression. Experimental results demonstrate that the proposed PICK significantly enhances the capability of MLLMs in psychological analysis. It is further validated as a general framework through extensions to emotion understanding tasks.",
    "summary": "",
    "translation": "像专家一样推理：利用多模态大语言模型进行基于绘画的心理分析",
    "relevance_score": 1,
    "reasoning": "该论文专注于心理学领域的多模态应用，属于明确的无关主题（医学/生物学领域特定应用）。虽然涉及多模态LLM技术，但其应用场景（心理分析）与推荐系统、搜索或广告领域没有任何关联，也不具备在这些领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19432v1": {
    "title": "Multi-Camera Worker Tracking in Logistics Warehouse Considering Wide-Angle Distortion",
    "url": "https://www.alphaxiv.org/abs/2510.19432v1",
    "arxiv_id": "2510.19432v1",
    "authors": "Yuki Mori, Kazuma Kano, Yusuke Asai, Shin Katayama, Kenta Urano, Takuro Yonezawa, Nobuo Kawaguchi",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 10:00:40",
    "ori_summary": "With the spread of e-commerce, the logistics market is growing around the world. Therefore, improving the efficiency of warehouse operations is essential. To achieve this, various approaches have been explored, and among them, the use of digital twins is gaining attention. To make this approach possible, it is necessary to accurately collect the positions of workers in a warehouse and reflect them in a virtual space. However, a single camera has limitations in its field of view, therefore sensing with multiple cameras is necessary. In this study, we explored a method to track workers using 19 wide-angle cameras installed on the ceiling, looking down at the floor of the logistics warehouse. To understand the relationship between the camera coordinates and the actual positions in the warehouse, we performed alignment based on the floor surface. However, due to the characteristics of wide-angle cameras, significant distortion occurs at the edges of the image, particularly in the vertical direction. To address this, the detected worker positions from each camera were aligned based on foot positions, reducing the effects of image distortion, and enabling accurate position alignment across cameras. As a result, we confirmed an improvement of over 20% in tracking accuracy. Furthermore, we compared multiple methods for utilizing appearance features and validated the effectiveness of the proposed approach.",
    "summary": "",
    "translation": "考虑广角畸变的物流仓库多摄像头工人跟踪",
    "relevance_score": 1,
    "reasoning": "该论文专注于物流仓库中的多摄像头工人跟踪和计算机视觉技术，属于纯粹的视觉应用领域。虽然跟踪技术在某些场景下可能与用户行为分析相关，但该论文明确关注广角畸变校正和工人跟踪，没有显示出与推荐系统、搜索或广告的直接或间接关联，也不涉及LLM、Transformer架构或异构数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19430v1": {
    "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model",
    "url": "https://www.alphaxiv.org/abs/2510.19430v1",
    "arxiv_id": "2510.19430v1",
    "authors": "GigaBrain Team, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jie Li, Jiagang Zhu, Lv Feng, Peng Li, Qiuping Deng, Runqi Ouyang, Wenkang Qin, Xinze Chen, Xiaofeng Wang, Yang Wang, Yifan Li, Yilong Li, Yiran Ding, Yuan Xu, Yun Ye, Yukun Zhou, Zhehao Dong, Zhenan Wang, Zhichao Liu, Zheng Zhu",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-22 09:57:13",
    "ori_summary": "Training Vision-Language-Action (VLA) models for generalist robots typically requires large-scale real-world robot data, which is expensive and time-consuming to collect. The inefficiency of physical data collection severely limits the scalability, and generalization capacity of current VLA systems. To address this challenge, we introduce GigaBrain-0, a novel VLA foundation model empowered by world model-generated data (e.g., video generation, real2real transfer, human transfer, view transfer, sim2real transfer data). By leveraging world models to generate diverse data at scale, GigaBrain-0 significantly reduces reliance on real robot data while improving cross-task generalization. Our approach further improves policy robustness through RGBD input modeling and embodied Chain-of-Thought (CoT) supervision, enabling the model to reason about spatial geometry, object states, and long-horizon dependencies during task execution. This leads to substantial gains in real-world performance on dexterous, long-horizon, and mobile manipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves superior generalization across variations in appearances (e.g., textures, colors), object placements, and camera viewpoints. Additionally, we present GigaBrain-0-Small, an optimized lightweight variant designed to run efficiently on devices such as the NVIDIA Jetson AGX Orin.",
    "summary": "",
    "translation": "GigaBrain-0：一种世界模型驱动的视觉-语言-动作模型",
    "relevance_score": 3,
    "reasoning": "虽然该论文涉及视觉-语言-动作多模态建模，与VLM类比异构数据的理念有一定关联，但其核心焦点是世界模型和动作规划，这主要面向具身智能和机器人控制领域。在搜索、推荐或广告系统中，世界模型和动作规划的直接应用场景有限，相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19418v1": {
    "title": "From See to Shield: ML-Assisted Fine-Grained Access Control for Visual Data",
    "url": "https://www.alphaxiv.org/abs/2510.19418v1",
    "arxiv_id": "2510.19418v1",
    "authors": "Mete Harun Akcay, Buse Gul Atli, Siddharth Prakash Rao, Alexandros Bakas",
    "categories": "cs.CR, cs.CV, cs.LG",
    "pub_date": "2025-10-22 09:41:31",
    "ori_summary": "As the volume of stored data continues to grow, identifying and protecting sensitive information within large repositories becomes increasingly challenging, especially when shared with multiple users with different roles and permissions. This work presents a system architecture for trusted data sharing with policy-driven access control, enabling selective protection of sensitive regions while maintaining scalability. The proposed architecture integrates four core modules that combine automated detection of sensitive regions, post-correction, key management, and access control. Sensitive regions are secured using a hybrid scheme that employs symmetric encryption for efficiency and Attribute-Based Encryption for policy enforcement. The system supports efficient key distribution and isolates key storage to strengthen overall security. To demonstrate its applicability, we evaluate the system on visual datasets, where Privacy-Sensitive Objects in images are automatically detected, reassessed, and selectively encrypted prior to sharing in a data repository. Experimental results show that our system provides effective PSO detection, increases macro-averaged F1 score (5%) and mean Average Precision (10%), and maintains an average policy-enforced decryption time of less than 1 second per image. These results demonstrate the effectiveness, efficiency and scalability of our proposed solution for fine-grained access control.",
    "summary": "",
    "translation": "从可见到防护：基于机器学习的视觉数据细粒度访问控制",
    "relevance_score": 1,
    "reasoning": "该论文主要关注视觉数据的访问控制和安全防护，这属于隐私和安全领域，明确在无关主题列表中。虽然涉及机器学习技术，但其核心应用场景（视觉数据访问控制）与推荐系统、搜索或广告的排名和建模问题没有直接关联，也没有展示在相关领域应用的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19400v1": {
    "title": "Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes",
    "url": "https://www.alphaxiv.org/abs/2510.19400v1",
    "arxiv_id": "2510.19400v1",
    "authors": "Zhiyuan Feng, Zhaolu Kang, Qijie Wang, Zhiying Du, Jiongrui Yan, Shubin Shi, Chengbo Yuan, Huizhi Liang, Yu Deng, Qixiu Li, Rushuai Yang, Arctanx An, Leqi Zheng, Weijie Wang, Shawn Chen, Sicheng Xu, Yaobo Liang, Jiaolong Yang, Baining Guo",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 09:20:09",
    "ori_summary": "Vision-language models (VLMs) are essential to Embodied AI, enabling robots to perceive, reason, and act in complex environments. They also serve as the foundation for the recent Vision-Language-Action (VLA) models. Yet most evaluations of VLMs focus on single-view settings, leaving their ability to integrate multi-view information underexplored. At the same time, multi-camera setups are increasingly standard in robotic platforms, as they provide complementary perspectives to mitigate occlusion and depth ambiguity. Whether VLMs can effectively leverage such multi-view inputs for robotic reasoning therefore remains an open question. To bridge this gap, we introduce MV-RoboBench, a benchmark specifically designed to evaluate the multi-view spatial reasoning capabilities of VLMs in robotic manipulation. MV-RoboBench consists of 1.7k manually curated QA items across eight subtasks, divided into two primary categories: spatial understanding and robotic execution. We evaluate a diverse set of existing VLMs, including both open-source and closed-source models, along with enhanced versions incorporating CoT-inspired techniques. The results show that state-of-the-art models remain far below human performance, underscoring the substantial challenges VLMs face in multi-view robotic perception. Additionally, our analysis uncovers two key findings: (i) spatial intelligence and robotic task execution are positively correlated in multi-view robotic scenarios; and (ii) strong performance on existing general-purpose single-view spatial understanding benchmarks does not reliably translate to success in the robotic spatial tasks assessed by our benchmark. We release MV-RoboBench as an open resource to foster progress in spatially grounded VLMs and VLAs, providing not only data but also a standardized evaluation protocol for multi-view embodied reasoning.",
    "summary": "",
    "translation": "跨视图观察：机器人场景中视觉语言模型空间推理能力基准测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注机器人场景中的视觉语言模型空间推理基准测试，属于纯粹的视觉和机器人应用领域。虽然涉及视觉语言模型技术，但缺乏与推荐系统、搜索或广告领域的明确关联，且机器人场景的应用与当前关注的商业应用场景相距甚远。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19371v1": {
    "title": "AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields",
    "url": "https://www.alphaxiv.org/abs/2510.19371v1",
    "arxiv_id": "2510.19371v1",
    "authors": "Woo Jae Kim, Kyu Beom Han, Yoonki Cho, Youngju Na, Junsik Jung, Sooel Son, Sung-eui Yoon",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 08:45:42",
    "ori_summary": "As Neural Radiance Fields (NeRFs) have emerged as a powerful tool for 3D scene representation and novel view synthesis, protecting their intellectual property (IP) from unauthorized use is becoming increasingly crucial. In this work, we aim to protect the IP of NeRFs by injecting adversarial perturbations that disrupt their unauthorized applications. However, perturbing the 3D geometry of NeRFs can easily deform the underlying scene structure and thus substantially degrade the rendering quality, which has led existing attempts to avoid geometric perturbations or restrict them to explicit spaces like meshes. To overcome this limitation, we introduce a learnable sensitivity to quantify the spatially varying impact of geometric perturbations on rendering quality. Building upon this, we propose AegisRF, a novel framework that consists of a Perturbation Field, which injects adversarial perturbations into the pre-rendering outputs (color and volume density) of NeRF models to fool an unauthorized downstream target model, and a Sensitivity Field, which learns the sensitivity to adaptively constrain geometric perturbations, preserving rendering quality while disrupting unauthorized use. Our experimental evaluations demonstrate the generalized applicability of AegisRF across diverse downstream tasks and modalities, including multi-view image classification and voxel-based 3D localization, while maintaining high visual fidelity. Codes are available at https://github.com/wkim97/AegisRF.",
    "summary": "",
    "translation": "AegisRF：基于敏感度引导的对抗性扰动用于保护神经辐射场的知识产权",
    "relevance_score": 1,
    "reasoning": "该论文专注于神经辐射场（NeRF）的知识产权保护，属于计算机视觉领域的特定应用。虽然涉及对抗性扰动技术，但主要关注安全保护而非推荐系统、搜索或广告的核心技术。论文内容与我的关注领域（推荐系统、搜索、广告、LLM技术、Transformer架构等）没有直接关联，属于明确的无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19353v1": {
    "title": "DARE: A Deformable Adaptive Regularization Estimator for Learning-Based Medical Image Registration",
    "url": "https://www.alphaxiv.org/abs/2510.19353v1",
    "arxiv_id": "2510.19353v1",
    "authors": "Ahsan Raza Siyal, Markus Haltmeier, Ruth Steiger, Malik Galijasevic, Elke Ruth Gizewski, Astrid Ellen Grams",
    "categories": "cs.CV, cs.NA, math.NA",
    "pub_date": "2025-10-22 08:21:05",
    "ori_summary": "Deformable medical image registration is a fundamental task in medical image analysis. While deep learning-based methods have demonstrated superior accuracy and computational efficiency compared to traditional techniques, they often overlook the critical role of regularization in ensuring robustness and anatomical plausibility. We propose DARE (Deformable Adaptive Regularization Estimator), a novel registration framework that dynamically adjusts elastic regularization based on the gradient norm of the deformation field. Our approach integrates strain and shear energy terms, which are adaptively modulated to balance stability and flexibility. To ensure physically realistic transformations, DARE includes a folding-prevention mechanism that penalizes regions with negative deformation Jacobian. This strategy mitigates non-physical artifacts such as folding, avoids over-smoothing, and improves both registration accuracy and anatomical plausibility",
    "summary": "",
    "translation": "DARE：一种用于基于学习的医学图像配准的可变形自适应正则化估计器",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于医学图像配准，这是一个与医疗领域高度相关的特定应用。虽然提到了基于学习的方法，但该技术没有明显的潜在应用可以转移到推荐系统、搜索或广告领域。医学图像处理与我的关注焦点中的任何技术方向都没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19351v1": {
    "title": "Learning To Defer To A Population With Limited Demonstrations",
    "url": "https://www.alphaxiv.org/abs/2510.19351v1",
    "arxiv_id": "2510.19351v1",
    "authors": "Nilesh Ramgolam, Gustavo Carneiro, Hsiang-Ting, Chen",
    "categories": "cs.HC, cs.AI, cs.CV",
    "pub_date": "2025-10-22 08:18:02",
    "ori_summary": "This paper addresses the critical data scarcity that hinders the practical deployment of learning to defer (L2D) systems to the population. We introduce a context-aware, semi-supervised framework that uses meta-learning to generate expert-specific embeddings from only a few demonstrations. We demonstrate the efficacy of a dual-purpose mechanism, where these embeddings are used first to generate a large corpus of pseudo-labels for training, and subsequently to enable on-the-fly adaptation to new experts at test-time. The experiment results on three different datasets confirm that a model trained on these synthetic labels rapidly approaches oracle-level performance, validating the data efficiency of our approach. By resolving a key training bottleneck, this work makes adaptive L2D systems more practical and scalable, paving the way for human-AI collaboration in real-world environments. To facilitate reproducibility and address implementation details not covered in the main text, we provide our source code and training configurations at https://github.com/nil123532/learning-to-defer-to-a-population-with-limited-demonstrations.",
    "summary": "",
    "translation": "基于有限演示样本学习向群体决策进行延迟",
    "relevance_score": 2,
    "reasoning": "该论文主要研究延迟决策机制，虽然决策系统在推荐和搜索中有应用，但论文标题未明确涉及推荐系统、搜索或广告的核心技术。对于使能技术，延迟决策可能应用于处理不确定性场景，但潜在应用不够直接和明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19336v1": {
    "title": "DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents",
    "url": "https://www.alphaxiv.org/abs/2510.19336v1",
    "arxiv_id": "2510.19336v1",
    "authors": "Kai Shi, Jun Yang, Ni Yang, Binqiang Pan, Qingsong Xie, Chao Zhang, Zhenyu Yang, Tianhuang Su, Haonan Lu",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 07:57:59",
    "ori_summary": "Mobile Phone Agents (MPAs) have emerged as a promising research direction due to their broad applicability across diverse scenarios. While Multimodal Large Language Models (MLLMs) serve as the foundation for MPAs, their effectiveness in handling multiple mobile phone tasks simultaneously remains limited. Although multitask supervised fine-tuning (SFT) is widely adopted for multitask learning, existing approaches struggle to determine optimal training data compositions for peak performance. To address this challenge, we propose DaMo (Data Mixture Optimizer) - a novel solution employing a trainable network that predicts optimal data mixtures by forecasting downstream task performance for any given dataset ratio. To support comprehensive evaluation, we introduce PhoneAgentBench, the first specialized benchmark to evaluate MLLMs on multimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse real-world industrial mobile application scenarios. Demonstrating strong predictive capability (R^2=0.81) in small-scale pilot experiments, DaMo efficiently extrapolates optimal data mixing configurations. Our results show DaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to alternative methods. Furthermore, extensive experiments across established benchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench reveal DaMo's superior generalization, outperforming other approaches by 2.57% in terms of average score. When used solely for MLLM optimization on the BFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably, DaMo maintains robust scalability, preserving its effectiveness when applied to other model architectures. The code and dataset are available at https://github.com/OPPO-Mente-Lab/DaMo.git",
    "summary": "",
    "translation": "DaMo：面向移动手机智能体的多模态大语言模型微调中的数据混合优化器",
    "relevance_score": 3,
    "reasoning": "该论文主要关注多模态LLM的微调优化和移动智能体应用，属于LLM技术使能范畴。虽然数据混合优化技术可能提升模型效率，但论文聚焦于移动设备智能体这一特定应用场景，与推荐系统、搜索或广告的核心技术关联度较低，潜在应用场景不够明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19333v1": {
    "title": "A Training-Free Framework for Open-Vocabulary Image Segmentation and Recognition with EfficientNet and CLIP",
    "url": "https://www.alphaxiv.org/abs/2510.19333v1",
    "arxiv_id": "2510.19333v1",
    "authors": "Ying Dai, Wei Yu Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 07:54:18",
    "ori_summary": "This paper presents a novel training-free framework for open-vocabulary image segmentation and object recognition (OVSR), which leverages EfficientNetB0, a convolutional neural network, for unsupervised segmentation and CLIP, a vision-language model, for open-vocabulary object recognition. The proposed framework adopts a two stage pipeline: unsupervised image segmentation followed by segment-level recognition via vision-language alignment. In the first stage, pixel-wise features extracted from EfficientNetB0 are decomposed using singular value decomposition to obtain latent representations, which are then clustered using hierarchical clustering to segment semantically meaningful regions. The number of clusters is adaptively determined by the distribution of singular values. In the second stage, the segmented regions are localized and encoded into image embeddings using the Vision Transformer backbone of CLIP. Text embeddings are precomputed using CLIP's text encoder from category-specific prompts, including a generic something else prompt to support open set recognition. The image and text embeddings are concatenated and projected into a shared latent feature space via SVD to enhance cross-modal alignment. Recognition is performed by computing the softmax over the similarities between the projected image and text embeddings. The proposed method is evaluated on standard benchmarks, including COCO, ADE20K, and PASCAL VOC, achieving state-of-the-art performance in terms of Hungarian mIoU, precision, recall, and F1-score. These results demonstrate the effectiveness, flexibility, and generalizability of the proposed framework.",
    "summary": "",
    "translation": "基于EfficientNet和CLIP的无训练开放词汇图像分割与识别框架",
    "relevance_score": 3,
    "reasoning": "该论文主要关注计算机视觉领域的开放词汇图像分割与识别，属于纯粹的视觉技术范畴。虽然CLIP技术具有多模态潜力，但论文标题明确聚焦于图像处理任务，没有显示出与推荐系统、搜索或广告的直接关联或潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19332v1": {
    "title": "BrainMCLIP: Brain Image Decoding with Multi-Layer feature Fusion of CLIP",
    "url": "https://www.alphaxiv.org/abs/2510.19332v1",
    "arxiv_id": "2510.19332v1",
    "authors": "Tian Xia, Zihan Ma, Xinlong Wang, Qing Liu, Xiaowei He, Tianming Liu, Yudan Ren",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 07:51:52",
    "ori_summary": "Decoding images from fMRI often involves mapping brain activity to CLIP's final semantic layer. To capture finer visual details, many approaches add a parameter-intensive VAE-based pipeline. However, these approaches overlook rich object information within CLIP's intermediate layers and contradicts the brain's functionally hierarchical. We introduce BrainMCLIP, which pioneers a parameter-efficient, multi-layer fusion approach guided by human visual system's functional hierarchy, eliminating the need for such a separate VAE pathway. BrainMCLIP aligns fMRI signals from functionally distinct visual areas (low-/high-level) to corresponding intermediate and final CLIP layers, respecting functional hierarchy. We further introduce a Cross-Reconstruction strategy and a novel multi-granularity loss. Results show BrainMCLIP achieves highly competitive performance, particularly excelling on high-level semantic metrics where it matches or surpasses SOTA(state-of-the-art) methods, including those using VAE pipelines. Crucially, it achieves this with substantially fewer parameters, demonstrating a reduction of 71.7\\%(Table.\\ref{tab:compare_clip_vae}) compared to top VAE-based SOTA methods, by avoiding the VAE pathway. By leveraging intermediate CLIP features, it effectively captures visual details often missed by CLIP-only approaches, striking a compelling balance between semantic accuracy and detail fidelity without requiring a separate VAE pipeline.",
    "summary": "",
    "translation": "BrainMCLIP：基于CLIP多层特征融合的脑图像解码",
    "relevance_score": 1,
    "reasoning": "该论文专注于脑图像解码这一生物医学领域应用，虽然使用了CLIP模型，但其核心应用场景是医学图像分析而非推荐系统、搜索或广告领域。论文内容涉及脑科学和医学成像，属于明确的无关主题范畴，与当前关注的推荐系统、搜索广告技术发展无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19330v1": {
    "title": "Exploring Scale Shift in Crowd Localization under the Context of Domain Generalization",
    "url": "https://www.alphaxiv.org/abs/2510.19330v1",
    "arxiv_id": "2510.19330v1",
    "authors": "Juncheng Wang, Lei Shang, Ziqi Liu, Wang Lu, Xixu Hu, Zhe Hu, Jindong Wang, Shujun Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 07:45:03",
    "ori_summary": "Crowd localization plays a crucial role in visual scene understanding towards predicting each pedestrian location in a crowd, thus being applicable to various downstream tasks. However, existing approaches suffer from significant performance degradation due to discrepancies in head scale distributions (scale shift) between training and testing data, a challenge known as domain generalization (DG). This paper aims to comprehend the nature of scale shift within the context of domain generalization for crowd localization models. To this end, we address four critical questions: (i) How does scale shift influence crowd localization in a DG scenario? (ii) How can we quantify this influence? (iii) What causes this influence? (iv) How to mitigate the influence? Initially, we conduct a systematic examination of how crowd localization performance varies with different levels of scale shift. Then, we establish a benchmark, ScaleBench, and reproduce 20 advanced DG algorithms to quantify the influence. Through extensive experiments, we demonstrate the limitations of existing algorithms and underscore the importance and complexity of scale shift, a topic that remains insufficiently explored. To deepen our understanding, we provide a rigorous theoretical analysis on scale shift. Building on these insights, we further propose an effective algorithm called Causal Feature Decomposition and Anisotropic Processing (Catto) to mitigate the influence of scale shift in DG settings. Later, we also provide extensive analytical experiments, revealing four significant insights for future research. Our results emphasize the importance of this novel and applicable research direction, which we term Scale Shift Domain Generalization.",
    "summary": "",
    "translation": "在领域泛化背景下探索人群定位中的尺度偏移问题",
    "relevance_score": 1,
    "reasoning": "该论文关注计算机视觉中的人群定位和领域泛化问题，属于纯粹的视觉任务。标题内容表明其研究重点在于视觉场景中的人群检测和定位，与推荐系统、搜索或广告中的排序、检索、用户建模等核心任务没有直接关联。该技术缺乏在RecSys/Search/Ads领域的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19329v1": {
    "title": "Seabed-Net: A multi-task network for joint bathymetry estimation and seabed classification from remote sensing imagery in shallow waters",
    "url": "https://www.alphaxiv.org/abs/2510.19329v1",
    "arxiv_id": "2510.19329v1",
    "authors": "Panagiotis Agrafiotis, Begüm Demir",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-22 07:43:03",
    "ori_summary": "Accurate, detailed, and regularly updated bathymetry, coupled with complex semantic content, is essential for under-mapped shallow-water environments facing increasing climatological and anthropogenic pressures. However, existing approaches that derive either depth or seabed classes from remote sensing imagery treat these tasks in isolation, forfeiting the mutual benefits of their interaction and hindering the broader adoption of deep learning methods. To address these limitations, we introduce Seabed-Net, a unified multi-task framework that simultaneously predicts bathymetry and pixel-based seabed classification from remote sensing imagery of various resolutions. Seabed-Net employs dual-branch encoders for bathymetry estimation and pixel-based seabed classification, integrates cross-task features via an Attention Feature Fusion module and a windowed Swin-Transformer fusion block, and balances objectives through dynamic task uncertainty weighting. In extensive evaluations at two heterogeneous coastal sites, it consistently outperforms traditional empirical models and traditional machine learning regression methods, achieving up to 75\\% lower RMSE. It also reduces bathymetric RMSE by 10-30\\% compared to state-of-the-art single-task and multi-task baselines and improves seabed classification accuracy up to 8\\%. Qualitative analyses further demonstrate enhanced spatial consistency, sharper habitat boundaries, and corrected depth biases in low-contrast regions. These results confirm that jointly modeling depth with both substrate and seabed habitats yields synergistic gains, offering a robust, open solution for integrated shallow-water mapping. Code and pretrained weights are available at https://github.com/pagraf/Seabed-Net.",
    "summary": "",
    "translation": "Seabed-Net：一种用于从浅水区遥感影像中联合进行水深估计和海床分类的多任务网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于遥感图像处理中的海底地形估计和分类任务，属于纯粹的计算机视觉应用领域。虽然采用了多任务网络架构，但其应用场景（海洋遥感、水深测量）与推荐系统、搜索或广告领域没有任何直接或间接的关联，也不涉及LLM、Transformer技术或异构数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19321v1": {
    "title": "Online Handwritten Signature Verification Based on Temporal-Spatial Graph Attention Transformer",
    "url": "https://www.alphaxiv.org/abs/2510.19321v1",
    "arxiv_id": "2510.19321v1",
    "authors": "Hai-jie Yuan, Heng Zhang, Fei Yin",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-22 07:32:55",
    "ori_summary": "Handwritten signature verification is a crucial aspect of identity authentication, with applications in various domains such as finance and e-commerce. However, achieving high accuracy in signature verification remains challenging due to intra-user variability and the risk of forgery. This paper introduces a novel approach for dynamic signature verification: the Temporal-Spatial Graph Attention Transformer (TS-GATR). TS-GATR combines the Graph Attention Network (GAT) and the Gated Recurrent Unit (GRU) to model both spatial and temporal dependencies in signature data. TS-GATR enhances verification performance by representing signatures as graphs, where each node captures dynamic features (e.g. position, velocity, pressure), and by using attention mechanisms to model their complex relationships. The proposed method further employs a Dual-Graph Attention Transformer (DGATR) module, which utilizes k-step and k-nearest neighbor adjacency graphs to model local and global spatial features, respectively. To capture long-term temporal dependencies, the model integrates GRU, thereby enhancing its ability to learn dynamic features during signature verification. Comprehensive experiments conducted on benchmark datasets such as MSDS and DeepSignDB show that TS-GATR surpasses current state-of-the-art approaches, consistently achieving lower Equal Error Rates (EER) across various scenarios.",
    "summary": "",
    "translation": "基于时空图注意力Transformer的在线手写签名验证",
    "relevance_score": 1,
    "reasoning": "该论文专注于生物特征安全领域的签名验证技术，属于身份认证和生物识别范畴。虽然使用了Transformer架构，但其应用场景（签名验证）与推荐系统、搜索或广告领域没有直接关联，也不涉及用户行为建模、内容理解或排序等核心任务。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19307v1": {
    "title": "Unified Reinforcement and Imitation Learning for Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.19307v1",
    "arxiv_id": "2510.19307v1",
    "authors": "Byung-Kwan Lee, Ryo Hachiuma, Yong Man Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 07:12:14",
    "ori_summary": "Vision-Language Models (VLMs) have achieved remarkable progress, yet their large scale often renders them impractical for resource-constrained environments. This paper introduces Unified Reinforcement and Imitation Learning (RIL), a novel and efficient training algorithm designed to create powerful, lightweight VLMs. RIL distinctively combines the strengths of reinforcement learning with adversarial imitation learning. This enables smaller student VLMs not only to mimic the sophisticated text generation of large teacher models but also to systematically improve their generative capabilities through reinforcement signals. Key to our imitation framework is an LLM-based discriminator that adeptly distinguishes between student and teacher outputs, complemented by guidance from multiple large teacher VLMs to ensure diverse learning. This unified learning strategy, leveraging both reinforcement and imitation, empowers student models to achieve significant performance gains, making them competitive with leading closed-source VLMs. Extensive experiments on diverse vision-language benchmarks demonstrate that RIL significantly narrows the performance gap with state-of-the-art open- and closed-source VLMs and, in several instances, surpasses them.",
    "summary": "",
    "translation": "面向视觉语言模型的统一强化与模仿学习",
    "relevance_score": 3,
    "reasoning": "虽然论文涉及视觉语言模型和强化学习，但主要聚焦于VLM的训练方法改进，而非直接应用于推荐系统、搜索或广告领域。强化学习与模仿学习的统一方法可能为多模态推荐提供潜在启发，但缺乏明确的RecSys/Search/Ads应用场景说明，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19305v1": {
    "title": "FrogDeepSDM: Improving Frog Counting and Occurrence Prediction Using Multimodal Data and Pseudo-Absence Imputation",
    "url": "https://www.alphaxiv.org/abs/2510.19305v1",
    "arxiv_id": "2510.19305v1",
    "authors": "Chirag Padubidri, Pranesh Velmurugan, Andreas Lanitis, Andreas Kamilaris",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-22 07:09:36",
    "ori_summary": "Monitoring species distribution is vital for conservation efforts, enabling the assessment of environmental impacts and the development of effective preservation strategies. Traditional data collection methods, including citizen science, offer valuable insights but remain limited in coverage and completeness. Species Distribution Modelling (SDM) helps address these gaps by using occurrence data and environmental variables to predict species presence across large regions. In this study, we enhance SDM accuracy for frogs (Anura) by applying deep learning and data imputation techniques using data from the \"EY - 2022 Biodiversity Challenge.\" Our experiments show that data balancing significantly improved model performance, reducing the Mean Absolute Error (MAE) from 189 to 29 in frog counting tasks. Feature selection identified key environmental factors influencing occurrence, optimizing inputs while maintaining predictive accuracy. The multimodal ensemble model, integrating land cover, NDVI, and other environmental inputs, outperformed individual models and showed robust generalization across unseen regions. The fusion of image and tabular data improved both frog counting and habitat classification, achieving 84.9% accuracy with an AUC of 0.90. This study highlights the potential of multimodal learning and data preprocessing techniques such as balancing and imputation to improve predictive ecological modeling when data are sparse or incomplete, contributing to more precise and scalable biodiversity monitoring.",
    "summary": "",
    "translation": "FrogDeepSDM：利用多模态数据和伪缺失值插补改进青蛙计数与出现预测",
    "relevance_score": 1,
    "reasoning": "该论文专注于生物多样性监测和生态学中的青蛙计数预测，属于明确的生物学领域应用。虽然提到了多模态数据和预测建模，但其核心应用场景（青蛙监测）与推荐系统、搜索或广告领域完全无关，且不涉及任何LLM或Transformer技术在这些领域的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19292v1": {
    "title": "Vision-Based Mistake Analysis in Procedural Activities: A Review of Advances and Challenges",
    "url": "https://www.alphaxiv.org/abs/2510.19292v1",
    "arxiv_id": "2510.19292v1",
    "authors": "Konstantinos Bacharidis, Antonis A. Argyros",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 06:48:31",
    "ori_summary": "Mistake analysis in procedural activities is a critical area of research with applications spanning industrial automation, physical rehabilitation, education and human-robot collaboration. This paper reviews vision-based methods for detecting and predicting mistakes in structured tasks, focusing on procedural and executional errors. By leveraging advancements in computer vision, including action recognition, anticipation and activity understanding, vision-based systems can identify deviations in task execution, such as incorrect sequencing, use of improper techniques, or timing errors. We explore the challenges posed by intra-class variability, viewpoint differences and compositional activity structures, which complicate mistake detection. Additionally, we provide a comprehensive overview of existing datasets, evaluation metrics and state-of-the-art methods, categorizing approaches based on their use of procedural structure, supervision levels and learning strategies. Open challenges, such as distinguishing permissible variations from true mistakes and modeling error propagation are discussed alongside future directions, including neuro-symbolic reasoning and counterfactual state modeling. This work aims to establish a unified perspective on vision-based mistake analysis in procedural activities, highlighting its potential to enhance safety, efficiency and task performance across diverse domains.",
    "summary": "",
    "translation": "基于视觉的程序性活动错误分析：进展与挑战综述",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的程序性活动错误分析，属于纯粹的视觉研究范畴。虽然标题提到'程序性活动'，但核心是视觉分析技术，没有明确涉及推荐系统、搜索或广告领域的应用潜力，也不涉及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19282v1": {
    "title": "Enhancing Early Alzheimer Disease Detection through Big Data and Ensemble Few-Shot Learning",
    "url": "https://www.alphaxiv.org/abs/2510.19282v1",
    "arxiv_id": "2510.19282v1",
    "authors": "Safa Ben Atitallah, Maha Driss, Wadii Boulila, Anis Koubaa",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-22 06:35:03",
    "ori_summary": "Alzheimer disease is a severe brain disorder that causes harm in various brain areas and leads to memory damage. The limited availability of labeled medical data poses a significant challenge for accurate Alzheimer disease detection. There is a critical need for effective methods to improve the accuracy of Alzheimer disease detection, considering the scarcity of labeled data, the complexity of the disease, and the constraints related to data privacy. To address this challenge, our study leverages the power of big data in the form of pre-trained Convolutional Neural Networks (CNNs) within the framework of Few-Shot Learning (FSL) and ensemble learning. We propose an ensemble approach based on a Prototypical Network (ProtoNet), a powerful method in FSL, integrating various pre-trained CNNs as encoders. This integration enhances the richness of features extracted from medical images. Our approach also includes a combination of class-aware loss and entropy loss to ensure a more precise classification of Alzheimer disease progression levels. The effectiveness of our method was evaluated using two datasets, the Kaggle Alzheimer dataset and the ADNI dataset, achieving an accuracy of 99.72% and 99.86%, respectively. The comparison of our results with relevant state-of-the-art studies demonstrated that our approach achieved superior accuracy and highlighted its validity and potential for real-world applications in early Alzheimer disease detection.",
    "summary": "",
    "translation": "通过大数据与集成少样本学习增强早期阿尔茨海默病检测",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于医学领域的阿尔茨海默病检测，属于明确的无关主题。虽然涉及大数据和少样本学习技术，但核心应用场景是医疗诊断，与推荐系统、搜索或广告领域无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19278v1": {
    "title": "D2D: Detector-to-Differentiable Critic for Improved Numeracy in Text-to-Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.19278v1",
    "arxiv_id": "2510.19278v1",
    "authors": "Nobline Yoo, Olga Russakovsky, Ye Zhu",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 06:27:05",
    "ori_summary": "Text-to-image (T2I) diffusion models have achieved strong performance in semantic alignment, yet they still struggle with generating the correct number of objects specified in prompts. Existing approaches typically incorporate auxiliary counting networks as external critics to enhance numeracy. However, since these critics must provide gradient guidance during generation, they are restricted to regression-based models that are inherently differentiable, thus excluding detector-based models with superior counting ability, whose count-via-enumeration nature is non-differentiable. To overcome this limitation, we propose Detector-to-Differentiable (D2D), a novel framework that transforms non-differentiable detection models into differentiable critics, thereby leveraging their superior counting ability to guide numeracy generation. Specifically, we design custom activation functions to convert detector logits into soft binary indicators, which are then used to optimize the noise prior at inference time with pre-trained T2I models. Our extensive experiments on SDXL-Turbo, SD-Turbo, and Pixart-DMD across four benchmarks of varying complexity (low-density, high-density, and multi-object scenarios) demonstrate consistent and substantial improvements in object counting accuracy (e.g., boosting up to 13.7% on D2D-Small, a 400-prompt, low-density benchmark), with minimal degradation in overall image quality and computational overhead.",
    "summary": "",
    "translation": "D2D：用于改进文生图生成中数值能力的检测器到可微分评判器",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本到图像生成中的数值精度问题，属于纯粹的视觉内容生成领域。虽然提到了检测器和评判器架构，但核心应用场景是AIGC和图像生成，与推荐系统、搜索或广告的排名和建模需求没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19273v1": {
    "title": "MobiAct: Efficient MAV Action Recognition Using MobileNetV4 with Contrastive Learning and Knowledge Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.19273v1",
    "arxiv_id": "2510.19273v1",
    "authors": "Zhang Nengbo, Ho Hann Woei",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 06:18:53",
    "ori_summary": "Accurate and efficient recognition of Micro Air Vehicle (MAV) motion is essential for enabling real-time perception and coordination in autonomous aerial swarm. However, most existing approaches rely on large, computationally intensive models that are unsuitable for resource-limited MAV platforms, which results in a trade-off between recognition accuracy and inference speed. To address these challenges, this paper proposes a lightweight MAV action recognition framework, MobiAct, designed to achieve high accuracy with low computational cost. Specifically, MobiAct adopts MobileNetV4 as the backbone network and introduces a Stage-wise Orthogonal Knowledge Distillation (SOKD) strategy to effectively transfer MAV motion features from a teacher network (ResNet18) to a student network, thereby enhancing knowledge transfer efficiency. Furthermore, a parameter-free attention mechanism is integrated into the architecture to improve recognition accuracy without increasing model complexity. In addition, a hybrid loss training strategy is developed to combine multiple loss objectives, which ensures stable and robust optimization during training. Experimental results demonstrate that the proposed MobiAct achieves low-energy and low-computation MAV action recognition, while maintaining the fastest action decoding speed among compared methods. Across all three self-collected datasets, MobiAct achieves an average recognition accuracy of 92.12%, while consuming only 136.16 pJ of energy and processing recognition at a rate of 8.84 actions per second. Notably, MobiAct decodes actions up to 2 times faster than the leading method, with highly comparable recognition accuracy, highlighting its superior efficiency in MAV action recognition.",
    "summary": "",
    "translation": "MobiAct：使用MobileNetV4结合对比学习和知识蒸馏的高效MAV动作识别",
    "relevance_score": 1,
    "reasoning": "该论文专注于移动设备上的动作识别，属于纯粹的计算机视觉应用领域。虽然使用了高效的MobileNet架构和对比学习技术，但这些技术在当前形式下没有明确的推荐系统、搜索或广告应用场景，与指定的核心领域和使能技术焦点无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19272v1": {
    "title": "SCEESR: Semantic-Control Edge Enhancement for Diffusion-Based Super-Resolution",
    "url": "https://www.alphaxiv.org/abs/2510.19272v1",
    "arxiv_id": "2510.19272v1",
    "authors": "Yun Kai Zhuang",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 06:06:01",
    "ori_summary": "Real-world image super-resolution (Real-ISR) must handle complex degradations and inherent reconstruction ambiguities. While generative models have improved perceptual quality, a key trade-off remains with computational cost. One-step diffusion models offer speed but often produce structural inaccuracies due to distillation artifacts. To address this, we propose a novel SR framework that enhances a one-step diffusion model using a ControlNet mechanism for semantic edge guidance. This integrates edge information to provide dynamic structural control during single-pass inference. We also introduce a hybrid loss combining L2, LPIPS, and an edge-aware AME loss to optimize for pixel accuracy, perceptual quality, and geometric precision. Experiments show our method effectively improves structural integrity and realism while maintaining the efficiency of one-step generation, achieving a superior balance between output quality and inference speed. The results of test datasets will be published at https://drive.google.com/drive/folders/1amddXQ5orIyjbxHgGpzqFHZ6KTolinJF?usp=drive_link and the related code will be published at https://github.com/ARBEZ-ZEBRA/SCEESR.",
    "summary": "",
    "translation": "SCEESR：基于扩散模型的超分辨率语义控制边缘增强",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的超分辨率技术，属于纯粹的图像处理范畴，与推荐系统、搜索或广告的核心技术栈没有直接关联。虽然扩散模型是重要的生成模型，但该工作的应用场景局限于视觉质量提升，无法为RecSys/Search/Ads领域提供可迁移的技术价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19255v1": {
    "title": "Advances in 4D Representation: Geometry, Motion, and Interaction",
    "url": "https://www.alphaxiv.org/abs/2510.19255v1",
    "arxiv_id": "2510.19255v1",
    "authors": "Mingrui Zhao, Sauradip Nag, Kai Wang, Aditya Vora, Guangda Ji, Peter Chun, Ali Mahdavi-Amiri, Hao Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 05:22:20",
    "ori_summary": "We present a survey on 4D generation and reconstruction, a fast-evolving subfield of computer graphics whose developments have been propelled by recent advances in neural fields, geometric and motion deep learning, as well 3D generative artificial intelligence (GenAI). While our survey is not the first of its kind, we build our coverage of the domain from a unique and distinctive perspective of 4D representations\\/}, to model 3D geometry evolving over time while exhibiting motion and interaction. Specifically, instead of offering an exhaustive enumeration of many works, we take a more selective approach by focusing on representative works to highlight both the desirable properties and ensuing challenges of each representation under different computation, application, and data scenarios. The main take-away message we aim to convey to the readers is on how to select and then customize the appropriate 4D representations for their tasks. Organizationally, we separate the 4D representations based on three key pillars: geometry, motion, and interaction. Our discourse will not only encompass the most popular representations of today, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS), but also bring attention to relatively under-explored representations in the 4D context, such as structured models and long-range motions. Throughout our survey, we will reprise the role of large language models (LLMs) and video foundational models (VFMs) in a variety of 4D applications, while steering our discussion towards their current limitations and how they can be addressed. We also provide a dedicated coverage on what 4D datasets are currently available, as well as what is lacking, in driving the subfield forward. Project page:https://mingrui-zhao.github.io/4DRep-GMI/",
    "summary": "",
    "translation": "4D表示学习进展：几何、运动与交互",
    "relevance_score": 1,
    "reasoning": "该论文专注于4D表示学习，涉及几何、运动和交互，属于计算机视觉和图形学领域。这些主题与推荐系统、搜索或广告的核心技术没有直接关联，也不涉及LLM或Transformer架构的进展。论文内容主要处理时空视觉数据，在当前关注领域中缺乏明确的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19250v1": {
    "title": "Background Fades, Foreground Leads: Curriculum-Guided Background Pruning for Efficient Foreground-Centric Collaborative Perception",
    "url": "https://www.alphaxiv.org/abs/2510.19250v1",
    "arxiv_id": "2510.19250v1",
    "authors": "Yuheng Wu, Xiangbo Gao, Quang Tau, Zhengzhong Tu, Dongman Lee",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-22 05:10:26",
    "ori_summary": "Collaborative perception enhances the reliability and spatial coverage of autonomous vehicles by sharing complementary information across vehicles, offering a promising solution to long-tail scenarios that challenge single-vehicle perception. However, the bandwidth constraints of vehicular networks make transmitting the entire feature map impractical. Recent methods, therefore, adopt a foreground-centric paradigm, transmitting only predicted foreground-region features while discarding the background, which encodes essential context. We propose FadeLead, a foreground-centric framework that overcomes this limitation by learning to encapsulate background context into compact foreground features during training. At the core of our design is a curricular learning strategy that leverages background cues early on but progressively prunes them away, forcing the model to internalize context into foreground representations without transmitting background itself. Extensive experiments on both simulated and real-world benchmarks show that FadeLead outperforms prior methods under different bandwidth settings, underscoring the effectiveness of context-enriched foreground sharing.",
    "summary": "",
    "translation": "背景淡出，前景主导：基于课程学习的背景剪枝实现高效以前景为中心的协同感知",
    "relevance_score": 2,
    "reasoning": "该论文主要关注协同感知中的背景剪枝技术，属于计算机视觉和自动驾驶领域。虽然提到了课程学习和效率优化，但其核心应用场景是感知系统而非推荐系统、搜索或广告。该技术缺乏明确的路径应用于异构数据建模或Transformer架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19220v1": {
    "title": "Space Object Detection using Multi-frame Temporal Trajectory Completion Method",
    "url": "https://www.alphaxiv.org/abs/2510.19220v1",
    "arxiv_id": "2510.19220v1",
    "authors": "Xiaoqing Lan, Biqiao Xin, Bingshu Wang, Han Zhang, Laixian Zhang",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 04:04:27",
    "ori_summary": "Space objects in Geostationary Earth Orbit (GEO) present significant detection challenges in optical imaging due to weak signals, complex stellar backgrounds, and environmental interference. In this paper, we enhance high-frequency features of GEO targets while suppressing background noise at the single-frame level through wavelet transform. Building on this, we propose a multi-frame temporal trajectory completion scheme centered on the Hungarian algorithm for globally optimal cross-frame matching. To effectively mitigate missing and false detections, a series of key steps including temporal matching and interpolation completion, temporal-consistency-based noise filtering, and progressive trajectory refinement are designed in the post-processing pipeline. Experimental results on the public SpotGEO dataset demonstrate the effectiveness of the proposed method, achieving an F_1 score of 90.14%.",
    "summary": "",
    "translation": "基于多帧时序轨迹补全方法的空间目标检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于空间目标检测和轨迹补全，属于计算机视觉中的特定应用领域，与推荐系统、搜索或广告的核心技术没有直接关联。多帧时序处理技术虽然具有时序建模特性，但其在空间目标检测中的应用场景与推荐/搜索/广告中的用户行为序列建模存在本质差异，缺乏明确的跨领域应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19215v1": {
    "title": "SFGFusion: Surface Fitting Guided 3D Object Detection with 4D Radar and Camera Fusion",
    "url": "https://www.alphaxiv.org/abs/2510.19215v1",
    "arxiv_id": "2510.19215v1",
    "authors": "Xiaozhi Li, Huijun Di, Jian Li, Feng Liu, Wei Liang",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 03:56:27",
    "ori_summary": "3D object detection is essential for autonomous driving. As an emerging sensor, 4D imaging radar offers advantages as low cost, long-range detection, and accurate velocity measurement, making it highly suitable for object detection. However, its sparse point clouds and low resolution limit object geometric representation and hinder multi-modal fusion. In this study, we introduce SFGFusion, a novel camera-4D imaging radar detection network guided by surface fitting. By estimating quadratic surface parameters of objects from image and radar data, the explicit surface fitting model enhances spatial representation and cross-modal interaction, enabling more reliable prediction of fine-grained dense depth. The predicted depth serves two purposes: 1) in an image branch to guide the transformation of image features from perspective view (PV) to a unified bird's-eye view (BEV) for multi-modal fusion, improving spatial mapping accuracy; and 2) in a surface pseudo-point branch to generate dense pseudo-point cloud, mitigating the radar point sparsity. The original radar point cloud is also encoded in a separate radar branch. These two point cloud branches adopt a pillar-based method and subsequently transform the features into the BEV space. Finally, a standard 2D backbone and detection head are used to predict object labels and bounding boxes from BEV features. Experimental results show that SFGFusion effectively fuses camera and 4D radar features, achieving superior performance on the TJ4DRadSet and view-of-delft (VoD) object detection benchmarks.",
    "summary": "",
    "translation": "SFGFusion：基于曲面拟合引导的4D雷达与相机融合三维目标检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D视觉和多模态传感器融合技术，主要应用于自动驾驶领域。虽然涉及多模态融合概念，但其核心是3D目标检测和4D雷达处理，与推荐系统、搜索或广告领域没有直接关联，也不具备明显的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19210v1": {
    "title": "MoE-GS: Mixture of Experts for Dynamic Gaussian Splatting",
    "url": "https://www.alphaxiv.org/abs/2510.19210v1",
    "arxiv_id": "2510.19210v1",
    "authors": "In-Hwan Jin, Hyeongju Mun, Joonsoo Kim, Kugjin Yun, Kyeongbo Kong",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 03:41:59",
    "ori_summary": "Recent advances in dynamic scene reconstruction have significantly benefited from 3D Gaussian Splatting, yet existing methods show inconsistent performance across diverse scenes, indicating no single approach effectively handles all dynamic challenges. To overcome these limitations, we propose Mixture of Experts for Dynamic Gaussian Splatting (MoE-GS), a unified framework integrating multiple specialized experts via a novel Volume-aware Pixel Router. Our router adaptively blends expert outputs by projecting volumetric Gaussian-level weights into pixel space through differentiable weight splatting, ensuring spatially and temporally coherent results. Although MoE-GS improves rendering quality, the increased model capacity and reduced FPS are inherent to the MoE architecture. To mitigate this, we explore two complementary directions: (1) single-pass multi-expert rendering and gate-aware Gaussian pruning, which improve efficiency within the MoE framework, and (2) a distillation strategy that transfers MoE performance to individual experts, enabling lightweight deployment without architectural changes. To the best of our knowledge, MoE-GS is the first approach incorporating Mixture-of-Experts techniques into dynamic Gaussian splatting. Extensive experiments on the N3V and Technicolor datasets demonstrate that MoE-GS consistently outperforms state-of-the-art methods with improved efficiency. Video demonstrations are available at https://anonymous.4open.science/w/MoE-GS-68BA/.",
    "summary": "",
    "translation": "MoE-GS：用于动态高斯泼溅的专家混合模型",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及计算机视觉中的3D场景表示和渲染技术（高斯泼溅），属于纯粹的视觉领域。虽然提到了专家混合（MoE）架构，但其应用场景是动态3D重建，与推荐系统、搜索或广告的核心技术栈没有直接关联，也没有明显的跨模态应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19200v1": {
    "title": "GRASPLAT: Enabling dexterous grasping through novel view synthesis",
    "url": "https://www.alphaxiv.org/abs/2510.19200v1",
    "arxiv_id": "2510.19200v1",
    "authors": "Matteo Bortolon, Nuno Ferreira Duarte, Plinio Moreno, Fabio Poiesi, José Santos-Victor, Alessio Del Bue",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-22 03:19:26",
    "ori_summary": "Achieving dexterous robotic grasping with multi-fingered hands remains a significant challenge. While existing methods rely on complete 3D scans to predict grasp poses, these approaches face limitations due to the difficulty of acquiring high-quality 3D data in real-world scenarios. In this paper, we introduce GRASPLAT, a novel grasping framework that leverages consistent 3D information while being trained solely on RGB images. Our key insight is that by synthesizing physically plausible images of a hand grasping an object, we can regress the corresponding hand joints for a successful grasp. To achieve this, we utilize 3D Gaussian Splatting to generate high-fidelity novel views of real hand-object interactions, enabling end-to-end training with RGB data. Unlike prior methods, our approach incorporates a photometric loss that refines grasp predictions by minimizing discrepancies between rendered and real images. We conduct extensive experiments on both synthetic and real-world grasping datasets, demonstrating that GRASPLAT improves grasp success rates up to 36.9% over existing image-based methods. Project page: https://mbortolon97.github.io/grasplat/",
    "summary": "",
    "translation": "GRASPLAT：通过新颖视图合成实现灵巧抓取",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于机器人抓取和视图合成，属于纯粹的计算机视觉和机器人技术领域。虽然视图合成技术本身很先进，但它与推荐系统、搜索或广告的核心技术栈没有直接关联，也没有明显的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19195v1": {
    "title": "Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks",
    "url": "https://www.alphaxiv.org/abs/2510.19195v1",
    "arxiv_id": "2510.19195v1",
    "authors": "Kai Zeng, Zhanqian Wu, Kaixin Xiong, Xiaobao Wei, Xiangyu Guo, Zhenxin Zhu, Kalok Ho, Lijun Zhou, Bohan Zeng, Ming Lu, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Wentao Zhang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-22 03:02:38",
    "ori_summary": "Recent advancements in driving world models enable controllable generation of high-quality RGB videos or multimodal videos. Existing methods primarily focus on metrics related to generation quality and controllability. However, they often overlook the evaluation of downstream perception tasks, which are $\\mathbf{really\\ crucial}$ for the performance of autonomous driving. Existing methods usually leverage a training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). When we double the epochs in the baseline, the benefit of synthetic data becomes negligible. To thoroughly demonstrate the benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data generation framework designed for enhancing the downstream perception tasks. Dream4Drive first decomposes the input video into several 3D-aware guidance maps and subsequently renders the 3D assets onto these guidance maps. Finally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models. Dream4Drive enables unprecedented flexibility in generating multi-view corner cases at scale, significantly boosting corner case perception in autonomous driving. To facilitate future research, we also contribute a large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios and enabling diverse 3D-aware video editing. We conduct comprehensive experiments to show that Dream4Drive can effectively boost the performance of downstream perception models under various training epochs. Project: $\\href{https://wm-research.github.io/Dream4Drive/}{this\\ https\\ URL}$",
    "summary": "",
    "translation": "重新思考驾驶世界模型作为感知任务的合成数据生成器",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域的感知任务和世界模型，属于纯粹的计算机视觉应用。虽然提到了合成数据生成，但这与推荐系统、搜索或广告的核心技术领域没有直接关联。论文内容明显属于被排除的'纯粹视觉'类别，没有任何证据表明其技术可以应用于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19193v1": {
    "title": "Video Consistency Distance: Enhancing Temporal Consistency for Image-to-Video Generation via Reward-Based Fine-Tuning",
    "url": "https://www.alphaxiv.org/abs/2510.19193v1",
    "arxiv_id": "2510.19193v1",
    "authors": "Takehiro Aoshima, Yusuke Shinohara, Park Byeongseon",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 02:59:45",
    "ori_summary": "Reward-based fine-tuning of video diffusion models is an effective approach to improve the quality of generated videos, as it can fine-tune models without requiring real-world video datasets. However, it can sometimes be limited to specific performances because conventional reward functions are mainly aimed at enhancing the quality across the whole generated video sequence, such as aesthetic appeal and overall consistency. Notably, the temporal consistency of the generated video often suffers when applying previous approaches to image-to-video (I2V) generation tasks. To address this limitation, we propose Video Consistency Distance (VCD), a novel metric designed to enhance temporal consistency, and fine-tune a model with the reward-based fine-tuning framework. To achieve coherent temporal consistency relative to a conditioning image, VCD is defined in the frequency space of video frame features to capture frame information effectively through frequency-domain analysis. Experimental results across multiple I2V datasets demonstrate that fine-tuning a video generation model with VCD significantly enhances temporal consistency without degrading other performance compared to the previous method.",
    "summary": "",
    "translation": "视频一致性距离：通过基于奖励的微调增强图像到视频生成的时序一致性",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于图像到视频生成的时序一致性技术，属于纯粹的视觉内容生成领域。虽然提到了奖励微调，但该方法专门针对视频生成任务，与推荐系统、搜索或广告中的排序、检索或用户建模没有明显关联。该技术没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19183v1": {
    "title": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning",
    "url": "https://www.alphaxiv.org/abs/2510.19183v1",
    "arxiv_id": "2510.19183v1",
    "authors": "Fengyuan Sun, Hui Chen, Xinhao Xu, Dandan Zheng, Jingdong Chen, Jun Zhou, Jungong Han, Guiguang Ding",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-22 02:41:07",
    "ori_summary": "While multi-modal large language models (MLLMs) have made significant progress in recent years, the issue of hallucinations remains a major challenge. To mitigate this phenomenon, existing solutions either introduce additional data for further training or incorporate external or internal information during inference. However, these approaches inevitably introduce extra computational costs. In this paper, we observe that hallucinations in MLLMs are strongly associated with insufficient attention allocated to visual tokens. In particular, the presence of redundant visual tokens disperses the model's attention, preventing it from focusing on the most informative ones. As a result, critical visual cues are often under-attended, which in turn exacerbates the occurrence of hallucinations. Building on this observation, we propose \\textbf{PruneHal}, a training-free, simple yet effective method that leverages adaptive KV cache pruning to enhance the model's focus on critical visual information, thereby mitigating hallucinations. To the best of our knowledge, we are the first to apply token pruning for hallucination mitigation in MLLMs. Notably, our method don't require additional training and incurs nearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be seamlessly integrated with different decoding strategies, including those specifically designed for hallucination mitigation. We evaluate PruneHal on several widely used hallucination evaluation benchmarks using four mainstream MLLMs, achieving robust and outstanding results that highlight the effectiveness and superiority of our method. Our code will be publicly available.",
    "summary": "",
    "translation": "PruneHal：通过自适应KV缓存剪枝减少多模态大语言模型中的幻觉",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态LLM中的幻觉减少问题，这属于纯粹的LLM中心主题，与RecSys/Search/Ads的核心技术进展无关。虽然KV缓存剪枝可能带来效率提升，但论文的核心目标是解决幻觉问题而非提升推荐或搜索系统的性能，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19182v1": {
    "title": "Malaria Detection from Blood Cell Images Using XceptionNet",
    "url": "https://www.alphaxiv.org/abs/2510.19182v1",
    "arxiv_id": "2510.19182v1",
    "authors": "Warisa Nusrat, Mostafijur Rahman, Ayatullah Faruk Mollah",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 02:41:01",
    "ori_summary": "Malaria, which primarily spreads with the bite of female anopheles mosquitos, often leads to death of people - specifically children in the age-group of 0-5 years. Clinical experts identify malaria by observing RBCs in blood smeared images with a microscope. Lack of adequate professional knowledge and skills, and most importantly manual involvement may cause incorrect diagnosis. Therefore, computer aided automatic diagnosis stands as a preferred substitute. In this paper, well-demonstrated deep networks have been applied to extract deep intrinsic features from blood cell images and thereafter classify them as malaria infected or healthy cells. Among the six deep convolutional networks employed in this work viz. AlexNet, XceptionNet, VGG-19, Residual Attention Network, DenseNet-121 and Custom-CNN. Residual Attention Network and XceptionNet perform relatively better than the rest on a publicly available malaria cell image dataset. They yield an average accuracy of 97.28% and 97.55% respectively, that surpasses other related methods on the same dataset. These findings highly encourage the reality of deep learning driven method for automatic and reliable detection of malaria while minimizing direct manual involvement.",
    "summary": "",
    "translation": "使用XceptionNet从血细胞图像中检测疟疾",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于医学领域的疟疾检测，使用计算机视觉技术分析血细胞图像，这与搜索、推荐或广告系统完全无关。该研究属于明确的医学应用领域，属于需要排除的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19170v1": {
    "title": "FootFormer: Estimating Stability from Visual Input",
    "url": "https://www.alphaxiv.org/abs/2510.19170v1",
    "arxiv_id": "2510.19170v1",
    "authors": "Keaton Kraiger, Jingjing Li, Skanda Bharadwaj, Jesse Scott, Robert T. Collins, Yanxi Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-22 02:05:18",
    "ori_summary": "We propose FootFormer, a cross-modality approach for jointly predicting human motion dynamics directly from visual input. On multiple datasets, FootFormer achieves statistically significantly better or equivalent estimates of foot pressure distributions, foot contact maps, and center of mass (CoM), as compared with existing methods that generate one or two of those measures. Furthermore, FootFormer achieves SOTA performance in estimating stability-predictive components (CoP, CoM, BoS) used in classic kinesiology metrics. Code and data are available at https://github.com/keatonkraiger/Vision-to-Stability.git.",
    "summary": "",
    "translation": "FootFormer：基于视觉输入估计稳定性",
    "relevance_score": 1,
    "reasoning": "该论文标题表明其专注于从视觉输入估计稳定性（可能指步态或身体稳定性），这属于计算机视觉在生物力学或医疗领域的应用。该主题与推荐系统、搜索、广告或相关使能技术没有任何明显关联，完全超出了当前关注范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.19150v1": {
    "title": "X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning",
    "url": "https://www.alphaxiv.org/abs/2510.19150v1",
    "arxiv_id": "2510.19150v1",
    "authors": "Yunzhe Wang, Soham Hans, Volkan Ustun",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-22 00:48:35",
    "ori_summary": "Human team tactics emerge from each player's individual perspective and their ability to anticipate, interpret, and adapt to teammates' intentions. While advances in video understanding have improved the modeling of team interactions in sports, most existing work relies on third-person broadcast views and overlooks the synchronous, egocentric nature of multi-agent learning. We introduce X-Ego-CS, a benchmark dataset consisting of 124 hours of gameplay footage from 45 professional-level matches of the popular e-sports game Counter-Strike 2, designed to facilitate research on multi-agent decision-making in complex 3D environments. X-Ego-CS provides cross-egocentric video streams that synchronously capture all players' first-person perspectives along with state-action trajectories. Building on this resource, we propose Cross-Ego Contrastive Learning (CECL), which aligns teammates' egocentric visual streams to foster team-level tactical situational awareness from an individual's perspective. We evaluate CECL on a teammate-opponent location prediction task, demonstrating its effectiveness in enhancing an agent's ability to infer both teammate and opponent positions from a single first-person view using state-of-the-art video encoders. Together, X-Ego-CS and CECL establish a foundation for cross-egocentric multi-agent benchmarking in esports. More broadly, our work positions gameplay understanding as a testbed for multi-agent modeling and tactical learning, with implications for spatiotemporal reasoning and human-AI teaming in both virtual and real-world domains. Code and dataset are available at https://github.com/HATS-ICT/x-ego.",
    "summary": "",
    "translation": "X-Ego：通过跨自我中心对比视频表征学习获取团队级战术态势感知",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频理解和团队态势感知，属于计算机视觉领域。虽然涉及对比学习和表征学习技术，但这些技术本身过于通用，且论文的应用场景（战术态势感知）与推荐系统、搜索或广告没有明确的直接关联。没有证据表明该方法可以迁移到处理用户序列或上下文特征等异构数据模态。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  }
}