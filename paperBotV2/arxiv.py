import re
import os
import requests
import json
import time
import feedparser
from openai import OpenAI
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from tenacity import retry, stop_after_attempt, wait_random_exponential
from openai import APIConnectionError, RateLimitError, APIStatusError
from .prompts import PRERANK_PROMPT, FINERANK_PROMPT

# ä»ç¯å¢ƒå˜é‡è·å–é…ç½®ï¼ŒåŒæ—¶æä¾›é»˜è®¤å€¼
FEISHU_URL = os.environ.get("FEISHU_URL", None)
DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY", None)
# TARGET_CATEGORYS ä½¿ç”¨é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²æ ¼å¼
TARGET_CATEGORYS = os.environ.get("TARGET_CATEGORYS", "cs.IR,cs.CL,cs.CV")
TARGET_CATEGORYS = [cat.strip() for cat in TARGET_CATEGORYS.split(',')]
MAX_PAPERS = int(os.environ.get("MAX_PAPERS", "100"))
ROUGH_SCORE_THRESHOLD = int(os.environ.get("ROUGH_SCORE_THRESHOLD", "4"))
RETURN_PAPERS = int(os.environ.get("RETURN_PAPERS", "20"))

@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))
def call_deepseek_api(prompt_content: str,
                      
                      api_key: str = None,
                      model: str = "deepseek-chat",
                      base_url: str = "https://api.deepseek.com/v1"):
    """
    è°ƒç”¨ DeepSeek API å¹¶ä»¥ JSON æ ¼å¼è¿”å›ç»“æœã€‚
    Args:
        prompt_content (str): å‘é€ç»™æ¨¡å‹çš„å®Œæ•´æç¤ºè¯å†…å®¹ã€‚
        api_key (str, optional): ä½ çš„ DeepSeek API Keyã€‚å¦‚æœä¸º Noneï¼Œä¼šå°è¯•ä»ç¯å¢ƒå˜é‡ 'DEEPSEEK_API_KEY' è¯»å–ã€‚
        model (str, optional): ä½¿ç”¨çš„æ¨¡å‹åç§°ã€‚é»˜è®¤ä¸º "deepseek-chat"ã€‚
        base_url (str, optional): API çš„åŸºç¡€ URLã€‚é»˜è®¤ä¸º DeepSeek çš„å®˜æ–¹åœ°å€ã€‚
    Returns:
        dict: è§£æåçš„ JSON å¯¹è±¡ã€‚å¦‚æœå‘ç”Ÿé”™è¯¯ï¼Œåˆ™è¿”å› Noneã€‚
    """
    # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ api_keyï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
    if api_key is None:
        api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        print("ğŸ”‘ é”™è¯¯: API Key æœªæä¾›ã€‚è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡æˆ–é€šè¿‡å‚æ•°ä¼ å…¥ã€‚")
        return None
    try:
        client = OpenAI(api_key=api_key, base_url=base_url)
        messages = [
            {"role": "user", "content": prompt_content}
        ]
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            response_format={'type': 'json_object'}  # å¼ºåˆ¶è¿”å› JSON
        )
        response_content = response.choices[0].message.content
        return json.loads(response_content)
    except (APIConnectionError, RateLimitError, APIStatusError) as e:
        print(f"ğŸ”„ API é‡åˆ°å¯é‡è¯•é”™è¯¯: {e}ã€‚æ­£åœ¨ç”± tenacity è¿›è¡Œé‡è¯•...")
        raise  # å¿…é¡»é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œtenacity æ‰èƒ½æ•è·å¹¶æ‰§è¡Œé‡è¯•ç­–ç•¥
    except ImportError:
        print("ğŸ“¦ é”™è¯¯ï¼š'openai' åº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ 'pip install openai'ã€‚")
        return None
    except Exception as e:
        print(f"âŒ è°ƒç”¨ API æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None


def get_daily_arxiv_papers(category='cs.CL', max_results=20):
    """
    è·å–æŒ‡å®š arXiv é¢†åŸŸä»Šå¤©å‘å¸ƒçš„è®ºæ–‡ã€‚

    Args:
        category (str): ä½ æ„Ÿå…´è¶£çš„ arXiv ç±»åˆ«ï¼Œä¾‹å¦‚ 'cs.CL', 'cs.AI', 'stat.ML'ã€‚
        max_results (int): å¸Œæœ›è·å–çš„æœ€å¤§è®ºæ–‡æ•°é‡ã€‚
    """
    # 0. results json
    results = {}

    # 1. æ„å»º API è¯·æ±‚ URL
    base_url = 'http://export.arxiv.org/api/query?'

    # 2. å®šä¹‰æœç´¢å‚æ•°
    # è·å–ä»Šå¤©çš„æ—¥æœŸèŒƒå›´ï¼ˆè€ƒè™‘åˆ° UTC æ—¶é—´ï¼‰
    # arXiv çš„æœåŠ¡å™¨åœ¨ç¾å›½ä¸œéƒ¨ï¼Œä½† API é€šå¸¸ä½¿ç”¨ UTC æ—¶é—´
    today_utc = datetime.now(timezone.utc)
    start_of_day = today_utc.replace(hour=0, minute=0, second=0, microsecond=0)

    # æ ¼å¼åŒ–ä¸º API éœ€è¦çš„ YYYYMMDDHHMMSS æ ¼å¼
    # ä¸ºäº†ç¡®ä¿èƒ½æŠ“å–åˆ°ä»Šå¤©å‘å¸ƒçš„æ‰€æœ‰è®ºæ–‡ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠèŒƒå›´ç¨å¾®æ”¾å®½ä¸€ç‚¹
    # æ¯”å¦‚ä»æ˜¨å¤©æ™šä¸Šåˆ°ä»Šå¤©æ™šä¸Š
    yesterday_utc = start_of_day - timedelta(days=1)
    start_date_str = yesterday_utc.strftime('%Y%m%d%H%M%S')
    end_date_str = today_utc.strftime('%Y%m%d%H%M%S')

    # æ„å»º search_query
    # è¯­æ³•: cat:cs.CL AND submittedDate:[YYYYMMDDHHMMSS TO YYYYMMDDHHMMSS]
    search_query = f'cat:{category} AND submittedDate:[{start_date_str} TO {end_date_str}]'

    query_params = {
        'search_query': search_query,
        'sortBy': 'submittedDate',
        'sortOrder': 'descending',
        'start': 0,
        'max_results': max_results
    }

    # 3. å‘é€è¯·æ±‚å¹¶è·å–æ•°æ®
    try:
        response = requests.get(base_url, params=query_params)
        response.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥ (ä¾‹å¦‚ 404, 500), ä¼šæŠ›å‡ºå¼‚å¸¸
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        return {}

    # 4. ä½¿ç”¨ feedparser è§£æè¿”å›çš„ XML æ•°æ®
    feed = feedparser.parse(response.content)

    # 5. æ‰“å°è®ºæ–‡ä¿¡æ¯
    print(f"ğŸ” åœ¨åˆ†ç±» '{category}' ä¸­æ‰¾åˆ° {len(feed.entries)} ç¯‡ä»Šå¤©æ›´æ–°çš„è®ºæ–‡ï¼š")
    print("=" * 50)

    if not feed.entries:
        print("ğŸ“­ ä»Šå¤©è¿˜æ²¡æœ‰æ–°è®ºæ–‡ï¼Œæˆ–è€…æŸ¥è¯¢èŒƒå›´æœ‰è¯¯ã€‚")
        return {}

    for i, entry in enumerate(feed.entries):
        # æå–æ ¸å¿ƒä¿¡æ¯
        title = re.sub(r'\s+', ' ', entry.title.replace('\n', ' ').strip())
        arxiv_id = entry.id.split('/abs/')[-1]
        alphaxiv_link = f"https://www.alphaxiv.org/abs/{arxiv_id}"
        authors = ', '.join(author.name for author in entry.authors)
        summary = re.sub(r'\s+', ' ', entry.summary.replace('\n', ' ').strip())
        published_date = entry.published_parsed
        published_str = datetime(
            *published_date[:6]).strftime('%Y-%m-%d %H:%M:%S')
        categories = ', '.join(tag.term for tag in entry.tags)

        results[arxiv_id] = {
            'title': title,
            'url': alphaxiv_link,
            'arxiv_id': arxiv_id,
            'authors': authors,
            'categories': categories,
            'pub_date': published_str,
            'ori_summary': summary,
            'summary': '',  # å ä½ï¼Œåç»­ç”± LLM å¡«å……
            'translation': '',  # å ä½ï¼Œåç»­ç”± LLM å¡«å……
            'relevance_score': 0,  # å ä½ï¼Œåç»­ç”± LLM å¡«å……
            'reasoning': '',  # å ä½ï¼Œåç»­ç”± LLM å¡«å……
            'rerank_relevance_score': 0,  # å ä½ï¼Œåç»­ç”± LLM å¡«å……
            'rerank_reasoning': '',  # å ä½ï¼Œåç»­ç”± LLM å¡«å……
        }

    return results


def rough_analyze_paper(arxiv_id, paper):
    prompt = PRERANK_PROMPT.format(title=paper['title'])
    analysis = call_deepseek_api(
        prompt, api_key=DEEPSEEK_API_KEY)
    if analysis:
        # å°†åˆ†æç»“æœåˆå¹¶åˆ°åŸå§‹è®ºæ–‡ä¿¡æ¯ä¸­
        paper.update(analysis)
        return paper
    else:
        # å³ä½¿åˆ†æå¤±è´¥ï¼Œä¹Ÿæ‰“å°æ—¥å¿—ï¼Œä½†è¿”å› None
        print(f"âŒ [{arxiv_id}] {paper['title']} - åˆ†æå¤±è´¥")
        return None


def rough_analyze_papers_cocurrent(results, max_workers=10):
    analyzed_papers = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_paper = {
            executor.submit(rough_analyze_paper, arxiv_id, paper): paper
            for arxiv_id, paper in results.items()
        }
        print(f"\nğŸš€ å¼€å§‹å¹¶å‘åˆ†æ {len(results)} ç¯‡è®ºæ–‡ï¼Œä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹...")
        progress_bar = tqdm(as_completed(future_to_paper),
                            total=len(results), desc="åˆ†æè¿›åº¦")
        for future in progress_bar:
            try:
                updated_paper = future.result()
                if updated_paper:
                    analyzed_papers.append(updated_paper)
            except Exception as exc:
                paper_info = future_to_paper[future]
                print(f"âš ï¸ å¤„ç†è®ºæ–‡ {paper_info['title']} æ—¶äº§ç”Ÿå¼‚å¸¸: {exc}")
    if not analyzed_papers:
        print("ğŸ“­ \næ²¡æœ‰æˆåŠŸåˆ†æä»»ä½•è®ºæ–‡ã€‚")
        return []
    print(f"\nâœ… æ‰€æœ‰è®ºæ–‡åˆ†æå®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(analyzed_papers)} ç¯‡ã€‚")
    return analyzed_papers


def rough_rank_papers(results, filter_threshold=2, max_workers=10):
    # LLM rough analyze papers concurrently
    analyzed_papers = rough_analyze_papers_cocurrent(
        results, max_workers=max_workers)

    # æ ¸å¿ƒæ’åºé€»è¾‘ï¼šæŒ‰ relevance_score é™åºæ’åº
    analyzed_papers.sort(key=lambda p: p.get(
        'relevance_score', 0), reverse=True)

    # æ‰“å°æ’åºå’Œè¿‡æ»¤å‰çš„ç»“æœé¢„è§ˆ
    print("\n--- åˆ†æç»“æœé¢„è§ˆ (æŒ‰ç›¸å…³æ€§æ’åº) ---")
    for paper in analyzed_papers:
        print("-" * 60)
        print(f"âœ… [{paper['arxiv_id']}] {paper['title']}")
        print(f"  - ç¿»è¯‘: {paper.get('translation', 'N/A')}")
        print(f"  - ç›¸å…³æ€§è¯„åˆ†: {paper.get('relevance_score', 'N/A')}/10")
        print(f"  - ç†ç”±: {paper.get('reasoning', 'N/A')}")
    print("-" * 60)

    # è¿‡æ»¤ä½åˆ†è®ºæ–‡
    filtered_papers = [p for p in analyzed_papers if p.get(
        'relevance_score', 0) >= filter_threshold]
    print(
        f"\nâš ï¸ è¿‡æ»¤æ‰ {len(analyzed_papers) - len(filtered_papers)} ç¯‡ä½åˆ†è®ºæ–‡ï¼Œå‰©ä½™ {len(filtered_papers)} ç¯‡é«˜è´¨é‡è®ºæ–‡ã€‚")
    return filtered_papers


def fine_analyze_paper(arxiv_id, paper):
    prompt = FINERANK_PROMPT.format(
        title=paper['title'], summary=paper['ori_summary'])
    analysis = call_deepseek_api(
        prompt, api_key=DEEPSEEK_API_KEY)
    if analysis:
        # å°†åˆ†æç»“æœåˆå¹¶åˆ°åŸå§‹è®ºæ–‡ä¿¡æ¯ä¸­
        paper.update(analysis)
        return paper
    else:
        # å³ä½¿åˆ†æå¤±è´¥ï¼Œä¹Ÿæ‰“å°æ—¥å¿—ï¼Œä½†è¿”å› None
        print(f"âŒ [{arxiv_id}] {paper['title']} - åˆ†æå¤±è´¥")
        return None


def fine_analyze_papers_cocurrent(papers, max_workers=10):
    analyzed_papers = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_paper = {
            executor.submit(fine_analyze_paper, paper['arxiv_id'], paper): paper
            for paper in papers
        }
        print(f"\nğŸš€ å¼€å§‹å¹¶å‘ç²¾æ’ {len(papers)} ç¯‡è®ºæ–‡ï¼Œä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹...")
        progress_bar = tqdm(as_completed(future_to_paper),
                            total=len(papers), desc="ç²¾æ’è¿›åº¦")
        for future in progress_bar:
            try:
                updated_paper = future.result()
                if updated_paper:
                    analyzed_papers.append(updated_paper)
            except Exception as exc:
                paper_info = future_to_paper[future]
                print(f"å¤„ç†è®ºæ–‡ {paper_info['title']} æ—¶äº§ç”Ÿå¼‚å¸¸: {exc}")
    if not analyzed_papers:
        print("\næ²¡æœ‰æˆåŠŸç²¾æ’ä»»ä½•è®ºæ–‡ã€‚")
        return []
    print(f"\nâœ… æ‰€æœ‰è®ºæ–‡ç²¾æ’å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(analyzed_papers)} ç¯‡ã€‚")
    return analyzed_papers


def fine_rank_papers(papers, max_workers=10, paper_count=5):
    papers = papers[:paper_count]  # åªç²¾æ’å‰ N ç¯‡è®ºæ–‡

    # LLM fine analyze papers concurrently
    analyzed_papers = fine_analyze_papers_cocurrent(
        papers, max_workers=max_workers)

    # æ ¸å¿ƒæ’åºé€»è¾‘ï¼šæŒ‰ rerank_relevance_score é™åºæ’åº
    analyzed_papers.sort(key=lambda p: p.get(
        'rerank_relevance_score', 0), reverse=True)

    # æ‰“å°æ’åºåçš„ç»“æœé¢„è§ˆ
    print("\n--- ç²¾æ’ç»“æœé¢„è§ˆ (æŒ‰ç²¾æ’è¯„åˆ†æ’åº) ---")
    for paper in analyzed_papers:
        print("-" * 60)
        print(f"âœ… [{paper['arxiv_id']}] {paper['title']}")
        print(f"  - ç¿»è¯‘: {paper.get('translation', 'N/A')}")
        print(f"  - ç²¾æ’ç›¸å…³æ€§è¯„åˆ†: {paper.get('rerank_relevance_score', 'N/A')}/10")
        print(f"  - ç†ç”±: {paper.get('rerank_reasoning', 'N/A')}")
        print(f"  - æ€»ç»“: {paper.get('summary', 'N/A')}")
    print("-" * 60)

    return analyzed_papers


def send_papers_to_feishu(papers, feishu_url=FEISHU_URL):
    
    date = datetime.now().strftime('%Y-%m-%d')
    
    card_data = {
        "type": "template",
        "data": {
            "template_id": "AAqxH62u1uNko",
            "template_version_name": "1.0.5",
            "template_variable": {
                "loop": [],
                "date": date
            }
        }
    }

    for paper in papers:
        title = paper['title']
        translation = paper.get('translation', 'N/A')
        score = paper.get('rerank_relevance_score', 'N/A')
        summary = paper.get('summary', 'N/A')
        url = paper['url']
        
        paper = f"[{title}]({url})"
        score = "â­ï¸" * score + f" <text_tag color='blue'>{score}åˆ†</text_tag>" if isinstance(score, int) else "N/A"
        
        card_data['data']['template_variable']['loop'].append({
            "paper": paper,
            "translation": translation,
            "score": score,
            "summary": summary
        })
        
    card = json.dumps(card_data)
    body = json.dumps({"msg_type": "interactive", "card": card})
    headers = {"Content-Type": "application/json"}
    ret = requests.post(url=feishu_url, data=body, headers=headers)
    print(f"âœ‰ï¸ é£ä¹¦æ¨é€è¿”å›çŠ¶æ€: {ret.status_code}")

def get_papers_from_all_categories():
    """ä»æ‰€æœ‰æŒ‡å®šåˆ†ç±»è·å–è®ºæ–‡å¹¶åˆå§‹åŒ–çŠ¶æ€æ ‡è®°ï¼Œå»é™¤ä¸å‰ä¸€å¤©é‡å¤çš„è®ºæ–‡"""
    all_papers = {}
    
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•ï¼ˆpaperBotV2ç›®å½•ï¼‰
    current_dir = os.path.dirname(os.path.abspath(__file__))
    
    # è·å–å‰ä¸€å¤©çš„æ—¥æœŸ
    yesterday = datetime.now() - timedelta(days=1)
    yesterday_file = os.path.join(current_dir, "arxiv_daily", f"{yesterday.strftime('%Y%m%d')}.json")
    
    # è¯»å–å‰ä¸€å¤©çš„è®ºæ–‡IDé›†åˆï¼ˆç”¨äºå»é‡ï¼‰
    yesterday_paper_ids = set()
    if os.path.exists(yesterday_file):
        try:
            with open(yesterday_file, 'r', encoding='utf-8') as f:
                yesterday_data = json.load(f)
                yesterday_paper_ids = set(yesterday_data.keys())
            print(f"ğŸ“‹ å·²åŠ è½½å‰ä¸€å¤©çš„è®ºæ–‡IDé›†åˆï¼Œå…± {len(yesterday_paper_ids)} ç¯‡è®ºæ–‡ã€‚")
        except Exception as e:
            print(f"âŒ è¯»å–å‰ä¸€å¤©è®ºæ–‡æ–‡ä»¶å¤±è´¥: {e}")
    
    # è·å–å½“å‰æ—¥æœŸçš„æ‰€æœ‰åˆ†ç±»è®ºæ–‡
    for category in TARGET_CATEGORYS:
        category_results = get_daily_arxiv_papers(category=category, max_results=MAX_PAPERS)
        
        # æ·»åŠ åˆ°all_paperså¹¶åˆå§‹åŒ–çŠ¶æ€æ ‡è®°ï¼Œè·³è¿‡ä¸å‰ä¸€å¤©é‡å¤çš„è®ºæ–‡
        for arxiv_id, paper in category_results.items():
            if arxiv_id not in yesterday_paper_ids:
                paper['is_filtered'] = False  # é»˜è®¤ä¸ºæœªè¿‡æ»¤
                paper['is_fine_ranked'] = False  # é»˜è®¤ä¸ºæœªç²¾æ’
                all_papers[arxiv_id] = paper
        
        time.sleep(3)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
    
    print(f"ğŸ“š è·å–åˆ° {len(all_papers)} ç¯‡è®ºæ–‡ï¼ˆå·²å»é™¤ä¸å‰ä¸€å¤©é‡å¤çš„è®ºæ–‡ï¼‰ã€‚")
    return all_papers


def perform_rough_ranking(all_papers):
    """æ‰§è¡Œç²—æ’å¹¶æ ‡è®°è¿‡æ»¤çŠ¶æ€"""
    filtered_papers = []
    
    for arxiv_id, paper in all_papers.items():
        analyzed_paper = rough_analyze_paper(arxiv_id, paper.copy())
        if analyzed_paper:
            all_papers[arxiv_id].update(analyzed_paper)
            # æ ‡è®°è¿‡æ»¤çŠ¶æ€
            if analyzed_paper.get('relevance_score', 0) >= ROUGH_SCORE_THRESHOLD:
                all_papers[arxiv_id]['is_filtered'] = False
                filtered_papers.append(analyzed_paper)
            else:
                all_papers[arxiv_id]['is_filtered'] = True
    
    print(f"âœ¨ ç²—æ’ç­›é€‰ {len(filtered_papers)} ç¯‡é«˜è´¨é‡è®ºæ–‡ã€‚")
    return filtered_papers


def perform_fine_ranking(filtered_papers, all_papers):
    """æ‰§è¡Œç²¾æ’å¹¶æ ‡è®°ç²¾æ’çŠ¶æ€"""
    final_papers = fine_rank_papers(filtered_papers, paper_count=RETURN_PAPERS)
    
    for paper in final_papers:
        arxiv_id = paper['arxiv_id']
        all_papers[arxiv_id].update(paper)  # æ›´æ–°ç²¾æ’ä¿¡æ¯
        all_papers[arxiv_id]['is_fine_ranked'] = True  # æ ‡è®°ä¸ºå·²ç²¾æ’
    
    print(f"ğŸ† ç²¾æ’å¾—åˆ° {len(final_papers)} ç¯‡é¡¶çº§è®ºæ–‡ã€‚")
    return final_papers


def save_results_to_json(all_papers):
    """ä¿å­˜æ‰€æœ‰ç»“æœåˆ°æŒ‡å®šè·¯å¾„çš„JSONæ–‡ä»¶ï¼ŒåŒ…æ‹¬å¤©çº§æ–‡ä»¶å’Œå…¨é‡æ–‡ä»¶"""
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•ï¼ˆpaperBotV2ç›®å½•ï¼‰
    current_dir = os.path.dirname(os.path.abspath(__file__))
    
    # ç¡®ä¿arxiv_dailyç›®å½•å­˜åœ¨äºpaperBotV2ç›®å½•ä¸‹
    save_dir = os.path.join(current_dir, "arxiv_daily")
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    # 1. ä¿å­˜å½“å¤©çš„ç»“æœåˆ°æ—¥æœŸæ ¼å¼çš„æ–‡ä»¶
    daily_file = os.path.join(save_dir, f"{datetime.now().strftime('%Y%m%d')}.json")
    with open(daily_file, 'w', encoding='utf-8') as f:
        json.dump(all_papers, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ å½“å¤©è®ºæ–‡ç»“æœå·²ä¿å­˜åˆ° {daily_file}")
    
    # 2. ä¿å­˜/æ›´æ–°å…¨é‡results.jsonæ–‡ä»¶
    all_results_file = os.path.join(save_dir, "results.json")
    
    # è¯»å–å·²æœ‰å…¨é‡ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    all_results = {}
    if os.path.exists(all_results_file):
        try:
            with open(all_results_file, 'r', encoding='utf-8') as f:
                all_results = json.load(f)
            print(f"ğŸ“‹ å·²åŠ è½½ç°æœ‰å…¨é‡ç»“æœï¼Œå…± {len(all_results)} ç¯‡è®ºæ–‡ã€‚")
        except Exception as e:
            print(f"âŒ è¯»å–å…¨é‡ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
    
    # å¢é‡æ›´æ–°å…¨é‡ç»“æœï¼ˆä½¿ç”¨arxiv_idä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼‰
    new_papers_count = 0
    for arxiv_id, paper in all_papers.items():
        if arxiv_id not in all_results:
            all_results[arxiv_id] = paper
            new_papers_count += 1
    
    # ä¿å­˜æ›´æ–°åçš„å…¨é‡ç»“æœ
    with open(all_results_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“Š å…¨é‡ç»“æœå·²æ›´æ–°åˆ° {all_results_file}ï¼Œæ–°å¢ {new_papers_count} ç¯‡è®ºæ–‡ï¼Œæ€»è®ºæ–‡æ•°: {len(all_results)}")


def process_papers():
    """å¤„ç†å¹¶ä¿å­˜è®ºæ–‡çš„ä¸»å‡½æ•° - åè°ƒå„ä¸ªå­å‡½æ•°çš„æ‰§è¡Œ"""
    # 1. è·å–è®ºæ–‡
    all_papers = get_papers_from_all_categories()
    
    # 2. ç²—æ’
    filtered_papers = perform_rough_ranking(all_papers)
    
    # 3. ç²¾æ’
    final_papers = perform_fine_ranking(filtered_papers, all_papers)
    
    # 4. ä¿å­˜ç»“æœ
    save_results_to_json(all_papers)
    
    print("âœ… è®ºæ–‡å¤„ç†æµç¨‹å·²å…¨éƒ¨å®Œæˆï¼")
    
    # 5. å‘é€åˆ°é£ä¹¦
    send_papers_to_feishu(final_papers)


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    process_papers()