import re
import os
import requests
import json
import time
import feedparser
from openai import OpenAI
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from tenacity import retry, stop_after_attempt, wait_random_exponential
from openai import APIConnectionError, RateLimitError, APIStatusError

PRERANK_PROMPT = """
# Role
You are a highly experienced Research Engineer specializing in Large Language Models (LLMs) and Large-Scale Recommendation Systems, with deep knowledge of the search, recommendation, and advertising domains.

# My Current Focus

- **Core Domain Advances:** Core advances within RecSys, Search, or Ads itself, even if they do not involve LLMs.
- **Enabling LLM Tech:** Trends and Foundational progress in the core LLM which must have potential applications in RecSys, Search or Ads.
- **Enabling Transformer Tech: Advances in Transformer architecture (e.g., efficiency, new attention mechanisms, MoE, etc.).
- **Direct LLM Applications:* Novel ideas and direct applications of LLM technology for RecSys, Search or Ads.
- **VLM Analogy for Heterogeneous Data:** Ideas inspired by **Vision-Language Models** that treat heterogeneous data (like context features and user sequences) as distinct modalities for unified modeling. 

# Irrelevant Topics
- Federated learning, Security, Privacy, Fairness, Ethics, or other non-technical topics
- Medical, Biology, Chemistry, Physics or other domain-specific applications
- Neural Architectures Search (NAS) or general AutoML
- Purely theoretical papers without clear practical implications
- Hallucination, Evaluation benchmarks, or other purely NLP-centric topics
- Purely Visionã€3D Vision, Graphic or Speech papers without clear relevance to RecSys/Search/Ads
- Ads creative generation, auction, bidding or other Non-Ranking Ads topics 
- AIGC, Content generation, Summarization, or other purely LLM-centric topics
- Reinforcement Learning (RL) papers without clear relevance to RecSys/Search/Ads

# Goal
Screen new papers based on my focus. DO NOT include irrelevant topics.

# Task
Based ONLY on the paper's title, provide a quick evaluation.
1. **Academic Translation**: Translate the title into professional Chinese, prioritizing accurate technical terms and faithful meaning.
2. **Relevance Score (1-10)**: How relevant is it to **My Current Focus**?
3. **Reasoning**: A 2-3 sentence explanation for your score. **For "Enabling Tech" papers, you MUST explain their potential application in RecSys/Search/Ads.**

# Input Paper
- **Title**: {title}

# Output Format
Provide your analysis strictly in the following JSON format.
{{
  "translation": "...",
  "relevance_score": <integer>,
  "reasoning": "..."
}}
"""

FINERANK_PROMPT = """
# Role
You are a highly experienced Research Engineer specializing in Large Language Models (LLMs) and Large-Scale Recommendation Systems, with deep knowledge of the search, recommendation, and advertising domains.

# My Current Focus

- **Core Domain Advances:** Core advances within RecSys, Search, or Ads itself, even if they do not involve LLMs.
- **Enabling LLM Tech:** Trends and Foundational progress in the core LLM which must have potential applications in RecSys, Search or Ads.
- **Enabling Transformer Tech: Advances in Transformer architecture (e.g., efficiency, new attention mechanisms, MoE, etc.).
- **Direct LLM Applications:* Novel ideas and direct applications of LLM technology for RecSys, Search or Ads.
- **VLM Analogy for Heterogeneous Data:** Ideas inspired by **Vision-Language Models** that treat heterogeneous data (like context features and user sequences) as distinct modalities for unified modeling. 

# Goal
Perform a detailed analysis of the provided paper based on its title and abstract. Identify its core contributions and relevance to my focus areas.

# Task
Based on the paper's **Title** and **Abstract**, provide a comprehensive analysis.
1.  **Relevance Score (1-10)**: Re-evaluate the relevance score (1-10) based on the detailed information in the abstract.
2.  **Reasoning**: A 2-3 sentence explanation for your score in Chinese.
3.  **Summary**: Distill the abstract into a high-density summary of no more than 150 Chinese words, prioritizing accurate technical terms and faithful meaning.

# Input Paper
- **Title**: {title}
- **Abstract**: {summary}

# Output Format
Provide your analysis strictly in the following JSON format.
{{
  "rerank_relevance_score": <integer>,
  "rerank_reasoning": "...",
  "summary": "..."
}}
"""

FEISHU_URL = os.environ.get("FEISHU_URL", None)
DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY", None)
TARGET_CATEGORYS = ['cs.IR']
MAX_PAPERS = 100

@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))
def call_deepseek_api(prompt_content: str,
                      
                      api_key: str = None,
                      model: str = "deepseek-chat",
                      base_url: str = "https://api.deepseek.com/v1"):
    """
    è°ƒç”¨ DeepSeek API å¹¶ä»¥ JSON æ ¼å¼è¿”å›ç»“æœã€‚
    Args:
        prompt_content (str): å‘é€ç»™æ¨¡å‹çš„å®Œæ•´æç¤ºè¯å†…å®¹ã€‚
        api_key (str, optional): ä½ çš„ DeepSeek API Keyã€‚å¦‚æœä¸º Noneï¼Œä¼šå°è¯•ä»ç¯å¢ƒå˜é‡ 'DEEPSEEK_API_KEY' è¯»å–ã€‚
        model (str, optional): ä½¿ç”¨çš„æ¨¡å‹åç§°ã€‚é»˜è®¤ä¸º "deepseek-chat"ã€‚
        base_url (str, optional): API çš„åŸºç¡€ URLã€‚é»˜è®¤ä¸º DeepSeek çš„å®˜æ–¹åœ°å€ã€‚
    Returns:
        dict: è§£æåçš„ JSON å¯¹è±¡ã€‚å¦‚æœå‘ç”Ÿé”™è¯¯ï¼Œåˆ™è¿”å› Noneã€‚
    """
    # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ api_keyï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
    if api_key is None:
        api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        print("é”™è¯¯ï¼šAPI Key æœªæä¾›ã€‚è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡æˆ–é€šè¿‡å‚æ•°ä¼ å…¥ã€‚")
        return None
    try:
        client = OpenAI(api_key=api_key, base_url=base_url)
        messages = [
            {"role": "user", "content": prompt_content}
        ]
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            response_format={'type': 'json_object'}  # å¼ºåˆ¶è¿”å› JSON
        )
        response_content = response.choices[0].message.content
        return json.loads(response_content)
    except (APIConnectionError, RateLimitError, APIStatusError) as e:
        print(f"API é‡åˆ°å¯é‡è¯•é”™è¯¯: {e}ã€‚æ­£åœ¨ç”± tenacity è¿›è¡Œé‡è¯•...")
        raise  # å¿…é¡»é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œtenacity æ‰èƒ½æ•è·å¹¶æ‰§è¡Œé‡è¯•ç­–ç•¥
    except ImportError:
        print("é”™è¯¯ï¼š'openai' åº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ 'pip install openai'ã€‚")
        return None
    except Exception as e:
        print(f"è°ƒç”¨ API æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None


def get_daily_arxiv_papers(category='cs.CL', max_results=20):
    """
    è·å–æŒ‡å®š arXiv é¢†åŸŸä»Šå¤©å‘å¸ƒçš„è®ºæ–‡ã€‚

    Args:
        category (str): ä½ æ„Ÿå…´è¶£çš„ arXiv ç±»åˆ«ï¼Œä¾‹å¦‚ 'cs.CL', 'cs.AI', 'stat.ML'ã€‚
        max_results (int): å¸Œæœ›è·å–çš„æœ€å¤§è®ºæ–‡æ•°é‡ã€‚
    """
    # 0. results json
    results = {}

    # 1. æ„å»º API è¯·æ±‚ URL
    base_url = 'http://export.arxiv.org/api/query?'

    # 2. å®šä¹‰æœç´¢å‚æ•°
    # è·å–ä»Šå¤©çš„æ—¥æœŸèŒƒå›´ï¼ˆè€ƒè™‘åˆ° UTC æ—¶é—´ï¼‰
    # arXiv çš„æœåŠ¡å™¨åœ¨ç¾å›½ä¸œéƒ¨ï¼Œä½† API é€šå¸¸ä½¿ç”¨ UTC æ—¶é—´
    today_utc = datetime.now(timezone.utc)
    start_of_day = today_utc.replace(hour=0, minute=0, second=0, microsecond=0)

    # æ ¼å¼åŒ–ä¸º API éœ€è¦çš„ YYYYMMDDHHMMSS æ ¼å¼
    # ä¸ºäº†ç¡®ä¿èƒ½æŠ“å–åˆ°ä»Šå¤©å‘å¸ƒçš„æ‰€æœ‰è®ºæ–‡ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠèŒƒå›´ç¨å¾®æ”¾å®½ä¸€ç‚¹
    # æ¯”å¦‚ä»æ˜¨å¤©æ™šä¸Šåˆ°ä»Šå¤©æ™šä¸Š
    yesterday_utc = start_of_day - timedelta(days=1)
    start_date_str = yesterday_utc.strftime('%Y%m%d%H%M%S')
    end_date_str = today_utc.strftime('%Y%m%d%H%M%S')

    # æ„å»º search_query
    # è¯­æ³•: cat:cs.CL AND submittedDate:[YYYYMMDDHHMMSS TO YYYYMMDDHHMMSS]
    search_query = f'cat:{category} AND submittedDate:[{start_date_str} TO {end_date_str}]'

    query_params = {
        'search_query': search_query,
        'sortBy': 'submittedDate',
        'sortOrder': 'descending',
        'start': 0,
        'max_results': max_results
    }

    # 3. å‘é€è¯·æ±‚å¹¶è·å–æ•°æ®
    try:
        response = requests.get(base_url, params=query_params)
        response.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥ (ä¾‹å¦‚ 404, 500), ä¼šæŠ›å‡ºå¼‚å¸¸
    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return {}

    # 4. ä½¿ç”¨ feedparser è§£æè¿”å›çš„ XML æ•°æ®
    feed = feedparser.parse(response.content)

    # 5. æ‰“å°è®ºæ–‡ä¿¡æ¯
    print(f"ğŸ” åœ¨åˆ†ç±» '{category}' ä¸­æ‰¾åˆ° {len(feed.entries)} ç¯‡ä»Šå¤©æ›´æ–°çš„è®ºæ–‡ï¼š")
    print("=" * 50)

    if not feed.entries:
        print("ä»Šå¤©è¿˜æ²¡æœ‰æ–°è®ºæ–‡ï¼Œæˆ–è€…æŸ¥è¯¢èŒƒå›´æœ‰è¯¯ã€‚")
        return {}

    for i, entry in enumerate(feed.entries):
        # æå–æ ¸å¿ƒä¿¡æ¯
        title = re.sub(r'\s+', ' ', entry.title.replace('\n', ' ').strip())
        arxiv_id = entry.id.split('/abs/')[-1]
        alphaxiv_link = f"https://www.alphaxiv.org/abs/{arxiv_id}"
        authors = ', '.join(author.name for author in entry.authors)
        summary = re.sub(r'\s+', ' ', entry.summary.replace('\n', ' ').strip())
        published_date = entry.published_parsed
        published_str = datetime(
            *published_date[:6]).strftime('%Y-%m-%d %H:%M:%S')
        categories = ', '.join(tag.term for tag in entry.tags)

        results[arxiv_id] = {
            'title': title,
            'url': alphaxiv_link,
            'arxiv_id': arxiv_id,
            'authors': authors,
            'categories': categories,
            'pub_date': published_str,
            'summary': summary
        }

    return results


def rough_analyze_paper(arxiv_id, paper):
    prompt = PRERANK_PROMPT.format(title=paper['title'])
    analysis = call_deepseek_api(
        prompt, api_key=DEEPSEEK_API_KEY)
    if analysis:
        # å°†åˆ†æç»“æœåˆå¹¶åˆ°åŸå§‹è®ºæ–‡ä¿¡æ¯ä¸­
        paper.update(analysis)
        return paper
    else:
        # å³ä½¿åˆ†æå¤±è´¥ï¼Œä¹Ÿæ‰“å°æ—¥å¿—ï¼Œä½†è¿”å› None
        print(f"âŒ [{arxiv_id}] {paper['title']} - åˆ†æå¤±è´¥")
        return None


def rough_analyze_papers_cocurrent(results, max_workers=10):
    analyzed_papers = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_paper = {
            executor.submit(rough_analyze_paper, arxiv_id, paper): paper
            for arxiv_id, paper in results.items()
        }
        print(f"\nğŸš€ å¼€å§‹å¹¶å‘åˆ†æ {len(results)} ç¯‡è®ºæ–‡ï¼Œä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹...")
        progress_bar = tqdm(as_completed(future_to_paper),
                            total=len(results), desc="åˆ†æè¿›åº¦")
        for future in progress_bar:
            try:
                updated_paper = future.result()
                if updated_paper:
                    analyzed_papers.append(updated_paper)
            except Exception as exc:
                paper_info = future_to_paper[future]
                print(f"å¤„ç†è®ºæ–‡ {paper_info['title']} æ—¶äº§ç”Ÿå¼‚å¸¸: {exc}")
    if not analyzed_papers:
        print("\næ²¡æœ‰æˆåŠŸåˆ†æä»»ä½•è®ºæ–‡ã€‚")
        return []
    print(f"\nâœ… æ‰€æœ‰è®ºæ–‡åˆ†æå®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(analyzed_papers)} ç¯‡ã€‚")
    return analyzed_papers


def rough_rank_papers(results, filter_threshold=2, max_workers=10):
    # LLM rough analyze papers concurrently
    analyzed_papers = rough_analyze_papers_cocurrent(
        results, max_workers=max_workers)

    # æ ¸å¿ƒæ’åºé€»è¾‘ï¼šæŒ‰ relevance_score é™åºæ’åº
    analyzed_papers.sort(key=lambda p: p.get(
        'relevance_score', 0), reverse=True)

    # æ‰“å°æ’åºå’Œè¿‡æ»¤å‰çš„ç»“æœé¢„è§ˆ
    print("\n--- åˆ†æç»“æœé¢„è§ˆ (æŒ‰ç›¸å…³æ€§æ’åº) ---")
    for paper in analyzed_papers:
        print("-" * 60)
        print(f"âœ… [{paper['arxiv_id']}] {paper['title']}")
        print(f"  - ç¿»è¯‘: {paper.get('translation', 'N/A')}")
        print(f"  - ç›¸å…³æ€§è¯„åˆ†: {paper.get('relevance_score', 'N/A')}/10")
        print(f"  - ç†ç”±: {paper.get('reasoning', 'N/A')}")
    print("-" * 60)

    # è¿‡æ»¤ä½åˆ†è®ºæ–‡
    filtered_papers = [p for p in analyzed_papers if p.get(
        'relevance_score', 0) >= filter_threshold]
    print(
        f"\nâš ï¸ è¿‡æ»¤æ‰ {len(analyzed_papers) - len(filtered_papers)} ç¯‡ä½åˆ†è®ºæ–‡ï¼Œå‰©ä½™ {len(filtered_papers)} ç¯‡é«˜è´¨é‡è®ºæ–‡ã€‚")
    return filtered_papers


def fine_analyze_paper(arxiv_id, paper):
    prompt = FINERANK_PROMPT.format(
        title=paper['title'], summary=paper['summary'])
    analysis = call_deepseek_api(
        prompt, api_key=DEEPSEEK_API_KEY)
    if analysis:
        # å°†åˆ†æç»“æœåˆå¹¶åˆ°åŸå§‹è®ºæ–‡ä¿¡æ¯ä¸­
        paper.update(analysis)
        return paper
    else:
        # å³ä½¿åˆ†æå¤±è´¥ï¼Œä¹Ÿæ‰“å°æ—¥å¿—ï¼Œä½†è¿”å› None
        print(f"âŒ [{arxiv_id}] {paper['title']} - åˆ†æå¤±è´¥")
        return None


def fine_analyze_papers_cocurrent(papers, max_workers=10):
    analyzed_papers = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_paper = {
            executor.submit(fine_analyze_paper, paper['arxiv_id'], paper): paper
            for paper in papers
        }
        print(f"\nğŸš€ å¼€å§‹å¹¶å‘ç²¾æ’ {len(papers)} ç¯‡è®ºæ–‡ï¼Œä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹...")
        progress_bar = tqdm(as_completed(future_to_paper),
                            total=len(papers), desc="ç²¾æ’è¿›åº¦")
        for future in progress_bar:
            try:
                updated_paper = future.result()
                if updated_paper:
                    analyzed_papers.append(updated_paper)
            except Exception as exc:
                paper_info = future_to_paper[future]
                print(f"å¤„ç†è®ºæ–‡ {paper_info['title']} æ—¶äº§ç”Ÿå¼‚å¸¸: {exc}")
    if not analyzed_papers:
        print("\næ²¡æœ‰æˆåŠŸç²¾æ’ä»»ä½•è®ºæ–‡ã€‚")
        return []
    print(f"\nâœ… æ‰€æœ‰è®ºæ–‡ç²¾æ’å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(analyzed_papers)} ç¯‡ã€‚")
    return analyzed_papers


def fine_rank_papers(papers, max_workers=10, paper_count=5):
    papers = papers[:paper_count]  # åªç²¾æ’å‰ N ç¯‡è®ºæ–‡

    # LLM fine analyze papers concurrently
    analyzed_papers = fine_analyze_papers_cocurrent(
        papers, max_workers=max_workers)

    # æ ¸å¿ƒæ’åºé€»è¾‘ï¼šæŒ‰ rerank_relevance_score é™åºæ’åº
    analyzed_papers.sort(key=lambda p: p.get(
        'rerank_relevance_score', 0), reverse=True)

    # æ‰“å°æ’åºåçš„ç»“æœé¢„è§ˆ
    print("\n--- ç²¾æ’ç»“æœé¢„è§ˆ (æŒ‰ç²¾æ’è¯„åˆ†æ’åº) ---")
    for paper in analyzed_papers:
        print("-" * 60)
        print(f"âœ… [{paper['arxiv_id']}] {paper['title']}")
        print(f"  - ç¿»è¯‘: {paper.get('translation', 'N/A')}")
        print(f"  - ç²¾æ’ç›¸å…³æ€§è¯„åˆ†: {paper.get('rerank_relevance_score', 'N/A')}/10")
        print(f"  - ç†ç”±: {paper.get('rerank_reasoning', 'N/A')}")
        print(f"  - æ‘˜è¦: {paper.get('summary', 'N/A')}")
    print("-" * 60)

    return analyzed_papers


def send_papers_to_feishu(papers, feishu_url=FEISHU_URL):
    
    date = datetime.now().strftime('%Y-%m-%d')
    
    card_data = {
        "type": "template",
        "data": {
            "template_id": "AAqxH62u1uNko",
            "template_version_name": "1.0.5",
            "template_variable": {
                "loop": [],
                "date": date
            }
        }
    }

    for paper in papers:
        title = paper['title']
        translation = paper.get('translation', 'N/A')
        score = paper.get('rerank_relevance_score', 'N/A')
        summary = paper.get('summary', 'N/A')
        url = paper['url']
        
        paper = f"[{title}]({url})"
        score = "â­ï¸" * score + f" <text_tag color='blue'>{score}åˆ†</text_tag>" if isinstance(score, int) else "N/A"
        
        card_data['data']['template_variable']['loop'].append({
            "paper": paper,
            "translation": translation,
            "score": score,
            "summary": summary
        })
        
    card = json.dumps(card_data)
    body = json.dumps({"msg_type": "interactive", "card": card})
    headers = {"Content-Type": "application/json"}
    ret = requests.post(url=feishu_url, data=body, headers=headers)
    print(f"é£ä¹¦æ¨é€è¿”å›çŠ¶æ€: {ret.status_code}")


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    results = {}
    for TARGET_CATEGORY in TARGET_CATEGORYS:
        results.update(get_daily_arxiv_papers(
            category=TARGET_CATEGORY, max_results=MAX_PAPERS))
        time.sleep(3)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
    print(f"å¬å›åˆ° {len(results)} ç¯‡è®ºæ–‡ã€‚")

    filtered_papers = rough_rank_papers(results, filter_threshold=4)
    print(f"ç²—æ’ç­›é€‰ {len(filtered_papers)} ç¯‡é«˜è´¨é‡è®ºæ–‡ã€‚")

    final_papers = fine_rank_papers(filtered_papers, paper_count=20)
    print(f"ç²¾æ’å¾—åˆ° {len(final_papers)} ç¯‡é¡¶çº§è®ºæ–‡ã€‚")

    send_papers_to_feishu(final_papers)
    
    print("æ‰€æœ‰æ“ä½œå®Œæˆï¼")