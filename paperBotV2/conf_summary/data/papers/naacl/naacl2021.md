# NAACL2021

## 会议论文列表

本会议共有 552 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages](https://doi.org/10.18653/v1/2021.naacl-industry.1) |  | 0 | Email and chat communication tools are increasingly important for completing daily tasks. Accurate real-time phrase completion can save time and bolster productivity. Modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. We examine how additional contextual signals (from previous messages, time, and subject) affect the performance of a commercial text prediction model. We compare contextual text... | Stojan Trajanovski, Chad Atalla, Kunho Kim, Vipul Agarwal, Milad Shokouhi, Chris Quirk |  |
| 2 |  |  [Identifying and Resolving Annotation Changes for Natural Language Understanding](https://doi.org/10.18653/v1/2021.naacl-industry.2) |  | 0 | Annotation conflict resolution is crucial towards building machine learning models with acceptable performance. Past work on annotation conflict resolution had assumed that data is collected at once, with a fixed set of annotators and fixed annotation guidelines. Moreover, previous work dealt with atomic labeling tasks. In this paper, we address annotation conflict resolution for Natural Language Understanding (NLU), a structured prediction task, in a real-world setting of commercial... | Jose Garrido Ramas, Giorgio Pessot, Abdalghani Abujabal, Martin Rajman |  |
| 3 |  |  [Optimizing NLU Reranking Using Entity Resolution Signals in Multi-domain Dialog Systems](https://doi.org/10.18653/v1/2021.naacl-industry.3) |  | 0 | In dialog systems, the Natural Language Understanding (NLU) component typically makes the interpretation decision (including domain, intent and slots) for an utterance before the mentioned entities are resolved. This may result in intent classification and slot tagging errors. In this work, we propose to leverage Entity Resolution (ER) features in NLU reranking and introduce a novel loss term based on ER signals to better learn model weights in the reranking framework. In addition, for a... | Tong Wang, Jiangning Chen, Mohsen Malmir, Shuyan Dong, Xin He, Han Wang, Chengwei Su, Yue Liu, Yang Liu |  |
| 4 |  |  [Entity Resolution in Open-domain Conversations](https://doi.org/10.18653/v1/2021.naacl-industry.4) |  | 0 | In recent years, incorporating external knowledge for response generation in open-domain conversation systems has attracted great interest. To improve the relevancy of retrieved knowledge, we propose a neural entity linking (NEL) approach. Different from formal documents, such as news, conversational utterances are informal and multi-turn, which makes it more challenging to disambiguate the entities. Therefore, we present a context-aware named entity recognition model (NER) and entity... | Mingyue Shang, Tong Wang, Mihail Eric, Jiangning Chen, Jiyang Wang, Matthew Welch, Tiantong Deng, Akshay Grewal, Han Wang, Yue Liu, Yang Liu, Dilek HakkaniTür |  |
| 5 |  |  [Pretrain-Finetune Based Training of Task-Oriented Dialogue Systems in a Real-World Setting](https://doi.org/10.18653/v1/2021.naacl-industry.5) |  | 0 | One main challenge in building task-oriented dialogue systems is the limited amount of supervised training data available. In this work, we present a method for training retrieval-based dialogue systems using a small amount of high-quality, annotated data and a larger, unlabeled dataset. We show that pretraining using unlabeled data can bring better model performance with a 31% boost in Recall@1 compared with no pretraining. The proposed finetuning technique based on a small amount of... | Manisha Srivastava, Yichao Lu, Riley Peschon, Chenyang Li |  |
| 6 |  |  [Contextual Domain Classification with Temporal Representations](https://doi.org/10.18653/v1/2021.naacl-industry.6) |  | 0 | In commercial dialogue systems, the Spoken Language Understanding (SLU) component tends to have numerous domains thus context is needed to help resolve ambiguities. Previous works that incorporate context for SLU have mostly focused on domains where context is limited to a few minutes. However, there are domains that have related context that could span up to hours and days. In this paper, we propose temporal representations that combine wall-clock second difference and turn order offset... | TzuHsiang Lin, Yipeng Shi, Chentao Ye, Yang Fan, Weitong Ruan, Emre Barut, Wael Hamza, Chengwei Su |  |
| 7 |  |  [Bootstrapping a Music Voice Assistant with Weak Supervision](https://doi.org/10.18653/v1/2021.naacl-industry.7) |  | 0 | One of the first building blocks to create a voice assistant relates to the task of tagging entities or attributes in user queries. This can be particularly challenging when entities are in the tenth of millions, as is the case of e.g. music catalogs. Training slot tagging models at an industrial scale requires large quantities of accurately labeled user queries, which are often hard and costly to gather. On the other hand, voice assistants typically collect plenty of unlabeled queries that... | Sergio Oramas, Massimo Quadrana, Fabien Gouyon |  |
| 8 |  |  [Continuous Model Improvement for Language Understanding with Machine Translation](https://doi.org/10.18653/v1/2021.naacl-industry.8) |  | 0 | Scaling conversational personal assistants to a multitude of languages puts high demands on collecting and labelling data, a setting in which cross-lingual learning techniques can help to reconcile the need for well-performing Natural Language Understanding (NLU) with a desideratum to support many languages without incurring unacceptable cost. In this work, we show that automatically annotating unlabeled utterances using Machine Translation in an offline fashion and adding them to the training... | Abdalghani Abujabal, Claudio Delli Bovi, Sungho Ryu, Turan Gojayev, Fabian Triefenbach, Yannick Versley |  |
| 9 |  |  [A Hybrid Approach to Scalable and Robust Spoken Language Understanding in Enterprise Virtual Agents](https://doi.org/10.18653/v1/2021.naacl-industry.9) |  | 0 | Spoken language understanding (SLU) extracts the intended mean- ing from a user utterance and is a critical component of conversational virtual agents. In enterprise virtual agents (EVAs), language understanding is substantially challenging. First, the users are infrequent callers who are unfamiliar with the expectations of a pre-designed conversation flow. Second, the users are paying customers of an enterprise who demand a reliable, consistent and efficient user experience when resolving... | Ryan Price, Mahnoosh Mehrabani, Narendra Gupta, YeonJun Kim, Shahab Jalalvand, Minhua Chen, Yanjie Zhao, Srinivas Bangalore |  |
| 10 |  |  [Proteno: Text Normalization with Limited Data for Fast Deployment in Text to Speech Systems](https://doi.org/10.18653/v1/2021.naacl-industry.10) |  | 0 | Developing Text Normalization (TN) systems for Text-to-Speech (TTS) on new languages is hard. We propose a novel architecture to facilitate it for multiple languages while using data less than 3% of the size of the data used by the state of the art results on English. We treat TN as a sequence classification problem and propose a granular tokenization mechanism that enables the system to learn majority of the classes and their normalizations from the training data itself. This is further... | Shubhi Tyagi, Antonio Bonafonte, Jaime LorenzoTrueba, Javier Latorre |  |
| 11 |  |  [Addressing the Vulnerability of NMT in Input Perturbations](https://doi.org/10.18653/v1/2021.naacl-industry.11) |  | 0 | Neural Machine Translation (NMT) has achieved significant breakthrough in performance but is known to suffer vulnerability to input perturbations. As real input noise is difficult to predict during training, robustness is a big issue for system deployment. In this paper, we improve the robustness of NMT models by reducing the effect of noisy words through a Context-Enhanced Reconstruction (CER) approach. CER trains the model to resist noise in two steps: (1) perturbation step that breaks the... | Weiwen Xu, Ai Ti Aw, Yang Ding, Kui Wu, Shafiq R. Joty |  |
| 12 |  |  [Cross-lingual Supervision Improves Unsupervised Neural Machine Translation](https://doi.org/10.18653/v1/2021.naacl-industry.12) |  | 0 | We propose to improve unsupervised neural machine translation with cross-lingual supervision (), which utilizes supervision signals from high resource language pairs to improve the translation of zero-source languages. Specifically, for training En-Ro system without parallel corpus, we can leverage the corpus from En-Fr and En-De to collectively train the translation from one language into many languages under one model. % is based on multilingual models which require no changes to the standard... | Mingxuan Wang, Hongxiao Bai, Lei Li, Hai Zhao |  |
| 13 |  |  [Should we find another model?: Improving Neural Machine Translation Performance with ONE-Piece Tokenization Method without Model Modification](https://doi.org/10.18653/v1/2021.naacl-industry.13) |  | 0 | Most of the recent Natural Language Processing(NLP) studies are based on the Pretrain-Finetuning Approach (PFA), but in small and medium-sized enterprises or companies with insufficient hardware there are many limitations to servicing NLP application software using such technology due to slow speed and insufficient memory. The latest PFA technologies require large amounts of data, especially for low-resource languages, making them much more difficult to work with. We propose a new tokenization... | Chanjun Park, Sugyeong Eo, Hyeonseok Moon, Heuiseok Lim |  |
| 14 |  |  [Autocorrect in the Process of Translation - Multi-task Learning Improves Dialogue Machine Translation](https://doi.org/10.18653/v1/2021.naacl-industry.14) |  | 0 | Automatic translation of dialogue texts is a much needed demand in many real life scenarios. However, the currently existing neural machine translation delivers unsatisfying results. In this paper, we conduct a deep analysis of a dialogue corpus and summarize three major issues on dialogue translation, including pronoun dropping (), punctuation dropping (), and typos (). In response to these challenges, we propose a joint learning method to identify omission and typo, and utilize context to... | Tao Wang, Chengqi Zhao, Mingxuan Wang, Lei Li, Deyi Xiong |  |
| 15 |  |  [LightSeq: A High Performance Inference Library for Transformers](https://doi.org/10.18653/v1/2021.naacl-industry.15) |  | 0 | Transformer and its variants have achieved great success in natural language processing. Since Transformer models are huge in size, serving these models is a challenge for real industrial applications. In this paper, we propose , a highly efficient inference library for models in the Transformer family. includes a series of GPU optimization techniques to both streamline the computation of Transformer layers and reduce memory footprint. supports models trained using PyTorch and Tensorflow.... | Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, Lei Li |  |
| 16 |  |  [Practical Transformer-based Multilingual Text Classification](https://doi.org/10.18653/v1/2021.naacl-industry.16) |  | 0 | Transformer-based methods are appealing for multilingual text classification, but common research benchmarks like XNLI (Conneau et al., 2018) do not reflect the data availability and task variety of industry applications. We present an empirical comparison of transformer-based text classification models in a variety of practical monolingual and multilingual pretraining and fine-tuning settings. We evaluate these methods on two distinct tasks in five different languages. Departing from prior... | Cindy Wang, Michele Banko |  |
| 17 |  |  [An Emotional Comfort Framework for Improving User Satisfaction in E-Commerce Customer Service Chatbots](https://doi.org/10.18653/v1/2021.naacl-industry.17) |  | 0 | E-commerce has grown substantially over the last several years, and chatbots for intelligent customer service are concurrently drawing attention. We presented AliMe Assist, a Chinese intelligent assistant designed for creating an innovative online shopping experience in E-commerce. Based on question answering (QA), AliMe Assist offers assistance service, customer service, and chatting service. According to the survey of user studies and the real online testing, emotional comfort of customers’... | Shuangyong Song, Chao Wang, Haiqing Chen, Huan Chen |  |
| 18 |  |  [Language Scaling for Universal Suggested Replies Model](https://doi.org/10.18653/v1/2021.naacl-industry.18) |  | 0 | We consider the problem of scaling automated suggested replies for a commercial email application to multiple languages. Faced with increased compute requirements and low language resources for language expansion, we build a single universal model for improving the quality and reducing run-time costs of our production system. However, restricted data movement across regional centers prevents joint training across languages. To this end, we propose a multi-lingual multi-task continual learning... | Qianlan Ying, Payal Bajaj, Budhaditya Deb, Yu Yang, Wei Wang, Bojia Lin, Milad Shokouhi, Xia Song, Yang Yang, Daxin Jiang |  |
| 19 |  |  [Graph-based Multilingual Product Retrieval in E-Commerce Search](https://doi.org/10.18653/v1/2021.naacl-industry.19) |  | 0 | Nowadays, with many e-commerce platforms conducting global business, e-commerce search systems are required to handle product retrieval under multilingual scenarios. Moreover, comparing with maintaining per-country specific e-commerce search systems, having an universal system across countries can further reduce the operational and computational costs, and facilitate business expansion to new countries. In this paper, we introduce an universal end-to-end multilingual retrieval system, and... | Hanqing Lu, Youna Hu, Tong Zhao, Tony Wu, Yiwei Song, Bing Yin |  |
| 20 |  |  [Query2Prod2Vec: Grounded Word Embeddings for eCommerce](https://doi.org/10.18653/v1/2021.naacl-industry.20) |  | 0 | We present Query2Prod2Vec, a model that grounds lexical representations for product search in product embeddings: in our model, meaning is a mapping between words and a latent space of products in a digital shop. We leverage shopping sessions to learn the underlying space and use merchandising annotations to build lexical analogies for evaluation: our experiments show that our model is more accurate than known techniques from the NLP and IR literature. Finally, we stress the importance of data... | Federico Bianchi, Jacopo Tagliabue, Bingqing Yu |  |
| 21 |  |  [An Architecture for Accelerated Large-Scale Inference of Transformer-Based Language Models](https://doi.org/10.18653/v1/2021.naacl-industry.21) |  | 0 | This work demonstrates the development process of a machine learning architecture for inference that can scale to a large volume of requests. We used a BERT model that was fine-tuned for emotion analysis, returning a probability distribution of emotions given a paragraph. The model was deployed as a gRPC service on Kubernetes. Apache Spark was used to perform inference in batches by calling the service. We encountered some performance and concurrency challenges and created solutions to achieve... | Amir Ganiev, Colton Chapin, Anderson de Andrade, Chen Liu |  |
| 22 |  |  [When and Why a Model Fails? A Human-in-the-loop Error Detection Framework for Sentiment Analysis](https://doi.org/10.18653/v1/2021.naacl-industry.22) |  | 0 | Although deep neural networks have been widely employed and proven effective in sentiment analysis tasks, it remains challenging for model developers to assess their models for erroneous predictions that might exist prior to deployment. Once deployed, emergent errors can be hard to identify in prediction run-time and impossible to trace back to their sources. To address such gaps, in this paper we propose an error detection framework for sentiment analysis based on explainable features. We... | Zhe Liu, Yufan Guo, Jalal Mahmud |  |
| 23 |  |  [Technical Question Answering across Tasks and Domains](https://doi.org/10.18653/v1/2021.naacl-industry.23) |  | 0 | Building automatic technical support system is an important yet challenge task. Conceptually, to answer a user question on a technical forum, a human expert has to first retrieve relevant documents, and then read them carefully to identify the answer snippet. Despite huge success the researchers have achieved in coping with general domain question answering (QA), much less attentions have been paid for investigating technical QA. Specifically, existing methods suffer from several unique... | Wenhao Yu, Lingfei Wu, Yu Deng, Qingkai Zeng, Ruchi Mahindru, Sinem Güven, Meng Jiang |  |
| 24 |  |  [Cost-effective Deployment of BERT Models in Serverless Environment](https://doi.org/10.18653/v1/2021.naacl-industry.24) |  | 0 | In this study, we demonstrate the viability of deploying BERT-style models to AWS Lambda in a production environment. Since the freely available pre-trained models are too large to be deployed in this environment, we utilize knowledge distillation and fine-tune the models on proprietary datasets for two real-world tasks: sentiment analysis and semantic textual similarity. As a result, we obtain models that are tuned for a specific domain and deployable in the serverless environment. The... | Marek Suppa, Katarína Benesová, Andrej Svec |  |
| 25 |  |  [Noise Robust Named Entity Understanding for Voice Assistants](https://doi.org/10.18653/v1/2021.naacl-industry.25) |  | 0 | Named Entity Recognition (NER) and Entity Linking (EL) play an essential role in voice assistant interaction, but are challenging due to the special difficulties associated with spoken user queries. In this paper, we propose a novel architecture that jointly solves the NER and EL tasks by combining them in a joint reranking module. We show that our proposed framework improves NER accuracy by up to 3.13% and EL accuracy by up to 3.6% in F1 score. The features used also lead to better accuracies... | Deepak Muralidharan, Joel Ruben Antony Moniz, Sida Gao, Xiao Yang, Justine T. Kao, Stephen Pulman, Atish Kothari, Ray Shen, Yinying Pan, Vivek Kaul, Mubarak Seyed Ibrahim, Gang Xiang, Nan Dun, Yidan Zhou, Andy O, Yuan Zhang, Pooja Chitkara, Xuan Wang, Alkesh Patel, Kushal Tayal, Roger Zheng, Peter Grasch, Jason D. Williams, Lin Li |  |
| 26 |  |  [Goodwill Hunting: Analyzing and Repurposing Off-the-Shelf Named Entity Linking Systems](https://doi.org/10.18653/v1/2021.naacl-industry.26) |  | 0 | Named entity linking (NEL) or mapping “strings” to “things” in a knowledge base is a fundamental preprocessing step in systems that require knowledge of entities such as information extraction and question answering. In this work, we lay out and investigate two challenges faced by individuals or organizations building NEL systems. Can they directly use an off-the-shelf system? If not, how easily can such a system be repurposed for their use case? First, we conduct a study of off-the-shelf... | Karan Goel, Laurel J. Orr, Nazneen Fatema Rajani, Jesse Vig, Christopher Ré |  |
| 27 |  |  [Intent Features for Rich Natural Language Understanding](https://doi.org/10.18653/v1/2021.naacl-industry.27) |  | 0 | Complex natural language understanding modules in dialog systems have a richer understanding of user utterances, and thus are critical in providing a better user experience. However, these models are often created from scratch, for specific clients and use cases and require the annotation of large datasets. This encourages the sharing of annotated data across multiple clients. To facilitate this we introduce the idea of intent features: domain and topic agnostic properties of intents that can... | Brian Lester, Sagnik Ray Choudhury, Rashmi Prasad, Srinivas Bangalore |  |
| 28 |  |  [Development of an Enterprise-Grade Contract Understanding System](https://doi.org/10.18653/v1/2021.naacl-industry.28) |  | 0 | Contracts are arguably the most important type of business documents. Despite their significance in business, legal contract review largely remains an arduous, expensive and manual process. In this paper, we describe TECUS: a commercial system designed and deployed for contract understanding and used by a wide range of enterprise users for the past few years. We reflect on the challenges and design decisions when building TECUS. We also summarize the data science life cycle of TECUS and share... | Arvind Agarwal, Laura Chiticariu, Poornima Chozhiyath Raman, Marina Danilevsky, Diman Ghazi, Ankush Gupta, Shanmukha C. Guttula, Yannis Katsis, Rajasekar Krishnamurthy, Yunyao Li, Shubham Mudgal, Vitobha Munigala, Nicholas Phan, Dhaval Sonawane, Sneha Srinivasan, Sudarshan R. Thitte, Mitesh Vasa, Ramiya Venkatachalam, Vinitha Yaski, Huaiyu Zhu |  |
| 29 |  |  [Discovering Better Model Architectures for Medical Query Understanding](https://doi.org/10.18653/v1/2021.naacl-industry.29) |  | 0 | In developing an online question-answering system for the medical domains, natural language inference (NLI) models play a central role in question matching and intention detection. However, which models are best for our datasets? Manually selecting or tuning a model is time-consuming. Thus we experiment with automatically optimizing the model architectures on the task at hand via neural architecture search (NAS). First, we formulate a novel architecture search space based on the previous NAS... | Wei Zhu, Yuan Ni, Xiaoling Wang, Guotong Xie |  |
| 30 |  |  [OodGAN: Generative Adversarial Network for Out-of-Domain Data Generation](https://doi.org/10.18653/v1/2021.naacl-industry.30) |  | 0 | Detecting an Out-of-Domain (OOD) utterance is crucial for a robust dialog system. Most dialog systems are trained on a pool of annotated OOD data to achieve this goal. However, collecting the annotated OOD data for a given domain is an expensive process. To mitigate this issue, previous works have proposed generative adversarial networks (GAN) based models to generate OOD data for a given domain automatically. However, these proposed models do not work directly with the text. They work with the... | Petr Marek, Vishal Ishwar Naik, Anuj Goyal, Vincent Auvray |  |
| 31 |  |  [Coherent and Concise Radiology Report Generation via Context Specific Image Representations and Orthogonal Sentence States](https://doi.org/10.18653/v1/2021.naacl-industry.31) |  | 0 | Neural models for text generation are often designed in an end-to-end fashion, typically with zero control over intermediate computations, limiting their practical usability in downstream applications. In this work, we incorporate explicit means into neural models to ensure topical continuity, informativeness and content diversity of generated radiology reports. For the purpose we propose a method to compute image representations specific to each sentential context and eliminate redundant... | Litton J. Kurisinkel, Ai Ti Aw, Nancy F. Chen |  |
| 32 |  |  [An Empirical Study of Generating Texts for Search Engine Advertising](https://doi.org/10.18653/v1/2021.naacl-industry.32) |  | 0 | Although there are many studies on neural language generation (NLG), few trials are put into the real world, especially in the advertising domain. Generating ads with NLG models can help copywriters in their creation. However, few studies have adequately evaluated the effect of generated ads with actual serving included because it requires a large amount of training data and a particular environment. In this paper, we demonstrate a practical use case of generating ad-text with an NLG model.... | Hidetaka Kamigaito, Peinan Zhang, Hiroya Takamura, Manabu Okumura |  |
| 33 |  |  [Ad Headline Generation using Self-Critical Masked Language Model](https://doi.org/10.18653/v1/2021.naacl-industry.33) |  | 0 | For any E-commerce website it is a nontrivial problem to build enduring advertisements that attract shoppers. It is hard to pass the creative quality bar of the website, especially at a large scale. We thus propose a programmatic solution to generate product advertising headlines using retail content. We propose a state of the art application of Reinforcement Learning (RL) Policy gradient methods on Transformer (Vaswani et al., 2017) based Masked Language Models (Devlin et al., 2019). Our... | Yashal Shakti Kanungo, Sumit Negi, Aruna Rajan |  |
| 34 |  |  [LATEX-Numeric: Language Agnostic Text Attribute Extraction for Numeric Attributes](https://doi.org/10.18653/v1/2021.naacl-industry.34) |  | 0 | In this paper, we present LATEX-Numeric - a high-precision fully-automated scalable framework for extracting E-commerce numeric attributes from unstructured product text like product description. Most of the past work on attribute extraction is not scalable as they rely on manually curated training data, either with or without use of active learning. We rely on distant supervision for training data generation, removing dependency on manual labels. One issue with distant supervision is that it... | Kartik Mehta, Ioana Oprea, Nikhil Rasiwasia |  |
| 35 |  |  [Training Language Models under Resource Constraints for Adversarial Advertisement Detection](https://doi.org/10.18653/v1/2021.naacl-industry.35) |  | 0 | Advertising on e-commerce and social media sites deliver ad impressions at web scale on a daily basis driving value to both shoppers and advertisers. This scale necessitates programmatic ways of detecting unsuitable content in ads to safeguard customer experience and trust. This paper focusses on techniques for training text classification models under resource constraints, built as part of automated solutions for advertising content moderation. We show how weak supervision, curriculum learning... | Eshwar Shamanna Girishekar, Shiv Surya, Nishant Nikhil, Dyut Kumar Sil, Sumit Negi, Aruna Rajan |  |
| 36 |  |  [Combining Weakly Supervised ML Techniques for Low-Resource NLU](https://doi.org/10.18653/v1/2021.naacl-industry.36) |  | 0 | Recent advances in transfer learning have improved the performance of virtual assistants considerably. Nevertheless, creating sophisticated voice-enabled applications for new domains remains a challenge, and meager training data is often a key bottleneck. Accordingly, unsupervised learning and SSL (semi-supervised learning) techniques continue to be of vital importance. While a number of such methods have been explored previously in isolation, in this paper we investigate the synergistic use of... | Victor Soto, Konstantine Arkoudas |  |
| 37 |  |  [Label-Guided Learning for Item Categorization in e-Commerce](https://doi.org/10.18653/v1/2021.naacl-industry.37) |  | 0 | Item categorization is an important application of text classification in e-commerce due to its impact on the online shopping experience of users. One class of text classification techniques that has gained attention recently is using the semantic information of the labels to guide the classification task. We have conducted a systematic investigation of the potential benefits of these methods on a real data set from a major e-commerce company in Japan. Furthermore, using a hyperbolic space to... | Lei Chen, Hirokazu Miyake |  |
| 38 |  |  [Benchmarking Commercial Intent Detection Services with Practice-Driven Evaluations](https://doi.org/10.18653/v1/2021.naacl-industry.38) |  | 0 | Intent detection is a key component of modern goal-oriented dialog systems that accomplish a user task by predicting the intent of users’ text input. There are three primary challenges in designing robust and accurate intent detection models. First, typical intent detection models require a large amount of labeled data to achieve high accuracy. Unfortunately, in practical scenarios it is more common to find small, unbalanced, and noisy datasets. Secondly, even with large training data, the... | Haode Qi, Lin Pan, Atin Sood, Abhishek Shah, Ladislav Kunc, Mo Yu, Saloni Potdar |  |
| 39 |  |  [Industry Scale Semi-Supervised Learning for Natural Language Understanding](https://doi.org/10.18653/v1/2021.naacl-industry.39) |  | 0 | This paper presents a production Semi-Supervised Learning (SSL) pipeline based on the student-teacher framework, which leverages millions of unlabeled examples to improve Natural Language Understanding (NLU) tasks. We investigate two questions related to the use of unlabeled data in production SSL context: 1) how to select samples from a huge unlabeled data pool that are beneficial for SSL training, and 2) how does the selected data affect the performance of different state-of-the-art SSL... | Luoxin Chen, Francisco Garcia, Varun Kumar, He Xie, Jianhua Lu |  |
| 40 |  |  [Sampling and Filtering of Neural Machine Translation Distillation Data](https://doi.org/10.18653/v1/2021.naacl-srw.1) |  | 0 | In most of neural machine translation distillation or stealing scenarios, the highest-scoring hypothesis of the target model (teacher) is used to train a new model (student). If reference translations are also available, then better hypotheses (with respect to the references) can be oversampled and poor hypotheses either removed or undersampled. This paper explores the sampling method landscape (pruning, hypothesis oversampling and undersampling, deduplication and their combination) with... | Vilém Zouhar |  |
| 41 |  |  [IceSum: An Icelandic Text Summarization Corpus](https://doi.org/10.18653/v1/2021.naacl-srw.2) |  | 0 | Automatic Text Summarization (ATS) is the task of generating concise and fluent summaries from one or more documents. In this paper, we present IceSum, the first Icelandic corpus annotated with human-generated summaries. IceSum consists of 1,000 online news articles and their extractive summaries. We train and evaluate several neural network-based models on this dataset, comparing them against a selection of baseline methods. We find that an encoder-decoder model with a sequence-to-sequence... | Jón Daðason, Hrafn Loftsson, Salome Sigurdardóttir, Torsteinn Björnsson |  |
| 42 |  |  [Negation typology and general representation models for cross-lingual zero-shot negation scope resolution in Russian, French, and Spanish](https://doi.org/10.18653/v1/2021.naacl-srw.3) |  | 0 | Negation is a linguistic universal that poses difficulties for cognitive and computational processing. Despite many advances in text analytics, negation resolution remains an acute and continuously researched question in Natural Language Processing. Reliable negation parsing affects results in biomedical text mining, sentiment analysis, machine translation, and many other fields. The availability of multilingual pre-trained general representation models makes it possible to experiment with... | Anastassia Shaitarova, Fabio Rinaldi |  |
| 43 |  |  [Representations of Meaning in Neural Networks for NLP: a Thesis Proposal](https://doi.org/10.18653/v1/2021.naacl-srw.4) |  | 0 | Neural networks are the state-of-the-art method of machine learning for many problems in NLP. Their success in machine translation and other NLP tasks is phenomenal, but their interpretability is challenging. We want to find out how neural networks represent meaning. In order to do this, we propose to examine the distribution of meaning in the vector space representation of words in neural networks trained for NLP tasks. Furthermore, we propose to consider various theories of meaning in the... | Tomás Musil |  |
| 44 |  |  [Towards Layered Events and Schema Representations in Long Documents](https://doi.org/10.18653/v1/2021.naacl-srw.5) |  | 0 | In this thesis proposal, we explore the application of event extraction to literary texts. Considering the lengths of literary documents modeling events in different granularities may be more adequate to extract meaningful information, as individual elements contribute little to the overall semantics. We adapt the concept of schemas as sequences of events all describing a single process, connected through shared participants extending it to for multiple schemas in a document. Segmentation of... | Hans Ole Hatzel, Chris Biemann |  |
| 45 |  |  [Parallel Text Alignment and Monolingual Parallel Corpus Creation from Philosophical Texts for Text Simplification](https://doi.org/10.18653/v1/2021.naacl-srw.6) |  | 0 | Text simplification is a growing field with many potential useful applications. Training text simplification algorithms generally requires a lot of annotated data, however there are not many corpora suitable for this task. We propose a new unsupervised method for aligning text based on Doc2Vec embeddings and a new alignment algorithm, capable of aligning texts at different levels. Initial evaluation shows promising results for the new approach. We used the newly developed approach to create a... | Stefan Paun |  |
| 46 |  |  [Syntax-Based Attention Masking for Neural Machine Translation](https://doi.org/10.18653/v1/2021.naacl-srw.7) |  | 0 | We present a simple method for extending transformers to source-side trees. We define a number of masks that limit self-attention based on relationships among tree nodes, and we allow each attention head to learn which mask or masks to use. On translation from English to various low-resource languages, and translation in both directions between English and German, our method always improves over simple linearization of the source-side parse tree and almost always improves over a... | Colin McDonald, David Chiang |  |
| 47 |  |  [Multi-Modal Image Captioning for the Visually Impaired](https://doi.org/10.18653/v1/2021.naacl-srw.8) |  | 0 | One of the ways blind people understand their surroundings is by clicking images and relying on descriptions generated by image-captioning systems. Current work on captioning images for the visually impaired do not use the textual data present in the image when generating captions. This problem is critical as many visual scenes contain text, and 21% of the questions asked by blind people about the images they click pertain to the text present in them. In this work, we propose altering AoANet, a... | Hiba Ahsan, Daivat Bhatt, Kaivankumar Shah, Nikita Bhalla |  |
| 48 |  |  [Open-Domain Question Answering with Pre-Constructed Question Spaces](https://doi.org/10.18653/v1/2021.naacl-srw.9) |  | 0 | Open-domain question answering aims at locating the answers to user-generated questions in massive collections of documents. Retriever-readers and knowledge graph approaches are two big families of solutions to this task. A retriever-reader first applies information retrieval techniques to locate a few passages that are likely to be relevant, and then feeds the retrieved text to a neural network reader to extract the answer. Alternatively, knowledge graphs can be constructed and queried to... | Jinfeng Xiao, Lidan Wang, Franck Dernoncourt, Trung Bui, Tong Sun, Jiawei Han |  |
| 49 |  |  [A Sliding-Window Approach to Automatic Creation of Meeting Minutes](https://doi.org/10.18653/v1/2021.naacl-srw.10) |  | 0 | Meeting minutes record any subject matter discussed, decisions reached and actions taken at the meeting. The importance of automatic minuting cannot be overstated. In this paper, we present a sliding window approach to automatic generation of meeting minutes. It aims at addressing issues pertaining to the nature of spoken text, including the lengthy transcript and lack of document structure, which make it difficult to identify salient content to be included in meeting minutes. Our approach... | Jia Jin Koay, Alexander Roustai, Xiaojin Dai, Fei Liu |  |
| 50 |  |  [Exploration and Discovery of the COVID-19 Literature through Semantic Visualization](https://doi.org/10.18653/v1/2021.naacl-srw.11) |  | 0 | We propose semantic visualization as a linguistic visual analytic method. It can enable exploration and discovery over large datasets of complex networks by exploiting the semantics of the relations in them. This involves extracting information, applying parameter reduction operations, building hierarchical data representation and designing visualization. We also present the accompanying COVID-SemViz a searchable and interactive visualization system for knowledge exploration of COVID-19 data to... | Jingxuan Tu, Marc Verhagen, Brent Cochran, James Pustejovsky |  |
| 51 |  |  [Shuffled-token Detection for Refining Pre-trained RoBERTa](https://doi.org/10.18653/v1/2021.naacl-srw.12) |  | 0 | State-of-the-art transformer models have achieved robust performance on a variety of NLP tasks. Many of these approaches have employed domain agnostic pre-training tasks to train models that yield highly generalized sentence representations that can be fine-tuned for specific downstream tasks. We propose refining a pre-trained NLP model using the objective of detecting shuffled tokens. We use a sequential approach by starting with the pre-trained RoBERTa model and training it using our... | Subhadarshi Panda, Anjali Agrawal, Jeewon Ha, Benjamin Bloch |  |
| 52 |  |  [Morphology-Aware Meta-Embeddings for Tamil](https://doi.org/10.18653/v1/2021.naacl-srw.13) |  | 0 | In this work, we explore generating morphologically enhanced word embeddings for Tamil, a highly agglutinative South Indian language with rich morphology that remains low-resource with regards to NLP tasks. We present here the first-ever word analogy dataset for Tamil, consisting of 4499 hand-curated word tetrads across 10 semantic and 13 morphological relation types. Using a rules-based segmenter to capture morphology as well as meta-embedding techniques, we train meta-embeddings that... | Arjun Sai Krishnan, Seyoon Ragavan |  |
| 53 |  |  [Seed Word Selection for Weakly-Supervised Text Classification with Unsupervised Error Estimation](https://doi.org/10.18653/v1/2021.naacl-srw.14) |  | 0 | Weakly-supervised text classification aims to induce text classifiers from only a few user-provided seed words. The vast majority of previous work assumes high-quality seed words are given. However, the expert-annotated seed words are sometimes non-trivial to come up with. Furthermore, in the weakly-supervised learning setting, we do not have any labeled document to measure the seed words’ efficacy, making the seed word selection process “a walk in the dark”. In this work, we remove the need... | Yiping Jin, Akshay Bhatia, Dittaya Wanvarie |  |
| 54 |  |  [Multi-Task Learning of Generation and Classification for Emotion-Aware Dialogue Response Generation](https://doi.org/10.18653/v1/2021.naacl-srw.15) |  | 0 | For a computer to naturally interact with a human, it needs to be human-like. In this paper, we propose a neural response generation model with multi-task learning of generation and classification, focusing on emotion. Our model based on BART (Lewis et al., 2020), a pre-trained transformer encoder-decoder model, is trained to generate responses and recognize emotions simultaneously. Furthermore, we weight the losses for the tasks to control the update of parameters. Automatic evaluations and... | Tatsuya Ide, Daisuke Kawahara |  |
| 55 |  |  [Comparison of Grammatical Error Correction Using Back-Translation Models](https://doi.org/10.18653/v1/2021.naacl-srw.16) |  | 0 | Grammatical error correction (GEC) suffers from a lack of sufficient parallel data. Studies on GEC have proposed several methods to generate pseudo data, which comprise pairs of grammatical and artificially produced ungrammatical sentences. Currently, a mainstream approach to generate pseudo data is back-translation (BT). Most previous studies using BT have employed the same architecture for both the GEC and BT models. However, GEC models have different correction tendencies depending on the... | Aomi Koyama, Kengo Hotate, Masahiro Kaneko, Mamoru Komachi |  |
| 56 |  |  [Parallel sentences mining with transfer learning in an unsupervised setting](https://doi.org/10.18653/v1/2021.naacl-srw.17) |  | 0 | The quality and quantity of parallel sentences are known as very important training data for constructing neural machine translation (NMT) systems. However, these resources are not available for many low-resource language pairs. Many existing methods need strong supervision are not suitable. Although several attempts at developing unsupervised models, they ignore the language-invariant between languages. In this paper, we propose an approach based on transfer learning to mine parallel sentences... | Yu Sun, ShaoLin Zhu, Feng Yifan, Chenggang Mi |  |
| 57 |  |  [Sentence Concatenation Approach to Data Augmentation for Neural Machine Translation](https://doi.org/10.18653/v1/2021.naacl-srw.18) |  | 0 | Recently, neural machine translation is widely used for its high translation accuracy, but it is also known to show poor performance at long sentence translation. Besides, this tendency appears prominently for low resource languages. We assume that these problems are caused by long sentences being few in the train data. Therefore, we propose a data augmentation method for handling long sentences. Our method is simple; we only use given parallel corpora as train data and generate long sentences... | Seiichiro Kondo, Kengo Hotate, Tosho Hirasawa, Masahiro Kaneko, Mamoru Komachi |  |
| 58 |  |  [Emotion Classification in a Resource Constrained Language Using Transformer-based Approach](https://doi.org/10.18653/v1/2021.naacl-srw.19) |  | 0 | Although research on emotion classification has significantly progressed in high-resource languages, it is still infancy for resource-constrained languages like Bengali. However, unavailability of necessary language processing tools and deficiency of benchmark corpora makes the emotion classification task in Bengali more challenging and complicated. This work proposes a transformer-based technique to classify the Bengali text into one of the six basic emotions: anger, fear, disgust, sadness,... | Avishek Das, Omar Sharif, Mohammed Moshiul Hoque, Iqbal H. Sarker |  |
| 59 |  |  [Hie-BART: Document Summarization with Hierarchical BART](https://doi.org/10.18653/v1/2021.naacl-srw.20) |  | 0 | This paper proposes a new abstractive document summarization model, hierarchical BART (Hie-BART), which captures hierarchical structures of a document (i.e., sentence-word structures) in the BART model. Although the existing BART model has achieved a state-of-the-art performance on document summarization tasks, the model does not have the interactions between sentence-level information and word-level information. In machine translation tasks, the performance of neural machine translation models... | Kazuki Akiyama, Akihiro Tamura, Takashi Ninomiya |  |
| 60 |  |  [PhoNLP: A joint multi-task learning model for Vietnamese part-of-speech tagging, named entity recognition and dependency parsing](https://doi.org/10.18653/v1/2021.naacl-demos.1) |  | 0 | We present the first multi-task learning model – named PhoNLP – for joint Vietnamese part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing. Experiments on Vietnamese benchmark datasets show that PhoNLP produces state-of-the-art results, outperforming a single-task learning approach that fine-tunes the pre-trained Vietnamese language model PhoBERT (Nguyen and Nguyen, 2020) for each task independently. We publicly release PhoNLP as an open-source toolkit under the... | Linh The Nguyen, Dat Quoc Nguyen |  |
| 61 |  |  [Machine-Assisted Script Curation](https://doi.org/10.18653/v1/2021.naacl-demos.2) |  | 0 | We describe Machine-Aided Script Curator (MASC), a system for human-machine collaborative script authoring. Scripts produced with MASC include (1) English descriptions of sub-events that comprise a larger, complex event; (2) event types for each of those events; (3) a record of entities expected to participate in multiple sub-events; and (4) temporal sequencing between the sub-events. MASC automates portions of the script creation process with suggestions for event types, links to Wikidata, and... | Manuel R. Ciosici, Joseph Cummings, Mitchell DeHaven, Alex Hedges, Yash Kankanampati, DongHo Lee, Ralph M. Weischedel, Marjorie Freedman |  |
| 62 |  |  [NAMER: A Node-Based Multitasking Framework for Multi-Hop Knowledge Base Question Answering](https://doi.org/10.18653/v1/2021.naacl-demos.3) |  | 0 | We present NAMER, an open-domain Chinese knowledge base question answering system based on a novel node-based framework that better grasps the structural mapping between questions and KB queries by aligning the nodes in a query with their corresponding mentions in question. Equipped with techniques including data augmentation and multitasking, we show that the proposed framework outperforms the previous SoTA on CCKS CKBQA dataset. Moreover, we develop a novel data annotation strategy that... | Minhao Zhang, Ruoyu Zhang, Lei Zou, Yinnian Lin, Sen Hu |  |
| 63 |  |  [DiSCoL: Toward Engaging Dialogue Systems through Conversational Line Guided Response Generation](https://doi.org/10.18653/v1/2021.naacl-demos.4) |  | 0 | Having engaging and informative conversations with users is the utmost goal for open-domain conversational systems. Recent advances in transformer-based language models and their applications to dialogue systems have succeeded to generate fluent and human-like responses. However, they still lack control over the generation process towards producing contentful responses and achieving engaging conversations. To achieve this goal, we present DiSCoL (Dialogue Systems through Coversational Line... | Sarik Ghazarian, Zixi Liu, Tuhin Chakrabarty, Xuezhe Ma, Aram Galstyan, Nanyun Peng |  |
| 64 |  |  [FITAnnotator: A Flexible and Intelligent Text Annotation System](https://doi.org/10.18653/v1/2021.naacl-demos.5) |  | 0 | In this paper, we introduce FITAnnotator, a generic web-based tool for efficient text annotation. Benefiting from the fully modular architecture design, FITAnnotator provides a systematic solution for the annotation of a variety of natural language processing tasks, including classification, sequence tagging and semantic role annotation, regardless of the language. Three kinds of interfaces are developed to annotate instances, evaluate annotation quality and manage the annotation task for... | Yanzeng Li, Bowen Yu, Quangang Li, Tingwen Liu |  |
| 65 |  |  [Robustness Gym: Unifying the NLP Evaluation Landscape](https://doi.org/10.18653/v1/2021.naacl-demos.6) |  | 0 | Despite impressive performance on standard benchmarks, natural language processing (NLP) models are often brittle when deployed in real-world systems. In this work, we identify challenges with evaluating NLP systems and propose a solution in the form of Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms: subpopulations, transformations, evaluation sets, and adversarial attacks. By providing a common platform for evaluation, RG enables... | Karan Goel, Nazneen Fatema Rajani, Jesse Vig, Zachary Taschdjian, Mohit Bansal, Christopher Ré |  |
| 66 |  |  [EventPlus: A Temporal Event Understanding Pipeline](https://doi.org/10.18653/v1/2021.naacl-demos.7) |  | 0 | We present EventPlus, a temporal event understanding pipeline that integrates various state-of-the-art event understanding components including event trigger and type detection, event argument detection, event duration and temporal relation extraction. Event information, especially event temporal knowledge, is a type of common sense knowledge that helps people understand how stories evolve and provides predictive hints for future events. EventPlus as the first comprehensive temporal event... | Mingyu Derek Ma, Jiao Sun, Mu Yang, KungHsiang Huang, Nuan Wen, Shikhar Singh, Rujun Han, Nanyun Peng |  |
| 67 |  |  [COVID-19 Literature Knowledge Graph Construction and Drug Repurposing Report Generation](https://doi.org/10.18653/v1/2021.naacl-demos.8) |  | 0 | To combat COVID-19, both clinicians and scientists need to digest the vast amount of relevant biomedical knowledge in literature to understand the disease mechanism and the related biological functions. We have developed a novel and comprehensive knowledge discovery framework, COVID-KG to extract fine-grained multimedia knowledge elements (entities, relations and events) from scientific literature. We then exploit the constructed multimedia knowledge graphs (KGs) for question answering and... | Qingyun Wang, Manling Li, Xuan Wang, Nikolaus Nova Parulian, Guangxing Han, Jiawei Ma, Jingxuan Tu, Ying Lin, Haoran Zhang, Weili Liu, Aabhas Chauhan, Yingjun Guan, Bangzheng Li, Ruisong Li, Xiangchen Song, Yi R. Fung, Heng Ji, Jiawei Han, ShihFu Chang, James Pustejovsky, Jasmine Rah, David Liem, Ahmed Elsayed, Martha Palmer, Clare R. Voss, Cynthia Schneider, Boyan A. Onyshkevych |  |
| 68 |  |  [Multifaceted Domain-Specific Document Embeddings](https://doi.org/10.18653/v1/2021.naacl-demos.9) |  | 0 | Current document embeddings require large training corpora but fail to learn high-quality representations when confronted with a small number of domain-specific documents and rare terms. Further, they transform each document into a single embedding vector, making it hard to capture different notions of document similarity or explain why two documents are considered similar. In this work, we propose our Faceted Domain Encoder, a novel approach to learn multifaceted embeddings for domain-specific... | Julian Risch, Philipp Hager, Ralf Krestel |  |
| 69 |  |  [Improving Evidence Retrieval for Automated Explainable Fact-Checking](https://doi.org/10.18653/v1/2021.naacl-demos.10) |  | 0 | Automated fact-checking on a large-scale is a challenging task that has not been studied systematically until recently. Large noisy document collections like the web or news articles make the task more difficult. We describe a three-stage automated fact-checking system, named Quin+, using evidence retrieval and selection methods. We demonstrate that using dense passage representations leads to much higher evidence recall in a noisy setting. We also propose two sentence selection approaches, an... | Chris Samarinas, Wynne Hsu, MongLi Lee |  |
| 70 |  |  [Interactive Plot Manipulation using Natural Language](https://doi.org/10.18653/v1/2021.naacl-demos.11) |  | 0 | We present an interactive Plotting Agent, a system that enables users to directly manipulate plots using natural language instructions within an interactive programming environment. The Plotting Agent maps language to plot updates. We formulate this problem as a slot-based task-oriented dialog problem, which we tackle with a sequence-to-sequence model. This plotting model while accurate in most cases, still makes errors, therefore, the system allows a feedback mode, wherein the user is... | Yihan Wang, Yutong Shao, Ndapa Nakashole |  |
| 71 |  |  [ActiveAnno: General-Purpose Document-Level Annotation Tool with Active Learning Integration](https://doi.org/10.18653/v1/2021.naacl-demos.12) |  | 0 | ActiveAnno is an annotation tool focused on document-level annotation tasks developed both for industry and research settings. It is designed to be a general-purpose tool with a wide variety of use cases. It features a modern and responsive web UI for creating annotation projects, conducting annotations, adjudicating disagreements, and analyzing annotation results. ActiveAnno embeds a highly configurable and interactive user interface. The tool also integrates a RESTful API that enables... | Max Wiechmann, Seid Muhie Yimam, Chris Biemann |  |
| 72 |  |  [TextEssence: A Tool for Interactive Analysis of Semantic Shifts Between Corpora](https://doi.org/10.18653/v1/2021.naacl-demos.13) |  | 0 | Embeddings of words and concepts capture syntactic and semantic regularities of language; however, they have seen limited use as tools to study characteristics of different corpora and how they relate to one another. We introduce TextEssence, an interactive system designed to enable comparative analysis of corpora using embeddings. TextEssence includes visual, neighbor-based, and similarity-based modes of embedding analysis in a lightweight, web-based interface. We further propose a new measure... | Denis NewmanGriffis, Venkatesh Sivaraman, Adam Perer, Eric FoslerLussier, Harry Hochheiser |  |
| 73 |  |  [Supporting Spanish Writers using Automated Feedback](https://doi.org/10.18653/v1/2021.naacl-demos.14) |  | 0 | We present a tool that provides automated feedback to students studying Spanish writing. The feedback is given for four categories: topic development, coherence, writing conventions, and essay organization. The tool is made freely available via a Google Docs add-on. A small user study with third-level students in Mexico shows that students found the tool generally helpful and that most of them plan to continue using it as they work to improve their writing skills. | Aoife Cahill, James Bruno, James Ramey, Gilmar Ayala Meneses, Ian Blood, Florencia Tolentino, Tamar Lavee, Slava Andreyev |  |
| 74 |  |  [Alexa Conversations: An Extensible Data-driven Approach for Building Task-oriented Dialogue Systems](https://doi.org/10.18653/v1/2021.naacl-demos.15) |  | 0 | Traditional goal-oriented dialogue systems rely on various components such as natural language understanding, dialogue state tracking, policy learning and response generation. Training each component requires annotations which are hard to obtain for every new domain, limiting scalability of such systems. Similarly, rule-based dialogue systems require extensive writing and maintenance of rules and do not scale either. End-to-End dialogue systems, on the other hand, do not require module-specific... | Anish Acharya, Suranjit Adhikari, Sanchit Agarwal, Vincent Auvray, Nehal Belgamwar, Arijit Biswas, Shubhra Chandra, Tagyoung Chung, Maryam FazelZarandi, Raefer Gabriel, Shuyang Gao, Rahul Goel, Dilek HakkaniTür, Jan Jezabek, Abhay Jha, JiunYu Kao, Prakash Krishnan, Peter Ku, Anuj Goyal, ChienWei Lin, Qing Liu, Arindam Mandal, Angeliki Metallinou, Vishal Ishwar Naik, Yi Pan, Shachi Paul, Vittorio Perera, Abhishek Sethi, Minmin Shen, Nikko Strom, Eddie Wang |  |
| 75 |  |  [RESIN: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System](https://doi.org/10.18653/v1/2021.naacl-demos.16) |  | 0 | We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects: (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event... | Haoyang Wen, Ying Lin, Tuan Manh Lai, Xiaoman Pan, Sha Li, Xudong Lin, Ben Zhou, Manling Li, Haoyu Wang, Hongming Zhang, Xiaodong Yu, Alexander Dong, Zhenhailong Wang, Yi Ren Fung, Piyush Mishra, Qing Lyu, Dídac Surís, Brian Chen, Susan Windisch Brown, Martha Palmer, Chris CallisonBurch, Carl Vondrick, Jiawei Han, Dan Roth, ShihFu Chang, Heng Ji |  |
| 76 |  |  [MUDES: Multilingual Detection of Offensive Spans](https://doi.org/10.18653/v1/2021.naacl-demos.17) |  | 0 | The interest in offensive content identification in social media has grown substantially in recent years. Previous work has dealt mostly with post level annotations. However, identifying offensive spans is useful in many ways. To help coping with this important challenge, we present MUDES, a multilingual system to detect offensive spans in texts. MUDES features pre-trained models, a Python API for developers, and a user-friendly web-based interface. A detailed description of MUDES’ components... | Tharindu Ranasinghe, Marcos Zampieri |  |
| 77 |  |  [Knowledge Router: Learning Disentangled Representations for Knowledge Graphs](https://doi.org/10.18653/v1/2021.naacl-main.1) |  | 0 | The design of expressive representations of entities and relations in a knowledge graph is an important endeavor. While many of the existing approaches have primarily focused on learning from relational patterns and structural information, the intrinsic complexity of KG entities has been more or less overlooked. More concretely, we hypothesize KG entities may be more complex than we think, i.e., an entity may wear many hats and relational triplets may form due to more than a single reason. To... | Shuai Zhang, Xi Rao, Yi Tay, Ce Zhang |  |
| 78 |  |  [Distantly Supervised Relation Extraction with Sentence Reconstruction and Knowledge Base Priors](https://doi.org/10.18653/v1/2021.naacl-main.2) |  | 0 | We propose a multi-task, probabilistic approach to facilitate distantly supervised relation extraction by bringing closer the representations of sentences that contain the same Knowledge Base pairs. To achieve this, we bias the latent space of sentences via a Variational Autoencoder (VAE) that is trained jointly with a relation classifier. The latent code guides the pair representations and influences sentence reconstruction. Experimental results on two datasets created via distant supervision... | Fenia Christopoulou, Makoto Miwa, Sophia Ananiadou |  |
| 79 |  |  [Cross-Task Instance Representation Interactions and Label Dependencies for Joint Information Extraction with Graph Convolutional Networks](https://doi.org/10.18653/v1/2021.naacl-main.3) |  | 0 | Existing works on information extraction (IE) have mainly solved the four main tasks separately (entity mention recognition, relation extraction, event trigger detection, and argument extraction), thus failing to benefit from inter-dependencies between tasks. This paper presents a novel deep learning model to simultaneously solve the four tasks of IE in a single model (called FourIE). Compared to few prior work on jointly performing four IE tasks, FourIE features two novel contributions to... | Minh Van Nguyen, Viet Dac Lai, Thien Huu Nguyen |  |
| 80 |  |  [Abstract Meaning Representation Guided Graph Encoding and Decoding for Joint Information Extraction](https://doi.org/10.18653/v1/2021.naacl-main.4) |  | 0 | The tasks of Rich Semantic Parsing, such as Abstract Meaning Representation (AMR), share similar goals with Information Extraction (IE) to convert natural language texts into structured semantic representations. To take advantage of such similarity, we propose a novel AMR-guided framework for joint information extraction to discover entities, relations, and events with the help of a pre-trained AMR parser. Our framework consists of two novel components: 1) an AMR based semantic graph aggregator... | Zixuan Zhang, Heng Ji |  |
| 81 |  |  [A Frustratingly Easy Approach for Entity and Relation Extraction](https://doi.org/10.18653/v1/2021.naacl-main.5) |  | 0 | End-to-end relation extraction aims to identify named entities and extract relations between them. Most recent work models these two subtasks jointly, either by casting them in one structured prediction framework, or performing multi-task learning through shared representations. In this work, we present a simple pipelined approach for entity and relation extraction, and establish the new state-of-the-art on standard benchmarks (ACE04, ACE05 and SciERC), obtaining a 1.7%-2.8% absolute... | Zexuan Zhong, Danqi Chen |  |
| 82 |  |  [Event Time Extraction and Propagation via Graph Attention Networks](https://doi.org/10.18653/v1/2021.naacl-main.6) |  | 0 | Grounding events into a precise timeline is important for natural language understanding but has received limited attention in recent work. This problem is challenging due to the inherent ambiguity of language and the requirement for information propagation over inter-related events. This paper first formulates this problem based on a 4-tuple temporal representation used in entity slot filling, which allows us to represent fuzzy time spans more conveniently. We then propose a graph attention... | Haoyang Wen, Yanru Qu, Heng Ji, Qiang Ning, Jiawei Han, Avi Sil, Hanghang Tong, Dan Roth |  |
| 83 |  |  [Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers](https://doi.org/10.18653/v1/2021.naacl-main.7) |  | 0 | Due to its effectiveness and performance, the Transformer translation model has attracted wide attention, most recently in terms of probing-based approaches. Previous work focuses on using or probing source linguistic features in the encoder. To date, the way word translation evolves in Transformer layers has not yet been investigated. Naively, one might assume that encoder layers capture source information while decoder layers translate. In this work, we show that this is not quite the case:... | Hongfei Xu, Josef van Genabith, Qiuhui Liu, Deyi Xiong |  |
| 84 |  |  [Mediators in Determining what Processing BERT Performs First](https://doi.org/10.18653/v1/2021.naacl-main.8) |  | 0 | Probing neural models for the ability to perform downstream tasks using their activation patterns is often used to localize what parts of the network specialize in performing what tasks. However, little work addressed potential mediating factors in such comparisons. As a test-case mediating factor, we consider the prediction’s context length, namely the length of the span whose processing is minimally required to perform the prediction. We show that not controlling for context length may lead... | Aviv Slobodkin, Leshem Choshen, Omri Abend |  |
| 85 |  |  [Automatic Generation of Contrast Sets from Scene Graphs: Probing the Compositional Consistency of GQA](https://doi.org/10.18653/v1/2021.naacl-main.9) |  | 0 | Recent works have shown that supervised models often exploit data artifacts to achieve good test scores while their performance severely degrades on samples outside their training distribution. Contrast sets (Gardneret al., 2020) quantify this phenomenon by perturbing test samples in a minimal way such that the output label is modified. While most contrast sets were created manually, requiring intensive annotation effort, we present a novel method which leverages rich semantic input... | Yonatan Bitton, Gabriel Stanovsky, Roy Schwartz, Michael Elhadad |  |
| 86 |  |  [Multilingual Language Models Predict Human Reading Behavior](https://doi.org/10.18653/v1/2021.naacl-main.10) |  | 0 | We analyze if large language models are able to predict patterns of human reading behavior. We compare the performance of language-specific and multilingual pretrained transformer models to predict reading time measures reflecting natural human sentence processing on Dutch, English, German, and Russian texts. This results in accurate models of human reading behavior, which indicates that transformer models implicitly encode relative importance in language in a way that is comparable to human... | Nora Hollenstein, Federico Pirovano, Ce Zhang, Lena A. Jäger, Lisa Beinborn |  |
| 87 |  |  [Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing](https://doi.org/10.18653/v1/2021.naacl-main.11) |  | 0 | Analysing whether neural language models encode linguistic information has become popular in NLP. One method of doing so, which is frequently cited to support the claim that models like BERT encode syntax, is called probing; probes are small supervised models trained to extract linguistic information from another model’s output. If a probe is able to predict a particular structure, it is argued that the model whose output it is trained on must have implicitly learnt to encode it. However,... | Rowan Hall Maudslay, Ryan Cotterell |  |
| 88 |  |  [A Non-Linear Structural Probe](https://doi.org/10.18653/v1/2021.naacl-main.12) |  | 0 | Probes are models devised to investigate the encoding of knowledge—e.g. syntactic structure—in contextual representations. Probes are often designed for simplicity, which has led to restrictions on probe design that may not allow for the full exploitation of the structure of encoded information; one such restriction is linearity. We examine the case of a structural probe (Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic structure in contextual representations... | Jennifer C. White, Tiago Pimentel, Naomi Saphra, Ryan Cotterell |  |
| 89 |  |  [Concealed Data Poisoning Attacks on NLP Models](https://doi.org/10.18653/v1/2021.naacl-main.13) |  | 0 | Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model’s training set that causes the model to frequently... | Eric Wallace, Tony Z. Zhao, Shi Feng, Sameer Singh |  |
| 90 |  |  [Backtranslation Feedback Improves User Confidence in MT, Not Quality](https://doi.org/10.18653/v1/2021.naacl-main.14) |  | 0 | Translating text into a language unknown to the text’s author, dubbed outbound translation, is a modern need for which the user experience has significant room for improvement, beyond the basic machine translation facility. We demonstrate this by showing three ways in which user confidence in the outbound translation, as well as its overall final quality, can be affected: backward translation, quality estimation (with alignment) and source paraphrasing. In this paper, we describe an experiment... | Vilém Zouhar, Michal Novák, Matús Zilinec, Ondrej Bojar, Mateo Obregón, Robin L. Hill, Frédéric Blain, Marina Fomicheva, Lucia Specia, Lisa Yankovskaya |  |
| 91 |  |  [Data Filtering using Cross-Lingual Word Embeddings](https://doi.org/10.18653/v1/2021.naacl-main.15) |  | 0 | Data filtering for machine translation (MT) describes the task of selecting a subset of a given, possibly noisy corpus with the aim to maximize the performance of an MT system trained on this selected data. Over the years, many different filtering approaches have been proposed. However, varying task definitions and data conditions make it difficult to draw a meaningful comparison. In the present work, we aim for a more systematic approach to the task at hand. First, we analyze the performance... | Christian Herold, Jan Rosendahl, Joris Vanvinckenroye, Hermann Ney |  |
| 92 |  |  [Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.16) |  | 0 | Successful methods for unsupervised neural machine translation (UNMT) employ cross-lingual pretraining via self-supervision, often in the form of a masked language modeling or a sequence generation task, which requires the model to align the lexical- and high-level representations of the two languages. While cross-lingual pretraining works for similar languages with abundant corpora, it performs poorly in low-resource and distant languages. Previous research has shown that this is because the... | Alexandra Chronopoulou, Dario Stojanovski, Alexander Fraser |  |
| 93 |  |  [Neural Machine Translation without Embeddings](https://doi.org/10.18653/v1/2021.naacl-main.17) |  | 0 | Many NLP models operate over sequences of subword tokens produced by hand-crafted tokenization rules and heuristic subword induction algorithms. A simple universal alternative is to represent every computerized text as a sequence of bytes via UTF-8, obviating the need for an embedding layer since there are fewer token types (256) than dimensions. Surprisingly, replacing the ubiquitous embedding layer with one-hot representations of each byte does not hurt performance; experiments on... | Uri Shaham, Omer Levy |  |
| 94 |  |  [Counterfactual Data Augmentation for Neural Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.18) |  | 0 | We propose a data augmentation method for neural machine translation. It works by interpreting language models and phrasal alignment causally. Specifically, it creates augmented parallel translation corpora by generating (path-specific) counterfactual aligned phrases. We generate these by sampling new source phrases from a masked language model, then sampling an aligned counterfactual target phrase by noting that a translation language model can be interpreted as a Gumbel-Max Structural Causal... | Qi Liu, Matt J. Kusner, Phil Blunsom |  |
| 95 |  |  [Cultural and Geographical Influences on Image Translatability of Words across Languages](https://doi.org/10.18653/v1/2021.naacl-main.19) |  | 0 | Neural Machine Translation (NMT) models have been observed to produce poor translations when there are few/no parallel sentences to train the models. In the absence of parallel data, several approaches have turned to the use of images to learn translations. Since images of words, e.g., horse may be unchanged across languages, translations can be identified via images associated with words in different languages that have a high degree of visual similarity. However, translating via images has... | Nikzad Khani, Isidora Chara Tourni, Mohammad Sadegh Rasooli, Chris CallisonBurch, Derry Tanti Wijaya |  |
| 96 |  |  [Multilingual BERT Post-Pretraining Alignment](https://doi.org/10.18653/v1/2021.naacl-main.20) |  | 0 | We propose a simple method to align multilingual contextual embeddings as a post-pretraining step for improved cross-lingual transferability of the pretrained language models. Using parallel data, our method aligns embeddings on the word level through the recently proposed Translation Language Modeling objective as well as on the sentence level via contrastive learning and random input shuffling. We also perform sentence-level code-switching with English when finetuning on downstream tasks. On... | Lin Pan, ChungWei Hang, Haode Qi, Abhishek Shah, Saloni Potdar, Mo Yu |  |
| 97 |  |  [A Million Tweets Are Worth a Few Points: Tuning Transformers for Customer Service Tasks](https://doi.org/10.18653/v1/2021.naacl-main.21) |  | 0 | In online domain-specific customer service applications, many companies struggle to deploy advanced NLP models successfully, due to the limited availability of and noise in their datasets. While prior research demonstrated the potential of migrating large open-domain pretrained models for domain-specific tasks, the appropriate (pre)training strategies have not yet been rigorously evaluated in such social media customer service settings, especially under multilingual conditions. We address this... | Amir Hadifar, Sofie Labat, Véronique Hoste, Chris Develder, Thomas Demeester |  |
| 98 |  |  [Paragraph-level Rationale Extraction through Regularization: A case study on European Court of Human Rights Cases](https://doi.org/10.18653/v1/2021.naacl-main.22) |  | 0 | Interpretability or explainability is an emerging research field in NLP. From a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. To this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases.... | Ilias Chalkidis, Manos Fergadiotis, Dimitrios Tsarapatsanis, Nikolaos Aletras, Ion Androutsopoulos, Prodromos Malakasiotis |  |
| 99 |  |  [Answering Product-Questions by Utilizing Questions from Other Contextually Similar Products](https://doi.org/10.18653/v1/2021.naacl-main.23) |  | 0 | Predicting the answer to a product-related question is an emerging field of research that recently attracted a lot of attention. Answering subjective and opinion-based questions is most challenging due to the dependency on customer generated content. Previous works mostly focused on review-aware answer prediction; however, these approaches fail for new or unpopular products, having no (or only a few) reviews at hand. In this work, we propose a novel and complementary approach for predicting the... | Ohad Rozen, David Carmel, Avihai Mejer, Vitaly Mirkis, Yftah Ziser |  |
| 100 |  |  [EnSidNet: Enhanced Hybrid Siamese-Deep Network for grouping clinical trials into drug-development pathways](https://doi.org/10.18653/v1/2021.naacl-main.24) |  | 0 | Siamese Neural Networks have been widely used to perform similarity classification in multi-class settings. Their architecture can be used to group the clinical trials belonging to the same drug-development pathway along the several clinical trial phases. Here we present an approach for the unmet need of drug-development pathway reconstruction, based on an Enhanced hybrid Siamese-Deep Neural Network (EnSidNet). The proposed model demonstrates significant improvement above baselines in a 1-shot... | Lucia Pagani |  |
| 101 |  |  [DATE: Detecting Anomalies in Text via Self-Supervision of Transformers](https://doi.org/10.18653/v1/2021.naacl-main.25) |  | 0 | Leveraging deep learning models for Anomaly Detection (AD) has seen widespread use in recent years due to superior performances over traditional methods. Recent deep methods for anomalies in images learn better features of normality in an end-to-end self-supervised setting. These methods train a model to discriminate between different transformations applied to visual data and then use the output to compute an anomaly score. We use this approach for AD in text, by introducing a novel pretext... | Andrei Manolache, Florin Brad, Elena Burceanu |  |
| 102 |  |  [A Simple Approach for Handling Out-of-Vocabulary Identifiers in Deep Learning for Source Code](https://doi.org/10.18653/v1/2021.naacl-main.26) |  | 0 | There is an emerging interest in the application of natural language processing models to source code processing tasks. One of the major problems in applying deep learning to software engineering is that source code often contains a lot of rare identifiers, resulting in huge vocabularies. We propose a simple, yet effective method, based on identifier anonymization, to handle out-of-vocabulary (OOV) identifiers. Our method can be treated as a preprocessing step and, therefore, allows for easy... | Nadezhda Chirkova, Sergey Troshin |  |
| 103 |  |  [Fast and Scalable Dialogue State Tracking with Explicit Modular Decomposition](https://doi.org/10.18653/v1/2021.naacl-main.27) |  | 0 | We present a fast and scalable architecture called Explicit Modular Decomposition (EMD), in which we incorporate both classification-based and extraction-based methods and design four modules (for clas- sification and sequence labelling) to jointly extract dialogue states. Experimental results based on the MultiWoz 2.0 dataset validates the superiority of our proposed model in terms of both complexity and scalability when compared to the state-of-the-art methods, especially in the scenario of... | Dingmin Wang, Chenghua Lin, Qi Liu, KamFai Wong |  |
| 104 |  |  [Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks](https://doi.org/10.18653/v1/2021.naacl-main.28) |  | 0 | There are two approaches for pairwise sentence scoring: Cross-encoders, which perform full-attention over the input pair, and Bi-encoders, which map each input independently to a dense vector space. While cross-encoders often achieve higher performance, they are too slow for many practical use cases. Bi-encoders, on the other hand, require substantial training data and fine-tuning over the target task to achieve competitive performance. We present a simple yet efficient data augmentation... | Nandan Thakur, Nils Reimers, Johannes Daxenberger, Iryna Gurevych |  |
| 105 |  |  [SmBoP: Semi-autoregressive Bottom-up Semantic Parsing](https://doi.org/10.18653/v1/2021.naacl-main.29) |  | 0 | The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depth-first traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SmBoP) that constructs at decoding step t the top-K sub-trees of height ≤ t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, bottom-up parsing... | Ohad Rubin, Jonathan Berant |  |
| 106 |  |  [SGL: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation](https://doi.org/10.18653/v1/2021.naacl-main.30) |  | 0 | Graph-based semantic parsing aims to represent textual meaning through directed graphs. As one of the most promising general-purpose meaning representations, these structures and their parsing have gained a significant interest momentum during recent years, with several diverse formalisms being proposed. Yet, owing to this very heterogeneity, most of the research effort has focused mainly on solutions specific to a given formalism. In this work, instead, we reframe semantic parsing towards... | Luigi Procopio, Rocco Tripodi, Roberto Navigli |  |
| 107 |  |  [Unifying Cross-Lingual Semantic Role Labeling with Heterogeneous Linguistic Resources](https://doi.org/10.18653/v1/2021.naacl-main.31) |  | 0 | While cross-lingual techniques are finding increasing success in a wide range of Natural Language Processing tasks, their application to Semantic Role Labeling (SRL) has been strongly limited by the fact that each language adopts its own linguistic formalism, from PropBank for English to AnCora for Spanish and PDT-Vallex for Czech, inter alia. In this work, we address this issue and present a unified model to perform cross-lingual SRL over heterogeneous linguistic resources. Our model... | Simone Conia, Andrea Bacciu, Roberto Navigli |  |
| 108 |  |  [Fool Me Twice: Entailment from Wikipedia Gamification](https://doi.org/10.18653/v1/2021.naacl-main.32) |  | 0 | We release FoolMeTwice (FM2 for short), a large dataset of challenging entailment pairs collected through a fun multi-player game. Gamification encourages adversarial examples, drastically lowering the number of examples that can be solved using “shortcuts” compared to other popular entailment datasets. Players are presented with two tasks. The first task asks the player to write a plausible claim based on the evidence from a Wikipedia page. The second one shows two plausible claims written by... | Julian Martin Eisenschlos, Bhuwan Dhingra, Jannis Bulian, Benjamin Börschinger, Jordan L. BoydGraber |  |
| 109 |  |  [Meta-Learning for Domain Generalization in Semantic Parsing](https://doi.org/10.18653/v1/2021.naacl-main.33) |  | 0 | The importance of building semantic parsers which can be applied to new domains and generate programs unseen at training has long been acknowledged, and datasets testing out-of-domain performance are becoming increasingly available. However, little or no attention has been devoted to learning algorithms or objectives which promote domain generalization, with virtually all existing approaches relying on standard supervised learning. In this work, we use a meta-learning framework which targets... | Bailin Wang, Mirella Lapata, Ivan Titov |  |
| 110 |  |  [Aspect-Controlled Neural Argument Generation](https://doi.org/10.18653/v1/2021.naacl-main.34) |  | 0 | We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we present the Arg-CTRL - a language model for argument generation that can be controlled to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with... | Benjamin Schiller, Johannes Daxenberger, Iryna Gurevych |  |
| 111 |  |  [Text Generation from Discourse Representation Structures](https://doi.org/10.18653/v1/2021.naacl-main.35) |  | 0 | We propose neural models to generate text from formal meaning representations based on Discourse Representation Structures (DRSs). DRSs are document-level representations which encode rich semantic detail pertaining to rhetorical relations, presupposition, and co-reference within and across sentences. We formalize the task of neural DRS-to-text generation and provide modeling solutions for the problems of condition ordering and variable naming which render generation from DRSs non-trivial. Our... | Jiangming Liu, Shay B. Cohen, Mirella Lapata |  |
| 112 |  |  [APo-VAE: Text Generation in Hyperbolic Space](https://doi.org/10.18653/v1/2021.naacl-main.36) |  | 0 | Natural language often exhibits inherent hierarchical structure ingrained with complex syntax and semantics. However, most state-of-the-art deep generative models learn embeddings only in Euclidean vector space, without accounting for this structural property of language. In this paper, we investigate text generation in a hyperbolic latent space to learn continuous hierarchical representations. An Adversarial Poincare Variational Autoencoder (APo-VAE) is presented, where both the prior and... | Shuyang Dai, Zhe Gan, Yu Cheng, Chenyang Tao, Lawrence Carin, Jingjing Liu |  |
| 113 |  |  [DART: Open-Domain Structured Data Record to Text Generation](https://doi.org/10.18653/v1/2021.naacl-main.37) |  | 0 | We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction... | Linyong Nan, Dragomir R. Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, Nazneen Fatema Rajani |  |
| 114 |  |  [When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models](https://doi.org/10.18653/v1/2021.naacl-main.38) |  | 0 | Transfer learning based on pretraining language models on a large amount of raw data has become a new norm to reach state-of-the-art performance in NLP. Still, it remains unclear how this approach should be applied for unseen languages that are not covered by any available large-scale multilingual language model and for which only a small amount of raw data is generally available. In this work, by comparing multilingual and monolingual models, we show that such models behave in multiple ways on... | Benjamin Muller, Antonios Anastasopoulos, Benoît Sagot, Djamé Seddah |  |
| 115 |  |  [Multi-Adversarial Learning for Cross-Lingual Word Embeddings](https://doi.org/10.18653/v1/2021.naacl-main.39) |  | 0 | Generative adversarial networks (GANs) have succeeded in inducing cross-lingual word embeddings - maps of matching words across languages - without supervision. Despite these successes, GANs’ performance for the difficult case of distant languages is still not satisfactory. These limitations have been explained by GANs’ incorrect assumption that source and target embedding spaces are related by a single linear mapping and are approximately isomorphic. We assume instead that, especially across... | Haozhou Wang, James Henderson, Paola Merlo |  |
| 116 |  |  [Multi-view Subword Regularization](https://doi.org/10.18653/v1/2021.naacl-main.40) |  | 0 | Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods (Kudo, 2018; Provilkov et al., 2020) during fine-tuning of pre-trained... | Xinyi Wang, Sebastian Ruder, Graham Neubig |  |
| 117 |  |  [mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer](https://doi.org/10.18653/v1/2021.naacl-main.41) |  | 0 | The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to... | Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami AlRfou, Aditya Siddhant, Aditya Barua, Colin Raffel |  |
| 118 |  |  [MetaXL: Meta Representation Transformation for Low-resource Cross-lingual Learning](https://doi.org/10.18653/v1/2021.naacl-main.42) |  | 0 | The combination of multilingual pre-trained representations and cross-lingual transfer learning is one of the most effective methods for building functional NLP systems for low-resource languages. However, for extremely low-resource languages without large-scale monolingual corpora for pre-training or sufficient annotated data for fine-tuning, transfer learning remains an understudied and challenging task. Moreover, recent work shows that multilingual representations are surprisingly disjoint... | Mengzhou Xia, Guoqing Zheng, Subhabrata Mukherjee, Milad Shokouhi, Graham Neubig, Ahmed Hassan Awadallah |  |
| 119 |  |  [Open Domain Question Answering over Tables via Dense Retrieval](https://doi.org/10.18653/v1/2021.naacl-main.43) |  | 0 | Recent advances in open-domain QA have led to strong models based on dense retrieval, but only focused on retrieving textual passages. In this work, we tackle open-domain QA over tables for the first time, and show that retrieval can be improved by a retriever designed to handle tabular context. We present an effective pre-training procedure for our retriever and improve retrieval quality with mined hard negatives. As relevant datasets are missing, we extract a subset of Natural Questions... | Jonathan Herzig, Thomas Müller, Syrine Krichene, Julian Martin Eisenschlos |  |
| 120 |  |  [Open-Domain Question Answering Goes Conversational via Question Rewriting](https://doi.org/10.18653/v1/2021.naacl-main.44) |  | 0 | We introduce a new dataset for Question Rewriting in Conversational Context (QReCC), which contains 14K conversations with 80K question-answer pairs. The task in QReCC is to find answers to conversational questions within a collection of 10M web pages (split into 54M passages). Answers to questions in the same conversation may be distributed across several web pages. QReCC provides annotations that allow us to train and evaluate individual subtasks of question rewriting, passage retrieval and... | Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, Srinivas Chappidi |  |
| 121 |  |  [QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering](https://doi.org/10.18653/v1/2021.naacl-main.45) |  | 0 | The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. Here we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes... | Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, Jure Leskovec |  |
| 122 |  |  [XOR QA: Cross-lingual Open-Retrieval Question Answering](https://doi.org/10.18653/v1/2021.naacl-main.46) |  | 0 | Multilingual question answering tasks typically assume that answers exist in the same language as the question. Yet in practice, many languages face both information scarcity—where languages have few reference articles—and information asymmetry—where questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a... | Akari Asai, Jungo Kasai, Jonathan H. Clark, Kenton Lee, Eunsol Choi, Hannaneh Hajishirzi |  |
| 123 |  |  [SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval](https://doi.org/10.18653/v1/2021.naacl-main.47) |  | 0 | We introduce SPARTA, a novel neural retrieval method that shows great promise in performance, generalization, and interpretability for open-domain question answering. Unlike many neural ranking methods that use dense vector nearest neighbor search, SPARTA learns a sparse representation that can be efficiently implemented as an Inverted Index. The resulting representation enables scalable neural retrieval that does not require expensive approximate vector search and leads to better performance... | Tiancheng Zhao, Xiaopeng Lu, Kyusong Lee |  |
| 124 |  |  [Implicitly Abusive Language - What does it actually look like and why are we not getting there?](https://doi.org/10.18653/v1/2021.naacl-main.48) |  | 0 | Abusive language detection is an emerging field in natural language processing which has received a large amount of attention recently. Still the success of automatic detection is limited. Particularly, the detection of implicitly abusive language, i.e. abusive language that is not conveyed by abusive words (e.g. dumbass or scum), is not working well. In this position paper, we explain why existing datasets make learning implicit abuse difficult and what needs to be changed in the design of... | Michael Wiegand, Josef Ruppenhofer, Elisabeth Eder |  |
| 125 |  |  [The Importance of Modeling Social Factors of Language: Theory and Practice](https://doi.org/10.18653/v1/2021.naacl-main.49) |  | 0 | Natural language processing (NLP) applications are now more powerful and ubiquitous than ever before. With rapidly developing (neural) models and ever-more available data, current NLP models have access to more information than any human speaker during their life. Still, it would be hard to argue that NLP models have reached human-level capacity. In this position paper, we argue that the reason for the current limitations is a focus on information content while ignoring language’s social... | Dirk Hovy, Diyi Yang |  |
| 126 |  |  [On learning and representing social meaning in NLP: a sociolinguistic perspective](https://doi.org/10.18653/v1/2021.naacl-main.50) |  | 0 | The field of NLP has made substantial progress in building meaning representations. However, an important aspect of linguistic meaning, social meaning, has been largely overlooked. We introduce the concept of social meaning to NLP and discuss how insights from sociolinguistics can inform work on representation learning in NLP. We also identify key challenges for this new line of research. | Dong Nguyen, Laura Rosseel, Jack Grieve |  |
| 127 |  |  [Preregistering NLP research](https://doi.org/10.18653/v1/2021.naacl-main.51) |  | 0 | Preregistration refers to the practice of specifying what you are going to do, and what you expect to find in your study, before carrying out the study. This practice is increasingly common in medicine and psychology, but is rarely discussed in NLP. This paper discusses preregistration in more detail, explores how NLP researchers could preregister their work, and presents several preregistration questions for different kinds of studies. Finally, we argue in favour of registered reports, which... | Emiel van Miltenburg, Chris van der Lee, Emiel Krahmer |  |
| 128 |  |  [Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence](https://doi.org/10.18653/v1/2021.naacl-main.52) |  | 0 | Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and... | Tal Schuster, Adam Fisch, Regina Barzilay |  |
| 129 |  |  [Representing Numbers in NLP: a Survey and a Vision](https://doi.org/10.18653/v1/2021.naacl-main.53) |  | 0 | NLP systems rarely give special consideration to numbers found in text. This starkly contrasts with the consensus in neuroscience that, in the brain, numbers are represented differently from words. We arrange recent NLP work on numeracy into a comprehensive taxonomy of tasks and methods. We break down the subjective notion of numeracy into 7 subtasks, arranged along two dimensions: granularity (exact vs approximate) and units (abstract vs grounded). We analyze the myriad representational... | Avijit Thawani, Jay Pujara, Filip Ilievski, Pedro A. Szekely |  |
| 130 |  |  [Extending Multi-Document Summarization Evaluation to the Interactive Setting](https://doi.org/10.18653/v1/2021.naacl-main.54) |  | 0 | Allowing users to interact with multi-document summarizers is a promising direction towards improving and customizing summary results. Different ideas for interactive summarization have been proposed in previous work but these solutions are highly divergent and incomparable. In this paper, we develop an end-to-end evaluation framework for interactive summarization, focusing on expansion-based interaction, which considers the accumulating information along a user session. Our framework includes... | Ori Shapira, Ramakanth Pasunuru, Hadar Ronen, Mohit Bansal, Yael Amsterdamer, Ido Dagan |  |
| 131 |  |  [Identifying Helpful Sentences in Product Reviews](https://doi.org/10.18653/v1/2021.naacl-main.55) |  | 0 | In recent years online shopping has gained momentum and became an important venue for customers wishing to save time and simplify their shopping process. A key advantage of shopping online is the ability to read what other customers are saying about products of interest. In this work, we aim to maintain this advantage in situations where extreme brevity is needed, for example, when shopping by voice. We suggest a novel task of extracting a single representative helpful sentence from a set of... | Iftah Gamzu, Hila Gonen, Gilad Kutiel, Ran Levy, Eugene Agichtein |  |
| 132 |  |  [Noisy Self-Knowledge Distillation for Text Summarization](https://doi.org/10.18653/v1/2021.naacl-main.56) |  | 0 | In this paper we apply self-knowledge distillation to text summarization which we argue can alleviate problems with maximum-likelihood training on single reference and noisy datasets. Instead of relying on one-hot annotation labels, our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training. Furthermore, to better model uncertainty during training, we introduce multiple noise signals for both teacher and student models. We... | Yang Liu, Sheng Shen, Mirella Lapata |  |
| 133 |  |  [Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation](https://doi.org/10.18653/v1/2021.naacl-main.57) |  | 0 | Models pretrained with self-supervised objectives on large text corpora achieve state-of-the-art performance on English text summarization tasks. However, these models are typically fine-tuned on hundreds of thousands of data points, an infeasible requirement when applying summarization to new, niche domains. In this work, we introduce a novel and generalizable method, called WikiTransfer, for fine-tuning pretrained models for summarization in an unsupervised, dataset-specific manner.... | Alexander R. Fabbri, Simeng Han, Haoyuan Li, Haoran Li, Marjan Ghazvininejad, Shafiq R. Joty, Dragomir R. Radev, Yashar Mehdad |  |
| 134 |  |  [Enhancing Factual Consistency of Abstractive Summarization](https://doi.org/10.18653/v1/2021.naacl-main.58) |  | 0 | Automatic abstractive summaries are found to often distort or fabricate facts in the article. This inconsistency between summary and original text has seriously impacted its applicability. We propose a fact-aware summarization model FASum to extract and integrate factual relations into the summary generation process via graph attention. We then design a factual corrector model FC to automatically correct factual errors from summaries generated by existing systems. Empirical results show that... | Chenguang Zhu, William Hinthorn, Ruochen Xu, Qingkai Zeng, Michael Zeng, Xuedong Huang, Meng Jiang |  |
| 135 |  |  [Few-shot Intent Classification and Slot Filling with Retrieved Examples](https://doi.org/10.18653/v1/2021.naacl-main.59) |  | 0 | Few-shot learning arises in important practical scenarios, such as when a natural language understanding system needs to learn new semantic labels for an emerging, resource-scarce domain. In this paper, we explore retrieval-based methods for intent classification and slot filling tasks in few-shot settings. Retrieval-based methods make predictions based on labeled examples in the retrieval index that are similar to the input, and thus can adapt to new domains simply by changing the index... | Dian Yu, Luheng He, Yuan Zhang, Xinya Du, Panupong Pasupat, Qi Li |  |
| 136 |  |  ["Nice Try, Kiddo": Investigating Ad Hominems in Dialogue Responses](https://doi.org/10.18653/v1/2021.naacl-main.60) |  | 0 | Ad hominem attacks are those that target some feature of a person’s character instead of the position the person is maintaining. These attacks are harmful because they propagate implicit biases and diminish a person’s credibility. Since dialogue systems respond directly to user input, it is important to study ad hominems in dialogue responses. To this end, we propose categories of ad hominems, compose an annotated dataset, and build a classifier to analyze human and dialogue system responses to... | Emily Sheng, KaiWei Chang, Prem Natarajan, Nanyun Peng |  |
| 137 |  |  [Human-like informative conversations: Better acknowledgements using conditional mutual information](https://doi.org/10.18653/v1/2021.naacl-main.61) |  | 0 | This work aims to build a dialogue agent that can weave new factual content into conversations as naturally as humans. We draw insights from linguistic principles of conversational analysis and annotate human-human conversations from the Switchboard Dialog Act Corpus to examine humans strategies for acknowledgement, transition, detail selection and presentation. When current chatbots (explicitly provided with new factual content) introduce facts into a conversation, their generated responses do... | Ashwin Paranjape, Christopher D. Manning |  |
| 138 |  |  [A Comparative Study on Schema-Guided Dialogue State Tracking](https://doi.org/10.18653/v1/2021.naacl-main.62) |  | 0 | Frame-based state representation is widely used in modern task-oriented dialog systems to model user intentions and slot values. However, a fixed design of domain ontology makes it difficult to extend to new services and APIs. Recent work proposed to use natural language descriptions to define the domain ontology instead of tag names for each intent or slot, thus offering a dynamic set of schema. In this paper, we conduct in-depth comparative studies to understand the use of natural language... | Jie Cao, Yi Zhang |  |
| 139 |  |  [Spoken Language Understanding for Task-oriented Dialogue Systems with Augmented Memory Networks](https://doi.org/10.18653/v1/2021.naacl-main.63) |  | 0 | Spoken language understanding, usually including intent detection and slot filling, is a core component to build a spoken dialog system. Recent research shows promising results by jointly learning of those two tasks based on the fact that slot filling and intent detection are sharing semantic knowledge. Furthermore, attention mechanism boosts joint learning to achieve state-of-the-art results. However, current joint learning models ignore the following important facts: 1. Long-term slot context... | Jie Wu, Ian G. Harris, Hongzhi Zhao |  |
| 140 |  |  [How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds](https://doi.org/10.18653/v1/2021.naacl-main.64) |  | 0 | We seek to create agents that both act and communicate with other agents in pursuit of a goal. Towards this end, we extend LIGHT (Urbanek et al. 2019)—a large-scale crowd-sourced fantasy text-game—with a dataset of quests. These contain natural language motivations paired with in-game goals and human demonstrations; completing a quest might require dialogue or actions (or both). We introduce a reinforcement learning system that (1) incorporates large-scale language modeling-based and... | Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, Arthur Szlam, Tim Rocktäschel, Jason Weston |  |
| 141 |  |  [Linking Entities to Unseen Knowledge Bases with Arbitrary Schemas](https://doi.org/10.18653/v1/2021.naacl-main.65) |  | 0 | In entity linking, mentions of named entities in raw text are disambiguated against a knowledge base (KB). This work focuses on linking to unseen KBs that do not have training data and whose schema is unknown during training. Our approach relies on methods to flexibly convert entities with several attribute-value pairs from arbitrary KBs into flat strings, which we use in conjunction with state-of-the-art models for zero-shot linking. We further improve the generalization of our model using two... | Yogarshi Vyas, Miguel Ballesteros |  |
| 142 |  |  [Self-Training with Weak Supervision](https://doi.org/10.18653/v1/2021.naacl-main.66) |  | 0 | State-of-the-art deep neural networks require large-scale labeled training data that is often expensive to obtain or not available for many tasks. Weak supervision in the form of domain-specific rules has been shown to be useful in such settings to automatically generate weakly labeled training data. However, learning with weak rules is challenging due to their inherent heuristic and noisy nature. An additional challenge is rule coverage and overlap, where prior work on weak supervision only... | Giannis Karamanolakis, Subhabrata Mukherjee, Guoqing Zheng, Ahmed Hassan Awadallah |  |
| 143 |  |  [Neural Language Modeling for Contextualized Temporal Graph Generation](https://doi.org/10.18653/v1/2021.naacl-main.67) |  | 0 | This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with human-annotated events and temporal links. We address this challenge by using existing IE/NLP tools... | Aman Madaan, Yiming Yang |  |
| 144 |  |  [Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2021.naacl-main.68) |  | 0 | Knowledge bases often consist of facts which are harvested from a variety of sources, many of which are noisy and some of which conflict, resulting in a level of uncertainty for each triple. Knowledge bases are also often incomplete, prompting the use of embedding methods to generalize from known facts, however, existing embedding methods only model triple-level uncertainty, and reasoning results lack global consistency. To address these shortcomings, we propose BEUrRE, a novel uncertain... | Xuelu Chen, Michael Boratko, Muhao Chen, Shib Sankar Dasgupta, Xiang Lorraine Li, Andrew McCallum |  |
| 145 |  |  [Document-Level Event Argument Extraction by Conditional Generation](https://doi.org/10.18653/v1/2021.naacl-main.69) |  | 0 | Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference... | Sha Li, Heng Ji, Jiawei Han |  |
| 146 |  |  [Template Filling with Generative Transformers](https://doi.org/10.18653/v1/2021.naacl-main.70) |  | 0 | Template filling is generally tackled by a pipeline of two separate supervised systems – one for role-filler extraction and another for template/event recognition. Since pipelines consider events in isolation, they can suffer from error propagation. We introduce a framework based on end-to-end generative transformers for this task (i.e., GTT). It naturally models the dependence between entities both within a single event and across the multiple events described in a document. Experiments... | Xinya Du, Alexander M. Rush, Claire Cardie |  |
| 147 |  |  [Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU models](https://doi.org/10.18653/v1/2021.naacl-main.71) |  | 0 | Recent studies indicate that NLU models are prone to rely on shortcut features for prediction, without achieving true language understanding. As a result, these models fail to generalize to real-world out-of-distribution data. In this work, we show that the words in the NLU training set can be modeled as a long-tailed distribution. There are two findings: 1) NLU models have strong preference for features located at the head of the long-tailed distribution, and 2) Shortcut features are picked up... | Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong Sun, Xia Hu |  |
| 148 |  |  [On Attention Redundancy: A Comprehensive Study](https://doi.org/10.18653/v1/2021.naacl-main.72) |  | 0 | Multi-layer multi-head self-attention mechanism is widely applied in modern neural language models. Attention redundancy has been observed among attention heads but has not been deeply studied in the literature. Using BERT-base model as an example, this paper provides a comprehensive study on attention redundancy which is helpful for model interpretation and model compression. We analyze the attention redundancy with Five-Ws and How. (What) We define and focus the study on redundancy matrices... | Yuchen Bian, Jiaji Huang, Xingyu Cai, Jiahong Yuan, Kenneth Church |  |
| 149 |  |  [Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?](https://doi.org/10.18653/v1/2021.naacl-main.73) |  | 0 | Large Transformers pretrained over clinical notes from Electronic Health Records (EHR) have afforded substantial gains in performance on predictive clinical tasks. The cost of training such models (and the necessity of data access to do so) coupled with their utility motivates parameter sharing, i.e., the release of pretrained models such as ClinicalBERT. While most efforts have used deidentified EHR, many researchers have access to large sets of sensitive, non-deidentified EHR with which they... | Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, Byron C. Wallace |  |
| 150 |  |  [Low-Complexity Probing via Finding Subnetworks](https://doi.org/10.18653/v1/2021.naacl-main.74) |  | 0 | The dominant approach in probing neural networks for linguistic properties is to train a new shallow multi-layer perceptron (MLP) on top of the model’s internal representations. This approach can detect properties encoded in the model, but at the cost of adding new parameters that may learn the task directly. We instead propose a subtractive pruning-based probe, where we find an existing subnetwork that performs the linguistic task of interest. Compared to an MLP, the subnetwork probe achieves... | Victor Sanh, Alexander M. Rush |  |
| 151 |  |  [An Empirical Comparison of Instance Attribution Methods for NLP](https://doi.org/10.18653/v1/2021.naacl-main.75) |  | 0 | Widespread adoption of deep models has motivated a pressing need for approaches to interpret network outputs and to facilitate model debugging. Instance attribution methods constitute one means of accomplishing these goals by retrieving training instances that (may have) led to a particular prediction. Influence functions (IF; Koh and Liang 2017) provide machinery for doing this by quantifying the effect that perturbing individual train instances would have on a specific test prediction.... | Pouya Pezeshkpour, Sarthak Jain, Byron C. Wallace, Sameer Singh |  |
| 152 |  |  [Generalization in Instruction Following Systems](https://doi.org/10.18653/v1/2021.naacl-main.76) |  | 0 | Understanding and executing natural language instructions in a grounded domain is one of the hallmarks of artificial intelligence. In this paper, we focus on instruction understanding in the blocks world domain and investigate the language understanding abilities of two top-performing systems for the task. We aim to understand if the test performance of these models indicates an understanding of the spatial domain and of the natural language instructions relative to it, or whether they merely... | Soham Dan, Michael Zhou, Dan Roth |  |
| 153 |  |  [LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval](https://doi.org/10.18653/v1/2021.naacl-main.77) |  | 0 | Multimodal pre-training has propelled great advancement in vision-and-language research. These large-scale pre-trained models, although successful, fatefully suffer from slow inference speed due to enormous computational cost mainly from cross-modal attention in Transformer architecture. When applied to real-life applications, such latency and computation demand severely deter the practical use of pre-trained models. In this paper, we study Image-text retrieval (ITR), the most mature scenario... | Siqi Sun, YenChun Chen, Linjie Li, Shuohang Wang, Yuwei Fang, Jingjing Liu |  |
| 154 |  |  [Measuring Social Biases in Grounded Vision and Language Embeddings](https://doi.org/10.18653/v1/2021.naacl-main.78) |  | 0 | We generalize the notion of measuring social biases in word embeddings to visually grounded word embeddings. Biases are present in grounded embeddings, and indeed seem to be equally or more significant than for ungrounded embeddings. This is despite the fact that vision and language can suffer from different biases, which one might hope could attenuate the biases in both. Multiple ways exist to generalize metrics measuring bias in word embeddings to this new setting. We introduce the space of... | Candace Ross, Boris Katz, Andrei Barbu |  |
| 155 |  |  [MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences](https://doi.org/10.18653/v1/2021.naacl-main.79) |  | 0 | Human communication is multimodal in nature; it is through multiple modalities such as language, voice, and facial expressions, that opinions and emotions are expressed. Data in this domain exhibits complex multi-relational and temporal interactions. Learning from this data is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal... | Jianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu, Azaan Rehman, Amir Zadeh, Soujanya Poria, LouisPhilippe Morency |  |
| 156 |  |  [Grounding Open-Domain Instructions to Automate Web Support Tasks](https://doi.org/10.18653/v1/2021.naacl-main.80) |  | 0 | Grounding natural language instructions on the web to perform previously unseen tasks enables accessibility and automation. We introduce a task and dataset to train AI agents from open-domain, step-by-step instructions originally written for people. We build RUSS (Rapid Universal Support Service) to tackle this problem. RUSS consists of two models: First, a BERT-LSTM with pointers parses instructions to WebLang, a domain-specific language we design for grounding natural language on the web.... | Nancy Xu, Sam Masling, Michael Du, Giovanni Campagna, Larry Heck, James A. Landay, Monica Lam |  |
| 157 |  |  [Modular Networks for Compositional Instruction Following](https://doi.org/10.18653/v1/2021.naacl-main.81) |  | 0 | Standard architectures used in instruction following often struggle on novel compositions of subgoals (e.g. navigating to landmarks or picking up objects) observed during training. We propose a modular architecture for following natural language instructions that describe sequences of diverse subgoals. In our approach, subgoal modules each carry out natural language instructions for a specific subgoal type. A sequence of modules to execute is chosen by learning to segment the instructions and... | Rodolfo Corona, Daniel Fried, Coline Devin, Dan Klein, Trevor Darrell |  |
| 158 |  |  [Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information](https://doi.org/10.18653/v1/2021.naacl-main.82) |  | 0 | Vision language navigation is the task that requires an agent to navigate through a 3D environment based on natural language instructions. One key challenge in this task is to ground instructions with the current visual information that the agent perceives. Most of the existing work employs soft attention over individual words to locate the instruction required for the next action. However, different words have different functions in a sentence (e.g., modifiers convey attributes, verbs convey... | Jialu Li, Hao Tan, Mohit Bansal |  |
| 159 |  |  [Improving Pretrained Models for Zero-shot Multi-label Text Classification through Reinforced Label Hierarchy Reasoning](https://doi.org/10.18653/v1/2021.naacl-main.83) |  | 0 | Exploiting label hierarchies has become a promising approach to tackling the zero-shot multi-label text classification (ZS-MTC) problem. Conventional methods aim to learn a matching model between text and labels, using a graph encoder to incorporate label hierarchies to obtain effective label representations (Rios and Kavuluru, 2018). More recently, pretrained models like BERT (Devlin et al., 2018) have been used to convert classification tasks into a textual entailment task (Yin et al., 2019).... | Hui Liu, Danqing Zhang, Bing Yin, Xiaodan Zhu |  |
| 160 |  |  [Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach](https://doi.org/10.18653/v1/2021.naacl-main.84) |  | 0 | Fine-tuned pre-trained language models (LMs) have achieved enormous success in many natural language processing (NLP) tasks, but they still require excessive labeled data in the fine-tuning stage. We study the problem of fine-tuning pre-trained LMs using only weak supervision, without any labeled data. This problem is challenging because the high capacity of LMs makes them prone to overfitting the noisy labels generated by weak supervision. To address this problem, we develop a contrastive... | Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, Chao Zhang |  |
| 161 |  |  [Posterior Differential Regularization with f-divergence for Improving Model Robustness](https://doi.org/10.18653/v1/2021.naacl-main.85) |  | 0 | We address the problem of enhancing model robustness through regularization. Specifically, we focus on methods that regularize the model posterior difference between clean and noisy inputs. Theoretically, we provide a connection of two recent methods, Jacobian Regularization and Virtual Adversarial Training, under this framework. Additionally, we generalize the posterior differential regularization to the family of f-divergences and characterize the overall framework in terms of the Jacobian... | Hao Cheng, Xiaodong Liu, Lis Pereira, Yaoliang Yu, Jianfeng Gao |  |
| 162 |  |  [Understanding Hard Negatives in Noise Contrastive Estimation](https://doi.org/10.18653/v1/2021.naacl-main.86) |  | 0 | The choice of negative examples is important in noise contrastive estimation. Recent works find that hard negatives—highest-scoring incorrect examples under the model—are effective in practice, but they are used without a formal justification. We develop analytical tools to understand the role of hard negatives. Specifically, we view the contrastive loss as a biased estimator of the gradient of the cross-entropy loss, and show both theoretically and empirically that setting the negative... | Wenzheng Zhang, Karl Stratos |  |
| 163 |  |  [Certified Robustness to Word Substitution Attack with Differential Privacy](https://doi.org/10.18653/v1/2021.naacl-main.87) |  | 0 | The robustness and security of natural language processing (NLP) models are significantly important in real-world applications. In the context of text classification tasks, adversarial examples can be designed by substituting words with synonyms under certain semantic and syntactic constraints, such that a well-trained model will give a wrong prediction. Therefore, it is crucial to develop techniques to provide a rigorous and provable robustness guarantee against such attacks. In this paper, we... | Wenjie Wang, Pengfei Tang, Jian Lou, Li Xiong |  |
| 164 |  |  [DReCa: A General Task Augmentation Strategy for Few-Shot Natural Language Inference](https://doi.org/10.18653/v1/2021.naacl-main.88) |  | 0 | Meta-learning promises few-shot learners that can adapt to new distributions by repurposing knowledge acquired from previous training. However, we believe meta-learning has not yet succeeded in NLP due to the lack of a well-defined task distribution, leading to attempts that treat datasets as tasks. Such an ad hoc task distribution causes problems of quantity and quality. Since there’s only a handful of datasets for any NLP problem, meta-learners tend to overfit their adaptation mechanism and,... | Shikhar Murty, Tatsunori Hashimoto, Christopher D. Manning |  |
| 165 |  |  [Harnessing Multilinguality in Unsupervised Machine Translation for Rare Languages](https://doi.org/10.18653/v1/2021.naacl-main.89) |  | 0 | Unsupervised translation has reached impressive performance on resource-rich language pairs such as English-French and English-German. However, early studies have shown that in more realistic settings involving low-resource, rare languages, unsupervised translation performs poorly, achieving less than 3.0 BLEU. In this work, we show that multilinguality is critical to making unsupervised systems practical for low-resource settings. In particular, we present a single model for 5 low-resource... | Xavier Garcia, Aditya Siddhant, Orhan Firat, Ankur P. Parikh |  |
| 166 |  |  [Macro-Average: Rare Types Are Important Too](https://doi.org/10.18653/v1/2021.naacl-main.90) |  | 0 | While traditional corpus-level evaluation metrics for machine translation (MT) correlate well with fluency, they struggle to reflect adequacy. Model-based MT metrics trained on segment-level human judgments have emerged as an attractive replacement due to strong correlation results. These models, however, require potentially expensive re-training for new domains and languages. Furthermore, their decisions are inherently non-transparent and appear to reflect unwelcome biases. We explore the... | Thamme Gowda, Weiqiu You, Constantine Lignos, Jonathan May |  |
| 167 |  |  [Assessing Reference-Free Peer Evaluation for Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.91) |  | 0 | Reference-free evaluation has the potential to make machine translation evaluation substantially more scalable, allowing us to pivot easily to new languages or domains. It has been recently shown that the probabilities given by a large, multilingual model can achieve state of the art results when used as a reference-free metric. We experiment with various modifications to this model, and demonstrate that by scaling it up we can match the performance of BLEU. We analyze various potential... | Sweta Agrawal, George F. Foster, Markus Freitag, Colin Cherry |  |
| 168 |  |  [The Curious Case of Hallucinations in Neural Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.92) |  | 0 | In this work, we study hallucinations in Neural Machine Translation (NMT), which lie at an extreme end on the spectrum of NMT pathologies. Firstly, we connect the phenomenon of hallucinations under source perturbation to the Long-Tail theory of Feldman, and present an empirically validated hypothesis that explains hallucinations under source perturbation. Secondly, we consider hallucinations under corpus-level noise (without any source perturbation) and demonstrate that two prominent types of... | Vikas Raunak, Arul Menezes, Marcin JunczysDowmunt |  |
| 169 |  |  [Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution](https://doi.org/10.18653/v1/2021.naacl-main.93) |  | 0 | We propose a straightforward vocabulary adaptation scheme to extend the language capacity of multilingual machine translation models, paving the way towards efficient continual learning for multilingual machine translation. Our approach is suitable for large-scale datasets, applies to distant languages with unseen scripts, incurs only minor degradation on the translation performance for the original language pairs and provides competitive performance even in the case where we only possess... | Xavier Garcia, Noah Constant, Ankur P. Parikh, Orhan Firat |  |
| 170 |  |  [Towards Modeling the Style of Translators in Neural Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.94) |  | 0 | One key ingredient of neural machine translation is the use of large datasets from different domains and resources (e.g. Europarl, TED talks). These datasets contain documents translated by professional translators using different but consistent translation styles. Despite that, the model is usually trained in a way that neither explicitly captures the variety of translation styles present in the data nor translates new data in different and controllable styles. In this work, we investigate... | Yue Wang, Cuong Hoang, Marcello Federico |  |
| 171 |  |  [Self-Supervised Test-Time Learning for Reading Comprehension](https://doi.org/10.18653/v1/2021.naacl-main.95) |  | 0 | Recent work on unsupervised question answering has shown that models can be trained with procedurally generated question-answer pairs and can achieve performance competitive with supervised methods. In this work, we consider the task of unsupervised reading comprehension and present a method that performs “test-time learning” (TTL) on a given context (text passage), without requiring training on large-scale human-authored datasets containing context-question-answer triplets. This method... | Pratyay Banerjee, Tejas Gokhale, Chitta Baral |  |
| 172 |  |  [Capturing Row and Column Semantics in Transformer Based Question Answering over Tables](https://doi.org/10.18653/v1/2021.naacl-main.96) |  | 0 | Transformer based architectures are recently used for the task of answering questions over tables. In order to improve the accuracy on this task, specialized pre-training techniques have been developed and applied on millions of open-domain web tables. In this paper, we propose two novel approaches demonstrating that one can achieve superior performance on table QA task without even using any of these specialized pre-training techniques. The first model, called RCI interaction, leverages a... | Michael R. Glass, Mustafa Canim, Alfio Gliozzo, Saneem A. Chemmengath, Vishwajeet Kumar, Rishav Chakravarti, Avi Sil, Feifei Pan, Samarth Bharadwaj, Nicolas Rodolfo Fauceglia |  |
| 173 |  |  [Explainable Multi-hop Verbal Reasoning Through Internal Monologue](https://doi.org/10.18653/v1/2021.naacl-main.97) |  | 0 | Many state-of-the-art (SOTA) language models have achieved high accuracy on several multi-hop reasoning problems. However, these approaches tend to not be interpretable because they do not make the intermediate reasoning steps explicit. Moreover, models trained on simpler tasks tend to fail when directly tested on more complex problems. We propose the Explainable multi-hop Verbal Reasoner (EVR) to solve these limitations by (a) decomposing multi-hop reasoning problems into several simple ones,... | Zhengzhong Liang, Steven Bethard, Mihai Surdeanu |  |
| 174 |  |  [Robust Question Answering Through Sub-part Alignment](https://doi.org/10.18653/v1/2021.naacl-main.98) |  | 0 | Current textual question answering (QA) models achieve strong performance on in-domain test sets, but often do so by fitting surface-level patterns, so they fail to generalize to out-of-distribution settings. To make a more robust and understandable QA system, we model question answering as an alignment problem. We decompose both the question and context into smaller units based on off-the-shelf semantic representations (here, semantic roles), and align the question to a subgraph of the context... | Jifan Chen, Greg Durrett |  |
| 175 |  |  [Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models](https://doi.org/10.18653/v1/2021.naacl-main.99) |  | 0 | We propose a general framework called Text Modular Networks(TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions... | Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, Ashish Sabharwal |  |
| 176 |  |  [RECONSIDER: Improved Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering](https://doi.org/10.18653/v1/2021.naacl-main.100) |  | 0 | State-of-the-art Machine Reading Comprehension (MRC) models for Open-domain Question Answering (QA) are typically trained for span selection using distantly supervised positive examples and heuristically retrieved negative examples. This training scheme possibly explains empirical observations that these models achieve a high recall amongst their top few predictions, but a low overall accuracy, motivating the need for answer re-ranking. We develop a successful re-ranking approach (RECONSIDER)... | Srinivasan Iyer, Sewon Min, Yashar Mehdad, Wentau Yih |  |
| 177 |  |  [On the Transferability of Minimal Prediction Preserving Inputs in Question Answering](https://doi.org/10.18653/v1/2021.naacl-main.101) |  | 0 | Recent work (Feng et al., 2018) establishes the presence of short, uninterpretable input fragments that yield high confidence and accuracy in neural models. We refer to these as Minimal Prediction Preserving Inputs (MPPIs). In the context of question answering, we investigate competing hypotheses for the existence of MPPIs, including poor posterior calibration of neural models, lack of pretraining, and “dataset bias” (where a model learns to attend to spurious, non-generalizable cues in the... | Shayne Longpre, Yi Lu, Chris DuBois |  |
| 178 |  |  [Understanding by Understanding Not: Modeling Negation in Language Models](https://doi.org/10.18653/v1/2021.naacl-main.102) |  | 0 | Negation is a core construction in natural language. Despite being very successful on many tasks, state-of-the-art pre-trained language models often handle negation incorrectly. To improve language models in this regard, we propose to augment the language modeling objective with an unlikelihood objective that is based on negated generic sentences from a raw text corpus. By training BERT with the resulting combined objective we reduce the mean top 1 error rate to 4% on the negated LAMA dataset.... | Arian Hosseini, Siva Reddy, Dzmitry Bahdanau, R. Devon Hjelm, Alessandro Sordoni, Aaron C. Courville |  |
| 179 |  |  [DuoRAT: Towards Simpler Text-to-SQL Models](https://doi.org/10.18653/v1/2021.naacl-main.103) |  | 0 | Recent neural text-to-SQL models can effectively translate natural language questions to corresponding SQL queries on unseen databases. Working mostly on the Spider dataset, researchers have proposed increasingly sophisticated solutions to the problem. Contrary to this trend, in this paper we focus on simplifications. We begin by building DuoRAT, a re-implementation of the state-of-the-art RAT-SQL model that unlike RAT-SQL is using only relation-aware or vanilla transformers as the building... | Torsten Scholak, Raymond Li, Dzmitry Bahdanau, Harm de Vries, Chris Pal |  |
| 180 |  |  [Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization](https://doi.org/10.18653/v1/2021.naacl-main.104) |  | 0 | Natural Language Inference (NLI) has garnered significant attention in recent years; however, the promise of applying NLI breakthroughs to other downstream NLP tasks has remained unfulfilled. In this work, we use the multiple-choice reading comprehension (MCRC) and checking factual correctness of textual summarization (CFCS) tasks to investigate potential reasons for this. Our findings show that: (1) the relatively shorter length of premises in traditional NLI datasets is the primary challenge... | Anshuman Mishra, Dhruvesh Patel, Aparna Vijayakumar, Xiang Lorraine Li, Pavan Kapanipathi, Kartik Talamadupula |  |
| 181 |  |  [Structure-Grounded Pretraining for Text-to-SQL](https://doi.org/10.18653/v1/2021.naacl-main.105) |  | 0 | Learning to capture text-table alignment is essential for tasks like text-to-SQL. A model needs to correctly recognize natural language references to columns and values and to ground them in the given database schema. In this paper, we present a novel weakly supervised Structure-Grounded pretraining framework (STRUG) for text-to-SQL that can effectively learn to capture text-table alignment based on a parallel text-table corpus. We identify a set of novel pretraining tasks: column grounding,... | Xiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr Polozov, Huan Sun, Matthew Richardson |  |
| 182 |  |  [Incremental Few-shot Text Classification with Multi-round New Classes: Formulation, Dataset and System](https://doi.org/10.18653/v1/2021.naacl-main.106) |  | 0 | Text classification is usually studied by labeling natural language texts with relevant categories from a predefined set. In the real world, new classes might keep challenging the existing system with limited labeled data. The system should be intelligent enough to recognize upcoming new classes with a few examples. In this work, we define a new task in the NLP domain, incremental few-shot text classification, where the system incrementally handles multiple rounds of new classes. For each... | Congying Xia, Wenpeng Yin, Yihao Feng, Philip S. Yu |  |
| 183 |  |  [Temporal Reasoning on Implicit Events from Distant Supervision](https://doi.org/10.18653/v1/2021.naacl-main.107) |  | 0 | We propose TRACIE, a novel temporal reasoning dataset that evaluates the degree to which systems understand implicit events—events that are not mentioned explicitly in natural language text but can be inferred from it. This introduces a new challenge in temporal reasoning research, where prior work has focused on explicitly mentioned events. Human readers can infer implicit events via commonsense reasoning, resulting in a more comprehensive understanding of the situation and, consequently,... | Ben Zhou, Kyle Richardson, Qiang Ning, Tushar Khot, Ashish Sabharwal, Dan Roth |  |
| 184 |  |  [Disentangling Semantics and Syntax in Sentence Embeddings with Pre-trained Language Models](https://doi.org/10.18653/v1/2021.naacl-main.108) |  | 0 | Pre-trained language models have achieved huge success on a wide range of NLP tasks. However, contextual representations from pre-trained models contain entangled semantic and syntactic information, and therefore cannot be directly used to derive useful semantic sentence embeddings for some tasks. Paraphrase pairs offer an effective way of learning the distinction between semantics and syntax, as they naturally share semantics and often vary in syntax. In this work, we present ParaBART, a... | James Y. Huang, KuanHao Huang, KaiWei Chang |  |
| 185 |  |  [Structure-Aware Abstractive Conversation Summarization via Discourse and Action Graphs](https://doi.org/10.18653/v1/2021.naacl-main.109) |  | 0 | Abstractive conversation summarization has received much attention recently. However, these generated summaries often suffer from insufficient, redundant, or incorrect content, largely due to the unstructured and complex characteristics of human-human interactions. To this end, we propose to explicitly model the rich structures in conversations for more precise and accurate conversation summarization, by first incorporating discourse relations between utterances and action triples... | Jiaao Chen, Diyi Yang |  |
| 186 |  |  [A New Approach to Overgenerating and Scoring Abstractive Summaries](https://doi.org/10.18653/v1/2021.naacl-main.110) |  | 0 | We propose a new approach to generate multiple variants of the target summary with diverse content and varying lengths, then score and select admissible ones according to users’ needs. Abstractive summarizers trained on single reference summaries may struggle to produce outputs that achieve multiple desirable properties, i.e., capturing the most important information, being faithful to the original, grammatical and fluent. In this paper, we propose a two-staged strategy to generate a diverse... | Kaiqiang Song, Bingqing Wang, Zhe Feng, Fei Liu |  |
| 187 |  |  [D2S: Document-to-Slide Generation Via Query-Based Text Summarization](https://doi.org/10.18653/v1/2021.naacl-main.111) |  | 0 | Presentations are critical for communication in all areas of our lives, yet the creation of slide decks is often tedious and time-consuming. There has been limited research aiming to automate the document-to-slides generation process and all face a critical challenge: no publicly available dataset for training and benchmarking. In this work, we first contribute a new dataset, SciDuet, consisting of pairs of papers and their corresponding slides decks from recent years’ NLP and ML conferences... | Edward Sun, Yufang Hou, Dakuo Wang, Yunfeng Zhang, Nancy Xin Ru Wang |  |
| 188 |  |  [Efficient Attentions for Long Document Summarization](https://doi.org/10.18653/v1/2021.naacl-main.112) |  | 0 | The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose Hepos, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with Hepos, we are able to process ten times more tokens than existing models that use full attentions.... | Luyang Huang, Shuyang Cao, Nikolaus Nova Parulian, Heng Ji, Lu Wang |  |
| 189 |  |  [RefSum: Refactoring Neural Summarization](https://doi.org/10.18653/v1/2021.naacl-main.113) |  | 0 | Although some recent works show potential complementarity among different state-of-the-art systems, few works try to investigate this problem in text summarization. Researchers in other areas commonly refer to the techniques of reranking or stacking to approach this problem. In this work, we highlight several limitations of previous methods, which motivates us to present a new framework Refactor that provides a unified view of text summarization and summaries combination. Experimentally, we... | Yixin Liu, ZiYi Dou, Pengfei Liu |  |
| 190 |  |  [Annotating and Modeling Fine-grained Factuality in Summarization](https://doi.org/10.18653/v1/2021.naacl-main.114) |  | 0 | Recent pre-trained abstractive summarization systems have started to achieve credible performance, but a major barrier to their use in practice is their propensity to output summaries that are not faithful to the input and that contain factual errors. While a number of annotated datasets and statistical models for assessing factuality have been explored, there is no clear picture of what errors are most important to target or where current techniques are succeeding and failing. We explore both... | Tanya Goyal, Greg Durrett |  |
| 191 |  |  [Larger-Context Tagging: When and Why Does It Work?](https://doi.org/10.18653/v1/2021.naacl-main.115) |  | 0 | The development of neural networks and pretraining techniques has spawned many sentence-level tagging systems that achieved superior performance on typical benchmarks. However, a relatively less discussed topic is what if more context information is introduced into current top-scoring tagging systems. Although several existing works have attempted to shift tagging systems from sentence-level to document-level, there is still no consensus conclusion about when and why it works, which limits the... | Jinlan Fu, Liangjing Feng, Qi Zhang, Xuanjing Huang, Pengfei Liu |  |
| 192 |  |  [Neural Sequence Segmentation as Determining the Leftmost Segments](https://doi.org/10.18653/v1/2021.naacl-main.116) |  | 0 | Prior methods to text segmentation are mostly at token level. Despite the adequacy, this nature limits their full potential to capture the long-term dependencies among segments. In this work, we propose a novel framework that incrementally segments natural language sentences at segment level. For every step in segmentation, it recognizes the leftmost segment of the remaining sequence. Implementations involve LSTM-minus technique to construct the phrase representations and recurrent neural... | Yangming Li, Lemao Liu, Kaisheng Yao |  |
| 193 |  |  [PCFGs Can Do Better: Inducing Probabilistic Context-Free Grammars with Many Symbols](https://doi.org/10.18653/v1/2021.naacl-main.117) |  | 0 | Probabilistic context-free grammars (PCFGs) with neural parameterization have been shown to be effective in unsupervised phrase-structure grammar induction. However, due to the cubic computational complexity of PCFG representation and parsing, previous approaches cannot scale up to a relatively large number of (nonterminal and preterminal) symbols. In this work, we present a new parameterization form of PCFGs based on tensor decomposition, which has at most quadratic computational complexity in... | Songlin Yang, Yanpeng Zhao, Kewei Tu |  |
| 194 |  |  [GEMNET: Effective Gated Gazetteer Representations for Recognizing Complex Entities in Low-context Input](https://doi.org/10.18653/v1/2021.naacl-main.118) |  | 0 | Named Entity Recognition (NER) remains difficult in real-world settings; current challenges include short texts (low context), emerging entities, and complex entities (e.g. movie names). Gazetteer features can help, but results have been mixed due to challenges with adding extra features, and a lack of realistic evaluation data. It has been shown that including gazetteer features can cause models to overuse or underuse them, leading to poor generalization. We propose GEMNET, a novel approach... | Tao Meng, Anjie Fang, Oleg Rokhlenko, Shervin Malmasi |  |
| 195 |  |  [Video-aided Unsupervised Grammar Induction](https://doi.org/10.18653/v1/2021.naacl-main.119) |  | 0 | We investigate video-aided grammar induction, which learns a constituency parser from both unlabeled text and its corresponding video. Existing methods of multi-modal grammar induction focus on grammar induction from text-image pairs, with promising results showing that the information from static images is useful in induction. However, videos provide even richer information, including not only static objects but also actions and state changes useful for inducing verb phrases. In this paper, we... | Songyang Zhang, Linfeng Song, Lifeng Jin, Kun Xu, Dong Yu, Jiebo Luo |  |
| 196 |  |  [Generating Negative Samples by Manipulating Golden Responses for Unsupervised Learning of a Response Evaluation Model](https://doi.org/10.18653/v1/2021.naacl-main.120) |  | 0 | Evaluating the quality of responses generated by open-domain conversation systems is a challenging task. This is partly because there can be multiple appropriate responses to a given dialogue history. Reference-based metrics that rely on comparisons to a set of known correct responses often fail to account for this variety, and consequently correlate poorly with human judgment. To address this problem, researchers have investigated the possibility of assessing response quality without using a... | ChaeHun Park, Eugene Jang, Wonsuk Yang, Jong Park |  |
| 197 |  |  [How Robust are Fact Checking Systems on Colloquial Claims?](https://doi.org/10.18653/v1/2021.naacl-main.121) |  | 0 | Knowledge is now starting to power neural dialogue agents. At the same time, the risk of misinformation and disinformation from dialogue agents also rises. Verifying the veracity of information from formal sources are widely studied in computational fact checking. In this work, we ask: How robust are fact checking systems on claims in colloquial style? We aim to open up new discussions in the intersection of fact verification and dialogue safety. In order to investigate how fact checking... | Byeongchang Kim, Hyunwoo Kim, Seokhee Hong, Gunhee Kim |  |
| 198 |  |  [Fine-grained Post-training for Improving Retrieval-based Dialogue Systems](https://doi.org/10.18653/v1/2021.naacl-main.122) |  | 0 | Retrieval-based dialogue systems display an outstanding performance when pre-trained language models are used, which includes bidirectional encoder representations from transformers (BERT). During the multi-turn response selection, BERT focuses on training the relationship between the context with multiple utterances and the response. However, this method of training is insufficient when considering the relations between each utterance in the context. This leads to a problem of not completely... | Janghoon Han, Taesuk Hong, Byoungjae Kim, Youngjoong Ko, Jungyun Seo |  |
| 199 |  |  [Put Chatbot into Its Interlocutor's Shoes: New Framework to Learn Chatbot Responding with Intention](https://doi.org/10.18653/v1/2021.naacl-main.123) |  | 0 | Most chatbot literature that focuses on improving the fluency and coherence of a chatbot, is dedicated to making chatbots more human-like. However, very little work delves into what really separates humans from chatbots – humans intrinsically understand the effect their responses have on the interlocutor and often respond with an intention such as proposing an optimistic view to make the interlocutor feel better. This paper proposes an innovative framework to train chatbots to possess... | Hsuan Su, JiunHao Jhan, FanYun Sun, Saurav Sahay, Hungyi Lee |  |
| 200 |  |  [Adding Chit-Chat to Enhance Task-Oriented Dialogues](https://doi.org/10.18653/v1/2021.naacl-main.124) |  | 0 | Existing dialogue corpora and models are typically designed under two disjoint motives: while task-oriented systems focus on achieving functional goals (e.g., booking hotels), open-domain chatbots aim at making socially engaging conversations. In this work, we propose to integrate both types of systems by Adding Chit-Chat to ENhance Task-ORiented dialogues (ACCENTOR), with the goal of making virtual assistant conversations more engaging and interactive. Specifically, we propose a Human <-> AI... | Kai Sun, Seungwhan Moon, Paul A. Crook, Stephen Roller, Becka Silvert, Bing Liu, Zhiguang Wang, Honglei Liu, Eunjoon Cho, Claire Cardie |  |
| 201 |  |  [Incorporating Syntax and Semantics in Coreference Resolution with Heterogeneous Graph Attention Network](https://doi.org/10.18653/v1/2021.naacl-main.125) |  | 0 | External syntactic and semantic information has been largely ignored by existing neural coreference resolution models. In this paper, we present a heterogeneous graph-based model to incorporate syntactic and semantic structures of sentences. The proposed graph contains a syntactic sub-graph where tokens are connected based on a dependency tree, and a semantic sub-graph that contains arguments and predicates as nodes and semantic role labels as edges. By applying a graph attention network, we... | Fan Jiang, Trevor Cohn |  |
| 202 |  |  [Context Tracking Network: Graph-based Context Modeling for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2021.naacl-main.126) |  | 0 | Implicit discourse relation recognition (IDRR) aims to identify logical relations between two adjacent sentences in the discourse. Existing models fail to fully utilize the contextual information which plays an important role in interpreting each local sentence. In this paper, we thus propose a novel graph-based Context Tracking Network (CT-Net) to model the discourse context for IDRR. The CT-Net firstly converts the discourse into the paragraph association graph (PAG), where each sentence... | Yingxue Zhang, Fandong Meng, Peng Li, Ping Jian, Jie Zhou |  |
| 203 |  |  [Improving Neural RST Parsing Model with Silver Agreement Subtrees](https://doi.org/10.18653/v1/2021.naacl-main.127) |  | 0 | Most of the previous Rhetorical Structure Theory (RST) parsing methods are based on supervised learning such as neural networks, that require an annotated corpus of sufficient size and quality. However, the RST Discourse Treebank (RST-DT), the benchmark corpus for RST parsing in English, is small due to the costly annotation of RST trees. The lack of large annotated training data causes poor performance especially in relation labeling. Therefore, we propose a method for improving neural RST... | Naoki Kobayashi, Tsutomu Hirao, Hidetaka Kamigaito, Manabu Okumura, Masaaki Nagata |  |
| 204 |  |  [RST Parsing from Scratch](https://doi.org/10.18653/v1/2021.naacl-main.128) |  | 0 | We introduce a novel top-down end-to-end formulation of document level discourse parsing in the Rhetorical Structure Theory (RST) framework. In this formulation, we consider discourse parsing as a sequence of splitting decisions at token boundaries and use a seq2seq network to model the splitting decisions. Our framework facilitates discourse parsing from scratch without requiring discourse segmentation as a prerequisite; rather, it yields segmentation as part of the parsing process. Our... | ThanhTung Nguyen, XuanPhi Nguyen, Shafiq R. Joty, Xiaoli Li |  |
| 205 |  |  [Did they answer? Subjective acts and intents in conversational discourse](https://doi.org/10.18653/v1/2021.naacl-main.129) |  | 0 | Discourse signals are often implicit, leaving it up to the interpreter to draw the required inferences. At the same time, discourse is embedded in a social context, meaning that interpreters apply their own assumptions and beliefs when resolving these inferences, leading to multiple, valid interpretations. However, current discourse data and frameworks ignore the social aspect, expecting only a single ground truth. We present the first discourse dataset with multiple and subjective... | Elisa Ferracane, Greg Durrett, Junyi Jessy Li, Katrin Erk |  |
| 206 |  |  [Evaluating the Impact of a Hierarchical Discourse Representation on Entity Coreference Resolution Performance](https://doi.org/10.18653/v1/2021.naacl-main.130) |  | 0 | Recent work on entity coreference resolution (CR) follows current trends in Deep Learning applied to embeddings and relatively simple task-related features. SOTA models do not make use of hierarchical representations of discourse structure. In this work, we leverage automatically constructed discourse parse trees within a neural approach and demonstrate a significant improvement on two benchmark entity coreference-resolution datasets. We explore how the impact varies depending upon the type of... | Sopan Khosla, James Fiacco, Carolyn P. Rosé |  |
| 207 |  |  [Bridging Resolution: Making Sense of the State of the Art](https://doi.org/10.18653/v1/2021.naacl-main.131) |  | 0 | While Yu and Poesio (2020) have recently demonstrated the superiority of their neural multi-task learning (MTL) model to rule-based approaches for bridging anaphora resolution, there is little understanding of (1) how it is better than the rule-based approaches (e.g., are the two approaches making similar or complementary mistakes?) and (2) what should be improved. To shed light on these issues, we (1) propose a hybrid rule-based and MTL approach that would enable a better understanding of... | Hideo Kobayashi, Vincent Ng |  |
| 208 |  |  [Explicitly Modeling Syntax in Language Models with Incremental Parsing and a Dynamic Oracle](https://doi.org/10.18653/v1/2021.naacl-main.132) |  | 0 | Syntax is fundamental to our thinking about language. Failing to capture the structure of input language could lead to generalization problems and over-parametrization. In the present work, we propose a new syntax-aware language model: Syntactic Ordered Memory (SOM). The model explicitly models the structure with an incremental parser and maintains the conditional probability setting of a standard language model (left-to-right). To train the incremental parser and avoid exposure bias, we also... | Yikang Shen, Shawn Tan, Alessandro Sordoni, Siva Reddy, Aaron C. Courville |  |
| 209 |  |  [Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.133) |  | 0 | Policy gradient algorithms have found wide adoption in NLP, but have recently become subject to criticism, doubting their suitability for NMT. Choshen et al. (2020) identify multiple weaknesses and suspect that their success is determined by the shape of output distributions rather than the reward. In this paper, we revisit these claims and study them under a wider range of configurations. Our experiments on in-domain and cross-domain adaptation reveal the importance of exploration and reward... | Samuel Kiegeland, Julia Kreutzer |  |
| 210 |  |  [Learning to Organize a Bag of Words into Sentences with Neural Networks: An Empirical Study](https://doi.org/10.18653/v1/2021.naacl-main.134) |  | 0 | Sequential information, a.k.a., orders, is assumed to be essential for processing a sequence with recurrent neural network or convolutional neural network based encoders. However, is it possible to encode natural languages without orders? Given a bag of words from a disordered sentence, humans may still be able to understand what those words mean by reordering or reconstructing them. Inspired by such an intuition, in this paper, we perform a study to investigate how “order” information takes... | Chongyang Tao, Shen Gao, Juntao Li, Yansong Feng, Dongyan Zhao, Rui Yan |  |
| 211 |  |  [Mask Attention Networks: Rethinking and Strengthen Transformer](https://doi.org/10.18653/v1/2021.naacl-main.135) |  | 0 | Transformer is an attention-based neural network, which consists of two sublayers, namely, Self-Attention Network (SAN) and Feed-Forward Network (FFN). Existing research explores to enhance the two sublayers separately to improve the capability of Transformer for text representation. In this paper, we present a novel understanding of SAN and FFN as Mask Attention Networks (MANs) and show that they are two special cases of MANs with static mask matrices. However, their static mask matrices limit... | Zhihao Fan, Yeyun Gong, Dayiheng Liu, Zhongyu Wei, Siyuan Wang, Jian Jiao, Nan Duan, Ruofei Zhang, Xuanjing Huang |  |
| 212 |  |  [ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding](https://doi.org/10.18653/v1/2021.naacl-main.136) |  | 0 | Coarse-grained linguistic information, such as named entities or phrases, facilitates adequately representation learning in pre-training. Previous works mainly focus on extending the objective of BERT’s Masked Language Modeling (MLM) from masking individual tokens to contiguous sequences of n tokens. We argue that such contiguously masking method neglects to model the intra-dependencies and inter-relation of coarse-grained linguistic information. As an alternative, we propose ERNIE-Gram, an... | Dongling Xiao, YuKun Li, Han Zhang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang |  |
| 213 |  |  [Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models](https://doi.org/10.18653/v1/2021.naacl-main.137) |  | 0 | Chinese pre-trained language models usually process text as a sequence of characters, while ignoring more coarse granularity, e.g., words. In this work, we propose a novel pre-training paradigm for Chinese — Lattice-BERT, which explicitly incorporates word representations along with characters, thus can model a sentence in a multi-granularity manner. Specifically, we construct a lattice graph from the characters and words in a sentence and feed all these text units into transformers. We design... | Yuxuan Lai, Yijia Liu, Yansong Feng, Songfang Huang, Dongyan Zhao |  |
| 214 |  |  [Modeling Event Plausibility with Consistent Conceptual Abstraction](https://doi.org/10.18653/v1/2021.naacl-main.138) |  | 0 | Understanding natural language requires common sense, one aspect of which is the ability to discern the plausibility of events. While distributional models—most recently pre-trained, Transformer language models—have demonstrated improvements in modeling event plausibility, their performance still falls short of humans’. In this work, we show that Transformer-based plausibility models are markedly inconsistent across the conceptual classes of a lexical hierarchy, inferring that “a person... | Ian Porada, Kaheer Suleman, Adam Trischler, Jackie Chi Kit Cheung |  |
| 215 |  |  [UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus](https://doi.org/10.18653/v1/2021.naacl-main.139) |  | 0 | Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have achieved state-of-the-art results in biomedical natural language processing tasks by focusing their pre-training process on domain-specific corpora. However, such models do not take into consideration structured expert domain knowledge from a knowledge base. We introduce UmlsBERT, a contextual embedding model that integrates domain knowledge during the pre-training process via a novel knowledge augmentation strategy.... | George Michalopoulos, Yuanxin Wang, Hussam Kaka, Helen H. Chen, Alexander Wong |  |
| 216 |  |  [Field Embedding: A Unified Grain-Based Framework for Word Representation](https://doi.org/10.18653/v1/2021.naacl-main.140) |  | 0 | Word representations empowered with additional linguistic information have been widely studied and proved to outperform traditional embeddings. Current methods mainly focus on learning embeddings for words while embeddings of linguistic information (referred to as grain embeddings) are discarded after the learning. This work proposes a framework field embedding to jointly learn both word and grain embeddings by incorporating morphological, phonetic, and syntactical linguistic fields. The... | Junjie Luo, Xi Chen, Jichao Sun, Yuejia Xiang, Ningyu Zhang, Xiang Wan |  |
| 217 |  |  [MelBERT: Metaphor Detection via Contextualized Late Interaction using Metaphorical Identification Theories](https://doi.org/10.18653/v1/2021.naacl-main.141) |  | 0 | Automated metaphor detection is a challenging task to identify the metaphorical expression of words in a sentence. To tackle this problem, we adopt pre-trained contextualized models, e.g., BERT and RoBERTa. To this end, we propose a novel metaphor detection model, namely metaphor-aware late interaction over BERT (MelBERT). Our model not only leverages contextualized word representation but also benefits from linguistic metaphor identification theories to detect whether the target word is... | Minjin Choi, Sunkyung Lee, Eunseong Choi, Heesoo Park, Junhyuk Lee, Dongwon Lee, Jongwuk Lee |  |
| 218 |  |  [Non-Parametric Few-Shot Learning for Word Sense Disambiguation](https://doi.org/10.18653/v1/2021.naacl-main.142) |  | 0 | Word sense disambiguation (WSD) is a long-standing problem in natural language processing. One significant challenge in supervised all-words WSD is to classify among senses for a majority of words that lie in the long-tail distribution. For instance, 84% of the annotated words have less than 10 examples in the SemCor training data. This issue is more pronounced as the imbalance occurs in both word and sense distributions. In this work, we propose MetricWSD, a non-parametric few-shot learning... | Howard Chen, Mengzhou Xia, Danqi Chen |  |
| 219 |  |  [Why Do Document-Level Polarity Classifiers Fail?](https://doi.org/10.18653/v1/2021.naacl-main.143) |  | 0 | Machine learning solutions are often criticized for the lack of explanation of their successes and failures. Understanding which instances are misclassified and why is essential to improve the learning process. This work helps to fill this gap by proposing a methodology to characterize, quantify and measure the impact of hard instances in the task of polarity classification of movie reviews. We characterize such instances into two categories: neutrality, where the text does not convey a clear... | Karen S. Martins, Pedro O. S. Vaz de Melo, Rodrygo L. T. Santos |  |
| 220 |  |  [A Unified Span-Based Approach for Opinion Mining with Syntactic Constituents](https://doi.org/10.18653/v1/2021.naacl-main.144) |  | 0 | Fine-grained opinion mining (OM) has achieved increasing attraction in the natural language processing (NLP) community, which aims to find the opinion structures of “Who expressed what opinions towards what” in one sentence. In this work, motivated by its span-based representations of opinion expressions and roles, we propose a unified span-based approach for the end-to-end OM setting. Furthermore, inspired by the unified span-based formalism of OM and constituent parsing, we explore two... | Qingrong Xia, Bo Zhang, Rui Wang, Zhenghua Li, Yue Zhang, Fei Huang, Luo Si, Min Zhang |  |
| 221 |  |  [Target-specified Sequence Labeling with Multi-head Self-attention for Target-oriented Opinion Words Extraction](https://doi.org/10.18653/v1/2021.naacl-main.145) |  | 0 | Opinion target extraction and opinion term extraction are two fundamental tasks in Aspect Based Sentiment Analysis (ABSA). Many recent works on ABSA focus on Target-oriented Opinion Words (or Terms) Extraction (TOWE), which aims at extracting the corresponding opinion words for a given opinion target. TOWE can be further applied to Aspect-Opinion Pair Extraction (AOPE) which aims at extracting aspects (i.e., opinion targets) and opinion terms in pairs. In this paper, we propose Target-Specified... | Yuhao Feng, Yanghui Rao, Yuyao Tang, Ninghua Wang, He Liu |  |
| 222 |  |  [Does syntax matter? A strong baseline for Aspect-based Sentiment Analysis with RoBERTa](https://doi.org/10.18653/v1/2021.naacl-main.146) |  | 0 | Aspect-based Sentiment Analysis (ABSA), aiming at predicting the polarities for aspects, is a fine-grained task in the field of sentiment analysis. Previous work showed syntactic information, e.g. dependency trees, can effectively improve the ABSA performance. Recently, pre-trained models (PTMs) also have shown their effectiveness on ABSA. Therefore, the question naturally arises whether PTMs contain sufficient syntactic information for ABSA so that we can obtain a good ABSA model only based on... | Junqi Dai, Hang Yan, Tianxiang Sun, Pengfei Liu, Xipeng Qiu |  |
| 223 |  |  [Domain Divergences: A Survey and Empirical Analysis](https://doi.org/10.18653/v1/2021.naacl-main.147) |  | 0 | Domain divergence plays a significant role in estimating the performance of a model in new domains. While there is a significant literature on divergence measures, researchers find it hard to choose an appropriate divergence for a given NLP application. We address this shortcoming by both surveying the literature and through an empirical study. We develop a taxonomy of divergence measures consisting of three classes — Information-theoretic, Geometric, and Higher-order measures and identify the... | Abhinav Ramesh Kashyap, Devamanyu Hazarika, MinYen Kan, Roger Zimmermann |  |
| 224 |  |  [Target-Aware Data Augmentation for Stance Detection](https://doi.org/10.18653/v1/2021.naacl-main.148) |  | 0 | The goal of stance detection is to identify whether the author of a text is in favor of, neutral or against a specific target. Despite substantial progress on this task, one of the remaining challenges is the scarcity of annotations. Data augmentation is commonly used to address annotation scarcity by generating more training samples. However, the augmented sentences that are generated by existing methods are either less diversified or inconsistent with the given target and stance label. In... | Yingjie Li, Cornelia Caragea |  |
| 225 |  |  [End-to-end ASR to jointly predict transcriptions and linguistic annotations](https://doi.org/10.18653/v1/2021.naacl-main.149) |  | 0 | We propose a Transformer-based sequence-to-sequence model for automatic speech recognition (ASR) capable of simultaneously transcribing and annotating audio with linguistic information such as phonemic transcripts or part-of-speech (POS) tags. Since linguistic information is important in natural language processing (NLP), the proposed ASR is especially useful for speech interface applications, including spoken dialogue systems and speech translation, which combine ASR and NLP. To produce... | Motoi Omachi, Yuya Fujita, Shinji Watanabe, Matthew Wiesner |  |
| 226 |  |  [Source and Target Bidirectional Knowledge Distillation for End-to-end Speech Translation](https://doi.org/10.18653/v1/2021.naacl-main.150) |  | 0 | A conventional approach to improving the performance of end-to-end speech translation (E2E-ST) models is to leverage the source transcription via pre-training and joint training with automatic speech recognition (ASR) and neural machine translation (NMT) tasks. However, since the input modalities are different, it is difficult to leverage source language text successfully. In this work, we focus on sequence-level knowledge distillation (SeqKD) from external text-based NMT models. To leverage... | Hirofumi Inaguma, Tatsuya Kawahara, Shinji Watanabe |  |
| 227 |  |  [Searchable Hidden Intermediates for End-to-End Models of Decomposable Sequence Tasks](https://doi.org/10.18653/v1/2021.naacl-main.151) |  | 0 | End-to-end approaches for sequence tasks are becoming increasingly popular. Yet for complex sequence tasks, like speech translation, systems that cascade several models trained on sub-tasks have shown to be superior, suggesting that the compositionality of cascaded systems simplifies learning and enables sophisticated search capabilities. In this work, we present an end-to-end framework that exploits compositionality to learn searchable hidden representations at intermediate stages of a... | Siddharth Dalmia, Brian Yan, Vikas Raunak, Florian Metze, Shinji Watanabe |  |
| 228 |  |  [SPLAT: Speech-Language Joint Pre-Training for Spoken Language Understanding](https://doi.org/10.18653/v1/2021.naacl-main.152) |  | 0 | Spoken language understanding (SLU) requires a model to analyze input acoustic signal to understand its linguistic content and make predictions. To boost the models’ performance, various pre-training methods have been proposed to learn rich representations from large-scale unannotated speech and text. However, the inherent disparities between the two modalities necessitate a mutual analysis. In this paper, we propose a novel semi-supervised learning framework, SPLAT, to jointly pre-train the... | YuAn Chung, Chenguang Zhu, Michael Zeng |  |
| 229 |  |  [Worldly Wise (WoW) - Cross-Lingual Knowledge Fusion for Fact-based Visual Spoken-Question Answering](https://doi.org/10.18653/v1/2021.naacl-main.153) |  | 0 | Although Question-Answering has long been of research interest, its accessibility to users through a speech interface and its support to multiple languages have not been addressed in prior studies. Towards these ends, we present a new task and a synthetically-generated dataset to do Fact-based Visual Spoken-Question Answering (FVSQA). FVSQA is based on the FVQA dataset, which requires a system to retrieve an entity from Knowledge Graphs (KGs) to answer a question about an image. In FVSQA, the... | Kiran Ramnath, Leda Sari, Mark HasegawaJohnson, Chang D. Yoo |  |
| 230 |  |  [Align-Refine: Non-Autoregressive Speech Recognition via Iterative Realignment](https://doi.org/10.18653/v1/2021.naacl-main.154) |  | 0 | Non-autoregressive encoder-decoder models greatly improve decoding speed over autoregressive models, at the expense of generation quality. To mitigate this, iterative decoding models repeatedly infill or refine the proposal of a non-autoregressive model. However, editing at the level of output sequences limits model flexibility. We instead propose \*iterative realignment\*, which by refining latent alignments allows more flexible edits in fewer steps. Our model, Align-Refine, is an end-to-end... | Ethan A. Chi, Julian Salazar, Katrin Kirchhoff |  |
| 231 |  |  [Everything Has a Cause: Leveraging Causal Inference in Legal Text Analysis](https://doi.org/10.18653/v1/2021.naacl-main.155) |  | 0 | Causal inference is the process of capturing cause-effect relationship among variables. Most existing works focus on dealing with structured data, while mining causal relationship among factors from unstructured data, like text, has been less examined, but is of great importance, especially in the legal domain. In this paper, we propose a novel Graph-based Causal Inference (GCI) framework, which builds causal graphs from fact descriptions without much human involvement and enables causal... | Xiao Liu, Da Yin, Yansong Feng, Yuting Wu, Dongyan Zhao |  |
| 232 |  |  [Counterfactual Supporting Facts Extraction for Explainable Medical Record Based Diagnosis with Graph Network](https://doi.org/10.18653/v1/2021.naacl-main.156) |  | 0 | Providing a reliable explanation for clinical diagnosis based on the Electronic Medical Record (EMR) is fundamental to the application of Artificial Intelligence in the medical field. Current methods mostly treat the EMR as a text sequence and provide explanations based on a precise medical knowledge base, which is disease-specific and difficult to obtain for experts in reality. Therefore, we propose a counterfactual multi-granularity graph supporting facts extraction (CMGE) method to extract... | Haoran Wu, Wei Chen, Shuang Xu, Bo Xu |  |
| 233 |  |  [Personalized Response Generation via Generative Split Memory Network](https://doi.org/10.18653/v1/2021.naacl-main.157) |  | 0 | Despite the impressive successes of generation and dialogue systems, how to endow a text generation system with particular personality traits to deliver more personalized responses remains under-investigated. In this work, we look at how to generate personalized responses for questions on Reddit by utilizing personalized user profiles and posting histories. Specifically, we release an open-domain single-turn dialog dataset made up of 1.5M conversation pairs together with 300k profiles of users... | Yuwei Wu, Xuezhe Ma, Diyi Yang |  |
| 234 |  |  [Towards Few-shot Fact-Checking via Perplexity](https://doi.org/10.18653/v1/2021.naacl-main.158) |  | 0 | Few-shot learning has drawn researchers’ attention to overcome the problem of data scarcity. Recently, large pre-trained language models have shown great performance in few-shot learning for various downstream tasks, such as question answering and machine translation. Nevertheless, little exploration has been made to achieve few-shot learning for the fact-checking task. However, fact-checking is an important problem, especially when the amount of information online is growing exponentially... | Nayeon Lee, Yejin Bang, Andrea Madotto, Pascale Fung |  |
| 235 |  |  [Active² Learning: Actively reducing redundancies in Active Learning methods for Sequence Tagging and Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.159) |  | 0 | While deep learning is a powerful tool for natural language processing (NLP) problems, successful solutions to these problems rely heavily on large amounts of annotated samples. However, manually annotating data is expensive and time-consuming. Active Learning (AL) strategies reduce the need for huge volumes of labeled data by iteratively selecting a small number of examples for manual annotation based on their estimated utility in training the given model. In this paper, we argue that since AL... | Rishi Hazra, Parag Dutta, Shubham Gupta, Mohammed Abdul Qaathir, Ambedkar Dukkipati |  |
| 236 |  |  [Generating An Optimal Interview Question Plan Using A Knowledge Graph And Integer Linear Programming](https://doi.org/10.18653/v1/2021.naacl-main.160) |  | 0 | Given the diversity of the candidates and complexity of job requirements, and since interviewing is an inherently subjective process, it is an important task to ensure consistent, uniform, efficient and objective interviews that result in high quality recruitment. We propose an interview assistant system to automatically, and in an objective manner, select an optimal set of technical questions (from question banks) personalized for a candidate. This set can help a human interviewer to plan for... | Soham Datta, Prabir Mallick, Sangameshwar Patil, Indrajit Bhattacharya, Girish K. Palshikar |  |
| 237 |  |  [Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!](https://doi.org/10.18653/v1/2021.naacl-main.161) |  | 0 | Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API... | Xuanli He, Lingjuan Lyu, Lichao Sun, Qiongkai Xu |  |
| 238 |  |  [A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models](https://doi.org/10.18653/v1/2021.naacl-main.162) |  | 0 | Early exit mechanism aims to accelerate the inference speed of large-scale pre-trained language models. The essential idea is to exit early without passing through all the inference layers at the inference stage. To make accurate predictions for downstream tasks, the hierarchical linguistic information embedded in all layers should be jointly considered. However, much of the research up to now has been limited to use local representations of the exit layer. Such treatment inevitably loses... | Kaiyuan Liao, Yi Zhang, Xuancheng Ren, Qi Su, Xu Sun, Bin He |  |
| 239 |  |  [Masked Conditional Random Fields for Sequence Labeling](https://doi.org/10.18653/v1/2021.naacl-main.163) |  | 0 | Conditional Random Field (CRF) based neural models are among the most performant methods for solving sequence labeling problems. Despite its great success, CRF has the shortcoming of occasionally generating illegal sequences of tags, e.g. sequences containing an “I-” tag immediately after an “O” tag, which is forbidden by the underlying BIO tagging scheme. In this work, we propose Masked Conditional Random Field (MCRF), an easy to implement variant of CRF that impose restrictions on candidate... | Tianwen Wei, Jianwei Qi, Shenghuan He, Songtao Sun |  |
| 240 |  |  [Heterogeneous Graph Neural Networks for Concept Prerequisite Relation Learning in Educational Data](https://doi.org/10.18653/v1/2021.naacl-main.164) |  | 0 | Prerequisite relations among concepts are crucial for educational applications, such as curriculum planning and intelligent tutoring. In this paper, we propose a novel concept prerequisite relation learning approach, named CPRL, which combines both concept representation learned from a heterogeneous graph and concept pairwise features. Furthermore, we extend CPRL under weakly supervised settings to make our method more practical, including learning prerequisite relations from learning object... | Chenghao Jia, Yongliang Shen, Yechun Tang, Lu Sun, Weiming Lu |  |
| 241 |  |  [Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models](https://doi.org/10.18653/v1/2021.naacl-main.165) |  | 0 | Recent studies have revealed a security threat to natural language processing (NLP) models, called the Backdoor Attack. Victim models can maintain competitive performance on clean samples while behaving abnormally on samples with a specific trigger word inserted. Previous backdoor attacking methods usually assume that attackers have a certain degree of data knowledge, either the dataset which users would use or proxy datasets for a similar task, for implementing the data poisoning procedure.... | Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, Bin He |  |
| 242 |  |  [DA-Transformer: Distance-aware Transformer](https://doi.org/10.18653/v1/2021.naacl-main.166) |  | 0 | Transformer has achieved great success in the NLP field by composing various advanced models like BERT and GPT. However, Transformer and its existing variants may not be optimal in capturing token distances because the position or distance embeddings used by these methods usually cannot keep the precise information of real distances, which may not be beneficial for modeling the orders and relations of contexts. In this paper, we propose DA-Transformer, which is a distance-aware Transformer that... | Chuhan Wu, Fangzhao Wu, Yongfeng Huang |  |
| 243 |  |  [ASAP: A Chinese Review Dataset Towards Aspect Category Sentiment Analysis and Rating Prediction](https://doi.org/10.18653/v1/2021.naacl-main.167) |  | 0 | Sentiment analysis has attracted increasing attention in e-commerce. The sentiment polarities underlying user reviews are of great value for business intelligence. Aspect category sentiment analysis (ACSA) and review rating prediction (RP) are two essential tasks to detect the fine-to-coarse sentiment polarities. ACSA and RP are highly correlated and usually employed jointly in real-world e-commerce scenarios. While most public datasets are constructed for ACSA and RP separately, which may... | Jiahao Bu, Lei Ren, Shuang Zheng, Yang Yang, Jingang Wang, Fuzheng Zhang, Wei Wu |  |
| 244 |  |  [Are NLP Models really able to Solve Simple Math Word Problems?](https://doi.org/10.18653/v1/2021.naacl-main.168) |  | 0 | The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered “solved” with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We... | Arkil Patel, Satwik Bhattamishra, Navin Goyal |  |
| 245 |  |  [WRIME: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations](https://doi.org/10.18653/v1/2021.naacl-main.169) |  | 0 | We annotate 17,000 SNS posts with both the writer’s subjective emotional intensity and the reader’s objective one to construct a Japanese emotion analysis dataset. In this study, we explore the difference between the emotional intensity of the writer and that of the readers with this dataset. We found that the reader cannot fully detect the emotions of the writer, especially anger and trust. In addition, experimental results in estimating the emotional intensity show that it is more difficult... | Tomoyuki Kajiwara, Chenhui Chu, Noriko Takemura, Yuta Nakashima, Hajime Nagahara |  |
| 246 |  |  [KPQA: A Metric for Generative Question Answering Using Keyphrase Weights](https://doi.org/10.18653/v1/2021.naacl-main.170) |  | 0 | In the automatic evaluation of generative question answering (GenQA) systems, it is difficult to assess the correctness of generated answers due to the free-form of the answer. Especially, widely used n-gram similarity metrics often fail to discriminate the incorrect answers since they equally consider all of the tokens. To alleviate this problem, we propose KPQA metric, a new metric for evaluating the correctness of GenQA. Specifically, our new metric assigns different weights to each token... | Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Joongbo Shin, Kyomin Jung |  |
| 247 |  |  [StylePTB: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer](https://doi.org/10.18653/v1/2021.naacl-main.171) |  | 0 | Text style transfer aims to controllably generate text with targeted stylistic changes while maintaining core meaning from the source sentence constant. Many of the existing style transfer benchmarks primarily focus on individual high-level semantic changes (e.g. positive to negative), which enable controllability at a high level but do not offer fine-grained control involving sentence structure, emphasis, and content of the sentence. In this paper, we introduce a large-scale benchmark,... | Yiwei Lyu, Paul Pu Liang, Hai Pham, Eduard H. Hovy, Barnabás Póczos, Ruslan Salakhutdinov, LouisPhilippe Morency |  |
| 248 |  |  [Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with Common Sense and World Knowledge](https://doi.org/10.18653/v1/2021.naacl-main.172) |  | 0 | Cant is important for understanding advertising, comedies and dog-whistle politics. However, computational research on cant is hindered by a lack of available datasets. In this paper, we propose a large and diverse Chinese dataset for creating and understanding cant from a computational linguistics perspective. We formulate a task for cant understanding and provide both quantitative and qualitative analysis for tested word embedding similarity and pretrained language models. Experiments suggest... | Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian J. McAuley, Furu Wei |  |
| 249 |  |  [COVID-19 Named Entity Recognition for Vietnamese](https://doi.org/10.18653/v1/2021.naacl-main.173) |  | 0 | The current COVID-19 pandemic has lead to the creation of many corpora that facilitate NLP research and downstream applications to help fight the pandemic. However, most of these corpora are exclusively for English. As the pandemic is a global problem, it is worth creating COVID-19 related datasets for languages other than English. In this paper, we present the first manually-annotated COVID-19 domain-specific dataset for Vietnamese. Particularly, our dataset is annotated for the named entity... | Thinh Hung Truong, Mai Hoang Dao, Dat Quoc Nguyen |  |
| 250 |  |  [Framing Unpacked: A Semi-Supervised Interpretable Multi-View Model of Media Frames](https://doi.org/10.18653/v1/2021.naacl-main.174) |  | 0 | Understanding how news media frame political issues is important due to its impact on public attitudes, yet hard to automate. Computational approaches have largely focused on classifying the frame of a full news article while framing signals are often subtle and local. Furthermore, automatic news analysis is a sensitive domain, and existing classifiers lack transparency in their predictions. This paper addresses both issues with a novel semi-supervised model, which jointly learns to embed local... | Shima Khanehzar, Trevor Cohn, Gosia Mikolajczak, Andrew Turpin, Lea Frermann |  |
| 251 |  |  [Automatic Classification of Neutralization Techniques in the Narrative of Climate Change Scepticism](https://doi.org/10.18653/v1/2021.naacl-main.175) |  | 0 | Neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. We first draw on social science to introduce the problem to the community of nlp, present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change, and experiment with supervised and semi- supervised BERT-based models. | Shraey Bhatia, Jey Han Lau, Timothy Baldwin |  |
| 252 |  |  [Suicide Ideation Detection via Social and Temporal User Representations using Hyperbolic Learning](https://doi.org/10.18653/v1/2021.naacl-main.176) |  | 0 | Recent psychological studies indicate that individuals exhibiting suicidal ideation increasingly turn to social media rather than mental health practitioners. Personally contextualizing the buildup of such ideation is critical for accurate identification of users at risk. In this work, we propose a framework jointly leveraging a user’s emotional history and social information from a user’s neighborhood in a network to contextualize the interpretation of the latest tweet of a user on Twitter.... | Ramit Sawhney, Harshit Joshi, Rajiv Ratn Shah, Lucie Flek |  |
| 253 |  |  [WikiTalkEdit: A Dataset for modeling Editors' behaviors on Wikipedia](https://doi.org/10.18653/v1/2021.naacl-main.177) |  | 0 | This study introduces and analyzes WikiTalkEdit, a dataset of conversations and edit histories from Wikipedia, for research in online cooperation and conversation modeling. The dataset comprises dialog triplets from the Wikipedia Talk pages, and editing actions on the corresponding articles being discussed. We show how the data supports the classic understanding of style matching, where positive emotion and the use of first-person pronouns predict a positive emotional change in a Wikipedia... | Kokil Jaidka, Andrea Ceolin, Iknoor Singh, Niyati Chhaya, Lyle H. Ungar |  |
| 254 |  |  [The structure of online social networks modulates the rate of lexical change](https://doi.org/10.18653/v1/2021.naacl-main.178) |  | 0 | New words are regularly introduced to communities, yet not all of these words persist in a community’s lexicon. Among the many factors contributing to lexical change, we focus on the understudied effect of social networks. We conduct a large-scale analysis of over 80k neologisms in 4420 online communities across a decade. Using Poisson regression and survival analysis, our study demonstrates that the community’s network structure plays a significant role in lexical change. Apart from overall... | Jian Zhu, David Jurgens |  |
| 255 |  |  [Modeling Framing in Immigration Discourse on Social Media](https://doi.org/10.18653/v1/2021.naacl-main.179) |  | 0 | The framing of political issues can influence policy and public opinion. Even though the public plays a key role in creating and spreading frames, little is known about how ordinary people on social media frame political issues. By creating a new dataset of immigration-related tweets labeled for multiple framing typologies from political communication theory, we develop supervised models to detect frames. We demonstrate how users’ ideology and region impact framing choices, and how a message’s... | Julia Mendelsohn, Ceren Budak, David Jurgens |  |
| 256 |  |  [Modeling the Severity of Complaints in Social Media](https://doi.org/10.18653/v1/2021.naacl-main.180) |  | 0 | The speech act of complaining is used by humans to communicate a negative mismatch between reality and expectations as a reaction to an unfavorable situation. Linguistic theory of pragmatics categorizes complaints into various severity levels based on the face-threat that the complainer is willing to undertake. This is particularly useful for understanding the intent of complainers and how humans develop suitable apology strategies. In this paper, we study the severity level of complaints for... | Mali Jin, Nikolaos Aletras |  |
| 257 |  |  [What About the Precedent: An Information-Theoretic Analysis of Common Law](https://doi.org/10.18653/v1/2021.naacl-main.181) |  | 0 | In common law, the outcome of a new case is determined mostly by precedent cases, rather than by existing statutes. However, how exactly does the precedent influence the outcome of a new case? Answering this question is crucial for guaranteeing fair and consistent judicial decision-making. We are the first to approach this question computationally by comparing two longstanding jurisprudential views; Halsbury’s, who believes that the arguments of the precedent are the main determinant of the... | Josef Valvoda, Tiago Pimentel, Niklas Stoehr, Ryan Cotterell, Simone Teufel |  |
| 258 |  |  [Introducing CAD: the Contextual Abuse Dataset](https://doi.org/10.18653/v1/2021.naacl-main.182) |  | 0 | Online abuse can inflict harm on users and communities, making online spaces unsafe and toxic. Progress in automatically detecting and classifying abusive content is often held back by the lack of high quality and detailed datasets. We introduce a new dataset of primarily English Reddit entries which addresses several limitations of prior work. It (1) contains six conceptually distinct primary categories as well as secondary categories, (2) has labels annotated in the context of the... | Bertie Vidgen, Dong Nguyen, Helen Z. Margetts, Patrícia G. C. Rossini, Rebekah Tromble |  |
| 259 |  |  [Lifelong Learning of Hate Speech Classification on Social Media](https://doi.org/10.18653/v1/2021.naacl-main.183) |  | 0 | Existing work on automated hate speech classification assumes that the dataset is fixed and the classes are pre-defined. However, the amount of data in social media increases every day, and the hot topics changes rapidly, requiring the classifiers to be able to continuously adapt to new data without forgetting the previously learned knowledge. This ability, referred to as lifelong learning, is crucial for the real-word application of hate speech classifiers in social media. In this work, we... | Jing Qian, Hong Wang, Mai ElSherief, Xifeng Yan |  |
| 260 |  |  [Learning to Recognize Dialect Features](https://doi.org/10.18653/v1/2021.naacl-main.184) |  | 0 | Building NLP systems that serve everyone requires accounting for dialect differences. But dialects are not monolithic entities: rather, distinctions between and within dialects are captured by the presence, absence, and frequency of dozens of dialect features in speech and text, such as the deletion of the copula in “He ∅ running”. In this paper, we introduce the task of dialect feature detection, and present two multitask learning approaches, both based on pretrained transformers. For most... | Dorottya Demszky, Devyani Sharma, Jonathan H. Clark, Vinodkumar Prabhakaran, Jacob Eisenstein |  |
| 261 |  |  [It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners](https://doi.org/10.18653/v1/2021.naacl-main.185) |  | 0 | When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much “greener” in that their parameter count is several... | Timo Schick, Hinrich Schütze |  |
| 262 |  |  [Static Embeddings as Efficient Knowledge Bases?](https://doi.org/10.18653/v1/2021.naacl-main.186) |  | 0 | Recent research investigates factual knowledge stored in large pretrained language models (PLMs). Instead of structural knowledge base (KB) queries, masked sentences such as “Paris is the capital of [MASK]” are used as probes. The good performance on this analysis task has been interpreted as PLMs becoming potential repositories of factual knowledge. In experiments across ten linguistically diverse languages, we study knowledge contained in static embeddings. We show that, when restricting the... | Philipp Dufter, Nora Kassner, Hinrich Schütze |  |
| 263 |  |  [Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis](https://doi.org/10.18653/v1/2021.naacl-main.187) |  | 0 | Knowledge Graph Embeddings (KGEs) have been intensively explored in recent years due to their promise for a wide range of applications. However, existing studies focus on improving the final model performance without acknowledging the computational cost of the proposed approaches, in terms of execution time and environmental impact. This paper proposes a simple yet effective KGE framework which can reduce the training time and carbon footprint by orders of magnitudes compared with... | Xutan Peng, Guanyi Chen, Chenghua Lin, Mark Stevenson |  |
| 264 |  |  [Rethinking Network Pruning - under the Pre-train and Fine-tune Paradigm](https://doi.org/10.18653/v1/2021.naacl-main.188) |  | 0 | Transformer-based pre-trained language models have significantly improved the performance of various natural language processing (NLP) tasks in the recent years. While effective and prevalent, these models are usually prohibitively large for resource-limited deployment scenarios. A thread of research has thus been working on applying network pruning techniques under the pretrain-then-finetune paradigm widely adopted in NLP. However, the existing pruning results on benchmark transformers, such... | Dongkuan Xu, Ian EnHsu Yen, Jinxi Zhao, Zhibin Xiao |  |
| 265 |  |  [Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers](https://doi.org/10.18653/v1/2021.naacl-main.189) |  | 0 | The ease of access to pre-trained transformers has enabled developers to leverage large-scale language models to build exciting applications for their users. While such pre-trained models offer convenient starting points for researchers and developers, there is little consideration for the societal biases captured within these model risking perpetuation of racial, gender, and other harmful biases when these models are deployed at scale. In this paper, we investigate gender and racial bias... | Andrew Silva, Pradyumna Tambwekar, Matthew C. Gombolay |  |
| 266 |  |  [Detoxifying Language Models Risks Marginalizing Minority Voices](https://doi.org/10.18653/v1/2021.naacl-main.190) |  | 0 | Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity: they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic... | Albert Xu, Eshaan Pathak, Eric Wallace, Suchin Gururangan, Maarten Sap, Dan Klein |  |
| 267 |  |  [HONEST: Measuring Hurtful Sentence Completion in Language Models](https://doi.org/10.18653/v1/2021.naacl-main.191) |  | 0 | Language models have revolutionized the field of NLP. However, language models capture and proliferate hurtful stereotypes, especially in text generation. Our results show that 4.3% of the time, language models complete a sentence with a hurtful word. These cases are not random, but follow language and gender-specific patterns. We propose a score to measure hurtful sentence completions in language models (HONEST). It uses a systematic template- and lexicon-based bias evaluation methodology for... | Debora Nozza, Federico Bianchi, Dirk Hovy |  |
| 268 |  |  [EaSe: A Diagnostic Tool for VQA based on Answer Diversity](https://doi.org/10.18653/v1/2021.naacl-main.192) |  | 0 | We propose EASE, a simple diagnostic tool for Visual Question Answering (VQA) which quantifies the difficulty of an image, question sample. EASE is based on the pattern of answers provided by multiple annotators to a given question. In particular, it considers two aspects of the answers: (i) their Entropy; (ii) their Semantic content. First, we prove the validity of our diagnostic to identify samples that are easy/hard for state-of-art VQA models. Second, we show that EASE can be successfully... | Shailza Jolly, Sandro Pezzelle, Moin Nabi |  |
| 269 |  |  [DeCEMBERT: Learning from Noisy Instructional Videos via Dense Captions and Entropy Minimization](https://doi.org/10.18653/v1/2021.naacl-main.193) |  | 0 | Leveraging large-scale unlabeled web videos such as instructional videos for pre-training followed by task-specific finetuning has become the de facto approach for many video-and-language tasks. However, these instructional videos are very noisy, the accompanying ASR narrations are often incomplete, and can be irrelevant to or temporally misaligned with the visual content, limiting the performance of the models trained on such data. To address these issues, we propose an improved... | Zineng Tang, Jie Lei, Mohit Bansal |  |
| 270 |  |  [Improving Generation and Evaluation of Visual Stories via Semantic Consistency](https://doi.org/10.18653/v1/2021.naacl-main.194) |  | 0 | Story visualization is an underexplored task that falls at the intersection of many important research directions in both computer vision and natural language processing. In this task, given a series of natural language captions which compose a story, an agent must generate a sequence of images that correspond to the captions. Prior work has introduced recurrent generative models which outperform text-to-image synthesis models on this task. However, there is room for improvement of generated... | Adyasha Maharana, Darryl Hannan, Mohit Bansal |  |
| 271 |  |  [Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models](https://doi.org/10.18653/v1/2021.naacl-main.195) |  | 0 | This paper studies zero-shot cross-lingual transfer of vision-language models. Specifically, we focus on multilingual text-to-video search and propose a Transformer-based model that learns contextual multilingual multimodal embeddings. Under a zero-shot setting, we empirically demonstrate that performance degrades significantly when we query the multilingual text-video model with non-English sentences. To address this problem, we introduce a multilingual multimodal pre-training strategy, and... | Poyao Huang, Mandela Patrick, Junjie Hu, Graham Neubig, Florian Metze, Alex Hauptmann |  |
| 272 |  |  [Video Question Answering with Phrases via Semantic Roles](https://doi.org/10.18653/v1/2021.naacl-main.196) |  | 0 | Video Question Answering (VidQA) evaluation metrics have been limited to a single-word answer or selecting a phrase from a fixed set of phrases. These metrics limit the VidQA models’ application scenario. In this work, we leverage semantic roles derived from video descriptions to mask out certain phrases, to introduce VidQAP which poses VidQA as a fill-in-the-phrase task. To enable evaluation of answer phrases, we compute the relative improvement of the predicted answer compared to an empty... | Arka Sadhu, Kan Chen, Ram Nevatia |  |
| 273 |  |  [From Masked Language Modeling to Translation: Non-English Auxiliary Tasks Improve Zero-shot Spoken Language Understanding](https://doi.org/10.18653/v1/2021.naacl-main.197) |  | 0 | The lack of publicly available evaluation data for low-resource languages limits progress in Spoken Language Understanding (SLU). As key tasks like intent classification and slot filling require abundant training data, it is desirable to reuse existing data in high-resource languages to develop models for low-resource scenarios. We introduce xSID, a new benchmark for cross-lingual (x) Slot and Intent Detection in 13 languages from 6 language families, including a very low-resource dialect. To... | Rob van der Goot, Ibrahim Sharaf, Aizhan Imankulova, Ahmet Üstün, Marija Stepanovic, Alan Ramponi, Siti Oryza Khairunnisa, Mamoru Komachi, Barbara Plank |  |
| 274 |  |  [WEC: Deriving a Large-scale Cross-document Event Coreference dataset from Wikipedia](https://doi.org/10.18653/v1/2021.naacl-main.198) |  | 0 | Cross-document event coreference resolution is a foundational task for NLP applications involving multi-text processing. However, existing corpora for this task are scarce and relatively small, while annotating only modest-size clusters of documents belonging to the same topic. To complement these resources and enhance future research, we present Wikipedia Event Coreference (WEC), an efficient methodology for gathering a large-scale dataset for cross-document event coreference from Wikipedia,... | Alon Eirew, Arie Cattan, Ido Dagan |  |
| 275 |  |  [Challenging distributional models with a conceptual network of philosophical terms](https://doi.org/10.18653/v1/2021.naacl-main.199) |  | 0 | Computational linguistic research on language change through distributional semantic (DS) models has inspired researchers from fields such as philosophy and literary studies, who use these methods for the exploration and comparison of comparatively small datasets traditionally analyzed by close reading. Research on methods for small data is still in early stages and it is not clear which methods achieve the best results. We investigate the possibilities and limitations of using distributional... | Yvette Oortwijn, Jelke Bloem, Pia Sommerauer, Francois Meyer, Wei Zhou, Antske Fokkens |  |
| 276 |  |  [KILT: a Benchmark for Knowledge Intensive Language Tasks](https://doi.org/10.18653/v1/2021.naacl-main.200) |  | 0 | Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, external knowledge sources. While some models do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on models that condition on specific information in large textual resources, we present a benchmark... | Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, Sebastian Riedel |  |
| 277 |  |  [A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios](https://doi.org/10.18653/v1/2021.naacl-main.201) |  | 0 | Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and fine-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of... | Michael A. Hedderich, Lukas Lange, Heike Adel, Jannik Strötgen, Dietrich Klakow |  |
| 278 |  |  [Temporal Knowledge Graph Completion using a Linear Temporal Regularizer and Multivector Embeddings](https://doi.org/10.18653/v1/2021.naacl-main.202) |  | 0 | Representation learning approaches for knowledge graphs have been mostly designed for static data. However, many knowledge graphs involve evolving data, e.g., the fact (The President of the United States is Barack Obama) is valid only from 2009 to 2017. This introduces important challenges for knowledge representation learning since the knowledge graphs change over time. In this paper, we present a novel time-aware knowledge graph embebdding approach, TeLM, which performs 4th-order tensor... | Chengjin Xu, YungYu Chen, Mojtaba Nayyeri, Jens Lehmann |  |
| 279 |  |  [UDALM: Unsupervised Domain Adaptation through Language Modeling](https://doi.org/10.18653/v1/2021.naacl-main.203) |  | 0 | In this work we explore Unsupervised Domain Adaptation (UDA) of pretrained language models for downstream tasks. We introduce UDALM, a fine-tuning procedure, using a mixed classification and Masked Language Model loss, that can adapt to the target domain distribution in a robust and sample efficient manner. Our experiments show that performance of models trained with the mixed loss scales with the amount of available target data and the mixed loss can be effectively used as a stopping criterion... | Constantinos Karouzos, Georgios Paraskevopoulos, Alexandros Potamianos |  |
| 280 |  |  [Beyond Black & White: Leveraging Annotator Disagreement via Soft-Label Multi-Task Learning](https://doi.org/10.18653/v1/2021.naacl-main.204) |  | 0 | Supervised learning assumes that a ground truth label exists. However, the reliability of this ground truth depends on human annotators, who often disagree. Prior work has shown that this disagreement can be helpful in training models. We propose a novel method to incorporate this disagreement as information: in addition to the standard error computation, we use soft-labels (i.e., probability distributions over the annotator labels) as an auxiliary task in a multi-task neural network. We... | Tommaso Fornaciari, Alexandra Uma, Silviu Paun, Barbara Plank, Dirk Hovy, Massimo Poesio |  |
| 281 |  |  [Clustering-based Inference for Biomedical Entity Linking](https://doi.org/10.18653/v1/2021.naacl-main.205) |  | 0 | Due to large number of entities in biomedical knowledge bases, only a small fraction of entities have corresponding labelled training data. This necessitates entity linking models which are able to link mentions of unseen entities using learned representations of entities. Previous approaches link each mention independently, ignoring the relationships within and across documents between the entity mentions. These relations can be very useful for linking mentions in biomedical text where linking... | Rico Angell, Nicholas Monath, Sunil Mohan, Nishant Yadav, Andrew McCallum |  |
| 282 |  |  [Variance-reduced First-order Meta-learning for Natural Language Processing Tasks](https://doi.org/10.18653/v1/2021.naacl-main.206) |  | 0 | First-order meta-learning algorithms have been widely used in practice to learn initial model parameters that can be quickly adapted to new tasks due to their efficiency and effectiveness. However, existing studies find that meta-learner can overfit to some specific adaptation when we have heterogeneous tasks, leading to significantly degraded performance. In Natural Language Processing (NLP) applications, datasets are often diverse and each task has its unique characteristics. Therefore, to... | Lingxiao Wang, Kevin Huang, Tengyu Ma, Quanquan Gu, Jing Huang |  |
| 283 |  |  [Diversity-Aware Batch Active Learning for Dependency Parsing](https://doi.org/10.18653/v1/2021.naacl-main.207) |  | 0 | While the predictive performance of modern statistical dependency parsers relies heavily on the availability of expensive expert-annotated treebank data, not all annotations contribute equally to the training of the parsers. In this paper, we attempt to reduce the number of labeled examples needed to train a strong dependency parser using batch active learning (AL). In particular, we investigate whether enforcing diversity in the sampled batches, using determinantal point processes (DPPs), can... | Tianze Shi, Adrian Benton, Igor Malioutov, Ozan Irsoy |  |
| 284 |  |  [How many data points is a prompt worth?](https://doi.org/10.18653/v1/2021.naacl-main.208) |  | 0 | When fine-tuning pretrained models for classification, researchers either use a generic model head or a task-specific prompt for prediction. Proponents of prompting have argued that prompts provide a method for injecting task-specific guidance, which is beneficial in low-data regimes. We aim to quantify this benefit through rigorous testing of prompts in a fair setting: comparing prompted and head-based fine-tuning in equal conditions across many tasks and data sizes. By controlling for many... | Teven Le Scao, Alexander M. Rush |  |
| 285 |  |  [Can Latent Alignments Improve Autoregressive Machine Translation?](https://doi.org/10.18653/v1/2021.naacl-main.209) |  | 0 | Latent alignment objectives such as CTC and AXE significantly improve non-autoregressive machine translation models. Can they improve autoregressive models as well? We explore the possibility of training autoregressive machine translation models with latent alignment objectives, and observe that, in practice, this approach results in degenerate models. We provide a theoretical explanation for these empirical results, and prove that latent alignment objectives are incompatible with teacher... | Adi Haviv, Lior Vassertail, Omer Levy |  |
| 286 |  |  [Smoothing and Shrinking the Sparse Seq2Seq Search Space](https://doi.org/10.18653/v1/2021.naacl-main.210) |  | 0 | Current sequence-to-sequence models are trained to minimize cross-entropy and use softmax to compute the locally normalized probabilities over target sequences. While this setup has led to strong results in a variety of tasks, one unsatisfying aspect is its length bias: models give high scores to short, inadequate hypotheses and often make the empty string the argmax—the so-called cat got your tongue problem. Recently proposed entmax-based sparse sequence-to-sequence models present a possible... | Ben Peters, André F. T. Martins |  |
| 287 |  |  [Unified Pre-training for Program Understanding and Generation](https://doi.org/10.18653/v1/2021.naacl-main.211) |  | 0 | Code summarization and generation empower conversion between programming language (PL) and natural language (NL), while code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART, a sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks. PLBART is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding. Experiments on... | Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, KaiWei Chang |  |
| 288 |  |  [Hyperparameter-free Continuous Learning for Domain Classification in Natural Language Understanding](https://doi.org/10.18653/v1/2021.naacl-main.212) |  | 0 | Domain classification is the fundamental task in natural language understanding (NLU), which often requires fast accommodation to new emerging domains. This constraint makes it impossible to retrain all previous domains, even if they are accessible to the new model. Most existing continual learning approaches suffer from low accuracy and performance fluctuation, especially when the distributions of old and new data are significantly different. In fact, the key real-world problem is not the... | Ting Hua, Yilin Shen, Changsheng Zhao, YenChang Hsu, Hongxia Jin |  |
| 289 |  |  [On the Embeddings of Variables in Recurrent Neural Networks for Source Code](https://doi.org/10.18653/v1/2021.naacl-main.213) |  | 0 | Source code processing heavily relies on the methods widely used in natural language processing (NLP), but involves specifics that need to be taken into account to achieve higher quality. An example of this specificity is that the semantics of a variable is defined not only by its name but also by the contexts in which the variable occurs. In this work, we develop dynamic embeddings, a recurrent mechanism that adjusts the learned semantics of the variable when it obtains more information about... | Nadezhda Chirkova |  |
| 290 |  |  [Cross-Lingual Word Embedding Refinement by $\ell_1$ Norm Optimisation](https://doi.org/10.18653/v1/2021.naacl-main.214) |  | 0 | Cross-Lingual Word Embeddings (CLWEs) encode words from two or more languages in a shared high-dimensional space in which vectors representing words with similar meaning (regardless of language) are closely located. Existing methods for building high-quality CLWEs learn mappings that minimise the ℓ2 norm loss function. However, this optimisation objective has been demonstrated to be sensitive to outliers. Based on the more robust Manhattan norm (aka. ℓ1 norm) goodness-of-fit criterion, this... | Xutan Peng, Chenghua Lin, Mark Stevenson |  |
| 291 |  |  [Semantic Frame Forecast](https://doi.org/10.18653/v1/2021.naacl-main.215) |  | 0 | This paper introduces Semantic Frame Forecast, a task that predicts the semantic frames that will occur in the next 10, 100, or even 1,000 sentences in a running story. Prior work focused on predicting the immediate future of a story, such as one to a few sentences ahead. However, when novelists write long stories, generating a few sentences is not enough to help them gain high-level insight to develop the follow-up story. In this paper, we formulate a long story as a sequence of “story... | ChiehYang Huang, TingHao K. Huang |  |
| 292 |  |  [MUSER: MUltimodal Stress detection using Emotion Recognition as an Auxiliary Task](https://doi.org/10.18653/v1/2021.naacl-main.216) |  | 0 | The capability to automatically detect human stress can benefit artificial intelligent agents involved in affective computing and human-computer interaction. Stress and emotion are both human affective states, and stress has proven to have important implications on the regulation and expression of emotion. Although a series of methods have been established for multimodal stress detection, limited steps have been taken to explore the underlying inter-dependence between stress and emotion. In... | Yiqun Yao, Michalis Papakostas, Mihai Burzo, Mohamed Abouelenien, Rada Mihalcea |  |
| 293 |  |  [Learning to Decompose and Organize Complex Tasks](https://doi.org/10.18653/v1/2021.naacl-main.217) |  | 0 | People rely on digital task management tools, such as email or to-do apps, to manage their tasks. Some of these tasks are large and complex, leading to action paralysis and feelings of being overwhelmed on the part of the user. The micro-productivity literature has shown that such tasks could benefit from being decomposed and organized, in order to reduce user cognitive load. Thus in this paper, we propose a novel end-to-end pipeline that consumes a complex task and induces a dependency graph... | Yi Zhang, Sujay Kumar Jauhar, Julia Kiseleva, Ryen White, Dan Roth |  |
| 294 |  |  [Continual Learning for Text Classification with Information Disentanglement Based Regularization](https://doi.org/10.18653/v1/2021.naacl-main.218) |  | 0 | Continual learning has become increasingly important as it enables NLP models to constantly learn and gain knowledge over time. Previous continual learning methods are mainly designed to preserve knowledge from previous tasks, without much emphasis on how to well generalize models to new tasks. In this work, we propose an information disentanglement based regularization method for continual learning on text classification. Our proposed method first disentangles text hidden spaces into... | Yufan Huang, Yanzhe Zhang, Jiaao Chen, Xuezhi Wang, Diyi Yang |  |
| 295 |  |  [Learning from Executions for Semantic Parsing](https://doi.org/10.18653/v1/2021.naacl-main.219) |  | 0 | Semantic parsing aims at translating natural language (NL) utterances onto machine-interpretable programs, which can be executed against a real-world environment. The expensive annotation of utterance-program pairs has long been acknowledged as a major bottleneck for the deployment of contemporary neural models to real-life applications. In this work, we focus on the task of semi-supervised learning where a limited amount of annotated data is available together with many unlabeled NL... | Bailin Wang, Mirella Lapata, Ivan Titov |  |
| 296 |  |  [Learning to Synthesize Data for Semantic Parsing](https://doi.org/10.18653/v1/2021.naacl-main.220) |  | 0 | Synthesizing data for semantic parsing has gained increasing attention recently. However, most methods require handcrafted (high-precision) rules in their generative process, hindering the exploration of diverse unseen data. In this work, we propose a generative model which features a (non-neural) PCFG that models the composition of programs (e.g., SQL), and a BART-based translation model that maps a program to an utterance. Due to the simplicity of PCFG and pre-trained BART, our generative... | Bailin Wang, Wenpeng Yin, Xi Victoria Lin, Caiming Xiong |  |
| 297 |  |  [Edge: Enriching Knowledge Graph Embeddings with External Text](https://doi.org/10.18653/v1/2021.naacl-main.221) |  | 0 | Knowledge graphs suffer from sparsity which degrades the quality of representations generated by various methods. While there is an abundance of textual information throughout the web and many existing knowledge bases, aligning information across these diverse data sources remains a challenge in the literature. Previous work has partially addressed this issue by enriching knowledge graph entities based on “hard” co-occurrence of words present in the entities of the knowledge graphs and external... | Saed Rezayi, Handong Zhao, Sungchul Kim, Ryan A. Rossi, Nedim Lipka, Sheng Li |  |
| 298 |  |  [FLIN: A Flexible Natural Language Interface for Web Navigation](https://doi.org/10.18653/v1/2021.naacl-main.222) |  | 0 | AI assistants can now carry out tasks for users by directly interacting with website UIs. Current semantic parsing and slot-filling techniques cannot flexibly adapt to many different websites without being constantly re-trained. We propose FLIN, a natural language interface for web navigation that maps user commands to concept-level actions (rather than low-level UI actions), thus being able to flexibly adapt to different websites and handle their transient nature. We frame this as a ranking... | Sahisnu Mazumder, Oriana Riva |  |
| 299 |  |  [Game-theoretic Vocabulary Selection via the Shapley Value and Banzhaf Index](https://doi.org/10.18653/v1/2021.naacl-main.223) |  | 0 | The input vocabulary and the representations learned are crucial to the performance of neural NLP models. Using the full vocabulary results in less explainable and more memory intensive models, with the embedding layer often constituting the majority of model parameters. It is thus common to use a smaller vocabulary to lower memory requirements and construct more interpertable models. We propose a vocabulary selection method that views words as members of a team trying to maximize the model’s... | Roma Patel, Marta Garnelo, Ian Gemp, Chris Dyer, Yoram Bachrach |  |
| 300 |  |  [Incorporating External Knowledge to Enhance Tabular Reasoning](https://doi.org/10.18653/v1/2021.naacl-main.224) |  | 0 | Reasoning about tabular information presents unique challenges to modern NLP approaches which largely rely on pre-trained contextualized embeddings of text. In this paper, we study these challenges through the problem of tabular natural language inference. We propose easy and effective modifications to how information is presented to a model for this task. We show via systematic experiments that these strategies substantially improve tabular inference performance. | J. Neeraja, Vivek Gupta, Vivek Srikumar |  |
| 301 |  |  [Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention](https://doi.org/10.18653/v1/2021.naacl-main.225) |  | 0 | We describe a span-level supervised attention loss that improves compositional generalization in semantic parsers. Our approach builds on existing losses that encourage attention maps in neural sequence-to-sequence models to imitate the output of classical word alignment algorithms. Where past work has used word-level alignments, we focus on spans; borrowing ideas from phrase-based machine translation, we align subtrees in semantic parses to spans of input sentences, and encourage neural... | Pengcheng Yin, Hao Fang, Graham Neubig, Adam Pauls, Emmanouil Antonios Platanios, Yu Su, Sam Thomson, Jacob Andreas |  |
| 302 |  |  [Domain Adaptation for Arabic Cross-Domain and Cross-Dialect Sentiment Analysis from Contextualized Word Embedding](https://doi.org/10.18653/v1/2021.naacl-main.226) |  | 0 | Finetuning deep pre-trained language models has shown state-of-the-art performances on a wide range of Natural Language Processing (NLP) applications. Nevertheless, their generalization performance drops under domain shift. In the case of Arabic language, diglossia makes building and annotating corpora for each dialect and/or domain a more challenging task. Unsupervised Domain Adaptation tackles this issue by transferring the learned knowledge from labeled source domain data to unlabeled target... | Abdellah El Mekki, Abdelkader El Mahdaouy, Ismail Berrada, Ahmed Khoumsi |  |
| 303 |  |  [Multi-task Learning of Negation and Speculation for Targeted Sentiment Classification](https://doi.org/10.18653/v1/2021.naacl-main.227) |  | 0 | The majority of work in targeted sentiment analysis has concentrated on finding better methods to improve the overall results. Within this paper we show that these models are not robust to linguistic phenomena, specifically negation and speculation. In this paper, we propose a multi-task learning method to incorporate information from syntactic and semantic auxiliary tasks, including negation and speculation scope detection, to create English-language models that are more robust to these... | Andrew Moore, Jeremy Barnes |  |
| 304 |  |  [A Disentangled Adversarial Neural Topic Model for Separating Opinions from Plots in User Reviews](https://doi.org/10.18653/v1/2021.naacl-main.228) |  | 0 | The flexibility of the inference process in Variational Autoencoders (VAEs) has recently led to revising traditional probabilistic topic models giving rise to Neural Topic Models (NTM). Although these approaches have achieved significant results, surprisingly very little work has been done on how to disentangle the latent topics. Existing topic models when applied to reviews may extract topics associated with writers’ subjective opinions mixed with those related to factual descriptions such as... | Gabriele Pergola, Lin Gui, Yulan He |  |
| 305 |  |  [Graph Ensemble Learning over Multiple Dependency Trees for Aspect-level Sentiment Classification](https://doi.org/10.18653/v1/2021.naacl-main.229) |  | 0 | Recent work on aspect-level sentiment classification has demonstrated the efficacy of incorporating syntactic structures such as dependency trees with graph neural networks (GNN), but these approaches are usually vulnerable to parsing errors. To better leverage syntactic information in the face of unavoidable errors, we propose a simple yet effective graph ensemble technique, GraphMerge, to make use of the predictions from different parsers. Instead of assigning one set of model parameters to... | Xiaochen Hou, Peng Qi, Guangtao Wang, Rex Ying, Jing Huang, Xiaodong He, Bowen Zhou |  |
| 306 |  |  [Emotion-Infused Models for Explainable Psychological Stress Detection](https://doi.org/10.18653/v1/2021.naacl-main.230) |  | 0 | The problem of detecting psychological stress in online posts, and more broadly, of detecting people in distress or in need of help, is a sensitive application for which the ability to interpret models is vital. Here, we present work exploring the use of a semantically related task, emotion detection, for equally competent but more explainable and human-like psychological stress detection as compared to a black-box model. In particular, we explore the use of multi-task learning as well as... | Elsbeth Turcan, Smaranda Muresan, Kathleen R. McKeown |  |
| 307 |  |  [Aspect-based Sentiment Analysis with Type-aware Graph Convolutional Networks and Layer Ensemble](https://doi.org/10.18653/v1/2021.naacl-main.231) |  | 0 | It is popular that neural graph-based models are applied in existing aspect-based sentiment analysis (ABSA) studies for utilizing word relations through dependency parses to facilitate the task with better semantic guidance for analyzing context and aspect words. However, most of these studies only leverage dependency relations without considering their dependency types, and are limited in lacking efficient mechanisms to distinguish the important relations as well as learn from different layers... | Yuanhe Tian, Guimin Chen, Yan Song |  |
| 308 |  |  [Supertagging-based Parsing with Linear Context-free Rewriting Systems](https://doi.org/10.18653/v1/2021.naacl-main.232) |  | 0 | We present the first supertagging-based parser for linear context-free rewriting systems (LCFRS). It utilizes neural classifiers and outperforms previous LCFRS-based parsers in both accuracy and parsing speed by a wide margin. Our results keep up with the best (general) discontinuous parsers, particularly the scores for discontinuous constituents establish a new state of the art. The heart of our approach is an efficient lexicalization procedure which induces a lexical LCFRS from any... | Thomas Ruprecht, Richard Mörbitz |  |
| 309 |  |  [Outside Computation with Superior Functions](https://doi.org/10.18653/v1/2021.naacl-main.233) |  | 0 | We show that a general algorithm for efficient computation of outside values under the minimum of superior functions framework proposed by Knuth (1977) would yield a sub-exponential time algorithm for SAT, violating the Strong Exponential Time Hypothesis (SETH). | Parker Riley, Daniel Gildea |  |
| 310 |  |  [Learning Syntax from Naturally-Occurring Bracketings](https://doi.org/10.18653/v1/2021.naacl-main.234) |  | 0 | Naturally-occurring bracketings, such as answer fragments to natural language questions and hyperlinks on webpages, can reflect human syntactic intuition regarding phrasal boundaries. Their availability and approximate correspondence to syntax make them appealing as distant information sources to incorporate into unsupervised constituency parsing. But they are noisy and incomplete; to address this challenge, we develop a partial-brackets-aware structured ramp loss in learning. Experiments... | Tianze Shi, Ozan Irsoy, Igor Malioutov, Lillian Lee |  |
| 311 |  |  [Bot-Adversarial Dialogue for Safe Conversational Agents](https://doi.org/10.18653/v1/2021.naacl-main.235) |  | 0 | Conversational agents trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior. We introduce a new human-and-model-in-the-loop framework for evaluating the toxicity of such models, and compare a variety of existing methods in both the cases of non-adversarial and adversarial users that expose their weaknesses. We then go on to propose two novel methods for safe conversational agents, by either... | Jing Xu, Da Ju, Margaret Li, YLan Boureau, Jason Weston, Emily Dinan |  |
| 312 |  |  [Non-Autoregressive Semantic Parsing for Compositional Task-Oriented Dialog](https://doi.org/10.18653/v1/2021.naacl-main.236) |  | 0 | Semantic parsing using sequence-to-sequence models allows parsing of deeper representations compared to traditional word tagging based models. In spite of these advantages, widespread adoption of these models for real-time conversational use cases has been stymied by higher compute requirements and thus higher latency. In this work, we propose a non-autoregressive approach to predict semantic parse trees with an efficient seq2seq model architecture. By combining non-autoregressive prediction... | Arun Babu, Akshat Shrivastava, Armen Aghajanyan, Ahmed Aly, Angela Fan, Marjan Ghazvininejad |  |
| 313 |  |  [Example-Driven Intent Prediction with Observers](https://doi.org/10.18653/v1/2021.naacl-main.237) |  | 0 | A key challenge of dialog systems research is to effectively and efficiently adapt to new domains. A scalable paradigm for adaptation necessitates the development of generalizable models that perform well in few-shot settings. In this paper, we focus on the intent classification problem which aims to identify user intents given utterances addressed to the dialog system. We propose two approaches for improving the generalizability of utterance classification models: (1) observers and (2)... | Shikib Mehri, Mihail Eric |  |
| 314 |  |  [Imperfect also Deserves Reward: Multi-Level and Sequential Reward Modeling for Better Dialog Management](https://doi.org/10.18653/v1/2021.naacl-main.238) |  | 0 | For task-oriented dialog systems, training a Reinforcement Learning (RL) based Dialog Management module suffers from low sample efficiency and slow convergence speed due to the sparse rewards in RL. To solve this problem, many strategies have been proposed to give proper rewards when training RL, but their rewards lack interpretability and cannot accurately estimate the distribution of state-action pairs in real dialogs. In this paper, we propose a multi-level reward modeling approach that... | Zhengxu Hou, Bang Liu, Ruihui Zhao, Zijing Ou, Yafei Liu, Xi Chen, Yefeng Zheng |  |
| 315 |  |  [Action-Based Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2021.naacl-main.239) |  | 0 | Existing goal-oriented dialogue datasets focus mainly on identifying slots and values. However, customer support interactions in reality often involve agents following multi-step procedures derived from explicitly-defined company policies as well. To study customer service dialogue systems in more realistic settings, we introduce the Action-Based Conversations Dataset (ABCD), a fully-labeled dataset with over 10K human-to-human dialogues containing 55 distinct user intents requiring unique... | Derek Chen, Howard Chen, Yi Yang, Alexander Lin, Zhou Yu |  |
| 316 |  |  [Controlling Dialogue Generation with Semantic Exemplars](https://doi.org/10.18653/v1/2021.naacl-main.240) |  | 0 | Dialogue systems pretrained with large language models generate locally coherent responses, but lack fine-grained control over responses necessary to achieve specific goals. A promising method to control response generation is exemplar-based generation, in which models edit exemplar responses that are retrieved from training data, or hand-written to strategically address discourse-level goals, to fit new dialogue contexts. We present an Exemplar-based Dialogue Generation model, EDGE, that uses... | Prakhar Gupta, Jeffrey P. Bigham, Yulia Tsvetkov, Amy Pavel |  |
| 317 |  |  [COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List](https://doi.org/10.18653/v1/2021.naacl-main.241) |  | 0 | Classical information retrieval systems such as BM25 rely on exact lexical match and can carry out search efficiently with inverted list index. Recent neural IR models shifts towards soft matching all query document terms, but they lose the computation efficiency of exact match systems. This paper presents COIL, a contextualized exact match retrieval architecture, where scoring is based on overlapping query document tokens’ contextualized representations. The new architecture stores... | Luyu Gao, Zhuyun Dai, Jamie Callan |  |
| 318 |  |  [X-Class: Text Classification with Extremely Weak Supervision](https://doi.org/10.18653/v1/2021.naacl-main.242) |  | 0 | In this paper, we explore text classification with extremely weak supervision, i.e., only relying on the surface text of class names. This is a more challenging setting than the seed-driven weak supervision, which allows a few seed words per class. We opt to attack this problem from a representation learning perspective—ideal document representations should lead to nearly the same results between clustering and the desired classification. In particular, one can classify the same corpus... | Zihan Wang, Dheeraj Mekala, Jingbo Shang |  |
| 319 |  |  [Fine-tuning Encoders for Improved Monolingual and Zero-shot Polylingual Neural Topic Modeling](https://doi.org/10.18653/v1/2021.naacl-main.243) |  | 0 | Neural topic models can augment or replace bag-of-words inputs with the learned representations of deep pre-trained transformer-based word prediction models. One added benefit when using representations from multilingual models is that they facilitate zero-shot polylingual topic modeling. However, while it has been widely observed that pre-trained embeddings should be fine-tuned to a given task, it is not immediately clear what supervision should look like for an unsupervised task such as topic... | Aaron Mueller, Mark Dredze |  |
| 320 |  |  [Exploring the Relationship Between Algorithm Performance, Vocabulary, and Run-Time in Text Classification](https://doi.org/10.18653/v1/2021.naacl-main.244) |  | 0 | Text classification is a significant branch of natural language processing, and has many applications including document classification and sentiment analysis. Unsurprisingly, those who do text classification are concerned with the run-time of their algorithms, many of which depend on the size of the corpus’ vocabulary due to their bag-of-words representation. Although many studies have examined the effect of preprocessing techniques on vocabulary size and accuracy, none have examined how these... | Wilson Fearn, Orion Weller, Kevin D. Seppi |  |
| 321 |  |  [Faithfully Explainable Recommendation via Neural Logic Reasoning](https://doi.org/10.18653/v1/2021.naacl-main.245) |  | 0 | Knowledge graphs (KG) have become increasingly important to endow modern recommender systems with the ability to generate traceable reasoning paths to explain the recommendation process. However, prior research rarely considers the faithfulness of the derived explanations to justify the decision-making process. To the best of our knowledge, this is the first work that models and evaluates faithfully explainable recommendation under the framework of KG reasoning. Specifically, we propose neural... | Yaxin Zhu, Yikun Xian, Zuohui Fu, Gerard de Melo, Yongfeng Zhang |  |
| 322 |  |  [You Sound Like Someone Who Watches Drama Movies: Towards Predicting Movie Preferences from Conversational Interactions](https://doi.org/10.18653/v1/2021.naacl-main.246) |  | 0 | The increasing popularity of voice-based personal assistants provides new opportunities for conversational recommendation. One particularly interesting area is movie recommendation, which can benefit from an open-ended interaction with the user, through a natural conversation. We explore one promising direction for conversational recommendation: mapping a conversational user, for whom there is limited or no data available, to most similar external reviewers, whose preferences are known, by... | Sergey Volokhin, Joyce C. Ho, Oleg Rokhlenko, Eugene Agichtein |  |
| 323 |  |  [Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents](https://doi.org/10.18653/v1/2021.naacl-main.247) |  | 0 | Text-based games simulate worlds and interact with players using natural language. Recent work has used them as a testbed for autonomous language-understanding agents, with the motivation being that understanding the meanings of words or semantics is a key component of how humans understand, reason, and act in these worlds. However, it remains unclear to what extent artificial agents utilize semantic understanding of the text. To this end, we perform experiments to systematically reduce the... | Shunyu Yao, Karthik Narasimhan, Matthew J. Hausknecht |  |
| 324 |  |  [SOrT-ing VQA Models : Contrastive Gradient Learning for Improved Consistency](https://doi.org/10.18653/v1/2021.naacl-main.248) |  | 0 | Recent research in Visual Question Answering (VQA) has revealed state-of-the-art models to be inconsistent in their understanding of the world - they answer seemingly difficult questions requiring reasoning correctly but get simpler associated sub-questions wrong. These sub-questions pertain to lower level visual concepts in the image that models ideally should understand to be able to answer the reasoning question correctly. To address this, we first present a gradient-based interpretability... | Sameer Dharur, Purva Tendulkar, Dhruv Batra, Devi Parikh, Ramprasaath R. Selvaraju |  |
| 325 |  |  [Semi-Supervised Policy Initialization for Playing Games with Language Hints](https://doi.org/10.18653/v1/2021.naacl-main.249) |  | 0 | Using natural language as a hint can supply an additional reward for playing sparse-reward games. Achieving a goal should involve several different hints, while the given hints are usually incomplete. Those unmentioned latent hints still rely on the sparse reward signal, and make the learning process difficult. In this paper, we propose semi-supervised initialization (SSI) that allows the agent to learn from various possible hints before training under different tasks. Experiments show that SSI... | TsuJui Fu, William Yang Wang |  |
| 326 |  |  [Revisiting Document Representations for Large-Scale Zero-Shot Learning](https://doi.org/10.18653/v1/2021.naacl-main.250) |  | 0 | Zero-shot learning aims to recognize unseen objects using their semantic representations. Most existing works use visual attributes labeled by humans, not suitable for large-scale applications. In this paper, we revisit the use of documents as semantic representations. We argue that documents like Wikipedia pages contain rich visual information, which however can easily be buried by the vast amount of non-visual sentences. To address this issue, we propose a semi-automatic mechanism for visual... | Jihyung Kil, WeiLun Chao |  |
| 327 |  |  [Negative language transfer in learner English: A new dataset](https://doi.org/10.18653/v1/2021.naacl-main.251) |  | 0 | Automatic personalized corrective feedback can help language learners from different backgrounds better acquire a new language. This paper introduces a learner English dataset in which learner errors are accompanied by information about possible error sources. This dataset contains manually annotated error causes for learner writing errors. These causes tie learner mistakes to structures from their first languages, when the rules in English and in the first language diverge. This new dataset... | Leticia Farias Wanderley, Nicole Zhao, Carrie Demmans Epp |  |
| 328 |  |  [SentSim: Crosslingual Semantic Evaluation of Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.252) |  | 0 | Machine translation (MT) is currently evaluated in one of two ways: in a monolingual fashion, by comparison with the system output to one or more human reference translations, or in a trained crosslingual fashion, by building a supervised model to predict quality scores from human-labeled data. In this paper, we propose a more cost-effective, yet well performing unsupervised alternative SentSim: relying on strong pretrained multilingual word and sentence representations, we directly compare the... | Yurun Song, Junchen Zhao, Lucia Specia |  |
| 329 |  |  [Quality Estimation for Image Captions Based on Large-scale Human Evaluations](https://doi.org/10.18653/v1/2021.naacl-main.253) |  | 0 | Automatic image captioning has improved significantly over the last few years, but the problem is far from being solved, with state of the art models still often producing low quality captions when used in the wild. In this paper, we focus on the task of Quality Estimation (QE) for image captions, which attempts to model the caption quality from a human perspective and \*without\* access to ground-truth references, so that it can be applied at prediction time to detect low-quality captions... | Tomer Levinboim, Ashish V. Thapliyal, Piyush Sharma, Radu Soricut |  |
| 330 |  |  [CaSiNo: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems](https://doi.org/10.18653/v1/2021.naacl-main.254) |  | 0 | Automated systems that negotiate with humans have broad applications in pedagogy and conversational AI. To advance the development of practical negotiation systems, we present CaSiNo: a novel corpus of over a thousand negotiation dialogues in English. Participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip. Our design results in diverse and linguistically rich negotiations while maintaining a tractable, closed-domain... | Kushal Chawla, Jaysa Ramirez, Rene Clever, Gale M. Lucas, Jonathan May, Jonathan Gratch |  |
| 331 |  |  [News Headline Grouping as a Challenging NLU Task](https://doi.org/10.18653/v1/2021.naacl-main.255) |  | 0 | Recent progress in Natural Language Understanding (NLU) has seen the latest models outperform human performance on many standard tasks. These impressive results have led the community to introspect on dataset limitations, and iterate on more nuanced challenges. In this paper, we introduce the task of HeadLine Grouping (HLG) and a corresponding dataset (HLGD) consisting of 20,056 pairs of news headlines, each labeled with a binary judgement as to whether the pair belongs within the same group.... | Philippe Laban, Lucas Bandarkar, Marti A. Hearst |  |
| 332 |  |  [Olá, Bonjour, Salve! XFORMAL: A Benchmark for Multilingual Formality Style Transfer](https://doi.org/10.18653/v1/2021.naacl-main.256) |  | 0 | We take the first step towards multilingual style transfer by creating and releasing XFORMAL, a benchmark of multiple formal reformulations of informal text in Brazilian Portuguese, French, and Italian. Results on XFORMAL suggest that state-of-the-art style transfer approaches perform close to simple baselines, indicating that style transfer is even more challenging when moving multilingual. | Eleftheria Briakou, Di Lu, Ke Zhang, Joel R. Tetreault |  |
| 333 |  |  [Grouping Words with Semantic Diversity](https://doi.org/10.18653/v1/2021.naacl-main.257) |  | 0 | Deep Learning-based NLP systems can be sensitive to unseen tokens and hard to learn with high-dimensional inputs, which critically hinder learning generalization. We introduce an approach by grouping input words based on their semantic diversity to simplify input language representation with low ambiguity. Since the semantically diverse words reside in different contexts, we are able to substitute words with their groups and still distinguish word meanings relying on their contexts. We design... | Karine Chubarian, Abdul Rafae Khan, Anastasios Sidiropoulos, Jia Xu |  |
| 334 |  |  [Noise Stability Regularization for Improving BERT Fine-tuning](https://doi.org/10.18653/v1/2021.naacl-main.258) |  | 0 | Fine-tuning pre-trained language models suchas BERT has become a common practice dom-inating leaderboards across various NLP tasks. Despite its recent success and wide adoption,this process is unstable when there are onlya small number of training samples available. The brittleness of this process is often reflectedby the sensitivity to random seeds. In this pa-per, we propose to tackle this problem basedon the noise stability property of deep nets,which is investigated in recent literature... | Hang Hua, Xingjian Li, Dejing Dou, ChengZhong Xu, Jiebo Luo |  |
| 335 |  |  [FlowPrior: Learning Expressive Priors for Latent Variable Sentence Models](https://doi.org/10.18653/v1/2021.naacl-main.259) |  | 0 | Variational autoencoders (VAEs) are widely used for latent variable modeling of text. We focus on variations that learn expressive prior distributions over the latent variable. We find that existing training strategies are not effective for learning rich priors, so we propose adding the importance-sampled log marginal likelihood as a second term to the standard VAE objective to help when learning the prior. Doing so improves results for all priors evaluated, including a novel choice for... | Xiaoan Ding, Kevin Gimpel |  |
| 336 |  |  [HTCInfoMax: A Global Model for Hierarchical Text Classification via Information Maximization](https://doi.org/10.18653/v1/2021.naacl-main.260) |  | 0 | The current state-of-the-art model HiAGM for hierarchical text classification has two limitations. First, it correlates each text sample with all labels in the dataset which contains irrelevant information. Second, it does not consider any statistical constraint on the label representations learned by the structure encoder, while constraints for representation learning are proved to be helpful in previous work. In this paper, we propose HTCInfoMax to address these issues by introducing... | Zhongfen Deng, Hao Peng, Dongxiao He, Jianxin Li, Philip S. Yu |  |
| 337 |  |  [Knowledge Guided Metric Learning for Few-Shot Text Classification](https://doi.org/10.18653/v1/2021.naacl-main.261) |  | 0 | Humans can distinguish new categories very efficiently with few examples, largely due to the fact that human beings can leverage knowledge obtained from relevant tasks. However, deep learning based text classification model tends to struggle to achieve satisfactory performance when labeled data are scarce. Inspired by human intelligence, we propose to introduce external knowledge into few-shot learning to imitate human knowledge. A novel parameter generator network is investigated to this end,... | Dianbo Sui, Yubo Chen, Binjie Mao, Delai Qiu, Kang Liu, Jun Zhao |  |
| 338 |  |  [Ensemble of MRR and NDCG models for Visual Dialog](https://doi.org/10.18653/v1/2021.naacl-main.262) |  | 0 | Assessing an AI agent that can converse in human language and understand visual content is challenging. Generation metrics, such as BLEU scores favor correct syntax over semantics. Hence a discriminative approach is often used, where an agent ranks a set of candidate options. The mean reciprocal rank (MRR) metric evaluates the model performance by taking into account the rank of a single human-derived answer. This approach, however, raises a new challenge: the ambiguity and synonymy of answers,... | Idan Schwartz |  |
| 339 |  |  [Supervised Neural Clustering via Latent Structured Output Learning: Application to Question Intents](https://doi.org/10.18653/v1/2021.naacl-main.263) |  | 0 | Previous pre-neural work on structured prediction has produced very effective supervised clustering algorithms using linear classifiers, e.g., structured SVM or perceptron. However, these cannot exploit the representation learning ability of neural networks, which would make supervised clustering even more powerful, i.e., general clustering patterns can be learned automatically. In this paper, we design neural networks based on latent structured prediction loss and Transformer models to... | Iryna Haponchyk, Alessandro Moschitti |  |
| 340 |  |  [ConVEx: Data-Efficient and Few-Shot Slot Labeling](https://doi.org/10.18653/v1/2021.naacl-main.264) |  | 0 | We propose ConVEx (Conversational Value Extractor), an efficient pretraining and fine-tuning neural approach for slot-labeling dialog tasks. Instead of relying on more general pretraining objectives from prior work (e.g., language modeling, response selection), ConVEx’s pretraining objective, a novel pairwise cloze task using Reddit data, is well aligned with its intended usage on sequence labeling tasks. This enables learning domain-specific slot labelers by simply fine-tuning decoding layers... | Matthew Henderson, Ivan Vulic |  |
| 341 |  |  [CREAD: Combined Resolution of Ellipses and Anaphora in Dialogues](https://doi.org/10.18653/v1/2021.naacl-main.265) |  | 0 | Anaphora and ellipses are two common phenomena in dialogues. Without resolving referring expressions and information omission, dialogue systems may fail to generate consistent and coherent responses. Traditionally, anaphora is resolved by coreference resolution and ellipses by query rewrite. In this work, we propose a novel joint learning framework of modeling coreference resolution and query rewriting for complex, multi-turn dialogue understanding. Given an ongoing dialogue between a user and... | BoHsiang Tseng, Shruti Bhargava, Jiarui Lu, Joel Ruben Antony Moniz, Dhivya Piraviperumal, Lin Li, Hong Yu |  |
| 342 |  |  [Knowledge-Driven Slot Constraints for Goal-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2021.naacl-main.266) |  | 0 | In goal-oriented dialogue systems, users provide information through slot values to achieve specific goals. Practically, some combinations of slot values can be invalid according to external knowledge. For example, a combination of “cheese pizza” (a menu item) and “oreo cookies” (a topping) from an input utterance “Can I order a cheese pizza with oreo cookies on top?” exemplifies such invalid combinations according to the menu of a restaurant business. Traditional dialogue systems allow... | Piyawat Lertvittayakumjorn, Daniele Bonadiman, Saab Mansour |  |
| 343 |  |  [Clipping Loops for Sample-Efficient Dialogue Policy Optimisation](https://doi.org/10.18653/v1/2021.naacl-main.267) |  | 0 | Training dialogue agents requires a large number of interactions with users: agents have no idea about which responses are bad among a lengthy dialogue. In this paper, we propose loop-clipping policy optimisation (LCPO) to eliminate useless responses. LCPO consists of two stages: loop clipping and advantage clipping. In loop clipping, we clip off useless responses (called loops) from dialogue history (called trajectories). The clipped trajectories are more succinct than the original ones, and... | YenChen Wu, Carl Edward Rasmussen |  |
| 344 |  |  [Integrating Lexical Information into Entity Neighbourhood Representations for Relation Prediction](https://doi.org/10.18653/v1/2021.naacl-main.268) |  | 0 | Relation prediction informed from a combination of text corpora and curated knowledge bases, combining knowledge graph completion with relation extraction, is a relatively little studied task. A system that can perform this task has the ability to extend an arbitrary set of relational database tables with information extracted from a document corpus. OpenKi[1] addresses this task through extraction of named entities and predicates via OpenIE tools then learning relation embeddings from the... | Ian D. Wood, Mark Johnson, Stephen Wan |  |
| 345 |  |  [Noisy-Labeled NER with Confidence Estimation](https://doi.org/10.18653/v1/2021.naacl-main.269) |  | 0 | Recent studies in deep learning have shown significant progress in named entity recognition (NER). However, most existing works assume clean data annotation, while real-world scenarios typically involve a large amount of noises from a variety of sources (e.g., pseudo, weak, or distant annotations). This work studies NER under a noisy labeled setting with calibrated confidence estimation. Based on empirical observations of different training dynamics of noisy and clean labels, we propose... | Kun Liu, Yao Fu, Chuanqi Tan, Mosha Chen, Ningyu Zhang, Songfang Huang, Sheng Gao |  |
| 346 |  |  [TABBIE: Pretrained Representations of Tabular Data](https://doi.org/10.18653/v1/2021.naacl-main.270) |  | 0 | Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection)... | Hiroshi Iida, Dung Thai, Varun Manjunatha, Mohit Iyyer |  |
| 347 |  |  [Better Feature Integration for Named Entity Recognition](https://doi.org/10.18653/v1/2021.naacl-main.271) |  | 0 | It has been shown that named entity recognition (NER) could benefit from incorporating the long-distance structured information captured by dependency trees. We believe this is because both types of features - the contextual information captured by the linear sequences and the structured information captured by the dependency trees may complement each other. However, existing approaches largely focused on stacking the LSTM and graph neural networks such as graph convolutional networks (GCNs)... | Lu Xu, Zhanming Jie, Wei Lu, Lidong Bing |  |
| 348 |  |  [ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute Representation Learning](https://doi.org/10.18653/v1/2021.naacl-main.272) |  | 0 | While relation extraction is an essential task in knowledge acquisition and representation, and new-generated relations are common in the real world, less effort is made to predict unseen relations that cannot be observed at the training stage. In this paper, we formulate the zero-shot relation extraction problem by incorporating the text description of seen and unseen relations. We propose a novel multi-task learning model, Zero-Shot BERT (ZS-BERT), to directly predict unseen relations without... | ChihYao Chen, ChengTe Li |  |
| 349 |  |  [Graph Convolutional Networks for Event Causality Identification with Rich Document-level Structures](https://doi.org/10.18653/v1/2021.naacl-main.273) |  | 0 | We study the problem of Event Causality Identification (ECI) to detect causal relation between event mention pairs in text. Although deep learning models have recently shown state-of-the-art performance for ECI, they are limited to the intra-sentence setting where event mention pairs are presented in the same sentences. This work addresses this issue by developing a novel deep learning model for document-level ECI (DECI) to accept inter-sentence event mention pairs. As such, we propose a... | Minh Tran Phu, Thien Huu Nguyen |  |
| 350 |  |  [A Context-Dependent Gated Module for Incorporating Symbolic Semantics into Event Coreference Resolution](https://doi.org/10.18653/v1/2021.naacl-main.274) |  | 0 | Event coreference resolution is an important research problem with many applications. Despite the recent remarkable success of pre-trained language models, we argue that it is still highly beneficial to utilize symbolic features for the task. However, as the input for coreference resolution typically comes from upstream components in the information extraction pipeline, the automatically extracted symbolic features can be noisy and contain errors. Also, depending on the specific context, some... | Tuan Manh Lai, Heng Ji, Trung Bui, Quan Hung Tran, Franck Dernoncourt, Walter Chang |  |
| 351 |  |  [Multi-Style Transfer with Discriminative Feedback on Disjoint Corpus](https://doi.org/10.18653/v1/2021.naacl-main.275) |  | 0 | Style transfer has been widely explored in natural language generation with non-parallel corpus by directly or indirectly extracting a notion of style from source and target domain corpus. A common shortcoming of existing approaches is the prerequisite of joint annotations across all the stylistic dimensions under consideration. Availability of such dataset across a combination of styles limits the extension of these setups to multiple style dimensions. While cascading single-dimensional models... | Navita Goyal, Balaji Vasan Srinivasan, Anandhavelu Natarajan, Abhilasha Sancheti |  |
| 352 |  |  [FUDGE: Controlled Text Generation With Future Discriminators](https://doi.org/10.18653/v1/2021.naacl-main.276) |  | 0 | We propose Future Discriminators for Generation (FUDGE), a flexible and modular method for controlled text generation. Given a pre-existing model G for generating text from a distribution of interest, FUDGE enables conditioning on a desired attribute a (for example, formality) while requiring access only to G’s output logits. FUDGE learns an attribute predictor operating on a partial sequence, and uses this predictor’s outputs to adjust G’s original probabilities. We show that FUDGE models... | Kevin Yang, Dan Klein |  |
| 353 |  |  [Controllable Text Simplification with Explicit Paraphrasing](https://doi.org/10.18653/v1/2021.naacl-main.277) |  | 0 | Text Simplification improves the readability of sentences through several rewriting transformations, such as lexical paraphrasing, deletion, and splitting. Current simplification systems are predominantly sequence-to-sequence models that are trained end-to-end to perform all these operations simultaneously. However, such systems limit themselves to mostly deleting words and cannot easily adapt to the requirements of different target audiences. In this paper, we propose a novel hybrid approach... | Mounica Maddela, Fernando AlvaManchego, Wei Xu |  |
| 354 |  |  [Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training](https://doi.org/10.18653/v1/2021.naacl-main.278) |  | 0 | Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domain-specific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, large-scale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many... | Oshin Agarwal, Heming Ge, Siamak Shakeri, Rami AlRfou |  |
| 355 |  |  [Choose Your Own Adventure: Paired Suggestions in Collaborative Writing for Evaluating Story Generation Models](https://doi.org/10.18653/v1/2021.naacl-main.279) |  | 0 | Story generation is an open-ended and subjective task, which poses a challenge for evaluating story generation models. We present Choose Your Own Adventure, a collaborative writing setup for pairwise model evaluation. Two models generate suggestions to people as they write a short story; we ask writers to choose one of the two suggestions, and we observe which model’s suggestions they prefer. The setup also allows further analysis based on the revisions people make to the suggestions. We show... | Elizabeth Clark, Noah A. Smith |  |
| 356 |  |  [InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training](https://doi.org/10.18653/v1/2021.naacl-main.280) |  | 0 | In this work, we present an information-theoretic framework that formulates cross-lingual language model pre-training as maximizing mutual information between multilingual-multi-granularity texts. The unified view helps us to better understand the existing methods for learning cross-lingual representations. More importantly, inspired by the framework, we propose a new pre-training task based on contrastive learning. Specifically, we regard a bilingual sentence pair as two views of the same... | Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, XianLing Mao, Heyan Huang, Ming Zhou |  |
| 357 |  |  [Context-Interactive Pre-Training for Document Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.281) |  | 0 | Document machine translation aims to translate the source sentence into the target language in the presence of additional contextual information. However, it typically suffers from a lack of doc-level bilingual data. To remedy this, here we propose a simple yet effective context-interactive pre-training approach, which targets benefiting from external large-scale corpora. The proposed model performs inter sentence generation to capture the cross-sentence dependency within the target document,... | Pengcheng Yang, Pei Zhang, Boxing Chen, Jun Xie, Weihua Luo |  |
| 358 |  |  [Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots](https://doi.org/10.18653/v1/2021.naacl-main.282) |  | 0 | Multilingual models have demonstrated impressive cross-lingual transfer performance. However, test sets like XNLI are monolingual at the example level. In multilingual communities, it is common for polyglots to code-mix when conversing with each other. Inspired by this phenomenon, we present two strong black-box adversarial attacks (one word-level, one phrase-level) for multilingual models that push their ability to handle code-mixed sentences to the limit. The former uses bilingual... | Samson Tan, Shafiq R. Joty |  |
| 359 |  |  [X-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering](https://doi.org/10.18653/v1/2021.naacl-main.283) |  | 0 | Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. However, their generalization ability is still inconsistent for typologically diverse languages and across different benchmarks. Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in Natural Language Understanding (NLU). In... | Meryem M'hamdi, Doo Soon Kim, Franck Dernoncourt, Trung Bui, Xiang Ren, Jonathan May |  |
| 360 |  |  [Explicit Alignment Objectives for Multilingual Bidirectional Encoders](https://doi.org/10.18653/v1/2021.naacl-main.284) |  | 0 | Pre-trained cross-lingual encoders such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) have proven impressively effective at enabling transfer-learning of NLP systems from high-resource languages to low-resource languages. This success comes despite the fact that there is no explicit objective to align the contextual embeddings of words/sentences with similar meanings across languages together in the same space. In this paper, we present a new method for learning multilingual... | Junjie Hu, Melvin Johnson, Orhan Firat, Aditya Siddhant, Graham Neubig |  |
| 361 |  |  [Cross-lingual Cross-modal Pretraining for Multimodal Retrieval](https://doi.org/10.18653/v1/2021.naacl-main.285) |  | 0 | Recent pretrained vision-language models have achieved impressive performance on cross-modal retrieval tasks in English. Their success, however, heavily depends on the availability of many annotated image-caption datasets for pretraining, where the texts are not necessarily in English. Although we can utilize machine translation (MT) tools to translate non-English text to English, the performance still largely relies on MT’s quality and may suffer from high latency problems in real-world... | Hongliang Fei, Tan Yu, Ping Li |  |
| 362 |  |  [Wikipedia Entities as Rendezvous across Languages: Grounding Multilingual Language Models by Predicting Wikipedia Hyperlinks](https://doi.org/10.18653/v1/2021.naacl-main.286) |  | 0 | Masked language models have quickly become the de facto standard when processing text. Recently, several approaches have been proposed to further enrich word representations with external knowledge sources such as knowledge graphs. However, these models are devised and evaluated in a monolingual setting only. In this work, we propose a language-independent entity prediction task as an intermediate training procedure to ground word representations on entity semantics and bridge the gap across... | Iacer Calixto, Alessandro Raganato, Tommaso Pasini |  |
| 363 |  |  [multiPRover: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning](https://doi.org/10.18653/v1/2021.naacl-main.287) |  | 0 | We focus on a type of linguistic formal reasoning where the goal is to reason over explicit knowledge in the form of natural language facts and rules (Clark et al., 2020). A recent work, named PRover (Saha et al., 2020), performs such reasoning by answering a question and also generating a proof graph that explains the answer. However, compositional reasoning is not always unique and there may be multiple ways of reaching the correct answer. Thus, in our work, we address a new and challenging... | Swarnadeep Saha, Prateek Yadav, Mohit Bansal |  |
| 364 |  |  [Adaptable and Interpretable Neural MemoryOver Symbolic Knowledge](https://doi.org/10.18653/v1/2021.naacl-main.288) |  | 0 | Past research has demonstrated that large neural language models (LMs) encode surprising amounts of factual information: however, augmenting or modifying this information requires modifying a corpus and retraining, which is computationally expensive. To address this problem, we develop a neural LM that includes an interpretable neuro-symbolic KB in the form of a “fact memory”. Each element of the fact memory is formed from a triple of vectors, where each vector corresponds to a KB entity or... | Pat Verga, Haitian Sun, Livio Baldini Soares, William W. Cohen |  |
| 365 |  |  [CLEVR_HYP: A Challenge Dataset and Baselines for Visual Question Answering with Hypothetical Actions over Images](https://doi.org/10.18653/v1/2021.naacl-main.289) |  | 0 | Most existing research on visual question answering (VQA) is limited to information explicitly present in an image or a video. In this paper, we take visual understanding to a higher level where systems are challenged to answer questions that involve mentally simulating the hypothetical consequences of performing specific actions in a given scenario. Towards that end, we formulate a vision-language question answering task based on the CLEVR (Johnson et. al., 2017) dataset. We then modify the... | Shailaja Keyur Sampat, Akshay Kumar, Yezhou Yang, Chitta Baral |  |
| 366 |  |  [Refining Targeted Syntactic Evaluation of Language Models](https://doi.org/10.18653/v1/2021.naacl-main.290) |  | 0 | Targeted syntactic evaluation of subject-verb number agreement in English (TSE) evaluates language models’ syntactic knowledge using hand-crafted minimal pairs of sentences that differ only in the main verb’s conjugation. The method evaluates whether language models rate each grammatical sentence as more likely than its ungrammatical counterpart. We identify two distinct goals for TSE. First, evaluating the systematicity of a language model’s syntactic knowledge: given a sentence, can it... | Benjamin Newman, KaiSiang Ang, Julia Gong, John Hewitt |  |
| 367 |  |  [Universal Adversarial Attacks with Natural Triggers for Text Classification](https://doi.org/10.18653/v1/2021.naacl-main.291) |  | 0 | Recent work has demonstrated the vulnerability of modern text classifiers to universal adversarial attacks, which are input-agnostic sequences of words added to text processed by classifiers. Despite being successful, the word sequences produced in such attacks are often ungrammatical and can be easily distinguished from natural text. We develop adversarial attacks that appear closer to natural English phrases and yet confuse classification systems when added to benign inputs. We leverage an... | Liwei Song, Xinwei Yu, HsuanTung Peng, Karthik Narasimhan |  |
| 368 |  |  [QuadrupletBERT: An Efficient Model For Embedding-Based Large-Scale Retrieval](https://doi.org/10.18653/v1/2021.naacl-main.292) |  | 0 | The embedding-based large-scale query-document retrieval problem is a hot topic in the information retrieval (IR) field. Considering that pre-trained language models like BERT have achieved great success in a wide variety of NLP tasks, we present a QuadrupletBERT model for effective and efficient retrieval in this paper. Unlike most existing BERT-style retrieval models, which only focus on the ranking phase in retrieval systems, our model makes considerable improvements to the retrieval phase... | Peiyang Liu, Sen Wang, Xi Wang, Wei Ye, Shikun Zhang |  |
| 369 |  |  [Dynamically Disentangling Social Bias from Task-Oriented Representations with Adversarial Attack](https://doi.org/10.18653/v1/2021.naacl-main.293) |  | 0 | Representation learning is widely used in NLP for a vast range of tasks. However, representations derived from text corpora often reflect social biases. This phenomenon is pervasive and consistent across different neural models, causing serious concern. Previous methods mostly rely on a pre-specified, user-provided direction or suffer from unstable training. In this paper, we propose an adversarial disentangled debiasing model to dynamically decouple social bias attributes from the intermediate... | Liwen Wang, Yuanmeng Yan, Keqing He, Yanan Wu, Weiran Xu |  |
| 370 |  |  [An Empirical Investigation of Bias in the Multimodal Analysis of Financial Earnings Calls](https://doi.org/10.18653/v1/2021.naacl-main.294) |  | 0 | Volatility prediction is complex due to the stock market’s stochastic nature. Existing research focuses on the textual elements of financial disclosures like earnings calls transcripts to forecast stock volatility and risk, but ignores the rich acoustic features in the company executives’ speech. Recently, new multimodal approaches that leverage the verbal and vocal cues of speakers in financial disclosures significantly outperform previous state-of-the-art approaches demonstrating the benefits... | Ramit Sawhney, Arshiya Aggarwal, Rajiv Ratn Shah |  |
| 371 |  |  [Beyond Fair Pay: Ethical Implications of NLP Crowdsourcing](https://doi.org/10.18653/v1/2021.naacl-main.295) |  | 0 | The use of crowdworkers in NLP research is growing rapidly, in tandem with the exponential increase in research production in machine learning and AI. Ethical discussion regarding the use of crowdworkers within the NLP research community is typically confined in scope to issues related to labor conditions such as fair pay. We draw attention to the lack of ethical considerations related to the various tasks performed by workers, including labeling, evaluation, and production. We find that the... | Boaz Shmueli, Jan Fell, Soumya Ray, LunWei Ku |  |
| 372 |  |  [On Transferability of Bias Mitigation Effects in Language Model Fine-Tuning](https://doi.org/10.18653/v1/2021.naacl-main.296) |  | 0 | Fine-tuned language models have been shown to exhibit biases against protected groups in a host of modeling tasks such as text classification and coreference resolution. Previous works focus on detecting these biases, reducing bias in data representations, and using auxiliary training objectives to mitigate bias during fine-tuning. Although these techniques achieve bias reduction for the task and domain at hand, the effects of bias mitigation may not directly transfer to new tasks, requiring... | Xisen Jin, Francesco Barbieri, Brendan Kennedy, Aida Mostafazadeh Davani, Leonardo Neves, Xiang Ren |  |
| 373 |  |  [Case Study: Deontological Ethics in NLP](https://doi.org/10.18653/v1/2021.naacl-main.297) |  | 0 | Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the... | Shrimai Prabhumoye, Brendon Boldt, Ruslan Salakhutdinov, Alan W. Black |  |
| 374 |  |  [Privacy Regularization: Joint Privacy-Utility Optimization in LanguageModels](https://doi.org/10.18653/v1/2021.naacl-main.298) |  | 0 | Neural language models are known to have a high capacity for memorization of training samples. This may have serious privacy im- plications when training models on user content such as email correspondence. Differential privacy (DP), a popular choice to train models with privacy guarantees, comes with significant costs in terms of utility degradation and disparate impact on subgroups of users. In this work, we introduce two privacy-preserving regularization methods for training language models... | Fatemehsadat Mireshghallah, Huseyin A. Inan, Marcello Hasegawa, Victor Rühle, Taylor BergKirkpatrick, Robert Sim |  |
| 375 |  |  [On the Impact of Random Seeds on the Fairness of Clinical Classifiers](https://doi.org/10.18653/v1/2021.naacl-main.299) |  | 0 | Recent work has shown that fine-tuning large networks is surprisingly sensitive to changes in random seed(s). We explore the implications of this phenomenon for model fairness across demographic groups in clinical prediction tasks over electronic health records (EHR) in MIMIC-III —— the standard dataset in clinical NLP research. Apparent subgroup performance varies substantially for seeds that yield similar overall performance, although there is no evidence of a trade-off between overall and... | Silvio Amir, JanWillem van de Meent, Byron C. Wallace |  |
| 376 |  |  [Topic Model or Topic Twaddle? Re-evaluating Semantic Interpretability Measures](https://doi.org/10.18653/v1/2021.naacl-main.300) |  | 0 | When developing topic models, a critical question that should be asked is: How well will this model work in an applied setting? Because standard performance evaluation of topic interpretability uses automated measures modeled on human evaluation tests that are dissimilar to applied usage, these models’ generalizability remains in question. In this paper, we probe the issue of validity in topic model evaluation and assess how informative coherence measures are for specialized collections used in... | Caitlin Doogan, Wray L. Buntine |  |
| 377 |  |  [Discourse Probing of Pretrained Language Models](https://doi.org/10.18653/v1/2021.naacl-main.301) |  | 0 | Existing work on probing of pretrained language models (LMs) has predominantly focused on sentence-level syntactic tasks. In this paper, we introduce document-level discourse probing to evaluate the ability of pretrained LMs to capture document-level relations. We experiment with 7 pretrained LMs, 4 languages, and 7 discourse probing tasks, and find BART to be overall the best model at capturing discourse — but only in its encoder, with BERT performing surprisingly well as the baseline model.... | Fajri Koto, Jey Han Lau, Timothy Baldwin |  |
| 378 |  |  [UniDrop: A Simple yet Effective Technique to Improve Transformer without Extra Cost](https://doi.org/10.18653/v1/2021.naacl-main.302) |  | 0 | Transformer architecture achieves great success in abundant natural language processing tasks. The over-parameterization of the Transformer model has motivated plenty of works to alleviate its overfitting for superior performances. With some explorations, we find simple techniques such as dropout, can greatly boost model performance with a careful design. Therefore, in this paper, we integrate different dropout techniques into the training of Transformer models. Specifically, we propose an... | Zhen Wu, Lijun Wu, Qi Meng, Yingce Xia, Shufang Xie, Tao Qin, Xinyu Dai, TieYan Liu |  |
| 379 |  |  [tWT-WT: A Dataset to Assert the Role of Target Entities for Detecting Stance of Tweets](https://doi.org/10.18653/v1/2021.naacl-main.303) |  | 0 | The stance detection task aims at detecting the stance of a tweet or a text for a target. These targets can be named entities or free-form sentences (claims). Though the task involves reasoning of the tweet with respect to a target, we find that it is possible to achieve high accuracy on several publicly available Twitter stance detection datasets without looking at the target sentence. Specifically, a simple tweet classification model achieved human-level performance on the WT–WT dataset and... | Ayush Kaushal, Avirup Saha, Niloy Ganguly |  |
| 380 |  |  [Learning to Learn to be Right for the Right Reasons](https://doi.org/10.18653/v1/2021.naacl-main.304) |  | 0 | Improving model generalization on held-out data is one of the core objectives in common- sense reasoning. Recent work has shown that models trained on the dataset with superficial cues tend to perform well on the easy test set with superficial cues but perform poorly on the hard test set without superficial cues. Previous approaches have resorted to manual methods of encouraging models not to overfit to superficial cues. While some of the methods have improved performance on hard instances,... | Pride Kavumba, Benjamin Heinzerling, Ana Brassard, Kentaro Inui |  |
| 381 |  |  [Double Perturbation: On the Robustness of Robustness and Counterfactual Bias Evaluation](https://doi.org/10.18653/v1/2021.naacl-main.305) |  | 0 | Robustness and counterfactual bias are usually evaluated on a test dataset. However, are these evaluations robust? If the test dataset is perturbed slightly, will the evaluation results keep the same? In this paper, we propose a “double perturbation” framework to uncover model weaknesses beyond the test dataset. The framework first perturbs the test dataset to construct abundant natural sentences similar to the test data, and then diagnoses the prediction change regarding a single-word... | Chong Zhang, Jieyu Zhao, Huan Zhang, KaiWei Chang, ChoJui Hsieh |  |
| 382 |  |  [Explaining Neural Network Predictions on Sentence Pairs via Learning Word-Group Masks](https://doi.org/10.18653/v1/2021.naacl-main.306) |  | 0 | Explaining neural network models is important for increasing their trustworthiness in real-world applications. Most existing methods generate post-hoc explanations for neural network models by identifying individual feature attributions or detecting interactions between adjacent features. However, for models with text pairs as inputs (e.g., paraphrase identification), existing methods are not sufficient to capture feature interactions between two texts and their simple extension of computing... | Hanjie Chen, Song Feng, Jatin Ganhotra, Hui Wan, R. Chulaka Gunasekara, Sachindra Joshi, Yangfeng Ji |  |
| 383 |  |  [Almost Free Semantic Draft for Neural Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.307) |  | 0 | Translation quality can be improved by global information from the required target sentence because the decoder can understand both past and future information. However, the model needs additional cost to produce and consider such global information. In this work, to inject global information but also save cost, we present an efficient method to sample and consider a semantic draft as global information from semantic space for decoding with almost free of cost. Unlike other successful... | Xi Ai, Bin Fang |  |
| 384 |  |  [Pruning-then-Expanding Model for Domain Adaptation of Neural Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.308) |  | 0 | Domain Adaptation is widely used in practical applications of neural machine translation, which aims to achieve good performance on both general domain and in-domain data. However, the existing methods for domain adaptation usually suffer from catastrophic forgetting, large domain divergence, and model explosion. To address these three problems, we propose a method of “divide and conquer” which is based on the importance of neurons or parameters for the translation model. In this method, we... | Shuhao Gu, Yang Feng, Wanying Xie |  |
| 385 |  |  [Multi-Hop Transformer for Document-Level Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.309) |  | 0 | Document-level neural machine translation (NMT) has proven to be of profound value for its effectiveness on capturing contextual information. Nevertheless, existing approaches 1) simply introduce the representations of context sentences without explicitly characterizing the inter-sentence reasoning process; and 2) feed ground-truth target contexts as extra inputs at the training time, thus facing the problem of exposure bias. We approach these problems with an inspiration from human behavior –... | Long Zhang, Tong Zhang, Haibo Zhang, Baosong Yang, Wei Ye, Shikun Zhang |  |
| 386 |  |  [Continual Learning for Neural Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.310) |  | 0 | Neural machine translation (NMT) models are data-driven and require large-scale training corpus. In practical applications, NMT models are usually trained on a general domain corpus and then fine-tuned by continuing training on the in-domain corpus. However, this bears the risk of catastrophic forgetting that the performance on the general domain is decreased drastically. In this work, we propose a new continual learning framework for NMT models. We consider a scenario where the training is... | Yue Cao, HaoRan Wei, Boxing Chen, Xiaojun Wan |  |
| 387 |  |  [Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios](https://doi.org/10.18653/v1/2021.naacl-main.311) |  | 0 | Unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has achieved remarkable results in several translation tasks. However, in real-world scenarios, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian, and UNMT systems usually perform poorly when there is not adequate training corpus for one language. In this paper, we first define and analyze the unbalanced training data scenario for UNMT. Based on this... | Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita, Tiejun Zhao |  |
| 388 |  |  [Smart-Start Decoding for Neural Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.312) |  | 0 | Most current neural machine translation models adopt a monotonic decoding order of either left-to-right or right-to-left. In this work, we propose a novel method that breaks up the limitation of these decoding orders, called Smart-Start decoding. More specifically, our method first predicts a median word. It starts to decode the words on the right side of the median word and then generates words on the left. We evaluate the proposed Smart-Start decoding method on three datasets. Experimental... | Jian Yang, Shuming Ma, Dongdong Zhang, Juncheng Wan, Zhoujun Li, Ming Zhou |  |
| 389 |  |  [Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.313) |  | 0 | Non-Autoregressive machine Translation (NAT) models have demonstrated significant inference speedup but suffer from inferior translation accuracy. The common practice to tackle the problem is transferring the Autoregressive machine Translation (AT) knowledge to NAT models, e.g., with knowledge distillation. In this work, we hypothesize and empirically verify that AT and NAT encoders capture different linguistic properties of source sentences. Therefore, we propose to adopt multi-task learning... | Yongchang Hao, Shilin He, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu, Xing Wang |  |
| 390 |  |  [ER-AE: Differentially Private Text Generation for Authorship Anonymization](https://doi.org/10.18653/v1/2021.naacl-main.314) |  | 0 | Most of privacy protection studies for textual data focus on removing explicit sensitive identifiers. However, personal writing style, as a strong indicator of the authorship, is often neglected. Recent studies, such as SynTF, have shown promising results on privacy-preserving text mining. However, their anonymization algorithm can only output numeric term vectors which are difficult for the recipients to interpret. We propose a novel text generation model with a two-set exponential mechanism... | Haohan Bo, Steven H. H. Ding, Benjamin C. M. Fung, Farkhund Iqbal |  |
| 391 |  |  [Distantly Supervised Transformers For E-Commerce Product QA](https://doi.org/10.18653/v1/2021.naacl-main.315) |  | 0 | We propose a practical instant question answering (QA) system on product pages of e-commerce services, where for each user query, relevant community question answer (CQA) pairs are retrieved. User queries and CQA pairs differ significantly in language characteristics making relevance learning difficult. Our proposed transformer-based model learns a robust relevance function by jointly learning unified syntactic and semantic representations without the need for human labeled data. This is... | Happy Mittal, Aniket Chakrabarti, Belhassen Bayar, Animesh Anant Sharma, Nikhil Rasiwasia |  |
| 392 |  |  [Quantitative Day Trading from Natural Language using Reinforcement Learning](https://doi.org/10.18653/v1/2021.naacl-main.316) |  | 0 | It is challenging to design profitable and practical trading strategies, as stock price movements are highly stochastic, and the market is heavily influenced by chaotic data across sources like news and social media. Existing NLP approaches largely treat stock prediction as a classification or regression problem and are not optimized to make profitable investment decisions. Further, they do not model the temporal dynamics of large volumes of diversely influential text to which the market... | Ramit Sawhney, Arnav Wadhwa, Shivam Agarwal, Rajiv Ratn Shah |  |
| 393 |  |  [Restoring and Mining the Records of the Joseon Dynasty via Neural Language Modeling and Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.317) |  | 0 | Understanding voluminous historical records provides clues on the past in various aspects, such as social and political issues and even natural science facts. However, it is generally difficult to fully utilize the historical records, since most of the documents are not written in a modern language and part of the contents are damaged over time. As a result, restoring the damaged or unrecognizable parts as well as translating the records into modern languages are crucial tasks. In response, we... | Kyeongpil Kang, Kyohoon Jin, Soyoung Yang, Soojin Jang, Jaegul Choo, Youngbin Kim |  |
| 394 |  |  [Modeling Diagnostic Label Correlation for Automatic ICD Coding](https://doi.org/10.18653/v1/2021.naacl-main.318) |  | 0 | Given the clinical notes written in electronic health records (EHRs), it is challenging to predict the diagnostic codes which is formulated as a multi-label classification task. The large set of labels, the hierarchical dependency, and the imbalanced data make this prediction task extremely hard. Most existing work built a binary prediction for each label independently, ignoring the dependencies between labels. To address this problem, we propose a two-stage framework to improve automatic ICD... | ShangChi Tsai, ChaoWei Huang, YunNung Chen |  |
| 395 |  |  [Self-Supervised Contrastive Learning for Efficient User Satisfaction Prediction in Conversational Agents](https://doi.org/10.18653/v1/2021.naacl-main.319) |  | 0 | Turn-level user satisfaction is one of the most important performance metrics for conversational agents. It can be used to monitor the agent’s performance and provide insights about defective user experiences. While end-to-end deep learning has shown promising results, having access to a large number of reliable annotated samples required by these methods remains challenging. In a large-scale conversational system, there is a growing number of newly developed skills, making the traditional data... | Mohammad Kachuee, Hao Yuan, YoungBum Kim, Sungjin Lee |  |
| 396 |  |  [A recipe for annotating grounded clarifications](https://doi.org/10.18653/v1/2021.naacl-main.320) |  | 0 | In order to interpret the communicative intents of an utterance, it needs to be grounded in something that is outside of language; that is, grounded in world modalities. In this paper, we argue that dialogue clarification mechanisms make explicit the process of interpreting the communicative intents of the speaker’s utterances by grounding them in the various modalities in which the dialogue is situated. This paper frames dialogue clarification mechanisms as an understudied research problem and... | Luciana Benotti, Patrick Blackburn |  |
| 397 |  |  [Grey-box Adversarial Attack And Defence For Sentiment Classification](https://doi.org/10.18653/v1/2021.naacl-main.321) |  | 0 | We introduce a grey-box adversarial attack and defence framework for sentiment classification. We address the issues of differentiability, label preservation and input reconstruction for adversarial attack and defence in one unified framework. Our results show that once trained, the attacking model is capable of generating high-quality adversarial examples substantially faster (one order of magnitude less in time) than state-of-the-art attacking methods. These examples also preserve the... | Ying Xu, Xu Zhong, Antonio JimenoYepes, Jey Han Lau |  |
| 398 |  |  [How low is too low? A monolingual take on lemmatisation in Indian languages](https://doi.org/10.18653/v1/2021.naacl-main.322) |  | 0 | Lemmatization aims to reduce the sparse data problem by relating the inflected forms of a word to its dictionary form. Most prior work on ML based lemmatization has focused on high resource languages, where data sets (word forms) are readily available. For languages which have no linguistic work available, especially on morphology or in languages where the computational realization of linguistic rules is complex and cumbersome, machine learning based lemmatizers are the way togo. In this paper,... | Saunack Kumar, Saurav Kumar, Pushpak Bhattacharyya |  |
| 399 |  |  [Causal Effects of Linguistic Properties](https://doi.org/10.18653/v1/2021.naacl-main.323) |  | 0 | We consider the problem of using observational data to estimate the causal effects of linguistic properties. For example, does writing a complaint politely lead to a faster response time? How much will a positive product review increase sales? This paper addresses two technical challenges related to the problem before developing a practical method. First, we formalize the causal quantity of interest as the effect of a writer’s intent, and establish the assumptions necessary to identify this... | Reid Pryzant, Dallas Card, Dan Jurafsky, Victor Veitch, Dhanya Sridhar |  |
| 400 |  |  [Dynabench: Rethinking Benchmarking in NLP](https://doi.org/10.18653/v1/2021.naacl-main.324) |  | 0 | We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple... | Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, Adina Williams |  |
| 401 |  |  [Translational NLP: A New Paradigm and General Principles for Natural Language Processing Research](https://doi.org/10.18653/v1/2021.naacl-main.325) |  | 0 | Natural language processing (NLP) research combines the study of universal principles, through basic science, with applied science targeting specific use cases and settings. However, the process of exchange between basic NLP and applications is often assumed to emerge naturally, resulting in many innovations going unapplied and many important questions left unstudied. We describe a new paradigm of Translational NLP, which aims to structure and facilitate the processes by which basic and applied... | Denis NewmanGriffis, Jill Fain Lehman, Carolyn P. Rosé, Harry Hochheiser |  |
| 402 |  |  [Predicting Discourse Trees from Transformer-based Neural Summarizers](https://doi.org/10.18653/v1/2021.naacl-main.326) |  | 0 | Previous work indicates that discourse information benefits summarization. In this paper, we explore whether this synergy between discourse and summarization is bidirectional, by inferring document-level discourse trees from pre-trained neural summarizers. In particular, we generate unlabeled RST-style discourse trees from the self-attention matrices of the transformer model. Experiments across models and datasets reveal that the summarizer learns both, dependency- and constituency-style... | Wen Xiao, Patrick Huber, Giuseppe Carenini |  |
| 403 |  |  [Probing for Bridging Inference in Transformer Language Models](https://doi.org/10.18653/v1/2021.naacl-main.327) |  | 0 | We probe pre-trained transformer language models for bridging inference. We first investigate individual attention heads in BERT and observe that attention heads at higher layers prominently focus on bridging relations in-comparison with the lower and middle layers, also, few specific attention heads concentrate consistently on bridging. More importantly, we consider language models as a whole in our second approach where bridging anaphora resolution is formulated as a masked token prediction... | Onkar Pandit, Yufang Hou |  |
| 404 |  |  [Is Incoherence Surprising? Targeted Evaluation of Coherence Prediction from Language Models](https://doi.org/10.18653/v1/2021.naacl-main.328) |  | 0 | Coherent discourse is distinguished from a mere collection of utterances by the satisfaction of a diverse set of constraints, for example choice of expression, logical relation between denoted events, and implicit compatibility with world-knowledge. Do neural language models encode such constraints? We design an extendable set of test suites addressing different aspects of discourse and dialogue coherence. Unlike most previous coherence evaluation studies, we address specific linguistic devices... | Anne Beyer, Sharid Loáiciga, David Schlangen |  |
| 405 |  |  [Stay Together: A System for Single and Split-antecedent Anaphora Resolution](https://doi.org/10.18653/v1/2021.naacl-main.329) |  | 0 | The state-of-the-art on basic, single-antecedent anaphora has greatly improved in recent years. Researchers have therefore started to pay more attention to more complex cases of anaphora such as split-antecedent anaphora, as in “Time-Warner is considering a legal challenge to Telecommunications Inc’s plan to buy half of Showtime Networks Inc–a move that could lead to all-out war between the two powerful companies”. Split-antecedent anaphora is rarer and more complex to resolve than... | Juntao Yu, Nafise Sadat Moosavi, Silviu Paun, Massimo Poesio |  |
| 406 |  |  [Redefining Absent Keyphrases and their Effect on Retrieval Effectiveness](https://doi.org/10.18653/v1/2021.naacl-main.330) |  | 0 | Neural keyphrase generation models have recently attracted much interest due to their ability to output absent keyphrases, that is, keyphrases that do not appear in the source text. In this paper, we discuss the usefulness of absent keyphrases from an Information Retrieval (IR) perspective, and show that the commonly drawn distinction between present and absent keyphrases is not made explicit enough. We introduce a finer-grained categorization scheme that sheds more light on the impact of... | Florian Boudin, Ygor Gallina |  |
| 407 |  |  [CoRT: Complementary Rankings from Transformers](https://doi.org/10.18653/v1/2021.naacl-main.331) |  | 0 | Many recent approaches towards neural information retrieval mitigate their computational costs by using a multi-stage ranking pipeline. In the first stage, a number of potentially relevant candidates are retrieved using an efficient retrieval model such as BM25. Although BM25 has proven decent performance as a first-stage ranker, it tends to miss relevant passages. In this context we propose CoRT, a simple neural first-stage ranking model that leverages contextual representations from... | Marco Wrzalik, Dirk Krechel |  |
| 408 |  |  [Multi-source Neural Topic Modeling in Multi-view Embedding Spaces](https://doi.org/10.18653/v1/2021.naacl-main.332) |  | 0 | Though word embeddings and topics are complementary representations, several past works have only used pretrained word embeddings in (neural) topic modeling to address data sparsity in short-text or small collection of documents. This work presents a novel neural topic modeling framework using multi-view embed ding spaces: (1) pretrained topic-embeddings, and (2) pretrained word-embeddings (context-insensitive from Glove and context-sensitive from BERT models) jointly from one or many sources... | Pankaj Gupta, Yatin Chaudhary, Hinrich Schütze |  |
| 409 |  |  [Inductive Topic Variational Graph Auto-Encoder for Text Classification](https://doi.org/10.18653/v1/2021.naacl-main.333) |  | 0 | Graph convolutional networks (GCNs) have been applied recently to text classification and produced an excellent performance. However, existing GCN-based methods do not assume an explicit latent semantic structure of documents, making learned representations less effective and difficult to interpret. They are also transductive in nature, thus cannot handle out-of-graph documents. To address these issues, we propose a novel model named inductive Topic Variational Graph Auto-Encoder (T-VGAE),... | Qianqian Xie, Jimin Huang, Pan Du, Min Peng, JianYun Nie |  |
| 410 |  |  [Self-Alignment Pretraining for Biomedical Entity Representations](https://doi.org/10.18653/v1/2021.naacl-main.334) |  | 0 | Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a... | Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, Nigel Collier |  |
| 411 |  |  [TaxoClass: Hierarchical Multi-Label Text Classification Using Only Class Names](https://doi.org/10.18653/v1/2021.naacl-main.335) |  | 0 | Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a taxonomic class hierarchy. Most existing HMTC methods train classifiers using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the... | Jiaming Shen, Wenda Qiu, Yu Meng, Jingbo Shang, Xiang Ren, Jiawei Han |  |
| 412 |  |  [MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding](https://doi.org/10.18653/v1/2021.naacl-main.336) |  | 0 | Generating metaphors is a challenging task as it requires a proper understanding of abstract concepts, making connections between unrelated concepts, and deviating from the literal meaning. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Based on a theoretically-grounded connection between metaphors and symbols, we propose a method to automatically construct a parallel corpus by transforming a large number of metaphorical sentences... | Tuhin Chakrabarty, Xurui Zhang, Smaranda Muresan, Nanyun Peng |  |
| 413 |  |  [On Learning Text Style Transfer with Direct Rewards](https://doi.org/10.18653/v1/2021.naacl-main.337) |  | 0 | In most cases, the lack of parallel corpora makes it impossible to directly train supervised models for the text style transfer task. In this paper, we explore training algorithms that instead optimize reward functions that explicitly consider different aspects of the style-transferred outputs. In particular, we leverage semantic similarity metrics originally used for fine-tuning neural machine translation models to explicitly assess the preservation of content between system outputs and input... | Yixin Liu, Graham Neubig, John Wieting |  |
| 414 |  |  [Focused Attention Improves Document-Grounded Generation](https://doi.org/10.18653/v1/2021.naacl-main.338) |  | 0 | Document grounded generation is the task of using the information provided in a document to improve text generation. This work focuses on two different document grounded generation tasks: Wikipedia Update Generation task and Dialogue response generation. Our work introduces two novel adaptations of large scale pre-trained encoder-decoder models focusing on building context driven representation of the document and enabling specific attention to the information in the document. Additionally, we... | Shrimai Prabhumoye, Kazuma Hashimoto, Yingbo Zhou, Alan W. Black, Ruslan Salakhutdinov |  |
| 415 |  |  [NeuroLogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints](https://doi.org/10.18653/v1/2021.naacl-main.339) |  | 0 | Conditional text generation often requires lexical constraints, i.e., which words should or shouldn’t be included in the output text. While the dominant recipe for conditional text generation has been large-scale pretrained language models that are finetuned on the task-specific training data, such models do not learn to follow the underlying constraints reliably, even when supervised with large amounts of task-specific examples. We propose NeuroLogic Decoding, a simple yet effective algorithm... | Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi |  |
| 416 |  |  [Ask what's missing and what's useful: Improving Clarification Question Generation using Global Knowledge](https://doi.org/10.18653/v1/2021.naacl-main.340) |  | 0 | The ability to generate clarification questions i.e., questions that identify useful missing information in a given context, is important in reducing ambiguity. Humans use previous experience with similar contexts to form a global view and compare it to the given context to ascertain what is missing and what is useful in the context. Inspired by this, we propose a model for clarification question generation where we first identify what is missing by taking a difference between the global and... | Bodhisattwa Prasad Majumder, Sudha Rao, Michel Galley, Julian J. McAuley |  |
| 417 |  |  [Progressive Generation of Long Text with Pretrained Language Models](https://doi.org/10.18653/v1/2021.naacl-main.341) |  | 0 | Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains. To overcome the limitations,... | Bowen Tan, Zichao Yang, Maruan AlShedivat, Eric P. Xing, Zhiting Hu |  |
| 418 |  |  [SOCCER: An Information-Sparse Discourse State Tracking Collection in the Sports Commentary Domain](https://doi.org/10.18653/v1/2021.naacl-main.342) |  | 0 | In the pursuit of natural language understanding, there has been a long standing interest in tracking state changes throughout narratives. Impressive progress has been made in modeling the state of transaction-centric dialogues and procedural texts. However, this problem has been less intensively studied in the realm of general discourse where ground truth descriptions of states may be loosely defined and state changes are less densely distributed over utterances. This paper proposes to turn to... | Ruochen Zhang, Carsten Eickhoff |  |
| 419 |  |  [Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation](https://doi.org/10.18653/v1/2021.naacl-main.343) |  | 0 | With the recent advances of open-domain story generation, the lack of reliable automatic evaluation metrics becomes an increasingly imperative issue that hinders the fast development of story generation. According to conducted researches in this regard, learnable evaluation metrics have promised more accurate assessments by having higher correlations with human judgments. A critical bottleneck of obtaining a reliable learnable evaluation metric is the lack of high-quality training data for... | Sarik Ghazarian, Zixi Liu, Akash SM, Ralph M. Weischedel, Aram Galstyan, Nanyun Peng |  |
| 420 |  |  [MultiOpEd: A Corpus of Multi-Perspective News Editorials](https://doi.org/10.18653/v1/2021.naacl-main.344) |  | 0 | We propose MultiOpEd, an open-domain news editorial corpus that supports various tasks pertaining to the argumentation structure in news editorials, focusing on automatic perspective discovery. News editorial is a genre of persuasive text, where the argumentation structure is usually implicit. However, the arguments presented in an editorial typically center around a concise, focused thesis, which we refer to as their perspective. MultiOpEd aims at supporting the study of multiple tasks... | Siyi Liu, Sihao Chen, Xander Uyttendaele, Dan Roth |  |
| 421 |  |  [Swords: A Benchmark for Lexical Substitution with Improved Data Coverage and Quality](https://doi.org/10.18653/v1/2021.naacl-main.345) |  | 0 | We release a new benchmark for lexical substitution, the task of finding appropriate substitutes for a target word in a context. For writing, lexical substitution systems can assist humans by suggesting words that humans cannot easily think of. However, existing benchmarks depend on human recall as the only source of data, and therefore lack coverage of the substitutes that would be most helpful to humans. Furthermore, annotators often provide substitutes of low quality, which are not actually... | Mina Lee, Chris Donahue, Robin Jia, Alexander Iyabor, Percy Liang |  |
| 422 |  |  ["I'm Not Mad": Commonsense Implications of Negation and Contradiction](https://doi.org/10.18653/v1/2021.naacl-main.346) |  | 0 | Natural language inference requires reasoning about contradictions, negations, and their commonsense implications. Given a simple premise (e.g., “I’m mad at you”), humans can reason about the varying shades of contradictory statements ranging from straightforward negations (“I’m not mad at you”) to commonsense contradictions (“I’m happy”). Moreover, these negated or contradictory statements shift the commonsense implications of the original premise in interesting and nontrivial ways. For... | Liwei Jiang, Antoine Bosselut, Chandra Bhagavatula, Yejin Choi |  |
| 423 |  |  [Identifying Medical Self-Disclosure in Online Communities](https://doi.org/10.18653/v1/2021.naacl-main.347) |  | 0 | Self-disclosure in online health conversations may offer a host of benefits, including earlier detection and treatment of medical issues that may have otherwise gone unaddressed. However, research analyzing medical self-disclosure in online communities is limited. We address this shortcoming by introducing a new dataset of health-related posts collected from online social platforms, categorized into three groups (No Self-Disclosure, Possible Self-Disclosure, and Clear Self-Disclosure) with high... | Mina Valizadeh, Pardis RanjbarNoiey, Cornelia Caragea, Natalie Parde |  |
| 424 |  |  [Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction](https://doi.org/10.18653/v1/2021.naacl-main.348) |  | 0 | We investigate grounded language learning through real-world data, by modelling a teacher-learner dynamics through the natural interactions occurring between users and search engines; in particular, we explore the emergence of semantic generalization from unsupervised dense representations outside of synthetic environments. A grounding domain, a denotation function and a composition function are learned from user data only. We show how the resulting semantics for noun phrases exhibits... | Federico Bianchi, Ciro Greco, Jacopo Tagliabue |  |
| 425 |  |  [Finding Concept-specific Biases in Form-Meaning Associations](https://doi.org/10.18653/v1/2021.naacl-main.349) |  | 0 | This work presents an information-theoretic operationalisation of cross-linguistic non-arbitrariness. It is not a new idea that there are small, cross-linguistic associations between the forms and meanings of words. For instance, it has been claimed (Blasi et al., 2016) that the word for “tongue” is more likely than chance to contain the phone [l]. By controlling for the influence of language family and geographic proximity within a very large concept-aligned, cross-lingual lexicon, we extend... | Tiago Pimentel, Brian Roark, Søren Wichmann, Ryan Cotterell, Damián E. Blasi |  |
| 426 |  |  [How (Non-)Optimal is the Lexicon?](https://doi.org/10.18653/v1/2021.naacl-main.350) |  | 0 | The mapping of lexical meanings to wordforms is a major feature of natural languages. While usage pressures might assign short words to frequent meanings (Zipf’s law of abbreviation), the need for a productive and open-ended vocabulary, local constraints on sequences of symbols, and various other factors all shape the lexicons of the world’s languages. Despite their importance in shaping lexical structure, the relative contributions of these factors have not been fully quantified. Taking a... | Tiago Pimentel, Irene Nikkarinen, Kyle Mahowald, Ryan Cotterell, Damián E. Blasi |  |
| 427 |  |  [Word Complexity is in the Eye of the Beholder](https://doi.org/10.18653/v1/2021.naacl-main.351) |  | 0 | Lexical complexity is a highly subjective notion, yet this factor is often neglected in lexical simplification and readability systems which use a ”one-size-fits-all” approach. In this paper, we investigate which aspects contribute to the notion of lexical complexity in various groups of readers, focusing on native and non-native speakers of English, and how the notion of complexity changes depending on the proficiency level of a non-native reader. To facilitate reproducibility of our approach... | Sian Gooding, Ekaterina Kochmar, Seid Muhie Yimam, Chris Biemann |  |
| 428 |  |  [Linguistic Complexity Loss in Text-Based Therapy](https://doi.org/10.18653/v1/2021.naacl-main.352) |  | 0 | The complexity loss paradox, which posits that individuals suffering from disease exhibit surprisingly predictable behavioral dynamics, has been observed in a variety of both human and animal physiological systems. The recent advent of online text-based therapy presents a new opportunity to analyze the complexity loss paradox in a novel operationalization: linguistic complexity loss in text-based therapy conversations. In this paper, we analyze linguistic complexity correlates of mental health... | Jason Wei, Kelly Finn, Emma Templeton, Thalia Wheatley, Soroush Vosoughi |  |
| 429 |  |  [Ab Antiquo: Neural Proto-language Reconstruction](https://doi.org/10.18653/v1/2021.naacl-main.353) |  | 0 | Historical linguists have identified regularities in the process of historic sound change. The comparative method utilizes those regularities to reconstruct proto-words based on observed forms in daughter languages. Can this process be efficiently automated? We address the task of proto-word reconstruction, in which the model is exposed to cognates in contemporary daughter languages, and has to predict the proto word in the ancestor language. We provide a novel dataset for this task,... | Carlo Meloni, Shauli Ravfogel, Yoav Goldberg |  |
| 430 |  |  [On Biasing Transformer Attention Towards Monotonicity](https://doi.org/10.18653/v1/2021.naacl-main.354) |  | 0 | Many sequence-to-sequence tasks in natural language processing are roughly monotonic in the alignment between source and target sequence, and previous work has facilitated or enforced learning of monotonic attention behavior via specialized attention functions or pretraining. In this work, we introduce a monotonicity loss function that is compatible with standard attention mechanisms and test it on several sequence-to-sequence tasks: grapheme-to-phoneme conversion, morphological inflection,... | Annette Rios, Chantal Amrhein, Noëmi Aepli, Rico Sennrich |  |
| 431 |  |  [Extracting a Knowledge Base of Mechanisms from COVID-19 Papers](https://doi.org/10.18653/v1/2021.naacl-main.355) |  | 0 | The COVID-19 pandemic has spawned a diverse body of scientific literature that is challenging to navigate, stimulating interest in automated tools to help find useful knowledge. We pursue the construction of a knowledge base (KB) of mechanisms—a fundamental concept across the sciences, which encompasses activities, functions and causal relations, ranging from cellular processes to economic impacts. We extract this information from the natural language of scientific papers by developing a broad,... | Tom Hope, Aida Amini, David Wadden, Madeleine van Zuylen, Sravanthi Parasa, Eric Horvitz, Daniel S. Weld, Roy Schwartz, Hannaneh Hajishirzi |  |
| 432 |  |  [Constrained Multi-Task Learning for Event Coreference Resolution](https://doi.org/10.18653/v1/2021.naacl-main.356) |  | 0 | We propose a neural event coreference model in which event coreference is jointly trained with five tasks: trigger detection, entity coreference, anaphoricity determination, realis detection, and argument extraction. To guide the learning of this complex model, we incorporate cross-task consistency constraints into the learning process as soft constraints via designing penalty functions. In addition, we propose the novel idea of viewing entity coreference and event coreference as a single... | Jing Lu, Vincent Ng |  |
| 433 |  |  [Empirical Evaluation of Pre-trained Transformers for Human-Level NLP: The Role of Sample Size and Dimensionality](https://doi.org/10.18653/v1/2021.naacl-main.357) |  | 0 | In human-level NLP tasks, such as predicting mental health, personality, or demographics, the number of observations is often smaller than the standard 768+ hidden state sizes of each layer within modern transformer-based language models, limiting the ability to effectively leverage transformers. Here, we provide a systematic study on the role of dimension reduction methods (principal components analysis, factorization techniques, or multi-layer auto-encoders) as well as the dimensionality of... | Adithya V. Ganesan, Matthew Matero, Aravind Reddy Ravula, Huy Vu, H. Andrew Schwartz |  |
| 434 |  |  [Leveraging Deep Representations of Radiology Reports in Survival Analysis for Predicting Heart Failure Patient Mortality](https://doi.org/10.18653/v1/2021.naacl-main.358) |  | 0 | Utilizing clinical texts in survival analysis is difficult because they are largely unstructured. Current automatic extraction models fail to capture textual information comprehensively since their labels are limited in scope. Furthermore, they typically require a large amount of data and high-quality expert annotations for training. In this work, we present a novel method of using BERT-based hidden layer representations of clinical texts as covariates for proportional hazards models to predict... | Hyun Gi Lee, Evan Sholle, Ashley Beecy, Subhi J. Al'Aref, Yifan Peng |  |
| 435 |  |  [On the Use of Context for Predicting Citation Worthiness of Sentences in Scholarly Articles](https://doi.org/10.18653/v1/2021.naacl-main.359) |  | 0 | In this paper, we study the importance of context in predicting the citation worthiness of sentences in scholarly articles. We formulate this problem as a sequence labeling task solved using a hierarchical BiLSTM model. We contribute a new benchmark dataset containing over two million sentences and their corresponding labels. We preserve the sentence order in this dataset and perform document-level train/test splits, which importantly allows incorporating contextual information in the modeling... | Rakesh Gosangi, Ravneet Arora, Mohsen Gheisarieha, Debanjan Mahata, Haimin Zhang |  |
| 436 |  |  [Data and Model Distillation as a Solution for Domain-transferable Fact Verification](https://doi.org/10.18653/v1/2021.naacl-main.360) |  | 0 | While neural networks produce state-of-the-art performance in several NLP tasks, they generally depend heavily on lexicalized information, which transfer poorly between domains. We present a combination of two strategies to mitigate this dependence on lexicalized information in fact verification tasks. We present a data distillation technique for delexicalization, which we then combine with a model distillation method to prevent aggressive data distillation. We show that by using our solution,... | Mitch Paul Mithun, Sandeep Suntwal, Mihai Surdeanu |  |
| 437 |  |  [Adapting Coreference Resolution for Processing Violent Death Narratives](https://doi.org/10.18653/v1/2021.naacl-main.361) |  | 0 | Coreference resolution is an important compo-nent in analyzing narrative text from admin-istrative data (e.g., clinical or police sources).However, existing coreference models trainedon general language corpora suffer from poortransferability due to domain gaps, especiallywhen they are applied to gender-inclusive datawith lesbian, gay, bisexual, and transgender(LGBT) individuals. In this paper, we an-alyzed the challenges of coreference resolu-tion in an exemplary form of administrativetext... | Ankith Uppunda, Susan D. Cochran, Jacob G. Foster, Alina ArsenievKoehler, Vickie M. Mays, KaiWei Chang |  |
| 438 |  |  [Time-Stamped Language Model: Teaching Language Models to Understand The Flow of Events](https://doi.org/10.18653/v1/2021.naacl-main.362) |  | 0 | Tracking entities throughout a procedure described in a text is challenging due to the dynamic nature of the world described in the process. Firstly, we propose to formulate this task as a question answering problem. This enables us to use pre-trained transformer-based language models on other QA benchmarks by adapting those to the procedural text understanding. Secondly, since the transformer-based language models cannot encode the flow of events by themselves, we propose a Time-Stamped... | Hossein Rajaby Faghihi, Parisa Kordjamshidi |  |
| 439 |  |  [If You Want to Go Far Go Together: Unsupervised Joint Candidate Evidence Retrieval for Multi-hop Question Answering](https://doi.org/10.18653/v1/2021.naacl-main.363) |  | 0 | Multi-hop reasoning requires aggregation and inference from multiple facts. To retrieve such facts, we propose a simple approach that retrieves and reranks set of evidence facts jointly. Our approach first generates unsupervised clusters of sentences as candidate evidence by accounting links between sentences and coverage with the given query. Then, a RoBERTa-based reranker is trained to bring the most representative evidence cluster to the top. We specifically emphasize on the importance of... | Vikas Yadav, Steven Bethard, Mihai Surdeanu |  |
| 440 |  |  [SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning](https://doi.org/10.18653/v1/2021.naacl-main.364) |  | 0 | This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on... | Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, Parisa Kordjamshidi |  |
| 441 |  |  [A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers](https://doi.org/10.18653/v1/2021.naacl-main.365) |  | 0 | Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type... | Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, Matt Gardner |  |
| 442 |  |  [Differentiable Open-Ended Commonsense Reasoning](https://doi.org/10.18653/v1/2021.naacl-main.366) |  | 0 | Current commonsense reasoning research focuses on developing models that use commonsense knowledge to answer multiple-choice questions. However, systems designed to answer multiple-choice questions may not be useful in applications that do not provide a small list of candidate answers to choose from. As a step towards making commonsense reasoning research more realistic, we propose to study open-ended commonsense reasoning (OpenCSR) — the task of answering a commonsense question without any... | Bill Yuchen Lin, Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Xiang Ren, William W. Cohen |  |
| 443 |  |  [Does Structure Matter? Encoding Documents for Machine Reading Comprehension](https://doi.org/10.18653/v1/2021.naacl-main.367) |  | 0 | Machine reading comprehension is a challenging task especially for querying documents with deep and interconnected contexts. Transformer-based methods have shown advanced performances on this task; however, most of them still treat documents as a flat sequence of tokens. This work proposes a new Transformer-based method that reads a document as tree slices. It contains two modules for identifying more relevant text passage and the best answer span respectively, which are not only jointly... | Hui Wan, Song Feng, R. Chulaka Gunasekara, Siva Sankalp Patel, Sachindra Joshi, Luis A. Lastras |  |
| 444 |  |  [Multi-Step Reasoning Over Unstructured Text with Beam Dense Retrieval](https://doi.org/10.18653/v1/2021.naacl-main.368) |  | 0 | Complex question answering often requires finding a reasoning chain that consists of multiple evidence pieces. Current approaches incorporate the strengths of structured knowledge and unstructured text, assuming text corpora is semi-structured. Building on dense retrieval methods, we propose a new multi-step retrieval approach (BeamDR) that iteratively forms an evidence chain through beam search in dense representations. When evaluated on multi-hop question answering, BeamDR is competitive to... | Chen Zhao, Chenyan Xiong, Jordan L. BoydGraber, Hal Daumé III |  |
| 445 |  |  [Scalable and Interpretable Semantic Change Detection](https://doi.org/10.18653/v1/2021.naacl-main.369) |  | 0 | Several cluster-based methods for semantic change detection with contextual embeddings emerged recently. They allow a fine-grained analysis of word use change by aggregating embeddings into clusters that reflect the different usages of the word. However, these methods are unscalable in terms of memory consumption and computation time. Therefore, they require a limited set of target words to be picked in advance. This drastically limits the usability of these methods in open exploratory tasks,... | Syrielle Montariol, Matej Martinc, Lidia Pivovarova |  |
| 446 |  |  [Scalar Adjective Identification and Multilingual Ranking](https://doi.org/10.18653/v1/2021.naacl-main.370) |  | 0 | The intensity relationship that holds between scalar adjectives (e.g., nice < great < wonderful) is highly relevant for natural language inference and common-sense reasoning. Previous research on scalar adjective ranking has focused on English, mainly due to the availability of datasets for evaluation. We introduce a new multilingual dataset in order to promote research on scalar adjectives in new languages. We perform a series of experiments and set performance baselines on this dataset, using... | Aina Garí Soler, Marianna Apidianaki |  |
| 447 |  |  [ESC: Redesigning WSD with Extractive Sense Comprehension](https://doi.org/10.18653/v1/2021.naacl-main.371) |  | 0 | Word Sense Disambiguation (WSD) is a historical NLP task aimed at linking words in contexts to discrete sense inventories and it is usually cast as a multi-label classification task. Recently, several neural approaches have employed sense definitions to better represent word meanings. Yet, these approaches do not observe the input sentence and the sense definition candidates all at once, thus potentially reducing the model performance and generalization power. We cope with this issue by... | Edoardo Barba, Tommaso Pasini, Roberto Navigli |  |
| 448 |  |  [Recent advances in neural metaphor processing: A linguistic, cognitive and social perspective](https://doi.org/10.18653/v1/2021.naacl-main.372) |  | 0 | Metaphor is an indispensable part of human cognition and everyday communication. Much research has been conducted elucidating metaphor processing in the mind/brain and the role it plays in communication. in recent years, metaphor processing systems have benefited greatly from these studies, as well as the rapid advances in deep learning for natural language processing (NLP). This paper provides a comprehensive review and discussion of recent developments in automated metaphor processing, in... | Xiaoyu Tong, Ekaterina Shutova, Martha Lewis |  |
| 449 |  |  [Constructing Taxonomies from Pretrained Language Models](https://doi.org/10.18653/v1/2021.naacl-main.373) |  | 0 | We present a method for constructing taxonomic trees (e.g., WordNet) using pretrained language models. Our approach is composed of two modules, one that predicts parenthood relations and another that reconciles those pairwise predictions into trees. The parenthood prediction module produces likelihood scores for each potential parent-child pair, creating a graph of parent-child relation scores. The tree reconciliation module treats the task as a graph optimization problem and outputs the... | Catherine Chen, Kevin Lin, Dan Klein |  |
| 450 |  |  [Event Representation with Sequential, Semi-Supervised Discrete Variables](https://doi.org/10.18653/v1/2021.naacl-main.374) |  | 0 | Within the context of event modeling and understanding, we propose a new method for neural sequence modeling that takes partially-observed sequences of discrete, external knowledge into account. We construct a sequential neural variational autoencoder, which uses Gumbel-Softmax reparametrization within a carefully defined encoder, to allow for successful backpropagation during training. The core idea is to allow semi-supervised external discrete knowledge to guide, but not restrict, the... | Mehdi Rezaee, Francis Ferraro |  |
| 451 |  |  [Seq2Emo: A Sequence to Multi-Label Emotion Classification Model](https://doi.org/10.18653/v1/2021.naacl-main.375) |  | 0 | Multi-label emotion classification is an important task in NLP and is essential to many applications. In this work, we propose a sequence-to-emotion (Seq2Emo) approach, which implicitly models emotion correlations in a bi-directional decoder. Experiments on SemEval’18 and GoEmotions datasets show that our approach outperforms state-of-the-art methods (without using external data). In particular, Seq2Emo outperforms the binary relevance (BR) and classifier chain (CC) approaches in a fair setting. | Chenyang Huang, Amine Trabelsi, Xuebin Qin, Nawshad Farruque, Lili Mou, Osmar R. Zaïane |  |
| 452 |  |  [Knowledge Enhanced Masked Language Model for Stance Detection](https://doi.org/10.18653/v1/2021.naacl-main.376) |  | 0 | Detecting stance on Twitter is especially challenging because of the short length of each tweet, the continuous coinage of new terminology and hashtags, and the deviation of sentence structure from standard prose. Fine-tuned language models using large-scale in-domain data have been shown to be the new state-of-the-art for many NLP tasks, including stance detection. In this paper, we propose a novel BERT-based fine-tuning method that enhances the masked language model for stance detection.... | Kornraphop Kawintiranon, Lisa Singh |  |
| 453 |  |  [Learning Paralinguistic Features from Audiobooks through Style Voice Conversion](https://doi.org/10.18653/v1/2021.naacl-main.377) |  | 0 | Paralinguistics, the non-lexical components of speech, play a crucial role in human-human interaction. Models designed to recognize paralinguistic information, particularly speech emotion and style, are difficult to train because of the limited labeled datasets available. In this work, we present a new framework that enables a neural network to learn to extract paralinguistic attributes from speech using data that are not annotated for emotion. We assess the utility of the learned embeddings on... | Zakaria Aldeneh, Matthew Perez, Emily Mower Provost |  |
| 454 |  |  [Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks](https://doi.org/10.18653/v1/2021.naacl-main.378) |  | 0 | This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks. Although some CL techniques have been proposed for document sentiment classification, we are not aware of any CL work on ASC. A CL system that incrementally learns a sequence of ASC tasks should address the following two issues: (1) transfer knowledge learned from previous tasks to the new task to help it learn a better model, and (2) maintain the performance of the models for previous tasks... | Zixuan Ke, Hu Xu, Bing Liu |  |
| 455 |  |  [Adversarial Learning for Zero-Shot Stance Detection on Social Media](https://doi.org/10.18653/v1/2021.naacl-main.379) |  | 0 | Stance detection on social media can help to identify and understand slanted news or commentary in everyday life. In this work, we propose a new model for zero-shot stance detection on Twitter that uses adversarial learning to generalize across topics. Our model achieves state-of-the-art performance on a number of unseen test topics with minimal computational costs. In addition, we extend zero-shot stance detection to topics not previously considered, highlighting future directions for... | Emily Allaway, Malavika Srikanth, Kathleen R. McKeown |  |
| 456 |  |  [Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters](https://doi.org/10.18653/v1/2021.naacl-main.380) |  | 0 | This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents... | Ramakanth Pasunuru, Mengwen Liu, Mohit Bansal, Sujith Ravi, Markus Dreyer |  |
| 457 |  |  [Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization](https://doi.org/10.18653/v1/2021.naacl-main.381) |  | 0 | Abstractive summarization, the task of generating a concise summary of input documents, requires: (1) reasoning over the source document to determine the salient pieces of information scattered across the long document, and (2) composing a cohesive text by reconstructing these salient facts into a shorter summary that faithfully reflects the complex relations connecting these facts. In this paper, we adapt TP-Transformer (Schlag et al., 2019), an architecture that enriches the original... | Yichen Jiang, Asli Celikyilmaz, Paul Smolensky, Paul Soulos, Sudha Rao, Hamid Palangi, Roland Fernandez, Caitlin Smith, Mohit Bansal, Jianfeng Gao |  |
| 458 |  |  [What's in a Summary? Laying the Groundwork for Advances in Hospital-Course Summarization](https://doi.org/10.18653/v1/2021.naacl-main.382) |  | 0 | Summarization of clinical narratives is a long-standing research problem. Here, we introduce the task of hospital-course summarization. Given the documentation authored throughout a patient’s hospitalization, generate a paragraph that tells the story of the patient admission. We construct an English, text-to-text dataset of 109,000 hospitalizations (2M source notes) and their corresponding summary proxy: the clinician-authored “Brief Hospital Course” paragraph written as part of a discharge... | Griffin Adams, Emily Alsentzer, Mert Ketenci, Jason Zucker, Noémie Elhadad |  |
| 459 |  |  [Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics](https://doi.org/10.18653/v1/2021.naacl-main.383) |  | 0 | Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual... | Artidoro Pagnoni, Vidhisha Balachandran, Yulia Tsvetkov |  |
| 460 |  |  [GSum: A General Framework for Guided Neural Abstractive Summarization](https://doi.org/10.18653/v1/2021.naacl-main.384) |  | 0 | Neural abstractive summarization models are flexible and can produce coherent summaries, but they are sometimes unfaithful and can be difficult to control. While previous studies attempt to provide different types of guidance to control the output and increase faithfulness, it is not clear how these strategies compare and contrast to each other. In this paper, we propose a general and extensible guided summarization framework (GSum) that can effectively take different kinds of external guidance... | ZiYi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, Graham Neubig |  |
| 461 |  |  [What Will it Take to Fix Benchmarking in Natural Language Understanding?](https://doi.org/10.18653/v1/2021.naacl-main.385) |  | 0 | Evaluation for many natural language understanding (NLU) tasks is broken: Unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. The recent trend to abandon IID benchmarks in favor of adversarially-constructed, out-of-distribution test sets ensures that current models will perform poorly, but ultimately only obscures the abilities that we want our benchmarks to measure. In this... | Samuel R. Bowman, George E. Dahl |  |
| 462 |  |  [TuringAdvice: A Generative and Dynamic Evaluation of Language Use](https://doi.org/10.18653/v1/2021.naacl-main.386) |  | 0 | We propose TuringAdvice, a new challenge task and dataset for language understanding models. Given a written situation that a real person is currently facing, a model must generate helpful advice in natural language. Our evaluation framework tests a fundamental aspect of human language understanding: our ability to use language to resolve open-ended situations by communicating with each other. Empirical results show that today’s models struggle at TuringAdvice, even multibillion parameter... | Rowan Zellers, Ari Holtzman, Elizabeth Clark, Lianhui Qin, Ali Farhadi, Yejin Choi |  |
| 463 |  |  [Multitask Learning for Emotionally Analyzing Sexual Abuse Disclosures](https://doi.org/10.18653/v1/2021.naacl-main.387) |  | 0 | The #MeToo movement on social media platforms initiated discussions over several facets of sexual harassment in our society. Prior work by the NLP community for automated identification of the narratives related to sexual abuse disclosures barely explored this social phenomenon as an independent task. However, emotional attributes associated with textual conversations related to the #MeToo social movement are complexly intertwined with such narratives. We formulate the task of identifying... | Ramit Sawhney, Puneet Mathur, Taru Jain, Akash Kumar Gautam, Rajiv Ratn Shah |  |
| 464 |  |  [Self Promotion in US Congressional Tweets](https://doi.org/10.18653/v1/2021.naacl-main.388) |  | 0 | Prior studies have found that women self-promote less than men due to gender stereotypes. In this study we built a BERT-based NLP model to predict whether a Congressional tweet shows self-promotion or not and then used this model to examine whether a gender gap in self-promotion exists among Congressional tweets. After analyzing 2 million Congressional tweets from July 2017 to March 2021, controlling for a number of factors that include political party, chamber, age, number of terms in... | Jun Wang, Kelly Cui, Bei Yu |  |
| 465 |  |  [Profiling of Intertextuality in Latin Literature Using Word Embeddings](https://doi.org/10.18653/v1/2021.naacl-main.389) |  | 0 | Identifying intertextual relationships between authors is of central importance to the study of literature. We report an empirical analysis of intertextuality in classical Latin literature using word embedding models. To enable quantitative evaluation of intertextual search methods, we curate a new dataset of 945 known parallels drawn from traditional scholarship on Latin epic poetry. We train an optimized word2vec model on a large corpus of lemmatized Latin, which achieves state-of-the-art... | Patrick J. Burns, James Brofos, Kyle Li, Pramit Chaudhuri, Joseph P. Dexter |  |
| 466 |  |  [Identifying inherent disagreement in natural language inference](https://doi.org/10.18653/v1/2021.naacl-main.390) |  | 0 | Natural language inference (NLI) is the task of determining whether a piece of text is entailed, contradicted by or unrelated to another piece of text. In this paper, we investigate how to tease systematic inferences (i.e., items for which people agree on the NLI label) apart from disagreement items (i.e., items which lead to different annotations), which most prior work has overlooked. To distinguish systematic inferences from disagreement items, we propose Artificial Annotators (AAs) to... | Xinliang Frederick Zhang, MarieCatherine de Marneffe |  |
| 467 |  |  [Modeling Human Mental States with an Entity-based Narrative Graph](https://doi.org/10.18653/v1/2021.naacl-main.391) |  | 0 | Understanding narrative text requires capturing characters’ motivations, goals, and mental states. This paper proposes an Entity-based Narrative Graph (ENG) to model the internal- states of characters in a story. We explicitly model entities, their interactions and the context in which they appear, and learn rich representations for them. We experiment with different task-adaptive pre-training objectives, in-domain training, and symbolic inference to capture dependencies between different... | ITa Lee, Maria Leonor Pacheco, Dan Goldwasser |  |
| 468 |  |  [A Simple and Efficient Multi-Task Learning Approach for Conditioned Dialogue Generation](https://doi.org/10.18653/v1/2021.naacl-main.392) |  | 0 | Conditioned dialogue generation suffers from the scarcity of labeled responses. In this work, we exploit labeled non-dialogue text data related to the condition, which are much easier to collect. We propose a multi-task learning approach to leverage both labeled dialogue and text data. The 3 tasks jointly optimize the same pre-trained Transformer – conditioned dialogue generation task on the labeled dialogue data, conditioned language encoding task and conditioned language generation task on... | Yan Zeng, JianYun Nie |  |
| 469 |  |  [Hurdles to Progress in Long-form Question Answering](https://doi.org/10.18653/v1/2021.naacl-main.393) |  | 0 | The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and... | Kalpesh Krishna, Aurko Roy, Mohit Iyyer |  |
| 470 |  |  [ENTRUST: Argument Reframing with Language Models and Entailment](https://doi.org/10.18653/v1/2021.naacl-main.394) |  | 0 | Framing involves the positive or negative presentation of an argument or issue depending on the audience and goal of the speaker. Differences in lexical framing, the focus of our work, can have large effects on peoples’ opinions and beliefs. To make progress towards reframing arguments for positive effects, we create a dataset and method for this task. We use a lexical resource for “connotations” to create a parallel corpus and propose a method for argument reframing that combines controllable... | Tuhin Chakrabarty, Christopher Hidey, Smaranda Muresan |  |
| 471 |  |  [Paragraph-level Simplification of Medical Texts](https://doi.org/10.18653/v1/2021.naacl-main.395) |  | 0 | We consider the problem of learning to simplify medical texts. This is important because most reliable, up-to-date information in biomedicine is dense with jargon and thus practically inaccessible to the lay audience. Furthermore, manual simplification does not scale to the rapidly growing body of biomedical literature, motivating the need for automated approaches. Unfortunately, there are no large-scale resources available for this task. In this work we introduce a new corpus of parallel texts... | Ashwin Devaraj, Iain James Marshall, Byron C. Wallace, Junyi Jessy Li |  |
| 472 |  |  [An Empirical Study on Neural Keyphrase Generation](https://doi.org/10.18653/v1/2021.naacl-main.396) |  | 0 | Recent years have seen a flourishing of neural keyphrase generation (KPG) works, including the release of several large-scale datasets and a host of new models to tackle them. Model performance on KPG tasks has increased significantly with evolving deep learning research. However, there lacks a comprehensive comparison among different model designs, and a thorough investigation on related factors that may affect a KPG system’s generalization performance. In this empirical study, we aim to fill... | Rui Meng, Xingdi Yuan, Tong Wang, Sanqiang Zhao, Adam Trischler, Daqing He |  |
| 473 |  |  [Attention Head Masking for Inference Time Content Selection in Abstractive Summarization](https://doi.org/10.18653/v1/2021.naacl-main.397) |  | 0 | How can we effectively inform content selection in Transformer-based abstractive summarization models? In this work, we present a simple-yet-effective attention head masking technique, which is applied on encoder-decoder attentions to pinpoint salient content at inference time. Using attention head masking, we are able to reveal the relation between encoder-decoder attentions and content selection behaviors of summarization models. We then demonstrate its effectiveness on three document... | Shuyang Cao, Lu Wang |  |
| 474 |  |  [Factual Probing Is [MASK]: Learning vs. Learning to Recall](https://doi.org/10.18653/v1/2021.naacl-main.398) |  | 0 | Petroni et al. (2019) demonstrated that it is possible to retrieve world facts from a pre-trained language model by expressing them as cloze-style prompts and interpret the model’s prediction accuracy as a lower bound on the amount of factual information it encodes. Subsequent work has attempted to tighten the estimate by searching for better prompts, using a disjoint set of facts as training data. In this work, we make two complementary contributions to better understand these factual probing... | Zexuan Zhong, Dan Friedman, Danqi Chen |  |
| 475 |  |  [Evaluating Saliency Methods for Neural Language Models](https://doi.org/10.18653/v1/2021.naacl-main.399) |  | 0 | Saliency methods are widely used to interpret neural network predictions, but different variants of saliency methods often disagree even on the interpretations of the same prediction made by the same model. In these cases, how do we identify when are these interpretations trustworthy enough to be used in analyses? To address this question, we conduct a comprehensive and quantitative evaluation of saliency methods on a fundamental category of NLP models: neural language models. We evaluate the... | Shuoyang Ding, Philipp Koehn |  |
| 476 |  |  [Contextualized Perturbation for Textual Adversarial Attack](https://doi.org/10.18653/v1/2021.naacl-main.400) |  | 0 | Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure.... | Dianqi Li, Yizhe Zhang, Hao Peng, Liqun Chen, Chris Brockett, MingTing Sun, Bill Dolan |  |
| 477 |  |  [DirectProbe: Studying Representations without Classifiers](https://doi.org/10.18653/v1/2021.naacl-main.401) |  | 0 | Understanding how linguistic structure is encoded in contextualized embedding could help explain their impressive performance across NLP. Existing approaches for probing them usually call for training classifiers and use the accuracy, mutual information, or complexity as a proxy for the representation’s goodness. In this work, we argue that doing so can be unreliable because different representations may need different classifiers. We develop a heuristic, DirectProbe, that directly studies the... | Yichu Zhou, Vivek Srikumar |  |
| 478 |  |  [Evaluating the Values of Sources in Transfer Learning](https://doi.org/10.18653/v1/2021.naacl-main.402) |  | 0 | Transfer learning that adapts a model trained on data-rich sources to low-resource targets has been widely applied in natural language processing (NLP). However, when training a transfer model over multiple sources, not every source is equally useful for the target. To better transfer a model, it is essential to understand the values of the sources. In this paper, we develop , an efficient source valuation framework for quantifying the usefulness of the sources (e.g., ) in transfer learning... | Md. Rizwan Parvez, KaiWei Chang |  |
| 479 |  |  [Too Much in Common: Shifting of Embeddings in Transformer Language Models and its Implications](https://doi.org/10.18653/v1/2021.naacl-main.403) |  | 0 | The success of language models based on the Transformer architecture appears to be inconsistent with observed anisotropic properties of representations learned by such models. We resolve this by showing, contrary to previous studies, that the representations do not occupy a narrow cone, but rather drift in common directions. At any training step, all of the embeddings except for the ground-truth target embedding are updated with gradient in the same direction. Compounded over the training set,... | Daniel Bis, Maksim Podkorytov, Xiuwen Liu |  |
| 480 |  |  [On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies](https://doi.org/10.18653/v1/2021.naacl-main.404) |  | 0 | We study how masking and predicting tokens in an unsupervised fashion can give rise to linguistic structures and downstream performance gains. Recent theories have suggested that pretrained language models acquire useful inductive biases through masks that implicitly act as cloze reductions for downstream tasks. While appealing, we show that the success of the random masking strategy used in practice cannot be explained by such cloze-like masks alone. We construct cloze-like masks using... | Tianyi Zhang, Tatsunori Hashimoto |  |
| 481 |  |  [Limitations of Autoregressive Models and Their Alternatives](https://doi.org/10.18653/v1/2021.naacl-main.405) |  | 0 | Standard autoregressive language models perform only polynomial-time computation to compute the probability of the next symbol. While this is attractive, it means they cannot model distributions whose next-symbol probability is hard to compute. Indeed, they cannot even model them well enough to solve associated easy decision problems for which an engineer might want to consult a language model. These limitations apply no matter how much computation and data are used to train the model, unless... | ChuCheng Lin, Aaron Jaech, Xin Li, Matthew R. Gormley, Jason Eisner |  |
| 482 |  |  [On the Transformer Growth for Progressive BERT Training](https://doi.org/10.18653/v1/2021.naacl-main.406) |  | 0 | As the excessive pre-training cost arouses the need to improve efficiency, considerable efforts have been made to train BERT progressively–start from an inferior but low-cost model and gradually increase the computational complexity. Our objective is to help advance the understanding of such Transformer growth and discover principles that guide progressive training. First, we find that similar to network architecture selection, Transformer growth also favors compound scaling. Specifically,... | Xiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, Jiawei Han |  |
| 483 |  |  [Revisiting Simple Neural Probabilistic Language Models](https://doi.org/10.18653/v1/2021.naacl-main.407) |  | 0 | Recent progress in language modeling has been driven not only by advances in neural architectures, but also through hardware and optimization improvements. In this paper, we revisit the neural probabilistic language model (NPLM) of Bengio et al. (2003), which simply concatenates word embeddings within a fixed window and passes the result through a feed-forward network to predict the next word. When scaled up to modern hardware, this model (despite its many limitations) performs much better than... | Simeng Sun, Mohit Iyyer |  |
| 484 |  |  [ReadTwice: Reading Very Large Documents with Memories](https://doi.org/10.18653/v1/2021.naacl-main.408) |  | 0 | Knowledge-intensive tasks such as question answering often require assimilating information from different sections of large inputs such as books or article collections. We propose ReadTwice, a simple and effective technique that combines several strengths of prior approaches to model long-range dependencies with Transformers. The main idea is to read text in small segments, in parallel, summarizing each segment into a memory table to be used in a second read of the text. We show that the... | Yury Zemlyanskiy, Joshua Ainslie, Michiel de Jong, Philip Pham, Ilya Eckstein, Fei Sha |  |
| 485 |  |  [SCRIPT: Self-Critic PreTraining of Transformers](https://doi.org/10.18653/v1/2021.naacl-main.409) |  | 0 | We introduce Self-CRItic Pretraining Transformers (SCRIPT) for representation learning of text. The popular masked language modeling (MLM) pretraining methods like BERT replace some tokens with [MASK] and an encoder is trained to recover them, while ELECTRA trains a discriminator to detect replaced tokens proposed by a generator. In contrast, we train a language model as in MLM and further derive a discriminator or critic on top of the encoder without using any additional parameters. That is,... | Erik Nijkamp, Bo Pang, Ying Nian Wu, Caiming Xiong |  |
| 486 |  |  [Learning How to Ask: Querying LMs with Mixtures of Soft Prompts](https://doi.org/10.18653/v1/2021.naacl-main.410) |  | 0 | Natural-language prompts have recently been used to coax pretrained language models into performing other AI tasks, using a fill-in-the-blank paradigm (Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al., 2020). For example, language models retain factual knowledge from their training corpora that can be extracted by asking them to “fill in the blank” in a sentential prompt. However, where does this prompt come from? We explore the idea of learning prompts by gradient... | Guanghui Qin, Jason Eisner |  |
| 487 |  |  [Nutri-bullets Hybrid: Consensual Multi-document Summarization](https://doi.org/10.18653/v1/2021.naacl-main.411) |  | 0 | We present a method for generating comparative summaries that highlight similarities and contradictions in input documents. The key challenge in creating such summaries is the lack of large parallel training data required for training typical summarization systems. To this end, we introduce a hybrid generation approach inspired by traditional concept-to-text systems. To enable accurate comparison between different sources, the model first learns to extract pertinent relations from input... | Darsh J. Shah, Lili Yu, Tao Lei, Regina Barzilay |  |
| 488 |  |  [AVA: an Automatic eValuation Approach for Question Answering Systems](https://doi.org/10.18653/v1/2021.naacl-main.412) |  | 0 | We introduce AVA, an automatic evaluation approach for Question Answering, which given a set of questions associated with Gold Standard answers (references), can estimate system Accuracy. AVA uses Transformer-based language models to encode question, answer, and reference texts. This allows for effectively assessing answer correctness using similarity between the reference and an automatic answer, biased towards the question semantics. To design, train, and test AVA, we built multiple large... | Thuy Vu, Alessandro Moschitti |  |
| 489 |  |  [SpanPredict: Extraction of Predictive Document Spans with Neural Attention](https://doi.org/10.18653/v1/2021.naacl-main.413) |  | 0 | In many natural language processing applications, identifying predictive text can be as important as the predictions themselves. When predicting medical diagnoses, for example, identifying predictive content in clinical notes not only enhances interpretability, but also allows unknown, descriptive (i.e., text-based) risk factors to be identified. We here formalize this problem as predictive extraction and address it using a simple mechanism based on linear attention. Our method preserves... | Vivek Subramanian, Matthew Engelhard, Samuel Berchuck, Liqun Chen, Ricardo Henao, Lawrence Carin |  |
| 490 |  |  [Text Editing by Command](https://doi.org/10.18653/v1/2021.naacl-main.414) |  | 0 | A prevailing paradigm in neural text generation is one-shot generation, where text is produced in a single step. The one-shot setting is inadequate, however, when the constraints the user wishes to impose on the generated text are dynamic, especially when authoring longer documents. We address this limitation with an interactive text generation setting in which the user interacts with the system by issuing commands to edit existing text. To this end, we propose a novel text editing task, and... | Felix Faltings, Michel Galley, Gerold Hintz, Chris Brockett, Chris Quirk, Jianfeng Gao, Bill Dolan |  |
| 491 |  |  [A Deep Metric Learning Approach to Account Linking](https://doi.org/10.18653/v1/2021.naacl-main.415) |  | 0 | We consider the task of linking social media accounts that belong to the same author in an automated fashion on the basis of the content and meta-data of the corresponding document streams. We focus on learning an embedding that maps variable-sized samples of user activity–ranging from single posts to entire months of activity–to a vector space, where samples by the same author map to nearby points. Our approach does not require human-annotated data for training purposes, which allows us to... | Aleem Khan, Elizabeth Fleming, Noah Schofield, Marcus Bishop, Nicholas Andrews |  |
| 492 |  |  [Improving Factual Completeness and Consistency of Image-to-Text Radiology Report Generation](https://doi.org/10.18653/v1/2021.naacl-main.416) |  | 0 | Neural image-to-text radiology report generation systems offer the potential to improve radiology reporting by reducing the repetitive process of report drafting and identifying possible medical errors. However, existing report generation systems, despite achieving high performances on natural language generation metrics such as CIDEr or BLEU, still suffer from incomplete and inconsistent generations. Here we introduce two new simple rewards to encourage the generation of factually complete and... | Yasuhide Miura, Yuhao Zhang, Emily Bao Tsai, Curtis P. Langlotz, Dan Jurafsky |  |
| 493 |  |  [Multimodal End-to-End Sparse Model for Emotion Recognition](https://doi.org/10.18653/v1/2021.naacl-main.417) |  | 0 | Existing works in multimodal affective computing tasks, such as emotion recognition and personality recognition, generally adopt a two-phase pipeline by first extracting feature representations for each single modality with hand crafted algorithms, and then performing end-to-end learning with extracted features. However, the extracted features are fixed and cannot be further fine-tuned on different target tasks, and manually finding feature extracting algorithms does not generalize or scale... | Wenliang Dai, Samuel Cahyawijaya, Zihan Liu, Pascale Fung |  |
| 494 |  |  [MIMOQA: Multimodal Input Multimodal Output Question Answering](https://doi.org/10.18653/v1/2021.naacl-main.418) |  | 0 | Multimodal research has picked up significantly in the space of question answering with the task being extended to visual question answering, charts question answering as well as multimodal input question answering. However, all these explorations produce a unimodal textual output as the answer. In this paper, we propose a novel task - MIMOQA - Multimodal Input Multimodal Output Question Answering in which the output is also multimodal. Through human experiments, we empirically show that such... | Hrituraj Singh, Anshul Nasery, Denil Mehta, Aishwarya Agarwal, Jatin Lamba, Balaji Vasan Srinivasan |  |
| 495 |  |  [OCID-Ref: A 3D Robotic Dataset With Embodied Language For Clutter Scene Grounding](https://doi.org/10.18653/v1/2021.naacl-main.419) |  | 0 | To effectively apply robots in working environments and assist humans, it is essential to develop and evaluate how visual grounding (VG) can affect machine performance on occluded objects. However, current VG works are limited in working environments, such as offices and warehouses, where objects are usually occluded due to space utilization issues. In our work, we propose a novel OCID-Ref dataset featuring a referring expression segmentation task with referring expressions of occluded objects.... | KeJyun Wang, YunHsuan Liu, HungTing Su, JenWei Wang, YuSiang Wang, Winston H. Hsu, WenChin Chen |  |
| 496 |  |  [Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions](https://doi.org/10.18653/v1/2021.naacl-main.420) |  | 0 | Pre-trained contextual vision-and-language (V&L) models have achieved impressive performance on various benchmarks. However, existing models require a large amount of parallel image-caption data for pre-training. Such data are costly to collect and require cumbersome curation. Inspired by unsupervised machine translation, we investigate if a strong V&L representation model can be learned through unsupervised pre-training without image-caption corpora. In particular, we propose to conduct... | Liunian Harold Li, Haoxuan You, Zhecan Wang, Alireza Zareian, ShihFu Chang, KaiWei Chang |  |
| 497 |  |  [Multitasking Inhibits Semantic Drift](https://doi.org/10.18653/v1/2021.naacl-main.421) |  | 0 | When intelligent agents communicate to accomplish shared goals, how do these goals shape the agents’ language? We study the dynamics of learning in latent language policies (LLPs), in which instructor agents generate natural-language subgoal descriptions and executor agents map these descriptions to low-level actions. LLPs can solve challenging long-horizon reinforcement learning problems and provide a rich model for studying task-oriented language use. But previous work has found that LLP... | Athul Paul Jacob, Mike Lewis, Jacob Andreas |  |
| 498 |  |  [Probing Contextual Language Models for Common Ground with Visual Representations](https://doi.org/10.18653/v1/2021.naacl-main.422) |  | 0 | The success of large-scale contextual language models has attracted great interest in probing what is encoded in their representations. In this work, we consider a new question: to what extent contextual representations of concrete nouns are aligned with corresponding visual representations? We design a probing model that evaluates how effective are text-only representations in distinguishing between matching and non-matching visual representations. Our findings show that language... | Gabriel Ilharco, Rowan Zellers, Ali Farhadi, Hannaneh Hajishirzi |  |
| 499 |  |  [BBAEG: Towards BERT-based Biomedical Adversarial Example Generation for Text Classification](https://doi.org/10.18653/v1/2021.naacl-main.423) |  | 0 | Healthcare predictive analytics aids medical decision-making, diagnosis prediction and drug review analysis. Therefore, prediction accuracy is an important criteria which also necessitates robust predictive language models. However, the models using deep learning have been proven vulnerable towards insignificantly perturbed input instances which are less likely to be misclassified by humans. Recent efforts of generating adversaries using rule-based synonyms and BERT-MLMs have been witnessed in... | Ishani Mondal |  |
| 500 |  |  [Targeted Adversarial Training for Natural Language Understanding](https://doi.org/10.18653/v1/2021.naacl-main.424) |  | 0 | We present a simple yet effective Targeted Adversarial Training (TAT) algorithm to improve adversarial training for natural language understanding. The key idea is to introspect current mistakes and prioritize adversarial training steps to where the model errs the most. Experiments show that TAT can significantly improve accuracy over standard adversarial training on GLUE and attain new state-of-the-art zero-shot results on XNLI. Our code will be released upon acceptance of the paper. | Lis Pereira, Xiaodong Liu, Hao Cheng, Hoifung Poon, Jianfeng Gao, Ichiro Kobayashi |  |
| 501 |  |  [Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection](https://doi.org/10.18653/v1/2021.naacl-main.425) |  | 0 | The existence of multiple datasets for sarcasm detection prompts us to apply transfer learning to exploit their commonality. The adversarial neural transfer (ANT) framework utilizes multiple loss terms that encourage the source-domain and the target-domain feature distributions to be similar while optimizing for domain-specific performance. However, these objectives may be in conflict, which can lead to optimization difficulties and sometimes diminished transfer. We propose a generalized latent... | Xu Guo, Boyang Li, Han Yu, Chunyan Miao |  |
| 502 |  |  [Self-training Improves Pre-training for Natural Language Understanding](https://doi.org/10.18653/v1/2021.naacl-main.426) |  | 0 | Unsupervised pre-training has led to much recent progress in natural language understanding. In this paper, we study self-training as another way to leverage unlabeled data through semi-supervised learning. To obtain additional data for a specific task, we introduce SentAugment, a data augmentation method which computes task-specific query embeddings from labeled data to retrieve sentences from a bank of billions of unlabeled sentences crawled from the web. Unlike previous semi-supervised... | Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav Chaudhary, Onur Celebi, Michael Auli, Veselin Stoyanov, Alexis Conneau |  |
| 503 |  |  [Supporting Clustering with Contrastive Learning](https://doi.org/10.18653/v1/2021.naacl-main.427) |  | 0 | Unsupervised clustering aims at discovering the semantic categories of data according to some distance measured in the representation space. However, different categories often overlap with each other in the representation space at the beginning of the learning process, which poses a significant challenge for distance-based clustering in achieving good separation between different categories. To this end, we propose Supporting Clustering with Contrastive Learning (SCCL) – a novel framework to... | Dejiao Zhang, Feng Nan, Xiaokai Wei, ShangWen Li, Henghui Zhu, Kathleen R. McKeown, Ramesh Nallapati, Andrew O. Arnold, Bing Xiang |  |
| 504 |  |  [TITA: A Two-stage Interaction and Topic-Aware Text Matching Model](https://doi.org/10.18653/v1/2021.naacl-main.428) |  | 0 | In this paper, we focus on the problem of keyword and document matching by considering different relevance levels. In our recommendation system, different people follow different hot keywords with interest. We need to attach documents to each keyword and then distribute the documents to people who follow these keywords. The ideal documents should have the same topic with the keyword, which we call topic-aware relevance. In other words, topic-aware relevance documents are better than... | Xingwu Sun, Yanling Cui, Hongyin Tang, Qiuyu Zhu, Fuzheng Zhang, Beihong Jin |  |
| 505 |  |  [Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction](https://doi.org/10.18653/v1/2021.naacl-main.429) |  | 0 | Grammatical Error Correction (GEC) aims to correct writing errors and help language learners improve their writing skills. However, existing GEC models tend to produce spurious corrections or fail to detect lots of errors. The quality estimation model is necessary to ensure learners get accurate GEC results and avoid misleading from poorly corrected sentences. Well-trained GEC models can generate several high-quality hypotheses through decoding, such as beam search, which provide valuable GEC... | Zhenghao Liu, Xiaoyuan Yi, Maosong Sun, Liner Yang, TatSeng Chua |  |
| 506 |  |  [Neural Network Surgery: Injecting Data Patterns into Pre-trained Models with Minimal Instance-wise Side Effects](https://doi.org/10.18653/v1/2021.naacl-main.430) |  | 0 | Side effects during neural network tuning are typically measured by overall accuracy changes. However, we find that even with similar overall accuracy, existing tuning methods result in non-negligible instance-wise side effects. Motivated by neuroscientific evidence and theoretical results, we demonstrate that side effects can be controlled by the number of changed parameters and thus, we propose to conduct neural network surgery by only modifying a limited number of parameters. Neural network... | Zhiyuan Zhang, Xuancheng Ren, Qi Su, Xu Sun, Bin He |  |
| 507 |  |  [Discrete Argument Representation Learning for Interactive Argument Pair Identification](https://doi.org/10.18653/v1/2021.naacl-main.431) |  | 0 | In this paper, we focus on identifying interactive argument pairs from two posts with opposite stances to a certain topic. Considering opinions are exchanged from different perspectives of the discussing topic, we study the discrete representations for arguments to capture varying aspects in argumentation languages (e.g., the debate focus and the participant behavior). Moreover, we utilize hierarchical structure to model post-wise information incorporating contextual knowledge. Experimental... | Lu Ji, Zhongyu Wei, Jing Li, Qi Zhang, Xuanjing Huang |  |
| 508 |  |  [On Unifying Misinformation Detection](https://doi.org/10.18653/v1/2021.naacl-main.432) |  | 0 | In this paper, we introduce UnifiedM2, a general-purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news, and verifying rumors. By grouping these tasks together, UnifiedM2 learns a richer representation of misinformation, which leads to state-of-the-art or comparable performance across all tasks. Furthermore, we demonstrate that UnifiedM2’s learned... | Nayeon Lee, Belinda Z. Li, Sinong Wang, Pascale Fung, Hao Ma, Wentau Yih, Madian Khabsa |  |
| 509 |  |  [Frustratingly Easy Edit-based Linguistic Steganography with a Masked Language Model](https://doi.org/10.18653/v1/2021.naacl-main.433) |  | 0 | With advances in neural language models, the focus of linguistic steganography has shifted from edit-based approaches to generation-based ones. While the latter’s payload capacity is impressive, generating genuine-looking texts remains challenging. In this paper, we revisit edit-based linguistic steganography, with the idea that a masked language model offers an off-the-shelf solution. The proposed method eliminates painstaking rule construction and has a high payload capacity for an edit-based... | Honai Ueoka, Yugo Murawaki, Sadao Kurohashi |  |
| 510 |  |  [Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning](https://doi.org/10.18653/v1/2021.naacl-main.434) |  | 0 | Few-shot text classification is a fundamental NLP task in which a model aims to classify text into a large number of categories, given only a few training examples per category. This paper explores data augmentation—a technique particularly suitable for training with limited data—for this few-shot, highly-multiclass text classification setting. On four diverse text classification tasks, we find that common data augmentation techniques can improve the performance of triplet networks by up to... | Jason Wei, Chengyu Huang, Soroush Vosoughi, Yu Cheng, Shiqi Xu |  |
| 511 |  |  [Do RNN States Encode Abstract Phonological Alternations?](https://doi.org/10.18653/v1/2021.naacl-main.435) |  | 0 | Sequence-to-sequence models have delivered impressive results in word formation tasks such as morphological inflection, often learning to model subtle morphophonological details with limited training data. Despite the performance, the opacity of neural models makes it difficult to determine whether complex generalizations are learned, or whether a kind of separate rote memorization of each morphophonological process takes place. To investigate whether complex alternations are simply memorized... | Miikka Silfverberg, Francis M. Tyers, Garrett Nicolai, Mans Hulden |  |
| 512 |  |  [Pre-training with Meta Learning for Chinese Word Segmentation](https://doi.org/10.18653/v1/2021.naacl-main.436) |  | 0 | Recent researches show that pre-trained models (PTMs) are beneficial to Chinese Word Segmentation (CWS). However, PTMs used in previous works usually adopt language modeling as pre-training tasks, lacking task-specific prior segmentation knowledge and ignoring the discrepancy between pre-training tasks and downstream CWS tasks. In this paper, we propose a CWS-specific pre-trained model MetaSeg, which employs a unified architecture and incorporates meta learning algorithm into a multi-criteria... | Zhen Ke, Liang Shi, Songtao Sun, Erli Meng, Bin Wang, Xipeng Qiu |  |
| 513 |  |  [Decompose, Fuse and Generate: A Formation-Informed Method for Chinese Definition Generation](https://doi.org/10.18653/v1/2021.naacl-main.437) |  | 0 | In this paper, we tackle the task of Definition Generation (DG) in Chinese, which aims at automatically generating a definition for a word. Most existing methods take the source word as an indecomposable semantic unit. However, in parataxis languages like Chinese, word meanings can be composed using the word formation process, where a word (“桃花”, peach-blossom) is formed by formation components (“桃”, peach; “花”, flower) using a formation rule (Modifier-Head). Inspired by this process, we... | Hua Zheng, Damai Dai, Lei Li, Tianyu Liu, Zhifang Sui, Baobao Chang, Yang Liu |  |
| 514 |  |  [User-Generated Text Corpus for Evaluating Japanese Morphological Analysis and Lexical Normalization](https://doi.org/10.18653/v1/2021.naacl-main.438) |  | 0 | Morphological analysis (MA) and lexical normalization (LN) are both important tasks for Japanese user-generated text (UGT). To evaluate and compare different MA/LN systems, we have constructed a publicly available Japanese UGT corpus. Our corpus comprises 929 sentences annotated with morphological and normalization information, along with category information we classified for frequent UGT-specific phenomena. Experiments on the corpus demonstrated the low performance of existing MA/LN methods... | Shohei Higashiyama, Masao Utiyama, Taro Watanabe, Eiichiro Sumita |  |
| 515 |  |  [GPT Perdetry Test: Generating new meanings for new words](https://doi.org/10.18653/v1/2021.naacl-main.439) |  | 0 | Human innovation in language, such as inventing new words, is a challenge for pretrained language models. We assess the ability of one large model, GPT-3, to process new words and decide on their meaning. We create a set of nonce words and prompt GPT-3 to generate their dictionary definitions. We find GPT-3 produces plausible definitions that align with human judgments. Moreover, GPT-3’s definitions are sometimes preferred to those invented by humans, signaling its intriguing ability not just... | Nikolay Malkin, Sameera Lanka, Pranav Goel, Sudha Rao, Nebojsa Jojic |  |
| 516 |  |  [Universal Semantic Tagging for English and Mandarin Chinese](https://doi.org/10.18653/v1/2021.naacl-main.440) |  | 0 | Universal Semantic Tagging aims to provide lightweight unified analysis for all languages at the word level. Though the proposed annotation scheme is conceptually promising, the feasibility is only examined in four Indo–European languages. This paper is concerned with extending the annotation scheme to handle Mandarin Chinese and empirically study the plausibility of unifying meaning representations for multiple languages. We discuss a set of language-specific semantic phenomena, propose new... | Wenxi Li, Yiyang Hou, Yajie Ye, Li Liang, Weiwei Sun |  |
| 517 |  |  [ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser](https://doi.org/10.18653/v1/2021.naacl-main.441) |  | 0 | Given a database schema, Text-to-SQL aims to translate a natural language question into the corresponding SQL query. Under the setup of cross-domain, traditional semantic parsing models struggle to adapt to unseen database schemas. To improve the model generalization capability for rare and unseen schemas, we propose a new architecture, ShadowGNN, which processes schemas at abstract and semantic levels. By ignoring names of semantic items in databases, abstract schemas are exploited in a... | Zhi Chen, Lu Chen, Yanbin Zhao, Ruisheng Cao, Zihan Xu, Su Zhu, Kai Yu |  |
| 518 |  |  [Contextualized and Generalized Sentence Representations by Contrastive Self-Supervised Learning: A Case Study on Discourse Relation Analysis](https://doi.org/10.18653/v1/2021.naacl-main.442) |  | 0 | We propose a method to learn contextualized and generalized sentence representations using contrastive self-supervised learning. In the proposed method, a model is given a text consisting of multiple sentences. One sentence is randomly selected as a target sentence. The model is trained to maximize the similarity between the representation of the target sentence with its context and that of the masked target sentence with the same context. Simultaneously, the model minimizes the similarity... | Hirokazu Kiyomaru, Sadao Kurohashi |  |
| 519 |  |  [AMR Parsing with Action-Pointer Transformer](https://doi.org/10.18653/v1/2021.naacl-main.443) |  | 0 | Abstract Meaning Representation parsing is a sentence-to-graph prediction task where target nodes are not explicitly aligned to sentence tokens. However, since graph nodes are semantically based on one or more sentence tokens, implicit alignments can be derived. Transition-based parsers operate over the sentence from left to right, capturing this inductive bias via alignments at the cost of limited expressiveness. In this work, we propose a transition-based system that combines hard-attention... | Jiawei Zhou, Tahira Naseem, Ramón Fernandez Astudillo, Radu Florian |  |
| 520 |  |  [NL-EDIT: Correcting Semantic Parse Errors through Natural Language Interaction](https://doi.org/10.18653/v1/2021.naacl-main.444) |  | 0 | We study semantic parsing in an interactive setting in which users correct errors with natural language feedback. We present NL-EDIT, a model for interpreting natural language feedback in the interaction context to generate a sequence of edits that can be applied to the initial parse to correct its errors. We show that NL-EDIT can boost the accuracy of existing text-to-SQL parsers by up to 20% with only one turn of correction. We analyze the limitations of the model and discuss directions for... | Ahmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney, Gonzalo A. Ramos, Ahmed Hassan Awadallah |  |
| 521 |  |  [Unsupervised Concept Representation Learning for Length-Varying Text Similarity](https://doi.org/10.18653/v1/2021.naacl-main.445) |  | 0 | Measuring document similarity plays an important role in natural language processing tasks. Most existing document similarity approaches suffer from the information gap caused by context and vocabulary mismatches when comparing varying-length texts. In this paper, we propose an unsupervised concept representation learning approach to address the above issues. Specifically, we propose a novel Concept Generation Network (CGNet) to learn concept representations from the perspective of the entire... | Xuchao Zhang, Bo Zong, Wei Cheng, Jingchao Ni, Yanchi Liu, Haifeng Chen |  |
| 522 |  |  [Augmenting Knowledge-grounded Conversations with Sequential Knowledge Transition](https://doi.org/10.18653/v1/2021.naacl-main.446) |  | 0 | Knowledge data are massive and widespread in the real-world, which can serve as good external sources to enrich conversations. However, in knowledge-grounded conversations, current models still lack the fine-grained control over knowledge selection and integration with dialogues, which finally leads to the knowledge-irrelevant response generation problems: 1) knowledge selection merely relies on the dialogue context, ignoring the inherent knowledge transitions along with conversation flows; 2)... | Haolan Zhan, Hainan Zhang, Hongshen Chen, Zhuoye Ding, Yongjun Bao, Yanyan Lan |  |
| 523 |  |  [Adversarial Self-Supervised Learning for Out-of-Domain Detection](https://doi.org/10.18653/v1/2021.naacl-main.447) |  | 0 | Detecting out-of-domain (OOD) intents is crucial for the deployed task-oriented dialogue system. Previous unsupervised OOD detection methods only extract discriminative features of different in-domain intents while supervised counterparts can directly distinguish OOD and in-domain intents but require extensive labeled OOD data. To combine the benefits of both types, we propose a self-supervised contrastive learning framework to model discriminative semantic features of both in-domain intents... | Zhiyuan Zeng, Keqing He, Yuanmeng Yan, Hong Xu, Weiran Xu |  |
| 524 |  |  [Leveraging Slot Descriptions for Zero-Shot Cross-Domain Dialogue StateTracking](https://doi.org/10.18653/v1/2021.naacl-main.448) |  | 0 | Zero-shot cross-domain dialogue state tracking (DST) enables us to handle unseen domains without the expense of collecting in-domain data. In this paper, we propose a slot descriptions enhanced generative approach for zero-shot cross-domain DST. Specifically, our model first encodes a dialogue context and a slot with a pre-trained self-attentive encoder, and generates slot value in auto-regressive manner. In addition, we incorporate Slot Type Informed Descriptions that capture the shared... | Zhaojiang Lin, Bing Liu, Seungwhan Moon, Paul A. Crook, Zhenpeng Zhou, Zhiguang Wang, Zhou Yu, Andrea Madotto, Eunjoon Cho, Rajen Subba |  |
| 525 |  |  [Hierarchical Transformer for Task Oriented Dialog Systems](https://doi.org/10.18653/v1/2021.naacl-main.449) |  | 0 | Generative models for dialog systems have gained much interest because of the recent success of RNN and Transformer based models in tasks like question answering and summarization. Although the task of dialog response generation is generally seen as a sequence to sequence (Seq2Seq) problem, researchers in the past have found it challenging to train dialog systems using the standard Seq2Seq models. Therefore, to help the model learn meaningful utterance and conversation level features, Sordoni... | Bishal Santra, Potnuru Anusha, Pawan Goyal |  |
| 526 |  |  [Measuring the 'I don't know' Problem through the Lens of Gricean Quantity](https://doi.org/10.18653/v1/2021.naacl-main.450) |  | 0 | We consider the intrinsic evaluation of neural generative dialog models through the lens of Grice’s Maxims of Conversation (1975). Based on the maxim of Quantity (be informative), we propose Relative Utterance Quantity (RUQ) to diagnose the ‘I don’t know’ problem, in which a dialog system produces generic responses. The linguistically motivated RUQ diagnostic compares the model score of a generic response to that of the reference response. We find that for reasonable baseline models, ‘I don’t... | Huda Khayrallah, João Sedoc |  |
| 527 |  |  [RTFE: A Recursive Temporal Fact Embedding Framework for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2021.naacl-main.451) |  | 0 | Static knowledge graph (SKG) embedding (SKGE) has been studied intensively in the past years. Recently, temporal knowledge graph (TKG) embedding (TKGE) has emerged. In this paper, we propose a Recursive Temporal Fact Embedding (RTFE) framework to transplant SKGE models to TKGs and to enhance the performance of existing TKGE models for TKG completion. Different from previous work which ignores the continuity of states of TKG in time evolution, we treat the sequence of graphs as a Markov chain,... | Youri Xu, Haihong E, Meina Song, Wenyu Song, Xiaodong Lv, Haotian Wang, Jinrui Yang |  |
| 528 |  |  [Open Hierarchical Relation Extraction](https://doi.org/10.18653/v1/2021.naacl-main.452) |  | 0 | Open relation extraction (OpenRE) aims to extract novel relation types from open-domain corpora, which plays an important role in completing the relation schemes of knowledge bases (KBs). Most OpenRE methods cast different relation types in isolation without considering their hierarchical dependency. We argue that OpenRE is inherently in close connection with relation hierarchies. To establish the bidirectional connections between OpenRE and relation hierarchy, we propose the task of open... | Kai Zhang, Yuan Yao, Ruobing Xie, Xu Han, Zhiyuan Liu, Fen Lin, Leyu Lin, Maosong Sun |  |
| 529 |  |  [Jointly Extracting Explicit and Implicit Relational Triples with Reasoning Pattern Enhanced Binary Pointer Network](https://doi.org/10.18653/v1/2021.naacl-main.453) |  | 0 | Relational triple extraction is a crucial task for knowledge graph construction. Existing methods mainly focused on explicit relational triples that are directly expressed, but usually suffer from ignoring implicit triples that lack explicit expressions. This will lead to serious incompleteness of the constructed knowledge graphs. Fortunately, other triples in the sentence provide supplementary information for discovering entity pairs that may have implicit relations. Also, the relation types... | Yubo Chen, Yunqi Zhang, Changran Hu, Yongfeng Huang |  |
| 530 |  |  [Multi-Grained Knowledge Distillation for Named Entity Recognition](https://doi.org/10.18653/v1/2021.naacl-main.454) |  | 0 | Although pre-trained big models (e.g., BERT, ERNIE, XLNet, GPT3 etc.) have delivered top performance in Seq2seq modeling, their deployments in real-world applications are often hindered by the excessive computations and memory demand involved. For many applications, including named entity recognition (NER), matching the state-of-the-art result under budget has attracted considerable attention. Drawing power from the recent advance in knowledge distillation (KD), this work presents a novel... | Xuan Zhou, Xiao Zhang, Chenyang Tao, Junya Chen, Bing Xu, Wei Wang, Jing Xiao |  |
| 531 |  |  [SGG: Learning to Select, Guide, and Generate for Keyphrase Generation](https://doi.org/10.18653/v1/2021.naacl-main.455) |  | 0 | Keyphrases, that concisely summarize the high-level topics discussed in a document, can be categorized into present keyphrase which explicitly appears in the source text and absent keyphrase which does not match any contiguous subsequence but is highly semantically related to the source. Most existing keyphrase generation approaches synchronously generate present and absent keyphrases without explicitly distinguishing these two categories. In this paper, a Select-Guide-Generate (SGG) approach... | Jing Zhao, Junwei Bao, Yifan Wang, Youzheng Wu, Xiaodong He, Bowen Zhou |  |
| 532 |  |  [Towards Sentiment and Emotion aided Multi-modal Speech Act Classification in Twitter](https://doi.org/10.18653/v1/2021.naacl-main.456) |  | 0 | Speech Act Classification determining the communicative intent of an utterance has been investigated widely over the years as a standalone task. This holds true for discussion in any fora including social media platform such as Twitter. But the emotional state of the tweeter which has a considerable effect on the communication has not received the attention it deserves. Closely related to emotion is sentiment, and understanding of one helps understand the other. In this work, we firstly create... | Tulika Saha, Apoorva Upadhyaya, Sriparna Saha, Pushpak Bhattacharyya |  |
| 533 |  |  [Generative Imagination Elevates Machine Translation](https://doi.org/10.18653/v1/2021.naacl-main.457) |  | 0 | There are common semantics shared across text and images. Given a sentence in a source language, whether depicting the visual scene helps translation into a target language? Existing multimodal neural machine translation methods (MNMT) require triplets of bilingual sentence - image for training and tuples of source sentence - image for inference. In this paper, we propose ImagiT, a novel machine translation method via visual imagination. ImagiT first learns to generate visual representation... | Quanyu Long, Mingxuan Wang, Lei Li |  |
| 534 |  |  [Non-Autoregressive Translation by Learning Target Categorical Codes](https://doi.org/10.18653/v1/2021.naacl-main.458) |  | 0 | Non-autoregressive Transformer is a promising text generation model. However, current non-autoregressive models still fall behind their autoregressive counterparts in translation quality. We attribute this accuracy gap to the lack of dependency modeling among decoder inputs. In this paper, we propose CNAT, which learns implicitly categorical codes as latent variables into the non-autoregressive decoding. The interaction among these categorical codes remedies the missing dependencies and... | Yu Bao, Shujian Huang, Tong Xiao, Dongqi Wang, Xinyu Dai, Jiajun Chen |  |
| 535 |  |  [Training Data Augmentation for Code-Mixed Translation](https://doi.org/10.18653/v1/2021.naacl-main.459) |  | 0 | Machine translation of user-generated code-mixed inputs to English is of crucial importance in applications like web search and targeted advertising. We address the scarcity of parallel training data for training such models by designing a strategy of converting existing non-code-mixed parallel data sources to code-mixed parallel data. We present an m-BERT based procedure whose core learnable component is a ternary sequence labeling model, that can be trained with a limited code-mixed corpus... | Abhirut Gupta, Aditya Vavre, Sunita Sarawagi |  |
| 536 |  |  [Rethinking Perturbations in Encoder-Decoders for Fast Training](https://doi.org/10.18653/v1/2021.naacl-main.460) |  | 0 | We often use perturbations to regularize neural models. For neural encoder-decoders, previous studies applied the scheduled sampling (Bengio et al., 2015) and adversarial perturbations (Sato et al., 2019) as perturbations but these methods require considerable computational time. Thus, this study addresses the question of whether these approaches are efficient enough for training time. We compare several perturbations in sequence-to-sequence problems with respect to computational time.... | Sho Takase, Shun Kiyono |  |
| 537 |  |  [Context-aware Decoder for Neural Machine Translation using a Target-side Document-Level Language Model](https://doi.org/10.18653/v1/2021.naacl-main.461) |  | 0 | Although many end-to-end context-aware neural machine translation models have been proposed to incorporate inter-sentential contexts in translation, these models can be trained only in domains where parallel documents with sentential alignments exist. We therefore present a simple method to perform context-aware decoding with any pre-trained sentence-level translation model by using a document-level language model. Our context-aware decoder is built upon sentence-level parallel data and... | Amane Sugiyama, Naoki Yoshinaga |  |
| 538 |  |  [Machine Translated Text Detection Through Text Similarity with Round-Trip Translation](https://doi.org/10.18653/v1/2021.naacl-main.462) |  | 0 | Translated texts have been used for malicious purposes, i.e., plagiarism or fake reviews. Existing detectors have been built around a specific translator (e.g., Google) but fail to detect a translated text from a strange translator. If we use the same translator, the translated text is similar to its round-trip translation, which is when text is translated into another language and translated back into the original language. However, a round-trip translated text is significantly different from... | HoangQuoc NguyenSon, Tran Thao Phuong, Seira Hidano, Ishita Gupta, Shinsaku Kiyomoto |  |
| 539 |  |  [TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference](https://doi.org/10.18653/v1/2021.naacl-main.463) |  | 0 | Existing pre-trained language models (PLMs) are often computationally expensive in inference, making them impractical in various resource-limited real-world applications. To address this issue, we propose a dynamic token reduction approach to accelerate PLMs’ inference, named TR-BERT, which could flexibly adapt the layer number of each token in inference to avoid redundant calculation. Specially, TR-BERT formulates the token reduction process as a multi-step token selection problem and... | Deming Ye, Yankai Lin, Yufei Huang, Maosong Sun |  |
| 540 |  |  [Breadth First Reasoning Graph for Multi-hop Question Answering](https://doi.org/10.18653/v1/2021.naacl-main.464) |  | 0 | Recently Graph Neural Network (GNN) has been used as a promising tool in multi-hop question answering task. However, the unnecessary updations and simple edge constructions prevent an accurate answer span extraction in a more direct and interpretable way. In this paper, we propose a novel model of Breadth First Reasoning Graph (BFR-Graph), which presents a new message passing way that better conforms to the reasoning process. In BFR-Graph, the reasoning message is required to start from the... | Yongjie Huang, Meng Yang |  |
| 541 |  |  [Improving Zero-Shot Cross-lingual Transfer for Multilingual Question Answering over Knowledge Graph](https://doi.org/10.18653/v1/2021.naacl-main.465) |  | 0 | Multilingual question answering over knowledge graph (KGQA) aims to derive answers from a knowledge graph (KG) for questions in multiple languages. To be widely applicable, we focus on its zero-shot transfer setting. That is, we can only access training data in a high-resource language, while need to answer multilingual questions without any labeled data in target languages. A straightforward approach is resorting to pre-trained multilingual models (e.g., mBERT) for cross-lingual transfer, but... | Yucheng Zhou, Xiubo Geng, Tao Shen, Wenqiang Zhang, Daxin Jiang |  |
| 542 |  |  [RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.naacl-main.466) |  | 0 | In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these... | Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, Haifeng Wang |  |
| 543 |  |  [DAGN: Discourse-Aware Graph Network for Logical Reasoning](https://doi.org/10.18653/v1/2021.naacl-main.467) |  | 0 | Recent QA with logical reasoning questions requires passage-level relations among the sentences. However, current approaches still focus on sentence-level relations interacting among tokens. In this work, we explore aggregating passage-level clues for solving logical reasoning QA by using discourse-based information. We propose a discourse-aware graph network (DAGN) that reasons relying on the discourse structure of the texts. The model encodes discourse information as a graph with elementary... | Yinya Huang, Meng Fang, Yu Cao, Liwei Wang, Xiaodan Liang |  |
| 544 |  |  [Designing a Minimal Retrieve-and-Read System for Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.naacl-main.468) |  | 0 | In open-domain question answering (QA), retrieve-and-read mechanism has the inherent benefit of interpretability and the easiness of adding, removing, or editing knowledge compared to the parametric approaches of closed-book QA models. However, it is also known to suffer from its large storage footprint due to its document corpus and index. Here, we discuss several orthogonal strategies to drastically reduce the footprint of a retrieve-and-read open-domain QA system by up to 160x. Our results... | Sohee Yang, Minjoon Seo |  |
| 545 |  |  [Unsupervised Multi-hop Question Answering by Question Generation](https://doi.org/10.18653/v1/2021.naacl-main.469) |  | 0 | Obtaining training data for multi-hop question answering (QA) is time-consuming and resource-intensive. We explore the possibility to train a well-performed multi-hop QA model without referencing any human-labeled multi-hop question-answer pairs, i.e., unsupervised multi-hop QA. We propose MQA-QG, an unsupervised framework that can generate human-like multi-hop training data from both homogeneous and heterogeneous data sources. MQA-QG generates questions by first selecting/generating relevant... | Liangming Pan, Wenhu Chen, Wenhan Xiong, MinYen Kan, William Yang Wang |  |
| 546 |  |  [Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long Documents](https://doi.org/10.18653/v1/2021.naacl-main.470) |  | 0 | Neural-based summarization models suffer from the length limitation of text encoder. Long documents have to been truncated before they are sent to the model, which results in huge loss of summary-relevant contents. To address this issue, we propose the sliding selector network with dynamic memory for extractive summarization of long-form documents, which employs a sliding window to extract summary sentences segment by segment. Moreover, we adopt memory mechanism to preserve and update the... | Peng Cui, Le Hu |  |
| 547 |  |  [AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive Summarization](https://doi.org/10.18653/v1/2021.naacl-main.471) |  | 0 | State-of-the-art abstractive summarization models generally rely on extensive labeled data, which lowers their generalization ability on domains where such data are not available. In this paper, we present a study of domain adaptation for the abstractive summarization task across six diverse target domains in a low-resource setting. Specifically, we investigate the second phase of pre-training on large-scale generative models under three different settings: 1) source domain pre-training; 2)... | Tiezheng Yu, Zihan Liu, Pascale Fung |  |
| 548 |  |  [QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization](https://doi.org/10.18653/v1/2021.naacl-main.472) |  | 0 | Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new... | Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, Dragomir R. Radev |  |
| 549 |  |  [MM-AVS: A Full-Scale Dataset for Multi-modal Summarization](https://doi.org/10.18653/v1/2021.naacl-main.473) |  | 0 | Multimodal summarization becomes increasingly significant as it is the basis for question answering, Web search, and many other downstream tasks. However, its learning materials have been lacking a holistic organization by integrating resources from various modalities, thereby lagging behind the research progress of this field. In this study, we release a full-scale multimodal dataset comprehensively gathering documents, summaries, images, captions, videos, audios, transcripts, and titles in... | Xiyan Fu, Jun Wang, Zhenglu Yang |  |
| 550 |  |  [MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization](https://doi.org/10.18653/v1/2021.naacl-main.474) |  | 0 | This paper introduces MediaSum, a large-scale media interview dataset consisting of 463.6K transcripts with abstractive summaries. To create this dataset, we collect interview transcripts from NPR and CNN and employ the overview and topic descriptions as summaries. Compared with existing public corpora for dialogue summarization, our dataset is an order of magnitude larger and contains complex multi-party conversations from multiple domains. We conduct statistical analysis to demonstrate the... | Chenguang Zhu, Yang Liu, Jie Mei, Michael Zeng |  |
| 551 |  |  [Improving Faithfulness in Abstractive Summarization with Contrast Candidate Generation and Selection](https://doi.org/10.18653/v1/2021.naacl-main.475) |  | 0 | Despite significant progress in neural abstractive summarization, recent studies have shown that the current models are prone to generating summaries that are unfaithful to the original context. To address the issue, we study contrast candidate generation and selection as a model-agnostic post-processing technique to correct the extrinsic hallucinations (i.e. information not present in the source text) in unfaithful summaries. We learn a discriminative correction model by generating alternative... | Sihao Chen, Fan Zhang, Kazoo Sone, Dan Roth |  |
| 552 |  |  [Inference Time Style Control for Summarization](https://doi.org/10.18653/v1/2021.naacl-main.476) |  | 0 | How to generate summaries of different styles without requiring corpora in the target styles, or training separate models? We present two novel methods that can be deployed during summary decoding on any pre-trained Transformer-based summarization model. (1) Decoder state adjustment instantly modifies decoder final states with externally trained style scorers, to iteratively refine the output against a target style. (2) Word unit prediction constrains the word usage to impose strong lexical... | Shuyang Cao, Lu Wang |  |
