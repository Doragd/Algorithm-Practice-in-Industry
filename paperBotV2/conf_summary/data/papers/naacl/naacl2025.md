# NAACL2025

## 会议论文列表

本会议共有 718 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Complete Chess Games Enable LLM Become A Chess Master](https://doi.org/10.18653/v1/2025.naacl-short.1) |  | 0 | Large language models (LLM) have shown remarkable abilities in text generation, question answering, language translation, reasoning and many other tasks. It continues to advance rapidly and is becoming increasingly influential in various fields, from technology and business to education and entertainment. Despite LLM’s success in multiple areas, its ability to play abstract games, such as chess, is underexplored. Chess-playing requires the language models to output legal and reasonable moves from textual inputs. Here, we propose the Large language model ChessLLM to play full chess games. We transform the game into a textual format with the best move represented in the Forsyth-Edwards Notation. We show that by simply supervised fine-tuning, our model has achieved a professional-level Elo rating of 1788 in matches against the standard Elo-rated Stockfish when permitted to sample 10 times. We further show that data quality is important. Long-round data supervision enjoys a 350 Elo rating improvement over short-round data. | Yinqi Zhang, Xintian Han, Haolong Li, Kedi Chen, Shaohui Lin |  |
| 2 |  |  [Predicting the Target Word of Game-playing Conversations using a Low-Rank Dialect Adapter for Decoder Models](https://doi.org/10.18653/v1/2025.naacl-short.2) |  | 0 | Dialect adapters that improve the performance of LLMs for NLU tasks on certain sociolects/dialects/national varieties (‘dialects’ for the sake of brevity) have been reported for encoder models. In this paper, we extend the idea of dialect adapters to decoder models in our architecture called LoRDD. Using MD-3, a publicly available dataset of word game-playing conversations between dialectal speakers, our task is Target Word Prediction (TWP) from a masked conversation. LoRDD combines task adapters and dialect adapters where the latter employ contrastive learning on pseudo-parallel conversations from MD-3. Our experiments on Indian English and Nigerian English conversations with two models (Mistral and Gemma) demonstrate that LoRDD outperforms four baselines on TWP. Additionally, it significantly reduces the performance gap with American English, narrowing it to 12% and 5.8% for word similarity, and 25% and 4.5% for accuracy, respectively. The focused contribution of LoRDD is in its promise for dialect adaptation of decoder models using TWP, a simplified version of the commonly used next-word prediction task. | Dipankar Srirag, Aditya Joshi, Jacob Eisenstein |  |
| 3 |  |  [ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with LLM-based Chatbots](https://doi.org/10.18653/v1/2025.naacl-short.3) |  | 0 | The rise of LLMs has deflected a growing portion of human-computer interactions towards LLM-based chatbots.The remarkable abilities of these models allow users to interact using long, diverse natural language text covering a wide range of topics and styles. Phrasing these messages is a time and effort consuming task, calling for an autocomplete solution to assist users. We present \*\*ChaI-TeA\*\*: \*\*Cha\*\*t \*\*I\*\*n\*\*te\*\*raction \*\*A\*\*utocomplete; An autocomplete evaluation framework for LLM-based chatbot interactions. The framework includes a formal definition of the task, curated datasets and suitable metrics. We use it to evaluate 11 models on this task, finding that while current off-the-shelf models perform fairly, there is still much room for improvement, mainly in ranking of the generated suggestions. We provide insights for practitioners working on this task and open new research directions for researchers in the field. We release our framework to serve as a foundation for future research. | Shani Goren, Oren Kalinsky, Tomer Stav, Yuri Rapoport, Yaron Fairstein, Ram Yazdi, Nachshon Cohen, Alexander Libov, Guy Kushilevitz |  |
| 4 |  |  [Cross-Lingual Transfer Learning for Speech Translation](https://doi.org/10.18653/v1/2025.naacl-short.4) |  | 0 | There has been increasing interest in building multilingual foundation models for NLP and speech research. This paper examines how to expand the speech translation capability of these models with restricted data. Whisper, a speech foundation model with strong performance on speech recognition and English translation, is used as the example model. Using speech-to-speech retrieval to analyse the audio representations generated by the encoder, we show that utterances from different languages are mapped to a shared semantic space. This shared embedding space can then be leveraged for zero-shot cross-lingual transfer in speech translation. By fine-tuning the Whisper decoder with only English-to-Chinese speech translation data, improved performance for translation to Chinese can be obtained for multiple languages, in addition to English. Furthermore, for languages related to those seen in training it is possible to perform speech translation, despite the model never seeing the language in training, or being able to perform transcription. | Rao Ma, Mengjie Qian, Yassir Fathullah, Siyuan Tang, Mark J. F. Gales, Kate M. Knill |  |
| 5 |  |  [Reverse Question Answering: Can an LLM Write a Question so Hard (or Bad) that it Can't Answer?](https://doi.org/10.18653/v1/2025.naacl-short.5) |  | 0 | Question answering (QA)—giving correct answers to questions—is a popular task, but we test \*\*reverse question answering (RQA)\*\*: for an input answer, give a question with that answer. Past work tests QA and RQA separately, but we test them jointly, comparing their difficulty, aiding benchmark design, and checking reasoning consistency. We run 16 LLMs on QA and RQA with trivia questions/answers, revealing: 1) Versus RQA, LLMs are much less accurate in RQA for numerical answers, but slightly more accurate in RQA for textual answers; 2) LLMs often answer their own invalid questions from RQA accurately in QA, so RQA errors are not just from knowledge gaps; 3) RQA errors correlate with question difficulty and inversely correlate with answer frequencies in the Dolma corpus; and 4) LLMs struggle to give valid multi-hop questions. By finding question and answer types that lead to RQA errors, we suggest improvements for LLM reasoning. | Nishant Balepur, Feng Gu, Abhilasha Ravichander, Shi Feng, Jordan Lee BoydGraber, Rachel Rudinger |  |
| 6 |  |  [Personalized Help for Optimizing Low-Skilled Users' Strategy](https://doi.org/10.18653/v1/2025.naacl-short.6) |  | 0 | AIs can beat humans in game environments; however, how helpful those agents are to human remains understudied. We augment Cicero, a natural language agent that demonstrates superhuman performance in Diplomacy, to generate both move and message advice based on player intentions. A dozen Diplomacy games with novice and experienced players, with varying advice settings, show that some of the generated advice is beneficial. It helps novices compete with experienced players and in some instances even surpass them. The mere presence of advice can be advantageous, even if players do not follow it. | Feng Gu, Wichayaporn Wongkamjan, Jordan Lee BoydGraber, Jonathan K. Kummerfeld, Denis Peskoff, Jonathan May |  |
| 7 |  |  [Local Prompt Optimization](https://doi.org/10.18653/v1/2025.naacl-short.7) |  | 0 | In recent years, the use of prompts to guide the output of Large Language Models have increased dramatically. However, even the best of experts struggle to choose the correct words to stitch up a prompt for the desired task. To solve this, LLM driven prompt optimization emerged as an important problem. Existing prompt optimization methods optimize a prompt globally, where in all the prompt tokens have to be optimized over a large vocabulary while solving a complex task. The large optimization space (tokens) leads to insufficient guidance for a better prompt. In this work, we introduce Local Prompt Optimization (LPO) that integrates with any general automatic prompt engineering method. We identify the optimization tokens in a prompt and nudge the LLM to focus only on those tokens in its optimization step. We observe remarkable performance improvements on Math Reasoning (GSM8k and MultiArith) and BIG-bench Hard benchmarks across various automatic prompt engineering methods. Further, we show that LPO converges to the optimal prompt faster than global methods. | Yash Jain, Vishal Chowdhary |  |
| 8 |  |  [Cross-lingual Transfer of Reward Models in Multilingual Alignment](https://doi.org/10.18653/v1/2025.naacl-short.8) |  | 0 | Reinforcement learning with human feedback (RLHF) is shown to largely benefit from precise reward models (RMs). However, recent studies in reward modeling schemes are skewed towards English, limiting the applicability of RLHF in multilingual alignments. In this work, we investigate the cross-lingual transfer of RMs trained in diverse languages, primarily from English. Our experimental results demonstrate the strong cross-lingual transfer of English RMs, exceeding target language RMs by 3~4% average increase in Multilingual RewardBench. Furthermore, we analyze the cross-lingual transfer of RMs through the representation shifts. Finally, we perform multilingual alignment to exemplify how cross-lingual transfer in RM propagates to enhanced multilingual instruction-following capability. | Jiwoo Hong, Noah Lee, Rodrigo MartínezCastaño, César Rodríguez, James Thorne |  |
| 9 |  |  [Inference-Time Selective Debiasing to Enhance Fairness in Text Classification Models](https://doi.org/10.18653/v1/2025.naacl-short.9) |  | 0 | We propose selective debiasing – an inference-time safety mechanism designed to enhance the overall model quality in terms of prediction performance and fairness, especially in scenarios where retraining the model is impractical. The method draws inspiration from selective classification, where at inference time, predictions with low quality, as indicated by their uncertainty scores, are discarded. In our approach, we identify the potentially biased model predictions and, instead of discarding them, we remove bias from these predictions using LEACE – a post-processing debiasing method. To select problematic predictions, we propose a bias quantification approach based on KL divergence, which achieves better results than standard uncertainty quantification methods. Experiments on text classification datasets with encoder-based classification models demonstrate that selective debiasing helps to reduce the performance gap between post-processing methods and debiasing techniques from the at-training and pre-processing categories. | Gleb Kuzmin, Neemesh Yadav, Ivan V. Smirnov, Timothy Baldwin, Artem Shelmanov |  |
| 10 |  |  [Automatic Evaluation of Healthcare LLMs Beyond Question-Answering](https://doi.org/10.18653/v1/2025.naacl-short.10) |  | 0 | Current Large Language Models (LLMs) benchmarks are often based on open-ended or close-ended QA evaluations, avoiding the requirement of human labor. Close-ended measurements evaluate the factuality of responses but lack expressiveness. Open-ended capture the model’s capacity to produce discourse responses but are harder to assess for correctness. These two approaches are commonly used, either independently or together, though their relationship remains poorly understood. This work is focused on the healthcare domain, where both factuality and discourse matter greatly. It introduces a comprehensive, multi-axis suite for healthcare LLM evaluation, exploring correlations between open and close benchmarks and metrics. Findings include blind spots and overlaps in current methodologies. As an updated sanity check, we release a new medical benchmark–CareQA–, with both open and closed variants. Finally, we propose a novel metric for open-ended evaluations –Relaxed Perplexity– to mitigate the identified limitations. | Anna AriasDuart, Pablo Agustin MartinTorres, Daniel Hinjos, Pablo BernabeuPerez, Lucia UrcelayGanzabal, Marta GonzalezMallo, Ashwin Kumar Gururajan, Enrique LopezCuena, Sergio ÁlvarezNapagao, Dario GarciaGasulla |  |
| 11 |  |  [STRUX: An LLM for Decision-Making with Structured Explanations](https://doi.org/10.18653/v1/2025.naacl-short.11) |  | 0 | Countless decisions shape our lives, and it is crucial to understand the how and why behind them. In this paper, we introduce a new LLM decision-making framework called STRUX, which enhances LLM decision-making by providing structured explanations. These include favorable and adverse facts related to the decision, along with their respective strengths. STRUX begins by distilling lengthy information into a concise table of key facts. It then employs a series of self-reflection steps to determine which of these facts are pivotal, categorizing them as either favorable or adverse in relation to a specific decision. Lastly, we fine-tune an LLM to identify and prioritize these key facts to optimize decision-making. STRUX has been evaluated on the challenging task of forecasting stock investment decisions based on earnings call transcripts and demonstrated superior performance against strong baselines. It enhances decision transparency by allowing users to understand the impact of different factors, representing a meaningful step towards practical decision-making with LLMs. | Yiming Lu, Yebowen Hu, Hassan Foroosh, Wei Jin, Fei Liu |  |
| 12 |  |  [Improving Vietnamese-English Cross-Lingual Retrieval for Legal and General Domains](https://doi.org/10.18653/v1/2025.naacl-short.12) |  | 0 | Document retrieval plays a crucial role in numerous question-answering systems, yet research has concentrated on the general knowledge domain and resource-rich languages like English. In contrast, it remains largely underexplored in low-resource languages and cross-lingual scenarios within specialized domain knowledge such as legal. We present a novel dataset designed for cross-lingual retrieval between Vietnamese and English, which not only covers the general domain but also extends to the legal field. Additionally, we propose auxiliary loss function and symmetrical training strategy that significantly enhance the performance of state-of-the-art models on these retrieval tasks. Our contributions offer a significant resource and methodology aimed at improving cross-lingual retrieval in both legal and general QA settings, facilitating further advancements in document retrieval research across multiple languages and a broader spectrum of specialized domains. All the resources related to our work can be accessed at huggingface.co/datasets/bkai-foundation-models/crosslingual. | Toan Ngoc Nguyen, Nam Le Hai, Nguyen Doan Hieu, Dai An Nguyen, Linh Ngo Van, Thien Huu Nguyen, Sang Dinh |  |
| 13 |  |  [Computational Discovery of Chiasmus in Ancient Religious Text](https://doi.org/10.18653/v1/2025.naacl-short.13) |  | 0 | Chiasmus, a debated literary device in Biblical texts, has captivated mystics while sparking ongoing scholarly discussion. In this paper, we introduce the first computational approach to systematically detect chiasmus within Biblical passages. Our method leverages neural embeddings to capture lexical and semantic patterns associated with chiasmus, applied at multiple levels of textual granularity (half-verses, verses). We also involve expert annotators to review a subset of the detected patterns. Despite its computational efficiency, our method achieves robust results, with high inter-annotator agreement and system accuracy of 0.80 at the verse level and 0.60 at the half-verse level. We further provide a qualitative analysis of the distribution of detected chiasmi, along with selected examples that highlight the effectiveness of our approach. | Hope McGovern, Hale Sirin, Tom Lippincott |  |
| 14 |  |  [Characterizing the Effects of Translation on Intertextuality using Multilingual Embedding Spaces](https://doi.org/10.18653/v1/2025.naacl-short.14) |  | 0 | Rhetorical devices are difficult to translate, but they are crucial to the translation of literary documents. We investigate the use of multilingual embedding spaces to characterize the preservation of intertextuality, one common rhetorical device, across human and machine translation. To do so, we use Biblical texts, which are both full of intertextual references and are highly translated works. We provide a metric to characterize intertextuality at the corpus level and provide a quantitative analysis of the preservation of this rhetorical device across extant human translations and machine-generated counterparts. We go on to provide qualitative analysis of cases wherein human translations over- or underemphasize the intertextuality present in the text, whereas machine translations provide a neutral baseline. This provides support for established scholarship proposing that human translators have a propensity to amplify certain literary characteristics of the original manuscripts. | Hope McGovern, Hale Sirin, Tom Lippincott |  |
| 15 |  |  [LLM2: Let Large Language Models Harness System 2 Reasoning](https://doi.org/10.18653/v1/2025.naacl-short.15) |  | 0 | Large language models (LLMs) have exhibited impressive capabilities across a myriad of tasks, yet they occasionally yield undesirable outputs. We posit that these limitations are rooted in the foundational autoregressive architecture of LLMs, which inherently lacks mechanisms for differentiating between desirable and undesirable results. Drawing inspiration from the dual-process theory of human cognition, we introduce LLM2, a novel framework that combines an LLM (System 1) with a process-based verifier (System 2). Within LLM2, the LLM is responsible for generating plausible candidates, while the verifier provides timely process-based feedback to distinguish desirable and undesirable outputs. The verifier is trained with a pairwise comparison loss on synthetic process-supervision data generated through our token quality exploration strategy. Empirical results on mathematical reasoning benchmarks substantiate the efficacy of LLM2, exemplified by an accuracy enhancement from 50.3 to 57.8 (+7.5) for Llama3-1B on GSM8K. Furthermore, when combined with self-consistency, LLM2 achieves additional improvements, boosting major@20 accuracy from 56.2 to 70.2 (+14.0). | Cheng Yang, Chufan Shi, Siheng Li, Bo Shui, Yujiu Yang, Wai Lam |  |
| 16 |  |  [Context-Efficient Retrieval with Factual Decomposition](https://doi.org/10.18653/v1/2025.naacl-short.16) |  | 0 | There has recently been considerable interest in incorporating information retrieval into large language models (LLMs). Retrieval from a dynamically expanding external corpus of text allows a model to incorporate current events and can be viewed as a form of episodic memory. Here we demonstrate that pre-processing the external corpus into semi-structured “atomic facts” makes retrieval more efficient. More specifically, we demonstrate that our particular form of atomic facts improves performance on various question answering tasks when the amount of retrieved text is limited. Limiting the amount of retrieval reduces the size of the context and improves inference efficiency. | Yanhong Li, David Yunis, David McAllester, Jiawei Zhou |  |
| 17 |  |  [Sports and Women's Sports: Gender Bias in Text Generation with Olympic Data](https://doi.org/10.18653/v1/2025.naacl-short.17) |  | 0 | Large Language Models (LLMs) have been shown to be biased in prior work, as they generate text that is in line with stereotypical views of the world or that is not representative of the viewpoints and values of historically marginalized demographic groups. In this work, we propose using data from parallel men’s and women’s events at the Olympic Games to investigate different forms of gender bias in language models. We define three metrics to measure bias, and find that models are consistently biased against women when the gender is ambiguous in the prompt. In this case, the model frequently retrieves only the results of the men’s event with or without acknowledging them as such, revealing pervasive gender bias in LLMs in the context of athletics. | Laura Biester |  |
| 18 |  |  [Alligators All Around: Mitigating Lexical Confusion in Low-resource Machine Translation](https://doi.org/10.18653/v1/2025.naacl-short.18) |  | 0 | Current machine translation (MT) systems for low-resource languages have a particular failure mode: When translating words in a given domain, they tend to confuse words within that domain. So, for example, “lion” might be translated as “alligator”, and “orange” might be rendered as “purple.” We propose a recall-based metric for measuring this problem and show that the problem exists in 122 low-resource languages. We then show that this problem can be mitigated by using a large language model (LLM) to post-edit the MT output, specifically by including the entire GATITOS lexicon for the relevant language as a very long context prompt. We show gains in average ChrF score over the set of 122 languages, and we show that the recall score for relevant lexical items also improves. Finally, we demonstrate that a small dedicated MT system with a general-purpose LLM as a post-editor is outperforms a lexicon-based RAG-LLM translator, suggesting a new paradigm for LLM use. | Elizabeth Nielsen, Isaac Caswell, Jiaming Luo, Colin Cherry |  |
| 19 |  |  [PROM: Pivoted and Regulated Optimization for Multilingual Instruction Learning](https://doi.org/10.18653/v1/2025.naacl-short.19) |  | 0 | Large language models (LLMs) have become standard for natural language generation tasks, with instruction-tuning enhancing their capabilities. However, the lack of instruction-tuning datasets in languages other than English limits their application to diverse languages. To address this, researchers have adapted English-centric LLMs to other languages by appending English tuning data with its translated pair, from which we observe negative interference between the two. To resolve this, our contribution is identifying English as an internal pivot language, based on which we disentangle the roles of English and target language data in training. Specifically, we first design two roles as pivoted objectives, and also propose to regulate between the two, to better generalize for under-represented languages. Experiments across various languages demonstrate the effectiveness of our approach on multiple benchmarks. The code is publicly available for further exploration. | Jaeseong Lee, Seungwon Hwang, Hojin Lee, Yunju Bak, Changmin Lee |  |
| 20 |  |  [Concept-Reversed Winograd Schema Challenge: Evaluating and Improving Robust Reasoning in Large Language Models via Abstraction](https://doi.org/10.18653/v1/2025.naacl-short.20) |  | 0 | While Large Language Models (LLMs) have showcased remarkable proficiency in reasoning, there is still a concern about hallucinations and unreliable reasoning issues due to semantic associations and superficial logical chains. To evaluate the extent to which LLMs perform robust reasoning instead of relying on superficial logical chains, we propose a new evaluation dataset, the Concept-Reversed Winograd Schema Challenge (CR-WSC), based on the famous Winograd Schema Challenge (WSC) dataset. By simply reversing the concepts to those that are more associated with the wrong answer, we find that the performance of LLMs drops significantly despite the rationale of reasoning remaining the same. Furthermore, we propose Abstraction-of-Thought (AoT), a novel prompt method for recovering adversarial cases to normal cases using conceptual abstraction to improve LLMs’ robustness and consistency in reasoning, as demonstrated by experiments on CR-WSC. | Kaiqiao Han, Tianqing Fang, Zhaowei Wang, Yangqiu Song, Mark Steedman |  |
| 21 |  |  [Defense against Prompt Injection Attacks via Mixture of Encodings](https://doi.org/10.18653/v1/2025.naacl-short.21) |  | 0 | Large Language Models (LLMs) have emerged as a dominant approach for a wide range of NLP tasks, with their access to external information further enhancing their capabilities. However, this introduces new vulnerabilities, known as prompt injection attacks, where external content embeds malicious instructions that manipulate the LLM’s output. Recently, the Base64 defense has been recognized as one of the most effective methods for reducing success rate of prompt injection attacks. Despite its efficacy, this method can degrade LLM performance on certain NLP tasks. To address this challenge, we propose a novel defense mechanism: mixture of encodings, which utilizes multiple character encodings, including Base64. Extensive experimental results show that our method achieves one of the lowest attack success rates under prompt injection attacks, while maintaining high performance across all NLP tasks, outperforming existing character encoding-based defense methods. This underscores the effectiveness of our mixture of encodings strategy for both safety and task performance metrics. | Ruiyi Zhang, David Sullivan, Kyle Jackson, Pengtao Xie, Mei Chen |  |
| 22 |  |  [Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers](https://doi.org/10.18653/v1/2025.naacl-short.22) |  | 0 | AI Safety Moderation (ASM) classifiers are designed to moderate content on social media platforms and to serve as guardrails that prevent Large Language Models (LLMs) from being fine-tuned on unsafe inputs. Owing to their potential for disparate impact, it is crucial to ensure that these classifiers: (1) do not unfairly classify content belonging to users from minority groups as unsafe compared to those from majority groups and (2) that their behavior remains robust and consistent across similar inputs. In this work, we thus examine the fairness and robustness of four widely-used, closed-source ASM classifiers: OpenAI Moderation API, Perspective API, Google Cloud Natural Language (GCNL) API, and Clarifai API. We assess fairness using metrics such as demographic parity and conditional statistical parity, comparing their performance against ASM models and a fair-only baseline. Additionally, we analyze robustness by testing the classifiers’ sensitivity to small and natural input perturbations. Our findings reveal potential fairness and robustness gaps, highlighting the need to mitigate these issues in future versions of these models. | Akshit Achara, Anshuman Chhabra |  |
| 23 |  |  [CoRAG: Collaborative Retrieval-Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-short.23) |  | 0 | Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive tasks, especially under few-shot learning constraints. We introduce CoRAG, a framework extending RAG to collaborative settings, where clients jointly train a shared model using a collaborative passage store. To evaluate CoRAG, we introduce CRAB, a benchmark for collaborative homogeneous open-domain question answering. Our experiments demonstrate that CoRAG consistently outperforms both parametric collaborative learning methods and locally trained RAG models in low-resource scenarios. Further analysis reveals the critical importance of relevant passages within the shared store, the surprising benefits of incorporating irrelevant passages, and the potential for hard negatives to negatively impact performance. This introduces a novel consideration in collaborative RAG: the trade-off between leveraging a collectively enriched knowledge base and the potential risk of incorporating detrimental passages from other clients. Our findings underscore the viability of CoRAG, while also highlighting key design challenges and promising avenues for future research. | Aashiq Muhamed, Mona T. Diab, Virginia Smith |  |
| 24 |  |  [Is It Navajo? Accurate Language Detection for Endangered Athabaskan Languages](https://doi.org/10.18653/v1/2025.naacl-short.24) |  | 0 | Endangered languages, such as Navajo—the most widely spoken Native American language—are significantly underrepresented in contemporary language technologies, exacerbating the challenges of their preservation and revitalization. This study evaluates Google’s Language Identification (LangID) tool, which does not currently support any Native American languages. To address this, we introduce a random forest classifier trained on Navajo and twenty erroneously suggested languages by LangID. Despite its simplicity, the classifier achieves near-perfect accuracy (97-100%). Additionally, the model demonstrates robustness across other Athabaskan languages—a family of Native American languages spoken primarily in Alaska, the Pacific Northwest, and parts of the Southwestern United States—suggesting its potential for broader application. Our findings underscore the pressing need for NLP systems that prioritize linguistic diversity and adaptability over centralized, one-size-fits-all solutions, especially in supporting underrepresented languages in a multicultural world. This work directly contributes to ongoing efforts to address cultural biases in language models and advocates for the development of culturally localized NLP tools that serve diverse linguistic communities. | Ivory Yang, Weicheng Ma, Chunhui Zhang, Soroush Vosoughi |  |
| 25 |  |  [Don't Touch My Diacritics](https://doi.org/10.18653/v1/2025.naacl-short.25) |  | 0 | The common practice of preprocessing text before feeding it into NLP models introduces many decision points which have unintended consequences on model performance. In this opinion piece, we focus on the handling of diacritics in texts originating in many languages and scripts. We demonstrate, through several case studies, the adverse effects of inconsistent encoding of diacritized characters and of removing diacritics altogether. We call on the community to adopt simple but necessary steps across all models and toolkits in order to improve handling of diacritized text and, by extension, increase equity in multilingual NLP. | Kyle Gorman, Yuval Pinter |  |
| 26 |  |  [Pretrained Image-Text Models are Secretly Video Captioners](https://doi.org/10.18653/v1/2025.naacl-short.26) |  | 0 | Developing video captioning models is computationally expensive. The dynamic nature of video also complicates the design of multimodal models that can effectively caption these sequences. However, we find that by using minimal computational resources and without complex modifications to address video dynamics, an image-based model can be repurposed to outperform several specialised video captioning systems. Our adapted model demonstrates top-tier performance on major benchmarks, ranking 2nd on MSR-VTT and MSVD, and 3rd on VATEX. We transform it into a competitive video captioner by post-training a typical image captioning model BLIP-2 with only 6,000 video-text pairs and simply concatenating frames—significantly fewer data than other methods, which use 2.5 to 144 million pairs. From a resource optimization perspective, this video captioning study focuses on three fundamental factors: optimizing model scale, maximizing data efficiency, and incorporating reinforcement learning. This extensive study demonstrates that a lightweight, image-based adaptation strategy can rival state-of-the-art video captioning systems, offering a practical solution for low-resource scenarios. | Chunhui Zhang, Yiren Jian, Zhongyu Ouyang, Soroush Vosoughi |  |
| 27 |  |  [Reverse Modeling in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-short.27) |  | 0 | Humans are accustomed to reading and writing in a forward manner, and this natural bias extends to text understanding in auto-regressive large language models (LLMs). This paper investigates whether LLMs, like humans, struggle with reverse modeling, specifically with reversed text inputs. We found that publicly available pre-trained LLMs cannot understand such inputs. However, LLMs trained from scratch with both forward and reverse texts can understand them equally well during inference across multiple languages.Our case study shows that different-content texts result in different losses if input (to LLMs) in different directions—some get lower losses for forward while some for reverse. This leads us to a simple and nice solution for data selection based on the loss differences between forward and reverse directions. Using our selected data in continued pretraining can boost LLMs’ performance by a large margin across different language understanding benchmarks. | Sicheng Yu, Yuanchen Xu, Cunxiao Du, Yanying Zhou, Minghui Qiu, Qianru Sun, Hao Zhang, Jiawei Wu |  |
| 28 |  |  [Preserving Multilingual Quality While Tuning Query Encoder on English Only](https://doi.org/10.18653/v1/2025.naacl-short.28) |  | 0 | A query encoder of a dual passage retrieval system can be tuned for specific types of queries or domains, while the precomputed and stored documents representations are kept intact. Switching from one query encoder to another when needed is easily feasible, unlike overhauling the embeddings of a whole knowledge base. In this work we raise a question: Can the generic, original qualities of the encoder be preserved or at least left not too degraded when it is tuned on a narrow domain? We conducted experiments on a high quality multilingual embedding model: Tuning it on a single English-only dataset, we observe that the tuning not only preserves the multilingual qualities, but even improves them. The embedding qualities on distinctly different data are also improved or at least preserved. Drawing on our observations, we suggest a more general hypothesis: Tuning with intentionally low learning rate can preserve or improve a system’s properties acquired in training, but not specifically targeted by tuning. We call this adiabatic tuning and provide tentative explanations. | Oleg Vasilyev, Randy Sawaya, John Bohannon |  |
| 29 |  |  [Using Contextually Aligned Online Reviews to Measure LLMs' Performance Disparities Across Language Varieties](https://doi.org/10.18653/v1/2025.naacl-short.29) |  | 0 | A language can have different varieties. These varieties can affect the performance of natural language processing (NLP) models, including large language models (LLMs), which are often trained on data from widely spoken varieties. This paper introduces a novel and cost-effective approach to benchmark model performance across language varieties. We argue that international online review platforms,such as Booking.com, can serve as effective data sources for constructing datasets that capture comments in different language varieties from similar real-world scenarios, like reviews for the same hotel with the same rating using the same language (e.g., Mandarin Chinese) but different language varieties (e.g., Taiwan Mandarin, Mainland Mandarin). To prove this concept, we constructed a contextually aligned dataset comprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs in a sentiment analysis task. Our results show that LLMs consistently underperform in Taiwan Mandarin. | Zixin Tang, ChiehYang Huang, TsungChi Li, Ho Yin Sam Ng, HenHsen Huang, TingHao Kenneth Huang |  |
| 30 |  |  [Towards Federated Low-Rank Adaptation of Language Models with Rank Heterogeneity](https://doi.org/10.18653/v1/2025.naacl-short.30) |  | 0 | Low-rank adaptation (LoRA) offers an efficient alternative to full-weight adaptation in federated fine-tuning of language models, significantly reducing computational costs. By adjusting ranks for each client, federated LoRA enables flexible resource allocation. However, we observe that heterogeneous ranks among clients lead to unstable performance. Our analysis attributes this instability to the conventional zero-padding aggregation strategy, which dilutes information from high-rank clients during model aggregation. To address this issue, we propose a replication-based padding strategy that better retains valuable information from clients with high-quality data. Empirically, this approach accelerates convergence and enhances the global model’s predictive performance. | Yuji Byun, Jaeho Lee |  |
| 31 |  |  [Related Knowledge Perturbation Matters: Rethinking Multiple Pieces of Knowledge Editing in Same-Subject](https://doi.org/10.18653/v1/2025.naacl-short.31) |  | 0 |  | Zenghao Duan, Wenbin Duan, Zhiyi Yin, Yinghan Shen, Shaoling Jing, Jie Zhang, Huawei Shen, Xueqi Cheng |  |
| 32 |  |  [STEP: Staged Parameter-Efficient Pre-training for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-short.32) |  | 0 | Pre-training large language models (LLMs) faces significant memory challenges due to the large size of model weights. We introduce STaged parameter-Efficient Pre-training (STEP), which integrates parameter-efficient tuning techniques with model growth. We conduct experiments on pre-training LLMs of various sizes and demonstrate that STEP achieves up to a 53.9% reduction in maximum memory requirements compared to vanilla pre-training while maintaining equivalent performance. Furthermore, we show that the model by STEP performs comparably to vanilla pre-trained models on downstream tasks after instruction tuning. | Kazuki Yano, Takumi Ito, Jun Suzuki |  |
| 33 |  |  [Language Models Encode Numbers Using Digit Representations in Base 10](https://doi.org/10.18653/v1/2025.naacl-short.33) |  | 0 | Large language models (LLMs) frequently make errors when handling even simple numerical problems, such as comparing two small numbers. A natural hypothesis is that these errors stem from how LLMs represent numbers, and specifically, whether their representations of numbers capture their numeric values. We tackle this question from the observation that LLM errors on numerical tasks are often distributed across the digits of the answer rather than normally around its numeric value. Through a series of probing experiments and causal interventions, we show that LLMs internally represent numbers with individual circular representations per-digit in base 10.This digit-wise representation, as opposed to a value representation, sheds light on the error patterns of models on tasks involving numerical reasoning and could serve as a basis for future studies on analyzing numerical mechanisms in LLMs. | Amit A. Levy, Mor Geva |  |
| 34 |  |  [A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference](https://doi.org/10.18653/v1/2025.naacl-short.34) |  | 0 | Recently, sharing key-value (KV) cache across layers has been found effective in efficient inference of large language models (LLMs). To systematically investigate different techniques of cross-layer KV sharing, we propose a unified framework that covers several recent methods and their novel variants. We conduct comprehensive experiments on all the configurations of the framework, evaluating their generation throughput and performance in language modeling and downstream tasks. We find that when reducing the size of the KV cache by 2×, most configurations can achieve higher throughput than standard transformers while maintaining competitive performance.When further reducing the size of the KV cache, however, pairing queries of all layers with KVs of upper layers performs better, at the expense of additional training cost and prefilling latency. We hope that this work will help users make more informed choices of cross-layer KV sharing approaches and facilitate future research on efficient LLM inference. | You Wu, Haoyi Wu, Kewei Tu |  |
| 35 |  |  [AMPS: ASR with Multimodal Paraphrase Supervision](https://doi.org/10.18653/v1/2025.naacl-short.35) |  | 0 | Spontaneous or conversational multilingual speech presents many challenges for state-of-the-art automatic speech recognition (ASR) systems. In this work, we present a new technique AMPS, that augments a multilingual multimodal ASR system with paraphrase-based supervision for improved conversational ASR in multiple languages, including Hindi, Marathi, Malayalam, Kannada, and Nyanja. We use paraphrases of the reference transcriptions as additional supervision while training the multimodal ASR model and selectively invoke this paraphrase objective for utterances with poor ASR performance. Using AMPS with a state-of-the-art multimodal model SeamlessM4T, we obtain significant relative reductions in word error rates (WERs) of up to 5%. We present detailed analyses of our system using both objective and human evaluation metrics. | Abhishek Gupta, Amruta Parulekar, Sameep Chattopadhyay, Preethi Jyothi |  |
| 36 |  |  [Taxi1500: A Dataset for Multilingual Text Classification in 1500 Languages](https://doi.org/10.18653/v1/2025.naacl-short.36) |  | 0 | While broad-coverage multilingual natural language processing tools have been developed, a significant portion of the world’s over 7000 languages are still neglected. One reason is the lack of evaluation datasets that cover a diverse range of languages, particularly those that are low-resource or endangered. To address this gap, we present a large-scale text classification dataset encompassing 1504 languages many of which have otherwise limited or no annotated data. This dataset is constructed using parallel translations of the Bible. We develop relevant topics, annotate the English data through crowdsourcing and project these annotations onto other languages via aligned verses. We benchmark a range of existing multilingual models on this dataset. We make our dataset and code available to the public. | Chunlan Ma, Ayyoob Imani, Haotian Ye, Renhao Pei, Ehsaneddin Asgari, Hinrich Schütze |  |
| 37 |  |  [GameTox: A Comprehensive Dataset and Analysis for Enhanced Toxicity Detection in Online Gaming Communities](https://doi.org/10.18653/v1/2025.naacl-short.37) |  | 0 | The prevalence of toxic behavior in online gaming communities necessitates robust detection methods to ensure user safety. We introduce GameTox, a novel dataset comprising 53K game chat utterances annotated for toxicity detection through intent classification and slot filling. This dataset captures the complex relationship between user intent and specific linguistic features that contribute to toxic interactions. We extensively analyze the dataset to uncover key insights into the nature of toxic speech in gaming environments. Furthermore, we establish baseline performance metrics using state-of-the-art natural language processing and large language models, demonstrating the dataset’s contribution towards enhancing the detection of toxic behavior and revealing the limitations of contemporary models. Our results indicate that leveraging both intent detection and slot filling provides a significantly more granular and context-aware understanding of harmful messages. This dataset serves as a valuable resource to train advanced models that can effectively mitigate toxicity in online gaming and foster healthier digital spaces. Our dataset is publicly available at: https://github.com/shucoll/GameTox. | Usman Naseem, Shuvam Shiwakoti, Siddhant Bikram Shah, Surendrabikram Thapa, Qi Zhang |  |
| 38 |  |  [FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern LLMs](https://doi.org/10.18653/v1/2025.naacl-short.38) |  | 0 | Summarization is one of the most common tasks performed by large language models (LLMs), especially in applications like Retrieval-Augmented Generation (RAG). However, existing evaluations of hallucinations in LLM-generated summaries, and evaluations of hallucination detection models both suffer from a lack of diversity and recency in the LLM and LLM families considered. This paper introduces FaithBench, a summarization hallucination benchmark comprising challenging hallucinations made by 10 modern LLMs from 8 different families, with ground truth annotations by human experts. “Challenging” here means summaries on which popular, state-of-the-art hallucination detection models, including GPT-4o-as-a-judge, disagreed on. Our results show GPT-4o and GPT-3.5-Turbo produce the least hallucinations. However, most state-of-the-art hallucination detection models have near 50% accuracies on FaithBench, indicating lots of room for future improvement. | Forrest Sheng Bao, Miaoran Li, Renyi Qu, Ge Luo, Erana Wan, Yujia Tang, Weisi Fan, Manveer Singh Tamber, Suleman Kazi, Vivek Sourabh, Mike Qi, Ruixuan Tu, Chenyu Xu, Matthew Gonzales, Ofer Mendelevitch, Amin Ahmad |  |
| 39 |  |  [Debate-Feedback: A Multi-Agent Framework for Efficient Legal Judgment Prediction](https://doi.org/10.18653/v1/2025.naacl-short.39) |  | 0 | The use of AI in legal analysis and prediction (LegalAI) has gained attention, with past research focusing on retrieval-based methods and fine-tuning large models. However, these approaches often require large datasets and underutilize the capabilities of modern large language models (LLMs). In this paper, inspired by the debate phase of real courtroom trials, we propose a novel legal judgment prediction model based on the Debate-Feedback architecture, which integrates LLM multi-agent debate and reliability evaluation models. Unlike traditional methods, our model achieves significant improvements in efficiency by minimizing the need for large historical datasets, thus offering a lightweight yet robust solution. Comparative experiments show that it outperforms several general-purpose and domain-specific legal models, offering a dynamic reasoning process and a promising direction for future LegalAI research. | Xi Chen, Mao Mao, Shuo Li, Haotian Shangguan |  |
| 40 |  |  [Great Memory, Shallow Reasoning: Limits of kNN-LMs](https://doi.org/10.18653/v1/2025.naacl-short.40) |  | 0 | K-nearest neighbor language models (kNN-LMs), which integrate retrieval with next-word prediction, have demonstrated strong performance in language modeling as well as some downstream NLP benchmarks. These results have led researchers to argue that models trained on poor quality or outdated data could perform well by employing a kNN extension that has access to a higher-quality datastore. In this work, we ask whether this improved ability to recall information really translates into downstream abilities. We extensively evaluate kNN-LMs on a diverse set of tasks, ranging from sentiment classification and commonsense reasoning to multi-hop reasoning. Results show that kNN-LMs excel at memory-intensive tasks, where utilizing the patterns in the input is sufficient for determining the output, but struggle with reasoning tasks that require integrating multiple pieces of information to derive new knowledge. We further demonstrate through oracle experiments and qualitative analysis that even with perfect retrieval, kNN-LMs still fail to determine the correct answers, placing an upper bound on their reasoning performance. | Shangyi Geng, Wenting Zhao, Alexander M. Rush |  |
| 41 |  |  [Repetition Neurons: How Do Language Models Produce Repetitions?](https://doi.org/10.18653/v1/2025.naacl-short.41) |  | 0 | This paper introduces repetition neurons, which can be regarded as “skill neurons” responsible for the repetition problem in text generation tasks. These neurons are progressively activated more strongly as repetition continues, indicating that they perceive repetition as a task to copy the previous context repeatedly, similar to in-context learning. We identify these repetition neurons by comparing activation values before and after the onset of repetition in texts generated by recent pre-trained language models. We analyze the repetition neurons in three English and one Japanese pre-trained language models and observe similar patterns across them. | Tatsuya Hiraoka, Kentaro Inui |  |
| 42 |  |  [STAR: Spectral Truncation and Rescale for Model Merging](https://doi.org/10.18653/v1/2025.naacl-short.42) |  | 0 | Model merging is an efficient way of obtaining a multi-task model from several pretrained models without further fine-tuning, and it has gained attention in various domains, including natural language processing (NLP). Despite the efficiency, a key challenge in model merging is the seemingly inevitable decrease in task performance as the number of models increases. In this paper, we propose \*\*S\*\*pectral \*\*T\*\*runcation \*\*A\*\*nd \*\*R\*\*escale (STAR) that aims at mitigating “merging conflicts” by truncating small components in the respective spectral spaces, which is followed by an automatic parameter rescaling scheme to retain the nuclear norm of the original matrix. STAR requires no additional inference on original training data and is robust to hyperparamater choice. We demonstrate the effectiveness of STAR through extensive model merging cases on diverse NLP tasks. Specifically, STAR works robustly across varying model sizes, and can outperform baselines by 4.2% when merging 12 models on Flan-T5. Our code is publicly available at https://github.com/IBM/STAR. | YuAng Lee, ChingYun Ko, Tejaswini Pedapati, IHsin Chung, MiYen Yeh, PinYu Chen |  |
| 43 |  |  [Task-driven Layerwise Additive Activation Intervention](https://doi.org/10.18653/v1/2025.naacl-short.43) |  | 0 | Modern language models (LMs) have significantly advanced generative modeling in natural language processing (NLP). Despite their success, LMs often struggle with adaptation to new contexts in real-time applications. A promising approach to task adaptation is activation intervention, which steers the LMs’ generation process by identifying and manipulating the activations. However, existing interventions rely heavily on heuristic rules or require many prompt inputs to determine effective interventions. In this paper, we propose a layer-wise additive activation intervention framework that optimizes the intervention process, thereby enhancing sample efficiency. We evaluate our framework on various datasets, demonstrating improvements in the accuracy of pretrained LMs and competing intervention baselines. | Hieu Trung Nguyen, Bao Nguyen, Binh Nguyen, Viet Anh Nguyen |  |
| 44 |  |  [Scaling Multi-Document Event Summarization: Evaluating Compression vs. Full-Text Approaches](https://doi.org/10.18653/v1/2025.naacl-short.44) |  | 0 | Automatically summarizing large text collections is a valuable tool for document research, with applications in journalism, academic research, legal work, and many other fields. In this work, we contrast two classes of systems for large-scale multi-document summarization (MDS): compression and full-text. Compression-based methods use a multi-stage pipeline and often lead to lossy summaries. Full-text methods promise a lossless summary by relying on recent advances in long-context reasoning. To understand their utility on large-scale MDS, we evaluated them on three datasets, each containing approximately one hundred documents per summary. Our experiments cover a diverse set of long-context transformers (Llama-3.1, Command-R, Jamba-1.5-Mini) and compression methods (retrieval-augmented, hierarchical, incremental). Overall, we find that full-text and retrieval methods perform the best in most settings. With further analysis into the salient information retention patterns, we show that compression-based methods show strong promise at intermediate stages, even outperforming full-context. However, they suffer information loss due to their multi-stage pipeline and lack of global context. Our results highlight the need to develop hybrid approaches that combine compression and full-text approaches for optimal performance on large-scale multi-document summarization. | Adithya Pratapa, Teruko Mitamura |  |
| 45 |  |  [Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models](https://doi.org/10.18653/v1/2025.naacl-short.45) |  | 0 | Large Vision Language Models (LVLMs) often suffer from object hallucination, which undermines their reliability. Surprisingly, we find that simple object-based visual prompting—overlaying visual cues (e.g., bounding box, circle) on images—can significantly mitigate such hallucination; however, different visual prompts (VPs) vary in effectiveness. To address this, we propose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify optimal VPs that enhance LVLM responses without needing access to model internals. Our approach employs a pool of candidate VPs and trains a router model to dynamically select the most effective VP for a given input image. This black-box approach is model-agnostic, making it applicable to both open-source and proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR demonstrate that BBVPE effectively reduces object hallucination. | Sangmin Woo, Kang Zhou, Yun Zhou, Shuai Wang, Sheng Guan, Haibo Ding, Lin Lee Cheong |  |
| 46 |  |  [A Layered Debating Multi-Agent System for Similar Disease Diagnosis](https://doi.org/10.18653/v1/2025.naacl-short.46) |  | 0 | Distinguishing between extremely similar diseases is a critical and challenging aspect of clinical decision-making. Traditional classification, contrastive learning, and Large Language Models (LLMs) based methods fail to detect the subtle clues necessary for differentiation. This task demands complex reasoning and a variety of tools to identify minor differences and make informed decisions. This paper probes a novel framework that leverages LLMs and a multi-agent system to achieve accurate disease diagnosis through a process of repeated debate and reassessment. The approach aims to identify subtle differences between similar disease candidates. We structure patient information and integrate extensive medical knowledge to guide the analysis towards discerning these differences for precise diagnosis. Comprehensive experiments were conducted on two public datasets and two newly introduced datasets, JarvisD2-Chinese and JarvisD2-English, to validate the effectiveness of our method. The results confirm the efficacy of our approach, demonstrating its potential to enhance diagnostic precision in healthcare. | Yutian Zhao, Huimin Wang, Yefeng Zheng, Xian Wu |  |
| 47 |  |  [The Geometry of Numerical Reasoning: Language Models Compare Numeric Properties in Linear Subspaces](https://doi.org/10.18653/v1/2025.naacl-short.47) |  | 0 | This paper investigates whether large language models (LLMs) utilize numerical attributes encoded in a low-dimensional subspace of theembedding space when answering questions involving numeric comparisons, e.g., Was Cristiano born before Messi? We first identified,using partial least squares regression, these subspaces, which effectively encode the numerical attributes associated with the entities in comparison prompts. Further, we demonstrate causality, by intervening in these subspaces to manipulate hidden states, thereby altering the LLM’s comparison outcomes. Experiments conducted on three different LLMs showed that our results hold across different numerical attributes, indicating that LLMs utilize the linearly encoded information for numerical reasoning. | Ahmed Oumar ElShangiti, Tatsuya Hiraoka, Hilal AlQuabeh, Benjamin Heinzerling, Kentaro Inui |  |
| 48 |  |  [AlignFreeze: Navigating the Impact of Realignment on the Layers of Multilingual Models Across Diverse Languages](https://doi.org/10.18653/v1/2025.naacl-short.48) |  | 0 | Realignment techniques are often employed to enhance cross-lingual transfer in multilingual language models, still, they can sometimes degrade performance in languages that differ significantly from the fine-tuned source language. This paper introduces AlignFreeze, a method that freezes either the layers’ lower half or upper half during realignment. Through controlled experiments on 4 tasks, 3 models, and in 35 languages, we find that realignment affects all the layers but can be the most detrimental to the lower ones. Freezing the lower layers can prevent performance degradation. Particularly, AlignFreeze improves Part-of-Speech (PoS) tagging performances in languages where full realignment fails: with XLM-R, it provides improvements of more than one standard deviation in accuracy in seven more languages than full realignment. | Steve Bakos, David Guzmán, Riddhi More, Kelly Chutong Li, Félix Gaschi, EnShiun Annie Lee |  |
| 49 |  |  [FLIQA-AD: a Fusion Model with Large Language Model for Better Diagnose and MMSE Prediction of Alzheimer's Disease](https://doi.org/10.18653/v1/2025.naacl-short.49) |  | 0 | Tracking a patient’s cognitive status early in the onset of the disease provides an opportunity to diagnose and intervene in Alzheimer’s disease (AD). However, relying solely on magnetic resonance imaging (MRI) images with traditional classification and regression models may not fully extract finer-grained information. This study proposes a multi-task Fusion Language Image Question Answering model (FLIQA-AD) to perform AD identification and Mini Mental State Examination (MMSE) prediction. Specifically, a 3D Adapter is introduced in Vision Transformer (ViT) model for image feature extraction. The patient electronic health records (EHR) information and questions related to the disease work as text prompts to be encoded. Then, an ADFormer model, which combines self-attention and cross-attention mechanisms, is used to capture the correlation between EHR information and structure features. After that, the extracted brain structural information and textual content are combined as input sequences for the large language model (LLM) to identify AD and predict the corresponding MMSE score. Experimental results demonstrate the strong discrimination and MMSE prediction performance of the model, as well as question-answer capabilities. | Junhao Chen, Zhiyuan Ding, Yan Liu, Xiangzhu Zeng, Ling Wang |  |
| 50 |  |  [Transform Retrieval for Textual Entailment in RAG](https://doi.org/10.18653/v1/2025.naacl-short.50) |  | 0 | In this paper, we introduce Transform Retrieval, a novel approach aimed at improving Textual Entailment Retrieval within the framework of Retrieval-Augmented Generation (RAG). While RAG has shown promise in enhancing Large Language Models by retrieving relevant documents to extract specific knowledge or mitigate hallucination, current retrieval methods often prioritize relevance without ensuring the retrieved documents semantically support answering the queries. Transform Retrieval addresses this gap by transforming query embeddings to better align with semantic entailment without re-encoding the document corpus. We achieve this by using a transform model and employing a contrastive learning strategy to optimize the alignment between transformed query embeddings and document embeddings for better entailment.We evaluated the framework using BERT as frozen pre-trained encoder and compared it with a fully fine-tuned skyline model. Experimental results show that Transform Retrieval with simple MLP consistently approaches the skyline across multiple datasets, demonstrating the method’s effectiveness. The high performance on HotpotQA highlights its strength in many-to-many retrieval scenarios. | Quan Guo, Xin Liang |  |
| 51 |  |  [How do Multimodal Foundation Models Encode Text and Speech? An Analysis of Cross-Lingual and Cross-Modal Representations](https://doi.org/10.18653/v1/2025.naacl-short.51) |  | 0 | Multimodal foundation models aim to create a unified representation space that abstracts away from surface features like language syntax or modality differences. To investigate this, we study the internal representations of three recent models, analyzing the model activations from semantically equivalent sentences across languages in the text and speech modalities. Our findings reveal that: 1) Cross-modal representations converge over model layers, except in the initial layers specialized at text and speech processing. 2) Length adaptation is crucial for reducing the cross-modal gap between text and speech, although current approaches’ effectiveness is primarily limited to high-resource languages. 3) Speech exhibits larger cross-lingual differences than text. 4) For models not explicitly trained for modality-agnostic representations, the modality gap is more prominent than the language gap. | Hyunji Lee, Danni Liu, Supriti Sinhamahapatra, Jan Niehues |  |
| 52 |  |  [Explore the Reasoning Capability of LLMs in the Chess Testbed](https://doi.org/10.18653/v1/2025.naacl-short.52) |  | 0 | Reasoning is a central capability of human intelligence. In recent years, with the advent of large-scale datasets, pretrained large language models have emerged with new capabilities, including reasoning. However, these models still struggle with long-term, complex reasoning tasks, such as playing chess. Based on the observation that expert chess players employ a dual approach combining long-term strategic play with short-term tactical play along with language explanation, we propose improving the reasoning capability of large language models in chess by integrating annotated strategy and tactic. Specifically, we collect a dataset named MATE, which consists of 1 million chess positions with candidate moves annotated for strategy and tactics. We finetune the LLaMA-3-8B model and compare it against state-of-the-art commercial language models in the task of selecting better chess moves. Our experiments show that our models perform better than GPT, Claude, and Gemini models. We find that language explanations can enhance the reasoning capability of large language models. | Shu Wang, Lei Ji, Renxi Wang, Wenxiao Zhao, Haokun Liu, Yifan Hou, Ying Nian Wu |  |
| 53 |  |  [Auto-Cypher: Improving LLMs on Cypher generation via LLM-supervised generation-verification framework](https://doi.org/10.18653/v1/2025.naacl-short.53) |  | 0 | Graph databases like Neo4j are gaining popularity for handling complex, interconnected data, over traditional relational databases in modeling and querying relationships. While translating natural language into SQL queries is well-researched, generating Cypher queries for Neo4j remains relatively underexplored. In this work, we present an automated, LLM Supervised, pipeline to generate high quality synthetic data for Text2Cypher. Our Cypher data generation pipeline introduces LLM-As-Database-Filler, a novel strategy for ensuring Cypher query correctness, thus resulting in high quality generations. Using our pipeline, we generate high quality Text2Cypher data - SynthCypher containing 29.8k instances across various domains and queries with varying complexities. Training open-source LLMs like LLaMa-3.1-8B, Mistral-7B, and QWEN7B on SynthCypher results in performance gains of up to 40% on the Text2Cypher test split and 30% on the SPIDER benchmark, adapted for graph databases. | Aman Tiwari, Shiva Krishna Reddy Malay, Vikas Yadav, Masoud Hashemi, Sathwik Tejaswi Madhusudhan |  |
| 54 |  |  [Leveraging Moment Injection for Enhanced Semi-supervised Natural Language Inference with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-short.54) |  | 0 | Natural Language Inference (NLI) is crucial for evaluating models’ Natural Language Understanding (NLU) and reasoning abilities. The development of NLI, in part, has been driven by the creation of large datasets, which require significant human effort. This has spurred interest in semi-supervised learning (SSL) that leverages both labeled and unlabeled data. However, the absence of hypotheses and class labels in NLI tasks complicates SSL. Prior work has used class-specific fine-tuned large language models (LLMs) to generate hypotheses and assign pseudo-labels but discarded many LLM-constructed samples during training to ensure the quality. In contrast, we propose to leverage all LLM-constructed samples by handling potentially noisy samples by injecting the moments of labeled samples during training to properly adjust the level of noise. Our method outperforms strong baselines on multiple NLI datasets in low-resource settings. | Seo Yeon Park |  |
| 55 |  |  [A Fair Comparison without Translationese: English vs. Target-language Instructions for Multilingual LLMs](https://doi.org/10.18653/v1/2025.naacl-short.55) |  | 0 | Most large language models are multilingual instruction executors. Prior studies suggested that English instructions are more effective than target-language instructions even for non-English tasks; however, these studies often use datasets and instructions translated from English, which introduce biases known as translationese, hindering an unbiased comparison. To address this issue, we conduct a fair comparison between English and target-language instructions by eliminating translationese effects. Contrary to previous studies, our experiments across several tasks reveal that the advantage of adopting English instructions is not overwhelming. Additionally, we report on the features of generated texts and the instruction-following abilities when using respective instructions. | Taisei Enomoto, Hwichan Kim, Zhousi Chen, Mamoru Komachi |  |
| 56 |  |  [Evaluating Multimodal Generative AI with Korean Educational Standards](https://doi.org/10.18653/v1/2025.naacl-short.56) |  | 0 | This paper presents the Korean National Educational Test Benchmark (KoNET), a new benchmark designed to evaluate Multimodal Generative AI Systems using Korean national educational tests. KoNET comprises four exams: the Korean Elementary General Educational Development Test (KoEGED), Middle (KoMGED), High (KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are renowned for their rigorous standards and diverse questions, facilitating a comprehensive analysis of AI performance across different educational levels. By focusing on Korean, KoNET provides insights into model performance in less-explored languages. We assess a range of models—open-source, open-access, and closed APIs—by examining difficulties, subject diversity, and human error rates. The code and dataset builder will be made fully open-source. | Sanghee Park, Geewook Kim |  |
| 57 |  |  [ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large Multimodal Models with Visual Programming Challenges](https://doi.org/10.18653/v1/2025.naacl-short.57) |  | 0 | Recent advancements in large multimodal models (LMMs) have showcased impressive code generation capabilities, primarily evaluated through image-to-code benchmarks. However, these benchmarks are limited to specific visual programming scenarios where the logic reasoning and the multimodal understanding capacities are split apart. To fill this gap, we propose ScratchEval, a novel benchmark designed to evaluate the visual programming reasoning ability of LMMs. ScratchEval is based on Scratch, a block-based visual programming language widely used in children’s programming education. By integrating visual elements and embedded programming logic, ScratchEval requires the model to process both visual information and code structure, thereby comprehensively evaluating its programming intent understanding ability. Our evaluation approach goes beyond the traditional image-to-code mapping and focuses on unified logical thinking and problem-solving abilities, providing a more comprehensive and challenging framework for evaluating the visual programming ability of LMMs. ScratchEval not only fills the gap in existing evaluation methods, but also provides new insights for the future development of LMMs in the field of visual programming. | Rao Fu, Ziyang Luo, Hongzhan Lin, Zhen Ye, Jing Ma |  |
| 58 |  |  [Interpret and Control Dense Retrieval with Sparse Latent Features](https://doi.org/10.18653/v1/2025.naacl-short.58) |  | 0 | Dense embeddings deliver strong retrieval performance but often lack interpretability and controllability. This paper introduces a novel approach using sparse autoencoders (SAE) to interpret and control dense embeddings via the learned latent sparse features. Our key contribution is the development of a retrieval-oriented contrastive loss, which ensures the sparse latent features remain effective for retrieval tasks and thus meaningful to interpret. Experimental results demonstrate that both the learned latent sparse features and their reconstructed embeddings retain nearly the same retrieval accuracy as the original dense vectors, affirming their faithfulness. Our further examination of the sparse latent space reveals interesting features underlying the dense embeddings and we can control the retrieval behaviors via manipulating the latent sparse features, for example, prioritizing documents from specific perspectives in the retrieval results. | Hao Kang, Tevin Wang, Chenyan Xiong |  |
| 59 |  |  [DART: An AIGT Detector using AMR of Rephrased Text](https://doi.org/10.18653/v1/2025.naacl-short.59) |  | 0 | As large language models (LLMs) generate more human-like texts, concerns about the side effects of AI-generated texts (AIGT) have grown. So, researchers have developed methods for detecting AIGT. However, two challenges remain. First, the performance of detecting black-box LLMs is low because existing models focus on probabilistic features. Second, most AIGT detectors have been tested on a single-candidate setting, which assumes that we know the origin of an AIGT and which may deviate from the real-world scenario. To resolve these challenges, we propose DART, which consists of four steps: rephrasing, semantic parsing, scoring, and multiclass classification. We conducted three experiments to test the performance of DART. The experimental result shows that DART can discriminate multiple black-box LLMs without probabilistic features and the origin of AIGT. | Hyeonchu Park, Byungjun Kim, Bugeun Kim |  |
| 60 |  |  [Scaling Graph-Based Dependency Parsing with Arc Vectorization and Attention-Based Refinement](https://doi.org/10.18653/v1/2025.naacl-short.60) |  | 0 | We propose a novel architecture for graph-based dependency parsing that explicitly constructs vectors, from which both arcs and labels are scored. Our method addresses key limitations of the standard two-pipeline approach by unifying arc scoring and labeling into a single network, reducing scalability issues caused by the information bottleneck and lack of parameter sharing. Additionally, our architecture overcomes limited arc interactions with transformer layers to efficiently simulate higher-order dependencies. Experiments on PTB and UD show that our model outperforms state-of-the-art parsers in both accuracy and efficiency. | Nicolas Floquet, Joseph Le Roux, Nadi Tomeh, Thierry Charnois |  |
| 61 |  |  [Language Models "Grok" to Copy](https://doi.org/10.18653/v1/2025.naacl-short.61) |  | 0 | We examine the pre-training dynamics of language models, focusing on their ability to copy text from preceding context—a fundamental skill for various LLM applications, including in-context learning (ICL) and retrieval-augmented generation (RAG). We propose a novel perspective that Transformer-based language models develop copying abilities similarly to grokking, which refers to sudden generalization on test set long after the model fit to the training set. Our experiments yield three arguments: (1) The pre-training loss decreases rapidly, while the context copying ability of models initially lags and then abruptly saturates. (2) The speed of developing copying ability is independent of the number of tokens trained, similarly to how grokking speed is unaffected by dataset size as long as the data distribution is preserved. (3) Induction heads, the attention heads responsible for copying, form from shallow to deep layers during training, mirroring the development of circuits in deeper layers during grokking. We contend that the connection between grokking and context copying can provide valuable insights for more effective language model training, ultimately improving in-context performance. For example, we demonstrated that techniques that enhance grokking, such as regularization, either accelerate or enhance the development of context copying. | Ang Lv, Ruobing Xie, Xingwu Sun, Zhanhui Kang, Rui Yan |  |
| 62 |  |  [Evaluating LLMs for Quotation Attribution in Literary Texts: A Case Study of LLaMa3](https://doi.org/10.18653/v1/2025.naacl-short.62) |  | 0 | Large Language Models (LLMs) have shown promising results in a variety of literary tasks, often using complex memorized details of narration and fictional characters. In this work, we evaluate the ability of Llama-3 at attributing utterances of direct-speech to their speaker in novels. The LLM shows impressive results on a corpus of 28 novels, surpassing published results with ChatGPT and encoder-based baselines by a large margin. We then validate these results by assessing the impact of book memorization and annotation contamination.We found that these types of memorization do not explain the large performance gain, making Llama-3 the new state-of-the-art for quotation attribution in English literature. We release publicly our code and data. | Gaspard Michel, Elena V. Epure, Romain Hennequin, Christophe Cerisara |  |
| 63 |  |  [Beyond Literal Token Overlap: Token Alignability for Multilinguality](https://doi.org/10.18653/v1/2025.naacl-short.63) |  | 0 | Previous work has considered token overlap, or even similarity of token distributions, as predictors for multilinguality and cross-lingual knowledge transfer in language models. However, these very literal metrics assign large distances to language pairs with different scripts, which can nevertheless show good cross-linguality. This limits the explanatory strength of token overlap for knowledge transfer between language pairs that use distinct scripts or follow different orthographic conventions. In this paper, we propose subword token alignability as a new way to understand the impact and quality of multilingual tokenisation. In particular, this metric predicts multilinguality much better when scripts are disparate and the overlap of literal tokens is low. We analyse this metric in the context of both encoder and decoder models, look at data size as a potential distractor, and discuss how this insight may be applied to multilingual tokenisation in future work. We recommend our subword token alignability metric for identifying optimal language pairs for cross-lingual transfer, as well as to guide the construction of better multilingual tokenisers in the future. We publish our code and reproducibility details. | Katharina Hämmerl, Tomasz Limisiewicz, Jindrich Libovický, Alexander Fraser |  |
| 64 |  |  [IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark for LLMs](https://doi.org/10.18653/v1/2025.naacl-short.64) |  | 0 | Recent evaluations of LLMs on coreference resolution have revealed that traditional output formats and evaluation metrics do not fully capture the models’ referential understanding. To address this, we introduce IdentifyMe, a new benchmark for mention resolution presented in a multiple-choice question (MCQ) format, commonly used for evaluating LLMs. IdentifyMe features long narratives and employs heuristics to exclude easily identifiable mentions, creating a more challenging task. The benchmark also consists of a curated mixture of different mention types and corresponding entities, allowing for a fine-grained model performance analysis. We evaluate both closed- and open-source LLMs on IdentifyMe and observe a significant performance gap (20-30%) between the state-of-the-art sub-10B open models vs. closed ones. We observe that pronominal mentions, which have limited surface information, are typically harder for models to resolve than nominal mentions. Additionally, we find that LLMs often confuse entities when their mentions overlap in nested structures. The highest scoring model, GPT-4o, achieves 81.9% accuracy, highlighting the strong referential capabilities of state-of-the-art LLMs while also indicating room for further improvement. | Kawshik Manikantan, Makarand Tapaswi, Vineet Gandhi, Shubham Toshniwal |  |
| 65 |  |  [kNN Retrieval for Simple and Effective Zero-Shot Multi-speaker Text-to-Speech](https://doi.org/10.18653/v1/2025.naacl-short.65) |  | 0 | While recent zero-shot multi-speaker text-to-speech (TTS) models achieve impressive results, they typically rely on extensive transcribed speech datasets from numerous speakers and intricate training pipelines. Meanwhile, self-supervised learning (SSL) speech features have emerged as effective intermediate representations for TTS. Further, SSL features from different speakers that are linearly close share phonetic information while maintaining individual speaker identity. In this study, we introduce kNN-TTS, a simple and effective framework for zero-shot multi-speaker TTS using retrieval methods which leverage the linear relationships between SSL features. Objective and subjective evaluations show that our models, trained on transcribed speech from a single speaker only, achieve performance comparable to state-of-the-art models that are trained on significantly larger training datasets. The low training data requirements mean that kNN-TTS is well suited for the development of multi-speaker TTS systems for low-resource domains and languages. We also introduce an interpolation parameter which enables fine-grained voice morphing. Demo samples are available at https://idiap.github.io/knn-tts . | Karl El Hajal, Ajinkya Kulkarni, Enno Hermann, Mathew MagimaiDoss |  |
| 66 |  |  [CORD: Balancing COnsistency and Rank Distillation for Robust Retrieval-Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-short.66) |  | 0 | With the adoption of retrieval-augmented generation (RAG), large language models (LLMs) are expected to ground their generation to the retrieved contexts. Yet, this is hindered by position bias of LLMs, failing to evenly attend to all contexts. Previous work has addressed this by synthesizing contexts with perturbed positions of gold segment, creating a position-diversified train set. We extend this intuition to propose consistency regularization with augmentation and distillation. First, we augment each training instance with its position perturbation to encourage consistent predictions, regardless of ordering. We also distill behaviors of this pair, although it can be counterproductive in certain RAG scenarios where the given order from the retriever is crucial for generation quality. We thus propose CORD, balancing COnsistency and Rank Distillation: CORD adaptively samples noise-controlled perturbations from an interpolation space, ensuring both consistency and respect for the rank prior. Empirical results show this balance enables CORD to outperform consistently in diverse RAG benchmarks. | Youngwon Lee, Seungwon Hwang, Daniel F. Campos, Filip Gralinski, Zhewei Yao, Yuxiong He |  |
| 67 |  |  [GraphLSS: Integrating Lexical, Structural, and Semantic Features for Long Document Extractive Summarization](https://doi.org/10.18653/v1/2025.naacl-short.67) |  | 0 | Heterogeneous graph neural networks have recently gained attention for long document summarization, modeling the extraction as a node classification task. Although effective, these models often require external tools or additional machine learning models to define graph components, producing highly complex and less intuitive structures. We present GraphLSS, a heterogeneous graph construction for long document extractive summarization, incorporating Lexical, Structural, and Semantic features. It defines two levels of information (words and sentences) and four types of edges (sentence semantic similarity, sentence occurrence order, word in sentence, and word semantic similarity) without any need for auxiliary learning models. Experiments on two benchmark datasets show that GraphLSS is competitive with top-performing graph-based methods, outperforming recent non-graph models. We release our code on GitHub. | Margarita Bugueño, Hazem Abou Hamdan, Gerard de Melo |  |
| 68 |  |  [Step-by-Step Fact Verification System for Medical Claims with Explainable Reasoning](https://doi.org/10.18653/v1/2025.naacl-short.68) |  | 0 | Fact verification (FV) aims to assess the veracity of a claim based on relevant evidence. The traditional approach for automated FV includes a three-part pipeline relying on short evidence snippets and encoder-only inference models. More recent approaches leverage the multi-turn nature of LLMs to address FV as a step-by-step problem where questions inquiring additional context are generated and answered until there is enough information to make a decision. This iterative method makes the verification process rational and explainable. While these methods have been tested for encyclopedic claims, exploration on domain-specific and realistic claims is missing. In this work, we apply an iterative FV system on three medical fact-checking datasets and evaluate it with multiple settings, including different LLMs, external web search, and structured reasoning using logic predicates. We demonstrate improvements in the final performance over traditional approaches and the high potential of step-by-step FV systems for domain-specific claims. | Juraj Vladika, Ivana Hacajová, Florian Matthes |  |
| 69 |  |  [Developing multilingual speech synthesis system for Ojibwe, Mi'kmaq, and Maliseet](https://doi.org/10.18653/v1/2025.naacl-short.69) |  | 0 | We present lightweight flow matching multilingual text-to-speech (TTS) systems for Ojibwe, Mi’kmaq, and Maliseet, three Indigenous languages in North America. Our results show that training a multilingual TTS model on three typologically similar languages can improve the performance over monolingual models, especially when data are scarce. Attention-free architectures are highly competitive with self-attention architecture with higher memory efficiency. Our research provides technical development to language revitalization for low-resource languages but also highlights the cultural gap in human evaluation protocols, calling for a more community-centered approach to human evaluation. | Shenran Wang, Changbing Yang, Mike Parkhill, Chad Quinn, Christopher Hammerly, Jian Zhu |  |
| 70 |  |  [Bottom-Up Synthesis of Knowledge-Grounded Task-Oriented Dialogues with Iteratively Self-Refined Prompts](https://doi.org/10.18653/v1/2025.naacl-short.70) |  | 0 | Training conversational question-answering (QA) systems demands a substantial amount of in-domain data, which is often scarce in practice. A common solution to this challenge is to generate synthetic data. Traditional methods typically follow a top-down approach, where a large language model (LLM) generates multi-turn dialogues from a broad prompt. While this method produces coherent conversations, it offers limited fine-grained control over the content and is susceptible to hallucinations. We introduce a bottom-up conversation synthesis approach, where QA pairs are generated first and then combined into a coherent dialogue. This method offers greater control and precision by dividing the process into two distinct steps, enabling refined instructions and validations to be handled separately. Additionally, this structure allows the use of non-local models in stages that do not involve proprietary knowledge, enhancing the overall quality of the generated data. Both human and automated evaluations demonstrate that our approach produces more realistic and higher-quality dialogues compared to top-down methods. | Kun Qian, Maximillian Chen, Siyan Li, Arpit Sharma, Zhou Yu |  |
| 71 |  |  [Sociodemographic Prompting is Not Yet an Effective Approach for Simulating Subjective Judgments with LLMs](https://doi.org/10.18653/v1/2025.naacl-short.71) |  | 0 | Human judgments are inherently subjective and are actively affected by personal traits such as gender and ethnicity. While Large LanguageModels (LLMs) are widely used to simulate human responses across diverse contexts, their ability to account for demographic differencesin subjective tasks remains uncertain. In this study, leveraging the POPQUORN dataset, we evaluate nine popular LLMs on their abilityto understand demographic differences in two subjective judgment tasks: politeness and offensiveness. We find that in zero-shot settings, most models’ predictions for both tasks align more closely with labels from White participants than those from Asian or Black participants, while only a minor gender bias favoring women appears in the politeness task. Furthermore, sociodemographic prompting does not consistently improve and, in some cases, worsens LLMs’ ability to perceive language from specific sub-populations. These findings highlight potential demographic biases in LLMs when performing subjective judgment tasks and underscore the limitations of sociodemographic prompting as a strategy to achieve pluralistic alignment. Code and data are available at: https://github.com/Jiaxin-Pei/LLM-as-Subjective-Judge. | Huaman Sun, Jiaxin Pei, Minje Choi, David Jurgens |  |
| 72 |  |  [Identifying Power Relations in Conversations using Multi-Agent Social Reasoning](https://doi.org/10.18653/v1/2025.naacl-short.72) |  | 0 | Large language models (LLMs) struggle in social science domains, where critical thinking and human-level inference are crucial. In this work, we propose a multi-agent social reasoning framework that leverages the generative and reasoning capabilities of LLMs to generate and evaluate reasons from multiple perspectives grounded in social science theories, and construct a factor graph for inference. Experimental results on understanding power dynamics in conversations show that our method outperforms standard prompting baselines, demonstrating its potential for tackling hard Computational Social Science (CSS) tasks. | Zhaoqing Wu, Dan Goldwasser, Maria Leonor Pacheco, Leora Morgenstern |  |
| 73 |  |  [Examining Spanish Counseling with MIDAS: a Motivational Interviewing Dataset in Spanish](https://doi.org/10.18653/v1/2025.naacl-short.73) |  | 0 | Cultural and language factors significantly influence counseling, but Natural Language Processing research has not yet examined whether the findings of conversational analysis for counseling conducted in English apply to other languages. This paper presents a first step towards this direction. We introduce MIDAS (Motivational Interviewing Dataset in Spanish), a counseling dataset created from public video sources that contains expert annotations for counseling reflections and questions. Using this dataset, we explore language-based differences in counselor behavior in English and Spanish and develop classifiers in monolingual and multilingual settings, demonstrating its applications in counselor behavioral coding tasks. | Aylin Gunal, Bowen Yi, John Piette, Rada Mihalcea, Verónica PérezRosas |  |
| 74 |  |  [Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes](https://doi.org/10.18653/v1/2025.naacl-short.74) |  | 0 | Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mitigation. | Isabel O. Gallegos, Ryan Aponte, Ryan A. Rossi, Joe Barrow, Md. Mehrab Tanjim, Tong Yu, Hanieh Deilamsalehy, Ruiyi Zhang, Sungchul Kim, Franck Dernoncourt, Nedim Lipka, Deonna M. Owens, Jiuxiang Gu |  |
| 75 |  |  [EqualizeIR: Mitigating Linguistic Biases in Retrieval Models](https://doi.org/10.18653/v1/2025.naacl-short.75) |  | 0 | This study finds that existing information retrieval (IR) models show significant biases based on the linguistic complexity of input queries, performing well on linguistically simpler (or more complex) queries while underperforming on linguistically more complex (or simpler) queries.To address this issue, we propose EqualizeIR, a framework to mitigate linguistic biases in IR models. EqualizeIR uses a linguistically biased weak learner to capture linguistic biases in IR datasets and then trains a robust model by regularizing and refining its predictions using the biased weak learner. This approach effectively prevents the robust model from overfitting to specific linguistic patterns in data. We propose four approaches for developing linguistically-biased models. Extensive experiments on several datasets show that our method reduces performance disparities across linguistically simple and complex queries, while improving overall retrieval performance. | Jiali Cheng, Hadi Amiri |  |
| 76 |  |  [Do Audio-Language Models Understand Linguistic Variations?](https://doi.org/10.18653/v1/2025.naacl-short.76) |  | 0 | Open-vocabulary audio language models (ALMs), like Contrastive Language Audio Pretraining (CLAP), represent a promising new paradigm for audio-text retrieval using natural language queries. In this paper, for the first time, we perform controlled experiments on various benchmarks to show that existing ALMs struggle to generalize to linguistic variations in textual queries. To address this issue, we propose RobustCLAP, a novel and compute-efficient technique to learn audio-language representations agnostic to linguistic variations. Specifically, we reformulate the contrastive loss used in CLAP architectures by introducing a multi-view contrastive learning objective, where paraphrases are treated as different views of the same audio scene and use this for training. Our proposed approach improves the text-to-audio retrieval performance of CLAP by 0.8%-13% across benchmarks and enhances robustness to linguistic variation. We make our code publicly available | Ramaneswaran Selvakumar, Sonal Kumar, Hemant Kumar Giri, Nishit Anand, Ashish Seth, Sreyan Ghosh, Dinesh Manocha |  |
| 77 |  |  [Giving the Old a Fresh Spin: Quality Estimation-Assisted Constrained Decoding for Automatic Post-Editing](https://doi.org/10.18653/v1/2025.naacl-short.77) |  | 0 | Automatic Post-Editing (APE) systems often struggle with over-correction, where unnecessary modifications are made to a translation, diverging from the principle of minimal editing. In this paper, we propose a novel technique to mitigate over-correction by incorporating word-level Quality Estimation (QE) information during the decoding process. This method is architecture-agnostic, making it adaptable to any APE system, regardless of the underlying model or training approach. Our experiments on English-German, English-Hindi, and English-Marathi language pairs show the proposed approach yields significant improvements over their corresponding baseline APE systems, with TER gains of 0.65, 1.86, and 1.44 points, respectively. These results underscore the complementary relationship between QE and APE tasks and highlight the effectiveness of integrating QE information to reduce over-correction in APE systems. | Sourabh Dattatray Deoghare, Diptesh Kanojia, Pushpak Bhattacharyya |  |
| 78 |  |  [RuleR: Improving LLM Controllability by Rule-based Data Recycling](https://doi.org/10.18653/v1/2025.naacl-short.78) |  | 0 | Large language models (LLMs) still lack delicate controllability over their responses, which is critical to enhancing their performance and the user experience. However, curating supervised fine-tuning (SFT) datasets to improve LLM controllability usually relies on human experts or proprietary LLMs, which requires additional costs. To bridge this gap, we propose Rule-based Data Recycling (RuleR), a data augmentation method incorporating multiple constraints into the original data samples according to predefined rules, which creates new training tasks to consolidate the controllability of LLMs. Instead of creating new data from scratch, RuleR “recycles” existing data by simply applying rule-based edits to their responses and appending the rule-instructions in their original instructions. Experimental results demonstrate RuleR’s effectiveness in improving LLM controllability while maintaining general instruction-following capabilities. | Ming Li, Han Chen, Chenguang Wang, Dang Nguyen, Dianqi Li, Tianyi Zhou |  |
| 79 |  |  [MixRevDetect: Towards Detecting AI-Generated Content in Hybrid Peer Reviews](https://doi.org/10.18653/v1/2025.naacl-short.79) |  | 0 | The growing use of large language models (LLMs) in academic peer review poses significant challenges, particularly in distinguishing AI-generated content from human-written feedback. This research addresses the problem of identifying AI-generated peer review comments, which are crucial to maintaining the integrity of scholarly evaluation. Prior research has primarily focused on generic AI-generated text detection or on estimating the fraction of peer reviews that may be AI-generated, often treating reviews as monolithic units. However, these methods fail to detect finer-grained AI-generated points within mixed-authorship reviews. To address this gap, we propose MixRevDetect, a novel method to identify AI-generated points in peer reviews. Our approach achieved an F1 score of 88.86%, significantly outperforming existing AI text detection methods. | Sandeep Kumar, Samarth Garg, Sagnik Sengupta, Tirthankar Ghosal, Asif Ekbal |  |
| 80 |  |  [DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie Character-Aware Discourse Graph](https://doi.org/10.18653/v1/2025.naacl-short.80) |  | 0 | Summarizing movie screenplays presents a unique set of challenges compared to standard document summarization. Screenplays are not only lengthy, but also feature a complex interplay of characters, dialogues, and scenes, with numerous direct and subtle relationships and contextual nuances that are difficult for machine learning models to accurately capture and comprehend. Recent attempts at screenplay summarization focus on fine-tuning transformer-based pre-trained models, but these models often fall short in capturing long-term dependencies and latent relationships, and frequently encounter the “lost in the middle” issue. To address these challenges, we introduce DiscoGraMS, a novel resource that represents movie scripts as a movie character-aware discourse graph (CaD Graph). This approach is well-suited for various downstream tasks, such as summarization, question-answering, and salience detection. The model aims to preserve all salient information, offering a more comprehensive and faithful representation of the screenplay’s content. We further explore a baseline method that combines the CaD Graph with the corresponding movie script through a late fusion of graph and text modalities, and we present very initial promising results. We have made our code and dataset publicly available. | Maitreya Prafulla Chitale, Uday Bindal, Rajakrishnan Rajkumar, Rahul Mishra |  |
| 81 |  |  [Capturing Human Cognitive Styles with Language: Towards an Experimental Evaluation Paradigm](https://doi.org/10.18653/v1/2025.naacl-short.81) |  | 0 | While NLP models often seek to capture cognitive states via language, the validity of predicted states is determined by comparing them to annotations created without access the cognitive states of the authors. In behavioral sciences, cognitive states are instead measured via experiments. Here, we introduce an experiment-based framework for evaluating language-based cognitive style models against human behavior. We explore the phenomenon of decision making, and its relationship to the linguistic style of an individual talking about a recent decision they made. The participants then follow a classical decision-making experiment that captures their cognitive style, determined by how preferences change during a decision exercise. We find that language features, intended to capture cognitive style, can predict participants’ decision style with moderate-to-high accuracy (AUC 0.8), demonstrating that cognitive style can be partly captured and revealed by discourse patterns. | Vasudha Varadarajan, Syeda Mahwish, Xiaoran Liu, Julia Buffolino, Christian C. Luhmann, Ryan L. Boyd, H. Andrew Schwartz |  |
| 82 |  |  [Understanding Figurative Meaning through Explainable Visual Entailment](https://doi.org/10.18653/v1/2025.naacl-long.1) |  | 0 | Large Vision-Language Models (VLMs) have demonstrated strong capabilities in tasks requiring a fine-grained understanding of literal meaning in images and text, such as visual question-answering or visual entailment. However, there has been little exploration of the capabilities of these models when presented with images and captions containing figurative meaning, such as metaphors or humor. To close this gap, we propose a new task framing the figurative meaning understanding problem as an explainable visual entailment task, where the model has to predict whether the image (premise) entails a caption (hypothesis) and justify the predicted label with a textual explanation. The figurative phenomena can be present in the image, in the caption, or both. Using a human-AI collaboration approach, we build the accompanying expert-verified dataset V-FLUTE, containing 6,027 image, caption, label, explanation instances spanning five diverse figurative phenomena: metaphors, similes, idioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs struggle to generalize from literal to figurative meaning, particularly when it is present in images. Further, we identify common types of errors in VLM reasoning (hallucination and incomplete or unsound reasoning) across classes of models via human evaluation. | Arkadiy Saakyan, Shreyas Kulkarni, Tuhin Chakrabarty, Smaranda Muresan |  |
| 83 |  |  [Benchmarking Distributional Alignment of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.2) |  | 0 | Language models (LMs) are increasingly used as simulacra for people, yet their ability to match the distribution of views of a specific demographic group and be distributionally aligned remains uncertain. This notion of distributional alignment is complex, as there is significant variation in the types of attributes that are simulated. Prior works have underexplored the role of three critical variables—the question domain, steering method, and distribution expression method—which motivates our contribution of a benchmark explicitly addressing these dimensions. We construct a dataset expanding beyond political values, create human baselines for this task, and evaluate the extent to which an LM can align with a particular group’s opinion distribution to inform design choices of such simulation systems. Our analysis reveals open problems regarding if, and how, LMs can be used to simulate humans, and that LLMs can more accurately describe the opinion distribution than simulate such distributions. | Nicole Meister, Carlos Guestrin, Tatsunori Hashimoto |  |
| 84 |  |  [World Models with Hints of Large Language Models for Goal Achieving](https://doi.org/10.18653/v1/2025.naacl-long.3) |  | 0 | Reinforcement learning struggles in the face of long-horizon tasks and sparse goals due to the difficulty in manual reward specification. While existing methods address this by adding intrinsic rewards, they may fail to provide meaningful guidance in long-horizon decision-making tasks with large state and action spaces, lacking purposeful exploration. Inspired by human cognition, we propose a new multi-modal model-based RL approach named Dreaming with Large Language Models (DLLM). DLLM integrates the proposed hinting subgoals from the LLMs into the model rollouts to encourage goal discovery and reaching in challenging tasks. By assigning higher intrinsic rewards to samples that align with the hints outlined by the language model during model rollouts, DLLM guides the agent toward meaningful and efficient exploration. Extensive experiments demonstrate that the DLLM outperforms recent methods in various challenging, sparse-reward environments such as HomeGrid, Crafter, and Minecraft by 41.8%, 21.1%, and 9.9%, respectively. | Zeyuan Liu, Ziyu Huan, Xiyao Wang, Jiafei Lyu, Jian Tao, Xiu Li, Furong Huang, Huazhe Xu |  |
| 85 |  |  [CogLM: Tracking Cognitive Development of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.4) |  | 0 | Piaget’s Theory of Cognitive Development (PTC) posits that the development of cognitive levels forms the foundation for human learning across various abilities. As Large Language Models (LLMs) have recently shown remarkable abilities across a wide variety of tasks, we are curious about the cognitive levels of current LLMs: to what extent they have developed and how this development has been achieved. To this end, we construct a benchmark CogLM (Cognitive Ability Evaluation for Language Model) based on PTC to assess the cognitive levels of LLMs. CogLM comprises 1,220 questions spanning 10 cognitive abilities crafted by more than 20 human experts, providing a comprehensive testbed for the cognitive levels of LLMs. Through extensive experiments across multiple mainstream LLMs with CogLM, we find that: (1) In our testing framework, advanced LLMs (such as GPT-4) have demonstrated human-like cognitive abilities, comparable to those of a 20-year-old human. (2) The parameter size and optimization objective are two key factors affecting the cognitive levels of LLMs. (3) The performance on downstream tasks is positively correlated with the level of cognitive abilities. These findings fill the gap in research on the cognitive abilities of LLMs, tracing the development of LLMs from a cognitive perspective and guiding the future direction of their evolution. | Xinglin Wang, Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Boyuan Pan, Heda Wang, Yao Hu, Kan Li |  |
| 86 |  |  [Improving and Assessing the Fidelity of Large Language Models Alignment to Online Communities](https://doi.org/10.18653/v1/2025.naacl-long.5) |  | 0 | Large language models (LLMs) have shown promise in representing individuals and communities, offering new ways to study complex social dynamics. However, effectively aligning LLMs with specific human groups and systematically assessing the fidelity of the alignment remains a challenge. This paper presents a robust framework for aligning LLMs with online communities via instruction-tuning and comprehensively evaluating alignment across various aspects of language, including authenticity, emotional tone, toxicity, and harm. We demonstrate the utility of our approach by applying it to online communities centered on dieting and body image. We administer an eating disorder psychometric test to the aligned LLMs to reveal unhealthy beliefs and successfully differentiate communities with varying levels of eating disorder risk. Our results highlight the potential of LLMs in automated moderation and broader applications in public health and social science research. | Minh Duc Chu, Zihao He, Rebecca Dorn, Kristina Lerman |  |
| 87 |  |  [Improving Retrospective Language Agents via Joint Policy Gradient Optimization](https://doi.org/10.18653/v1/2025.naacl-long.6) |  | 0 | In recent research advancements within the community, large language models (LLMs) have sparked great interest in creating autonomous agents. However, current prompt-based agents often heavily rely on large-scale LLMs. Meanwhile, although fine-tuning methods significantly enhance the capabilities of smaller LLMs, the fine-tuned agents often lack the potential for self-reflection and self-improvement. To address these challenges, we introduce a novel agent framework named RetroAct, which is a framework that jointly optimizes both task-planning and self-reflective evolution capabilities in language agents. Specifically, we develop a two-stage joint optimization process that integrates imitation learning and reinforcement learning, and design an off-policy joint policy gradient optimization algorithm with imitation learning regularization to enhance the data efficiency and training stability in agent tasks. RetroAct significantly improves the performance of open-source models, reduces dependency on closed-source LLMs, and enables fine-tuned agents to learn and evolve continuously. We conduct extensive experiments across various testing environments, demonstrating RetroAct has substantial improvements in task performance and decision-making processes. | Xueyang Feng, Bo Lan, Quanyu Dai, Lei Wang, Jiakai Tang, Xu Chen, Zhenhua Dong, JiRong Wen |  |
| 88 |  |  [CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases](https://doi.org/10.18653/v1/2025.naacl-long.7) |  | 0 | Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval and MBPP, but struggle with handling entire code repositories. This challenge has prompted research on enhancing LLM-codebase interaction at a repository scale. Current solutions rely on similarity-based retrieval or manual tools and APIs, each with notable drawbacks. Similarity-based retrieval often has low recall in complex tasks, while manual tools and APIs are typically task-specific and require expert knowledge, reducing their generalizability across diverse code tasks and real-world applications. To mitigate these limitations, we introduce CodexGraph, a system that integrates LLM agents with graph database interfaces extracted from code repositories. By leveraging the structural properties of graph databases and the flexibility of the graph query language, CodexGraph enables the LLM agent to construct and execute queries, allowing for precise, code structure-aware context retrieval and code navigation. We assess CodexGraph using three benchmarks: CrossCodeEval, SWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding applications. With a unified graph database schema, CodexGraph demonstrates competitive performance and potential in both academic and real-world environments, showcasing its versatility and efficacy in software engineering. Our code and demo will be released soon. | Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Fei Wang, Michael Qizhe Shieh, Wenmeng Zhou |  |
| 89 |  |  [Instantly Learning Preference Alignment via In-context DPO](https://doi.org/10.18653/v1/2025.naacl-long.8) |  | 0 | Human Preference Alignment (HPA) can assist large language models (LLMs) to generate safe content. Due to the heavy cost of fine-tuning, tuning-free methods have emerged, typically modifying LLM decoding via post-processing. In this paper, we propose a novel and effective approach for HPA in a tuning-free way, named In-Context Direct Preference Optimization (ICDPO). We first rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the LLM before and after ICL. It enables LLMs to both generate and select the well-aligned response, which is precisely estimated by the aforementioned instant scorer, thereby enhancing the final performance. ICDPO can be further enhanced with a two-stage retriever and an upgraded scorer. Extensive experiments show its effectiveness, particularly in outperforming multiple tuning-free baselines, even competitiveness with SFT and DPO. We also conduct detailed analyses to offer comprehensive insights into ICDPO. | Feifan Song, Yuxuan Fan, Xin Zhang, Peiyi Wang, Houfeng Wang |  |
| 90 |  |  [ALTER: Augmentation for Large-Table-Based Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.9) |  | 0 |  | Han Zhang, Yuheng Ma, Hanfang Yang |  |
| 91 |  |  [What the #?\*!: Disentangling Hate Across Target Identities](https://doi.org/10.18653/v1/2025.naacl-long.10) |  | 0 | Hate speech (HS) classifiers do not perform equally well in detecting hateful expressions towards different target identities. They also demonstrate systematic biases in predicted hatefulness scores. Tapping on two recently proposed functionality test datasets for HS detection, we quantitatively analyze the impact of different factors on HS prediction. Experiments on popular industrial and academic models demonstrate that HS detectors assign a higher hatefulness score merely based on the mention of specific target identities. Besides, models often confuse hatefulness and the polarity of emotions. This result is worrisome as the effort to build HS detectors might harm the vulnerable identity groups we wish to protect: posts expressing anger or disapproval of hate expressions might be flagged as hateful themselves. We also carry out a study inspired by social psychology theory, which reveals that the accuracy of hatefulness prediction correlates strongly with the intensity of the stereotype. | Yiping Jin, Leo Wanner, Aneesh Moideen Koya |  |
| 92 |  |  [MAD Speech: Measures of Acoustic Diversity of Speech](https://doi.org/10.18653/v1/2025.naacl-long.11) |  | 0 | Generative spoken language models produce speech in a wide range of voices, prosody, and recording conditions, seemingly approaching the diversity of natural speech. However, the extent to which generated speech is acoustically diverse remains unclear due to a lack of appropriate metrics. We address this gap by developing lightweight metrics of acoustic diversity, which we collectively refer to as MAD Speech. We focus on measuring five facets of acoustic diversity: voice, gender, emotion, accent, and background noise. We construct the metrics as a composition of specialized, per-facet embedding models and an aggregation function that measures diversity within the embedding space. Next, we build a series of datasets with a priori known diversity preferences for each facet. Using these datasets, we demonstrate that our proposed metrics achieve a stronger agreement with the ground-truth diversity than baselines. Finally, we showcase the applicability of our proposed metrics across several real-life evaluation scenarios. MAD Speech is made publicly available. | Matthieu Futeral, Andrea Agostinelli, Marco Tagliasacchi, Neil Zeghidour, Eugene Kharitonov |  |
| 93 |  |  [The Russian-focused embedders' exploration: ruMTEB benchmark and Russian embedding model design](https://doi.org/10.18653/v1/2025.naacl-long.12) |  | 0 | Embedding models play a crucial role in Natural Language Processing (NLP) by creating text embeddings used in various tasks such as information retrieval and assessing semantic text similarity. This paper focuses on research related to embedding models in the Russian language. It introduces a new Russian-focused embedding model called ru-en-RoSBERTa and the ruMTEB benchmark, the Russian version extending the Massive Text Embedding Benchmark (MTEB). Our benchmark includes seven categories of tasks, such as semantic textual similarity, text classification, reranking, and retrieval.The research also assesses a representative set of Russian and multilingual models on the proposed benchmark. The findings indicate that the new model achieves results that are on par with state-of-the-art models in Russian. We release the model ru-en-RoSBERTa, and the ruMTEB framework comes with open-source code, integration into the original framework and a public leaderboard. | Artem Snegirev, Maria Tikhonova, Anna Maksimova, Alena Fenogenova, Aleksandr Abramov |  |
| 94 |  |  [PRACTIQ: A Practical Conversational Text-to-SQL dataset with Ambiguous and Unanswerable Queries](https://doi.org/10.18653/v1/2025.naacl-long.13) |  | 0 | Previous text-to-SQL datasets and systems have primarily focused on user questions with clear intentions that can be answered. However, real user questions can often be ambiguous with multiple interpretations or unanswerable due to a lack of relevant data. In this work, we construct a practical conversational text-to-SQL dataset called PRACTIQ, consisting of ambiguous and unanswerable questions inspired by real-world user questions. We first identified four categories of ambiguous questions and four categories of unanswerable questions by studying existing text-to-SQL datasets. Then, we generate conversations with four turns: the initial user question, an assistant response seeking clarification, the user’s clarification, and the assistant’s clarified SQL response with the natural language explanation of the execution results. For some ambiguous queries, we also directly generate helpful SQL responses, that consider multiple aspects of ambiguity, instead of requesting user clarification. To benchmark the performance on ambiguous, unanswerable, and answerable questions, we implemented large language model (LLM)-based baselines using various LLMs. Our approach involves two steps: question category classification and clarification SQL prediction. Our experiments reveal that state-of-the-art systems struggle to handle ambiguous and unanswerable questions effectively. We release our code for data generation and experiments on GitHub. | Mingwen Dong, Nischal Ashok Kumar, Yiqun Hu, Anuj Chauhan, ChungWei Hang, Shuaichen Chang, Lin Pan, Wuwei Lan, Henghui Zhu, Jiarong Jiang, Patrick Ng, Zhiguo Wang |  |
| 95 |  |  [MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems](https://doi.org/10.18653/v1/2025.naacl-long.14) |  | 0 | Traditional retrieval-augmented generation (RAG) benchmarks evaluate systems using heuristic-based metrics, but these require human preferences as the ground truth for reference. In contrast, arena-based benchmarks, where systems compete against each other, require an expensive large language model (LLM) as a judge for a reliable evaluation. We present a simple efficient technique to combine the best of both worlds. The idea is to train a surrogate judge using heuristic metrics as input, to output the LLM as a judge prediction.In our work, we develop MIRAGE-Bench, a synthetic arena-based RAG benchmark for 18 diverse languages on Wikipedia focused on multilingual answer generation evaluation. It extensively couples both heuristic features and LLM as a judge for evaluation. We benchmark 19 multilingual LLMs, and observe a high correlation (Kendall Tau (𝜏) = 0.909) using our surrogate judge and between GPT-4o as a teacher using the Bradley-Terry framework. Our results show proprietary and large open-source LLMs currently dominate on MIRAGE-Bench. Our code and datasets are made publicly available here: https://github.com/vectara/mirage-bench. | Nandan Thakur, Suleman Kazi, Ge Luo, Jimmy Lin, Amin Ahmad |  |
| 96 |  |  [LLMs Are Biased Towards Output Formats! Systematically Evaluating and Mitigating Output Format Bias of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.15) |  | 0 | We present the first systematic evaluation examining format bias in performance of large language models (LLMs). Our approach distinguishes between two categories of an evaluation metric under format constraints to reliably and accurately assess performance: one measures performance when format constraints are adhered to, while the other evaluates performance regardless of constraint adherence. We then define a metric for measuring the format bias of LLMs and establish effective strategies to reduce it. Subsequently, we present our empirical format bias evaluation spanning four commonly used categories—multiple-choice question-answer, wrapping, list, and mapping—covering 15 widely-used formats. Our evaluation on eight generation tasks uncovers significant format bias across state-of-the-art LLMs. We further discover that improving the format-instruction following capabilities of LLMs across formats potentially reduces format bias. Based on our evaluation findings, we study prompting and fine-tuning with synthesized format data techniques to mitigate format bias. Our methods successfully reduce the variance in ChatGPT’s performance among wrapping formats from 235.33 to 0.71 (%^2) | Do Xuan Long, NgocHai Nguyen, Tiviatis Sim, Hieu Dao, Shafiq Joty, Kenji Kawaguchi, Nancy F. Chen, MinYen Kan |  |
| 97 |  |  [The Impact of Visual Information in Chinese Characters: Evaluating Large Models' Ability to Recognize and Utilize Radicals](https://doi.org/10.18653/v1/2025.naacl-long.16) |  | 0 | The glyphic writing system of Chinese incorporates information-rich visual features in each character, such as radicals that provide hints about meaning or pronunciation. However, there has been no investigation into whether contemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can harness these sub-character features in Chinese through prompting. In this study, we establish a benchmark to evaluate LLMs’ and VLMs’ understanding of visual elements in Chinese characters, including radicals, composition structures, strokes, and stroke counts. Our results reveal that models surprisingly exhibit some, but still limited, knowledge of the visual information, regardless of whether images of characters are provided. To incite models’ ability to use radicals, we further experiment with incorporating radicals into the prompts for Chinese language processing (CLP) tasks. We observe consistent improvement in Part-Of-Speech tagging when providing additional information about radicals, suggesting the potential to enhance CLP by integrating sub-character information. | Xiaofeng Wu, Karl Stratos, Wei Xu |  |
| 98 |  |  [PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic Languages with Example Selection from related Example Banks](https://doi.org/10.18653/v1/2025.naacl-long.17) |  | 0 | Large Language Models (LLMs) have recently demonstrated impressive few-shot learning capabilities through in-context learning (ICL). However, ICL performance is highly dependent on the choice of few-shot demonstrations, making the selection of the most optimal examples a persistent research challenge. This issue is further amplified in low-resource Indic languages, where the scarcity of ground-truth data complicates the selection process. In this work, we propose PromptRefine, a novel Alternating Minimization approach for example selection that improves ICL performance on low-resource Indic languages. PromptRefine leverages auxiliary example banks from related high-resource Indic languages and employs multi-task learning techniques to align language-specific retrievers, enabling effective cross-language retrieval. Additionally, we incorporate diversity in the selected examples to enhance generalization and reduce bias. Through comprehensive evaluations on four text generation tasks—Cross-Lingual Question Answering, Multilingual Question Answering, Machine Translation, and Cross-Lingual Summarization using state-of-the-art LLMs such as LLAMA-3.1-8B, LLAMA-2-7B, Qwen-2-7B, and Qwen-2.5-7B, we demonstrate that PromptRefine significantly outperforms existing frameworks for retrieving examples. | Soumya Suvra Ghosal, Soumyabrata Pal, Koyel Mukherjee, Dinesh Manocha |  |
| 99 |  |  [Unlocking Decoding-time Controllability: Gradient-Free Multi-Objective Alignment with Contrastive Prompts](https://doi.org/10.18653/v1/2025.naacl-long.18) |  | 0 | The task of multi-objective alignment aims at balancing and controlling the different alignment objectives, e.g., helpfulness, harmlessness and honesty) of large language models to meet the personalized requirements of different users. However, previous methods tend to train multiple models to deal with various user preferences, with the number of trained models growing linearly with the number of alignment objectives and the number of different preferences. Meanwhile, existing methods are generally poor in extensibility and require significant re-training for each new alignment objective considered. Considering the limitation of previous approaches, we propose MCA, which constructs an expert prompt and an adversarial prompt for each objective to contrast at the decoding time and balances the objectives through combining the contrast. Our approach is verified to be superior to previous methods in obtaining a well-distributed Pareto front among different alignment objectives. | Tingchen Fu, Yupeng Hou, Julian J. McAuley, Rui Yan |  |
| 100 |  |  [Fingerspelling within Sign Language Translation](https://doi.org/10.18653/v1/2025.naacl-long.19) |  | 0 |  | Garrett Tanzer |  |
| 101 |  |  [MoDS: Moderating a Mixture of Document Speakers to Summarize Debatable Queries in Document Collections](https://doi.org/10.18653/v1/2025.naacl-long.20) |  | 0 | Query-focused summarization (QFS) gives a summary of documents to answer a query.Past QFS work assumes queries have one answer, ignoring debatable ones (\*Is law school worth it?\*).We introduce \*\*Debatable QFS (DQFS)\*\*, a task to create summaries that answer debatable queries via documents with opposing perspectives; summaries must \*comprehensively cover\* all sources and \*balance perspectives\*, favoring no side.These goals elude LLM QFS systems, which: 1) lack structured content plans, failing to guide LLMs to write balanced summaries, and 2) employ the same query to retrieve contexts across documents, failing to cover all perspectives specific to each document’s content.To overcome this, we design MoDS, a multi-LLM framework mirroring human panel discussions.MoDS treats documents as individual Speaker LLMs and has a Moderator LLM that picks speakers to respond to tailored queries for planned topics.Speakers use tailored queries to retrieve relevant contexts from their documents and supply perspectives, which are tracked in a rich outline, yielding a content plan to guide the final summary.Experiments on ConflictingQA with controversial web queries and DebateQFS, our new dataset of debate queries from Debatepedia, show MoDS beats SOTA by 38-59% in topic paragraph coverage and balance, based on new citation metrics. Users also find MoDS’s summaries to be readable and more balanced. | Nishant Balepur, Alexa F. Siu, Nedim Lipka, Franck Dernoncourt, Tong Sun, Jordan Lee BoydGraber, Puneet Mathur |  |
| 102 |  |  [Aligning Sentence Simplification with ESL Learner's Proficiency for Language Acquisition](https://doi.org/10.18653/v1/2025.naacl-long.21) |  | 0 | Text simplification is crucial for improving accessibility and comprehension for English as a Second Language (ESL) learners. This study goes a step further and aims to facilitate ESL learners’ language acquisition by simplification. Specifically, we propose simplifying complex sentences to appropriate levels for learners while also increasing vocabulary coverage of the target level in the simplifications. We achieve this without a parallel corpus by conducting reinforcement learning on a large language model. Our method employs token-level and sentence-level rewards, and iteratively trains the model on its self-generated outputs to guide the model to search for simplification hypotheses that satisfy the target attributes. Experiment results on CEFR-SP and TurkCorpus datasets show that the proposed method can effectively increase the frequency and diversity of vocabulary of the target level by more than 20% compared to baseline models, while maintaining high simplification quality. | Guanlin Li, Yuki Arase, Noël Crespi |  |
| 103 |  |  [PeerQA: A Scientific Question Answering Dataset from Peer Reviews](https://doi.org/10.18653/v1/2025.naacl-long.22) |  | 0 | We present PeerQA, a real-world, scientific, document-level Question Answering (QA) dataset. PeerQA questions have been sourced from peer reviews, which contain questions that reviewers raised while thoroughly examining the scientific article. Answers have been annotated by the original authors of each paper. The dataset contains 579 QA pairs from 208 academic articles, with a majority from ML and NLP, as well as a subset of other scientific communities like Geoscience and Public Health.PeerQA supports three critical tasks for developing practical QA systems: Evidence retrieval, unanswerable question classification, and answer generation. We provide a detailed analysis of the collected dataset and conduct experiments establishing baseline systems for all three tasks. Our experiments and analyses reveal the need for decontextualization in document-level retrieval, where we find that even simple decontextualization approaches consistently improve retrieval performance across architectures. On answer generation, PeerQA serves as a challenging benchmark for long-context modeling, as the papers have an average size of 12k tokens. | Tim Baumgärtner, Ted Briscoe, Iryna Gurevych |  |
| 104 |  |  [ALiiCE: Evaluating Positional Fine-grained Citation Generation](https://doi.org/10.18653/v1/2025.naacl-long.23) |  | 0 | Large Language Model (LLM) can enhance its credibility and verifiability by generating text with citations. However, existing research on citation generation is predominantly limited to sentence-level statements, neglecting the significance of positional fine-grained citations that can appear anywhere within sentences. To facilitate further exploration of the positional fine-grained citation generation, we propose ALiiCE, the first automatic evaluation framework for this task. Our method employs a dependency tree based approach to parse the sentence-level claim into atomic claims. Then ALiiCE evaluates citation quality using three metrics, including positional fine-grained citation recall, precision, and coefficient of variation of citation positions. We evaluate the positional fine-grained citation generation performance of several LLMs on long-form QA datasets. Our experiments and analyses demonstrate the effectiveness and reasonableness of ALiiCE. We offer our insights into the current advancements and future directions for the positional fine-grained citation generation task. | Yilong Xu, Jinhua Gao, Xiaoming Yu, Baolong Bi, Huawei Shen, Xueqi Cheng |  |
| 105 |  |  [An LLM-Based Approach for Insight Generation in Data Analysis](https://doi.org/10.18653/v1/2025.naacl-long.24) |  | 0 | Generating insightful and actionable information from databases is critical in data analysis. This paper introduces a novel approach using Large Language Models (LLMs) to automatically generate textual insights. Given a multi-table database as input, our method leverages LLMs to produce concise, text-based insights that reflect interesting patterns in the tables. Our framework includes a Hypothesis Generator to formulate domain-relevant questions, a Query Agent to answer such questions by generating SQL queries against a database, and a Summarization module to verbalize the insights. The insights are evaluated for both correctness and subjective insightfulness using a hybrid model of human judgment and automated metrics. Experimental results on public and enterprise databases demonstrate that our approach generates more insightful insights than other approaches while maintaining correctness. | Alberto Sánchez Pérez, Alaa Boukhary, Paolo Papotti, Luis Castejón Lozano, Adam Elwood |  |
| 106 |  |  [WebQuality: A Large-scale Multi-modal Web Page Quality Assessment Dataset with Multiple Scoring Dimensions](https://doi.org/10.18653/v1/2025.naacl-long.25) |  | 0 | The assessment of web page quality plays a critical role in a range of downstream applications, yet there is a notable absence of datasets for the evaluation of web page quality. This research presents the pioneering task of web page quality assessment and introduces the first comprehensive, multi-modal Chinese dataset named WebQuality specifically designed for this task. The dataset includes over 65,000 detailed an-notations spanning four sub-dimensions and incorporates elements such as HTML+CSS, text, and visual screenshot, facilitating in-depth modeling and assessment of web page quality. We performed evaluations using a variety of baseline models to demonstrate the complexity of the task. Additionally, we propose Hydra, an integrated multi-modal analysis model, and rigorously assess its performance and limitations through extensive ablation studies. To advance the field of web quality assessment, we offer unrestricted access to our dataset and codebase for the research community, available at https://github.com/incredible-smurf/WebQuality | Tao Zhang, Yige Wang, ZhuHangyu ZhuHangyu, Li Xin, Chen Xiang, Tian Hua Zhou, Jin Ma |  |
| 107 |  |  [UFO: A UI-Focused Agent for Windows OS Interaction](https://doi.org/10.18653/v1/2025.naacl-long.26) |  | 0 | We introduce UFO, a UI-Fcused agent designed to fulfill user requests tailored to Windows OS applications by observing and analyzing the GUI and control information of these applications. UFO utilizes a hierarchical dual-agent framework that decomposes user requests using a divide-and-conquer approach, enabling seamless navigation and addressing sub-tasks across multiple applications. It also incorporates a control interaction module tailored for Windows OS, which detects control elements effectively and allows for fully automated execution. As a result, UFO simplifies complex and time-consuming processes into tasks that can be completed with natural language commands.We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios. The results derived from both quantitative metrics and real-case studies, underscore the superior effectiveness of UFOin fulfilling user requests. To the best of our knowledge, UFO stands as the first UI agent specifically tailored for task completion within the Windows OS. | Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang |  |
| 108 |  |  [Is your benchmark truly adversarial? AdvScore: Evaluating Human-Grounded Adversarialness](https://doi.org/10.18653/v1/2025.naacl-long.27) |  | 0 | Adversarial datasets should validate AI robustness by providing samples on which humans perform well, but models do not. However, as models evolve, datasets can become obsolete. Measuring whether a dataset remains adversarial is hindered by the lack of a standardized metric for measuring adversarialness. We propose ADVSCORE, a human-grounded evaluation metric that assesses a dataset’s adversarialness by capturing models’ and humans’ varying abilities, while also identifying poor examples. We then use ADVSCORE to motivate a new dataset creation pipeline for realistic and high-quality adversarial samples, enabling us to collect an adversarial question answering (QA) dataset, ADVQA. We apply ADVSCORE using 9,347 human responses and ten language models’ predictions to track model improvement over five years (2020–2024). ADVSCORE thus provides guidance for achieving robustness comparable with human capabilities. Furthermore, it helps determine to what extent adversarial datasets continue to pose challenges, ensuring that, rather than reflecting outdated or overly artificial difficulties, they effectively test model capabilities. | Yoo Yeon Sung, Maharshi Gor, Eve Fleisig, Ishani Mondal, Jordan Lee BoydGraber |  |
| 109 |  |  [Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation](https://doi.org/10.18653/v1/2025.naacl-long.28) |  | 0 | Multimodal foundation models hold significant potential for automating radiology report generation, thereby assisting clinicians in diagnosing cardiac diseases. However, generated reports often suffer from serious factual inaccuracy. In this paper, we introduce a fact-aware multimodal retrieval-augmented pipeline in generating accurate radiology reports (FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then integrate factual knowledge to train a universal multimodal retriever. Given a radiology image, our retriever can identify high-quality reference reports to augment multimodal foundation models, thus enhancing the factual completeness and correctness of report generation. Experiments on two benchmark datasets demonstrate that our multimodal retriever significantly outperforms other state-of-the-art retrievers on both language generation and radiology-specific metrics, up to 6.5% and 2% score in F1CheXbert and F1RadGraph. Further analysis indicates that employing our factually-informed training strategy imposes an effective supervision signal, without relying on explicit diagnostic label guidance, and successfully propagate fact-aware capabilities from the multimodal retriever to the multimodal foundation model in radiology report generation. | Liwen Sun, James (Jialun) Zhao, Wenjing Han, Chenyan Xiong |  |
| 110 |  |  [On Behalf of the Stakeholders: Trends in NLP Model Interpretability in the Era of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.29) |  | 0 | Recent advancements in NLP systems, particularly with the introduction of LLMs, have led to widespread adoption of these systems by a broad spectrum of users across various domains, impacting decision-making, the job market, society, and scientific research. This surge in usage has led to an explosion in NLP model interpretability and analysis research, accompanied by numerous technical surveys. Yet, these surveys often overlook the needs and perspectives of explanation stakeholders. In this paper, we address three fundamental questions: Why do we need interpretability, what are we interpreting, and how? By exploring these questions, we examine existing interpretability paradigms, their properties, and their relevance to different stakeholders. We further explore the practical implications of these paradigms by analyzing trends from the past decade across multiple research fields. To this end, we retrieved thousands of papers and employed an LLM to characterize them. Our analysis reveals significant disparities between NLP developers and non-developer users, as well as between research fields, underscoring the diverse needs of stakeholders. For example, explanations of internal model components are rarely used outside the NLP field. We hope this paper informs the future design, development, and application of methods that align with the objectives and requirements of various stakeholders. | Nitay Calderon, Roi Reichart |  |
| 111 |  |  [Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward](https://doi.org/10.18653/v1/2025.naacl-long.30) |  | 0 |  | Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander G. Hauptmann, Yonatan Bisk, Yiming Yang |  |
| 112 |  |  [FlexiGPT: Pruning and Extending Large Language Models with Low-Rank Weight Sharing](https://doi.org/10.18653/v1/2025.naacl-long.31) |  | 0 | The rapid proliferation of large language models (LLMs) in natural language processing (NLP) has created a critical need for techniques that enable efficient deployment on memory-constrained devices without compromising performance. We present a method to prune LLMs that selectively prunes model blocks based on an importance score and replaces them with a low-parameter replacement strategy. Specifically, we propose a principled metric to replace each pruned block using a weight-sharing mechanism that leverages unpruned counterparts from the model and block-specific low-rank adapters. Furthermore, we facilitate the learning of these replacement blocks with output feature normalization and an adapter initialization scheme built on low-rank SVD reconstructions. Empirical evaluations demonstrate substantial performance gains over existing methods, achieving state-of-the-art performance on 5/6 benchmarks for a compression rate of 30% and 6/6 benchmarks for a compression rate of 40%. We also demonstrate that our approach can extend smaller models, boosting performance on 6/6 benchmarks using only ~0.3% tokens of extended training with minimal additional parameter costs. | James Seale Smith, ChiHeng Lin, Shikhar Tuli, Haris Jeelani, Shangqian Gao, Yilin Shen, Hongxia Jin, YenChang Hsu |  |
| 113 |  |  [Conformalized Answer Set Prediction for Knowledge Graph Embedding](https://doi.org/10.18653/v1/2025.naacl-long.32) |  | 0 | Knowledge graph embeddings (KGE) apply machine learning methods on knowledge graphs (KGs) to provide non-classical reasoning capabilities based on similarities and analogies. The learned KG embeddings are typically used to answer queries by ranking all potential answers, but rankings often lack a meaningful probabilistic interpretation - lower-ranked answers do not necessarily have a lower probability of being true. This limitation makes it difficult to quantify uncertainty of model’s predictions, posing challenges for the application of KGE methods in high-stakes domains like medicine. We address this issue by applying the theory of conformal prediction that allows generating answer sets, which contain the correct answer with probabilistic guarantees. We explain how conformal prediction can be used to generate such answer sets for link prediction tasks. Our empirical evaluation on four benchmark datasets using six representative KGE methods validates that the generated answer sets satisfy the probabilistic guarantees given by the theory of conformal prediction. We also demonstrate that the generated answer sets often have a sensible size and that the size adapts well with respect to the difficulty of the query. | Yuqicheng Zhu, Nico Potyka, Jiarong Pan, Bo Xiong, Yunjie He, Evgeny Kharlamov, Steffen Staab |  |
| 114 |  |  [Parameter-free and Accessible Prompt Learning to Enhance Adversarial Robustness for Pre-trained Vision-Language Models](https://doi.org/10.18653/v1/2025.naacl-long.33) |  | 0 | Large pre-trained Vision-Language Models (VLMs) have revolutionized both computer vision and natural language processing. Despite their success, adversarial examples can still mislead VLMs into producing incorrect results. This work focuses on boosting the adversarial robustness of VLMs by searching for text prompts at the word level, rather than optimizing continuous textual embeddings. We introduce Parameter-Free Prompt Tuning (PFPT) to learn defense words that enhance resilience against adversarial attacks when appended to existing prompts, thereby offering ease of use due to the simplicity of this approach. These defense words are naturally present in the inherent vocabulary of VLMs, providing a human-readable property. PFPT employs a coarse-to-fine strategy with carefully designed optimization objectives to guide the word search. Extensive experiments demonstrate our method’s superiority over hand-engineered prompts and other state-of-the-art methods. PFPT significantly boosts accuracy and robustness, outperforming hand-engineered prompts with average gains of +4.9% and +5.8%, respectively (epsilon=1/255). | Xingran Zhou, Kun Yang, Changtao Miao, Bingyu Hu, Zhuoer Xu, Shiwen Cui, Changhua Meng, Dan Hong |  |
| 115 |  |  [Fine-grained Fallacy Detection with Human Label Variation](https://doi.org/10.18653/v1/2025.naacl-long.34) |  | 0 | We introduce FAINA, the first dataset for fallacy detection that embraces multiple plausible answers and natural disagreement. FAINA includes over 11K span-level annotations with overlaps across 20 fallacy types on social media posts in Italian about migration, climate change, and public health given by two expert annotators. Through an extensive annotation study that allowed discussion over multiple rounds, we minimize annotation errors whilst keeping signals of human label variation. Moreover, we devise a framework that goes beyond “single ground truth” evaluation and simultaneously accounts for multiple (equally reliable) test sets and the peculiarities of the task, i.e., partial span matches, overlaps, and the varying severity of labeling errors. Our experiments across four fallacy detection setups show that multi-task and multi-label transformer-based approaches are strong baselines across all settings. We release our data, code, and annotation guidelines to foster research on fallacy detection and human label variation more broadly. | Alan Ramponi, Agnese Daffara, Sara Tonelli |  |
| 116 |  |  [Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models](https://doi.org/10.18653/v1/2025.naacl-long.35) |  | 0 | Despite their wide adoption, the biases and unintended behaviors of language models remain poorly understood. In this paper, we identify and characterize a phenomenon never discussed before, which we call semantic leakage, where models leak irrelevant information from the prompt into the generation in unexpected ways. We propose an evaluation setting to detect semantic leakage both by humans and automatically, curate a diverse test suite for diagnosing this behavior, and measure significant semantic leakage in 13 flagship models. We also show that models exhibit semantic leakage in languages besides English and across different settings and generation scenarios. This discovery highlights yet another type of bias in language models that affects their generation patterns and behavior. | Hila Gonen, Terra Blevins, Alisa Liu, Luke Zettlemoyer, Noah A. Smith |  |
| 117 |  |  [SELFGOAL: Your Language Agents Already Know How to Achieve High-level Goals](https://doi.org/10.18653/v1/2025.naacl-long.36) |  | 0 | Language agents powered by large language models (LLMs) are increasingly valuable as decision-making tools in domains such as gaming and programming. However, these agents often face challenges in achieving high-level goals without detailed instructions and in adapting to environments where feedback is delayed. In this paper, we present SELFGOAL, a novel automatic approach designed to enhance agents’ capabilities to achieve high-level goals with limited human prior and environmental feedback. The core concept of SELFGOAL involves adaptively breaking down a high-level goal into a tree structure of more practical subgoals during the interaction with environments while identifying the most useful subgoals and progressively updating this structure. Experimental results demonstrate that SELFGOAL significantly enhances the performance of language agents across various tasks, including competitive, cooperative, and deferred feedback environments. | Ruihan Yang, Jiangjie Chen, Yikai Zhang, Siyu Yuan, Aili Chen, Kyle Richardson, Yanghua Xiao, Deqing Yang |  |
| 118 |  |  [Familarity: Better Evaluation of Zero-Shot Named Entity Recognition by Quantifying Label Shifts in Synthetic Training Data](https://doi.org/10.18653/v1/2025.naacl-long.37) |  | 0 | Zero-shot named entity recognition (NER) is the task of detecting named entities of specific types (such as Person or Medicine) without any training examples. Current research increasingly relies on large synthetic datasets, automatically generated to cover tens of thousands of distinct entity types, to train zero-shot NER models. However, in this paper, we find that these synthetic datasets often contain entity types that are semantically highly similar to (or even the same as) those in standard evaluation benchmarks. Because of this overlap, we argue that reported F1 scores for zero-shot NER overestimate the true capabilities of these approaches. Further, we argue that current evaluation setups provide an incomplete picture of zero-shot abilities since they do not quantify the label shift (i.e., the similarity of labels) between training and evaluation datasets. To address these issues, we propose Familarity, a novel metric that captures both the semantic similarity between entity types in training and evaluation, as well as their frequency in the training data, to provide an estimate of label shift. It allows researchers to contextualize reported zero-shot NER scores when using custom synthetic training datasets. Further, it enables researchers to generate evaluation setups of various transfer difficulties for fine-grained analysis of zero-shot NER. | Jonas Golde, Patrick Haller, Max Ploner, Fabio Barth, Nicolaas Paul Jedema, Alan Akbik |  |
| 119 |  |  [Learning to Summarize from LLM-generated Feedback](https://doi.org/10.18653/v1/2025.naacl-long.38) |  | 0 |  | Hwanjun Song, Taewon Yun, Yuho Lee, Jihwan Oh, Gihun Lee, Jason Cai, Hang Su |  |
| 120 |  |  [Hybrid Graphs for Table-and-Text based Question Answering using LLMs](https://doi.org/10.18653/v1/2025.naacl-long.39) |  | 0 | Answering questions that require reasoning and aggregation across both structured (tables) and unstructured (raw text) data sources presents significant challenges. Current methods rely on fine-tuning and high-quality, human-curated data, which is difficult to obtain. Recent advances in Large Language Models (LLMs) have shown promising results for multi-hop question answering (QA) over single-source text data in a zero-shot setting, yet exploration into multi-source Table-Text QA remains limited. In this paper, we present a novel Hybrid Graph-based approach for Table-Text QA that leverages LLMs without fine-tuning. Our method constructs a unified Hybrid Graph from textual and tabular data, pruning information based on the input question to provide the LLM with relevant context concisely. We evaluate our approach on the challenging Hybrid-QA and OTT-QA datasets using state-of-the-art LLMs, including GPT-3.5, GPT-4, and LLaMA-3. Our method achieves the best zero-shot performance on both datasets, improving Exact Match scores by up to 10% on Hybrid-QA and 5.4% on OTT-QA. Moreover, our approach reduces token usage by up to 53% compared to the original context. | Ankush Agarwal, Chaitanya Devaguptapu, Ganesh S |  |
| 121 |  |  [CFinBench: A Comprehensive Chinese Financial Benchmark for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.40) |  | 0 | Large language models (LLMs) have achieved remarkable performance on various NLP tasks, yet their potential in more challenging task like finance, has not been fully explored. In this paper, we present CFinBench: a meticulously crafted, the most comprehensive evaluation benchmark to date, for assessing the financial knowledge of LLMs under Chinese context. In practice, to better align with the career trajectory of Chinese financial practitioners, we build a systematic evaluation from 4 first-level categories: (1) Financial Subject: whether LLMs can memorize the necessary basic knowledge of financial subjects, such as economics, statistics and auditing. (2) Financial Qualification: whether LLMs can obtain the needed financial qualified certifications, such as certified public accountant, securities qualification and banking qualification. (3) Financial Practice: whether LLMs can fulfill the practical financial jobs, such as tax consultant, junior accountant and securities analyst. (4) Financial Law: whether LLMs can meet the requirement of financial laws and regulations, such as tax law, insurance law and economic law. CFinBench comprises 99,100 questions spanning 43 second-level categories with 3 question types: single-choice, multiple-choice and judgment. We conduct extensive experiments on a wide spectrum of representative LLMs with various model size on CFinBench. The results show that GPT4 and some Chinese-oriented models lead the benchmark, with the highest average accuracy being 66.02%, highlighting the challenge presented by CFinBench. All the data and evaluation code are open sourced at https://cfinbench.github.io/ | Ying Nie, Binwei Yan, Tianyu Guo, Hao Liu, Haoyu Wang, Wei He, Binfan Zheng, Weihao Wang, Qiang Li, Weijian Sun, Yunhe Wang, Dacheng Tao |  |
| 122 |  |  [LLM-Based Explicit Models of Opponents for Multi-Agent Games](https://doi.org/10.18653/v1/2025.naacl-long.41) |  | 0 | In multi-agent scenarios, the ability to anticipate and respond to opponents is essential, particularly in environments involving adversarial and collaborative interactions. In this paper, we introduce Explicit Models of Opponents (EMO) based on Large Language Models (LLMs), enabling agents to better predict and adapt to diverse, dynamic multi-agent interactions. Unlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs an individual model for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework. We test EMO alongside several reasoning methods in multi-player deduction games, where agents must infer hidden information about their opponents. The results show that EMO significantly enhances agents’ decision-making, outperforming traditional single-model approaches. Our findings demonstrate that EMO can be a powerful tool for enhancing LLM-based agents in complex multi-agent systems. | Xiaopeng Yu, Wanpeng Zhang, Zongqing Lu |  |
| 123 |  |  [SeqAR: Jailbreak LLMs with Sequential Auto-Generated Characters](https://doi.org/10.18653/v1/2025.naacl-long.42) |  | 0 | The widespread applications of large language models (LLMs) have brought about concerns regarding their potential misuse. Although aligned with human preference data before release, LLMs remain vulnerable to various malicious attacks. In this paper, we adopt a red-teaming strategy to enhance LLM safety and introduce SeqAR, a simple yet effective framework to design jailbreak prompts automatically. The SeqAR framework generates and optimizes multiple jailbreak characters and then applies sequential jailbreak characters in a single query to bypass the guardrails of the target LLM. Different from previous work which relies on proprietary LLMs or seed jailbreak templates crafted by human expertise, SeqAR can generate and optimize the jailbreak prompt in a cold-start scenario using open-sourced LLMs without any seed jailbreak templates. Experimental results show that SeqAR achieves attack success rates of 88% and 60% in bypassing the safety alignment of GPT-3.5-1106 and GPT-4, respectively. Furthermore, we extensively evaluate the transferability of the generated templates across different LLMs and held-out malicious requests, while also exploring defense strategies against the jailbreak attack designed by SeqAR. | Yan Yang, Zeguan Xiao, Xin Lu, Hongru Wang, Xuetao Wei, Hailiang Huang, Guanhua Chen, Yun Chen |  |
| 124 |  |  [JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.43) |  | 0 |  | Shota Onohara, Atsuyuki Miyai, Yuki Imajuku, Kazuki Egashira, Jeonghun Baek, Xiang Yue, Graham Neubig, Kiyoharu Aizawa |  |
| 125 |  |  [EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction](https://doi.org/10.18653/v1/2025.naacl-long.44) |  | 0 | There has been a rising interest in utilizing tools in applications of autonomous agents based on large language models (LLMs) to address intricate real-world tasks. To develop LLMbased agents, it usually requires LLMs to understand many tool functions from different tool documentations. However, these documentations could be diverse, redundant, or incomplete, which immensely affects the capability of LLMs in using tools. Current LLMs exhibit satisfactory instruction-following capabilities based on instruction-following fine-tuning process. Motivated by this, in this paper, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction to fully leverage instruction-following capabilities of LLMs for easier tool usage. EASYTOOL purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EASYTOOL can significantly reduce token consumption and improve the performance of LLM-based agents on tool utilization in real-world scenarios. Our code is available in supplemental materials. Our code is available at https://github.com/microsoft/JARVIS/tree/main/easytool. | Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Kan Ren, Dongsheng Li, Deqing Yang |  |
| 126 |  |  [Decoding Hate: Exploring Language Models' Reactions to Hate Speech](https://doi.org/10.18653/v1/2025.naacl-long.45) |  | 0 | Hate speech is a harmful form of online expression, often manifesting as derogatory posts. It is a significant risk in digital environments. With the rise of Large Language Models (LLMs), there is concern about their potential to replicate hate speech patterns, given their training on vast amounts of unmoderated internet data. Understanding how LLMs respond to hate speech is crucial for their responsible deployment. However, the behaviour of LLMs towards hate speech has been limited compared. This paper investigates the reactions of seven state-of-the-art LLMs (LLaMA 2, Vicuna, LLaMA 3, Mistral, GPT-3.5, GPT-4, and Gemini Pro) to hate speech. Through qualitative analysis, we aim to reveal the spectrum of responses these models produce, highlighting their capacity to handle hate speech inputs. We also discuss strategies to mitigate hate speech generation by LLMs, particularly through fine-tuning and guideline guardrailing. Finally, we explore the models’ responses to hate speech framed in politically correct language. | Paloma Piot, Javier Parapar |  |
| 127 |  |  [Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations](https://doi.org/10.18653/v1/2025.naacl-long.46) |  | 0 | Humans are efficient language learners and inherently social creatures. Our language development is largely shaped by our social interactions, for example, the demonstration and feedback from caregivers. Contrary to human language learning, recent advancements in large language models have primarily adopted a non-interactive training paradigm, and refined pre-trained models through feedback afterward. In this work, we explore how corrective feedback from interactions influences neural language acquisition from scratch through systematically controlled experiments, assessing whether it contributes to word learning efficiency in language models. We introduce a trial-and-demonstration (TnD) learning framework that incorporates three distinct components: student trials, teacher demonstrations, and a reward conditioned on language competence at various developmental stages. Our experiments reveal that the TnD approach accelerates word acquisition for student models of equal and smaller numbers of parameters, and we highlight the significance of both trials and demonstrations. We further show that the teacher’s choices of words influence students’ word-specific learning efficiency, and a practice-makes-perfect effect is evident by a strong correlation between the frequency of words in trials and their respective learning curves. Our findings suggest that interactive language learning, with teacher demonstrations and active trials, can facilitate efficient word learning in language models. | Ziqiao Ma, Zekun Wang, Joyce Chai |  |
| 128 |  |  [MoCE: Adaptive Mixture of Contextualization Experts for Byte-based Neural Machine Translation](https://doi.org/10.18653/v1/2025.naacl-long.47) |  | 0 | Byte-based machine translation systems have shown significant potential in massively multilingual settings. Unicode encoding, which maps each character to specific byte(s), eliminates the emergence of unknown words, even in new languages, enabling broad language scalability. However, byte-level tokenization results in sequences that are hard to interpret due to limited semantic information per byte. Local contextualization has proven effective in assigning initial semantics to tokens, improving sentence comprehension. Nevertheless, variations in encoding rules across languages necessitate an adaptive approach for effective contextualization. To this end, we propose Adaptive MultiScale-Headed Attention (Ada-MSHA), adaptively selecting and mixing attention heads, which are treated as contextualization experts. This enhances the flexibility of contextualization scales and improves the potential to discover a better strategy than previous methods. Experiment results show that our method outperforms existing methods without extensive manual adjustment of hyper-parameters and surpasses subword-based models with fewer parameters in Ted-59 dataset. | Langlin Huang, Mengyu Bu, Yang Feng |  |
| 129 |  |  [LLM-Human Pipeline for Cultural Grounding of Conversations](https://doi.org/10.18653/v1/2025.naacl-long.48) |  | 0 | Conversations often adhere to well-understood social norms that vary across cultures. For example, while addressing parents by name is commonplace in the West, it is rare in most Asian cultures. Adherence or violation of such norms often dictates the tenor of conversations. Humans are able to navigate social situations requiring cultural awareness quite adeptly. However, it is a hard task for NLP models.In this paper, we tackle this problem by introducing a Cultural Context Schema for conversations. It comprises (1) conversational information such as emotions, dialogue acts, etc., and (2) cultural information such as social norms, violations, etc. We generate ~110k social norm and violation descriptions for ~23k conversations from Chinese culture using LLMs. We refine them using automated verification strategies which are evaluated against culturally aware human judgements. We organize these descriptions into meaningful structures we call Norm Concepts, using an interactive human-in-loop framework. We ground the norm concepts and the descriptions in conversations using symbolic annotation. Finally, we use the obtained dataset for downstream tasks such as emotion, sentiment, and dialogue act detection. We show that it significantly improves the empirical performance. | Rajkumar Pujari, Dan Goldwasser |  |
| 130 |  |  [ACCESS : A Benchmark for Abstract Causal Event Discovery and Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.49) |  | 0 |  | Vy Vo, Lizhen Qu, Tao Feng, Yuncheng Hua, Xiaoxi Kang, Songhai Fan, Tim Dwyer, LayKi Soon, Gholamreza Haffari |  |
| 131 |  |  [Unmasking Implicit Bias: Evaluating Persona-Prompted LLM Responses in Power-Disparate Social Scenarios](https://doi.org/10.18653/v1/2025.naacl-long.50) |  | 0 | Large language models (LLMs) have demonstrated remarkable capabilities in simulating human behaviour and social intelligence. However, they risk perpetuating societal biases, especially when demographic information is involved. We introduce a novel framework using cosine distance to measure semantic shifts in responses and an LLM-judged Preference Win Rate (WR) to assess how demographic prompts affect response quality across power-disparate social scenarios. Evaluating five LLMs over 100 diverse social scenarios and nine demographic axes, our findings suggest a “default persona” bias toward middle-aged, able-bodied, native-born, Caucasian, atheistic males with centrist views. Moreover, interactions involving specific demographics are associated with lower-quality responses. Lastly, the presence of power disparities increases variability in response semantics and quality across demographic groups, suggesting that implicit biases may be heightened under power-imbalanced conditions. These insights expose the demographic biases inherent in LLMs and offer potential paths toward future bias mitigation efforts in LLMs. | Bryan Chen Zhengyu Tan, Roy KaWei Lee |  |
| 132 |  |  [GloCOM: A Short Text Neural Topic Model via Global Clustering Context](https://doi.org/10.18653/v1/2025.naacl-long.51) |  | 0 | Uncovering hidden topics from short texts is challenging for traditional and neural models due to data sparsity, which limits word co-occurrence patterns, and label sparsity, stemming from incomplete reconstruction targets. Although data aggregation offers a potential solution, existing neural topic models often overlook it due to time complexity, poor aggregation quality, and difficulty in inferring topic proportions for individual documents. In this paper, we propose a novel model, \*\*GloCOM\*\* (\*\*Glo\*\*bal \*\*C\*\*lustering C\*\*O\*\*ntexts for Topic \*\*M\*\*odels), which addresses these challenges by constructing aggregated global clustering contexts for short documents, leveraging text embeddings from pre-trained language models. GloCOM can infer both global topic distributions for clustering contexts and local distributions for individual short texts. Additionally, the model incorporates these global contexts to augment the reconstruction loss, effectively handling the label sparsity issue. Extensive experiments on short text datasets show that our approach outperforms other state-of-the-art models in both topic quality and document representations. | Quang Duc Nguyen, Tung Nguyen, Duc Anh Nguyen, Linh Ngo Van, Sang Dinh, Thien Huu Nguyen |  |
| 133 |  |  [Reversed Attention: On The Gradient Descent Of Attention Layers In GPT](https://doi.org/10.18653/v1/2025.naacl-long.52) |  | 0 | The success of Transformer-based Language Models (LMs) stems from their attention mechanism. While this mechanism has been extensively studied in explainability research, particularly through the attention values obtained during the forward pass of LMs, the backward pass of attention has been largely overlooked.In this work, we study the mathematics of the backward pass of attention, revealing that it implicitly calculates an attention matrix we refer to as “Reversed Attention”.We visualized Reversed Attention and examine its properties, demonstrating its ability to elucidate the models’ behavior and edit dynamics.In an experimental setup, we showcase the ability of Reversed Attention to directly alter the forward pass of attention, without modifying the model’s weights, using a novel method called “attention patching”.In addition to enhancing the comprehension of how LMs configure attention layers during backpropagation, Reversed Attention maps contribute to a more interpretable backward pass. | Shahar Katz, Lior Wolf |  |
| 134 |  |  [Self-Harmonized Chain of Thought](https://doi.org/10.18653/v1/2025.naacl-long.53) |  | 0 |  | Ziqi Jin, Wei Lu |  |
| 135 |  |  [AnaScore: Understanding Semantic Parallelism in Proportional Analogies](https://doi.org/10.18653/v1/2025.naacl-long.54) |  | 0 | Formulaic criteria for proportional analogies, which capture relational mappings between two ratios of terms, are mainly confined to the formal level. As analogy datasets grow more complex, especially in evaluating the cognitive abilities of Large Language Models (LLMs), assessing parallelism in them becomes increasingly challenging and often requires human annotation. In this work, we propose AnaScore, an automatic metric for evaluating the strength of semantic parallelism in sentence analogies. AnaScore systematically provides formalized explanations for shared relational patterns at the level of conceptual knowledge. We apply AnaScore to annotate several existing datasets, considering different directions of the relations, and uncover artifacts in data construction. Our experiments with various LLMs demonstrate the efficacy of the AnaScore metric in capturing the inherent quality of analogical relationships, showing a positive correlation between analogy quality and model performance. Thanks to this metric, we clearly demonstrate that formally explainable examples are more beneficial for analogical reasoning, while ambiguous analogies with no clear criterion tend to hinder inference. | Liyan Wang, Haotong Wang, Yves Lepage |  |
| 136 |  |  [Generating Complex Question Decompositions in the Face of Distribution Shifts](https://doi.org/10.18653/v1/2025.naacl-long.55) |  | 0 | Question decomposition has been found to help large language models’ (LLMs) performance on complex question answering (QA) by breaking these questions into simpler sub-questions for answering. Nonetheless, performance on the task remains dominated by supervised approaches, suggesting room for making LLMs better decomposers. One way of improving LLM training and fine-tuning is to leverage synthetic training data, but the superior performance of supervised approaches collapses in the face of distribution shifts, making them unsuitable for generating synthetic data across new domains and at scale. To address this, we propose an approach to generate synthetic decomposition data with only five annotated examples; we do this by (i) extending recent advancements in using LLM-as-judge and for reranking in novel ways, as well as (ii) using a panel of smaller-sized LLMs for data generation instead of resource-intensive larger models. Through careful validation of our approach over two benchmark datasets, we show that our data generation and modelling approaches bring consistent improvements over using few-shot prompting with LLMs for the task. Our code and models can be found at https://github.com/hankelvin/complex_question_decomposition. | Kelvin Han, Claire Gardent |  |
| 137 |  |  [Diversify-verify-adapt: Efficient and Robust Retrieval-Augmented Ambiguous Question Answering](https://doi.org/10.18653/v1/2025.naacl-long.56) |  | 0 | The retrieval augmented generation (RAG) framework addresses an ambiguity in user queries in QA systems by retrieving passages that cover all plausible interpretations and generating comprehensive responses based on the passages. However, our preliminary studies reveal that a single retrieval process often suffers from low-quality results, as the retrieved passages frequently fail to capture all plausible interpretations. Although the iterative RAG approach has been proposed to address this problem, it comes at the cost of significantly reduced efficiency. To address these issues, we propose the diversify-verify-adapt (DIVA) framework. DIVA first diversifies the retrieved passages to encompass diverse interpretations. Subsequently, DIVA verifies the quality of the passages and adapts the most suitable approach tailored to their quality. This approach improves the QA systems’ accuracy and robustness by handling low quality retrieval issue in ambiguous questions, while enhancing efficiency. | Yeonjun In, Sungchul Kim, Ryan A. Rossi, Md. Mehrab Tanjim, Tong Yu, Ritwik Sinha, Chanyoung Park |  |
| 138 |  |  [Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical Ability Assessment of LLM-Powered AI Tutors](https://doi.org/10.18653/v1/2025.naacl-long.57) |  | 0 | In this paper, we investigate whether current state-of-the-art large language models (LLMs) are effective as AI tutors and whether they demonstrate pedagogical abilities necessary for good AI tutoring in educational dialogues. Previous efforts towards evaluation have beenlimited to subjective protocols and benchmarks. To bridge this gap, we propose a unified evaluation taxonomy with eight pedagogical dimensions based on key learning sciences principles, which is designed to assess the pedagogical value of LLM-powered AI tutor responses grounded in student mistakes or confusions in the mathematical domain. We release MRBench – a new evaluation benchmark containing 192 conversations and 1,596 responses from seven state-of-the-art LLM-based and human tutors, providing gold annotations for eight pedagogical dimensions. We assess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as evaluators and analyze each tutor’s pedagogical abilities, highlighting which LLMs are good tutors and which ones are more suitable as question-answering systems. We believe that the presented taxonomy, benchmark, and human-annotated labels will streamline the evaluation process and help track the progress in AI tutors’ development. | Kaushal Kumar Maurya, KV Aditya Srivatsa, Kseniia Petukhova, Ekaterina Kochmar |  |
| 139 |  |  [Where is the answer? An empirical study of positional bias for parametric knowledge extraction in language model](https://doi.org/10.18653/v1/2025.naacl-long.58) |  | 0 | Language model (LM) stores diverse factual knowledge in their parameters, which is learned during self-supervised training on unlabeled documents and is made extractable by instruction-tuning. For knowledge-intensive tasks, it is essential to memorize information in a way that makes it extractable from LM’s parameters with diverse queries. However, LMs suffer from a phenomenon called “perplexity curse”; despite minimizing document perplexity during training, LMs struggle to extract information via a question prompt. In this paper, we study the problem by fine-tuning LMs for new data and find a very intriguing fact that all studied LMs suffer from positional bias in the training document, i.e., they struggle to answer questions about the information described in the middle or at the end of the training document. Our study indicates that this problem stems from the auto-regressive training, ie., predicting the next token given all previous tokens, thus adding regularization mitigates the issue. Our discoveries supported by extensive analysis will be an important key to extracting knowledge from the parameters of LMs. We will publish our code and dataset upon acceptance. | Kuniaki Saito, ChenYu Lee, Kihyuk Sohn, Yoshitaka Ushiku |  |
| 140 |  |  [Evaluating Morphological Compositional Generalization in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.59) |  | 0 | Large language models (LLMs) have demonstrated significant progress in various natural language generation and understanding tasks. However, their linguistic generalization capabilities remain questionable, raising doubts about whether these models learn language similarly to humans. While humans exhibit compositional generalization and linguistic creativity in language use, the extent to which LLMs replicate these abilities, particularly in morphology, is under-explored. In this work, we systematically investigate the morphological generalization abilities of LLMs through the lens of compositionality. We define morphemes as compositional primitives and design a novel suite of generative and discriminative tasks to assess morphological productivity and systematicity. Focusing on agglutinative languages such as Turkish and Finnish, we evaluate several state-of-the-art instruction-finetuned multilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs struggle with morphological compositional generalization particularly when applied to novel word roots, with performance declining sharply as morphological complexity increases. While models can identify individual morphological combinations better than chance, their performance lacks systematicity, leading to significant accuracy gaps compared to humans. | Mete Ismayilzada, Defne Circi, Jonne Sälevä, Hale Sirin, Abdullatif Köksal, Bhuwan Dhingra, Antoine Bosselut, Duygu Ataman, Lonneke van der Plas |  |
| 141 |  |  [Balancing Forget Quality and Model Utility: A Reverse KL-Divergence Knowledge Distillation Approach for Better Unlearning in LLMs](https://doi.org/10.18653/v1/2025.naacl-long.60) |  | 0 | As concern for privacy rights has grown and the size of language model training datasets has expanded, research into machine unlearning for large language models (LLMs) has become crucial. Before the era of LLMs, research on machine unlearning mainly focused on classification tasks in small parameter models. However, as parameter sizes have grown and unlearning targets have become more complex, unlearning has become more challenging, especially in scenarios involving generation instead of classification, as the output space of such models is significantly larger and more diverse. Existing methods based on gradient ascent and its variants often struggle with balancing forget quality and model utility, leading to either over unlearning or partial unlearning. To address this challenge, we propose Reverse KL-Divergence based Knowledge Distillation for Unlearning (RKLU), a novel unlearning method for LLMs. RKLU focuses on precisely unlearning the components of the token distribution related to the unlearning target, allowing us to achieve significant forget quality while maintaining model utility in our experiments. | Bichen Wang, Yuzhe Zi, Yixin Sun, Yanyan Zhao, Bing Qin |  |
| 142 |  |  [AgentMove: A Large Language Model based Agentic Framework for Zero-shot Next Location Prediction](https://doi.org/10.18653/v1/2025.naacl-long.61) |  | 0 | Next location prediction plays a crucial role in various real-world applications. Recently, due to the limitation of existing deep learning methods, attempts have been made to apply large language models (LLMs) to zero-shot next location prediction task. However, they directly generate the final output using LLMs without systematic design, which limits the potential of LLMs to uncover complex mobility patterns and underestimates their extensive reserve of global geospatial knowledge. In this paper, we introduce AgentMove, a systematic agentic prediction framework to achieve generalized next location prediction. In AgentMove, we first decompose the mobility prediction task and design specific modules to complete them, including spatial-temporal memory for individual mobility pattern mining, world knowledge generator for modeling the effects of urban structure and collective knowledge extractor for capturing the shared patterns among population. Finally, we combine the results of three modules and conduct a reasoning step to generate the final predictions. Extensive experiments utilizing mobility data from two distinct sources reveal that AgentMove surpasses the leading baseline by 3.33% to 8.57% across 8 out of 12 metrics and it shows robust predictions with various LLMs as base and also less geographical bias across cities. Our codes are available via https://github.com/tsinghua-fib-lab/AgentMove. | Jie Feng, Yuwei Du, Jie Zhao, Yong Li |  |
| 143 |  |  [Embedding derived animacy rankings offer insights into the sources of grammatical animacy](https://doi.org/10.18653/v1/2025.naacl-long.62) |  | 0 | In this study, we applied the semantic projection approach to animacy, a feature that has not been previously explored using this method. We compared the relative animacy rankings of nouns denoting animals, humans, objects, and first-, second-, and third-person pronouns, as derived from word embeddings, with rankings derived from human behavioral ratings of animacy and from grammatical patterns. Our results support the semantic projection approach as an effective method for deriving proxies of human perception from word embeddings and offer insights into the sources of grammatical animacy. | Vivian G. Li |  |
| 144 |  |  [Generating Long-form Story Using Dynamic Hierarchical Outlining with Memory-Enhancement](https://doi.org/10.18653/v1/2025.naacl-long.63) |  | 0 | Long-form story generation task aims to produce coherent and sufficiently lengthy text, essential for applications such as novel writingand interactive storytelling. However, existing methods, including LLMs, rely on rigid outlines or lack macro-level planning, making it difficult to achieve both contextual consistency and coherent plot development in long-form story generation. To address this issues, we propose Dynamic Hierarchical Outlining with Memory-Enhancement long-form story generation method, named DOME, to generate the long-form story with coherent content and plot. Specifically, the Dynamic Hierarchical Outline(DHO) mechanism incorporates the novel writing theory into outline planning and fuses the plan and writing stages together, improving the coherence of the plot by ensuring the plot completeness and adapting to the uncertainty during story generation. A Memory-Enhancement Module (MEM) based on temporal knowledge graphs is introduced to store and access the generated content, reducing contextual conflicts and improving story coherence. Finally, we propose a Temporal Conflict Analyzer leveraging temporal knowledge graphs to automatically evaluate the contextual consistency of long-form story. Experiments demonstrate that DOME significantly improves the fluency, coherence, and overall quality of generated long stories compared to state-of-the-art methods. | Qianyue Wang, Jinwu Hu, Zhengping Li, Yufeng Wang, Daiyuan Li, Yu Hu, Mingkui Tan |  |
| 145 |  |  [Little Giants: Synthesizing High-Quality Embedding Data at Scale](https://doi.org/10.18653/v1/2025.naacl-long.64) |  | 0 | Synthetic data generation has become an increasingly popular way of training models without the need for large, manually labeled datasets. For tasks like text embedding, synthetic data offers diverse and scalable training examples, significantly reducing the cost of human annotation. However, most current approaches rely heavily on proprietary models like GPT-4, which are expensive and inefficient for generating large-scale embedding data. In this paper, we introduce SPEED, a framework that aligns open-source small models (8B) to efficiently generate large-scale synthetic embedding data. Through supervised fine-tuning, preference optimization, and self-improvement, SPEED enables small open-source models to produce high-quality data. Remarkably, SPEED uses only less than 1/10 of the GPT API calls, outperforming the state-of-the-art embedding model E5_mistral when both are trained solely on their synthetic data. Using this efficient generator, we conduct a comprehensive study on how various factors within the alignment pipeline impact data quality and reveal the scaling law for synthetic embedding data. Our codes and models are released in https://github.com/haon-chen/SPEED. | Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, Zhicheng Dou |  |
| 146 |  |  [Can LLMs Convert Graphs to Text-Attributed Graphs?](https://doi.org/10.18653/v1/2025.naacl-long.65) |  | 0 | Graphs are ubiquitous structures found in numerous real-world applications, such as drug discovery, recommender systems, and social network analysis. To model graph-structured data, graph neural networks (GNNs) have become a popular tool. However, existing GNN architectures encounter challenges in cross-graph learning where multiple graphs have different feature spaces. To address this, recent approaches introduce text-attributed graphs (TAGs), where each node is associated with a textual description, which can be projected into a unified feature space using textual encoders. While promising, this method relies heavily on the availability of text-attributed graph data, which is difficult to obtain in practice. To bridge this gap, we propose a novel method named Topology-Aware Node description Synthesis (TANS), leveraging large language models (LLMs) to convert existing graphs into text-attributed graphs. The key idea is to integrate topological information into LLMs to explain how graph topology influences node semantics. We evaluate our TANS on text-rich, text-limited, and text-free graphs, demonstrating its applicability. Notably, on text-free graphs, our method significantly outperforms existing approaches that manually design node features, showcasing the potential of LLMs for preprocessing graph-structured data in the absence of textual information. The code and data are available at https://github.com/Zehong-Wang/TANS. | Zehong Wang, Sidney Liu, Zheyuan Zhang, Tianyi Ma, Chuxu Zhang, Yanfang Ye |  |
| 147 |  |  [Forest for the Trees: Overarching Prompting Evokes High-Level Reasoning in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.66) |  | 0 | Chain-of-thought (CoT) and subsequent methods adopted a deductive paradigm that decomposes the reasoning process, demonstrating remarkable performances across NLP tasks. However, such a paradigm faces the challenge of getting bogged down in low-level semantic details, hindering large language models (LLMs) from correctly understanding, selecting, and compositing conditions. In this work, we present Overarching Prompting (OaP), a simple prompting method that elicits the high-level thinking of LLMs. Specifically, OaP first abstracts the whole problem into a simplified archetype and formulates strategies grounded in concepts and principles, establishing an overarching perspective for guiding reasoning. We conducted experiments with SoTA models, including ChatGPT, InstructGPT, and Llama3-70B-instruct, and received promising performances across tasks including Knowledge QA, Mathematical, and Open-Domain Reasoning. For instance, OaP improved ChatGPT and CoT by 19.0% and 3.1% on MMLU’s College Physics, 8.8% and 2.3% on GSM8k, and 10.3% and 2.5% on StrategyQA, respectively. | Haoran Liao, Shaohua Hu, Zhihao Zhu, Hao He, Yaohui Jin |  |
| 148 |  |  [On the Role of Speech Data in Reducing Toxicity Detection Bias](https://doi.org/10.18653/v1/2025.naacl-long.67) |  | 0 | Text toxicity detection systems exhibit significant biases, producing disproportionate rates of false positives on samples mentioning demographic groups. But what about toxicity detection in speech? To investigate the extent to which text-based biases are mitigated by speech-based systems, we produce a set of high-quality group annotations for the multilingual MuTOX dataset, and then leverage these annotations to systematically compare speech- and text-based toxicity classifiers. Our findings indicate that access to speech data during inference supports reduced bias against group mentions, particularly for ambiguous and disagreement-inducing samples. Our results also suggest that improving classifiers, rather than transcription pipelines, is more helpful for reducing group bias. We publicly release our annotations and provide recommendations for future toxicity dataset construction. | Samuel J. Bell, Mariano Coria Meglioli, Megan Richards, Eduardo Sánchez, Christophe Ropers, Skyler Wang, Adina Williams, Levent Sagun, Marta R. Costajussà |  |
| 149 |  |  [ITALIC: An Italian Culture-Aware Natural Language Benchmark](https://doi.org/10.18653/v1/2025.naacl-long.68) |  | 0 | We present ITALIC, a large-scale benchmark dataset of 10,000 multiple-choice questions designed to evaluate the natural language understanding of the Italian language and culture. ITALIC spans 12 domains, exploiting public tests to score domain experts in real-world scenarios. We detail our data collection process, stratification techniques, and selection strategies. ITALIC provides a comprehensive assessment suite that captures commonsense reasoning and linguistic proficiency in a morphologically rich language. We establish baseline performances using 17 state-of-the-art LLMs, revealing current limitations in Italian language understanding and highlighting significant linguistic complexity and cultural specificity challenges. ITALIC serves as a benchmark for evaluating existing models and as a roadmap for future research, encouraging the development of more sophisticated and culturally aware natural language systems. | Andrea Seveso, Daniele Potertì, Edoardo Federici, Mario Mezzanzanica, Fabio Mercorio |  |
| 150 |  |  [RAP: A Metric for Balancing Repetition and Performance in Open-Source Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.69) |  | 0 | Large Language Models (LLMs) have significantly advanced natural language processing, but content repetition in open-source LLMs remains a critical challenge that adversely affects user experience. The repetition penalty parameter (RPP) aims to mitigate this issue by preventing repeated content generation, but excessive use of RPP can compromise the overall quality. In this paper, we propose Repetition-Aware Performance (RAP), a novel evaluation metric that quantifies and integrates repetition penalty into the assessment of model performance, enabling tuning of RPP. We evaluate our approach using twelve open-source LLMs, ranging from 2 billion to 70 billion parameters, tested on question answering and machine translation tasks across three datasets with varying prompting techniques. Experimental results show that RAP effectively tunes RPP, helping to identify a trade-off value that significantly reduces repetition while minimizing performance loss. Upon acceptance, we will release the code and the dataset of generated text, providing a valuable resource for further research on repetition detection and LLMs evaluation. | Donghao Huang, ThanhSon Nguyen, Fiona Liausvia, Zhaoxia Wang |  |
| 151 |  |  [Improving Data Annotation for Low-Resource Relation Extraction with Logical Rule-Augmented Collaborative Language Models](https://doi.org/10.18653/v1/2025.naacl-long.70) |  | 0 | Low-resource relation extraction aims to identify semantic relationships between entities using scarce labeled data. Recent studies exploit large language models to recognize relations based on retrieved examplars, yielding promising results. However, the reliability of predictions from these methods is constrained by the presence of irrelevant context within demonstrations and the inherent flaws of large language models in producing undesired outputs. Inspired by the precision and generalization of abstract logic, in this paper, we propose distilling logical rules to uniformly represent task knowledge sourced from distinct origins and facilitate deductive reasoning. We develop a collaborative annotating framework that iteratively integrates high-confidence predictions of rule-enhanced relation extractors with varying scales, efficiently obtaining reliable pseudo annotations from massive unlabeled samples without human supervision. Experiments under two inference settings show that our approach achieves new state-of-the-art performance on benchmark datasets in few-shot scenarios. | Xiyang Liu, Chunming Hu, Richong Zhang, Junfan Chen, Baowen Xu |  |
| 152 |  |  [CompAct: Compressed Activations for Memory-Efficient LLM Training](https://doi.org/10.18653/v1/2025.naacl-long.71) |  | 0 | We introduce CompAct, a technique that reduces peak memory utilization on GPU by 25-30% for pretraining and 50% for fine-tuning of LLMs. Peak device memory is a major limiting factor in training LLMs, with various recent works aiming to reduce model memory. However most works don’t target the largest component of allocated memory during training: the model’s compute graph, which is stored for the backward pass. By storing low-rank, compressed activations to be used in the backward pass we greatly reduce the required memory, unlike previous methods which only reduce optimizer overheads or the number of trained parameters. Our compression uses random projection matrices, thus avoiding additional memory overheads. Comparisons with previous techniques for either pretraining or fine-tuning show that CompAct substantially improves existing compute-performance tradeoffs. We expect CompAct’s savings to scale even higher for larger models. | Yara Shamshoum, Nitzan Hodos, Yuval Sieradzki, Assaf Schuster |  |
| 153 |  |  [Large Language Models Are Cross-Lingual Knowledge-Free Reasoners](https://doi.org/10.18653/v1/2025.naacl-long.72) |  | 0 | Large Language Models have demonstrated impressive reasoning capabilities across multiple languages. However, the relationship between capabilities in different languages is less explored. In this work, we decompose the process of reasoning tasks into two separated components: knowledge retrieval and knowledge-free reasoning, and analyze the relationship between cross-lingual transferability and these two components. With adapted commonsense reasoning datasets and constructed knowledge-free reasoning datasets, we show that the knowledge-free reasoning capability can be nearly perfectly transferred across various source-target language directions despite the secondary impact of resource in some specific target languages, while cross-lingual knowledge retrieval significantly hinders the transfer. Moreover, by analyzing the hidden states and feed-forward network neuron activation during the reasoning, we show that higher similarity of hidden representations and larger overlap of activated neurons could explain the better cross-lingual transferability of knowledge-free reasoning than knowledge retrieval. Thus, we hypothesize that knowledge-free reasoning shares similar neurons in different languages for reasoning, while knowledge is stored separately in different languages. | Peng Hu, Sizhe Liu, Changjiang Gao, Xin Huang, Xue Han, Junlan Feng, Chao Deng, Shujian Huang |  |
| 154 |  |  [What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to Prompt Engineering](https://doi.org/10.18653/v1/2025.naacl-long.73) |  | 0 | Large Language Models (LLMs) changed the way we design and interact with software systems. Their ability to process and extract information from text has drastically improved productivity in a number of routine tasks. Developers that want to include these models in their software stack, however, face a dreadful challenge: debugging LLMs’ inconsistent behavior across minor variations of the prompt. We therefore introduce two metrics for classification tasks, namely \*sensitivity\* and \*consistency\*, which are complementary to task performance. First, sensitivity measures changes of predictions across rephrasings of the prompt, and does not require access to ground truth labels. Instead, consistency measures how predictions vary across rephrasings for elements of the same class. We perform an empirical comparison of these metrics on text classification tasks, using them as guideline for understanding failure modes of the LLM. Our hope is that sensitivity and consistency will be helpful to guide prompt engineering and obtain LLMs that balance robustness with performance. | Federico Errica, Davide Sanvito, Giuseppe Siracusano, Roberto Bifulco |  |
| 155 |  |  [Detect, Disambiguate, and Translate: On-Demand Visual Reasoning for Multimodal Machine Translation with Large Vision-Language Models](https://doi.org/10.18653/v1/2025.naacl-long.74) |  | 0 | Multimodal machine translation (MMT) aims to leverage additional modalities to assist in language translation. With limited parallel data, current MMT systems rely heavily on monolingual English captioning data. These systems face three key issues: they often overlook that visual signals are unnecessary in many cases, they lack transparency in how visual information is used for disambiguation when needed, and they have yet to fully explore the potential of large-scale vision-language models (LVLMs) for MMT tasks. To address these issues, we propose the Detect, Disambiguate, and Translate (DeDiT) framework, the first reasoning-based framework for MMT leveraging LVLMs. DeDiT detects ambiguity in the input sentence, performs visual reasoning only when ambiguity is found, and generates the final translation.We implemented two versions of DeDiT: a prompting method for large proprietary LVLMs and a fine-tuning method for smaller LVLMs using synthetic data. Experiments on the Multi30K and CoMMuTE benchmarks show that DeDiT outperforms state-of-the-art models in disambiguation accuracy and translation quality. We also introduce an improved evaluation metric for disambiguation accuracy that enhances performance assessment and can be applied to proprietary models accessed via APIs. | Danyang Liu, Fanjie Kong, Xiaohang Sun, Dhruva Patil, Avijit Vajpayee, Zhu Liu, Vimal Bhat, Najmeh Sadoughi |  |
| 156 |  |  [Mitigating Hallucinations in Multi-modal Large Language Models via Image Token Attention-Guided Decoding](https://doi.org/10.18653/v1/2025.naacl-long.75) |  | 0 | Multi-modal large language models (MLLMs) integrate the inherent text generation capabilities of large language models with an understanding of other modalities, promising wide applications in open-ended tasks. Despite their success, they often generate plausible but incorrect content. This phenomenon, known as hallucination, significantly impacts their practical deployment. In this paper, we delve into the intrinsic characteristics of hallucination from the perspective of interaction between input and output tokens. We find that the hallucination typically occurs with attention reduction of output tokens to image tokens. Based on this observation, we introduce image Token attention-guided Decoding (iTaD), a plug-and-play method which leverages MLLMs’ internal representations to mitigate their hallucinations. We first define an image token attention vector to measure the inter-layer differences in attention of output tokens to image tokens across different layers. Based on the vector, we design a novel layer selection strategy and conduct inter-layer contrastive decoding to highlight the progression in image understanding, thereby exploiting attention to image tokens to mitigate hallucinations. Extensive experiments well demonstrate iTaD’s effectiveness across different MLLMs and benchmarks. | Xinhao Xu, Hui Chen, Mengyao Lyu, Sicheng Zhao, Yizhe Xiong, Zijia Lin, Jungong Han, Guiguang Ding |  |
| 157 |  |  [A Multi-modal Large Language Model with Graph-of-Thought for Effective Recommendation](https://doi.org/10.18653/v1/2025.naacl-long.76) |  | 0 | Chain-of-Thought (CoT) prompting has been shown to be effective in guiding Large Language Models (LLMs) to decompose complex tasks into multiple intermediate steps, and constructing a rational reasoning chain for inferring answers. However, the linear nature of CoT falls short from enabling LLMs to effectively handle graph structures, which are essential for personalized recommendation tasks that rely on user-item interaction graphs. To bridge this gap, we introduce GollaRec, which leverages a Graph-of-Thought (GoT) prompting technique in a Multi-modal LLM, namely LLaVA, to effectively exploit the complex structure of the interaction graphs. GollaRec enhances the recommendation effectiveness by integrating both visual and textual “thoughts” into a graph-structured prompt, using both item images and descriptions to produce richer multi-modal user/item representations. In our proposed approach, GollaRec leverages text-graph alignment and graph instruction tuning to allow the Multi-modal LLM to capture complex graph structures. In addition, GollaRec leverages a graph adaptor to integrate user-item interactions into the resulting user/item embeddings, therefore effectively adapting the model to the recommendation task. Our extensive experiments on 6 benchmark datasets demonstrate the superiority of our proposed GollaRec model over 12 existing state-of-the-art models in various multi-modal recommendation tasks, including general and multi-domain recommendation tasks. | Zixuan Yi, Iadh Ounis |  |
| 158 |  |  [Investigating Human Values in Online Communities](https://doi.org/10.18653/v1/2025.naacl-long.77) |  | 0 | Studying human values is instrumental for cross-cultural research, enabling a better understanding of preferences and behaviour of society at large and communities therein. To study the dynamics of communities online, we propose a method to computationally analyse values present on Reddit. Our method allows analysis at scale, complementing survey based approaches. We train a value relevance and a value polarity classifier, which we thoroughly evaluate using in-domain and out-of-domain human annotations. Using these, we automatically annotate over nine million posts across 12k subreddits with Schwartz values. Our analysis unveils both previously recorded and novel insights into the values prevalent within various online communities. For instance, we discover a very negative stance towards conformity in the Vegan and AbolishTheMonarchy subreddits. Additionally, our study of geographically specific subreddits highlights the correlation between traditional values and conservative U.S. states. Through our work, we demonstrate how our dataset and method can be used as a complementary tool for qualitative study of online communication. | Nadav Borenstein, Arnav Arora, LucieAimée Kaffee, Isabelle Augenstein |  |
| 159 |  |  [Pointwise Mutual Information as a Performance Gauge for Retrieval-Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-long.78) |  | 0 | Recent work suggests that large language models enhanced with retrieval-augmented generation are easily influenced by the order in which the retrieved documents are presented to the model when solving tasks such as question answering (QA).However, there is no method to date that exploits this phenomenon to improve generation.To fill this gap, in this study, we show that the pointwise mutual information between a context and a question is an effective gauge for language model performance.Importantly, this gauge does not depend on knowing the answer to the question a priori.Through experiments on two question-answering datasets using a variety of large language models, we find evidence for an empirical correlation between answer accuracy and pointwise mutual information.Additionally, we propose two methods that use the pointwise mutual information between a document and a question as a gauge for selecting and constructing prompts that lead to better performance, whose effectiveness we demonstrate through experimentation. | Tianyu Liu, Jirui Qi, Paul He, Arianna Bisazza, Mrinmaya Sachan, Ryan Cotterell |  |
| 160 |  |  [MATO: A Model-Agnostic Training Optimization for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2025.naacl-long.79) |  | 0 | As an important fine-grained sentiment analysis task, aspect sentiment triplet extraction (ASTE) aims to identify three elements, i.e., aspect, opinion and sentiment polarity as a triplet. Advanced ASTE researches have mostly explored triplet-wise ability to achieve superior improvement. However, existing models with strong in-house performances may struggle to generalize to the challenging cases with the diverse expression of inter-triplet and intra-triplet elements. To this end, we propose a \*\*M\*\*odel-\*\*A\*\*gnostic \*\*T\*\*raining \*\*O\*\*ptimization (\*\*MATO\*\*) to improve ASTE model inference consistent with expected results facing triplet element diversity. Specifically, we design inter-triplet and intra-triplet metamorphic relations (MRs), and calculate the violation rate (VR) on each element of one triplet through metamorphic testing (MT), indicating the capacity to accommodate the diverse elements. Moreover, we propose an element-wise diversity-aware loss based on the VRs of aspect, opinion and sentiment, which can be jointly trained with existed ASTE models via uncertainty weighing. Conducted on four benchmark datasets and seven ASTE models, experimental results show that our MATO can enhance their diversity capacity, decreasing the average element-wise VRs by 3.28% to 15.36%. Meanwhile, our MATO is comparable to or better than those in terms of F1-score. | Shaopeng Tang, Lin Li, Xiaohui Tao, Leqi Zhong, Qing Xie |  |
| 161 |  |  [Dynamic Data Mixing Maximizes Instruction Tuning for Mixture-of-Experts](https://doi.org/10.18653/v1/2025.naacl-long.80) |  | 0 | Mixture-of-Experts (MoE) models have shown remarkable capability in instruction tuning, especially when the number of tasks scales. However, previous methods simply merge all training tasks (e.g. creative writing, coding, and mathematics) and apply fixed sampling weights, without considering the importance of different tasks as the model training state changes. In this way, the most helpful data cannot be effectively distinguished, leading to suboptimal model performance. To reduce the potential redundancies of datasets, we make the first attempt and propose a novel dynamic data mixture for MoE instruction tuning. Specifically, inspired by MoE’s token routing preference, we build dataset-level representations and then capture the subtle differences among datasets. Finally, we propose to dynamically adjust the sampling weight of datasets by their inter-redundancies, thus maximizing global performance under a limited training budget. The experimental results on two MoE models demonstrate the effectiveness of our approach on both downstream knowledge & reasoning tasks and open-ended queries. | Tong Zhu, Daize Dong, Xiaoye Qu, Jiacheng Ruan, Wenliang Chen, Yu Cheng |  |
| 162 |  |  [EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics](https://doi.org/10.18653/v1/2025.naacl-long.81) |  | 0 | Designing emotionally intelligent conversational systems to provide comfort and advice to people experiencing distress is a compelling area of research. Recently, with advancements in large language models (LLMs), end-to-end dialogue agents without explicit strategy prediction steps have become prevalent. However, implicit strategy planning lacks transparency, and recent studies show that LLMs’ inherent preference bias towards certain socio-emotional strategies hinders the delivery of high-quality emotional support. To address this challenge, we propose decoupling strategy prediction from language generation, and introduce a novel dialogue strategy prediction framework, EmoDynamiX, which models the discourse dynamics between user fine-grained emotions and system strategies using a heterogeneous graph for better performance and transparency. Experimental results on two ESC datasets show EmoDynamiX outperforms previous state-of-the-art methods with a significant margin (better proficiency and lower preference bias). Our approach also exhibits better transparency by allowing backtracing of decision making. | Chenwei Wan, Matthieu Labeau, Chloé Clavel |  |
| 163 |  |  [ReasVQA: Advancing VideoQA with Imperfect Reasoning Process](https://doi.org/10.18653/v1/2025.naacl-long.82) |  | 0 |  | Jianxin Liang, Xiaojun Meng, Huishuai Zhang, Yueqian Wang, Jiansheng Wei, Dongyan Zhao |  |
| 164 |  |  [Divergent Thoughts toward One Goal: LLM-based Multi-Agent Collaboration System for Electronic Design Automation](https://doi.org/10.18653/v1/2025.naacl-long.83) |  | 0 | Recently, with the development of tool-calling capabilities in large language models (LLMs), these models have demonstrated significant potential for automating electronic design automation (EDA) flows by interacting with EDA tool APIs via EDA scripts.However, considering the limited understanding of EDA tools, LLMs face challenges in practical scenarios where diverse interfaces of EDA tools exist across different platforms.Additionally, EDA flow automation often involves intricate, long-chain tool-calling processes, increasing the likelihood of errors in intermediate steps.Any errors will lead to the instability and failure of EDA flow automation.To address these challenges, we introduce EDAid, a multi-agent collaboration system where multiple agents harboring divergent thoughts converge towards a common goal, ensuring reliable and successful EDA flow automation. Specifically, each agent is controlled by ChipLlama models, which are expert LLMs fine-tuned for EDA flow automation.Our experiments demonstrate the state-of-the-art (SOTA) performance of our ChipLlama models and validate the effectiveness of our EDAid in the automation of complex EDA flows, showcasing superior performance compared to single-agent systems. | Haoyuan Wu, Haisheng Zheng, Zhuolun He, Bei Yu |  |
| 165 |  |  [A Survey of QUD Models for Discourse Processing](https://doi.org/10.18653/v1/2025.naacl-long.84) |  | 0 | Question Under Discussion (QUD), which is originally a linguistic analytic framework, gains increasing attention in the community of natural language processing over the years. Various models have been proposed for implementing QUD for discourse processing. This survey summarizes these models, with a focus on application to written texts, and examines studies that explore the relationship between QUD and mainstream discourse frameworks, including RST, PDTB and SDRT. Some questions that may require further study are suggested. | Yingxue Fu |  |
| 166 |  |  [SafetyQuizzer: Timely and Dynamic Evaluation on the Safety of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.85) |  | 0 | With the expansion of the application of Large Language Models (LLMs), concerns about their safety have grown among researchers. Numerous studies have demonstrated the potential risks of LLMs generating harmful content and have proposed various safety assessment benchmarks to evaluate these risks. However, the evaluation questions in current benchmarks, especially for Chinese, are too straightforward, making them easily rejected by target LLMs, and difficult to update with practical relevance due to their lack of correlation with real-world events. This hinders the effective application of these benchmarks in continuous evaluation tasks. To address these limitations, we propose SafetyQuizzer, a question-generation framework designed to evaluate the safety of LLMs more sustainably in the Chinese context. SafetyQuizzer leverages a finetuned LLM and jailbreaking attack templates to generate subtly offensive questions, which reduces the decline rate. Additionally, by utilizing retrieval-augmented generation, SafetyQuizzer incorporates the latest real-world events into evaluation questions, improving the adaptability of the benchmarks. Our experiments demonstrate that evaluation questions generated by SafetyQuizzer significantly reduce the decline rate compared to other benchmarks while maintaining a comparable attack success rate. Our code is available at https://github.com/zhichao-stone/SafetyQuizzer. Warning: this paper contains examples that may be offensive or upsetting. | Zhichao Shi, Shaoling Jing, Yi Cheng, Hao Zhang, Yuanzhuo Wang, Jie Zhang, Huawei Shen, Xueqi Cheng |  |
| 167 |  |  [Privacy Checklist: Privacy Violation Detection Grounding on Contextual Integrity Theory](https://doi.org/10.18653/v1/2025.naacl-long.86) |  | 0 | Privacy research has attracted wide attention as individuals worry that their private data can be easily leaked during interactions with smart devices, social platforms, and AI applications. Existing works mostly consider privacy attacks and defenses on various sub-fields. Within each field, various privacy attacks and defenses are studied to address patterns of personally identifiable information (PII). In this paper, we argue that privacy is not solely about PII patterns. We ground on the Contextual Integrity (CI) theory which posits that people’s perceptions of privacy are highly correlated with the corresponding social context. Based on such an assumption, we formulate privacy as a reasoning problem rather than naive PII matching. We develop the first comprehensive checklist that covers social identities, private attributes, and existing privacy regulations. Unlike prior works on CI that either cover limited expert annotated norms or model incomplete social context, our proposed privacy checklist uses the whole Health Insurance Portability and Accountability Act of 1996 (HIPAA) as an example, to show that we can resort to large language models (LLMs) to completely cover the HIPAA’s regulations. Additionally, our checklist also gathers expert annotations across multiple ontologies to determine private information including but not limited to PII. We use our preliminary results on the HIPAA to shed light on future context-centric privacy research to cover more privacy regulations, social norms and standards. We will release the reproducible code and data. | Haoran Li, Wei Fan, Yulin Chen, Cheng Jiayang, Tianshu Chu, Xuebing Zhou, Peizhao Hu, Yangqiu Song |  |
| 168 |  |  [Investigating the (De)Composition Capabilities of Large Language Models in Natural-to-Formal Language Conversion](https://doi.org/10.18653/v1/2025.naacl-long.87) |  | 0 | Humans have strong capabilities of decomposition and composition in natural-to-formal language conversion (N2F) when faced with an unfamiliar formal language, and can easily cope with compositional gaps and counter-intuitive symbolic names. To investigate whether large language models (LLMs) have this set of basic capabilities in N2F, we propose the STD framework. This framework semi-automatically performs sample and task construction, allowing decoupled evaluation of the set of decomposition and composition capabilities of LLMs in N2F. Based on this framework, we evaluate and analyze the most advanced LLMs, and the main findings include that: (1) the LLMs are deficient in both decomposition and composition; (2) the LLMs show a wide coverage of error types that can be attributed to deficiencies in natural language understanding and the learning and use of symbolic systems; (3) compositional gaps and counter-intuitive symbolic names both affect the decomposition and composition of the LLMs. Our work provides a new perspective for investigating the basic capabilities of decomposition and composition of LLMs in N2F. The detailed analysis of deficiencies and attributions can help subsequent improvements of LLMs. | Ziyao Xu, Houfeng Wang |  |
| 169 |  |  [Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring](https://doi.org/10.18653/v1/2025.naacl-long.88) |  | 0 | Large language model (LLM) safety is a critical issue, with numerous studies employing red team testing to enhance model security. Among these, jailbreak methods explore potential vulnerabilities by crafting malicious prompts that induce model outputs contrary to safety alignments. Existing black-box jailbreak methods often rely on model feedback, repeatedly submitting queries with detectable malicious instructions during the attack search process. Although these approaches are effective, the attacks may be intercepted by content moderators during the search process. We propose an improved transfer attack method that guides malicious prompt construction by locally training a mirror model of the target black-box model through benign data distillation. This method offers enhanced stealth, as it does not involve submitting identifiable malicious instructions to the target model during the search phase. Our approach achieved a maximum attack success rate of 92%, or a balanced value of 80% with an average of 1.5 detectable jailbreak queries per sample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore the need for more robust defense mechanisms. | Honglin Mu, Han He, Yuxin Zhou, Yunlong Feng, Yang Xu, Libo Qin, Xiaoming Shi, Zeming Liu, Xudong Han, Qi Shi, Qingfu Zhu, Wanxiang Che |  |
| 170 |  |  [VividMed: Vision Language Model with Versatile Visual Grounding for Medicine](https://doi.org/10.18653/v1/2025.naacl-long.89) |  | 0 | Recent advancements in Vision Language Models (VLMs) have demonstrated remarkable promise in generating visually grounded responses. However, their application in the medical domain is hindered by unique challenges. For instance, most VLMs rely on a single method of visual grounding, whereas complex medical tasks demand more versatile approaches. Additionally, while most VLMs process only 2D images, a large portion of medical images are 3D. The lack of medical data further compounds these obstacles. To address these challenges, we present VividMed, a vision language model with versatile visual grounding for medicine. Our model supports generating both semantic segmentation masks and instance-level bounding boxes, and accommodates various imaging modalities, including both 2D and 3D data. We design a three-stage training procedure and an automatic data synthesis pipeline based on open datasets and models. Besides visual grounding tasks, VividMed also excels in other common downstream tasks, including Visual Question Answering (VQA) and report generation. Ablation studies empirically show that the integration of visual grounding ability leads to improved performance on these tasks. Our code is publicly available at https://github.com/function2-llx/MMMM. | Lingxiao Luo, Bingda Tang, Xuanzhong Chen, Rong Han, Ting Chen |  |
| 171 |  |  [Mixture of Multimodal Adapters for Sentiment Analysis](https://doi.org/10.18653/v1/2025.naacl-long.90) |  | 0 | Pre-trained language model (PLM) have achieved great success in text sentiment analysis. However, in practical applications, sentiment is not only conveyed through language but also hidden in other modalities. Therefore, multimodal sentiment analysis (MSA) has attracted increasing research interest. Compared to text sentiment analysis, MSA is challenging since (1) emotions hidden in body movements or vocal timbres eclipse traditional analytical methods, and (2) transferring PLM to MSA task requires huge training parameters. Thus, to solve these issues, we introduce the Mixture of Multimodal Adapters (MMA) into the PLM. Specifically, we first design a mixture-of-multimodal-experts module to capture and fuse emotional movements from different data. Meanwhile, we use a compression parameter for each expert to reduce the training burden. We apply our method to two benchmark datasets and achieve state-of-the-art performance with a tiny trainable parameter count. For example, compared to the current state-of-the-art method, AcFormer, we only need 1/22 of its training parameters amount (130M→6M) to achieve better results. | Kezhou Chen, Shuo Wang, Huixia Ben, Shengeng Tang, Yanbin Hao |  |
| 172 |  |  [The Impact of Inference Acceleration on Bias of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.91) |  | 0 | Last few years have seen unprecedented advances in capabilities of Large Language Models (LLMs). These advancements promise to benefit a vast array of application domains. However, due to their immense size, performing inference with LLMs is both costly and slow. Consequently, a plethora of recent work has proposed strategies to enhance inference efficiency, e.g., quantization, pruning, and caching. These acceleration strategies reduce the inference cost and latency, often by several factors, while maintaining much of the predictive performance measured via common benchmarks. In this work, we explore another critical aspect of LLM performance: demographic bias in model generations due to inference acceleration optimizations. Using a wide range of metrics, we probe bias in model outputs from a number of angles. Analysis of outputs before and after inference acceleration shows significant change in bias. Worryingly, these bias effects are complex and unpredictable. A combination of an acceleration strategy and bias type may show little bias change in one model but may lead to a large effect in another. Our results highlight a need for in-depth and case-by-case evaluation of model bias after it has been modified to accelerate inference.This paper contains prompts and outputs which may be deemed offensive. | Elisabeth Kirsten, Ivan Habernal, Vedant Nanda, Muhammad Bilal Zafar |  |
| 173 |  |  [AfriHate: A Multilingual Collection of Hate Speech and Abusive Language Datasets for African Languages](https://doi.org/10.18653/v1/2025.naacl-long.92) |  | 0 | Hate speech and abusive language are global phenomena that need socio-cultural background knowledge to be understood, identified, and moderated. However, in many regions of the Global South, there have been several documented occurrences of (1) absence of moderation and (2) censorship due to the reliance on keyword spotting out of context. Further, high-profile individuals have frequently been at the center of the moderation process, while large and targeted hate speech campaigns against minorities have been overlooked.These limitations are mainly due to the lack of high-quality data in the local languages and the failure to include local communities in the collection, annotation, and moderation processes. To address this issue, we present AfriHate: a multilingual collection of hate speech and abusive language datasets in 15 African languages. Each instance in AfriHate is a tweet annotated by native speakers familiar with the regional culture. We report the challenges related to the construction of the datasets and present various classification baseline results with and without using LLMs. We find that model performance highly depends on the language and that multilingual models can help boost performance in low-resource settings. | Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Abinew Ali Ayele, David Ifeoluwa Adelani, Ibrahim Said Ahmad, Saminu Mohammad Aliyu, Paul Röttger, Abigail Oppong, Andiswa Bukula, Chiamaka Ijeoma Chukwuneke, Ebrahim Chekol Jibril, Elyas Abdi Ismail, Esubalew Alemneh, Hagos Tesfahun Gebremichael, Lukman Jibril Aliyu, Meriem Beloucif, Oumaima Hourrane, Rooweither Mabuya, Salomey Osei, Samuel Rutunda, Tadesse Destaw Belay, Tadesse Kebede Guge, Tesfa Tegegne Asfaw, Lilian Diana Awuor Wanzare, Nelson Odhiambo Onyango, Seid Muhie Yimam, Nedjma Ousidhoum |  |
| 174 |  |  [Revealing the Barriers of Language Agents in Planning](https://doi.org/10.18653/v1/2025.naacl-long.93) |  | 0 | Autonomous planning has been an ongoing pursuit since the inception of artificial intelligence. Based on curated problem solvers, early planning agents could deliver precise solutions for specific tasks but lacked generalization. The emergence of large language models (LLMs) and their powerful reasoning capabilities has reignited interest in autonomous planning by automatically generating reasonable solutions for given tasks. However, prior research and our experiments show that current language agents still lack human-level planning abilities. Even the state-of-the-art reasoning model, OpenAI o1, achieves only 15.6% on one of the complex real-world planning benchmarks. This highlights a critical question: What hinders language agents from achieving human-level planning? Although existing studies have highlighted weak performance in agent planning, the deeper underlying issues and the mechanisms and limitations of the strategies proposed to address them remain insufficiently understood. In this work, we apply the feature attribution study and identify two key factors that hinder agent planning: the limited role of constraints and the diminishing influence of questions. We also find that although current strategies help mitigate these challenges, they do not fully resolve them, indicating that agents still have a long way to go before reaching human-level intelligence. | Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, Yanghua Xiao |  |
| 175 |  |  [You Only Read Once (YORO): Learning to Internalize Database Knowledge for Text-to-SQL](https://doi.org/10.18653/v1/2025.naacl-long.94) |  | 0 |  | Hideo Kobayashi, Wuwei Lan, Peng Shi, Shuaichen Chang, Jiang Guo, Henghui Zhu, Zhiguo Wang, Patrick Ng |  |
| 176 |  |  [Option Symbol Matters: Investigating and Mitigating Multiple-Choice Option Symbol Bias of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.95) |  | 0 |  | Zhen Yang, Ping Jian, Chengzhi Li |  |
| 177 |  |  [DAWN-ICL: Strategic Planning of Problem-solving Trajectories for Zero-Shot In-Context Learning](https://doi.org/10.18653/v1/2025.naacl-long.96) |  | 0 |  | Xinyu Tang, Xiaolei Wang, Xin Zhao, JiRong Wen |  |
| 178 |  |  [LLaSA: Large Language and Structured Data Assistant](https://doi.org/10.18653/v1/2025.naacl-long.97) |  | 0 |  | Yao Xu, Shizhu He, Jiabei Chen, ZengXiangrong ZengXiangrong, Bingning Wang, Guang Liu, Jun Zhao, Kang Liu |  |
| 179 |  |  [Towards Efficient and Multifaceted Computer-assisted Pronunciation Training Leveraging Hierarchical Selective State Space Model and Decoupled Cross-entropy Loss](https://doi.org/10.18653/v1/2025.naacl-long.98) |  | 0 |  | FuAn Chao, Berlin Chen |  |
| 180 |  |  [Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.99) |  | 0 |  | Abhilasha Ravichander, Jillian Fisher, Taylor Sorensen, Ximing Lu, Maria Antoniak, Bill Yuchen Lin, Niloofar Mireshghallah, Chandra Bhagavatula, Yejin Choi |  |
| 181 |  |  [An Interpretable and Crosslingual Method for Evaluating Second-Language Dialogues](https://doi.org/10.18653/v1/2025.naacl-long.100) |  | 0 |  | Rena Gao, Xuetong Wu, Carsten Roever, Jing Wu, Long Lv, Jingxuan Wu, Jey Han Lau |  |
| 182 |  |  [From Allies to Adversaries: Manipulating LLM Tool-Calling through Adversarial Injection](https://doi.org/10.18653/v1/2025.naacl-long.101) |  | 0 |  | Rupeng Zhang, Haowei Wang, Junjie Wang, Mingyang Li, Yuekai Huang, Dandan Wang, Qing Wang |  |
| 183 |  |  [COVE: COntext and VEracity prediction for out-of-context images](https://doi.org/10.18653/v1/2025.naacl-long.102) |  | 0 |  | Jonathan Tonglet, Gabriel Thiem, Iryna Gurevych |  |
| 184 |  |  [Discourse-Driven Evaluation: Unveiling Factual Inconsistency in Long Document Summarization](https://doi.org/10.18653/v1/2025.naacl-long.103) |  | 0 |  | Yang Zhong, Diane J. Litman |  |
| 185 |  |  [Language Models are Crossword Solvers](https://doi.org/10.18653/v1/2025.naacl-long.104) |  | 0 |  | Soumadeep Saha, Sutanoya Chakraborty, Saptarshi Saha, Utpal Garain |  |
| 186 |  |  [WHoW: A Cross-domain Approach for Analysing Conversation Moderation](https://doi.org/10.18653/v1/2025.naacl-long.105) |  | 0 |  | MingBin Chen, Lea Frermann, Jey Han Lau |  |
| 187 |  |  [Uplifting Lower-Income Data: Strategies for Socioeconomic Perspective Shifts in Large Multi-modal Models](https://doi.org/10.18653/v1/2025.naacl-long.106) |  | 0 |  | Joan Nwatu, Oana Ignat, Rada Mihalcea |  |
| 188 |  |  [MSc-SQL: Multi-Sample Critiquing Small Language Models For Text-To-SQL Translation](https://doi.org/10.18653/v1/2025.naacl-long.107) |  | 0 |  | Satya Krishna Gorti, Ilan Gofman, Zhaoyan Liu, Jiapeng Wu, Noël Vouitsis, Guangwei Yu, Jesse C. Cresswell, Rasa Hosseinzadeh |  |
| 189 |  |  [Mitigating Heterogeneity among Factor Tensors via Lie Group Manifolds for Tensor Decomposition Based Temporal Knowledge Graph Embedding](https://doi.org/10.18653/v1/2025.naacl-long.108) |  | 0 |  | Jiang Li, Xiangdong Su, Guanglai Gao |  |
| 190 |  |  [What Goes Into a LM Acceptability Judgment? Rethinking the Impact of Frequency and Length](https://doi.org/10.18653/v1/2025.naacl-long.109) |  | 0 |  | Lindia Tjuatja, Graham Neubig, Tal Linzen, Sophie Hao |  |
| 191 |  |  [WaveFM: A High-Fidelity and Efficient Vocoder Based on Flow Matching](https://doi.org/10.18653/v1/2025.naacl-long.110) |  | 0 |  | Tianze Luo, Xingchen Miao, Wenbo Duan |  |
| 192 |  |  [Analyzing and Evaluating Correlation Measures in NLG Meta-Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.111) |  | 0 |  | Mingqi Gao, Xinyu Hu, Li Lin, Xiaojun Wan |  |
| 193 |  |  [Cascading Large Language Models for Salient Event Graph Generation](https://doi.org/10.18653/v1/2025.naacl-long.112) |  | 0 |  | Xingwei Tan, Yuxiang Zhou, Gabriele Pergola, Yulan He |  |
| 194 |  |  [Token-Level Density-Based Uncertainty Quantification Methods for Eliciting Truthfulness of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.113) |  | 0 |  | Artem Vazhentsev, Lyudmila Rvanova, Ivan Lazichny, Alexander Panchenko, Maxim Panov, Timothy Baldwin, Artem Shelmanov |  |
| 195 |  |  [How Can We Diagnose and Treat Bias in Large Language Models for Clinical Decision-Making?](https://doi.org/10.18653/v1/2025.naacl-long.114) |  | 0 |  | Kenza Benkirane, Jackie Kay, María PérezOrtiz |  |
| 196 |  |  [From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks](https://doi.org/10.18653/v1/2025.naacl-long.115) |  | 0 |  | Xiaofeng Zhang, Yihao Quan, Chen Shen, Xiaosong Yuan, Shaotian Yan, Liang Xie, Wenxiao Wang, Chaochen Gu, Hao Tang, Jieping Ye |  |
| 197 |  |  [Patent-CR: A Dataset for Patent Claim Revision](https://doi.org/10.18653/v1/2025.naacl-long.116) |  | 0 |  | Lekang Jiang, Pascal A. Scherz, Stefan Goetz |  |
| 198 |  |  [MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs](https://doi.org/10.18653/v1/2025.naacl-long.117) |  | 0 |  | Yuhang Zhou, Giannis Karamanolakis, Victor Soto, Anna Rumshisky, Mayank Kulkarni, Furong Huang, Wei Ai, Jianhua Lu |  |
| 199 |  |  [Fine-Tuned LLMs are "Time Capsules" for Tracking Societal Bias Through Books](https://doi.org/10.18653/v1/2025.naacl-long.118) |  | 0 |  | Sangmitra Madhusudan, Robert Morabito, Skye Reid, Nikta Gohari Sadr, Ali Emami |  |
| 200 |  |  [Exploring the Cost-Effectiveness of Perspective Taking in Crowdsourcing Subjective Assessment: A Case Study of Toxicity Detection](https://doi.org/10.18653/v1/2025.naacl-long.119) |  | 0 |  | Xiaoni Duan, Zhuoyan Li, ChienJu Ho, Ming Yin |  |
| 201 |  |  [NormAd: A Framework for Measuring the Cultural Adaptability of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.120) |  | 0 |  | Abhinav Rao, Akhila Yerukola, Vishwa Shah, Katharina Reinecke, Maarten Sap |  |
| 202 |  |  [LiPO: Listwise Preference Optimization through Learning-to-Rank](https://doi.org/10.18653/v1/2025.naacl-long.121) |  | 0 |  | Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh Joshi, Yao Zhao, Mohammad Saleh, Simon Baumgartner, Jialu Liu, Peter J. Liu, Xuanhui Wang |  |
| 203 |  |  [Adaptive Prompting: Ad-hoc Prompt Composition for Social Bias Detection](https://doi.org/10.18653/v1/2025.naacl-long.122) |  | 0 |  | Maximilian Spliethöver, Tim Knebler, Fabian Fumagalli, Maximilian Muschalik, Barbara Hammer, Eyke Hüllermeier, Henning Wachsmuth |  |
| 204 |  |  [Enhancing Discriminative Representation in Similar Relation Clusters for Few-Shot Continual Relation Extraction](https://doi.org/10.18653/v1/2025.naacl-long.123) |  | 0 |  | Anh Duc Le, Nam Le Hai, Thanh Xuan Nguyen, Linh Ngo Van, Nguyen Thi Ngoc Diep, Sang Dinh, Thien Huu Nguyen |  |
| 205 |  |  [SymBa: Symbolic Backward Chaining for Structured Natural Language Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.124) |  | 0 |  | Jinu Lee, Wonseok Hwang |  |
| 206 |  |  [MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference](https://doi.org/10.18653/v1/2025.naacl-long.125) |  | 0 |  | Zhongwei Wan, Hui Shen, Xin Wang, Che Liu, Zheda Mai, Mi Zhang |  |
| 207 |  |  [Language Models Largely Exhibit Human-like Constituent Ordering Preferences](https://doi.org/10.18653/v1/2025.naacl-long.126) |  | 0 |  | Ada Defne Tur, Gaurav Kamath, Siva Reddy |  |
| 208 |  |  [SafeQuant: LLM Safety Analysis via Quantized Gradient Inspection](https://doi.org/10.18653/v1/2025.naacl-long.127) |  | 0 |  | Sindhu Padakandla, Sadbhavana Babar, Rathod Darshan D, Manohar Kaul |  |
| 209 |  |  [Exploring Large Language Models for Effective Rumor Detection on Social Media](https://doi.org/10.18653/v1/2025.naacl-long.128) |  | 0 |  | Yirong Zeng, Xiao Ding, Bibo Cai, Ting Liu, Bing Qin |  |
| 210 |  |  [No Simple Answer to Data Complexity: An Examination of Instance-Level Complexity Metrics for Classification Tasks](https://doi.org/10.18653/v1/2025.naacl-long.129) |  | 0 |  | Ryan A. Cook, John P. Lalor, Ahmed Abbasi |  |
| 211 |  |  [NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals](https://doi.org/10.18653/v1/2025.naacl-long.130) |  | 0 |  | Neha Srikanth, Rachel Rudinger |  |
| 212 |  |  [HISTOIRESMORALES: A French Dataset for Assessing Moral Alignment](https://doi.org/10.18653/v1/2025.naacl-long.131) |  | 0 |  | Thibaud Leteno, Irina Proskurina, Antoine Gourru, Julien Velcin, Charlotte Laclau, Guillaume Metzler, Christophe Gravier |  |
| 213 |  |  [Leveraging Allophony in Self-Supervised Speech Models for Atypical Pronunciation Assessment](https://doi.org/10.18653/v1/2025.naacl-long.132) |  | 0 |  | Kwanghee Choi, Eunjung Yeo, Kalvin Chang, Shinji Watanabe, David R. Mortensen |  |
| 214 |  |  [SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search](https://doi.org/10.18653/v1/2025.naacl-long.133) |  | 0 |  | Hanwen Du, Bo Peng, Xia Ning |  |
| 215 |  |  [Reliability of Topic Modeling](https://doi.org/10.18653/v1/2025.naacl-long.134) |  | 0 |  | Kayla Schroeder, Zach WoodDoughty |  |
| 216 |  |  [Style Transfer with Multi-iteration Preference Optimization](https://doi.org/10.18653/v1/2025.naacl-long.135) |  | 0 |  | Shuai Liu, Jonathan May |  |
| 217 |  |  [DTELS: Towards Dynamic Granularity of Timeline Summarization](https://doi.org/10.18653/v1/2025.naacl-long.136) |  | 0 |  | Chenlong Zhang, Tong Zhou, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao |  |
| 218 |  |  [ALERT: An LLM-powered Benchmark for Automatic Evaluation of Recommendation Explanations](https://doi.org/10.18653/v1/2025.naacl-long.137) |  | 0 |  | Yichuan Li, Xinyang Zhang, Chenwei Zhang, Mao Li, Tianyi Liu, Pei Chen, Yifan Gao, Kyumin Lee, Kaize Ding, Zhengyang Wang, Zhihan Zhang, Jingbo Shang, Xian Li, Trishul Chilimbi |  |
| 219 |  |  [DETQUS: Decomposition-Enhanced Transformers for QUery-focused Summarization](https://doi.org/10.18653/v1/2025.naacl-long.138) |  | 0 |  | Yasir Khan, Xinlei Wu, Sangpil Youm, Justin Ho, Aryaan Shaikh, Jairo Garciga, Rohan Sharma, Bonnie J. Dorr |  |
| 220 |  |  [IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.139) |  | 0 |  | David Ifeoluwa Adelani, Jessica Ojo, Israel Abebe Azime, Jian Yun Zhuang, Jesujoba Oluwadara Alabi, Xuanli He, Millicent Ochieng, Sara Hooker, Andiswa Bukula, EnShiun Annie Lee, Chiamaka Ijeoma Chukwuneke, Happy Buzaaba, Blessing K. Sibanda, Godson Koffi Kalipe, Jonathan Mukiibi, Salomon Kabongo Kabenamualu, Foutse Yuehgoh, Mmasibidi Setaka, Lolwethu Ndolela, Nkiruka Odu, Rooweither Mabuya, Salomey Osei, Shamsuddeen Hassan Muhammad, Sokhar Samb, Tadesse Kebede Guge, Tombekai Vangoni Sherman, Pontus Stenetorp |  |
| 221 |  |  [The Impact of Domain-Specific Terminology on Machine Translation for Finance in European Languages](https://doi.org/10.18653/v1/2025.naacl-long.140) |  | 0 |  | Arturo Oncevay, Charese Smiley, Xiaomo Liu |  |
| 222 |  |  [Benchmarking Language Model Creativity: A Case Study on Code Generation](https://doi.org/10.18653/v1/2025.naacl-long.141) |  | 0 |  | Yining Lu, Dixuan Wang, Tianjian Li, Dongwei Jiang, Sanjeev Khudanpur, Meng Jiang, Daniel Khashabi |  |
| 223 |  |  [Have LLMs Reopened the Pandora's Box of AI-Generated Fake News?](https://doi.org/10.18653/v1/2025.naacl-long.142) |  | 0 |  | Xinyu Wang, Wenbo Zhang, Sai Dileep Koneru, Hangzhi Guo, Bonam Mingole, S. Shyam Sundar, Sarah Rajtmajer, Amulya Yadav |  |
| 224 |  |  [Probe-Free Low-Rank Activation Intervention](https://doi.org/10.18653/v1/2025.naacl-long.143) |  | 0 |  | Chonghe Jiang, Bao Nguyen, Anthony ManCho So, Viet Anh Nguyen |  |
| 225 |  |  [FactTrack: Time-Aware World State Tracking in Story Outlines](https://doi.org/10.18653/v1/2025.naacl-long.144) |  | 0 |  | Zhiheng Lyu, Kevin Yang, Lingpeng Kong, Dan Klein |  |
| 226 |  |  [A Bayesian Optimization Approach to Machine Translation Reranking](https://doi.org/10.18653/v1/2025.naacl-long.145) |  | 0 |  | Julius Cheng, Maike Züfle, Vilém Zouhar, Andreas Vlachos |  |
| 227 |  |  [Multi-Conditional Ranking with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.146) |  | 0 |  | Pouya Pezeshkpour, Estevam Hruschka |  |
| 228 |  |  [ReGLA: Refining Gated Linear Attention](https://doi.org/10.18653/v1/2025.naacl-long.147) |  | 0 |  | Peng Lu, Ivan Kobyzev, Mehdi Rezagholizadeh, Boxing Chen, Philippe Langlais |  |
| 229 |  |  [Intrinsic Bias is Predicted by Pretraining Data and Correlates with Downstream Performance in Vision-Language Encoders](https://doi.org/10.18653/v1/2025.naacl-long.148) |  | 0 |  | Kshitish Ghate, Isaac Slaughter, Kyra Wilson, Mona T. Diab, Aylin Caliskan |  |
| 230 |  |  [Benchmarking Failures in Tool-Augmented Language Models](https://doi.org/10.18653/v1/2025.naacl-long.149) |  | 0 |  | Eduardo Treviño, Hugo Contant, James Ngai, Graham Neubig, Zora Zhiruo Wang |  |
| 231 |  |  [Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity Recognition Framework](https://doi.org/10.18653/v1/2025.naacl-long.150) |  | 0 |  | Reza Averly, Xia Ning |  |
| 232 |  |  [Towards Knowledge Checking in Retrieval-augmented Generation: A Representation Perspective](https://doi.org/10.18653/v1/2025.naacl-long.151) |  | 0 |  | Shenglai Zeng, Jiankun Zhang, Bingheng Li, Yuping Lin, Tianqi Zheng, Dante Everaert, Hanqing Lu, Hui Liu, Yue Xing, Monica Xiao Cheng, Jiliang Tang |  |
| 233 |  |  [The Power of Many: Multi-Agent Multimodal Models for Cultural Image Captioning](https://doi.org/10.18653/v1/2025.naacl-long.152) |  | 0 |  | Longju Bai, Angana Borah, Oana Ignat, Rada Mihalcea |  |
| 234 |  |  [Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison](https://doi.org/10.18653/v1/2025.naacl-long.153) |  | 0 |  | Tsz Kin Lam, Marco Gaido, Sara Papi, Luisa Bentivogli, Barry Haddow |  |
| 235 |  |  [CORRECT: Context- and Reference-Augmented Reasoning and Prompting for Fact-Checking](https://doi.org/10.18653/v1/2025.naacl-long.154) |  | 0 |  | Delvin Ce Zhang, Dongwon Lee |  |
| 236 |  |  [Racing Thoughts: Explaining Contextualization Errors in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.155) |  | 0 |  | Michael A. Lepori, Michael Curtis Mozer, Asma Ghandeharioun |  |
| 237 |  |  [DREAM: Improving Video-Text Retrieval Through Relevance-Based Augmentation Using Large Foundation Models](https://doi.org/10.18653/v1/2025.naacl-long.156) |  | 0 |  | Yimu Wang, Shuai Yuan, Bo Xue, Xiangru Jian, Wei Pang, Mushi Wang, Ning Yu |  |
| 238 |  |  [ToW: Thoughts of Words Improve Reasoning in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.157) |  | 0 |  | Zhikun Xu, Ming Shen, Jacob Dineen, Zhaonan Li, Xiao Ye, Shijie Lu, Aswin RRV, Chitta Baral, Ben Zhou |  |
| 239 |  |  [A Probabilistic Framework for LLM Hallucination Detection via Belief Tree Propagation](https://doi.org/10.18653/v1/2025.naacl-long.158) |  | 0 |  | Bairu Hou, Yang Zhang, Jacob Andreas, Shiyu Chang |  |
| 240 |  |  [ERAS: Evaluating the Robustness of Chinese NLP Models to Morphological Garden Path Errors](https://doi.org/10.18653/v1/2025.naacl-long.159) |  | 0 |  | Qinchan Li, Sophie Hao |  |
| 241 |  |  [Superlatives in Context: Modeling the Implicit Semantics of Superlatives](https://doi.org/10.18653/v1/2025.naacl-long.160) |  | 0 |  | Valentina Pyatkin, Bonnie Webber, Ido Dagan, Reut Tsarfaty |  |
| 242 |  |  [LLMs Are Not Intelligent Thinkers: Introducing Mathematical Topic Tree Benchmark for Comprehensive Evaluation of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.161) |  | 0 |  | Arash Gholami Davoodi, Seyed Pouyan Mousavi Davoudi, Pouya Pezeshkpour |  |
| 243 |  |  [Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations](https://doi.org/10.18653/v1/2025.naacl-long.162) |  | 0 |  | Yong Cao, Haijiang Liu, Arnav Arora, Isabelle Augenstein, Paul Röttger, Daniel Hershcovich |  |
| 244 |  |  [Representing Rule-based Chatbots with Transformers](https://doi.org/10.18653/v1/2025.naacl-long.163) |  | 0 |  | Dan Friedman, Abhishek Panigrahi, Danqi Chen |  |
| 245 |  |  [Incremental Sentence Processing Mechanisms in Autoregressive Transformer Language Models](https://doi.org/10.18653/v1/2025.naacl-long.164) |  | 0 |  | Michael Hanna, Aaron Mueller |  |
| 246 |  |  [Entangled Relations: Leveraging NLI and Meta-analysis to Enhance Biomedical Relation Extraction](https://doi.org/10.18653/v1/2025.naacl-long.165) |  | 0 |  | William Hogan, Jingbo Shang |  |
| 247 |  |  [Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.166) |  | 0 |  | Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, Hao Wang |  |
| 248 |  |  [WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines](https://doi.org/10.18653/v1/2025.naacl-long.167) |  | 0 |  | Genta Indra Winata, Frederikus Hudi, Patrick Amadeus Irawan, David Anugraha, Rifki Afina Putri, Yutong Wang, Adam Nohejl, Ubaidillah Ariq Prathama, Nedjma Ousidhoum, Afifa Amriani, Anar Rzayev, Anirban Das, Ashmari Pramodya, Aulia Adila, Bryan Wilie, Candy Olivia Mawalim, Cheng Ching Lam, Daud Abolade, Emmanuele Chersoni, Enrico Santus, Fariz Ikhwantri, Garry Kuwanto, Hanyang Zhao, Haryo Akbarianto Wibowo, Holy Lovenia, Jan Christian Blaise Cruz, Jan Wira Gotama Putra, Junho Myung, Lucky Susanto, Maria Angelica Riera Machin, Marina Zhukova, Michael Anugraha, Muhammad Farid Adilazuarda, Natasha Christabelle Santosa, Peerat Limkonchotiwat, Raj Dabre, Rio Alexander Audino, Samuel Cahyawijaya, ShiXiong Zhang, Stephanie Yulia Salim, Yi Zhou, Yinxuan Gui, David Ifeoluwa Adelani, EnShiun Annie Lee, Shogo Okada, Ayu Purwarianti, Alham Fikri Aji, Taro Watanabe, Derry Tanti Wijaya, Alice Oh, ChongWah Ngo |  |
| 249 |  |  [Extracting and Understanding the Superficial Knowledge in Alignment](https://doi.org/10.18653/v1/2025.naacl-long.168) |  | 0 |  | Runjin Chen, Gabriel J. Perin, Xuxi Chen, Xilun Chen, Yan Han, Nina S. T. Hirata, Junyuan Hong, Bhavya Kailkhura |  |
| 250 |  |  [Smurfs: Multi-Agent System using Context-Efficient DFSDT for Tool Planning](https://doi.org/10.18653/v1/2025.naacl-long.169) |  | 0 |  | Junzhi Chen, Juhao Liang, Benyou Wang |  |
| 251 |  |  [From Introspection to Best Practices: Principled Analysis of Demonstrations in Multimodal In-Context Learning](https://doi.org/10.18653/v1/2025.naacl-long.170) |  | 0 |  | Nan Xu, Fei Wang, Sheng Zhang, Hoifung Poon, Muhao Chen |  |
| 252 |  |  [Upsample or Upweight? Balanced Training on Heavily Imbalanced Datasets](https://doi.org/10.18653/v1/2025.naacl-long.171) |  | 0 |  | Tianjian Li, Haoran Xu, Weiting Tan, Kenton Murray, Daniel Khashabi |  |
| 253 |  |  [LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with Simple Word-based Counting Problems](https://doi.org/10.18653/v1/2025.naacl-long.172) |  | 0 |  | Nan Xu, Xuezhe Ma |  |
| 254 |  |  [PAPILLON: Privacy Preservation from Internet-based and Local Language Model Ensembles](https://doi.org/10.18653/v1/2025.naacl-long.173) |  | 0 |  | Siyan Li, Vethavikashini Chithrra Raghuram, Omar Khattab, Julia Hirschberg, Zhou Yu |  |
| 255 |  |  [When2Call: When (not) to Call Tools](https://doi.org/10.18653/v1/2025.naacl-long.174) |  | 0 |  | Hayley Ross, Ameya Sunil Mahabaleshwarkar, Yoshi Suhara |  |
| 256 |  |  [Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused Preference Optimization](https://doi.org/10.18653/v1/2025.naacl-long.175) |  | 0 |  | Zilu Tang, Rajen Chatterjee, Sarthak Garg |  |
| 257 |  |  [Large Language Models Can Solve Real-World Planning Rigorously with Formal Verification Tools](https://doi.org/10.18653/v1/2025.naacl-long.176) |  | 0 |  | Yilun Hao, Yongchao Chen, Yang Zhang, Chuchu Fan |  |
| 258 |  |  [Who Relies More on World Knowledge and Bias for Syntactic Ambiguity Resolution: Humans or LLMs?](https://doi.org/10.18653/v1/2025.naacl-long.177) |  | 0 |  | So Young Lee, Russell Scheinberg, Amber Shore, Ameeta Agrawal |  |
| 259 |  |  [Beyond Benchmarks: Building a Richer Cross-Document Event Coreference Dataset with Decontextualization](https://doi.org/10.18653/v1/2025.naacl-long.178) |  | 0 |  | Jin Zhao, Jingxuan Tu, Bingyang Ye, Xinrui Hu, Nianwen Xue, James Pustejovsky |  |
| 260 |  |  [Can Unconfident LLM Annotations Be Used for Confident Conclusions?](https://doi.org/10.18653/v1/2025.naacl-long.179) |  | 0 |  | Kristina Gligoric, Tijana Zrnic, Cinoo Lee, Emmanuel J. Candès, Dan Jurafsky |  |
| 261 |  |  [Beyond End-to-End VLMs: Leveraging Intermediate Text Representations for Superior Flowchart Understanding](https://doi.org/10.18653/v1/2025.naacl-long.180) |  | 0 |  | Junyi Ye, Ankan Dash, Wenpeng Yin, Guiling Wang |  |
| 262 |  |  [Ihquin tlahtouah in Tetelahtzincocah: An annotated, multi-purpose audio and text corpus of Western Sierra Puebla Nahuatl](https://doi.org/10.18653/v1/2025.naacl-long.181) |  | 0 |  | Robert Pugh, Cheyenne Wing, María Ximena Juárez Huerta, Angeles Márquez Hernandez, Francis M. Tyers |  |
| 263 |  |  [Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions](https://doi.org/10.18653/v1/2025.naacl-long.182) |  | 0 |  | Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze |  |
| 264 |  |  [Unfamiliar Finetuning Examples Control How Language Models Hallucinate](https://doi.org/10.18653/v1/2025.naacl-long.183) |  | 0 |  | Katie Kang, Eric Wallace, Claire J. Tomlin, Aviral Kumar, Sergey Levine |  |
| 265 |  |  [Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling](https://doi.org/10.18653/v1/2025.naacl-long.184) |  | 0 |  | Guangya Wan, Yuqi Wu, Jie Chen, Sheng Li |  |
| 266 |  |  [MatViX: Multimodal Information Extraction from Visually Rich Articles](https://doi.org/10.18653/v1/2025.naacl-long.185) |  | 0 |  | Ghazal Khalighinejad, Sharon Scott, Ollie Liu, Kelly L. Anderson, Rickard Stureborg, Aman Tyagi, Bhuwan Dhingra |  |
| 267 |  |  [Towards Rationality in Language and Multimodal Agents: A Survey](https://doi.org/10.18653/v1/2025.naacl-long.186) |  | 0 |  | Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Yuan Yuan, Zhuoqun Hao, Xinyi Bai, Weijie J. Su, Camillo Jose Taylor, Tanwi Mallick |  |
| 268 |  |  [CluSanT: Differentially Private and Semantically Coherent Text Sanitization](https://doi.org/10.18653/v1/2025.naacl-long.187) |  | 0 |  | Ahmed Musa Awon, Yun Lu, Shera Potka, Alex Thomo |  |
| 269 |  |  [TurkingBench: A Challenge Benchmark for Web Agents](https://doi.org/10.18653/v1/2025.naacl-long.188) |  | 0 |  | Kevin Xu, Yeganeh Kordi, Tanay Nayak, Adi Asija, Yizhong Wang, Kate Sanders, Adam Byerly, Jingyu Zhang, Benjamin Van Durme, Daniel Khashabi |  |
| 270 |  |  [CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.189) |  | 0 |  | Jierui Li, Hung Le, Yingbo Zhou, Caiming Xiong, Silvio Savarese, Doyen Sahoo |  |
| 271 |  |  [DPL: Diverse Preference Learning Without A Reference Model](https://doi.org/10.18653/v1/2025.naacl-long.190) |  | 0 |  | Abhijnan Nath, Andrey Volozin, Saumajit Saha, Albert Nanda, Galina Grunin, Rahul Bhotika, Nikhil Krishnaswamy |  |
| 272 |  |  [Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data](https://doi.org/10.18653/v1/2025.naacl-long.191) |  | 0 |  | Jingyu Zhang, Marc Marone, Tianjian Li, Benjamin Van Durme, Daniel Khashabi |  |
| 273 |  |  [VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models](https://doi.org/10.18653/v1/2025.naacl-long.192) |  | 0 |  | Zejun Li, Ruipu Luo, Jiwen Zhang, Minghui Qiu, Xuanjing Huang, Zhongyu Wei |  |
| 274 |  |  [ACCORD: Closing the Commonsense Measurability Gap](https://doi.org/10.18653/v1/2025.naacl-long.193) |  | 0 |  | François RoewerDesprés, Jinyue Feng, Zining Zhu, Frank Rudzicz |  |
| 275 |  |  [CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments](https://doi.org/10.18653/v1/2025.naacl-long.194) |  | 0 |  | KungHsiang Huang, Akshara Prabhakar, Sidharth Dhawan, Yixin Mao, Huan Wang, Silvio Savarese, Caiming Xiong, Philippe Laban, ChienSheng Wu |  |
| 276 |  |  [Mamba-Shedder: Post-Transformer Compression for Efficient Selective Structured State Space Models](https://doi.org/10.18653/v1/2025.naacl-long.195) |  | 0 |  | Juan Pablo Muñoz, Jinjie Yuan, Nilesh Jain |  |
| 277 |  |  [CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy](https://doi.org/10.18653/v1/2025.naacl-long.196) |  | 0 |  | Mian Zhang, Xianjun Yang, Xinlu Zhang, Travis Labrum, Jamie C. Chiu, Shaun M. Eack, Fei Fang, William Yang Wang, Zhiyu Chen |  |
| 278 |  |  [An Efficient Gloss-Free Sign Language Translation Using Spatial Configurations and Motion Dynamics with LLMs](https://doi.org/10.18653/v1/2025.naacl-long.197) |  | 0 |  | Eui Jun Hwang, Sukmin Cho, Junmyeong Lee, Jong C. Park |  |
| 279 |  |  [Sketch2Code: Evaluating Vision-Language Models for Interactive Web Design Prototyping](https://doi.org/10.18653/v1/2025.naacl-long.198) |  | 0 |  | Ryan Li, Yanzhe Zhang, Diyi Yang |  |
| 280 |  |  [Design2Code: Benchmarking Multimodal Code Generation for Automated Front-End Engineering](https://doi.org/10.18653/v1/2025.naacl-long.199) |  | 0 |  | Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, Diyi Yang |  |
| 281 |  |  [Temporal-Aware Soft Prompt Tuning for Automatic Text Dating](https://doi.org/10.18653/v1/2025.naacl-long.200) |  | 0 |  | Hai Wang, Yuzhi Liang, Han Ren |  |
| 282 |  |  [Sparser Mixture-of-Adapters with Cross-Layer Generalization](https://doi.org/10.18653/v1/2025.naacl-long.201) |  | 0 |  | Ziyue Li, Tianyi Zhou |  |
| 283 |  |  [How to Align Multiple Signed Language Corpora for Better Sign-to-Sign Translations?](https://doi.org/10.18653/v1/2025.naacl-long.202) |  | 0 |  | Mert Inan, Yang Zhong, Vidya Ganesh, Malihe Alikhani |  |
| 284 |  |  [Communication Makes Perfect: Persuasion Dataset Construction via Multi-LLM Communication](https://doi.org/10.18653/v1/2025.naacl-long.203) |  | 0 |  | Weicheng Ma, Hefan Zhang, Ivory Yang, Shiyu Ji, Joice Chen, Farnoosh Hashemi, Shubham Mohole, Ethan Gearey, Michael Macy, Saeed Hassanpour, Soroush Vosoughi |  |
| 285 |  |  [Soft Prompting for Unlearning in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.204) |  | 0 |  | Karuna Bhaila, MinhHao Van, Xintao Wu |  |
| 286 |  |  [Mutual-pairing Data Augmentation for Fewshot Continual Relation Extraction](https://doi.org/10.18653/v1/2025.naacl-long.205) |  | 0 |  | Nguyen Hoang Anh, Quyen Tran, Thanh Xuan Nguyen, Nguyen Thi Ngoc Diep, Linh Ngo Van, Thien Huu Nguyen, Trung Le |  |
| 287 |  |  [KMMLU: Measuring Massive Multitask Language Understanding in Korean](https://doi.org/10.18653/v1/2025.naacl-long.206) |  | 0 |  | Guijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheonbok Park, Kang Min Yoo, Stella Biderman |  |
| 288 |  |  [Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench](https://doi.org/10.18653/v1/2025.naacl-long.207) |  | 0 |  | Zheyuan Liu, Guangyao Dou, Mengzhao Jia, Zhaoxuan Tan, Qingkai Zeng, Yongle Yuan, Meng Jiang |  |
| 289 |  |  [LLM4DistReconfig: A Fine-tuned Large Language Model for Power Distribution Network Reconfiguration](https://doi.org/10.18653/v1/2025.naacl-long.208) |  | 0 |  | Panayiotis Christou, Md. Zahidul Islam, Yuzhang Lin, Jingwei Xiong |  |
| 290 |  |  [WaterPool: A Language Model Watermark Mitigating Trade-Offs among Imperceptibility, Efficacy and Robustness](https://doi.org/10.18653/v1/2025.naacl-long.209) |  | 0 |  | Baizhou Huang, Xiaojun Wan |  |
| 291 |  |  [Tricking Retrievers with Influential Tokens: An Efficient Black-Box Corpus Poisoning Attack](https://doi.org/10.18653/v1/2025.naacl-long.210) |  | 0 |  | Cheng Wang, Yiwei Wang, Yujun Cai, Bryan Hooi |  |
| 292 |  |  [The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism](https://doi.org/10.18653/v1/2025.naacl-long.211) |  | 0 |  | Yifan Song, Guoyin Wang, Sujian Li, Bill Yuchen Lin |  |
| 293 |  |  [CVE-Bench: Benchmarking LLM-based Software Engineering Agent's Ability to Repair Real-World CVE Vulnerabilities](https://doi.org/10.18653/v1/2025.naacl-long.212) |  | 0 |  | Peiran Wang, Xiaogeng Liu, Chaowei Xiao |  |
| 294 |  |  [PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines](https://doi.org/10.18653/v1/2025.naacl-long.213) |  | 0 |  | Reya Vir, Shreya Shankar, Harrison Chase, Will FuHinthorn, Aditya G. Parameswaran |  |
| 295 |  |  [ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent Dialogue Synthesis](https://doi.org/10.18653/v1/2025.naacl-long.214) |  | 0 |  | Zezhong Wang, Xingshan Zeng, Weiwen Liu, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, KamFai Wong |  |
| 296 |  |  [Fighting Spurious Correlations in Text Classification via a Causal Learning Perspective](https://doi.org/10.18653/v1/2025.naacl-long.215) |  | 0 |  | Yuqing Zhou, Ziwei Zhu |  |
| 297 |  |  [Knowledge-Aware Query Expansion with Large Language Models for Textual and Relational Retrieval](https://doi.org/10.18653/v1/2025.naacl-long.216) |  | 0 |  | Yu Xia, Junda Wu, Sungchul Kim, Tong Yu, Ryan A. Rossi, Haoliang Wang, Julian J. McAuley |  |
| 298 |  |  [SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression](https://doi.org/10.18653/v1/2025.naacl-long.217) |  | 0 |  | Xin Wang, Samiul Alam, Zhongwei Wan, Hui Shen, Mi Zhang |  |
| 299 |  |  [AudioBench: A Universal Benchmark for Audio Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.218) |  | 0 |  | Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, Nancy F. Chen |  |
| 300 |  |  [Efficient Prompting for Continual Adaptation to Missing Modalities](https://doi.org/10.18653/v1/2025.naacl-long.219) |  | 0 |  | Zirun Guo, Shulei Wang, Wang Lin, Weicai Yan, Yangyang Wu, Tao Jin |  |
| 301 |  |  [Benchmarking and Building Zero-Shot Hindi Retrieval Model with Hindi-BEIR and NLLB-E5](https://doi.org/10.18653/v1/2025.naacl-long.220) |  | 0 |  | Arkadeep Acharya, Rudra Murthy, Vishwajeet Kumar, Jaydeep Sen |  |
| 302 |  |  [Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion](https://doi.org/10.18653/v1/2025.naacl-long.221) |  | 0 |  | Muzhi Li, Cehao Yang, Chengjin Xu, Xuhui Jiang, Yiyan Qi, Jian Guo, Hofung Leung, Irwin King |  |
| 303 |  |  [See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language Balance to Mitigate Dominant Modality Bias](https://doi.org/10.18653/v1/2025.naacl-long.222) |  | 0 |  | Junehyoung Kwon, Mihyeon Kim, Eunju Lee, Juhwan Choi, YoungBin Kim |  |
| 304 |  |  [Harnessing and Evaluating the Intrinsic Extrapolation Ability of Large Language Models for Vehicle Trajectory Prediction](https://doi.org/10.18653/v1/2025.naacl-long.223) |  | 0 |  | Jiawei Liu, Yanjiao Liu, Xun Gong, Tingting Wang, Hong Chen, Yunfeng Hu |  |
| 305 |  |  [Stronger Models are Not Always Stronger Teachers for Instruction Tuning](https://doi.org/10.18653/v1/2025.naacl-long.224) |  | 0 |  | Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Radha Poovendran |  |
| 306 |  |  [Efficient and Effective Prompt Tuning via Prompt Decomposition and Compressed Outer Product](https://doi.org/10.18653/v1/2025.naacl-long.225) |  | 0 |  | Pengxiang Lan, Haoyu Xu, Enneng Yang, Yuliang Liang, Guibing Guo, Jianzhe Zhao, Xingwei Wang |  |
| 307 |  |  [Threshold Filtering Packing for Supervised Fine-Tuning: Training Related Samples within Packs](https://doi.org/10.18653/v1/2025.naacl-long.226) |  | 0 |  | Jiancheng Dong, Lei Jiang, Wei Jin, Lu Cheng |  |
| 308 |  |  [Transferable Post-training via Inverse Value Learning](https://doi.org/10.18653/v1/2025.naacl-long.227) |  | 0 |  | Xinyu Lu, Xueru Wen, Yaojie Lu, Bowen Yu, Hongyu Lin, Haiyang Yu, Le Sun, Xianpei Han, Yongbin Li |  |
| 309 |  |  [FLEX: Expert-level False-Less EXecution Metric for Text-to-SQL Benchmark](https://doi.org/10.18653/v1/2025.naacl-long.228) |  | 0 |  | Heegyu Kim, Taeyang Jeon, Seunghwan Choi, Seungtaek Choi, Hyunsouk Cho |  |
| 310 |  |  [AID: Adaptive Integration of Detectors for Safe AI with Language Models](https://doi.org/10.18653/v1/2025.naacl-long.229) |  | 0 |  | Xinran Wang, Enmao Diao, Qi Le, Jie Ding, Ali Anwar |  |
| 311 |  |  [SSMLoRA: Enhancing Low-Rank Adaptation with State Space Model](https://doi.org/10.18653/v1/2025.naacl-long.230) |  | 0 |  | Jiayang Yu, Yihang Zhang, Bin Wang, Peiqin Lin, Yongkang Liu, Shi Feng |  |
| 312 |  |  [Sharpness-Aware Minimization for Topic Models with High-Quality Document Representations](https://doi.org/10.18653/v1/2025.naacl-long.231) |  | 0 |  | Tung Nguyen, Tue Le, Hoang Tran Vuong, Quang Duc Nguyen, Duc Anh Nguyen, Linh Ngo Van, Sang Dinh, Thien Huu Nguyen |  |
| 313 |  |  [C²: Scalable Auto-Feedback for LLM-based Chart Generation](https://doi.org/10.18653/v1/2025.naacl-long.232) |  | 0 |  | Woosung Koh, Jang Han Yoon, Minhyung Lee, Youngjin Song, Jaegwan Cho, Jaehyun Kang, Taehyeon Kim, SeYoung Yun, Youngjae Yu, Bongshin Lee |  |
| 314 |  |  [A Top-down Graph-based Tool for Modeling Classical Semantic Maps: A Case Study of Supplementary Adverbs](https://doi.org/10.18653/v1/2025.naacl-long.233) |  | 0 |  | Zhu Liu, Cunliang Kong, Ying Liu, Maosong Sun |  |
| 315 |  |  [UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers](https://doi.org/10.18653/v1/2025.naacl-long.234) |  | 0 |  | Dehai Min, Zhiyang Xu, Guilin Qi, Lifu Huang, Chenyu You |  |
| 316 |  |  [Improving Model Evaluation using SMART Filtering of Benchmark Datasets](https://doi.org/10.18653/v1/2025.naacl-long.235) |  | 0 |  | Vipul Gupta, Candace Ross, David Pantoja, Rebecca J. Passonneau, Megan Ung, Adina Williams |  |
| 317 |  |  [Entropy-Based Decoding for Retrieval-Augmented Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.236) |  | 0 |  | Zexuan Qiu, Zijing Ou, Bin Wu, Jingjing Li, Aiwei Liu, Irwin King |  |
| 318 |  |  [What We Talk About When We Talk About LMs: Implicit Paradigm Shifts and the Ship of Language Models](https://doi.org/10.18653/v1/2025.naacl-long.237) |  | 0 |  | Shengqi Zhu, Jeffrey M. Rzeszotarski |  |
| 319 |  |  [Diversity Helps Jailbreak Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.238) |  | 0 |  | Weiliang Zhao, Daniel BenLevi, Wei Hao, Junfeng Yang, Chengzhi Mao |  |
| 320 |  |  [Constrained Decoding with Speculative Lookaheads](https://doi.org/10.18653/v1/2025.naacl-long.239) |  | 0 |  | Nishanth Sridhar Nakshatri, Shamik Roy, Rajarshi Das, Suthee Chaidaroon, Leonid Boytsov, Rashmi Gangadharaiah |  |
| 321 |  |  [DyPCL: Dynamic Phoneme-level Contrastive Learning for Dysarthric Speech Recognition](https://doi.org/10.18653/v1/2025.naacl-long.240) |  | 0 |  | Wonjun Lee, Solee Im, Heejin Do, Yunsu Kim, Jungseul Ok, Gary Lee |  |
| 322 |  |  [Revisiting Early Detection of Sexual Predators via Turn-level Optimization](https://doi.org/10.18653/v1/2025.naacl-long.241) |  | 0 |  | Jinmyeong An, Sangwon Ryu, Heejin Do, Yunsu Kim, Jungseul Ok, Gary Lee |  |
| 323 |  |  [StyleTTS-ZS: Efficient High-Quality Zero-Shot Text-to-Speech Synthesis with Distilled Time-Varying Style Diffusion](https://doi.org/10.18653/v1/2025.naacl-long.242) |  | 0 |  | Yinghao Aaron Li, Xilin Jiang, Cong Han, Nima Mesgarani |  |
| 324 |  |  [Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-long.243) |  | 0 |  | Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, Manaal Faruqui |  |
| 325 |  |  [ReachAgent: Enhancing Mobile Agent via Page Reaching and Operation](https://doi.org/10.18653/v1/2025.naacl-long.244) |  | 0 |  | Qinzhuo Wu, Wei Liu, Jian Luan, Bin Wang |  |
| 326 |  |  [Learning to Solve Domain-Specific Calculation Problems with Knowledge-Intensive Programs Generator](https://doi.org/10.18653/v1/2025.naacl-long.245) |  | 0 |  | Chengyuan Liu, Shihang Wang, Lizhi Qing, Jun Lin, Ji Zhang, Fei Wu, Kun Kuang |  |
| 327 |  |  [SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity Mixture](https://doi.org/10.18653/v1/2025.naacl-long.246) |  | 0 |  | Jiayi Han, Liang Du, Hongwei Du, Xiangguo Zhou, Yiwen Wu, Yuanfang Zhang, Weibo Zheng, Donghong Han |  |
| 328 |  |  [MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.247) |  | 0 |  | Jinsheng Huang, Liang Chen, Taian Guo, Fu Zeng, Yusheng Zhao, Bohan Wu, Ye Yuan, Haozhe Zhao, Zhihui Guo, Yichi Zhang, Jingyang Yuan, Wei Ju, Luchen Liu, Tianyu Liu, Baobao Chang, Ming Zhang |  |
| 329 |  |  [MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning](https://doi.org/10.18653/v1/2025.naacl-long.248) |  | 0 |  | Hanqing Wang, Yixia Li, Shuo Wang, Guanhua Chen, Yun Chen |  |
| 330 |  |  [Analyzing (In)Abilities of SAEs via Formal Languages](https://doi.org/10.18653/v1/2025.naacl-long.249) |  | 0 |  | Abhinav Menon, Manish Shrivastava, David Krueger, Ekdeep Singh Lubana |  |
| 331 |  |  [Multimodal Cognitive Reframing Therapy via Multi-hop Psychotherapeutic Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.250) |  | 0 |  | Subin Kim, Hoonrae Kim, Heejin Do, Gary Lee |  |
| 332 |  |  [Explanation based In-Context Demonstrations Retrieval for Multilingual Grammatical Error Correction](https://doi.org/10.18653/v1/2025.naacl-long.251) |  | 0 |  | Wei Li, Wen Luo, Guangyue Peng, Houfeng Wang |  |
| 333 |  |  [A Unified Supervised and Unsupervised Dialogue Topic Segmentation Framework Based on Utterance Pair Modeling](https://doi.org/10.18653/v1/2025.naacl-long.252) |  | 0 |  | Shihao Yang, Ziyi Zhang, Yue Jiang, Chunsheng Qin, Shuhua Liu |  |
| 334 |  |  [Evaluating Small Language Models for News Summarization: Implications and Factors Influencing Performance](https://doi.org/10.18653/v1/2025.naacl-long.253) |  | 0 |  | Borui Xu, Yao Chen, Zeyi Wen, Weiguo Liu, Bingsheng He |  |
| 335 |  |  [Dynamic Fisher-weighted Model Merging via Bayesian Optimization](https://doi.org/10.18653/v1/2025.naacl-long.254) |  | 0 |  | Sanwoo Lee, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Yunfang Wu |  |
| 336 |  |  [AI-Assisted Human Evaluation of Machine Translation](https://doi.org/10.18653/v1/2025.naacl-long.255) |  | 0 |  | Vilém Zouhar, Tom Kocmi, Mrinmaya Sachan |  |
| 337 |  |  [MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria](https://doi.org/10.18653/v1/2025.naacl-long.256) |  | 0 |  | Wentao Ge, Shunian Chen, Hardy Chen, Nuo Chen, Junying Chen, Zhihong Chen, Wenya Xie, Shuo Yan, ChenghaoZhu ChenghaoZhu, Ziyue Lin, Dingjie Song, Xidong Wang, Anningzhe Gao, Zhiyi Zhang, Jianquan Li, Xiang Wan, Benyou Wang |  |
| 338 |  |  [AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios](https://doi.org/10.18653/v1/2025.naacl-long.257) |  | 0 |  | Xinyi Mou, Jingcong Liang, Jiayu Lin, Xinnong Zhang, Xiawei Liu, Shiyue Yang, Rong Ye, Lei Chen, Haoyu Kuang, Xuanjing Huang, Zhongyu Wei |  |
| 339 |  |  [FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data](https://doi.org/10.18653/v1/2025.naacl-long.258) |  | 0 |  | Deren Lei, Yaxi Li, Siyao Li, Mengya Hu, Rui Xu, Ken Archer, Mingyu Wang, Emily Ching, Alex Deng |  |
| 340 |  |  [Label Drop for Multi-Aspect Relation Modeling in Universal Information Extraction](https://doi.org/10.18653/v1/2025.naacl-long.259) |  | 0 |  | Lu Yang, Jiajia Li, En Ci, Lefei Zhang, Zuchao Li, Ping Wang |  |
| 341 |  |  [Test-Time Code-Switching for Cross-lingual Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2025.naacl-long.260) |  | 0 |  | Dongming Sheng, Kexin Han, Hao Li, Yan Zhang, Yucheng Huang, Jun Lang, Wenqiang Liu |  |
| 342 |  |  [VisCGEC: Benchmarking the Visual Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2025.naacl-long.261) |  | 0 |  | Xiaoman Wang, Dan Yuan, Xin Liu, Yike Zhao, Xiaoxiao Zhang, Xizhi Chen, Yunshi Lan |  |
| 343 |  |  [Are We Done with MMLU?](https://doi.org/10.18653/v1/2025.naacl-long.262) |  | 0 |  | Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, Pasquale Minervini |  |
| 344 |  |  [MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling](https://doi.org/10.18653/v1/2025.naacl-long.263) |  | 0 |  | Yakun Zhu, Shaohang Wei, Xu Wang, Kui Xue, Shaoting Zhang, Xiaofan Zhang |  |
| 345 |  |  [Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering](https://doi.org/10.18653/v1/2025.naacl-long.264) |  | 0 |  | Yu Zhao, Alessio Devoto, Giwon Hong, Xiaotang Du, Aryo Pradipta Gema, Hongru Wang, Xuanli He, KamFai Wong, Pasquale Minervini |  |
| 346 |  |  [MoDification: Mixture of Depths Made Easy](https://doi.org/10.18653/v1/2025.naacl-long.265) |  | 0 |  | Chen Zhang, Meizhi Zhong, Qimeng Wang, Xuantao Lu, Zheyu Ye, Chengqiang Lu, Yan Gao, Yao Hu, Kehai Chen, Min Zhang, Dawei Song |  |
| 347 |  |  [On the Vulnerability of Text Sanitization](https://doi.org/10.18653/v1/2025.naacl-long.266) |  | 0 |  | Meng Tong, Kejiang Chen, Xiaojian Yuan, Jiayang Liu, Weiming Zhang, Nenghai Yu, Jie Zhang |  |
| 348 |  |  [Multilingual Needle in a Haystack: Investigating Long-Context Behavior of Multilingual Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.267) |  | 0 |  | Amey Hengle, Prasoon Bajpai, Soham Dan, Tanmoy Chakraborty |  |
| 349 |  |  [Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation](https://doi.org/10.18653/v1/2025.naacl-long.268) |  | 0 |  | Hoang Pham, ThanhDo Nguyen, KhacHoai Nam Bui |  |
| 350 |  |  [Exploring the Potential of Large Language Models for Heterophilic Graphs](https://doi.org/10.18653/v1/2025.naacl-long.269) |  | 0 |  | Yuxia Wu, Shujie Li, Yuan Fang, Chuan Shi |  |
| 351 |  |  [Exploiting Edited Large Language Models as General Scientific Optimizers](https://doi.org/10.18653/v1/2025.naacl-long.270) |  | 0 |  | Qitan Lv, Tianyu Liu, Hong Wang |  |
| 352 |  |  [DIRAS: Efficient LLM Annotation of Document Relevance for Retrieval Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-long.271) |  | 0 |  | Jingwei Ni, Tobias Schimanski, Meihong Lin, Mrinmaya Sachan, Elliott Ash, Markus Leippold |  |
| 353 |  |  [Hello Again! LLM-powered Personalized Agent for Long-term Dialogue](https://doi.org/10.18653/v1/2025.naacl-long.272) |  | 0 |  | Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, TatSeng Chua |  |
| 354 |  |  [My LLM might Mimic AAE - But When Should It?](https://doi.org/10.18653/v1/2025.naacl-long.273) |  | 0 |  | Sandra Sandoval, Christabel Acquaye, Kwesi A. Cobbina, Mohammad Nayeem Teli, Hal Daumé III |  |
| 355 |  |  [High-Dimension Human Value Representation in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.274) |  | 0 |  | Samuel Cahyawijaya, Delong Chen, Yejin Bang, Leila Khalatbari, Bryan Wilie, Ziwei Ji, Etsuko Ishii, Pascale Fung |  |
| 356 |  |  [Not all Hallucinations are Good to Throw Away When it Comes to Legal Abstractive Summarization](https://doi.org/10.18653/v1/2025.naacl-long.275) |  | 0 |  | Nihed Bendahman, Karen PinelSauvagnat, Gilles Hubert, Mokhtar Boumedyen Billami |  |
| 357 |  |  [Query-focused Referentiability Learning for Zero-shot Retrieval](https://doi.org/10.18653/v1/2025.naacl-long.276) |  | 0 |  | Jaeyoung Kim, Dohyeon Lee, Seungwon Hwang |  |
| 358 |  |  [A Novel Computational Modeling Foundation for Automatic Coherence Assessment](https://doi.org/10.18653/v1/2025.naacl-long.277) |  | 0 |  | Aviya Maimon |  |
| 359 |  |  [Token-based Decision Criteria Are Suboptimal in In-context Learning](https://doi.org/10.18653/v1/2025.naacl-long.278) |  | 0 |  | Hakaze Cho, Yoshihiro Sakai, Mariko Kato, Kenshiro Tanaka, Akira Ishii, Naoya Inoue |  |
| 360 |  |  [CSEval: Towards Automated, Multi-Dimensional, and Reference-Free Counterspeech Evaluation using Auto-Calibrated LLMs](https://doi.org/10.18653/v1/2025.naacl-long.279) |  | 0 |  | Amey Hengle, Aswini Kumar Padhi, Anil Bandhakavi, Tanmoy Chakraborty |  |
| 361 |  |  [Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study](https://doi.org/10.18653/v1/2025.naacl-long.280) |  | 0 |  | Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, Bin Wang |  |
| 362 |  |  [RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.281) |  | 0 |  | Bang An, Shiyue Zhang, Mark Dredze |  |
| 363 |  |  [Evaluating Evidence Attribution in Generated Fact Checking Explanations](https://doi.org/10.18653/v1/2025.naacl-long.282) |  | 0 |  | Rui Xing, Timothy Baldwin, Jey Han Lau |  |
| 364 |  |  [ETHIC: Evaluating Large Language Models on Long-Context Tasks with High Information Coverage](https://doi.org/10.18653/v1/2025.naacl-long.283) |  | 0 |  | Taewhoo Lee, Chanwoong Yoon, Kyochul Jang, Donghyeon Lee, Minju Song, Hyunjae Kim, Jaewoo Kang |  |
| 365 |  |  [Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors](https://doi.org/10.18653/v1/2025.naacl-long.284) |  | 0 |  | Georgios Chochlakis, Alexandros Potamianos, Kristina Lerman, Shrikanth Narayanan |  |
| 366 |  |  [Arabic Dataset for LLM Safeguard Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.285) |  | 0 |  | Yasser Ashraf, Yuxia Wang, Bin Gu, Preslav Nakov, Timothy Baldwin |  |
| 367 |  |  [Anticipating Future with Large Language Model for Simultaneous Machine Translation](https://doi.org/10.18653/v1/2025.naacl-long.286) |  | 0 |  | Siqi Ouyang, Oleksii Hrinchuk, Zhehuai Chen, Vitaly Lavrukhin, Jagadeesh Balam, Lei Li, Boris Ginsburg |  |
| 368 |  |  [GuideLLM: Exploring LLM-Guided Conversation with Applications in Autobiography Interviewing](https://doi.org/10.18653/v1/2025.naacl-long.287) |  | 0 |  | Jinhao Duan, Xinyu Zhao, Zhuoxuan Zhang, Eunhye Grace Ko, Lily Boddy, Chenan Wang, Tianhao Li, Alexander Rasgon, Junyuan Hong, Min Kyung Lee, Chenxi Yuan, Qi Long, Ying Ding, Tianlong Chen, Kaidi Xu |  |
| 369 |  |  [Fine-Tuning Large Language Models with Sequential Instructions](https://doi.org/10.18653/v1/2025.naacl-long.288) |  | 0 |  | Hanxu Hu, Simon Yu, Pinzhen Chen, Edoardo M. Ponti |  |
| 370 |  |  [Diverse In-Context Example Selection After Decomposing Programs and Aligned Utterances Improves Semantic Parsing](https://doi.org/10.18653/v1/2025.naacl-long.289) |  | 0 |  | Mayank Kothyari, Sunita Sarawagi, Soumen Chakrabarti, Gaurav Arora, Srujana Merugu |  |
| 371 |  |  [Elevating Legal LLM Responses: Harnessing Trainable Logical Structures and Semantic Knowledge with Legal Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.290) |  | 0 |  | Rujing Yao, Yang Wu, Chenghao Wang, Jingwei Xiong, Fang Wang, Xiaozhong Liu |  |
| 372 |  |  [Efficient One-shot Compression via Low-Rank Local Feature Distillation](https://doi.org/10.18653/v1/2025.naacl-long.291) |  | 0 |  | Yaya Sy, Christophe Cerisara, Irina Illina |  |
| 373 |  |  [Waste Not, Want Not; Recycled Gumbel Noise Improves Consistency in Natural Language Generation](https://doi.org/10.18653/v1/2025.naacl-long.292) |  | 0 |  | Damien de Mijolla, Hannan Saddiq, Kim Moore |  |
| 374 |  |  [ConQRet: A New Benchmark for Fine-Grained Automatic Evaluation of Retrieval Augmented Computational Argumentation](https://doi.org/10.18653/v1/2025.naacl-long.293) |  | 0 |  | Kaustubh D. Dhole, Kai Shu, Eugene Agichtein |  |
| 375 |  |  [SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators](https://doi.org/10.18653/v1/2025.naacl-long.294) |  | 0 |  | Daniil Moskovskiy, Nikita Sushko, Sergey Pletenev, Elena Tutubalina, Alexander Panchenko |  |
| 376 |  |  [BEMEAE: Moving Beyond Exact Span Match for Event Argument Extraction](https://doi.org/10.18653/v1/2025.naacl-long.295) |  | 0 |  | Enfa Fane, Md Nayem Uddin, Oghenevovwe Ikumariegbe, Daniyal Kashif, Eduardo Blanco, Steven R. Corman |  |
| 377 |  |  [uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in Low-Data Regimes](https://doi.org/10.18653/v1/2025.naacl-long.296) |  | 0 |  | Abdul Waheed, Karima Kadaoui, Bhiksha Raj, Muhammad AbdulMageed |  |
| 378 |  |  [Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities](https://doi.org/10.18653/v1/2025.naacl-long.297) |  | 0 |  | ChungEn Sun, Xiaodong Liu, Weiwei Yang, TsuiWei Weng, Hao Cheng, Aidan San, Michel Galley, Jianfeng Gao |  |
| 379 |  |  [VoiceTextBlender: Augmenting Large Language Models with Speech Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning](https://doi.org/10.18653/v1/2025.naacl-long.298) |  | 0 |  | Yifan Peng, Krishna C. Puvvada, Zhehuai Chen, Piotr Zelasko, He Huang, Kunal Dhawan, Ke Hu, Shinji Watanabe, Jagadeesh Balam, Boris Ginsburg |  |
| 380 |  |  [Rethinking Word Similarity: Semantic Similarity through Classification Confusion](https://doi.org/10.18653/v1/2025.naacl-long.299) |  | 0 |  | Kaitlyn Zhou, Haishan Gao, Sarah Li Chen, Dan Edelstein, Dan Jurafsky, Chen Shani |  |
| 381 |  |  [SUNAR: Semantic Uncertainty based Neighborhood Aware Retrieval for Complex QA](https://doi.org/10.18653/v1/2025.naacl-long.300) |  | 0 |  | Venktesh V, Mandeep Rathee, Avishek Anand |  |
| 382 |  |  [Do RAG Systems Cover What Matters? Evaluating and Optimizing Responses with Sub-Question Coverage](https://doi.org/10.18653/v1/2025.naacl-long.301) |  | 0 |  | Kaige Xie, Philippe Laban, Prafulla Kumar Choubey, Caiming Xiong, ChienSheng Wu |  |
| 383 |  |  [Stronger Universal and Transferable Attacks by Suppressing Refusals](https://doi.org/10.18653/v1/2025.naacl-long.302) |  | 0 |  | David Huang, Avidan Shah, Alexandre Araujo, David A. Wagner, Chawin Sitawarin |  |
| 384 |  |  [The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models](https://doi.org/10.18653/v1/2025.naacl-long.303) |  | 0 |  | Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Choi, Sheikh Shafayat, Jinheon Baek, Sue Hyun Park, Hyeonbin Hwang, Jinkyung Jo, Hyowon Cho, Haebin Shin, Seongyun Lee, Hanseok Oh, Noah Lee, Namgyu Ho, Se June Joo, Miyoung Ko, Yoonjoo Lee, Hyungjoo Chae, Jamin Shin, Joel Jang, Seonghyeon Ye, Bill Yuchen Lin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo |  |
| 385 |  |  [DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback](https://doi.org/10.18653/v1/2025.naacl-long.304) |  | 0 |  | Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin, DaCheng Juan, Dana Alon, Charles Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, Cyrus Rashtchian |  |
| 386 |  |  [Uncovering Bias in Large Vision-Language Models at Scale with Counterfactuals](https://doi.org/10.18653/v1/2025.naacl-long.305) |  | 0 |  | Phillip Howard, Kathleen C. Fraser, Anahita Bhiwandiwalla, Svetlana Kiritchenko |  |
| 387 |  |  [AEGIS2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails](https://doi.org/10.18653/v1/2025.naacl-long.306) |  | 0 |  | Shaona Ghosh, Prasoon Varshney, Makesh Narsimhan Sreedhar, Aishwarya Padmakumar, Traian Rebedea, Jibin Rajan Varghese, Christopher Parisien |  |
| 388 |  |  [UOREX: Towards Uncertainty-Aware Open Relation Extraction](https://doi.org/10.18653/v1/2025.naacl-long.307) |  | 0 |  | Rebii Jamal, Mounir Ourekouch, Mohammed Erradi |  |
| 389 |  |  [Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training](https://doi.org/10.18653/v1/2025.naacl-long.308) |  | 0 |  | Yuchen Zhuang, Jingfeng Yang, Haoming Jiang, Xin Liu, Kewei Cheng, Sanket Lokegaonkar, Yifan Gao, Qing Ping, Tianyi Liu, Binxuan Huang, Zheng Li, Zhengyang Wang, Pei Chen, Ruijie Wang, Rongzhi Zhang, Nasser Zalmout, Priyanka Nigam, Bing Yin, Chao Zhang |  |
| 390 |  |  [TinyThinker: Distilling Reasoning through Coarse-to-Fine Knowledge Internalization with Self-Reflection](https://doi.org/10.18653/v1/2025.naacl-long.309) |  | 0 |  | Shengmin Piao, Sanghyun Park |  |
| 391 |  |  [VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-long.310) |  | 0 |  | Manan Suri, Puneet Mathur, Franck Dernoncourt, Kanika Goswami, Ryan A. Rossi, Dinesh Manocha |  |
| 392 |  |  [VTechAGP: An Academic-to-General-Audience Text Paraphrase Dataset and Benchmark Models](https://doi.org/10.18653/v1/2025.naacl-long.311) |  | 0 |  | Ming Cheng, Jiaying Gong, Chenhan Yuan, William A. Ingram, Edward A. Fox, Hoda Eldardiry |  |
| 393 |  |  [Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages](https://doi.org/10.18653/v1/2025.naacl-long.312) |  | 0 |  | Jannik Brinkmann, Chris Wendler, Christian Bartelt, Aaron Mueller |  |
| 394 |  |  [Examining and Adapting Time for Multilingual Classification via Mixture of Temporal Experts](https://doi.org/10.18653/v1/2025.naacl-long.313) |  | 0 |  | Weisi Liu, Guangzeng Han, Xiaolei Huang |  |
| 395 |  |  [FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.314) |  | 0 |  | Garrett Tanzer |  |
| 396 |  |  [EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms](https://doi.org/10.18653/v1/2025.naacl-long.315) |  | 0 |  | Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang |  |
| 397 |  |  [EmoCharacter: Evaluating the Emotional Fidelity of Role-Playing Agents in Dialogues](https://doi.org/10.18653/v1/2025.naacl-long.316) |  | 0 |  | Qiming Feng, Qiujie Xie, Xiaolong Wang, Qingqiu Li, Yuejie Zhang, Rui Feng, Tao Zhang, Shang Gao |  |
| 398 |  |  [Language Models can Categorize System Inputs for Performance Analysis](https://doi.org/10.18653/v1/2025.naacl-long.317) |  | 0 |  | Dominic Sobhani, Ruiqi Zhong, Edison MarreseTaylor, Keisuke Sakaguchi, Yutaka Matsuo |  |
| 399 |  |  [FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.318) |  | 0 |  | Xin Guo, Haotian Xia, Zhaowei Liu, Hanyang Cao, Zhi Yang, Zhiqiang Liu, Sizhe Wang, Jinyi Niu, Chuqi Wang, Yanhui Wang, Xiaolong Liang, Xiaoming Huang, Bing Zhu, Zhongyu Wei, Yun Chen, Weining Shen, Liwen Zhang |  |
| 400 |  |  [Rethinking the Role of LLMs for Document-level Relation Extraction: a Refiner with Task Distribution and Probability Fusion](https://doi.org/10.18653/v1/2025.naacl-long.319) |  | 0 |  | Fu Zhang, Xinlong Jin, Jingwei Cheng, Hongsen Yu, Huangming Xu |  |
| 401 |  |  [Decomposition Dilemmas: Does Claim Decomposition Boost or Burden Fact-Checking Performance?](https://doi.org/10.18653/v1/2025.naacl-long.320) |  | 0 |  | Qisheng Hu, Quanyu Long, Wenya Wang |  |
| 402 |  |  [Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing](https://doi.org/10.18653/v1/2025.naacl-long.321) |  | 0 |  | Huanqian Wang, Yang Yue, Rui Lu, Jingxin Shi, Andrew Zhao, Shenzhi Wang, Shiji Song, Gao Huang |  |
| 403 |  |  [Effective Skill Unlearning through Intervention and Abstention](https://doi.org/10.18653/v1/2025.naacl-long.322) |  | 0 |  | Yongce Li, ChungEn Sun, TsuiWei Weng |  |
| 404 |  |  [CharacterBox: Evaluating the Role-Playing Capabilities of LLMs in Text-Based Virtual Worlds](https://doi.org/10.18653/v1/2025.naacl-long.323) |  | 0 |  | Lei Wang, Jianxun Lian, Yi Huang, Yanqi Dai, Haoxuan Li, Xu Chen, Xing Xie, JiRong Wen |  |
| 405 |  |  [A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision-Language Models](https://doi.org/10.18653/v1/2025.naacl-long.324) |  | 0 |  | Xiujie Song, Mengyue Wu, Kenny Q. Zhu, Chunhao Zhang, Yanyi Chen |  |
| 406 |  |  [CoME: An Unlearning-based Approach to Conflict-free Model Editing](https://doi.org/10.18653/v1/2025.naacl-long.325) |  | 0 |  | Dahyun Jung, Jaehyung Seo, Jaewook Lee, Chanjun Park, Heuiseok Lim |  |
| 407 |  |  [On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena](https://doi.org/10.18653/v1/2025.naacl-long.326) |  | 0 |  | Tarek Naous, Wei Xu |  |
| 408 |  |  [Adapting Sentence-level Automatic Metrics for Document-level Simplification Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.327) |  | 0 |  | Mounica Maddela, Fernando AlvaManchego |  |
| 409 |  |  [Decoding Speculative Decoding](https://doi.org/10.18653/v1/2025.naacl-long.328) |  | 0 |  | Minghao Yan, Saurabh Agarwal, Shivaram Venkataraman |  |
| 410 |  |  [Leveraging LLM For Synchronizing Information Across Multilingual Tables](https://doi.org/10.18653/v1/2025.naacl-long.329) |  | 0 |  | Siddharth Khincha, Tushar Kataria, Ankita Anand, Dan Roth, Vivek Gupta |  |
| 411 |  |  [ConMeC: A Dataset for Metonymy Resolution with Common Nouns](https://doi.org/10.18653/v1/2025.naacl-long.330) |  | 0 |  | Saptarshi Ghosh, Tianyu Jiang |  |
| 412 |  |  [Self-DC: When to Reason and When to Act? Self Divide-and-Conquer for Compositional Unknown Questions](https://doi.org/10.18653/v1/2025.naacl-long.331) |  | 0 |  | Hongru Wang, Boyang Xue, Baohang Zhou, Tianhua Zhang, Cunxiang Wang, Huimin Wang, Guanhua Chen, KamFai Wong |  |
| 413 |  |  [TRANSIENTTABLES: Evaluating LLMs' Reasoning on Temporally Evolving Semi-structured Tables](https://doi.org/10.18653/v1/2025.naacl-long.332) |  | 0 |  | Abhilash Reddy Shankarampeta, Harsh Mahajan, Tushar Kataria, Dan Roth, Vivek Gupta |  |
| 414 |  |  [AdvisorQA: Towards Helpful and Harmless Advice-seeking Question Answering with Collective Intelligence](https://doi.org/10.18653/v1/2025.naacl-long.333) |  | 0 |  | Minbeom Kim, Hwanhee Lee, Joonsuk Park, Hwaran Lee, Kyomin Jung |  |
| 415 |  |  [tRAG: Term-level Retrieval-Augmented Generation for Domain-Adaptive Retrieval](https://doi.org/10.18653/v1/2025.naacl-long.334) |  | 0 |  | Dohyeon Lee, Jongyoon Kim, Jihyuk Kim, Seungwon Hwang, Joonsuk Park |  |
| 416 |  |  [JRE-L: Journalist, Reader, and Editor LLMs in the Loop for Science Journalism for the General Audience](https://doi.org/10.18653/v1/2025.naacl-long.335) |  | 0 |  | Gongyao Jiang, Xinran Shi, Qiong Luo |  |
| 417 |  |  [Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.336) |  | 0 |  | Ziche Liu, Rui Ke, Yajiao Liu, Feng Jiang, Haizhou Li |  |
| 418 |  |  [Graph Neural Network Enhanced Retrieval for Question Answering of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.337) |  | 0 |  | Zijian Li, Qingyan Guo, Jiawei Shao, Lei Song, Jiang Bian, Jun Zhang, Rui Wang |  |
| 419 |  |  [Pula: Training Large Language Models for Setswana](https://doi.org/10.18653/v1/2025.naacl-long.338) |  | 0 |  | Nathan Brown, Vukosi Marivate |  |
| 420 |  |  [LegalViz: Legal Text Visualization by Text To Diagram Generation](https://doi.org/10.18653/v1/2025.naacl-long.339) |  | 0 |  | Eri Onami, Taiki Miyanishi, Koki Maeda, Shuhei Kurita |  |
| 421 |  |  [Active Few-Shot Learning for Text Classification](https://doi.org/10.18653/v1/2025.naacl-long.340) |  | 0 |  | Saeed Ahmadnia, Arash Yousefi Jordehi, Mahsa Hosseini Khasheh Heyran, Seyed Abolghasem Mirroshandel, Owen Rambow, Cornelia Caragea |  |
| 422 |  |  [Enhancing Multimodal Entity Linking with Jaccard Distance-based Conditional Contrastive Learning and Contextual Visual Augmentation](https://doi.org/10.18653/v1/2025.naacl-long.341) |  | 0 |  | CongDuy T. Nguyen, Xiaobao Wu, Thong Thanh Nguyen, Shuai Zhao, Khoi M. Le, VietAnh Nguyen, Yichao Feng, Anh Tuan Luu |  |
| 423 |  |  [ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.342) |  | 0 |  | Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang |  |
| 424 |  |  [Logit Separability-Driven Samples and Multiple Class-Related Words Selection for Advancing In-Context Learning](https://doi.org/10.18653/v1/2025.naacl-long.343) |  | 0 |  | Zixiao Zhu, Zijian Feng, Hanzhang Zhou, Junlang Qian, Kezhi Mao |  |
| 425 |  |  [Identifying Emerging Concepts in Large Corpora](https://doi.org/10.18653/v1/2025.naacl-long.344) |  | 0 |  | Sibo Ma, Julian Nyarko |  |
| 426 |  |  [CodeSCM: Causal Analysis for Multi-Modal Code Generation](https://doi.org/10.18653/v1/2025.naacl-long.345) |  | 0 |  | Mukur Gupta, Noopur Bhatt, Suman Jana |  |
| 427 |  |  [From Distributional to Overton Pluralism: Investigating Large Language Model Alignment](https://doi.org/10.18653/v1/2025.naacl-long.346) |  | 0 |  | Thom Lake, Eunsol Choi, Greg Durrett |  |
| 428 |  |  [Advancing MoE Efficiency: A Collaboration-Constrained Routing (C2R) Strategy for Better Expert Parallelism Design](https://doi.org/10.18653/v1/2025.naacl-long.347) |  | 0 |  | Mohan Zhang, Pingzhi Li, Jie Peng, Mufan Qiu, Tianlong Chen |  |
| 429 |  |  [LibEvolutionEval: A Benchmark and Study for Version-Specific Code Generation](https://doi.org/10.18653/v1/2025.naacl-long.348) |  | 0 |  | Sachit Kuhar, Wasi Uddin Ahmad, Zijian Wang, Nihal Jain, Haifeng Qian, Baishakhi Ray, Murali Krishna Ramanathan, Xiaofei Ma, Anoop Deoras |  |
| 430 |  |  [Evaluating and Mitigating Object Hallucination in Large Vision-Language Models: Can They Still See Removed Objects?](https://doi.org/10.18653/v1/2025.naacl-long.349) |  | 0 |  | Yixiao He, Haifeng Sun, Pengfei Ren, Jingyu Wang, Huazheng Wang, Qi Qi, Zirui Zhuang, Jing Wang |  |
| 431 |  |  [Self-Pluralising Culture Alignment for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.350) |  | 0 |  | Shaoyang Xu, Yongqi Leng, Linhao Yu, Deyi Xiong |  |
| 432 |  |  [K-COMP: Retrieval-Augmented Medical Domain Question Answering With Knowledge-Injected Compressor](https://doi.org/10.18653/v1/2025.naacl-long.351) |  | 0 |  | Jeonghun Cho, Gary Lee |  |
| 433 |  |  [DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students' Hand-Drawn Math Images](https://doi.org/10.18653/v1/2025.naacl-long.352) |  | 0 |  | Sami Baral, Li Lucy, Ryan Knight, Alice Ng, Luca Soldaini, Neil T. Heffernan, Kyle Lo |  |
| 434 |  |  [Knowledge Graph Guided Evaluation of Abstention Techniques](https://doi.org/10.18653/v1/2025.naacl-long.353) |  | 0 |  | Kinshuk Vasisht, Navreet Kaur, Danish Pruthi |  |
| 435 |  |  [Wav2Prompt: End-to-End Speech Prompt Learning and Task-based Fine-tuning for Text-based LLMs](https://doi.org/10.18653/v1/2025.naacl-long.354) |  | 0 |  | Keqi Deng, Guangzhi Sun, Philip C. Woodland |  |
| 436 |  |  [Legal Judgment Prediction based on Knowledge-enhanced Multi-Task and Multi-Label Text Classification](https://doi.org/10.18653/v1/2025.naacl-long.355) |  | 0 |  | Ang Li, Yiquan Wu, Ming Cai, Adam Jatowt, Xiang Zhou, Weiming Lu, Changlong Sun, Fei Wu, Kun Kuang |  |
| 437 |  |  [SPeCtrum: A Grounded Framework for Multidimensional Identity Representation in LLM-Based Agent](https://doi.org/10.18653/v1/2025.naacl-long.356) |  | 0 |  | Keyeun Lee, SeoHyeong Kim, Seolhee Lee, Jinsu Eun, Yena Ko, Hayeon Jeon, Esther Hehsun Kim, Seonghye Cho, Soeun Yang, Eunmee Kim, Hajin Lim |  |
| 438 |  |  [Beemo: Benchmark of Expert-edited Machine-generated Outputs](https://doi.org/10.18653/v1/2025.naacl-long.357) |  | 0 |  | Ekaterina Artemova, Jason Samuel Lucas, Saranya Venkatraman, Jooyoung Lee, Sergei Tilga, Adaku Uchendu, Vladislav Mikhailov |  |
| 439 |  |  [SANDWiCH: Semantical Analysis of Neighbours for Disambiguating Words in Context ad Hoc](https://doi.org/10.18653/v1/2025.naacl-long.358) |  | 0 |  | Daniel GuzmanOlivares, Lara Quijano Sánchez, Federico Liberatore |  |
| 440 |  |  [Towards Automatic Evaluation for Image Transcreation](https://doi.org/10.18653/v1/2025.naacl-long.359) |  | 0 |  | Simran Khanuja, Vivek Iyer, Xiaoyu He, Graham Neubig |  |
| 441 |  |  [ImgTrojan: Jailbreaking Vision-Language Models with ONE Image](https://doi.org/10.18653/v1/2025.naacl-long.360) |  | 0 |  | Xijia Tao, Shuai Zhong, Lei Li, Qi Liu, Lingpeng Kong |  |
| 442 |  |  [RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement](https://doi.org/10.18653/v1/2025.naacl-long.361) |  | 0 |  | Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Xin Zhao, Yang Song, Tao Zhang |  |
| 443 |  |  [Mitigating Biases of Large Language Models in Stance Detection with Counterfactual Augmented Calibration](https://doi.org/10.18653/v1/2025.naacl-long.362) |  | 0 |  | Ang Li, Jingqian Zhao, Bin Liang, Lin Gui, Hui Wang, Xi Zeng, Xingwei Liang, KamFai Wong, Ruifeng Xu |  |
| 444 |  |  [Beyond the Next Token: Towards Prompt-Robust Zero-Shot Classification via Efficient Multi-Token Prediction](https://doi.org/10.18653/v1/2025.naacl-long.363) |  | 0 |  | Junlang Qian, Zixiao Zhu, Hanzhang Zhou, Zijian Feng, Zepeng Zhai, Kezhi Mao |  |
| 445 |  |  [Investigating Hallucinations in Simultaneous Machine Translation: Knowledge Distillation Solution and Components Analysis](https://doi.org/10.18653/v1/2025.naacl-long.364) |  | 0 |  | Donglei Yu, Xiaomian Kang, Yuchen Liu, Feifei Zhai, Nanchang Cheng, Yu Zhou, Chengqing Zong |  |
| 446 |  |  [Markov Chain of Thought for Efficient Mathematical Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.365) |  | 0 |  | Wen Yang, Minpeng Liao, Kai Fan |  |
| 447 |  |  [Towards Inducing Long-Context Abilities in Multilingual Neural Machine Translation Models](https://doi.org/10.18653/v1/2025.naacl-long.366) |  | 0 |  | Varun Gumma, Pranjal A. Chitale, Kalika Bali |  |
| 448 |  |  [Yeah, Un, Oh: Continuous and Real-time Backchannel Prediction with Fine-tuning of Voice Activity Projection](https://doi.org/10.18653/v1/2025.naacl-long.367) |  | 0 |  | Koji Inoue, Divesh Lala, Gabriel Skantze, Tatsuya Kawahara |  |
| 449 |  |  [Prompt Compression for Large Language Models: A Survey](https://doi.org/10.18653/v1/2025.naacl-long.368) |  | 0 |  | Zongqian Li, Yinhong Liu, Yixuan Su, Nigel Collier |  |
| 450 |  |  [Goal-Conditioned DPO: Prioritizing Safety in Misaligned Instructions](https://doi.org/10.18653/v1/2025.naacl-long.369) |  | 0 |  | Joo Bon Maeng, Seongmin Lee, Seokin Seo, KeeEung Kim |  |
| 451 |  |  [K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.370) |  | 0 |  | Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Man Lan, Furu Wei |  |
| 452 |  |  [SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.371) |  | 0 |  | Magdalena Wysocka, Danilo S. Carvalho, Oskar Wysocki, Marco Valentino, André Freitas |  |
| 453 |  |  [The State and Fate of Summarization Datasets: A Survey](https://doi.org/10.18653/v1/2025.naacl-long.372) |  | 0 |  | Noam Dahan, Gabriel Stanovsky |  |
| 454 |  |  [MGM: Global Understanding of Audience Overlap Graphs for Predicting the Factuality and the Bias of News Media](https://doi.org/10.18653/v1/2025.naacl-long.373) |  | 0 |  | Muhammad Arslan Manzoor, Ruihong Zeng, Dilshod Azizov, Preslav Nakov, Shangsong Liang |  |
| 455 |  |  [A Logical Fallacy-Informed Framework for Argument Generation](https://doi.org/10.18653/v1/2025.naacl-long.374) |  | 0 |  | Luca Mouchel, Debjit Paul, Shaobo Cui, Robert West, Antoine Bosselut, Boi Faltings |  |
| 456 |  |  [LLaMA-Berry: Pairwise Optimization for Olympiad-level Mathematical Reasoning via O1-like Monte Carlo Tree Search](https://doi.org/10.18653/v1/2025.naacl-long.375) |  | 0 |  | Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, Dongzhan Zhou |  |
| 457 |  |  [Generative Prompt Internalization](https://doi.org/10.18653/v1/2025.naacl-long.376) |  | 0 |  | Haebin Shin, Lei Ji, Yeyun Gong, Sungdong Kim, Eunbi Choi, Minjoon Seo |  |
| 458 |  |  [Script-Agnosticism and its Impact on Language Identification for Dravidian Languages](https://doi.org/10.18653/v1/2025.naacl-long.377) |  | 0 |  | Milind Agarwal, Joshua Otten, Antonios Anastasopoulos |  |
| 459 |  |  [NAT: Enhancing Agent Tuning with Negative Samples](https://doi.org/10.18653/v1/2025.naacl-long.378) |  | 0 |  | Renxi Wang, Xudong Han, Yixuan Zhang, Timothy Baldwin, Haonan Li |  |
| 460 |  |  [Hazards in Daily Life? Enabling Robots to Proactively Detect and Resolve Anomalies](https://doi.org/10.18653/v1/2025.naacl-long.379) |  | 0 |  | Zirui Song, Guangxian Ouyang, Meng Fang, Hongbin Na, Zijing Shi, Zhenhao Chen, Yujie Fu, Zeyu Zhang, Shiyu Jiang, Miao Fang, Ling Chen, Xiuying Chen |  |
| 461 |  |  [How to Make the Most of LLMs' Grammatical Knowledge for Acceptability Judgments](https://doi.org/10.18653/v1/2025.naacl-long.380) |  | 0 |  | Yusuke Ide, Yuto Nishida, Justin Vasselli, Miyu Oba, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe |  |
| 462 |  |  [Is Your LLM Outdated? A Deep Look at Temporal Generalization](https://doi.org/10.18653/v1/2025.naacl-long.381) |  | 0 |  | ChenghaoZhu ChenghaoZhu, Nuo Chen, Yufei Gao, Yunyi Zhang, Prayag Tiwari, Benyou Wang |  |
| 463 |  |  [Towards a Perspectivist Turn in Argument Quality Assessment](https://doi.org/10.18653/v1/2025.naacl-long.382) |  | 0 |  | Julia Romberg, Maximilian Maurer, Henning Wachsmuth, Gabriella Lapesa |  |
| 464 |  |  [A Picture is Worth A Thousand Numbers: Enabling LLMs Reason about Time Series via Visualization](https://doi.org/10.18653/v1/2025.naacl-long.383) |  | 0 |  | Haoxin Liu, Chenghao Liu, B. Aditya Prakash |  |
| 465 |  |  [PlagBench: Exploring the Duality of Large Language Models in Plagiarism Generation and Detection](https://doi.org/10.18653/v1/2025.naacl-long.384) |  | 0 |  | Jooyoung Lee, Toshini Agrawal, Adaku Uchendu, Thai Le, Jinghui Chen, Dongwon Lee |  |
| 466 |  |  [Commonality and Individuality! Integrating Humor Commonality with Speaker Individuality for Humor Recognition](https://doi.org/10.18653/v1/2025.naacl-long.385) |  | 0 |  | Haohao Zhu, Xiaokun Zhang, Zeyuan Zeng, Junyu Lu, Zewen Bai, Liang Yang, Hongfei Lin |  |
| 467 |  |  [CAST: Corpus-Aware Self-similarity Enhanced Topic modelling](https://doi.org/10.18653/v1/2025.naacl-long.386) |  | 0 |  | Yanan Ma, Chenghao Xiao, Chenhan Yuan, Sabine N. van der Veer, Lamiece Hassan, Chenghua Lin, Goran Nenadic |  |
| 468 |  |  [A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding](https://doi.org/10.18653/v1/2025.naacl-long.387) |  | 0 |  | Abdulfattah Safa, Gözde Gül Sahin |  |
| 469 |  |  [Navigating the Cultural Kaleidoscope: A Hitchhiker's Guide to Sensitivity in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.388) |  | 0 |  | Somnath Banerjee, Sayan Layek, Hari Shrawgi, Rajarshi Mandal, Avik Halder, Shanu Kumar, Sagnik Basu, Parag Agrawal, Rima Hazra, Animesh Mukherjee |  |
| 470 |  |  [Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models](https://doi.org/10.18653/v1/2025.naacl-long.389) |  | 0 |  | Michael Toker, Ido Galil, Hadas Orgad, Rinon Gal, Yoad Tewel, Gal Chechik, Yonatan Belinkov |  |
| 471 |  |  [In-Context Learning (and Unlearning) of Length Biases](https://doi.org/10.18653/v1/2025.naacl-long.390) |  | 0 |  | Stephanie Schoch, Yangfeng Ji |  |
| 472 |  |  [AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising](https://doi.org/10.18653/v1/2025.naacl-long.391) |  | 0 |  | Peinan Zhang, Yusuke Sakai, Masato Mita, Hiroki Ouchi, Taro Watanabe |  |
| 473 |  |  [Empowering Retrieval-based Conversational Recommendation with Contrasting User Preferences](https://doi.org/10.18653/v1/2025.naacl-long.392) |  | 0 |  | Heejin Kook, Junyoung Kim, Seongmin Park, Jongwuk Lee |  |
| 474 |  |  [LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices](https://doi.org/10.18653/v1/2025.naacl-long.393) |  | 0 |  | Jung Hyun Lee, Jeonghoon Kim, June Yong Yang, Se Jung Kwon, Eunho Yang, Kang Min Yoo, Dongsoo Lee |  |
| 475 |  |  [Towards Robust Knowledge Representations in Multilingual LLMs for Equivalence and Inheritance based Consistent Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.394) |  | 0 |  | Gaurav Arora, Srujana Merugu, Shreya Jain, Vaibhav Saxena |  |
| 476 |  |  [LLMs as Meta-Reviewers' Assistants: A Case Study](https://doi.org/10.18653/v1/2025.naacl-long.395) |  | 0 |  | Eftekhar Hossain, Sanjeev Kumar Sinha, Naman Bansal, R. Alexander Knipper, Souvika Sarkar, John Salvador, Yash Mahajan, Sri Guttikonda, Mousumi Akter, Md. Mahadi Hassan, Matthew Freestone, Matthew C. Williams Jr., Dongji Feng, Santu Karmaker |  |
| 477 |  |  [A Survey of NLP Progress in Sino-Tibetan Low-Resource Languages](https://doi.org/10.18653/v1/2025.naacl-long.396) |  | 0 |  | Shuheng Liu, Michael Best |  |
| 478 |  |  [Enhancing Language Model Hypernetworks with Restart: A Study on Optimization](https://doi.org/10.18653/v1/2025.naacl-long.397) |  | 0 |  | Yihan Zhang, Jie Fu, Rongrong Ji, Jie Chen |  |
| 479 |  |  [Functional Lexicon in Subword Tokenization](https://doi.org/10.18653/v1/2025.naacl-long.398) |  | 0 |  | Zachary William Hopton, Yves Scherrer, Tanja Samardzic |  |
| 480 |  |  [Getting More Juice Out of Your Data: Hard Pair Refinement Enhances Visual-Language Models Without Extra Data](https://doi.org/10.18653/v1/2025.naacl-long.399) |  | 0 |  | Haonan Wang, Minbin Huang, Runhui Huang, Lanqing Hong, Hang Xu, Tianyang Hu, Xiaodan Liang, Zhenguo Li, Hong Cheng, Kenji Kawaguchi |  |
| 481 |  |  [Evaluating the Prompt Steerability of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.400) |  | 0 |  | Erik Miehling, Michael Desmond, Karthikeyan Natesan Ramamurthy, Elizabeth M. Daly, Kush R. Varshney, Eitan Farchi, Pierre Dognin, Jesus Rios, Djallel Bouneffouf, Miao Liu, Prasanna Sattigeri |  |
| 482 |  |  [A Data-Driven Method for Analyzing and Quantifying Lyrics-Dance Motion Relationships](https://doi.org/10.18653/v1/2025.naacl-long.401) |  | 0 |  | Kento Watanabe, Masataka Goto |  |
| 483 |  |  [CROPE: Evaluating In-Context Adaptation of Vision and Language Models to Culture-Specific Concepts](https://doi.org/10.18653/v1/2025.naacl-long.402) |  | 0 |  | Malvina Nikandrou, Georgios Pantazopoulos, Nikolas Vitsakis, Ioannis Konstas, Alessandro Suglia |  |
| 484 |  |  [PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona](https://doi.org/10.18653/v1/2025.naacl-long.403) |  | 0 |  | Jihyun Lee, Yejin Jeon, Seungyeon Seo, Gary Lee |  |
| 485 |  |  [Scaling LLM Inference Efficiently with Optimized Sample Compute Allocation](https://doi.org/10.18653/v1/2025.naacl-long.404) |  | 0 |  | Kexun Zhang, Shang Zhou, Danqing Wang, William Yang Wang, Lei Li |  |
| 486 |  |  [Large Language Models for Persian-English Idiom Translation](https://doi.org/10.18653/v1/2025.naacl-long.405) |  | 0 |  | Sara Rezaeimanesh, Faezeh Hosseini, Yadollah Yaghoobzadeh |  |
| 487 |  |  [Follow the Beaten Path: The Role of Route Patterns on Vision-Language Navigation Agents Generalization Abilities](https://doi.org/10.18653/v1/2025.naacl-long.406) |  | 0 |  | Kourosh T. Baghaei, Dieter Pfoser, Antonios Anastasopoulos |  |
| 488 |  |  [Sneaking Syntax into Transformer Language Models with Tree Regularization](https://doi.org/10.18653/v1/2025.naacl-long.407) |  | 0 |  | Ananjan Nandi, Christopher D. Manning, Shikhar Murty |  |
| 489 |  |  [Meta-Cultural Competence: Climbing the Right Hill of Cultural Awareness](https://doi.org/10.18653/v1/2025.naacl-long.408) |  | 0 |  | Sougata Saha, Saurabh Kumar Pandey, Monojit Choudhury |  |
| 490 |  |  [Reading between the Lines: Can LLMs Identify Cross-Cultural Communication Gaps?](https://doi.org/10.18653/v1/2025.naacl-long.409) |  | 0 |  | Sougata Saha, Saurabh Kumar Pandey, Harshit Gupta, Monojit Choudhury |  |
| 491 |  |  [HMT: Hierarchical Memory Transformer for Efficient Long Context Language Processing](https://doi.org/10.18653/v1/2025.naacl-long.410) |  | 0 |  | Zifan He, Yingqi Cao, Zongyue Qin, Neha Prakriya, Yizhou Sun, Jason Cong |  |
| 492 |  |  [Faux Polyglot: A Study on Information Disparity in Multilingual Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.411) |  | 0 |  | Nikhil Sharma, Kenton Murray, Ziang Xiao |  |
| 493 |  |  [Teaching Models to Balance Resisting and Accepting Persuasion](https://doi.org/10.18653/v1/2025.naacl-long.412) |  | 0 |  | Elias StengelEskin, Peter Hase, Mohit Bansal |  |
| 494 |  |  [Making Language Models Robust Against Negation](https://doi.org/10.18653/v1/2025.naacl-long.413) |  | 0 |  | MohammadHossein Rezaei, Eduardo Blanco |  |
| 495 |  |  [Through the Lens of History: Methods for Analyzing Temporal Variation in Content and Framing of State-run Chinese Newspapers](https://doi.org/10.18653/v1/2025.naacl-long.414) |  | 0 |  | Shijia Liu, David A. Smith |  |
| 496 |  |  [PoisonedParrot: Subtle Data Poisoning Attacks to Elicit Copyright-Infringing Content from Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.415) |  | 0 |  | MichaelAndrei PanaitescuLiess, Pankayaraj Pathmanathan, Yigitcan Kaya, Zora Che, Bang An, Sicheng Zhu, Aakriti Agrawal, Furong Huang |  |
| 497 |  |  [Towards Operationalizing Right to Data Protection](https://doi.org/10.18653/v1/2025.naacl-long.416) |  | 0 |  | Abhinav Java, Simra Shahid, Chirag Agarwal |  |
| 498 |  |  [Learning vs Retrieval: The Role of In-Context Examples in Regression with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.417) |  | 0 |  | Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi |  |
| 499 |  |  [GLiREL - Generalist Model for Zero-Shot Relation Extraction](https://doi.org/10.18653/v1/2025.naacl-long.418) |  | 0 |  | Jack Boylan, Chris Hokamp, Demian Gholipour Ghalandari |  |
| 500 |  |  [ComPO: Community Preferences for Language Model Personalization](https://doi.org/10.18653/v1/2025.naacl-long.419) |  | 0 |  | Sachin Kumar, Chan Young Park, Yulia Tsvetkov, Noah A. Smith, Hannaneh Hajishirzi |  |
| 501 |  |  [GroundCocoa: A Benchmark for Evaluating Compositional & Conditional Reasoning in Language Models](https://doi.org/10.18653/v1/2025.naacl-long.420) |  | 0 |  | Harsh Kohli, Sachin Kumar, Huan Sun |  |
| 502 |  |  [ALPACA AGAINST VICUNA: Using LLMs to Uncover Memorization of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.421) |  | 0 |  | Aly M. Kassem, Omar Mahmoud, Niloofar Mireshghallah, Hyunwoo Kim, Yulia Tsvetkov, Yejin Choi, Sherif Saad, Santu Rana |  |
| 503 |  |  [Evaluating Contextualized Representations of (Spanish) Ambiguous Words: A New Lexical Resource and Empirical Analysis](https://doi.org/10.18653/v1/2025.naacl-long.422) |  | 0 |  | Pamela D. Rivière, Anne L. BeattyMartínez, Sean Trott |  |
| 504 |  |  [Understanding LLMs' Fluid Intelligence Deficiency: An Analysis of the ARC Task](https://doi.org/10.18653/v1/2025.naacl-long.423) |  | 0 |  | Junjie Wu, Mo Yu, Lemao Liu, DitYan Yeung, Jie Zhou |  |
| 505 |  |  [FedSpaLLM: Federated Pruning of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.424) |  | 0 |  | Guangji Bai, Yijiang Li, Zilinghan Li, Liang Zhao, Kibaek Kim |  |
| 506 |  |  [IHEval: Evaluating Language Models on Following the Instruction Hierarchy](https://doi.org/10.18653/v1/2025.naacl-long.425) |  | 0 |  | Zhihan Zhang, Shiyang Li, Zixuan Zhang, Xin Liu, Haoming Jiang, Xianfeng Tang, Yifan Gao, Zheng Li, Haodong Wang, Zhaoxuan Tan, Yichuan Li, Qingyu Yin, Bing Yin, Meng Jiang |  |
| 507 |  |  [Afrispeech-Dialog: A Benchmark Dataset for Spontaneous English Conversations in Healthcare and Beyond](https://doi.org/10.18653/v1/2025.naacl-long.426) |  | 0 |  | Mardhiyah Sanni, Tassallah Abdullahi, Devendra Deepak Kayande, Emmanuel Ayodele, Naome A. Etori, Michael S. Mollel, Moshood Yekini, Chibuzor Okocha, Lukman E. Ismaila, Folafunmi Omofoye, Boluwatife Adeleye Adewale, Tobi Olatunji |  |
| 508 |  |  [THREAD: Thinking Deeper with Recursive Spawning](https://doi.org/10.18653/v1/2025.naacl-long.427) |  | 0 |  | Philip Schroeder, Nathaniel Morgan, Hongyin Luo, James R. Glass |  |
| 509 |  |  [CORG: Generating Answers from Complex, Interrelated Contexts](https://doi.org/10.18653/v1/2025.naacl-long.428) |  | 0 |  | Hyunji Lee, Franck Dernoncourt, Trung Bui, Seunghyun Yoon |  |
| 510 |  |  [Generating Diverse Hypotheses for Inductive Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.429) |  | 0 |  | Kangil Lee, Hyukhun Koh, Dongryeol Lee, Seunghyun Yoon, Minsung Kim, Kyomin Jung |  |
| 511 |  |  [On the Analysis and Distillation of Emergent Outlier Properties in Pre-trained Language Models](https://doi.org/10.18653/v1/2025.naacl-long.430) |  | 0 |  | Tianyang Zhao, Kunwar Yashraj Singh, Srikar Appalaraju, Peng Tang, Ying Nian Wu, Li Erran Li |  |
| 512 |  |  [Open-World Evaluation for Retrieving Diverse Perspectives](https://doi.org/10.18653/v1/2025.naacl-long.431) |  | 0 |  | HungTing Chen, Eunsol Choi |  |
| 513 |  |  [Analyzing the Inner Workings of Transformers in Compositional Generalization](https://doi.org/10.18653/v1/2025.naacl-long.432) |  | 0 |  | Ryoma Kumon, Hitomi Yanaka |  |
| 514 |  |  [Substance Beats Style: Why Beginning Students Fail to Code with LLMs](https://doi.org/10.18653/v1/2025.naacl-long.433) |  | 0 |  | Francesca Lucchetti, Zixuan Wu, Arjun Guha, Molly Q. Feldman, Carolyn Jane Anderson |  |
| 515 |  |  [Reverse Thinking Makes LLMs Stronger Reasoners](https://doi.org/10.18653/v1/2025.naacl-long.434) |  | 0 |  | Justin ChihYao Chen, Zifeng Wang, Hamid Palangi, Rujun Han, Sayna Ebrahimi, Long T. Le, Vincent Perot, Swaroop Mishra, Mohit Bansal, ChenYu Lee, Tomas Pfister |  |
| 516 |  |  [Towards Lifelong Dialogue Agents via Timeline-based Memory Management](https://doi.org/10.18653/v1/2025.naacl-long.435) |  | 0 |  | Kai Tzuiunn Ong, Namyoung Kim, Minju Gwak, Hyungjoo Chae, Taeyoon Kwon, Yohan Jo, Seungwon Hwang, Dongha Lee, Jinyoung Yeo |  |
| 517 |  |  [StyleDistance: Stronger Content-Independent Style Embeddings with Synthetic Parallel Examples](https://doi.org/10.18653/v1/2025.naacl-long.436) |  | 0 |  | Ajay Patel, Jiacheng Zhu, Justin Qiu, Zachary Horvitz, Marianna Apidianaki, Kathleen McKeown, Chris CallisonBurch |  |
| 518 |  |  [FiNE: Filtering and Improving Noisy Data Elaborately with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.437) |  | 0 |  | Junliang He, Ziyue Fan, Shaohui Kuang, Li Xiaoqing, Kai Song, Yaqian Zhou, Xipeng Qiu |  |
| 519 |  |  [CAMIEval: Enhancing NLG Evaluation through Multidimensional Comparative Instruction-Following Analysis](https://doi.org/10.18653/v1/2025.naacl-long.438) |  | 0 |  | Ziyue Fan, Junliang He, Li Xiaoqing, Shaohui Kuang, Kai Song, Yaqian Zhou, Xipeng Qiu |  |
| 520 |  |  [LongLeader: A Comprehensive Leaderboard for Large Language Models in Long-context Scenarios](https://doi.org/10.18653/v1/2025.naacl-long.439) |  | 0 |  | Pei Chen, Hongye Jin, ChengChe Lee, Rulin Shao, Jingfeng Yang, Mingyu Zhao, Zhaoyu Zhang, Qin Lu, Kaiwen Men, Ning Xie, Huasheng Li, Bing Yin, Han Li, Lingyun Wang |  |
| 521 |  |  [Language Models Can Infer Action Semantics for Symbolic Planners from Environment Feedback](https://doi.org/10.18653/v1/2025.naacl-long.440) |  | 0 |  | Wang Bill Zhu, Ishika Singh, Robin Jia, Jesse Thomason |  |
| 522 |  |  [SLM-Mod: Small Language Models Surpass LLMs at Content Moderation](https://doi.org/10.18653/v1/2025.naacl-long.441) |  | 0 |  | Xianyang Zhan, Agam Goyal, Yilun Chen, Eshwar Chandrasekharan, Koustuv Saha |  |
| 523 |  |  [On Positional Bias of Faithfulness for Long-form Summarization](https://doi.org/10.18653/v1/2025.naacl-long.442) |  | 0 |  | David Wan, Jesse Vig, Mohit Bansal, Shafiq Joty |  |
| 524 |  |  [BPO: Towards Balanced Preference Optimization between Knowledge Breadth and Depth in Alignment](https://doi.org/10.18653/v1/2025.naacl-long.443) |  | 0 |  | Sizhe Wang, Yongqi Tong, Hengyuan Zhang, Dawei Li, Xin Zhang, Tianlong Chen |  |
| 525 |  |  [UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.444) |  | 0 |  | Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramón Huerta, Ivan Vulic |  |
| 526 |  |  [H-STAR: LLM-driven Hybrid SQL-Text Adaptive Reasoning on Tables](https://doi.org/10.18653/v1/2025.naacl-long.445) |  | 0 |  | Nikhil Abhyankar, Vivek Gupta, Dan Roth, Chandan K. Reddy |  |
| 527 |  |  [Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations](https://doi.org/10.18653/v1/2025.naacl-long.446) |  | 0 |  | Yinghan Zhou, Juan Wen, Wanli Peng, Yiming Xue, Ziwei Zhang, Zhengxian Wu |  |
| 528 |  |  [Vision-Language Models Can Self-Improve Reasoning via Reflection](https://doi.org/10.18653/v1/2025.naacl-long.447) |  | 0 |  | Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou, Yang Liu |  |
| 529 |  |  [Emergence of Episodic Memory in Transformers: Characterizing Changes in Temporal Structure of Attention Scores During Training](https://doi.org/10.18653/v1/2025.naacl-long.448) |  | 0 |  | Deven Mahesh Mistry, Anooshka Bajaj, Yash Aggarwal, Sahaj Singh Maini, Zoran Tiganj |  |
| 530 |  |  [Knowledge Graph-Guided Retrieval Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-long.449) |  | 0 |  | Xiangrong Zhu, Yuexiang Xie, Yi Liu, Yaliang Li, Wei Hu |  |
| 531 |  |  [Amphista: Bi-directional Multi-head Decoding for Accelerating LLM Inference](https://doi.org/10.18653/v1/2025.naacl-long.450) |  | 0 |  | Zeping Li, Xinlong Yang, Ziheng Gao, Ji Liu, Guanchen Li, Zhuang Liu, Dong Li, Jinzhang Peng, Lu Tian, Emad Barsoum |  |
| 532 |  |  [CAVE: Controllable Authorship Verification Explanations](https://doi.org/10.18653/v1/2025.naacl-long.451) |  | 0 |  | Sahana Ramnath, Kartik Pandey, Elizabeth Boschee, Xiang Ren |  |
| 533 |  |  [Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.452) |  | 0 |  | Dongryeol Lee, Yerin Hwang, Yongil Kim, Joonsuk Park, Kyomin Jung |  |
| 534 |  |  [Dynamic Uncertainty Ranking: Enhancing Retrieval-Augmented In-Context Learning for Long-Tail Knowledge in LLMs](https://doi.org/10.18653/v1/2025.naacl-long.453) |  | 0 |  | Shuyang Yu, Runxue Bao, Parminder Bhatia, Taha A. KassHout, Jiayu Zhou, Cao Xiao |  |
| 535 |  |  [Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training](https://doi.org/10.18653/v1/2025.naacl-long.454) |  | 0 |  | Sun Ao, Weilin Zhao, Xu Han, Cheng Yang, Xinrong Zhang, Zhiyuan Liu, Chuan Shi, Maosong Sun |  |
| 536 |  |  [Differentially Private Learning Needs Better Model Initialization and Self-Distillation](https://doi.org/10.18653/v1/2025.naacl-long.455) |  | 0 |  | Ivoline C. Ngong, Joseph P. Near, Niloofar Mireshghallah |  |
| 537 |  |  [Is a Peeled Apple Still Red? Evaluating LLMs' Ability for Conceptual Combination with Property Type](https://doi.org/10.18653/v1/2025.naacl-long.456) |  | 0 |  | Seokwon Song, Taehyun Lee, Jaewoo Ahn, Jae Hyuk Sung, Gunhee Kim |  |
| 538 |  |  [CRScore: Grounding Automated Evaluation of Code Review Comments in Code Claims and Smells](https://doi.org/10.18653/v1/2025.naacl-long.457) |  | 0 |  | Atharva Naik, Marcus Alenius, Daniel Fried, Carolyn P. Rosé |  |
| 539 |  |  [KS-Lottery: Finding Certified Lottery Tickets for Multilingual Transfer in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.458) |  | 0 |  | Fei Yuan, Chang Ma, Shuai Yuan, Qiushi Sun, Lei Li |  |
| 540 |  |  [PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization](https://doi.org/10.18653/v1/2025.naacl-long.459) |  | 0 |  | Jiayi Wu, Hengyi Cai, Lingyong Yan, Hao Sun, Xiang Li, Shuaiqiang Wang, Dawei Yin, Ming Gao |  |
| 541 |  |  [B⁴: A Black-Box Scrubbing Attack on LLM Watermarks](https://doi.org/10.18653/v1/2025.naacl-long.460) |  | 0 |  | Baizhou Huang, Xiao Pu, Xiaojun Wan |  |
| 542 |  |  [IMRRF: Integrating Multi-Source Retrieval and Redundancy Filtering for LLM-based Fake News Detection](https://doi.org/10.18653/v1/2025.naacl-long.461) |  | 0 |  | Dayang Li, Fanxiao Li, Bingbing Song, Li Tang, Wei Zhou |  |
| 543 |  |  [Matina: A Large-Scale 73B Token Persian Text Corpus](https://doi.org/10.18653/v1/2025.naacl-long.462) |  | 0 |  | Sara Bourbour Hosseinbeigi, Fatemeh Taherinezhad, Heshaam Faili, Hamed Baghbani, Fatemeh Nadi, Mostafa Amiri |  |
| 544 |  |  [SMAB: MAB based word Sensitivity Estimation Framework and its Applications in Adversarial Text Generation](https://doi.org/10.18653/v1/2025.naacl-long.463) |  | 0 |  | Saurabh Kumar Pandey, Sachin Vashistha, Debrup Das, Somak Aditya, Monojit Choudhury |  |
| 545 |  |  [ManaTTS Persian: a recipe for creating TTS datasets for lower resource languages](https://doi.org/10.18653/v1/2025.naacl-long.464) |  | 0 |  | Mahta Fetrat Qharabagh, Zahra Dehghanian, Hamid R. Rabiee |  |
| 546 |  |  [CultureInstruct: Curating Multi-Cultural Instructions at Scale](https://doi.org/10.18653/v1/2025.naacl-long.465) |  | 0 |  | Viet Thanh Pham, Zhuang Li, Lizhen Qu, Gholamreza Haffari |  |
| 547 |  |  [Lost in Inference: Rediscovering the Role of Natural Language Inference for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.466) |  | 0 |  | Lovish Madaan, David Esiobu, Pontus Stenetorp, Barbara Plank, Dieuwke Hupkes |  |
| 548 |  |  [DenseSSM: State Space Models with Dense Hidden Connection for Efficient Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.467) |  | 0 |  | Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang |  |
| 549 |  |  [A Mixed-Language Multi-Document News Summarization Dataset and a Graphs-Based Extract-Generate Model](https://doi.org/10.18653/v1/2025.naacl-long.468) |  | 0 |  | Shengxiang Gao, Fang Nan, Yongbing Zhang, Yuxin Huang, Kaiwen Tan, Zhengtao Yu |  |
| 550 |  |  [Measuring memorization in language models via probabilistic extraction](https://doi.org/10.18653/v1/2025.naacl-long.469) |  | 0 |  | Jamie Hayes, Marika Swanberg, Harsh Chaudhari, Itay Yona, Ilia Shumailov, Milad Nasr, Christopher A. ChoquetteChoo, Katherine Lee, A. Feder Cooper |  |
| 551 |  |  [Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models](https://doi.org/10.18653/v1/2025.naacl-long.470) |  | 0 |  | Hao Yang, Lizhen Qu, Ehsan Shareghi, Gholamreza Haffari |  |
| 552 |  |  [EMS-SD: Efficient Multi-sample Speculative Decoding for Accelerating Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.471) |  | 0 |  | Yunsheng Ni, Chuanjian Liu, Yehui Tang, Kai Han, Yunhe Wang |  |
| 553 |  |  [Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for Language Model Alignment](https://doi.org/10.18653/v1/2025.naacl-long.472) |  | 0 |  | Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, Kenshi Abe |  |
| 554 |  |  [MAPWise: Evaluating Vision-Language Models for Advanced Map Queries](https://doi.org/10.18653/v1/2025.naacl-long.473) |  | 0 |  | Srija Mukhopadhyay, Abhishek Rajgaria, Prerana Khatiwada, Manish Shrivastava, Dan Roth, Vivek Gupta |  |
| 555 |  |  [Pay More Attention to Images: Numerous Images-Oriented Multimodal Summarization](https://doi.org/10.18653/v1/2025.naacl-long.474) |  | 0 |  | Min Xiao, Junnan Zhu, Feifei Zhai, Chengqing Zong, Yu Zhou |  |
| 556 |  |  [S²-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate Efficiency](https://doi.org/10.18653/v1/2025.naacl-long.475) |  | 0 |  | Yuting Zeng, Weizhe Huang, Lei Jiang, Tongxuan Liu, Xitai Jin, Chen Tianying Tiana, Jing Li, Xiaohua Xu |  |
| 557 |  |  [MASTER: A Multi-Agent System with LLM Specialized MCTS](https://doi.org/10.18653/v1/2025.naacl-long.476) |  | 0 |  | Bingzheng Gan, Yufan Zhao, Tianyi Zhang, Jing Huang, Yusu Li, Shu Xian Teo, Changwang Zhang, Wei Shi |  |
| 558 |  |  [ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots](https://doi.org/10.18653/v1/2025.naacl-long.477) |  | 0 |  | YuChung Hsiao, Fedir Zubach, Gilles Baechler, Srinivas Sunkara, Victor Carbune, Jason Lin, Maria Wang, Yun Zhu, Jindong Chen |  |
| 559 |  |  [Cross-Lingual and Cross-Cultural Variation in Image Descriptions](https://doi.org/10.18653/v1/2025.naacl-long.478) |  | 0 |  | Uri Berger, Edoardo M. Ponti |  |
| 560 |  |  [Soft Syntactic Reinforcement for Neural Event Extraction](https://doi.org/10.18653/v1/2025.naacl-long.479) |  | 0 |  | Anran Hao, Jian Su, Shuo Sun, Teo Yong Sen |  |
| 561 |  |  [Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models](https://doi.org/10.18653/v1/2025.naacl-long.480) |  | 0 |  | Hyegang Son, Yonglak Son, Changhoon Kim, Young Geun Kim |  |
| 562 |  |  [Bridging the Gap between Expert and Language Models: Concept-guided Chess Commentary Generation and Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.481) |  | 0 |  | Jaechang Kim, Jinmin Goh, Inseok Hwang, Jaewoong Cho, Jungseul Ok |  |
| 563 |  |  [TCProF:Time-Complexity Prediction SSL Framework](https://doi.org/10.18653/v1/2025.naacl-long.482) |  | 0 |  | Joonghyuk Hahn, Hyeseon Ahn, Jungin Kim, Soohan Lim, YoSub Han |  |
| 564 |  |  [Culture-TRIP: Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinement](https://doi.org/10.18653/v1/2025.naacl-long.483) |  | 0 |  | Suchae Jeong, Inseong Choi, Youngsik Yun, Jihie Kim |  |
| 565 |  |  [Behavior-SD: Behaviorally Aware Spoken Dialogue Generation with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.484) |  | 0 |  | Sehun Lee, Kangwook Kim, Gunhee Kim |  |
| 566 |  |  [Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.485) |  | 0 |  | Chaoqun Liu, Wenxuan Zhang, Yiran Zhao, Anh Tuan Luu, Lidong Bing |  |
| 567 |  |  [AlgoPuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Algorithmic Multimodal Puzzles](https://doi.org/10.18653/v1/2025.naacl-long.486) |  | 0 |  | Deepanway Ghosal, Vernon Toh, Yew Ken Chia, Soujanya Poria |  |
| 568 |  |  [Towards Quantifying Commonsense Reasoning with Mechanistic Insights](https://doi.org/10.18653/v1/2025.naacl-long.487) |  | 0 |  | Abhinav Joshi, Areeb Ahmad, Divyaksh Shukla, Ashutosh Modi |  |
| 569 |  |  [Beyond Logit Lens: Contextual Embeddings for Robust Hallucination Detection & Grounding in VLMs](https://doi.org/10.18653/v1/2025.naacl-long.488) |  | 0 |  | Anirudh Phukan, Divyansh, Harshit Kumar Morj, Vaishnavi, Apoorv Saxena, Koustava Goswami |  |
| 570 |  |  [M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.489) |  | 0 |  | Rishabh Maheshwary, Vikas Yadav, Hoang Nguyen, Khyati Mahajan, Sathwik Tejaswi Madhusudhan |  |
| 571 |  |  [Multi³Hate: Multimodal, Multilingual, and Multicultural Hate Speech Detection with Vision-Language Models](https://doi.org/10.18653/v1/2025.naacl-long.490) |  | 0 |  | Minh Duc Bui, Katharina von der Wense, Anne Lauscher |  |
| 572 |  |  [Grounding Fallacies Misrepresenting Scientific Publications in Evidence](https://doi.org/10.18653/v1/2025.naacl-long.491) |  | 0 |  | Max Glockner, Yufang Hou, Preslav Nakov, Iryna Gurevych |  |
| 573 |  |  [Has this Fact been Edited? Detecting Knowledge Edits in Language Models](https://doi.org/10.18653/v1/2025.naacl-long.492) |  | 0 |  | Paul Youssef, Zhixue Zhao, Christin Seifert, Jörg Schlötterer |  |
| 574 |  |  [AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging](https://doi.org/10.18653/v1/2025.naacl-long.493) |  | 0 |  | Yiran Zhao, Wenxuan Zhang, Huiming Wang, Kenji Kawaguchi, Lidong Bing |  |
| 575 |  |  [Coverage-based Fairness in Multi-document Summarization](https://doi.org/10.18653/v1/2025.naacl-long.494) |  | 0 |  | Haoyuan Li, Yusen Zhang, Rui Zhang, Snigdha Chaturvedi |  |
| 576 |  |  [Grammar Control in Dialogue Response Generation for Language Learning Chatbots](https://doi.org/10.18653/v1/2025.naacl-long.495) |  | 0 |  | Dominik Glandorf, Peng Cui, Detmar Meurers, Mrinmaya Sachan |  |
| 577 |  |  [Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge](https://doi.org/10.18653/v1/2025.naacl-long.496) |  | 0 |  | Li Zhou, Taelin Karidi, Wanlong Liu, Nicolas Garneau, Yong Cao, Wenyu Chen, Haizhou Li, Daniel Hershcovich |  |
| 578 |  |  [Palette of Language Models: A Solver for Controlled Text Generation](https://doi.org/10.18653/v1/2025.naacl-long.497) |  | 0 |  | Zhe Yang, Yi Huang, Yaqin Chen, XiaotingWu XiaotingWu, Junlan Feng, Chao Deng |  |
| 579 |  |  [MAMM-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration](https://doi.org/10.18653/v1/2025.naacl-long.498) |  | 0 |  | David Wan, Justin ChihYao Chen, Elias StengelEskin, Mohit Bansal |  |
| 580 |  |  [MADial-Bench: Towards Real-world Evaluation of Memory-Augmented Dialogue Generation](https://doi.org/10.18653/v1/2025.naacl-long.499) |  | 0 |  | Junqing He, Liang Zhu, Rui Wang, Xi Wang, Gholamreza Haffari, Jiaxing Zhang |  |
| 581 |  |  [Assessing the State of the Art in Scene Segmentation](https://doi.org/10.18653/v1/2025.naacl-long.500) |  | 0 |  | Albin Zehe, Elisabeth Fischer, Andreas Hotho |  |
| 582 |  |  [DCE-LLM: Dead Code Elimination with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.501) |  | 0 |  | Minyu Chen, Guoqiang Li, LingI Wu, Ruibang Liu |  |
| 583 |  |  [Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction](https://doi.org/10.18653/v1/2025.naacl-long.502) |  | 0 |  | Liping Liu, Chunhong Zhang, Likang Wu, Chuang Zhao, Zheng Hu, Ming He, Jianping Fan |  |
| 584 |  |  [Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment](https://doi.org/10.18653/v1/2025.naacl-long.503) |  | 0 |  | Sangwon Yu, Jongyoon Song, Bongkyu Hwang, Hoyoung Kang, Sooah Cho, Junhwa Choi, Seongho Joe, Taehee Lee, Youngjune Gwon, Sungroh Yoon |  |
| 585 |  |  [MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps](https://doi.org/10.18653/v1/2025.naacl-long.504) |  | 0 |  | Xiongtao Zhou, Jie He, Lanyu Chen, Jingyu Li, Haojing Chen, Víctor GutiérrezBasulto, Jeff Z. Pan, Hanjie Chen |  |
| 586 |  |  [CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian Product Routing in Mixture-of-Experts](https://doi.org/10.18653/v1/2025.naacl-long.505) |  | 0 |  | Zhenpeng Su, Xing Wu, Zijia Lin, Yizhe Xiong, Minxuan Lv, Guangyuan Ma, Hui Chen, Songlin Hu, Guiguang Ding |  |
| 587 |  |  [Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language](https://doi.org/10.18653/v1/2025.naacl-long.506) |  | 0 |  | Amalie Brogaard Pauli, Isabelle Augenstein, Ira Assent |  |
| 588 |  |  [MILU: A Multi-task Indic Language Understanding Benchmark](https://doi.org/10.18653/v1/2025.naacl-long.507) |  | 0 |  | Sshubam Verma, Mohammed Safi Ur Rahman Khan, Vishwajeet Kumar, Rudra Murthy, Jaydeep Sen |  |
| 589 |  |  [AutoEval-ToD: Automated Evaluation of Task-oriented Dialog Systems](https://doi.org/10.18653/v1/2025.naacl-long.508) |  | 0 |  | Arihant Jain, Purav Aggarwal, Rishav Sahay, Chaosheng Dong, Anoop Saladi |  |
| 590 |  |  [Self-calibration for Language Model Quantization and Pruning](https://doi.org/10.18653/v1/2025.naacl-long.509) |  | 0 |  | Miles Williams, George Chrysostomou, Nikolaos Aletras |  |
| 591 |  |  [Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.510) |  | 0 |  | Tongxuan Liu, Wenjiang Xu, Weizhe Huang, Yuting Zeng, Jiaxing Wang, Xingyu Wang, Hailong Yang, Jing Li |  |
| 592 |  |  [IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval](https://doi.org/10.18653/v1/2025.naacl-long.511) |  | 0 |  | Tingyu Song, Guo Gan, Mingsheng Shang, Yilun Zhao |  |
| 593 |  |  [QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models](https://doi.org/10.18653/v1/2025.naacl-long.512) |  | 0 |  | Yudong Zhang, Ruobing Xie, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, Yu Wang |  |
| 594 |  |  [Evaluating and Improving Graph to Text Generation with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.513) |  | 0 |  | Jie He, Yijun Yang, Wanqiu Long, Deyi Xiong, Víctor GutiérrezBasulto, Jeff Z. Pan |  |
| 595 |  |  [The Plagiarism Singularity Conjecture](https://doi.org/10.18653/v1/2025.naacl-long.514) |  | 0 |  | Sriram Ranga, Rui Mao, Erik Cambria, Anupam Chattopadhyay |  |
| 596 |  |  [Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.515) |  | 0 |  | Sungjin Park, Xiao Liu, Yeyun Gong, Edward Choi |  |
| 597 |  |  [One Unified Model for Diverse Tasks: Emotion Cause Analysis via Self-Promote Cognitive Structure Modeling](https://doi.org/10.18653/v1/2025.naacl-long.516) |  | 0 |  | Zhaoxin Yu, Xinglin Xiao, Wenji Mao |  |
| 598 |  |  [Soft Language Prompts for Language Transfer](https://doi.org/10.18653/v1/2025.naacl-long.517) |  | 0 |  | Ivan Vykopal, Simon Ostermann, Marián Simko |  |
| 599 |  |  [PICLe: Pseudo-annotations for In-Context Learning in Low-Resource Named Entity Detection](https://doi.org/10.18653/v1/2025.naacl-long.518) |  | 0 |  | Sepideh Mamooler, Syrielle Montariol, Alexander Mathis, Antoine Bosselut |  |
| 600 |  |  [Can Large Language Models Invent Algorithms to Improve Themselves?](https://doi.org/10.18653/v1/2025.naacl-long.519) |  | 0 |  | Yoichi Ishibashi, Taro Yano, Masafumi Oyamada |  |
| 601 |  |  [Simulating Classroom Education with LLM-Empowered Agents](https://doi.org/10.18653/v1/2025.naacl-long.520) |  | 0 |  | Zheyuan Zhang, Daniel ZhangLi, Jifan Yu, Linlu Gong, Jinchang Zhou, Zhanxin Hao, Jianxiao Jiang, Jie Cao, Huiqin Liu, Zhiyuan Liu, Lei Hou, Juanzi Li |  |
| 602 |  |  [A Grounded Typology of Word Classes](https://doi.org/10.18653/v1/2025.naacl-long.521) |  | 0 |  | Coleman Haley, Sharon Goldwater, Edoardo M. Ponti |  |
| 603 |  |  [SSH: Sparse Spectrum Adaptation via Discrete Hartley Transformation](https://doi.org/10.18653/v1/2025.naacl-long.522) |  | 0 |  | Yixian Shen, Qi Bi, JiaHong Huang, Hongyi Zhu, Andy D. Pimentel, Anuj Pathania |  |
| 604 |  |  [LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable User Satisfaction Estimation in Dialogue](https://doi.org/10.18653/v1/2025.naacl-long.523) |  | 0 |  | Sangyeop Kim, Sohhyung Park, Jaewon Jung, Jinseok Kim, Sungzoon Cho |  |
| 605 |  |  [LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs](https://doi.org/10.18653/v1/2025.naacl-long.524) |  | 0 |  | Sumin An, Junyoung Sung, Wonpyo Park, Chanjun Park, Paul Hongsuck Seo |  |
| 606 |  |  [A Template Is All You Meme](https://doi.org/10.18653/v1/2025.naacl-long.525) |  | 0 |  | Luke Bates, Peter Ebert Christensen, Preslav Nakov, Iryna Gurevych |  |
| 607 |  |  [LLMs vs Established Text Augmentation Techniques for Classification: When do the Benefits Outweight the Costs?](https://doi.org/10.18653/v1/2025.naacl-long.526) |  | 0 |  | Ján Cegin, Jakub Simko, Peter Brusilovsky |  |
| 608 |  |  [Bridging the Visual Gap: Fine-Tuning Multimodal Models with Knowledge-Adapted Captions](https://doi.org/10.18653/v1/2025.naacl-long.527) |  | 0 |  | Moran Yanuka, Assaf BenKish, Yonatan Bitton, Idan Szpektor, Raja Giryes |  |
| 609 |  |  [Self-Training Meets Consistency: Improving LLMs' Reasoning with Consistency-Driven Rationale Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.528) |  | 0 |  | Jaehyeok Lee, Keisuke Sakaguchi, JinYeong Bak |  |
| 610 |  |  [Evaluating Defeasible Reasoning in LLMs with DEFREASING](https://doi.org/10.18653/v1/2025.naacl-long.529) |  | 0 |  | Emily Allaway, Kathleen McKeown |  |
| 611 |  |  [Evaluating Input Feature Explanations through a Unified Diagnostic Evaluation Framework](https://doi.org/10.18653/v1/2025.naacl-long.530) |  | 0 |  | Jingyi Sun, Pepa Atanasova, Isabelle Augenstein |  |
| 612 |  |  [From Evidence to Belief: A Bayesian Epistemology Approach to Language Models](https://doi.org/10.18653/v1/2025.naacl-long.531) |  | 0 |  | Minsu Kim, Sangryul Kim, James Thorne |  |
| 613 |  |  [Private Synthetic Text Generation with Diffusion Models](https://doi.org/10.18653/v1/2025.naacl-long.532) |  | 0 |  | Sebastian Ochs, Ivan Habernal |  |
| 614 |  |  [Mitigating Tail Narrowing in LLM Self-Improvement via Socratic-Guided Sampling](https://doi.org/10.18653/v1/2025.naacl-long.533) |  | 0 |  | Yiwen Ding, Zhiheng Xi, Wei He, Lizhuoyuan Lizhuoyuan, Yitao Zhai, Shi Xiaowei, Xunliang Cai, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 615 |  |  [FactEval: Evaluating the Robustness of Fact Verification Systems in the Era of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.534) |  | 0 |  | Mamta Mamta, Oana Cocarascu |  |
| 616 |  |  [Analyzing Memorization in Large Language Models through the Lens of Model Attribution](https://doi.org/10.18653/v1/2025.naacl-long.535) |  | 0 |  | Tarun Ram Menta, Susmit Agrawal, Chirag Agarwal |  |
| 617 |  |  [Track-SQL: Enhancing Generative Language Models with Dual-Extractive Modules for Schema and Context Tracking in Multi-turn Text-to-SQL](https://doi.org/10.18653/v1/2025.naacl-long.536) |  | 0 |  | Bingfeng Chen, Shaobin Shi, Yongqi Luo, Boyan Xu, Ruichu Cai, Zhifeng Hao |  |
| 618 |  |  [Prototypical Extreme Multi-label Classification with a Dynamic Margin Loss](https://doi.org/10.18653/v1/2025.naacl-long.537) |  | 0 |  | Kunal Dahiya, Diego Ortego, David JimenezCabello |  |
| 619 |  |  [MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback](https://doi.org/10.18653/v1/2025.naacl-long.538) |  | 0 |  | Zonghai Yao, Aditya Parashar, Huixue Zhou, Won Seok Jang, Feiyun Ouyang, Zhichao Yang, Hong Yu |  |
| 620 |  |  [Main Predicate and Their Arguments as Explanation Signals For Intent Classification](https://doi.org/10.18653/v1/2025.naacl-long.539) |  | 0 |  | Sameer Pimparkhede, Pushpak Bhattacharyya |  |
| 621 |  |  [Handling Missing Entities in Zero-Shot Named Entity Recognition: Integrated Recall and Retrieval Augmentation](https://doi.org/10.18653/v1/2025.naacl-long.540) |  | 0 |  | Ruichu Cai, Junhao Lu, Zhongjie Chen, Boyan Xu, Zhifeng Hao |  |
| 622 |  |  [KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy](https://doi.org/10.18653/v1/2025.naacl-long.541) |  | 0 |  | Hyunjong Kim, Suyeon Lee, Yeongjae Cho, Eunseo Ryu, Yohan Jo, Suran Seong, Sungzoon Cho |  |
| 623 |  |  [Automatic Input Rewriting Improves Translation with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.542) |  | 0 |  | Dayeon Ki, Marine Carpuat |  |
| 624 |  |  [HIGGS: Pushing the Limits of Large Language Model Quantization via the Linearity Theorem](https://doi.org/10.18653/v1/2025.naacl-long.543) |  | 0 |  | Vladimir Malinovskii, Andrei Panferov, Ivan Ilin, Han Guo, Peter Richtárik, Dan Alistarh |  |
| 625 |  |  [The LLM Language Network: A Neuroscientific Approach for Identifying Causally Task-Relevant Units](https://doi.org/10.18653/v1/2025.naacl-long.544) |  | 0 |  | Badr AlKhamissi, Greta Tuckute, Antoine Bosselut, Martin Schrimpf |  |
| 626 |  |  [MixLLM: Dynamic Routing in Mixed Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.545) |  | 0 |  | Xinyuan Wang, Yanchi Liu, Wei Cheng, Xujiang Zhao, Zhengzhang Chen, Wenchao Yu, Yanjie Fu, Haifeng Chen |  |
| 627 |  |  [Continual Learning in Multilingual Sign Language Translation](https://doi.org/10.18653/v1/2025.naacl-long.546) |  | 0 |  | Shakib Yazdani, Josef van Genabith, Cristina EspañaBonet |  |
| 628 |  |  [Few-Shot Natural Language to First-Order Logic Translation via Code Generation](https://doi.org/10.18653/v1/2025.naacl-long.547) |  | 0 |  | Junnan Liu |  |
| 629 |  |  [How Good Are LLMs for Literary Translation, Really? Literary Translation Evaluation with Humans and LLMs](https://doi.org/10.18653/v1/2025.naacl-long.548) |  | 0 |  | Ran Zhang, Wei Zhao, Steffen Eger |  |
| 630 |  |  [PORT: Preference Optimization on Reasoning Traces](https://doi.org/10.18653/v1/2025.naacl-long.549) |  | 0 |  | Salem Lahlou, Abdalgader Abubaker, Hakim Hacid |  |
| 631 |  |  [Guiding Through Complexity: What Makes Good Supervision for Hard Reasoning Tasks?](https://doi.org/10.18653/v1/2025.naacl-long.550) |  | 0 |  | Xuan He, Da Yin, Nanyun Peng |  |
| 632 |  |  [Fine-Grained Transfer Learning for Harmful Content Detection through Label-Specific Soft Prompt Tuning](https://doi.org/10.18653/v1/2025.naacl-long.551) |  | 0 |  | Faeze Ghorbanpour, Viktor Hangya, Alexander Fraser |  |
| 633 |  |  [A Systematic Examination of Preference Learning through the Lens of Instruction-Following](https://doi.org/10.18653/v1/2025.naacl-long.552) |  | 0 |  | Joongwon Kim, Anirudh Goyal, Aston Zhang, Bo Xiong, Rui Hou, Melanie Kambadur, Dhruv Mahajan, Hannaneh Hajishirzi, Liang Tan |  |
| 634 |  |  [Lived Experience Not Found: LLMs Struggle to Align with Experts on Addressing Adverse Drug Reactions from Psychiatric Medication Use](https://doi.org/10.18653/v1/2025.naacl-long.553) |  | 0 |  | Mohit Chandra, Siddharth Sriraman, Gaurav Verma, Harneet Singh Khanuja, Jose Suarez Campayo, Zihang Li, Michael L. Birnbaum, Munmun De Choudhury |  |
| 635 |  |  [Latent Factor Models Meets Instructions: Goal-conditioned Latent Factor Discovery without Task Supervision](https://doi.org/10.18653/v1/2025.naacl-long.554) |  | 0 |  | Zhouhang Xie, Tushar Khot, Bhavana Dalvi Mishra, Harshit Surana, Julian J. McAuley, Peter Clark, Bodhisattwa Prasad Majumder |  |
| 636 |  |  [LLM-Supported Natural Language to Bash Translation](https://doi.org/10.18653/v1/2025.naacl-long.555) |  | 0 |  | Finnian Westenfelder, Erik Hemberg, Stephen Moskal, UnaMay O'Reilly, Silviu Chiricescu |  |
| 637 |  |  [REL-A.I.: An Interaction-Centered Approach To Measuring Human-LM Reliance](https://doi.org/10.18653/v1/2025.naacl-long.556) |  | 0 |  | Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, Nouha Dziri, Dan Jurafsky, Maarten Sap |  |
| 638 |  |  [Eliciting Critical Reasoning in Retrieval-Augmented Generation via Contrastive Explanations](https://doi.org/10.18653/v1/2025.naacl-long.557) |  | 0 |  | Leonardo Ranaldi, Marco Valentino, André Freitas |  |
| 639 |  |  [A Distributional Perspective on Word Learning in Neural Language Models](https://doi.org/10.18653/v1/2025.naacl-long.558) |  | 0 |  | Filippo Ficarra, Ryan Cotterell, Alex Warstadt |  |
| 640 |  |  [Disentangling language change: sparse autoencoders quantify the semantic evolution of indigeneity in French](https://doi.org/10.18653/v1/2025.naacl-long.559) |  | 0 |  | Jacob Matthews, Laurent Dubreuil, Imane Terhmina, Yunci Sun, Matthew Wilkens, Marten van Schijndel |  |
| 641 |  |  [Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages](https://doi.org/10.18653/v1/2025.naacl-long.560) |  | 0 |  | Max Zuo, Francisco Piedrahita Velez, Xiaochen Li, Michael Littman, Stephen H. Bach |  |
| 642 |  |  [One fish, two fish, but not the whole sea: Alignment reduces language models' conceptual diversity](https://doi.org/10.18653/v1/2025.naacl-long.561) |  | 0 |  | Sonia K. Murthy, Tomer D. Ullman, Jennifer Hu |  |
| 643 |  |  [Using Text-Based Causal Inference to Disentangle Factors Influencing Online Review Ratings](https://doi.org/10.18653/v1/2025.naacl-long.562) |  | 0 |  | Linsen Li, Aron Culotta, Nicholas Mattei |  |
| 644 |  |  [Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate](https://doi.org/10.18653/v1/2025.naacl-long.563) |  | 0 |  | Xiaomeng Jin, Zhiqi Bu, Bhanukiran Vinzamuri, Anil Ramakrishna, KaiWei Chang, Volkan Cevher, Mingyi Hong |  |
| 645 |  |  [REFFLY: Melody-Constrained Lyrics Editing Model](https://doi.org/10.18653/v1/2025.naacl-long.564) |  | 0 |  | Songyan Zhao, Bingxuan Li, Yufei Tian, Nanyun Peng |  |
| 646 |  |  [Exploring Safety-Utility Trade-Offs in Personalized Language Models](https://doi.org/10.18653/v1/2025.naacl-long.565) |  | 0 |  | Anvesh Rao Vijjini, Somnath Basu Roy Chowdhury, Snigdha Chaturvedi |  |
| 647 |  |  [MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart Problems](https://doi.org/10.18653/v1/2025.naacl-long.566) |  | 0 |  | Zifeng Zhu, Mengzhao Jia, Zhihan Zhang, Lang Li, Meng Jiang |  |
| 648 |  |  [It Is Not Only the Negative that Deserves Attention! Understanding, Generation & Evaluation of (Positive) Moderation](https://doi.org/10.18653/v1/2025.naacl-long.567) |  | 0 |  | Iman Jundi, Eva Maria Vecchi, Carlotta Quensel, Neele Falk, Gabriella Lapesa |  |
| 649 |  |  [Social Norms in Cinema: A Cross-Cultural Analysis of Shame, Pride and Prejudice](https://doi.org/10.18653/v1/2025.naacl-long.568) |  | 0 |  | Sunny Rai, Khushang Jilesh Zaveri, Shreya Havaldar, Soumna Nema, Lyle H. Ungar, Sharath Chandra Guntuku |  |
| 650 |  |  [The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding](https://doi.org/10.18653/v1/2025.naacl-long.569) |  | 0 |  | Mo Yu, Lemao Liu, Junjie Wu, Tsz Ting Chung, Shunchi Zhang, Jiangnan Li, DitYan Yeung, Jie Zhou |  |
| 651 |  |  [mHumanEval - A Multilingual Benchmark to Evaluate Large Language Models for Code Generation](https://doi.org/10.18653/v1/2025.naacl-long.570) |  | 0 |  | Md. Nishat Raihan, Antonios Anastasopoulos, Marcos Zampieri |  |
| 652 |  |  [What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for Gaussian-Noise-free Text-Image Corruption and Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.571) |  | 0 |  | Michal Golovanevsky, William Rudman, Vedant Palit, Carsten Eickhoff, Ritambhara Singh |  |
| 653 |  |  [Are explicit belief representations necessary? A comparison between Large Language Models and Bayesian probabilistic models](https://doi.org/10.18653/v1/2025.naacl-long.572) |  | 0 |  | Dingyi Pan, Benjamin K. Bergen |  |
| 654 |  |  [Self-Generated Critiques Boost Reward Modeling for Language Models](https://doi.org/10.18653/v1/2025.naacl-long.573) |  | 0 |  | Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, Rui Hou |  |
| 655 |  |  [Characterizing the Role of Similarity in the Property Inferences of Language Models](https://doi.org/10.18653/v1/2025.naacl-long.574) |  | 0 |  | Juan Diego Rodriguez, Aaron Mueller, Kanishka Misra |  |
| 656 |  |  [SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains](https://doi.org/10.18653/v1/2025.naacl-long.575) |  | 0 |  | Ran Xu, Hui Liu, Sreyashi Nag, Zhenwei Dai, Yaochen Xie, Xianfeng Tang, Chen Luo, Yang Li, Joyce C. Ho, Carl Yang, Qi He |  |
| 657 |  |  [Learning to Substitute Words with Model-based Score Ranking](https://doi.org/10.18653/v1/2025.naacl-long.576) |  | 0 |  | Hongye Liu, Ricardo Henao |  |
| 658 |  |  [Multilingual Reasoning via Self-training](https://doi.org/10.18653/v1/2025.naacl-long.577) |  | 0 |  | Leonardo Ranaldi, Giulia Pucci |  |
| 659 |  |  [xLAM: A Family of Large Action Models to Empower AI Agent Systems](https://doi.org/10.18653/v1/2025.naacl-long.578) |  | 0 |  | Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara Prabhakar, Haolin Chen, Zhiwei Liu, Yihao Feng, Tulika Manoj Awalgaonkar, Rithesh R. N., Zeyuan Chen, Ran Xu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong |  |
| 660 |  |  [ProMQA: Question Answering Dataset for Multimodal Procedural Activity Understanding](https://doi.org/10.18653/v1/2025.naacl-long.579) |  | 0 |  | Kimihiro Hasegawa, Wiradee Imrattanatrai, ZhiQi Cheng, Masaki Asada, Susan Holm, Yuran Wang, Ken Fukuda, Teruko Mitamura |  |
| 661 |  |  [Ethical Concern Identification in NLP: A Corpus of ACL Anthology Ethics Statements](https://doi.org/10.18653/v1/2025.naacl-long.580) |  | 0 |  | Antonia Karamolegkou, Sandrine Schiller Hansen, Ariadni Christopoulou, Filippos Stamatiou, Anne Lauscher, Anders Søgaard |  |
| 662 |  |  [AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge](https://doi.org/10.18653/v1/2025.naacl-long.581) |  | 0 |  | Han Wang, Archiki Prasad, Elias StengelEskin, Mohit Bansal |  |
| 663 |  |  [Are Multimodal LLMs Robust Against Adversarial Perturbations? RoMMath: A Systematic Evaluation on Multimodal Math Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.582) |  | 0 |  | Yilun Zhao, Guo Gan, Chen Zhao, Arman Cohan |  |
| 664 |  |  [LBC: Language-Based-Classifier for Out-Of-Variable Generalization](https://doi.org/10.18653/v1/2025.naacl-long.583) |  | 0 |  | Kangjun Noh, Baekryun Seong, Hoyoon Byun, Youngjun Choi, Sungjin Song, Kyungwoo Song |  |
| 665 |  |  [On the Impact of Fine-Tuning on Chain-of-Thought Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.584) |  | 0 |  | Elita A. Lobo, Chirag Agarwal, Himabindu Lakkaraju |  |
| 666 |  |  [InfoPO: On Mutual Information Maximization for Large Language Model Alignment](https://doi.org/10.18653/v1/2025.naacl-long.585) |  | 0 |  | Teng Xiao, Zhen Ge, Sujay Sanghavi, Tian Wang, Julian KatzSamuels, Marc Versage, Qingjun Cui, Trishul Chilimbi |  |
| 667 |  |  [Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse Frequency Effect in Structural Priming](https://doi.org/10.18653/v1/2025.naacl-long.586) |  | 0 |  | Zhenghao Zhou, Robert Frank, R. Thomas McCoy |  |
| 668 |  |  [Guiding Medical Vision-Language Models with Diverse Visual Prompts: Framework Design and Comprehensive Exploration of Prompt Variations](https://doi.org/10.18653/v1/2025.naacl-long.587) |  | 0 |  | Kangyu Zhu, Ziyuan Qin, Huahui Yi, Zekun Jiang, Qicheng Lao, Shaoting Zhang, Kang Li |  |
| 669 |  |  [Analyzing and Improving Coherence of Large Language Models in Question Answering](https://doi.org/10.18653/v1/2025.naacl-long.588) |  | 0 |  | Ivano Lauriola, Stefano Campese, Alessandro Moschitti |  |
| 670 |  |  [ALinFiK: Learning to Approximate Linearized Future Influence Kernel for Scalable Third-Parity LLM Data Valuation](https://doi.org/10.18653/v1/2025.naacl-long.589) |  | 0 |  | Yanzhou Pan, Huawei Lin, Yide Ran, Jiamin Chen, Xiaodong Yu, Weijie Zhao, Denghui Zhang, Zhaozhuo Xu |  |
| 671 |  |  [E-Gen: Leveraging E-Graphs to Improve Continuous Representations of Symbolic Expressions](https://doi.org/10.18653/v1/2025.naacl-long.590) |  | 0 |  | Hongbo Zheng, Suyuan Wang, Neeraj Gangwar, Nickvash Kani |  |
| 672 |  |  [Robust and Unbounded Length Generalization in Autoregressive Transformer-Based Text-to-Speech](https://doi.org/10.18653/v1/2025.naacl-long.591) |  | 0 |  | Eric Battenberg, R. J. SkerryRyan, Daisy Stanton, Soroosh Mariooryad, Matt Shannon, Julian Salazar, David Kao |  |
| 673 |  |  [PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics](https://doi.org/10.18653/v1/2025.naacl-long.592) |  | 0 |  | Daniil Larionov, Steffen Eger |  |
| 674 |  |  [AutoParLLM: GNN-guided Context Generation for Zero-Shot Code Parallelization using LLMs](https://doi.org/10.18653/v1/2025.naacl-long.593) |  | 0 |  | Quazi Ishtiaque Mahmud, Ali TehraniJamsaz, Hung D. Phan, Le Chen, Mihai Capota, Theodore L. Willke, Nesreen K. Ahmed, Ali Jannesari |  |
| 675 |  |  [Causally Modeling the Linguistic and Social Factors that Predict Email Response](https://doi.org/10.18653/v1/2025.naacl-long.594) |  | 0 |  | Yinuo Xu, Hong Chen, Sushrita Rakshit, Aparna Ananthasubramaniam, Omkar Yadav, Mingqian Zheng, Michael Jiang, Lechen Zhang, Bowen Yi, Kenan Alkiek, Abraham Israeli, Bangzhao Shu, Hua Shen, Jiaxin Pei, Haotian Zhang, Miriam Schirmer, David Jurgens |  |
| 676 |  |  [AI-LieDar : Examine the Trade-off Between Utility and Truthfulness in LLM Agents](https://doi.org/10.18653/v1/2025.naacl-long.595) |  | 0 |  | Zhe Su, Xuhui Zhou, Sanketh Rangreji, Anubha Kabra, Julia Mendelsohn, Faeze Brahman, Maarten Sap |  |
| 677 |  |  [Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset](https://doi.org/10.18653/v1/2025.naacl-long.596) |  | 0 |  | Khaoula Chehbouni, Jonathan Colaço Carr, Yash More, Jackie CK Cheung, Golnoosh Farnadi |  |
| 678 |  |  [FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions](https://doi.org/10.18653/v1/2025.naacl-long.597) |  | 0 |  | Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn J. Lawrie, Luca Soldaini |  |
| 679 |  |  [Few-shot Personalization of LLMs with Mis-aligned Responses](https://doi.org/10.18653/v1/2025.naacl-long.598) |  | 0 |  | Jaehyung Kim, Yiming Yang |  |
| 680 |  |  [Prompting with Phonemes: Enhancing LLMs' Multilinguality for Non-Latin Script Languages](https://doi.org/10.18653/v1/2025.naacl-long.599) |  | 0 |  | Hoang Nguyen, Khyati Mahajan, Vikas Yadav, Julian Salazar, Philip S. Yu, Masoud Hashemi, Rishabh Maheshwary |  |
| 681 |  |  [SHADES: Towards a Multilingual Assessment of Stereotypes in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.600) |  | 0 |  | Margaret Mitchell, Giuseppe Attanasio, Ioana Baldini, Miruna Clinciu, Jordan Clive, Pieter Delobelle, Manan Dey, Sil Hamilton, Timm Dill, Jad Doughman, Ritam Dutt, Avijit Ghosh, Jessica Zosa Forde, Carolin Holtermann, LucieAimée Kaffee, Tanmay Laud, Anne Lauscher, Roberto L. LopezDavila, Maraim Masoud, Nikita Nangia, Anaelia Ovalle, Giada Pistilli, Dragomir Radev, Beatrice Savoldi, Vipul Raheja, Jeremy Qin, Esther Ploeger, Arjun Subramonian, Kaustubh D. Dhole, Kaiser Sun, Amirbek Djanibekov, Jonibek Mansurov, Kayo Yin, Emilio Villa Cueva, Sagnik Mukherjee, Jerry Huang, Xudong Shen, Jay Gala, Hamdan AlAli, Tair Djanibekov, Nurdaulet Mukhituly, Shangrui Nie, Shanya Sharma, Karolina Stanczak, Eliza Szczechla, Tiago Timponi Torrent, Deepak Tunuguntla, Marcelo Viridiano, Oskar Van Der Wal, Adina Yakefu, Aurélie Névéol, Mike Zhang, Sydney Zink, Zeerak Talat |  |
| 682 |  |  [Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion](https://doi.org/10.18653/v1/2025.naacl-long.601) |  | 0 |  | Jacob K. Christopher, Brian R. Bartoldson, Tal BenNun, Michael Cardei, Bhavya Kailkhura, Ferdinando Fioretto |  |
| 683 |  |  [Bayelemabaga: Creating Resources for Bambara NLP](https://doi.org/10.18653/v1/2025.naacl-long.602) |  | 0 |  | Allahsera Auguste Tapo, Kevin Assogba, Christopher M. Homan, M. Mustafa Rafique, Marcos Zampieri |  |
| 684 |  |  [Single Ground Truth Is Not Enough: Adding Flexibility to Aspect-Based Sentiment Analysis Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.603) |  | 0 |  | Soyoung Yang, Hojun Cho, Jiyoung Lee, Sohee Yoon, Edward Choi, Jaegul Choo, Won Ik Cho |  |
| 685 |  |  [DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.604) |  | 0 |  | Jianyu Liu, Hangyu Guo, Ranjie Duan, Xingyuan Bu, Yancheng He, Shilong Li, Hui Huang, Jiaheng Liu, Yucheng Wang, Chenchen Jing, Xingwei Qu, Xiao Zhang, Pei Wang, Yanan Wu, Jihao Gu, Yangguang Li, Jianke Zhu |  |
| 686 |  |  [In-Context Learning with Long-Context Models: An In-Depth Exploration](https://doi.org/10.18653/v1/2025.naacl-long.605) |  | 0 |  | Amanda Bertsch, Maor Ivgi, Emily Xiao, Uri Alon, Jonathan Berant, Matthew R. Gormley, Graham Neubig |  |
| 687 |  |  [Preference Consistency Matters: Enhancing Preference Learning in Language Models with Automated Self-Curation of Training Corpora](https://doi.org/10.18653/v1/2025.naacl-long.606) |  | 0 |  | JoonHo Lee, JuYoun Son, Juree Seok, Wooseok Jang, YeongDae Kwon |  |
| 688 |  |  [TurtleBench: A Visual Programming Benchmark in Turtle Geometry](https://doi.org/10.18653/v1/2025.naacl-long.607) |  | 0 |  | Sina Rismanchian, Yasaman Razeghi, Sameer Singh, Shayan Doroudi |  |
| 689 |  |  [Automatically Discovering How Misogyny is Framed on Social Media](https://doi.org/10.18653/v1/2025.naacl-long.608) |  | 0 |  | Rakshitha Rao Ailneni, Sanda M. Harabagiu |  |
| 690 |  |  [Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.609) |  | 0 |  | Mahnaz Koupaee, Jake W. Vincent, Saab Mansour, Igor Shalyminov, Han He, Hwanjun Song, Raphael Shu, Jianfeng He, Yi Nian, Amy Wingmei Wong, Kyu J. Han, Hang Su |  |
| 691 |  |  [ReIFE: Re-evaluating Instruction-Following Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.610) |  | 0 |  | Yixin Liu, Kejian Shi, Alexander R. Fabbri, Yilun Zhao, Peifeng Wang, ChienSheng Wu, Shafiq Joty, Arman Cohan |  |
| 692 |  |  [Language Models Predict Empathy Gaps Between Social In-groups and Out-groups](https://doi.org/10.18653/v1/2025.naacl-long.611) |  | 0 |  | Yu Hou, Hal Daumé III, Rachel Rudinger |  |
| 693 |  |  [HARP: Hesitation-Aware Reframing in Transformer Inference Pass](https://doi.org/10.18653/v1/2025.naacl-long.612) |  | 0 |  | Romain Storaï, Seungwon Hwang |  |
| 694 |  |  [JAWAHER: A Multidialectal Dataset of Arabic Proverbs for LLM Benchmarking](https://doi.org/10.18653/v1/2025.naacl-long.613) |  | 0 |  | Samar Mohamed Magdy, Sang Yun Kwon, Fakhraddin Alwajih, Safaa Taher Abdelfadil, Shady Shehata, Muhammad AbdulMageed |  |
| 695 |  |  [EmojiPrompt: Generative Prompt Obfuscation for Privacy-Preserving Communication with Cloud-based LLMs](https://doi.org/10.18653/v1/2025.naacl-long.614) |  | 0 |  | Sam Lin, Wenyue Hua, Zhenting Wang, Mingyu Jin, Lizhou Fan, Yongfeng Zhang |  |
| 696 |  |  [MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools](https://doi.org/10.18653/v1/2025.naacl-long.615) |  | 0 |  | Nishant Subramani, Jason Eisner, Justin Svegliato, Benjamin Van Durme, Yu Su, Sam Thomson |  |
| 697 |  |  [PAT: Parameter-Free Audio-Text Aligner to Boost Zero-Shot Audio Classification](https://doi.org/10.18653/v1/2025.naacl-long.616) |  | 0 |  | Ashish Seth, Ramaneswaran Selvakumar, Sonal Kumar, Sreyan Ghosh, Dinesh Manocha |  |
| 698 |  |  [Language Model Council: Democratically Benchmarking Foundation Models on Highly Subjective Tasks](https://doi.org/10.18653/v1/2025.naacl-long.617) |  | 0 |  | Justin Zhao, Flor Miriam Plaza del Arco, Amanda Cercas Curry |  |
| 699 |  |  [SCIURus: Shared Circuits for Interpretable Uncertainty Representations in Language Models](https://doi.org/10.18653/v1/2025.naacl-long.618) |  | 0 |  | Carter Teplica, Yixin Liu, Arman Cohan, Tim G. J. Rudner |  |
| 700 |  |  [ProSE: Diffusion Priors for Speech Enhancement](https://doi.org/10.18653/v1/2025.naacl-long.619) |  | 0 |  | Sonal Kumar, Sreyan Ghosh, Utkarsh Tyagi, Anton Jeran Ratnarajah, Chandra Kiran Reddy Evuru, Ramani Duraiswami, Dinesh Manocha |  |
| 701 |  |  [Mastering the Craft of Data Synthesis for CodeLLMs](https://doi.org/10.18653/v1/2025.naacl-long.620) |  | 0 |  | Meng Chen, Philip Arthur, Qianyu Feng, Cong Duy Vu Hoang, YuHeng Hong, Mahdi Kazemi Moghaddam, Omid Nezami, Duc Thien Nguyen, Gioacchino Tangari, Duy Vu, Thanh Vu, Mark Johnson, Krishnaram Kenthapadi, Don Dharmasiri, Long Duong, YuanFang Li |  |
| 702 |  |  [ParaICL: Towards Parallel In-Context Learning](https://doi.org/10.18653/v1/2025.naacl-long.621) |  | 0 |  | Xingxuan Li, XuanPhi Nguyen, Shafiq Joty, Lidong Bing |  |
| 703 |  |  [CausalEval: Towards Better Causal Reasoning in Language Models](https://doi.org/10.18653/v1/2025.naacl-long.622) |  | 0 |  | Longxuan Yu, Delin Chen, Siheng Xiong, Qingyang Wu, Dawei Li, Zhikai Chen, Xiaoze Liu, Liangming Pan |  |
| 704 |  |  [Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense](https://doi.org/10.18653/v1/2025.naacl-long.623) |  | 0 |  | Yang Ouyang, Hengrui Gu, Shuhang Lin, Wenyue Hua, Jie Peng, Bhavya Kailkhura, Meijun Gao, Tianlong Chen, Kaixiong Zhou |  |
| 705 |  |  [DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot Question Answering in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.624) |  | 0 |  | Suyoung Bae, YunSeok Choi, JeeHyong Lee |  |
| 706 |  |  [Reward-Guided Tree Search for Inference Time Alignment of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.625) |  | 0 |  | ChiaYu Hung, Navonil Majumder, Ambuj Mehrish, Soujanya Poria |  |
| 707 |  |  [Typographic Attacks in a Multi-Image Setting](https://doi.org/10.18653/v1/2025.naacl-long.626) |  | 0 |  | Xiaomeng Wang, Zhengyu Zhao, Martha A. Larson |  |
| 708 |  |  [Tonguescape: Exploring Language Models Understanding of Vowel Articulation](https://doi.org/10.18653/v1/2025.naacl-long.627) |  | 0 |  | Haruki Sakajo, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe |  |
| 709 |  |  [CoRAC: Integrating Selective API Document Retrieval with Question Semantic Intent for Code Question Answering](https://doi.org/10.18653/v1/2025.naacl-long.628) |  | 0 |  | YunSeok Choi, CheolWon Na, JeeHyong Lee |  |
| 710 |  |  [Pipeline Analysis for Developing Instruct LLMs in Low-Resource Languages: A Case Study on Basque](https://doi.org/10.18653/v1/2025.naacl-long.629) |  | 0 |  | Ander Corral, Ixak Sarasua, Xabier Saralegi |  |
| 711 |  |  [How to Make LLMs Forget: On Reversing In-Context Knowledge Edits](https://doi.org/10.18653/v1/2025.naacl-long.630) |  | 0 |  | Paul Youssef, Zhixue Zhao, Jörg Schlötterer, Christin Seifert |  |
| 712 |  |  [PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian](https://doi.org/10.18653/v1/2025.naacl-long.631) |  | 0 |  | Erfan Moosavi Monazzah, Vahid Rahimzadeh, Yadollah Yaghoobzadeh, Azadeh Shakery, Mohammad Taher Pilehvar |  |
| 713 |  |  [Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.632) |  | 0 |  | Soham Poddar, Paramita Koley, Janardan Misra, Niloy Ganguly, Saptarshi Ghosh |  |
| 714 |  |  [CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science Research Repositories](https://doi.org/10.18653/v1/2025.naacl-long.633) |  | 0 |  | Yijia Xiao, Runhui Wang, Luyang Kong, Davor Golac, Wei Wang |  |
| 715 |  |  [SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data](https://doi.org/10.18653/v1/2025.naacl-long.634) |  | 0 |  | Suyoung Bae, YunSeok Choi, Hyojun Kim, JeeHyong Lee |  |
| 716 |  |  [Rationale-Guided Retrieval Augmented Generation for Medical Question Answering](https://doi.org/10.18653/v1/2025.naacl-long.635) |  | 0 |  | Jiwoong Sohn, Yein Park, Chanwoong Yoon, Sihyeon Park, Hyeon Hwang, Mujeen Sung, Hyunjae Kim, Jaewoo Kang |  |
| 717 |  |  [Prototype Conditioned Generative Replay for Continual Learning in NLP](https://doi.org/10.18653/v1/2025.naacl-long.636) |  | 0 |  | Xi Chen, Min Zeng |  |
| 718 |  |  [KODIS: A Multicultural Dispute Resolution Dialogue Corpus](https://doi.org/10.18653/v1/2025.naacl-long.637) |  | 0 |  | James Hale, Sushrita Rakshit, Kushal Chawla, Jeanne M. Brett, Jonathan Gratch |  |
