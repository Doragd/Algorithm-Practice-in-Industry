# NAACL2025

## 会议论文列表

本会议共有 718 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Complete Chess Games Enable LLM Become A Chess Master](https://doi.org/10.18653/v1/2025.naacl-short.1) |  | 0 | Large language models (LLM) have shown remarkable abilities in text generation, question answering, language translation, reasoning and many other tasks. It continues to advance rapidly and is becoming increasingly influential in various fields, from technology and business to education and entertainment. Despite LLM’s success in multiple areas, its ability to play abstract games, such as chess, is underexplored. Chess-playing requires the language models to output legal and reasonable moves from textual inputs. Here, we propose the Large language model ChessLLM to play full chess games. We transform the game into a textual format with the best move represented in the Forsyth-Edwards... | Haolong Li, Kedi Chen, Shaohui Lin, Xintian Han, Yinqi Zhang |  |
| 2 |  |  [Predicting the Target Word of Game-playing Conversations using a Low-Rank Dialect Adapter for Decoder Models](https://doi.org/10.18653/v1/2025.naacl-short.2) |  | 0 | Dialect adapters that improve the performance of LLMs for NLU tasks on certain sociolects/dialects/national varieties (‘dialects’ for the sake of brevity) have been reported for encoder models. In this paper, we extend the idea of dialect adapters to decoder models in our architecture called LoRDD. Using MD-3, a publicly available dataset of word game-playing conversations between dialectal speakers, our task is Target Word Prediction (TWP) from a masked conversation. LoRDD combines task adapters and dialect adapters where the latter employ contrastive learning on pseudo-parallel conversations from MD-3. Our experiments on Indian English and Nigerian English conversations with two models... | Aditya Joshi, Dipankar Srirag, Jacob Eisenstein |  |
| 3 |  |  [ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with LLM-based Chatbots](https://doi.org/10.18653/v1/2025.naacl-short.3) |  | 0 | The rise of LLMs has deflected a growing portion of human-computer interactions towards LLM-based chatbots.The remarkable abilities of these models allow users to interact using long, diverse natural language text covering a wide range of topics and styles. Phrasing these messages is a time and effort consuming task, calling for an autocomplete solution to assist users. We present \*\*ChaI-TeA\*\*: \*\*Cha\*\*t \*\*I\*\*n\*\*te\*\*raction \*\*A\*\*utocomplete; An autocomplete evaluation framework for LLM-based chatbot interactions. The framework includes a formal definition of the task, curated datasets and suitable metrics. We use it to evaluate 11 models on this task, finding that while... | Alexander Libov, Guy Kushilevitz, Nachshon Cohen, Oren Kalinsky, Ram Yazdi, Shani Goren, Tomer Stav, Yaron Fairstein, Yuri Rapoport |  |
| 4 |  |  [Cross-Lingual Transfer Learning for Speech Translation](https://doi.org/10.18653/v1/2025.naacl-short.4) |  | 0 | There has been increasing interest in building multilingual foundation models for NLP and speech research. This paper examines how to expand the speech translation capability of these models with restricted data. Whisper, a speech foundation model with strong performance on speech recognition and English translation, is used as the example model. Using speech-to-speech retrieval to analyse the audio representations generated by the encoder, we show that utterances from different languages are mapped to a shared semantic space. This shared embedding space can then be leveraged for zero-shot cross-lingual transfer in speech translation. By fine-tuning the Whisper decoder with only... | Kate M. Knill, Mark J. F. Gales, Mengjie Qian, Rao Ma, Siyuan Tang, Yassir Fathullah |  |
| 5 |  |  [Reverse Question Answering: Can an LLM Write a Question so Hard (or Bad) that it Can't Answer?](https://doi.org/10.18653/v1/2025.naacl-short.5) |  | 0 | Question answering (QA)—giving correct answers to questions—is a popular task, but we test \*\*reverse question answering (RQA)\*\*: for an input answer, give a question with that answer. Past work tests QA and RQA separately, but we test them jointly, comparing their difficulty, aiding benchmark design, and checking reasoning consistency. We run 16 LLMs on QA and RQA with trivia questions/answers, revealing: 1) Versus RQA, LLMs are much less accurate in RQA for numerical answers, but slightly more accurate in RQA for textual answers; 2) LLMs often answer their own invalid questions from RQA accurately in QA, so RQA errors are not just from knowledge gaps; 3) RQA errors correlate with... | Abhilasha Ravichander, Feng Gu, Jordan Lee BoydGraber, Nishant Balepur, Rachel Rudinger, Shi Feng |  |
| 6 |  |  [Personalized Help for Optimizing Low-Skilled Users' Strategy](https://doi.org/10.18653/v1/2025.naacl-short.6) |  | 0 | AIs can beat humans in game environments; however, how helpful those agents are to human remains understudied. We augment Cicero, a natural language agent that demonstrates superhuman performance in Diplomacy, to generate both move and message advice based on player intentions. A dozen Diplomacy games with novice and experienced players, with varying advice settings, show that some of the generated advice is beneficial. It helps novices compete with experienced players and in some instances even surpass them. The mere presence of advice can be advantageous, even if players do not follow it. | Denis Peskoff, Feng Gu, Jonathan K. Kummerfeld, Jonathan May, Jordan Lee BoydGraber, Wichayaporn Wongkamjan |  |
| 7 |  |  [Local Prompt Optimization](https://doi.org/10.18653/v1/2025.naacl-short.7) |  | 0 | In recent years, the use of prompts to guide the output of Large Language Models have increased dramatically. However, even the best of experts struggle to choose the correct words to stitch up a prompt for the desired task. To solve this, LLM driven prompt optimization emerged as an important problem. Existing prompt optimization methods optimize a prompt globally, where in all the prompt tokens have to be optimized over a large vocabulary while solving a complex task. The large optimization space (tokens) leads to insufficient guidance for a better prompt. In this work, we introduce Local Prompt Optimization (LPO) that integrates with any general automatic prompt engineering method. We... | Vishal Chowdhary, Yash Jain |  |
| 8 |  |  [Cross-lingual Transfer of Reward Models in Multilingual Alignment](https://doi.org/10.18653/v1/2025.naacl-short.8) |  | 0 | Reinforcement learning with human feedback (RLHF) is shown to largely benefit from precise reward models (RMs). However, recent studies in reward modeling schemes are skewed towards English, limiting the applicability of RLHF in multilingual alignments. In this work, we investigate the cross-lingual transfer of RMs trained in diverse languages, primarily from English. Our experimental results demonstrate the strong cross-lingual transfer of English RMs, exceeding target language RMs by 3~4% average increase in Multilingual RewardBench. Furthermore, we analyze the cross-lingual transfer of RMs through the representation shifts. Finally, we perform multilingual alignment to exemplify how... | César Rodríguez, James Thorne, Jiwoo Hong, Noah Lee, Rodrigo MartínezCastaño |  |
| 9 |  |  [Inference-Time Selective Debiasing to Enhance Fairness in Text Classification Models](https://doi.org/10.18653/v1/2025.naacl-short.9) |  | 0 | We propose selective debiasing – an inference-time safety mechanism designed to enhance the overall model quality in terms of prediction performance and fairness, especially in scenarios where retraining the model is impractical. The method draws inspiration from selective classification, where at inference time, predictions with low quality, as indicated by their uncertainty scores, are discarded. In our approach, we identify the potentially biased model predictions and, instead of discarding them, we remove bias from these predictions using LEACE – a post-processing debiasing method. To select problematic predictions, we propose a bias quantification approach based on KL divergence,... | Artem Shelmanov, Gleb Kuzmin, Ivan V. Smirnov, Neemesh Yadav, Timothy Baldwin |  |
| 10 |  |  [Automatic Evaluation of Healthcare LLMs Beyond Question-Answering](https://doi.org/10.18653/v1/2025.naacl-short.10) |  | 0 | Current Large Language Models (LLMs) benchmarks are often based on open-ended or close-ended QA evaluations, avoiding the requirement of human labor. Close-ended measurements evaluate the factuality of responses but lack expressiveness. Open-ended capture the model’s capacity to produce discourse responses but are harder to assess for correctness. These two approaches are commonly used, either independently or together, though their relationship remains poorly understood. This work is focused on the healthcare domain, where both factuality and discourse matter greatly. It introduces a comprehensive, multi-axis suite for healthcare LLM evaluation, exploring correlations between open and... | Anna AriasDuart, Ashwin Kumar Gururajan, Daniel Hinjos, Dario GarciaGasulla, Enrique LopezCuena, Lucia UrcelayGanzabal, Marta GonzalezMallo, Pablo Agustin MartinTorres, Pablo BernabeuPerez, Sergio ÁlvarezNapagao |  |
| 11 |  |  [STRUX: An LLM for Decision-Making with Structured Explanations](https://doi.org/10.18653/v1/2025.naacl-short.11) |  | 0 | Countless decisions shape our lives, and it is crucial to understand the how and why behind them. In this paper, we introduce a new LLM decision-making framework called STRUX, which enhances LLM decision-making by providing structured explanations. These include favorable and adverse facts related to the decision, along with their respective strengths. STRUX begins by distilling lengthy information into a concise table of key facts. It then employs a series of self-reflection steps to determine which of these facts are pivotal, categorizing them as either favorable or adverse in relation to a specific decision. Lastly, we fine-tune an LLM to identify and prioritize these key facts to... | Fei Liu, Hassan Foroosh, Wei Jin, Yebowen Hu, Yiming Lu |  |
| 12 |  |  [Improving Vietnamese-English Cross-Lingual Retrieval for Legal and General Domains](https://doi.org/10.18653/v1/2025.naacl-short.12) |  | 0 | Document retrieval plays a crucial role in numerous question-answering systems, yet research has concentrated on the general knowledge domain and resource-rich languages like English. In contrast, it remains largely underexplored in low-resource languages and cross-lingual scenarios within specialized domain knowledge such as legal. We present a novel dataset designed for cross-lingual retrieval between Vietnamese and English, which not only covers the general domain but also extends to the legal field. Additionally, we propose auxiliary loss function and symmetrical training strategy that significantly enhance the performance of state-of-the-art models on these retrieval tasks. Our... | Dai An Nguyen, Linh Ngo Van, Nam Le Hai, Nguyen Doan Hieu, Sang Dinh, Thien Huu Nguyen, Toan Ngoc Nguyen |  |
| 13 |  |  [Computational Discovery of Chiasmus in Ancient Religious Text](https://doi.org/10.18653/v1/2025.naacl-short.13) |  | 0 | Chiasmus, a debated literary device in Biblical texts, has captivated mystics while sparking ongoing scholarly discussion. In this paper, we introduce the first computational approach to systematically detect chiasmus within Biblical passages. Our method leverages neural embeddings to capture lexical and semantic patterns associated with chiasmus, applied at multiple levels of textual granularity (half-verses, verses). We also involve expert annotators to review a subset of the detected patterns. Despite its computational efficiency, our method achieves robust results, with high inter-annotator agreement and system accuracy of 0.80 at the verse level and 0.60 at the half-verse level. We... | Hale Sirin, Hope McGovern, Tom Lippincott |  |
| 14 |  |  [Characterizing the Effects of Translation on Intertextuality using Multilingual Embedding Spaces](https://doi.org/10.18653/v1/2025.naacl-short.14) |  | 0 | Rhetorical devices are difficult to translate, but they are crucial to the translation of literary documents. We investigate the use of multilingual embedding spaces to characterize the preservation of intertextuality, one common rhetorical device, across human and machine translation. To do so, we use Biblical texts, which are both full of intertextual references and are highly translated works. We provide a metric to characterize intertextuality at the corpus level and provide a quantitative analysis of the preservation of this rhetorical device across extant human translations and machine-generated counterparts. We go on to provide qualitative analysis of cases wherein human... | Hale Sirin, Hope McGovern, Tom Lippincott |  |
| 15 |  |  [LLM2: Let Large Language Models Harness System 2 Reasoning](https://doi.org/10.18653/v1/2025.naacl-short.15) |  | 0 | Large language models (LLMs) have exhibited impressive capabilities across a myriad of tasks, yet they occasionally yield undesirable outputs. We posit that these limitations are rooted in the foundational autoregressive architecture of LLMs, which inherently lacks mechanisms for differentiating between desirable and undesirable results. Drawing inspiration from the dual-process theory of human cognition, we introduce LLM2, a novel framework that combines an LLM (System 1) with a process-based verifier (System 2). Within LLM2, the LLM is responsible for generating plausible candidates, while the verifier provides timely process-based feedback to distinguish desirable and undesirable... | Bo Shui, Cheng Yang, Chufan Shi, Siheng Li, Wai Lam, Yujiu Yang |  |
| 16 |  |  [Context-Efficient Retrieval with Factual Decomposition](https://doi.org/10.18653/v1/2025.naacl-short.16) |  | 0 | There has recently been considerable interest in incorporating information retrieval into large language models (LLMs). Retrieval from a dynamically expanding external corpus of text allows a model to incorporate current events and can be viewed as a form of episodic memory. Here we demonstrate that pre-processing the external corpus into semi-structured “atomic facts” makes retrieval more efficient. More specifically, we demonstrate that our particular form of atomic facts improves performance on various question answering tasks when the amount of retrieved text is limited. Limiting the amount of retrieval reduces the size of the context and improves inference efficiency. | David McAllester, David Yunis, Jiawei Zhou, Yanhong Li |  |
| 17 |  |  [Sports and Women's Sports: Gender Bias in Text Generation with Olympic Data](https://doi.org/10.18653/v1/2025.naacl-short.17) |  | 0 | Large Language Models (LLMs) have been shown to be biased in prior work, as they generate text that is in line with stereotypical views of the world or that is not representative of the viewpoints and values of historically marginalized demographic groups. In this work, we propose using data from parallel men’s and women’s events at the Olympic Games to investigate different forms of gender bias in language models. We define three metrics to measure bias, and find that models are consistently biased against women when the gender is ambiguous in the prompt. In this case, the model frequently retrieves only the results of the men’s event with or without acknowledging them as such, revealing... | Laura Biester |  |
| 18 |  |  [Alligators All Around: Mitigating Lexical Confusion in Low-resource Machine Translation](https://doi.org/10.18653/v1/2025.naacl-short.18) |  | 0 | Current machine translation (MT) systems for low-resource languages have a particular failure mode: When translating words in a given domain, they tend to confuse words within that domain. So, for example, “lion” might be translated as “alligator”, and “orange” might be rendered as “purple.” We propose a recall-based metric for measuring this problem and show that the problem exists in 122 low-resource languages. We then show that this problem can be mitigated by using a large language model (LLM) to post-edit the MT output, specifically by including the entire GATITOS lexicon for the relevant language as a very long context prompt. We show gains in average ChrF score over the set of 122... | Colin Cherry, Elizabeth Nielsen, Isaac Caswell, Jiaming Luo |  |
| 19 |  |  [PROM: Pivoted and Regulated Optimization for Multilingual Instruction Learning](https://doi.org/10.18653/v1/2025.naacl-short.19) |  | 0 | Large language models (LLMs) have become standard for natural language generation tasks, with instruction-tuning enhancing their capabilities. However, the lack of instruction-tuning datasets in languages other than English limits their application to diverse languages. To address this, researchers have adapted English-centric LLMs to other languages by appending English tuning data with its translated pair, from which we observe negative interference between the two. To resolve this, our contribution is identifying English as an internal pivot language, based on which we disentangle the roles of English and target language data in training. Specifically, we first design two roles as... | Changmin Lee, Hojin Lee, Jaeseong Lee, Seungwon Hwang, Yunju Bak |  |
| 20 |  |  [Concept-Reversed Winograd Schema Challenge: Evaluating and Improving Robust Reasoning in Large Language Models via Abstraction](https://doi.org/10.18653/v1/2025.naacl-short.20) |  | 0 | While Large Language Models (LLMs) have showcased remarkable proficiency in reasoning, there is still a concern about hallucinations and unreliable reasoning issues due to semantic associations and superficial logical chains. To evaluate the extent to which LLMs perform robust reasoning instead of relying on superficial logical chains, we propose a new evaluation dataset, the Concept-Reversed Winograd Schema Challenge (CR-WSC), based on the famous Winograd Schema Challenge (WSC) dataset. By simply reversing the concepts to those that are more associated with the wrong answer, we find that the performance of LLMs drops significantly despite the rationale of reasoning remaining the same.... | Kaiqiao Han, Mark Steedman, Tianqing Fang, Yangqiu Song, Zhaowei Wang |  |
| 21 |  |  [Defense against Prompt Injection Attacks via Mixture of Encodings](https://doi.org/10.18653/v1/2025.naacl-short.21) |  | 0 | Large Language Models (LLMs) have emerged as a dominant approach for a wide range of NLP tasks, with their access to external information further enhancing their capabilities. However, this introduces new vulnerabilities, known as prompt injection attacks, where external content embeds malicious instructions that manipulate the LLM’s output. Recently, the Base64 defense has been recognized as one of the most effective methods for reducing success rate of prompt injection attacks. Despite its efficacy, this method can degrade LLM performance on certain NLP tasks. To address this challenge, we propose a novel defense mechanism: mixture of encodings, which utilizes multiple character... | David Sullivan, Kyle Jackson, Mei Chen, Pengtao Xie, Ruiyi Zhang |  |
| 22 |  |  [Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers](https://doi.org/10.18653/v1/2025.naacl-short.22) |  | 0 | AI Safety Moderation (ASM) classifiers are designed to moderate content on social media platforms and to serve as guardrails that prevent Large Language Models (LLMs) from being fine-tuned on unsafe inputs. Owing to their potential for disparate impact, it is crucial to ensure that these classifiers: (1) do not unfairly classify content belonging to users from minority groups as unsafe compared to those from majority groups and (2) that their behavior remains robust and consistent across similar inputs. In this work, we thus examine the fairness and robustness of four widely-used, closed-source ASM classifiers: OpenAI Moderation API, Perspective API, Google Cloud Natural Language (GCNL)... | Akshit Achara, Anshuman Chhabra |  |
| 23 |  |  [CoRAG: Collaborative Retrieval-Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-short.23) |  | 0 | Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive tasks, especially under few-shot learning constraints. We introduce CoRAG, a framework extending RAG to collaborative settings, where clients jointly train a shared model using a collaborative passage store. To evaluate CoRAG, we introduce CRAB, a benchmark for collaborative homogeneous open-domain question answering. Our experiments demonstrate that CoRAG consistently outperforms both parametric collaborative learning methods and locally trained RAG models in low-resource scenarios. Further analysis reveals the critical importance of relevant passages within the shared store, the surprising benefits of incorporating... | Aashiq Muhamed, Mona T. Diab, Virginia Smith |  |
| 24 |  |  [Is It Navajo? Accurate Language Detection for Endangered Athabaskan Languages](https://doi.org/10.18653/v1/2025.naacl-short.24) |  | 0 | Endangered languages, such as Navajo—the most widely spoken Native American language—are significantly underrepresented in contemporary language technologies, exacerbating the challenges of their preservation and revitalization. This study evaluates Google’s Language Identification (LangID) tool, which does not currently support any Native American languages. To address this, we introduce a random forest classifier trained on Navajo and twenty erroneously suggested languages by LangID. Despite its simplicity, the classifier achieves near-perfect accuracy (97-100%). Additionally, the model demonstrates robustness across other Athabaskan languages—a family of Native American languages spoken... | Chunhui Zhang, Ivory Yang, Soroush Vosoughi, Weicheng Ma |  |
| 25 |  |  [Don't Touch My Diacritics](https://doi.org/10.18653/v1/2025.naacl-short.25) |  | 0 | The common practice of preprocessing text before feeding it into NLP models introduces many decision points which have unintended consequences on model performance. In this opinion piece, we focus on the handling of diacritics in texts originating in many languages and scripts. We demonstrate, through several case studies, the adverse effects of inconsistent encoding of diacritized characters and of removing diacritics altogether. We call on the community to adopt simple but necessary steps across all models and toolkits in order to improve handling of diacritized text and, by extension, increase equity in multilingual NLP. | Kyle Gorman, Yuval Pinter |  |
| 26 |  |  [Pretrained Image-Text Models are Secretly Video Captioners](https://doi.org/10.18653/v1/2025.naacl-short.26) |  | 0 | Developing video captioning models is computationally expensive. The dynamic nature of video also complicates the design of multimodal models that can effectively caption these sequences. However, we find that by using minimal computational resources and without complex modifications to address video dynamics, an image-based model can be repurposed to outperform several specialised video captioning systems. Our adapted model demonstrates top-tier performance on major benchmarks, ranking 2nd on MSR-VTT and MSVD, and 3rd on VATEX. We transform it into a competitive video captioner by post-training a typical image captioning model BLIP-2 with only 6,000 video-text pairs and simply... | Chunhui Zhang, Soroush Vosoughi, Yiren Jian, Zhongyu Ouyang |  |
| 27 |  |  [Reverse Modeling in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-short.27) |  | 0 | Humans are accustomed to reading and writing in a forward manner, and this natural bias extends to text understanding in auto-regressive large language models (LLMs). This paper investigates whether LLMs, like humans, struggle with reverse modeling, specifically with reversed text inputs. We found that publicly available pre-trained LLMs cannot understand such inputs. However, LLMs trained from scratch with both forward and reverse texts can understand them equally well during inference across multiple languages.Our case study shows that different-content texts result in different losses if input (to LLMs) in different directions—some get lower losses for forward while some for reverse.... | Cunxiao Du, Hao Zhang, Jiawei Wu, Minghui Qiu, Qianru Sun, Sicheng Yu, Yanying Zhou, Yuanchen Xu |  |
| 28 |  |  [Preserving Multilingual Quality While Tuning Query Encoder on English Only](https://doi.org/10.18653/v1/2025.naacl-short.28) |  | 0 | A query encoder of a dual passage retrieval system can be tuned for specific types of queries or domains, while the precomputed and stored documents representations are kept intact. Switching from one query encoder to another when needed is easily feasible, unlike overhauling the embeddings of a whole knowledge base. In this work we raise a question: Can the generic, original qualities of the encoder be preserved or at least left not too degraded when it is tuned on a narrow domain? We conducted experiments on a high quality multilingual embedding model: Tuning it on a single English-only dataset, we observe that the tuning not only preserves the multilingual qualities, but even improves... | John Bohannon, Oleg Vasilyev, Randy Sawaya |  |
| 29 |  |  [Using Contextually Aligned Online Reviews to Measure LLMs' Performance Disparities Across Language Varieties](https://doi.org/10.18653/v1/2025.naacl-short.29) |  | 0 | A language can have different varieties. These varieties can affect the performance of natural language processing (NLP) models, including large language models (LLMs), which are often trained on data from widely spoken varieties. This paper introduces a novel and cost-effective approach to benchmark model performance across language varieties. We argue that international online review platforms,such as Booking.com, can serve as effective data sources for constructing datasets that capture comments in different language varieties from similar real-world scenarios, like reviews for the same hotel with the same rating using the same language (e.g., Mandarin Chinese) but different language... | ChiehYang Huang, HenHsen Huang, Ho Yin Sam Ng, TingHao Kenneth Huang, TsungChi Li, Zixin Tang |  |
| 30 |  |  [Towards Federated Low-Rank Adaptation of Language Models with Rank Heterogeneity](https://doi.org/10.18653/v1/2025.naacl-short.30) |  | 0 | Low-rank adaptation (LoRA) offers an efficient alternative to full-weight adaptation in federated fine-tuning of language models, significantly reducing computational costs. By adjusting ranks for each client, federated LoRA enables flexible resource allocation. However, we observe that heterogeneous ranks among clients lead to unstable performance. Our analysis attributes this instability to the conventional zero-padding aggregation strategy, which dilutes information from high-rank clients during model aggregation. To address this issue, we propose a replication-based padding strategy that better retains valuable information from clients with high-quality data. Empirically, this approach... | Jaeho Lee, Yuji Byun |  |
| 31 |  |  [Related Knowledge Perturbation Matters: Rethinking Multiple Pieces of Knowledge Editing in Same-Subject](https://doi.org/10.18653/v1/2025.naacl-short.31) |  | 0 |  | Huawei Shen, Jie Zhang, Shaoling Jing, Wenbin Duan, Xueqi Cheng, Yinghan Shen, Zenghao Duan, Zhiyi Yin |  |
| 32 |  |  [STEP: Staged Parameter-Efficient Pre-training for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-short.32) |  | 0 | Pre-training large language models (LLMs) faces significant memory challenges due to the large size of model weights. We introduce STaged parameter-Efficient Pre-training (STEP), which integrates parameter-efficient tuning techniques with model growth. We conduct experiments on pre-training LLMs of various sizes and demonstrate that STEP achieves up to a 53.9% reduction in maximum memory requirements compared to vanilla pre-training while maintaining equivalent performance. Furthermore, we show that the model by STEP performs comparably to vanilla pre-trained models on downstream tasks after instruction tuning. | Jun Suzuki, Kazuki Yano, Takumi Ito |  |
| 33 |  |  [Language Models Encode Numbers Using Digit Representations in Base 10](https://doi.org/10.18653/v1/2025.naacl-short.33) |  | 0 | Large language models (LLMs) frequently make errors when handling even simple numerical problems, such as comparing two small numbers. A natural hypothesis is that these errors stem from how LLMs represent numbers, and specifically, whether their representations of numbers capture their numeric values. We tackle this question from the observation that LLM errors on numerical tasks are often distributed across the digits of the answer rather than normally around its numeric value. Through a series of probing experiments and causal interventions, we show that LLMs internally represent numbers with individual circular representations per-digit in base 10.This digit-wise representation, as... | Amit A. Levy, Mor Geva |  |
| 34 |  |  [A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference](https://doi.org/10.18653/v1/2025.naacl-short.34) |  | 0 | Recently, sharing key-value (KV) cache across layers has been found effective in efficient inference of large language models (LLMs). To systematically investigate different techniques of cross-layer KV sharing, we propose a unified framework that covers several recent methods and their novel variants. We conduct comprehensive experiments on all the configurations of the framework, evaluating their generation throughput and performance in language modeling and downstream tasks. We find that when reducing the size of the KV cache by 2×, most configurations can achieve higher throughput than standard transformers while maintaining competitive performance.When further reducing the size of the... | Haoyi Wu, Kewei Tu, You Wu |  |
| 35 |  |  [AMPS: ASR with Multimodal Paraphrase Supervision](https://doi.org/10.18653/v1/2025.naacl-short.35) |  | 0 | Spontaneous or conversational multilingual speech presents many challenges for state-of-the-art automatic speech recognition (ASR) systems. In this work, we present a new technique AMPS, that augments a multilingual multimodal ASR system with paraphrase-based supervision for improved conversational ASR in multiple languages, including Hindi, Marathi, Malayalam, Kannada, and Nyanja. We use paraphrases of the reference transcriptions as additional supervision while training the multimodal ASR model and selectively invoke this paraphrase objective for utterances with poor ASR performance. Using AMPS with a state-of-the-art multimodal model SeamlessM4T, we obtain significant relative... | Abhishek Gupta, Amruta Parulekar, Preethi Jyothi, Sameep Chattopadhyay |  |
| 36 |  |  [Taxi1500: A Dataset for Multilingual Text Classification in 1500 Languages](https://doi.org/10.18653/v1/2025.naacl-short.36) |  | 0 | While broad-coverage multilingual natural language processing tools have been developed, a significant portion of the world’s over 7000 languages are still neglected. One reason is the lack of evaluation datasets that cover a diverse range of languages, particularly those that are low-resource or endangered. To address this gap, we present a large-scale text classification dataset encompassing 1504 languages many of which have otherwise limited or no annotated data. This dataset is constructed using parallel translations of the Bible. We develop relevant topics, annotate the English data through crowdsourcing and project these annotations onto other languages via aligned verses. We... | Ayyoob Imani, Chunlan Ma, Ehsaneddin Asgari, Haotian Ye, Hinrich Schütze, Renhao Pei |  |
| 37 |  |  [GameTox: A Comprehensive Dataset and Analysis for Enhanced Toxicity Detection in Online Gaming Communities](https://doi.org/10.18653/v1/2025.naacl-short.37) |  | 0 | The prevalence of toxic behavior in online gaming communities necessitates robust detection methods to ensure user safety. We introduce GameTox, a novel dataset comprising 53K game chat utterances annotated for toxicity detection through intent classification and slot filling. This dataset captures the complex relationship between user intent and specific linguistic features that contribute to toxic interactions. We extensively analyze the dataset to uncover key insights into the nature of toxic speech in gaming environments. Furthermore, we establish baseline performance metrics using state-of-the-art natural language processing and large language models, demonstrating the dataset’s... | Qi Zhang, Shuvam Shiwakoti, Siddhant Bikram Shah, Surendrabikram Thapa, Usman Naseem |  |
| 38 |  |  [FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern LLMs](https://doi.org/10.18653/v1/2025.naacl-short.38) |  | 0 | Summarization is one of the most common tasks performed by large language models (LLMs), especially in applications like Retrieval-Augmented Generation (RAG). However, existing evaluations of hallucinations in LLM-generated summaries, and evaluations of hallucination detection models both suffer from a lack of diversity and recency in the LLM and LLM families considered. This paper introduces FaithBench, a summarization hallucination benchmark comprising challenging hallucinations made by 10 modern LLMs from 8 different families, with ground truth annotations by human experts. “Challenging” here means summaries on which popular, state-of-the-art hallucination detection models, including... | Amin Ahmad, Chenyu Xu, Erana Wan, Forrest Sheng Bao, Ge Luo, Manveer Singh Tamber, Matthew Gonzales, Miaoran Li, Mike Qi, Ofer Mendelevitch, Renyi Qu, Ruixuan Tu, Suleman Kazi, Vivek Sourabh, Weisi Fan, Yujia Tang |  |
| 39 |  |  [Debate-Feedback: A Multi-Agent Framework for Efficient Legal Judgment Prediction](https://doi.org/10.18653/v1/2025.naacl-short.39) |  | 0 | The use of AI in legal analysis and prediction (LegalAI) has gained attention, with past research focusing on retrieval-based methods and fine-tuning large models. However, these approaches often require large datasets and underutilize the capabilities of modern large language models (LLMs). In this paper, inspired by the debate phase of real courtroom trials, we propose a novel legal judgment prediction model based on the Debate-Feedback architecture, which integrates LLM multi-agent debate and reliability evaluation models. Unlike traditional methods, our model achieves significant improvements in efficiency by minimizing the need for large historical datasets, thus offering a... | Haotian Shangguan, Mao Mao, Shuo Li, Xi Chen |  |
| 40 |  |  [Great Memory, Shallow Reasoning: Limits of kNN-LMs](https://doi.org/10.18653/v1/2025.naacl-short.40) |  | 0 | K-nearest neighbor language models (kNN-LMs), which integrate retrieval with next-word prediction, have demonstrated strong performance in language modeling as well as some downstream NLP benchmarks. These results have led researchers to argue that models trained on poor quality or outdated data could perform well by employing a kNN extension that has access to a higher-quality datastore. In this work, we ask whether this improved ability to recall information really translates into downstream abilities. We extensively evaluate kNN-LMs on a diverse set of tasks, ranging from sentiment classification and commonsense reasoning to multi-hop reasoning. Results show that kNN-LMs excel at... | Alexander M. Rush, Shangyi Geng, Wenting Zhao |  |
| 41 |  |  [Repetition Neurons: How Do Language Models Produce Repetitions?](https://doi.org/10.18653/v1/2025.naacl-short.41) |  | 0 | This paper introduces repetition neurons, which can be regarded as “skill neurons” responsible for the repetition problem in text generation tasks. These neurons are progressively activated more strongly as repetition continues, indicating that they perceive repetition as a task to copy the previous context repeatedly, similar to in-context learning. We identify these repetition neurons by comparing activation values before and after the onset of repetition in texts generated by recent pre-trained language models. We analyze the repetition neurons in three English and one Japanese pre-trained language models and observe similar patterns across them. | Kentaro Inui, Tatsuya Hiraoka |  |
| 42 |  |  [STAR: Spectral Truncation and Rescale for Model Merging](https://doi.org/10.18653/v1/2025.naacl-short.42) |  | 0 | Model merging is an efficient way of obtaining a multi-task model from several pretrained models without further fine-tuning, and it has gained attention in various domains, including natural language processing (NLP). Despite the efficiency, a key challenge in model merging is the seemingly inevitable decrease in task performance as the number of models increases. In this paper, we propose \*\*S\*\*pectral \*\*T\*\*runcation \*\*A\*\*nd \*\*R\*\*escale (STAR) that aims at mitigating “merging conflicts” by truncating small components in the respective spectral spaces, which is followed by an automatic parameter rescaling scheme to retain the nuclear norm of the original matrix. STAR... | ChingYun Ko, IHsin Chung, MiYen Yeh, PinYu Chen, Tejaswini Pedapati, YuAng Lee |  |
| 43 |  |  [Task-driven Layerwise Additive Activation Intervention](https://doi.org/10.18653/v1/2025.naacl-short.43) |  | 0 | Modern language models (LMs) have significantly advanced generative modeling in natural language processing (NLP). Despite their success, LMs often struggle with adaptation to new contexts in real-time applications. A promising approach to task adaptation is activation intervention, which steers the LMs’ generation process by identifying and manipulating the activations. However, existing interventions rely heavily on heuristic rules or require many prompt inputs to determine effective interventions. In this paper, we propose a layer-wise additive activation intervention framework that optimizes the intervention process, thereby enhancing sample efficiency. We evaluate our framework on... | Bao Nguyen, Binh Nguyen, Hieu Trung Nguyen, Viet Anh Nguyen |  |
| 44 |  |  [Scaling Multi-Document Event Summarization: Evaluating Compression vs. Full-Text Approaches](https://doi.org/10.18653/v1/2025.naacl-short.44) |  | 0 | Automatically summarizing large text collections is a valuable tool for document research, with applications in journalism, academic research, legal work, and many other fields. In this work, we contrast two classes of systems for large-scale multi-document summarization (MDS): compression and full-text. Compression-based methods use a multi-stage pipeline and often lead to lossy summaries. Full-text methods promise a lossless summary by relying on recent advances in long-context reasoning. To understand their utility on large-scale MDS, we evaluated them on three datasets, each containing approximately one hundred documents per summary. Our experiments cover a diverse set of long-context... | Adithya Pratapa, Teruko Mitamura |  |
| 45 |  |  [Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models](https://doi.org/10.18653/v1/2025.naacl-short.45) |  | 0 | Large Vision Language Models (LVLMs) often suffer from object hallucination, which undermines their reliability. Surprisingly, we find that simple object-based visual prompting—overlaying visual cues (e.g., bounding box, circle) on images—can significantly mitigate such hallucination; however, different visual prompts (VPs) vary in effectiveness. To address this, we propose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify optimal VPs that enhance LVLM responses without needing access to model internals. Our approach employs a pool of candidate VPs and trains a router model to dynamically select the most effective VP for a given input image. This black-box approach is... | Haibo Ding, Kang Zhou, Lin Lee Cheong, Sangmin Woo, Sheng Guan, Shuai Wang, Yun Zhou |  |
| 46 |  |  [A Layered Debating Multi-Agent System for Similar Disease Diagnosis](https://doi.org/10.18653/v1/2025.naacl-short.46) |  | 0 | Distinguishing between extremely similar diseases is a critical and challenging aspect of clinical decision-making. Traditional classification, contrastive learning, and Large Language Models (LLMs) based methods fail to detect the subtle clues necessary for differentiation. This task demands complex reasoning and a variety of tools to identify minor differences and make informed decisions. This paper probes a novel framework that leverages LLMs and a multi-agent system to achieve accurate disease diagnosis through a process of repeated debate and reassessment. The approach aims to identify subtle differences between similar disease candidates. We structure patient information and... | Huimin Wang, Xian Wu, Yefeng Zheng, Yutian Zhao |  |
| 47 |  |  [The Geometry of Numerical Reasoning: Language Models Compare Numeric Properties in Linear Subspaces](https://doi.org/10.18653/v1/2025.naacl-short.47) |  | 0 | This paper investigates whether large language models (LLMs) utilize numerical attributes encoded in a low-dimensional subspace of theembedding space when answering questions involving numeric comparisons, e.g., Was Cristiano born before Messi? We first identified,using partial least squares regression, these subspaces, which effectively encode the numerical attributes associated with the entities in comparison prompts. Further, we demonstrate causality, by intervening in these subspaces to manipulate hidden states, thereby altering the LLM’s comparison outcomes. Experiments conducted on three different LLMs showed that our results hold across different numerical attributes, indicating... | Ahmed Oumar ElShangiti, Benjamin Heinzerling, Hilal AlQuabeh, Kentaro Inui, Tatsuya Hiraoka |  |
| 48 |  |  [AlignFreeze: Navigating the Impact of Realignment on the Layers of Multilingual Models Across Diverse Languages](https://doi.org/10.18653/v1/2025.naacl-short.48) |  | 0 | Realignment techniques are often employed to enhance cross-lingual transfer in multilingual language models, still, they can sometimes degrade performance in languages that differ significantly from the fine-tuned source language. This paper introduces AlignFreeze, a method that freezes either the layers’ lower half or upper half during realignment. Through controlled experiments on 4 tasks, 3 models, and in 35 languages, we find that realignment affects all the layers but can be the most detrimental to the lower ones. Freezing the lower layers can prevent performance degradation. Particularly, AlignFreeze improves Part-of-Speech (PoS) tagging performances in languages where full... | David Guzmán, EnShiun Annie Lee, Félix Gaschi, Kelly Chutong Li, Riddhi More, Steve Bakos |  |
| 49 |  |  [FLIQA-AD: a Fusion Model with Large Language Model for Better Diagnose and MMSE Prediction of Alzheimer's Disease](https://doi.org/10.18653/v1/2025.naacl-short.49) |  | 0 | Tracking a patient’s cognitive status early in the onset of the disease provides an opportunity to diagnose and intervene in Alzheimer’s disease (AD). However, relying solely on magnetic resonance imaging (MRI) images with traditional classification and regression models may not fully extract finer-grained information. This study proposes a multi-task Fusion Language Image Question Answering model (FLIQA-AD) to perform AD identification and Mini Mental State Examination (MMSE) prediction. Specifically, a 3D Adapter is introduced in Vision Transformer (ViT) model for image feature extraction. The patient electronic health records (EHR) information and questions related to the disease work... | Junhao Chen, Ling Wang, Xiangzhu Zeng, Yan Liu, Zhiyuan Ding |  |
| 50 |  |  [Transform Retrieval for Textual Entailment in RAG](https://doi.org/10.18653/v1/2025.naacl-short.50) |  | 0 | In this paper, we introduce Transform Retrieval, a novel approach aimed at improving Textual Entailment Retrieval within the framework of Retrieval-Augmented Generation (RAG). While RAG has shown promise in enhancing Large Language Models by retrieving relevant documents to extract specific knowledge or mitigate hallucination, current retrieval methods often prioritize relevance without ensuring the retrieved documents semantically support answering the queries. Transform Retrieval addresses this gap by transforming query embeddings to better align with semantic entailment without re-encoding the document corpus. We achieve this by using a transform model and employing a contrastive... | Quan Guo, Xin Liang |  |
| 51 |  |  [How do Multimodal Foundation Models Encode Text and Speech? An Analysis of Cross-Lingual and Cross-Modal Representations](https://doi.org/10.18653/v1/2025.naacl-short.51) |  | 0 | Multimodal foundation models aim to create a unified representation space that abstracts away from surface features like language syntax or modality differences. To investigate this, we study the internal representations of three recent models, analyzing the model activations from semantically equivalent sentences across languages in the text and speech modalities. Our findings reveal that: 1) Cross-modal representations converge over model layers, except in the initial layers specialized at text and speech processing. 2) Length adaptation is crucial for reducing the cross-modal gap between text and speech, although current approaches’ effectiveness is primarily limited to high-resource... | Danni Liu, Hyunji Lee, Jan Niehues, Supriti Sinhamahapatra |  |
| 52 |  |  [Explore the Reasoning Capability of LLMs in the Chess Testbed](https://doi.org/10.18653/v1/2025.naacl-short.52) |  | 0 | Reasoning is a central capability of human intelligence. In recent years, with the advent of large-scale datasets, pretrained large language models have emerged with new capabilities, including reasoning. However, these models still struggle with long-term, complex reasoning tasks, such as playing chess. Based on the observation that expert chess players employ a dual approach combining long-term strategic play with short-term tactical play along with language explanation, we propose improving the reasoning capability of large language models in chess by integrating annotated strategy and tactic. Specifically, we collect a dataset named MATE, which consists of 1 million chess positions... | Haokun Liu, Lei Ji, Renxi Wang, Shu Wang, Wenxiao Zhao, Yifan Hou, Ying Nian Wu |  |
| 53 |  |  [Auto-Cypher: Improving LLMs on Cypher generation via LLM-supervised generation-verification framework](https://doi.org/10.18653/v1/2025.naacl-short.53) |  | 0 | Graph databases like Neo4j are gaining popularity for handling complex, interconnected data, over traditional relational databases in modeling and querying relationships. While translating natural language into SQL queries is well-researched, generating Cypher queries for Neo4j remains relatively underexplored. In this work, we present an automated, LLM Supervised, pipeline to generate high quality synthetic data for Text2Cypher. Our Cypher data generation pipeline introduces LLM-As-Database-Filler, a novel strategy for ensuring Cypher query correctness, thus resulting in high quality generations. Using our pipeline, we generate high quality Text2Cypher data - SynthCypher containing 29.8k... | Aman Tiwari, Masoud Hashemi, Sathwik Tejaswi Madhusudhan, Shiva Krishna Reddy Malay, Vikas Yadav |  |
| 54 |  |  [Leveraging Moment Injection for Enhanced Semi-supervised Natural Language Inference with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-short.54) |  | 0 | Natural Language Inference (NLI) is crucial for evaluating models’ Natural Language Understanding (NLU) and reasoning abilities. The development of NLI, in part, has been driven by the creation of large datasets, which require significant human effort. This has spurred interest in semi-supervised learning (SSL) that leverages both labeled and unlabeled data. However, the absence of hypotheses and class labels in NLI tasks complicates SSL. Prior work has used class-specific fine-tuned large language models (LLMs) to generate hypotheses and assign pseudo-labels but discarded many LLM-constructed samples during training to ensure the quality. In contrast, we propose to leverage all... | Seo Yeon Park |  |
| 55 |  |  [A Fair Comparison without Translationese: English vs. Target-language Instructions for Multilingual LLMs](https://doi.org/10.18653/v1/2025.naacl-short.55) |  | 0 | Most large language models are multilingual instruction executors. Prior studies suggested that English instructions are more effective than target-language instructions even for non-English tasks; however, these studies often use datasets and instructions translated from English, which introduce biases known as translationese, hindering an unbiased comparison. To address this issue, we conduct a fair comparison between English and target-language instructions by eliminating translationese effects. Contrary to previous studies, our experiments across several tasks reveal that the advantage of adopting English instructions is not overwhelming. Additionally, we report on the features of... | Hwichan Kim, Mamoru Komachi, Taisei Enomoto, Zhousi Chen |  |
| 56 |  |  [Evaluating Multimodal Generative AI with Korean Educational Standards](https://doi.org/10.18653/v1/2025.naacl-short.56) |  | 0 | This paper presents the Korean National Educational Test Benchmark (KoNET), a new benchmark designed to evaluate Multimodal Generative AI Systems using Korean national educational tests. KoNET comprises four exams: the Korean Elementary General Educational Development Test (KoEGED), Middle (KoMGED), High (KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are renowned for their rigorous standards and diverse questions, facilitating a comprehensive analysis of AI performance across different educational levels. By focusing on Korean, KoNET provides insights into model performance in less-explored languages. We assess a range of models—open-source, open-access, and closed... | Geewook Kim, Sanghee Park |  |
| 57 |  |  [ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large Multimodal Models with Visual Programming Challenges](https://doi.org/10.18653/v1/2025.naacl-short.57) |  | 0 | Recent advancements in large multimodal models (LMMs) have showcased impressive code generation capabilities, primarily evaluated through image-to-code benchmarks. However, these benchmarks are limited to specific visual programming scenarios where the logic reasoning and the multimodal understanding capacities are split apart. To fill this gap, we propose ScratchEval, a novel benchmark designed to evaluate the visual programming reasoning ability of LMMs. ScratchEval is based on Scratch, a block-based visual programming language widely used in children’s programming education. By integrating visual elements and embedded programming logic, ScratchEval requires the model to process both... | Hongzhan Lin, Jing Ma, Rao Fu, Zhen Ye, Ziyang Luo |  |
| 58 |  |  [Interpret and Control Dense Retrieval with Sparse Latent Features](https://doi.org/10.18653/v1/2025.naacl-short.58) |  | 0 | Dense embeddings deliver strong retrieval performance but often lack interpretability and controllability. This paper introduces a novel approach using sparse autoencoders (SAE) to interpret and control dense embeddings via the learned latent sparse features. Our key contribution is the development of a retrieval-oriented contrastive loss, which ensures the sparse latent features remain effective for retrieval tasks and thus meaningful to interpret. Experimental results demonstrate that both the learned latent sparse features and their reconstructed embeddings retain nearly the same retrieval accuracy as the original dense vectors, affirming their faithfulness. Our further examination of... | Chenyan Xiong, Hao Kang, Tevin Wang |  |
| 59 |  |  [DART: An AIGT Detector using AMR of Rephrased Text](https://doi.org/10.18653/v1/2025.naacl-short.59) |  | 0 | As large language models (LLMs) generate more human-like texts, concerns about the side effects of AI-generated texts (AIGT) have grown. So, researchers have developed methods for detecting AIGT. However, two challenges remain. First, the performance of detecting black-box LLMs is low because existing models focus on probabilistic features. Second, most AIGT detectors have been tested on a single-candidate setting, which assumes that we know the origin of an AIGT and which may deviate from the real-world scenario. To resolve these challenges, we propose DART, which consists of four steps: rephrasing, semantic parsing, scoring, and multiclass classification. We conducted three experiments... | Bugeun Kim, Byungjun Kim, Hyeonchu Park |  |
| 60 |  |  [Scaling Graph-Based Dependency Parsing with Arc Vectorization and Attention-Based Refinement](https://doi.org/10.18653/v1/2025.naacl-short.60) |  | 0 | We propose a novel architecture for graph-based dependency parsing that explicitly constructs vectors, from which both arcs and labels are scored. Our method addresses key limitations of the standard two-pipeline approach by unifying arc scoring and labeling into a single network, reducing scalability issues caused by the information bottleneck and lack of parameter sharing. Additionally, our architecture overcomes limited arc interactions with transformer layers to efficiently simulate higher-order dependencies. Experiments on PTB and UD show that our model outperforms state-of-the-art parsers in both accuracy and efficiency. | Joseph Le Roux, Nadi Tomeh, Nicolas Floquet, Thierry Charnois |  |
| 61 |  |  [Language Models "Grok" to Copy](https://doi.org/10.18653/v1/2025.naacl-short.61) |  | 0 | We examine the pre-training dynamics of language models, focusing on their ability to copy text from preceding context—a fundamental skill for various LLM applications, including in-context learning (ICL) and retrieval-augmented generation (RAG). We propose a novel perspective that Transformer-based language models develop copying abilities similarly to grokking, which refers to sudden generalization on test set long after the model fit to the training set. Our experiments yield three arguments: (1) The pre-training loss decreases rapidly, while the context copying ability of models initially lags and then abruptly saturates. (2) The speed of developing copying ability is independent of... | Ang Lv, Rui Yan, Ruobing Xie, Xingwu Sun, Zhanhui Kang |  |
| 62 |  |  [Evaluating LLMs for Quotation Attribution in Literary Texts: A Case Study of LLaMa3](https://doi.org/10.18653/v1/2025.naacl-short.62) |  | 0 | Large Language Models (LLMs) have shown promising results in a variety of literary tasks, often using complex memorized details of narration and fictional characters. In this work, we evaluate the ability of Llama-3 at attributing utterances of direct-speech to their speaker in novels. The LLM shows impressive results on a corpus of 28 novels, surpassing published results with ChatGPT and encoder-based baselines by a large margin. We then validate these results by assessing the impact of book memorization and annotation contamination.We found that these types of memorization do not explain the large performance gain, making Llama-3 the new state-of-the-art for quotation attribution in... | Christophe Cerisara, Elena V. Epure, Gaspard Michel, Romain Hennequin |  |
| 63 |  |  [Beyond Literal Token Overlap: Token Alignability for Multilinguality](https://doi.org/10.18653/v1/2025.naacl-short.63) |  | 0 | Previous work has considered token overlap, or even similarity of token distributions, as predictors for multilinguality and cross-lingual knowledge transfer in language models. However, these very literal metrics assign large distances to language pairs with different scripts, which can nevertheless show good cross-linguality. This limits the explanatory strength of token overlap for knowledge transfer between language pairs that use distinct scripts or follow different orthographic conventions. In this paper, we propose subword token alignability as a new way to understand the impact and quality of multilingual tokenisation. In particular, this metric predicts multilinguality much better... | Alexander Fraser, Jindrich Libovický, Katharina Hämmerl, Tomasz Limisiewicz |  |
| 64 |  |  [IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark for LLMs](https://doi.org/10.18653/v1/2025.naacl-short.64) |  | 0 | Recent evaluations of LLMs on coreference resolution have revealed that traditional output formats and evaluation metrics do not fully capture the models’ referential understanding. To address this, we introduce IdentifyMe, a new benchmark for mention resolution presented in a multiple-choice question (MCQ) format, commonly used for evaluating LLMs. IdentifyMe features long narratives and employs heuristics to exclude easily identifiable mentions, creating a more challenging task. The benchmark also consists of a curated mixture of different mention types and corresponding entities, allowing for a fine-grained model performance analysis. We evaluate both closed- and open-source LLMs on... | Kawshik Manikantan, Makarand Tapaswi, Shubham Toshniwal, Vineet Gandhi |  |
| 65 |  |  [kNN Retrieval for Simple and Effective Zero-Shot Multi-speaker Text-to-Speech](https://doi.org/10.18653/v1/2025.naacl-short.65) |  | 0 | While recent zero-shot multi-speaker text-to-speech (TTS) models achieve impressive results, they typically rely on extensive transcribed speech datasets from numerous speakers and intricate training pipelines. Meanwhile, self-supervised learning (SSL) speech features have emerged as effective intermediate representations for TTS. Further, SSL features from different speakers that are linearly close share phonetic information while maintaining individual speaker identity. In this study, we introduce kNN-TTS, a simple and effective framework for zero-shot multi-speaker TTS using retrieval methods which leverage the linear relationships between SSL features. Objective and subjective... | Ajinkya Kulkarni, Enno Hermann, Karl El Hajal, Mathew MagimaiDoss |  |
| 66 |  |  [CORD: Balancing COnsistency and Rank Distillation for Robust Retrieval-Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-short.66) |  | 0 | With the adoption of retrieval-augmented generation (RAG), large language models (LLMs) are expected to ground their generation to the retrieved contexts. Yet, this is hindered by position bias of LLMs, failing to evenly attend to all contexts. Previous work has addressed this by synthesizing contexts with perturbed positions of gold segment, creating a position-diversified train set. We extend this intuition to propose consistency regularization with augmentation and distillation. First, we augment each training instance with its position perturbation to encourage consistent predictions, regardless of ordering. We also distill behaviors of this pair, although it can be counterproductive... | Daniel F. Campos, Filip Gralinski, Seungwon Hwang, Youngwon Lee, Yuxiong He, Zhewei Yao |  |
| 67 |  |  [GraphLSS: Integrating Lexical, Structural, and Semantic Features for Long Document Extractive Summarization](https://doi.org/10.18653/v1/2025.naacl-short.67) |  | 0 | Heterogeneous graph neural networks have recently gained attention for long document summarization, modeling the extraction as a node classification task. Although effective, these models often require external tools or additional machine learning models to define graph components, producing highly complex and less intuitive structures. We present GraphLSS, a heterogeneous graph construction for long document extractive summarization, incorporating Lexical, Structural, and Semantic features. It defines two levels of information (words and sentences) and four types of edges (sentence semantic similarity, sentence occurrence order, word in sentence, and word semantic similarity) without any... | Gerard de Melo, Hazem Abou Hamdan, Margarita Bugueño |  |
| 68 |  |  [Step-by-Step Fact Verification System for Medical Claims with Explainable Reasoning](https://doi.org/10.18653/v1/2025.naacl-short.68) |  | 0 | Fact verification (FV) aims to assess the veracity of a claim based on relevant evidence. The traditional approach for automated FV includes a three-part pipeline relying on short evidence snippets and encoder-only inference models. More recent approaches leverage the multi-turn nature of LLMs to address FV as a step-by-step problem where questions inquiring additional context are generated and answered until there is enough information to make a decision. This iterative method makes the verification process rational and explainable. While these methods have been tested for encyclopedic claims, exploration on domain-specific and realistic claims is missing. In this work, we apply an... | Florian Matthes, Ivana Hacajová, Juraj Vladika |  |
| 69 |  |  [Developing multilingual speech synthesis system for Ojibwe, Mi'kmaq, and Maliseet](https://doi.org/10.18653/v1/2025.naacl-short.69) |  | 0 | We present lightweight flow matching multilingual text-to-speech (TTS) systems for Ojibwe, Mi’kmaq, and Maliseet, three Indigenous languages in North America. Our results show that training a multilingual TTS model on three typologically similar languages can improve the performance over monolingual models, especially when data are scarce. Attention-free architectures are highly competitive with self-attention architecture with higher memory efficiency. Our research provides technical development to language revitalization for low-resource languages but also highlights the cultural gap in human evaluation protocols, calling for a more community-centered approach to human evaluation. | Chad Quinn, Changbing Yang, Christopher Hammerly, Jian Zhu, Mike Parkhill, Shenran Wang |  |
| 70 |  |  [Bottom-Up Synthesis of Knowledge-Grounded Task-Oriented Dialogues with Iteratively Self-Refined Prompts](https://doi.org/10.18653/v1/2025.naacl-short.70) |  | 0 | Training conversational question-answering (QA) systems demands a substantial amount of in-domain data, which is often scarce in practice. A common solution to this challenge is to generate synthetic data. Traditional methods typically follow a top-down approach, where a large language model (LLM) generates multi-turn dialogues from a broad prompt. While this method produces coherent conversations, it offers limited fine-grained control over the content and is susceptible to hallucinations. We introduce a bottom-up conversation synthesis approach, where QA pairs are generated first and then combined into a coherent dialogue. This method offers greater control and precision by dividing the... | Arpit Sharma, Kun Qian, Maximillian Chen, Siyan Li, Zhou Yu |  |
| 71 |  |  [Sociodemographic Prompting is Not Yet an Effective Approach for Simulating Subjective Judgments with LLMs](https://doi.org/10.18653/v1/2025.naacl-short.71) |  | 0 | Human judgments are inherently subjective and are actively affected by personal traits such as gender and ethnicity. While Large LanguageModels (LLMs) are widely used to simulate human responses across diverse contexts, their ability to account for demographic differencesin subjective tasks remains uncertain. In this study, leveraging the POPQUORN dataset, we evaluate nine popular LLMs on their abilityto understand demographic differences in two subjective judgment tasks: politeness and offensiveness. We find that in zero-shot settings, most models’ predictions for both tasks align more closely with labels from White participants than those from Asian or Black participants, while only a... | David Jurgens, Huaman Sun, Jiaxin Pei, Minje Choi |  |
| 72 |  |  [Identifying Power Relations in Conversations using Multi-Agent Social Reasoning](https://doi.org/10.18653/v1/2025.naacl-short.72) |  | 0 | Large language models (LLMs) struggle in social science domains, where critical thinking and human-level inference are crucial. In this work, we propose a multi-agent social reasoning framework that leverages the generative and reasoning capabilities of LLMs to generate and evaluate reasons from multiple perspectives grounded in social science theories, and construct a factor graph for inference. Experimental results on understanding power dynamics in conversations show that our method outperforms standard prompting baselines, demonstrating its potential for tackling hard Computational Social Science (CSS) tasks. | Dan Goldwasser, Leora Morgenstern, Maria Leonor Pacheco, Zhaoqing Wu |  |
| 73 |  |  [Examining Spanish Counseling with MIDAS: a Motivational Interviewing Dataset in Spanish](https://doi.org/10.18653/v1/2025.naacl-short.73) |  | 0 | Cultural and language factors significantly influence counseling, but Natural Language Processing research has not yet examined whether the findings of conversational analysis for counseling conducted in English apply to other languages. This paper presents a first step towards this direction. We introduce MIDAS (Motivational Interviewing Dataset in Spanish), a counseling dataset created from public video sources that contains expert annotations for counseling reflections and questions. Using this dataset, we explore language-based differences in counselor behavior in English and Spanish and develop classifiers in monolingual and multilingual settings, demonstrating its applications in... | Aylin Gunal, Bowen Yi, John Piette, Rada Mihalcea, Verónica PérezRosas |  |
| 74 |  |  [Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes](https://doi.org/10.18653/v1/2025.naacl-short.74) |  | 0 | Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of... | Deonna M. Owens, Franck Dernoncourt, Hanieh Deilamsalehy, Isabel O. Gallegos, Jiuxiang Gu, Joe Barrow, Md. Mehrab Tanjim, Nedim Lipka, Ruiyi Zhang, Ryan A. Rossi, Ryan Aponte, Sungchul Kim, Tong Yu |  |
| 75 |  |  [EqualizeIR: Mitigating Linguistic Biases in Retrieval Models](https://doi.org/10.18653/v1/2025.naacl-short.75) |  | 0 | This study finds that existing information retrieval (IR) models show significant biases based on the linguistic complexity of input queries, performing well on linguistically simpler (or more complex) queries while underperforming on linguistically more complex (or simpler) queries.To address this issue, we propose EqualizeIR, a framework to mitigate linguistic biases in IR models. EqualizeIR uses a linguistically biased weak learner to capture linguistic biases in IR datasets and then trains a robust model by regularizing and refining its predictions using the biased weak learner. This approach effectively prevents the robust model from overfitting to specific linguistic patterns in... | Hadi Amiri, Jiali Cheng |  |
| 76 |  |  [Do Audio-Language Models Understand Linguistic Variations?](https://doi.org/10.18653/v1/2025.naacl-short.76) |  | 0 | Open-vocabulary audio language models (ALMs), like Contrastive Language Audio Pretraining (CLAP), represent a promising new paradigm for audio-text retrieval using natural language queries. In this paper, for the first time, we perform controlled experiments on various benchmarks to show that existing ALMs struggle to generalize to linguistic variations in textual queries. To address this issue, we propose RobustCLAP, a novel and compute-efficient technique to learn audio-language representations agnostic to linguistic variations. Specifically, we reformulate the contrastive loss used in CLAP architectures by introducing a multi-view contrastive learning objective, where paraphrases are... | Ashish Seth, Dinesh Manocha, Hemant Kumar Giri, Nishit Anand, Ramaneswaran Selvakumar, Sonal Kumar, Sreyan Ghosh |  |
| 77 |  |  [Giving the Old a Fresh Spin: Quality Estimation-Assisted Constrained Decoding for Automatic Post-Editing](https://doi.org/10.18653/v1/2025.naacl-short.77) |  | 0 | Automatic Post-Editing (APE) systems often struggle with over-correction, where unnecessary modifications are made to a translation, diverging from the principle of minimal editing. In this paper, we propose a novel technique to mitigate over-correction by incorporating word-level Quality Estimation (QE) information during the decoding process. This method is architecture-agnostic, making it adaptable to any APE system, regardless of the underlying model or training approach. Our experiments on English-German, English-Hindi, and English-Marathi language pairs show the proposed approach yields significant improvements over their corresponding baseline APE systems, with TER gains of 0.65,... | Diptesh Kanojia, Pushpak Bhattacharyya, Sourabh Dattatray Deoghare |  |
| 78 |  |  [RuleR: Improving LLM Controllability by Rule-based Data Recycling](https://doi.org/10.18653/v1/2025.naacl-short.78) |  | 0 | Large language models (LLMs) still lack delicate controllability over their responses, which is critical to enhancing their performance and the user experience. However, curating supervised fine-tuning (SFT) datasets to improve LLM controllability usually relies on human experts or proprietary LLMs, which requires additional costs. To bridge this gap, we propose Rule-based Data Recycling (RuleR), a data augmentation method incorporating multiple constraints into the original data samples according to predefined rules, which creates new training tasks to consolidate the controllability of LLMs. Instead of creating new data from scratch, RuleR “recycles” existing data by simply applying... | Chenguang Wang, Dang Nguyen, Dianqi Li, Han Chen, Ming Li, Tianyi Zhou |  |
| 79 |  |  [MixRevDetect: Towards Detecting AI-Generated Content in Hybrid Peer Reviews](https://doi.org/10.18653/v1/2025.naacl-short.79) |  | 0 | The growing use of large language models (LLMs) in academic peer review poses significant challenges, particularly in distinguishing AI-generated content from human-written feedback. This research addresses the problem of identifying AI-generated peer review comments, which are crucial to maintaining the integrity of scholarly evaluation. Prior research has primarily focused on generic AI-generated text detection or on estimating the fraction of peer reviews that may be AI-generated, often treating reviews as monolithic units. However, these methods fail to detect finer-grained AI-generated points within mixed-authorship reviews. To address this gap, we propose MixRevDetect, a novel method... | Asif Ekbal, Sagnik Sengupta, Samarth Garg, Sandeep Kumar, Tirthankar Ghosal |  |
| 80 |  |  [DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie Character-Aware Discourse Graph](https://doi.org/10.18653/v1/2025.naacl-short.80) |  | 0 | Summarizing movie screenplays presents a unique set of challenges compared to standard document summarization. Screenplays are not only lengthy, but also feature a complex interplay of characters, dialogues, and scenes, with numerous direct and subtle relationships and contextual nuances that are difficult for machine learning models to accurately capture and comprehend. Recent attempts at screenplay summarization focus on fine-tuning transformer-based pre-trained models, but these models often fall short in capturing long-term dependencies and latent relationships, and frequently encounter the “lost in the middle” issue. To address these challenges, we introduce DiscoGraMS, a novel... | Maitreya Prafulla Chitale, Rahul Mishra, Rajakrishnan Rajkumar, Uday Bindal |  |
| 81 |  |  [Capturing Human Cognitive Styles with Language: Towards an Experimental Evaluation Paradigm](https://doi.org/10.18653/v1/2025.naacl-short.81) |  | 0 | While NLP models often seek to capture cognitive states via language, the validity of predicted states is determined by comparing them to annotations created without access the cognitive states of the authors. In behavioral sciences, cognitive states are instead measured via experiments. Here, we introduce an experiment-based framework for evaluating language-based cognitive style models against human behavior. We explore the phenomenon of decision making, and its relationship to the linguistic style of an individual talking about a recent decision they made. The participants then follow a classical decision-making experiment that captures their cognitive style, determined by how... | Christian C. Luhmann, H. Andrew Schwartz, Julia Buffolino, Ryan L. Boyd, Syeda Mahwish, Vasudha Varadarajan, Xiaoran Liu |  |
| 82 |  |  [Understanding Figurative Meaning through Explainable Visual Entailment](https://doi.org/10.18653/v1/2025.naacl-long.1) |  | 0 | Large Vision-Language Models (VLMs) have demonstrated strong capabilities in tasks requiring a fine-grained understanding of literal meaning in images and text, such as visual question-answering or visual entailment. However, there has been little exploration of the capabilities of these models when presented with images and captions containing figurative meaning, such as metaphors or humor. To close this gap, we propose a new task framing the figurative meaning understanding problem as an explainable visual entailment task, where the model has to predict whether the image (premise) entails a caption (hypothesis) and justify the predicted label with a textual explanation. The figurative... | Arkadiy Saakyan, Shreyas Kulkarni, Smaranda Muresan, Tuhin Chakrabarty |  |
| 83 |  |  [Benchmarking Distributional Alignment of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.2) |  | 0 | Language models (LMs) are increasingly used as simulacra for people, yet their ability to match the distribution of views of a specific demographic group and be distributionally aligned remains uncertain. This notion of distributional alignment is complex, as there is significant variation in the types of attributes that are simulated. Prior works have underexplored the role of three critical variables—the question domain, steering method, and distribution expression method—which motivates our contribution of a benchmark explicitly addressing these dimensions. We construct a dataset expanding beyond political values, create human baselines for this task, and evaluate the extent to which an... | Carlos Guestrin, Nicole Meister, Tatsunori Hashimoto |  |
| 84 |  |  [World Models with Hints of Large Language Models for Goal Achieving](https://doi.org/10.18653/v1/2025.naacl-long.3) |  | 0 | Reinforcement learning struggles in the face of long-horizon tasks and sparse goals due to the difficulty in manual reward specification. While existing methods address this by adding intrinsic rewards, they may fail to provide meaningful guidance in long-horizon decision-making tasks with large state and action spaces, lacking purposeful exploration. Inspired by human cognition, we propose a new multi-modal model-based RL approach named Dreaming with Large Language Models (DLLM). DLLM integrates the proposed hinting subgoals from the LLMs into the model rollouts to encourage goal discovery and reaching in challenging tasks. By assigning higher intrinsic rewards to samples that align with... | Furong Huang, Huazhe Xu, Jiafei Lyu, Jian Tao, Xiu Li, Xiyao Wang, Zeyuan Liu, Ziyu Huan |  |
| 85 |  |  [CogLM: Tracking Cognitive Development of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.4) |  | 0 | Piaget’s Theory of Cognitive Development (PTC) posits that the development of cognitive levels forms the foundation for human learning across various abilities. As Large Language Models (LLMs) have recently shown remarkable abilities across a wide variety of tasks, we are curious about the cognitive levels of current LLMs: to what extent they have developed and how this development has been achieved. To this end, we construct a benchmark CogLM (Cognitive Ability Evaluation for Language Model) based on PTC to assess the cognitive levels of LLMs. CogLM comprises 1,220 questions spanning 10 cognitive abilities crafted by more than 20 human experts, providing a comprehensive testbed for the... | Boyuan Pan, Heda Wang, Kan Li, Peiwen Yuan, Shaoxiong Feng, Xinglin Wang, Yao Hu, Yiwei Li |  |
| 86 |  |  [Improving and Assessing the Fidelity of Large Language Models Alignment to Online Communities](https://doi.org/10.18653/v1/2025.naacl-long.5) |  | 0 | Large language models (LLMs) have shown promise in representing individuals and communities, offering new ways to study complex social dynamics. However, effectively aligning LLMs with specific human groups and systematically assessing the fidelity of the alignment remains a challenge. This paper presents a robust framework for aligning LLMs with online communities via instruction-tuning and comprehensively evaluating alignment across various aspects of language, including authenticity, emotional tone, toxicity, and harm. We demonstrate the utility of our approach by applying it to online communities centered on dieting and body image. We administer an eating disorder psychometric test to... | Kristina Lerman, Minh Duc Chu, Rebecca Dorn, Zihao He |  |
| 87 |  |  [Improving Retrospective Language Agents via Joint Policy Gradient Optimization](https://doi.org/10.18653/v1/2025.naacl-long.6) |  | 0 | In recent research advancements within the community, large language models (LLMs) have sparked great interest in creating autonomous agents. However, current prompt-based agents often heavily rely on large-scale LLMs. Meanwhile, although fine-tuning methods significantly enhance the capabilities of smaller LLMs, the fine-tuned agents often lack the potential for self-reflection and self-improvement. To address these challenges, we introduce a novel agent framework named RetroAct, which is a framework that jointly optimizes both task-planning and self-reflective evolution capabilities in language agents. Specifically, we develop a two-stage joint optimization process that integrates... | Bo Lan, JiRong Wen, Jiakai Tang, Lei Wang, Quanyu Dai, Xu Chen, Xueyang Feng, Zhenhua Dong |  |
| 88 |  |  [CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases](https://doi.org/10.18653/v1/2025.naacl-long.7) |  | 0 | Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval and MBPP, but struggle with handling entire code repositories. This challenge has prompted research on enhancing LLM-codebase interaction at a repository scale. Current solutions rely on similarity-based retrieval or manual tools and APIs, each with notable drawbacks. Similarity-based retrieval often has low recall in complex tasks, while manual tools and APIs are typically task-specific and require expert knowledge, reducing their generalizability across diverse code tasks and real-world applications. To mitigate these limitations, we introduce CodexGraph, a system that integrates LLM agents with graph database... | Bo Lan, Fei Wang, Michael Qizhe Shieh, Wenmeng Zhou, Xiangyan Liu, Yang Liu, Zhicheng Zhang, Zhiyuan Hu |  |
| 89 |  |  [Instantly Learning Preference Alignment via In-context DPO](https://doi.org/10.18653/v1/2025.naacl-long.8) |  | 0 | Human Preference Alignment (HPA) can assist large language models (LLMs) to generate safe content. Due to the heavy cost of fine-tuning, tuning-free methods have emerged, typically modifying LLM decoding via post-processing. In this paper, we propose a novel and effective approach for HPA in a tuning-free way, named In-Context Direct Preference Optimization (ICDPO). We first rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the LLM before and after ICL. It enables LLMs to both generate and select the well-aligned response, which is precisely estimated by the aforementioned instant scorer, thereby enhancing the final... | Feifan Song, Houfeng Wang, Peiyi Wang, Xin Zhang, Yuxuan Fan |  |
| 90 |  |  [ALTER: Augmentation for Large-Table-Based Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.9) |  | 0 |  | Han Zhang, Hanfang Yang, Yuheng Ma |  |
| 91 |  |  [What the #?\*!: Disentangling Hate Across Target Identities](https://doi.org/10.18653/v1/2025.naacl-long.10) |  | 0 | Hate speech (HS) classifiers do not perform equally well in detecting hateful expressions towards different target identities. They also demonstrate systematic biases in predicted hatefulness scores. Tapping on two recently proposed functionality test datasets for HS detection, we quantitatively analyze the impact of different factors on HS prediction. Experiments on popular industrial and academic models demonstrate that HS detectors assign a higher hatefulness score merely based on the mention of specific target identities. Besides, models often confuse hatefulness and the polarity of emotions. This result is worrisome as the effort to build HS detectors might harm the vulnerable... | Aneesh Moideen Koya, Leo Wanner, Yiping Jin |  |
| 92 |  |  [MAD Speech: Measures of Acoustic Diversity of Speech](https://doi.org/10.18653/v1/2025.naacl-long.11) |  | 0 | Generative spoken language models produce speech in a wide range of voices, prosody, and recording conditions, seemingly approaching the diversity of natural speech. However, the extent to which generated speech is acoustically diverse remains unclear due to a lack of appropriate metrics. We address this gap by developing lightweight metrics of acoustic diversity, which we collectively refer to as MAD Speech. We focus on measuring five facets of acoustic diversity: voice, gender, emotion, accent, and background noise. We construct the metrics as a composition of specialized, per-facet embedding models and an aggregation function that measures diversity within the embedding space. Next, we... | Andrea Agostinelli, Eugene Kharitonov, Marco Tagliasacchi, Matthieu Futeral, Neil Zeghidour |  |
| 93 |  |  [The Russian-focused embedders' exploration: ruMTEB benchmark and Russian embedding model design](https://doi.org/10.18653/v1/2025.naacl-long.12) |  | 0 | Embedding models play a crucial role in Natural Language Processing (NLP) by creating text embeddings used in various tasks such as information retrieval and assessing semantic text similarity. This paper focuses on research related to embedding models in the Russian language. It introduces a new Russian-focused embedding model called ru-en-RoSBERTa and the ruMTEB benchmark, the Russian version extending the Massive Text Embedding Benchmark (MTEB). Our benchmark includes seven categories of tasks, such as semantic textual similarity, text classification, reranking, and retrieval.The research also assesses a representative set of Russian and multilingual models on the proposed benchmark.... | Aleksandr Abramov, Alena Fenogenova, Anna Maksimova, Artem Snegirev, Maria Tikhonova |  |
| 94 |  |  [PRACTIQ: A Practical Conversational Text-to-SQL dataset with Ambiguous and Unanswerable Queries](https://doi.org/10.18653/v1/2025.naacl-long.13) |  | 0 | Previous text-to-SQL datasets and systems have primarily focused on user questions with clear intentions that can be answered. However, real user questions can often be ambiguous with multiple interpretations or unanswerable due to a lack of relevant data. In this work, we construct a practical conversational text-to-SQL dataset called PRACTIQ, consisting of ambiguous and unanswerable questions inspired by real-world user questions. We first identified four categories of ambiguous questions and four categories of unanswerable questions by studying existing text-to-SQL datasets. Then, we generate conversations with four turns: the initial user question, an assistant response seeking... | Anuj Chauhan, ChungWei Hang, Henghui Zhu, Jiarong Jiang, Lin Pan, Mingwen Dong, Nischal Ashok Kumar, Patrick Ng, Shuaichen Chang, Wuwei Lan, Yiqun Hu, Zhiguo Wang |  |
| 95 |  |  [MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems](https://doi.org/10.18653/v1/2025.naacl-long.14) |  | 0 | Traditional retrieval-augmented generation (RAG) benchmarks evaluate systems using heuristic-based metrics, but these require human preferences as the ground truth for reference. In contrast, arena-based benchmarks, where systems compete against each other, require an expensive large language model (LLM) as a judge for a reliable evaluation. We present a simple efficient technique to combine the best of both worlds. The idea is to train a surrogate judge using heuristic metrics as input, to output the LLM as a judge prediction.In our work, we develop MIRAGE-Bench, a synthetic arena-based RAG benchmark for 18 diverse languages on Wikipedia focused on multilingual answer generation... | Amin Ahmad, Ge Luo, Jimmy Lin, Nandan Thakur, Suleman Kazi |  |
| 96 |  |  [LLMs Are Biased Towards Output Formats! Systematically Evaluating and Mitigating Output Format Bias of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.15) |  | 0 | We present the first systematic evaluation examining format bias in performance of large language models (LLMs). Our approach distinguishes between two categories of an evaluation metric under format constraints to reliably and accurately assess performance: one measures performance when format constraints are adhered to, while the other evaluates performance regardless of constraint adherence. We then define a metric for measuring the format bias of LLMs and establish effective strategies to reduce it. Subsequently, we present our empirical format bias evaluation spanning four commonly used categories—multiple-choice question-answer, wrapping, list, and mapping—covering 15 widely-used... | Do Xuan Long, Hieu Dao, Kenji Kawaguchi, MinYen Kan, Nancy F. Chen, NgocHai Nguyen, Shafiq Joty, Tiviatis Sim |  |
| 97 |  |  [The Impact of Visual Information in Chinese Characters: Evaluating Large Models' Ability to Recognize and Utilize Radicals](https://doi.org/10.18653/v1/2025.naacl-long.16) |  | 0 | The glyphic writing system of Chinese incorporates information-rich visual features in each character, such as radicals that provide hints about meaning or pronunciation. However, there has been no investigation into whether contemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can harness these sub-character features in Chinese through prompting. In this study, we establish a benchmark to evaluate LLMs’ and VLMs’ understanding of visual elements in Chinese characters, including radicals, composition structures, strokes, and stroke counts. Our results reveal that models surprisingly exhibit some, but still limited, knowledge of the visual information, regardless of... | Karl Stratos, Wei Xu, Xiaofeng Wu |  |
| 98 |  |  [PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic Languages with Example Selection from related Example Banks](https://doi.org/10.18653/v1/2025.naacl-long.17) |  | 0 | Large Language Models (LLMs) have recently demonstrated impressive few-shot learning capabilities through in-context learning (ICL). However, ICL performance is highly dependent on the choice of few-shot demonstrations, making the selection of the most optimal examples a persistent research challenge. This issue is further amplified in low-resource Indic languages, where the scarcity of ground-truth data complicates the selection process. In this work, we propose PromptRefine, a novel Alternating Minimization approach for example selection that improves ICL performance on low-resource Indic languages. PromptRefine leverages auxiliary example banks from related high-resource Indic languages... | Dinesh Manocha, Koyel Mukherjee, Soumya Suvra Ghosal, Soumyabrata Pal |  |
| 99 |  |  [Unlocking Decoding-time Controllability: Gradient-Free Multi-Objective Alignment with Contrastive Prompts](https://doi.org/10.18653/v1/2025.naacl-long.18) |  | 0 | The task of multi-objective alignment aims at balancing and controlling the different alignment objectives, e.g., helpfulness, harmlessness and honesty) of large language models to meet the personalized requirements of different users. However, previous methods tend to train multiple models to deal with various user preferences, with the number of trained models growing linearly with the number of alignment objectives and the number of different preferences. Meanwhile, existing methods are generally poor in extensibility and require significant re-training for each new alignment objective considered. Considering the limitation of previous approaches, we propose MCA, which constructs an... | Julian J. McAuley, Rui Yan, Tingchen Fu, Yupeng Hou |  |
| 100 |  |  [Fingerspelling within Sign Language Translation](https://doi.org/10.18653/v1/2025.naacl-long.19) |  | 0 | Fingerspelling poses challenges for sign language processing due to its high-frequency motion and use for open-vocabulary terms. While prior work has studied fingerspelling recognition, there has been little attention to evaluating how well sign language translation models understand fingerspelling in the context of entire sentences—and improving this capability. We manually annotate instances of fingerspelling within FLEURS-ASL and use them to evaluate the effect of two simple measures to improve fingerspelling recognition within American Sign Language to English translation: 1) use a model family (ByT5) with character- rather than subword-level tokenization, and 2) mix fingerspelling... | Garrett Tanzer |  |
| 101 |  |  [MoDS: Moderating a Mixture of Document Speakers to Summarize Debatable Queries in Document Collections](https://doi.org/10.18653/v1/2025.naacl-long.20) |  | 0 | Query-focused summarization (QFS) gives a summary of documents to answer a query.Past QFS work assumes queries have one answer, ignoring debatable ones (\*Is law school worth it?\*).We introduce \*\*Debatable QFS (DQFS)\*\*, a task to create summaries that answer debatable queries via documents with opposing perspectives; summaries must \*comprehensively cover\* all sources and \*balance perspectives\*, favoring no side.These goals elude LLM QFS systems, which: 1) lack structured content plans, failing to guide LLMs to write balanced summaries, and 2) employ the same query to retrieve contexts across documents, failing to cover all perspectives specific to each document’s content.To... | Alexa F. Siu, Franck Dernoncourt, Jordan Lee BoydGraber, Nedim Lipka, Nishant Balepur, Puneet Mathur, Tong Sun |  |
| 102 |  |  [Aligning Sentence Simplification with ESL Learner's Proficiency for Language Acquisition](https://doi.org/10.18653/v1/2025.naacl-long.21) |  | 0 | Text simplification is crucial for improving accessibility and comprehension for English as a Second Language (ESL) learners. This study goes a step further and aims to facilitate ESL learners’ language acquisition by simplification. Specifically, we propose simplifying complex sentences to appropriate levels for learners while also increasing vocabulary coverage of the target level in the simplifications. We achieve this without a parallel corpus by conducting reinforcement learning on a large language model. Our method employs token-level and sentence-level rewards, and iteratively trains the model on its self-generated outputs to guide the model to search for simplification hypotheses... | Guanlin Li, Noël Crespi, Yuki Arase |  |
| 103 |  |  [PeerQA: A Scientific Question Answering Dataset from Peer Reviews](https://doi.org/10.18653/v1/2025.naacl-long.22) |  | 0 | We present PeerQA, a real-world, scientific, document-level Question Answering (QA) dataset. PeerQA questions have been sourced from peer reviews, which contain questions that reviewers raised while thoroughly examining the scientific article. Answers have been annotated by the original authors of each paper. The dataset contains 579 QA pairs from 208 academic articles, with a majority from ML and NLP, as well as a subset of other scientific communities like Geoscience and Public Health.PeerQA supports three critical tasks for developing practical QA systems: Evidence retrieval, unanswerable question classification, and answer generation. We provide a detailed analysis of the collected... | Iryna Gurevych, Ted Briscoe, Tim Baumgärtner |  |
| 104 |  |  [ALiiCE: Evaluating Positional Fine-grained Citation Generation](https://doi.org/10.18653/v1/2025.naacl-long.23) |  | 0 | Large Language Model (LLM) can enhance its credibility and verifiability by generating text with citations. However, existing research on citation generation is predominantly limited to sentence-level statements, neglecting the significance of positional fine-grained citations that can appear anywhere within sentences. To facilitate further exploration of the positional fine-grained citation generation, we propose ALiiCE, the first automatic evaluation framework for this task. Our method employs a dependency tree based approach to parse the sentence-level claim into atomic claims. Then ALiiCE evaluates citation quality using three metrics, including positional fine-grained citation recall,... | Baolong Bi, Huawei Shen, Jinhua Gao, Xiaoming Yu, Xueqi Cheng, Yilong Xu |  |
| 105 |  |  [An LLM-Based Approach for Insight Generation in Data Analysis](https://doi.org/10.18653/v1/2025.naacl-long.24) |  | 0 | Generating insightful and actionable information from databases is critical in data analysis. This paper introduces a novel approach using Large Language Models (LLMs) to automatically generate textual insights. Given a multi-table database as input, our method leverages LLMs to produce concise, text-based insights that reflect interesting patterns in the tables. Our framework includes a Hypothesis Generator to formulate domain-relevant questions, a Query Agent to answer such questions by generating SQL queries against a database, and a Summarization module to verbalize the insights. The insights are evaluated for both correctness and subjective insightfulness using a hybrid model of human... | Adam Elwood, Alaa Boukhary, Alberto Sánchez Pérez, Luis Castejón Lozano, Paolo Papotti |  |
| 106 |  |  [WebQuality: A Large-scale Multi-modal Web Page Quality Assessment Dataset with Multiple Scoring Dimensions](https://doi.org/10.18653/v1/2025.naacl-long.25) |  | 0 | The assessment of web page quality plays a critical role in a range of downstream applications, yet there is a notable absence of datasets for the evaluation of web page quality. This research presents the pioneering task of web page quality assessment and introduces the first comprehensive, multi-modal Chinese dataset named WebQuality specifically designed for this task. The dataset includes over 65,000 detailed an-notations spanning four sub-dimensions and incorporates elements such as HTML+CSS, text, and visual screenshot, facilitating in-depth modeling and assessment of web page quality. We performed evaluations using a variety of baseline models to demonstrate the complexity of the... | Chen Xiang, Jin Ma, Li Xin, Tao Zhang, Tian Hua Zhou, Yige Wang, ZhuHangyu ZhuHangyu |  |
| 107 |  |  [UFO: A UI-Focused Agent for Windows OS Interaction](https://doi.org/10.18653/v1/2025.naacl-long.26) |  | 0 | We introduce UFO, a UI-Fcused agent designed to fulfill user requests tailored to Windows OS applications by observing and analyzing the GUI and control information of these applications. UFO utilizes a hierarchical dual-agent framework that decomposes user requests using a divide-and-conquer approach, enabling seamless navigation and addressing sub-tasks across multiple applications. It also incorporates a control interaction module tailored for Windows OS, which detects control elements effectively and allows for fully automated execution. As a result, UFO simplifies complex and time-consuming processes into tasks that can be completed with natural language commands.We conducted testing... | Bo Qiao, Chaoyun Zhang, Dongmei Zhang, Liqun Li, Minghua Ma, Qi Zhang, Qingwei Lin, Saravan Rajmohan, Shilin He, Si Qin, Xu Zhang, Yu Kang |  |
| 108 |  |  [Is your benchmark truly adversarial? AdvScore: Evaluating Human-Grounded Adversarialness](https://doi.org/10.18653/v1/2025.naacl-long.27) |  | 0 | Adversarial datasets should validate AI robustness by providing samples on which humans perform well, but models do not. However, as models evolve, datasets can become obsolete. Measuring whether a dataset remains adversarial is hindered by the lack of a standardized metric for measuring adversarialness. We propose ADVSCORE, a human-grounded evaluation metric that assesses a dataset’s adversarialness by capturing models’ and humans’ varying abilities, while also identifying poor examples. We then use ADVSCORE to motivate a new dataset creation pipeline for realistic and high-quality adversarial samples, enabling us to collect an adversarial question answering (QA) dataset, ADVQA. We apply... | Eve Fleisig, Ishani Mondal, Jordan Lee BoydGraber, Maharshi Gor, Yoo Yeon Sung |  |
| 109 |  |  [Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation](https://doi.org/10.18653/v1/2025.naacl-long.28) |  | 0 | Multimodal foundation models hold significant potential for automating radiology report generation, thereby assisting clinicians in diagnosing cardiac diseases. However, generated reports often suffer from serious factual inaccuracy. In this paper, we introduce a fact-aware multimodal retrieval-augmented pipeline in generating accurate radiology reports (FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then integrate factual knowledge to train a universal multimodal retriever. Given a radiology image, our retriever can identify high-quality reference reports to augment multimodal foundation models, thus enhancing the factual completeness and correctness of report... | Chenyan Xiong, James (Jialun) Zhao, Liwen Sun, Wenjing Han |  |
| 110 |  |  [On Behalf of the Stakeholders: Trends in NLP Model Interpretability in the Era of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.29) |  | 0 | Recent advancements in NLP systems, particularly with the introduction of LLMs, have led to widespread adoption of these systems by a broad spectrum of users across various domains, impacting decision-making, the job market, society, and scientific research. This surge in usage has led to an explosion in NLP model interpretability and analysis research, accompanied by numerous technical surveys. Yet, these surveys often overlook the needs and perspectives of explanation stakeholders. In this paper, we address three fundamental questions: Why do we need interpretability, what are we interpreting, and how? By exploring these questions, we examine existing interpretability paradigms, their... | Nitay Calderon, Roi Reichart |  |
| 111 |  |  [Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward](https://doi.org/10.18653/v1/2025.naacl-long.30) |  | 0 | Preference modeling techniques, such as direct preference optimization (DPO), has shown effective in enhancing the generalization abilities of large language model (LLM). However, in tasks involving video instruction-following, providing informative feedback, especially for open-ended conversations, remains a significant challenge. While previous studies have explored using large multimodal models (LMMs) as reward models for guiding preference modeling, their ability to accurately assess the quality of generated responses and their alignment with video content has not been conclusively demonstrated. This paper introduces a novel framework that utilizes detailed video captions as a proxy of... | Alexander G. Hauptmann, Chunyuan Li, Di Fu, Keyang Xu, Liangke Gui, Ruohong Zhang, Yihao Feng, Yiming Yang, Yonatan Bisk, Yuanhan Zhang, Zhiqing Sun |  |
| 112 |  |  [FlexiGPT: Pruning and Extending Large Language Models with Low-Rank Weight Sharing](https://doi.org/10.18653/v1/2025.naacl-long.31) |  | 0 | The rapid proliferation of large language models (LLMs) in natural language processing (NLP) has created a critical need for techniques that enable efficient deployment on memory-constrained devices without compromising performance. We present a method to prune LLMs that selectively prunes model blocks based on an importance score and replaces them with a low-parameter replacement strategy. Specifically, we propose a principled metric to replace each pruned block using a weight-sharing mechanism that leverages unpruned counterparts from the model and block-specific low-rank adapters. Furthermore, we facilitate the learning of these replacement blocks with output feature normalization and... | ChiHeng Lin, Haris Jeelani, Hongxia Jin, James Seale Smith, Shangqian Gao, Shikhar Tuli, YenChang Hsu, Yilin Shen |  |
| 113 |  |  [Conformalized Answer Set Prediction for Knowledge Graph Embedding](https://doi.org/10.18653/v1/2025.naacl-long.32) |  | 0 | Knowledge graph embeddings (KGE) apply machine learning methods on knowledge graphs (KGs) to provide non-classical reasoning capabilities based on similarities and analogies. The learned KG embeddings are typically used to answer queries by ranking all potential answers, but rankings often lack a meaningful probabilistic interpretation - lower-ranked answers do not necessarily have a lower probability of being true. This limitation makes it difficult to quantify uncertainty of model’s predictions, posing challenges for the application of KGE methods in high-stakes domains like medicine. We address this issue by applying the theory of conformal prediction that allows generating answer sets,... | Bo Xiong, Evgeny Kharlamov, Jiarong Pan, Nico Potyka, Steffen Staab, Yunjie He, Yuqicheng Zhu |  |
| 114 |  |  [Parameter-free and Accessible Prompt Learning to Enhance Adversarial Robustness for Pre-trained Vision-Language Models](https://doi.org/10.18653/v1/2025.naacl-long.33) |  | 0 | Large pre-trained Vision-Language Models (VLMs) have revolutionized both computer vision and natural language processing. Despite their success, adversarial examples can still mislead VLMs into producing incorrect results. This work focuses on boosting the adversarial robustness of VLMs by searching for text prompts at the word level, rather than optimizing continuous textual embeddings. We introduce Parameter-Free Prompt Tuning (PFPT) to learn defense words that enhance resilience against adversarial attacks when appended to existing prompts, thereby offering ease of use due to the simplicity of this approach. These defense words are naturally present in the inherent vocabulary of VLMs,... | Bingyu Hu, Changhua Meng, Changtao Miao, Dan Hong, Kun Yang, Shiwen Cui, Xingran Zhou, Zhuoer Xu |  |
| 115 |  |  [Fine-grained Fallacy Detection with Human Label Variation](https://doi.org/10.18653/v1/2025.naacl-long.34) |  | 0 | We introduce FAINA, the first dataset for fallacy detection that embraces multiple plausible answers and natural disagreement. FAINA includes over 11K span-level annotations with overlaps across 20 fallacy types on social media posts in Italian about migration, climate change, and public health given by two expert annotators. Through an extensive annotation study that allowed discussion over multiple rounds, we minimize annotation errors whilst keeping signals of human label variation. Moreover, we devise a framework that goes beyond “single ground truth” evaluation and simultaneously accounts for multiple (equally reliable) test sets and the peculiarities of the task, i.e., partial span... | Agnese Daffara, Alan Ramponi, Sara Tonelli |  |
| 116 |  |  [Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models](https://doi.org/10.18653/v1/2025.naacl-long.35) |  | 0 | Despite their wide adoption, the biases and unintended behaviors of language models remain poorly understood. In this paper, we identify and characterize a phenomenon never discussed before, which we call semantic leakage, where models leak irrelevant information from the prompt into the generation in unexpected ways. We propose an evaluation setting to detect semantic leakage both by humans and automatically, curate a diverse test suite for diagnosing this behavior, and measure significant semantic leakage in 13 flagship models. We also show that models exhibit semantic leakage in languages besides English and across different settings and generation scenarios. This discovery highlights... | Alisa Liu, Hila Gonen, Luke Zettlemoyer, Noah A. Smith, Terra Blevins |  |
| 117 |  |  [SELFGOAL: Your Language Agents Already Know How to Achieve High-level Goals](https://doi.org/10.18653/v1/2025.naacl-long.36) |  | 0 | Language agents powered by large language models (LLMs) are increasingly valuable as decision-making tools in domains such as gaming and programming. However, these agents often face challenges in achieving high-level goals without detailed instructions and in adapting to environments where feedback is delayed. In this paper, we present SELFGOAL, a novel automatic approach designed to enhance agents’ capabilities to achieve high-level goals with limited human prior and environmental feedback. The core concept of SELFGOAL involves adaptively breaking down a high-level goal into a tree structure of more practical subgoals during the interaction with environments while identifying the most... | Aili Chen, Deqing Yang, Jiangjie Chen, Kyle Richardson, Ruihan Yang, Siyu Yuan, Yanghua Xiao, Yikai Zhang |  |
| 118 |  |  [Familarity: Better Evaluation of Zero-Shot Named Entity Recognition by Quantifying Label Shifts in Synthetic Training Data](https://doi.org/10.18653/v1/2025.naacl-long.37) |  | 0 | Zero-shot named entity recognition (NER) is the task of detecting named entities of specific types (such as Person or Medicine) without any training examples. Current research increasingly relies on large synthetic datasets, automatically generated to cover tens of thousands of distinct entity types, to train zero-shot NER models. However, in this paper, we find that these synthetic datasets often contain entity types that are semantically highly similar to (or even the same as) those in standard evaluation benchmarks. Because of this overlap, we argue that reported F1 scores for zero-shot NER overestimate the true capabilities of these approaches. Further, we argue that current evaluation... | Alan Akbik, Fabio Barth, Jonas Golde, Max Ploner, Nicolaas Paul Jedema, Patrick Haller |  |
| 119 |  |  [Learning to Summarize from LLM-generated Feedback](https://doi.org/10.18653/v1/2025.naacl-long.38) |  | 0 | Developing effective text summarizers remains a challenge due to issues like hallucinations, key information omissions, and verbosity in LLM-generated summaries. This work explores using LLM-generated feedback to improve summary quality by aligning the summaries with human preferences for faithfulness, completeness, and conciseness. We introduce FeedSum, a large-scale dataset containing multi-dimensional LLM feedback on summaries of varying quality across diverse domains. Our experiments show how feedback quality, dimensionality, and granularity influence preference learning, revealing that high-quality, multi-dimensional, fine-grained feedback significantly improves summary generation. We... | Gihun Lee, Hang Su, Hwanjun Song, Jason Cai, Jihwan Oh, Taewon Yun, Yuho Lee |  |
| 120 |  |  [Hybrid Graphs for Table-and-Text based Question Answering using LLMs](https://doi.org/10.18653/v1/2025.naacl-long.39) |  | 0 | Answering questions that require reasoning and aggregation across both structured (tables) and unstructured (raw text) data sources presents significant challenges. Current methods rely on fine-tuning and high-quality, human-curated data, which is difficult to obtain. Recent advances in Large Language Models (LLMs) have shown promising results for multi-hop question answering (QA) over single-source text data in a zero-shot setting, yet exploration into multi-source Table-Text QA remains limited. In this paper, we present a novel Hybrid Graph-based approach for Table-Text QA that leverages LLMs without fine-tuning. Our method constructs a unified Hybrid Graph from textual and tabular data,... | Ankush Agarwal, Chaitanya Devaguptapu, Ganesh S |  |
| 121 |  |  [CFinBench: A Comprehensive Chinese Financial Benchmark for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.40) |  | 0 | Large language models (LLMs) have achieved remarkable performance on various NLP tasks, yet their potential in more challenging task like finance, has not been fully explored. In this paper, we present CFinBench: a meticulously crafted, the most comprehensive evaluation benchmark to date, for assessing the financial knowledge of LLMs under Chinese context. In practice, to better align with the career trajectory of Chinese financial practitioners, we build a systematic evaluation from 4 first-level categories: (1) Financial Subject: whether LLMs can memorize the necessary basic knowledge of financial subjects, such as economics, statistics and auditing. (2) Financial Qualification: whether... | Binfan Zheng, Binwei Yan, Dacheng Tao, Hao Liu, Haoyu Wang, Qiang Li, Tianyu Guo, Wei He, Weihao Wang, Weijian Sun, Ying Nie, Yunhe Wang |  |
| 122 |  |  [LLM-Based Explicit Models of Opponents for Multi-Agent Games](https://doi.org/10.18653/v1/2025.naacl-long.41) |  | 0 | In multi-agent scenarios, the ability to anticipate and respond to opponents is essential, particularly in environments involving adversarial and collaborative interactions. In this paper, we introduce Explicit Models of Opponents (EMO) based on Large Language Models (LLMs), enabling agents to better predict and adapt to diverse, dynamic multi-agent interactions. Unlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs an individual model for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework. We test EMO alongside several reasoning methods in multi-player deduction games,... | Wanpeng Zhang, Xiaopeng Yu, Zongqing Lu |  |
| 123 |  |  [SeqAR: Jailbreak LLMs with Sequential Auto-Generated Characters](https://doi.org/10.18653/v1/2025.naacl-long.42) |  | 0 | The widespread applications of large language models (LLMs) have brought about concerns regarding their potential misuse. Although aligned with human preference data before release, LLMs remain vulnerable to various malicious attacks. In this paper, we adopt a red-teaming strategy to enhance LLM safety and introduce SeqAR, a simple yet effective framework to design jailbreak prompts automatically. The SeqAR framework generates and optimizes multiple jailbreak characters and then applies sequential jailbreak characters in a single query to bypass the guardrails of the target LLM. Different from previous work which relies on proprietary LLMs or seed jailbreak templates crafted by human... | Guanhua Chen, Hailiang Huang, Hongru Wang, Xin Lu, Xuetao Wei, Yan Yang, Yun Chen, Zeguan Xiao |  |
| 124 |  |  [JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.43) |  | 0 |  | Atsuyuki Miyai, Graham Neubig, Jeonghun Baek, Kazuki Egashira, Kiyoharu Aizawa, Shota Onohara, Xiang Yue, Yuki Imajuku |  |
| 125 |  |  [EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction](https://doi.org/10.18653/v1/2025.naacl-long.44) |  | 0 | There has been a rising interest in utilizing tools in applications of autonomous agents based on large language models (LLMs) to address intricate real-world tasks. To develop LLMbased agents, it usually requires LLMs to understand many tool functions from different tool documentations. However, these documentations could be diverse, redundant, or incomplete, which immensely affects the capability of LLMs in using tools. Current LLMs exhibit satisfactory instruction-following capabilities based on instruction-following fine-tuning process. Motivated by this, in this paper, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise... | Deqing Yang, Dongsheng Li, Jiangjie Chen, Kaitao Song, Kan Ren, Siyu Yuan, Xu Tan, Yongliang Shen |  |
| 126 |  |  [Decoding Hate: Exploring Language Models' Reactions to Hate Speech](https://doi.org/10.18653/v1/2025.naacl-long.45) |  | 0 | Hate speech is a harmful form of online expression, often manifesting as derogatory posts. It is a significant risk in digital environments. With the rise of Large Language Models (LLMs), there is concern about their potential to replicate hate speech patterns, given their training on vast amounts of unmoderated internet data. Understanding how LLMs respond to hate speech is crucial for their responsible deployment. However, the behaviour of LLMs towards hate speech has been limited compared. This paper investigates the reactions of seven state-of-the-art LLMs (LLaMA 2, Vicuna, LLaMA 3, Mistral, GPT-3.5, GPT-4, and Gemini Pro) to hate speech. Through qualitative analysis, we aim to reveal... | Javier Parapar, Paloma Piot |  |
| 127 |  |  [Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations](https://doi.org/10.18653/v1/2025.naacl-long.46) |  | 0 | Humans are efficient language learners and inherently social creatures. Our language development is largely shaped by our social interactions, for example, the demonstration and feedback from caregivers. Contrary to human language learning, recent advancements in large language models have primarily adopted a non-interactive training paradigm, and refined pre-trained models through feedback afterward. In this work, we explore how corrective feedback from interactions influences neural language acquisition from scratch through systematically controlled experiments, assessing whether it contributes to word learning efficiency in language models. We introduce a trial-and-demonstration (TnD)... | Joyce Chai, Zekun Wang, Ziqiao Ma |  |
| 128 |  |  [MoCE: Adaptive Mixture of Contextualization Experts for Byte-based Neural Machine Translation](https://doi.org/10.18653/v1/2025.naacl-long.47) |  | 0 | Byte-based machine translation systems have shown significant potential in massively multilingual settings. Unicode encoding, which maps each character to specific byte(s), eliminates the emergence of unknown words, even in new languages, enabling broad language scalability. However, byte-level tokenization results in sequences that are hard to interpret due to limited semantic information per byte. Local contextualization has proven effective in assigning initial semantics to tokens, improving sentence comprehension. Nevertheless, variations in encoding rules across languages necessitate an adaptive approach for effective contextualization. To this end, we propose Adaptive... | Langlin Huang, Mengyu Bu, Yang Feng |  |
| 129 |  |  [LLM-Human Pipeline for Cultural Grounding of Conversations](https://doi.org/10.18653/v1/2025.naacl-long.48) |  | 0 | Conversations often adhere to well-understood social norms that vary across cultures. For example, while addressing parents by name is commonplace in the West, it is rare in most Asian cultures. Adherence or violation of such norms often dictates the tenor of conversations. Humans are able to navigate social situations requiring cultural awareness quite adeptly. However, it is a hard task for NLP models.In this paper, we tackle this problem by introducing a Cultural Context Schema for conversations. It comprises (1) conversational information such as emotions, dialogue acts, etc., and (2) cultural information such as social norms, violations, etc. We generate ~110k social norm and... | Dan Goldwasser, Rajkumar Pujari |  |
| 130 |  |  [ACCESS : A Benchmark for Abstract Causal Event Discovery and Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.49) |  | 0 |  | Gholamreza Haffari, LayKi Soon, Lizhen Qu, Songhai Fan, Tao Feng, Tim Dwyer, Vy Vo, Xiaoxi Kang, Yuncheng Hua |  |
| 131 |  |  [Unmasking Implicit Bias: Evaluating Persona-Prompted LLM Responses in Power-Disparate Social Scenarios](https://doi.org/10.18653/v1/2025.naacl-long.50) |  | 0 | Large language models (LLMs) have demonstrated remarkable capabilities in simulating human behaviour and social intelligence. However, they risk perpetuating societal biases, especially when demographic information is involved. We introduce a novel framework using cosine distance to measure semantic shifts in responses and an LLM-judged Preference Win Rate (WR) to assess how demographic prompts affect response quality across power-disparate social scenarios. Evaluating five LLMs over 100 diverse social scenarios and nine demographic axes, our findings suggest a “default persona” bias toward middle-aged, able-bodied, native-born, Caucasian, atheistic males with centrist views. Moreover,... | Bryan Chen Zhengyu Tan, Roy KaWei Lee |  |
| 132 |  |  [GloCOM: A Short Text Neural Topic Model via Global Clustering Context](https://doi.org/10.18653/v1/2025.naacl-long.51) |  | 0 | Uncovering hidden topics from short texts is challenging for traditional and neural models due to data sparsity, which limits word co-occurrence patterns, and label sparsity, stemming from incomplete reconstruction targets. Although data aggregation offers a potential solution, existing neural topic models often overlook it due to time complexity, poor aggregation quality, and difficulty in inferring topic proportions for individual documents. In this paper, we propose a novel model, \*\*GloCOM\*\* (\*\*Glo\*\*bal \*\*C\*\*lustering C\*\*O\*\*ntexts for Topic \*\*M\*\*odels), which addresses these challenges by constructing aggregated global clustering contexts for short documents,... | Duc Anh Nguyen, Linh Ngo Van, Quang Duc Nguyen, Sang Dinh, Thien Huu Nguyen, Tung Nguyen |  |
| 133 |  |  [Reversed Attention: On The Gradient Descent Of Attention Layers In GPT](https://doi.org/10.18653/v1/2025.naacl-long.52) |  | 0 | The success of Transformer-based Language Models (LMs) stems from their attention mechanism. While this mechanism has been extensively studied in explainability research, particularly through the attention values obtained during the forward pass of LMs, the backward pass of attention has been largely overlooked.In this work, we study the mathematics of the backward pass of attention, revealing that it implicitly calculates an attention matrix we refer to as “Reversed Attention”.We visualized Reversed Attention and examine its properties, demonstrating its ability to elucidate the models’ behavior and edit dynamics.In an experimental setup, we showcase the ability of Reversed Attention to... | Lior Wolf, Shahar Katz |  |
| 134 |  |  [Self-Harmonized Chain of Thought](https://doi.org/10.18653/v1/2025.naacl-long.53) |  | 0 | Chain-of-thought (CoT) prompting has demonstrated the capacity of large language models to perform complex reasoning through intermediate steps. While effective, current CoT methods face challenges: Zero-shot-CoT can lead to reasoning errors, and Few-shot-CoT requires labor-intensive manual demonstrations. Auto-CoT attempts to address these issues by automatically generating diverse demonstrations, but this diversity can lead to inconsistent reasoning patterns. We propose ECHO (Self-Harmonized Chain of Thought), a novel method that unifies diverse solution paths into a consistent and effective reasoning pattern. ECHO employs an iterative process to refine and harmonize automatically... | Wei Lu, Ziqi Jin |  |
| 135 |  |  [AnaScore: Understanding Semantic Parallelism in Proportional Analogies](https://doi.org/10.18653/v1/2025.naacl-long.54) |  | 0 | Formulaic criteria for proportional analogies, which capture relational mappings between two ratios of terms, are mainly confined to the formal level. As analogy datasets grow more complex, especially in evaluating the cognitive abilities of Large Language Models (LLMs), assessing parallelism in them becomes increasingly challenging and often requires human annotation. In this work, we propose AnaScore, an automatic metric for evaluating the strength of semantic parallelism in sentence analogies. AnaScore systematically provides formalized explanations for shared relational patterns at the level of conceptual knowledge. We apply AnaScore to annotate several existing datasets, considering... | Haotong Wang, Liyan Wang, Yves Lepage |  |
| 136 |  |  [Generating Complex Question Decompositions in the Face of Distribution Shifts](https://doi.org/10.18653/v1/2025.naacl-long.55) |  | 0 | Question decomposition has been found to help large language models’ (LLMs) performance on complex question answering (QA) by breaking these questions into simpler sub-questions for answering. Nonetheless, performance on the task remains dominated by supervised approaches, suggesting room for making LLMs better decomposers. One way of improving LLM training and fine-tuning is to leverage synthetic training data, but the superior performance of supervised approaches collapses in the face of distribution shifts, making them unsuitable for generating synthetic data across new domains and at scale. To address this, we propose an approach to generate synthetic decomposition data with only five... | Claire Gardent, Kelvin Han |  |
| 137 |  |  [Diversify-verify-adapt: Efficient and Robust Retrieval-Augmented Ambiguous Question Answering](https://doi.org/10.18653/v1/2025.naacl-long.56) |  | 0 | The retrieval augmented generation (RAG) framework addresses an ambiguity in user queries in QA systems by retrieving passages that cover all plausible interpretations and generating comprehensive responses based on the passages. However, our preliminary studies reveal that a single retrieval process often suffers from low-quality results, as the retrieved passages frequently fail to capture all plausible interpretations. Although the iterative RAG approach has been proposed to address this problem, it comes at the cost of significantly reduced efficiency. To address these issues, we propose the diversify-verify-adapt (DIVA) framework. DIVA first diversifies the retrieved passages to... | Chanyoung Park, Md. Mehrab Tanjim, Ritwik Sinha, Ryan A. Rossi, Sungchul Kim, Tong Yu, Yeonjun In |  |
| 138 |  |  [Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical Ability Assessment of LLM-Powered AI Tutors](https://doi.org/10.18653/v1/2025.naacl-long.57) |  | 0 | In this paper, we investigate whether current state-of-the-art large language models (LLMs) are effective as AI tutors and whether they demonstrate pedagogical abilities necessary for good AI tutoring in educational dialogues. Previous efforts towards evaluation have beenlimited to subjective protocols and benchmarks. To bridge this gap, we propose a unified evaluation taxonomy with eight pedagogical dimensions based on key learning sciences principles, which is designed to assess the pedagogical value of LLM-powered AI tutor responses grounded in student mistakes or confusions in the mathematical domain. We release MRBench – a new evaluation benchmark containing 192 conversations and... | Ekaterina Kochmar, KV Aditya Srivatsa, Kaushal Kumar Maurya, Kseniia Petukhova |  |
| 139 |  |  [Where is the answer? An empirical study of positional bias for parametric knowledge extraction in language model](https://doi.org/10.18653/v1/2025.naacl-long.58) |  | 0 | Language model (LM) stores diverse factual knowledge in their parameters, which is learned during self-supervised training on unlabeled documents and is made extractable by instruction-tuning. For knowledge-intensive tasks, it is essential to memorize information in a way that makes it extractable from LM’s parameters with diverse queries. However, LMs suffer from a phenomenon called “perplexity curse”; despite minimizing document perplexity during training, LMs struggle to extract information via a question prompt. In this paper, we study the problem by fine-tuning LMs for new data and find a very intriguing fact that all studied LMs suffer from positional bias in the training document,... | ChenYu Lee, Kihyuk Sohn, Kuniaki Saito, Yoshitaka Ushiku |  |
| 140 |  |  [Evaluating Morphological Compositional Generalization in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.59) |  | 0 | Large language models (LLMs) have demonstrated significant progress in various natural language generation and understanding tasks. However, their linguistic generalization capabilities remain questionable, raising doubts about whether these models learn language similarly to humans. While humans exhibit compositional generalization and linguistic creativity in language use, the extent to which LLMs replicate these abilities, particularly in morphology, is under-explored. In this work, we systematically investigate the morphological generalization abilities of LLMs through the lens of compositionality. We define morphemes as compositional primitives and design a novel suite of generative... | Abdullatif Köksal, Antoine Bosselut, Bhuwan Dhingra, Defne Circi, Duygu Ataman, Hale Sirin, Jonne Sälevä, Lonneke van der Plas, Mete Ismayilzada |  |
| 141 |  |  [Balancing Forget Quality and Model Utility: A Reverse KL-Divergence Knowledge Distillation Approach for Better Unlearning in LLMs](https://doi.org/10.18653/v1/2025.naacl-long.60) |  | 0 | As concern for privacy rights has grown and the size of language model training datasets has expanded, research into machine unlearning for large language models (LLMs) has become crucial. Before the era of LLMs, research on machine unlearning mainly focused on classification tasks in small parameter models. However, as parameter sizes have grown and unlearning targets have become more complex, unlearning has become more challenging, especially in scenarios involving generation instead of classification, as the output space of such models is significantly larger and more diverse. Existing methods based on gradient ascent and its variants often struggle with balancing forget quality and... | Bichen Wang, Bing Qin, Yanyan Zhao, Yixin Sun, Yuzhe Zi |  |
| 142 |  |  [AgentMove: A Large Language Model based Agentic Framework for Zero-shot Next Location Prediction](https://doi.org/10.18653/v1/2025.naacl-long.61) |  | 0 | Next location prediction plays a crucial role in various real-world applications. Recently, due to the limitation of existing deep learning methods, attempts have been made to apply large language models (LLMs) to zero-shot next location prediction task. However, they directly generate the final output using LLMs without systematic design, which limits the potential of LLMs to uncover complex mobility patterns and underestimates their extensive reserve of global geospatial knowledge. In this paper, we introduce AgentMove, a systematic agentic prediction framework to achieve generalized next location prediction. In AgentMove, we first decompose the mobility prediction task and design... | Jie Feng, Jie Zhao, Yong Li, Yuwei Du |  |
| 143 |  |  [Embedding derived animacy rankings offer insights into the sources of grammatical animacy](https://doi.org/10.18653/v1/2025.naacl-long.62) |  | 0 | In this study, we applied the semantic projection approach to animacy, a feature that has not been previously explored using this method. We compared the relative animacy rankings of nouns denoting animals, humans, objects, and first-, second-, and third-person pronouns, as derived from word embeddings, with rankings derived from human behavioral ratings of animacy and from grammatical patterns. Our results support the semantic projection approach as an effective method for deriving proxies of human perception from word embeddings and offer insights into the sources of grammatical animacy. | Vivian G. Li |  |
| 144 |  |  [Generating Long-form Story Using Dynamic Hierarchical Outlining with Memory-Enhancement](https://doi.org/10.18653/v1/2025.naacl-long.63) |  | 0 | Long-form story generation task aims to produce coherent and sufficiently lengthy text, essential for applications such as novel writingand interactive storytelling. However, existing methods, including LLMs, rely on rigid outlines or lack macro-level planning, making it difficult to achieve both contextual consistency and coherent plot development in long-form story generation. To address this issues, we propose Dynamic Hierarchical Outlining with Memory-Enhancement long-form story generation method, named DOME, to generate the long-form story with coherent content and plot. Specifically, the Dynamic Hierarchical Outline(DHO) mechanism incorporates the novel writing theory into outline... | Daiyuan Li, Jinwu Hu, Mingkui Tan, Qianyue Wang, Yu Hu, Yufeng Wang, Zhengping Li |  |
| 145 |  |  [Little Giants: Synthesizing High-Quality Embedding Data at Scale](https://doi.org/10.18653/v1/2025.naacl-long.64) |  | 0 | Synthetic data generation has become an increasingly popular way of training models without the need for large, manually labeled datasets. For tasks like text embedding, synthetic data offers diverse and scalable training examples, significantly reducing the cost of human annotation. However, most current approaches rely heavily on proprietary models like GPT-4, which are expensive and inefficient for generating large-scale embedding data. In this paper, we introduce SPEED, a framework that aligns open-source small models (8B) to efficiently generate large-scale synthetic embedding data. Through supervised fine-tuning, preference optimization, and self-improvement, SPEED enables small... | Furu Wei, Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Zhicheng Dou, Ziliang Zhao |  |
| 146 |  |  [Can LLMs Convert Graphs to Text-Attributed Graphs?](https://doi.org/10.18653/v1/2025.naacl-long.65) |  | 0 | Graphs are ubiquitous structures found in numerous real-world applications, such as drug discovery, recommender systems, and social network analysis. To model graph-structured data, graph neural networks (GNNs) have become a popular tool. However, existing GNN architectures encounter challenges in cross-graph learning where multiple graphs have different feature spaces. To address this, recent approaches introduce text-attributed graphs (TAGs), where each node is associated with a textual description, which can be projected into a unified feature space using textual encoders. While promising, this method relies heavily on the availability of text-attributed graph data, which is difficult... | Chuxu Zhang, Sidney Liu, Tianyi Ma, Yanfang Ye, Zehong Wang, Zheyuan Zhang |  |
| 147 |  |  [Forest for the Trees: Overarching Prompting Evokes High-Level Reasoning in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.66) |  | 0 | Chain-of-thought (CoT) and subsequent methods adopted a deductive paradigm that decomposes the reasoning process, demonstrating remarkable performances across NLP tasks. However, such a paradigm faces the challenge of getting bogged down in low-level semantic details, hindering large language models (LLMs) from correctly understanding, selecting, and compositing conditions. In this work, we present Overarching Prompting (OaP), a simple prompting method that elicits the high-level thinking of LLMs. Specifically, OaP first abstracts the whole problem into a simplified archetype and formulates strategies grounded in concepts and principles, establishing an overarching perspective for guiding... | Hao He, Haoran Liao, Shaohua Hu, Yaohui Jin, Zhihao Zhu |  |
| 148 |  |  [On the Role of Speech Data in Reducing Toxicity Detection Bias](https://doi.org/10.18653/v1/2025.naacl-long.67) |  | 0 | Text toxicity detection systems exhibit significant biases, producing disproportionate rates of false positives on samples mentioning demographic groups. But what about toxicity detection in speech? To investigate the extent to which text-based biases are mitigated by speech-based systems, we produce a set of high-quality group annotations for the multilingual MuTOX dataset, and then leverage these annotations to systematically compare speech- and text-based toxicity classifiers. Our findings indicate that access to speech data during inference supports reduced bias against group mentions, particularly for ambiguous and disagreement-inducing samples. Our results also suggest that improving... | Adina Williams, Christophe Ropers, Eduardo Sánchez, Levent Sagun, Mariano Coria Meglioli, Marta R. Costajussà, Megan Richards, Samuel J. Bell, Skyler Wang |  |
| 149 |  |  [ITALIC: An Italian Culture-Aware Natural Language Benchmark](https://doi.org/10.18653/v1/2025.naacl-long.68) |  | 0 | We present ITALIC, a large-scale benchmark dataset of 10,000 multiple-choice questions designed to evaluate the natural language understanding of the Italian language and culture. ITALIC spans 12 domains, exploiting public tests to score domain experts in real-world scenarios. We detail our data collection process, stratification techniques, and selection strategies. ITALIC provides a comprehensive assessment suite that captures commonsense reasoning and linguistic proficiency in a morphologically rich language. We establish baseline performances using 17 state-of-the-art LLMs, revealing current limitations in Italian language understanding and highlighting significant linguistic... | Andrea Seveso, Daniele Potertì, Edoardo Federici, Fabio Mercorio, Mario Mezzanzanica |  |
| 150 |  |  [RAP: A Metric for Balancing Repetition and Performance in Open-Source Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.69) |  | 0 | Large Language Models (LLMs) have significantly advanced natural language processing, but content repetition in open-source LLMs remains a critical challenge that adversely affects user experience. The repetition penalty parameter (RPP) aims to mitigate this issue by preventing repeated content generation, but excessive use of RPP can compromise the overall quality. In this paper, we propose Repetition-Aware Performance (RAP), a novel evaluation metric that quantifies and integrates repetition penalty into the assessment of model performance, enabling tuning of RPP. We evaluate our approach using twelve open-source LLMs, ranging from 2 billion to 70 billion parameters, tested on question... | Donghao Huang, Fiona Liausvia, ThanhSon Nguyen, Zhaoxia Wang |  |
| 151 |  |  [Improving Data Annotation for Low-Resource Relation Extraction with Logical Rule-Augmented Collaborative Language Models](https://doi.org/10.18653/v1/2025.naacl-long.70) |  | 0 | Low-resource relation extraction aims to identify semantic relationships between entities using scarce labeled data. Recent studies exploit large language models to recognize relations based on retrieved examplars, yielding promising results. However, the reliability of predictions from these methods is constrained by the presence of irrelevant context within demonstrations and the inherent flaws of large language models in producing undesired outputs. Inspired by the precision and generalization of abstract logic, in this paper, we propose distilling logical rules to uniformly represent task knowledge sourced from distinct origins and facilitate deductive reasoning. We develop a... | Baowen Xu, Chunming Hu, Junfan Chen, Richong Zhang, Xiyang Liu |  |
| 152 |  |  [CompAct: Compressed Activations for Memory-Efficient LLM Training](https://doi.org/10.18653/v1/2025.naacl-long.71) |  | 0 | We introduce CompAct, a technique that reduces peak memory utilization on GPU by 25-30% for pretraining and 50% for fine-tuning of LLMs. Peak device memory is a major limiting factor in training LLMs, with various recent works aiming to reduce model memory. However most works don’t target the largest component of allocated memory during training: the model’s compute graph, which is stored for the backward pass. By storing low-rank, compressed activations to be used in the backward pass we greatly reduce the required memory, unlike previous methods which only reduce optimizer overheads or the number of trained parameters. Our compression uses random projection matrices, thus avoiding... | Assaf Schuster, Nitzan Hodos, Yara Shamshoum, Yuval Sieradzki |  |
| 153 |  |  [Large Language Models Are Cross-Lingual Knowledge-Free Reasoners](https://doi.org/10.18653/v1/2025.naacl-long.72) |  | 0 | Large Language Models have demonstrated impressive reasoning capabilities across multiple languages. However, the relationship between capabilities in different languages is less explored. In this work, we decompose the process of reasoning tasks into two separated components: knowledge retrieval and knowledge-free reasoning, and analyze the relationship between cross-lingual transferability and these two components. With adapted commonsense reasoning datasets and constructed knowledge-free reasoning datasets, we show that the knowledge-free reasoning capability can be nearly perfectly transferred across various source-target language directions despite the secondary impact of resource in... | Changjiang Gao, Chao Deng, Junlan Feng, Peng Hu, Shujian Huang, Sizhe Liu, Xin Huang, Xue Han |  |
| 154 |  |  [What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to Prompt Engineering](https://doi.org/10.18653/v1/2025.naacl-long.73) |  | 0 | Large Language Models (LLMs) changed the way we design and interact with software systems. Their ability to process and extract information from text has drastically improved productivity in a number of routine tasks. Developers that want to include these models in their software stack, however, face a dreadful challenge: debugging LLMs’ inconsistent behavior across minor variations of the prompt. We therefore introduce two metrics for classification tasks, namely \*sensitivity\* and \*consistency\*, which are complementary to task performance. First, sensitivity measures changes of predictions across rephrasings of the prompt, and does not require access to ground truth labels. Instead,... | Davide Sanvito, Federico Errica, Giuseppe Siracusano, Roberto Bifulco |  |
| 155 |  |  [Detect, Disambiguate, and Translate: On-Demand Visual Reasoning for Multimodal Machine Translation with Large Vision-Language Models](https://doi.org/10.18653/v1/2025.naacl-long.74) |  | 0 | Multimodal machine translation (MMT) aims to leverage additional modalities to assist in language translation. With limited parallel data, current MMT systems rely heavily on monolingual English captioning data. These systems face three key issues: they often overlook that visual signals are unnecessary in many cases, they lack transparency in how visual information is used for disambiguation when needed, and they have yet to fully explore the potential of large-scale vision-language models (LVLMs) for MMT tasks. To address these issues, we propose the Detect, Disambiguate, and Translate (DeDiT) framework, the first reasoning-based framework for MMT leveraging LVLMs. DeDiT detects... | Avijit Vajpayee, Danyang Liu, Dhruva Patil, Fanjie Kong, Najmeh Sadoughi, Vimal Bhat, Xiaohang Sun, Zhu Liu |  |
| 156 |  |  [Mitigating Hallucinations in Multi-modal Large Language Models via Image Token Attention-Guided Decoding](https://doi.org/10.18653/v1/2025.naacl-long.75) |  | 0 | Multi-modal large language models (MLLMs) integrate the inherent text generation capabilities of large language models with an understanding of other modalities, promising wide applications in open-ended tasks. Despite their success, they often generate plausible but incorrect content. This phenomenon, known as hallucination, significantly impacts their practical deployment. In this paper, we delve into the intrinsic characteristics of hallucination from the perspective of interaction between input and output tokens. We find that the hallucination typically occurs with attention reduction of output tokens to image tokens. Based on this observation, we introduce image Token attention-guided... | Guiguang Ding, Hui Chen, Jungong Han, Mengyao Lyu, Sicheng Zhao, Xinhao Xu, Yizhe Xiong, Zijia Lin |  |
| 157 |  |  [A Multi-modal Large Language Model with Graph-of-Thought for Effective Recommendation](https://doi.org/10.18653/v1/2025.naacl-long.76) |  | 0 | Chain-of-Thought (CoT) prompting has been shown to be effective in guiding Large Language Models (LLMs) to decompose complex tasks into multiple intermediate steps, and constructing a rational reasoning chain for inferring answers. However, the linear nature of CoT falls short from enabling LLMs to effectively handle graph structures, which are essential for personalized recommendation tasks that rely on user-item interaction graphs. To bridge this gap, we introduce GollaRec, which leverages a Graph-of-Thought (GoT) prompting technique in a Multi-modal LLM, namely LLaVA, to effectively exploit the complex structure of the interaction graphs. GollaRec enhances the recommendation... | Iadh Ounis, Zixuan Yi |  |
| 158 |  |  [Investigating Human Values in Online Communities](https://doi.org/10.18653/v1/2025.naacl-long.77) |  | 0 | Studying human values is instrumental for cross-cultural research, enabling a better understanding of preferences and behaviour of society at large and communities therein. To study the dynamics of communities online, we propose a method to computationally analyse values present on Reddit. Our method allows analysis at scale, complementing survey based approaches. We train a value relevance and a value polarity classifier, which we thoroughly evaluate using in-domain and out-of-domain human annotations. Using these, we automatically annotate over nine million posts across 12k subreddits with Schwartz values. Our analysis unveils both previously recorded and novel insights into the values... | Arnav Arora, Isabelle Augenstein, LucieAimée Kaffee, Nadav Borenstein |  |
| 159 |  |  [Pointwise Mutual Information as a Performance Gauge for Retrieval-Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-long.78) |  | 0 | Recent work suggests that large language models enhanced with retrieval-augmented generation are easily influenced by the order in which the retrieved documents are presented to the model when solving tasks such as question answering (QA).However, there is no method to date that exploits this phenomenon to improve generation.To fill this gap, in this study, we show that the pointwise mutual information between a context and a question is an effective gauge for language model performance.Importantly, this gauge does not depend on knowing the answer to the question a priori.Through experiments on two question-answering datasets using a variety of large language models, we find evidence for... | Arianna Bisazza, Jirui Qi, Mrinmaya Sachan, Paul He, Ryan Cotterell, Tianyu Liu |  |
| 160 |  |  [MATO: A Model-Agnostic Training Optimization for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2025.naacl-long.79) |  | 0 | As an important fine-grained sentiment analysis task, aspect sentiment triplet extraction (ASTE) aims to identify three elements, i.e., aspect, opinion and sentiment polarity as a triplet. Advanced ASTE researches have mostly explored triplet-wise ability to achieve superior improvement. However, existing models with strong in-house performances may struggle to generalize to the challenging cases with the diverse expression of inter-triplet and intra-triplet elements. To this end, we propose a \*\*M\*\*odel-\*\*A\*\*gnostic \*\*T\*\*raining \*\*O\*\*ptimization (\*\*MATO\*\*) to improve ASTE model inference consistent with expected results facing triplet element diversity. Specifically, we... | Leqi Zhong, Lin Li, Qing Xie, Shaopeng Tang, Xiaohui Tao |  |
| 161 |  |  [Dynamic Data Mixing Maximizes Instruction Tuning for Mixture-of-Experts](https://doi.org/10.18653/v1/2025.naacl-long.80) |  | 0 | Mixture-of-Experts (MoE) models have shown remarkable capability in instruction tuning, especially when the number of tasks scales. However, previous methods simply merge all training tasks (e.g. creative writing, coding, and mathematics) and apply fixed sampling weights, without considering the importance of different tasks as the model training state changes. In this way, the most helpful data cannot be effectively distinguished, leading to suboptimal model performance. To reduce the potential redundancies of datasets, we make the first attempt and propose a novel dynamic data mixture for MoE instruction tuning. Specifically, inspired by MoE’s token routing preference, we build... | Daize Dong, Jiacheng Ruan, Tong Zhu, Wenliang Chen, Xiaoye Qu, Yu Cheng |  |
| 162 |  |  [EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics](https://doi.org/10.18653/v1/2025.naacl-long.81) |  | 0 | Designing emotionally intelligent conversational systems to provide comfort and advice to people experiencing distress is a compelling area of research. Recently, with advancements in large language models (LLMs), end-to-end dialogue agents without explicit strategy prediction steps have become prevalent. However, implicit strategy planning lacks transparency, and recent studies show that LLMs’ inherent preference bias towards certain socio-emotional strategies hinders the delivery of high-quality emotional support. To address this challenge, we propose decoupling strategy prediction from language generation, and introduce a novel dialogue strategy prediction framework, EmoDynamiX, which... | Chenwei Wan, Chloé Clavel, Matthieu Labeau |  |
| 163 |  |  [ReasVQA: Advancing VideoQA with Imperfect Reasoning Process](https://doi.org/10.18653/v1/2025.naacl-long.82) |  | 0 |  | Dongyan Zhao, Huishuai Zhang, Jiansheng Wei, Jianxin Liang, Xiaojun Meng, Yueqian Wang |  |
| 164 |  |  [Divergent Thoughts toward One Goal: LLM-based Multi-Agent Collaboration System for Electronic Design Automation](https://doi.org/10.18653/v1/2025.naacl-long.83) |  | 0 | Recently, with the development of tool-calling capabilities in large language models (LLMs), these models have demonstrated significant potential for automating electronic design automation (EDA) flows by interacting with EDA tool APIs via EDA scripts.However, considering the limited understanding of EDA tools, LLMs face challenges in practical scenarios where diverse interfaces of EDA tools exist across different platforms.Additionally, EDA flow automation often involves intricate, long-chain tool-calling processes, increasing the likelihood of errors in intermediate steps.Any errors will lead to the instability and failure of EDA flow automation.To address these challenges, we introduce... | Bei Yu, Haisheng Zheng, Haoyuan Wu, Zhuolun He |  |
| 165 |  |  [A Survey of QUD Models for Discourse Processing](https://doi.org/10.18653/v1/2025.naacl-long.84) |  | 0 | Question Under Discussion (QUD), which is originally a linguistic analytic framework, gains increasing attention in the community of natural language processing over the years. Various models have been proposed for implementing QUD for discourse processing. This survey summarizes these models, with a focus on application to written texts, and examines studies that explore the relationship between QUD and mainstream discourse frameworks, including RST, PDTB and SDRT. Some questions that may require further study are suggested. | Yingxue Fu |  |
| 166 |  |  [SafetyQuizzer: Timely and Dynamic Evaluation on the Safety of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.85) |  | 0 | With the expansion of the application of Large Language Models (LLMs), concerns about their safety have grown among researchers. Numerous studies have demonstrated the potential risks of LLMs generating harmful content and have proposed various safety assessment benchmarks to evaluate these risks. However, the evaluation questions in current benchmarks, especially for Chinese, are too straightforward, making them easily rejected by target LLMs, and difficult to update with practical relevance due to their lack of correlation with real-world events. This hinders the effective application of these benchmarks in continuous evaluation tasks. To address these limitations, we propose... | Hao Zhang, Huawei Shen, Jie Zhang, Shaoling Jing, Xueqi Cheng, Yi Cheng, Yuanzhuo Wang, Zhichao Shi |  |
| 167 |  |  [Privacy Checklist: Privacy Violation Detection Grounding on Contextual Integrity Theory](https://doi.org/10.18653/v1/2025.naacl-long.86) |  | 0 | Privacy research has attracted wide attention as individuals worry that their private data can be easily leaked during interactions with smart devices, social platforms, and AI applications. Existing works mostly consider privacy attacks and defenses on various sub-fields. Within each field, various privacy attacks and defenses are studied to address patterns of personally identifiable information (PII). In this paper, we argue that privacy is not solely about PII patterns. We ground on the Contextual Integrity (CI) theory which posits that people’s perceptions of privacy are highly correlated with the corresponding social context. Based on such an assumption, we formulate privacy as a... | Cheng Jiayang, Haoran Li, Peizhao Hu, Tianshu Chu, Wei Fan, Xuebing Zhou, Yangqiu Song, Yulin Chen |  |
| 168 |  |  [Investigating the (De)Composition Capabilities of Large Language Models in Natural-to-Formal Language Conversion](https://doi.org/10.18653/v1/2025.naacl-long.87) |  | 0 | Humans have strong capabilities of decomposition and composition in natural-to-formal language conversion (N2F) when faced with an unfamiliar formal language, and can easily cope with compositional gaps and counter-intuitive symbolic names. To investigate whether large language models (LLMs) have this set of basic capabilities in N2F, we propose the STD framework. This framework semi-automatically performs sample and task construction, allowing decoupled evaluation of the set of decomposition and composition capabilities of LLMs in N2F. Based on this framework, we evaluate and analyze the most advanced LLMs, and the main findings include that: (1) the LLMs are deficient in both... | Houfeng Wang, Ziyao Xu |  |
| 169 |  |  [Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring](https://doi.org/10.18653/v1/2025.naacl-long.88) |  | 0 | Large language model (LLM) safety is a critical issue, with numerous studies employing red team testing to enhance model security. Among these, jailbreak methods explore potential vulnerabilities by crafting malicious prompts that induce model outputs contrary to safety alignments. Existing black-box jailbreak methods often rely on model feedback, repeatedly submitting queries with detectable malicious instructions during the attack search process. Although these approaches are effective, the attacks may be intercepted by content moderators during the search process. We propose an improved transfer attack method that guides malicious prompt construction by locally training a mirror model... | Han He, Honglin Mu, Libo Qin, Qi Shi, Qingfu Zhu, Wanxiang Che, Xiaoming Shi, Xudong Han, Yang Xu, Yunlong Feng, Yuxin Zhou, Zeming Liu |  |
| 170 |  |  [VividMed: Vision Language Model with Versatile Visual Grounding for Medicine](https://doi.org/10.18653/v1/2025.naacl-long.89) |  | 0 | Recent advancements in Vision Language Models (VLMs) have demonstrated remarkable promise in generating visually grounded responses. However, their application in the medical domain is hindered by unique challenges. For instance, most VLMs rely on a single method of visual grounding, whereas complex medical tasks demand more versatile approaches. Additionally, while most VLMs process only 2D images, a large portion of medical images are 3D. The lack of medical data further compounds these obstacles. To address these challenges, we present VividMed, a vision language model with versatile visual grounding for medicine. Our model supports generating both semantic segmentation masks and... | Bingda Tang, Lingxiao Luo, Rong Han, Ting Chen, Xuanzhong Chen |  |
| 171 |  |  [Mixture of Multimodal Adapters for Sentiment Analysis](https://doi.org/10.18653/v1/2025.naacl-long.90) |  | 0 | Pre-trained language model (PLM) have achieved great success in text sentiment analysis. However, in practical applications, sentiment is not only conveyed through language but also hidden in other modalities. Therefore, multimodal sentiment analysis (MSA) has attracted increasing research interest. Compared to text sentiment analysis, MSA is challenging since (1) emotions hidden in body movements or vocal timbres eclipse traditional analytical methods, and (2) transferring PLM to MSA task requires huge training parameters. Thus, to solve these issues, we introduce the Mixture of Multimodal Adapters (MMA) into the PLM. Specifically, we first design a mixture-of-multimodal-experts module to... | Huixia Ben, Kezhou Chen, Shengeng Tang, Shuo Wang, Yanbin Hao |  |
| 172 |  |  [The Impact of Inference Acceleration on Bias of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.91) |  | 0 | Last few years have seen unprecedented advances in capabilities of Large Language Models (LLMs). These advancements promise to benefit a vast array of application domains. However, due to their immense size, performing inference with LLMs is both costly and slow. Consequently, a plethora of recent work has proposed strategies to enhance inference efficiency, e.g., quantization, pruning, and caching. These acceleration strategies reduce the inference cost and latency, often by several factors, while maintaining much of the predictive performance measured via common benchmarks. In this work, we explore another critical aspect of LLM performance: demographic bias in model generations due to... | Elisabeth Kirsten, Ivan Habernal, Muhammad Bilal Zafar, Vedant Nanda |  |
| 173 |  |  [AfriHate: A Multilingual Collection of Hate Speech and Abusive Language Datasets for African Languages](https://doi.org/10.18653/v1/2025.naacl-long.92) |  | 0 | Hate speech and abusive language are global phenomena that need socio-cultural background knowledge to be understood, identified, and moderated. However, in many regions of the Global South, there have been several documented occurrences of (1) absence of moderation and (2) censorship due to the reliance on keyword spotting out of context. Further, high-profile individuals have frequently been at the center of the moderation process, while large and targeted hate speech campaigns against minorities have been overlooked.These limitations are mainly due to the lack of high-quality data in the local languages and the failure to include local communities in the collection, annotation, and... | Abigail Oppong, Abinew Ali Ayele, Andiswa Bukula, Chiamaka Ijeoma Chukwuneke, David Ifeoluwa Adelani, Ebrahim Chekol Jibril, Elyas Abdi Ismail, Esubalew Alemneh, Hagos Tesfahun Gebremichael, Ibrahim Said Ahmad, Idris Abdulmumin, Lilian Diana Awuor Wanzare, Lukman Jibril Aliyu, Meriem Beloucif, Nedjma Ousidhoum, Nelson Odhiambo Onyango, Oumaima Hourrane, Paul Röttger, Rooweither Mabuya, Salomey Osei, Saminu Mohammad Aliyu, Samuel Rutunda, Seid Muhie Yimam, Shamsuddeen Hassan Muhammad, Tadesse Destaw Belay, Tadesse Kebede Guge, Tesfa Tegegne Asfaw |  |
| 174 |  |  [Revealing the Barriers of Language Agents in Planning](https://doi.org/10.18653/v1/2025.naacl-long.93) |  | 0 | Autonomous planning has been an ongoing pursuit since the inception of artificial intelligence. Based on curated problem solvers, early planning agents could deliver precise solutions for specific tasks but lacked generalization. The emergence of large language models (LLMs) and their powerful reasoning capabilities has reignited interest in autonomous planning by automatically generating reasonable solutions for given tasks. However, prior research and our experiments show that current language agents still lack human-level planning abilities. Even the state-of-the-art reasoning model, OpenAI o1, achieves only 15.6% on one of the complex real-world planning benchmarks. This highlights a... | Jian Xie, Jiangjie Chen, Kai Zhang, Kexun Zhang, Lei Li, Siyu Yuan, Yanghua Xiao, Yikai Zhang |  |
| 175 |  |  [You Only Read Once (YORO): Learning to Internalize Database Knowledge for Text-to-SQL](https://doi.org/10.18653/v1/2025.naacl-long.94) |  | 0 | While significant progress has been made on the text-to-SQL task, recent solutions repeatedly encode the same database schema for every question, resulting in unnecessary high inference cost and often overlooking crucial database knowledge. To address these issues, we propose You Only Read Once (YORO), a novel paradigm that directly internalizes database knowledge into the parametric knowledge of a text-to-SQL model during training and eliminates the need for schema encoding during inference. YORO significantly reduces the input token length by 66%-98%. Despite its shorter inputs, our empirical results demonstrate YORO’s competitive performances with traditional systems on three benchmarks... | Henghui Zhu, Hideo Kobayashi, Jiang Guo, Patrick Ng, Peng Shi, Shuaichen Chang, Wuwei Lan, Zhiguo Wang |  |
| 176 |  |  [Option Symbol Matters: Investigating and Mitigating Multiple-Choice Option Symbol Bias of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.95) |  | 0 | Multiple-Choice Question Answering (MCQA) is a widely used task in the evaluation of Large Language Models (LLMs). In this work, we reveal that current LLMs’ performance in MCQA could be heavily influenced by the choice of option symbol sets, due to the option symbol bias. That is, when altering only the option symbols (e.g., A/B/C/D→i/ii/iii/iv), the results could vary sharply, leading to a margin of approximately 10% in accuracy. To uncover the mechanisms behind this, we investigate the internal components of LLMs from a causal perspective. By measuring the causal effects, we identify a small subset of attention heads responsible for the symbol bias. Subsequently, we interpret these key... | Chengzhi Li, Ping Jian, Zhen Yang |  |
| 177 |  |  [DAWN-ICL: Strategic Planning of Problem-solving Trajectories for Zero-Shot In-Context Learning](https://doi.org/10.18653/v1/2025.naacl-long.96) |  | 0 | Zero-shot in-context learning (ZS-ICL) aims to conduct in-context learning (ICL) without using human-annotated demonstrations.Existing ZS-ICL methods either use large language models (LLMs) to generate (input, label) pairs as pseudo-demonstrations or leverage historical pseudo-demonstrations to help solve the current problem.They assume that all problems are from the same task and traverse them in a random order.However, in real-world scenarios, problems usually come from diverse tasks, and only a few belong to the same task.The random traversing order may generate unreliable pseudo-demonstrations and lead to error accumulation.To address this problem, we reformulate ZS-\*\*ICL\*\* as a... | JiRong Wen, Xiaolei Wang, Xin Zhao, Xinyu Tang |  |
| 178 |  |  [LLaSA: Large Language and Structured Data Assistant](https://doi.org/10.18653/v1/2025.naacl-long.97) |  | 0 |  | Bingning Wang, Guang Liu, Jiabei Chen, Jun Zhao, Kang Liu, Shizhu He, Yao Xu, ZengXiangrong ZengXiangrong |  |
| 179 |  |  [Towards Efficient and Multifaceted Computer-assisted Pronunciation Training Leveraging Hierarchical Selective State Space Model and Decoupled Cross-entropy Loss](https://doi.org/10.18653/v1/2025.naacl-long.98) |  | 0 | Prior efforts in building computer-assisted pronunciation training (CAPT) systems often treat automatic pronunciation assessment (APA) and mispronunciation detection and diagnosis (MDD) as separate fronts: the former aims to provide multiple pronunciation aspect scores across diverse linguistic levels, while the latter focuses instead on pinpointing the precise phonetic pronunciation errors made by non-native language learners. However, it is generally expected that a full-fledged CAPT system should perform both functionalities simultaneously and efficiently. In response to this surging demand, we in this work first propose HMamba, a novel CAPT approach that seamlessly integrates APA and... | Berlin Chen, FuAn Chao |  |
| 180 |  |  [Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.99) |  | 0 | High-quality training data has proven crucial for developing performant large language models (LLMs). However, commercial LLM providers disclose few, if any, details about the data used for training. This lack of transparency creates multiple challenges: it limits external oversight and inspection of LLMs for issues such as copyright infringement, it undermines the agency of data authors, and it hinders scientific research on critical issues such as data contamination and data selection. How can we recover what training data is known to LLMs? In this work we demonstrate a new method to identify training data known to proprietary LLMs like GPT-4 without requiring any access to model weights... | Abhilasha Ravichander, Bill Yuchen Lin, Chandra Bhagavatula, Jillian Fisher, Maria Antoniak, Niloofar Mireshghallah, Taylor Sorensen, Ximing Lu, Yejin Choi |  |
| 181 |  |  [An Interpretable and Crosslingual Method for Evaluating Second-Language Dialogues](https://doi.org/10.18653/v1/2025.naacl-long.100) |  | 0 | We analyse the cross-lingual transferability of a dialogue evaluation framework that assesses the relationships between micro-level linguistic features (e.g. backchannels) and macro-level interactivity labels (e.g. topic management), originally designed for English-as-a-second-language dialogues. To this end, we develop CNIMA (\*\*C\*\*hinese \*\*N\*\*on-Native \*\*I\*\*nteractivity \*\*M\*\*easurement and \*\*A\*\*utomation), a Chinese-as-a-second-language labelled dataset with 10K dialogues. We found the evaluation framework to be robust across languages, revealing language-specific and language-universal relationships between micro-level and macro-level features. Next, we propose an... | Carsten Roever, Jey Han Lau, Jing Wu, Jingxuan Wu, Long Lv, Rena Gao, Xuetong Wu |  |
| 182 |  |  [From Allies to Adversaries: Manipulating LLM Tool-Calling through Adversarial Injection](https://doi.org/10.18653/v1/2025.naacl-long.101) |  | 0 | Tool-calling has changed Large Language Model (LLM) applications by integrating external tools, significantly enhancing their functionality across diverse tasks. However, this integration also introduces new security vulnerabilities, particularly in the tool scheduling mechanisms of LLM, which have not been extensively studied. To fill this gap, we present ToolCommander, a novel framework designed to exploit vulnerabilities in LLM tool-calling systems through adversarial tool injection. Our framework employs a well-designed two-stage attack strategy. Firstly, it injects malicious tools to collect user queries, then dynamically updates the injected tools based on the stolen information to... | Dandan Wang, Haowei Wang, Junjie Wang, Mingyang Li, Qing Wang, Rupeng Zhang, Yuekai Huang |  |
| 183 |  |  [COVE: COntext and VEracity prediction for out-of-context images](https://doi.org/10.18653/v1/2025.naacl-long.102) |  | 0 | Images taken out of their context are the most prevalent form of multimodal misinformation. Debunking them requires (1) providing the true context of the image and (2) checking the veracity of the image’s caption. However, existing automated fact-checking methods fail to tackle both objectives explicitly. In this work, we introduce COVE, a new method that predicts first the true COntext of the image and then uses it to predict the VEracity of the caption. COVE beats the SOTA context prediction model on all context items, often by more than five percentage points. It is competitive with the best veracity prediction models on synthetic data and outperforms them on real-world data, showing... | Gabriel Thiem, Iryna Gurevych, Jonathan Tonglet |  |
| 184 |  |  [Discourse-Driven Evaluation: Unveiling Factual Inconsistency in Long Document Summarization](https://doi.org/10.18653/v1/2025.naacl-long.103) |  | 0 | Detecting factual inconsistency for long document summarization remains challenging, given the complex structure of the source article and long summary length. In this work, we study factual inconsistency errors and connect them with a line of discourse analysis. We find that errors are more common in complex sentences and are associated with several discourse features. We propose a framework that decomposes long texts into discourse-inspired chunks and utilizes discourse information to better aggregate sentence-level scores predicted by NLI models. Our approach shows improved performance on top of different model baselines over several evaluation benchmarks, covering rich domains of... | Diane J. Litman, Yang Zhong |  |
| 185 |  |  [Language Models are Crossword Solvers](https://doi.org/10.18653/v1/2025.naacl-long.104) |  | 0 | Crosswords are a form of word puzzle that require a solver to demonstrate a high degree of proficiency in natural language understanding, wordplay, reasoning, and world knowledge, along with adherence to character and length constraints. In this paper we tackle the challenge of solving crosswords with large language models (LLMs). We demonstrate that the current generation of language models shows significant competence at deciphering cryptic crossword clues and outperforms previously reported state-of-the-art (SoTA) results by a factor of 2-3 in relevant benchmarks. We also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids... | Saptarshi Saha, Soumadeep Saha, Sutanoya Chakraborty, Utpal Garain |  |
| 186 |  |  [WHoW: A Cross-domain Approach for Analysing Conversation Moderation](https://doi.org/10.18653/v1/2025.naacl-long.105) |  | 0 | We propose WHoW, an evaluation framework for analyzing the facilitation strategies of moderators across different domains/scenarios by examining their motives (Why), dialogue acts (How) and target speaker (Who). Using this framework, we annotated 5,657 moderation sentences with human judges and 15,494 sentences with GPT-4o from two domains: TV debates and radio panel discussions. Comparative analysis demonstrates the framework’s cross-domain generalisability and reveals distinct moderation strategies: debate moderators emphasise coordination and facilitate interaction through questions and instructions, while panel discussion moderators prioritize information provision and actively... | Jey Han Lau, Lea Frermann, MingBin Chen |  |
| 187 |  |  [Uplifting Lower-Income Data: Strategies for Socioeconomic Perspective Shifts in Large Multi-modal Models](https://doi.org/10.18653/v1/2025.naacl-long.106) |  | 0 | Recent work has demonstrated that the unequal representation of cultures and socioeconomic groups in training data leads to biased Large Multi-modal (LMM) models. To improve LMM model performance on underrepresented data, we propose and evaluate several prompting strategies using non-English, geographic, and socioeconomic attributes. We show that these geographic and socioeconomic integrated prompts favor retrieving topic appearances commonly found in data from low-income households across different countries leading to improved LMM model performance on lower-income data. Our analyses identify and highlight contexts where these strategies yield the most improvements. | Joan Nwatu, Oana Ignat, Rada Mihalcea |  |
| 188 |  |  [MSc-SQL: Multi-Sample Critiquing Small Language Models For Text-To-SQL Translation](https://doi.org/10.18653/v1/2025.naacl-long.107) |  | 0 | Text-to-SQL generation enables non-experts to interact with databases via natural language. Recent advances rely on large closed-source models like GPT-4 that present challenges in accessibility, privacy, and latency. To address these issues, we focus on developing small, efficient, and open-source text-to-SQL models. We demonstrate the benefits of sampling multiple candidate SQL generations and propose our method, MSc-SQL, to critique them using associated metadata. Our sample critiquing model evaluates multiple outputs simultaneously, achieving state-of-the-art performance compared to other open-source models while remaining competitive with larger models at a much lower cost. Full code... | Guangwei Yu, Ilan Gofman, Jesse C. Cresswell, Jiapeng Wu, Noël Vouitsis, Rasa Hosseinzadeh, Satya Krishna Gorti, Zhaoyan Liu |  |
| 189 |  |  [Mitigating Heterogeneity among Factor Tensors via Lie Group Manifolds for Tensor Decomposition Based Temporal Knowledge Graph Embedding](https://doi.org/10.18653/v1/2025.naacl-long.108) |  | 0 | Recent studies have highlighted the effectiveness of tensor decomposition methods in the Temporal Knowledge Graphs Embedding (TKGE) task. However, we found that inherent heterogeneity among factor tensors in tensor decomposition significantly hinders the tensor fusion process and further limits the performance of link prediction. To overcome this limitation, we introduce a novel method that maps factor tensors onto a unified smooth Lie group manifold to make the distribution of factor tensors approximating homogeneous in tensor decomposition. We provide the theoretical proof of our motivation that homogeneous tensors are more effective than heterogeneous tensors in tensor fusion and... | Guanglai Gao, Jiang Li, Xiangdong Su |  |
| 190 |  |  [What Goes Into a LM Acceptability Judgment? Rethinking the Impact of Frequency and Length](https://doi.org/10.18653/v1/2025.naacl-long.109) |  | 0 | When comparing the linguistic capabilities of language models (LMs) with humans using LM probabilities, factors such as the length of the sequence and the unigram frequency of lexical items have a significant effect on LM probabilities in ways that humans are largely robust to. Prior works in comparing LM and human acceptability judgments treat these effects uniformly across models, making a strong assumption that models require the same degree of adjustment to control for length and unigram frequency effects. We propose MORCELA, a new linking theory between LM scores and acceptability judgments where the optimal level of adjustment for these effects is estimated from data via learned... | Graham Neubig, Lindia Tjuatja, Sophie Hao, Tal Linzen |  |
| 191 |  |  [WaveFM: A High-Fidelity and Efficient Vocoder Based on Flow Matching](https://doi.org/10.18653/v1/2025.naacl-long.110) |  | 0 | Flow matching offers a robust and stable approach to training diffusion models. However, directly applying flow matching to neural vocoders can result in subpar audio quality. In this work, we present WaveFM, a reparameterized flow matching model for mel-spectrogram conditioned speech synthesis, designed to enhance both sample quality and generation speed for diffusion vocoders. Since mel-spectrograms represent the energy distribution of waveforms, WaveFM adopts a mel-conditioned prior distribution instead of a standard Gaussian prior to minimize unnecessary transportation costs during synthesis. Moreover, while most diffusion vocoders rely on a single loss function, we argue that... | Tianze Luo, Wenbo Duan, Xingchen Miao |  |
| 192 |  |  [Analyzing and Evaluating Correlation Measures in NLG Meta-Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.111) |  | 0 | The correlation between NLG automatic evaluation metrics and human evaluation is often regarded as a critical criterion for assessing the capability of an evaluation metric. However, different grouping methods and correlation coefficients result in various types of correlation measures used in meta-evaluation. In specific evaluation scenarios, prior work often directly follows conventional measure settings, but the characteristics and differences between these measures have not gotten sufficient attention. Therefore, this paper analyzes 12 common correlation measures using a large amount of real-world data from six widely-used NLG evaluation datasets and 32 evaluation metrics, revealing... | Li Lin, Mingqi Gao, Xiaojun Wan, Xinyu Hu |  |
| 193 |  |  [Cascading Large Language Models for Salient Event Graph Generation](https://doi.org/10.18653/v1/2025.naacl-long.112) |  | 0 | Generating event graphs from long documents is challenging due to the inherent complexity of multiple tasks involved such as detecting events, identifying their relationships, and reconciling unstructured input with structured graphs. Recent studies typically consider all events with equal importance, failing to distinguish salient events crucial for understanding narratives. This paper presents CALLMSAE, a CAscading Large Language Model framework for SAlient Event graph generation, which leverages the capabilities of LLMs and eliminates the need for costly human annotations. We first identify salient events by prompting LLMs to generate summaries, from which salient events are identified.... | Gabriele Pergola, Xingwei Tan, Yulan He, Yuxiang Zhou |  |
| 194 |  |  [Token-Level Density-Based Uncertainty Quantification Methods for Eliciting Truthfulness of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.113) |  | 0 | Uncertainty quantification (UQ) is a prominent approach for eliciting truthful answers from large language models (LLMs). To date, information-based and consistency-based UQ have been the dominant UQ methods for text generation via LLMs. Density-based methods, despite being very effective for UQ in text classification with encoder-based models, have not been very successful with generative LLMs. In this work, we adapt Mahalanobis Distance (MD) – a well-established UQ technique in classification tasks – for text generation and introduce a new supervised UQ method. Our method extracts token embeddings from multiple layers of LLMs, computes MD scores for each token, and uses linear regression... | Alexander Panchenko, Artem Shelmanov, Artem Vazhentsev, Ivan Lazichny, Lyudmila Rvanova, Maxim Panov, Timothy Baldwin |  |
| 195 |  |  [How Can We Diagnose and Treat Bias in Large Language Models for Clinical Decision-Making?](https://doi.org/10.18653/v1/2025.naacl-long.114) |  | 0 | Recent advancements in Large Language Models (LLMs) have positioned them as powerful tools for clinical decision-making, with rapidly expanding applications in healthcare. However, concerns about bias remain a significant challenge in the clinical implementation of LLMs, particularly regarding gender and ethnicity. This research investigates the evaluation and mitigation of bias in LLMs applied to complex clinical cases, focusing on gender and ethnicity biases. We introduce a novel Counterfactual Patient Variations (CPV) dataset derived from the JAMA Clinical ChallengeUsing this dataset, we built a framework for bias evaluation, employing both Multiple Choice Questions (MCQs) and... | Jackie Kay, Kenza Benkirane, María PérezOrtiz |  |
| 196 |  |  [From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks](https://doi.org/10.18653/v1/2025.naacl-long.115) |  | 0 | Large Vision Language Models (LVLMs) achieve great performance on visual-language reasoning tasks, however, the black-box nature of LVLMs hinders in-depth research on the reasoning mechanism. As all images need to be converted into image tokens to fit the input format of large language models (LLMs) along with natural language prompts, sequential visual representation is essential to the performance of LVLMs, and the information flow analysis approach can be an effective tool for determining interactions between these representations. In this paper, we propose integrating attention analysis with LLaVA-CAM, concretely, attention scores highlight relevant regions during forward propagation,... | Chaochen Gu, Chen Shen, Hao Tang, Jieping Ye, Liang Xie, Shaotian Yan, Wenxiao Wang, Xiaofeng Zhang, Xiaosong Yuan, Yihao Quan |  |
| 197 |  |  [Patent-CR: A Dataset for Patent Claim Revision](https://doi.org/10.18653/v1/2025.naacl-long.116) |  | 0 | This paper presents Patent-CR, the first dataset created for the patent claim revision task in English. It includes both initial patent applications rejected by patent examiners and the final granted versions. Unlike normal text revision tasks that predominantly focus on enhancing sentence quality, such as grammar correction and coherence improvement, patent claim revision aims at ensuring the claims meet stringent legal criteria. These criteria are beyond novelty and inventiveness, including clarity of scope, technical accuracy, language precision, and legal robustness. We assess various large language models (LLMs) through professional human evaluation, including general LLMs with... | Lekang Jiang, Pascal A. Scherz, Stefan Goetz |  |
| 198 |  |  [MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs](https://doi.org/10.18653/v1/2025.naacl-long.117) |  | 0 | The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and... | Anna Rumshisky, Furong Huang, Giannis Karamanolakis, Jianhua Lu, Mayank Kulkarni, Victor Soto, Wei Ai, Yuhang Zhou |  |
| 199 |  |  [Fine-Tuned LLMs are "Time Capsules" for Tracking Societal Bias Through Books](https://doi.org/10.18653/v1/2025.naacl-long.118) |  | 0 | Books, while often rich in cultural insights, can also mirror societal biases of their eras—biases that Large Language Models (LLMs) may learn and perpetuate during training. We introduce a novel method to trace and quantify these biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising 593 fictional books across seven decades (1950-2019), to track bias evolution. By fine-tuning LLMs on books from each decade and using targeted prompts, we examine shifts in biases related to gender, sexual orientation, race, and religion. Our findings indicate that LLMs trained on decade-specific books manifest biases reflective of their times, with both gradual trends and notable shifts. For... | Ali Emami, Nikta Gohari Sadr, Robert Morabito, Sangmitra Madhusudan, Skye Reid |  |
| 200 |  |  [Exploring the Cost-Effectiveness of Perspective Taking in Crowdsourcing Subjective Assessment: A Case Study of Toxicity Detection](https://doi.org/10.18653/v1/2025.naacl-long.119) |  | 0 | Crowdsourcing has been increasingly utilized to gather subjective assessment, such as evaluating the toxicity of texts. Since there doesnot exist a single “ground truth” answer for subjective annotations, obtaining annotations to accurately reflect the opinions of differentsubgroups becomes a key objective for these subjective assessment tasks. Traditionally, this objective is accomplished by directly soliciting a large number of annotations from each subgroup, which can be costly especially when annotators of certain subgroups are hard to access. In this paper, using toxicity evaluation as an example, we explore the feasibility of using perspective taking—that is, asking annotators to... | ChienJu Ho, Ming Yin, Xiaoni Duan, Zhuoyan Li |  |
| 201 |  |  [NormAd: A Framework for Measuring the Cultural Adaptability of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.120) |  | 0 | To be effectively and safely deployed to global user populations, large language models (LLMs) may need to adapt outputs to user values and cultures, not just know about them. We introduce NormAd, an evaluation framework to assess LLMs’ cultural adaptability, specifically measuring their ability to judge social acceptability across varying levels of cultural norm specificity, from abstract values to explicit social norms. As an instantiation of our framework, we create NormAd-Eti, a benchmark of 2.6k situational descriptions representing social-etiquette related cultural norms from 75 countries. Through comprehensive experiments on NormAd-Eti, we find that LLMs struggle to accurately judge... | Abhinav Rao, Akhila Yerukola, Katharina Reinecke, Maarten Sap, Vishwa Shah |  |
| 202 |  |  [LiPO: Listwise Preference Optimization through Learning-to-Rank](https://doi.org/10.18653/v1/2025.naacl-long.121) |  | 0 | Aligning language models (LMs) with curated human feedback is critical to control their behaviors in real-world applications. Several recent policy optimization methods, such as DPO and SLiC, serve as promising alternatives to the traditional Reinforcement Learning from Human Feedback (RLHF) approach.In practice, human feedback often comes in a format of a ranked list over multiple responses to amortize the cost of reading prompt. Multiple responses can also be ranked by reward models or AI feedback. There lacks such a thorough study on directly fitting upon a list of responses. In this work, we formulate the LM alignment as a listwise ranking problem and describe the LiPO framework, where... | Jialu Liu, Jiaming Shen, Junru Wu, Misha Khalman, Mohammad Saleh, Peter J. Liu, Rishabh Joshi, Simon Baumgartner, Tianqi Liu, Xuanhui Wang, Yao Zhao, Zhen Qin |  |
| 203 |  |  [Adaptive Prompting: Ad-hoc Prompt Composition for Social Bias Detection](https://doi.org/10.18653/v1/2025.naacl-long.122) |  | 0 | Recent advances on instruction fine-tuning have led to the development of various prompting techniques for large language models, such as explicit reasoning steps. However, the success of techniques depends on various parameters, such as the task, language model, and context provided. Finding an effective prompt is, therefore, often a trial-and-error process. Most existing approaches to automatic prompting aim to optimize individual techniques instead of compositions of techniques and their dependence on the input. To fill this gap, we propose an adaptive prompting approach that predicts the optimal prompt composition ad-hoc for a given input. We apply our approach to social bias... | Barbara Hammer, Eyke Hüllermeier, Fabian Fumagalli, Henning Wachsmuth, Maximilian Muschalik, Maximilian Spliethöver, Tim Knebler |  |
| 204 |  |  [Enhancing Discriminative Representation in Similar Relation Clusters for Few-Shot Continual Relation Extraction](https://doi.org/10.18653/v1/2025.naacl-long.123) |  | 0 | Few-shot Continual Relation Extraction (FCRE) has emerged as a significant challenge in information extraction, necessitating that relation extraction (RE) systems can sequentially identify new relations with limited labeled samples. While existing studies have demonstrated promising results in FCRE, they often overlook the issue of similar relations, which is a critical factor contributing to catastrophic forgetting. In this work, we propose Sirus–a novel method that utilizes relation descriptions and dynamic clustering on these descriptions to identify similar relations. Leveraging this information, we introduce innovative loss functions specifically designed to enhance the distinction... | Anh Duc Le, Linh Ngo Van, Nam Le Hai, Nguyen Thi Ngoc Diep, Sang Dinh, Thanh Xuan Nguyen, Thien Huu Nguyen |  |
| 205 |  |  [SymBa: Symbolic Backward Chaining for Structured Natural Language Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.124) |  | 0 | To improve the performance and explainability of LLM-based natural language reasoning, structured reasoning can be applied to generate explicitly structured proofs. Among different methods for structured reasoning, we specifically focus on backward chaining, where the proof goal is recursively decomposed to subgoals by searching and applying rules. We argue that current LLM-based backward chaining systems (e.g. Least-to-most prompting and LAMBADA) are incomplete, as they omit crucial algorithmic components identified from the classic backward chaining algorithm in computational logic (SLD Resolution). To this end, we propose a novel backward chaining system, SymBa (Symbolic Backward... | Jinu Lee, Wonseok Hwang |  |
| 206 |  |  [MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference](https://doi.org/10.18653/v1/2025.naacl-long.125) |  | 0 | Long-context Multimodal Large Language Models (MLLMs) that incorporate long text-image and text-video modalities, demand substantial computational resources as their multimodal Key-Value (KV) cache grows with increasing input lengths, challenging memory and time efficiency. For multimodal scenarios, the cross-modal interactions inevitablely increase complexity, and prior methods for KV cache compression, in both text-only and multimodal LLMs, have neglected attention density variations across layers, often adopting uniform or progressive reduction strategis for layer-wise cache allocation. This results in precision loss and suboptimal performance. We propose MEDA, a novel approach... | Che Liu, Hui Shen, Mi Zhang, Xin Wang, Zheda Mai, Zhongwei Wan |  |
| 207 |  |  [Language Models Largely Exhibit Human-like Constituent Ordering Preferences](https://doi.org/10.18653/v1/2025.naacl-long.126) |  | 0 | Though English sentences are typically inflexible vis-à-vis word order, constituents often show far more variability in ordering. One prominent theory presents the notion that constituent ordering is directly correlated with constituent weight: a measure of the constituent’s length or complexity. Such theories are interesting in the context of natural language processing (NLP), because while recent advances in NLP have led to significant gains in the performance of large language models (LLMs), much remains unclear about how these models process language, and how this compares to human language processing. In particular, the question remains whether LLMs display the same patterns with... | Ada Defne Tur, Gaurav Kamath, Siva Reddy |  |
| 208 |  |  [SafeQuant: LLM Safety Analysis via Quantized Gradient Inspection](https://doi.org/10.18653/v1/2025.naacl-long.127) |  | 0 | Contemporary jailbreak attacks on Large Language Models (LLMs) employ sophisticated techniques with obfuscated content to bypass safety guardrails. Existing defenses either use computationally intensive LLM verification or require adversarial fine-tuning, leaving models vulnerable to advanced attacks. We introduce SafeQuant, a novel defense framework that leverages quantized gradient patterns to identify harmful prompts efficiently. Our key insight is that when generating identical responses like “Sure”, LLMs exhibit distinctly different internal gradient patterns for safe versus harmful prompts, reflecting conflicts with safety training. By capturing these patterns through selective... | Manohar Kaul, Rathod Darshan D, Sadbhavana Babar, Sindhu Padakandla |  |
| 209 |  |  [Exploring Large Language Models for Effective Rumor Detection on Social Media](https://doi.org/10.18653/v1/2025.naacl-long.128) |  | 0 | In this paper, we explore using Large Language Models (LLMs) for rumor detection on social media. It involves assessing the veracity of claims on social media based on social context (e.g., comments, propagation patterns). LLMs, despite their impressive capabilities in text-based reasoning tasks, struggle to achieve promising rumor detection performance when facing long structured social contexts. Our preliminary analysis shows that large-scale contexts hinder LLMs’ reasoning abilities, while moderate contexts perform better for LLMs, highlighting the need for refined contexts. Accordingly, we propose a semantic-propagation collaboration-base framework that integrates small language models... | Bibo Cai, Bing Qin, Ting Liu, Xiao Ding, Yirong Zeng |  |
| 210 |  |  [No Simple Answer to Data Complexity: An Examination of Instance-Level Complexity Metrics for Classification Tasks](https://doi.org/10.18653/v1/2025.naacl-long.129) |  | 0 | Natural Language Processing research has become increasingly concerned with understanding data quality and complexity at the instance level. Instance-level complexity scores can be used for tasks such as filtering out noisy observations and subsampling informative examples. However, there exists a diverse taxonomy of complexity metrics that can be used for a classification task, making metric selection itself a difficult task. We empirically examine the relationship between these metrics and find that simply storing training loss provides similar complexity rankings as other more computationally intensive techniques. Metric similarity allows us to subsample data with higher aggregate... | Ahmed Abbasi, John P. Lalor, Ryan A. Cook |  |
| 211 |  |  [NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals](https://doi.org/10.18653/v1/2025.naacl-long.130) |  | 0 | Decomposition of text into atomic propositions is a flexible framework allowing for the closer inspection of input and output text. We use atomic decomposition of hypotheses in two natural language reasoning tasks, traditional NLI and defeasible NLI, to form atomic sub-problems, or granular inferences that models must weigh when solving the overall problem. These atomic sub-problems serve as a tool to further understand the structure of both NLI and defeasible reasoning, probe a model’s consistency and understanding of different inferences, and measure the diversity of examples in benchmark datasets. Our results indicate that LLMs still struggle with logical consistency on atomic NLI and... | Neha Srikanth, Rachel Rudinger |  |
| 212 |  |  [HISTOIRESMORALES: A French Dataset for Assessing Moral Alignment](https://doi.org/10.18653/v1/2025.naacl-long.131) |  | 0 | Aligning language models with human values is crucial, especially as they become more integrated into everyday life. While models are often adapted to user preferences, it is equally important to ensure they align with moral norms and behaviours in real-world social situations. Despite significant progress in languages like English and Chinese, French has seen little attention in this area, leaving a gap in understanding how LLMs handle moral reasoning in this language. To address this gap, we introduce HistoiresMorales, a French dataset derived from MoralStories, created through translation and subsequently refined with the assistance of native speakers to guarantee grammatical accuracy... | Antoine Gourru, Charlotte Laclau, Christophe Gravier, Guillaume Metzler, Irina Proskurina, Julien Velcin, Thibaud Leteno |  |
| 213 |  |  [Leveraging Allophony in Self-Supervised Speech Models for Atypical Pronunciation Assessment](https://doi.org/10.18653/v1/2025.naacl-long.132) |  | 0 | Allophony refers to the variation in the phonetic realization of a phoneme based on its phonetic environment. Modeling allophones is crucial for atypical pronunciation assessment, which involves distinguishing atypical from typical pronunciations. However, recent phoneme classifier-based approaches often simplify this by treating various realizations as a single phoneme, bypassing the complexity of modeling allophonic variation. Motivated by the acoustic modeling capabilities of frozen self-supervised speech model (S3M) features, we propose MixGoP, a novel approach that leverages Gaussian mixture models to model phoneme distributions with multiple subclusters. Our experiments show that... | David R. Mortensen, Eunjung Yeo, Kalvin Chang, Kwanghee Choi, Shinji Watanabe |  |
| 214 |  |  [SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search](https://doi.org/10.18653/v1/2025.naacl-long.133) |  | 0 | Conversational Recommender Systems (CRS) proactively engage users in interactive dialogues to elicit user preferences and provide personalized recommendations. Existing methods train Reinforcement Learning (RL)-based agent with greedy action selection or sampling strategy, and may suffer from suboptimal conversational planning. To address this, we present a novel Monte Carlo Tree Search (MCTS)-based CRS framework SAPIENT. SAPIENT consists of a conversational agent (S-agent) and a conversational planner (S-planner). S-planner builds a conversational search tree with MCTS based on the initial actions proposed by S-agent to find conversation plans. The best conversation plans from S-planner... | Bo Peng, Hanwen Du, Xia Ning |  |
| 215 |  |  [Reliability of Topic Modeling](https://doi.org/10.18653/v1/2025.naacl-long.134) |  | 0 | Topic models allow researchers to extract latent factors from text data and use those variables in downstream statistical analyses. However, these methodologies can vary significantly due to initialization differences, randomness in sampling procedures, or noisy data. Reliability of these methods is of particular concern as many researchers treat learned topic models as ground truth for subsequent analyses. In this work, we show that the standard practice for quantifying topic model reliability fails to capture essential aspects of the variation in two widely-used topic models. Drawing from a extensive literature on measurement theory, we provide empirical and theoretical analyses of three... | Kayla Schroeder, Zach WoodDoughty |  |
| 216 |  |  [Style Transfer with Multi-iteration Preference Optimization](https://doi.org/10.18653/v1/2025.naacl-long.135) |  | 0 | Numerous recent techniques for text style transfer characterize their approaches as variants of reinforcement learning and preference optimization. In this work, we consider the relationship between these approaches and a class of optimization approaches developed primarily for (non-neural) statistical machine translation, formerly known as ‘tuning’. Inspired by these techniques from the past, we improve upon established preference optimization approaches, incorporating multiple iterations of exploration and optimization, and choosing contrastive examples by following a ‘hope’ vs ‘fear’ sampling strategy. Cognizant of the difference between machine translation and style transfer, however,... | Jonathan May, Shuai Liu |  |
| 217 |  |  [DTELS: Towards Dynamic Granularity of Timeline Summarization](https://doi.org/10.18653/v1/2025.naacl-long.136) |  | 0 |  | Chenlong Zhang, Jun Zhao, Kang Liu, Pengfei Cao, Tong Zhou, Yubo Chen, Zhuoran Jin |  |
| 218 |  |  [ALERT: An LLM-powered Benchmark for Automatic Evaluation of Recommendation Explanations](https://doi.org/10.18653/v1/2025.naacl-long.137) |  | 0 | Recommendation explanation systems have become increasingly vital with the widespread adoption of recommender systems. However, existing recommendation explanation evaluation benchmarks suffer from limited item diversity, impractical user profiling requirements, and unreliable and unscalable evaluation protocols. We present ALERT, a model-agnostic recommendation explanation evaluation benchmark. The benchmark comprises three main contributions: 1) a diverse dataset encompassing 15 Amazon e-commerce categories with 2,761 user-item interactions, incorporating implicit preferences through purchase histories;2) two novel LLM-powered automatic evaluators that enable scalable and... | Chenwei Zhang, Jingbo Shang, Kaize Ding, Kyumin Lee, Mao Li, Pei Chen, Tianyi Liu, Trishul Chilimbi, Xian Li, Xinyang Zhang, Yichuan Li, Yifan Gao, Zhengyang Wang, Zhihan Zhang |  |
| 219 |  |  [DETQUS: Decomposition-Enhanced Transformers for QUery-focused Summarization](https://doi.org/10.18653/v1/2025.naacl-long.138) |  | 0 | Query-focused tabular summarization is an emerging task in table-to-text generation that synthesizes a summary response from tabular data based on user queries. Traditional transformer-based approaches face challenges due to token limitations and the complexity of reasoning over large tables. To address these challenges, we introduce DETQUS (Decomposition-Enhanced Transformers for QUery-focused Summarization), a system designed to improve summarization accuracy by leveraging tabular decomposition alongside a fine-tuned encoder-decoder model. DETQUS employs a large language model to selectively reduce table size, retaining only query-relevant columns while preserving essential information.... | Aryaan Shaikh, Bonnie J. Dorr, Jairo Garciga, Justin Ho, Rohan Sharma, Sangpil Youm, Xinlei Wu, Yasir Khan |  |
| 220 |  |  [IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.139) |  | 0 | Despite the widespread adoption of Large language models (LLMs), their remarkable capabilities remain limited to a few high-resource languages. Additionally, many low-resource languages (e.g. African languages) are often evaluated only on basic text classification tasks due to the lack of appropriate or comprehensive benchmarks outside of high-resource languages. In this paper, we introduce IrokoBench—a human-translated benchmark dataset for 17 typologically-diverse low-resource African languages covering three tasks: natural language inference(AfriXNLI), mathematical reasoning(AfriMGSM), and multi-choice knowledge-based QA(AfriMMLU). We use IrokoBench to evaluate zero-shot, few-shot, and... | Andiswa Bukula, Blessing K. Sibanda, Chiamaka Ijeoma Chukwuneke, David Ifeoluwa Adelani, EnShiun Annie Lee, Foutse Yuehgoh, Godson Koffi Kalipe, Happy Buzaaba, Israel Abebe Azime, Jessica Ojo, Jesujoba Oluwadara Alabi, Jian Yun Zhuang, Jonathan Mukiibi, Lolwethu Ndolela, Millicent Ochieng, Mmasibidi Setaka, Nkiruka Odu, Pontus Stenetorp, Rooweither Mabuya, Salomey Osei, Salomon Kabongo Kabenamualu, Sara Hooker, Shamsuddeen Hassan Muhammad, Sokhar Samb, Tadesse Kebede Guge, Tombekai Vangoni Sherman, Xuanli He |  |
| 221 |  |  [The Impact of Domain-Specific Terminology on Machine Translation for Finance in European Languages](https://doi.org/10.18653/v1/2025.naacl-long.140) |  | 0 | Domain-specific machine translation (MT) poses significant challenges due to specialized terminology, particularly when translating across multiple languages with scarce resources. In this study, we present the first impact analysis of domain-specific terminology on multilingual MT for finance, focusing on European languages within the subdomain of macroeconomics. To this end, we construct a multi-parallel corpus from the European Central Bank, aligned across 22 languages. Using this resource, we compare open-source multilingual MT systems with large language models (LLMs) that possess multilingual capabilities. Furthermore, by developing and curating an English financial glossary, we... | Arturo Oncevay, Charese Smiley, Xiaomo Liu |  |
| 222 |  |  [Benchmarking Language Model Creativity: A Case Study on Code Generation](https://doi.org/10.18653/v1/2025.naacl-long.141) |  | 0 | As LLMs become increasingly prevalent, it is interesting to consider how “creative” these models can be. From cognitive science, creativity consists of at least two key characteristics: convergent thinking (purposefulness to achieve a given goal) and divergent thinking (adaptability to explore new environments or constraints) (CITATION). In this work, we introduce a framework for quantifying LLM creativity that incorporates the two design ingredients: (1) We introduce DENIAL PROMPTING which pushes LLMs to develop more creative solutions to a given problem by incrementally imposing new constraints on the previous solution, compelling LLMs to adopt new strategies. (2) We define NEOGAUGE, a... | Daniel Khashabi, Dixuan Wang, Dongwei Jiang, Meng Jiang, Sanjeev Khudanpur, Tianjian Li, Yining Lu |  |
| 223 |  |  [Have LLMs Reopened the Pandora's Box of AI-Generated Fake News?](https://doi.org/10.18653/v1/2025.naacl-long.142) |  | 0 | With the rise of AI-generated content spewed at scale from large language models (LLMs), genuine concerns about the spread of fake news have intensified. The perceived ability of LLMs to produce convincing fake news at scale poses new challenges for both human and automated fake news detection systems. To address this gap, this paper presents the findings from a university-level competition that aimed to explore how LLMs can be used by humans to create fake news, and to assess the ability of human annotators and AI models to detect it. A total of 110 participants used LLMs to create 252 unique fake news stories, and 84 annotators participated in the detection tasks. Our findings indicate... | Amulya Yadav, Bonam Mingole, Hangzhi Guo, S. Shyam Sundar, Sai Dileep Koneru, Sarah Rajtmajer, Wenbo Zhang, Xinyu Wang |  |
| 224 |  |  [Probe-Free Low-Rank Activation Intervention](https://doi.org/10.18653/v1/2025.naacl-long.143) |  | 0 | Language models (LMs) can produce texts that appear accurate and coherent but contain untruthful or toxic content. Inference-time interventions that edit the hidden activations have shown promising results in steering the LMs towards desirable generations. Existing activation intervention methods often comprise an activation probe to detect undesirable generation, triggering the activation modification to steer subsequent generation. This paper proposes a probe-free intervention method FLORAIN for all attention heads in a specific activation layer. It eliminates the need to train classifiers for probing purposes. The intervention function is parametrized by a sample-wise nonlinear low-rank... | Anthony ManCho So, Bao Nguyen, Chonghe Jiang, Viet Anh Nguyen |  |
| 225 |  |  [FactTrack: Time-Aware World State Tracking in Story Outlines](https://doi.org/10.18653/v1/2025.naacl-long.144) |  | 0 | While accurately detecting and correcting factual contradictions in language model outputs has become increasingly important as their capabilities improve, doing so is highly challenging. We propose a novel method, FactTrack, for tracking atomic facts and addressing factual contradictions. Crucially, FactTrack also maintains time-aware validity intervals for each fact, allowing for change over time. At a high level, FactTrack consists of a four-step pipeline to update a world state data structure for each new event: (1) decompose the event into directional atomic facts; (2) determine the validity interval of each atomic fact using the world state; (3) detect contradictions with existing... | Dan Klein, Kevin Yang, Lingpeng Kong, Zhiheng Lyu |  |
| 226 |  |  [A Bayesian Optimization Approach to Machine Translation Reranking](https://doi.org/10.18653/v1/2025.naacl-long.145) |  | 0 | Reranking, or scoring a list of prediction candidates from a machine translation system with an external scoring model and returning the highest-scoring candidate, remains a simple and effective method for improving prediction quality. However, reranking with high quality scoring models can add substantial computational cost to the translation pipeline, which we address in this work by framing list reranking as a Bayesian optimization (BayesOpt) problem over the candidate list, where unknown scores are modeled with a Gaussian process. This algorithm scores candidates iteratively, choosing next candidates by balancing between exploration, choosing to score those that differ from candidates... | Andreas Vlachos, Julius Cheng, Maike Züfle, Vilém Zouhar |  |
| 227 |  |  [Multi-Conditional Ranking with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.146) |  | 0 | Utilizing large language models (LLMs) to rank a set of items has become a common approach in recommendation and retrieval systems. Typically, these systems focus on ordering a substantial number of documents in a monotonic order based on a given query. However, real-world scenarios often present a different challenge: ranking a comparatively smaller set of items, but according to a variety of diverse and occasionally conflicting conditions. In this paper, we define and explore the task of multi-conditional ranking by introducing MCRank, a benchmark tailored for assessing multi-conditional ranking across various item types and conditions. Our analysis of LLMs using MCRank indicates a... | Estevam Hruschka, Pouya Pezeshkpour |  |
| 228 |  |  [ReGLA: Refining Gated Linear Attention](https://doi.org/10.18653/v1/2025.naacl-long.147) |  | 0 | Recent advancements in Large Language Models (LLMs) have set themselves apart with their exceptional performance in complex language modelling tasks. However, these models are also known for their significant computational and storage requirements, primarily due to the quadratic computation complexity of softmax attention. To mitigate this issue, linear attention has been designed to reduce the quadratic space-time complexity that is inherent in standard transformers. In this work, we embarked on a comprehensive exploration of three key components that substantially impact the performance of the Gated Linear Attention module: feature maps, normalization, and the gating mechanism. We... | Boxing Chen, Ivan Kobyzev, Mehdi Rezagholizadeh, Peng Lu, Philippe Langlais |  |
| 229 |  |  [Intrinsic Bias is Predicted by Pretraining Data and Correlates with Downstream Performance in Vision-Language Encoders](https://doi.org/10.18653/v1/2025.naacl-long.148) |  | 0 | While recent work has found that vision-language models trained under the Contrastive Language Image Pre-training (CLIP) framework contain intrinsic social biases, the extent to which different upstream pre-training features of the framework relate to these biases, and hence how intrinsic bias and downstream performance are connected has been unclear. In this work, we present the largest comprehensive analysis to-date of how the upstream pre-training factors and downstream performance of CLIP models relate to their intrinsic biases. Studying 131 unique CLIP models, trained on 26 datasets, using 55 architectures, and in a variety of sizes, we evaluate bias in each model using 26... | Aylin Caliskan, Isaac Slaughter, Kshitish Ghate, Kyra Wilson, Mona T. Diab |  |
| 230 |  |  [Benchmarking Failures in Tool-Augmented Language Models](https://doi.org/10.18653/v1/2025.naacl-long.149) |  | 0 | The integration of tools has extended the capabilities of language models (LMs) beyond vanilla text generation to versatile scenarios. However, tool-augmented language models (TaLMs) often assume ‘perfect’ information access and tool availability, which may not hold in the real world. To systematically study TaLMs imperfections, we introduce the FAIL-TaLMs benchmark, featuring two major failures: under-specified user queries and non-available tools. FAIL-TaLMS contains 1,749 examples using 906 tools across 21 categories, including single- and multi-tool usage. We evaluate top-performing proprietary and open-source models, and find all current models except for Claude struggle to recognize... | Eduardo Treviño, Graham Neubig, Hugo Contant, James Ngai, Zora Zhiruo Wang |  |
| 231 |  |  [Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity Recognition Framework](https://doi.org/10.18653/v1/2025.naacl-long.150) |  | 0 | Clinical named entity recognition (NER) aims to retrieve important entities within clinical narratives. Recent works have demonstrated that large language models (LLMs) can achieve strong performance in this task. While previous works focus on proprietary LLMs, we investigate how open NER LLMs, trained specifically for entity recognition, perform in clinical NER. Our initial experiment reveals significant contrast in performance for some clinical entities and how a simple exploitment on entity types can alleviate this issue. In this paper, we introduce a novel framework, entity decomposition with filtering, or EDF. Our key idea is to decompose the entity recognition task into several... | Reza Averly, Xia Ning |  |
| 232 |  |  [Towards Knowledge Checking in Retrieval-augmented Generation: A Representation Perspective](https://doi.org/10.18653/v1/2025.naacl-long.151) |  | 0 | Retrieval-Augmented Generation (RAG) systems have shown promise in enhancing the performance of Large Language Models (LLMs). However, these systems face challenges in effectively integrating external knowledge with the LLM’s internal knowledge, often leading to issues with misleading or unhelpful information. This work aims to provide a systematic study on knowledge checking in RAG systems. We conduct a comprehensive analysis of LLM representation behaviors and demonstrate the significance of using representations in knowledge checking. Motivated by the findings, we further develop representation-based classifiers for knowledge filtering. We show substantial improvements in RAG... | Bingheng Li, Dante Everaert, Hanqing Lu, Hui Liu, Jiankun Zhang, Jiliang Tang, Monica Xiao Cheng, Shenglai Zeng, Tianqi Zheng, Yue Xing, Yuping Lin |  |
| 233 |  |  [The Power of Many: Multi-Agent Multimodal Models for Cultural Image Captioning](https://doi.org/10.18653/v1/2025.naacl-long.152) |  | 0 | Large Multimodal Models (LMMs) exhibit impressive performance across various multimodal tasks. However, their effectiveness in cross-cultural contexts remains limited due to the predominantly Western-centric nature of most data and models. Conversely, multi-agent models have shown significant capability in solving complex tasks. Our study evaluates the collective performance of LMMs in a multi-agent interaction setting for the novel task of cultural image captioning. Our contributions are as follows: (1) We introduce MosAIC, a Multi-Agent framework to enhance cross-cultural Image Captioning using LMMs with distinct cultural personas; (2) We provide a dataset of culturally enriched image... | Angana Borah, Longju Bai, Oana Ignat, Rada Mihalcea |  |
| 234 |  |  [Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison](https://doi.org/10.18653/v1/2025.naacl-long.153) |  | 0 | Following the remarkable success of Large Language Models (LLMs) in NLP tasks, there is increasing interest in extending their capabilities to speech—the most common form of communication. The most widespread approach to integrating speech into LLMs is dense feature prepending (DFP), which prepends the projected speech representations to the textual representations, allowing end-to-end training with a speech encoder. This raises questions about the need for a sophisticated speech encoder for DFP and how its performance compares with a standard encoder-decoder (i.e., cross-attention) architecture. We compare DFP and cross-attention under a variety of configurations, such as CTC compression,... | Barry Haddow, Luisa Bentivogli, Marco Gaido, Sara Papi, Tsz Kin Lam |  |
| 235 |  |  [CORRECT: Context- and Reference-Augmented Reasoning and Prompting for Fact-Checking](https://doi.org/10.18653/v1/2025.naacl-long.154) |  | 0 | Fact-checking the truthfulness of claims usually requires reasoning over multiple evidence sentences. Oftentimes, evidence sentences may not be always self-contained, and may require additional contexts and references from elsewhere to understand coreferential expressions, acronyms, and the scope of a reported finding. For example, evidence sentences from an academic paper may need contextual sentences in the paper and descriptions in its cited papers to determine the scope of a research discovery. However, most fact-checking models mainly focus on the reasoning within evidence sentences, and ignore the auxiliary contexts and references. To address this problem, we propose a novel method,... | Delvin Ce Zhang, Dongwon Lee |  |
| 236 |  |  [Racing Thoughts: Explaining Contextualization Errors in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.155) |  | 0 | The profound success of transformer-based language models can largely be attributed to their ability to integrate relevant contextual information from an input sequence in order to generate a response or complete a task. However, we know very little about the algorithms that a model employs to implement this capability, nor do we understand their failure modes. For example, given the prompt “John is going fishing, so he walks over to the bank. Can he make an ATM transaction?”, a model may incorrectly respond “Yes” if it has not properly contextualized “bank” as a geographical feature, rather than a financial institution. We propose the LLM Race Conditions Hypothesis as an explanation of... | Asma Ghandeharioun, Michael A. Lepori, Michael Curtis Mozer |  |
| 237 |  |  [DREAM: Improving Video-Text Retrieval Through Relevance-Based Augmentation Using Large Foundation Models](https://doi.org/10.18653/v1/2025.naacl-long.156) |  | 0 | Recent progress in video-text retrieval has been driven largely by advancements in model architectures and training strategies. However, the representation learning capabilities of video-text retrieval models remain constrained by low-quality and limited training data annotations. To address this issue, we present a novel Video-Text Retrieval Paradigm with Relevance-based Augmentation, namely dReAm, which enhances video and text data using large foundation models to learn more generalized features. Specifically, we first adopt a simple augmentation method, which generates self-similar data by randomly duplicating or dropping subwords and frames. In addition, inspired by the recent... | Bo Xue, Mushi Wang, Ning Yu, Shuai Yuan, Wei Pang, Xiangru Jian, Yimu Wang |  |
| 238 |  |  [ToW: Thoughts of Words Improve Reasoning in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.157) |  | 0 | We introduce thoughts of words (ToW), a novel training-time data-augmentation method for next-word prediction. ToW views next-word prediction as a core reasoning task and injects fine-grained thoughts explaining what the next word should be and how it is related to the previous contexts in pre-training texts. Our formulation addresses two fundamental drawbacks of existing next-word prediction learning schemes: they induce factual hallucination and are inefficient for models to learn the implicit reasoning processes in raw texts. While there are many ways to acquire such thoughts of words, we explore the first step of acquiring ToW annotations through distilling from larger models. After... | Aswin RRV, Ben Zhou, Chitta Baral, Jacob Dineen, Ming Shen, Shijie Lu, Xiao Ye, Zhaonan Li, Zhikun Xu |  |
| 239 |  |  [A Probabilistic Framework for LLM Hallucination Detection via Belief Tree Propagation](https://doi.org/10.18653/v1/2025.naacl-long.158) |  | 0 | We describe Belief Tree Propagation (BTProp), a probabilistic framework for LLM hallucination detection. To judge the truth of a statement, BTProp generates a belief tree by recursively expanding the initial statement into a set of logically related claims, then reasoning globally about the relationships between these claims. BTProp works by constructing a probabilistic model of the LM itself: it reasons jointly about logical relationships between claims and relationships between claim probabilities and LM factuality judgments via probabilistic inference in a “hidden Markov tree”. This method improves over state-of-the-art baselines by 3%-9% (evaluated by AUROC and AUC-PR) on multiple... | Bairu Hou, Jacob Andreas, Shiyu Chang, Yang Zhang |  |
| 240 |  |  [ERAS: Evaluating the Robustness of Chinese NLP Models to Morphological Garden Path Errors](https://doi.org/10.18653/v1/2025.naacl-long.159) |  | 0 | In languages without orthographic word boundaries, NLP models perform _word segmentation_, either as an explicit preprocessing step or as an implicit step in an end-to-end computation. This paper shows that Chinese NLP models are vulnerable to _morphological garden path errors_—errors caused by a failure to resolve local word segmentation ambiguities using sentence-level morphosyntactic context. We propose a benchmark, _ERAS_, that tests a model’s vulnerability to morphological garden path errors by comparing its behavior on sentences with and without local segmentation ambiguities. Using ERAS, we show that word segmentation models make morphological garden path errors on locally ambiguous... | Qinchan Li, Sophie Hao |  |
| 241 |  |  [Superlatives in Context: Modeling the Implicit Semantics of Superlatives](https://doi.org/10.18653/v1/2025.naacl-long.160) |  | 0 | Superlatives are used to single out elements with a maximal/minimal property. Semantically, superlatives perform a set comparison: something (or some things) has the min/max property out of a set. As such, superlatives provide an ideal phenomenon for studying implicit phenomena and discourse restrictions. While this comparison set is often not explicitly defined, its (implicit) restrictions can be inferred from the discourse context the expression appears in. In this work we provide an extensive computational study on the semantics of superlatives. We propose a unified account of superlative semantics which allows us to derive a broad-coverage annotation schema. Using this unified schema... | Bonnie Webber, Ido Dagan, Reut Tsarfaty, Valentina Pyatkin |  |
| 242 |  |  [LLMs Are Not Intelligent Thinkers: Introducing Mathematical Topic Tree Benchmark for Comprehensive Evaluation of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.161) |  | 0 | Large language models (LLMs) demonstrate impressive capabilities in mathematical reasoning. However, despite these achievements, current evaluations are mostly limited to specific mathematical topics, and it remains unclear whether LLMs are genuinely engaging in reasoning. To address these gaps, we present the Mathematical Topics Tree (MaTT) benchmark, a challenging and structured benchmark that offers 1,958 questions across a wide array of mathematical subjects, each paired with a detailed hierarchical chain of topics. Upon assessing different LLMs using the MaTT benchmark, we find that GPT-4 achieved a mere 54% accuracy in a multiple-choice scenario. Interestingly, even when employing... | Arash Gholami Davoodi, Pouya Pezeshkpour, Seyed Pouyan Mousavi Davoudi |  |
| 243 |  |  [Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations](https://doi.org/10.18653/v1/2025.naacl-long.162) |  | 0 | Large-scale surveys are essential tools for informing social science research and policy, but running surveys is costly and time-intensive. If we could accurately simulate group-level survey results, this would therefore be very valuable to social science research. Prior work has explored the use of large language models (LLMs) for simulating human behaviors, mostly through prompting. In this paper, we are the first to specialize LLMs for the task of simulating survey response distributions. As a testbed, we use country-level results from two global cultural surveys. We devise a fine-tuning method based on first-token probabilities to minimize divergence between predicted and actual... | Arnav Arora, Daniel Hershcovich, Haijiang Liu, Isabelle Augenstein, Paul Röttger, Yong Cao |  |
| 244 |  |  [Representing Rule-based Chatbots with Transformers](https://doi.org/10.18653/v1/2025.naacl-long.163) |  | 0 | What kind of internal mechanisms might Transformers use to conduct fluid, natural-sounding conversations? Prior work has illustrated by construction how Transformers can solve various synthetic tasks, such as sorting a list or recognizing formal languages, but it remains unclear how to extend this approach to a conversational setting. In this work, we propose using ELIZA, a classic rule-based chatbot, as a setting for formal, mechanistic analysis of Transformer-based chatbots. ELIZA allows us to formally model key aspects of conversation, including local pattern matching and long-term dialogue state tracking. We first present a theoretical construction of a Transformer that implements the... | Abhishek Panigrahi, Dan Friedman, Danqi Chen |  |
| 245 |  |  [Incremental Sentence Processing Mechanisms in Autoregressive Transformer Language Models](https://doi.org/10.18653/v1/2025.naacl-long.164) |  | 0 | Autoregressive transformer language models (LMs) possess strong syntactic abilities, often successfully handling phenomena from agreement to NPI licensing. However, the features they use to incrementally process their linguistic input are not well understood. In this paper, we fill this gap by studying the mechanisms underlying garden path sentence processing in LMs. Specifically, we ask: (1) Do LMs use syntactic features or shallow heuristics to perform incremental sentence processing? (2) Do LMs represent only one potential interpretation, or multiple? and (3) Do LMs reanalyze or repair their initial incorrect representations? To address these questions, we use sparse autoencoders to... | Aaron Mueller, Michael Hanna |  |
| 246 |  |  [Entangled Relations: Leveraging NLI and Meta-analysis to Enhance Biomedical Relation Extraction](https://doi.org/10.18653/v1/2025.naacl-long.165) |  | 0 | Recent research efforts have explored the potential of leveraging natural language inference (NLI) techniques to enhance relation extraction (RE). In this vein, we introduce MetaEntail-RE, a novel adaptation method that harnesses NLI principles to enhance RE performance. Our approach follows past works by verbalizing relation classes into class-indicative hypotheses, aligning a traditionally multi-class classification task to one of textual entailment. We introduce three key enhancements: (1) Meta-class analysis which, instead of labeling non-entailed premise-hypothesis pairs with the less informative “neutral” entailment label, provides additional context by analyzing overarching... | Jingbo Shang, William Hogan |  |
| 247 |  |  [Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.166) |  | 0 | Multimodal Large Language Models (MLLMs) have shown significant promise in various applications, leading to broad interest from researchers and practitioners alike. However, a comprehensive evaluation of their long-context capabilities remains underexplored. To address these gaps, we introduce the MultiModal Needle-in-a-haystack (MMNeedle) benchmark, specifically designed to assess the long-context capabilities of MLLMs. Besides multi-image input, we employ image stitching to further increase the input context length, and develop a protocol to automatically generate labels for sub-image level retrieval. Essentially, MMNeedle evaluates MLLMs by stress-testing their capability to locate a... | Akshay Nambi, Haizhou Shi, Hao Wang, Hengyi Wang, Shiwei Tan, Tanuja Ganu, Tunyu Zhang, Weiyi Qin, Wenyuan Wang |  |
| 248 |  |  [WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines](https://doi.org/10.18653/v1/2025.naacl-long.167) |  | 0 | Vision Language Models (VLMs) often struggle with culture-specific knowledge, particularly in languages other than English and in underrepresented cultural contexts. To evaluate their understanding of such knowledge, we introduce WorldCuisines, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes a visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural VQA benchmark to date. It includes tasks for identifying dish names and their origins. We provide evaluation datasets in... | Adam Nohejl, Afifa Amriani, Alham Fikri Aji, Alice Oh, Anar Rzayev, Anirban Das, Ashmari Pramodya, Aulia Adila, Ayu Purwarianti, Bryan Wilie, Candy Olivia Mawalim, Cheng Ching Lam, ChongWah Ngo, Daud Abolade, David Anugraha, David Ifeoluwa Adelani, Derry Tanti Wijaya, Emmanuele Chersoni, EnShiun Annie Lee, Enrico Santus, Fariz Ikhwantri, Frederikus Hudi, Garry Kuwanto, Genta Indra Winata, Hanyang Zhao, Haryo Akbarianto Wibowo, Holy Lovenia, Jan Christian Blaise Cruz, Jan Wira Gotama Putra, Junho Myung, Lucky Susanto, Maria Angelica Riera Machin, Marina Zhukova, Michael Anugraha, Muhammad Farid Adilazuarda, Natasha Christabelle Santosa, Nedjma Ousidhoum, Patrick Amadeus Irawan, Peerat Limkonchotiwat, Raj Dabre, Rifki Afina Putri, Rio Alexander Audino, Samuel Cahyawijaya, ShiXiong Zhang, Shogo Okada, Stephanie Yulia Salim, Taro Watanabe, Ubaidillah Ariq Prathama, Yi Zhou, Yinxuan Gui, Yutong Wang |  |
| 249 |  |  [Extracting and Understanding the Superficial Knowledge in Alignment](https://doi.org/10.18653/v1/2025.naacl-long.168) |  | 0 | Alignment of large language models (LLMs) with human values and preferences, often achieved through fine-tuning based on human feedback, is essential for ensuring safe and responsible AI behaviors. However, the process typically requires substantial data and computation resources. Recent studies have revealed that alignment might be attainable at lower costs through simpler methods, such as in-context learning. This leads to the question: Is alignment predominantly superficial? In this paper, we delve into this question and provide a quantitative analysis. We formalize the concept of superficial knowledge, defining it as knowledge that can be acquired through easily token restyling,... | Bhavya Kailkhura, Gabriel J. Perin, Junyuan Hong, Nina S. T. Hirata, Runjin Chen, Xilun Chen, Xuxi Chen, Yan Han |  |
| 250 |  |  [Smurfs: Multi-Agent System using Context-Efficient DFSDT for Tool Planning](https://doi.org/10.18653/v1/2025.naacl-long.169) |  | 0 | Teaching large language models (LLMs) to use tools for solving complex problems can grant them human-like reasoning abilities. ReAct and its variants are popular frameworks for tool use in both single-agent and multi-agent systems. To address issues like error propagation and limited exploration in ReAct, the Deep First Search Decision Tree (DFSDT) was proposed, but it faces challenges such as rollback instability, redundant context, and premature termination in single-agent settings. We introduce “Smurfs,” a novel multi-agent system (MAS) that enhances DFSDT with a modular, context-efficient, and training-free design. Smurfs surpasses baseline methods in both the open-ended... | Benyou Wang, Juhao Liang, Junzhi Chen |  |
| 251 |  |  [From Introspection to Best Practices: Principled Analysis of Demonstrations in Multimodal In-Context Learning](https://doi.org/10.18653/v1/2025.naacl-long.170) |  | 0 | Motivated by in-context learning (ICL) capabilities of Large Language Models (LLMs), multimodal LLMs with additional visual modality are also exhibited with similar ICL abilities when multiple image-text pairs are provided as demonstrations. However, relatively less work has been done to investigate the principles behind how and why multimodal ICL works. We conduct a systematic and principled evaluation of multimodal ICL for models of different scales on a broad spectrum of new yet critical tasks. Through perturbations over different modality information, we show that modalities matter differently across tasks in multimodal ICL. Guided by task-specific modality impact, we recommend... | Fei Wang, Hoifung Poon, Muhao Chen, Nan Xu, Sheng Zhang |  |
| 252 |  |  [Upsample or Upweight? Balanced Training on Heavily Imbalanced Datasets](https://doi.org/10.18653/v1/2025.naacl-long.171) |  | 0 | Data abundance across different domains exhibits a long-tailed distribution: few domains have abundant data, while most face data scarcity. Our work focuses on a multilingual setting, where available data is heavily skewed toward high-resource languages, creating significant imbalances in training data sizes across languages. This disparity challenges training language models to perform uniformly well in all languages. Two common strategies to address this issue are upsampling low-resource languages (Temperature Sampling) and upweighting their loss functions (Scalarization). These methods are often assumed to be equivalent, but this equivalence has not been rigorously established,... | Daniel Khashabi, Haoran Xu, Kenton Murray, Tianjian Li, Weiting Tan |  |
| 253 |  |  [LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with Simple Word-based Counting Problems](https://doi.org/10.18653/v1/2025.naacl-long.172) |  | 0 | Interestingly, LLMs yet struggle with some basic tasks that humans find trivial to handle, e.g., counting the number of character r’s in the word “strawberry”. There are several popular conjectures (e.g., tokenization, architecture and training data) regarding the reason for deficiency of LLMs in simple word-based counting problems, sharing the similar belief that such failure stems from model pretraining hence probably inevitable during deployment. In this paper, we carefully design multiple evaluation settings to investigate validity of prevalent conjectures. Meanwhile, we measure transferability of advanced mathematical and coding reasoning capabilities from specialized LLMs to simple... | Nan Xu, Xuezhe Ma |  |
| 254 |  |  [PAPILLON: Privacy Preservation from Internet-based and Local Language Model Ensembles](https://doi.org/10.18653/v1/2025.naacl-long.173) |  | 0 | Users can divulge sensitive information to proprietary LLM providers, raising significant privacy concerns. While open-source models, hosted locally on the user’s machine, alleviate some concerns, models that users can host locally are often less capable than proprietary frontier models. Toward preserving user privacy while retaining the best quality, we propose Privacy-Conscious Delegation, a novel task for chaining API-based and local models. We utilize recent public collections of user-LLM interactions to construct a natural benchmark called PUPA, which contains personally identifiable information (PII). To study potential approaches, we devise PAPILLON, a multi-stage LLM pipeline that... | Julia Hirschberg, Omar Khattab, Siyan Li, Vethavikashini Chithrra Raghuram, Zhou Yu |  |
| 255 |  |  [When2Call: When (not) to Call Tools](https://doi.org/10.18653/v1/2025.naacl-long.174) |  | 0 | Leveraging external tools is a key feature for modern Language Models (LMs) to expand their capabilities and integrate them into existing systems. However, existing benchmarks primarily focus on the accuracy of tool calling—whether the correct tool is called with the correct parameters—and less on evaluating when LMs should (not) call tools. We develop a new benchmark, When2Call, which evaluates tool-calling decision-making: when to generate a tool call, when to ask follow-up questions and when to admit the question can’t be answered with the tools provided. We find that state-of-the-art tool-calling LMs show significant room for improvement on When2Call, indicating the importance of this... | Ameya Sunil Mahabaleshwarkar, Hayley Ross, Yoshi Suhara |  |
| 256 |  |  [Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused Preference Optimization](https://doi.org/10.18653/v1/2025.naacl-long.175) |  | 0 | Machine Translation (MT) is undergoing a paradigm shift, with systems based on fine-tuned large language models (LLM) becoming increasingly competitive with traditional encoder-decoder models trained specifically for translation tasks. However, LLM-based systems are at a higher risk of generating hallucinations, which can severely undermine user’s trust and safety. Most prior research on hallucination mitigation focuses on traditional MT models, with solutions that involve \*post-hoc\* mitigation - detecting hallucinated translations and re-translating them. While effective, this approach introduces additional complexity in deploying extra tools in production and also increases latency.To... | Rajen Chatterjee, Sarthak Garg, Zilu Tang |  |
| 257 |  |  [Large Language Models Can Solve Real-World Planning Rigorously with Formal Verification Tools](https://doi.org/10.18653/v1/2025.naacl-long.176) |  | 0 | Large Language Models (LLMs) struggle to directly generate correct plans for complex multi-constraint planning problems, even with self-verification and self-critique. For example, a U.S. domestic travel planning benchmark TravelPlanner was proposed in Xie et al. (2024), where the best LLM OpenAI o1-preview can only find viable travel plans with a 10% success rate given all needed information. In this work, we tackle this by proposing an LLM-based planning framework that formalizes and solves complex multi-constraint planning problems as constrained satisfiability problems, which are further consumed by sound and complete satisfiability solvers. We start with TravelPlanner as the primary... | Chuchu Fan, Yang Zhang, Yilun Hao, Yongchao Chen |  |
| 258 |  |  [Who Relies More on World Knowledge and Bias for Syntactic Ambiguity Resolution: Humans or LLMs?](https://doi.org/10.18653/v1/2025.naacl-long.177) |  | 0 | This study explores how recent large language models (LLMs) navigate relative clause attachment ambiguity and use world knowledge biases for disambiguation in six typologically diverse languages: English, Chinese, Japanese, Korean, Russian, and Spanish. We describe the process of creating a novel dataset – MultiWho – for fine-grained evaluation of relative clause attachment preferences in ambiguous and unambiguous contexts. Our experiments with three LLMs indicate that, contrary to humans, LLMs consistently exhibit a preference for local attachment, displaying limited responsiveness to syntactic variations or language-specific attachment patterns.Although LLMs performed well in unambiguous... | Amber Shore, Ameeta Agrawal, Russell Scheinberg, So Young Lee |  |
| 259 |  |  [Beyond Benchmarks: Building a Richer Cross-Document Event Coreference Dataset with Decontextualization](https://doi.org/10.18653/v1/2025.naacl-long.178) |  | 0 | Cross-Document Event Coreference (CDEC) annotation is challenging and difficult to scale, resulting in existing datasets being small and lacking diversity. We introduce a new approach leveraging large language models (LLMs) to decontextualize event mentions, by simplifying the document-level annotation task to sentence pairs with enriched context, enabling the creation of Richer EventCorefBank (RECB), a denser and more expressive dataset annotated at faster speed. Decontextualization has been shown to improve annotation speed without compromising quality and to enhance model performance. Our baseline experiment indicates that systems trained on RECB achieve comparable results on the... | Bingyang Ye, James Pustejovsky, Jin Zhao, Jingxuan Tu, Nianwen Xue, Xinrui Hu |  |
| 260 |  |  [Can Unconfident LLM Annotations Be Used for Confident Conclusions?](https://doi.org/10.18653/v1/2025.naacl-long.179) |  | 0 | Large language models (LLMs) have shown high agreement with human raters across a variety of tasks, demonstrating potential to ease the challenges of human data collection. In computational social science (CSS), researchers are increasingly leveraging LLM annotations to complement slow and expensive human annotations. Still, guidelines for collecting and using LLM annotations, without compromising the validity of downstream conclusions, remain limited. We introduce Confidence-driven inference: a method that combines LLM annotations and LLM confidence indicators to strategically select which human annotations should be collected, with the goal of producing accurate statistical estimates and... | Cinoo Lee, Dan Jurafsky, Emmanuel J. Candès, Kristina Gligoric, Tijana Zrnic |  |
| 261 |  |  [Beyond End-to-End VLMs: Leveraging Intermediate Text Representations for Superior Flowchart Understanding](https://doi.org/10.18653/v1/2025.naacl-long.180) |  | 0 | Flowcharts are typically presented as images, driving the trend of using vision-language models (VLMs) for end-to-end flowchart understanding. However, two key challenges arise: (i) Limited controllability—users have minimal influence over the downstream task, as they can only modify input images, while the training of VLMs is often out of reach for most researchers. (ii) Lack of explainability—it is difficult to trace VLM errors to specific causes, such as failures in visual encoding or reasoning. We propose TextFlow, addressing aforementioned issues with two stages: (i) Vision Textualizer—which generates textual representations from flowchart images; and (ii) Textual Reasoner—which... | Ankan Dash, Guiling Wang, Junyi Ye, Wenpeng Yin |  |
| 262 |  |  [Ihquin tlahtouah in Tetelahtzincocah: An annotated, multi-purpose audio and text corpus of Western Sierra Puebla Nahuatl](https://doi.org/10.18653/v1/2025.naacl-long.181) |  | 0 | The development of digital linguistic resources is essential for enhancing the inclusion of indigenous and marginalized languages in the digital domain. Indigenous languages of Mexico, despite representing vast typological diversity and millions of speakers, have largely been overlooked in NLP until recently. In this paper, we present a corpus of audio and annotated transcriptions of Western Sierra Puebla Nahuatl, an endangered variety of Nahuatl spoken in Puebla, Mexico. The data made available in this corpus are useful for ASR, spelling normalization, and word-level language identification. We detail the corpus-creation process, and describe experiments to report benchmark results for... | Angeles Márquez Hernandez, Cheyenne Wing, Francis M. Tyers, María Ximena Juárez Huerta, Robert Pugh |  |
| 263 |  |  [Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions](https://doi.org/10.18653/v1/2025.naacl-long.182) |  | 0 | LLMs have demonstrated impressive performance in answering medical questions, such as achieving passing scores on medical licensing examinations. However, medical board exams or general clinical questions do not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the reasoning of model decisions, a crucial component of supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises simulated clinical questions. Both... | Hanjie Chen, Mark Dredze, Yash Singla, Zhouxiang Fang |  |
| 264 |  |  [Unfamiliar Finetuning Examples Control How Language Models Hallucinate](https://doi.org/10.18653/v1/2025.naacl-long.183) |  | 0 | Large language models are known to hallucinate, but the underlying mechanism that govern how models hallucinate are not yet fully understood. In this work, we find that unfamiliar examples in the models’ finetuning data – those that introduce concepts beyond the base model’s scope of knowledge – are crucial in shaping these errors. In particular, we find that an LLM’s hallucinated predictions tend to mirror the responses associated with its unfamiliar finetuning examples. This suggests that by modifying how unfamiliar finetuning examples are supervised, we can influence a model’s responses to unfamiliar queries (e.g., say “I don’t know”). We empirically validate this observation in a... | Aviral Kumar, Claire J. Tomlin, Eric Wallace, Katie Kang, Sergey Levine |  |
| 265 |  |  [Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling](https://doi.org/10.18653/v1/2025.naacl-long.184) |  | 0 | Self-consistency mitigates hallucinations in Large Language Models (LLMs) by sampling multiple reasoning paths, but it lacks a systematic approach to determine the optimal number of samples or select the most faithful rationale. To address this limitation, we introduce Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales. RASC assesses the quality of reasoning and the consistency of answers for each generated sample, using these assessments to guide early stopping decisions and rationale selection. The framework employs criteria-based stopping and weighted majority... | Guangya Wan, Jie Chen, Sheng Li, Yuqi Wu |  |
| 266 |  |  [MatViX: Multimodal Information Extraction from Visually Rich Articles](https://doi.org/10.18653/v1/2025.naacl-long.185) |  | 0 | Multimodal information extraction (MIE) is crucial for scientific literature, where valuable data is often spread across text, figures, and tables. In materials science, extracting structured information from research articles can accelerate the discovery of new materials. However, the multimodal nature and complex interconnections of scientific content present challenges for traditional text-based methods. We introduce MatViX, a benchmark consisting of 324 full-length research articles and 1,688 complex structured JSON files, carefully curated by domain experts in polymer nanocomposites and biodegradation. These JSON files are extracted from text, tables, and figures in full-length... | Aman Tyagi, Bhuwan Dhingra, Ghazal Khalighinejad, Kelly L. Anderson, Ollie Liu, Rickard Stureborg, Sharon Scott |  |
| 267 |  |  [Towards Rationality in Language and Multimodal Agents: A Survey](https://doi.org/10.18653/v1/2025.naacl-long.186) |  | 0 | This work discusses how to build more rational language and multimodal agents and what criteria define rationality in intelligent systems.Rationality is the quality of being guided by reason, characterized by decision-making that aligns with evidence and logical principles. It plays a crucial role in reliable problem-solving by ensuring well-grounded and consistent solutions. Despite their progress, large language models (LLMs) often fall short of rationality due to their bounded knowledge space and inconsistent outputs. In response, recent efforts have shifted toward developing multimodal and multi-agent systems, as well as integrating modules like external tools, programming codes,... | Bowen Jiang, Camillo Jose Taylor, Tanwi Mallick, Weijie J. Su, Xiaomeng Wang, Xinyi Bai, Yangxinyu Xie, Yuan Yuan, Zhuoqun Hao |  |
| 268 |  |  [CluSanT: Differentially Private and Semantically Coherent Text Sanitization](https://doi.org/10.18653/v1/2025.naacl-long.187) |  | 0 | We introduce CluSanT, a novel text sanitization framework based on Metric Local Differential Privacy (MLDP). Our framework consists of three components: token clustering, cluster embedding, and token sanitization. For the first, CluSanT employs Large Language Models (LLMs) to create—a set of potential substitute tokens which we meaningfully cluster. Then, we develop a parameterized cluster embedding that balances the trade-off between privacy and utility. Lastly, we propose a MLDP algorithm which sanitizes/substitutes sensitive tokens in a text with the help of our embedding. Notably, our MLDP-based framework can be tuned with parameters such that (1) existing state-of-the-art (SOTA) token... | Ahmed Musa Awon, Alex Thomo, Shera Potka, Yun Lu |  |
| 269 |  |  [TurkingBench: A Challenge Benchmark for Web Agents](https://doi.org/10.18653/v1/2025.naacl-long.188) |  | 0 | Can advanced multi-modal models effectively tackle complex web-based tasks? Such tasks are often found on crowdsourcing platforms, where crowdworkers engage in challenging micro-tasks within web-based environments.Building on this idea, we present TurkingBench, a benchmark consisting of tasks presented as web pages with textual instructions and multi-modal contexts. Unlike previous approaches that rely on artificially synthesized web pages, our benchmark uses natural HTML pages originally designed for crowdsourcing workers to perform various annotation tasks. Each task’s HTML instructions are instantiated with different values derived from crowdsourcing tasks, creating diverse instances.... | Adam Byerly, Adi Asija, Benjamin Van Durme, Daniel Khashabi, Jingyu Zhang, Kate Sanders, Kevin Xu, Tanay Nayak, Yeganeh Kordi, Yizhong Wang |  |
| 270 |  |  [CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.189) |  | 0 | Pretrained on massive amounts of code and text data, large language models (LLMs) have demonstrated remarkable achievements in performing code generation tasks. With additional execution-based feedback, these models can act as agents with capabilities to self-refine and improve generated code autonomously. However, on challenging coding tasks with extremely large search space, current agentic approaches still struggle with multi-stage planning, generating, and debugging. To address this problem, we propose CodeTree, a framework for LLM agents to efficiently explore the search space in different stages of the code generation process. Specifically, we adopted a unified tree structure to... | Caiming Xiong, Doyen Sahoo, Hung Le, Jierui Li, Silvio Savarese, Yingbo Zhou |  |
| 271 |  |  [DPL: Diverse Preference Learning Without A Reference Model](https://doi.org/10.18653/v1/2025.naacl-long.190) |  | 0 | In direct preference alignment in LLMs, most existing methods seek to retrieve the reward function directly from preference data. However, real-world preference data often contains diversity in preference annotations reflective of true human preferences. Existing algorithms, including KTO, do not directly utilize such nuances in the annotations which limits their applicability. In this work, we propose Diverse Preference Learning (DPL), a reference model-free method that simultaneously learns a baseline desirability in LLM responses while being robust to the diversity of preference annotations. Our experiments for instruction-following on Ultrafeedback and AlpacaEval 2.0 and for... | Abhijnan Nath, Albert Nanda, Andrey Volozin, Galina Grunin, Nikhil Krishnaswamy, Rahul Bhotika, Saumajit Saha |  |
| 272 |  |  [Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data](https://doi.org/10.18653/v1/2025.naacl-long.191) |  | 0 | To trust the fluent generations of large language models (LLMs), humans must be able to _verify_ their correctness against trusted, external sources. Recent efforts, such as providing citations via retrieved documents or post-hoc provenance, enhance verifiability but provide no guarantees on their correctness. To address these limitations, we tackle the verifiability goal with a different philosophy: _trivializing the verification process by developing models that quote verbatim statements from trusted sources in their pre-training data._We propose Quote-Tuning, which demonstrates the feasibility of aligning models to quote. The core of Quote-Tuning is a fast membership inference function... | Benjamin Van Durme, Daniel Khashabi, Jingyu Zhang, Marc Marone, Tianjian Li |  |
| 273 |  |  [VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models](https://doi.org/10.18653/v1/2025.naacl-long.192) |  | 0 |  | Jiwen Zhang, Minghui Qiu, Ruipu Luo, Xuanjing Huang, Zejun Li, Zhongyu Wei |  |
| 274 |  |  [ACCORD: Closing the Commonsense Measurability Gap](https://doi.org/10.18653/v1/2025.naacl-long.193) |  | 0 | We present ACCORD, a framework and benchmark suite for disentangling the commonsense grounding and reasoning abilities of large language models (LLMs) through controlled, multi-hop counterfactuals. ACCORD introduces formal elements to commonsense reasoning to explicitly control and quantify reasoning complexity beyond the typical 1 or 2 hops. Uniquely, ACCORD can automatically generate benchmarks of arbitrary reasoning complexity, so it scales with future LLM improvements. Indeed, our experiments on state-of-the-art LLMs show performance degrading to below random chance with only moderate scaling, leaving substantial headroom for improvement. We release a leaderboard of the benchmark suite... | Frank Rudzicz, François RoewerDesprés, Jinyue Feng, Zining Zhu |  |
| 275 |  |  [CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments](https://doi.org/10.18653/v1/2025.naacl-long.194) |  | 0 | Customer Relationship Management (CRM) systems are vital for modern enterprises, providing a foundation for managing customer interactions and data. Integrating AI agents into CRM systems can automate routine processes and enhance personalized service. However, deploying and evaluating these agents is challenging due to the lack of realistic benchmarks that reflect the complexity of real-world CRM tasks. To address this issue, we introduce CRMArena, a novel benchmark designed to evaluate AI agents on realistic tasks grounded in professional work environments. Following guidance from CRM experts and industry best practices, we designed CRMArena with nine customer service tasks distributed... | Akshara Prabhakar, Caiming Xiong, ChienSheng Wu, Huan Wang, KungHsiang Huang, Philippe Laban, Sidharth Dhawan, Silvio Savarese, Yixin Mao |  |
| 276 |  |  [Mamba-Shedder: Post-Transformer Compression for Efficient Selective Structured State Space Models](https://doi.org/10.18653/v1/2025.naacl-long.195) |  | 0 | Large pre-trained models have achieved outstanding results in sequence modeling. The Transformer block and its attention mechanism have been the main drivers of the success of these models. Recently, alternative architectures, such as Selective Structured State Space Models (SSMs), have been proposed to address the inefficiencies of Transformers. This paper explores the compression of SSM-based models, particularly Mamba and its hybrids. We study the sensitivity of these models to the removal of selected components at different granularities to reduce the model size and computational overhead, thus improving their efficiency while maintaining accuracy. The proposed solutions, collectively... | Jinjie Yuan, Juan Pablo Muñoz, Nilesh Jain |  |
| 277 |  |  [CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy](https://doi.org/10.18653/v1/2025.naacl-long.196) |  | 0 | There is a significant gap between patient needs and available mental health support today. In this paper, we aim to thoroughly examine the potential of using Large Language Models (LLMs) to assist professional psychotherapy. To this end, we propose a new benchmark, CBT-Bench, for the systematic evaluation of cognitive behavioral therapy (CBT) assistance. We include three levels of tasks in CBT-Bench: \*\*I: Basic CBT knowledge acquisition\*\*, with the task of multiple-choice questions; \*\*II: Cognitive model understanding\*\*, with the tasks of cognitive distortion classification, primary core belief classification, and fine-grained core belief classification; \*\*III: Therapeutic... | Fei Fang, Jamie C. Chiu, Mian Zhang, Shaun M. Eack, Travis Labrum, William Yang Wang, Xianjun Yang, Xinlu Zhang, Zhiyu Chen |  |
| 278 |  |  [An Efficient Gloss-Free Sign Language Translation Using Spatial Configurations and Motion Dynamics with LLMs](https://doi.org/10.18653/v1/2025.naacl-long.197) |  | 0 | Gloss-free Sign Language Translation (SLT) converts sign videos into spoken language sentences without relying on glosses, which are the written representations of signs. Recently, Large Language Models (LLMs) have shown remarkable translation performance in gloss-free methods by harnessing their powerful natural language generation capabilities. However, these methods often rely on domain-specific fine-tuning of visual encoders to achieve optimal results. By contrast, we emphasize the importance of capturing the spatial configurations and motion dynamics in sign language. With this in mind, we introduce Spatial and Motion-based Sign Language Translation (SpaMo), a novel LLM-based SLT... | Eui Jun Hwang, Jong C. Park, Junmyeong Lee, Sukmin Cho |  |
| 279 |  |  [Sketch2Code: Evaluating Vision-Language Models for Interactive Web Design Prototyping](https://doi.org/10.18653/v1/2025.naacl-long.198) |  | 0 | Sketches are a natural and accessible medium for UI designers to conceptualize early-stage ideas. However, existing research on UI/UX automation often requires high-fidelity inputs like Figma designs or detailed screenshots, limiting accessibility and impeding efficient design iteration. To bridge this gap, we introduce Sketch2Code, a benchmark that evaluates state-of-the-art Vision Language Models (VLMs) on automating the conversion of rudimentary sketches into webpage prototypes. Beyond end-to-end benchmarking, Sketch2Code supports interactive agent evaluation that mimics real-world design workflows, where a VLM-based agent iteratively refines its generations by communicating with a... | Diyi Yang, Ryan Li, Yanzhe Zhang |  |
| 280 |  |  [Design2Code: Benchmarking Multimodal Code Generation for Automated Front-End Engineering](https://doi.org/10.18653/v1/2025.naacl-long.199) |  | 0 | Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development in which multimodal large language models (MLLMs) directly convert visual designs into code implementations. In this work, we construct Design2Code – the first real-world benchmark for this task. Specifically, we manually curate 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. We... | Chenglei Si, Diyi Yang, Ruibo Liu, Ryan Li, Yanzhe Zhang, Zhengyuan Yang |  |
| 281 |  |  [Temporal-Aware Soft Prompt Tuning for Automatic Text Dating](https://doi.org/10.18653/v1/2025.naacl-long.200) |  | 0 | This paper presents Temporal-aware Soft Prompt Tuning (TASPT), a novel approach for automatic text dating. Unlike existing methods, which often overlook the evolution of word meanings in texts spanning long periods, TASPT incorporates the unique characteristics of historical texts. It introduces a temporal-aware text representation that dynamically captures both semantic variance and invariance. This representation is combined with a soft prompt, enabling efficient parameter tuning for automatic text dating. Experiments show that TASPT outperforms all existing methods on two diachronic datasets: the Twenty-Four Histories and the Royal Society Corpus. | Hai Wang, Han Ren, Yuzhi Liang |  |
| 282 |  |  [Sparser Mixture-of-Adapters with Cross-Layer Generalization](https://doi.org/10.18653/v1/2025.naacl-long.201) |  | 0 |  | Tianyi Zhou, Ziyue Li |  |
| 283 |  |  [How to Align Multiple Signed Language Corpora for Better Sign-to-Sign Translations?](https://doi.org/10.18653/v1/2025.naacl-long.202) |  | 0 | There are more than 300 documented signed languages worldwide, which are indispensable avenues for computational linguists to study cross-cultural and cross-linguistic factors that affect automatic sign understanding and generation. Yet, these are studied under critically low-resource settings, especially when examining multiple signed languages simultaneously. In this work, we hypothesize that a linguistically informed alignment algorithm can improve the results of sign-to-sign translation models. To this end, we first conduct a qualitative analysis of similarities and differences across three signed languages: American Sign Language (ASL), Chinese Sign Language (CSL), and German Sign... | Malihe Alikhani, Mert Inan, Vidya Ganesh, Yang Zhong |  |
| 284 |  |  [Communication Makes Perfect: Persuasion Dataset Construction via Multi-LLM Communication](https://doi.org/10.18653/v1/2025.naacl-long.203) |  | 0 | Large Language Models (LLMs) have shown proficiency in generating persuasive dialogue, yet concerns about the fluency and sophistication of their outputs persist. This paper presents a multi-LLM communication framework designed to enhance the generation of persuasive data automatically. This framework facilitates the efficient production of high-quality, diverse linguistic content with minimal human oversight. Through extensive evaluations, we demonstrate that the generated data excels in naturalness, linguistic diversity, and the strategic use of persuasion, even in complex scenarios involving social taboos. The framework also proves adept at generalizing across novel contexts. Our... | Ethan Gearey, Farnoosh Hashemi, Hefan Zhang, Ivory Yang, Joice Chen, Michael Macy, Saeed Hassanpour, Shiyu Ji, Shubham Mohole, Soroush Vosoughi, Weicheng Ma |  |
| 285 |  |  [Soft Prompting for Unlearning in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.204) |  | 0 | The widespread popularity of Large Language Models (LLMs), partly due to their emerging in-context learning ability, has highlighted the importance of ethical and safety considerations for deployment. Motivated by corresponding data protection guidelines, we investigate machine unlearning for LLMs. In contrast to the growing literature on fine-tuning methods to achieve unlearning, we focus on a comparatively lightweight alternative called soft prompting to realize unlearning in LLMs. With losses designed to enforce forgetting as well as utility preservation, our framework Soft Prompting for Unlearning (SPUL) learns prompt tokens that are prepended to a query to induce unlearning of... | Karuna Bhaila, MinhHao Van, Xintao Wu |  |
| 286 |  |  [Mutual-pairing Data Augmentation for Fewshot Continual Relation Extraction](https://doi.org/10.18653/v1/2025.naacl-long.205) |  | 0 | Data scarcity is a major challenge in Few-shot Continual Relation Extraction (FCRE), where models must learn new relations from limited data while retaining past knowledge. Current methods, restricted by minimal data streams, struggle with catastrophic forgetting and overfitting. To overcome this, we introduce a novel \*data augmentation strategy\* that transforms single input sentences into complex texts by integrating both old and new data. Our approach sharpens model focus, enabling precise identification of word relationships based on specified relation types. By embedding adversarial training effects and leveraging new training perspectives through special objective functions, our... | Linh Ngo Van, Nguyen Hoang Anh, Nguyen Thi Ngoc Diep, Quyen Tran, Thanh Xuan Nguyen, Thien Huu Nguyen, Trung Le |  |
| 287 |  |  [KMMLU: Measuring Massive Multitask Language Understanding in Korean](https://doi.org/10.18653/v1/2025.naacl-long.206) |  | 0 | We propose KMMLU, a Korean benchmark with 35,030 expert-level multiple-choice questions across 45 subjects ranging from humanities to STEM. While prior Korean evaluation tools heavily rely on translated versions of existing English benchmarks, KMMLU is collected from original Korean exams, thereby capturing linguistic and cultural aspects of the Korean language. Recent models struggle to show performance over 60%, significantly below the pass mark of the source exams (80%), highlighting the room for improvement. Notably, one-fifth of the questions in KMMLU require knowledge of Korean culture for accurate resolution. KMMLU thus provides a more accurate reflection of human preferences... | Cheonbok Park, Guijin Son, Hanwool Lee, Kang Min Yoo, Niklas Muennighoff, Seungone Kim, Stella Biderman, Sungdong Kim, Taekyoon Choi |  |
| 288 |  |  [Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench](https://doi.org/10.18653/v1/2025.naacl-long.207) |  | 0 | Generative models such as Large Language Models (LLM) and Multimodal Large Language models (MLLMs) trained on massive web corpora can memorize and disclose individuals’ confidential and private data, raising legal and ethical concerns. While many previous works have addressed this issue in LLM via machine unlearning, it remains largely unexplored for MLLMs. To tackle this challenge, we introduce Multimodal Large Language Model Unlearning Benchmark (MLLMU-Bench), a novel benchmark aimed at advancing the understanding of multimodal machine unlearning. MLLMU-Bench consists of 500 fictitious profiles and 153 profiles for public celebrities, each profile feature over 14 customized... | Guangyao Dou, Meng Jiang, Mengzhao Jia, Qingkai Zeng, Yongle Yuan, Zhaoxuan Tan, Zheyuan Liu |  |
| 289 |  |  [LLM4DistReconfig: A Fine-tuned Large Language Model for Power Distribution Network Reconfiguration](https://doi.org/10.18653/v1/2025.naacl-long.208) |  | 0 | Power distribution networks are evolving due to the integration of distributed energy resources (DERs) and increased customer participation. To maintain optimal operation, minimize losses, and meet varying load demands, frequent network reconfiguration is necessary. Traditionally, the reconfiguration task relies on optimization software and expert operators, but as systems grow more complex, faster and more adaptive solutions are required without expert intervention. Data-driven reconfiguration is gaining traction for its accuracy, speed, and robustness against incomplete network data. Large language models (LLMs), with their ability to capture complex patterns, offer a promising approach... | Jingwei Xiong, Md. Zahidul Islam, Panayiotis Christou, Yuzhang Lin |  |
| 290 |  |  [WaterPool: A Language Model Watermark Mitigating Trade-Offs among Imperceptibility, Efficacy and Robustness](https://doi.org/10.18653/v1/2025.naacl-long.209) |  | 0 | Watermarking is a prominent technique to trace the usage of specific large language models (LLMs) by injecting patterns into model-generated content. An ideal watermark should be imperceptible, easily detectable, and robust to text alterations, yet existing methods typically face trade-offs among these properties. This paper utilizes a key-centered scheme to unify existing methods by decomposing a watermark into two components: a key module and a mark module. We show that the trade-off issue is the reflection of the conflict between the scale of the key sampling space during generation and the complexity of key restoration during detection within the key module. To this end, we introduce... | Baizhou Huang, Xiaojun Wan |  |
| 291 |  |  [Tricking Retrievers with Influential Tokens: An Efficient Black-Box Corpus Poisoning Attack](https://doi.org/10.18653/v1/2025.naacl-long.210) |  | 0 | Retrieval-augmented generation (RAG) systems enhance large language models by incorporating external knowledge, addressing issues like outdated internal knowledge and hallucination. However, their reliance on external knowledge bases makes them vulnerable to corpus poisoning attacks, where adversarial passages can be injected to manipulate retrieval results. Existing methods for crafting such passages, such as random token replacement or training inversion models, are often slow and computationally expensive, requiring either access to retriever’s gradients or large computational resources. To address these limitations, we propose Dynamic Importance-Guided Genetic Algorithm (DIGA), an... | Bryan Hooi, Cheng Wang, Yiwei Wang, Yujun Cai |  |
| 292 |  |  [The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism](https://doi.org/10.18653/v1/2025.naacl-long.211) |  | 0 | Current evaluations of large language models (LLMs) often overlook non-determinism, typically focusing on a single output per example. This limits our understanding of LLM performance variability in real-world applications. Our study addresses this issue by exploring key questions about the performance differences between greedy decoding and sampling, identifying benchmarks’ consistency regarding non-determinism, and examining unique model behaviors. Through extensive experiments, we observe that greedy decoding generally outperforms sampling methods for most evaluated tasks. We also observe consistent performance across different LLM sizes and alignment methods, noting that alignment can... | Bill Yuchen Lin, Guoyin Wang, Sujian Li, Yifan Song |  |
| 293 |  |  [CVE-Bench: Benchmarking LLM-based Software Engineering Agent's Ability to Repair Real-World CVE Vulnerabilities](https://doi.org/10.18653/v1/2025.naacl-long.212) |  | 0 | Automated vulnerability repair is a crucial field within software engineering and security research. Large Language Models (LLMs) and LLM agents have demonstrated significant potential in this domain by understanding descriptions in natural language and generating corresponding formal code. Although the coding capabilities of LLMs have advanced rapidly, evaluation benchmarks for real-world programming setups are still lagging, preventing the development of LLM and LLM agents in real-world vulnerability repair. To this end, we introduce CVE-Bench, an evaluation framework consisting of 509 Common Vulnerabilities and Exposures (CVEs) from four programming languages and 120 popular open-source... | Chaowei Xiao, Peiran Wang, Xiaogeng Liu |  |
| 294 |  |  [PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines](https://doi.org/10.18653/v1/2025.naacl-long.213) |  | 0 | Large language models (LLMs) are increasingly deployed in specialized production data processing pipelines across diverse domains—such as finance, marketing, and e-commerce. However, when running them in production across many inputs, they often fail to follow instructions or meet developer expectations. To improve reliability in these applications, creating assertions or guardrails for LLM outputs to run alongside the pipelines is essential. Yet, determining the right set of assertions that capture developer requirements for a task is challenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM pipeline prompts with 12623 corresponding assertion criteria, sourced from... | Aditya G. Parameswaran, Harrison Chase, Reya Vir, Shreya Shankar, Will FuHinthorn |  |
| 295 |  |  [ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent Dialogue Synthesis](https://doi.org/10.18653/v1/2025.naacl-long.214) |  | 0 | Supervised fine-tuning (SFT) is a common method to enhance the tool calling capabilities of Large Language Models (LLMs), with the training data often being synthesized. The current data synthesis process generally involves sampling a set of tools, formulating a requirement based on these tools, and generating the call statements. However, tools sampled randomly lack relevance, making them difficult to combine and thus reducing the diversity of the data. Additionally, current work overlooks the coherence between turns of dialogues, leading to a gap between the synthesized data and real-world scenarios. To address these issues, we propose a Graph-based Sampling strategy to sample more... | KamFai Wong, Liangyou Li, Lifeng Shang, Qun Liu, Weiwen Liu, Xin Jiang, Xingshan Zeng, Yasheng Wang, Zezhong Wang |  |
| 296 |  |  [Fighting Spurious Correlations in Text Classification via a Causal Learning Perspective](https://doi.org/10.18653/v1/2025.naacl-long.215) |  | 0 | In text classification tasks, models often rely on spurious correlations for predictions, incorrectly associating irrelevant features with the target labels. This issue limits the robustness and generalization of models, especially when faced with out-of-distribution data where such spurious correlations no longer hold. To address this challenge, we propose the Causally Calibrated Robust Classifier (CCR), which aims to reduce models’ reliance on spurious correlations and improve model robustness. Our approach integrates a causal feature selection method based on counterfactual reasoning, along with an unbiased inverse propensity weighting (IPW) loss function. By focusing on selecting... | Yuqing Zhou, Ziwei Zhu |  |
| 297 |  |  [Knowledge-Aware Query Expansion with Large Language Models for Textual and Relational Retrieval](https://doi.org/10.18653/v1/2025.naacl-long.216) |  | 0 | Large language models (LLMs) have been used to generate query expansions augmenting original queries for improving information search. Recent studies also explore providing LLMs with initial retrieval results to generate query expansions more grounded to document corpus. However, these methods mostly focus on enhancing textual similarities between search queries and target documents, overlooking document relations. For queries like “Find me a highly rated camera for wildlife photography compatible with my Nikon F-Mount lenses”, existing methods may generate expansions that are semantically similar but structurally unrelated to user intents. To handle such semi-structured queries with both... | Haoliang Wang, Julian J. McAuley, Junda Wu, Ryan A. Rossi, Sungchul Kim, Tong Yu, Yu Xia |  |
| 298 |  |  [SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression](https://doi.org/10.18653/v1/2025.naacl-long.217) |  | 0 | Despite significant advancements, the practical deployment of Large Language Models (LLMs) is often hampered by their immense sizes, highlighting the need for effective compression techniques. Singular Value Decomposition (SVD) emerges as a promising method for compressing LLMs. However, existing SVD-based compression approaches suffer from substantial truncation losses, leading to severe performance degradation in compressed models. In this work, we introduce , a novel SVD-based LLM compression method that optimizes singular value truncation in SVD compression with two key strategies. First, employs dynamic compression ratio allocation to effectively balance the extremely large truncation... | Hui Shen, Mi Zhang, Samiul Alam, Xin Wang, Zhongwei Wan |  |
| 299 |  |  [AudioBench: A Universal Benchmark for Audio Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.218) |  | 0 | We introduce AudioBench, a universal benchmark designed to evaluate Audio Large Language Models (AudioLLMs). It encompasses 8 distinct tasks and 26 datasets, among which, 7 are newly proposed datasets. The evaluation targets three main aspects: speech understanding, audio scene understanding, and voice understanding (paralinguistic). Despite recent advancements, there lacks a comprehensive benchmark for AudioLLMs on instruction following capabilities conditioned on audio signals. AudioBench addresses this gap by setting up datasets as well as desired evaluation metrics. Besides, we also evaluated the capabilities of five popular models and found that no single model excels consistently... | AiTi Aw, Bin Wang, Geyu Lin, Nancy F. Chen, Shuo Sun, Wenyu Zhang, Xunlong Zou, Zhengyuan Liu, Zhuohan Liu |  |
| 300 |  |  [Efficient Prompting for Continual Adaptation to Missing Modalities](https://doi.org/10.18653/v1/2025.naacl-long.219) |  | 0 | Missing modality issues are common in real-world applications, arising from factors such as equipment failures and privacy concerns. When fine-tuning pre-trained models on downstream datasets with missing modalities, performance can degrade significantly. Current methods often aggregate various missing cases to train recovery modules or align multimodal features, resulting in suboptimal performance, high computational costs, and the risk of catastrophic forgetting in continual environments where data arrives sequentially. In this paper, we formulate the dynamic missing modality problem as a continual learning task and introduce the continual multimodal missing modality task. To address... | Shulei Wang, Tao Jin, Wang Lin, Weicai Yan, Yangyang Wu, Zirun Guo |  |
| 301 |  |  [Benchmarking and Building Zero-Shot Hindi Retrieval Model with Hindi-BEIR and NLLB-E5](https://doi.org/10.18653/v1/2025.naacl-long.220) |  | 0 | Given the large number of Hindi speakers worldwide, there is a pressing need for robust and efficient information retrieval systems for Hindi. Despite ongoing research, comprehensive benchmarks for evaluating retrieval models in Hindi are lacking. To address this gap, we introduce the Hindi-BEIR benchmark, comprising 15 datasets across seven distinct tasks. We evaluate state-of-the-art multilingual retrieval models on the Hindi-BEIR benchmark, identifying task and domain-specific challenges that impact Hindi retrieval performance. Building on the insights from these results, we introduce NLLB-E5, a multilingual retrieval model that leverages a zero-shot approach to support Hindi without... | Arkadeep Acharya, Jaydeep Sen, Rudra Murthy, Vishwajeet Kumar |  |
| 302 |  |  [Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion](https://doi.org/10.18653/v1/2025.naacl-long.221) |  | 0 | The Knowledge Graph Completion (KGC) task aims to infer the missing entity from an incomplete triple. Existing embedding-based methods rely solely on triples in the KG, which is vulnerable to specious relation patterns and long-tail entities. On the other hand, text-based methods struggle with the semantic gap between KG triples and natural language. Apart from triples, entity contexts (e.g., labels, descriptions, aliases) also play a significant role in augmenting KGs. To address these limitations, we propose KGR3, a context-enriched framework for KGC. KGR3 is composed of three modules. Firstly, the Retrieval module gathers supporting triples from the KG, collects plausible candidate... | Cehao Yang, Chengjin Xu, Hofung Leung, Irwin King, Jian Guo, Muzhi Li, Xuhui Jiang, Yiyan Qi |  |
| 303 |  |  [See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language Balance to Mitigate Dominant Modality Bias](https://doi.org/10.18653/v1/2025.naacl-long.222) |  | 0 | Vision-language (VL) models have demonstrated strong performance across various tasks. However, these models often rely on a specific modality for predictions, leading to “dominant modality bias.” This bias significantly hurts performance, especially when one modality is impaired. In this study, we analyze model behavior under dominant modality bias and theoretically show that unaligned gradients or differences in gradient magnitudes prevent balanced convergence of the loss. Based on these findings, we propose a novel framework, \*\*BalGrad\*\* to mitigate dominant modality bias. Our approach includes inter-modality gradient reweighting, adjusting the gradient of KL divergence based on... | Eunju Lee, Juhwan Choi, Junehyoung Kwon, Mihyeon Kim, YoungBin Kim |  |
| 304 |  |  [Harnessing and Evaluating the Intrinsic Extrapolation Ability of Large Language Models for Vehicle Trajectory Prediction](https://doi.org/10.18653/v1/2025.naacl-long.223) |  | 0 | Emergent abilities of large language models (LLMs) have significantly advanced their application in autonomous vehicle (AV) research. Safe integration of LLMs into vehicles, however, necessitates their thorough understanding of dynamic traffic environments. Towards this end, this study introduces a framework leveraging LLMs’ built-in extrapolation capabilities for vehicle trajectory prediction, thereby evaluating their comprehension of the evolution of traffic agents’ behaviors and interactions over time. The framework employs a traffic encoder to extract spatial-level scene features from agents’ observed trajectories to facilitate efficient scene representation. To focus on LLM’s innate... | Hong Chen, Jiawei Liu, Tingting Wang, Xun Gong, Yanjiao Liu, Yunfeng Hu |  |
| 305 |  |  [Stronger Models are Not Always Stronger Teachers for Instruction Tuning](https://doi.org/10.18653/v1/2025.naacl-long.224) |  | 0 | Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions and engage with users meaningfully. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt larger models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive... | Bill Yuchen Lin, Fengqing Jiang, Luyao Niu, Radha Poovendran, Zhangchen Xu |  |
| 306 |  |  [Efficient and Effective Prompt Tuning via Prompt Decomposition and Compressed Outer Product](https://doi.org/10.18653/v1/2025.naacl-long.225) |  | 0 | Prompt tuning (PT) offers a cost-effective alternative to fine-tuning large-scale pre-trained language models (PLMs), requiring only a few parameters in soft prompt tokens added before the input text. However, existing PT approaches face two significant issues: i They overlook intrinsic semantic associations between soft prompt tokens, leading to high discreteness and limited interactions, thus reducing the model’s comprehension and effectiveness in complex tasks. ii Due to the complexity of downstream tasks, long soft prompt is necessitated to improve performance, but prompt length correlates positively with memory usage and computational costs. Achieving high efficiency and performance... | Enneng Yang, Guibing Guo, Haoyu Xu, Jianzhe Zhao, Pengxiang Lan, Xingwei Wang, Yuliang Liang |  |
| 307 |  |  [Threshold Filtering Packing for Supervised Fine-Tuning: Training Related Samples within Packs](https://doi.org/10.18653/v1/2025.naacl-long.226) |  | 0 | Packing for Supervised Fine-Tuning (SFT) in autoregressive models involves concatenating data points of varying lengths until reaching the designed maximum length to facilitate GPU processing. However, randomly concatenating data points can lead to cross-contamination of sequences due to the significant difference in their subject matter. The mainstream approaches in SFT ensure that each token in the attention calculation phase only focuses on tokens within its own short sequence, without providing additional learning signals for the preceding context. To address these challenges, we introduce Threshold Filtering Packing (TFP), a method that selects samples with related context while... | Jiancheng Dong, Lei Jiang, Lu Cheng, Wei Jin |  |
| 308 |  |  [Transferable Post-training via Inverse Value Learning](https://doi.org/10.18653/v1/2025.naacl-long.227) |  | 0 | As post-training processes utilize increasingly large datasets and base models continue to grow in size, the computational demands and implementation challenges of existing algorithms are escalating significantly. In this paper, we propose modeling the changes at the logits level during post-training using a separate neural network (i.e., the value network). After training this network on a small base model using demonstrations, this network can be seamlessly integrated with another pre-trained models during inference, enabling them to achieve similar capability enhancements. We systematically investigate the best practices for this paradigm in terms of pre-training weights and connection... | Bowen Yu, Haiyang Yu, Hongyu Lin, Le Sun, Xianpei Han, Xinyu Lu, Xueru Wen, Yaojie Lu, Yongbin Li |  |
| 309 |  |  [FLEX: Expert-level False-Less EXecution Metric for Text-to-SQL Benchmark](https://doi.org/10.18653/v1/2025.naacl-long.228) |  | 0 | Text-to-SQL systems have become crucial for translating natural language into SQL queries in various industries, enabling non-technical users to perform complex data operations. The need for accurate evaluation methods has increased as these systems have grown more sophisticated. However, the Execution Accuracy (EX), the most prevalent evaluation metric, still shows many false positives and negatives. Thus, this paper introduces \*\*FLEX(False-Less EXecution)\*\*, a novel approach to evaluating text-to-SQL systems using large language models (LLMs) to emulate human expert-level evaluation of SQL queries. Our metric improves agreement with human experts (from 62 to 87.04 in Cohen’s kappa)... | Heegyu Kim, Hyunsouk Cho, Seunghwan Choi, Seungtaek Choi, Taeyang Jeon |  |
| 310 |  |  [AID: Adaptive Integration of Detectors for Safe AI with Language Models](https://doi.org/10.18653/v1/2025.naacl-long.229) |  | 0 | As Large Language Models (LLMs) increasingly influence content generation across diverse platforms, there is a heightened urgency to regulate their outputs to ensure safe usage. However, defining safety is complex, given that entities across domains may interpret it through varied lenses and develop safety detectors—models trained to identify specific unsafe content based on predefined criteria. To address this complexity, we introduce the approach of Adaptive Integration of Detectors (AID) to orchestrate the strengths of multiple pretrained detectors to ensure comprehensive effectiveness in diverse scenarios. AID employs a Mixture-of-Experts (MoE) framework, wherein it dynamically assigns... | Ali Anwar, Enmao Diao, Jie Ding, Qi Le, Xinran Wang |  |
| 311 |  |  [SSMLoRA: Enhancing Low-Rank Adaptation with State Space Model](https://doi.org/10.18653/v1/2025.naacl-long.230) |  | 0 | Fine-tuning is a key approach for adapting language models to specific downstream tasks, but updating all model parameters becomes impractical as model sizes increase.Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), address this challenge by introducing additional adaptation parameters into pre-trained weight matrices.However, LoRA’s performance varies across different insertion points within the model, highlighting potential parameter inefficiency due to unnecessary insertions. To this end, we propose SSMLoRA (\*\*S\*\*tate \*\*S\*\*pace \*\*M\*\*odel \*\*L\*\*ow-\*\*R\*\*ank \*\*A\*\*daptation), an extension of LoRA that incorporates a State Space Model... | Bin Wang, Jiayang Yu, Peiqin Lin, Shi Feng, Yihang Zhang, Yongkang Liu |  |
| 312 |  |  [Sharpness-Aware Minimization for Topic Models with High-Quality Document Representations](https://doi.org/10.18653/v1/2025.naacl-long.231) |  | 0 | Recent advanced frameworks in topic models have significantly enhanced the performance compared to conventional probabilistic approaches. Such models, mostly constructed from neural network architecture together with other advanced techniques such as contextual embedding, optimal transport distance and pre-trained language model, etc. have effectively improved the topic quality and document topic distribution. Despite the improvements, these methods lack considerations of effective optimization for complex objective functions that contain log-likelihood and additional regularization terms. In this study, we propose to apply an efficient optimization method to improve the generalization and... | Duc Anh Nguyen, Hoang Tran Vuong, Linh Ngo Van, Quang Duc Nguyen, Sang Dinh, Thien Huu Nguyen, Tue Le, Tung Nguyen |  |
| 313 |  |  [C²: Scalable Auto-Feedback for LLM-based Chart Generation](https://doi.org/10.18653/v1/2025.naacl-long.232) |  | 0 |  | Bongshin Lee, Jaegwan Cho, Jaehyun Kang, Jang Han Yoon, Minhyung Lee, SeYoung Yun, Taehyeon Kim, Woosung Koh, Youngjae Yu, Youngjin Song |  |
| 314 |  |  [A Top-down Graph-based Tool for Modeling Classical Semantic Maps: A Case Study of Supplementary Adverbs](https://doi.org/10.18653/v1/2025.naacl-long.233) |  | 0 | Semantic map models (SMMs) construct a network-like conceptual space from cross-linguistic instances or forms, based on the connectivity hypothesis. This approach has been widely used to represent similarity and entailment relationships in cross-linguistic concept comparisons. However, most SMMs are manually built by human experts using bottom-up procedures, which are often labor-intensive and time-consuming. In this paper, we propose a novel graph-based algorithm that automatically generates conceptual spaces and SMMs in a top-down manner. The algorithm begins by creating a dense graph, which is subsequently pruned into minimal spanning trees, selected according to metrics we propose.... | Cunliang Kong, Maosong Sun, Ying Liu, Zhu Liu |  |
| 315 |  |  [UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers](https://doi.org/10.18653/v1/2025.naacl-long.234) |  | 0 | Existing information retrieval (IR) models often assume a homogeneous structure for knowledge sources and user queries, limiting their applicability in real-world settings where retrieval is inherently heterogeneous and diverse. In this paper, we introduce UniHGKR, a unified instruction-aware heterogeneous knowledge retriever that (1) builds a unified retrieval space for heterogeneous knowledge and (2) follows diverse user instructions to retrieve knowledge in specified types. UniHGKR consists of three principal stages, including heterogeneous self-supervised pretraining, text-anchored embedding alignment, and instruction-aware retriever fine-tuning, enabling it to generalize across varied... | Chenyu You, Dehai Min, Guilin Qi, Lifu Huang, Zhiyang Xu |  |
| 316 |  |  [Improving Model Evaluation using SMART Filtering of Benchmark Datasets](https://doi.org/10.18653/v1/2025.naacl-long.235) |  | 0 | One of the most challenging problems facing NLP today is evaluation. Some of the most pressing issues pertain to benchmark saturation, data contamination, and diversity in the quality of test examples. To address these concerns, we propose Selection Methodology for Accurate, Reduced, and Targeted (SMART) filtering, a novel approach to select a high-quality subset of examples from existing benchmark datasets by systematically removing less informative and lower quality examples. Our approach applies three filtering criteria, removing (i) easy examples, (ii) data-contaminated examples, and (iii) examples that are similar to each other based on distance in an embedding space. We demonstrate... | Adina Williams, Candace Ross, David Pantoja, Megan Ung, Rebecca J. Passonneau, Vipul Gupta |  |
| 317 |  |  [Entropy-Based Decoding for Retrieval-Augmented Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.236) |  | 0 | Augmenting Large Language Models (LLMs) with retrieved external knowledge has proven effective in improving the factual accuracy of generated responses. Despite their success, retrieval-augmented LLMs still face the distractibility issue, where the generated responses are negatively influenced by noise from both external and internal knowledge sources. In this paper, we introduce a novel, training-free decoding method guided by entropy considerations to mitigate this issue. Our approach utilizes entropy-based document-parallel ensemble decoding to prioritize low-entropy distributions from retrieved documents, thereby enhancing the extraction of relevant information of context.... | Aiwei Liu, Bin Wu, Irwin King, Jingjing Li, Zexuan Qiu, Zijing Ou |  |
| 318 |  |  [What We Talk About When We Talk About LMs: Implicit Paradigm Shifts and the Ship of Language Models](https://doi.org/10.18653/v1/2025.naacl-long.237) |  | 0 | The term Language Models (LMs) as a time-specific collection of models of interest is constantly reinvented, with its referents updated much like the \*Ship of Theseus\* replaces its parts but remains the same ship in essence. In this paper, we investigate this \*Ship of Language Models\* problem, wherein scientific evolution takes the form of continuous, implicit retrofits of key \*existing\* terms. We seek to initiate a novel perspective of scientific progress, in addition to the more well-studied emergence of \*new\* terms. To this end, we construct the data infrastructure based on recent NLP publications. Then, we perform a series of text-based analyses toward a detailed, quantitative... | Jeffrey M. Rzeszotarski, Shengqi Zhu |  |
| 319 |  |  [Diversity Helps Jailbreak Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.238) |  | 0 | We have uncovered a powerful jailbreak technique that leverages large language models’ ability to diverge from prior context, enabling them to bypass safety constraints and generate harmful outputs. By simply instructing the LLM to deviate and obfuscate previous attacks, our method dramatically outperforms existing approaches, achieving up to a 62.83% higher success rate in compromising ten leading chatbots, including GPT-4, Gemini, and Llama, while using only 12.9% of the queries. This revelation exposes a critical flaw in current LLM safety training, suggesting that existing methods may merely mask vulnerabilities rather than eliminate them. Our findings sound an urgent alarm for the... | Chengzhi Mao, Daniel BenLevi, Junfeng Yang, Wei Hao, Weiliang Zhao |  |
| 320 |  |  [Constrained Decoding with Speculative Lookaheads](https://doi.org/10.18653/v1/2025.naacl-long.239) |  | 0 | Constrained decoding with lookahead heuristics (CDLH) is a highly effective method for aligning LLM generations to human preferences. However, the extensive lookahead roll-out operations for each generated token makes CDLH prohibitively expensive, resulting in low adoption in practice. In contrast, common decoding strategies such as greedy decoding are extremely efficient, but achieve very low constraint satisfaction. We propose constrained decoding with speculative lookaheads (CDSL), a technique that significantly improves upon the inference efficiency of CDLH without experiencing the drastic performance reduction seen with greedy decoding. CDSL is motivated by the recently proposed idea... | Leonid Boytsov, Nishanth Sridhar Nakshatri, Rajarshi Das, Rashmi Gangadharaiah, Shamik Roy, Suthee Chaidaroon |  |
| 321 |  |  [DyPCL: Dynamic Phoneme-level Contrastive Learning for Dysarthric Speech Recognition](https://doi.org/10.18653/v1/2025.naacl-long.240) |  | 0 | Dysarthric speech recognition often suffers from performance degradation due to the intrinsic diversity of dysarthric severity and extrinsic disparity from normal speech. To bridge these gaps, we propose a Dynamic Phoneme-level Contrastive Learning (DyPCL) method, which leads to obtaining invariant representations across diverse speakers. We decompose the speech utterance into phoneme segments for phoneme-level contrastive learning, leveraging dynamic connectionist temporal classification alignment. Unlike prior studies focusing on utterance-level embeddings, our granular learning allows discrimination of subtle parts of speech. In addition, we introduce dynamic curriculum learning, which... | Gary Lee, Heejin Do, Jungseul Ok, Solee Im, Wonjun Lee, Yunsu Kim |  |
| 322 |  |  [Revisiting Early Detection of Sexual Predators via Turn-level Optimization](https://doi.org/10.18653/v1/2025.naacl-long.241) |  | 0 | Online grooming is a severe social threat where sexual predators gradually entrap child victims with subtle and gradual manipulation. Therefore, timely intervention for online grooming is critical for proactive protection. However, previous methods fail to determine the optimal intervention points (i.e., jump to conclusions) as they rely on chat-level risk labels by causing weak supervision of risky utterances. For timely detection, we propose speed control reinforcement learning (SCoRL), incorporating a practical strategy derived from luring communication theory (LCT). To capture the predator’s turn-level entrapment, we use a turn-level risk label based on the LCT. Then, we design a novel... | Gary Lee, Heejin Do, Jinmyeong An, Jungseul Ok, Sangwon Ryu, Yunsu Kim |  |
| 323 |  |  [StyleTTS-ZS: Efficient High-Quality Zero-Shot Text-to-Speech Synthesis with Distilled Time-Varying Style Diffusion](https://doi.org/10.18653/v1/2025.naacl-long.242) |  | 0 | The rapid development of large-scale text-to-speech (TTS) models has led to significant advancements in modeling diverse speaker prosody and voices. However, these models often face issues such as slow inference speeds, reliance on complex pre-trained neural codec representations, and difficulties in achieving naturalness and high similarity to reference speakers. To address these challenges, this work introduces StyleTTS-ZS, an efficient zero-shot TTS model that leverages distilled time-varying style diffusion to capture diverse speaker identities and prosodies. We propose a novel approach that represents human speech using input text and fixed-length time-varying discrete style codes to... | Cong Han, Nima Mesgarani, Xilin Jiang, Yinghao Aaron Li |  |
| 324 |  |  [Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-long.243) |  | 0 | Large Language Models (LLMs) have demonstrated significant performance improvements across various cognitive tasks. An emerging application is using LLMs to enhance retrieval-augmented generation (RAG) capabilities. These systems require LLMs to understand user queries, retrieve relevant information, and synthesize coherent and accurate responses. Given the increasing real-world deployment of such systems, comprehensive evaluation becomes crucial. To this end, we propose FRAMES (Factuality, Retrieval, And reasoning MEasurement Set), a high-quality evaluation dataset designed to test LLMs’ ability to provide factual responses, assess retrieval capabilities, and evaluate the reasoning... | Adam Stambler, Anhad Mohananey, Kalpesh Krishna, Manaal Faruqui, Satyapriya Krishna, Shyam Upadhyay, Steven Schwarcz |  |
| 325 |  |  [ReachAgent: Enhancing Mobile Agent via Page Reaching and Operation](https://doi.org/10.18653/v1/2025.naacl-long.244) |  | 0 | Recently, mobile AI agents have gained increasing attention. Given a task, mobile AI agents can interact with mobile devices in multiple steps and finally form a GUI flow that solves the task. However, existing agents tend to focus on most task-relevant elements at each step, leading to local optimal solutions and ignoring the overall GUI flow. To address this issue, we constructed a training dataset called MobileReach, which breaks the task into page reaching and operation subtasks. Furthermore, we propose ReachAgent, a two-stage framework that focuses on improving its task-completion abilities. It utilizes the page reaching and page operation subtasks, along with reward-based preference... | Bin Wang, Jian Luan, Qinzhuo Wu, Wei Liu |  |
| 326 |  |  [Learning to Solve Domain-Specific Calculation Problems with Knowledge-Intensive Programs Generator](https://doi.org/10.18653/v1/2025.naacl-long.245) |  | 0 | Domain Large Language Models (LLMs) are developed for domain-specific tasks based on general LLMs. But it still requires professional knowledge to facilitate the expertise for some domain-specific tasks. In this paper, we investigate into knowledge-intensive calculation problems. We find that the math problems to be challenging for LLMs, when involving complex domain-specific rules and knowledge documents, rather than simple formulations of terminologies. Therefore, we propose a pipeline to solve the domain-specific calculation problems with Knowledge-Intensive Programs Generator more effectively, named as KIPG. It generates knowledge-intensive programs according to the domain-specific... | Chengyuan Liu, Fei Wu, Ji Zhang, Jun Lin, Kun Kuang, Lizhi Qing, Shihang Wang |  |
| 327 |  |  [SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity Mixture](https://doi.org/10.18653/v1/2025.naacl-long.246) |  | 0 | Despite the recent efforts from the NLP community, balancing the training budget, downstream performance, and general capabilities of large language models (LLM) remains a challenge in many applications. Training the entire model for downstream tasks is expensive, and could easily result in catastrophic forgetting. Parameter-efficient fine-tuning (PEFT) could reduce the training cost, but it still suffers from forgetting, and limits the learning on the downstream tasks. To address the aforementioned issues, we propose a novel mixture of expert (MoE) framework based on Soft LoRA and Identity Mixture (SLIM). SLIM allows dynamic routing between LoRA adapters and identity layers, thus enabling... | Donghong Han, Hongwei Du, Jiayi Han, Liang Du, Weibo Zheng, Xiangguo Zhou, Yiwen Wu, Yuanfang Zhang |  |
| 328 |  |  [MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.247) |  | 0 | Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding and reasoning abilities, often assessed through multiple-choice questions (MCQs) that include an image, a question, and several options. However, many benchmarks used for such evaluations suffer from systematic biases. Remarkably, Large Language Models (LLMs) without any visual perception capabilities achieve non-trivial performance, undermining the credibility of these evaluations. To address this issue while maintaining the efficiency of MCQ evaluations, we propose MMEVALPRO, a benchmark designed to avoid Type-I errors through a trilogy evaluation pipeline and more rigorous metrics. For each original question... | Baobao Chang, Bohan Wu, Fu Zeng, Haozhe Zhao, Jingyang Yuan, Jinsheng Huang, Liang Chen, Luchen Liu, Ming Zhang, Taian Guo, Tianyu Liu, Wei Ju, Ye Yuan, Yichi Zhang, Yusheng Zhao, Zhihui Guo |  |
| 329 |  |  [MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning](https://doi.org/10.18653/v1/2025.naacl-long.248) |  | 0 | Efficient finetuning of large language models (LLMs) aims to adapt the LLMs with reduced computational and memory costs. Previous LoRA-based approaches initialize the low-rank matrices with Gaussian distribution and zero values while keeping the original weight matrices frozen. However, the trainable model parameters optimized in an unguided subspace might interfere with the well-learned subspace of the pretrained weight matrices. In this paper, we propose MiLoRA, a simple yet effective LLM finetuning approach that only updates the minor singular components of the weight matrix while keeping the principal singular components frozen. It is observed that the minor matrix corresponds to the... | Guanhua Chen, Hanqing Wang, Shuo Wang, Yixia Li, Yun Chen |  |
| 330 |  |  [Analyzing (In)Abilities of SAEs via Formal Languages](https://doi.org/10.18653/v1/2025.naacl-long.249) |  | 0 | Autoencoders have been used for finding interpretable and disentangled features underlying neural network representations in both image and text domains. While the efficacy and pitfalls of such methods are well-studied in vision, there is a lack of corresponding results, both qualitative and quantitative, for the text domain. We aim to address this gap by training sparse autoencoders (SAEs) on a synthetic testbed of formal languages. Specifically, we train SAEs on the hidden representations of models trained on formal languages (Dyck-2, Expr, and English PCFG) under a wide variety of hyperparameter settings, finding interpretable latents often emerge in the features learned by our SAEs.... | Abhinav Menon, David Krueger, Ekdeep Singh Lubana, Manish Shrivastava |  |
| 331 |  |  [Multimodal Cognitive Reframing Therapy via Multi-hop Psychotherapeutic Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.250) |  | 0 | Previous research has revealed the potential of large language models (LLMs) to support cognitive reframing therapy; however, their focus was primarily on text-based methods, often overlooking the importance of non-verbal evidence crucial in real-life therapy. To alleviate this gap, we extend the textual cognitive reframing to multimodality, incorporating visual clues. Specifically, we present a new dataset called Multi Modal-Cognitive Support Conversation (M2CoSC), which pairs each GPT-4-generated dialogue with an image that reflects the virtual client’s facial expressions.To better mirror real psychotherapy, where facial expressions lead to interpreting implicit emotional evidence, we... | Gary Lee, Heejin Do, Hoonrae Kim, Subin Kim |  |
| 332 |  |  [Explanation based In-Context Demonstrations Retrieval for Multilingual Grammatical Error Correction](https://doi.org/10.18653/v1/2025.naacl-long.251) |  | 0 | Grammatical error correction (GEC) aims to correct grammatical, spelling, and semantic errors in natural language text. With the growing of large language models (LLMs), direct text generation has gradually become the focus of the GEC methods, and few-shot in-context learning presents a cost-effective solution. However, selecting effective in-context examples remains challenging, as the similarity between input texts does not necessarily correspond to similar grammatical error patterns. In this paper, we propose a novel retrieval method based on natural language grammatical error explanations (GEE) to address this issue. Our method retrieves suitable few-shot demonstrations by matching the... | Guangyue Peng, Houfeng Wang, Wei Li, Wen Luo |  |
| 333 |  |  [A Unified Supervised and Unsupervised Dialogue Topic Segmentation Framework Based on Utterance Pair Modeling](https://doi.org/10.18653/v1/2025.naacl-long.252) |  | 0 | The Dialogue Topic Segmentation task aims to divide a dialogue into different topic paragraphs in order to better understand the structure and content of the dialogue. Due to the short sentences, serious references and non-standard language in the dialogue, it is difficult to determine the boundaries of the topic. Although the unsupervised approaches based on LLMs performs well, it is still difficult to surpass the supervised methods based on classical models in specific domains. To this end, this paper proposes UPS (Utterance Pair Segment), a dialogue topic segmentation method based on utterance pair relationship modeling, unifying the supervised and unsupervised network architectures.... | Chunsheng Qin, Shihao Yang, Shuhua Liu, Yue Jiang, Ziyi Zhang |  |
| 334 |  |  [Evaluating Small Language Models for News Summarization: Implications and Factors Influencing Performance](https://doi.org/10.18653/v1/2025.naacl-long.253) |  | 0 | The increasing demand for efficient summarization tools in resource-constrained environments highlights the need for effective solutions. While large language models (LLMs) deliver superior summarization quality, their high computational resource requirements limit practical use applications. In contrast, small language models (SLMs) present a more accessible alternative, capable of real-time summarization on edge devices. However, their summarization capabilities and comparative performance against LLMs remain underexplored. This paper addresses this gap by presenting a comprehensive evaluation of 19 SLMs for news summarization across 2,000 news samples, focusing on relevance, coherence,... | Bingsheng He, Borui Xu, Weiguo Liu, Yao Chen, Zeyi Wen |  |
| 335 |  |  [Dynamic Fisher-weighted Model Merging via Bayesian Optimization](https://doi.org/10.18653/v1/2025.naacl-long.254) |  | 0 | The fine-tuning of pre-trained language models has resulted in the widespread availability of task-specific models. Model merging offers an efficient way to create multi-task models by combining these fine-tuned models at the parameter level, without the need for training data or joint training on multiple datasets. Existing merging approaches typically involve scaling the parameters model-wise or integrating parameter importance parameter-wise. Both approaches exhibit their own weaknesses, leading to a notable performance gap compared to multi-task fine-tuning. In this paper, we unify these seemingly distinct strategies into a more general merging framework, and introduce Dynamic... | Jiahao Liu, Jingang Wang, Qifan Wang, Sanwoo Lee, Xunliang Cai, Yunfang Wu |  |
| 336 |  |  [AI-Assisted Human Evaluation of Machine Translation](https://doi.org/10.18653/v1/2025.naacl-long.255) |  | 0 |  | Mrinmaya Sachan, Tom Kocmi, Vilém Zouhar |  |
| 337 |  |  [MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria](https://doi.org/10.18653/v1/2025.naacl-long.256) |  | 0 | Multimodal large language models (MLLMs) have broadened the scope of AI applications. Existing automatic evaluation methodologies for MLLMs are mainly limited in evaluating objective queries without considering real-world user experiences, inadequately addressing the nuances of creative and associative multimodal tasks. However, the open-ended and subjective nature of such tasks poses a significant challenge to the evaluation methodology, where it is difficult to define the ground-truth answers for them. To this end, in our paper, we propose a new evaluation paradigm for MLLMs, which is evaluating MLLMs with per-sample criteria using potent MLLM as the judge. To validate the feasibility... | Anningzhe Gao, Benyou Wang, ChenghaoZhu ChenghaoZhu, Dingjie Song, Hardy Chen, Jianquan Li, Junying Chen, Nuo Chen, Shunian Chen, Shuo Yan, Wentao Ge, Wenya Xie, Xiang Wan, Xidong Wang, Zhihong Chen, Zhiyi Zhang, Ziyue Lin |  |
| 338 |  |  [AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios](https://doi.org/10.18653/v1/2025.naacl-long.257) |  | 0 | Large language models (LLMs) are increasingly leveraged to empower autonomous agents to simulate human beings in various fields of behavioral research. However, evaluating their capacity to navigate complex social interactions remains a challenge. Previous studies face limitations due to insufficient scenario diversity, complexity, and a single-perspective focus. To this end, we introduce AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios. Drawing on Dramaturgical Theory, AgentSense employs a bottom-up approach to create 1,225 diverse social scenarios constructed from extensive scripts. We evaluate LLM-driven agents through multi-turn... | Haoyu Kuang, Jiayu Lin, Jingcong Liang, Lei Chen, Rong Ye, Shiyue Yang, Xiawei Liu, Xinnong Zhang, Xinyi Mou, Xuanjing Huang, Zhongyu Wei |  |
| 339 |  |  [FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data](https://doi.org/10.18653/v1/2025.naacl-long.258) |  | 0 | Prior research on training grounded factuality classification models to detect hallucinations in large language models (LLMs) has relied on public natural language inference (NLI) data and synthetic data. However, conventional NLI datasets are not well-suited for document-level reasoning, which is critical for detecting LLM hallucinations. Recent approaches to document-level synthetic data generation involve iteratively removing sentences from documents and annotating factuality using LLM-based prompts. While effective, this method is computationally expensive for long documents and limited by the LLM’s capabilities. In this work, we analyze the differences between existing synthetic... | Alex Deng, Deren Lei, Emily Ching, Ken Archer, Mengya Hu, Mingyu Wang, Rui Xu, Siyao Li, Yaxi Li |  |
| 340 |  |  [Label Drop for Multi-Aspect Relation Modeling in Universal Information Extraction](https://doi.org/10.18653/v1/2025.naacl-long.259) |  | 0 | Universal Information Extraction (UIE) has garnered significant attention due to its ability to address model explosion problems effectively. Extractive UIE can achieve strong performance using a relatively small model, making it widely adopted. Extractive UIEs generally rely on task instructions for different tasks, including single-target instructions and multiple-target instructions. Single-target instruction UIE enables the extraction of only one type of relation at a time, limiting its ability to model correlations between relations and thus restricting its capability to extract complex relations. While multiple-target instruction UIE allows for the extraction of multiple relations... | En Ci, Jiajia Li, Lefei Zhang, Lu Yang, Ping Wang, Zuchao Li |  |
| 341 |  |  [Test-Time Code-Switching for Cross-lingual Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2025.naacl-long.260) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) is a thriving research area with impressive outcomes being achieved on high-resource languages. However, the application of cross-lingual transfer to the ASTE task has been relatively unexplored, and current code-switching methods still suffer from term boundary detection issues and out-of-dictionary problems. In this study, we introduce a novel Test-Time Code-SWitching (TT-CSW) framework, which bridges the gap between the bilingual training phase and the monolingual test-time prediction. During training, a generative model is developed based on bilingual code-switched training data and can produce bilingual ASTE triplets for bilingual inputs. In... | Dongming Sheng, Hao Li, Jun Lang, Kexin Han, Wenqiang Liu, Yan Zhang, Yucheng Huang |  |
| 342 |  |  [VisCGEC: Benchmarking the Visual Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2025.naacl-long.261) |  | 0 |  | Dan Yuan, Xiaoman Wang, Xiaoxiao Zhang, Xin Liu, Xizhi Chen, Yike Zhao, Yunshi Lan |  |
| 343 |  |  [Are We Done with MMLU?](https://doi.org/10.18653/v1/2025.naacl-long.262) |  | 0 | Maybe not. We identify and analyse errors in the popular Massive Multitask Language Understanding (MMLU) benchmark. Even though MMLU is widely adopted, our analysis demonstrates numerous ground truth errors that obscure the true capabilities of LLMs. For example, we find that 57% of the analysed questions in the Virology subset contain errors. To address this issue, we introduce a comprehensive framework for identifying dataset errors using a novel error annotation protocol. Then, we create MMLU-Redux, which is a subset of 5,700 manually re-annotated questions across all 57 MMLU subjects. Using MMLU-Redux, we demonstrate significant discrepancies with the model performance metrics that... | Alberto Carlo Maria Mancino, Alessio Devoto, Aryo Pradipta Gema, Claire Barale, Emile van Krieken, Giwon Hong, Jean Kaddour, Joshua Harris, Joshua Ong Jun Leang, Mohammad Reza Ghasemi Madani, Pasquale Minervini, Robert McHardy, Rohit Saxena, Xiaotang Du, Xuanli He, Yu Zhao |  |
| 344 |  |  [MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling](https://doi.org/10.18653/v1/2025.naacl-long.263) |  | 0 | Integrating tools into Large Language Models (LLMs) has facilitated the widespread application. Despite this, in specialized downstream task contexts, reliance solely on tools is insufficient to fully address the complexities of the real world. This particularly restricts the effective deployment of LLMs in fields such as medicine. In this paper, we focus on the downstream tasks of medical calculators, which use standardized tests to assess an individual’s health status. We introduce MeNTi, a universal agent architecture for LLMs. MeNTi integrates a specialized medical toolkit and employs meta-tool and nested calling mechanisms to enhance LLM tool utilization. Specifically, it achieves... | Kui Xue, Shaohang Wei, Shaoting Zhang, Xiaofan Zhang, Xu Wang, Yakun Zhu |  |
| 345 |  |  [Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering](https://doi.org/10.18653/v1/2025.naacl-long.264) |  | 0 | Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context—this phenomenon, known as context-memory knowledge conflicts, can lead to undesirable model behaviour, such as reliance on outdated or incorrect information. Analysing the internal activations of LLMs, we find that they can internally register the signals of knowledge conflict at mid-layers. Such signals allow us to detect whether a knowledge conflict occurs and use inference-time intervention strategies to resolve it. In this work, we propose SpARE, a training-free representation engineering... | Alessio Devoto, Aryo Pradipta Gema, Giwon Hong, Hongru Wang, KamFai Wong, Pasquale Minervini, Xiaotang Du, Xuanli He, Yu Zhao |  |
| 346 |  |  [MoDification: Mixture of Depths Made Easy](https://doi.org/10.18653/v1/2025.naacl-long.265) |  | 0 | Long-context efficiency has recently become a trending topic in serving large language models (LLMs). And mixture of depths (MoD) is proposed as a perfect fit to bring down both latency and memory. In this paper, however, we discover that MoD can barely transform existing LLMs without costly training over an extensive number of tokens. To enable the transformations from any LLMs to MoD ones, we showcase top-k operator in MoD should be promoted to threshold-p operator, and refinement to architecture and data should also be crafted along. All these designs form our method termed MoDification. Through a comprehensive set of experiments covering model scales from 3B to 70B, we exhibit... | Chen Zhang, Chengqiang Lu, Dawei Song, Kehai Chen, Meizhi Zhong, Min Zhang, Qimeng Wang, Xuantao Lu, Yan Gao, Yao Hu, Zheyu Ye |  |
| 347 |  |  [On the Vulnerability of Text Sanitization](https://doi.org/10.18653/v1/2025.naacl-long.266) |  | 0 | Text sanitization, which employs differential privacy to replace sensitive tokens with new ones, represents a significant technique for privacy protection. Typically, its performance in preserving privacy is evaluated by measuring the attack success rate (ASR) of reconstruction attacks, where attackers attempt to recover the original tokens from the sanitized ones. However, current reconstruction attacks on text sanitization are developed empirically, making it challenging to accurately assess the effectiveness of sanitization. In this paper, we aim to provide a more accurate evaluation of sanitization effectiveness. Inspired by the works of Palamidessi et al., we implement theoretically... | Jiayang Liu, Jie Zhang, Kejiang Chen, Meng Tong, Nenghai Yu, Weiming Zhang, Xiaojian Yuan |  |
| 348 |  |  [Multilingual Needle in a Haystack: Investigating Long-Context Behavior of Multilingual Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.267) |  | 0 | While recent large language models (LLMs) demonstrate remarkable abilities in responding to queries in diverse languages, their ability to handle long multilingual contexts is unexplored. As such, a systematic evaluation of the long-context capabilities of LLMs in multilingual settings is crucial, specifically in the context of information retrieval. To address this gap, we introduce the MultiLingual Needle-in-a-Haystack (MLNeedle) test, designed to assess a model’s ability to retrieve relevant information (the needle) from a collection of multilingual distractor texts (the haystack). This test serves as an extension of the multilingual question-answering task, encompassing both... | Amey Hengle, Prasoon Bajpai, Soham Dan, Tanmoy Chakraborty |  |
| 349 |  |  [Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation](https://doi.org/10.18653/v1/2025.naacl-long.268) |  | 0 | Claim verification is a long-standing and challenging task that demands not only high accuracy but also explainability and thoroughness of the verification process. This task becomes an emerging research issue in the era of large language models (LLMs) since real-world claims are often complex, featuring intricate semantic structures or obfuscated entities. Traditional approaches typically address this by decomposing claims into sub-claims and querying a knowledge base to resolve hidden or ambiguous entities. However, the absence of effective disambiguation strategies for these entities can compromise the entire verification process. To address these challenges, we propose... | Hoang Pham, KhacHoai Nam Bui, ThanhDo Nguyen |  |
| 350 |  |  [Exploring the Potential of Large Language Models for Heterophilic Graphs](https://doi.org/10.18653/v1/2025.naacl-long.269) |  | 0 | Large language models (LLMs) have presented significant opportunities to enhance various machine learning applications, including graph neural networks (GNNs). By leveraging the vast open-world knowledge within LLMs, we can more effectively interpret and utilize textual data to better characterize heterophilic graphs, where neighboring nodes often have different labels. However, existing approaches for heterophilic graphs overlook the rich textual data associated with nodes, which could unlock deeper insights into their heterophilic contexts. In this work, we explore the potential of LLMs for modeling heterophilic graphs and propose a novel two-stage framework: LLM-enhanced edge... | Chuan Shi, Shujie Li, Yuan Fang, Yuxia Wu |  |
| 351 |  |  [Exploiting Edited Large Language Models as General Scientific Optimizers](https://doi.org/10.18653/v1/2025.naacl-long.270) |  | 0 | Large language models (LLMs) have been widely adopted in mathematical optimization in scientific scenarios for their extensive knowledge and advanced reasoning capabilities. Existing methods mainly focus on utilizing LLMs to solve optimization problems in a prompt-based manner, which takes observational feedback as additional textual descriptions. However, due to LLM’s \*\*high sensitivity to the prompts\*\* and \*\*tendency to get lost in lengthy prompts\*\*, these methods struggle to effectively utilize the observational feedback from each optimization step, which severely hinders the applications for real-world scenarios. To address these challenges, we propose a conceptually simple and... | Hong Wang, Qitan Lv, Tianyu Liu |  |
| 352 |  |  [DIRAS: Efficient LLM Annotation of Document Relevance for Retrieval Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-long.271) |  | 0 | Retrieval Augmented Generation (RAG) is widely employed to ground responses to queries on domain-specific documents. But do RAG implementations leave out important information when answering queries that need an integrated analysis of information (e.g., Tell me good news in the stock market today.)? To address these concerns, RAG developers need to annotate information retrieval (IR) data for their domain of interest, which is challenging because (1) domain-specific queries usually need nuanced definitions of relevance beyond shallow semantic relevance; and (2) human or GPT-4 annotation is costly and cannot cover all (query, document) pairs (i.e., annotation selection bias), thus harming... | Elliott Ash, Jingwei Ni, Markus Leippold, Meihong Lin, Mrinmaya Sachan, Tobias Schimanski |  |
| 353 |  |  [Hello Again! LLM-powered Personalized Agent for Long-term Dialogue](https://doi.org/10.18653/v1/2025.naacl-long.272) |  | 0 | Open-domain dialogue systems have seen remarkable advancements with the development of large language models (LLMs). Nonetheless, most existing dialogue systems predominantly focus on brief single-session interactions, neglecting the real-world demands for long-term companionship and personalized interactions with chatbots. Crucial to addressing this real-world need are event summary and persona management, which enable reasoning for appropriate long-term dialogue responses. Recent progress in the human-like cognitive and reasoning capabilities of LLMs suggests that LLM-based agents could significantly enhance automated perception, decision-making, and problem-solving. In response to this... | An Zhang, Chenghao Yang, Hao Li, TatSeng Chua, Xiang Wang, Yang Deng |  |
| 354 |  |  [My LLM might Mimic AAE - But When Should It?](https://doi.org/10.18653/v1/2025.naacl-long.273) |  | 0 | We examine the representation of African American English (AAE) in large language models (LLMs), exploring (a) the perceptions Black Americans have of how effective these technologies are at producing authentic AAE, and (b) in what contexts Black Americans find this desirable. Through both a survey of Black Americans (n= 104) and annotation of LLM-produced AAE by Black Americans (n= 228), we find that Black Americans favor choice and autonomy in determining when AAE is appropriate in LLM output. They tend to prefer that LLMs default to communicating in Mainstream U.S. English in formal settings, with greater interest in AAE production in less formal settings. When LLMs were appropriately... | Christabel Acquaye, Hal Daumé III, Kwesi A. Cobbina, Mohammad Nayeem Teli, Sandra Sandoval |  |
| 355 |  |  [High-Dimension Human Value Representation in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.274) |  | 0 | The widespread application of Large Language Models (LLMs) across various tasks and fields has necessitated the alignment of these models with human values and preferences. Given various approaches of human value alignment, such as Reinforcement Learning with Human Feedback (RLHF), constitutional learning, and safety fine-tuning etc., there is an urgent need to understand the scope and nature of human values injected into these LLMs before their deployment and adoption. We propose UniVar, a high-dimensional neural representation of symbolic human value distributions in LLMs, orthogonal to model architecture and training data. This is a continuous and scalable representation,... | Bryan Wilie, Delong Chen, Etsuko Ishii, Leila Khalatbari, Pascale Fung, Samuel Cahyawijaya, Yejin Bang, Ziwei Ji |  |
| 356 |  |  [Not all Hallucinations are Good to Throw Away When it Comes to Legal Abstractive Summarization](https://doi.org/10.18653/v1/2025.naacl-long.275) |  | 0 | Automatic summarization of legal documents requires a thorough understanding of their specificities, mainly with respect to the vocabulary used by legal experts. Indeed, the latter rely heavily on their external knowledge when writing summaries, in order to contextualize the main entities of the source document. This leads to reference summaries containing many abstractions, that sota models struggle to generate. In this paper, we propose an entity-driven approach aiming at learning the model to generate factual hallucinations, as close as possible to the abstractions of the reference summaries. We evaluated our approach on two different datasets, with legal documents in English and... | Gilles Hubert, Karen PinelSauvagnat, Mokhtar Boumedyen Billami, Nihed Bendahman |  |
| 357 |  |  [Query-focused Referentiability Learning for Zero-shot Retrieval](https://doi.org/10.18653/v1/2025.naacl-long.276) |  | 0 | Dense passage retrieval enhances Information Retrieval (IR) by encoding queries and passages into representation space. However, passage representations often fail to be referenced by their gold queries under domain shifts, revealing a weakness in representation space. One desirable concept for representations is ”argmaxable”. Being argmaxable ensures that no representations are theoretically excluded from selection due to geometric constraints. To be argmaxable, a notable approach is to increase isotropy, where representations are evenly spread out in all directions. These findings, while desirable also for IR, focus on passage representation and not on query, making it challenging to... | Dohyeon Lee, Jaeyoung Kim, Seungwon Hwang |  |
| 358 |  |  [A Novel Computational Modeling Foundation for Automatic Coherence Assessment](https://doi.org/10.18653/v1/2025.naacl-long.277) |  | 0 | Coherence is an essential property of well-written texts, that refers to the way textual units relate to one another. In the era of generative AI, coherence assessment is essential for many NLP tasks such as summarization, long-form question-answering, and more.Current NLP approaches for modeling coherence often rely on a proxy task, specifically, sentence reordering. However, such an approach may not capture the full range of factors contributing to coherence.To remedy this, in this work we employ the formal linguistic definition by Reinhart:1980 of what makes a discourse coherent, consisting of three conditions, cohesion, consistency and relevance, and formalize these conditions as... | Aviya Maimon |  |
| 359 |  |  [Token-based Decision Criteria Are Suboptimal in In-context Learning](https://doi.org/10.18653/v1/2025.naacl-long.278) |  | 0 | In-Context Learning (ICL) typically utilizes classification criteria from output probabilities of manually selected label tokens. However, we argue that such token-based classification criteria lead to suboptimal decision boundaries, despite delicate calibrations through translation and constrained rotation applied. To address this problem, we propose Hidden Calibration, which renounces token probabilities and uses the nearest centroid classifier on the LM’s last hidden states. In detail, we assign the label of the nearest centroid previously estimated from a calibration set to the test sample as the predicted label. Our experiments on 6 models and 10 classification datasets indicate that... | Akira Ishii, Hakaze Cho, Kenshiro Tanaka, Mariko Kato, Naoya Inoue, Yoshihiro Sakai |  |
| 360 |  |  [CSEval: Towards Automated, Multi-Dimensional, and Reference-Free Counterspeech Evaluation using Auto-Calibrated LLMs](https://doi.org/10.18653/v1/2025.naacl-long.279) |  | 0 | Counterspeech has emerged as a popular and effective strategy for combating online hate speech, sparking growing research interest in automating its generation using language models. However, the field still lacks standardised evaluation protocols and reliable automated evaluation metrics that align with human judgement. Current automatic evaluation methods, primarily based on similarity metrics, do not effectively capture the complex and independent attributes of counterspeech quality, such as contextual relevance, aggressiveness, or argumentative coherence. This has led to an increased dependency on labor-intensive human evaluations to assess automated counter-speech generation methods.... | Amey Hengle, Anil Bandhakavi, Aswini Kumar Padhi, Tanmoy Chakraborty |  |
| 361 |  |  [Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study](https://doi.org/10.18653/v1/2025.naacl-long.280) |  | 0 | Large language models (LLMs) have shown continuously improving multilingual capabilities, and even small-scale open-source models have demonstrated rapid performance enhancement. In this paper, we systematically explore the abilities of open LLMs with less than ten billion parameters to handle multilingual machine translation (MT) tasks. We conduct comprehensive evaluations on six popular LLMs and find that models like Gemma2-9B exhibit impressive multilingual translation capabilities. We then introduce the Parallel-First Monolingual-Second (PFMS) data mixing strategy in the continual pretraining stage to further enhance the MT performance and present GemmaX2-28, a 9B model achieving... | Bin Wang, Jian Luan, Menglong Cui, Pengzhi Gao, Wei Liu |  |
| 362 |  |  [RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.281) |  | 0 | Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming.However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model’s safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for... | Bang An, Mark Dredze, Shiyue Zhang |  |
| 363 |  |  [Evaluating Evidence Attribution in Generated Fact Checking Explanations](https://doi.org/10.18653/v1/2025.naacl-long.282) |  | 0 | Automated fact-checking systems often struggle with trustworthiness, as their generated explanations can include hallucinations. In this work, we explore evidence attribution for fact-checking explanation generation. We introduce a novel evaluation protocol, citation masking and recovery, to assess attribution quality in generated explanations. We implement our protocol using both human annotators and automatic annotators and found that LLM annotation correlates with human annotation, suggesting that attribution assessment can be automated. Finally, our experiments reveal that: (1) the best-performing LLMs still generate explanations that are not always accurate in their attribution; and... | Jey Han Lau, Rui Xing, Timothy Baldwin |  |
| 364 |  |  [ETHIC: Evaluating Large Language Models on Long-Context Tasks with High Information Coverage](https://doi.org/10.18653/v1/2025.naacl-long.283) |  | 0 | Recent advancements in large language models (LLM) capable of processing extremely long texts highlight the need for a dedicated evaluation benchmark to assess their long-context capabilities. However, existing methods, like the needle-in-a-haystack test, do not effectively assess whether these models fully utilize contextual information, raising concerns about the reliability of current evaluation techniques. To thoroughly examine the effectiveness of existing benchmarks, we introduce a new metric called information coverage (IC), which quantifies the proportion of the input context necessary for answering queries. Our findings indicate that current benchmarks exhibit low IC; although the... | Chanwoong Yoon, Donghyeon Lee, Hyunjae Kim, Jaewoo Kang, Kyochul Jang, Minju Song, Taewhoo Lee |  |
| 365 |  |  [Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors](https://doi.org/10.18653/v1/2025.naacl-long.284) |  | 0 | In-context Learning (ICL) has become the primary method for performing natural language tasks with Large Language Models (LLMs). The knowledge acquired during pre-training is crucial for this few-shot capability, providing the model with task priors. However, recent studies have shown that ICL predominantly relies on retrieving task priors rather than “learning” to perform tasks. This limitation is particularly evident in complex subjective domains such as emotion and morality, where priors significantly influence posterior predictions. In this work, we examine whether this is the result of the aggregation used in corresponding datasets, where trying to combine low-agreement, disparate... | Alexandros Potamianos, Georgios Chochlakis, Kristina Lerman, Shrikanth Narayanan |  |
| 366 |  |  [Arabic Dataset for LLM Safeguard Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.285) |  | 0 | The growing use of large language models (LLMs) has raised concerns regarding their safety. While many studies have focused on English, the safety of LLMs in Arabic, with its linguistic and cultural complexities, remains under-explored. Here, we aim to bridge this gap. In particular, we present an Arab-region-specific safety evaluation dataset consisting of 5,799 questions, including direct attacks, indirect attacks, and harmless requests with sensitive words, adapted to reflect the socio-cultural context of the Arab world. To uncover the impact of different stances in handling sensitive and controversial topics, we propose a dual-perspective evaluation framework. It assesses the LLM... | Bin Gu, Preslav Nakov, Timothy Baldwin, Yasser Ashraf, Yuxia Wang |  |
| 367 |  |  [Anticipating Future with Large Language Model for Simultaneous Machine Translation](https://doi.org/10.18653/v1/2025.naacl-long.286) |  | 0 | Simultaneous machine translation (SMT) takes streaming input utterances and incrementally produces target text. Existing SMT methods only use the partial utterance that has already arrived at the input and the generated hypothesis. Motivated by human interpreters’ technique to forecast future words before hearing them, we propose Translation by Anticipating Future (TAF), a method to improve translation quality while retaining low latency. Its core idea is to use a large language model (LLM) to predict future source words and opportunistically translate without introducing too much risk. We evaluate our TAF and multiple baselines of SMT on four language directions. Experiments show that TAF... | Boris Ginsburg, Jagadeesh Balam, Lei Li, Oleksii Hrinchuk, Siqi Ouyang, Vitaly Lavrukhin, Zhehuai Chen |  |
| 368 |  |  [GuideLLM: Exploring LLM-Guided Conversation with Applications in Autobiography Interviewing](https://doi.org/10.18653/v1/2025.naacl-long.287) |  | 0 | Although Large Language Models (LLMs) succeed in human-guided conversations such as instruction following and question answering, the potential of LLM-guided conversations—where LLMs direct the discourse and steer the conversation’s objectives—remains under-explored. In this study, we first characterize LLM-guided conversation into three fundamental components: (i) Goal Navigation; (ii) Context Management; (iii) Empathetic Engagement, and propose GuideLLM as an installation. We then implement an interviewing environment for the evaluation of LLM-guided conversation. Specifically, various topics are involved in this environment for comprehensive interviewing evaluation, resulting in around... | Alexander Rasgon, Chenan Wang, Chenxi Yuan, Eunhye Grace Ko, Jinhao Duan, Junyuan Hong, Kaidi Xu, Lily Boddy, Min Kyung Lee, Qi Long, Tianhao Li, Tianlong Chen, Xinyu Zhao, Ying Ding, Zhuoxuan Zhang |  |
| 369 |  |  [Fine-Tuning Large Language Models with Sequential Instructions](https://doi.org/10.18653/v1/2025.naacl-long.288) |  | 0 | We find that existing instruction-tuned models usually struggle to adhere to a query with multiple intentions, which impairs their performance when the completion of several tasks is demanded by a single command. Hence, this paper teaches models to respond to sequential instructions. Our first attempt stems from a task-driven perspective, manually creating additional intermediate tasks to train multilingual and visual question answering. Next, we develop an automatic and generic process that turns instructions in existing data into diverse and complex task chains. Models that underwent sequential instruction tuning follow a list of instructions better and deliver higher results in coding,... | Edoardo M. Ponti, Hanxu Hu, Pinzhen Chen, Simon Yu |  |
| 370 |  |  [Diverse In-Context Example Selection After Decomposing Programs and Aligned Utterances Improves Semantic Parsing](https://doi.org/10.18653/v1/2025.naacl-long.289) |  | 0 | LLMs are increasingly used as seq2seq translators from natural language utterances to structured programs, a process called semantic interpretation. Unlike atomic labels or token sequences, programs are naturally represented as abstract syntax trees (ASTs). Such structured representation raises novel issues related to the design and selection of in-context examples (ICEs) presented to the LLM. We focus on decomposing the pool of available ICE trees into fragments, some of which may be better suited to solving the test instance. Next, we propose how to use (additional invocations of) an LLM with prompted syntax constraints to automatically map the fragments to corresponding utterances.... | Gaurav Arora, Mayank Kothyari, Soumen Chakrabarti, Srujana Merugu, Sunita Sarawagi |  |
| 371 |  |  [Elevating Legal LLM Responses: Harnessing Trainable Logical Structures and Semantic Knowledge with Legal Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.290) |  | 0 | Large Language Models (LLMs) have achieved impressive results across numerous domains, yet they experience notable deficiencies in legal question-answering tasks. LLMs often generate generalized responses that lack the logical specificity required for expert legal advice and are prone to hallucination, providing answers that appear correct but are unreliable. Retrieval-Augmented Generation (RAG) techniques offer partial solutions to address this challenge, but existing approaches typically focus only on semantic similarity, neglecting the logical structure essential to legal reasoning. In this paper, we propose the Logical-Semantic Integration Model (LSIM), a novel supervised framework... | Chenghao Wang, Fang Wang, Jingwei Xiong, Rujing Yao, Xiaozhong Liu, Yang Wu |  |
| 372 |  |  [Efficient One-shot Compression via Low-Rank Local Feature Distillation](https://doi.org/10.18653/v1/2025.naacl-long.291) |  | 0 | Current structured pruning approaches for large language models typically involve two steps: (1) compression using calibration data and (2) costly continued pretraining on billions of tokens to recover lost performance. This second step is necessary as the first significantly impacts model accuracy. Moreover, prior research suggests that pretrained Transformer weights are not necessarily low-rank, unlike their activations, making one-shot structured pruning challenging. Based on this observation, we propose Lillama, a compression method that locally distills activations with low-rank weights. Using SVD for initialization and a joint loss combining teacher and student activations, we... | Christophe Cerisara, Irina Illina, Yaya Sy |  |
| 373 |  |  [Waste Not, Want Not; Recycled Gumbel Noise Improves Consistency in Natural Language Generation](https://doi.org/10.18653/v1/2025.naacl-long.292) |  | 0 | Consistency in the output of language models is critical for their reliability and practical utility. Due to their training objective, language models learn to model the full space of possible continuations, leading to outputs that can vary significantly in style, content, and tone, even for similar inputs. To address this, we propose a novel decoding algorithm that enhances response consistency across different prompts with no degradation in response quality. By incorporating a latent variable into the next-token sampling process based on the Gumbel reparametrisation trick, our method outperforms standard sampling by up to 10% across semantic and stylistic consistency benchmarks.... | Damien de Mijolla, Hannan Saddiq, Kim Moore |  |
| 374 |  |  [ConQRet: A New Benchmark for Fine-Grained Automatic Evaluation of Retrieval Augmented Computational Argumentation](https://doi.org/10.18653/v1/2025.naacl-long.293) |  | 0 | Computational argumentation, which involves generating answers or summaries for controversial topics like abortion bans and vaccination, has become increasingly important in today’s polarized environment. Sophisticated LLM capabilities offer the potential to provide nuanced, evidence-based answers to such questions through Retrieval-Augmented Argumentation (RAArg), leveraging real-world evidence for high-quality, grounded arguments. However, evaluating RAArg remains challenging, as human evaluation is costly and difficult for complex, lengthy answers on complicated topics. At the same time, re-using existing argumentation datasets is no longer sufficient, as they lack long, complex... | Eugene Agichtein, Kai Shu, Kaustubh D. Dhole |  |
| 375 |  |  [SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators](https://doi.org/10.18653/v1/2025.naacl-long.294) |  | 0 | Existing approaches to multilingual text detoxification are hampered by the scarcity of parallel multilingual datasets. In this work, we introduce a pipeline for the generation of multilingual parallel detoxification data. We also introduce SynthDetoxM, a manually collected and synthetically generated multilingual parallel text detoxification dataset comprising 16,000 high-quality detoxification sentence pairs across German, French, Spanish and Russian. The data was sourced from different toxicity evaluation datasets and then rewritten with nine modern open-source LLMs in few-shot setting. Our experiments demonstrate that models trained on the produced synthetic datasets have superior... | Alexander Panchenko, Daniil Moskovskiy, Elena Tutubalina, Nikita Sushko, Sergey Pletenev |  |
| 376 |  |  [BEMEAE: Moving Beyond Exact Span Match for Event Argument Extraction](https://doi.org/10.18653/v1/2025.naacl-long.295) |  | 0 | Event Argument Extraction (EAE) is a key task in natural language processing, focusing on identifying and classifying event arguments in text. However, the widely adopted exact span match (ESM) evaluation metric has notable limitations due to its rigid span constraints, often misidentifying valid predictions as errors and underestimating system performance. In this paper, we evaluate nine state-of-the-art EAE models on the RAMS and GENEVA datasets, highlighting ESM’s limitations. To address these issues, we introduce BEMEAE (Beyond Exact Span Match for Event Argument Extraction), a novel evaluation metric that recognizes predictions that are semantically equivalent to or improve upon the... | Daniyal Kashif, Eduardo Blanco, Enfa Fane, Md Nayem Uddin, Oghenevovwe Ikumariegbe, Steven R. Corman |  |
| 377 |  |  [uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in Low-Data Regimes](https://doi.org/10.18653/v1/2025.naacl-long.296) |  | 0 | Recent work on distilling Whisper’s knowledge into small models using pseudo-labels shows promising performance while reducing the size by up to 50%. This results in small, efficient, and dedicated models. However, a critical step of distillation using pseudo-labels involves filtering high-quality predictions and using only those during training. This step requires ground truth labels to compare with and filter low-quality examples, making the process dependent on human labels. Additionally, the distillation process requires a large amount of data thereby limiting its applicability in low-resource settings. To address this, we propose a distillation framework that does not require any... | Abdul Waheed, Bhiksha Raj, Karima Kadaoui, Muhammad AbdulMageed |  |
| 378 |  |  [Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities](https://doi.org/10.18653/v1/2025.naacl-long.297) |  | 0 | Recent research has shown that Large Language Models (LLMs) are vulnerable to automated jailbreak attacks, where adversarial suffixes crafted by algorithms appended to harmful queries bypass safety alignment and trigger unintended responses. Current methods for generating these suffixes are computationally expensive and have low Attack Success Rates (ASR), especially against well-aligned models like Llama2 and Llama3. To overcome these limitations, we introduce \*\*ADV-LLM\*\*, an iterative self-tuning process that crafts adversarial LLMs with enhanced jailbreak ability. Our framework significantly reduces the computational cost of generating adversarial suffixes while achieving nearly... | Aidan San, ChungEn Sun, Hao Cheng, Jianfeng Gao, Michel Galley, TsuiWei Weng, Weiwei Yang, Xiaodong Liu |  |
| 379 |  |  [VoiceTextBlender: Augmenting Large Language Models with Speech Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning](https://doi.org/10.18653/v1/2025.naacl-long.298) |  | 0 | Recent studies have augmented large language models (LLMs) with speech capabilities, leading to the development of speech language models (SpeechLMs). Earlier SpeechLMs focused on single-turn speech-based question answering (QA), where user input comprised a speech context and a text question. More recent studies have extended this to multi-turn conversations, though they often require complex, multi-stage supervised fine-tuning (SFT) with diverse data. Another critical challenge with SpeechLMs is catastrophic forgetting, where models optimized for speech tasks suffer significant degradation in text-only performance. To mitigate these issues, we propose a novel single-stage joint... | Boris Ginsburg, He Huang, Jagadeesh Balam, Ke Hu, Krishna C. Puvvada, Kunal Dhawan, Piotr Zelasko, Shinji Watanabe, Yifan Peng, Zhehuai Chen |  |
| 380 |  |  [Rethinking Word Similarity: Semantic Similarity through Classification Confusion](https://doi.org/10.18653/v1/2025.naacl-long.299) |  | 0 | Word similarity has many applications to social science and cultural analytics tasks like measuring meaning change over time and making sense of contested terms. Yet traditional similarity methods based on cosine similarity between word embeddings cannot capture the context-dependent, asymmetrical, polysemous nature of semantic similarity. We propose a new measure of similarity, Word Confusion, that reframes semantic similarity in terms of feature-based classification confusion. Word Confusion is inspired by Tversky (1977)’s suggestion that similarity features be chosen dynamically. Here we train a classifier to map contextual embeddings to word identities and use the classifier confusion... | Chen Shani, Dan Edelstein, Dan Jurafsky, Haishan Gao, Kaitlyn Zhou, Sarah Li Chen |  |
| 381 |  |  [SUNAR: Semantic Uncertainty based Neighborhood Aware Retrieval for Complex QA](https://doi.org/10.18653/v1/2025.naacl-long.300) |  | 0 | Complex question-answering (QA) systems face significant challenges in retrieving and reasoning over information that addresses multifaceted queries. While large language models (LLMs) have advanced the reasoning capabilities of these systems, the bounded-recall problem persists, where procuring all relevant documents in first-stage retrieval remains a challenge. Missing pertinent documents at this stage leads to performance degradation that cannot be remedied in later stages, especially given the limited context windows of LLMs which necessitate high recall at smaller retrieval depths. In this paper, we introduce SUNAR, a novel approach that leverages LLMs to guide a Neighborhood Aware... | Avishek Anand, Mandeep Rathee, Venktesh V |  |
| 382 |  |  [Do RAG Systems Cover What Matters? Evaluating and Optimizing Responses with Sub-Question Coverage](https://doi.org/10.18653/v1/2025.naacl-long.301) |  | 0 | Evaluating retrieval-augmented generation (RAG) systems remains challenging, particularly for open-ended questions that lack definitive answers and require coverage of multiple sub-topics. In this paper, we introduce a novel evaluation framework based on sub-question coverage, which measures how well a RAG system addresses different facets of a question. We propose decomposing questions into sub-questions and classifying them into three types—core, background, and follow-up—to reflect their roles and importance. Using this categorization, we introduce a fine-grained evaluation protocol that provides insights into the retrieval and generation characteristics of RAG systems, including three... | Caiming Xiong, ChienSheng Wu, Kaige Xie, Philippe Laban, Prafulla Kumar Choubey |  |
| 383 |  |  [Stronger Universal and Transferable Attacks by Suppressing Refusals](https://doi.org/10.18653/v1/2025.naacl-long.302) |  | 0 | Making large language models (LLMs) safe for mass deployment is a complex and ongoing challenge. Efforts have focused on aligning models to human preferences (RLHF), essentially embedding a “safety feature” into the model’s parameters. The Greedy Coordinate Gradient (GCG) algorithm (Zou et al., 2023b) emerges as one of the most popular automated jailbreaks, an attack that circumvents this safety training. So far, it is believed that such optimization-based attacks (unlike hand-crafted ones) are sample-specific. To make them universal and transferable, one has to incorporate multiple samples and models into the objective function. Contrary to this belief, we find that the adversarial... | Alexandre Araujo, Avidan Shah, Chawin Sitawarin, David A. Wagner, David Huang |  |
| 384 |  |  [The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models](https://doi.org/10.18653/v1/2025.naacl-long.303) |  | 0 | As language models (LMs) become capable of handling a wide range of tasks, their evaluation is becoming as challenging as their development. Most generation benchmarks currently assess LMs using abstract evaluation criteria-like helpfulness and harmlessness-which often lack the flexibility and granularity of human assessment. Additionally, these benchmarks tend to focus disproportionately on specific capabilities such as instruction following, leading to coverage bias. To overcome these limitations, we introduce the BiGGen Bench, a principled generation benchmark designed to thoroughly evaluate nine distinct capabilities of LMs across 77 diverse tasks. A key feature of the BiGGen Bench is... | Bill Yuchen Lin, Chaeeun Kim, Dongkeun Yoon, Graham Neubig, Guijin Son, Haebin Shin, Hanseok Oh, Hyeonbin Hwang, Hyowon Cho, Hyungjoo Chae, Jamin Shin, Ji Yong Cho, Jinheon Baek, Jinkyung Jo, Joel Jang, Juyoung Suk, Kyungjae Lee, Minjoon Seo, Miyoung Ko, Moontae Lee, Namgyu Ho, Noah Lee, Se June Joo, Sean Welleck, Seonghyeon Ye, Seongyun Lee, Seungone Kim, Shayne Longpre, Sheikh Shafayat, Sue Hyun Park, Yejin Choi, Yoonjoo Lee |  |
| 385 |  |  [DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback](https://doi.org/10.18653/v1/2025.naacl-long.304) |  | 0 | Despite their widespread success, Text-to-Image models (T2I) still struggle to produce images that are both aesthetically pleasing and faithful to the user’s input text. We introduce DreamSync, a simple yet effective training algorithm that improves T2I models to be faithful to the text input. DreamSync utilizes large vision-language models (VLMs) to effectively identify the fine-grained discrepancies between generated images and the text inputs and enable T2I models to self-improve without labeled data. First, it prompts the model to generate several candidate images for a given input text. Then, it uses two VLMs to select the best generation: a Visual Question Answering model that... | Charles Herrmann, Cyrus Rashtchian, DaCheng Juan, Dana Alon, Deqing Fu, Jiao Sun, Ranjay Krishna, Royi Rassin, Sjoerd van Steenkiste, Su Wang, Yushi Hu |  |
| 386 |  |  [Uncovering Bias in Large Vision-Language Models at Scale with Counterfactuals](https://doi.org/10.18653/v1/2025.naacl-long.305) |  | 0 | With the advent of Large Language Models (LLMs) possessing increasingly impressive capabilities, a number of Large Vision-Language Models (LVLMs) have been proposed to augment LLMs with visual inputs. Such models condition generated text on both an input image and a text prompt, enabling a variety of use cases such as visual question answering and multimodal chat. While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs. Examining social biases in LVLMs is particularly challenging due to the confounding contributions of bias induced by information contained across the text and visual modalities. To address... | Anahita Bhiwandiwalla, Kathleen C. Fraser, Phillip Howard, Svetlana Kiritchenko |  |
| 387 |  |  [AEGIS2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails](https://doi.org/10.18653/v1/2025.naacl-long.306) |  | 0 | As Large Language Models (LLMs) and generative AI become increasingly widespread, concerns about content safety have grown in parallel. Currently, there is a clear lack of high-quality, human-annotated datasets that address the full spectrum of LLM-related safety risks and are usable for commercial applications. To bridge this gap, we propose a comprehensive and adaptable taxonomy for categorizing safety risks, structured into 12 top-level hazard categories with an extension to 9 fine-grained subcategories. This taxonomy is designed to meet the diverse requirements of downstream users, offering more granular and flexible tools for managing various risk types. Using a hybrid data generation... | Aishwarya Padmakumar, Christopher Parisien, Jibin Rajan Varghese, Makesh Narsimhan Sreedhar, Prasoon Varshney, Shaona Ghosh, Traian Rebedea |  |
| 388 |  |  [UOREX: Towards Uncertainty-Aware Open Relation Extraction](https://doi.org/10.18653/v1/2025.naacl-long.307) |  | 0 | Open relation extraction (OpenRE) aims to identify relational facts within open-domain corpora without relying on predefined relation types. A significant limitation of current state-of-the-art OpenRE approaches is their inability to accurately self-assess their performance. Which is caused by the reliance on pseudo-labels, that treats all points within a cluster equally, regardless of their actual relative position according to the cluster center. This leads to models that are often overconfident in their incorrect predictions , significantly undermining their reliability. In this paper, we introduce an approach that addresses this challenge by effectively modeling a part of the epistemic... | Mohammed Erradi, Mounir Ourekouch, Rebii Jamal |  |
| 389 |  |  [Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training](https://doi.org/10.18653/v1/2025.naacl-long.308) |  | 0 | Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To... | Bing Yin, Binxuan Huang, Chao Zhang, Haoming Jiang, Jingfeng Yang, Kewei Cheng, Nasser Zalmout, Pei Chen, Priyanka Nigam, Qing Ping, Rongzhi Zhang, Ruijie Wang, Sanket Lokegaonkar, Tianyi Liu, Xin Liu, Yifan Gao, Yuchen Zhuang, Zheng Li, Zhengyang Wang |  |
| 390 |  |  [TinyThinker: Distilling Reasoning through Coarse-to-Fine Knowledge Internalization with Self-Reflection](https://doi.org/10.18653/v1/2025.naacl-long.309) |  | 0 | Large Language Models exhibit impressive reasoning capabilities across diverse tasks, motivating efforts to distill these capabilities into smaller models through generated reasoning data. However, direct training on such synthesized reasoning data may lead to superficial imitation of reasoning process, rather than fostering a genuine integration of reasoning capabilities with underlying knowledge. To address this, we propose TinyThinker, a framework introducing two novel approaches. First, we introduce a three-stage process that incrementally guides the student model through the reasoning process, progressively refining knowledge from coarse to fine granularity. Second, we develop a... | Sanghyun Park, Shengmin Piao |  |
| 391 |  |  [VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-long.310) |  | 0 | Understanding information from a collection of multiple documents, particularly those with visually rich elements, is important for document-grounded question answering. This paper introduces VisDoMBench, the first comprehensive benchmark designed to evaluate QA systems in multi-document settings with rich multimodal content, including tables, charts, and presentation slides. We propose VisDoMRAG, a novel multimodal Retrieval Augmented Generation (RAG) approach that simultaneously utilizes visual and textual RAG, combining robust visual retrieval capabilities with sophisticated linguistic reasoning. VisDoMRAG employs a multi-step reasoning process encompassing evidence curation and... | Dinesh Manocha, Franck Dernoncourt, Kanika Goswami, Manan Suri, Puneet Mathur, Ryan A. Rossi |  |
| 392 |  |  [VTechAGP: An Academic-to-General-Audience Text Paraphrase Dataset and Benchmark Models](https://doi.org/10.18653/v1/2025.naacl-long.311) |  | 0 | Existing text simplification or paraphrase datasets mainly focus on sentence-level text generation in a general domain. These datasets are typically developed without using domain knowledge. In this paper, we release a novel dataset, VTechAGP, which is the first academic-to-general-audience text paraphrase dataset consisting of document-level these and dissertation academic and general-audience abstract pairs from 8 colleges authored over 25 years. We also propose a novel dynamic soft prompt generative language model, DSPT5. For training, we leverage a contrastive-generative loss function to learn the keyword vectors in the dynamic prompt. For inference, we adopt a crowd-sampling decoding... | Chenhan Yuan, Edward A. Fox, Hoda Eldardiry, Jiaying Gong, Ming Cheng, William A. Ingram |  |
| 393 |  |  [Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages](https://doi.org/10.18653/v1/2025.naacl-long.312) |  | 0 | Human bilinguals often use similar brain regions to process multiple languages, depending on when they learned their second language and their proficiency. In large language models (LLMs), how are multiple languages learned and encoded? In this work, we explore the extent to which LLMs share representations of morphsyntactic concepts such as grammatical number, gender, and tense across languages. We train sparse autoencoders on Llama-3-8B and Aya-23-8B, and demonstrate that abstract grammatical concepts are often encoded in feature directions shared across many languages. We use causal interventions to verify the multilingual nature of these representations; specifically, we show that... | Aaron Mueller, Chris Wendler, Christian Bartelt, Jannik Brinkmann |  |
| 394 |  |  [Examining and Adapting Time for Multilingual Classification via Mixture of Temporal Experts](https://doi.org/10.18653/v1/2025.naacl-long.313) |  | 0 | Time is implicitly embedded in classification process: classifiers are usually built on existing data while to be applied on future data whose distributions (e.g., label and token) may change. However, existing state-of-the-art classification models merely consider the temporal variations and primarily focus on English corpora, which leaves temporal studies less explored, let alone under multilingual settings. In this study, we fill the gap by treating time as domains (e.g., 2024 vs. 2025), examining temporal effects, and developing a domain adaptation framework to generalize classifiers over time on four languages, English, Danish, French, and German. Our framework proposes Mixture of... | Guangzeng Han, Weisi Liu, Xiaolei Huang |  |
| 395 |  |  [FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.314) |  | 0 | Sign language translation has historically been peripheral to mainstream machine translation research. In order to help converge the fields, we introduce FLEURS-ASL, an extension of the multiway parallel benchmarks FLORES (for text) and FLEURS (for speech) to support their first sign language (as video), American Sign Language, translated by 5 Certified Deaf Interpreters. FLEURS-ASL can be used to evaluate a variety of tasks—primarily sentence- and discourse-level translation—between ASL and 200 other languages as text, or 102 languages as speech. We provide baselines for tasks from ASL to English text using a unified modeling approach that incorporates timestamp tokens and previous text... | Garrett Tanzer |  |
| 396 |  |  [EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms](https://doi.org/10.18653/v1/2025.naacl-long.315) |  | 0 | The rise of powerful large language models (LLMs) has spurred a new trend in building LLM-based autonomous agents for solving complex tasks, especially multi-agent systems. Despite the remarkable progress, we notice that existing works are heavily dependent on human-designed frameworks, which greatly limits the functional scope and scalability of agent systems. How to automatically extend the specialized agent to multi-agent systems to improve task-solving capability still remains a significant challenge. In this paper, we introduce EVOAGENT, a generic method to automatically extend specialized agents to multi-agent systems via the evolutionary algorithm, thereby improving the... | Deqing Yang, Dongsheng Li, Jiangjie Chen, Kaitao Song, Siyu Yuan, Xu Tan |  |
| 397 |  |  [EmoCharacter: Evaluating the Emotional Fidelity of Role-Playing Agents in Dialogues](https://doi.org/10.18653/v1/2025.naacl-long.316) |  | 0 | Role-playing agents (RPAs) powered by large language models (LLMs) have been widely utilized in dialogue systems for their capability to deliver personalized interactions. Current evaluations of RPAs mainly focus on personality fidelity, tone imitation, and knowledge consistency, while overlooking emotional fidelity, a key factor that affects user experience. To this end, we propose a benchmark called EmoCharacter to assess emotional fidelity of RPAs in dialogues. EmoCharacter includes two benchmark datasets (single-turn and multi-turn dialogues), three evaluation settings, and six metrics to measure the emotional fidelity between RPAs and the characters they portray. Based on... | Qiming Feng, Qingqiu Li, Qiujie Xie, Rui Feng, Shang Gao, Tao Zhang, Xiaolong Wang, Yuejie Zhang |  |
| 398 |  |  [Language Models can Categorize System Inputs for Performance Analysis](https://doi.org/10.18653/v1/2025.naacl-long.317) |  | 0 | Language model systems are used to process diverse categories of input requests, ranging from improving creative writing to solving programming challenges. It would be useful to know which categories they are good at. However, existing evaluations compare model performance on pre-defined categories, failing to reflect a system’s performance on finer-grained or novel ones. We propose to automatically search for finer-grained categories based on inputs where a system performs well or poorly, and describe them in natural language. To search for these categories, we propose a large number of candidate category descriptions, e.g. “Communication Improvement”, find the subset of inputs that match... | Dominic Sobhani, Edison MarreseTaylor, Keisuke Sakaguchi, Ruiqi Zhong, Yutaka Matsuo |  |
| 399 |  |  [FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.318) |  | 0 | Large language models have demonstrated outstanding performance in various natural language processing tasks, but their security capabilities in the financial domain have not been explored, and their performance on complex tasks like financial agent remains unknown. This paper presents FinEval, a benchmark designed to evaluate LLMs’ financial domain knowledge and practical abilities. The dataset contains 8,351 questions categorized into four different key areas: Financial Academic Knowledge, Financial Industry Knowledge, Financial Security Knowledge, and Financial Agent. Financial Academic Knowledge comprises 4,661 multiple-choice questions spanning 34 subjects such as finance and... | Bing Zhu, Chuqi Wang, Hanyang Cao, Haotian Xia, Jinyi Niu, Liwen Zhang, Sizhe Wang, Weining Shen, Xiaolong Liang, Xiaoming Huang, Xin Guo, Yanhui Wang, Yun Chen, Zhaowei Liu, Zhi Yang, Zhiqiang Liu, Zhongyu Wei |  |
| 400 |  |  [Rethinking the Role of LLMs for Document-level Relation Extraction: a Refiner with Task Distribution and Probability Fusion](https://doi.org/10.18653/v1/2025.naacl-long.319) |  | 0 | Document-level relation extraction (DocRE) provides a broad context for extracting one or more relations for each entity pair. Large language models (LLMs) have made great progress in relation extraction tasks. However, one of the main challenges we face is that LLMs have difficulty in multi-label relation prediction tasks. Additionally, another noteworthy challenge and discovery we reveal: the small language models (SLMs) for DocRE tend to classify existing relations as ”no relation” (NA), while LLMs tend to predict existing relations for all entity pairs. To address these challenges, we propose a novel method that utilizes LLMs as a refiner, employing task distribution and probability... | Fu Zhang, Hongsen Yu, Huangming Xu, Jingwei Cheng, Xinlong Jin |  |
| 401 |  |  [Decomposition Dilemmas: Does Claim Decomposition Boost or Burden Fact-Checking Performance?](https://doi.org/10.18653/v1/2025.naacl-long.320) |  | 0 | Fact-checking pipelines increasingly adopt the Decompose-Then-Verify paradigm, where texts are broken down into smaller claims for individual verification and subsequently combined for a veracity decision. While decomposition is widely-adopted in such pipelines, its effects on final fact-checking performance remain underexplored. Some studies have reported improvements from decompostition, while others have observed performance declines, indicating its inconsistent impact. To date, no comprehensive analysis has been conducted to understand this variability. To address this gap, we present an in-depth analysis that explicitly examines the impact of decomposition on downstream verification... | Qisheng Hu, Quanyu Long, Wenya Wang |  |
| 402 |  |  [Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing](https://doi.org/10.18653/v1/2025.naacl-long.321) |  | 0 | Large Language Models (LLMs) have demonstrated great potential as generalist assistants, showcasing powerful task understanding and problem-solving capabilities. To deploy LLMs as AI assistants, it is crucial that these models exhibit desirable behavioral traits, such as non-toxicity and resilience against jailbreak attempts. Current approaches for detoxification or preventing jailbreaking usually involve Supervised Fine-Tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), which requires finetuning billions of parameters through gradient descent with substantial computational cost. Furthermore, models modified through SFT and RLHF may deviate from the pretrained models,... | Andrew Zhao, Gao Huang, Huanqian Wang, Jingxin Shi, Rui Lu, Shenzhi Wang, Shiji Song, Yang Yue |  |
| 403 |  |  [Effective Skill Unlearning through Intervention and Abstention](https://doi.org/10.18653/v1/2025.naacl-long.322) |  | 0 |  | ChungEn Sun, TsuiWei Weng, Yongce Li |  |
| 404 |  |  [CharacterBox: Evaluating the Role-Playing Capabilities of LLMs in Text-Based Virtual Worlds](https://doi.org/10.18653/v1/2025.naacl-long.323) |  | 0 | Role-playing is a crucial capability of Large Language Models (LLMs), enabling a wide range of practical applications, including intelligent non-player characters, digital twins, and emotional companions. Evaluating this capability in LLMs is challenging due to the complex dynamics involved in role-playing, such as maintaining character fidelity throughout a storyline and navigating open-ended narratives without a definitive ground truth. Current evaluation methods, which primarily focus on question-answering or conversational snapshots, fall short of adequately capturing the nuanced character traits and behaviors essential for authentic role-playing. In this paper, we propose... | Haoxuan Li, JiRong Wen, Jianxun Lian, Lei Wang, Xing Xie, Xu Chen, Yanqi Dai, Yi Huang |  |
| 405 |  |  [A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision-Language Models](https://doi.org/10.18653/v1/2025.naacl-long.324) |  | 0 | Large Vision-Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the Cookie Theft task in human cognitive tests, we propose a novel evaluation benchmark to evaluate high-level cognitive abilities of LVLMs using images with rich semantics. The benchmark consists of 251 images along with comprehensive annotations. It defines eight reasoning capabilities and comprises an image description task and a visual question answering task. Our evaluation of well-known LVLMs shows that there is still a significant gap in cognitive abilities between LVLMs and humans. | Chunhao Zhang, Kenny Q. Zhu, Mengyue Wu, Xiujie Song, Yanyi Chen |  |
| 406 |  |  [CoME: An Unlearning-based Approach to Conflict-free Model Editing](https://doi.org/10.18653/v1/2025.naacl-long.325) |  | 0 | Large language models (LLMs) often retain outdated or incorrect information from pre-training, which undermines their reliability. While model editing methods have been developed to address such errors without full re-training, they frequently suffer from knowledge conflicts, where outdated information interferes with new knowledge. In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing outdated knowledge. CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features. Through experiments on GPT-J... | Chanjun Park, Dahyun Jung, Heuiseok Lim, Jaehyung Seo, Jaewook Lee |  |
| 407 |  |  [On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena](https://doi.org/10.18653/v1/2025.naacl-long.326) |  | 0 | Language Models (LMs) have been shown to exhibit a strong preference towards entities associated with Western culture when operating in non-Western languages. In this paper, we aim to uncover the origins of entity-related cultural biases in LMs by analyzing several contributing factors, including the representation of entities in pre-training data and the impact of variations in linguistic phenomena across languages. We introduce CAMeL-2, a parallel Arabic-English benchmark of 58,086 entities associated with Arab and Western cultures and 367 masked natural contexts for entities. Our evaluations using CAMeL-2 reveal reduced performance gaps between cultures by LMs when tested in English... | Tarek Naous, Wei Xu |  |
| 408 |  |  [Adapting Sentence-level Automatic Metrics for Document-level Simplification Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.327) |  | 0 | Text simplification aims to enhance the clarity and comprehensibility of a complex text while preserving its original meaning. Previous research on the automatic evaluation of text simplification has primarily focused on sentence simplification, with commonly used metrics such as SARI and advanced metrics such as LENS being trained and evaluated at the sentence level. However, these metrics often underperform on longer texts. In our study, we propose a novel approach to adapt existing sentence-level metrics for paragraph- or document-level simplification. We benchmark our approach against a wide variety of existing reference-based and reference-less metrics across multiple domains.... | Fernando AlvaManchego, Mounica Maddela |  |
| 409 |  |  [Decoding Speculative Decoding](https://doi.org/10.18653/v1/2025.naacl-long.328) |  | 0 | Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without sacrificing quality. When performing inference, speculative decoding uses a smaller draft model to generate speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. In this work, we perform a detailed study comprising over 350 experiments with LLaMA-65B and OPT-66B using speculative decoding and delineate the factors that affect the performance gain provided by speculative decoding. Our experiments indicate that the performance of speculative decoding depends heavily... | Minghao Yan, Saurabh Agarwal, Shivaram Venkataraman |  |
| 410 |  |  [Leveraging LLM For Synchronizing Information Across Multilingual Tables](https://doi.org/10.18653/v1/2025.naacl-long.329) |  | 0 | The vast amount of online information today poses challenges for non-English speakers, as much of it is concentrated in high-resource languages such as English and French. Wikipedia reflects this imbalance, with content in low-resource languages frequently outdated or incomplete. Recent research has sought to improve cross-language synchronization of Wikipedia tables using rule-based methods. These approaches can be effective, but they struggle with complexity and generalization. This paper explores large language models (LLMs) for multilingual information synchronization, using zero-shot prompting as a scalable solution. We introduce the Information Updation dataset, simulating the... | Ankita Anand, Dan Roth, Siddharth Khincha, Tushar Kataria, Vivek Gupta |  |
| 411 |  |  [ConMeC: A Dataset for Metonymy Resolution with Common Nouns](https://doi.org/10.18653/v1/2025.naacl-long.330) |  | 0 | Metonymy plays an important role in our daily communication. People naturally think about things using their most salient properties or commonly related concepts. For example, by saying “The bus decided to skip our stop today,” we actually mean that the bus driver made the decision, not the bus. Prior work on metonymy resolution has mainly focused on named entities. However, metonymy involving common nouns (such as desk, baby, and school) is also a frequent and challenging phenomenon. We argue that NLP systems should be capable of identifying the metonymic use of common nouns in context. We create a new metonymy dataset ConMeC, which consists of 6,000 sentences, where each sentence is... | Saptarshi Ghosh, Tianyu Jiang |  |
| 412 |  |  [Self-DC: When to Reason and When to Act? Self Divide-and-Conquer for Compositional Unknown Questions](https://doi.org/10.18653/v1/2025.naacl-long.331) |  | 0 | Previous research has typically concentrated on leveraging the internal knowledge of Large Language Models (LLMs) to answer known questions (i.e., internal reasoning such as generate-then-read). In contrast, for questions that fall outside their known scope, these models rely on external knowledge retrieval to provide accurate responses (i.e., external acting such as retrieve-then-read). However, few previous works consider the compositional questions, which consist of several known and unknown sub-questions, necessitating the dynamic combination of previous two methods (i.e., internal reasoning and external acting) to achieve a better trade-off between effectiveness and efficiency. To... | Baohang Zhou, Boyang Xue, Cunxiang Wang, Guanhua Chen, Hongru Wang, Huimin Wang, KamFai Wong, Tianhua Zhang |  |
| 413 |  |  [TRANSIENTTABLES: Evaluating LLMs' Reasoning on Temporally Evolving Semi-structured Tables](https://doi.org/10.18653/v1/2025.naacl-long.332) |  | 0 | Humans continuously make new discoveries, and understanding temporal sequence of events leading to these breakthroughs is essential for advancing science and society. This ability to reason over time allows us to identify future steps and understand the effects of financial and political decisions on our lives. However, large language models (LLMs) are typically trained on static datasets, limiting their ability to perform effective temporal reasoning. To assess the temporal reasoning capabilities of LLMs, we present the TRANSIENTTABLES dataset, which comprises 3,971 questions derived from over 14,000 tables, spanning 1,238 entities across multiple time periods. We introduce a... | Abhilash Reddy Shankarampeta, Dan Roth, Harsh Mahajan, Tushar Kataria, Vivek Gupta |  |
| 414 |  |  [AdvisorQA: Towards Helpful and Harmless Advice-seeking Question Answering with Collective Intelligence](https://doi.org/10.18653/v1/2025.naacl-long.333) |  | 0 | As the integration of large language models into daily life is on the rise, there is still a lack of dataset for \*advising on subjective and personal dilemmas\*. To address this gap, we introduce AdvisorQA, which aims to improve LLMs’ capability to offer advice for deeply subjective concerns, utilizing the LifeProTips Reddit forum. This forum features a dynamic interaction where users post advice-seeking questions, receiving an average of 8.9 advice per query, with 164.2 upvotes from hundreds of users, embodying a \*collective intelligence\*. Therefore, we’ve completed a dataset encompassing daily life questions, diverse corresponding responses, and majority vote ranking, which we use to... | Hwanhee Lee, Hwaran Lee, Joonsuk Park, Kyomin Jung, Minbeom Kim |  |
| 415 |  |  [tRAG: Term-level Retrieval-Augmented Generation for Domain-Adaptive Retrieval](https://doi.org/10.18653/v1/2025.naacl-long.334) |  | 0 | Neural retrieval models have emerged as an effective tool for information retrieval, but their performance suffers when there is a domain shift between training and test data distributions. Recent work aims to construct pseudo-training data for the target domain by generating domain-adapted pseudo-queries using large language models (LLMs). However, we identifies that LLMs exhibit a “seen term bias” where the generated pseudo-queries fail to include relevant “unseen” terms as expected for domain adaptation purposes. To address this limitation, we propose to improve the term recall of unseen query terms, by using term-level Retrieval-Augmented Generation (tRAG). Specifically, unlike... | Dohyeon Lee, Jihyuk Kim, Jongyoon Kim, Joonsuk Park, Seungwon Hwang |  |
| 416 |  |  [JRE-L: Journalist, Reader, and Editor LLMs in the Loop for Science Journalism for the General Audience](https://doi.org/10.18653/v1/2025.naacl-long.335) |  | 0 | Science journalism reports current scientific discoveries to non-specialists, aiming to enable public comprehension of the state of the art. This task is challenging as the audience often lacks specific knowledge about the presented research. We propose JRE-L, a framework that integrates three LLMs mimicking the writing-reading-feedback-revision loop. In JRE-L, one LLM acts as the journalist, another LLM as the general public reader, and the third LLM as an editor. The journalist’s writing is iteratively refined by feedback from the reader and suggestions from the editor. Our experiments demonstrate that by leveraging the collaboration of two 7B and one 1.8B open-source LLMs, we can... | Gongyao Jiang, Qiong Luo, Xinran Shi |  |
| 417 |  |  [Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.336) |  | 0 | Data selection for fine-tuning large language models (LLMs) aims to choose a high-quality subset from existing datasets, allowing the trained model to outperform baselines trained on the full dataset. However, the expanding body of research lacks a clear, unified framework, and the variability in experimental settings complicates systematic comparisons.While existing surveys comprehensively overview the stages and methods of data selection, they often overlook an in-depth exploration of the fine-tuning phase. In this paper, we conduct a focused review of recent data selection techniques for fine-tuning LLMs, analyzing a dozen key studies. We introduce a novel three-stage scheme—comprising... | Feng Jiang, Haizhou Li, Rui Ke, Yajiao Liu, Ziche Liu |  |
| 418 |  |  [Graph Neural Network Enhanced Retrieval for Question Answering of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.337) |  | 0 | Retrieval augmented generation has revolutionized large language model (LLM) outputs by providing factual supports. Nevertheless, it struggles to capture all the necessary knowledge for complex reasoning questions. Existing retrieval methods typically divide reference documents into passages, treating them in isolation. These passages, however, are often interrelated, such as passages that are contiguous or share the same keywords. Therefore, it is crucial to recognize such relatedness for enhancing the retrieval process. In this paper, we propose a novel retrieval method, called GNN-Ret, which leverages graph neural networks (GNNs) to enhance retrieval by exploiting the relatedness... | Jiang Bian, Jiawei Shao, Jun Zhang, Lei Song, Qingyan Guo, Rui Wang, Zijian Li |  |
| 419 |  |  [Pula: Training Large Language Models for Setswana](https://doi.org/10.18653/v1/2025.naacl-long.338) |  | 0 | In this work we present Pula, a suite of bilingual language models proficient in both Setswana and English. Leveraging recent advancements in data availability and efficient fine-tuning, Pula 8B and Pula 14B outperform GPT-4o and Gemini 1.5 Pro on English-Setswana translation tasks and achieve state-of-the-art performance on Setswana reasoning tasks for their size. We release the weights for Pula 1B, 3B, 8B, and 14B as well as training logs and training and evaluation code. Alongside Pula, we release the largest-ever Setswana text corpus, Marothodi, and the first comprehensive Setswana instruction-tuning dataset, Medupi, consisting of reformatted datasets, translated corpora, and synthetic... | Nathan Brown, Vukosi Marivate |  |
| 420 |  |  [LegalViz: Legal Text Visualization by Text To Diagram Generation](https://doi.org/10.18653/v1/2025.naacl-long.339) |  | 0 | Legal documents including judgments and court orders require highly sophisticated legal knowledge for understanding. To disclose expert knowledge for non-experts, we explore the problem of visualizing legal texts with easy-to-understand diagrams and propose a novel dataset of LegalViz with 23 languages and 7,010 cases of legal document and visualization pairs, using the DOT graph description language of Graphviz. LegalViz provides a simple diagram from a complicated legal corpus identifying legal entities, transactions, legal sources, and statements at a glance, that are essential in each judgment. In addition, we provide new evaluation metrics for the legal diagram visualization by... | Eri Onami, Koki Maeda, Shuhei Kurita, Taiki Miyanishi |  |
| 421 |  |  [Active Few-Shot Learning for Text Classification](https://doi.org/10.18653/v1/2025.naacl-long.340) |  | 0 | The rise of Large Language Models (LLMs) has boosted the use of Few-Shot Learning (FSL) methods in natural language processing, achieving acceptable performance even when working with limited training data. The goal of FSL is to effectively utilize a small number of annotated samples in the learning process. However, the performance of FSL suffers when unsuitable support samples are chosen. This problem arises due to the heavy reliance on a limited number of support samples, which hampers consistent performance improvement even when more support samples are added. To address this challenge, we propose an active learning-based instance selection mechanism that identifies effective support... | Arash Yousefi Jordehi, Cornelia Caragea, Mahsa Hosseini Khasheh Heyran, Owen Rambow, Saeed Ahmadnia, Seyed Abolghasem Mirroshandel |  |
| 422 |  |  [Enhancing Multimodal Entity Linking with Jaccard Distance-based Conditional Contrastive Learning and Contextual Visual Augmentation](https://doi.org/10.18653/v1/2025.naacl-long.341) |  | 0 | Previous research on multimodal entity linking (MEL) has primarily employed contrastive learning as the primary objective. However, using the rest of the batch as negative samples without careful consideration, these studies risk leveraging easy features and potentially overlook essential details that make entities unique. In this work, we propose JD-CCL (Jaccard Distance-based Conditional Contrastive Learning), a novel approach designed to enhance the ability to match multimodal entity linking models. JD-CCL leverages meta-information to select negative samples with similar attributes, making the linking task more challenging and robust. Additionally, to address the limitations caused by... | Anh Tuan Luu, CongDuy T. Nguyen, Khoi M. Le, Shuai Zhao, Thong Thanh Nguyen, VietAnh Nguyen, Xiaobao Wu, Yichao Feng |  |
| 423 |  |  [ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.342) |  | 0 | The pace of scientific research, vital for improving human life, is complex, slow, and needs specialized expertise. Meanwhile, novel, impactful research often stems from both a deep understanding of prior work, and a cross-pollination of ideas across domains and fields. To enhance the productivity of researchers, we propose ResearchAgent, which leverages the encyclopedic knowledge and linguistic reasoning capabilities of Large Language Models (LLMs) to assist them in their work. This system automatically defines novel problems, proposes methods and designs experiments, while iteratively refining them based on the feedback from collaborative LLM-powered reviewing agents. Specifically,... | Jinheon Baek, Silviu Cucerzan, Sujay Kumar Jauhar, Sung Ju Hwang |  |
| 424 |  |  [Logit Separability-Driven Samples and Multiple Class-Related Words Selection for Advancing In-Context Learning](https://doi.org/10.18653/v1/2025.naacl-long.343) |  | 0 | Effective organization of in-context learning (ICL) demonstrations is key to improving the quality of large language model (LLM) responses. To create better sample-label pairs that instruct LLM understanding, we introduce logit separability, a criterion to assess the clarity of both samples and class-related words at the logit level. This facilitates the optimization of sample and label selection, enhancing the precision of information provided in ICL demonstrations. Additionally, we find that incorporating multiple class-related words for each sample, rather than relying on a single class name, improves performance by offering a broader range of label information. Building on these... | Hanzhang Zhou, Junlang Qian, Kezhi Mao, Zijian Feng, Zixiao Zhu |  |
| 425 |  |  [Identifying Emerging Concepts in Large Corpora](https://doi.org/10.18653/v1/2025.naacl-long.344) |  | 0 | We introduce a new method to identify emerging concepts in large text corpora. By analyzing changes in the heatmaps of the underlying embedding space, we are able to detect these concepts with high accuracy shortly after they originate, in turn outperforming common alternatives. We further demonstrate the utility of our approach by analyzing speeches in the U.S. Senate from 1941 to 2015. Our results suggest that the minority party is more active in introducing new concepts into the Senate discourse. We also identify specific concepts that closely correlate with the Senators’ racial, ethnic, and gender identities. An implementation of our method is publicly available. | Julian Nyarko, Sibo Ma |  |
| 426 |  |  [CodeSCM: Causal Analysis for Multi-Modal Code Generation](https://doi.org/10.18653/v1/2025.naacl-long.345) |  | 0 | In this paper, we propose CodeSCM, a Structural Causal Model (SCM) for analyzing multi-modal code generation using large language models (LLMs). By applying interventions to CodeSCM, we measure the causal effects of different prompt modalities, such as natural language, code, and input-output examples, on the model. CodeSCM introduces latent mediator variables to separate the code and natural language semantics of a multi-modal code generation prompt. Using the principles of Causal Mediation Analysis on these mediators we quantify direct effects representing the model’s spurious leanings. We find that, in addition to natural language instructions, input-output examples significantly... | Mukur Gupta, Noopur Bhatt, Suman Jana |  |
| 427 |  |  [From Distributional to Overton Pluralism: Investigating Large Language Model Alignment](https://doi.org/10.18653/v1/2025.naacl-long.346) |  | 0 | The alignment process changes several properties of a large language model’s (LLM’s) output distribution. We analyze two aspects of post-alignment distributional shift of LLM responses. First, we re-examine previously reported reductions in response diversity post-alignment. Our analysis suggests that an apparent drop in the diversity of responses is largely explained by quality control and information aggregation. Alignment suppresses irrelevant and unhelpful content while shifting the output distribution toward longer responses that cover information spanning several responses from the base LLM, essentially presenting diverse information in a single response. Finding little evidence that... | Eunsol Choi, Greg Durrett, Thom Lake |  |
| 428 |  |  [Advancing MoE Efficiency: A Collaboration-Constrained Routing (C2R) Strategy for Better Expert Parallelism Design](https://doi.org/10.18653/v1/2025.naacl-long.347) |  | 0 |  | Jie Peng, Mohan Zhang, Mufan Qiu, Pingzhi Li, Tianlong Chen |  |
| 429 |  |  [LibEvolutionEval: A Benchmark and Study for Version-Specific Code Generation](https://doi.org/10.18653/v1/2025.naacl-long.348) |  | 0 | Recent advancements in code completion models have primarily focused on local file contexts. However, these studies do not fully capture the complexity of real-world software development, which often requires the use of rapidly-evolving public libraries. To address this gap, we introduce LibEvolutionEval, a comprehensive study that emphasizes the need to understand library evolution to perform accurate in-line code completions. LibEvolutionEvaloffers a version-specific code-completion task across eight libraries as they evolve over the years, along with an in-depth analysis of the evolution of two widely used and well-maintained public libraries: PyTorch and Matplotlib. We evaluate several... | Anoop Deoras, Baishakhi Ray, Haifeng Qian, Murali Krishna Ramanathan, Nihal Jain, Sachit Kuhar, Wasi Uddin Ahmad, Xiaofei Ma, Zijian Wang |  |
| 430 |  |  [Evaluating and Mitigating Object Hallucination in Large Vision-Language Models: Can They Still See Removed Objects?](https://doi.org/10.18653/v1/2025.naacl-long.349) |  | 0 | Large Vision-Language Models (LVLMs) have a significant issue with object hallucinations, where researchers have noted that LVLMs often mistakenly determine objects as present in images where they do not actually exist. Some recent studies evaluate the occurrence of object hallucinations by asking LVLMs whether they see objects that do not exist in input images. However, we observe that these evaluation methods have some limitations, such as the objects being questioned potentially having little relevance to the image. In this paper, we introduce a more challenging benchmark for evaluating object hallucinations by removing objects from images and then asking the model whether it can still... | Haifeng Sun, Huazheng Wang, Jing Wang, Jingyu Wang, Pengfei Ren, Qi Qi, Yixiao He, Zirui Zhuang |  |
| 431 |  |  [Self-Pluralising Culture Alignment for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.350) |  | 0 | As large language models (LLMs) become increasingly accessible in many countries, it is essential to align them to serve pluralistic human values across cultures. However, pluralistic culture alignment in LLMs remain an open problem. In this paper, we propose CultureSPA, a Self-Pluralising Culture Alignment framework that allows LLMs to simultaneously align to pluralistic cultures. The framework first generates questions on various culture topics, then yields LLM outputs in response to these generated questions under both culture-aware and culture-unaware settings. By comparing culture-aware/unaware outputs, we are able to detect and collect culture-related instances. These instances are... | Deyi Xiong, Linhao Yu, Shaoyang Xu, Yongqi Leng |  |
| 432 |  |  [K-COMP: Retrieval-Augmented Medical Domain Question Answering With Knowledge-Injected Compressor](https://doi.org/10.18653/v1/2025.naacl-long.351) |  | 0 |  | Gary Lee, Jeonghun Cho |  |
| 433 |  |  [DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students' Hand-Drawn Math Images](https://doi.org/10.18653/v1/2025.naacl-long.352) |  | 0 | In real-world settings, vision language models (VLMs) should robustly handle naturalistic, noisy visual content as well as domain-specific language and concepts. For example, K-12 educators using digital learning platforms may need to examine and provide feedback across many images of students’ math work. To assess the potential of VLMs to support educators in settings like this one, we introduce DrawEduMath, an English-language dataset of 2,030 images of students’ handwritten responses to K-12 math problems. Teachers provided detailed annotations, including free-form descriptions of each image and 11,661 question-answer (QA) pairs. These annotations capture a wealth of pedagogical... | Alice Ng, Kyle Lo, Li Lucy, Luca Soldaini, Neil T. Heffernan, Ryan Knight, Sami Baral |  |
| 434 |  |  [Knowledge Graph Guided Evaluation of Abstention Techniques](https://doi.org/10.18653/v1/2025.naacl-long.353) |  | 0 | To deploy language models safely, it is crucial that they abstain from responding to inappropriate requests. Several prior studies test the safety promises of models based on their effectiveness in blocking malicious requests. In this work, we focus on evaluating the underlying techniques that cause models to abstain. We create ‘SELECT‘, a benchmark derived from a set of benign concepts (e.g., “rivers”) from a knowledge graph. Focusing on benign concepts isolates the effect of safety training, and grounding these concepts in a knowledge graph allows us to study the \*generalization\* and \*specificity\* of abstention techniques. Using ‘SELECT‘, we benchmark different abstention techniques... | Danish Pruthi, Kinshuk Vasisht, Navreet Kaur |  |
| 435 |  |  [Wav2Prompt: End-to-End Speech Prompt Learning and Task-based Fine-tuning for Text-based LLMs](https://doi.org/10.18653/v1/2025.naacl-long.354) |  | 0 | Wav2Prompt is proposed which allows integrating spoken input with a text-based large language model (LLM). Wav2Prompt uses a straightforward training process with only the same data used to train an automatic speech recognition (ASR) model. After training, Wav2Prompt learns continuous representations from speech and uses them as LLM prompts. To avoid task over-fitting issues found in prior work and preserve the emergent abilities of LLMs, Wav2Prompt takes LLM token embeddings as the training targets and utilises a continuous integrate-and-fire mechanism for explicit speech-text alignment. Therefore, a Wav2Prompt-LLM combination can be applied to zero-shot spoken language tasks such as... | Guangzhi Sun, Keqi Deng, Philip C. Woodland |  |
| 436 |  |  [Legal Judgment Prediction based on Knowledge-enhanced Multi-Task and Multi-Label Text Classification](https://doi.org/10.18653/v1/2025.naacl-long.355) |  | 0 | Legal judgment prediction (LJP) is an essential task for legal AI, aiming at predicting judgments based on the facts of a case. Legal judgments can involve multiple law articles and charges. Although recent methods in LJP have made notable progress, most are constrained to single-task settings (e.g., only predicting charges) or single-label settings (e.g., not accommodating cases with multiple charges), diverging from the complexities of real-world scenarios. In this paper, we address the challenge of predicting relevant law articles and charges within the framework of legal judgment prediction, treating it as a multi-task and multi-label text classification problem. We introduce a... | Adam Jatowt, Ang Li, Changlong Sun, Fei Wu, Kun Kuang, Ming Cai, Weiming Lu, Xiang Zhou, Yiquan Wu |  |
| 437 |  |  [SPeCtrum: A Grounded Framework for Multidimensional Identity Representation in LLM-Based Agent](https://doi.org/10.18653/v1/2025.naacl-long.356) |  | 0 | Existing methods for simulating individual identities often oversimplify human complexity, which may lead to incomplete or flattened representations. To address this, we introduce SPeCtrum, a grounded framework for constructing authentic LLM agent personas by incorporating an individual’s multidimensional self-concept. SPeCtrum integrates three core components: Social Identity (S), Personal Identity (P), and Personal Life Context (C), each contributing distinct yet interconnected aspects of identity. To evaluate SPeCtrum’s effectiveness in identity representation, we conducted automated and human evaluations. Automated evaluations using popular drama characters showed that Personal Life... | Esther Hehsun Kim, Eunmee Kim, Hajin Lim, Hayeon Jeon, Jinsu Eun, Keyeun Lee, SeoHyeong Kim, Seolhee Lee, Seonghye Cho, Soeun Yang, Yena Ko |  |
| 438 |  |  [Beemo: Benchmark of Expert-edited Machine-generated Outputs](https://doi.org/10.18653/v1/2025.naacl-long.357) |  | 0 | The rapid proliferation of large language models (LLMs) has increased the volume of machine-generated texts (MGTs) and blurred text authorship in various domains. However, most existing MGT benchmarks include single-author texts (human-written and machine-generated). This conventional design fails to capture more practical multi-author scenarios, where the user refines the LLM response for natural flow, coherence, and factual correctness. Our paper introduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo), which includes 6.5k texts written by humans, generated by ten instruction-finetuned LLMs, and edited by experts for various use cases, ranging from creative writing to... | Adaku Uchendu, Ekaterina Artemova, Jason Samuel Lucas, Jooyoung Lee, Saranya Venkatraman, Sergei Tilga, Vladislav Mikhailov |  |
| 439 |  |  [SANDWiCH: Semantical Analysis of Neighbours for Disambiguating Words in Context ad Hoc](https://doi.org/10.18653/v1/2025.naacl-long.358) |  | 0 | The rise of generative chat-based Large Language Models (LLMs) over the past two years has spurred a race to develop systems that promise near-human conversational and reasoning experiences. However, recent studies indicate that the language understanding offered by these models remains limited and far from human-like performance, particularly in grasping the contextual meanings of words—an essential aspect of reasoning. In this paper, we present a simple yet computationally efficient framework for multilingual Word Sense Disambiguation (WSD). Our approach reframes the WSD task as a cluster discrimination analysis over a semantic network refined from BabelNet using group algebra. We... | Daniel GuzmanOlivares, Federico Liberatore, Lara Quijano Sánchez |  |
| 440 |  |  [Towards Automatic Evaluation for Image Transcreation](https://doi.org/10.18653/v1/2025.naacl-long.359) |  | 0 | Beyond conventional paradigms of translating speech and text, recently, there has been interest in automated transcreation of images to facilitate localization of visual content across different cultures. Attempts to define this as a formal Machine Learning (ML) problem have been impeded by the lack of automatic evaluation mechanisms, with previous work relying solely on human evaluation. In this paper, we seek to close this gap by proposing a suite of automatic evaluation metrics inspired by machine translation (MT) metrics, categorized into: a) Object-based, b) Embedding-based, and c) VLM-based. Drawing on theories from translation studies and real-world transcreation practices, we... | Graham Neubig, Simran Khanuja, Vivek Iyer, Xiaoyu He |  |
| 441 |  |  [ImgTrojan: Jailbreaking Vision-Language Models with ONE Image](https://doi.org/10.18653/v1/2025.naacl-long.360) |  | 0 | There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions... | Lei Li, Lingpeng Kong, Qi Liu, Shuai Zhong, Xijia Tao |  |
| 442 |  |  [RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement](https://doi.org/10.18653/v1/2025.naacl-long.361) |  | 0 | Existing large language models (LLMs) show exceptional problem-solving capabilities but might struggle with complex reasoning tasks. Despite the successes of chain-of-thought and tree-based search methods, they mainly depend on the internal knowledge of LLMs to search over intermediate reasoning steps, limited to dealing with simple tasks involving fewer reasoning steps. In this paper, we propose RAG-Star, a novel RAG approach that integrates the retrieved information to guide the tree-based deliberative reasoning process that relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree Search, RAG-Star iteratively plans intermediate sub-queries and answers for reasoning based... | Jiayi Chen, Jinhao Jiang, Junyi Li, Ruiyang Ren, Shijie Wang, Tao Zhang, Xin Zhao, Yang Song |  |
| 443 |  |  [Mitigating Biases of Large Language Models in Stance Detection with Counterfactual Augmented Calibration](https://doi.org/10.18653/v1/2025.naacl-long.362) |  | 0 | Stance detection is critical for understanding the underlying position or attitude expressed toward a topic. Large language models (LLMs) have demonstrated significant advancements across various natural language processing tasks including stance detection, however, their performance in stance detection is limited by biases and spurious correlations inherent due to their data-driven nature. Our statistical experiment reveals that LLMs are prone to generate biased stances due to sentiment-stance spurious correlations and preference towards certain individuals and topics. Furthermore, the results demonstrate a strong negative correlation between stance bias and stance detection performance,... | Ang Li, Bin Liang, Hui Wang, Jingqian Zhao, KamFai Wong, Lin Gui, Ruifeng Xu, Xi Zeng, Xingwei Liang |  |
| 444 |  |  [Beyond the Next Token: Towards Prompt-Robust Zero-Shot Classification via Efficient Multi-Token Prediction](https://doi.org/10.18653/v1/2025.naacl-long.363) |  | 0 |  | Hanzhang Zhou, Junlang Qian, Kezhi Mao, Zepeng Zhai, Zijian Feng, Zixiao Zhu |  |
| 445 |  |  [Investigating Hallucinations in Simultaneous Machine Translation: Knowledge Distillation Solution and Components Analysis](https://doi.org/10.18653/v1/2025.naacl-long.364) |  | 0 | Simultaneous Machine Translation (SiMT) generates target translation before receiving the whole source sentence and faces a serious hallucination problem. In contrast, traditional offline machine translation (OMT) models exhibit significantly fewer hallucinations. Motivated by this disparity, we propose Knowledge Distillation for SiMT (KD-SiMT), a simple yet effective method that utilizes the OMT model to mitigate hallucinations in SiMT. Experiments on Zh→En and De→En tasks demonstrate that KD-SiMT effectively reduces hallucinations and enhances the SiMT performance. Furthermore, we systematically investigate the deficiencies in SiMT models related to serious hallucinations and the effect... | Chengqing Zong, Donglei Yu, Feifei Zhai, Nanchang Cheng, Xiaomian Kang, Yu Zhou, Yuchen Liu |  |
| 446 |  |  [Markov Chain of Thought for Efficient Mathematical Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.365) |  | 0 |  | Kai Fan, Minpeng Liao, Wen Yang |  |
| 447 |  |  [Towards Inducing Long-Context Abilities in Multilingual Neural Machine Translation Models](https://doi.org/10.18653/v1/2025.naacl-long.366) |  | 0 | Neural Machine Translation (NMT) models have traditionally used Sinusoidal Positional Embeddings (PEs), which often struggle to capture long-range dependencies and are inefficient for handling extended context or document-level translation tasks. This work addresses the challenge of transitioning pre-trained NMT models from absolute Sinusoidal PEs to Relative PEs, such as RoPE and ALiBi, without compromising performance. We demonstrate that parameter-efficient fine-tuning, using only a small amount of high-quality data, can successfully facilitate this transition. Experimental results indicate that switching from Sinusoidal to Relative PEs results in competitive translation quality on... | Kalika Bali, Pranjal A. Chitale, Varun Gumma |  |
| 448 |  |  [Yeah, Un, Oh: Continuous and Real-time Backchannel Prediction with Fine-tuning of Voice Activity Projection](https://doi.org/10.18653/v1/2025.naacl-long.367) |  | 0 | In human conversations, short backchannel utterances such as “yeah” and “oh” play a crucial role in facilitating smooth and engaging dialogue.These backchannels signal attentiveness and understanding without interrupting the speaker, making their accurate prediction essential for creating more natural conversational agents.This paper proposes a novel method for real-time, continuous backchannel prediction using a fine-tuned Voice Activity Projection (VAP) model.While existing approaches have relied on turn-based or artificially balanced datasets, our approach predicts both the timing and type of backchannels in a continuous and frame-wise manner on unbalanced, real-world datasets.We first... | Divesh Lala, Gabriel Skantze, Koji Inoue, Tatsuya Kawahara |  |
| 449 |  |  [Prompt Compression for Large Language Models: A Survey](https://doi.org/10.18653/v1/2025.naacl-long.368) |  | 0 | Leveraging large language models (LLMs) for complex natural language tasks typically requires long-form prompts to convey detailed requirements and information, which results in increased memory usage and inference costs. To mitigate these challenges, multiple efficient methods have been proposed, with prompt compression gaining significant research interest. This survey provides an overview of prompt compression techniques, categorized into hard prompt methods and soft prompt methods. First, the technical approaches of these methods are compared, followed by an exploration of various ways to understand their mechanisms, including the perspectives of attention optimization,... | Nigel Collier, Yinhong Liu, Yixuan Su, Zongqian Li |  |
| 450 |  |  [Goal-Conditioned DPO: Prioritizing Safety in Misaligned Instructions](https://doi.org/10.18653/v1/2025.naacl-long.369) |  | 0 | Large language models (LLMs) undergo extensive safety training to maximize both helpfulness and harmlessness in their responses. However, various jailbreak attacks jeopardize model safety, allowing malicious actors to bypass safety guidelines. Existing defense methods primarily focus on aligning the model’s output towards less harmful responses through post-processing or input perturbation. Consequently, these approaches are prone to general performance degradation and lack the ability to defend against a wide variety of attacks. In this paper, we propose goal-conditioned direct preference optimization (GC-DPO), which is trained to prioritize the system prompt over the user prompt through... | Joo Bon Maeng, KeeEung Kim, Seokin Seo, Seongmin Lee |  |
| 451 |  |  [K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.370) |  | 0 | Strategic reasoning is a complex yet essential capability for intelligent agents. It requires Large Language Model (LLM) agents to adapt their strategies dynamically in multi-agent environments. Unlike static reasoning tasks, success in these contexts depends on anticipating other agents’ beliefs and actions while continuously adjusting strategies to achieve individual goals. LLMs and LLM agents often struggle with strategic reasoning due to the absence of a reasoning framework that enables them to dynamically infer others’ perspectives and adapt to changing environments. Inspired by the Level-K framework from game theory and behavioral economics, which extends reasoning from simple... | Furu Wei, Man Lan, Shaoguang Mao, Tao Ge, Xun Wang, Yadong Zhang, Yan Xia |  |
| 452 |  |  [SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.371) |  | 0 | Syllogistic reasoning is crucial for Natural Language Inference (NLI). This capability is particularly significant in specialized domains such as biomedicine, where it can support automatic evidence interpretation and scientific discovery. This paper presents SylloBio-NLI, a novel framework that leverages external ontologies to systematically instantiate diverse syllogistic arguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language Models (LLMs) on identifying valid conclusions and extracting supporting evidence across 28 syllogistic schemes instantiated with human genome pathways. Extensive experiments reveal that biomedical syllogistic reasoning is particularly... | André Freitas, Danilo S. Carvalho, Magdalena Wysocka, Marco Valentino, Oskar Wysocki |  |
| 453 |  |  [The State and Fate of Summarization Datasets: A Survey](https://doi.org/10.18653/v1/2025.naacl-long.372) |  | 0 | Automatic summarization has consistently attracted attention due to its versatility and wide application in various downstream tasks. Despite its popularity, we find that annotation efforts have largely been disjointed, and have lacked common terminology. Consequently, it is challenging to discover existing resources or identify coherent research directions. To address this, we survey a large body of work spanning 133 datasets in over 100 languages, creating a novel ontology covering sample properties, collection methods and distribution. With this ontology we make key observations, including the lack of accessible high-quality datasets for low-resource languages, and the field’s... | Gabriel Stanovsky, Noam Dahan |  |
| 454 |  |  [MGM: Global Understanding of Audience Overlap Graphs for Predicting the Factuality and the Bias of News Media](https://doi.org/10.18653/v1/2025.naacl-long.373) |  | 0 | In the current era of rapidly growing digital data, evaluating the political bias and factuality of news outlets has become more important for seeking reliable information online. In this work, we study the classification problem of profiling news media from the lens of political bias and factuality. Traditional profiling methods, such as Pre-trained Language Models (PLMs) and Graph Neural Networks (GNNs) have shown promising results, but they face notable challenges. PLMs focus solely on textual features, causing them to overlook the complex relationships between entities, while GNNs often struggle with media graphs containing disconnected components and insufficient labels. To address... | Dilshod Azizov, Muhammad Arslan Manzoor, Preslav Nakov, Ruihong Zeng, Shangsong Liang |  |
| 455 |  |  [A Logical Fallacy-Informed Framework for Argument Generation](https://doi.org/10.18653/v1/2025.naacl-long.374) |  | 0 | Despite the remarkable performance of large language models (LLMs), they still struggle with generating logically sound arguments, resulting in potential risks such as spreading misinformation. An important factor contributing to LLMs’ suboptimal performance in generating coherent arguments is their oversight of logical fallacies. To address this issue, we introduce fallacy-informed preference optimization (FIPO) that helps steer LLMs toward generating logically sound arguments. FIPO includes a classification loss to capture the fine-grained information on fallacy types. Our results on argument generation tasks show that FIPO reduces the fallacy errors by up to 17.5%. Furthermore, our... | Antoine Bosselut, Boi Faltings, Debjit Paul, Luca Mouchel, Robert West, Shaobo Cui |  |
| 456 |  |  [LLaMA-Berry: Pairwise Optimization for Olympiad-level Mathematical Reasoning via O1-like Monte Carlo Tree Search](https://doi.org/10.18653/v1/2025.naacl-long.375) |  | 0 | This paper presents LLaMA-Berry, an advanced mathematical reasoning framework to enhance the problem-solving ability of large language models (LLMs). The framework combines Monte Carlo Tree Search with Self-Refine (SR-MCTS) to optimize the reasoning paths and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critique and rewriting capabilities of LLMs, our SR-MCTS overcomes the inefficiencies and limitations of conventional step-wise and greedy search algorithms, enabling a more efficient exploration of solution spaces. To guide the search process, we propose the Pairwise Preference Reward Model (PPRM), which predicts pairwise preferences between... | Di Zhang, Dongzhan Zhou, Jianbo Wu, Jiatong Li, Jingdi Lei, Marco Pavone, Shufei Zhang, Tong Che, Tong Xie, Wanli Ouyang, Xiaoshui Huang, Yuqiang Li |  |
| 457 |  |  [Generative Prompt Internalization](https://doi.org/10.18653/v1/2025.naacl-long.376) |  | 0 | Prompts used in recent large language model based applications are often fixed and lengthy, leading to significant computational overhead. To address this challenge, we propose Generative Prompt Internalization (GenPI), a lightweight method that employs a joint training approach. GenPI not only replicates the behavior of models with prompt inputs but also generates the content of the prompt along with reasons for why the model’s behavior should change accordingly. We demonstrate that our approach effectively internalizes complex prompts across various agent-based application scenarios. For effective training without interactions with the dedicated environments, we introduce a data... | Eunbi Choi, Haebin Shin, Lei Ji, Minjoon Seo, Sungdong Kim, Yeyun Gong |  |
| 458 |  |  [Script-Agnosticism and its Impact on Language Identification for Dravidian Languages](https://doi.org/10.18653/v1/2025.naacl-long.377) |  | 0 | Language identification is used as the first step in many data collection and crawling efforts because it allows us to sort online text into language-specific buckets. However, many modern languages, such as Konkani, Kashmiri, Punjabi etc., are synchronically written in several scripts. Moreover, languages with different writing systems do not share significant lexical, semantic, and syntactic properties in neural representation spaces, which is a disadvantage for closely related languages and low-resource languages, especially those from the Indian Subcontinent. To counter this, we propose learning script-agnostic representations using several different experimental strategies (upscaling,... | Antonios Anastasopoulos, Joshua Otten, Milind Agarwal |  |
| 459 |  |  [NAT: Enhancing Agent Tuning with Negative Samples](https://doi.org/10.18653/v1/2025.naacl-long.378) |  | 0 | Interaction trajectories between agents and environments have proven effective in tuning LLMs into task-specific agents. However, constructing these trajectories, especially successful trajectories, is often computationally and time intensive due to the relatively low success rates of even the most advanced LLMs, such as GPT-4 and Claude. Additionally, common training paradigms like supervised fine-tuning (SFT) and reinforcement learning (RL) not only require large volumes of data but also have specific demands regarding the trajectories used. For instance, existing SFT approaches typically utilize only positive examples, limiting their efficiency in low-resource scenarios. To address... | Haonan Li, Renxi Wang, Timothy Baldwin, Xudong Han, Yixuan Zhang |  |
| 460 |  |  [Hazards in Daily Life? Enabling Robots to Proactively Detect and Resolve Anomalies](https://doi.org/10.18653/v1/2025.naacl-long.379) |  | 0 | Existing household robots have made significant progress in performing routine tasks, such as cleaning floors or delivering objects. However, a key limitation of these robots is their inability to recognize potential problems or dangers in home environments. For example, a child may pick up and ingest medication that has fallen on the floor, posing a serious risk. We argue that household robots should proactively detect such hazards or anomalies within the home, and propose the task of anomaly scenario generation. To accomplish this task, we leverage foundational models instead of relying on manually labeled data to build simulated environments. Specifically, we introduce a multi-agent... | Guangxian Ouyang, Hongbin Na, Ling Chen, Meng Fang, Miao Fang, Shiyu Jiang, Xiuying Chen, Yujie Fu, Zeyu Zhang, Zhenhao Chen, Zijing Shi, Zirui Song |  |
| 461 |  |  [How to Make the Most of LLMs' Grammatical Knowledge for Acceptability Judgments](https://doi.org/10.18653/v1/2025.naacl-long.380) |  | 0 | The grammatical knowledge of language models (LMs) is often measured using a benchmark of linguistic minimal pairs, where LMs are presented with a pair of acceptable and unacceptable sentences and required to judge which is more acceptable. Conventional approaches compare sentence probabilities directly, but large language models (LLMs) provide nuanced evaluation methods using prompts and templates. We therefore investigate how to derive the most accurate acceptability judgments from LLMs to comprehensively evaluate their grammatical knowledge. Through extensive experiments in both English and Chinese, we compare nine judgment methods and demonstrate that two of them, in-template LP (a... | Hidetaka Kamigaito, Justin Vasselli, Miyu Oba, Taro Watanabe, Yusuke Ide, Yusuke Sakai, Yuto Nishida |  |
| 462 |  |  [Is Your LLM Outdated? A Deep Look at Temporal Generalization](https://doi.org/10.18653/v1/2025.naacl-long.381) |  | 0 | The rapid advancement of Large Language Models (LLMs) has led to the development of benchmarks that consider temporal dynamics, however, there remains a gap in understanding how well these models can generalize across temporal contexts due to the inherent dynamic nature of language and information. This paper introduces the concept of temporal generalization in LLMs, including bias in past and future generalizations. Then we introduce FreshBench, a new evaluation framework that employs fresh text and event prediction for assessing LLMs’ temporal adaptability, ensuring the evaluation process free from data leakage and subjective bias. The experiment shows significant temporal biases and a... | Benyou Wang, ChenghaoZhu ChenghaoZhu, Nuo Chen, Prayag Tiwari, Yufei Gao, Yunyi Zhang |  |
| 463 |  |  [Towards a Perspectivist Turn in Argument Quality Assessment](https://doi.org/10.18653/v1/2025.naacl-long.382) |  | 0 | The assessment of argument quality depends on well-established logical, rhetorical, and dialectical properties that are unavoidably subjective: multiple valid assessments may exist, there is no unequivocal ground truth. This aligns with recent paths in machine learning, which embrace the co-existence of different perspectives. However, this potential remains largely unexplored in NLP research on argument quality. One crucial reason seems to be the yet unexplored availability of suitable datasets. We fill this gap by conducting a systematic review of argument quality datasets. We assign them to a multi-layered categorization targeting two aspects: (a) What has been annotated: we collect the... | Gabriella Lapesa, Henning Wachsmuth, Julia Romberg, Maximilian Maurer |  |
| 464 |  |  [A Picture is Worth A Thousand Numbers: Enabling LLMs Reason about Time Series via Visualization](https://doi.org/10.18653/v1/2025.naacl-long.383) |  | 0 | Large language models (LLMs), with demonstrated reasoning abilities across multiple domains, have been largely underexplored fortime-series reasoning (TsR), which is ubiquitous in the real world. In this work, wepropose TimerBed, the first comprehensivetestbed for evaluating LLMs’ TsR performance.Specifically, TimerBed includes stratified reasoning patterns with real-world tasks, diversecombinations of LLMs and reasoning strategies, and various supervised models as comparison anchors. We perform extensive experiments with TimerBed, test multiple current beliefs, and observe the initial failuresof LLMs in TsR, as evidenced by the ineffectiveness of zero shot (ZST) and performancedegradation... | B. Aditya Prakash, Chenghao Liu, Haoxin Liu |  |
| 465 |  |  [PlagBench: Exploring the Duality of Large Language Models in Plagiarism Generation and Detection](https://doi.org/10.18653/v1/2025.naacl-long.384) |  | 0 | Recent studies have raised concerns about the potential threats large language models (LLMs) pose to academic integrity and copyright protection. Yet, their investigation is predominantly focused on literal copies of original texts. Also, how LLMs can facilitate the detection of LLM-generated plagiarism remains largely unexplored. To address these gaps, we introduce PlagBench, a dataset of 46.5K synthetic text pairs that represent three major types of plagiarism: verbatim copying, paraphrasing, and summarization. These samples are generated by three advanced LLMs. We rigorously validate the quality of PlagBench through a combination of fine-grained automatic evaluation and human... | Adaku Uchendu, Dongwon Lee, Jinghui Chen, Jooyoung Lee, Thai Le, Toshini Agrawal |  |
| 466 |  |  [Commonality and Individuality! Integrating Humor Commonality with Speaker Individuality for Humor Recognition](https://doi.org/10.18653/v1/2025.naacl-long.385) |  | 0 | Humor recognition aims to identify whether a specific speaker’s text is humorous. Current methods for humor recognition mainly suffer from two limitations: (1) they solely focus on one aspect of humor commonalities, ignoring the multifaceted nature of humor; and (2) they typically overlook the critical role of speaker individuality, which is essential for a comprehensive understanding of humor expressions. To bridge these gaps, we introduce the Commonality and Individuality Incorporated Network for Humor Recognition (CIHR), a novel model designed to enhance humor recognition by integrating multifaceted humor commonalities with the distinctive individuality of speakers. The CIHR features a... | Haohao Zhu, Hongfei Lin, Junyu Lu, Liang Yang, Xiaokun Zhang, Zewen Bai, Zeyuan Zeng |  |
| 467 |  |  [CAST: Corpus-Aware Self-similarity Enhanced Topic modelling](https://doi.org/10.18653/v1/2025.naacl-long.386) |  | 0 | Topic modelling is a pivotal unsupervised machine learning technique for extracting valuable insights from large document collections. Existing neural topic modelling methods often encode contextual information of documents, while ignoring contextual details of candidate centroid words, leading to the inaccurate selection of topic words due to the \*contextualization gap\*. In parallel, it is found that functional words are frequently selected over topical words. To address these limitations, we introduce \*\*CAST\*\*: \*\*C\*\*orpus-\*\*A\*\*ware \*\*S\*\*elf-similarity Enhanced \*\*T\*\*opic modelling, a novel topic modelling method that builds upon candidate centroid word embeddings... | Chenghao Xiao, Chenghua Lin, Chenhan Yuan, Goran Nenadic, Lamiece Hassan, Sabine N. van der Veer, Yanan Ma |  |
| 468 |  |  [A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding](https://doi.org/10.18653/v1/2025.naacl-long.387) |  | 0 | Dialogue State Tracking (DST) is crucial for understanding user needs and executing appropriate system actions in task-oriented dialogues. Majority of existing DST methods are designed to work within predefined ontologies and assume the availability of gold domain labels, struggling with adapting to new slots values. While Large Language Models (LLMs)-based systems show promising zero-shot DST performance, they either require extensive computational resources or they underperform existing fully-trained systems, limiting their practicality. To address these limitations, we propose a zero-shot, open-vocabulary system that integrates domain classification and DST in a single pipeline. Our... | Abdulfattah Safa, Gözde Gül Sahin |  |
| 469 |  |  [Navigating the Cultural Kaleidoscope: A Hitchhiker's Guide to Sensitivity in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.388) |  | 0 | Cultural harm stems in LLMs whereby these models fail to align with specific cultural norms, resulting in misrepresentations or violations of cultural values. This work addresses the challenges of ensuring cultural sensitivity in LLMs, especially in small-parameter models that often lack the extensive training data needed to capture global cultural nuances. We present two key contributions: (1) A cultural harm test dataset, created to assess model outputs across different cultural contexts through scenarios that expose potential cultural insensitivities, and (2) A culturally aligned preference dataset, aimed at restoring cultural sensitivity through fine-tuning based on feedback from... | Animesh Mukherjee, Avik Halder, Hari Shrawgi, Parag Agrawal, Rajarshi Mandal, Rima Hazra, Sagnik Basu, Sayan Layek, Shanu Kumar, Somnath Banerjee |  |
| 470 |  |  [Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models](https://doi.org/10.18653/v1/2025.naacl-long.389) |  | 0 | Text-to-image (T2I) diffusion models rely on encoded prompts to guide the image generation process. Typically, these prompts are extended to a fixed length by appending padding tokens to the input. Despite being a default practice, the influence of padding tokens on the image generation process has not been investigated. In this work, we conduct the first in-depth analysis of the role padding tokens play in T2I models. We develop two causal techniques to analyze how information is encoded in the representation of tokens across different components of the T2I pipeline. Using these techniques, we investigate when and how padding tokens impact the image generation process. Our findings reveal... | Gal Chechik, Hadas Orgad, Ido Galil, Michael Toker, Rinon Gal, Yoad Tewel, Yonatan Belinkov |  |
| 471 |  |  [In-Context Learning (and Unlearning) of Length Biases](https://doi.org/10.18653/v1/2025.naacl-long.390) |  | 0 | Large language models have demonstrated strong capabilities to learn in-context, where exemplar input-output pairings are appended to the prompt for demonstration. However, existing work has demonstrated the ability of models to learn lexical and label biases in-context, which negatively impacts both performance and robustness of models. The impact of other statistical data biases remains under-explored, which this work aims to address. We specifically investigate the impact of length biases on in-context learning. We demonstrate that models do learn length biases in the context window for their predictions, and further empirically analyze the factors that modulate the level of bias... | Stephanie Schoch, Yangfeng Ji |  |
| 472 |  |  [AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising](https://doi.org/10.18653/v1/2025.naacl-long.391) |  | 0 | As the fluency of ad texts automatically generated by natural language generation technologies continues to improve, there is an increasing demand to assess the quality of these creatives in real-world setting.We propose \*\*AdTEC\*\*, the first public benchmark to evaluate ad texts from multiple perspectives within practical advertising operations.Our contributions are as follows: (i) Defining five tasks for evaluating the quality of ad texts, as well as constructing a Japanese dataset based on the practical operational experiences of advertising agencies, which are typically maintained in-house. (ii) Validating the performance of existing pre-trained language models (PLMs) and human... | Hiroki Ouchi, Masato Mita, Peinan Zhang, Taro Watanabe, Yusuke Sakai |  |
| 473 |  |  [Empowering Retrieval-based Conversational Recommendation with Contrasting User Preferences](https://doi.org/10.18653/v1/2025.naacl-long.392) |  | 0 | Conversational recommender systems (CRSs) are designed to suggest the target item that the user is likely to prefer through multi-turn conversations. Recent studies stress that capturing sentiments in user conversations improves recommendation accuracy. However, they employ a single user representation, which may fail to distinguish between contrasting user intentions, such as likes and dislikes, potentially leading to suboptimal performance. To this end, we propose a novel conversational recommender model, called COntrasting user pReference expAnsion and Learning (CORAL). Firstly, CORAL extracts the user’s hidden pref- erences through contrasting preference expansion using the reasoning... | Heejin Kook, Jongwuk Lee, Junyoung Kim, Seongmin Park |  |
| 474 |  |  [LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices](https://doi.org/10.18653/v1/2025.naacl-long.393) |  | 0 | With the commercialization of large language models (LLMs), weight-activation quantization has emerged to compress and accelerate LLMs, achieving high throughput while reducing inference costs. However, existing post-training quantization (PTQ) techniques for quantizing weights and activations of LLMs still suffer from non-negligible accuracy drops, especially on massive multitask language understanding. To address this issue, we propose Low-Rank Quantization (LRQ) - a simple yet effective post-training weight quantization method for LLMs that reconstructs the outputs of an intermediate Transformer block by leveraging low-rank weight-scaling matrices, replacing the conventional full... | Dongsoo Lee, Eunho Yang, Jeonghoon Kim, June Yong Yang, Jung Hyun Lee, Kang Min Yoo, Se Jung Kwon |  |
| 475 |  |  [Towards Robust Knowledge Representations in Multilingual LLMs for Equivalence and Inheritance based Consistent Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.394) |  | 0 | Reasoning and linguistic skills form the cornerstone of human intelligence, facilitating problem-solving and decision-making. Recent advances in Large Language Models (LLMs) have led to impressive linguistic capabilities and emergent reasoning behaviors, fueling widespread adoption across application domains. However, LLMs still struggle with complex reasoning tasks, highlighting their systemic limitations. In this work, we focus on evaluating whether LLMs have the requisite representations to reason using two foundational relationships: “equivalence” and “inheritance”. We introduce novel tasks and benchmarks spanning six languages and observe that current SOTA LLMs often produce... | Gaurav Arora, Shreya Jain, Srujana Merugu, Vaibhav Saxena |  |
| 476 |  |  [LLMs as Meta-Reviewers' Assistants: A Case Study](https://doi.org/10.18653/v1/2025.naacl-long.395) |  | 0 | One of the most important yet onerous tasks in the academic peer-reviewing process is composing meta-reviews, which involves assimilating diverse opinions from multiple expert peers, formulating one’s self-judgment as a senior expert, and then summarizing all these perspectives into a concise holistic overview to make an overall recommendation. This process is time-consuming and can be compromised by human factors like fatigue, inconsistency, missing tiny details, etc. Given the latest major developments in Large Language Models (LLMs), it is very compelling to rigorously study whether LLMs can help meta-reviewers perform this important task better. In this paper, we perform a case study... | Dongji Feng, Eftekhar Hossain, John Salvador, Matthew C. Williams Jr., Matthew Freestone, Md. Mahadi Hassan, Mousumi Akter, Naman Bansal, R. Alexander Knipper, Sanjeev Kumar Sinha, Santu Karmaker, Souvika Sarkar, Sri Guttikonda, Yash Mahajan |  |
| 477 |  |  [A Survey of NLP Progress in Sino-Tibetan Low-Resource Languages](https://doi.org/10.18653/v1/2025.naacl-long.396) |  | 0 | Despite the increasing effort in including more low-resource languages in NLP/CL development, most of the world’s languages are still absent. In this paper, we take the example of the Sino-Tibetan language family which consists of hundreds of low-resource languages, and we look at the representation of these low-resource languages in papers archived on ACL Anthology. Our findings indicate that while more techniques and discussions on more languages are present in more publication venues over the years, the overall focus on this language family has been minimal. The lack of attention might be owing to the small number of native speakers and governmental support of these languages. The... | Michael Best, Shuheng Liu |  |
| 478 |  |  [Enhancing Language Model Hypernetworks with Restart: A Study on Optimization](https://doi.org/10.18653/v1/2025.naacl-long.397) |  | 0 | Hypernetworks are a class of meta-networks that generate weights for main neural networks. Their unique parameter spaces necessitate exploring suitable optimization strategies to enhance performance, especially for language models. However, a comprehensive investigation into optimization strategies for hypernetworks remains absent. To address this gap, we analyze the loss landscape of hypernetworks and propose that restart optimization strategies can improve their performance for language models. We find that hypernetworks have inherently more complicated loss landscapes compared to conventional networks due to their distinct parameter spaces. Consequently, a restart strategy that... | Jie Chen, Jie Fu, Rongrong Ji, Yihan Zhang |  |
| 479 |  |  [Functional Lexicon in Subword Tokenization](https://doi.org/10.18653/v1/2025.naacl-long.398) |  | 0 | The distinction between function and content units of the lexicon has been somewhat neglected in recent NLP work, but it could still be useful when working with low-resource languages, and, in particular, to improve cross-lingual transfer. In this paper, we investigate to what extent BPE subword tokenization can be used to identify units of the functional lexicon in a language without any annotated data. We analyze subword tokens in terms of their productivity and attempt to find thresholds that best distinguish function from content tokens. On a sample of seven diverse languages, we find that the best results are obtained with 50 BPE merges. We also show that this subword tokenization... | Tanja Samardzic, Yves Scherrer, Zachary William Hopton |  |
| 480 |  |  [Getting More Juice Out of Your Data: Hard Pair Refinement Enhances Visual-Language Models Without Extra Data](https://doi.org/10.18653/v1/2025.naacl-long.399) |  | 0 | Contrastive Language-Image Pre-training (CLIP) has become the standard for cross- modal image-text representation learning. Improving CLIP typically requires additional data and retraining with new loss functions, but these demands raise resource and time costs, limiting practical use. In this work, we introduce HELIP, a cost-effective strategy that improves CLIP models by exploiting challenging text-image pairs within existing datasets in continuous training. This eliminates the need for additional data or extensive retraining. Moreover, HELIP integrates effortlessly into current training pipelines with minimal code modifications, allowing for quick and seamless implementation. On... | Hang Xu, Haonan Wang, Hong Cheng, Kenji Kawaguchi, Lanqing Hong, Minbin Huang, Runhui Huang, Tianyang Hu, Xiaodan Liang, Zhenguo Li |  |
| 481 |  |  [Evaluating the Prompt Steerability of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.400) |  | 0 | Building pluralistic AI requires designing models that are able to be shaped to represent a wide range of value systems and cultures. Achieving this requires first being able to evaluate the degree to which a given model is capable of reflecting various personas. To this end, we propose a benchmark for evaluating the steerability of model personas as a function of prompting. Our design is based on a formal definition of prompt steerability, which analyzes the degree to which a model’s joint behavioral distribution can be shifted from its baseline. By defining steerability indices and inspecting how these indices change as a function of steering effort, we can estimate the steerability of a... | Djallel Bouneffouf, Eitan Farchi, Elizabeth M. Daly, Erik Miehling, Jesus Rios, Karthikeyan Natesan Ramamurthy, Kush R. Varshney, Miao Liu, Michael Desmond, Pierre Dognin, Prasanna Sattigeri |  |
| 482 |  |  [A Data-Driven Method for Analyzing and Quantifying Lyrics-Dance Motion Relationships](https://doi.org/10.18653/v1/2025.naacl-long.401) |  | 0 | Dancing to music with lyrics is a popular form of expression. While it is generally accepted that there are relationships between lyrics and dance motions, previous studies have not explored these relationships. A major challenge is that the relationships between lyrics and dance motions are not constant throughout a song but are instead localized to specific parts. To address this challenge, we hypothesize that lyrics and dance motions that co-occur across multiple songs are related. Based on this hypothesis, we propose a novel data-driven method to detect the parts of songs where meaningful relationships between lyrics and dance motions exist. We use clustering to transform lyrics and... | Kento Watanabe, Masataka Goto |  |
| 483 |  |  [CROPE: Evaluating In-Context Adaptation of Vision and Language Models to Culture-Specific Concepts](https://doi.org/10.18653/v1/2025.naacl-long.402) |  | 0 | As Vision and Language models (VLMs) become accessible across the globe, it is important that they demonstrate cultural knowledge. In his paper, we introduce CROPE, a visual question answering benchmark designed to probe the knowledge of culture-specific concepts and evaluate the capacity for cultural adaptation through contextual information. This allows us to distinguish between parametric knowledge acquired during training and contextual knowledge provided during inference via visual and textual descriptions. Our evaluation of several state-of-the-art open VLMs shows large performance disparities between culture-specific and common concepts in the parametric setting. Moreover,... | Alessandro Suglia, Georgios Pantazopoulos, Ioannis Konstas, Malvina Nikandrou, Nikolas Vitsakis |  |
| 484 |  |  [PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona](https://doi.org/10.18653/v1/2025.naacl-long.403) |  | 0 | Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests through natural language interactions, yet existing systems often produce generic, monotonic responses that lack individuality and fail to adapt to users’ personal attributes. To address this, we introduce PicPersona-TOD, a novel dataset that incorporates user images as part of the persona, enabling personalized responses tailored to user-specific factors such as age or emotional context. This is facilitated by first impressions, dialogue policy-guided prompting, and the use of external knowledge to reduce hallucinations. Human evaluations confirm that our dataset enhances user experience, with personalized... | Gary Lee, Jihyun Lee, Seungyeon Seo, Yejin Jeon |  |
| 485 |  |  [Scaling LLM Inference Efficiently with Optimized Sample Compute Allocation](https://doi.org/10.18653/v1/2025.naacl-long.404) |  | 0 | Sampling is a basic operation for large language models (LLMs). In reinforcement learning rollouts and meta generation algorithms such as Best-of-N, it is essential to sample correct trajectories within a given compute budget. To find an optimal allocation for sample compute budgets, several choices need to be made:Which sampling configurations (model, temperature, language, etc.) to use?How many samples to generate in each configuration?We formulate these choices as a learning problem and propose OSCA, an algorithm that Optimizes Sample Compute Allocation by finding an optimal mix of different inference configurations.Our experiments show that with our learned mixed allocation, we can... | Danqing Wang, Kexun Zhang, Lei Li, Shang Zhou, William Yang Wang |  |
| 486 |  |  [Large Language Models for Persian-English Idiom Translation](https://doi.org/10.18653/v1/2025.naacl-long.405) |  | 0 | Large language models (LLMs) have shown superior capabilities in translating figurative language compared to neural machine translation (NMT) systems. However, the impact of different prompting methods and LLM-NMT combinations on idiom translation has yet to be thoroughly investigated. This paper introduces two parallel datasets of sentences containing idiomatic expressions for Persian→English and English→Persian translations, with Persian idioms sampled from our PersianIdioms resource, a collection of 2,200 idioms and their meanings, with 700 including usage examples.Using these datasets, we evaluate various open- and closed-source LLMs, NMT models, and their combinations. Translation... | Faezeh Hosseini, Sara Rezaeimanesh, Yadollah Yaghoobzadeh |  |
| 487 |  |  [Follow the Beaten Path: The Role of Route Patterns on Vision-Language Navigation Agents Generalization Abilities](https://doi.org/10.18653/v1/2025.naacl-long.406) |  | 0 | Vision and language navigation (VLN) is a challenging task towards the creation of embodied agents that requires spatial and temporal reasoning over the instructions provided in natural language and aligning them with the visual perception of an environment. Although a number of methods and approaches have been developed, none achieves human level performance in outdoor settings (by up to 75 percent). The contributions of visual and language modalities to the success of VLN have been studied, however here we focus on an overlooked property of routes and show that navigational instructions can be represented as patterns of actions that also describe trajectory shapes. Through carefully... | Antonios Anastasopoulos, Dieter Pfoser, Kourosh T. Baghaei |  |
| 488 |  |  [Sneaking Syntax into Transformer Language Models with Tree Regularization](https://doi.org/10.18653/v1/2025.naacl-long.407) |  | 0 | While compositional accounts of human language understanding are based on a hierarchical tree-like process, neural models like transformers lack a direct inductive bias for such tree structures. Introducing syntactic inductive biases could unlock more robust and data-efficient learning in transformer language models (LMs), but existing methods for incorporating such structure greatly restrict models, either limiting their expressivity or increasing inference complexity. This work instead aims to softly inject syntactic inductive biases into given transformer circuits, through a structured regularizer. We introduce TreeReg, an auxiliary loss function that converts bracketing decisions from... | Ananjan Nandi, Christopher D. Manning, Shikhar Murty |  |
| 489 |  |  [Meta-Cultural Competence: Climbing the Right Hill of Cultural Awareness](https://doi.org/10.18653/v1/2025.naacl-long.408) |  | 0 | Numerous recent studies have shown that Large Language Models (LLMs) are biased towards a Western and Anglo-centric worldview, which compromises their usefulness in non-Western cultural settings. However, “culture” is a complex, multifaceted topic, and its awareness, representation, and modeling in LLMs and LLM-based applications can be defined and measured in numerous ways. In this position paper, we ask what does it mean for an LLM to possess “cultural awareness”, and through a thought experiment, which is an extension of the Octopus test proposed by Bender and Koller (2020), we argue that it is not cultural awareness or knowledge, rather meta-cultural competence, which is required of an... | Monojit Choudhury, Saurabh Kumar Pandey, Sougata Saha |  |
| 490 |  |  [Reading between the Lines: Can LLMs Identify Cross-Cultural Communication Gaps?](https://doi.org/10.18653/v1/2025.naacl-long.409) |  | 0 | In a rapidly globalizing and digital world, content such as book and product reviews created by people from diverse cultures are read and consumed by others from different corners of the world. In this paper, we investigate the extent and patterns of gaps in understandability of book reviews due to the presence of culturally-specific items and elements that might be alien to users from another culture. Our user-study on 57 book reviews from Goodreads reveal that 83% of the reviews had at least one culture-specific difficult-to-understand element. We also evaluate the efficacy of GPT-4o in identifying such items, given the cultural background of the reader; the results are mixed, implying a... | Harshit Gupta, Monojit Choudhury, Saurabh Kumar Pandey, Sougata Saha |  |
| 491 |  |  [HMT: Hierarchical Memory Transformer for Efficient Long Context Language Processing](https://doi.org/10.18653/v1/2025.naacl-long.410) |  | 0 | Transformer-based large language models (LLM) have been widely used in language processing applications. However, due to the memory constraints of the devices, most of them restrict the context window. Even though recurrent models in previous works can memorize past tokens to enable unlimited context and maintain effectiveness, they have “flat” memory architectures. Such architectures have limitations in selecting and filtering information. Since humans are good at learning and self-adjustment, we believe that imitating brain memory hierarchy is beneficial for model memorization. Thus, we propose the Hierarchical Memory Transformer (HMT), a novel framework that facilitates a model’s... | Jason Cong, Neha Prakriya, Yingqi Cao, Yizhou Sun, Zifan He, Zongyue Qin |  |
| 492 |  |  [Faux Polyglot: A Study on Information Disparity in Multilingual Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.411) |  | 0 | Although the multilingual capability of LLMs offers new opportunities to overcome the language barrier, do these capabilities translate into real-life scenarios where linguistic divide and knowledge conflicts between multilingual sources are known occurrences? In this paper, we studied LLM’s linguistic preference in a cross-language RAG-based information search setting. We found that LLMs displayed systemic bias towards information in the same language as the query language in both document retrieval and answer generation. Furthermore, in scenarios where no information is in the language of the query, LLMs prefer documents in high-resource languages during generation, potentially... | Kenton Murray, Nikhil Sharma, Ziang Xiao |  |
| 493 |  |  [Teaching Models to Balance Resisting and Accepting Persuasion](https://doi.org/10.18653/v1/2025.naacl-long.412) |  | 0 | Large language models (LLMs) are susceptible to persuasion, which can pose risks when models are faced with an adversarial interlocutor. We take a first step towards defending models against persuasion while also arguing that defense against adversarial (i.e. \*negative\*) persuasion is only half of the equation: models should also be able to accept beneficial (i.e. \*positive\*) persuasion to improve their answers. We show that optimizing models for only one side results in poor performance on the other. In order to balance positive and negative persuasion, we introduce \*\*P\*\*ersuasion-\*\*B\*\*alanced \*\*T\*\*raining (or \*\*PBT\*\*), which leverages multi-agent recursive dialogue... | Elias StengelEskin, Mohit Bansal, Peter Hase |  |
| 494 |  |  [Making Language Models Robust Against Negation](https://doi.org/10.18653/v1/2025.naacl-long.413) |  | 0 | Negation has been a long-standing challenge for language models.Previous studies have shown that they struggle with negation in many natural language understanding tasks.In this work, we propose a self-supervised method to make language models more robust against negation.We introduce a novel task, Next Sentence Polarity Prediction (NSPP), and a variation of the Next Sentence Prediction (NSP) task.We show that BERT and RoBERTa further pre-trained on our tasks outperform the off-the-shelf versions on nine negation-related benchmarks.Most notably, our pre-training tasks yield between 1.8% and 9.1% improvement on CondaQA, a large question-answering corpus requiring reasoning over negation. | Eduardo Blanco, MohammadHossein Rezaei |  |
| 495 |  |  [Through the Lens of History: Methods for Analyzing Temporal Variation in Content and Framing of State-run Chinese Newspapers](https://doi.org/10.18653/v1/2025.naacl-long.414) |  | 0 | State-run Chinese newspapers are believed to strategically select and frame news articles to align with the shifting political tides of the country. This paper describes methods to quantify these changes in content and framing over time. Looking at more than 50 years of articles from the People’s Daily and Reference News, we analyze differences in name mentions and sentiment in news articles for politicians before and after their deaths, as well as during and not during certain political events. We find significant estimates of difference, reflecting the changes in various aspects of the political environment in China during different time periods. We also apply change point detection... | David A. Smith, Shijia Liu |  |
| 496 |  |  [PoisonedParrot: Subtle Data Poisoning Attacks to Elicit Copyright-Infringing Content from Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.415) |  | 0 | As the capabilities of large language models (LLMs) continue to expand, their usage has become increasingly prevalent. However, as reflected in numerous ongoing lawsuits regarding LLM-generated content, addressing copyright infringement remains a significant challenge. In this paper, we introduce PoisonedParrot: the first stealthy data poisoning attack that induces an LLM to generate copyrighted content even when the model has not been directly trained on the specific copyrighted material. PoisonedParrot integrates small fragments of copyrighted text into the poison samples using an off-the-shelf LLM. Despite its simplicity, evaluated in a wide range of experiments, PoisonedParrot is... | Aakriti Agrawal, Bang An, Furong Huang, MichaelAndrei PanaitescuLiess, Pankayaraj Pathmanathan, Sicheng Zhu, Yigitcan Kaya, Zora Che |  |
| 497 |  |  [Towards Operationalizing Right to Data Protection](https://doi.org/10.18653/v1/2025.naacl-long.416) |  | 0 | The widespread practice of indiscriminate data scraping to fine-tune language models (LMs) raises significant legal and ethical concerns, particularly regarding compliance with data protection laws such as the General Data Protection Regulation (GDPR). This practice often results in the unauthorized use of personal information, prompting growing debate within the academic and regulatory communities. Recent works have introduced the concept of generating unlearnable datasets (by adding imperceptible noise to the clean data), such that the underlying model achieves lower loss during training but fails to generalize to the unseen test setting. Though somewhat effective, these approaches are... | Abhinav Java, Chirag Agarwal, Simra Shahid |  |
| 498 |  |  [Learning vs Retrieval: The Role of In-Context Examples in Regression with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.417) |  | 0 | Generative Large Language Models (LLMs) are capable of being in-context learners. However, the underlying mechanism of in-context learning (ICL) is still a major research question, and experimental research results about how models exploit ICL are not always consistent. In this work, we propose a framework for evaluating in-context learning mechanisms, which we claim are a combination of retrieving internal knowledge and learning from in-context examples by focusing on regression tasks. First, we show that LLMs can solve real-world regression problems and then design experiments to measure the extent to which the LLM retrieves its internal knowledge versus learning from in-context... | Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi |  |
| 499 |  |  [GLiREL - Generalist Model for Zero-Shot Relation Extraction](https://doi.org/10.18653/v1/2025.naacl-long.418) |  | 0 | We introduce GLiREL, an efficient architecture and training paradigm for zero-shot relation classification. Identifying relationships between entities is a key task in information extraction pipelines. The zero-shot setting for relation extraction, where a taxonomy of relations is not pre-specified, has proven to be particularly challenging because of the computational complexity of inference, and because of the lack of labeled training data with sufficient coverage. Existing approaches rely upon distant supervision using auxiliary models to generate training data for unseen labels, upon very large general-purpose large language models (LLMs), or upon complex pipelines models with multiple... | Chris Hokamp, Demian Gholipour Ghalandari, Jack Boylan |  |
| 500 |  |  [ComPO: Community Preferences for Language Model Personalization](https://doi.org/10.18653/v1/2025.naacl-long.419) |  | 0 | Conventional algorithms for training language models (LMs) with human feedback rely on preferences that are assumed to account for an “average” user, disregarding subjectivity and finer-grained variations. Recent studies have raised concerns that aggregating such diverse and often contradictory human feedback to finetune models results in generic models that generate outputs not preferred by many user groups, as they tend to average out styles and norms. To address this issue, we draw inspiration from recommendation systems and propose ComPO, a method to personalize preference optimization in LMs by contextualizing the probability distribution of model outputs with the preference provider.... | Chan Young Park, Hannaneh Hajishirzi, Noah A. Smith, Sachin Kumar, Yulia Tsvetkov |  |
| 501 |  |  [GroundCocoa: A Benchmark for Evaluating Compositional & Conditional Reasoning in Language Models](https://doi.org/10.18653/v1/2025.naacl-long.420) |  | 0 | The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their reasoning to address complex task requirements. However, LLMs are known to unexpectedly falter in simple tasks and under seemingly straightforward circumstances - underscoring the need for better and more diverse evaluation setups to measure their true capabilities. To this end, we choose to study compositional and conditional reasoning, two aspects that are central to human cognition, and introduce GroundCocoa - a lexically diverse benchmark connecting these reasoning... | Harsh Kohli, Huan Sun, Sachin Kumar |  |
| 502 |  |  [ALPACA AGAINST VICUNA: Using LLMs to Uncover Memorization of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.421) |  | 0 | In this paper, we investigate the overlooked impact of instruction-tuning on memorization in large language models (LLMs), which has largely been studied in base, pre-trained models. We propose a black-box prompt optimization method where an attacker LLM agent uncovers higher levels of memorization in a victim agent, surpassing traditional approaches that prompt the model directly with training data. Using an iterative rejection-sampling process, we design instruction-based prompts that minimize overlap with training data to avoid providing direct solutions while maximizing overlap between the victim’s output and the training data to induce memorization. Our method shows 23.7% more overlap... | Aly M. Kassem, Hyunwoo Kim, Niloofar Mireshghallah, Omar Mahmoud, Santu Rana, Sherif Saad, Yejin Choi, Yulia Tsvetkov |  |
| 503 |  |  [Evaluating Contextualized Representations of (Spanish) Ambiguous Words: A New Lexical Resource and Empirical Analysis](https://doi.org/10.18653/v1/2025.naacl-long.422) |  | 0 | Lexical ambiguity—where a single wordform takes on distinct, context-dependent meanings–serves as a useful tool to compare across different language models’ (LMs’) ability to form distinct, contextualized representations of the same stimulus. Few studies have systematically compared LMs’ contextualized word embeddings for languages beyond English. Here, we evaluate semantic representations of Spanish ambiguous nouns in context in a suite of Spanish-language monolingual and multilingual BERT-based models. We develop a novel dataset of minimal-pair sentences evoking the same or different sense for a target ambiguous noun. In a pre-registered study, we collect contextualized human relatedness... | Anne L. BeattyMartínez, Pamela D. Rivière, Sean Trott |  |
| 504 |  |  [Understanding LLMs' Fluid Intelligence Deficiency: An Analysis of the ARC Task](https://doi.org/10.18653/v1/2025.naacl-long.423) |  | 0 | While LLMs have exhibited strong performance on various NLP tasks, it is noteworthy that most of these tasks rely on utilizing the vast amount of knowledge encoded in LLMs’ parameters, rather than solving new problems without prior knowledge. In cognitive research, the latter ability is referred to as fluid intelligence, which is considered to be critical for assessing human intelligence. Recent research on fluid intelligence assessments has highlighted significant deficiencies in LLMs’ abilities. In this paper, we analyze the challenges LLMs face in demonstrating fluid intelligence through controlled experiments, using the most representative ARC task as an example. Our study revealed... | DitYan Yeung, Jie Zhou, Junjie Wu, Lemao Liu, Mo Yu |  |
| 505 |  |  [FedSpaLLM: Federated Pruning of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.424) |  | 0 | Large Language Models (LLMs) achieve state-of-the-art performance but are challenging to deploy due to their high computational and storage demands. Pruning can reduce model size, yet existing methods assume public access to calibration data, which is impractical for privacy-sensitive applications. To address the challenge of pruning LLMs in privacy-preserving settings, we propose FedSpaLLM, the first federated learning framework designed specifically for pruning LLMs. FedSpaLLM enables clients to locally prune their models based on private data while accounting for system heterogeneity and maintaining communication efficiency. Our framework introduces several key innovations: (1) a novel... | Guangji Bai, Kibaek Kim, Liang Zhao, Yijiang Li, Zilinghan Li |  |
| 506 |  |  [IHEval: Evaluating Language Models on Following the Instruction Hierarchy](https://doi.org/10.18653/v1/2025.naacl-long.425) |  | 0 | The instruction hierarchy, which establishes a priority order from system messages to user messages, conversation history, and tool outputs, is essential for ensuring consistent and safe behavior in language models (LMs). Despite its importance, this topic receives limited attention, and there is a lack of comprehensive benchmarks for evaluating models’ ability to follow the instruction hierarchy. We bridge this gap by introducing IHEval, a novel benchmark comprising 3,538 examples across nine tasks, covering cases where instructions in different priorities either align or conflict. Our evaluation of popular LMs highlights their struggle to recognize instruction priorities. All evaluated... | Bing Yin, Haodong Wang, Haoming Jiang, Meng Jiang, Qingyu Yin, Shiyang Li, Xianfeng Tang, Xin Liu, Yichuan Li, Yifan Gao, Zhaoxuan Tan, Zheng Li, Zhihan Zhang, Zixuan Zhang |  |
| 507 |  |  [Afrispeech-Dialog: A Benchmark Dataset for Spontaneous English Conversations in Healthcare and Beyond](https://doi.org/10.18653/v1/2025.naacl-long.426) |  | 0 | Speech technologies are transforming interactions across various sectors, from healthcare to call centers and robots, yet their performance on African-accented conversations remains underexplored. We introduce Afrispeech-Dialog, a benchmark dataset of 50 simulated medical and non-medical African-accented English conversations, designed to evaluate automatic speech recognition (ASR) and related technologies. We assess state-of-the-art (SOTA) speaker diarization and ASR systems on long-form, accented speech, comparing their performance with native accents and discover a 10%+ performance degradation. Additionally, we explore medical conversation summarization capabilities of large language... | Boluwatife Adeleye Adewale, Chibuzor Okocha, Devendra Deepak Kayande, Emmanuel Ayodele, Folafunmi Omofoye, Lukman E. Ismaila, Mardhiyah Sanni, Michael S. Mollel, Moshood Yekini, Naome A. Etori, Tassallah Abdullahi, Tobi Olatunji |  |
| 508 |  |  [THREAD: Thinking Deeper with Recursive Spawning](https://doi.org/10.18653/v1/2025.naacl-long.427) |  | 0 | Large language models (LLMs) have shown impressive capabilities across diverse settings, but still struggle as the length and complexity of the context increases. To address this challenge, we propose Thinking Recursively and Dynamically (ThReaD). THREAD frames model generation as a thread of execution that, based on the context, can run to completion or dynamically spawn new threads. By spawning, threads can offload work (e.g., thinking, retrieving information) to child threads, which only return tokens needed for the parent thread to do its work. We apply THREAD in the settings of LLM task solving and question answering, where the dynamic threading allows the model to recursively... | Hongyin Luo, James R. Glass, Nathaniel Morgan, Philip Schroeder |  |
| 509 |  |  [CORG: Generating Answers from Complex, Interrelated Contexts](https://doi.org/10.18653/v1/2025.naacl-long.428) |  | 0 | In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce Context Organizer (COrg), a framework that organizes multiple contexts into independently processed... | Franck Dernoncourt, Hyunji Lee, Seunghyun Yoon, Trung Bui |  |
| 510 |  |  [Generating Diverse Hypotheses for Inductive Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.429) |  | 0 | Inductive reasoning — the process of inferring general rules from a small number of observations — is a fundamental aspect of human intelligence. Recent works suggest that large language models (LLMs) can engage in inductive reasoning by sampling multiple hypotheses about the rules and selecting the one that best explains the observations. However, due to the IID sampling, semantically redundant hypotheses are frequently generated, leading to significant wastage of compute. In this paper, we 1) demonstrate that increasing the temperature to enhance the diversity is limited due to text degeneration issue, and 2) propose a novel method to improve the diversity while maintaining text quality.... | Dongryeol Lee, Hyukhun Koh, Kangil Lee, Kyomin Jung, Minsung Kim, Seunghyun Yoon |  |
| 511 |  |  [On the Analysis and Distillation of Emergent Outlier Properties in Pre-trained Language Models](https://doi.org/10.18653/v1/2025.naacl-long.430) |  | 0 | A small subset of dimensions within language Transformers’ representation spaces emerge as “outliers” during pretraining, encoding critical knowledge sparsely. We extend previous findings on emergent outliers to Encoder-Decoder Transformers and instruction-finetuned models, and tackle the problem of distilling a student Transformer from a larger teacher Transformer. Knowledge distillation reduces model size and cost by transferring knowledge from a larger teacher to a smaller student, necessitating a trade-off among representation dimensions. We show that emergent outlier dimensions contribute significantly more to zero-shot performance than non-outlier dimensions. Based on this, we... | Kunwar Yashraj Singh, Li Erran Li, Peng Tang, Srikar Appalaraju, Tianyang Zhao, Ying Nian Wu |  |
| 512 |  |  [Open-World Evaluation for Retrieving Diverse Perspectives](https://doi.org/10.18653/v1/2025.naacl-long.431) |  | 0 | We study retrieving a set of documents that covers various perspectives on a complex and contentious question (e.g., will ChatGPT do more harm than good?). We curate a Benchmark for Retrieval Diversity for Subjective questions (BERDS), where each example consists of a question and diverse perspectives associated with the question, sourced from survey questions and debate websites. On this data, retrievers paired with a corpus are evaluated to surface a document set that contains diverse perspectives. Our framing diverges from most retrieval tasks in that document relevancy cannot be decided by simple string matches to references. Instead, we build a language model-based automatic evaluator... | Eunsol Choi, HungTing Chen |  |
| 513 |  |  [Analyzing the Inner Workings of Transformers in Compositional Generalization](https://doi.org/10.18653/v1/2025.naacl-long.432) |  | 0 | The compositional generalization abilities of neural models have been sought after for human-like linguistic competence.The popular method to evaluate such abilities is to assess the models’ input-output behavior.However, that does not reveal the internal mechanisms, and the underlying competence of such models in compositional generalization remains unclear.To address this problem, we explore the inner workings of a Transformer model byfinding an existing subnetwork that contributes to the generalization performance and by performing causal analyses on how the model utilizes syntactic features.We find that the model depends on syntactic features to output the correct answer, but that the... | Hitomi Yanaka, Ryoma Kumon |  |
| 514 |  |  [Substance Beats Style: Why Beginning Students Fail to Code with LLMs](https://doi.org/10.18653/v1/2025.naacl-long.433) |  | 0 | Although LLMs are increasing the productivity of professional programmers, existing work shows that beginners struggle to prompt LLMs to solve text-to-code tasks (Nguyen et al., 2024; Prather et al., 2024b; Mordechai et al., 2024). Why is this the case? This paper explores two competing hypotheses about the cause of student-LLM miscommunication: (1) students simply lack the technical vocabulary needed to write good prompts, and (2) students do not understand the extent of information that LLMs need to solve code generation tasks. We study (1) with a causal intervention experiment on technical vocabulary and (2) by analyzing graphs that abstract how students edit prompts and the different... | Arjun Guha, Carolyn Jane Anderson, Francesca Lucchetti, Molly Q. Feldman, Zixuan Wu |  |
| 515 |  |  [Reverse Thinking Makes LLMs Stronger Reasoners](https://doi.org/10.18653/v1/2025.naacl-long.434) |  | 0 | Reverse thinking plays a crucial role in human reasoning. Humans can reason not only from a problem to a solution but also in reverse, i.e., start from the solution and reason towards the problem. This often enhances overall reasoning performance as it enables consistency checks between their forward and backward thinking. To enable Large Language Models (LLMs) to perform reverse thinking, we introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data augmentation and learning objectives. In RevThink, we augment the dataset by collecting structured forward-backward reasoning from a teacher model, consisting of: (1) the original question, (2) forward reasoning, (3) backward... | ChenYu Lee, Hamid Palangi, Justin ChihYao Chen, Long T. Le, Mohit Bansal, Rujun Han, Sayna Ebrahimi, Swaroop Mishra, Tomas Pfister, Vincent Perot, Zifeng Wang |  |
| 516 |  |  [Towards Lifelong Dialogue Agents via Timeline-based Memory Management](https://doi.org/10.18653/v1/2025.naacl-long.435) |  | 0 | To achieve lifelong human-agent interaction, dialogue agents need to constantly memorize perceived information and properly retrieve it for response generation (RG). While prior studies focus on getting rid of outdated memories to improve retrieval quality, we argue that such memories provide rich, important contextual cues for RG (e.g., changes in user behaviors) in long-term conversations. We present THEANINE, a framework for LLM-based lifelong dialogue agents. THEANINE discards memory removal and manages large-scale memories by linking them based on their temporal and cause-effect relation. Enabled by this linking structure, THEANINE augments RG with memory timelines - series of... | Dongha Lee, Hyungjoo Chae, Jinyoung Yeo, Kai Tzuiunn Ong, Minju Gwak, Namyoung Kim, Seungwon Hwang, Taeyoon Kwon, Yohan Jo |  |
| 517 |  |  [StyleDistance: Stronger Content-Independent Style Embeddings with Synthetic Parallel Examples](https://doi.org/10.18653/v1/2025.naacl-long.436) |  | 0 | Style representations aim to embed texts with similar writing styles closely and texts with different styles far apart, regardless of content. However, the contrastive triplets often used for training these representations may vary in both style and content, leading to potential content leakage in the representations. We introduce StyleDistance, a novel approach to training stronger content-independent style embeddings. We use a large language model to create a synthetic dataset of near-exact paraphrases with controlled style variations, and produce positive and negative examples across 40 distinct style features for precise contrastive learning. We assess the quality of our synthetic data... | Ajay Patel, Chris CallisonBurch, Jiacheng Zhu, Justin Qiu, Kathleen McKeown, Marianna Apidianaki, Zachary Horvitz |  |
| 518 |  |  [FiNE: Filtering and Improving Noisy Data Elaborately with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.437) |  | 0 | Data is the lifeblood of large language models (LLMs). While the quantity of open-source data available for training LLMs is substantial, its integrity often falls short. For instance, the open-source chat version of Yi-1.5-9B scores 5.20 on AlignBench, while the Chinese Alpaca-GPT4 version scores 4.12. This discrepancy makes it challenging for developers to create models that excel in downstream tasks and instruction following. Therefore, it is essential to improve data integrity. Currently, there are two mainstream methods for enhancing data integrity: data filtering and data augmentation. Due to the labor-intensive and time-consuming nature of performing these tasks manually, some of... | Junliang He, Kai Song, Li Xiaoqing, Shaohui Kuang, Xipeng Qiu, Yaqian Zhou, Ziyue Fan |  |
| 519 |  |  [CAMIEval: Enhancing NLG Evaluation through Multidimensional Comparative Instruction-Following Analysis](https://doi.org/10.18653/v1/2025.naacl-long.438) |  | 0 | With the rapid development of large language models (LLMs), due to their strong performance across various fields, LLM-based evaluation methods (LLM-as-a-Judge) have become widely used in natural language generation (NLG) evaluation. However, these methods encounter the following challenges: (1) distinguishing instruction-following ability, (2) being applicable across diverse NLG tasks, and (3) identifying low-quality outputs. To address these issues, we propose CAMIEval, a multidimensional comparative evaluation method based on instruction-following. Specifically, we define three fundamental dimensions of instruction-following: relevance, factuality, and adherence. Subsequently, we... | Junliang He, Kai Song, Li Xiaoqing, Shaohui Kuang, Xipeng Qiu, Yaqian Zhou, Ziyue Fan |  |
| 520 |  |  [LongLeader: A Comprehensive Leaderboard for Large Language Models in Long-context Scenarios](https://doi.org/10.18653/v1/2025.naacl-long.439) |  | 0 | Large Language Models (LLMs), exemplified by Claude and LLama, have exhibited impressive proficiency in tackling a myriad of Natural Language Processing (NLP) tasks. Yet, in pursuit of the ambitious goal of attaining Artificial General Intelligence (AGI), there remains ample room for enhancing LLM capabilities. Chief among these is the pressing need to bolster long-context comprehension. Numerous real-world scenarios demand LLMs to adeptly reason across extended contexts, such as multi-turn dialogues or agent workflow. Hence, recent advancements have been dedicated to stretching the upper bounds of long-context comprehension, with models like Claude 3 accommodating up to 200k tokens,... | Bing Yin, ChengChe Lee, Han Li, Hongye Jin, Huasheng Li, Jingfeng Yang, Kaiwen Men, Lingyun Wang, Mingyu Zhao, Ning Xie, Pei Chen, Qin Lu, Rulin Shao, Zhaoyu Zhang |  |
| 521 |  |  [Language Models Can Infer Action Semantics for Symbolic Planners from Environment Feedback](https://doi.org/10.18653/v1/2025.naacl-long.440) |  | 0 | Symbolic planners can discover a sequence of actions from initial to goal states given expert-defined, domain-specific logical action semantics. Large Language Models (LLMs) can directly generate such sequences, but limitations in reasoning and state-tracking often result in plans that are insufficient or unexecutable. We propose Predicting Semantics of Actions with Language Models (PSALM), which automatically learns action semantics by leveraging the strengths of both symbolic planners and LLMs. PSALM repeatedly proposes and executes plans, using the LLM to partially generate plans and to infer domain-specific action semantics based on execution outcomes. PSALM maintains a belief over... | Ishika Singh, Jesse Thomason, Robin Jia, Wang Bill Zhu |  |
| 522 |  |  [SLM-Mod: Small Language Models Surpass LLMs at Content Moderation](https://doi.org/10.18653/v1/2025.naacl-long.441) |  | 0 | Large language models (LLMs) have shown promise in many natural language understanding tasks, including content moderation. However, these models can be expensive to query in real-time and do not allow for a community-specific approach to content moderation. To address these challenges, we explore the use of open-source small language models (SLMs) for community-specific content moderation tasks. We fine-tune and evaluate SLMs (less than 15B parameters) by comparing their performance against much larger open- and closed-sourced models in both a zero-shot and few-shot setting. Using 150K comments from 15 popular Reddit communities, we find that SLMs outperform zero-shot LLMs at content... | Agam Goyal, Eshwar Chandrasekharan, Koustuv Saha, Xianyang Zhan, Yilun Chen |  |
| 523 |  |  [On Positional Bias of Faithfulness for Long-form Summarization](https://doi.org/10.18653/v1/2025.naacl-long.442) |  | 0 | Large Language Models (LLMs) often exhibit positional bias in long-context settings, under-attending to information in the middle of inputs. We investigate the presence of this bias in long-form summarization, its impact on faithfulness, and various techniques to mitigate this bias. To consistently evaluate faithfulness, we first compile a benchmark of eight human-annotated long-form summarization datasets and perform a meta-evaluation of faithfulness metrics. We show that LLM-based faithfulness metrics, though effective with full-context inputs, remain sensitive to document order, indicating positional bias. Analyzing LLM-generated summaries across six datasets, we find a “U-shaped” trend... | David Wan, Jesse Vig, Mohit Bansal, Shafiq Joty |  |
| 524 |  |  [BPO: Towards Balanced Preference Optimization between Knowledge Breadth and Depth in Alignment](https://doi.org/10.18653/v1/2025.naacl-long.443) |  | 0 | Reinforcement Learning with Human Feedback (RLHF) is the key to the success of large language models (LLMs) in recent years. In this work, we first introduce the concepts of knowledge breadth and knowledge depth, which measure the comprehensiveness and depth of an LLM or knowledge source respectively. We reveal that the imbalance in the number of prompts and responses can lead to a potential disparity in breadth and depth learning within alignment tuning datasets by showing that even a simple uniform method for balancing the number of instructions and responses can lead to significant improvements. Building on this, we further propose Balanced Preference Optimization (BPO), designed to... | Dawei Li, Hengyuan Zhang, Sizhe Wang, Tianlong Chen, Xin Zhang, Yongqi Tong |  |
| 525 |  |  [UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.444) |  | 0 | Mitigating the retention of sensitive or private information in large language models is essential for enhancing privacy and safety. Existing unlearning methods, like Gradient Ascent and Negative Preference Optimization, directly tune models to remove unwanted information. However, these methods often become unstable because they fine-tune by maximizing loss, which is the opposite of traditional loss minimization in learning. This reversal creates instability, especially on larger datasets, as the model struggles to balance unlearning with maintaining language capacity, leading to over-unlearning. In this paper, we introduce UnDIAL (Unlearning via Self-Distillation on Adjusted Logits), a... | Hongzhou Lin, Ivan Vulic, Mikhail Belkin, Ramón Huerta, Yijiang River Dong |  |
| 526 |  |  [H-STAR: LLM-driven Hybrid SQL-Text Adaptive Reasoning on Tables](https://doi.org/10.18653/v1/2025.naacl-long.445) |  | 0 | Tabular reasoning involves interpreting natural language queries about tabular data, which presents a unique challenge of combining language understanding with structured data analysis. Existing methods employ either textual reasoning, which excels in semantic interpretation but struggles with mathematical operations, or symbolic reasoning, which handles computations well but lacks semantic understanding. This paper introduces a novel algorithm H-STAR that integrates both symbolic and semantic (textual) approaches in a two-stage process to address these limitations. H-STAR employs: (1) step-wise table extraction using ‘multi-view’ column retrieval followed by row extraction, and (2)... | Chandan K. Reddy, Dan Roth, Nikhil Abhyankar, Vivek Gupta |  |
| 527 |  |  [Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations](https://doi.org/10.18653/v1/2025.naacl-long.446) |  | 0 | The growing popularity of large language models has raised concerns regarding the potential to misuse AI-generated text (AIGT). It becomes increasingly critical to establish an excellent AIGT detection method with high generalization and robustness.While, existing methods either focus on model generalization or concentrate on robustness.The unified mechanism, to simultaneously address the challenges of generalization and robustness, is less explored. In this paper, we first empirically reveal an intrinsic mechanism for model generalization and robustness of AIGT detection task.Then, we proposed a novel AIGT detection method (DP-Net) via dynamic perturbations introduced by a reinforcement... | Juan Wen, Wanli Peng, Yiming Xue, Yinghan Zhou, Zhengxian Wu, Ziwei Zhang |  |
| 528 |  |  [Vision-Language Models Can Self-Improve Reasoning via Reflection](https://doi.org/10.18653/v1/2025.naacl-long.447) |  | 0 | Chain-of-thought (CoT) has proven to improve the reasoning capability of large language models (LLMs). However, due to the complexity of multimodal scenarios and the difficulty in collecting high-quality CoT data, CoT reasoning in multimodal LLMs has been largely overlooked. To this end, we propose a simple yet effective self-training framework, R3V, which iteratively enhances the model’s Vision-language Reasoning by Reflecting on CoT Rationales. Our framework consists of two interleaved parts: (1) iteratively bootstrapping positive and negative solutions for reasoning datasets, and (2) reflection on rationale for learning from mistakes. Specifically, we introduce the self-refine and... | Fangzhi Xu, Hao Zhou, Jianbing Zhang, Kanzhi Cheng, Yang Liu, Yantao Li |  |
| 529 |  |  [Emergence of Episodic Memory in Transformers: Characterizing Changes in Temporal Structure of Attention Scores During Training](https://doi.org/10.18653/v1/2025.naacl-long.448) |  | 0 | We investigate in-context temporal biases in attention heads and transformer outputs. Using cognitive science methodologies, we analyze attention scores and outputs of the GPT-2 models of varying sizes. Across attention heads, we observe effects characteristic of human episodic memory, including temporal contiguity, primacy and recency. Transformer outputs demonstrate a tendency toward in-context serial recall. Importantly, this effect is eliminated after the ablation of the induction heads, which are the driving force behind the contiguity effect. Our findings offer insights into how transformers organize information temporally during in-context learning, shedding light on their... | Anooshka Bajaj, Deven Mahesh Mistry, Sahaj Singh Maini, Yash Aggarwal, Zoran Tiganj |  |
| 530 |  |  [Knowledge Graph-Guided Retrieval Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-long.449) |  | 0 | Retrieval-augmented generation (RAG) has emerged as a promising technology for addressing hallucination issues in the responses generated by large language models (LLMs). Existing studies on RAG primarily focus on applying semantic-based approaches to retrieve isolated relevant chunks, which ignore their intrinsic relationships. In this paper, we propose a novel Knowledge Graph-Guided Retrieval Augmented Generation (KG2RAG) framework that utilizes knowledge graphs (KGs) to provide fact-level relationships between chunks, improving the diversity and coherence of the retrieved results. Specifically, after performing a semantic-based retrieval to provide seed chunks, KG2RAG employs a... | Wei Hu, Xiangrong Zhu, Yaliang Li, Yi Liu, Yuexiang Xie |  |
| 531 |  |  [Amphista: Bi-directional Multi-head Decoding for Accelerating LLM Inference](https://doi.org/10.18653/v1/2025.naacl-long.450) |  | 0 | Large Language Models (LLMs) inherently use autoregressive decoding, which lacks parallelism in inference and results in significantly slow inference speed. While methods such as Medusa constructs parallelized heads, they lack adequate information interaction across different prediction positions. To overcome this limitation, we introduce Amphista, an enhanced speculative decoding framework that builds upon Medusa. Specifically, Amphista models an \*Auto-embedding Block\* capable of parallel inference, incorporating bi-directional attention to enable interaction between different drafting heads. Additionally, Amphista integrates \*Staged Adaptation Layers\*, which ensure a seamless... | Dong Li, Emad Barsoum, Guanchen Li, Ji Liu, Jinzhang Peng, Lu Tian, Xinlong Yang, Zeping Li, Zhuang Liu, Ziheng Gao |  |
| 532 |  |  [CAVE: Controllable Authorship Verification Explanations](https://doi.org/10.18653/v1/2025.naacl-long.451) |  | 0 | Authorship Verification (AV) (do two documents have the same author?) is essential in many real-life applications. AV is often used in privacy-sensitive domains that require an offline proprietary model that is deployed on premises, making publicly served online models (APIs) a suboptimal choice. Current offline AV models however have lower downstream utility due to limited accuracy (eg: traditional stylometry AV systems) and lack of accessible post-hoc explanations. In this work, we address the above challenges by developing a trained, offline model CAVE (Controllable Authorship Verification Explanations). CAVE generates free-text AV explanations that are controlled to be (1) accessible... | Elizabeth Boschee, Kartik Pandey, Sahana Ramnath, Xiang Ren |  |
| 533 |  |  [Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.452) |  | 0 | In line with the principle of honesty, there has been a growing effort to train large language models (LLMs) to generate outputs containing epistemic markers. However, evaluation in the presence of epistemic markers has been largely overlooked, raising a critical question: Could the use of epistemic markers in LLM-generated outputs lead to unintended negative consequences? To address this, we present EMBER, a benchmark designed to assess the robustness of LLM-judges to epistemic markers in both single and pairwise evaluation settings. Our findings, based on evaluations using \*\*EMBER\*\*, reveal that all tested LLM-judges, including GPT-4o, show a notable lack of robustness in the... | Dongryeol Lee, Joonsuk Park, Kyomin Jung, Yerin Hwang, Yongil Kim |  |
| 534 |  |  [Dynamic Uncertainty Ranking: Enhancing Retrieval-Augmented In-Context Learning for Long-Tail Knowledge in LLMs](https://doi.org/10.18653/v1/2025.naacl-long.453) |  | 0 | Large language models (LLMs) can learn vast amounts of knowledge from diverse domains during pre-training. However, long-tail knowledge from specialized domains is often scarce and underrepresented, rarely appearing in the models’ memorization. Prior work has shown that in-context learning (ICL) with retriever augmentation can help LLMs better capture long-tail knowledge, reducing their reliance on pre-trained data. Despite these advances, we observe that LLM predictions for long-tail questions remain uncertain to variations in retrieved samples. To take advantage of the uncertainty in ICL for guiding LLM predictions toward correct answers on long-tail samples, we propose a reinforcement... | Cao Xiao, Jiayu Zhou, Parminder Bhatia, Runxue Bao, Shuyang Yu, Taha A. KassHout |  |
| 535 |  |  [Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training](https://doi.org/10.18653/v1/2025.naacl-long.454) |  | 0 | Training large language models (LLMs) heavily relies on distributed training strategies, among which pipeline parallelism (PP) plays a crucial role. As training sequences extend to 32k or even 128k tokens, current PP methods face severe bottlenecks, including substantial pipeline bubbles and high memory footprint, greatly hindering training throughput and model scalability. This paper introduces a sequence-level one-forward-one-backward (1F1B) PP method, named Seq1F1B, tailored for training LLMs on long sequences with high training throughput and memory efficiency. Unlike typical PP methods, which adopt batch-level pipeline schedule, Seq1F1B schedules the pipeline of training LLMs at the... | Cheng Yang, Chuan Shi, Maosong Sun, Sun Ao, Weilin Zhao, Xinrong Zhang, Xu Han, Zhiyuan Liu |  |
| 536 |  |  [Differentially Private Learning Needs Better Model Initialization and Self-Distillation](https://doi.org/10.18653/v1/2025.naacl-long.455) |  | 0 | Differentially private SGD (DPSGD) enables privacy-preserving training of language models, but often reduces utility, diversity, and linguistic quality. We introduce DPRefine, a three-phase method that initializes a model using data synthesis from a small pre-trained LM with rigorous filtering, applies DP finetuning on private data, and performs self-distillation to refine outputs. This approach significantly outperforms vanilla DPSGD, with AlpacaEval preferring DPRefine’s generations in 78.38% of cases across all datasets and metrics, while also demonstrating substantial improvements in lexical diversity, achieving 85.31% in MSTTR and 86.82% in Jaccard similarity. Our fine-grained... | Ivoline C. Ngong, Joseph P. Near, Niloofar Mireshghallah |  |
| 537 |  |  [Is a Peeled Apple Still Red? Evaluating LLMs' Ability for Conceptual Combination with Property Type](https://doi.org/10.18653/v1/2025.naacl-long.456) |  | 0 | Conceptual combination is a cognitive process that merges basic concepts, enabling the creation of complex expressions. During this process, the properties of combination (e.g., the whiteness of a peeled apple) can be inherited from basic concepts, newly emerge, or be canceled. However, previous studies have evaluated a limited set of properties and have not examined the generative process.To address this gap, we introduce the Conceptual Combination with Property Type dataset (CCPT), which consists of 12.3K annotated triplets of noun phrases, properties, and property types. Using CCPT, we establish three types of tasks to evaluate LLMs for conceptual combination thoroughly.Our key findings... | Gunhee Kim, Jae Hyuk Sung, Jaewoo Ahn, Seokwon Song, Taehyun Lee |  |
| 538 |  |  [CRScore: Grounding Automated Evaluation of Code Review Comments in Code Claims and Smells](https://doi.org/10.18653/v1/2025.naacl-long.457) |  | 0 | The task of automated code review has recently gained a lot of attention from the machine learning community. However, current review comment evaluation metrics rely on comparisons with a human-written reference for a given code change (also called a diff ). Furthermore, code review is a one-to-many problem, like generation and summarization, with many “valid reviews” for a diff. Thus, we develop CRScore — a reference-free metric to measure dimensions of review quality like conciseness, comprehensiveness, and relevance. We design CRScore to evaluate reviews in a way that is grounded in claims and potential issues detected in the code by LLMs and static analyzers. We demonstrate that... | Atharva Naik, Carolyn P. Rosé, Daniel Fried, Marcus Alenius |  |
| 539 |  |  [KS-Lottery: Finding Certified Lottery Tickets for Multilingual Transfer in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.458) |  | 0 | The lottery ticket hypothesis posits the existence of “winning tickets” within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning. Comparing... | Chang Ma, Fei Yuan, Lei Li, Qiushi Sun, Shuai Yuan |  |
| 540 |  |  [PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization](https://doi.org/10.18653/v1/2025.naacl-long.459) |  | 0 | The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while... | Dawei Yin, Hao Sun, Hengyi Cai, Jiayi Wu, Lingyong Yan, Ming Gao, Shuaiqiang Wang, Xiang Li |  |
| 541 |  |  [B⁴: A Black-Box Scrubbing Attack on LLM Watermarks](https://doi.org/10.18653/v1/2025.naacl-long.460) |  | 0 | Watermarking has emerged as a prominent technique for LLM-generated content detection by embedding imperceptible patterns. Despite supreme performance, its robustness against adversarial attacks remains underexplored. Previous work typically considers a grey-box attack setting, where the specific type of watermark is already known. Some even necessitates knowledge about hyperparameters of the watermarking method. Such prerequisites are unattainable in real-world scenarios. Targeting at a more realistic black-box threat model with fewer assumptions, we here propose B4, a black-box scrubbing attack on watermarks. Specifically, we formulate the watermark scrubbing attack as a constrained... | Baizhou Huang, Xiao Pu, Xiaojun Wan |  |
| 542 |  |  [IMRRF: Integrating Multi-Source Retrieval and Redundancy Filtering for LLM-based Fake News Detection](https://doi.org/10.18653/v1/2025.naacl-long.461) |  | 0 | The widespread use of social networks has significantly accelerated the dissemination of information but has also facilitated the rapid spread of fake news, leading to various negative consequences. Recently, with the emergence of large language models (LLMs), researchers have focused on leveraging LLMs for automated fake news detection. Unfortunately, many issues remain to be addressed. First, the evidence retrieved to verify given fake news is often insufficient, limiting the performance of LLMs when reasoning directly from this evidence. Additionally, the retrieved evidence frequently contains substantial redundant information, which can interfere with the LLMs’ judgment. To address... | Bingbing Song, Dayang Li, Fanxiao Li, Li Tang, Wei Zhou |  |
| 543 |  |  [Matina: A Large-Scale 73B Token Persian Text Corpus](https://doi.org/10.18653/v1/2025.naacl-long.462) |  | 0 | Text corpora are essential for training models used in tasks like summarization, translation, and large language models (LLMs). While various efforts have been made to collect monolingual and multilingual datasets in many languages, Persian has often been underrepresented due to limited resources for data collection and preprocessing. Existing Persian datasets are typically small and lack content diversity, consisting mainly of weblogs and news articles. This shortage of high-quality, varied data has slowed the development of NLP models and open-source LLMs for Persian. Since model performance depends heavily on the quality of training data, we address this gap by introducing the Matina... | Fatemeh Nadi, Fatemeh Taherinezhad, Hamed Baghbani, Heshaam Faili, Mostafa Amiri, Sara Bourbour Hosseinbeigi |  |
| 544 |  |  [SMAB: MAB based word Sensitivity Estimation Framework and its Applications in Adversarial Text Generation](https://doi.org/10.18653/v1/2025.naacl-long.463) |  | 0 | To understand the complexity of sequence classification tasks, Hahn et al. (2021) proposed sensitivity as the number of disjoint subsets of the input sequence that can each be individually changed to change the output. Though effective, calculating sensitivity at scale using this framework is costly because of exponential time complexity. Therefore, we introduce a Sensitivity-based Multi-Armed Bandit framework (SMAB), which provides a scalable approach for calculating word-level local (sentence-level) and global (aggregated) sensitivities concerning an underlying text classifier for any dataset. We establish the effectiveness of our approach through various applications. We perform a case... | Debrup Das, Monojit Choudhury, Sachin Vashistha, Saurabh Kumar Pandey, Somak Aditya |  |
| 545 |  |  [ManaTTS Persian: a recipe for creating TTS datasets for lower resource languages](https://doi.org/10.18653/v1/2025.naacl-long.464) |  | 0 | In this study, we introduce ManaTTS, the most extensive publicly accessible single-speaker Persian corpus, and a comprehensive framework for collecting transcribed speech datasets for the Persian language. ManaTTS, released under the open CC-0 license, comprises approximately 86 hours of audio with a sampling rate of 44.1 kHz. The dataset is supported by a fully transparent, MIT-licensed pipeline, a testament to innovation in the field. It includes unique tools for sentence tokenization, bounded audio segmentation, and a novel forced alignment method. This alignment technique is specifically designed for low-resource languages, addressing a crucial need in the field. With this dataset, we... | Hamid R. Rabiee, Mahta Fetrat Qharabagh, Zahra Dehghanian |  |
| 546 |  |  [CultureInstruct: Curating Multi-Cultural Instructions at Scale](https://doi.org/10.18653/v1/2025.naacl-long.465) |  | 0 | Large language models, despite their remarkable success in recent years, still exhibit severe cultural bias. Therefore, in this paper, we introduce CultureInstruct, a large-scale instruction-tuning dataset designed to reduce cultural bias in LLMs. CultureInstruct is constructed with an automatic pipeline, utilizing public web sources and a specialized LLM to generate instruction. Our data comprises 430K instructions, ranging from classic NLP tasks to complex reasoning. CultureInstruct also covers 11 most relevant topics to cultural knowledge, making it highly diverse. Our experiments show that fine-tuning LLMs with CultureInstruct results in consistent improvements across three types of... | Gholamreza Haffari, Lizhen Qu, Viet Thanh Pham, Zhuang Li |  |
| 547 |  |  [Lost in Inference: Rediscovering the Role of Natural Language Inference for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.466) |  | 0 | In the recent past, a popular way of evaluating natural language understanding (NLU), was to consider a model’s ability to perform natural language inference (NLI) tasks. In this paper, we investigate if NLI tasks, that are rarely used for LLM evaluation, can still be informative for evaluating LLMs. Focusing on five different NLI benchmarks across six models of different scales, we investigate if they are able to discriminate models of different size and quality and how their accuracies develop during training. Furthermore, we investigate the extent to which the softmax distributions of models align with human distributions in cases where statements are ambiguous or vague. Overall, our... | Barbara Plank, David Esiobu, Dieuwke Hupkes, Lovish Madaan, Pontus Stenetorp |  |
| 548 |  |  [DenseSSM: State Space Models with Dense Hidden Connection for Efficient Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.467) |  | 0 | Large language models (LLMs) face a significant challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. This incremental improvement maintains the training parallelizability and inference... | Chengcheng Wang, Kai Han, Tianyu Guo, Wei He, Yehui Tang, Yujie Yang, Yunhe Wang |  |
| 549 |  |  [A Mixed-Language Multi-Document News Summarization Dataset and a Graphs-Based Extract-Generate Model](https://doi.org/10.18653/v1/2025.naacl-long.468) |  | 0 | Existing research on news summarization primarily focuses on single-language single-document (SLSD), single-language multi-document (SLMD) or cross-language single-document (CLSD). However, in real-world scenarios, news about an international event often involves multiple documents in different languages, i.e., mixed-language multi-document (MLMD). Therefore, summarizing MLMD news is of great significance. However, the lack of datasets for MLMD news summarization has constrained the development of research in this area. To fill this gap, we construct a mixed-language multi-document news summarization dataset (MLMD-news), which contains four different languages and 10,992 source document... | Fang Nan, Kaiwen Tan, Shengxiang Gao, Yongbing Zhang, Yuxin Huang, Zhengtao Yu |  |
| 550 |  |  [Measuring memorization in language models via probabilistic extraction](https://doi.org/10.18653/v1/2025.naacl-long.469) |  | 0 | Large language models (LLMs) are susceptible to memorizing training data, raising concerns about the potential extraction of sensitive information at generation time. Discoverable extraction is the most common method for measuring this issue: split a training example into a prefix and suffix, then prompt the LLM with the prefix, and deem the example extractable if the LLM generates the matching suffix using greedy sampling. This definition yields a yes-or-no determination of whether extraction was successful with respect to a single query. Though efficient to compute, we show that this definition is unreliable because it does not account for non-determinism present in more realistic... | A. Feder Cooper, Christopher A. ChoquetteChoo, Harsh Chaudhari, Ilia Shumailov, Itay Yona, Jamie Hayes, Katherine Lee, Marika Swanberg, Milad Nasr |  |
| 551 |  |  [Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models](https://doi.org/10.18653/v1/2025.naacl-long.470) |  | 0 | Large Multimodal Models (LMMs) have demonstrated the ability to interact with humans under real-world conditions by combining Large Language Models (LLMs) and modality encoders to align multimodal information (visual and auditory) with text. However, such models raise new safety challenges of whether models that are safety-aligned on text also exhibit consistent safeguards for multimodal inputs. Despite recent safety-alignment research on vision LMMs, the safety of audio LMMs remains under-explored. In this work, we comprehensively red team the safety of five advanced audio LMMs under three settings: (i) harmful questions in both audio and text formats, (ii) harmful questions in text... | Ehsan Shareghi, Gholamreza Haffari, Hao Yang, Lizhen Qu |  |
| 552 |  |  [EMS-SD: Efficient Multi-sample Speculative Decoding for Accelerating Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.471) |  | 0 | Speculative decoding emerges as a pivotal technique for enhancing the inference speed of Large Language Models (LLMs). Despite recent research aiming to improve prediction efficiency, multi-sample speculative decoding has been overlooked due to varying numbers of accepted tokens within a batch in the verification phase. Vanilla method adds padding tokens in order to ensure that the number of new tokens remains consistent across samples. However, this increases the computational and memory access overhead, thereby reducing the speedup ratio. We propose a novel method that can resolve the issue of inconsistent tokens accepted by different samples without necessitating an increase in memory... | Chuanjian Liu, Kai Han, Yehui Tang, Yunhe Wang, Yunsheng Ni |  |
| 553 |  |  [Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for Language Model Alignment](https://doi.org/10.18653/v1/2025.naacl-long.472) |  | 0 | Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking when the accuracy of the reward model is not high enough. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization, which ensures that the language model remains close to the reference model. In this research, we propose MBR-BoN,... | Kaito Ariu, Kenshi Abe, Tetsuro Morimura, Yuu Jinnai |  |
| 554 |  |  [MAPWise: Evaluating Vision-Language Models for Advanced Map Queries](https://doi.org/10.18653/v1/2025.naacl-long.473) |  | 0 | Vision-language models (VLMs) excel at tasks requiring joint understanding of visual and linguistic information. A particularly promising yet under-explored application for these models lies in answering questions based on various kinds of maps. This study investigates the efficacy of VLMs in answering questions based on choropleth maps, which are widely used for data analysis and representation. To facilitate and encourage research in this area, we introduce a novel map-based question-answering benchmark, consisting of maps from three geographical regions (United States, India, China), each containing around 1000 questions. Our benchmark incorporates 43 diverse question templates,... | Abhishek Rajgaria, Dan Roth, Manish Shrivastava, Prerana Khatiwada, Srija Mukhopadhyay, Vivek Gupta |  |
| 555 |  |  [Pay More Attention to Images: Numerous Images-Oriented Multimodal Summarization](https://doi.org/10.18653/v1/2025.naacl-long.474) |  | 0 | Existing multimodal summarization approaches struggle with scenarios involving numerous images as input, leading to a heavy load for readers. Summarizing both the input text and numerous images helps readers quickly grasp the key points of multimodal input. This paper introduces a novel task, Numerous Images-Oriented Multimodal Summarization (NIMMS). To benchmark this task, we first construct the dataset based on a public multimodal summarization dataset. Considering that most existing metrics evaluate summaries from a unimodal perspective, we propose a new Multimodal Information evaluation (M-info) method, measuring the differences between the generated summary and the multimodal input.... | Chengqing Zong, Feifei Zhai, Junnan Zhu, Min Xiao, Yu Zhou |  |
| 556 |  |  [S²-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate Efficiency](https://doi.org/10.18653/v1/2025.naacl-long.475) |  | 0 | Large language models (LLMs) have demonstrated remarkable capabilities across various natural language processing (NLP) scenarios, but they still face challenges when handling complex arithmetic and logical reasoning tasks. While Chain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction strategies have attempted to guide models in sequential, multi-step reasoning, Multi-agent Debate (MAD) has emerged as a viable approach for enhancing the reasoning capabilities of LLMs. By increasing both the number of agents and the frequency of debates, the performance of LLMs improves significantly. However, this strategy results in a significant increase in token costs, presenting a... | Chen Tianying Tiana, Jing Li, Lei Jiang, Tongxuan Liu, Weizhe Huang, Xiaohua Xu, Xitai Jin, Yuting Zeng |  |
| 557 |  |  [MASTER: A Multi-Agent System with LLM Specialized MCTS](https://doi.org/10.18653/v1/2025.naacl-long.476) |  | 0 | Large Language Models (LLM) are increasingly being explored for problem-solving tasks. However, their strategic planning capability is often viewed with skepticism. Recent studies have incorporated the Monte Carlo Tree Search (MCTS) algorithm to augment the planning capacity of LLM. Despite its potential, MCTS relies on extensive sampling simulations to approximate the true reward distribution, which leads to two primary issues. Firstly, MCTS is effective for tasks like the Game of Go, where simulation results can yield objective rewards (e.g., 1 for a win and 0 for a loss). However, for tasks such as question answering, the result of a simulation is the answer to the question, which... | Bingzheng Gan, Changwang Zhang, Jing Huang, Shu Xian Teo, Tianyi Zhang, Wei Shi, Yufan Zhao, Yusu Li |  |
| 558 |  |  [ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots](https://doi.org/10.18653/v1/2025.naacl-long.477) |  | 0 | We introduce ScreenQA, a novel benchmarking dataset designed to advance screen content understanding through question answering. The existing screen datasets are focused either on low-level structural and component understanding, or on a much higher-level composite task such as navigation and task completion for autonomous agents. ScreenQA attempts to bridge this gap. By annotating 86k question-answer pairs over the RICO dataset, we aim to benchmark the screen reading comprehension capacity, thereby laying the foundation for vision-based automation over screenshots. Our annotations encompass full answers, short answer phrases, and corresponding UI contents with bounding boxes, enabling... | Fedir Zubach, Gilles Baechler, Jason Lin, Jindong Chen, Maria Wang, Srinivas Sunkara, Victor Carbune, YuChung Hsiao, Yun Zhu |  |
| 559 |  |  [Cross-Lingual and Cross-Cultural Variation in Image Descriptions](https://doi.org/10.18653/v1/2025.naacl-long.478) |  | 0 | Do speakers of different languages talk differently about what they see? Behavioural and cognitive studies report cultural effects on perception; however, these are mostly limited in scope and hard to replicate. In this work, we conduct the first large-scale empirical study of cross-lingual variation in image descriptions. Using a multimodal dataset with 31 languages and images from diverse locations, we develop a method to accurately identify entities mentioned in captions and present in the images, then measure how they vary across languages. Our analysis reveals that pairs of languages that are geographically or genetically closer tend to mention the same entities more frequently. We... | Edoardo M. Ponti, Uri Berger |  |
| 560 |  |  [Soft Syntactic Reinforcement for Neural Event Extraction](https://doi.org/10.18653/v1/2025.naacl-long.479) |  | 0 | Recent event extraction (EE) methods rely on pre-trained language models (PLMs) but still suffer from errors due to a lack of syntactic knowledge. While syntactic information is crucial for EE, there is a need for effective methods to incorporate syntactic knowledge into PLMs. To address this gap, we present a novel method to incorporate syntactic information into PLM-based models for EE, which do not require external syntactic parsers to produce syntactic features of task data. Instead, our proposed soft syntactic reinforcement (SSR) mechanism learns to select syntax-related dimensions of PLM representation during pretraining on a standard dependency corpus. The adapted PLM weights and... | Anran Hao, Jian Su, Shuo Sun, Teo Yong Sen |  |
| 561 |  |  [Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models](https://doi.org/10.18653/v1/2025.naacl-long.480) |  | 0 | Transformer-based large-scale pre-trained models achieve great success. Fine-tuning is the standard practice for leveraging these models in downstream tasks. Among the fine-tuning methods, adapter-tuning provides a parameter-efficient fine-tuning by introducing lightweight trainable modules while keeping most pre-trained parameters frozen. However, existing adapter-tuning methods still impose substantial resource usage. Through our investigation, we show that each adapter unequally contributes to both task performance and resource usage. Motivated by this insight, we propose Selective Adapter FrEezing (SAFE), which gradually freezes less important adapters early to reduce unnecessary... | Changhoon Kim, Hyegang Son, Yonglak Son, Young Geun Kim |  |
| 562 |  |  [Bridging the Gap between Expert and Language Models: Concept-guided Chess Commentary Generation and Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.481) |  | 0 | Deep learning-based expert models have reached superhuman performance in decision-making domains such as chess and Go. However, it is under-explored to explain or comment on given decisions although it is important for model explainability and human education. The outputs of expert models are accurate, but yet difficult to interpret for humans. On the other hand, large language models (LLMs) can produce fluent commentary but are prone to hallucinations due to their limited decision-making capabilities. To bridge this gap between expert models and LLMs, we focus on chess commentary as a representative task of explaining complex decision-making processes through language and address both the... | Inseok Hwang, Jaechang Kim, Jaewoong Cho, Jinmin Goh, Jungseul Ok |  |
| 563 |  |  [TCProF:Time-Complexity Prediction SSL Framework](https://doi.org/10.18653/v1/2025.naacl-long.482) |  | 0 |  | Hyeseon Ahn, Joonghyuk Hahn, Jungin Kim, Soohan Lim, YoSub Han |  |
| 564 |  |  [Culture-TRIP: Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinement](https://doi.org/10.18653/v1/2025.naacl-long.483) |  | 0 |  | Inseong Choi, Jihie Kim, Suchae Jeong, Youngsik Yun |  |
| 565 |  |  [Behavior-SD: Behaviorally Aware Spoken Dialogue Generation with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.484) |  | 0 | Spoken dialogue involves behaviors like turn-taking, interruptions, filler words, and backchannels, which make interactions more natural and engaging but are often overlooked in language models. These models struggle to explicitly model these behavioral traits, resulting in a less natural and personalized communication style that aligns with user needs. To address this challenge, we make two key contributions. First, we introduce Behavior-SD, a large-scale dataset containing over 100K spoken dialogues (2,164 hours) annotated with various conversational behaviors, synthesized via LLMs to model diverse full-duplex interactions. Second, we propose BeDLM, the first dialogue model capable of... | Gunhee Kim, Kangwook Kim, Sehun Lee |  |
| 566 |  |  [Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.485) |  | 0 | Large language models (LLMs) have demonstrated multilingual capabilities, yet they are mostly English-centric due to the imbalanced training corpora. While prior works have leveraged this bias to enhance multilingual performance through translation, they have been largely limited to natural language processing (NLP) tasks. In this work, we extend the evaluation to real-world user queries and non-English-centric LLMs, offering a broader examination of multilingual performance. Our key contribution lies in demonstrating that while translation into English can boost the performance of English-centric LLMs on NLP tasks, it is not universally optimal. For culture-related tasks that need deep... | Anh Tuan Luu, Chaoqun Liu, Lidong Bing, Wenxuan Zhang, Yiran Zhao |  |
| 567 |  |  [AlgoPuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Algorithmic Multimodal Puzzles](https://doi.org/10.18653/v1/2025.naacl-long.486) |  | 0 | This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models in solving algorithmic puzzles that necessitate both visual understanding, language understanding, and complex algorithmic reasoning. We create the puzzles to encompass a diverse array of mathematical and algorithmic topics such as boolean logic, combinatorics, graph theory, optimization, search, etc., aiming to evaluate the gap between visual data interpretation and algorithmic problem-solving skills. The dataset is generated automatically from... | Deepanway Ghosal, Soujanya Poria, Vernon Toh, Yew Ken Chia |  |
| 568 |  |  [Towards Quantifying Commonsense Reasoning with Mechanistic Insights](https://doi.org/10.18653/v1/2025.naacl-long.487) |  | 0 |  | Abhinav Joshi, Areeb Ahmad, Ashutosh Modi, Divyaksh Shukla |  |
| 569 |  |  [Beyond Logit Lens: Contextual Embeddings for Robust Hallucination Detection & Grounding in VLMs](https://doi.org/10.18653/v1/2025.naacl-long.488) |  | 0 | The rapid development of Large Multimodal Models (LMMs) has significantly advanced multimodal understanding by harnessing the language abilities of Large Language Models (LLMs) and integrating modality-specific encoders. However, LMMs are plagued by hallucinations that limit their reliability and adoption. While traditional methods to detect and mitigate these hallucinations often involve costly training or rely heavily on external models, recent approaches utilizing internal model features present a promising alternative. In this paper, we critically assess the limitations of the state-of-the-art training-free technique, the logit lens, in handling generalized visual hallucinations. We... | Anirudh Phukan, Apoorv Saxena, Divyansh, Harshit Kumar Morj, Koustava Goswami, Vaishnavi |  |
| 570 |  |  [M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.489) |  | 0 | Collecting instruction fine-tuning (IFT) data is a resource and time intensive task especially in multilingual setting where finding proficient native speakers is challenging. Moreover, traditional data collection is prone to privacy risks, toxicity and lacks scalability. While, fully synthetic datasets are a promising alternative, research on their use in multilingual domain is limited as existing approaches still rely on machine translation to improve multilingual performance. To bridge this gap we introduce M2Lingual, the first fully synthetic, multi-turn multilingual dataset having 175K conversations across 70 languages with a balanced mix of high, low and mid-resourced languages.... | Hoang Nguyen, Khyati Mahajan, Rishabh Maheshwary, Sathwik Tejaswi Madhusudhan, Vikas Yadav |  |
| 571 |  |  [Multi³Hate: Multimodal, Multilingual, and Multicultural Hate Speech Detection with Vision-Language Models](https://doi.org/10.18653/v1/2025.naacl-long.490) |  | 0 | Hate speech moderation on global platforms poses unique challenges due to the multimodal and multilingual nature of content, along with the varying cultural perceptions. How well do current vision-language models (VLMs) navigate these nuances? To investigate this, we create the first multimodal and multilingual parallel hate speech dataset, annotated by a multiculturally diverse set of annotators, called Multi3Hate. It contains 300 parallel meme samples across 5 languages: English, German, Spanish, Hindi, and Mandarin. We demonstrate that cultural background significantly affects multimodal hate speech annotation in our dataset. The average pairwise agreement among countries is just 74%,... | Anne Lauscher, Katharina von der Wense, Minh Duc Bui |  |
| 572 |  |  [Grounding Fallacies Misrepresenting Scientific Publications in Evidence](https://doi.org/10.18653/v1/2025.naacl-long.491) |  | 0 | Health-related misinformation claims often falsely cite a credible biomedical publication as evidence. These publications only superficially seem to support the false claim, when logical fallacies are applied. In this work, we aim to detect and to highlight such fallacies, which requires assessing the exact content of the misrepresented publications. To achieve this, we introduce MissciPlus, an extension of the fallacy detection dataset Missci. MissciPlus extends Missci by grounding the applied fallacies in real-world passages from misrepresented studies. This creates a realistic test-bed for detecting and verbalizing fallacies under real-world input conditions, and enables new and... | Iryna Gurevych, Max Glockner, Preslav Nakov, Yufang Hou |  |
| 573 |  |  [Has this Fact been Edited? Detecting Knowledge Edits in Language Models](https://doi.org/10.18653/v1/2025.naacl-long.492) |  | 0 | Knowledge editing methods (KEs) can update language models’ obsolete or inaccurate knowledge learned from pre-training. However, KEs can be used for malicious applications, e.g., inserting misinformation and toxic content. Knowing whether a generated output is based on edited knowledge or first-hand knowledge from pre-training can increase users’ trust in generative models and provide more transparency. Driven by this, we propose a novel task: detecting knowledge edits in language models. Given an edited model and a fact retrieved by a prompt from an edited model, the objective is to classify the knowledge as either unedited (based on the pre-training), or edited (based on subsequent... | Christin Seifert, Jörg Schlötterer, Paul Youssef, Zhixue Zhao |  |
| 574 |  |  [AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging](https://doi.org/10.18653/v1/2025.naacl-long.493) |  | 0 |  | Huiming Wang, Kenji Kawaguchi, Lidong Bing, Wenxuan Zhang, Yiran Zhao |  |
| 575 |  |  [Coverage-based Fairness in Multi-document Summarization](https://doi.org/10.18653/v1/2025.naacl-long.494) |  | 0 | Fairness in multi-document summarization (MDS) measures whether a system can generate a summary fairly representing information from documents with different social attribute values. Fairness in MDS is crucial since a fair summary can offer readers a comprehensive view. Previous works focus on quantifying summary-level fairness using Proportional Representation, a fairness measure based on Statistical Parity. However, Proportional Representation does not consider redundancy in input documents and overlooks corpus-level unfairness. In this work, we propose a new summary-level fairness measure, Equal Coverage, which is based on coverage of documents with different social attribute values and... | Haoyuan Li, Rui Zhang, Snigdha Chaturvedi, Yusen Zhang |  |
| 576 |  |  [Grammar Control in Dialogue Response Generation for Language Learning Chatbots](https://doi.org/10.18653/v1/2025.naacl-long.495) |  | 0 | Chatbots based on large language models offer cheap conversation practice opportunities for language learners. However, they are hard to control for linguistic forms that correspond to learners’ current needs, such as grammar. We control grammar in chatbot conversation practice by grounding a dialogue response generation model in a pedagogical repository of grammar skills. We also explore how this control helps learners to produce specific grammar. We comprehensively evaluate prompting, fine-tuning, and decoding strategies for grammar-controlled dialogue response generation. Strategically decoding Llama3 outperforms GPT-3.5 when tolerating minor response quality losses. Our simulation... | Detmar Meurers, Dominik Glandorf, Mrinmaya Sachan, Peng Cui |  |
| 577 |  |  [Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge](https://doi.org/10.18653/v1/2025.naacl-long.496) |  | 0 | Recent studies have highlighted the presence of cultural biases in Large Language Models (LLMs), yet often lack a robust methodology to dissect these phenomena comprehensively. Our work aims to bridge this gap by delving into the Food domain—a universally relevant yet culturally diverse aspect of human life. We introduce FmLAMA, a multilingual dataset centered on food-related cultural facts and variations in food practices. We analyze LLMs across various architectures and configurations, evaluating their performance in both monolingual and multilingual settings. By leveraging templates in six different languages, we investigate how LLMs interact with language-specific and cultural... | Daniel Hershcovich, Haizhou Li, Li Zhou, Nicolas Garneau, Taelin Karidi, Wanlong Liu, Wenyu Chen, Yong Cao |  |
| 578 |  |  [Palette of Language Models: A Solver for Controlled Text Generation](https://doi.org/10.18653/v1/2025.naacl-long.497) |  | 0 | Recent advancements in large language models have revolutionized text generation with their remarkable capabilities. These models can produce controlled texts that closely adhere to specific requirements when prompted appropriately. However, designing an optimal prompt to control multiple attributes simultaneously can be challenging. A common approach is to linearly combine single-attribute models, but this strategy often overlooks attribute overlaps and can lead to conflicts. Therefore, we propose a novel combination strategy inspired by the Law of Total Probability and Conditional Mutual Information Minimization on generative language models. This method has been adapted for... | Chao Deng, Junlan Feng, XiaotingWu XiaotingWu, Yaqin Chen, Yi Huang, Zhe Yang |  |
| 579 |  |  [MAMM-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration](https://doi.org/10.18653/v1/2025.naacl-long.498) |  | 0 | Multi-agent collaboration among models has shown promise in reasoning tasks but is underexplored in long-form generation tasks like summarization and question-answering. We extend multi-agent multi-model reasoning to generation, specifically to improving faithfulness through refinement, i.e., revising model-generated outputs to remove factual inconsistencies. We investigate how iterative collaboration among multiple instances and types of large language models (LLMs) enhances subtasks in the refinement process, such as error detection, critiquing unfaithful sentences, and making corrections based on critiques. We design intrinsic evaluations for each subtask, with our findings indicating... | David Wan, Elias StengelEskin, Justin ChihYao Chen, Mohit Bansal |  |
| 580 |  |  [MADial-Bench: Towards Real-world Evaluation of Memory-Augmented Dialogue Generation](https://doi.org/10.18653/v1/2025.naacl-long.499) |  | 0 | Long-term memory is important for chatbots and dialogue systems (DS) to create consistent and human-like conversations, evidenced by numerous developed memory-augmented DS (MADS). To evaluate the effectiveness of such MADS, existing commonly used evaluation metrics, like retrieval accuracy and perplexity (PPL), mainly focus on query-oriented factualness and language quality assessment. However, these metrics often lack practical value. Moreover, the evaluation dimensions are insufficient for human-like assessment in DS. Regarding memory-recalling paradigms, current evaluation schemes only consider passive memory retrieval while ignoring diverse memory recall with rich triggering factors,... | Gholamreza Haffari, Jiaxing Zhang, Junqing He, Liang Zhu, Rui Wang, Xi Wang |  |
| 581 |  |  [Assessing the State of the Art in Scene Segmentation](https://doi.org/10.18653/v1/2025.naacl-long.500) |  | 0 | The detection of scenes in literary texts is a recently introduced segmentation task in computational literary studies. Its goal is to partition a fictional text into segments that are coherent across the dimensions time, space, action and character constellation. This task is very challenging for automatic methods, since it requires a high-level understanding of the text. In this paper, we provide a thorough analysis of the State of the Art and challenges in this task, identifying and solving a problem in the training procedure for previous approaches, analysing the generalisation capabilities of the models and comparing the BERT-based SotA to current Llama models, as well as providing an... | Albin Zehe, Andreas Hotho, Elisabeth Fischer |  |
| 582 |  |  [DCE-LLM: Dead Code Elimination with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.501) |  | 0 | Dead code introduces several challenges in software development, such as increased binary size and maintenance difficulties. It can also obscure logical errors and be exploited for obfuscation in malware. For LLM-based code-related tasks, dead code introduces vulnerabilities that can mislead these models, raising security concerns. Although modern compilers and IDEs offer dead code elimination, sophisticated patterns can bypass these tools. A universal approach that includes classification, location, explanation, and correction is needed, yet current tools often require significant manual effort. We present DCE-LLM, a framework for automated dead code elimination using a small CodeBERT... | Guoqiang Li, LingI Wu, Minyu Chen, Ruibang Liu |  |
| 583 |  |  [Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction](https://doi.org/10.18653/v1/2025.naacl-long.502) |  | 0 | Self-reflection for Large LanguageModels (LLMs) has gained significant attention. Existing approaches involve models iterating and improving their previous responses based on LLMs’ internal reflection ability or external feedback. However, recent research has raised doubts about whether intrinsic self-correction without external feedback may even degrade performance. Based on our empirical evidence, we find that current static reflection methods may lead to redundant, drift, and stubborn issues. To mitigate this, we introduce \*\*I\*\*nstruct-\*\*o\*\*f-\*\*R\*\*eflec\*\*t\*\*ion (\*\*IoRT\*\*), a novel and general reflection framework that leverages dynamic-meta instruction to enhance the... | Chuang Zhao, Chunhong Zhang, Jianping Fan, Likang Wu, Liping Liu, Ming He, Zheng Hu |  |
| 584 |  |  [Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment](https://doi.org/10.18653/v1/2025.naacl-long.503) |  | 0 | A binary decision task, like yes-no questions or answer verification, reflects a significant real-world scenario such as where users look for confirmation about the correctness of their decisions on specific issues. In this work, we observe that language models exhibit a negative bias in the binary decisions of complex reasoning tasks. Based on our observations and the rationale about attention-based model dynamics, we propose a negative attention score (NAS) to systematically and quantitatively formulate negative bias. Based on NAS, we identify attention heads that attend to negative tokens provided in the instructions as answer candidate of binary decisions, regardless of the question in... | Bongkyu Hwang, Hoyoung Kang, Jongyoon Song, Junhwa Choi, Sangwon Yu, Seongho Joe, Sooah Cho, Sungroh Yoon, Taehee Lee, Youngjune Gwon |  |
| 585 |  |  [MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps](https://doi.org/10.18653/v1/2025.naacl-long.504) |  | 0 | \*\*Multimodal Chain of Thought (MCoT)\*\* is a popular prompting strategy for improving the performance of multimodal large language models (MLLMs) across a range of complex reasoning tasks. Despite its popularity, there is a notable absence of automated methods for evaluating the quality of reasoning steps in MCoT. To address this gap, we propose \*\*Multimodal Chain-of-Thought Evaluation (MiCEval)\*\*, a framework designed to assess the correctness of reasoning chains by evaluating the quality of both the description and each reasoning step. The evaluation of the description component focuses on the accuracy of the image descriptions, while the reasoning step evaluates the quality of... | Hanjie Chen, Haojing Chen, Jeff Z. Pan, Jie He, Jingyu Li, Lanyu Chen, Víctor GutiérrezBasulto, Xiongtao Zhou |  |
| 586 |  |  [CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian Product Routing in Mixture-of-Experts](https://doi.org/10.18653/v1/2025.naacl-long.505) |  | 0 | Large language models (LLM) have been attracting much attention from the community recently, due to their remarkable performance in all kinds of downstream tasks. According to the well-known scaling law, scaling up a dense LLM enhances its capabilities, but also significantly increases the computational complexity. Mixture-of-Experts (MoE) models address that by allowing the model size to grow without substantially raising training or inference costs. Yet MoE models face challenges regarding knowledge sharing among experts, making their performance somehow sensitive to routing accuracy. To tackle that, previous works introduced shared experts and combined their outputs with those of the... | Guangyuan Ma, Guiguang Ding, Hui Chen, Minxuan Lv, Songlin Hu, Xing Wu, Yizhe Xiong, Zhenpeng Su, Zijia Lin |  |
| 587 |  |  [Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language](https://doi.org/10.18653/v1/2025.naacl-long.506) |  | 0 | We are exposed to much information trying to influence us, such as teaser messages, debates, politically framed news, and propaganda — all of which use persuasive language. With the recent interest in Large Language Models (LLMs), we study the ability of LLMs to produce persuasive text. As opposed to prior work which focuses on particular domains or types of persuasion, we conduct a general study across various domains to measure and benchmark to what degree LLMs produce persuasive language - both when explicitly instructed to rewrite text to be more or less persuasive and when only instructed to paraphrase. We construct the new dataset Persuasive-Pairs of pairs of a short text and its... | Amalie Brogaard Pauli, Ira Assent, Isabelle Augenstein |  |
| 588 |  |  [MILU: A Multi-task Indic Language Understanding Benchmark](https://doi.org/10.18653/v1/2025.naacl-long.507) |  | 0 | Evaluating Large Language Models (LLMs) in low-resource and linguistically diverse languages remains a significant challenge in NLP, particularly for languages using non-Latin scripts like those spoken in India. Existing benchmarks predominantly focus on English, leaving substantial gaps in assessing LLM capabilities in these languages. We introduce MILU, a Multi-task Indic Language Understanding Benchmark, a comprehensive evaluation benchmark designed to address this gap. MILU spans 8 domains and 41 subjects across 11 Indic languages, reflecting general and culturally specific knowledge. With an India-centric design, incorporates material from regional and state-level examinations,... | Jaydeep Sen, Mohammed Safi Ur Rahman Khan, Rudra Murthy, Sshubam Verma, Vishwajeet Kumar |  |
| 589 |  |  [AutoEval-ToD: Automated Evaluation of Task-oriented Dialog Systems](https://doi.org/10.18653/v1/2025.naacl-long.508) |  | 0 | Task-oriented Dialog systems (ToD) are essential in automating user interactions, but their complex design and dynamic nature make evaluation particularly challenging. Current evaluation methodologies heavily depend on human annotators, which can be inefficient, subjective, and expensive to scale. To advance the field, there is a pressing need for a reliable, scalable, and systematic evaluation framework that can provide comprehensive insights into ToD system performance. In this paper, we propose, AutoEval-TOD, an automated end-to-end evaluation framework using large language models (LLMs). Our framework first interacts with the ToD system and then assesses its performance across key... | Anoop Saladi, Arihant Jain, Chaosheng Dong, Purav Aggarwal, Rishav Sahay |  |
| 590 |  |  [Self-calibration for Language Model Quantization and Pruning](https://doi.org/10.18653/v1/2025.naacl-long.509) |  | 0 | Quantization and pruning are fundamental approaches for model compression, enabling efficient inference for language models. In a post-training setting, state-of-the-art quantization and pruning methods require calibration data, a small set of unlabeled examples. Conventionally, this is randomly sampled web text, aiming to reflect the model training data. However, this poses two key problems: (1) unrepresentative calibration examples can harm model performance, and (2) organizations increasingly avoid releasing model training data. In this paper, we propose self-calibration as a solution. Our approach requires no external data, instead leveraging the model itself to generate synthetic... | George Chrysostomou, Miles Williams, Nikolaos Aletras |  |
| 591 |  |  [Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.510) |  | 0 | Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can improve the reasoning ability of LLMs to some extent, they suffer from an unfaithful issue where derived conclusions may not align with the generated reasoning chain. To address this issue, some studies employ the approach of propositional logic to further enhance logical reasoning abilities of LLMs. However, the potential omissions in the extraction of logical expressions in these methods can cause information loss in the logical reasoning process, thereby... | Hailong Yang, Jiaxing Wang, Jing Li, Tongxuan Liu, Weizhe Huang, Wenjiang Xu, Xingyu Wang, Yuting Zeng |  |
| 592 |  |  [IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval](https://doi.org/10.18653/v1/2025.naacl-long.511) |  | 0 | We introduce IFIR, the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature. Each subset addresses one or more domain-specific retrieval tasks, replicating real-world scenarios where customized instructions are critical. IFIR enables a detailed analysis of instruction-following retrieval capabilities by incorporating instructions at different levels of complexity. We also propose a novel LLM-based evaluation method to provide a more precise and reliable assessment of model... | Guo Gan, Mingsheng Shang, Tingyu Song, Yilun Zhao |  |
| 593 |  |  [QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models](https://doi.org/10.18653/v1/2025.naacl-long.512) |  | 0 | In typical multimodal tasks, such as Visual Question Answering (VQA), adversarial attacks targeting a specific image and question can lead large vision-language models (LVLMs) to provide incorrect answers. However, it is common for a single image to be associated with multiple questions, and LVLMs may still answer other questions correctly even for an adversarial image attacked by a specific question. To address this, we introduce the query-agnostic visual attack (QAVA), which aims to create robust adversarial examples that generate incorrect responses to unspecified and unknown questions. Compared to traditional adversarial attacks focused on specific images and questions, QAVA... | Jiansheng Chen, Ruobing Xie, Xingwu Sun, Yu Wang, Yudong Zhang, Zhanhui Kang |  |
| 594 |  |  [Evaluating and Improving Graph to Text Generation with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.513) |  | 0 | Large language models (LLMs) have demonstrated immense potential across various tasks. However, research for exploring and improving the capabilities of LLMs in interpreting graph structures remains limited. To address this gap, we conduct a comprehensive evaluation of prompting current open-source LLMs on graph-to-text generation tasks. Although we explored the optimal prompting strategies and proposed a novel and effective diversity-difficulty-based few-shot sample selection method, we found that the improvements from tuning-free approaches were incremental, as LLMs struggle with planning on complex graphs, particularly those with a larger number of triples. To further improve LLMs in... | Deyi Xiong, Jeff Z. Pan, Jie He, Víctor GutiérrezBasulto, Wanqiu Long, Yijun Yang |  |
| 595 |  |  [The Plagiarism Singularity Conjecture](https://doi.org/10.18653/v1/2025.naacl-long.514) |  | 0 |  | Anupam Chattopadhyay, Erik Cambria, Rui Mao, Sriram Ranga |  |
| 596 |  |  [Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.515) |  | 0 | Despite recent advances in large language models, open-source models often struggle to consistently perform well on complex reasoning tasks. Existing ensemble methods, whether applied at the token or output levels, fail to address these challenges. In response, we present Language model Ensemble with Monte Carlo Tree Search (LE-MCTS), a novel framework for process-level ensembling of language models. LE-MCTS formulates step-by-step reasoning with an ensemble of language models as a Markov decision process. In this framework, states represent intermediate reasoning paths, while actions consist of generating the next reasoning step using one of the language models selected from a predefined... | Edward Choi, Sungjin Park, Xiao Liu, Yeyun Gong |  |
| 597 |  |  [One Unified Model for Diverse Tasks: Emotion Cause Analysis via Self-Promote Cognitive Structure Modeling](https://doi.org/10.18653/v1/2025.naacl-long.516) |  | 0 | Emotion cause analysis is a critical topic in natural language processing. Key tasks include emotion cause extraction (ECE), emotion-cause pair extraction (ECPE), social emotion cause identification (SECI) as well as social emotion mining and its cause identification (SEMCI). While current emotion cause analysis methods often focus on task-specific model design, they tend to overlook the underlying common ground across these tasks rooted in cognitive emotion theories, in particular, the cognitive structure of emotions. Drawing inspiration from this theory, in this paper, we propose a unified model capable of tackling diverse emotion cause analysis tasks, which constructs the emotion... | Wenji Mao, Xinglin Xiao, Zhaoxin Yu |  |
| 598 |  |  [Soft Language Prompts for Language Transfer](https://doi.org/10.18653/v1/2025.naacl-long.517) |  | 0 | Cross-lingual knowledge transfer, especially between high- and low-resource languages, remains challenging in natural language processing (NLP). This study offers insights for improving cross-lingual NLP applications through the combination of parameter-efficient fine-tuning methods. We systematically explore strategies for enhancing cross-lingual transfer through the incorporation of language-specific and task-specific adapters and soft prompts. We present a detailed investigation of various combinations of these methods, exploring their efficiency across 16 languages, focusing on 10 mid- and low-resource languages. We further present to our knowledge the first use of soft prompts for... | Ivan Vykopal, Marián Simko, Simon Ostermann |  |
| 599 |  |  [PICLe: Pseudo-annotations for In-Context Learning in Low-Resource Named Entity Detection](https://doi.org/10.18653/v1/2025.naacl-long.518) |  | 0 | In-context learning (ICL) enables Large Language Models (LLMs) to perform tasks using few demonstrations, facilitating task adaptation when labeled examples are hard to come by. However, ICL is sensitive to the choice of demonstrations, and it remains unclear which demonstration attributes enable in-context generalization. In this work, we conduct a perturbation study of in-context demonstrations for low-resource Named Entity Detection (NED). Our surprising finding is that in-context demonstrations with partially-correct annotated entity mentions can be as effective for task transfer as fully correct demonstrations. Based off our findings, we propose Pseudo-annotated In-Context Learning... | Alexander Mathis, Antoine Bosselut, Sepideh Mamooler, Syrielle Montariol |  |
| 600 |  |  [Can Large Language Models Invent Algorithms to Improve Themselves?](https://doi.org/10.18653/v1/2025.naacl-long.519) |  | 0 | Large Language Models (LLMs) have shown remarkable performance improvements and are rapidly gaining adoption in industry. However, the methods for improving LLMs are still designed by humans, which restricts the invention of new model-improving algorithms to human expertise and imagination. To address this, we propose the Self-Developing framework, which enables LLMs to autonomously generate and learn model-improvement algorithms. In this framework, the seed model generates, applies, and learns model-improving algorithms, continuously improving both the seed model and the algorithms themselves. Among model-improving strategies, we focus on model merging algorithms. In mathematical... | Masafumi Oyamada, Taro Yano, Yoichi Ishibashi |  |
| 601 |  |  [Simulating Classroom Education with LLM-Empowered Agents](https://doi.org/10.18653/v1/2025.naacl-long.520) |  | 0 | Large language models (LLMs) have been applied across various intelligent educational tasks to assist teaching. While preliminary studies have focused on task-specific, independent LLM-empowered agents, the potential of LLMs within a multi-agent collaborative framework for classroom simulation with real user participation remains unexplored. In this work, we propose SimClass, a multi-agent classroom simulation teaching framework. We recognize representative class roles and introduce a novel class control mechanism for automatic classroom teaching, and conduct user experiments in two real-world courses. Using the Flanders Interactive Analysis System and Community of Inquiry theoretical... | Daniel ZhangLi, Huiqin Liu, Jianxiao Jiang, Jie Cao, Jifan Yu, Jinchang Zhou, Juanzi Li, Lei Hou, Linlu Gong, Zhanxin Hao, Zheyuan Zhang, Zhiyuan Liu |  |
| 602 |  |  [A Grounded Typology of Word Classes](https://doi.org/10.18653/v1/2025.naacl-long.521) |  | 0 | In this work, we propose a grounded approach to meaning in language typology. Using images captioned across languages, we can treat the images as an empirical language agnostic representation of meaning, allowing the quantification of language function and semantics. Using principles from information theory, we define “groundedness”, an empirical measure of contextual semantic contentfulness which can be computed using multilingual (vision-and-)language models. As an initial application, we apply this measure to the typology of word classes. We find our measure captures the contentfulness asymmetry between functional (grammatical) and lexical (content) classes across languages, but... | Coleman Haley, Edoardo M. Ponti, Sharon Goldwater |  |
| 603 |  |  [SSH: Sparse Spectrum Adaptation via Discrete Hartley Transformation](https://doi.org/10.18653/v1/2025.naacl-long.522) |  | 0 | Low-rank adaptation (LoRA) has been demonstrated effective in reducing the trainable parameter number when fine-tuning a large foundation model (LLM). However, it still encounters computational and memory challenges when scaling to larger models or addressing more complex task adaptation.In this work, we introduce \*\*Sparse Spectrum Adaptation via Discrete Hartley Transformation (SSH)\*\*, a novel approach that significantly reduces the number of trainable parameters while enhancing model performance. It selects the most informative spectral components across all layers, under the guidance of the initial weights after a discrete Hartley transformation (DHT). The lightweight inverse DHT... | Andy D. Pimentel, Anuj Pathania, Hongyi Zhu, JiaHong Huang, Qi Bi, Yixian Shen |  |
| 604 |  |  [LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable User Satisfaction Estimation in Dialogue](https://doi.org/10.18653/v1/2025.naacl-long.523) |  | 0 | Understanding user satisfaction with conversational systems, known as User Satisfaction Estimation (USE), is essential for assessing dialogue quality and enhancing user experiences. However, existing methods for USE face challenges due to limited understanding of underlying reasons for user dissatisfaction and the high costs of annotating user intentions. To address these challenges, we propose PRAISE (Plan and Retrieval Alignment for Interpretable Satisfaction Estimation), an interpretable framework for effective user satisfaction prediction. PRAISE operates through three key modules. The Strategy Planner develops strategies, which are natural language criteria for classifying user... | Jaewon Jung, Jinseok Kim, Sangyeop Kim, Sohhyung Park, Sungzoon Cho |  |
| 605 |  |  [LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs](https://doi.org/10.18653/v1/2025.naacl-long.524) |  | 0 | While large language models (LLMs) excel in generating coherent and contextually rich outputs, their capacity to efficiently handle long-form contexts is limited by fixed-length position embeddings. Additionally, the computational cost of processing long sequences increases quadratically, making it challenging to extend context length. To address these challenges, we propose Long-form Context Injection with Recurrent Compression (LCIRC), a method that enables the efficient processing long-form sequences beyond the model’s length limit through recurrent compression without retraining the entire model. We further introduce query dependent context modeling, which selectively compresses... | Chanjun Park, Junyoung Sung, Paul Hongsuck Seo, Sumin An, Wonpyo Park |  |
| 606 |  |  [A Template Is All You Meme](https://doi.org/10.18653/v1/2025.naacl-long.525) |  | 0 | Templatic memes, characterized by a semantic structure adaptable to the creator’s intent, represent a significant yet underexplored area within meme processing literature. With the goal of establishing a new direction for computational meme analysis, here we create a knowledge base composed of more than 5,200 meme templates, information about them, and 54,000 examples of template instances (templatic memes). To investigate the semantic signal of meme templates, we show that we can match memes in datasets to base templates contained in our knowledge base with a distance-based lookup. To demonstrate the power of meme templates, we create TSplit, a method to reorganize datasets, where a... | Iryna Gurevych, Luke Bates, Peter Ebert Christensen, Preslav Nakov |  |
| 607 |  |  [LLMs vs Established Text Augmentation Techniques for Classification: When do the Benefits Outweight the Costs?](https://doi.org/10.18653/v1/2025.naacl-long.526) |  | 0 | The generative large language models (LLMs) are increasingly being used for data augmentation tasks, where text samples are LLM-paraphrased and then used for classifier fine-tuning. Previous studies have compared LLM-based augmentations with established augmentation techniques, but the results are contradictory: some report superiority of LLM-based augmentations, while other only marginal increases (and even decreases) in performance of downstream classifiers. A research that would confirm a clear cost-benefit advantage of LLMs over more established augmentation methods is largely missing. To study if (and when) is the LLM-based augmentation advantageous, we compared the effects of recent... | Jakub Simko, Ján Cegin, Peter Brusilovsky |  |
| 608 |  |  [Bridging the Visual Gap: Fine-Tuning Multimodal Models with Knowledge-Adapted Captions](https://doi.org/10.18653/v1/2025.naacl-long.527) |  | 0 | Recent research increasingly focuses on training vision-language models (VLMs) with long, detailed image captions. However, small-scale VLMs often struggle to balance the richness of these captions with the risk of hallucinating content during fine-tuning. In this paper, we explore how well VLMs adapt to such captions. To quantify caption quality, we propose Decomposed NLI (DNLI), an evaluation framework that breaks down generated captions into individual propositions, assessing each in isolation. This fine-grained analysis reveals a critical balance between capturing descriptive details and preventing hallucinations. Our findings show that simply reducing caption complexity or employing... | Assaf BenKish, Idan Szpektor, Moran Yanuka, Raja Giryes, Yonatan Bitton |  |
| 609 |  |  [Self-Training Meets Consistency: Improving LLMs' Reasoning with Consistency-Driven Rationale Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.528) |  | 0 | Self-training approach for large language models (LLMs) improves reasoning abilities by training the models on their self-generated rationales. Previous approaches have labeled rationales that produce correct answers for a given question as appropriate for training. However, a single measure risks misjudging rationale quality, leading the models to learn flawed reasoning patterns. To address this issue, we propose CREST (Consistency-driven Rationale Evaluation for Self-Training), a self-training framework that further evaluates each rationale through follow-up questions and leverages this evaluation to guide its training. Specifically, we introduce two methods: (1) filtering out rationales... | Jaehyeok Lee, JinYeong Bak, Keisuke Sakaguchi |  |
| 610 |  |  [Evaluating Defeasible Reasoning in LLMs with DEFREASING](https://doi.org/10.18653/v1/2025.naacl-long.529) |  | 0 |  | Emily Allaway, Kathleen McKeown |  |
| 611 |  |  [Evaluating Input Feature Explanations through a Unified Diagnostic Evaluation Framework](https://doi.org/10.18653/v1/2025.naacl-long.530) |  | 0 | Explaining the decision-making process of machine learning models is crucial for ensuring their reliability and transparency for end users. One popular explanation form highlights key input features, such as i) tokens (e.g., Shapley Values and Integrated Gradients), ii) interactions between tokens (e.g., Bivariate Shapley and Attention-based methods), or iii) interactions between spans of the input (e.g., Louvain Span Interactions). However, these explanation types have only been studied in isolation, making it difficult to judge their respective applicability. To bridge this gap, we develop a unified framework that facilitates an automated and direct comparison between highlight and... | Isabelle Augenstein, Jingyi Sun, Pepa Atanasova |  |
| 612 |  |  [From Evidence to Belief: A Bayesian Epistemology Approach to Language Models](https://doi.org/10.18653/v1/2025.naacl-long.531) |  | 0 | This paper investigates the knowledge of language models from the perspective of Bayesian epistemology. We explore how language models adjust their confidence and responses when presented with evidence with varying levels of informativeness and reliability. To study these properties, we create a dataset with various types of evidence and analyze language models’ responses and confidence using verbalized confidence, token probability, and sampling. We observed that language models do not consistently follow Bayesian epistemology: language models follow the Bayesian confirmation assumption well with true evidence but fail to adhere to other Bayesian assumptions when encountering different... | James Thorne, Minsu Kim, Sangryul Kim |  |
| 613 |  |  [Private Synthetic Text Generation with Diffusion Models](https://doi.org/10.18653/v1/2025.naacl-long.532) |  | 0 | How capable are diffusion models of generating synthetics texts? Recent research shows their strengths, with performance reaching that of auto-regressive LLMs. But are they also good in generating synthetic data if the training was under differential privacy? Here the evidence is missing, yet the promises from private image generation look strong. In this paper we address this open question by extensive experiments. At the same time, we critically assess (and reimplement) previous works on synthetic private text generation with LLMs and reveal some unmet assumptions that might have led to violating the differential privacy guarantees. Our results partly contradict previous non-private... | Ivan Habernal, Sebastian Ochs |  |
| 614 |  |  [Mitigating Tail Narrowing in LLM Self-Improvement via Socratic-Guided Sampling](https://doi.org/10.18653/v1/2025.naacl-long.533) |  | 0 | Self-improvement methods enable large language models (LLMs) to generate solutions themselves and iteratively train on filtered, high-quality rationales. This process proves effective and reduces the reliance on human supervision in LLMs’ reasoning, but the performance soon plateaus. We delve into the process and find that models tend to over-sample on easy queries and under-sample on queries they have yet to master. As iterations proceed, this imbalance in sampling is exacerbated, leading to a long-tail distribution where solutions to difficult queries almost diminish. This phenomenon limits the performance gain of self-improving models. A straightforward solution is brute-force sampling... | Lizhuoyuan Lizhuoyuan, Qi Zhang, Shi Xiaowei, Tao Gui, Wei He, Xuanjing Huang, Xunliang Cai, Yitao Zhai, Yiwen Ding, Zhiheng Xi |  |
| 615 |  |  [FactEval: Evaluating the Robustness of Fact Verification Systems in the Era of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.534) |  | 0 | Whilst large language models (LLMs) have made significant advances in every natural language processing task, studies have shown that these models are vulnerable to small perturbations in the inputs, raising concerns about their robustness in the real-world. Given the rise of misinformation online and its significant impact on society, fact verification is one area in which assessing the robustness of models developed for this task is crucial. However, the robustness of LLMs in fact verification remains largely unexplored. In this paper, we introduce FactEval, a novel large-scale benchmark for extensive evaluation of LLMs in the fact verification domain covering 17 realistic word-level and... | Mamta Mamta, Oana Cocarascu |  |
| 616 |  |  [Analyzing Memorization in Large Language Models through the Lens of Model Attribution](https://doi.org/10.18653/v1/2025.naacl-long.535) |  | 0 | Large Language Models (LLMs) are prevalent in modern applications but often memorize training data, leading to privacy breaches and copyright issues. Existing research has mainly focused on post-hoc analyses—such as extracting memorized content or developing memorization metrics—without exploring the underlying architectural factors that contribute to memorization. In this work, we investigate memorization from an architectural lens by analyzing how attention modules at different layers impact its memorization and generalization performance. Using attribution techniques, we systematically intervene in the LLM’s architecture by bypassing attention modules at specific blocks while keeping... | Chirag Agarwal, Susmit Agrawal, Tarun Ram Menta |  |
| 617 |  |  [Track-SQL: Enhancing Generative Language Models with Dual-Extractive Modules for Schema and Context Tracking in Multi-turn Text-to-SQL](https://doi.org/10.18653/v1/2025.naacl-long.536) |  | 0 | Generative language models have shown significant potential in single-turn Text-to-SQL. However, their performance does not extend equivalently to multi-turn Text-to-SQL. This is primarily due to generative language models’ inadequacy in handling the complexities of context information and dynamic schema linking in multi-turn interactions. In this paper, we propose a framework named Track-SQL, which enhances generative language models with dual-extractive modules designed to track schema and contextual changes in multi-turn Text-to-SQL. Specifically, Track-SQL incorporates a Semantic-enhanced Schema Extractor and a Schema-aware Context Extractor. Experimental results demonstrate that... | Bingfeng Chen, Boyan Xu, Ruichu Cai, Shaobin Shi, Yongqi Luo, Zhifeng Hao |  |
| 618 |  |  [Prototypical Extreme Multi-label Classification with a Dynamic Margin Loss](https://doi.org/10.18653/v1/2025.naacl-long.537) |  | 0 | Extreme Multi-label Classification (XMC) methods predict relevant labels for a given query in an extremely large label space. Recent works in XMC address this problem using deep encoders that project text descriptions to an embedding space suitable for recovering the closest labels. However, learning deep models can be computationally expensive in large output spaces, resulting in a trade-off between high performing brute-force approaches and efficient solutions. In this paper, we propose PRIME, a XMC method that employs a novel prototypical contrastive learning technique to reconcile efficiency and performance surpassing brute-force approaches. We frame XMC as a data-to-prototype... | David JimenezCabello, Diego Ortego, Kunal Dahiya |  |
| 619 |  |  [MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback](https://doi.org/10.18653/v1/2025.naacl-long.538) |  | 0 | Automatic question generation (QG) is essential for AI and NLP, particularly in intelligent tutoring, dialogue systems, and fact verification. Generating multiple-choice questions (MCQG) for professional exams, like the United States Medical Licensing Examination (USMLE), is particularly challenging, requiring domain expertise and complex multi-hop reasoning for high-quality questions. However, current large language models (LLMs) like GPT-4 struggle with professional MCQG due to outdated knowledge, hallucination issues, and prompt sensitivity, resulting in unsatisfactory quality and difficulty. To address these challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique and... | Aditya Parashar, Feiyun Ouyang, Hong Yu, Huixue Zhou, Won Seok Jang, Zhichao Yang, Zonghai Yao |  |
| 620 |  |  [Main Predicate and Their Arguments as Explanation Signals For Intent Classification](https://doi.org/10.18653/v1/2025.naacl-long.539) |  | 0 | Intent classification is crucial for conversational agents (chatbots), and deep learning models perform well in this area. However, little research has been done on the explainability of intent classification due to the absence of suitable benchmark data. Human annotation of explanation signals in text samples is time-consuming and costly. However, from inspection of data on intent classification, we see that, more often than not, the main verb denotes the action, and the direct object indicates the domain of conversation, serving as explanation signals for intent. This observation enables us to hypothesize that the main predicate in the text utterances, along with the arguments of the... | Pushpak Bhattacharyya, Sameer Pimparkhede |  |
| 621 |  |  [Handling Missing Entities in Zero-Shot Named Entity Recognition: Integrated Recall and Retrieval Augmentation](https://doi.org/10.18653/v1/2025.naacl-long.540) |  | 0 | Zero-shot Named Entity Recognition (ZS-NER) aims to recognize entities in unseen domains without specific annotated data. A key challenge is handling missing entities while ensuring accurate type recognition, hindered by: 1) the pre-training assumption that each entity has a single type, overlooking diversity, and 2) insufficient contextual knowledge for type reasoning. To address this, we propose IRRA (Integrated Recall and Retrieval Augmentation), a novel two-stage framework leveraging large language model techniques. In the Recall Augmented Entity Extracting stage, we built a perturbed dataset to induce the model to exhibit missing or erroneous extracted entities. Based on this, we... | Boyan Xu, Junhao Lu, Ruichu Cai, Zhifeng Hao, Zhongjie Chen |  |
| 622 |  |  [KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy](https://doi.org/10.18653/v1/2025.naacl-long.541) |  | 0 | The increasing demand for mental health services has led to the rise of AI-driven mental health chatbots, though challenges related to privacy, data collection, and expertise persist. Motivational Interviewing (MI) is gaining attention as a theoretical basis for boosting expertise in the development of these chatbots. However, existing datasets are showing limitations for training chatbots, leading to a substantial demand for publicly available resources in the field of MI and psychotherapy. These challenges are even more pronounced in non-English languages, where they receive less attention. In this paper, we propose a novel framework that simulates MI sessions enriched with the expertise... | Eunseo Ryu, Hyunjong Kim, Sungzoon Cho, Suran Seong, Suyeon Lee, Yeongjae Cho, Yohan Jo |  |
| 623 |  |  [Automatic Input Rewriting Improves Translation with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.542) |  | 0 | Can we improve machine translation (MT) with LLMs by rewriting their inputs automatically? Users commonly rely on the intuition that well-written text is easier to translate when using off-the-shelf MT systems. LLMs can rewrite text in many ways but in the context of MT, these capabilities have been primarily exploited to rewrite outputs via post-editing. We present an empirical study of 21 input rewriting methods with 3 open-weight LLMs for translating from English into 6 target languages. We show that text simplification is the most effective MT-agnostic rewrite strategy and that it can be improved further when using quality estimation to assess translatability. Human evaluation further... | Dayeon Ki, Marine Carpuat |  |
| 624 |  |  [HIGGS: Pushing the Limits of Large Language Model Quantization via the Linearity Theorem](https://doi.org/10.18653/v1/2025.naacl-long.543) |  | 0 | Quantizing large language models has become a standard way to reduce their memory and computational costs. Typically, existing methods focus on breaking down the problem into individual layer-wise sub-problems, and minimizing per-layer error, measured via various metrics. Yet, this approach currently lacks theoretical justification and the metrics employed may be sub-optimal. In this paper, we present a “linearity theorem” establishing a direct relationship between the layer-wise reconstruction error and the model perplexity increase due to quantization. This insight enables two novel applications: (1) a simple data-free LLM quantization method using Hadamard rotations and MSE-optimal... | Andrei Panferov, Dan Alistarh, Han Guo, Ivan Ilin, Peter Richtárik, Vladimir Malinovskii |  |
| 625 |  |  [The LLM Language Network: A Neuroscientific Approach for Identifying Causally Task-Relevant Units](https://doi.org/10.18653/v1/2025.naacl-long.544) |  | 0 | Large language models (LLMs) exhibit remarkable capabilities on not just language tasks, but also various tasks that are not linguistic in nature, such as logical reasoning and social inference. In the human brain, neuroscience has identified a core language system that selectively and causally supports language processing. We here ask whether similar specialization for language emerges in LLMs. We identify language-selective units within 18 popular LLMs, using the same localization approach that is used in neuroscience. We then establish the causal role of these units by demonstrating that ablating LLM language-selective units – but not random units – leads to drastic deficits in language... | Antoine Bosselut, Badr AlKhamissi, Greta Tuckute, Martin Schrimpf |  |
| 626 |  |  [MixLLM: Dynamic Routing in Mixed Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.545) |  | 0 | Large Language Models (LLMs) exhibit potential artificial generic intelligence recently, however, their usage is costly with high response latency. Given mixed LLMs with their own strengths and weaknesses, LLM routing aims to identify the most suitable model for each query in the stream to maximize response quality and minimize cost and latency. However, the challenges involve: (1) dynamic trade-offs among quality, cost, and latency; (2) enabling continual learning in deployed systems; and (3) navigating a varying (e.g., new LLM addition or old LLM removal) set of LLM candidates over time. To bridge these gaps, we develop MixLLM, a dynamic contextual-bandit-based routing system for... | Haifeng Chen, Wei Cheng, Wenchao Yu, Xinyuan Wang, Xujiang Zhao, Yanchi Liu, Yanjie Fu, Zhengzhang Chen |  |
| 627 |  |  [Continual Learning in Multilingual Sign Language Translation](https://doi.org/10.18653/v1/2025.naacl-long.546) |  | 0 | The field of sign language translation (SLT) is still in its infancy, as evidenced by the low translation quality, even when using deep learn- ing approaches. Probably because of this, many common approaches in other machine learning fields have not been explored in sign language. Here, we focus on continual learning for mul- tilingual SLT. We experiment with three con- tinual learning methods and compare them to four more naive baseline and fine-tuning ap- proaches. We work with four sign languages (ASL, BSL, CSL and DGS) and three spo- ken languages (Chinese, English and German). Our results show that incremental fine-tuning is the best performing approach both in terms of translation... | Cristina EspañaBonet, Josef van Genabith, Shakib Yazdani |  |
| 628 |  |  [Few-Shot Natural Language to First-Order Logic Translation via Code Generation](https://doi.org/10.18653/v1/2025.naacl-long.547) |  | 0 | Translation of natural language to first-order logical formula (NL-FOL) has recently gained significant attention for its critical role in logic-based NLP applications. Some studies attempt to utilize pretrained language models in a sequence-to-sequence manner for the NL-FOL task. However, these methods encounter challenges such as (1) inconsistency between the training and inference phases and (2) the data-intensive and resource-intensive finetuning process. This paper introduces a novel NL-FOL translation method, dubbed Code4Logic, which is based on in-context learning and employs code snippets to bridge the gap between natural language and first-order logic. By converting the... | Junnan Liu |  |
| 629 |  |  [How Good Are LLMs for Literary Translation, Really? Literary Translation Evaluation with Humans and LLMs](https://doi.org/10.18653/v1/2025.naacl-long.548) |  | 0 | Recent research has focused on literary machine translation (MT) as a new challenge in MT. However, the evaluation of literary MT remains an open problem. We contribute to this ongoing discussion by introducing LITEVAL-CORPUS, a paragraph-level parallel corpus containing verified human translations and outputs from 9 MT systems, which totals over 2k translations and 13k evaluated sentences across four language pairs, costing 4.5k€. This corpus enables us to (i) examine the consistency and adequacy of human evaluation schemes with various degrees of complexity, (ii) compare evaluations by students and professionals, assess the effectiveness of (iii) LLM-based metrics and (iv) LLMs... | Ran Zhang, Steffen Eger, Wei Zhao |  |
| 630 |  |  [PORT: Preference Optimization on Reasoning Traces](https://doi.org/10.18653/v1/2025.naacl-long.549) |  | 0 | Preference optimization methods have been successfully applied to improve not only the alignment of large language models (LLMs) with human values, but also specific natural language tasks such as summarization and stylistic continuations. This paper proposes using preference optimization methods on Chain-of-Thought steps in order to improve the mathematical reasoning performances of language models. While the chosen answers are obtained from datasets that include reasoning traces, we propose two complementary schemes for generating rejected answers: weak LLM prompting, and digit corruption. Our approach leads to increased accuracy on the GSM8K and AQuA-RAT mathematical reasoning... | Abdalgader Abubaker, Hakim Hacid, Salem Lahlou |  |
| 631 |  |  [Guiding Through Complexity: What Makes Good Supervision for Hard Reasoning Tasks?](https://doi.org/10.18653/v1/2025.naacl-long.550) |  | 0 | How can “weak teacher models” (Bowman et al., 2022) such as average human annotators or existing AI systems, effectively supervise LLMs to improve performance on hard reasoning tasks, especially those that challenge and requires expertise or daily practice from the teacher models? In this paper, we seek for empirical answers to this question by investigating various data-driven strategies that offer supervision data at different quality levels upon tasks of varying complexity. Two intuitive strategies emerge for teacher models to provide supervision during alignment training: 1) using lower-quality supervision from complete tasks that match the difficulty of the target reasoning tasks, and... | Da Yin, Nanyun Peng, Xuan He |  |
| 632 |  |  [Fine-Grained Transfer Learning for Harmful Content Detection through Label-Specific Soft Prompt Tuning](https://doi.org/10.18653/v1/2025.naacl-long.551) |  | 0 | The spread of harmful content online is a dynamic issue evolving over time. Existing detection models, reliant on static data, are becoming less effective and generalizable. Developing new models requires sufficient up-to-date data, which is challenging. A potential solution is to combine existing datasets with minimal new data. However, detection tasks vary—some focus on hate speech, offensive, or abusive content, which differ in the intent to harm, while others focus on identifying targets of harmful speech such as racism, sexism, etc.—raising the challenge of handling nuanced class differences. To address these issues, we introduce a novel transfer learning method that leverages... | Alexander Fraser, Faeze Ghorbanpour, Viktor Hangya |  |
| 633 |  |  [A Systematic Examination of Preference Learning through the Lens of Instruction-Following](https://doi.org/10.18653/v1/2025.naacl-long.552) |  | 0 | In this work we systematically investigate how specific attributes of preference datasets affect the alignment and downstream performance of LLMs in instruction-following tasks. We use a novel synthetic data generation pipeline to generate 48,000 unique instruction-following prompts with combinations of 23 verifiable constraints that enable fine-grained and automated quality assessments of model responses. With our synthetic prompts, we use rejection sampling (RS) and Monte Carlo Tree Search (MCTS) to obtain preference pairs. Then, we perform experiments investigating the effects of (1) the presence of shared prefixes between the chosen and rejected responses, (2) the contrast and quality... | Anirudh Goyal, Aston Zhang, Bo Xiong, Dhruv Mahajan, Hannaneh Hajishirzi, Joongwon Kim, Liang Tan, Melanie Kambadur, Rui Hou |  |
| 634 |  |  [Lived Experience Not Found: LLMs Struggle to Align with Experts on Addressing Adverse Drug Reactions from Psychiatric Medication Use](https://doi.org/10.18653/v1/2025.naacl-long.553) |  | 0 | Adverse Drug Reactions (ADRs) from psychiatric medications are the leading cause of hospitalizations among mental health patients. With healthcare systems and online communities facing limitations in resolving ADR-related issues, Large Language Models (LLMs) have the potential to fill this gap. Despite the increasing capabilities of LLMs, past research has not explored their capabilities in detecting ADRs related to psychiatric medications or in providing effective harm reduction strategies. To address this, we introduce the \*\*Psych-ADR\*\* benchmark and the \*\*A\*\*dverse \*\*D\*\*rug Reaction \*\*R\*\*esponse \*\*A\*\*ssessment (\*\*ADRA\*\*) framework to systematically evaluate LLM... | Gaurav Verma, Harneet Singh Khanuja, Jose Suarez Campayo, Michael L. Birnbaum, Mohit Chandra, Munmun De Choudhury, Siddharth Sriraman, Zihang Li |  |
| 635 |  |  [Latent Factor Models Meets Instructions: Goal-conditioned Latent Factor Discovery without Task Supervision](https://doi.org/10.18653/v1/2025.naacl-long.554) |  | 0 | Instruction-following LLMs have recently allowed systems to discover hidden concepts from a collection of unstructured documents based on a natural language description of the purpose of the discovery (i.e., goal). Still, the quality of the discovered concepts remains mixed, as it depends heavily on LLM’s reasoning ability and drops when the data is noisy or beyond LLM’s knowledge. We present Instruct-LF, a goal-oriented latent factor discovery system that integrates LLM’s instruction-following ability with statistical models to handle large, noisy datasets where LLM reasoning alone falls short. Instruct-LF uses LLMs to propose fine-grained, goal-related properties from documents,... | Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Harshit Surana, Julian J. McAuley, Peter Clark, Tushar Khot, Zhouhang Xie |  |
| 636 |  |  [LLM-Supported Natural Language to Bash Translation](https://doi.org/10.18653/v1/2025.naacl-long.555) |  | 0 | The Bourne-Again Shell (Bash) command-line interface for Linux systems has complex syntax and requires extensive specialized knowledge. Using the natural language to Bash command (NL2SH) translation capabilities of large language models (LLMs) for command composition circumvents these issues. However, the NL2SH performance of LLMs is difficult to assess due to inaccurate test data and unreliable heuristics for determining the functional equivalence of Bash commands. We present a manually verified test dataset of 600 instruction-command pairs and a training dataset of 40,939 pairs, increasing the size of previous datasets by 441% and 135%, respectively. Further, we present a novel... | Erik Hemberg, Finnian Westenfelder, Silviu Chiricescu, Stephen Moskal, UnaMay O'Reilly |  |
| 637 |  |  [REL-A.I.: An Interaction-Centered Approach To Measuring Human-LM Reliance](https://doi.org/10.18653/v1/2025.naacl-long.556) |  | 0 | The ability to communicate uncertainty and knowledge limitations is crucial for the safety of large language models (LLMs). Current evaluations of these abilities typically examine the correspondence between model accuracy and its internal probabilities or linguistic outputs. However, evaluation of the uncertainty of LLM communication should also focus on the behaviors of their human interlocutors: how much do users rely on what the LLM says? We introduce an interaction-centered evaluation approach called Rel-A.I. (pronounced “rely”) that quantifies whether and how humans rely on LLMs’ responses, complementing existing calibration evaluations. Through nine user studies with 450... | Dan Jurafsky, Jena D. Hwang, Kaitlyn Zhou, Maarten Sap, Nouha Dziri, Xiang Ren |  |
| 638 |  |  [Eliciting Critical Reasoning in Retrieval-Augmented Generation via Contrastive Explanations](https://doi.org/10.18653/v1/2025.naacl-long.557) |  | 0 | Retrieval-augmented generation (RAG) have emerged as a critical mechanism in contemporary NLP to support Large Language Models (LLMs) in systematically accessing richer factual context. However, the integration of RAG mechanisms bring its inherent challenges, as LLMs need to integrate potentially noisy contexts. Recent studies have shown that LLMs still struggle to critically analyse RAG-based in-context information, a limitation that may lead to incorrect inferences and hallucinations. In this paper, we investigate how to elicit critical arguments in RAG via contrastive explanations. In particular, we propose Contrastive-RAG (CRAG), a framework that (i) retrieves relevant documents given... | André Freitas, Leonardo Ranaldi, Marco Valentino |  |
| 639 |  |  [A Distributional Perspective on Word Learning in Neural Language Models](https://doi.org/10.18653/v1/2025.naacl-long.558) |  | 0 | Language models (LMs) are increasingly being studied as models of human language learners.Due to the nascency of the field, it is not well-established whether LMs exhibit similar learning dynamics to humans, and there are few direct comparisons between learning trajectories in humans and models.Word learning trajectories for children are relatively well-documented, and recent work has tried to extend these investigations to language models.However, there are no widely agreed-upon metrics for word learning in language models.We take a distributional approach to this problem, defining lexical knowledge in terms of properties of the learned distribution for a target word.We argue that... | Alex Warstadt, Filippo Ficarra, Ryan Cotterell |  |
| 640 |  |  [Disentangling language change: sparse autoencoders quantify the semantic evolution of indigeneity in French](https://doi.org/10.18653/v1/2025.naacl-long.559) |  | 0 | This study presents a novel approach to analyzing historical language change, focusing on the evolving semantics of the French term “indigène(s)” (“indigenous”) between 1825 and 1950. While existing approaches to measuring semantic change with contextual word embeddings (CWE) rely primarily on similarity measures or clustering, these methods may not be suitable for highly imbalanced datasets, and pose challenges for interpretation. For this reason, we propose an interpretable, feature-level approach to analyzing language change, which we use to trace the semantic evolution of “indigène(s)” over a 125-year period. Following recent work on sequence embeddings (O’Neill et al., 2024), we use... | Imane Terhmina, Jacob Matthews, Laurent Dubreuil, Marten van Schijndel, Matthew Wilkens, Yunci Sun |  |
| 641 |  |  [Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages](https://doi.org/10.18653/v1/2025.naacl-long.560) |  | 0 | Recent works have explored using language models for planning problems. One approach examines translating natural language descriptions of planning tasks into structured planning languages, such as the planning domain definition language (PDDL). Existing evaluation methods struggle to ensure semantic correctness and rely on simple or unrealistic datasets. To bridge this gap, we introduce Planetarium, a benchmark designed to evaluate language models’ ability to generate PDDL code from natural language descriptions of planning tasks. Planetarium features a novel PDDL equivalence algorithm that flexibly evaluates the correctness of generated PDDL against ground truth, along with a dataset of... | Francisco Piedrahita Velez, Max Zuo, Michael Littman, Stephen H. Bach, Xiaochen Li |  |
| 642 |  |  [One fish, two fish, but not the whole sea: Alignment reduces language models' conceptual diversity](https://doi.org/10.18653/v1/2025.naacl-long.561) |  | 0 | Researchers in social science and psychology have recently proposed using large language models (LLMs) as replacements for humans in behavioral research. In addition to arguments about whether LLMs accurately capture population-level patterns, this has raised questions about whether LLMs capture human-like conceptual diversity. Separately, it is debated whether post-training alignment (RLHF or RLAIF) affects models’ internal diversity. Inspired by human studies, we use a new way of measuring the conceptual diversity of synthetically-generated LLM “populations” by relating the internal variability of simulated individuals to the population-level variability. We use this approach to evaluate... | Jennifer Hu, Sonia K. Murthy, Tomer D. Ullman |  |
| 643 |  |  [Using Text-Based Causal Inference to Disentangle Factors Influencing Online Review Ratings](https://doi.org/10.18653/v1/2025.naacl-long.562) |  | 0 | Online reviews provide valuable insights into the perceived quality of facets of a product or service. While aspect-based sentiment analysis has focused on extracting these facets from reviews, there is less work understanding the impact of each aspect on overall perception. This is particularly challenging given correlations among aspects, making it difficult to isolate the effects of each. This paper introduces a methodology based on recent advances in text-based causal analysis, specifically CausalBERT, to disentangle the effect of each factor on overall review ratings. We enhance CausalBERT with three key improvements: temperature scaling for better calibrated treatment assignment... | Aron Culotta, Linsen Li, Nicholas Mattei |  |
| 644 |  |  [Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate](https://doi.org/10.18653/v1/2025.naacl-long.563) |  | 0 | Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler. We provide a theoretical analysis and empirically demonstrate the superior performance of among state-of-the-art unlearning methods on the TOFU and... | Anil Ramakrishna, Bhanukiran Vinzamuri, KaiWei Chang, Mingyi Hong, Volkan Cevher, Xiaomeng Jin, Zhiqi Bu |  |
| 645 |  |  [REFFLY: Melody-Constrained Lyrics Editing Model](https://doi.org/10.18653/v1/2025.naacl-long.564) |  | 0 | Automatic melody-to-lyric (M2L) generation aims to create lyrics that align with a given melody. While most previous approaches generate lyrics from scratch, revision—editing plain text draft to fit it into the melody—offers a much more flexible and practical alternative. This enables broad applications, such as generating lyrics from flexible inputs (keywords, themes, or full text that needs refining to be singable), song translation (preserving meaning across languages while keeping the melody intact), or style transfer (adapting lyrics to different genres). This paper introduces REFFLY (REvision Framework For LYrics), the first revision framework for editing and generating... | Bingxuan Li, Nanyun Peng, Songyan Zhao, Yufei Tian |  |
| 646 |  |  [Exploring Safety-Utility Trade-Offs in Personalized Language Models](https://doi.org/10.18653/v1/2025.naacl-long.565) |  | 0 | As large language models (LLMs) become increasingly integrated into daily applications, it is essential to ensure they function fairly across diverse user demographics. In this work, we show that LLMs suffer from personalization bias, where their performance is impacted when they are personalized to a user’s identity. We quantify personalization bias by evaluating the performance of LLMs along two axes - safety and utility. We measure safety by examining how benign LLM responses are to unsafe prompts. We measure utility by evaluating the LLM’s performance on various tasks, including general knowledge, mathematical abilities, programming, and reasoning skills. We find that various LLMs,... | Anvesh Rao Vijjini, Snigdha Chaturvedi, Somnath Basu Roy Chowdhury |  |
| 647 |  |  [MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart Problems](https://doi.org/10.18653/v1/2025.naacl-long.566) |  | 0 | Multimodal Large Language Models (MLLMs) have demonstrated impressive abilities across various tasks, including visual question answering and chart comprehension, yet existing benchmarks for chart-related tasks fall short in capturing the complexity of real-world multi-chart scenarios. Current benchmarks primarily focus on single-chart tasks, neglecting the multi-hop reasoning required to extract and integrate information from multiple charts, which is essential in practical applications. To fill this gap, we introduce MultiChartQA, a benchmark that evaluates MLLMs’ capabilities in four key areas: direct question answering, parallel question answering, comparative reasoning, and sequential... | Lang Li, Meng Jiang, Mengzhao Jia, Zhihan Zhang, Zifeng Zhu |  |
| 648 |  |  [It Is Not Only the Negative that Deserves Attention! Understanding, Generation & Evaluation of (Positive) Moderation](https://doi.org/10.18653/v1/2025.naacl-long.567) |  | 0 | Moderation is essential for maintaining and improving the quality of online discussions. This involves: (1) countering negativity, e.g. hate speech and toxicity, and (2) promoting positive discourse, e.g. broadening the discussion to involve other users and perspectives. While significant efforts have focused on addressing negativity, driven by an urgency to address such issues, this left moderation promoting positive discourse (henceforth PositiveModeration) under-studied. With the recent advancements in LLMs, Positive Moderation can potentially be scaled to vast conversations, fostering more thoughtful discussions and bridging the increasing divide in online interactions.We advance the... | Carlotta Quensel, Eva Maria Vecchi, Gabriella Lapesa, Iman Jundi, Neele Falk |  |
| 649 |  |  [Social Norms in Cinema: A Cross-Cultural Analysis of Shame, Pride and Prejudice](https://doi.org/10.18653/v1/2025.naacl-long.568) |  | 0 | Shame and pride are social emotions expressed across cultures to motivate and regulate people’s thoughts, feelings, and behaviors. In this paper, we introduce the first cross-cultural dataset of over 10k shame/pride-related expressions with underlying social expectations from ~5.4K Bollywood and Hollywood movies. We examine \*how\* and \*why\* shame and pride are expressed across cultures using a blend of psychology-informed language analysis combined with large language models. We find significant cross-cultural differences in shame and pride expression aligning with known cultural tendencies of the USA and India – e.g., in Hollywood, shame-expressions predominantly discuss \*self\*... | Khushang Jilesh Zaveri, Lyle H. Ungar, Sharath Chandra Guntuku, Shreya Havaldar, Soumna Nema, Sunny Rai |  |
| 650 |  |  [The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding](https://doi.org/10.18653/v1/2025.naacl-long.569) |  | 0 | In a systematic way, we investigate a widely asked question: Do LLMs really understand what they say?, which relates to the more familiar term Stochastic Parrot. To this end, we propose a summative assessment over a carefully designed physical concept understanding task, P HYSI C O. Our task alleviates the memorization issue via the usage of grid-format inputs that abstractly describe physical phenomena. The grids represents varying levels of understanding, from the core phenomenon, application examples to analogies to other abstract patterns in the grid world. A comprehensive study on our task demonstrates: (1) state-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag... | DitYan Yeung, Jiangnan Li, Jie Zhou, Junjie Wu, Lemao Liu, Mo Yu, Shunchi Zhang, Tsz Ting Chung |  |
| 651 |  |  [mHumanEval - A Multilingual Benchmark to Evaluate Large Language Models for Code Generation](https://doi.org/10.18653/v1/2025.naacl-long.570) |  | 0 | Recent advancements in large language models (LLMs) have significantly enhanced code generation from natural language prompts. The HumanEval Benchmark, developed by OpenAI, remains the most widely used code generation benchmark. However, this and other Code LLM benchmarks face critical limitations, particularly in task diversity, test coverage, and linguistic scope. Current evaluations primarily focus on English-to-Python conversion tasks with limited test cases, potentially overestimating model performance. While recent works have addressed test coverage and programming language (PL) diversity, code generation from low-resource language prompts remains largely unexplored. To address this... | Antonios Anastasopoulos, Marcos Zampieri, Md. Nishat Raihan |  |
| 652 |  |  [What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for Gaussian-Noise-free Text-Image Corruption and Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.571) |  | 0 | Vision-Language Models (VLMs) have gained prominence due to their success in solving complex cross-modal tasks. However, the internal mechanisms of VLMs, particularly the roles of cross-attention and self-attention in multimodal integration, are not fully understood. To address this gap, we introduce NOTICE, a Gaussian-Noise-free Text-Image Corruption and Evaluation pipeline for mechanistic interpretability in VLMs. NOTICE introduces Semantic Image Pairs (SIP) corruption, the first visual counterpart to Symmetric Token Replacement (STR) for text. Through NOTICE, we uncover a set of “universal attention heads” in BLIP and LLaVA that consistently contribute across different tasks and... | Carsten Eickhoff, Michal Golovanevsky, Ritambhara Singh, Vedant Palit, William Rudman |  |
| 653 |  |  [Are explicit belief representations necessary? A comparison between Large Language Models and Bayesian probabilistic models](https://doi.org/10.18653/v1/2025.naacl-long.572) |  | 0 | Large language models (LLMs) have exhibited certain indirect pragmatic capabilities, including interpreting indirect requests and non-literal meanings. Yet, it is unclear whether the success of LLMs on pragmatic tasks generalizes to phenomena that directly probe inferences about the beliefs of others. Indeed, LLMs’ performance on Theory of Mind (ToM) tasks is mixed. To date, the most successful computationally explicit approach to making inferences about others’ beliefs is the Rational Speech Act (RSA) framework, a Bayesian probabilistic model that encodes explicit representations of beliefs. In the present study, we ask whether LLMs outperform RSA in predicting human belief inferences,... | Benjamin K. Bergen, Dingyi Pan |  |
| 654 |  |  [Self-Generated Critiques Boost Reward Modeling for Language Models](https://doi.org/10.18653/v1/2025.naacl-long.573) |  | 0 | Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce scalar scores and struggle to incorporate critiques in a natural language format. We hypothesize that predicting both critiques and the scalar reward would improve reward modeling ability. Motivated by this, we propose Critic-RM, a framework that improves reward models using self-generated critiques without extra supervision. Critic-RM employs a two-stage process: generating and filtering high-quality critiques, followed by joint fine-tuning on reward prediction and critique generation.... | Aston Zhang, Chao Zhang, Chenguang Zhu, Dhruv Mahajan, Liang Tan, Melanie Kambadur, Richard Yuanzhe Pang, Rui Hou, Suchin Gururangan, Xuewei Wang, Yue Yu, Yundi Qian, Zhengxing Chen |  |
| 655 |  |  [Characterizing the Role of Similarity in the Property Inferences of Language Models](https://doi.org/10.18653/v1/2025.naacl-long.574) |  | 0 | Property inheritance—a phenomenon where novel properties are projected from higher level categories (e.g., birds) to lower level ones (e.g., sparrows)—provides a unique window into how humans organize and deploy conceptual knowledge. It is debated whether this ability arises due to explicitly stored taxonomic knowledge vs. simple computations of similarity between mental representations. How are these mechanistic hypotheses manifested in contemporary language models? In this work, we investigate how LMs perform property inheritance with behavioral and causal representational analysis experiments. We find that taxonomy and categorical similarities are not mutually exclusive in LMs’ property... | Aaron Mueller, Juan Diego Rodriguez, Kanishka Misra |  |
| 656 |  |  [SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains](https://doi.org/10.18653/v1/2025.naacl-long.575) |  | 0 | Retrieval-augmented generation (RAG) enhances the question answering (QA) abilities of large language models (LLMs) by integrating external knowledge. However, adapting general-purpose RAG systems to specialized fields such as science and medicine poses unique challenges due to distribution shifts and limited access to domain-specific data. To tackle this, we propose SimRAG, a self-training approach that equips LLMs with joint capabilities of question answering and question generation for domain adaptation. Our method first fine-tunes LLMs on instruction-following, question-answering, and search-related data. Then, it prompts LLMs to generate diverse domain-relevant questions from... | Carl Yang, Chen Luo, Hui Liu, Joyce C. Ho, Qi He, Ran Xu, Sreyashi Nag, Xianfeng Tang, Yang Li, Yaochen Xie, Zhenwei Dai |  |
| 657 |  |  [Learning to Substitute Words with Model-based Score Ranking](https://doi.org/10.18653/v1/2025.naacl-long.576) |  | 0 | Smart word substitution aims to enhance sentence quality by improving word choices, however current benchmarks rely on human-labeled data , which suffers from subjectivity and lacks diversity due to limitations in the number of annotators. Since word choices are inherently subjective, ground-truth word substitutions generated by a small group of annotators are often incomplete and likely not generalizable. To circumvent this issue, we instead employ a model-based scoring (BARTScore) to quantify sentence quality, thus forgoing the need for human annotations. Specifically, we use this score to define a distribution for each word substitution, allowing one to test whether a substitution is... | Hongye Liu, Ricardo Henao |  |
| 658 |  |  [Multilingual Reasoning via Self-training](https://doi.org/10.18653/v1/2025.naacl-long.577) |  | 0 | Although reasoning is innately language-agnostic, the multilingual capacities remains a significant challenge for large language models (LLMs). Their ability to generate structured, step-wise explanations is constantly restricted to dominant languages in pre-training data, making cross-lingual generalisation difficult and hindering broader global adoption. Recent works have introduced eclectic strategies to improve reasoning beyond English; however, these methods remain related to specific language that is not always optimal for reasoning.To improve LLMs’ multilingual reasoning abilities, we propose a modular approach that instructs the models to structure reasoning passages in a different... | Giulia Pucci, Leonardo Ranaldi |  |
| 659 |  |  [xLAM: A Family of Large Action Models to Empower AI Agent Systems](https://doi.org/10.18653/v1/2025.naacl-long.578) |  | 0 | Autonomous agents powered by large language models (LLMs) have attracted significant research interest. However, the open-source community faces many challenges in developing specialized models for agent tasks, driven by the scarcity of high-quality agent datasets and the absence of standard protocols in this area. We introduce xLAM, a series of large action models designed for AI agent tasks. The xLAM series includes five models with both dense and mixture-of-expert architectures, ranging from 1B to 8x22B parameters, trained using a scalable, flexible pipeline that unifies, augments, and synthesizes diverse datasets to enhance AI agents’ generalizability and performance across varied... | Akshara Prabhakar, Caiming Xiong, Haolin Chen, Huan Wang, Jianguo Zhang, Juan Carlos Niebles, Juntao Tan, Ming Zhu, Ran Xu, Rithesh R. N., Shelby Heinecke, Shirley Kokane, Silvio Savarese, Thai Hoang, Tian Lan, Tulika Manoj Awalgaonkar, Weiran Yao, Yihao Feng, Zeyuan Chen, Zhiwei Liu, Zuxin Liu |  |
| 660 |  |  [ProMQA: Question Answering Dataset for Multimodal Procedural Activity Understanding](https://doi.org/10.18653/v1/2025.naacl-long.579) |  | 0 | Multimodal systems have great potential to assist humans in procedural activities, where people follow instructions to achieve their goals. Despite diverse application scenarios, systems are typically evaluated on traditional classification tasks, e.g., action recognition or temporal action localization. In this paper, we present a novel evaluation dataset, ProMQA, to measure the advancement of systems in application-oriented scenarios. ProMQA consists of 401 multimodal procedural QA pairs on user recording of procedural activities, i.e., cooking, coupled with their corresponding instruction. For QA annotation, we take a cost-effective human-LLM collaborative approach, where the existing... | Ken Fukuda, Kimihiro Hasegawa, Masaki Asada, Susan Holm, Teruko Mitamura, Wiradee Imrattanatrai, Yuran Wang, ZhiQi Cheng |  |
| 661 |  |  [Ethical Concern Identification in NLP: A Corpus of ACL Anthology Ethics Statements](https://doi.org/10.18653/v1/2025.naacl-long.580) |  | 0 | What ethical concerns, if any, do LLM researchers have? We introduce EthiCon, a corpus of 1,580 ethical concern statements extracted from scientific papers published in the ACL Anthology. We extract ethical concern keywords from the statements and show promising results in automating the concern identification process. Through a survey (N=200), we compare the ethical concerns of the corpus to the concerns listed by the general public and professionals in the field. Finally, we compare our retrieved ethical concerns with existing taxonomies and guidelines pointing to gaps and actionable insights. | Anders Søgaard, Anne Lauscher, Antonia Karamolegkou, Ariadni Christopoulou, Filippos Stamatiou, Sandrine Schiller Hansen |  |
| 662 |  |  [AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge](https://doi.org/10.18653/v1/2025.naacl-long.581) |  | 0 | Knowledge conflict arises from discrepancies between information in the context of a large language model (LLM) and the knowledge stored in its parameters. This can hurt performance when using standard decoding techniques, which tend to ignore the context. Existing test-time contrastive methods seek to address this by comparing the LLM’s output distribution with and without the context and adjust the model according to the contrast between them. However, we find that these methods frequently misjudge the degree of conflict and struggle to handle instances that vary in their amount of conflict, with static methods over-adjusting when conflict is absent. We propose a fine-grained,... | Archiki Prasad, Elias StengelEskin, Han Wang, Mohit Bansal |  |
| 663 |  |  [Are Multimodal LLMs Robust Against Adversarial Perturbations? RoMMath: A Systematic Evaluation on Multimodal Math Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.582) |  | 0 | We introduce RoMMath, the first benchmark designed to evaluate the capabilities and robustness of multimodal large language models (MLLMs) in handling multimodal math reasoning, particularly when faced with adversarial perturbations. RoMMath consists of 4,800 expert-annotated examples, including an original set and seven adversarial sets, each targeting a specific type of perturbation at the text or vision levels. We evaluate a broad spectrum of 17 MLLMs on RoMMath and uncover a critical challenge regarding model robustness against adversarial perturbations. Through detailed error analysis by human experts, we gain a deeper understanding of the current limitations of MLLMs. Additionally,... | Arman Cohan, Chen Zhao, Guo Gan, Yilun Zhao |  |
| 664 |  |  [LBC: Language-Based-Classifier for Out-Of-Variable Generalization](https://doi.org/10.18653/v1/2025.naacl-long.583) |  | 0 | Large Language Models (LLMs) have great success in natural language processing tasks such as response generation. However, their use in tabular data has been limited due to their inferior performance compared to traditional machine learning models (TMLs) such as XGBoost. We find that the pre-trained knowledge of LLMs enables them to interpret new variables that appear in a test without additional training, a capability central to the concept of Out-of-Variable (OOV). From the findings, we propose a Language-Based-Classifier (LBC), a classifier that maximizes the benefits of LLMs to outperform TMLs on OOV tasks. LBC employs three key methodological strategies: 1) Categorical changes to... | Baekryun Seong, Hoyoon Byun, Kangjun Noh, Kyungwoo Song, Sungjin Song, Youngjun Choi |  |
| 665 |  |  [On the Impact of Fine-Tuning on Chain-of-Thought Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.584) |  | 0 | Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs’ task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this... | Chirag Agarwal, Elita A. Lobo, Himabindu Lakkaraju |  |
| 666 |  |  [InfoPO: On Mutual Information Maximization for Large Language Model Alignment](https://doi.org/10.18653/v1/2025.naacl-long.585) |  | 0 | We study the post-training of large language models (LLMs) with human preference data. Recently, direct preference optimization and its variants have shown considerable promise in aligning language models, eliminating the need for reward models and online sampling. Despite these benefits, these methods rely on explicit assumptions about the Bradley-Terry (BT) model, which makes them prone to overfitting and results in suboptimal performance, particularly on reasoning-heavy tasks. To address these challenges, we propose a principled preference fine-tuning algorithm called InfoPO, which effectively and efficiently aligns large language models using preference data. InfoPO eliminates the... | Julian KatzSamuels, Marc Versage, Qingjun Cui, Sujay Sanghavi, Teng Xiao, Tian Wang, Trishul Chilimbi, Zhen Ge |  |
| 667 |  |  [Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse Frequency Effect in Structural Priming](https://doi.org/10.18653/v1/2025.naacl-long.586) |  | 0 | Large language models (LLMs) have shown the emergent capability of in-context learning (ICL). One line of research has claimed that ICL is functionally equivalent to gradient descent, a type of error-driven learning mechanism. In this paper, we introduce a new way of diagnosing whether ICL is functionally performing error-driven learning. Our approach is based on the inverse frequency effect (IFE)—a phenomenon in which an agent’s behavior is influenced to a greater degree when presented with improbable examples as compared to more likely ones. The IFE has previously been identified in psycholinguistics where humans exhibit the IFE in the context of structural priming (the tendency for... | R. Thomas McCoy, Robert Frank, Zhenghao Zhou |  |
| 668 |  |  [Guiding Medical Vision-Language Models with Diverse Visual Prompts: Framework Design and Comprehensive Exploration of Prompt Variations](https://doi.org/10.18653/v1/2025.naacl-long.587) |  | 0 | While mainstream vision-language models (VLMs) have advanced rapidly in understanding image-level information, they still lack the ability to focus on specific areas designated by humans. Rather, they typically rely on large volumes of high-quality image-text paired data to learn and generate posterior attention maps. To address this critical issue, we propose leveraging visual prompts—simple visual markers in various forms—to guide and enhance the formation of region-specific attention. Thus, we introduce \*\*MedVP\*\*, a pioneering framework that integrates medical entity extraction, visual prompt generation, and dataset adaptation for visual prompt-guided fine-tuning. We successfully... | Huahui Yi, Kang Li, Kangyu Zhu, Qicheng Lao, Shaoting Zhang, Zekun Jiang, Ziyuan Qin |  |
| 669 |  |  [Analyzing and Improving Coherence of Large Language Models in Question Answering](https://doi.org/10.18653/v1/2025.naacl-long.588) |  | 0 | Large language models (LLMs) have recently revolutionized natural language processing. These models, however, often suffer from instability or lack of coherence, that is the ability of the models to generate semantically equivalent outputs when receiving diverse yet semantically equivalent input variations. In this work, we analyze the behavior of multiple LLMs, including Mixtral-8x7B, Llama2-70b, Smaug-72b, and Phi-3, when dealing with multiple lexical variations of the same info-seeking questions. Our results suggest that various LLMs struggle to consistently answer diverse equivalent queries. To address this issue, we show how redundant information encoded as a prompt can increase the... | Alessandro Moschitti, Ivano Lauriola, Stefano Campese |  |
| 670 |  |  [ALinFiK: Learning to Approximate Linearized Future Influence Kernel for Scalable Third-Parity LLM Data Valuation](https://doi.org/10.18653/v1/2025.naacl-long.589) |  | 0 | Large Language Models (LLMs) heavily rely on high-quality training data, making data valuation crucial for optimizing model performance, especially when working within a limited budget. In this work, we aim to offer a third-party data valuation approach that benefits both data providers and model developers. We introduce a linearized future influence kernel (LinFiK), which assesses the value of individual data samples in improving LLM performance during training. We further propose ALinFiK, a learning strategy to approximate LinFiK, enabling scalable data valuation. Our comprehensive evaluations demonstrate that this approach surpasses existing baselines in effectiveness and efficiency,... | Denghui Zhang, Huawei Lin, Jiamin Chen, Weijie Zhao, Xiaodong Yu, Yanzhou Pan, Yide Ran, Zhaozhuo Xu |  |
| 671 |  |  [E-Gen: Leveraging E-Graphs to Improve Continuous Representations of Symbolic Expressions](https://doi.org/10.18653/v1/2025.naacl-long.590) |  | 0 | Vector representations have been pivotal in advancing natural language processing (NLP), with prior research focusing on embedding techniques for mathematical expressions using mathematically equivalent formulations. While effective, these approaches are constrained by the size and diversity of training data. In this work, we address these limitations by introducing E-Gen, a novel e-graph-based dataset generation scheme that synthesizes large and diverse mathematical expression datasets, surpassing prior methods in size and operator variety. Leveraging this dataset, we train embedding models using two strategies: (1) generating mathematically equivalent expressions, and (2) contrastive... | Hongbo Zheng, Neeraj Gangwar, Nickvash Kani, Suyuan Wang |  |
| 672 |  |  [Robust and Unbounded Length Generalization in Autoregressive Transformer-Based Text-to-Speech](https://doi.org/10.18653/v1/2025.naacl-long.591) |  | 0 | Autoregressive (AR) Transformer-based sequence models are known to have difficulty generalizing to sequences longer than those seen during training. When applied to text-to-speech (TTS), these models tend to drop or repeat words or produce erratic output, especially for longer utterances. In this paper, we introduce enhancements aimed at AR Transformer-based encoder-decoder TTS systems that address these robustness and length generalization issues. Our approach uses an alignment mechanism to provide cross-attention operations with relative location information. The associated alignment position is learned as a latent property of the model via backpropagation and requires no external... | Daisy Stanton, David Kao, Eric Battenberg, Julian Salazar, Matt Shannon, R. J. SkerryRyan, Soroosh Mariooryad |  |
| 673 |  |  [PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics](https://doi.org/10.18653/v1/2025.naacl-long.592) |  | 0 | Evaluating the quality of machine-generated natural language content is a challenging task in Natural Language Processing (NLP). Recently, large language models (LLMs) like GPT-4 have been employed for this purpose, but they are computationally expensive due to the extensive token usage required by complex evaluation prompts. In this paper, we propose a prompt optimization approach that uses a smaller, fine-tuned language model to compress input data for evaluation prompt, thus reducing token usage and computational cost when using larger LLMs for downstream evaluation. Our method involves a two-stage fine-tuning process: supervised fine-tuning followed by preference optimization to refine... | Daniil Larionov, Steffen Eger |  |
| 674 |  |  [AutoParLLM: GNN-guided Context Generation for Zero-Shot Code Parallelization using LLMs](https://doi.org/10.18653/v1/2025.naacl-long.593) |  | 0 | In-Context Learning (ICL) has been shown to be a powerful technique to augment the capabilities of LLMs for a diverse range of tasks. This work proposes AutoParLLM, a novel way to generate context using guidance from graph neural networks (GNNs) to generate efficient parallel codes. We evaluate AutoParLLM on 12 applications from two well-known benchmark suites of parallel codes: NAS Parallel Benchmark and Rodinia Benchmark. Our results show that AutoParLLM improves the state-of-the-art LLMs (e.g., GPT-4) by 19.9% in NAS and 6.48% in Rodinia benchmark in terms of CodeBERTScore for the task of parallel code generation. Moreover, AutoParLLM improves the ability of the most powerful LLM to... | Ali Jannesari, Ali TehraniJamsaz, Hung D. Phan, Le Chen, Mihai Capota, Nesreen K. Ahmed, Quazi Ishtiaque Mahmud, Theodore L. Willke |  |
| 675 |  |  [Causally Modeling the Linguistic and Social Factors that Predict Email Response](https://doi.org/10.18653/v1/2025.naacl-long.594) |  | 0 | Email is a vital conduit for human communication across businesses, organizations, and broader societal contexts. In this study, we aim to model the intents, expectations, and responsiveness in email exchanges. To this end, we release SIZZLER, a new dataset containing 1800 emails annotated with nuanced types of intents and expectations. We benchmark models ranging from feature-based logistic regression to zero-shot prompting of large language models. Leveraging the predictive model for intent, expectations, and 14 other features, we analyze 11.3M emails from GMANE to study how linguistic and social factors influence the conversational dynamics in email exchanges. Through our causal... | Abraham Israeli, Aparna Ananthasubramaniam, Bangzhao Shu, Bowen Yi, David Jurgens, Haotian Zhang, Hong Chen, Hua Shen, Jiaxin Pei, Kenan Alkiek, Lechen Zhang, Michael Jiang, Mingqian Zheng, Miriam Schirmer, Omkar Yadav, Sushrita Rakshit, Yinuo Xu |  |
| 676 |  |  [AI-LieDar : Examine the Trade-off Between Utility and Truthfulness in LLM Agents](https://doi.org/10.18653/v1/2025.naacl-long.595) |  | 0 | Truthfulness (adherence to factual accuracy) and utility (satisfying human needs and instructions) are both fundamental aspects of Large Language Models, yet these goals often conflict (e.g., sell a car with known flaws), making it challenging to achieve both in real-world deployments. We propose AI-LieDar, a framework to study how LLM-based agents navigate these scenarios in an multi-turn interactive setting. We design a set of real-world scenarios where language agents are instructed to achieve goals that are in conflict with being truthful during a multi-turn conversation with simulated human agents. To evaluate the truthfulness at large scale, we develop a truthfulness detector... | Anubha Kabra, Faeze Brahman, Julia Mendelsohn, Maarten Sap, Sanketh Rangreji, Xuhui Zhou, Zhe Su |  |
| 677 |  |  [Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset](https://doi.org/10.18653/v1/2025.naacl-long.596) |  | 0 | In an effort to mitigate the harms of large language models (LLMs), learning from human feedback (LHF) has been used to steer LLMs towards outputs that are intended to be both less harmful and more helpful. Despite the widespread adoption of LHF in practice, the quality of this feedback and its effectiveness as a safety mitigation technique remain unclear. This study addresses these issues by auditing the widely-used Helpful and Harmless (HH) dataset by Anthropic. Our work includes: (1) a thorough investigation of the dataset’s content through both manual and automated evaluation; (2) experiments demonstrating the dataset’s impact on models’ safety; and (3) an analysis of the 100 most... | Golnoosh Farnadi, Jackie CK Cheung, Jonathan Colaço Carr, Khaoula Chehbouni, Yash More |  |
| 678 |  |  [FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions](https://doi.org/10.18653/v1/2025.naacl-long.597) |  | 0 | Modern Language Models (LMs) are capable of following long and complex instructions that enable a large and diverse set of user requests. While Information Retrieval (IR) models use these LMs as the backbone of their architectures, virtually none of them allow users to provide detailed instructions alongside queries, thus limiting their ability to satisfy complex information needs. In this work, we study the use of instructions in IR systems. First, we introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR repurposes detailed instructions – also known... | Arman Cohan, Benjamin Chang, Benjamin Van Durme, Dawn J. Lawrie, Kyle Lo, Luca Soldaini, Orion Weller, Sean MacAvaney |  |
| 679 |  |  [Few-shot Personalization of LLMs with Mis-aligned Responses](https://doi.org/10.18653/v1/2025.naacl-long.598) |  | 0 | As the diversity of users increases, the capability of providing personalized responses by large language models (LLMs) has become increasingly important. Existing approaches have only limited successes in LLM personalization, due to the absence of personalized learning or the reliance on shared personal data. This paper proposes a new approach for a few-shot personalization of LLMs with their mis-aligned responses (Fermi). Our key idea is to learn a set of personalized prompts for each user by progressively improving the prompts using LLMs, based on user profile (e.g., demographic information) and a few examples of previous opinions. During an iterative process of prompt improvement, we... | Jaehyung Kim, Yiming Yang |  |
| 680 |  |  [Prompting with Phonemes: Enhancing LLMs' Multilinguality for Non-Latin Script Languages](https://doi.org/10.18653/v1/2025.naacl-long.599) |  | 0 | Multilingual LLMs have achieved remarkable benchmark performance, but we find they continue to underperform on non-Latin script languages across contemporary LLM families. This discrepancy arises from the fact that LLMs are pretrained with orthographic scripts, which are dominated by Latin characters that obscure their shared phonology with non-Latin scripts. We propose leveraging phonemic transcriptions as complementary signals to induce script-invariant representations. Our study demonstrates that integrating phonemic signals improves performance across both non-Latin and Latin languages, with a particularly significant impact on closing the performance gap between the two. Through... | Hoang Nguyen, Julian Salazar, Khyati Mahajan, Masoud Hashemi, Philip S. Yu, Rishabh Maheshwary, Vikas Yadav |  |
| 681 |  |  [SHADES: Towards a Multilingual Assessment of Stereotypes in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.600) |  | 0 | Large Language Models (LLMs) reproduce and exacerbate the social biases present in their training data, and resources to quantify this issue are limited. While research has attempted to identify and mitigate such biases, most efforts have been concentrated around English, lagging the rapid advancement of LLMs in multilingual settings. In this paper, we introduce a new multilingual parallel dataset SHADES to help address this issue, designed for examining culturally-specific stereotypes that may be learned by LLMs. The dataset includes stereotypes from 20 regions around the world and 16 languages, spanning multiple identity categories subject to discrimination worldwide. We demonstrate its... | Adina Yakefu, Amirbek Djanibekov, Anaelia Ovalle, Anne Lauscher, Arjun Subramonian, Aurélie Névéol, Avijit Ghosh, Beatrice Savoldi, Carolin Holtermann, Deepak Tunuguntla, Dragomir Radev, Eliza Szczechla, Emilio Villa Cueva, Esther Ploeger, Giada Pistilli, Giuseppe Attanasio, Hamdan AlAli, Ioana Baldini, Jad Doughman, Jay Gala, Jeremy Qin, Jerry Huang, Jessica Zosa Forde, Jonibek Mansurov, Jordan Clive, Kaiser Sun, Karolina Stanczak, Kaustubh D. Dhole, Kayo Yin, LucieAimée Kaffee, Manan Dey, Maraim Masoud, Marcelo Viridiano, Margaret Mitchell, Mike Zhang, Miruna Clinciu, Nikita Nangia, Nurdaulet Mukhituly, Oskar Van Der Wal, Pieter Delobelle, Ritam Dutt, Roberto L. LopezDavila, Sagnik Mukherjee, Shangrui Nie, Shanya Sharma, Sil Hamilton, Sydney Zink, Tair Djanibekov, Tanmay Laud, Tiago Timponi Torrent, Timm Dill, Vipul Raheja, Xudong Shen, Zeerak Talat |  |
| 682 |  |  [Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion](https://doi.org/10.18653/v1/2025.naacl-long.601) |  | 0 | Speculative decoding has emerged as a widely adopted method to accelerate large language model inference without sacrificing the quality of the model outputs. While this technique has facilitated notable speed improvements by enabling parallel sequence verification, its efficiency remains inherently limited by the reliance on incremental token generation in existing draft models. To overcome this limitation, this paper proposes an adaptation of speculative decoding which uses discrete diffusion models to generate draft sequences. This allows parallelization of both the drafting and verification steps, providing significant speedups to the inference process. Our proposed approach,... | Bhavya Kailkhura, Brian R. Bartoldson, Ferdinando Fioretto, Jacob K. Christopher, Michael Cardei, Tal BenNun |  |
| 683 |  |  [Bayelemabaga: Creating Resources for Bambara NLP](https://doi.org/10.18653/v1/2025.naacl-long.602) |  | 0 | Data curation for under-resource languages enables the development of more accurate and culturally sensitive natural language processing models. However, the scarcity of well-structured multilingual datasets remains a challenge for advancing machine translation in these languages, especially for African languages. This paper focuses on creating high-quality parallel corpora that capture linguistic diversity to address this gap. We introduce Bayelemabaga, the most extensive curated multilingual dataset for machine translation in the Bambara language, the vehicular language of Mali. The dataset consists of 47K Bambara-French parallel sentences curated from 231 data sources, including short... | Allahsera Auguste Tapo, Christopher M. Homan, Kevin Assogba, M. Mustafa Rafique, Marcos Zampieri |  |
| 684 |  |  [Single Ground Truth Is Not Enough: Adding Flexibility to Aspect-Based Sentiment Analysis Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.603) |  | 0 | Aspect-based sentiment analysis (ABSA) is a challenging task of extracting sentiments along with their corresponding aspects and opinion terms from the text.The inherent subjectivity of span annotation makes variability in the surface forms of extracted terms, complicating the evaluation process.Traditional evaluation methods often constrain ground truths (GT) to a single term, potentially misrepresenting the accuracy of semantically valid predictions that differ in surface form.To address this limitation, we propose a novel and fully automated pipeline that expands existing evaluation sets by adding alternative valid terms for aspect and opinion. Our approach facilitates an equitable... | Edward Choi, Hojun Cho, Jaegul Choo, Jiyoung Lee, Sohee Yoon, Soyoung Yang, Won Ik Cho |  |
| 685 |  |  [DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.604) |  | 0 | Multimodal Large Language Models (MLLMs) pose unique safety challenges due to their integration of visual and textual data, thereby introducing new dimensions of potential attacks and complex risk combinations. In this paper, we begin with a detailed analysis aimed at disentangling risks through step-by-step reasoning within multimodal inputs. We find that systematic multimodal risk disentanglement substantially enhances the risk awareness of MLLMs. Via leveraging the strong discriminative abilities of multimodal risk disentanglement, we further introduce DREAM ( Disentangling Risks to Enhance Safety Alignment in MLLMs), a novel approach that enhances safety alignment in MLLMs through... | Chenchen Jing, Hangyu Guo, Hui Huang, Jiaheng Liu, Jianke Zhu, Jianyu Liu, Jihao Gu, Pei Wang, Ranjie Duan, Shilong Li, Xiao Zhang, Xingwei Qu, Xingyuan Bu, Yanan Wu, Yancheng He, Yangguang Li, Yucheng Wang |  |
| 686 |  |  [In-Context Learning with Long-Context Models: An In-Depth Exploration](https://doi.org/10.18653/v1/2025.naacl-long.605) |  | 0 | As model context lengths continue to increase, the number of demonstrations that can be provided in-context approaches the size of entire training datasets. We study the behavior of in-context learning (ICL) at this extreme scale on multiple datasets and models. We show that, for many datasets with large label spaces, performance continues to increase with thousands of demonstrations. We contrast this with example retrieval and finetuning: example retrieval shows excellent performance at low context lengths but has diminished gains with more demonstrations; finetuning is more data hungry than ICL but can exceed long-context ICL performance with additional data. We use the ICL setting to... | Amanda Bertsch, Emily Xiao, Graham Neubig, Jonathan Berant, Maor Ivgi, Matthew R. Gormley, Uri Alon |  |
| 687 |  |  [Preference Consistency Matters: Enhancing Preference Learning in Language Models with Automated Self-Curation of Training Corpora](https://doi.org/10.18653/v1/2025.naacl-long.606) |  | 0 | Inconsistent annotations in training corpora, particularly within preference learning datasets, pose challenges in developing advanced language models. These inconsistencies often arise from variability among annotators and inherent multi-dimensional nature of the preferences. To address these issues, we introduce a self-curation method that preprocesses annotated datasets by leveraging proxy models trained directly on them. Our method enhances preference learning by automatically detecting and selecting consistent annotations. We validate the proposed approach through extensive instruction-following tasks, demonstrating performance improvements of up to 33% across various learning... | JoonHo Lee, JuYoun Son, Juree Seok, Wooseok Jang, YeongDae Kwon |  |
| 688 |  |  [TurtleBench: A Visual Programming Benchmark in Turtle Geometry](https://doi.org/10.18653/v1/2025.naacl-long.607) |  | 0 | Humans have the ability to reason about geometric patterns in images and scenes from a young age. However, developing large multimodal models (LMMs) capable of similar reasoning remains a challenge, highlighting the need for robust evaluation methods to assess these capabilities. We introduce TurtleBench, a benchmark designed to evaluate LMMs’ capacity to interpret geometric patterns—given visual examples, textual instructions, or both—and generate precise code outputs. Inspired by turtle geometry, a notion used to teach children foundational coding and geometric concepts, TurtleBench features tasks with patterned shapes that have underlying algorithmic logic. Our evaluation reveals that... | Sameer Singh, Shayan Doroudi, Sina Rismanchian, Yasaman Razeghi |  |
| 689 |  |  [Automatically Discovering How Misogyny is Framed on Social Media](https://doi.org/10.18653/v1/2025.naacl-long.608) |  | 0 | Misogyny, which is widespread on social media, can be identified not only by recognizing its many forms but also by discovering how misogyny is framed. This paper considers the automatic discovery of misogyny problems and their frames through the Dis-MP&F method, which enables the generation of a data-driven, rich Taxonomy of Misogyny (ToM), offering new insights in the complexity of expressions of misogyny. Furthermore, the Dis-MP&F method, informed by the ToM, is capable of producing very promising results on a misogyny benchmark dataset. | Rakshitha Rao Ailneni, Sanda M. Harabagiu |  |
| 690 |  |  [Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.609) |  | 0 | Faithfulness evaluators based on Large Language Models (LLMs) are often fooled by the fluency of the text and struggle with identifying errors in the summaries, usually leading to high false negative rate. We propose an approach to summary faithfulness evaluation in which multiple LLM-based agents are assigned initial stances (regardless of what their belief might be) and forced to come up with a reason to justify the imposed belief, thus engaging in a multi-round debate to reach an agreement. The uniformly distributed initial assignments here result in a greater diversity of stances leading to more meaningful debates and ultimately more errors identified. Furthermore, by analyzing the... | Amy Wingmei Wong, Han He, Hang Su, Hwanjun Song, Igor Shalyminov, Jake W. Vincent, Jianfeng He, Kyu J. Han, Mahnaz Koupaee, Raphael Shu, Saab Mansour, Yi Nian |  |
| 691 |  |  [ReIFE: Re-evaluating Instruction-Following Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.610) |  | 0 | The automatic evaluation of instruction following typically involves using large language models (LLMs) to assess response quality. However, there is a lack of comprehensive evaluation of these LLM-based evaluators across two dimensions: the base LLMs and the evaluation protocols. Therefore, we present a thorough meta-evaluation of instruction following, including 25 base LLMs and 15 recently proposed evaluation protocols, on 4 human-annotated datasets, assessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows us to identify the best-performing base LLMs and evaluation protocols with a high degree of robustness. Moreover, our evaluation reveals key findings: (1) Base... | Alexander R. Fabbri, Arman Cohan, ChienSheng Wu, Kejian Shi, Peifeng Wang, Shafiq Joty, Yilun Zhao, Yixin Liu |  |
| 692 |  |  [Language Models Predict Empathy Gaps Between Social In-groups and Out-groups](https://doi.org/10.18653/v1/2025.naacl-long.611) |  | 0 | Studies of human psychology have demonstrated that people are more motivated to extend empathy to in-group members than out-group members (Cikara et al., 2011). In this study, we investigate how this aspect of intergroup relations in humans is replicated by LLMs in an emotion intensity prediction task. In this task, the LLM is given a short description of an experience a person had that caused them to feel a particular emotion; the LLM is then prompted to predict the intensity of the emotion the person experienced on a numerical scale. By manipulating the group identities assigned to the LLM’s persona (the “perceiver”) and the person in the narrative (the “experiencer”), we measure how... | Hal Daumé III, Rachel Rudinger, Yu Hou |  |
| 693 |  |  [HARP: Hesitation-Aware Reframing in Transformer Inference Pass](https://doi.org/10.18653/v1/2025.naacl-long.612) |  | 0 | This paper aims to improve the performance of large language models by addressing the variable computational demands in inference steps, where some tokens require more computational resources than others. We present HARP, a simple modification to “off-the-shelf” Transformer forward pass. Drawing from hesitation and the framing effect in decision-making, HARP selectively applies additional computation when the model encounters uncertainty during token generation. Our method mimics human cognitive processes by pausing at difficult decision points and reframing inputs for a different perspective. Unlike other approaches, HARP is model-agnostic, training-free, and easy to implement. We... | Romain Storaï, Seungwon Hwang |  |
| 694 |  |  [JAWAHER: A Multidialectal Dataset of Arabic Proverbs for LLM Benchmarking](https://doi.org/10.18653/v1/2025.naacl-long.613) |  | 0 | Recent advancements in instruction fine-tuning, alignment methods such as reinforcement learning from human feedback (RLHF), and optimization techniques like direct preference optimization (DPO), have significantly enhanced the adaptability of large language models (LLMs) to user preferences. However, despite these innovations, many LLMs continue to exhibit biases toward Western, Anglo-centric, or American cultures, with performance on English data consistently surpassing that of other languages. This reveals a persistent cultural gap in LLMs, which complicates their ability to accurately process culturally rich and diverse figurative language, such as proverbs. To address this, we... | Fakhraddin Alwajih, Muhammad AbdulMageed, Safaa Taher Abdelfadil, Samar Mohamed Magdy, Sang Yun Kwon, Shady Shehata |  |
| 695 |  |  [EmojiPrompt: Generative Prompt Obfuscation for Privacy-Preserving Communication with Cloud-based LLMs](https://doi.org/10.18653/v1/2025.naacl-long.614) |  | 0 | Cloud-based Large Language Models (LLMs) such as ChatGPT have become increasingly integral to daily operations. Nevertheless, they also introduce privacy concerns: firstly, numerous studies underscore the risks to user privacy posed by jailbreaking cloud-based LLMs; secondly, the LLM service providers have access to all user data, which deters individuals from confidently utilizing such services. To address such concerns, we propose a simple yet effective paradigm, \*\*EmojiPrompt\*\*, to protect user privacy. At its core, EmojiPrompt performs generative transformation, obfuscating private data within prompts with linguistic and non-linguistic elements before submitting them to cloud-based... | Lizhou Fan, Mingyu Jin, Sam Lin, Wenyue Hua, Yongfeng Zhang, Zhenting Wang |  |
| 696 |  |  [MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools](https://doi.org/10.18653/v1/2025.naacl-long.615) |  | 0 | Tool-using agents that act in the world need to be both useful and safe. Well-calibrated model confidences can be used to weigh the risk versus reward of potential actions, but prior work shows that many models are poorly calibrated. Inspired by interpretability literature exploring the internals of models, we propose a novel class of model-internal confidence estimators (MICE) to better assess confidence when calling tools. MICE first decodes from each intermediate layer of the language model using logit lens and then computes similarity scores between each layer’s generation and the final output. These features are fed into a learned probabilistic classifier to assess confidence in the... | Benjamin Van Durme, Jason Eisner, Justin Svegliato, Nishant Subramani, Sam Thomson, Yu Su |  |
| 697 |  |  [PAT: Parameter-Free Audio-Text Aligner to Boost Zero-Shot Audio Classification](https://doi.org/10.18653/v1/2025.naacl-long.616) |  | 0 | Audio-Language Models (ALMs) have demonstrated remarkable performance in zero-shot audio classification. In this paper, we introduce PAT (Parameter-free Audio-Text aligner), a simple and training-free method aimed at boosting zero-shot audio classification performance of CLAP-like ALMs. To achieve this, we propose to improve the cross-modal interaction between audio and language modalities by enhancing the representations for both modalities using mutual feedback. Precisely, to enhance textual representations, we propose a prompt ensemble algorithm that automatically selects and combines the most relevant prompts from a datastore with a large pool of handcrafted prompts and weighs them... | Ashish Seth, Dinesh Manocha, Ramaneswaran Selvakumar, Sonal Kumar, Sreyan Ghosh |  |
| 698 |  |  [Language Model Council: Democratically Benchmarking Foundation Models on Highly Subjective Tasks](https://doi.org/10.18653/v1/2025.naacl-long.617) |  | 0 | As Large Language Models (LLMs) continue to evolve, evaluating them remains a persistent challenge. Many recent evaluations use LLMs as judges to score outputs from other LLMs, often relying on a single large model like GPT-4o. However, using a single LLM judge is prone to intra-model bias, and many tasks – such as those related to emotional intelligence, creative writing, and persuasiveness – may be too subjective for a single model to judge fairly. We introduce the Language Model Council (LMC), where a group of LLMs collaborate to create tests, respond to them, and evaluate each other’s responses to produce a ranking in a democratic fashion. Unlike previous approaches that focus on... | Amanda Cercas Curry, Flor Miriam Plaza del Arco, Justin Zhao |  |
| 699 |  |  [SCIURus: Shared Circuits for Interpretable Uncertainty Representations in Language Models](https://doi.org/10.18653/v1/2025.naacl-long.618) |  | 0 | We investigate the mechanistic sources of uncertainty in large language models (LLMs), an area with important implications for language model reliability and trustworthiness. To do so, we conduct a series of experiments designed to identify whether the factuality of generated responses and a model’s uncertainty originate in separate or shared circuits in the model architecture. We approach this question by adapting the well-established mechanistic interpretability techniques of causal tracing and zero-ablation to study the effect of different circuits on LLM generations. Our experiments on eight different models and five datasets, representing tasks predominantly requiring factual recall,... | Arman Cohan, Carter Teplica, Tim G. J. Rudner, Yixin Liu |  |
| 700 |  |  [ProSE: Diffusion Priors for Speech Enhancement](https://doi.org/10.18653/v1/2025.naacl-long.619) |  | 0 | Speech enhancement (SE) is the fundamental task of enhancing the clarity and quality of speech in the presence of non-stationary additive noise. While deterministic deep learning models have been commonly employed for SE, recent research indicates that generative models, such as denoising diffusion probabilistic models (DDPMs), have shown promise. However, different from speech generation, SE has a strong constraint to generate results in accordance with the underlying ground-truth signal. Additionally, for a wide variety of applications, SE systems need to be employed in real-time, and traditional diffusion models (DMs) requiring many iterations of a large model during inference are... | Anton Jeran Ratnarajah, Chandra Kiran Reddy Evuru, Dinesh Manocha, Ramani Duraiswami, Sonal Kumar, Sreyan Ghosh, Utkarsh Tyagi |  |
| 701 |  |  [Mastering the Craft of Data Synthesis for CodeLLMs](https://doi.org/10.18653/v1/2025.naacl-long.620) |  | 0 | Large language models (LLMs) have shown impressive performance in code understanding and generation, making coding tasks a key focus for researchers due to their practical applications and value as a testbed for LLM evaluation. Data synthesis and filtering techniques have been widely adopted and shown to be highly effective in this context. In this paper, we present a focused survey and taxonomy of these techniques, emphasizing recent advancements. We highlight key challenges, explore future research directions, and offer practical guidance for new researchers entering the field. | Cong Duy Vu Hoang, Don Dharmasiri, Duc Thien Nguyen, Duy Vu, Gioacchino Tangari, Krishnaram Kenthapadi, Long Duong, Mahdi Kazemi Moghaddam, Mark Johnson, Meng Chen, Omid Nezami, Philip Arthur, Qianyu Feng, Thanh Vu, YuHeng Hong, YuanFang Li |  |
| 702 |  |  [ParaICL: Towards Parallel In-Context Learning](https://doi.org/10.18653/v1/2025.naacl-long.621) |  | 0 | Large language models (LLMs) have become the norm in natural language processing (NLP), excelling in few-shot in-context learning (ICL) with their remarkable abilities. Nonetheless, the success of ICL largely hinges on the choice of few-shot demonstration examples, making the selection process increasingly crucial. Existing methods have delved into optimizing the quantity and semantic similarity of these examples to improve ICL performances. However, our preliminary experiments indicate that the effectiveness of ICL is limited by the length of the input context. Moreover, varying combinations of few-shot demonstration examples can significantly boost accuracy across different test samples.... | Lidong Bing, Shafiq Joty, Xingxuan Li, XuanPhi Nguyen |  |
| 703 |  |  [CausalEval: Towards Better Causal Reasoning in Language Models](https://doi.org/10.18653/v1/2025.naacl-long.622) |  | 0 | Causal reasoning (CR) is a crucial aspect of intelligence, essential for problem-solving, decision-making, and understanding the world. While language models (LMs) can generate rationales for their outputs, their ability to reliably perform causal reasoning remains uncertain, often falling short in tasks requiring a deep understanding of causality. In this paper, we introduce CausalEval, a comprehensive review of research aimed at enhancing LMs for causal reasoning, coupled with an empirical evaluation of current models and methods. We categorize existing methods based on the role of LMs: either as reasoning engines or as helpers providing knowledge or data to traditional CR methods,... | Dawei Li, Delin Chen, Liangming Pan, Longxuan Yu, Qingyang Wu, Siheng Xiong, Xiaoze Liu, Zhikai Chen |  |
| 704 |  |  [Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense](https://doi.org/10.18653/v1/2025.naacl-long.623) |  | 0 | As large language models (LLMs) are increasingly deployed in diverse applications, including chatbot assistants and code generation, aligning their behavior with safety and ethical standards has become paramount. However, jailbreak attacks, which exploit vulnerabilities to elicit unintended or harmful outputs, threaten LLMs safety significantly. In this paper, we introduce Layer-AdvPatcher, a novel methodology designed to defend against jailbreak attacks by utilizing an unlearning strategy to patch specific layers within LLMs through self-augmented datasets. Our insight is that certain layer(s), tend to produce affirmative tokens when faced with harmful prompts. By identifying these layers... | Bhavya Kailkhura, Hengrui Gu, Jie Peng, Kaixiong Zhou, Meijun Gao, Shuhang Lin, Tianlong Chen, Wenyue Hua, Yang Ouyang |  |
| 705 |  |  [DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot Question Answering in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.624) |  | 0 | While Large Language Models (LLMs) excel in zero-shot Question Answering (QA), they tend to expose biases in their internal knowledge when faced with socially sensitive questions, leading to a degradation in performance. Existing zero-shot methods are efficient but failto consider context and prevent bias propagation in the answers. To address this, we propose \*DeCAP\*, a method for debiasing LLMs usingContext-Adaptive Prompt Generation. \*DeCAP\* leverages a \*Question Ambiguity Detection\* to take appropriate debiasing actions based on the context and a \*Neutral Answer Guidance Generation\* to suppress the LLMs make objective judgments about the context, minimizing thepropagation of... | JeeHyong Lee, Suyoung Bae, YunSeok Choi |  |
| 706 |  |  [Reward-Guided Tree Search for Inference Time Alignment of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.625) |  | 0 | Inference-time computation methods enhance the performance of Large Language Models (LLMs) by leveraging additional computational resources to achieve superior results. Common techniques, such as Best-of-N sampling, Majority Voting, and variants of tree-search algorithm have proven to be effective in boosting the performance of LLMs. These approaches strategically trade increased computational resource for improved model responses. In this work, we proposed DARWIN, an inference-time alignment method that leverage the guidance of a reward model to achieve alignment through reward-guided tree search. Empirical evidences indicates that our method outperform other inference-time alignment... | Ambuj Mehrish, ChiaYu Hung, Navonil Majumder, Soujanya Poria |  |
| 707 |  |  [Typographic Attacks in a Multi-Image Setting](https://doi.org/10.18653/v1/2025.naacl-long.626) |  | 0 | Large Vision-Language Models (LVLMs) are susceptible to typographic attacks, which are misclassifications caused by an attack text that is added to an image. In this paper, we introduce a multi-image setting for studying typographic attacks, broadening the current emphasis of the literature on attacking individual images. Specifically, our focus is on attacking image sets without repeating the attack query. Such non-repeating attacks are stealthier, as they are more likely to evade a gatekeeper than attacks that repeat the same attack text. We introduce two attack strategies for the multi-image setting, leveraging the difficulty of the target image, the strength of the attack text, and... | Martha A. Larson, Xiaomeng Wang, Zhengyu Zhao |  |
| 708 |  |  [Tonguescape: Exploring Language Models Understanding of Vowel Articulation](https://doi.org/10.18653/v1/2025.naacl-long.627) |  | 0 | Vowels are primarily characterized by tongue position. Humans have discovered these features of vowel articulation through their own experience and explicit objective observation such as using MRI. With this knowledge and our experience, we can explain and understand the relationship between tongue positions and vowels, and this knowledge is helpful for language learners to learn pronunciation. Since language models (LMs) are trained on a large amount of data that includes linguistic and medical fields, our preliminary studies indicate that an LM is able to explain the pronunciation mechanisms of vowels. However, it is unclear whether multi-modal LMs, such as vision LMs, align textual... | Haruki Sakajo, Hidetaka Kamigaito, Taro Watanabe, Yusuke Sakai |  |
| 709 |  |  [CoRAC: Integrating Selective API Document Retrieval with Question Semantic Intent for Code Question Answering](https://doi.org/10.18653/v1/2025.naacl-long.628) |  | 0 | Automatic code question answering aims to generate precise answers to questions about code by analyzing code snippets. To provide an appropriate answer, it is necessary to accurately understand the relevant part of the code and correctly interpret the intent of the question. However, in real-world scenarios, the questioner often provides only a portion of the code along with the question, making it challenging to find an answer. The responder should be capable of providing a suitable answer using such limited information. We propose a knowledge-based framework, CoRAC, an automatic code question responder that enhances understanding through selective API document retrieval and question... | CheolWon Na, JeeHyong Lee, YunSeok Choi |  |
| 710 |  |  [Pipeline Analysis for Developing Instruct LLMs in Low-Resource Languages: A Case Study on Basque](https://doi.org/10.18653/v1/2025.naacl-long.629) |  | 0 | Large language models (LLMs) are typically optimized for resource-rich languages like English, exacerbating the gap between high-resource and underrepresented languages. This work presents a detailed analysis of strategies for developing a model capable of following instructions in a low-resource language, specifically Basque, by focusing on three key stages: pre-training, instruction tuning, and alignment with human preferences. Our findings demonstrate that continual pre-training with a high-quality Basque corpus of around 600 million words improves natural language understanding (NLU) of the foundational model by over 12 points. Moreover, instruction tuning and human preference... | Ander Corral, Ixak Sarasua, Xabier Saralegi |  |
| 711 |  |  [How to Make LLMs Forget: On Reversing In-Context Knowledge Edits](https://doi.org/10.18653/v1/2025.naacl-long.630) |  | 0 | In-context knowledge editing (IKE) enables efficient modification of large language model (LLM) outputs without parameter changes and at zero-cost. However, it can be misused to manipulate responses opaquely, e.g., insert misinformation or offensive content. Such malicious interventions could be incorporated into high-level wrapped APIs where the final input prompt is not shown to end-users. To address this issue, we investigate the detection and reversal of IKE-edits. First, we demonstrate that IKE-edits can be detected with high accuracy (F1 > 80%) using only the top-10 output probabilities of the next token, even in a black-box setting, e.g. proprietary LLMs with limited output... | Christin Seifert, Jörg Schlötterer, Paul Youssef, Zhixue Zhao |  |
| 712 |  |  [PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian](https://doi.org/10.18653/v1/2025.naacl-long.631) |  | 0 | Large language models predominantly reflect Western cultures, largely due to the dominance of English-centric training data. This imbalance presents a significant challenge, as LLMs are increasingly used across diverse contexts without adequate evaluation of their cultural competence in non-English languages, including Persian. To address this gap, we introduce PerCul, a carefully constructed dataset designed to assess the sensitivity of LLMs toward Persian culture. PerCul features story-based, multiple-choice questions that capture culturally nuanced scenarios.Unlike existing benchmarks, PerCul is curated with input from native Persian annotators to ensure authenticity and to prevent the... | Azadeh Shakery, Erfan Moosavi Monazzah, Mohammad Taher Pilehvar, Vahid Rahimzadeh, Yadollah Yaghoobzadeh |  |
| 713 |  |  [Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.632) |  | 0 | Large language models (LLMs) are increasingly recognized for their exceptional generative capabilities and versatility across various tasks. However, the high inference costs associated with these models have not received adequate attention, particularly when compared to the focus on training costs in existing research. In response to this gap, our study conducts a comprehensive benchmarking of LLM inference energy across a wide range of NLP tasks, where we analyze the impact of different models, tasks, prompts, and system-related factors on inference energy. Specifically, our experiments reveal several interesting insights, including strong correlation of inference energy with output... | Janardan Misra, Niloy Ganguly, Paramita Koley, Saptarshi Ghosh, Soham Poddar |  |
| 714 |  |  [CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science Research Repositories](https://doi.org/10.18653/v1/2025.naacl-long.633) |  | 0 | The increasing complexity of computer science research projects demands more effective tools for deploying code repositories. Large Language Models (LLMs), such as Anthropic Claude and Meta Llama, have demonstrated significant advancements across various fields of computer science research, including the automation of diverse software engineering tasks. To evaluate the effectiveness of LLMs in handling complex code development tasks of research projects, particularly for NLP/CV/AI/ML/DM topics, we introduce CSR-Bench, a benchmark for Computer Science Research projects. This benchmark assesses LLMs from various aspects including accuracy, efficiency, and deployment script quality, aiming to... | Davor Golac, Luyang Kong, Runhui Wang, Wei Wang, Yijia Xiao |  |
| 715 |  |  [SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data](https://doi.org/10.18653/v1/2025.naacl-long.634) |  | 0 | In various natural language processing (NLP) tasks, fine-tuning Pre-trained Language Models (PLMs) often leads to the issue of spurious correlations, which negatively impacts performance, particularly when dealing with out-of-distribution data.To address this problem, we propose \*\*SALAD\*\* (\*\*S\*\*tructure \*\*A\*\*ware and \*\*L\*\*LM-driven \*\*A\*\*ugmented \*\*D\*\*ata), a novel approach designed to enhance model robustness and generalization by generating structure-aware and counterfactually augmented data for contrastive learning.Our method leverages a tagging-based approach to generate structure-aware positive samples and utilizes large language models (LLMs) to generate... | Hyojun Kim, JeeHyong Lee, Suyoung Bae, YunSeok Choi |  |
| 716 |  |  [Rationale-Guided Retrieval Augmented Generation for Medical Question Answering](https://doi.org/10.18653/v1/2025.naacl-long.635) |  | 0 | Large language models (LLM) hold significant potential for applications in biomedicine, but they struggle with hallucinations and outdated knowledge.While retrieval-augmented generation (RAG) is generally employed to address these issues, it also has its own set of challenges: (1) LLMs are vulnerable to irrelevant or unhelpful context, (2) medical queries are often not well-targeted for helpful information, and (3) retrievers are prone to bias toward the specific source corpus they were trained on. In this study, we present RAG2 (RAtionale-Guided RAG), a new framework for enhancing the reliability of RAG in biomedical contexts. RAG2 incorporates three key innovations: a small filtering... | Chanwoong Yoon, Hyeon Hwang, Hyunjae Kim, Jaewoo Kang, Jiwoong Sohn, Mujeen Sung, Sihyeon Park, Yein Park |  |
| 717 |  |  [Prototype Conditioned Generative Replay for Continual Learning in NLP](https://doi.org/10.18653/v1/2025.naacl-long.636) |  | 0 | Generative replay has proven effective in addressing the catastrophic forgetting issue of continual learning (CL) in natural language processing (NLP). However, relying on a single task-specific token or prompt often falls short in generating pseudo-samples that accurately reflect the true data distribution. This leads to issues of semantic inconsistency and scale inconsistency.To tackle these challenges, we propose a Prototype Conditioned Generative Replay (PCGR) method, which enhances generative reply by incorporating task-level statistics through a Prototype Conditioned Variational Autoencoder (PCVAE).Specifically, task-level embedding statistics are stored as prototypes for each old... | Min Zeng, Xi Chen |  |
| 718 |  |  [KODIS: A Multicultural Dispute Resolution Dialogue Corpus](https://doi.org/10.18653/v1/2025.naacl-long.637) |  | 0 |  | James Hale, Jeanne M. Brett, Jonathan Gratch, Kushal Chawla, Sushrita Rakshit |  |
