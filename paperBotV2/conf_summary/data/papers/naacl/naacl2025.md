# NAACL2025

## 会议论文列表

本会议共有 718 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Complete Chess Games Enable LLM Become A Chess Master](https://doi.org/10.18653/v1/2025.naacl-short.1) |  | 0 | Large language models (LLM) have shown remarkable abilities in text generation, question answering, language translation, reasoning and many other tasks. It continues to advance rapidly and is becoming increasingly influential in various fields, from technology and business to education and entertainment. Despite LLM’s success in multiple areas, its ability to play abstract games, such as chess, is underexplored. Chess-playing requires the language models to output legal and reasonable moves... | Yinqi Zhang, Xintian Han, Haolong Li, Kedi Chen, Shaohui Lin |  |
| 2 |  |  [Predicting the Target Word of Game-playing Conversations using a Low-Rank Dialect Adapter for Decoder Models](https://doi.org/10.18653/v1/2025.naacl-short.2) |  | 0 | Dialect adapters that improve the performance of LLMs for NLU tasks on certain sociolects/dialects/national varieties (‘dialects’ for the sake of brevity) have been reported for encoder models. In this paper, we extend the idea of dialect adapters to decoder models in our architecture called LoRDD. Using MD-3, a publicly available dataset of word game-playing conversations between dialectal speakers, our task is Target Word Prediction (TWP) from a masked conversation. LoRDD combines task... | Dipankar Srirag, Aditya Joshi, Jacob Eisenstein |  |
| 3 |  |  [ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with LLM-based Chatbots](https://doi.org/10.18653/v1/2025.naacl-short.3) |  | 0 | The rise of LLMs has deflected a growing portion of human-computer interactions towards LLM-based chatbots.The remarkable abilities of these models allow users to interact using long, diverse natural language text covering a wide range of topics and styles. Phrasing these messages is a time and effort consuming task, calling for an autocomplete solution to assist users. We present \*\*ChaI-TeA\*\*: \*\*Cha\*\*t \*\*I\*\*n\*\*te\*\*raction \*\*A\*\*utocomplete; An autocomplete evaluation... | Shani Goren, Oren Kalinsky, Tomer Stav, Yuri Rapoport, Yaron Fairstein, Ram Yazdi, Nachshon Cohen, Alexander Libov, Guy Kushilevitz |  |
| 4 |  |  [Cross-Lingual Transfer Learning for Speech Translation](https://doi.org/10.18653/v1/2025.naacl-short.4) |  | 0 | There has been increasing interest in building multilingual foundation models for NLP and speech research. This paper examines how to expand the speech translation capability of these models with restricted data. Whisper, a speech foundation model with strong performance on speech recognition and English translation, is used as the example model. Using speech-to-speech retrieval to analyse the audio representations generated by the encoder, we show that utterances from different languages are... | Rao Ma, Mengjie Qian, Yassir Fathullah, Siyuan Tang, Mark J. F. Gales, Kate M. Knill |  |
| 5 |  |  [Reverse Question Answering: Can an LLM Write a Question so Hard (or Bad) that it Can't Answer?](https://doi.org/10.18653/v1/2025.naacl-short.5) |  | 0 | Question answering (QA)—giving correct answers to questions—is a popular task, but we test \*\*reverse question answering (RQA)\*\*: for an input answer, give a question with that answer. Past work tests QA and RQA separately, but we test them jointly, comparing their difficulty, aiding benchmark design, and checking reasoning consistency. We run 16 LLMs on QA and RQA with trivia questions/answers, revealing: 1) Versus RQA, LLMs are much less accurate in RQA for numerical answers, but slightly... | Nishant Balepur, Feng Gu, Abhilasha Ravichander, Shi Feng, Jordan Lee BoydGraber, Rachel Rudinger |  |
| 6 |  |  [Personalized Help for Optimizing Low-Skilled Users' Strategy](https://doi.org/10.18653/v1/2025.naacl-short.6) |  | 0 | AIs can beat humans in game environments; however, how helpful those agents are to human remains understudied. We augment Cicero, a natural language agent that demonstrates superhuman performance in Diplomacy, to generate both move and message advice based on player intentions. A dozen Diplomacy games with novice and experienced players, with varying advice settings, show that some of the generated advice is beneficial. It helps novices compete with experienced players and in some instances... | Feng Gu, Wichayaporn Wongkamjan, Jordan Lee BoydGraber, Jonathan K. Kummerfeld, Denis Peskoff, Jonathan May |  |
| 7 |  |  [Local Prompt Optimization](https://doi.org/10.18653/v1/2025.naacl-short.7) |  | 0 | In recent years, the use of prompts to guide the output of Large Language Models have increased dramatically. However, even the best of experts struggle to choose the correct words to stitch up a prompt for the desired task. To solve this, LLM driven prompt optimization emerged as an important problem. Existing prompt optimization methods optimize a prompt globally, where in all the prompt tokens have to be optimized over a large vocabulary while solving a complex task. The large optimization... | Yash Jain, Vishal Chowdhary |  |
| 8 |  |  [Cross-lingual Transfer of Reward Models in Multilingual Alignment](https://doi.org/10.18653/v1/2025.naacl-short.8) |  | 0 | Reinforcement learning with human feedback (RLHF) is shown to largely benefit from precise reward models (RMs). However, recent studies in reward modeling schemes are skewed towards English, limiting the applicability of RLHF in multilingual alignments. In this work, we investigate the cross-lingual transfer of RMs trained in diverse languages, primarily from English. Our experimental results demonstrate the strong cross-lingual transfer of English RMs, exceeding target language RMs by 3~4%... | Jiwoo Hong, Noah Lee, Rodrigo MartínezCastaño, César Rodríguez, James Thorne |  |
| 9 |  |  [Inference-Time Selective Debiasing to Enhance Fairness in Text Classification Models](https://doi.org/10.18653/v1/2025.naacl-short.9) |  | 0 | We propose selective debiasing – an inference-time safety mechanism designed to enhance the overall model quality in terms of prediction performance and fairness, especially in scenarios where retraining the model is impractical. The method draws inspiration from selective classification, where at inference time, predictions with low quality, as indicated by their uncertainty scores, are discarded. In our approach, we identify the potentially biased model predictions and, instead of discarding... | Gleb Kuzmin, Neemesh Yadav, Ivan V. Smirnov, Timothy Baldwin, Artem Shelmanov |  |
| 10 |  |  [Automatic Evaluation of Healthcare LLMs Beyond Question-Answering](https://doi.org/10.18653/v1/2025.naacl-short.10) |  | 0 | Current Large Language Models (LLMs) benchmarks are often based on open-ended or close-ended QA evaluations, avoiding the requirement of human labor. Close-ended measurements evaluate the factuality of responses but lack expressiveness. Open-ended capture the model’s capacity to produce discourse responses but are harder to assess for correctness. These two approaches are commonly used, either independently or together, though their relationship remains poorly understood. This work is focused... | Anna AriasDuart, Pablo Agustin MartinTorres, Daniel Hinjos, Pablo BernabeuPerez, Lucia UrcelayGanzabal, Marta GonzalezMallo, Ashwin Kumar Gururajan, Enrique LopezCuena, Sergio ÁlvarezNapagao, Dario GarciaGasulla |  |
| 11 |  |  [STRUX: An LLM for Decision-Making with Structured Explanations](https://doi.org/10.18653/v1/2025.naacl-short.11) |  | 0 | Countless decisions shape our lives, and it is crucial to understand the how and why behind them. In this paper, we introduce a new LLM decision-making framework called STRUX, which enhances LLM decision-making by providing structured explanations. These include favorable and adverse facts related to the decision, along with their respective strengths. STRUX begins by distilling lengthy information into a concise table of key facts. It then employs a series of self-reflection steps to determine... | Yiming Lu, Yebowen Hu, Hassan Foroosh, Wei Jin, Fei Liu |  |
| 12 |  |  [Improving Vietnamese-English Cross-Lingual Retrieval for Legal and General Domains](https://doi.org/10.18653/v1/2025.naacl-short.12) |  | 0 | Document retrieval plays a crucial role in numerous question-answering systems, yet research has concentrated on the general knowledge domain and resource-rich languages like English. In contrast, it remains largely underexplored in low-resource languages and cross-lingual scenarios within specialized domain knowledge such as legal. We present a novel dataset designed for cross-lingual retrieval between Vietnamese and English, which not only covers the general domain but also extends to the... | Toan Ngoc Nguyen, Nam Le Hai, Nguyen Doan Hieu, Dai An Nguyen, Linh Ngo Van, Thien Huu Nguyen, Sang Dinh |  |
| 13 |  |  [Computational Discovery of Chiasmus in Ancient Religious Text](https://doi.org/10.18653/v1/2025.naacl-short.13) |  | 0 | Chiasmus, a debated literary device in Biblical texts, has captivated mystics while sparking ongoing scholarly discussion. In this paper, we introduce the first computational approach to systematically detect chiasmus within Biblical passages. Our method leverages neural embeddings to capture lexical and semantic patterns associated with chiasmus, applied at multiple levels of textual granularity (half-verses, verses). We also involve expert annotators to review a subset of the detected... | Hope McGovern, Hale Sirin, Tom Lippincott |  |
| 14 |  |  [Characterizing the Effects of Translation on Intertextuality using Multilingual Embedding Spaces](https://doi.org/10.18653/v1/2025.naacl-short.14) |  | 0 | Rhetorical devices are difficult to translate, but they are crucial to the translation of literary documents. We investigate the use of multilingual embedding spaces to characterize the preservation of intertextuality, one common rhetorical device, across human and machine translation. To do so, we use Biblical texts, which are both full of intertextual references and are highly translated works. We provide a metric to characterize intertextuality at the corpus level and provide a quantitative... | Hope McGovern, Hale Sirin, Tom Lippincott |  |
| 15 |  |  [LLM2: Let Large Language Models Harness System 2 Reasoning](https://doi.org/10.18653/v1/2025.naacl-short.15) |  | 0 | Large language models (LLMs) have exhibited impressive capabilities across a myriad of tasks, yet they occasionally yield undesirable outputs. We posit that these limitations are rooted in the foundational autoregressive architecture of LLMs, which inherently lacks mechanisms for differentiating between desirable and undesirable results. Drawing inspiration from the dual-process theory of human cognition, we introduce LLM2, a novel framework that combines an LLM (System 1) with a process-based... | Cheng Yang, Chufan Shi, Siheng Li, Bo Shui, Yujiu Yang, Wai Lam |  |
| 16 |  |  [Context-Efficient Retrieval with Factual Decomposition](https://doi.org/10.18653/v1/2025.naacl-short.16) |  | 0 | There has recently been considerable interest in incorporating information retrieval into large language models (LLMs). Retrieval from a dynamically expanding external corpus of text allows a model to incorporate current events and can be viewed as a form of episodic memory. Here we demonstrate that pre-processing the external corpus into semi-structured “atomic facts” makes retrieval more efficient. More specifically, we demonstrate that our particular form of atomic facts improves performance... | Yanhong Li, David Yunis, David McAllester, Jiawei Zhou |  |
| 17 |  |  [Sports and Women's Sports: Gender Bias in Text Generation with Olympic Data](https://doi.org/10.18653/v1/2025.naacl-short.17) |  | 0 | Large Language Models (LLMs) have been shown to be biased in prior work, as they generate text that is in line with stereotypical views of the world or that is not representative of the viewpoints and values of historically marginalized demographic groups. In this work, we propose using data from parallel men’s and women’s events at the Olympic Games to investigate different forms of gender bias in language models. We define three metrics to measure bias, and find that models are consistently... | Laura Biester |  |
| 18 |  |  [Alligators All Around: Mitigating Lexical Confusion in Low-resource Machine Translation](https://doi.org/10.18653/v1/2025.naacl-short.18) |  | 0 | Current machine translation (MT) systems for low-resource languages have a particular failure mode: When translating words in a given domain, they tend to confuse words within that domain. So, for example, “lion” might be translated as “alligator”, and “orange” might be rendered as “purple.” We propose a recall-based metric for measuring this problem and show that the problem exists in 122 low-resource languages. We then show that this problem can be mitigated by using a large language model... | Elizabeth Nielsen, Isaac Caswell, Jiaming Luo, Colin Cherry |  |
| 19 |  |  [PROM: Pivoted and Regulated Optimization for Multilingual Instruction Learning](https://doi.org/10.18653/v1/2025.naacl-short.19) |  | 0 | Large language models (LLMs) have become standard for natural language generation tasks, with instruction-tuning enhancing their capabilities. However, the lack of instruction-tuning datasets in languages other than English limits their application to diverse languages. To address this, researchers have adapted English-centric LLMs to other languages by appending English tuning data with its translated pair, from which we observe negative interference between the two. To resolve this, our... | Jaeseong Lee, Seungwon Hwang, Hojin Lee, Yunju Bak, Changmin Lee |  |
| 20 |  |  [Concept-Reversed Winograd Schema Challenge: Evaluating and Improving Robust Reasoning in Large Language Models via Abstraction](https://doi.org/10.18653/v1/2025.naacl-short.20) |  | 0 | While Large Language Models (LLMs) have showcased remarkable proficiency in reasoning, there is still a concern about hallucinations and unreliable reasoning issues due to semantic associations and superficial logical chains. To evaluate the extent to which LLMs perform robust reasoning instead of relying on superficial logical chains, we propose a new evaluation dataset, the Concept-Reversed Winograd Schema Challenge (CR-WSC), based on the famous Winograd Schema Challenge (WSC) dataset. By... | Kaiqiao Han, Tianqing Fang, Zhaowei Wang, Yangqiu Song, Mark Steedman |  |
| 21 |  |  [Defense against Prompt Injection Attacks via Mixture of Encodings](https://doi.org/10.18653/v1/2025.naacl-short.21) |  | 0 | Large Language Models (LLMs) have emerged as a dominant approach for a wide range of NLP tasks, with their access to external information further enhancing their capabilities. However, this introduces new vulnerabilities, known as prompt injection attacks, where external content embeds malicious instructions that manipulate the LLM’s output. Recently, the Base64 defense has been recognized as one of the most effective methods for reducing success rate of prompt injection attacks. Despite its... | Ruiyi Zhang, David Sullivan, Kyle Jackson, Pengtao Xie, Mei Chen |  |
| 22 |  |  [Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers](https://doi.org/10.18653/v1/2025.naacl-short.22) |  | 0 | AI Safety Moderation (ASM) classifiers are designed to moderate content on social media platforms and to serve as guardrails that prevent Large Language Models (LLMs) from being fine-tuned on unsafe inputs. Owing to their potential for disparate impact, it is crucial to ensure that these classifiers: (1) do not unfairly classify content belonging to users from minority groups as unsafe compared to those from majority groups and (2) that their behavior remains robust and consistent across... | Akshit Achara, Anshuman Chhabra |  |
| 23 |  |  [CoRAG: Collaborative Retrieval-Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-short.23) |  | 0 | Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive tasks, especially under few-shot learning constraints. We introduce CoRAG, a framework extending RAG to collaborative settings, where clients jointly train a shared model using a collaborative passage store. To evaluate CoRAG, we introduce CRAB, a benchmark for collaborative homogeneous open-domain question answering. Our experiments demonstrate that CoRAG consistently outperforms both parametric collaborative learning... | Aashiq Muhamed, Mona T. Diab, Virginia Smith |  |
| 24 |  |  [Is It Navajo? Accurate Language Detection for Endangered Athabaskan Languages](https://doi.org/10.18653/v1/2025.naacl-short.24) |  | 0 | Endangered languages, such as Navajo—the most widely spoken Native American language—are significantly underrepresented in contemporary language technologies, exacerbating the challenges of their preservation and revitalization. This study evaluates Google’s Language Identification (LangID) tool, which does not currently support any Native American languages. To address this, we introduce a random forest classifier trained on Navajo and twenty erroneously suggested languages by LangID. Despite... | Ivory Yang, Weicheng Ma, Chunhui Zhang, Soroush Vosoughi |  |
| 25 |  |  [Don't Touch My Diacritics](https://doi.org/10.18653/v1/2025.naacl-short.25) |  | 0 | The common practice of preprocessing text before feeding it into NLP models introduces many decision points which have unintended consequences on model performance. In this opinion piece, we focus on the handling of diacritics in texts originating in many languages and scripts. We demonstrate, through several case studies, the adverse effects of inconsistent encoding of diacritized characters and of removing diacritics altogether. We call on the community to adopt simple but necessary steps... | Kyle Gorman, Yuval Pinter |  |
| 26 |  |  [Pretrained Image-Text Models are Secretly Video Captioners](https://doi.org/10.18653/v1/2025.naacl-short.26) |  | 0 | Developing video captioning models is computationally expensive. The dynamic nature of video also complicates the design of multimodal models that can effectively caption these sequences. However, we find that by using minimal computational resources and without complex modifications to address video dynamics, an image-based model can be repurposed to outperform several specialised video captioning systems. Our adapted model demonstrates top-tier performance on major benchmarks, ranking 2nd on... | Chunhui Zhang, Yiren Jian, Zhongyu Ouyang, Soroush Vosoughi |  |
| 27 |  |  [Reverse Modeling in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-short.27) |  | 0 | Humans are accustomed to reading and writing in a forward manner, and this natural bias extends to text understanding in auto-regressive large language models (LLMs). This paper investigates whether LLMs, like humans, struggle with reverse modeling, specifically with reversed text inputs. We found that publicly available pre-trained LLMs cannot understand such inputs. However, LLMs trained from scratch with both forward and reverse texts can understand them equally well during inference across... | Sicheng Yu, Yuanchen Xu, Cunxiao Du, Yanying Zhou, Minghui Qiu, Qianru Sun, Hao Zhang, Jiawei Wu |  |
| 28 |  |  [Preserving Multilingual Quality While Tuning Query Encoder on English Only](https://doi.org/10.18653/v1/2025.naacl-short.28) |  | 0 | A query encoder of a dual passage retrieval system can be tuned for specific types of queries or domains, while the precomputed and stored documents representations are kept intact. Switching from one query encoder to another when needed is easily feasible, unlike overhauling the embeddings of a whole knowledge base. In this work we raise a question: Can the generic, original qualities of the encoder be preserved or at least left not too degraded when it is tuned on a narrow domain? We... | Oleg Vasilyev, Randy Sawaya, John Bohannon |  |
| 29 |  |  [Using Contextually Aligned Online Reviews to Measure LLMs' Performance Disparities Across Language Varieties](https://doi.org/10.18653/v1/2025.naacl-short.29) |  | 0 | A language can have different varieties. These varieties can affect the performance of natural language processing (NLP) models, including large language models (LLMs), which are often trained on data from widely spoken varieties. This paper introduces a novel and cost-effective approach to benchmark model performance across language varieties. We argue that international online review platforms,such as Booking.com, can serve as effective data sources for constructing datasets that capture... | Zixin Tang, ChiehYang Huang, TsungChi Li, Ho Yin Sam Ng, HenHsen Huang, TingHao Kenneth Huang |  |
| 30 |  |  [Towards Federated Low-Rank Adaptation of Language Models with Rank Heterogeneity](https://doi.org/10.18653/v1/2025.naacl-short.30) |  | 0 | Low-rank adaptation (LoRA) offers an efficient alternative to full-weight adaptation in federated fine-tuning of language models, significantly reducing computational costs. By adjusting ranks for each client, federated LoRA enables flexible resource allocation. However, we observe that heterogeneous ranks among clients lead to unstable performance. Our analysis attributes this instability to the conventional zero-padding aggregation strategy, which dilutes information from high-rank clients... | Yuji Byun, Jaeho Lee |  |
| 31 |  |  [Related Knowledge Perturbation Matters: Rethinking Multiple Pieces of Knowledge Editing in Same-Subject](https://doi.org/10.18653/v1/2025.naacl-short.31) |  | 0 |  | Zenghao Duan, Wenbin Duan, Zhiyi Yin, Yinghan Shen, Shaoling Jing, Jie Zhang, Huawei Shen, Xueqi Cheng |  |
| 32 |  |  [STEP: Staged Parameter-Efficient Pre-training for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-short.32) |  | 0 | Pre-training large language models (LLMs) faces significant memory challenges due to the large size of model weights. We introduce STaged parameter-Efficient Pre-training (STEP), which integrates parameter-efficient tuning techniques with model growth. We conduct experiments on pre-training LLMs of various sizes and demonstrate that STEP achieves up to a 53.9% reduction in maximum memory requirements compared to vanilla pre-training while maintaining equivalent performance. Furthermore, we show... | Kazuki Yano, Takumi Ito, Jun Suzuki |  |
| 33 |  |  [Language Models Encode Numbers Using Digit Representations in Base 10](https://doi.org/10.18653/v1/2025.naacl-short.33) |  | 0 | Large language models (LLMs) frequently make errors when handling even simple numerical problems, such as comparing two small numbers. A natural hypothesis is that these errors stem from how LLMs represent numbers, and specifically, whether their representations of numbers capture their numeric values. We tackle this question from the observation that LLM errors on numerical tasks are often distributed across the digits of the answer rather than normally around its numeric value. Through a... | Amit A. Levy, Mor Geva |  |
| 34 |  |  [A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference](https://doi.org/10.18653/v1/2025.naacl-short.34) |  | 0 | Recently, sharing key-value (KV) cache across layers has been found effective in efficient inference of large language models (LLMs). To systematically investigate different techniques of cross-layer KV sharing, we propose a unified framework that covers several recent methods and their novel variants. We conduct comprehensive experiments on all the configurations of the framework, evaluating their generation throughput and performance in language modeling and downstream tasks. We find that... | You Wu, Haoyi Wu, Kewei Tu |  |
| 35 |  |  [AMPS: ASR with Multimodal Paraphrase Supervision](https://doi.org/10.18653/v1/2025.naacl-short.35) |  | 0 | Spontaneous or conversational multilingual speech presents many challenges for state-of-the-art automatic speech recognition (ASR) systems. In this work, we present a new technique AMPS, that augments a multilingual multimodal ASR system with paraphrase-based supervision for improved conversational ASR in multiple languages, including Hindi, Marathi, Malayalam, Kannada, and Nyanja. We use paraphrases of the reference transcriptions as additional supervision while training the multimodal ASR... | Abhishek Gupta, Amruta Parulekar, Sameep Chattopadhyay, Preethi Jyothi |  |
| 36 |  |  [Taxi1500: A Dataset for Multilingual Text Classification in 1500 Languages](https://doi.org/10.18653/v1/2025.naacl-short.36) |  | 0 | While broad-coverage multilingual natural language processing tools have been developed, a significant portion of the world’s over 7000 languages are still neglected. One reason is the lack of evaluation datasets that cover a diverse range of languages, particularly those that are low-resource or endangered. To address this gap, we present a large-scale text classification dataset encompassing 1504 languages many of which have otherwise limited or no annotated data. This dataset is constructed... | Chunlan Ma, Ayyoob Imani, Haotian Ye, Renhao Pei, Ehsaneddin Asgari, Hinrich Schütze |  |
| 37 |  |  [GameTox: A Comprehensive Dataset and Analysis for Enhanced Toxicity Detection in Online Gaming Communities](https://doi.org/10.18653/v1/2025.naacl-short.37) |  | 0 | The prevalence of toxic behavior in online gaming communities necessitates robust detection methods to ensure user safety. We introduce GameTox, a novel dataset comprising 53K game chat utterances annotated for toxicity detection through intent classification and slot filling. This dataset captures the complex relationship between user intent and specific linguistic features that contribute to toxic interactions. We extensively analyze the dataset to uncover key insights into the nature of... | Usman Naseem, Shuvam Shiwakoti, Siddhant Bikram Shah, Surendrabikram Thapa, Qi Zhang |  |
| 38 |  |  [FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern LLMs](https://doi.org/10.18653/v1/2025.naacl-short.38) |  | 0 | Summarization is one of the most common tasks performed by large language models (LLMs), especially in applications like Retrieval-Augmented Generation (RAG). However, existing evaluations of hallucinations in LLM-generated summaries, and evaluations of hallucination detection models both suffer from a lack of diversity and recency in the LLM and LLM families considered. This paper introduces FaithBench, a summarization hallucination benchmark comprising challenging hallucinations made by 10... | Forrest Sheng Bao, Miaoran Li, Renyi Qu, Ge Luo, Erana Wan, Yujia Tang, Weisi Fan, Manveer Singh Tamber, Suleman Kazi, Vivek Sourabh, Mike Qi, Ruixuan Tu, Chenyu Xu, Matthew Gonzales, Ofer Mendelevitch, Amin Ahmad |  |
| 39 |  |  [Debate-Feedback: A Multi-Agent Framework for Efficient Legal Judgment Prediction](https://doi.org/10.18653/v1/2025.naacl-short.39) |  | 0 | The use of AI in legal analysis and prediction (LegalAI) has gained attention, with past research focusing on retrieval-based methods and fine-tuning large models. However, these approaches often require large datasets and underutilize the capabilities of modern large language models (LLMs). In this paper, inspired by the debate phase of real courtroom trials, we propose a novel legal judgment prediction model based on the Debate-Feedback architecture, which integrates LLM multi-agent debate... | Xi Chen, Mao Mao, Shuo Li, Haotian Shangguan |  |
| 40 |  |  [Great Memory, Shallow Reasoning: Limits of kNN-LMs](https://doi.org/10.18653/v1/2025.naacl-short.40) |  | 0 | K-nearest neighbor language models (kNN-LMs), which integrate retrieval with next-word prediction, have demonstrated strong performance in language modeling as well as some downstream NLP benchmarks. These results have led researchers to argue that models trained on poor quality or outdated data could perform well by employing a kNN extension that has access to a higher-quality datastore. In this work, we ask whether this improved ability to recall information really translates into downstream... | Shangyi Geng, Wenting Zhao, Alexander M. Rush |  |
| 41 |  |  [Repetition Neurons: How Do Language Models Produce Repetitions?](https://doi.org/10.18653/v1/2025.naacl-short.41) |  | 0 | This paper introduces repetition neurons, which can be regarded as “skill neurons” responsible for the repetition problem in text generation tasks. These neurons are progressively activated more strongly as repetition continues, indicating that they perceive repetition as a task to copy the previous context repeatedly, similar to in-context learning. We identify these repetition neurons by comparing activation values before and after the onset of repetition in texts generated by recent... | Tatsuya Hiraoka, Kentaro Inui |  |
| 42 |  |  [STAR: Spectral Truncation and Rescale for Model Merging](https://doi.org/10.18653/v1/2025.naacl-short.42) |  | 0 | Model merging is an efficient way of obtaining a multi-task model from several pretrained models without further fine-tuning, and it has gained attention in various domains, including natural language processing (NLP). Despite the efficiency, a key challenge in model merging is the seemingly inevitable decrease in task performance as the number of models increases. In this paper, we propose \*\*S\*\*pectral \*\*T\*\*runcation \*\*A\*\*nd \*\*R\*\*escale (STAR) that aims at mitigating “merging... | YuAng Lee, ChingYun Ko, Tejaswini Pedapati, IHsin Chung, MiYen Yeh, PinYu Chen |  |
| 43 |  |  [Task-driven Layerwise Additive Activation Intervention](https://doi.org/10.18653/v1/2025.naacl-short.43) |  | 0 | Modern language models (LMs) have significantly advanced generative modeling in natural language processing (NLP). Despite their success, LMs often struggle with adaptation to new contexts in real-time applications. A promising approach to task adaptation is activation intervention, which steers the LMs’ generation process by identifying and manipulating the activations. However, existing interventions rely heavily on heuristic rules or require many prompt inputs to determine effective... | Hieu Trung Nguyen, Bao Nguyen, Binh Nguyen, Viet Anh Nguyen |  |
| 44 |  |  [Scaling Multi-Document Event Summarization: Evaluating Compression vs. Full-Text Approaches](https://doi.org/10.18653/v1/2025.naacl-short.44) |  | 0 | Automatically summarizing large text collections is a valuable tool for document research, with applications in journalism, academic research, legal work, and many other fields. In this work, we contrast two classes of systems for large-scale multi-document summarization (MDS): compression and full-text. Compression-based methods use a multi-stage pipeline and often lead to lossy summaries. Full-text methods promise a lossless summary by relying on recent advances in long-context reasoning. To... | Adithya Pratapa, Teruko Mitamura |  |
| 45 |  |  [Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models](https://doi.org/10.18653/v1/2025.naacl-short.45) |  | 0 | Large Vision Language Models (LVLMs) often suffer from object hallucination, which undermines their reliability. Surprisingly, we find that simple object-based visual prompting—overlaying visual cues (e.g., bounding box, circle) on images—can significantly mitigate such hallucination; however, different visual prompts (VPs) vary in effectiveness. To address this, we propose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify optimal VPs that enhance LVLM responses without... | Sangmin Woo, Kang Zhou, Yun Zhou, Shuai Wang, Sheng Guan, Haibo Ding, Lin Lee Cheong |  |
| 46 |  |  [A Layered Debating Multi-Agent System for Similar Disease Diagnosis](https://doi.org/10.18653/v1/2025.naacl-short.46) |  | 0 | Distinguishing between extremely similar diseases is a critical and challenging aspect of clinical decision-making. Traditional classification, contrastive learning, and Large Language Models (LLMs) based methods fail to detect the subtle clues necessary for differentiation. This task demands complex reasoning and a variety of tools to identify minor differences and make informed decisions. This paper probes a novel framework that leverages LLMs and a multi-agent system to achieve accurate... | Yutian Zhao, Huimin Wang, Yefeng Zheng, Xian Wu |  |
| 47 |  |  [The Geometry of Numerical Reasoning: Language Models Compare Numeric Properties in Linear Subspaces](https://doi.org/10.18653/v1/2025.naacl-short.47) |  | 0 | This paper investigates whether large language models (LLMs) utilize numerical attributes encoded in a low-dimensional subspace of theembedding space when answering questions involving numeric comparisons, e.g., Was Cristiano born before Messi? We first identified,using partial least squares regression, these subspaces, which effectively encode the numerical attributes associated with the entities in comparison prompts. Further, we demonstrate causality, by intervening in these subspaces to... | Ahmed Oumar ElShangiti, Tatsuya Hiraoka, Hilal AlQuabeh, Benjamin Heinzerling, Kentaro Inui |  |
| 48 |  |  [AlignFreeze: Navigating the Impact of Realignment on the Layers of Multilingual Models Across Diverse Languages](https://doi.org/10.18653/v1/2025.naacl-short.48) |  | 0 | Realignment techniques are often employed to enhance cross-lingual transfer in multilingual language models, still, they can sometimes degrade performance in languages that differ significantly from the fine-tuned source language. This paper introduces AlignFreeze, a method that freezes either the layers’ lower half or upper half during realignment. Through controlled experiments on 4 tasks, 3 models, and in 35 languages, we find that realignment affects all the layers but can be the most... | Steve Bakos, David Guzmán, Riddhi More, Kelly Chutong Li, Félix Gaschi, EnShiun Annie Lee |  |
| 49 |  |  [FLIQA-AD: a Fusion Model with Large Language Model for Better Diagnose and MMSE Prediction of Alzheimer's Disease](https://doi.org/10.18653/v1/2025.naacl-short.49) |  | 0 | Tracking a patient’s cognitive status early in the onset of the disease provides an opportunity to diagnose and intervene in Alzheimer’s disease (AD). However, relying solely on magnetic resonance imaging (MRI) images with traditional classification and regression models may not fully extract finer-grained information. This study proposes a multi-task Fusion Language Image Question Answering model (FLIQA-AD) to perform AD identification and Mini Mental State Examination (MMSE) prediction.... | Junhao Chen, Zhiyuan Ding, Yan Liu, Xiangzhu Zeng, Ling Wang |  |
| 50 |  |  [Transform Retrieval for Textual Entailment in RAG](https://doi.org/10.18653/v1/2025.naacl-short.50) |  | 0 | In this paper, we introduce Transform Retrieval, a novel approach aimed at improving Textual Entailment Retrieval within the framework of Retrieval-Augmented Generation (RAG). While RAG has shown promise in enhancing Large Language Models by retrieving relevant documents to extract specific knowledge or mitigate hallucination, current retrieval methods often prioritize relevance without ensuring the retrieved documents semantically support answering the queries. Transform Retrieval addresses... | Quan Guo, Xin Liang |  |
| 51 |  |  [How do Multimodal Foundation Models Encode Text and Speech? An Analysis of Cross-Lingual and Cross-Modal Representations](https://doi.org/10.18653/v1/2025.naacl-short.51) |  | 0 | Multimodal foundation models aim to create a unified representation space that abstracts away from surface features like language syntax or modality differences. To investigate this, we study the internal representations of three recent models, analyzing the model activations from semantically equivalent sentences across languages in the text and speech modalities. Our findings reveal that: 1) Cross-modal representations converge over model layers, except in the initial layers specialized at... | Hyunji Lee, Danni Liu, Supriti Sinhamahapatra, Jan Niehues |  |
| 52 |  |  [Explore the Reasoning Capability of LLMs in the Chess Testbed](https://doi.org/10.18653/v1/2025.naacl-short.52) |  | 0 | Reasoning is a central capability of human intelligence. In recent years, with the advent of large-scale datasets, pretrained large language models have emerged with new capabilities, including reasoning. However, these models still struggle with long-term, complex reasoning tasks, such as playing chess. Based on the observation that expert chess players employ a dual approach combining long-term strategic play with short-term tactical play along with language explanation, we propose improving... | Shu Wang, Lei Ji, Renxi Wang, Wenxiao Zhao, Haokun Liu, Yifan Hou, Ying Nian Wu |  |
| 53 |  |  [Auto-Cypher: Improving LLMs on Cypher generation via LLM-supervised generation-verification framework](https://doi.org/10.18653/v1/2025.naacl-short.53) |  | 0 | Graph databases like Neo4j are gaining popularity for handling complex, interconnected data, over traditional relational databases in modeling and querying relationships. While translating natural language into SQL queries is well-researched, generating Cypher queries for Neo4j remains relatively underexplored. In this work, we present an automated, LLM Supervised, pipeline to generate high quality synthetic data for Text2Cypher. Our Cypher data generation pipeline introduces... | Aman Tiwari, Shiva Krishna Reddy Malay, Vikas Yadav, Masoud Hashemi, Sathwik Tejaswi Madhusudhan |  |
| 54 |  |  [Leveraging Moment Injection for Enhanced Semi-supervised Natural Language Inference with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-short.54) |  | 0 | Natural Language Inference (NLI) is crucial for evaluating models’ Natural Language Understanding (NLU) and reasoning abilities. The development of NLI, in part, has been driven by the creation of large datasets, which require significant human effort. This has spurred interest in semi-supervised learning (SSL) that leverages both labeled and unlabeled data. However, the absence of hypotheses and class labels in NLI tasks complicates SSL. Prior work has used class-specific fine-tuned large... | Seo Yeon Park |  |
| 55 |  |  [A Fair Comparison without Translationese: English vs. Target-language Instructions for Multilingual LLMs](https://doi.org/10.18653/v1/2025.naacl-short.55) |  | 0 | Most large language models are multilingual instruction executors. Prior studies suggested that English instructions are more effective than target-language instructions even for non-English tasks; however, these studies often use datasets and instructions translated from English, which introduce biases known as translationese, hindering an unbiased comparison. To address this issue, we conduct a fair comparison between English and target-language instructions by eliminating translationese... | Taisei Enomoto, Hwichan Kim, Zhousi Chen, Mamoru Komachi |  |
| 56 |  |  [Evaluating Multimodal Generative AI with Korean Educational Standards](https://doi.org/10.18653/v1/2025.naacl-short.56) |  | 0 | This paper presents the Korean National Educational Test Benchmark (KoNET), a new benchmark designed to evaluate Multimodal Generative AI Systems using Korean national educational tests. KoNET comprises four exams: the Korean Elementary General Educational Development Test (KoEGED), Middle (KoMGED), High (KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are renowned for their rigorous standards and diverse questions, facilitating a comprehensive analysis of AI performance... | Sanghee Park, Geewook Kim |  |
| 57 |  |  [ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large Multimodal Models with Visual Programming Challenges](https://doi.org/10.18653/v1/2025.naacl-short.57) |  | 0 | Recent advancements in large multimodal models (LMMs) have showcased impressive code generation capabilities, primarily evaluated through image-to-code benchmarks. However, these benchmarks are limited to specific visual programming scenarios where the logic reasoning and the multimodal understanding capacities are split apart. To fill this gap, we propose ScratchEval, a novel benchmark designed to evaluate the visual programming reasoning ability of LMMs. ScratchEval is based on Scratch, a... | Rao Fu, Ziyang Luo, Hongzhan Lin, Zhen Ye, Jing Ma |  |
| 58 |  |  [Interpret and Control Dense Retrieval with Sparse Latent Features](https://doi.org/10.18653/v1/2025.naacl-short.58) |  | 0 | Dense embeddings deliver strong retrieval performance but often lack interpretability and controllability. This paper introduces a novel approach using sparse autoencoders (SAE) to interpret and control dense embeddings via the learned latent sparse features. Our key contribution is the development of a retrieval-oriented contrastive loss, which ensures the sparse latent features remain effective for retrieval tasks and thus meaningful to interpret. Experimental results demonstrate that both... | Hao Kang, Tevin Wang, Chenyan Xiong |  |
| 59 |  |  [DART: An AIGT Detector using AMR of Rephrased Text](https://doi.org/10.18653/v1/2025.naacl-short.59) |  | 0 | As large language models (LLMs) generate more human-like texts, concerns about the side effects of AI-generated texts (AIGT) have grown. So, researchers have developed methods for detecting AIGT. However, two challenges remain. First, the performance of detecting black-box LLMs is low because existing models focus on probabilistic features. Second, most AIGT detectors have been tested on a single-candidate setting, which assumes that we know the origin of an AIGT and which may deviate from the... | Hyeonchu Park, Byungjun Kim, Bugeun Kim |  |
| 60 |  |  [Scaling Graph-Based Dependency Parsing with Arc Vectorization and Attention-Based Refinement](https://doi.org/10.18653/v1/2025.naacl-short.60) |  | 0 | We propose a novel architecture for graph-based dependency parsing that explicitly constructs vectors, from which both arcs and labels are scored. Our method addresses key limitations of the standard two-pipeline approach by unifying arc scoring and labeling into a single network, reducing scalability issues caused by the information bottleneck and lack of parameter sharing. Additionally, our architecture overcomes limited arc interactions with transformer layers to efficiently simulate... | Nicolas Floquet, Joseph Le Roux, Nadi Tomeh, Thierry Charnois |  |
| 61 |  |  [Language Models "Grok" to Copy](https://doi.org/10.18653/v1/2025.naacl-short.61) |  | 0 | We examine the pre-training dynamics of language models, focusing on their ability to copy text from preceding context—a fundamental skill for various LLM applications, including in-context learning (ICL) and retrieval-augmented generation (RAG). We propose a novel perspective that Transformer-based language models develop copying abilities similarly to grokking, which refers to sudden generalization on test set long after the model fit to the training set. Our experiments yield three... | Ang Lv, Ruobing Xie, Xingwu Sun, Zhanhui Kang, Rui Yan |  |
| 62 |  |  [Evaluating LLMs for Quotation Attribution in Literary Texts: A Case Study of LLaMa3](https://doi.org/10.18653/v1/2025.naacl-short.62) |  | 0 | Large Language Models (LLMs) have shown promising results in a variety of literary tasks, often using complex memorized details of narration and fictional characters. In this work, we evaluate the ability of Llama-3 at attributing utterances of direct-speech to their speaker in novels. The LLM shows impressive results on a corpus of 28 novels, surpassing published results with ChatGPT and encoder-based baselines by a large margin. We then validate these results by assessing the impact of book... | Gaspard Michel, Elena V. Epure, Romain Hennequin, Christophe Cerisara |  |
| 63 |  |  [Beyond Literal Token Overlap: Token Alignability for Multilinguality](https://doi.org/10.18653/v1/2025.naacl-short.63) |  | 0 | Previous work has considered token overlap, or even similarity of token distributions, as predictors for multilinguality and cross-lingual knowledge transfer in language models. However, these very literal metrics assign large distances to language pairs with different scripts, which can nevertheless show good cross-linguality. This limits the explanatory strength of token overlap for knowledge transfer between language pairs that use distinct scripts or follow different orthographic... | Katharina Hämmerl, Tomasz Limisiewicz, Jindrich Libovický, Alexander Fraser |  |
| 64 |  |  [IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark for LLMs](https://doi.org/10.18653/v1/2025.naacl-short.64) |  | 0 | Recent evaluations of LLMs on coreference resolution have revealed that traditional output formats and evaluation metrics do not fully capture the models’ referential understanding. To address this, we introduce IdentifyMe, a new benchmark for mention resolution presented in a multiple-choice question (MCQ) format, commonly used for evaluating LLMs. IdentifyMe features long narratives and employs heuristics to exclude easily identifiable mentions, creating a more challenging task. The benchmark... | Kawshik Manikantan, Makarand Tapaswi, Vineet Gandhi, Shubham Toshniwal |  |
| 65 |  |  [kNN Retrieval for Simple and Effective Zero-Shot Multi-speaker Text-to-Speech](https://doi.org/10.18653/v1/2025.naacl-short.65) |  | 0 | While recent zero-shot multi-speaker text-to-speech (TTS) models achieve impressive results, they typically rely on extensive transcribed speech datasets from numerous speakers and intricate training pipelines. Meanwhile, self-supervised learning (SSL) speech features have emerged as effective intermediate representations for TTS. Further, SSL features from different speakers that are linearly close share phonetic information while maintaining individual speaker identity. In this study, we... | Karl El Hajal, Ajinkya Kulkarni, Enno Hermann, Mathew MagimaiDoss |  |
| 66 |  |  [CORD: Balancing COnsistency and Rank Distillation for Robust Retrieval-Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-short.66) |  | 0 | With the adoption of retrieval-augmented generation (RAG), large language models (LLMs) are expected to ground their generation to the retrieved contexts. Yet, this is hindered by position bias of LLMs, failing to evenly attend to all contexts. Previous work has addressed this by synthesizing contexts with perturbed positions of gold segment, creating a position-diversified train set. We extend this intuition to propose consistency regularization with augmentation and distillation. First, we... | Youngwon Lee, Seungwon Hwang, Daniel F. Campos, Filip Gralinski, Zhewei Yao, Yuxiong He |  |
| 67 |  |  [GraphLSS: Integrating Lexical, Structural, and Semantic Features for Long Document Extractive Summarization](https://doi.org/10.18653/v1/2025.naacl-short.67) |  | 0 | Heterogeneous graph neural networks have recently gained attention for long document summarization, modeling the extraction as a node classification task. Although effective, these models often require external tools or additional machine learning models to define graph components, producing highly complex and less intuitive structures. We present GraphLSS, a heterogeneous graph construction for long document extractive summarization, incorporating Lexical, Structural, and Semantic features. It... | Margarita Bugueño, Hazem Abou Hamdan, Gerard de Melo |  |
| 68 |  |  [Step-by-Step Fact Verification System for Medical Claims with Explainable Reasoning](https://doi.org/10.18653/v1/2025.naacl-short.68) |  | 0 | Fact verification (FV) aims to assess the veracity of a claim based on relevant evidence. The traditional approach for automated FV includes a three-part pipeline relying on short evidence snippets and encoder-only inference models. More recent approaches leverage the multi-turn nature of LLMs to address FV as a step-by-step problem where questions inquiring additional context are generated and answered until there is enough information to make a decision. This iterative method makes the... | Juraj Vladika, Ivana Hacajová, Florian Matthes |  |
| 69 |  |  [Developing multilingual speech synthesis system for Ojibwe, Mi'kmaq, and Maliseet](https://doi.org/10.18653/v1/2025.naacl-short.69) |  | 0 | We present lightweight flow matching multilingual text-to-speech (TTS) systems for Ojibwe, Mi’kmaq, and Maliseet, three Indigenous languages in North America. Our results show that training a multilingual TTS model on three typologically similar languages can improve the performance over monolingual models, especially when data are scarce. Attention-free architectures are highly competitive with self-attention architecture with higher memory efficiency. Our research provides technical... | Shenran Wang, Changbing Yang, Mike Parkhill, Chad Quinn, Christopher Hammerly, Jian Zhu |  |
| 70 |  |  [Bottom-Up Synthesis of Knowledge-Grounded Task-Oriented Dialogues with Iteratively Self-Refined Prompts](https://doi.org/10.18653/v1/2025.naacl-short.70) |  | 0 | Training conversational question-answering (QA) systems demands a substantial amount of in-domain data, which is often scarce in practice. A common solution to this challenge is to generate synthetic data. Traditional methods typically follow a top-down approach, where a large language model (LLM) generates multi-turn dialogues from a broad prompt. While this method produces coherent conversations, it offers limited fine-grained control over the content and is susceptible to hallucinations. We... | Kun Qian, Maximillian Chen, Siyan Li, Arpit Sharma, Zhou Yu |  |
| 71 |  |  [Sociodemographic Prompting is Not Yet an Effective Approach for Simulating Subjective Judgments with LLMs](https://doi.org/10.18653/v1/2025.naacl-short.71) |  | 0 | Human judgments are inherently subjective and are actively affected by personal traits such as gender and ethnicity. While Large LanguageModels (LLMs) are widely used to simulate human responses across diverse contexts, their ability to account for demographic differencesin subjective tasks remains uncertain. In this study, leveraging the POPQUORN dataset, we evaluate nine popular LLMs on their abilityto understand demographic differences in two subjective judgment tasks: politeness and... | Huaman Sun, Jiaxin Pei, Minje Choi, David Jurgens |  |
| 72 |  |  [Identifying Power Relations in Conversations using Multi-Agent Social Reasoning](https://doi.org/10.18653/v1/2025.naacl-short.72) |  | 0 | Large language models (LLMs) struggle in social science domains, where critical thinking and human-level inference are crucial. In this work, we propose a multi-agent social reasoning framework that leverages the generative and reasoning capabilities of LLMs to generate and evaluate reasons from multiple perspectives grounded in social science theories, and construct a factor graph for inference. Experimental results on understanding power dynamics in conversations show that our method... | Zhaoqing Wu, Dan Goldwasser, Maria Leonor Pacheco, Leora Morgenstern |  |
| 73 |  |  [Examining Spanish Counseling with MIDAS: a Motivational Interviewing Dataset in Spanish](https://doi.org/10.18653/v1/2025.naacl-short.73) |  | 0 | Cultural and language factors significantly influence counseling, but Natural Language Processing research has not yet examined whether the findings of conversational analysis for counseling conducted in English apply to other languages. This paper presents a first step towards this direction. We introduce MIDAS (Motivational Interviewing Dataset in Spanish), a counseling dataset created from public video sources that contains expert annotations for counseling reflections and questions. Using... | Aylin Gunal, Bowen Yi, John Piette, Rada Mihalcea, Verónica PérezRosas |  |
| 74 |  |  [Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes](https://doi.org/10.18653/v1/2025.naacl-short.74) |  | 0 | Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a... | Isabel O. Gallegos, Ryan Aponte, Ryan A. Rossi, Joe Barrow, Md. Mehrab Tanjim, Tong Yu, Hanieh Deilamsalehy, Ruiyi Zhang, Sungchul Kim, Franck Dernoncourt, Nedim Lipka, Deonna M. Owens, Jiuxiang Gu |  |
| 75 |  |  [EqualizeIR: Mitigating Linguistic Biases in Retrieval Models](https://doi.org/10.18653/v1/2025.naacl-short.75) |  | 0 | This study finds that existing information retrieval (IR) models show significant biases based on the linguistic complexity of input queries, performing well on linguistically simpler (or more complex) queries while underperforming on linguistically more complex (or simpler) queries.To address this issue, we propose EqualizeIR, a framework to mitigate linguistic biases in IR models. EqualizeIR uses a linguistically biased weak learner to capture linguistic biases in IR datasets and then trains... | Jiali Cheng, Hadi Amiri |  |
| 76 |  |  [Do Audio-Language Models Understand Linguistic Variations?](https://doi.org/10.18653/v1/2025.naacl-short.76) |  | 0 | Open-vocabulary audio language models (ALMs), like Contrastive Language Audio Pretraining (CLAP), represent a promising new paradigm for audio-text retrieval using natural language queries. In this paper, for the first time, we perform controlled experiments on various benchmarks to show that existing ALMs struggle to generalize to linguistic variations in textual queries. To address this issue, we propose RobustCLAP, a novel and compute-efficient technique to learn audio-language... | Ramaneswaran Selvakumar, Sonal Kumar, Hemant Kumar Giri, Nishit Anand, Ashish Seth, Sreyan Ghosh, Dinesh Manocha |  |
| 77 |  |  [Giving the Old a Fresh Spin: Quality Estimation-Assisted Constrained Decoding for Automatic Post-Editing](https://doi.org/10.18653/v1/2025.naacl-short.77) |  | 0 | Automatic Post-Editing (APE) systems often struggle with over-correction, where unnecessary modifications are made to a translation, diverging from the principle of minimal editing. In this paper, we propose a novel technique to mitigate over-correction by incorporating word-level Quality Estimation (QE) information during the decoding process. This method is architecture-agnostic, making it adaptable to any APE system, regardless of the underlying model or training approach. Our experiments on... | Sourabh Dattatray Deoghare, Diptesh Kanojia, Pushpak Bhattacharyya |  |
| 78 |  |  [RuleR: Improving LLM Controllability by Rule-based Data Recycling](https://doi.org/10.18653/v1/2025.naacl-short.78) |  | 0 | Large language models (LLMs) still lack delicate controllability over their responses, which is critical to enhancing their performance and the user experience. However, curating supervised fine-tuning (SFT) datasets to improve LLM controllability usually relies on human experts or proprietary LLMs, which requires additional costs. To bridge this gap, we propose Rule-based Data Recycling (RuleR), a data augmentation method incorporating multiple constraints into the original data samples... | Ming Li, Han Chen, Chenguang Wang, Dang Nguyen, Dianqi Li, Tianyi Zhou |  |
| 79 |  |  [MixRevDetect: Towards Detecting AI-Generated Content in Hybrid Peer Reviews](https://doi.org/10.18653/v1/2025.naacl-short.79) |  | 0 | The growing use of large language models (LLMs) in academic peer review poses significant challenges, particularly in distinguishing AI-generated content from human-written feedback. This research addresses the problem of identifying AI-generated peer review comments, which are crucial to maintaining the integrity of scholarly evaluation. Prior research has primarily focused on generic AI-generated text detection or on estimating the fraction of peer reviews that may be AI-generated, often... | Sandeep Kumar, Samarth Garg, Sagnik Sengupta, Tirthankar Ghosal, Asif Ekbal |  |
| 80 |  |  [DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie Character-Aware Discourse Graph](https://doi.org/10.18653/v1/2025.naacl-short.80) |  | 0 | Summarizing movie screenplays presents a unique set of challenges compared to standard document summarization. Screenplays are not only lengthy, but also feature a complex interplay of characters, dialogues, and scenes, with numerous direct and subtle relationships and contextual nuances that are difficult for machine learning models to accurately capture and comprehend. Recent attempts at screenplay summarization focus on fine-tuning transformer-based pre-trained models, but these models often... | Maitreya Prafulla Chitale, Uday Bindal, Rajakrishnan Rajkumar, Rahul Mishra |  |
| 81 |  |  [Capturing Human Cognitive Styles with Language: Towards an Experimental Evaluation Paradigm](https://doi.org/10.18653/v1/2025.naacl-short.81) |  | 0 | While NLP models often seek to capture cognitive states via language, the validity of predicted states is determined by comparing them to annotations created without access the cognitive states of the authors. In behavioral sciences, cognitive states are instead measured via experiments. Here, we introduce an experiment-based framework for evaluating language-based cognitive style models against human behavior. We explore the phenomenon of decision making, and its relationship to the linguistic... | Vasudha Varadarajan, Syeda Mahwish, Xiaoran Liu, Julia Buffolino, Christian C. Luhmann, Ryan L. Boyd, H. Andrew Schwartz |  |
| 82 |  |  [Understanding Figurative Meaning through Explainable Visual Entailment](https://doi.org/10.18653/v1/2025.naacl-long.1) |  | 0 | Large Vision-Language Models (VLMs) have demonstrated strong capabilities in tasks requiring a fine-grained understanding of literal meaning in images and text, such as visual question-answering or visual entailment. However, there has been little exploration of the capabilities of these models when presented with images and captions containing figurative meaning, such as metaphors or humor. To close this gap, we propose a new task framing the figurative meaning understanding problem as an... | Arkadiy Saakyan, Shreyas Kulkarni, Tuhin Chakrabarty, Smaranda Muresan |  |
| 83 |  |  [Benchmarking Distributional Alignment of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.2) |  | 0 | Language models (LMs) are increasingly used as simulacra for people, yet their ability to match the distribution of views of a specific demographic group and be distributionally aligned remains uncertain. This notion of distributional alignment is complex, as there is significant variation in the types of attributes that are simulated. Prior works have underexplored the role of three critical variables—the question domain, steering method, and distribution expression method—which motivates our... | Nicole Meister, Carlos Guestrin, Tatsunori Hashimoto |  |
| 84 |  |  [World Models with Hints of Large Language Models for Goal Achieving](https://doi.org/10.18653/v1/2025.naacl-long.3) |  | 0 | Reinforcement learning struggles in the face of long-horizon tasks and sparse goals due to the difficulty in manual reward specification. While existing methods address this by adding intrinsic rewards, they may fail to provide meaningful guidance in long-horizon decision-making tasks with large state and action spaces, lacking purposeful exploration. Inspired by human cognition, we propose a new multi-modal model-based RL approach named Dreaming with Large Language Models (DLLM). DLLM... | Zeyuan Liu, Ziyu Huan, Xiyao Wang, Jiafei Lyu, Jian Tao, Xiu Li, Furong Huang, Huazhe Xu |  |
| 85 |  |  [CogLM: Tracking Cognitive Development of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.4) |  | 0 | Piaget’s Theory of Cognitive Development (PTC) posits that the development of cognitive levels forms the foundation for human learning across various abilities. As Large Language Models (LLMs) have recently shown remarkable abilities across a wide variety of tasks, we are curious about the cognitive levels of current LLMs: to what extent they have developed and how this development has been achieved. To this end, we construct a benchmark CogLM (Cognitive Ability Evaluation for Language Model)... | Xinglin Wang, Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Boyuan Pan, Heda Wang, Yao Hu, Kan Li |  |
| 86 |  |  [Improving and Assessing the Fidelity of Large Language Models Alignment to Online Communities](https://doi.org/10.18653/v1/2025.naacl-long.5) |  | 0 | Large language models (LLMs) have shown promise in representing individuals and communities, offering new ways to study complex social dynamics. However, effectively aligning LLMs with specific human groups and systematically assessing the fidelity of the alignment remains a challenge. This paper presents a robust framework for aligning LLMs with online communities via instruction-tuning and comprehensively evaluating alignment across various aspects of language, including authenticity,... | Minh Duc Chu, Zihao He, Rebecca Dorn, Kristina Lerman |  |
| 87 |  |  [Improving Retrospective Language Agents via Joint Policy Gradient Optimization](https://doi.org/10.18653/v1/2025.naacl-long.6) |  | 0 | In recent research advancements within the community, large language models (LLMs) have sparked great interest in creating autonomous agents. However, current prompt-based agents often heavily rely on large-scale LLMs. Meanwhile, although fine-tuning methods significantly enhance the capabilities of smaller LLMs, the fine-tuned agents often lack the potential for self-reflection and self-improvement. To address these challenges, we introduce a novel agent framework named RetroAct, which is a... | Xueyang Feng, Bo Lan, Quanyu Dai, Lei Wang, Jiakai Tang, Xu Chen, Zhenhua Dong, JiRong Wen |  |
| 88 |  |  [CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases](https://doi.org/10.18653/v1/2025.naacl-long.7) |  | 0 | Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval and MBPP, but struggle with handling entire code repositories. This challenge has prompted research on enhancing LLM-codebase interaction at a repository scale. Current solutions rely on similarity-based retrieval or manual tools and APIs, each with notable drawbacks. Similarity-based retrieval often has low recall in complex tasks, while manual tools and APIs are typically task-specific and require expert knowledge,... | Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Fei Wang, Michael Qizhe Shieh, Wenmeng Zhou |  |
| 89 |  |  [Instantly Learning Preference Alignment via In-context DPO](https://doi.org/10.18653/v1/2025.naacl-long.8) |  | 0 | Human Preference Alignment (HPA) can assist large language models (LLMs) to generate safe content. Due to the heavy cost of fine-tuning, tuning-free methods have emerged, typically modifying LLM decoding via post-processing. In this paper, we propose a novel and effective approach for HPA in a tuning-free way, named In-Context Direct Preference Optimization (ICDPO). We first rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the... | Feifan Song, Yuxuan Fan, Xin Zhang, Peiyi Wang, Houfeng Wang |  |
| 90 |  |  [ALTER: Augmentation for Large-Table-Based Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.9) |  | 0 |  | Han Zhang, Yuheng Ma, Hanfang Yang |  |
| 91 |  |  [What the #?\*!: Disentangling Hate Across Target Identities](https://doi.org/10.18653/v1/2025.naacl-long.10) |  | 0 | Hate speech (HS) classifiers do not perform equally well in detecting hateful expressions towards different target identities. They also demonstrate systematic biases in predicted hatefulness scores. Tapping on two recently proposed functionality test datasets for HS detection, we quantitatively analyze the impact of different factors on HS prediction. Experiments on popular industrial and academic models demonstrate that HS detectors assign a higher hatefulness score merely based on the... | Yiping Jin, Leo Wanner, Aneesh Moideen Koya |  |
| 92 |  |  [MAD Speech: Measures of Acoustic Diversity of Speech](https://doi.org/10.18653/v1/2025.naacl-long.11) |  | 0 | Generative spoken language models produce speech in a wide range of voices, prosody, and recording conditions, seemingly approaching the diversity of natural speech. However, the extent to which generated speech is acoustically diverse remains unclear due to a lack of appropriate metrics. We address this gap by developing lightweight metrics of acoustic diversity, which we collectively refer to as MAD Speech. We focus on measuring five facets of acoustic diversity: voice, gender, emotion,... | Matthieu Futeral, Andrea Agostinelli, Marco Tagliasacchi, Neil Zeghidour, Eugene Kharitonov |  |
| 93 |  |  [The Russian-focused embedders' exploration: ruMTEB benchmark and Russian embedding model design](https://doi.org/10.18653/v1/2025.naacl-long.12) |  | 0 | Embedding models play a crucial role in Natural Language Processing (NLP) by creating text embeddings used in various tasks such as information retrieval and assessing semantic text similarity. This paper focuses on research related to embedding models in the Russian language. It introduces a new Russian-focused embedding model called ru-en-RoSBERTa and the ruMTEB benchmark, the Russian version extending the Massive Text Embedding Benchmark (MTEB). Our benchmark includes seven categories of... | Artem Snegirev, Maria Tikhonova, Anna Maksimova, Alena Fenogenova, Aleksandr Abramov |  |
| 94 |  |  [PRACTIQ: A Practical Conversational Text-to-SQL dataset with Ambiguous and Unanswerable Queries](https://doi.org/10.18653/v1/2025.naacl-long.13) |  | 0 | Previous text-to-SQL datasets and systems have primarily focused on user questions with clear intentions that can be answered. However, real user questions can often be ambiguous with multiple interpretations or unanswerable due to a lack of relevant data. In this work, we construct a practical conversational text-to-SQL dataset called PRACTIQ, consisting of ambiguous and unanswerable questions inspired by real-world user questions. We first identified four categories of ambiguous questions and... | Mingwen Dong, Nischal Ashok Kumar, Yiqun Hu, Anuj Chauhan, ChungWei Hang, Shuaichen Chang, Lin Pan, Wuwei Lan, Henghui Zhu, Jiarong Jiang, Patrick Ng, Zhiguo Wang |  |
| 95 |  |  [MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems](https://doi.org/10.18653/v1/2025.naacl-long.14) |  | 0 | Traditional retrieval-augmented generation (RAG) benchmarks evaluate systems using heuristic-based metrics, but these require human preferences as the ground truth for reference. In contrast, arena-based benchmarks, where systems compete against each other, require an expensive large language model (LLM) as a judge for a reliable evaluation. We present a simple efficient technique to combine the best of both worlds. The idea is to train a surrogate judge using heuristic metrics as input, to... | Nandan Thakur, Suleman Kazi, Ge Luo, Jimmy Lin, Amin Ahmad |  |
| 96 |  |  [LLMs Are Biased Towards Output Formats! Systematically Evaluating and Mitigating Output Format Bias of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.15) |  | 0 | We present the first systematic evaluation examining format bias in performance of large language models (LLMs). Our approach distinguishes between two categories of an evaluation metric under format constraints to reliably and accurately assess performance: one measures performance when format constraints are adhered to, while the other evaluates performance regardless of constraint adherence. We then define a metric for measuring the format bias of LLMs and establish effective strategies to... | Do Xuan Long, NgocHai Nguyen, Tiviatis Sim, Hieu Dao, Shafiq Joty, Kenji Kawaguchi, Nancy F. Chen, MinYen Kan |  |
| 97 |  |  [The Impact of Visual Information in Chinese Characters: Evaluating Large Models' Ability to Recognize and Utilize Radicals](https://doi.org/10.18653/v1/2025.naacl-long.16) |  | 0 | The glyphic writing system of Chinese incorporates information-rich visual features in each character, such as radicals that provide hints about meaning or pronunciation. However, there has been no investigation into whether contemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can harness these sub-character features in Chinese through prompting. In this study, we establish a benchmark to evaluate LLMs’ and VLMs’ understanding of visual elements in Chinese characters,... | Xiaofeng Wu, Karl Stratos, Wei Xu |  |
| 98 |  |  [PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic Languages with Example Selection from related Example Banks](https://doi.org/10.18653/v1/2025.naacl-long.17) |  | 0 | Large Language Models (LLMs) have recently demonstrated impressive few-shot learning capabilities through in-context learning (ICL). However, ICL performance is highly dependent on the choice of few-shot demonstrations, making the selection of the most optimal examples a persistent research challenge. This issue is further amplified in low-resource Indic languages, where the scarcity of ground-truth data complicates the selection process. In this work, we propose PromptRefine, a novel... | Soumya Suvra Ghosal, Soumyabrata Pal, Koyel Mukherjee, Dinesh Manocha |  |
| 99 |  |  [Unlocking Decoding-time Controllability: Gradient-Free Multi-Objective Alignment with Contrastive Prompts](https://doi.org/10.18653/v1/2025.naacl-long.18) |  | 0 | The task of multi-objective alignment aims at balancing and controlling the different alignment objectives, e.g., helpfulness, harmlessness and honesty) of large language models to meet the personalized requirements of different users. However, previous methods tend to train multiple models to deal with various user preferences, with the number of trained models growing linearly with the number of alignment objectives and the number of different preferences. Meanwhile, existing methods are... | Tingchen Fu, Yupeng Hou, Julian J. McAuley, Rui Yan |  |
| 100 |  |  [Fingerspelling within Sign Language Translation](https://doi.org/10.18653/v1/2025.naacl-long.19) |  | 0 | Fingerspelling poses challenges for sign language processing due to its high-frequency motion and use for open-vocabulary terms. While prior work has studied fingerspelling recognition, there has been little attention to evaluating how well sign language translation models understand fingerspelling in the context of entire sentences—and improving this capability. We manually annotate instances of fingerspelling within FLEURS-ASL and use them to evaluate the effect of two simple measures to... | Garrett Tanzer |  |
| 101 |  |  [MoDS: Moderating a Mixture of Document Speakers to Summarize Debatable Queries in Document Collections](https://doi.org/10.18653/v1/2025.naacl-long.20) |  | 0 | Query-focused summarization (QFS) gives a summary of documents to answer a query.Past QFS work assumes queries have one answer, ignoring debatable ones (\*Is law school worth it?\*).We introduce \*\*Debatable QFS (DQFS)\*\*, a task to create summaries that answer debatable queries via documents with opposing perspectives; summaries must \*comprehensively cover\* all sources and \*balance perspectives\*, favoring no side.These goals elude LLM QFS systems, which: 1) lack structured content plans,... | Nishant Balepur, Alexa F. Siu, Nedim Lipka, Franck Dernoncourt, Tong Sun, Jordan Lee BoydGraber, Puneet Mathur |  |
| 102 |  |  [Aligning Sentence Simplification with ESL Learner's Proficiency for Language Acquisition](https://doi.org/10.18653/v1/2025.naacl-long.21) |  | 0 | Text simplification is crucial for improving accessibility and comprehension for English as a Second Language (ESL) learners. This study goes a step further and aims to facilitate ESL learners’ language acquisition by simplification. Specifically, we propose simplifying complex sentences to appropriate levels for learners while also increasing vocabulary coverage of the target level in the simplifications. We achieve this without a parallel corpus by conducting reinforcement learning on a large... | Guanlin Li, Yuki Arase, Noël Crespi |  |
| 103 |  |  [PeerQA: A Scientific Question Answering Dataset from Peer Reviews](https://doi.org/10.18653/v1/2025.naacl-long.22) |  | 0 | We present PeerQA, a real-world, scientific, document-level Question Answering (QA) dataset. PeerQA questions have been sourced from peer reviews, which contain questions that reviewers raised while thoroughly examining the scientific article. Answers have been annotated by the original authors of each paper. The dataset contains 579 QA pairs from 208 academic articles, with a majority from ML and NLP, as well as a subset of other scientific communities like Geoscience and Public Health.PeerQA... | Tim Baumgärtner, Ted Briscoe, Iryna Gurevych |  |
| 104 |  |  [ALiiCE: Evaluating Positional Fine-grained Citation Generation](https://doi.org/10.18653/v1/2025.naacl-long.23) |  | 0 | Large Language Model (LLM) can enhance its credibility and verifiability by generating text with citations. However, existing research on citation generation is predominantly limited to sentence-level statements, neglecting the significance of positional fine-grained citations that can appear anywhere within sentences. To facilitate further exploration of the positional fine-grained citation generation, we propose ALiiCE, the first automatic evaluation framework for this task. Our method... | Yilong Xu, Jinhua Gao, Xiaoming Yu, Baolong Bi, Huawei Shen, Xueqi Cheng |  |
| 105 |  |  [An LLM-Based Approach for Insight Generation in Data Analysis](https://doi.org/10.18653/v1/2025.naacl-long.24) |  | 0 | Generating insightful and actionable information from databases is critical in data analysis. This paper introduces a novel approach using Large Language Models (LLMs) to automatically generate textual insights. Given a multi-table database as input, our method leverages LLMs to produce concise, text-based insights that reflect interesting patterns in the tables. Our framework includes a Hypothesis Generator to formulate domain-relevant questions, a Query Agent to answer such questions by... | Alberto Sánchez Pérez, Alaa Boukhary, Paolo Papotti, Luis Castejón Lozano, Adam Elwood |  |
| 106 |  |  [WebQuality: A Large-scale Multi-modal Web Page Quality Assessment Dataset with Multiple Scoring Dimensions](https://doi.org/10.18653/v1/2025.naacl-long.25) |  | 0 | The assessment of web page quality plays a critical role in a range of downstream applications, yet there is a notable absence of datasets for the evaluation of web page quality. This research presents the pioneering task of web page quality assessment and introduces the first comprehensive, multi-modal Chinese dataset named WebQuality specifically designed for this task. The dataset includes over 65,000 detailed an-notations spanning four sub-dimensions and incorporates elements such as... | Tao Zhang, Yige Wang, ZhuHangyu ZhuHangyu, Li Xin, Chen Xiang, Tian Hua Zhou, Jin Ma |  |
| 107 |  |  [UFO: A UI-Focused Agent for Windows OS Interaction](https://doi.org/10.18653/v1/2025.naacl-long.26) |  | 0 | We introduce UFO, a UI-Fcused agent designed to fulfill user requests tailored to Windows OS applications by observing and analyzing the GUI and control information of these applications. UFO utilizes a hierarchical dual-agent framework that decomposes user requests using a divide-and-conquer approach, enabling seamless navigation and addressing sub-tasks across multiple applications. It also incorporates a control interaction module tailored for Windows OS, which detects control elements... | Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang |  |
| 108 |  |  [Is your benchmark truly adversarial? AdvScore: Evaluating Human-Grounded Adversarialness](https://doi.org/10.18653/v1/2025.naacl-long.27) |  | 0 | Adversarial datasets should validate AI robustness by providing samples on which humans perform well, but models do not. However, as models evolve, datasets can become obsolete. Measuring whether a dataset remains adversarial is hindered by the lack of a standardized metric for measuring adversarialness. We propose ADVSCORE, a human-grounded evaluation metric that assesses a dataset’s adversarialness by capturing models’ and humans’ varying abilities, while also identifying poor examples. We... | Yoo Yeon Sung, Maharshi Gor, Eve Fleisig, Ishani Mondal, Jordan Lee BoydGraber |  |
| 109 |  |  [Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation](https://doi.org/10.18653/v1/2025.naacl-long.28) |  | 0 | Multimodal foundation models hold significant potential for automating radiology report generation, thereby assisting clinicians in diagnosing cardiac diseases. However, generated reports often suffer from serious factual inaccuracy. In this paper, we introduce a fact-aware multimodal retrieval-augmented pipeline in generating accurate radiology reports (FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then integrate factual knowledge to train a universal multimodal... | Liwen Sun, James (Jialun) Zhao, Wenjing Han, Chenyan Xiong |  |
| 110 |  |  [On Behalf of the Stakeholders: Trends in NLP Model Interpretability in the Era of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.29) |  | 0 | Recent advancements in NLP systems, particularly with the introduction of LLMs, have led to widespread adoption of these systems by a broad spectrum of users across various domains, impacting decision-making, the job market, society, and scientific research. This surge in usage has led to an explosion in NLP model interpretability and analysis research, accompanied by numerous technical surveys. Yet, these surveys often overlook the needs and perspectives of explanation stakeholders. In this... | Nitay Calderon, Roi Reichart |  |
| 111 |  |  [Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward](https://doi.org/10.18653/v1/2025.naacl-long.30) |  | 0 | Preference modeling techniques, such as direct preference optimization (DPO), has shown effective in enhancing the generalization abilities of large language model (LLM). However, in tasks involving video instruction-following, providing informative feedback, especially for open-ended conversations, remains a significant challenge. While previous studies have explored using large multimodal models (LMMs) as reward models for guiding preference modeling, their ability to accurately assess the... | Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander G. Hauptmann, Yonatan Bisk, Yiming Yang |  |
| 112 |  |  [FlexiGPT: Pruning and Extending Large Language Models with Low-Rank Weight Sharing](https://doi.org/10.18653/v1/2025.naacl-long.31) |  | 0 | The rapid proliferation of large language models (LLMs) in natural language processing (NLP) has created a critical need for techniques that enable efficient deployment on memory-constrained devices without compromising performance. We present a method to prune LLMs that selectively prunes model blocks based on an importance score and replaces them with a low-parameter replacement strategy. Specifically, we propose a principled metric to replace each pruned block using a weight-sharing... | James Seale Smith, ChiHeng Lin, Shikhar Tuli, Haris Jeelani, Shangqian Gao, Yilin Shen, Hongxia Jin, YenChang Hsu |  |
| 113 |  |  [Conformalized Answer Set Prediction for Knowledge Graph Embedding](https://doi.org/10.18653/v1/2025.naacl-long.32) |  | 0 | Knowledge graph embeddings (KGE) apply machine learning methods on knowledge graphs (KGs) to provide non-classical reasoning capabilities based on similarities and analogies. The learned KG embeddings are typically used to answer queries by ranking all potential answers, but rankings often lack a meaningful probabilistic interpretation - lower-ranked answers do not necessarily have a lower probability of being true. This limitation makes it difficult to quantify uncertainty of model’s... | Yuqicheng Zhu, Nico Potyka, Jiarong Pan, Bo Xiong, Yunjie He, Evgeny Kharlamov, Steffen Staab |  |
| 114 |  |  [Parameter-free and Accessible Prompt Learning to Enhance Adversarial Robustness for Pre-trained Vision-Language Models](https://doi.org/10.18653/v1/2025.naacl-long.33) |  | 0 | Large pre-trained Vision-Language Models (VLMs) have revolutionized both computer vision and natural language processing. Despite their success, adversarial examples can still mislead VLMs into producing incorrect results. This work focuses on boosting the adversarial robustness of VLMs by searching for text prompts at the word level, rather than optimizing continuous textual embeddings. We introduce Parameter-Free Prompt Tuning (PFPT) to learn defense words that enhance resilience against... | Xingran Zhou, Kun Yang, Changtao Miao, Bingyu Hu, Zhuoer Xu, Shiwen Cui, Changhua Meng, Dan Hong |  |
| 115 |  |  [Fine-grained Fallacy Detection with Human Label Variation](https://doi.org/10.18653/v1/2025.naacl-long.34) |  | 0 | We introduce FAINA, the first dataset for fallacy detection that embraces multiple plausible answers and natural disagreement. FAINA includes over 11K span-level annotations with overlaps across 20 fallacy types on social media posts in Italian about migration, climate change, and public health given by two expert annotators. Through an extensive annotation study that allowed discussion over multiple rounds, we minimize annotation errors whilst keeping signals of human label variation.... | Alan Ramponi, Agnese Daffara, Sara Tonelli |  |
| 116 |  |  [Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models](https://doi.org/10.18653/v1/2025.naacl-long.35) |  | 0 | Despite their wide adoption, the biases and unintended behaviors of language models remain poorly understood. In this paper, we identify and characterize a phenomenon never discussed before, which we call semantic leakage, where models leak irrelevant information from the prompt into the generation in unexpected ways. We propose an evaluation setting to detect semantic leakage both by humans and automatically, curate a diverse test suite for diagnosing this behavior, and measure significant... | Hila Gonen, Terra Blevins, Alisa Liu, Luke Zettlemoyer, Noah A. Smith |  |
| 117 |  |  [SELFGOAL: Your Language Agents Already Know How to Achieve High-level Goals](https://doi.org/10.18653/v1/2025.naacl-long.36) |  | 0 | Language agents powered by large language models (LLMs) are increasingly valuable as decision-making tools in domains such as gaming and programming. However, these agents often face challenges in achieving high-level goals without detailed instructions and in adapting to environments where feedback is delayed. In this paper, we present SELFGOAL, a novel automatic approach designed to enhance agents’ capabilities to achieve high-level goals with limited human prior and environmental feedback.... | Ruihan Yang, Jiangjie Chen, Yikai Zhang, Siyu Yuan, Aili Chen, Kyle Richardson, Yanghua Xiao, Deqing Yang |  |
| 118 |  |  [Familarity: Better Evaluation of Zero-Shot Named Entity Recognition by Quantifying Label Shifts in Synthetic Training Data](https://doi.org/10.18653/v1/2025.naacl-long.37) |  | 0 | Zero-shot named entity recognition (NER) is the task of detecting named entities of specific types (such as Person or Medicine) without any training examples. Current research increasingly relies on large synthetic datasets, automatically generated to cover tens of thousands of distinct entity types, to train zero-shot NER models. However, in this paper, we find that these synthetic datasets often contain entity types that are semantically highly similar to (or even the same as) those in... | Jonas Golde, Patrick Haller, Max Ploner, Fabio Barth, Nicolaas Paul Jedema, Alan Akbik |  |
| 119 |  |  [Learning to Summarize from LLM-generated Feedback](https://doi.org/10.18653/v1/2025.naacl-long.38) |  | 0 | Developing effective text summarizers remains a challenge due to issues like hallucinations, key information omissions, and verbosity in LLM-generated summaries. This work explores using LLM-generated feedback to improve summary quality by aligning the summaries with human preferences for faithfulness, completeness, and conciseness. We introduce FeedSum, a large-scale dataset containing multi-dimensional LLM feedback on summaries of varying quality across diverse domains. Our experiments show... | Hwanjun Song, Taewon Yun, Yuho Lee, Jihwan Oh, Gihun Lee, Jason Cai, Hang Su |  |
| 120 |  |  [Hybrid Graphs for Table-and-Text based Question Answering using LLMs](https://doi.org/10.18653/v1/2025.naacl-long.39) |  | 0 | Answering questions that require reasoning and aggregation across both structured (tables) and unstructured (raw text) data sources presents significant challenges. Current methods rely on fine-tuning and high-quality, human-curated data, which is difficult to obtain. Recent advances in Large Language Models (LLMs) have shown promising results for multi-hop question answering (QA) over single-source text data in a zero-shot setting, yet exploration into multi-source Table-Text QA remains... | Ankush Agarwal, Chaitanya Devaguptapu, Ganesh S |  |
| 121 |  |  [CFinBench: A Comprehensive Chinese Financial Benchmark for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.40) |  | 0 | Large language models (LLMs) have achieved remarkable performance on various NLP tasks, yet their potential in more challenging task like finance, has not been fully explored. In this paper, we present CFinBench: a meticulously crafted, the most comprehensive evaluation benchmark to date, for assessing the financial knowledge of LLMs under Chinese context. In practice, to better align with the career trajectory of Chinese financial practitioners, we build a systematic evaluation from 4... | Ying Nie, Binwei Yan, Tianyu Guo, Hao Liu, Haoyu Wang, Wei He, Binfan Zheng, Weihao Wang, Qiang Li, Weijian Sun, Yunhe Wang, Dacheng Tao |  |
| 122 |  |  [LLM-Based Explicit Models of Opponents for Multi-Agent Games](https://doi.org/10.18653/v1/2025.naacl-long.41) |  | 0 | In multi-agent scenarios, the ability to anticipate and respond to opponents is essential, particularly in environments involving adversarial and collaborative interactions. In this paper, we introduce Explicit Models of Opponents (EMO) based on Large Language Models (LLMs), enabling agents to better predict and adapt to diverse, dynamic multi-agent interactions. Unlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs an individual... | Xiaopeng Yu, Wanpeng Zhang, Zongqing Lu |  |
| 123 |  |  [SeqAR: Jailbreak LLMs with Sequential Auto-Generated Characters](https://doi.org/10.18653/v1/2025.naacl-long.42) |  | 0 | The widespread applications of large language models (LLMs) have brought about concerns regarding their potential misuse. Although aligned with human preference data before release, LLMs remain vulnerable to various malicious attacks. In this paper, we adopt a red-teaming strategy to enhance LLM safety and introduce SeqAR, a simple yet effective framework to design jailbreak prompts automatically. The SeqAR framework generates and optimizes multiple jailbreak characters and then applies... | Yan Yang, Zeguan Xiao, Xin Lu, Hongru Wang, Xuetao Wei, Hailiang Huang, Guanhua Chen, Yun Chen |  |
| 124 |  |  [JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.43) |  | 0 |  | Shota Onohara, Atsuyuki Miyai, Yuki Imajuku, Kazuki Egashira, Jeonghun Baek, Xiang Yue, Graham Neubig, Kiyoharu Aizawa |  |
| 125 |  |  [EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction](https://doi.org/10.18653/v1/2025.naacl-long.44) |  | 0 | There has been a rising interest in utilizing tools in applications of autonomous agents based on large language models (LLMs) to address intricate real-world tasks. To develop LLMbased agents, it usually requires LLMs to understand many tool functions from different tool documentations. However, these documentations could be diverse, redundant, or incomplete, which immensely affects the capability of LLMs in using tools. Current LLMs exhibit satisfactory instruction-following capabilities... | Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Kan Ren, Dongsheng Li, Deqing Yang |  |
| 126 |  |  [Decoding Hate: Exploring Language Models' Reactions to Hate Speech](https://doi.org/10.18653/v1/2025.naacl-long.45) |  | 0 | Hate speech is a harmful form of online expression, often manifesting as derogatory posts. It is a significant risk in digital environments. With the rise of Large Language Models (LLMs), there is concern about their potential to replicate hate speech patterns, given their training on vast amounts of unmoderated internet data. Understanding how LLMs respond to hate speech is crucial for their responsible deployment. However, the behaviour of LLMs towards hate speech has been limited compared.... | Paloma Piot, Javier Parapar |  |
| 127 |  |  [Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations](https://doi.org/10.18653/v1/2025.naacl-long.46) |  | 0 | Humans are efficient language learners and inherently social creatures. Our language development is largely shaped by our social interactions, for example, the demonstration and feedback from caregivers. Contrary to human language learning, recent advancements in large language models have primarily adopted a non-interactive training paradigm, and refined pre-trained models through feedback afterward. In this work, we explore how corrective feedback from interactions influences neural language... | Ziqiao Ma, Zekun Wang, Joyce Chai |  |
| 128 |  |  [MoCE: Adaptive Mixture of Contextualization Experts for Byte-based Neural Machine Translation](https://doi.org/10.18653/v1/2025.naacl-long.47) |  | 0 | Byte-based machine translation systems have shown significant potential in massively multilingual settings. Unicode encoding, which maps each character to specific byte(s), eliminates the emergence of unknown words, even in new languages, enabling broad language scalability. However, byte-level tokenization results in sequences that are hard to interpret due to limited semantic information per byte. Local contextualization has proven effective in assigning initial semantics to tokens, improving... | Langlin Huang, Mengyu Bu, Yang Feng |  |
| 129 |  |  [LLM-Human Pipeline for Cultural Grounding of Conversations](https://doi.org/10.18653/v1/2025.naacl-long.48) |  | 0 | Conversations often adhere to well-understood social norms that vary across cultures. For example, while addressing parents by name is commonplace in the West, it is rare in most Asian cultures. Adherence or violation of such norms often dictates the tenor of conversations. Humans are able to navigate social situations requiring cultural awareness quite adeptly. However, it is a hard task for NLP models.In this paper, we tackle this problem by introducing a Cultural Context Schema for... | Rajkumar Pujari, Dan Goldwasser |  |
| 130 |  |  [ACCESS : A Benchmark for Abstract Causal Event Discovery and Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.49) |  | 0 |  | Vy Vo, Lizhen Qu, Tao Feng, Yuncheng Hua, Xiaoxi Kang, Songhai Fan, Tim Dwyer, LayKi Soon, Gholamreza Haffari |  |
| 131 |  |  [Unmasking Implicit Bias: Evaluating Persona-Prompted LLM Responses in Power-Disparate Social Scenarios](https://doi.org/10.18653/v1/2025.naacl-long.50) |  | 0 | Large language models (LLMs) have demonstrated remarkable capabilities in simulating human behaviour and social intelligence. However, they risk perpetuating societal biases, especially when demographic information is involved. We introduce a novel framework using cosine distance to measure semantic shifts in responses and an LLM-judged Preference Win Rate (WR) to assess how demographic prompts affect response quality across power-disparate social scenarios. Evaluating five LLMs over 100... | Bryan Chen Zhengyu Tan, Roy KaWei Lee |  |
| 132 |  |  [GloCOM: A Short Text Neural Topic Model via Global Clustering Context](https://doi.org/10.18653/v1/2025.naacl-long.51) |  | 0 | Uncovering hidden topics from short texts is challenging for traditional and neural models due to data sparsity, which limits word co-occurrence patterns, and label sparsity, stemming from incomplete reconstruction targets. Although data aggregation offers a potential solution, existing neural topic models often overlook it due to time complexity, poor aggregation quality, and difficulty in inferring topic proportions for individual documents. In this paper, we propose a novel model,... | Quang Duc Nguyen, Tung Nguyen, Duc Anh Nguyen, Linh Ngo Van, Sang Dinh, Thien Huu Nguyen |  |
| 133 |  |  [Reversed Attention: On The Gradient Descent Of Attention Layers In GPT](https://doi.org/10.18653/v1/2025.naacl-long.52) |  | 0 | The success of Transformer-based Language Models (LMs) stems from their attention mechanism. While this mechanism has been extensively studied in explainability research, particularly through the attention values obtained during the forward pass of LMs, the backward pass of attention has been largely overlooked.In this work, we study the mathematics of the backward pass of attention, revealing that it implicitly calculates an attention matrix we refer to as “Reversed Attention”.We visualized... | Shahar Katz, Lior Wolf |  |
| 134 |  |  [Self-Harmonized Chain of Thought](https://doi.org/10.18653/v1/2025.naacl-long.53) |  | 0 | Chain-of-thought (CoT) prompting has demonstrated the capacity of large language models to perform complex reasoning through intermediate steps. While effective, current CoT methods face challenges: Zero-shot-CoT can lead to reasoning errors, and Few-shot-CoT requires labor-intensive manual demonstrations. Auto-CoT attempts to address these issues by automatically generating diverse demonstrations, but this diversity can lead to inconsistent reasoning patterns. We propose ECHO (Self-Harmonized... | Ziqi Jin, Wei Lu |  |
| 135 |  |  [AnaScore: Understanding Semantic Parallelism in Proportional Analogies](https://doi.org/10.18653/v1/2025.naacl-long.54) |  | 0 | Formulaic criteria for proportional analogies, which capture relational mappings between two ratios of terms, are mainly confined to the formal level. As analogy datasets grow more complex, especially in evaluating the cognitive abilities of Large Language Models (LLMs), assessing parallelism in them becomes increasingly challenging and often requires human annotation. In this work, we propose AnaScore, an automatic metric for evaluating the strength of semantic parallelism in sentence... | Liyan Wang, Haotong Wang, Yves Lepage |  |
| 136 |  |  [Generating Complex Question Decompositions in the Face of Distribution Shifts](https://doi.org/10.18653/v1/2025.naacl-long.55) |  | 0 | Question decomposition has been found to help large language models’ (LLMs) performance on complex question answering (QA) by breaking these questions into simpler sub-questions for answering. Nonetheless, performance on the task remains dominated by supervised approaches, suggesting room for making LLMs better decomposers. One way of improving LLM training and fine-tuning is to leverage synthetic training data, but the superior performance of supervised approaches collapses in the face of... | Kelvin Han, Claire Gardent |  |
| 137 |  |  [Diversify-verify-adapt: Efficient and Robust Retrieval-Augmented Ambiguous Question Answering](https://doi.org/10.18653/v1/2025.naacl-long.56) |  | 0 | The retrieval augmented generation (RAG) framework addresses an ambiguity in user queries in QA systems by retrieving passages that cover all plausible interpretations and generating comprehensive responses based on the passages. However, our preliminary studies reveal that a single retrieval process often suffers from low-quality results, as the retrieved passages frequently fail to capture all plausible interpretations. Although the iterative RAG approach has been proposed to address this... | Yeonjun In, Sungchul Kim, Ryan A. Rossi, Md. Mehrab Tanjim, Tong Yu, Ritwik Sinha, Chanyoung Park |  |
| 138 |  |  [Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical Ability Assessment of LLM-Powered AI Tutors](https://doi.org/10.18653/v1/2025.naacl-long.57) |  | 0 | In this paper, we investigate whether current state-of-the-art large language models (LLMs) are effective as AI tutors and whether they demonstrate pedagogical abilities necessary for good AI tutoring in educational dialogues. Previous efforts towards evaluation have beenlimited to subjective protocols and benchmarks. To bridge this gap, we propose a unified evaluation taxonomy with eight pedagogical dimensions based on key learning sciences principles, which is designed to assess the... | Kaushal Kumar Maurya, KV Aditya Srivatsa, Kseniia Petukhova, Ekaterina Kochmar |  |
| 139 |  |  [Where is the answer? An empirical study of positional bias for parametric knowledge extraction in language model](https://doi.org/10.18653/v1/2025.naacl-long.58) |  | 0 | Language model (LM) stores diverse factual knowledge in their parameters, which is learned during self-supervised training on unlabeled documents and is made extractable by instruction-tuning. For knowledge-intensive tasks, it is essential to memorize information in a way that makes it extractable from LM’s parameters with diverse queries. However, LMs suffer from a phenomenon called “perplexity curse”; despite minimizing document perplexity during training, LMs struggle to extract information... | Kuniaki Saito, ChenYu Lee, Kihyuk Sohn, Yoshitaka Ushiku |  |
| 140 |  |  [Evaluating Morphological Compositional Generalization in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.59) |  | 0 | Large language models (LLMs) have demonstrated significant progress in various natural language generation and understanding tasks. However, their linguistic generalization capabilities remain questionable, raising doubts about whether these models learn language similarly to humans. While humans exhibit compositional generalization and linguistic creativity in language use, the extent to which LLMs replicate these abilities, particularly in morphology, is under-explored. In this work, we... | Mete Ismayilzada, Defne Circi, Jonne Sälevä, Hale Sirin, Abdullatif Köksal, Bhuwan Dhingra, Antoine Bosselut, Duygu Ataman, Lonneke van der Plas |  |
| 141 |  |  [Balancing Forget Quality and Model Utility: A Reverse KL-Divergence Knowledge Distillation Approach for Better Unlearning in LLMs](https://doi.org/10.18653/v1/2025.naacl-long.60) |  | 0 | As concern for privacy rights has grown and the size of language model training datasets has expanded, research into machine unlearning for large language models (LLMs) has become crucial. Before the era of LLMs, research on machine unlearning mainly focused on classification tasks in small parameter models. However, as parameter sizes have grown and unlearning targets have become more complex, unlearning has become more challenging, especially in scenarios involving generation instead of... | Bichen Wang, Yuzhe Zi, Yixin Sun, Yanyan Zhao, Bing Qin |  |
| 142 |  |  [AgentMove: A Large Language Model based Agentic Framework for Zero-shot Next Location Prediction](https://doi.org/10.18653/v1/2025.naacl-long.61) |  | 0 | Next location prediction plays a crucial role in various real-world applications. Recently, due to the limitation of existing deep learning methods, attempts have been made to apply large language models (LLMs) to zero-shot next location prediction task. However, they directly generate the final output using LLMs without systematic design, which limits the potential of LLMs to uncover complex mobility patterns and underestimates their extensive reserve of global geospatial knowledge. In this... | Jie Feng, Yuwei Du, Jie Zhao, Yong Li |  |
| 143 |  |  [Embedding derived animacy rankings offer insights into the sources of grammatical animacy](https://doi.org/10.18653/v1/2025.naacl-long.62) |  | 0 | In this study, we applied the semantic projection approach to animacy, a feature that has not been previously explored using this method. We compared the relative animacy rankings of nouns denoting animals, humans, objects, and first-, second-, and third-person pronouns, as derived from word embeddings, with rankings derived from human behavioral ratings of animacy and from grammatical patterns. Our results support the semantic projection approach as an effective method for deriving proxies of... | Vivian G. Li |  |
| 144 |  |  [Generating Long-form Story Using Dynamic Hierarchical Outlining with Memory-Enhancement](https://doi.org/10.18653/v1/2025.naacl-long.63) |  | 0 | Long-form story generation task aims to produce coherent and sufficiently lengthy text, essential for applications such as novel writingand interactive storytelling. However, existing methods, including LLMs, rely on rigid outlines or lack macro-level planning, making it difficult to achieve both contextual consistency and coherent plot development in long-form story generation. To address this issues, we propose Dynamic Hierarchical Outlining with Memory-Enhancement long-form story generation... | Qianyue Wang, Jinwu Hu, Zhengping Li, Yufeng Wang, Daiyuan Li, Yu Hu, Mingkui Tan |  |
| 145 |  |  [Little Giants: Synthesizing High-Quality Embedding Data at Scale](https://doi.org/10.18653/v1/2025.naacl-long.64) |  | 0 | Synthetic data generation has become an increasingly popular way of training models without the need for large, manually labeled datasets. For tasks like text embedding, synthetic data offers diverse and scalable training examples, significantly reducing the cost of human annotation. However, most current approaches rely heavily on proprietary models like GPT-4, which are expensive and inefficient for generating large-scale embedding data. In this paper, we introduce SPEED, a framework that... | Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, Zhicheng Dou |  |
| 146 |  |  [Can LLMs Convert Graphs to Text-Attributed Graphs?](https://doi.org/10.18653/v1/2025.naacl-long.65) |  | 0 | Graphs are ubiquitous structures found in numerous real-world applications, such as drug discovery, recommender systems, and social network analysis. To model graph-structured data, graph neural networks (GNNs) have become a popular tool. However, existing GNN architectures encounter challenges in cross-graph learning where multiple graphs have different feature spaces. To address this, recent approaches introduce text-attributed graphs (TAGs), where each node is associated with a textual... | Zehong Wang, Sidney Liu, Zheyuan Zhang, Tianyi Ma, Chuxu Zhang, Yanfang Ye |  |
| 147 |  |  [Forest for the Trees: Overarching Prompting Evokes High-Level Reasoning in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.66) |  | 0 | Chain-of-thought (CoT) and subsequent methods adopted a deductive paradigm that decomposes the reasoning process, demonstrating remarkable performances across NLP tasks. However, such a paradigm faces the challenge of getting bogged down in low-level semantic details, hindering large language models (LLMs) from correctly understanding, selecting, and compositing conditions. In this work, we present Overarching Prompting (OaP), a simple prompting method that elicits the high-level thinking of... | Haoran Liao, Shaohua Hu, Zhihao Zhu, Hao He, Yaohui Jin |  |
| 148 |  |  [On the Role of Speech Data in Reducing Toxicity Detection Bias](https://doi.org/10.18653/v1/2025.naacl-long.67) |  | 0 | Text toxicity detection systems exhibit significant biases, producing disproportionate rates of false positives on samples mentioning demographic groups. But what about toxicity detection in speech? To investigate the extent to which text-based biases are mitigated by speech-based systems, we produce a set of high-quality group annotations for the multilingual MuTOX dataset, and then leverage these annotations to systematically compare speech- and text-based toxicity classifiers. Our findings... | Samuel J. Bell, Mariano Coria Meglioli, Megan Richards, Eduardo Sánchez, Christophe Ropers, Skyler Wang, Adina Williams, Levent Sagun, Marta R. Costajussà |  |
| 149 |  |  [ITALIC: An Italian Culture-Aware Natural Language Benchmark](https://doi.org/10.18653/v1/2025.naacl-long.68) |  | 0 | We present ITALIC, a large-scale benchmark dataset of 10,000 multiple-choice questions designed to evaluate the natural language understanding of the Italian language and culture. ITALIC spans 12 domains, exploiting public tests to score domain experts in real-world scenarios. We detail our data collection process, stratification techniques, and selection strategies. ITALIC provides a comprehensive assessment suite that captures commonsense reasoning and linguistic proficiency in a... | Andrea Seveso, Daniele Potertì, Edoardo Federici, Mario Mezzanzanica, Fabio Mercorio |  |
| 150 |  |  [RAP: A Metric for Balancing Repetition and Performance in Open-Source Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.69) |  | 0 | Large Language Models (LLMs) have significantly advanced natural language processing, but content repetition in open-source LLMs remains a critical challenge that adversely affects user experience. The repetition penalty parameter (RPP) aims to mitigate this issue by preventing repeated content generation, but excessive use of RPP can compromise the overall quality. In this paper, we propose Repetition-Aware Performance (RAP), a novel evaluation metric that quantifies and integrates repetition... | Donghao Huang, ThanhSon Nguyen, Fiona Liausvia, Zhaoxia Wang |  |
| 151 |  |  [Improving Data Annotation for Low-Resource Relation Extraction with Logical Rule-Augmented Collaborative Language Models](https://doi.org/10.18653/v1/2025.naacl-long.70) |  | 0 | Low-resource relation extraction aims to identify semantic relationships between entities using scarce labeled data. Recent studies exploit large language models to recognize relations based on retrieved examplars, yielding promising results. However, the reliability of predictions from these methods is constrained by the presence of irrelevant context within demonstrations and the inherent flaws of large language models in producing undesired outputs. Inspired by the precision and... | Xiyang Liu, Chunming Hu, Richong Zhang, Junfan Chen, Baowen Xu |  |
| 152 |  |  [CompAct: Compressed Activations for Memory-Efficient LLM Training](https://doi.org/10.18653/v1/2025.naacl-long.71) |  | 0 | We introduce CompAct, a technique that reduces peak memory utilization on GPU by 25-30% for pretraining and 50% for fine-tuning of LLMs. Peak device memory is a major limiting factor in training LLMs, with various recent works aiming to reduce model memory. However most works don’t target the largest component of allocated memory during training: the model’s compute graph, which is stored for the backward pass. By storing low-rank, compressed activations to be used in the backward pass we... | Yara Shamshoum, Nitzan Hodos, Yuval Sieradzki, Assaf Schuster |  |
| 153 |  |  [Large Language Models Are Cross-Lingual Knowledge-Free Reasoners](https://doi.org/10.18653/v1/2025.naacl-long.72) |  | 0 | Large Language Models have demonstrated impressive reasoning capabilities across multiple languages. However, the relationship between capabilities in different languages is less explored. In this work, we decompose the process of reasoning tasks into two separated components: knowledge retrieval and knowledge-free reasoning, and analyze the relationship between cross-lingual transferability and these two components. With adapted commonsense reasoning datasets and constructed knowledge-free... | Peng Hu, Sizhe Liu, Changjiang Gao, Xin Huang, Xue Han, Junlan Feng, Chao Deng, Shujian Huang |  |
| 154 |  |  [What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to Prompt Engineering](https://doi.org/10.18653/v1/2025.naacl-long.73) |  | 0 | Large Language Models (LLMs) changed the way we design and interact with software systems. Their ability to process and extract information from text has drastically improved productivity in a number of routine tasks. Developers that want to include these models in their software stack, however, face a dreadful challenge: debugging LLMs’ inconsistent behavior across minor variations of the prompt. We therefore introduce two metrics for classification tasks, namely \*sensitivity\* and... | Federico Errica, Davide Sanvito, Giuseppe Siracusano, Roberto Bifulco |  |
| 155 |  |  [Detect, Disambiguate, and Translate: On-Demand Visual Reasoning for Multimodal Machine Translation with Large Vision-Language Models](https://doi.org/10.18653/v1/2025.naacl-long.74) |  | 0 | Multimodal machine translation (MMT) aims to leverage additional modalities to assist in language translation. With limited parallel data, current MMT systems rely heavily on monolingual English captioning data. These systems face three key issues: they often overlook that visual signals are unnecessary in many cases, they lack transparency in how visual information is used for disambiguation when needed, and they have yet to fully explore the potential of large-scale vision-language models... | Danyang Liu, Fanjie Kong, Xiaohang Sun, Dhruva Patil, Avijit Vajpayee, Zhu Liu, Vimal Bhat, Najmeh Sadoughi |  |
| 156 |  |  [Mitigating Hallucinations in Multi-modal Large Language Models via Image Token Attention-Guided Decoding](https://doi.org/10.18653/v1/2025.naacl-long.75) |  | 0 | Multi-modal large language models (MLLMs) integrate the inherent text generation capabilities of large language models with an understanding of other modalities, promising wide applications in open-ended tasks. Despite their success, they often generate plausible but incorrect content. This phenomenon, known as hallucination, significantly impacts their practical deployment. In this paper, we delve into the intrinsic characteristics of hallucination from the perspective of interaction between... | Xinhao Xu, Hui Chen, Mengyao Lyu, Sicheng Zhao, Yizhe Xiong, Zijia Lin, Jungong Han, Guiguang Ding |  |
| 157 |  |  [A Multi-modal Large Language Model with Graph-of-Thought for Effective Recommendation](https://doi.org/10.18653/v1/2025.naacl-long.76) |  | 0 | Chain-of-Thought (CoT) prompting has been shown to be effective in guiding Large Language Models (LLMs) to decompose complex tasks into multiple intermediate steps, and constructing a rational reasoning chain for inferring answers. However, the linear nature of CoT falls short from enabling LLMs to effectively handle graph structures, which are essential for personalized recommendation tasks that rely on user-item interaction graphs. To bridge this gap, we introduce GollaRec, which leverages a... | Zixuan Yi, Iadh Ounis |  |
| 158 |  |  [Investigating Human Values in Online Communities](https://doi.org/10.18653/v1/2025.naacl-long.77) |  | 0 | Studying human values is instrumental for cross-cultural research, enabling a better understanding of preferences and behaviour of society at large and communities therein. To study the dynamics of communities online, we propose a method to computationally analyse values present on Reddit. Our method allows analysis at scale, complementing survey based approaches. We train a value relevance and a value polarity classifier, which we thoroughly evaluate using in-domain and out-of-domain human... | Nadav Borenstein, Arnav Arora, LucieAimée Kaffee, Isabelle Augenstein |  |
| 159 |  |  [Pointwise Mutual Information as a Performance Gauge for Retrieval-Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-long.78) |  | 0 | Recent work suggests that large language models enhanced with retrieval-augmented generation are easily influenced by the order in which the retrieved documents are presented to the model when solving tasks such as question answering (QA).However, there is no method to date that exploits this phenomenon to improve generation.To fill this gap, in this study, we show that the pointwise mutual information between a context and a question is an effective gauge for language model... | Tianyu Liu, Jirui Qi, Paul He, Arianna Bisazza, Mrinmaya Sachan, Ryan Cotterell |  |
| 160 |  |  [MATO: A Model-Agnostic Training Optimization for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2025.naacl-long.79) |  | 0 | As an important fine-grained sentiment analysis task, aspect sentiment triplet extraction (ASTE) aims to identify three elements, i.e., aspect, opinion and sentiment polarity as a triplet. Advanced ASTE researches have mostly explored triplet-wise ability to achieve superior improvement. However, existing models with strong in-house performances may struggle to generalize to the challenging cases with the diverse expression of inter-triplet and intra-triplet elements. To this end, we propose a... | Shaopeng Tang, Lin Li, Xiaohui Tao, Leqi Zhong, Qing Xie |  |
| 161 |  |  [Dynamic Data Mixing Maximizes Instruction Tuning for Mixture-of-Experts](https://doi.org/10.18653/v1/2025.naacl-long.80) |  | 0 | Mixture-of-Experts (MoE) models have shown remarkable capability in instruction tuning, especially when the number of tasks scales. However, previous methods simply merge all training tasks (e.g. creative writing, coding, and mathematics) and apply fixed sampling weights, without considering the importance of different tasks as the model training state changes. In this way, the most helpful data cannot be effectively distinguished, leading to suboptimal model performance. To reduce the... | Tong Zhu, Daize Dong, Xiaoye Qu, Jiacheng Ruan, Wenliang Chen, Yu Cheng |  |
| 162 |  |  [EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics](https://doi.org/10.18653/v1/2025.naacl-long.81) |  | 0 | Designing emotionally intelligent conversational systems to provide comfort and advice to people experiencing distress is a compelling area of research. Recently, with advancements in large language models (LLMs), end-to-end dialogue agents without explicit strategy prediction steps have become prevalent. However, implicit strategy planning lacks transparency, and recent studies show that LLMs’ inherent preference bias towards certain socio-emotional strategies hinders the delivery of... | Chenwei Wan, Matthieu Labeau, Chloé Clavel |  |
| 163 |  |  [ReasVQA: Advancing VideoQA with Imperfect Reasoning Process](https://doi.org/10.18653/v1/2025.naacl-long.82) |  | 0 |  | Jianxin Liang, Xiaojun Meng, Huishuai Zhang, Yueqian Wang, Jiansheng Wei, Dongyan Zhao |  |
| 164 |  |  [Divergent Thoughts toward One Goal: LLM-based Multi-Agent Collaboration System for Electronic Design Automation](https://doi.org/10.18653/v1/2025.naacl-long.83) |  | 0 | Recently, with the development of tool-calling capabilities in large language models (LLMs), these models have demonstrated significant potential for automating electronic design automation (EDA) flows by interacting with EDA tool APIs via EDA scripts.However, considering the limited understanding of EDA tools, LLMs face challenges in practical scenarios where diverse interfaces of EDA tools exist across different platforms.Additionally, EDA flow automation often involves intricate, long-chain... | Haoyuan Wu, Haisheng Zheng, Zhuolun He, Bei Yu |  |
| 165 |  |  [A Survey of QUD Models for Discourse Processing](https://doi.org/10.18653/v1/2025.naacl-long.84) |  | 0 | Question Under Discussion (QUD), which is originally a linguistic analytic framework, gains increasing attention in the community of natural language processing over the years. Various models have been proposed for implementing QUD for discourse processing. This survey summarizes these models, with a focus on application to written texts, and examines studies that explore the relationship between QUD and mainstream discourse frameworks, including RST, PDTB and SDRT. Some questions that may... | Yingxue Fu |  |
| 166 |  |  [SafetyQuizzer: Timely and Dynamic Evaluation on the Safety of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.85) |  | 0 | With the expansion of the application of Large Language Models (LLMs), concerns about their safety have grown among researchers. Numerous studies have demonstrated the potential risks of LLMs generating harmful content and have proposed various safety assessment benchmarks to evaluate these risks. However, the evaluation questions in current benchmarks, especially for Chinese, are too straightforward, making them easily rejected by target LLMs, and difficult to update with practical relevance... | Zhichao Shi, Shaoling Jing, Yi Cheng, Hao Zhang, Yuanzhuo Wang, Jie Zhang, Huawei Shen, Xueqi Cheng |  |
| 167 |  |  [Privacy Checklist: Privacy Violation Detection Grounding on Contextual Integrity Theory](https://doi.org/10.18653/v1/2025.naacl-long.86) |  | 0 | Privacy research has attracted wide attention as individuals worry that their private data can be easily leaked during interactions with smart devices, social platforms, and AI applications. Existing works mostly consider privacy attacks and defenses on various sub-fields. Within each field, various privacy attacks and defenses are studied to address patterns of personally identifiable information (PII). In this paper, we argue that privacy is not solely about PII patterns. We ground on the... | Haoran Li, Wei Fan, Yulin Chen, Cheng Jiayang, Tianshu Chu, Xuebing Zhou, Peizhao Hu, Yangqiu Song |  |
| 168 |  |  [Investigating the (De)Composition Capabilities of Large Language Models in Natural-to-Formal Language Conversion](https://doi.org/10.18653/v1/2025.naacl-long.87) |  | 0 | Humans have strong capabilities of decomposition and composition in natural-to-formal language conversion (N2F) when faced with an unfamiliar formal language, and can easily cope with compositional gaps and counter-intuitive symbolic names. To investigate whether large language models (LLMs) have this set of basic capabilities in N2F, we propose the STD framework. This framework semi-automatically performs sample and task construction, allowing decoupled evaluation of the set of decomposition... | Ziyao Xu, Houfeng Wang |  |
| 169 |  |  [Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring](https://doi.org/10.18653/v1/2025.naacl-long.88) |  | 0 | Large language model (LLM) safety is a critical issue, with numerous studies employing red team testing to enhance model security. Among these, jailbreak methods explore potential vulnerabilities by crafting malicious prompts that induce model outputs contrary to safety alignments. Existing black-box jailbreak methods often rely on model feedback, repeatedly submitting queries with detectable malicious instructions during the attack search process. Although these approaches are effective, the... | Honglin Mu, Han He, Yuxin Zhou, Yunlong Feng, Yang Xu, Libo Qin, Xiaoming Shi, Zeming Liu, Xudong Han, Qi Shi, Qingfu Zhu, Wanxiang Che |  |
| 170 |  |  [VividMed: Vision Language Model with Versatile Visual Grounding for Medicine](https://doi.org/10.18653/v1/2025.naacl-long.89) |  | 0 | Recent advancements in Vision Language Models (VLMs) have demonstrated remarkable promise in generating visually grounded responses. However, their application in the medical domain is hindered by unique challenges. For instance, most VLMs rely on a single method of visual grounding, whereas complex medical tasks demand more versatile approaches. Additionally, while most VLMs process only 2D images, a large portion of medical images are 3D. The lack of medical data further compounds these... | Lingxiao Luo, Bingda Tang, Xuanzhong Chen, Rong Han, Ting Chen |  |
| 171 |  |  [Mixture of Multimodal Adapters for Sentiment Analysis](https://doi.org/10.18653/v1/2025.naacl-long.90) |  | 0 | Pre-trained language model (PLM) have achieved great success in text sentiment analysis. However, in practical applications, sentiment is not only conveyed through language but also hidden in other modalities. Therefore, multimodal sentiment analysis (MSA) has attracted increasing research interest. Compared to text sentiment analysis, MSA is challenging since (1) emotions hidden in body movements or vocal timbres eclipse traditional analytical methods, and (2) transferring PLM to MSA task... | Kezhou Chen, Shuo Wang, Huixia Ben, Shengeng Tang, Yanbin Hao |  |
| 172 |  |  [The Impact of Inference Acceleration on Bias of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.91) |  | 0 | Last few years have seen unprecedented advances in capabilities of Large Language Models (LLMs). These advancements promise to benefit a vast array of application domains. However, due to their immense size, performing inference with LLMs is both costly and slow. Consequently, a plethora of recent work has proposed strategies to enhance inference efficiency, e.g., quantization, pruning, and caching. These acceleration strategies reduce the inference cost and latency, often by several factors,... | Elisabeth Kirsten, Ivan Habernal, Vedant Nanda, Muhammad Bilal Zafar |  |
| 173 |  |  [AfriHate: A Multilingual Collection of Hate Speech and Abusive Language Datasets for African Languages](https://doi.org/10.18653/v1/2025.naacl-long.92) |  | 0 | Hate speech and abusive language are global phenomena that need socio-cultural background knowledge to be understood, identified, and moderated. However, in many regions of the Global South, there have been several documented occurrences of (1) absence of moderation and (2) censorship due to the reliance on keyword spotting out of context. Further, high-profile individuals have frequently been at the center of the moderation process, while large and targeted hate speech campaigns against... | Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Abinew Ali Ayele, David Ifeoluwa Adelani, Ibrahim Said Ahmad, Saminu Mohammad Aliyu, Paul Röttger, Abigail Oppong, Andiswa Bukula, Chiamaka Ijeoma Chukwuneke, Ebrahim Chekol Jibril, Elyas Abdi Ismail, Esubalew Alemneh, Hagos Tesfahun Gebremichael, Lukman Jibril Aliyu, Meriem Beloucif, Oumaima Hourrane, Rooweither Mabuya, Salomey Osei, Samuel Rutunda, Tadesse Destaw Belay, Tadesse Kebede Guge, Tesfa Tegegne Asfaw, Lilian Diana Awuor Wanzare, Nelson Odhiambo Onyango, Seid Muhie Yimam, Nedjma Ousidhoum |  |
| 174 |  |  [Revealing the Barriers of Language Agents in Planning](https://doi.org/10.18653/v1/2025.naacl-long.93) |  | 0 | Autonomous planning has been an ongoing pursuit since the inception of artificial intelligence. Based on curated problem solvers, early planning agents could deliver precise solutions for specific tasks but lacked generalization. The emergence of large language models (LLMs) and their powerful reasoning capabilities has reignited interest in autonomous planning by automatically generating reasonable solutions for given tasks. However, prior research and our experiments show that current... | Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, Yanghua Xiao |  |
| 175 |  |  [You Only Read Once (YORO): Learning to Internalize Database Knowledge for Text-to-SQL](https://doi.org/10.18653/v1/2025.naacl-long.94) |  | 0 | While significant progress has been made on the text-to-SQL task, recent solutions repeatedly encode the same database schema for every question, resulting in unnecessary high inference cost and often overlooking crucial database knowledge. To address these issues, we propose You Only Read Once (YORO), a novel paradigm that directly internalizes database knowledge into the parametric knowledge of a text-to-SQL model during training and eliminates the need for schema encoding during inference.... | Hideo Kobayashi, Wuwei Lan, Peng Shi, Shuaichen Chang, Jiang Guo, Henghui Zhu, Zhiguo Wang, Patrick Ng |  |
| 176 |  |  [Option Symbol Matters: Investigating and Mitigating Multiple-Choice Option Symbol Bias of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.95) |  | 0 | Multiple-Choice Question Answering (MCQA) is a widely used task in the evaluation of Large Language Models (LLMs). In this work, we reveal that current LLMs’ performance in MCQA could be heavily influenced by the choice of option symbol sets, due to the option symbol bias. That is, when altering only the option symbols (e.g., A/B/C/D→i/ii/iii/iv), the results could vary sharply, leading to a margin of approximately 10% in accuracy. To uncover the mechanisms behind this, we investigate the... | Zhen Yang, Ping Jian, Chengzhi Li |  |
| 177 |  |  [DAWN-ICL: Strategic Planning of Problem-solving Trajectories for Zero-Shot In-Context Learning](https://doi.org/10.18653/v1/2025.naacl-long.96) |  | 0 | Zero-shot in-context learning (ZS-ICL) aims to conduct in-context learning (ICL) without using human-annotated demonstrations.Existing ZS-ICL methods either use large language models (LLMs) to generate (input, label) pairs as pseudo-demonstrations or leverage historical pseudo-demonstrations to help solve the current problem.They assume that all problems are from the same task and traverse them in a random order.However, in real-world scenarios, problems usually come from diverse tasks, and... | Xinyu Tang, Xiaolei Wang, Xin Zhao, JiRong Wen |  |
| 178 |  |  [LLaSA: Large Language and Structured Data Assistant](https://doi.org/10.18653/v1/2025.naacl-long.97) |  | 0 |  | Yao Xu, Shizhu He, Jiabei Chen, ZengXiangrong ZengXiangrong, Bingning Wang, Guang Liu, Jun Zhao, Kang Liu |  |
| 179 |  |  [Towards Efficient and Multifaceted Computer-assisted Pronunciation Training Leveraging Hierarchical Selective State Space Model and Decoupled Cross-entropy Loss](https://doi.org/10.18653/v1/2025.naacl-long.98) |  | 0 | Prior efforts in building computer-assisted pronunciation training (CAPT) systems often treat automatic pronunciation assessment (APA) and mispronunciation detection and diagnosis (MDD) as separate fronts: the former aims to provide multiple pronunciation aspect scores across diverse linguistic levels, while the latter focuses instead on pinpointing the precise phonetic pronunciation errors made by non-native language learners. However, it is generally expected that a full-fledged CAPT system... | FuAn Chao, Berlin Chen |  |
| 180 |  |  [Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.99) |  | 0 | High-quality training data has proven crucial for developing performant large language models (LLMs). However, commercial LLM providers disclose few, if any, details about the data used for training. This lack of transparency creates multiple challenges: it limits external oversight and inspection of LLMs for issues such as copyright infringement, it undermines the agency of data authors, and it hinders scientific research on critical issues such as data contamination and data selection. How... | Abhilasha Ravichander, Jillian Fisher, Taylor Sorensen, Ximing Lu, Maria Antoniak, Bill Yuchen Lin, Niloofar Mireshghallah, Chandra Bhagavatula, Yejin Choi |  |
| 181 |  |  [An Interpretable and Crosslingual Method for Evaluating Second-Language Dialogues](https://doi.org/10.18653/v1/2025.naacl-long.100) |  | 0 | We analyse the cross-lingual transferability of a dialogue evaluation framework that assesses the relationships between micro-level linguistic features (e.g. backchannels) and macro-level interactivity labels (e.g. topic management), originally designed for English-as-a-second-language dialogues. To this end, we develop CNIMA (\*\*C\*\*hinese \*\*N\*\*on-Native \*\*I\*\*nteractivity \*\*M\*\*easurement and \*\*A\*\*utomation), a Chinese-as-a-second-language labelled dataset with 10K dialogues.... | Rena Gao, Xuetong Wu, Carsten Roever, Jing Wu, Long Lv, Jingxuan Wu, Jey Han Lau |  |
| 182 |  |  [From Allies to Adversaries: Manipulating LLM Tool-Calling through Adversarial Injection](https://doi.org/10.18653/v1/2025.naacl-long.101) |  | 0 | Tool-calling has changed Large Language Model (LLM) applications by integrating external tools, significantly enhancing their functionality across diverse tasks. However, this integration also introduces new security vulnerabilities, particularly in the tool scheduling mechanisms of LLM, which have not been extensively studied. To fill this gap, we present ToolCommander, a novel framework designed to exploit vulnerabilities in LLM tool-calling systems through adversarial tool injection. Our... | Rupeng Zhang, Haowei Wang, Junjie Wang, Mingyang Li, Yuekai Huang, Dandan Wang, Qing Wang |  |
| 183 |  |  [COVE: COntext and VEracity prediction for out-of-context images](https://doi.org/10.18653/v1/2025.naacl-long.102) |  | 0 | Images taken out of their context are the most prevalent form of multimodal misinformation. Debunking them requires (1) providing the true context of the image and (2) checking the veracity of the image’s caption. However, existing automated fact-checking methods fail to tackle both objectives explicitly. In this work, we introduce COVE, a new method that predicts first the true COntext of the image and then uses it to predict the VEracity of the caption. COVE beats the SOTA context prediction... | Jonathan Tonglet, Gabriel Thiem, Iryna Gurevych |  |
| 184 |  |  [Discourse-Driven Evaluation: Unveiling Factual Inconsistency in Long Document Summarization](https://doi.org/10.18653/v1/2025.naacl-long.103) |  | 0 | Detecting factual inconsistency for long document summarization remains challenging, given the complex structure of the source article and long summary length. In this work, we study factual inconsistency errors and connect them with a line of discourse analysis. We find that errors are more common in complex sentences and are associated with several discourse features. We propose a framework that decomposes long texts into discourse-inspired chunks and utilizes discourse information to better... | Yang Zhong, Diane J. Litman |  |
| 185 |  |  [Language Models are Crossword Solvers](https://doi.org/10.18653/v1/2025.naacl-long.104) |  | 0 | Crosswords are a form of word puzzle that require a solver to demonstrate a high degree of proficiency in natural language understanding, wordplay, reasoning, and world knowledge, along with adherence to character and length constraints. In this paper we tackle the challenge of solving crosswords with large language models (LLMs). We demonstrate that the current generation of language models shows significant competence at deciphering cryptic crossword clues and outperforms previously reported... | Soumadeep Saha, Sutanoya Chakraborty, Saptarshi Saha, Utpal Garain |  |
| 186 |  |  [WHoW: A Cross-domain Approach for Analysing Conversation Moderation](https://doi.org/10.18653/v1/2025.naacl-long.105) |  | 0 | We propose WHoW, an evaluation framework for analyzing the facilitation strategies of moderators across different domains/scenarios by examining their motives (Why), dialogue acts (How) and target speaker (Who). Using this framework, we annotated 5,657 moderation sentences with human judges and 15,494 sentences with GPT-4o from two domains: TV debates and radio panel discussions. Comparative analysis demonstrates the framework’s cross-domain generalisability and reveals distinct moderation... | MingBin Chen, Lea Frermann, Jey Han Lau |  |
| 187 |  |  [Uplifting Lower-Income Data: Strategies for Socioeconomic Perspective Shifts in Large Multi-modal Models](https://doi.org/10.18653/v1/2025.naacl-long.106) |  | 0 | Recent work has demonstrated that the unequal representation of cultures and socioeconomic groups in training data leads to biased Large Multi-modal (LMM) models. To improve LMM model performance on underrepresented data, we propose and evaluate several prompting strategies using non-English, geographic, and socioeconomic attributes. We show that these geographic and socioeconomic integrated prompts favor retrieving topic appearances commonly found in data from low-income households across... | Joan Nwatu, Oana Ignat, Rada Mihalcea |  |
| 188 |  |  [MSc-SQL: Multi-Sample Critiquing Small Language Models For Text-To-SQL Translation](https://doi.org/10.18653/v1/2025.naacl-long.107) |  | 0 | Text-to-SQL generation enables non-experts to interact with databases via natural language. Recent advances rely on large closed-source models like GPT-4 that present challenges in accessibility, privacy, and latency. To address these issues, we focus on developing small, efficient, and open-source text-to-SQL models. We demonstrate the benefits of sampling multiple candidate SQL generations and propose our method, MSc-SQL, to critique them using associated metadata. Our sample critiquing model... | Satya Krishna Gorti, Ilan Gofman, Zhaoyan Liu, Jiapeng Wu, Noël Vouitsis, Guangwei Yu, Jesse C. Cresswell, Rasa Hosseinzadeh |  |
| 189 |  |  [Mitigating Heterogeneity among Factor Tensors via Lie Group Manifolds for Tensor Decomposition Based Temporal Knowledge Graph Embedding](https://doi.org/10.18653/v1/2025.naacl-long.108) |  | 0 | Recent studies have highlighted the effectiveness of tensor decomposition methods in the Temporal Knowledge Graphs Embedding (TKGE) task. However, we found that inherent heterogeneity among factor tensors in tensor decomposition significantly hinders the tensor fusion process and further limits the performance of link prediction. To overcome this limitation, we introduce a novel method that maps factor tensors onto a unified smooth Lie group manifold to make the distribution of factor tensors... | Jiang Li, Xiangdong Su, Guanglai Gao |  |
| 190 |  |  [What Goes Into a LM Acceptability Judgment? Rethinking the Impact of Frequency and Length](https://doi.org/10.18653/v1/2025.naacl-long.109) |  | 0 | When comparing the linguistic capabilities of language models (LMs) with humans using LM probabilities, factors such as the length of the sequence and the unigram frequency of lexical items have a significant effect on LM probabilities in ways that humans are largely robust to. Prior works in comparing LM and human acceptability judgments treat these effects uniformly across models, making a strong assumption that models require the same degree of adjustment to control for length and unigram... | Lindia Tjuatja, Graham Neubig, Tal Linzen, Sophie Hao |  |
| 191 |  |  [WaveFM: A High-Fidelity and Efficient Vocoder Based on Flow Matching](https://doi.org/10.18653/v1/2025.naacl-long.110) |  | 0 | Flow matching offers a robust and stable approach to training diffusion models. However, directly applying flow matching to neural vocoders can result in subpar audio quality. In this work, we present WaveFM, a reparameterized flow matching model for mel-spectrogram conditioned speech synthesis, designed to enhance both sample quality and generation speed for diffusion vocoders. Since mel-spectrograms represent the energy distribution of waveforms, WaveFM adopts a mel-conditioned prior... | Tianze Luo, Xingchen Miao, Wenbo Duan |  |
| 192 |  |  [Analyzing and Evaluating Correlation Measures in NLG Meta-Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.111) |  | 0 | The correlation between NLG automatic evaluation metrics and human evaluation is often regarded as a critical criterion for assessing the capability of an evaluation metric. However, different grouping methods and correlation coefficients result in various types of correlation measures used in meta-evaluation. In specific evaluation scenarios, prior work often directly follows conventional measure settings, but the characteristics and differences between these measures have not gotten... | Mingqi Gao, Xinyu Hu, Li Lin, Xiaojun Wan |  |
| 193 |  |  [Cascading Large Language Models for Salient Event Graph Generation](https://doi.org/10.18653/v1/2025.naacl-long.112) |  | 0 | Generating event graphs from long documents is challenging due to the inherent complexity of multiple tasks involved such as detecting events, identifying their relationships, and reconciling unstructured input with structured graphs. Recent studies typically consider all events with equal importance, failing to distinguish salient events crucial for understanding narratives. This paper presents CALLMSAE, a CAscading Large Language Model framework for SAlient Event graph generation, which... | Xingwei Tan, Yuxiang Zhou, Gabriele Pergola, Yulan He |  |
| 194 |  |  [Token-Level Density-Based Uncertainty Quantification Methods for Eliciting Truthfulness of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.113) |  | 0 | Uncertainty quantification (UQ) is a prominent approach for eliciting truthful answers from large language models (LLMs). To date, information-based and consistency-based UQ have been the dominant UQ methods for text generation via LLMs. Density-based methods, despite being very effective for UQ in text classification with encoder-based models, have not been very successful with generative LLMs. In this work, we adapt Mahalanobis Distance (MD) – a well-established UQ technique in classification... | Artem Vazhentsev, Lyudmila Rvanova, Ivan Lazichny, Alexander Panchenko, Maxim Panov, Timothy Baldwin, Artem Shelmanov |  |
| 195 |  |  [How Can We Diagnose and Treat Bias in Large Language Models for Clinical Decision-Making?](https://doi.org/10.18653/v1/2025.naacl-long.114) |  | 0 | Recent advancements in Large Language Models (LLMs) have positioned them as powerful tools for clinical decision-making, with rapidly expanding applications in healthcare. However, concerns about bias remain a significant challenge in the clinical implementation of LLMs, particularly regarding gender and ethnicity. This research investigates the evaluation and mitigation of bias in LLMs applied to complex clinical cases, focusing on gender and ethnicity biases. We introduce a novel... | Kenza Benkirane, Jackie Kay, María PérezOrtiz |  |
| 196 |  |  [From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks](https://doi.org/10.18653/v1/2025.naacl-long.115) |  | 0 | Large Vision Language Models (LVLMs) achieve great performance on visual-language reasoning tasks, however, the black-box nature of LVLMs hinders in-depth research on the reasoning mechanism. As all images need to be converted into image tokens to fit the input format of large language models (LLMs) along with natural language prompts, sequential visual representation is essential to the performance of LVLMs, and the information flow analysis approach can be an effective tool for determining... | Xiaofeng Zhang, Yihao Quan, Chen Shen, Xiaosong Yuan, Shaotian Yan, Liang Xie, Wenxiao Wang, Chaochen Gu, Hao Tang, Jieping Ye |  |
| 197 |  |  [Patent-CR: A Dataset for Patent Claim Revision](https://doi.org/10.18653/v1/2025.naacl-long.116) |  | 0 | This paper presents Patent-CR, the first dataset created for the patent claim revision task in English. It includes both initial patent applications rejected by patent examiners and the final granted versions. Unlike normal text revision tasks that predominantly focus on enhancing sentence quality, such as grammar correction and coherence improvement, patent claim revision aims at ensuring the claims meet stringent legal criteria. These criteria are beyond novelty and inventiveness, including... | Lekang Jiang, Pascal A. Scherz, Stefan Goetz |  |
| 198 |  |  [MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs](https://doi.org/10.18653/v1/2025.naacl-long.117) |  | 0 | The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures.... | Yuhang Zhou, Giannis Karamanolakis, Victor Soto, Anna Rumshisky, Mayank Kulkarni, Furong Huang, Wei Ai, Jianhua Lu |  |
| 199 |  |  [Fine-Tuned LLMs are "Time Capsules" for Tracking Societal Bias Through Books](https://doi.org/10.18653/v1/2025.naacl-long.118) |  | 0 | Books, while often rich in cultural insights, can also mirror societal biases of their eras—biases that Large Language Models (LLMs) may learn and perpetuate during training. We introduce a novel method to trace and quantify these biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising 593 fictional books across seven decades (1950-2019), to track bias evolution. By fine-tuning LLMs on books from each decade and using targeted prompts, we examine shifts in biases related to... | Sangmitra Madhusudan, Robert Morabito, Skye Reid, Nikta Gohari Sadr, Ali Emami |  |
| 200 |  |  [Exploring the Cost-Effectiveness of Perspective Taking in Crowdsourcing Subjective Assessment: A Case Study of Toxicity Detection](https://doi.org/10.18653/v1/2025.naacl-long.119) |  | 0 | Crowdsourcing has been increasingly utilized to gather subjective assessment, such as evaluating the toxicity of texts. Since there doesnot exist a single “ground truth” answer for subjective annotations, obtaining annotations to accurately reflect the opinions of differentsubgroups becomes a key objective for these subjective assessment tasks. Traditionally, this objective is accomplished by directly soliciting a large number of annotations from each subgroup, which can be costly especially... | Xiaoni Duan, Zhuoyan Li, ChienJu Ho, Ming Yin |  |
| 201 |  |  [NormAd: A Framework for Measuring the Cultural Adaptability of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.120) |  | 0 | To be effectively and safely deployed to global user populations, large language models (LLMs) may need to adapt outputs to user values and cultures, not just know about them. We introduce NormAd, an evaluation framework to assess LLMs’ cultural adaptability, specifically measuring their ability to judge social acceptability across varying levels of cultural norm specificity, from abstract values to explicit social norms. As an instantiation of our framework, we create NormAd-Eti, a benchmark... | Abhinav Rao, Akhila Yerukola, Vishwa Shah, Katharina Reinecke, Maarten Sap |  |
| 202 |  |  [LiPO: Listwise Preference Optimization through Learning-to-Rank](https://doi.org/10.18653/v1/2025.naacl-long.121) |  | 0 | Aligning language models (LMs) with curated human feedback is critical to control their behaviors in real-world applications. Several recent policy optimization methods, such as DPO and SLiC, serve as promising alternatives to the traditional Reinforcement Learning from Human Feedback (RLHF) approach.In practice, human feedback often comes in a format of a ranked list over multiple responses to amortize the cost of reading prompt. Multiple responses can also be ranked by reward models or AI... | Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh Joshi, Yao Zhao, Mohammad Saleh, Simon Baumgartner, Jialu Liu, Peter J. Liu, Xuanhui Wang |  |
| 203 |  |  [Adaptive Prompting: Ad-hoc Prompt Composition for Social Bias Detection](https://doi.org/10.18653/v1/2025.naacl-long.122) |  | 0 | Recent advances on instruction fine-tuning have led to the development of various prompting techniques for large language models, such as explicit reasoning steps. However, the success of techniques depends on various parameters, such as the task, language model, and context provided. Finding an effective prompt is, therefore, often a trial-and-error process. Most existing approaches to automatic prompting aim to optimize individual techniques instead of compositions of techniques and their... | Maximilian Spliethöver, Tim Knebler, Fabian Fumagalli, Maximilian Muschalik, Barbara Hammer, Eyke Hüllermeier, Henning Wachsmuth |  |
| 204 |  |  [Enhancing Discriminative Representation in Similar Relation Clusters for Few-Shot Continual Relation Extraction](https://doi.org/10.18653/v1/2025.naacl-long.123) |  | 0 | Few-shot Continual Relation Extraction (FCRE) has emerged as a significant challenge in information extraction, necessitating that relation extraction (RE) systems can sequentially identify new relations with limited labeled samples. While existing studies have demonstrated promising results in FCRE, they often overlook the issue of similar relations, which is a critical factor contributing to catastrophic forgetting. In this work, we propose Sirus–a novel method that utilizes relation... | Anh Duc Le, Nam Le Hai, Thanh Xuan Nguyen, Linh Ngo Van, Nguyen Thi Ngoc Diep, Sang Dinh, Thien Huu Nguyen |  |
| 205 |  |  [SymBa: Symbolic Backward Chaining for Structured Natural Language Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.124) |  | 0 | To improve the performance and explainability of LLM-based natural language reasoning, structured reasoning can be applied to generate explicitly structured proofs. Among different methods for structured reasoning, we specifically focus on backward chaining, where the proof goal is recursively decomposed to subgoals by searching and applying rules. We argue that current LLM-based backward chaining systems (e.g. Least-to-most prompting and LAMBADA) are incomplete, as they omit crucial... | Jinu Lee, Wonseok Hwang |  |
| 206 |  |  [MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference](https://doi.org/10.18653/v1/2025.naacl-long.125) |  | 0 | Long-context Multimodal Large Language Models (MLLMs) that incorporate long text-image and text-video modalities, demand substantial computational resources as their multimodal Key-Value (KV) cache grows with increasing input lengths, challenging memory and time efficiency. For multimodal scenarios, the cross-modal interactions inevitablely increase complexity, and prior methods for KV cache compression, in both text-only and multimodal LLMs, have neglected attention density variations across... | Zhongwei Wan, Hui Shen, Xin Wang, Che Liu, Zheda Mai, Mi Zhang |  |
| 207 |  |  [Language Models Largely Exhibit Human-like Constituent Ordering Preferences](https://doi.org/10.18653/v1/2025.naacl-long.126) |  | 0 | Though English sentences are typically inflexible vis-à-vis word order, constituents often show far more variability in ordering. One prominent theory presents the notion that constituent ordering is directly correlated with constituent weight: a measure of the constituent’s length or complexity. Such theories are interesting in the context of natural language processing (NLP), because while recent advances in NLP have led to significant gains in the performance of large language models (LLMs),... | Ada Defne Tur, Gaurav Kamath, Siva Reddy |  |
| 208 |  |  [SafeQuant: LLM Safety Analysis via Quantized Gradient Inspection](https://doi.org/10.18653/v1/2025.naacl-long.127) |  | 0 | Contemporary jailbreak attacks on Large Language Models (LLMs) employ sophisticated techniques with obfuscated content to bypass safety guardrails. Existing defenses either use computationally intensive LLM verification or require adversarial fine-tuning, leaving models vulnerable to advanced attacks. We introduce SafeQuant, a novel defense framework that leverages quantized gradient patterns to identify harmful prompts efficiently. Our key insight is that when generating identical responses... | Sindhu Padakandla, Sadbhavana Babar, Rathod Darshan D, Manohar Kaul |  |
| 209 |  |  [Exploring Large Language Models for Effective Rumor Detection on Social Media](https://doi.org/10.18653/v1/2025.naacl-long.128) |  | 0 | In this paper, we explore using Large Language Models (LLMs) for rumor detection on social media. It involves assessing the veracity of claims on social media based on social context (e.g., comments, propagation patterns). LLMs, despite their impressive capabilities in text-based reasoning tasks, struggle to achieve promising rumor detection performance when facing long structured social contexts. Our preliminary analysis shows that large-scale contexts hinder LLMs’ reasoning abilities, while... | Yirong Zeng, Xiao Ding, Bibo Cai, Ting Liu, Bing Qin |  |
| 210 |  |  [No Simple Answer to Data Complexity: An Examination of Instance-Level Complexity Metrics for Classification Tasks](https://doi.org/10.18653/v1/2025.naacl-long.129) |  | 0 | Natural Language Processing research has become increasingly concerned with understanding data quality and complexity at the instance level. Instance-level complexity scores can be used for tasks such as filtering out noisy observations and subsampling informative examples. However, there exists a diverse taxonomy of complexity metrics that can be used for a classification task, making metric selection itself a difficult task. We empirically examine the relationship between these metrics and... | Ryan A. Cook, John P. Lalor, Ahmed Abbasi |  |
| 211 |  |  [NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals](https://doi.org/10.18653/v1/2025.naacl-long.130) |  | 0 | Decomposition of text into atomic propositions is a flexible framework allowing for the closer inspection of input and output text. We use atomic decomposition of hypotheses in two natural language reasoning tasks, traditional NLI and defeasible NLI, to form atomic sub-problems, or granular inferences that models must weigh when solving the overall problem. These atomic sub-problems serve as a tool to further understand the structure of both NLI and defeasible reasoning, probe a model’s... | Neha Srikanth, Rachel Rudinger |  |
| 212 |  |  [HISTOIRESMORALES: A French Dataset for Assessing Moral Alignment](https://doi.org/10.18653/v1/2025.naacl-long.131) |  | 0 | Aligning language models with human values is crucial, especially as they become more integrated into everyday life. While models are often adapted to user preferences, it is equally important to ensure they align with moral norms and behaviours in real-world social situations. Despite significant progress in languages like English and Chinese, French has seen little attention in this area, leaving a gap in understanding how LLMs handle moral reasoning in this language. To address this gap, we... | Thibaud Leteno, Irina Proskurina, Antoine Gourru, Julien Velcin, Charlotte Laclau, Guillaume Metzler, Christophe Gravier |  |
| 213 |  |  [Leveraging Allophony in Self-Supervised Speech Models for Atypical Pronunciation Assessment](https://doi.org/10.18653/v1/2025.naacl-long.132) |  | 0 | Allophony refers to the variation in the phonetic realization of a phoneme based on its phonetic environment. Modeling allophones is crucial for atypical pronunciation assessment, which involves distinguishing atypical from typical pronunciations. However, recent phoneme classifier-based approaches often simplify this by treating various realizations as a single phoneme, bypassing the complexity of modeling allophonic variation. Motivated by the acoustic modeling capabilities of frozen... | Kwanghee Choi, Eunjung Yeo, Kalvin Chang, Shinji Watanabe, David R. Mortensen |  |
| 214 |  |  [SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search](https://doi.org/10.18653/v1/2025.naacl-long.133) |  | 0 | Conversational Recommender Systems (CRS) proactively engage users in interactive dialogues to elicit user preferences and provide personalized recommendations. Existing methods train Reinforcement Learning (RL)-based agent with greedy action selection or sampling strategy, and may suffer from suboptimal conversational planning. To address this, we present a novel Monte Carlo Tree Search (MCTS)-based CRS framework SAPIENT. SAPIENT consists of a conversational agent (S-agent) and a conversational... | Hanwen Du, Bo Peng, Xia Ning |  |
| 215 |  |  [Reliability of Topic Modeling](https://doi.org/10.18653/v1/2025.naacl-long.134) |  | 0 | Topic models allow researchers to extract latent factors from text data and use those variables in downstream statistical analyses. However, these methodologies can vary significantly due to initialization differences, randomness in sampling procedures, or noisy data. Reliability of these methods is of particular concern as many researchers treat learned topic models as ground truth for subsequent analyses. In this work, we show that the standard practice for quantifying topic model reliability... | Kayla Schroeder, Zach WoodDoughty |  |
| 216 |  |  [Style Transfer with Multi-iteration Preference Optimization](https://doi.org/10.18653/v1/2025.naacl-long.135) |  | 0 | Numerous recent techniques for text style transfer characterize their approaches as variants of reinforcement learning and preference optimization. In this work, we consider the relationship between these approaches and a class of optimization approaches developed primarily for (non-neural) statistical machine translation, formerly known as ‘tuning’. Inspired by these techniques from the past, we improve upon established preference optimization approaches, incorporating multiple iterations of... | Shuai Liu, Jonathan May |  |
| 217 |  |  [DTELS: Towards Dynamic Granularity of Timeline Summarization](https://doi.org/10.18653/v1/2025.naacl-long.136) |  | 0 |  | Chenlong Zhang, Tong Zhou, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao |  |
| 218 |  |  [ALERT: An LLM-powered Benchmark for Automatic Evaluation of Recommendation Explanations](https://doi.org/10.18653/v1/2025.naacl-long.137) |  | 0 | Recommendation explanation systems have become increasingly vital with the widespread adoption of recommender systems. However, existing recommendation explanation evaluation benchmarks suffer from limited item diversity, impractical user profiling requirements, and unreliable and unscalable evaluation protocols. We present ALERT, a model-agnostic recommendation explanation evaluation benchmark. The benchmark comprises three main contributions: 1) a diverse dataset encompassing 15 Amazon... | Yichuan Li, Xinyang Zhang, Chenwei Zhang, Mao Li, Tianyi Liu, Pei Chen, Yifan Gao, Kyumin Lee, Kaize Ding, Zhengyang Wang, Zhihan Zhang, Jingbo Shang, Xian Li, Trishul Chilimbi |  |
| 219 |  |  [DETQUS: Decomposition-Enhanced Transformers for QUery-focused Summarization](https://doi.org/10.18653/v1/2025.naacl-long.138) |  | 0 | Query-focused tabular summarization is an emerging task in table-to-text generation that synthesizes a summary response from tabular data based on user queries. Traditional transformer-based approaches face challenges due to token limitations and the complexity of reasoning over large tables. To address these challenges, we introduce DETQUS (Decomposition-Enhanced Transformers for QUery-focused Summarization), a system designed to improve summarization accuracy by leveraging tabular... | Yasir Khan, Xinlei Wu, Sangpil Youm, Justin Ho, Aryaan Shaikh, Jairo Garciga, Rohan Sharma, Bonnie J. Dorr |  |
| 220 |  |  [IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.139) |  | 0 | Despite the widespread adoption of Large language models (LLMs), their remarkable capabilities remain limited to a few high-resource languages. Additionally, many low-resource languages (e.g. African languages) are often evaluated only on basic text classification tasks due to the lack of appropriate or comprehensive benchmarks outside of high-resource languages. In this paper, we introduce IrokoBench—a human-translated benchmark dataset for 17 typologically-diverse low-resource African... | David Ifeoluwa Adelani, Jessica Ojo, Israel Abebe Azime, Jian Yun Zhuang, Jesujoba Oluwadara Alabi, Xuanli He, Millicent Ochieng, Sara Hooker, Andiswa Bukula, EnShiun Annie Lee, Chiamaka Ijeoma Chukwuneke, Happy Buzaaba, Blessing K. Sibanda, Godson Koffi Kalipe, Jonathan Mukiibi, Salomon Kabongo Kabenamualu, Foutse Yuehgoh, Mmasibidi Setaka, Lolwethu Ndolela, Nkiruka Odu, Rooweither Mabuya, Salomey Osei, Shamsuddeen Hassan Muhammad, Sokhar Samb, Tadesse Kebede Guge, Tombekai Vangoni Sherman, Pontus Stenetorp |  |
| 221 |  |  [The Impact of Domain-Specific Terminology on Machine Translation for Finance in European Languages](https://doi.org/10.18653/v1/2025.naacl-long.140) |  | 0 | Domain-specific machine translation (MT) poses significant challenges due to specialized terminology, particularly when translating across multiple languages with scarce resources. In this study, we present the first impact analysis of domain-specific terminology on multilingual MT for finance, focusing on European languages within the subdomain of macroeconomics. To this end, we construct a multi-parallel corpus from the European Central Bank, aligned across 22 languages. Using this resource,... | Arturo Oncevay, Charese Smiley, Xiaomo Liu |  |
| 222 |  |  [Benchmarking Language Model Creativity: A Case Study on Code Generation](https://doi.org/10.18653/v1/2025.naacl-long.141) |  | 0 | As LLMs become increasingly prevalent, it is interesting to consider how “creative” these models can be. From cognitive science, creativity consists of at least two key characteristics: convergent thinking (purposefulness to achieve a given goal) and divergent thinking (adaptability to explore new environments or constraints) (CITATION). In this work, we introduce a framework for quantifying LLM creativity that incorporates the two design ingredients: (1) We introduce DENIAL PROMPTING which... | Yining Lu, Dixuan Wang, Tianjian Li, Dongwei Jiang, Sanjeev Khudanpur, Meng Jiang, Daniel Khashabi |  |
| 223 |  |  [Have LLMs Reopened the Pandora's Box of AI-Generated Fake News?](https://doi.org/10.18653/v1/2025.naacl-long.142) |  | 0 | With the rise of AI-generated content spewed at scale from large language models (LLMs), genuine concerns about the spread of fake news have intensified. The perceived ability of LLMs to produce convincing fake news at scale poses new challenges for both human and automated fake news detection systems. To address this gap, this paper presents the findings from a university-level competition that aimed to explore how LLMs can be used by humans to create fake news, and to assess the ability of... | Xinyu Wang, Wenbo Zhang, Sai Dileep Koneru, Hangzhi Guo, Bonam Mingole, S. Shyam Sundar, Sarah Rajtmajer, Amulya Yadav |  |
| 224 |  |  [Probe-Free Low-Rank Activation Intervention](https://doi.org/10.18653/v1/2025.naacl-long.143) |  | 0 | Language models (LMs) can produce texts that appear accurate and coherent but contain untruthful or toxic content. Inference-time interventions that edit the hidden activations have shown promising results in steering the LMs towards desirable generations. Existing activation intervention methods often comprise an activation probe to detect undesirable generation, triggering the activation modification to steer subsequent generation. This paper proposes a probe-free intervention method FLORAIN... | Chonghe Jiang, Bao Nguyen, Anthony ManCho So, Viet Anh Nguyen |  |
| 225 |  |  [FactTrack: Time-Aware World State Tracking in Story Outlines](https://doi.org/10.18653/v1/2025.naacl-long.144) |  | 0 | While accurately detecting and correcting factual contradictions in language model outputs has become increasingly important as their capabilities improve, doing so is highly challenging. We propose a novel method, FactTrack, for tracking atomic facts and addressing factual contradictions. Crucially, FactTrack also maintains time-aware validity intervals for each fact, allowing for change over time. At a high level, FactTrack consists of a four-step pipeline to update a world state data... | Zhiheng Lyu, Kevin Yang, Lingpeng Kong, Dan Klein |  |
| 226 |  |  [A Bayesian Optimization Approach to Machine Translation Reranking](https://doi.org/10.18653/v1/2025.naacl-long.145) |  | 0 | Reranking, or scoring a list of prediction candidates from a machine translation system with an external scoring model and returning the highest-scoring candidate, remains a simple and effective method for improving prediction quality. However, reranking with high quality scoring models can add substantial computational cost to the translation pipeline, which we address in this work by framing list reranking as a Bayesian optimization (BayesOpt) problem over the candidate list, where unknown... | Julius Cheng, Maike Züfle, Vilém Zouhar, Andreas Vlachos |  |
| 227 |  |  [Multi-Conditional Ranking with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.146) |  | 0 | Utilizing large language models (LLMs) to rank a set of items has become a common approach in recommendation and retrieval systems. Typically, these systems focus on ordering a substantial number of documents in a monotonic order based on a given query. However, real-world scenarios often present a different challenge: ranking a comparatively smaller set of items, but according to a variety of diverse and occasionally conflicting conditions. In this paper, we define and explore the task of... | Pouya Pezeshkpour, Estevam Hruschka |  |
| 228 |  |  [ReGLA: Refining Gated Linear Attention](https://doi.org/10.18653/v1/2025.naacl-long.147) |  | 0 | Recent advancements in Large Language Models (LLMs) have set themselves apart with their exceptional performance in complex language modelling tasks. However, these models are also known for their significant computational and storage requirements, primarily due to the quadratic computation complexity of softmax attention. To mitigate this issue, linear attention has been designed to reduce the quadratic space-time complexity that is inherent in standard transformers. In this work, we embarked... | Peng Lu, Ivan Kobyzev, Mehdi Rezagholizadeh, Boxing Chen, Philippe Langlais |  |
| 229 |  |  [Intrinsic Bias is Predicted by Pretraining Data and Correlates with Downstream Performance in Vision-Language Encoders](https://doi.org/10.18653/v1/2025.naacl-long.148) |  | 0 | While recent work has found that vision-language models trained under the Contrastive Language Image Pre-training (CLIP) framework contain intrinsic social biases, the extent to which different upstream pre-training features of the framework relate to these biases, and hence how intrinsic bias and downstream performance are connected has been unclear. In this work, we present the largest comprehensive analysis to-date of how the upstream pre-training factors and downstream performance of CLIP... | Kshitish Ghate, Isaac Slaughter, Kyra Wilson, Mona T. Diab, Aylin Caliskan |  |
| 230 |  |  [Benchmarking Failures in Tool-Augmented Language Models](https://doi.org/10.18653/v1/2025.naacl-long.149) |  | 0 | The integration of tools has extended the capabilities of language models (LMs) beyond vanilla text generation to versatile scenarios. However, tool-augmented language models (TaLMs) often assume ‘perfect’ information access and tool availability, which may not hold in the real world. To systematically study TaLMs imperfections, we introduce the FAIL-TaLMs benchmark, featuring two major failures: under-specified user queries and non-available tools. FAIL-TaLMS contains 1,749 examples using 906... | Eduardo Treviño, Hugo Contant, James Ngai, Graham Neubig, Zora Zhiruo Wang |  |
| 231 |  |  [Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity Recognition Framework](https://doi.org/10.18653/v1/2025.naacl-long.150) |  | 0 | Clinical named entity recognition (NER) aims to retrieve important entities within clinical narratives. Recent works have demonstrated that large language models (LLMs) can achieve strong performance in this task. While previous works focus on proprietary LLMs, we investigate how open NER LLMs, trained specifically for entity recognition, perform in clinical NER. Our initial experiment reveals significant contrast in performance for some clinical entities and how a simple exploitment on entity... | Reza Averly, Xia Ning |  |
| 232 |  |  [Towards Knowledge Checking in Retrieval-augmented Generation: A Representation Perspective](https://doi.org/10.18653/v1/2025.naacl-long.151) |  | 0 | Retrieval-Augmented Generation (RAG) systems have shown promise in enhancing the performance of Large Language Models (LLMs). However, these systems face challenges in effectively integrating external knowledge with the LLM’s internal knowledge, often leading to issues with misleading or unhelpful information. This work aims to provide a systematic study on knowledge checking in RAG systems. We conduct a comprehensive analysis of LLM representation behaviors and demonstrate the significance of... | Shenglai Zeng, Jiankun Zhang, Bingheng Li, Yuping Lin, Tianqi Zheng, Dante Everaert, Hanqing Lu, Hui Liu, Yue Xing, Monica Xiao Cheng, Jiliang Tang |  |
| 233 |  |  [The Power of Many: Multi-Agent Multimodal Models for Cultural Image Captioning](https://doi.org/10.18653/v1/2025.naacl-long.152) |  | 0 | Large Multimodal Models (LMMs) exhibit impressive performance across various multimodal tasks. However, their effectiveness in cross-cultural contexts remains limited due to the predominantly Western-centric nature of most data and models. Conversely, multi-agent models have shown significant capability in solving complex tasks. Our study evaluates the collective performance of LMMs in a multi-agent interaction setting for the novel task of cultural image captioning. Our contributions are as... | Longju Bai, Angana Borah, Oana Ignat, Rada Mihalcea |  |
| 234 |  |  [Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison](https://doi.org/10.18653/v1/2025.naacl-long.153) |  | 0 | Following the remarkable success of Large Language Models (LLMs) in NLP tasks, there is increasing interest in extending their capabilities to speech—the most common form of communication. The most widespread approach to integrating speech into LLMs is dense feature prepending (DFP), which prepends the projected speech representations to the textual representations, allowing end-to-end training with a speech encoder. This raises questions about the need for a sophisticated speech encoder for... | Tsz Kin Lam, Marco Gaido, Sara Papi, Luisa Bentivogli, Barry Haddow |  |
| 235 |  |  [CORRECT: Context- and Reference-Augmented Reasoning and Prompting for Fact-Checking](https://doi.org/10.18653/v1/2025.naacl-long.154) |  | 0 | Fact-checking the truthfulness of claims usually requires reasoning over multiple evidence sentences. Oftentimes, evidence sentences may not be always self-contained, and may require additional contexts and references from elsewhere to understand coreferential expressions, acronyms, and the scope of a reported finding. For example, evidence sentences from an academic paper may need contextual sentences in the paper and descriptions in its cited papers to determine the scope of a research... | Delvin Ce Zhang, Dongwon Lee |  |
| 236 |  |  [Racing Thoughts: Explaining Contextualization Errors in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.155) |  | 0 | The profound success of transformer-based language models can largely be attributed to their ability to integrate relevant contextual information from an input sequence in order to generate a response or complete a task. However, we know very little about the algorithms that a model employs to implement this capability, nor do we understand their failure modes. For example, given the prompt “John is going fishing, so he walks over to the bank. Can he make an ATM transaction?”, a model may... | Michael A. Lepori, Michael Curtis Mozer, Asma Ghandeharioun |  |
| 237 |  |  [DREAM: Improving Video-Text Retrieval Through Relevance-Based Augmentation Using Large Foundation Models](https://doi.org/10.18653/v1/2025.naacl-long.156) |  | 0 | Recent progress in video-text retrieval has been driven largely by advancements in model architectures and training strategies. However, the representation learning capabilities of video-text retrieval models remain constrained by low-quality and limited training data annotations. To address this issue, we present a novel Video-Text Retrieval Paradigm with Relevance-based Augmentation, namely dReAm, which enhances video and text data using large foundation models to learn more generalized... | Yimu Wang, Shuai Yuan, Bo Xue, Xiangru Jian, Wei Pang, Mushi Wang, Ning Yu |  |
| 238 |  |  [ToW: Thoughts of Words Improve Reasoning in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.157) |  | 0 | We introduce thoughts of words (ToW), a novel training-time data-augmentation method for next-word prediction. ToW views next-word prediction as a core reasoning task and injects fine-grained thoughts explaining what the next word should be and how it is related to the previous contexts in pre-training texts. Our formulation addresses two fundamental drawbacks of existing next-word prediction learning schemes: they induce factual hallucination and are inefficient for models to learn the... | Zhikun Xu, Ming Shen, Jacob Dineen, Zhaonan Li, Xiao Ye, Shijie Lu, Aswin RRV, Chitta Baral, Ben Zhou |  |
| 239 |  |  [A Probabilistic Framework for LLM Hallucination Detection via Belief Tree Propagation](https://doi.org/10.18653/v1/2025.naacl-long.158) |  | 0 | We describe Belief Tree Propagation (BTProp), a probabilistic framework for LLM hallucination detection. To judge the truth of a statement, BTProp generates a belief tree by recursively expanding the initial statement into a set of logically related claims, then reasoning globally about the relationships between these claims. BTProp works by constructing a probabilistic model of the LM itself: it reasons jointly about logical relationships between claims and relationships between claim... | Bairu Hou, Yang Zhang, Jacob Andreas, Shiyu Chang |  |
| 240 |  |  [ERAS: Evaluating the Robustness of Chinese NLP Models to Morphological Garden Path Errors](https://doi.org/10.18653/v1/2025.naacl-long.159) |  | 0 | In languages without orthographic word boundaries, NLP models perform _word segmentation_, either as an explicit preprocessing step or as an implicit step in an end-to-end computation. This paper shows that Chinese NLP models are vulnerable to _morphological garden path errors_—errors caused by a failure to resolve local word segmentation ambiguities using sentence-level morphosyntactic context. We propose a benchmark, _ERAS_, that tests a model’s vulnerability to morphological garden path... | Qinchan Li, Sophie Hao |  |
| 241 |  |  [Superlatives in Context: Modeling the Implicit Semantics of Superlatives](https://doi.org/10.18653/v1/2025.naacl-long.160) |  | 0 | Superlatives are used to single out elements with a maximal/minimal property. Semantically, superlatives perform a set comparison: something (or some things) has the min/max property out of a set. As such, superlatives provide an ideal phenomenon for studying implicit phenomena and discourse restrictions. While this comparison set is often not explicitly defined, its (implicit) restrictions can be inferred from the discourse context the expression appears in. In this work we provide an... | Valentina Pyatkin, Bonnie Webber, Ido Dagan, Reut Tsarfaty |  |
| 242 |  |  [LLMs Are Not Intelligent Thinkers: Introducing Mathematical Topic Tree Benchmark for Comprehensive Evaluation of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.161) |  | 0 | Large language models (LLMs) demonstrate impressive capabilities in mathematical reasoning. However, despite these achievements, current evaluations are mostly limited to specific mathematical topics, and it remains unclear whether LLMs are genuinely engaging in reasoning. To address these gaps, we present the Mathematical Topics Tree (MaTT) benchmark, a challenging and structured benchmark that offers 1,958 questions across a wide array of mathematical subjects, each paired with a detailed... | Arash Gholami Davoodi, Seyed Pouyan Mousavi Davoudi, Pouya Pezeshkpour |  |
| 243 |  |  [Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations](https://doi.org/10.18653/v1/2025.naacl-long.162) |  | 0 | Large-scale surveys are essential tools for informing social science research and policy, but running surveys is costly and time-intensive. If we could accurately simulate group-level survey results, this would therefore be very valuable to social science research. Prior work has explored the use of large language models (LLMs) for simulating human behaviors, mostly through prompting. In this paper, we are the first to specialize LLMs for the task of simulating survey response distributions. As... | Yong Cao, Haijiang Liu, Arnav Arora, Isabelle Augenstein, Paul Röttger, Daniel Hershcovich |  |
| 244 |  |  [Representing Rule-based Chatbots with Transformers](https://doi.org/10.18653/v1/2025.naacl-long.163) |  | 0 | What kind of internal mechanisms might Transformers use to conduct fluid, natural-sounding conversations? Prior work has illustrated by construction how Transformers can solve various synthetic tasks, such as sorting a list or recognizing formal languages, but it remains unclear how to extend this approach to a conversational setting. In this work, we propose using ELIZA, a classic rule-based chatbot, as a setting for formal, mechanistic analysis of Transformer-based chatbots. ELIZA allows us... | Dan Friedman, Abhishek Panigrahi, Danqi Chen |  |
| 245 |  |  [Incremental Sentence Processing Mechanisms in Autoregressive Transformer Language Models](https://doi.org/10.18653/v1/2025.naacl-long.164) |  | 0 | Autoregressive transformer language models (LMs) possess strong syntactic abilities, often successfully handling phenomena from agreement to NPI licensing. However, the features they use to incrementally process their linguistic input are not well understood. In this paper, we fill this gap by studying the mechanisms underlying garden path sentence processing in LMs. Specifically, we ask: (1) Do LMs use syntactic features or shallow heuristics to perform incremental sentence processing? (2) Do... | Michael Hanna, Aaron Mueller |  |
| 246 |  |  [Entangled Relations: Leveraging NLI and Meta-analysis to Enhance Biomedical Relation Extraction](https://doi.org/10.18653/v1/2025.naacl-long.165) |  | 0 | Recent research efforts have explored the potential of leveraging natural language inference (NLI) techniques to enhance relation extraction (RE). In this vein, we introduce MetaEntail-RE, a novel adaptation method that harnesses NLI principles to enhance RE performance. Our approach follows past works by verbalizing relation classes into class-indicative hypotheses, aligning a traditionally multi-class classification task to one of textual entailment. We introduce three key enhancements: (1)... | William Hogan, Jingbo Shang |  |
| 247 |  |  [Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.166) |  | 0 | Multimodal Large Language Models (MLLMs) have shown significant promise in various applications, leading to broad interest from researchers and practitioners alike. However, a comprehensive evaluation of their long-context capabilities remains underexplored. To address these gaps, we introduce the MultiModal Needle-in-a-haystack (MMNeedle) benchmark, specifically designed to assess the long-context capabilities of MLLMs. Besides multi-image input, we employ image stitching to further increase... | Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, Hao Wang |  |
| 248 |  |  [WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines](https://doi.org/10.18653/v1/2025.naacl-long.167) |  | 0 | Vision Language Models (VLMs) often struggle with culture-specific knowledge, particularly in languages other than English and in underrepresented cultural contexts. To evaluate their understanding of such knowledge, we introduce WorldCuisines, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes a visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families... | Genta Indra Winata, Frederikus Hudi, Patrick Amadeus Irawan, David Anugraha, Rifki Afina Putri, Yutong Wang, Adam Nohejl, Ubaidillah Ariq Prathama, Nedjma Ousidhoum, Afifa Amriani, Anar Rzayev, Anirban Das, Ashmari Pramodya, Aulia Adila, Bryan Wilie, Candy Olivia Mawalim, Cheng Ching Lam, Daud Abolade, Emmanuele Chersoni, Enrico Santus, Fariz Ikhwantri, Garry Kuwanto, Hanyang Zhao, Haryo Akbarianto Wibowo, Holy Lovenia, Jan Christian Blaise Cruz, Jan Wira Gotama Putra, Junho Myung, Lucky Susanto, Maria Angelica Riera Machin, Marina Zhukova, Michael Anugraha, Muhammad Farid Adilazuarda, Natasha Christabelle Santosa, Peerat Limkonchotiwat, Raj Dabre, Rio Alexander Audino, Samuel Cahyawijaya, ShiXiong Zhang, Stephanie Yulia Salim, Yi Zhou, Yinxuan Gui, David Ifeoluwa Adelani, EnShiun Annie Lee, Shogo Okada, Ayu Purwarianti, Alham Fikri Aji, Taro Watanabe, Derry Tanti Wijaya, Alice Oh, ChongWah Ngo |  |
| 249 |  |  [Extracting and Understanding the Superficial Knowledge in Alignment](https://doi.org/10.18653/v1/2025.naacl-long.168) |  | 0 | Alignment of large language models (LLMs) with human values and preferences, often achieved through fine-tuning based on human feedback, is essential for ensuring safe and responsible AI behaviors. However, the process typically requires substantial data and computation resources. Recent studies have revealed that alignment might be attainable at lower costs through simpler methods, such as in-context learning. This leads to the question: Is alignment predominantly superficial? In this paper,... | Runjin Chen, Gabriel J. Perin, Xuxi Chen, Xilun Chen, Yan Han, Nina S. T. Hirata, Junyuan Hong, Bhavya Kailkhura |  |
| 250 |  |  [Smurfs: Multi-Agent System using Context-Efficient DFSDT for Tool Planning](https://doi.org/10.18653/v1/2025.naacl-long.169) |  | 0 | Teaching large language models (LLMs) to use tools for solving complex problems can grant them human-like reasoning abilities. ReAct and its variants are popular frameworks for tool use in both single-agent and multi-agent systems. To address issues like error propagation and limited exploration in ReAct, the Deep First Search Decision Tree (DFSDT) was proposed, but it faces challenges such as rollback instability, redundant context, and premature termination in single-agent settings. We... | Junzhi Chen, Juhao Liang, Benyou Wang |  |
| 251 |  |  [From Introspection to Best Practices: Principled Analysis of Demonstrations in Multimodal In-Context Learning](https://doi.org/10.18653/v1/2025.naacl-long.170) |  | 0 | Motivated by in-context learning (ICL) capabilities of Large Language Models (LLMs), multimodal LLMs with additional visual modality are also exhibited with similar ICL abilities when multiple image-text pairs are provided as demonstrations. However, relatively less work has been done to investigate the principles behind how and why multimodal ICL works. We conduct a systematic and principled evaluation of multimodal ICL for models of different scales on a broad spectrum of new yet critical... | Nan Xu, Fei Wang, Sheng Zhang, Hoifung Poon, Muhao Chen |  |
| 252 |  |  [Upsample or Upweight? Balanced Training on Heavily Imbalanced Datasets](https://doi.org/10.18653/v1/2025.naacl-long.171) |  | 0 | Data abundance across different domains exhibits a long-tailed distribution: few domains have abundant data, while most face data scarcity. Our work focuses on a multilingual setting, where available data is heavily skewed toward high-resource languages, creating significant imbalances in training data sizes across languages. This disparity challenges training language models to perform uniformly well in all languages. Two common strategies to address this issue are upsampling low-resource... | Tianjian Li, Haoran Xu, Weiting Tan, Kenton Murray, Daniel Khashabi |  |
| 253 |  |  [LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with Simple Word-based Counting Problems](https://doi.org/10.18653/v1/2025.naacl-long.172) |  | 0 | Interestingly, LLMs yet struggle with some basic tasks that humans find trivial to handle, e.g., counting the number of character r’s in the word “strawberry”. There are several popular conjectures (e.g., tokenization, architecture and training data) regarding the reason for deficiency of LLMs in simple word-based counting problems, sharing the similar belief that such failure stems from model pretraining hence probably inevitable during deployment. In this paper, we carefully design multiple... | Nan Xu, Xuezhe Ma |  |
| 254 |  |  [PAPILLON: Privacy Preservation from Internet-based and Local Language Model Ensembles](https://doi.org/10.18653/v1/2025.naacl-long.173) |  | 0 | Users can divulge sensitive information to proprietary LLM providers, raising significant privacy concerns. While open-source models, hosted locally on the user’s machine, alleviate some concerns, models that users can host locally are often less capable than proprietary frontier models. Toward preserving user privacy while retaining the best quality, we propose Privacy-Conscious Delegation, a novel task for chaining API-based and local models. We utilize recent public collections of user-LLM... | Siyan Li, Vethavikashini Chithrra Raghuram, Omar Khattab, Julia Hirschberg, Zhou Yu |  |
| 255 |  |  [When2Call: When (not) to Call Tools](https://doi.org/10.18653/v1/2025.naacl-long.174) |  | 0 | Leveraging external tools is a key feature for modern Language Models (LMs) to expand their capabilities and integrate them into existing systems. However, existing benchmarks primarily focus on the accuracy of tool calling—whether the correct tool is called with the correct parameters—and less on evaluating when LMs should (not) call tools. We develop a new benchmark, When2Call, which evaluates tool-calling decision-making: when to generate a tool call, when to ask follow-up questions and when... | Hayley Ross, Ameya Sunil Mahabaleshwarkar, Yoshi Suhara |  |
| 256 |  |  [Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused Preference Optimization](https://doi.org/10.18653/v1/2025.naacl-long.175) |  | 0 | Machine Translation (MT) is undergoing a paradigm shift, with systems based on fine-tuned large language models (LLM) becoming increasingly competitive with traditional encoder-decoder models trained specifically for translation tasks. However, LLM-based systems are at a higher risk of generating hallucinations, which can severely undermine user’s trust and safety. Most prior research on hallucination mitigation focuses on traditional MT models, with solutions that involve \*post-hoc\*... | Zilu Tang, Rajen Chatterjee, Sarthak Garg |  |
| 257 |  |  [Large Language Models Can Solve Real-World Planning Rigorously with Formal Verification Tools](https://doi.org/10.18653/v1/2025.naacl-long.176) |  | 0 | Large Language Models (LLMs) struggle to directly generate correct plans for complex multi-constraint planning problems, even with self-verification and self-critique. For example, a U.S. domestic travel planning benchmark TravelPlanner was proposed in Xie et al. (2024), where the best LLM OpenAI o1-preview can only find viable travel plans with a 10% success rate given all needed information. In this work, we tackle this by proposing an LLM-based planning framework that formalizes and solves... | Yilun Hao, Yongchao Chen, Yang Zhang, Chuchu Fan |  |
| 258 |  |  [Who Relies More on World Knowledge and Bias for Syntactic Ambiguity Resolution: Humans or LLMs?](https://doi.org/10.18653/v1/2025.naacl-long.177) |  | 0 | This study explores how recent large language models (LLMs) navigate relative clause attachment ambiguity and use world knowledge biases for disambiguation in six typologically diverse languages: English, Chinese, Japanese, Korean, Russian, and Spanish. We describe the process of creating a novel dataset – MultiWho – for fine-grained evaluation of relative clause attachment preferences in ambiguous and unambiguous contexts. Our experiments with three LLMs indicate that, contrary to humans, LLMs... | So Young Lee, Russell Scheinberg, Amber Shore, Ameeta Agrawal |  |
| 259 |  |  [Beyond Benchmarks: Building a Richer Cross-Document Event Coreference Dataset with Decontextualization](https://doi.org/10.18653/v1/2025.naacl-long.178) |  | 0 | Cross-Document Event Coreference (CDEC) annotation is challenging and difficult to scale, resulting in existing datasets being small and lacking diversity. We introduce a new approach leveraging large language models (LLMs) to decontextualize event mentions, by simplifying the document-level annotation task to sentence pairs with enriched context, enabling the creation of Richer EventCorefBank (RECB), a denser and more expressive dataset annotated at faster speed. Decontextualization has been... | Jin Zhao, Jingxuan Tu, Bingyang Ye, Xinrui Hu, Nianwen Xue, James Pustejovsky |  |
| 260 |  |  [Can Unconfident LLM Annotations Be Used for Confident Conclusions?](https://doi.org/10.18653/v1/2025.naacl-long.179) |  | 0 | Large language models (LLMs) have shown high agreement with human raters across a variety of tasks, demonstrating potential to ease the challenges of human data collection. In computational social science (CSS), researchers are increasingly leveraging LLM annotations to complement slow and expensive human annotations. Still, guidelines for collecting and using LLM annotations, without compromising the validity of downstream conclusions, remain limited. We introduce Confidence-driven inference:... | Kristina Gligoric, Tijana Zrnic, Cinoo Lee, Emmanuel J. Candès, Dan Jurafsky |  |
| 261 |  |  [Beyond End-to-End VLMs: Leveraging Intermediate Text Representations for Superior Flowchart Understanding](https://doi.org/10.18653/v1/2025.naacl-long.180) |  | 0 | Flowcharts are typically presented as images, driving the trend of using vision-language models (VLMs) for end-to-end flowchart understanding. However, two key challenges arise: (i) Limited controllability—users have minimal influence over the downstream task, as they can only modify input images, while the training of VLMs is often out of reach for most researchers. (ii) Lack of explainability—it is difficult to trace VLM errors to specific causes, such as failures in visual encoding or... | Junyi Ye, Ankan Dash, Wenpeng Yin, Guiling Wang |  |
| 262 |  |  [Ihquin tlahtouah in Tetelahtzincocah: An annotated, multi-purpose audio and text corpus of Western Sierra Puebla Nahuatl](https://doi.org/10.18653/v1/2025.naacl-long.181) |  | 0 | The development of digital linguistic resources is essential for enhancing the inclusion of indigenous and marginalized languages in the digital domain. Indigenous languages of Mexico, despite representing vast typological diversity and millions of speakers, have largely been overlooked in NLP until recently. In this paper, we present a corpus of audio and annotated transcriptions of Western Sierra Puebla Nahuatl, an endangered variety of Nahuatl spoken in Puebla, Mexico. The data made... | Robert Pugh, Cheyenne Wing, María Ximena Juárez Huerta, Angeles Márquez Hernandez, Francis M. Tyers |  |
| 263 |  |  [Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions](https://doi.org/10.18653/v1/2025.naacl-long.182) |  | 0 | LLMs have demonstrated impressive performance in answering medical questions, such as achieving passing scores on medical licensing examinations. However, medical board exams or general clinical questions do not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the reasoning of model decisions, a crucial component of supporting doctors in making complex medical decisions. To address these challenges, we construct two... | Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze |  |
| 264 |  |  [Unfamiliar Finetuning Examples Control How Language Models Hallucinate](https://doi.org/10.18653/v1/2025.naacl-long.183) |  | 0 | Large language models are known to hallucinate, but the underlying mechanism that govern how models hallucinate are not yet fully understood. In this work, we find that unfamiliar examples in the models’ finetuning data – those that introduce concepts beyond the base model’s scope of knowledge – are crucial in shaping these errors. In particular, we find that an LLM’s hallucinated predictions tend to mirror the responses associated with its unfamiliar finetuning examples. This suggests that by... | Katie Kang, Eric Wallace, Claire J. Tomlin, Aviral Kumar, Sergey Levine |  |
| 265 |  |  [Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling](https://doi.org/10.18653/v1/2025.naacl-long.184) |  | 0 | Self-consistency mitigates hallucinations in Large Language Models (LLMs) by sampling multiple reasoning paths, but it lacks a systematic approach to determine the optimal number of samples or select the most faithful rationale. To address this limitation, we introduce Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales. RASC assesses the quality of reasoning and the... | Guangya Wan, Yuqi Wu, Jie Chen, Sheng Li |  |
| 266 |  |  [MatViX: Multimodal Information Extraction from Visually Rich Articles](https://doi.org/10.18653/v1/2025.naacl-long.185) |  | 0 | Multimodal information extraction (MIE) is crucial for scientific literature, where valuable data is often spread across text, figures, and tables. In materials science, extracting structured information from research articles can accelerate the discovery of new materials. However, the multimodal nature and complex interconnections of scientific content present challenges for traditional text-based methods. We introduce MatViX, a benchmark consisting of 324 full-length research articles and... | Ghazal Khalighinejad, Sharon Scott, Ollie Liu, Kelly L. Anderson, Rickard Stureborg, Aman Tyagi, Bhuwan Dhingra |  |
| 267 |  |  [Towards Rationality in Language and Multimodal Agents: A Survey](https://doi.org/10.18653/v1/2025.naacl-long.186) |  | 0 | This work discusses how to build more rational language and multimodal agents and what criteria define rationality in intelligent systems.Rationality is the quality of being guided by reason, characterized by decision-making that aligns with evidence and logical principles. It plays a crucial role in reliable problem-solving by ensuring well-grounded and consistent solutions. Despite their progress, large language models (LLMs) often fall short of rationality due to their bounded knowledge... | Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Yuan Yuan, Zhuoqun Hao, Xinyi Bai, Weijie J. Su, Camillo Jose Taylor, Tanwi Mallick |  |
| 268 |  |  [CluSanT: Differentially Private and Semantically Coherent Text Sanitization](https://doi.org/10.18653/v1/2025.naacl-long.187) |  | 0 | We introduce CluSanT, a novel text sanitization framework based on Metric Local Differential Privacy (MLDP). Our framework consists of three components: token clustering, cluster embedding, and token sanitization. For the first, CluSanT employs Large Language Models (LLMs) to create—a set of potential substitute tokens which we meaningfully cluster. Then, we develop a parameterized cluster embedding that balances the trade-off between privacy and utility. Lastly, we propose a MLDP algorithm... | Ahmed Musa Awon, Yun Lu, Shera Potka, Alex Thomo |  |
| 269 |  |  [TurkingBench: A Challenge Benchmark for Web Agents](https://doi.org/10.18653/v1/2025.naacl-long.188) |  | 0 | Can advanced multi-modal models effectively tackle complex web-based tasks? Such tasks are often found on crowdsourcing platforms, where crowdworkers engage in challenging micro-tasks within web-based environments.Building on this idea, we present TurkingBench, a benchmark consisting of tasks presented as web pages with textual instructions and multi-modal contexts. Unlike previous approaches that rely on artificially synthesized web pages, our benchmark uses natural HTML pages originally... | Kevin Xu, Yeganeh Kordi, Tanay Nayak, Adi Asija, Yizhong Wang, Kate Sanders, Adam Byerly, Jingyu Zhang, Benjamin Van Durme, Daniel Khashabi |  |
| 270 |  |  [CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.189) |  | 0 | Pretrained on massive amounts of code and text data, large language models (LLMs) have demonstrated remarkable achievements in performing code generation tasks. With additional execution-based feedback, these models can act as agents with capabilities to self-refine and improve generated code autonomously. However, on challenging coding tasks with extremely large search space, current agentic approaches still struggle with multi-stage planning, generating, and debugging. To address this... | Jierui Li, Hung Le, Yingbo Zhou, Caiming Xiong, Silvio Savarese, Doyen Sahoo |  |
| 271 |  |  [DPL: Diverse Preference Learning Without A Reference Model](https://doi.org/10.18653/v1/2025.naacl-long.190) |  | 0 | In direct preference alignment in LLMs, most existing methods seek to retrieve the reward function directly from preference data. However, real-world preference data often contains diversity in preference annotations reflective of true human preferences. Existing algorithms, including KTO, do not directly utilize such nuances in the annotations which limits their applicability. In this work, we propose Diverse Preference Learning (DPL), a reference model-free method that simultaneously learns a... | Abhijnan Nath, Andrey Volozin, Saumajit Saha, Albert Nanda, Galina Grunin, Rahul Bhotika, Nikhil Krishnaswamy |  |
| 272 |  |  [Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data](https://doi.org/10.18653/v1/2025.naacl-long.191) |  | 0 | To trust the fluent generations of large language models (LLMs), humans must be able to _verify_ their correctness against trusted, external sources. Recent efforts, such as providing citations via retrieved documents or post-hoc provenance, enhance verifiability but provide no guarantees on their correctness. To address these limitations, we tackle the verifiability goal with a different philosophy: _trivializing the verification process by developing models that quote verbatim statements from... | Jingyu Zhang, Marc Marone, Tianjian Li, Benjamin Van Durme, Daniel Khashabi |  |
| 273 |  |  [VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models](https://doi.org/10.18653/v1/2025.naacl-long.192) |  | 0 |  | Zejun Li, Ruipu Luo, Jiwen Zhang, Minghui Qiu, Xuanjing Huang, Zhongyu Wei |  |
| 274 |  |  [ACCORD: Closing the Commonsense Measurability Gap](https://doi.org/10.18653/v1/2025.naacl-long.193) |  | 0 | We present ACCORD, a framework and benchmark suite for disentangling the commonsense grounding and reasoning abilities of large language models (LLMs) through controlled, multi-hop counterfactuals. ACCORD introduces formal elements to commonsense reasoning to explicitly control and quantify reasoning complexity beyond the typical 1 or 2 hops. Uniquely, ACCORD can automatically generate benchmarks of arbitrary reasoning complexity, so it scales with future LLM improvements. Indeed, our... | François RoewerDesprés, Jinyue Feng, Zining Zhu, Frank Rudzicz |  |
| 275 |  |  [CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments](https://doi.org/10.18653/v1/2025.naacl-long.194) |  | 0 | Customer Relationship Management (CRM) systems are vital for modern enterprises, providing a foundation for managing customer interactions and data. Integrating AI agents into CRM systems can automate routine processes and enhance personalized service. However, deploying and evaluating these agents is challenging due to the lack of realistic benchmarks that reflect the complexity of real-world CRM tasks. To address this issue, we introduce CRMArena, a novel benchmark designed to evaluate AI... | KungHsiang Huang, Akshara Prabhakar, Sidharth Dhawan, Yixin Mao, Huan Wang, Silvio Savarese, Caiming Xiong, Philippe Laban, ChienSheng Wu |  |
| 276 |  |  [Mamba-Shedder: Post-Transformer Compression for Efficient Selective Structured State Space Models](https://doi.org/10.18653/v1/2025.naacl-long.195) |  | 0 | Large pre-trained models have achieved outstanding results in sequence modeling. The Transformer block and its attention mechanism have been the main drivers of the success of these models. Recently, alternative architectures, such as Selective Structured State Space Models (SSMs), have been proposed to address the inefficiencies of Transformers. This paper explores the compression of SSM-based models, particularly Mamba and its hybrids. We study the sensitivity of these models to the removal... | Juan Pablo Muñoz, Jinjie Yuan, Nilesh Jain |  |
| 277 |  |  [CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy](https://doi.org/10.18653/v1/2025.naacl-long.196) |  | 0 | There is a significant gap between patient needs and available mental health support today. In this paper, we aim to thoroughly examine the potential of using Large Language Models (LLMs) to assist professional psychotherapy. To this end, we propose a new benchmark, CBT-Bench, for the systematic evaluation of cognitive behavioral therapy (CBT) assistance. We include three levels of tasks in CBT-Bench: \*\*I: Basic CBT knowledge acquisition\*\*, with the task of multiple-choice questions;... | Mian Zhang, Xianjun Yang, Xinlu Zhang, Travis Labrum, Jamie C. Chiu, Shaun M. Eack, Fei Fang, William Yang Wang, Zhiyu Chen |  |
| 278 |  |  [An Efficient Gloss-Free Sign Language Translation Using Spatial Configurations and Motion Dynamics with LLMs](https://doi.org/10.18653/v1/2025.naacl-long.197) |  | 0 | Gloss-free Sign Language Translation (SLT) converts sign videos into spoken language sentences without relying on glosses, which are the written representations of signs. Recently, Large Language Models (LLMs) have shown remarkable translation performance in gloss-free methods by harnessing their powerful natural language generation capabilities. However, these methods often rely on domain-specific fine-tuning of visual encoders to achieve optimal results. By contrast, we emphasize the... | Eui Jun Hwang, Sukmin Cho, Junmyeong Lee, Jong C. Park |  |
| 279 |  |  [Sketch2Code: Evaluating Vision-Language Models for Interactive Web Design Prototyping](https://doi.org/10.18653/v1/2025.naacl-long.198) |  | 0 | Sketches are a natural and accessible medium for UI designers to conceptualize early-stage ideas. However, existing research on UI/UX automation often requires high-fidelity inputs like Figma designs or detailed screenshots, limiting accessibility and impeding efficient design iteration. To bridge this gap, we introduce Sketch2Code, a benchmark that evaluates state-of-the-art Vision Language Models (VLMs) on automating the conversion of rudimentary sketches into webpage prototypes. Beyond... | Ryan Li, Yanzhe Zhang, Diyi Yang |  |
| 280 |  |  [Design2Code: Benchmarking Multimodal Code Generation for Automated Front-End Engineering](https://doi.org/10.18653/v1/2025.naacl-long.199) |  | 0 | Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development in which multimodal large language models (MLLMs) directly convert visual designs into code implementations. In this work, we construct Design2Code – the first real-world benchmark for this task. Specifically, we manually curate 484 diverse real-world webpages as test cases and develop a set of... | Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, Diyi Yang |  |
| 281 |  |  [Temporal-Aware Soft Prompt Tuning for Automatic Text Dating](https://doi.org/10.18653/v1/2025.naacl-long.200) |  | 0 | This paper presents Temporal-aware Soft Prompt Tuning (TASPT), a novel approach for automatic text dating. Unlike existing methods, which often overlook the evolution of word meanings in texts spanning long periods, TASPT incorporates the unique characteristics of historical texts. It introduces a temporal-aware text representation that dynamically captures both semantic variance and invariance. This representation is combined with a soft prompt, enabling efficient parameter tuning for... | Hai Wang, Yuzhi Liang, Han Ren |  |
| 282 |  |  [Sparser Mixture-of-Adapters with Cross-Layer Generalization](https://doi.org/10.18653/v1/2025.naacl-long.201) |  | 0 |  | Ziyue Li, Tianyi Zhou |  |
| 283 |  |  [How to Align Multiple Signed Language Corpora for Better Sign-to-Sign Translations?](https://doi.org/10.18653/v1/2025.naacl-long.202) |  | 0 | There are more than 300 documented signed languages worldwide, which are indispensable avenues for computational linguists to study cross-cultural and cross-linguistic factors that affect automatic sign understanding and generation. Yet, these are studied under critically low-resource settings, especially when examining multiple signed languages simultaneously. In this work, we hypothesize that a linguistically informed alignment algorithm can improve the results of sign-to-sign translation... | Mert Inan, Yang Zhong, Vidya Ganesh, Malihe Alikhani |  |
| 284 |  |  [Communication Makes Perfect: Persuasion Dataset Construction via Multi-LLM Communication](https://doi.org/10.18653/v1/2025.naacl-long.203) |  | 0 | Large Language Models (LLMs) have shown proficiency in generating persuasive dialogue, yet concerns about the fluency and sophistication of their outputs persist. This paper presents a multi-LLM communication framework designed to enhance the generation of persuasive data automatically. This framework facilitates the efficient production of high-quality, diverse linguistic content with minimal human oversight. Through extensive evaluations, we demonstrate that the generated data excels in... | Weicheng Ma, Hefan Zhang, Ivory Yang, Shiyu Ji, Joice Chen, Farnoosh Hashemi, Shubham Mohole, Ethan Gearey, Michael Macy, Saeed Hassanpour, Soroush Vosoughi |  |
| 285 |  |  [Soft Prompting for Unlearning in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.204) |  | 0 | The widespread popularity of Large Language Models (LLMs), partly due to their emerging in-context learning ability, has highlighted the importance of ethical and safety considerations for deployment. Motivated by corresponding data protection guidelines, we investigate machine unlearning for LLMs. In contrast to the growing literature on fine-tuning methods to achieve unlearning, we focus on a comparatively lightweight alternative called soft prompting to realize unlearning in LLMs. With... | Karuna Bhaila, MinhHao Van, Xintao Wu |  |
| 286 |  |  [Mutual-pairing Data Augmentation for Fewshot Continual Relation Extraction](https://doi.org/10.18653/v1/2025.naacl-long.205) |  | 0 | Data scarcity is a major challenge in Few-shot Continual Relation Extraction (FCRE), where models must learn new relations from limited data while retaining past knowledge. Current methods, restricted by minimal data streams, struggle with catastrophic forgetting and overfitting. To overcome this, we introduce a novel \*data augmentation strategy\* that transforms single input sentences into complex texts by integrating both old and new data. Our approach sharpens model focus, enabling precise... | Nguyen Hoang Anh, Quyen Tran, Thanh Xuan Nguyen, Nguyen Thi Ngoc Diep, Linh Ngo Van, Thien Huu Nguyen, Trung Le |  |
| 287 |  |  [KMMLU: Measuring Massive Multitask Language Understanding in Korean](https://doi.org/10.18653/v1/2025.naacl-long.206) |  | 0 | We propose KMMLU, a Korean benchmark with 35,030 expert-level multiple-choice questions across 45 subjects ranging from humanities to STEM. While prior Korean evaluation tools heavily rely on translated versions of existing English benchmarks, KMMLU is collected from original Korean exams, thereby capturing linguistic and cultural aspects of the Korean language. Recent models struggle to show performance over 60%, significantly below the pass mark of the source exams (80%), highlighting the... | Guijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheonbok Park, Kang Min Yoo, Stella Biderman |  |
| 288 |  |  [Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench](https://doi.org/10.18653/v1/2025.naacl-long.207) |  | 0 | Generative models such as Large Language Models (LLM) and Multimodal Large Language models (MLLMs) trained on massive web corpora can memorize and disclose individuals’ confidential and private data, raising legal and ethical concerns. While many previous works have addressed this issue in LLM via machine unlearning, it remains largely unexplored for MLLMs. To tackle this challenge, we introduce Multimodal Large Language Model Unlearning Benchmark (MLLMU-Bench), a novel benchmark aimed at... | Zheyuan Liu, Guangyao Dou, Mengzhao Jia, Zhaoxuan Tan, Qingkai Zeng, Yongle Yuan, Meng Jiang |  |
| 289 |  |  [LLM4DistReconfig: A Fine-tuned Large Language Model for Power Distribution Network Reconfiguration](https://doi.org/10.18653/v1/2025.naacl-long.208) |  | 0 | Power distribution networks are evolving due to the integration of distributed energy resources (DERs) and increased customer participation. To maintain optimal operation, minimize losses, and meet varying load demands, frequent network reconfiguration is necessary. Traditionally, the reconfiguration task relies on optimization software and expert operators, but as systems grow more complex, faster and more adaptive solutions are required without expert intervention. Data-driven reconfiguration... | Panayiotis Christou, Md. Zahidul Islam, Yuzhang Lin, Jingwei Xiong |  |
| 290 |  |  [WaterPool: A Language Model Watermark Mitigating Trade-Offs among Imperceptibility, Efficacy and Robustness](https://doi.org/10.18653/v1/2025.naacl-long.209) |  | 0 | Watermarking is a prominent technique to trace the usage of specific large language models (LLMs) by injecting patterns into model-generated content. An ideal watermark should be imperceptible, easily detectable, and robust to text alterations, yet existing methods typically face trade-offs among these properties. This paper utilizes a key-centered scheme to unify existing methods by decomposing a watermark into two components: a key module and a mark module. We show that the trade-off issue is... | Baizhou Huang, Xiaojun Wan |  |
| 291 |  |  [Tricking Retrievers with Influential Tokens: An Efficient Black-Box Corpus Poisoning Attack](https://doi.org/10.18653/v1/2025.naacl-long.210) |  | 0 | Retrieval-augmented generation (RAG) systems enhance large language models by incorporating external knowledge, addressing issues like outdated internal knowledge and hallucination. However, their reliance on external knowledge bases makes them vulnerable to corpus poisoning attacks, where adversarial passages can be injected to manipulate retrieval results. Existing methods for crafting such passages, such as random token replacement or training inversion models, are often slow and... | Cheng Wang, Yiwei Wang, Yujun Cai, Bryan Hooi |  |
| 292 |  |  [The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism](https://doi.org/10.18653/v1/2025.naacl-long.211) |  | 0 | Current evaluations of large language models (LLMs) often overlook non-determinism, typically focusing on a single output per example. This limits our understanding of LLM performance variability in real-world applications. Our study addresses this issue by exploring key questions about the performance differences between greedy decoding and sampling, identifying benchmarks’ consistency regarding non-determinism, and examining unique model behaviors. Through extensive experiments, we observe... | Yifan Song, Guoyin Wang, Sujian Li, Bill Yuchen Lin |  |
| 293 |  |  [CVE-Bench: Benchmarking LLM-based Software Engineering Agent's Ability to Repair Real-World CVE Vulnerabilities](https://doi.org/10.18653/v1/2025.naacl-long.212) |  | 0 | Automated vulnerability repair is a crucial field within software engineering and security research. Large Language Models (LLMs) and LLM agents have demonstrated significant potential in this domain by understanding descriptions in natural language and generating corresponding formal code. Although the coding capabilities of LLMs have advanced rapidly, evaluation benchmarks for real-world programming setups are still lagging, preventing the development of LLM and LLM agents in real-world... | Peiran Wang, Xiaogeng Liu, Chaowei Xiao |  |
| 294 |  |  [PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines](https://doi.org/10.18653/v1/2025.naacl-long.213) |  | 0 | Large language models (LLMs) are increasingly deployed in specialized production data processing pipelines across diverse domains—such as finance, marketing, and e-commerce. However, when running them in production across many inputs, they often fail to follow instructions or meet developer expectations. To improve reliability in these applications, creating assertions or guardrails for LLM outputs to run alongside the pipelines is essential. Yet, determining the right set of assertions that... | Reya Vir, Shreya Shankar, Harrison Chase, Will FuHinthorn, Aditya G. Parameswaran |  |
| 295 |  |  [ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent Dialogue Synthesis](https://doi.org/10.18653/v1/2025.naacl-long.214) |  | 0 | Supervised fine-tuning (SFT) is a common method to enhance the tool calling capabilities of Large Language Models (LLMs), with the training data often being synthesized. The current data synthesis process generally involves sampling a set of tools, formulating a requirement based on these tools, and generating the call statements. However, tools sampled randomly lack relevance, making them difficult to combine and thus reducing the diversity of the data. Additionally, current work overlooks the... | Zezhong Wang, Xingshan Zeng, Weiwen Liu, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, KamFai Wong |  |
| 296 |  |  [Fighting Spurious Correlations in Text Classification via a Causal Learning Perspective](https://doi.org/10.18653/v1/2025.naacl-long.215) |  | 0 | In text classification tasks, models often rely on spurious correlations for predictions, incorrectly associating irrelevant features with the target labels. This issue limits the robustness and generalization of models, especially when faced with out-of-distribution data where such spurious correlations no longer hold. To address this challenge, we propose the Causally Calibrated Robust Classifier (CCR), which aims to reduce models’ reliance on spurious correlations and improve model... | Yuqing Zhou, Ziwei Zhu |  |
| 297 |  |  [Knowledge-Aware Query Expansion with Large Language Models for Textual and Relational Retrieval](https://doi.org/10.18653/v1/2025.naacl-long.216) |  | 0 | Large language models (LLMs) have been used to generate query expansions augmenting original queries for improving information search. Recent studies also explore providing LLMs with initial retrieval results to generate query expansions more grounded to document corpus. However, these methods mostly focus on enhancing textual similarities between search queries and target documents, overlooking document relations. For queries like “Find me a highly rated camera for wildlife photography... | Yu Xia, Junda Wu, Sungchul Kim, Tong Yu, Ryan A. Rossi, Haoliang Wang, Julian J. McAuley |  |
| 298 |  |  [SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression](https://doi.org/10.18653/v1/2025.naacl-long.217) |  | 0 | Despite significant advancements, the practical deployment of Large Language Models (LLMs) is often hampered by their immense sizes, highlighting the need for effective compression techniques. Singular Value Decomposition (SVD) emerges as a promising method for compressing LLMs. However, existing SVD-based compression approaches suffer from substantial truncation losses, leading to severe performance degradation in compressed models. In this work, we introduce , a novel SVD-based LLM... | Xin Wang, Samiul Alam, Zhongwei Wan, Hui Shen, Mi Zhang |  |
| 299 |  |  [AudioBench: A Universal Benchmark for Audio Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.218) |  | 0 | We introduce AudioBench, a universal benchmark designed to evaluate Audio Large Language Models (AudioLLMs). It encompasses 8 distinct tasks and 26 datasets, among which, 7 are newly proposed datasets. The evaluation targets three main aspects: speech understanding, audio scene understanding, and voice understanding (paralinguistic). Despite recent advancements, there lacks a comprehensive benchmark for AudioLLMs on instruction following capabilities conditioned on audio signals. AudioBench... | Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, Nancy F. Chen |  |
| 300 |  |  [Efficient Prompting for Continual Adaptation to Missing Modalities](https://doi.org/10.18653/v1/2025.naacl-long.219) |  | 0 | Missing modality issues are common in real-world applications, arising from factors such as equipment failures and privacy concerns. When fine-tuning pre-trained models on downstream datasets with missing modalities, performance can degrade significantly. Current methods often aggregate various missing cases to train recovery modules or align multimodal features, resulting in suboptimal performance, high computational costs, and the risk of catastrophic forgetting in continual environments... | Zirun Guo, Shulei Wang, Wang Lin, Weicai Yan, Yangyang Wu, Tao Jin |  |
| 301 |  |  [Benchmarking and Building Zero-Shot Hindi Retrieval Model with Hindi-BEIR and NLLB-E5](https://doi.org/10.18653/v1/2025.naacl-long.220) |  | 0 | Given the large number of Hindi speakers worldwide, there is a pressing need for robust and efficient information retrieval systems for Hindi. Despite ongoing research, comprehensive benchmarks for evaluating retrieval models in Hindi are lacking. To address this gap, we introduce the Hindi-BEIR benchmark, comprising 15 datasets across seven distinct tasks. We evaluate state-of-the-art multilingual retrieval models on the Hindi-BEIR benchmark, identifying task and domain-specific challenges... | Arkadeep Acharya, Rudra Murthy, Vishwajeet Kumar, Jaydeep Sen |  |
| 302 |  |  [Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion](https://doi.org/10.18653/v1/2025.naacl-long.221) |  | 0 | The Knowledge Graph Completion (KGC) task aims to infer the missing entity from an incomplete triple. Existing embedding-based methods rely solely on triples in the KG, which is vulnerable to specious relation patterns and long-tail entities. On the other hand, text-based methods struggle with the semantic gap between KG triples and natural language. Apart from triples, entity contexts (e.g., labels, descriptions, aliases) also play a significant role in augmenting KGs. To address these... | Muzhi Li, Cehao Yang, Chengjin Xu, Xuhui Jiang, Yiyan Qi, Jian Guo, Hofung Leung, Irwin King |  |
| 303 |  |  [See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language Balance to Mitigate Dominant Modality Bias](https://doi.org/10.18653/v1/2025.naacl-long.222) |  | 0 | Vision-language (VL) models have demonstrated strong performance across various tasks. However, these models often rely on a specific modality for predictions, leading to “dominant modality bias.” This bias significantly hurts performance, especially when one modality is impaired. In this study, we analyze model behavior under dominant modality bias and theoretically show that unaligned gradients or differences in gradient magnitudes prevent balanced convergence of the loss. Based on these... | Junehyoung Kwon, Mihyeon Kim, Eunju Lee, Juhwan Choi, YoungBin Kim |  |
| 304 |  |  [Harnessing and Evaluating the Intrinsic Extrapolation Ability of Large Language Models for Vehicle Trajectory Prediction](https://doi.org/10.18653/v1/2025.naacl-long.223) |  | 0 | Emergent abilities of large language models (LLMs) have significantly advanced their application in autonomous vehicle (AV) research. Safe integration of LLMs into vehicles, however, necessitates their thorough understanding of dynamic traffic environments. Towards this end, this study introduces a framework leveraging LLMs’ built-in extrapolation capabilities for vehicle trajectory prediction, thereby evaluating their comprehension of the evolution of traffic agents’ behaviors and interactions... | Jiawei Liu, Yanjiao Liu, Xun Gong, Tingting Wang, Hong Chen, Yunfeng Hu |  |
| 305 |  |  [Stronger Models are Not Always Stronger Teachers for Instruction Tuning](https://doi.org/10.18653/v1/2025.naacl-long.224) |  | 0 | Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions and engage with users meaningfully. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger... | Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Radha Poovendran |  |
| 306 |  |  [Efficient and Effective Prompt Tuning via Prompt Decomposition and Compressed Outer Product](https://doi.org/10.18653/v1/2025.naacl-long.225) |  | 0 | Prompt tuning (PT) offers a cost-effective alternative to fine-tuning large-scale pre-trained language models (PLMs), requiring only a few parameters in soft prompt tokens added before the input text. However, existing PT approaches face two significant issues: i They overlook intrinsic semantic associations between soft prompt tokens, leading to high discreteness and limited interactions, thus reducing the model’s comprehension and effectiveness in complex tasks. ii Due to the complexity of... | Pengxiang Lan, Haoyu Xu, Enneng Yang, Yuliang Liang, Guibing Guo, Jianzhe Zhao, Xingwei Wang |  |
| 307 |  |  [Threshold Filtering Packing for Supervised Fine-Tuning: Training Related Samples within Packs](https://doi.org/10.18653/v1/2025.naacl-long.226) |  | 0 | Packing for Supervised Fine-Tuning (SFT) in autoregressive models involves concatenating data points of varying lengths until reaching the designed maximum length to facilitate GPU processing. However, randomly concatenating data points can lead to cross-contamination of sequences due to the significant difference in their subject matter. The mainstream approaches in SFT ensure that each token in the attention calculation phase only focuses on tokens within its own short sequence, without... | Jiancheng Dong, Lei Jiang, Wei Jin, Lu Cheng |  |
| 308 |  |  [Transferable Post-training via Inverse Value Learning](https://doi.org/10.18653/v1/2025.naacl-long.227) |  | 0 | As post-training processes utilize increasingly large datasets and base models continue to grow in size, the computational demands and implementation challenges of existing algorithms are escalating significantly. In this paper, we propose modeling the changes at the logits level during post-training using a separate neural network (i.e., the value network). After training this network on a small base model using demonstrations, this network can be seamlessly integrated with another pre-trained... | Xinyu Lu, Xueru Wen, Yaojie Lu, Bowen Yu, Hongyu Lin, Haiyang Yu, Le Sun, Xianpei Han, Yongbin Li |  |
| 309 |  |  [FLEX: Expert-level False-Less EXecution Metric for Text-to-SQL Benchmark](https://doi.org/10.18653/v1/2025.naacl-long.228) |  | 0 | Text-to-SQL systems have become crucial for translating natural language into SQL queries in various industries, enabling non-technical users to perform complex data operations. The need for accurate evaluation methods has increased as these systems have grown more sophisticated. However, the Execution Accuracy (EX), the most prevalent evaluation metric, still shows many false positives and negatives. Thus, this paper introduces \*\*FLEX(False-Less EXecution)\*\*, a novel approach to evaluating... | Heegyu Kim, Taeyang Jeon, Seunghwan Choi, Seungtaek Choi, Hyunsouk Cho |  |
| 310 |  |  [AID: Adaptive Integration of Detectors for Safe AI with Language Models](https://doi.org/10.18653/v1/2025.naacl-long.229) |  | 0 | As Large Language Models (LLMs) increasingly influence content generation across diverse platforms, there is a heightened urgency to regulate their outputs to ensure safe usage. However, defining safety is complex, given that entities across domains may interpret it through varied lenses and develop safety detectors—models trained to identify specific unsafe content based on predefined criteria. To address this complexity, we introduce the approach of Adaptive Integration of Detectors (AID) to... | Xinran Wang, Enmao Diao, Qi Le, Jie Ding, Ali Anwar |  |
| 311 |  |  [SSMLoRA: Enhancing Low-Rank Adaptation with State Space Model](https://doi.org/10.18653/v1/2025.naacl-long.230) |  | 0 | Fine-tuning is a key approach for adapting language models to specific downstream tasks, but updating all model parameters becomes impractical as model sizes increase.Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), address this challenge by introducing additional adaptation parameters into pre-trained weight matrices.However, LoRA’s performance varies across different insertion points within the model, highlighting potential parameter inefficiency due to... | Jiayang Yu, Yihang Zhang, Bin Wang, Peiqin Lin, Yongkang Liu, Shi Feng |  |
| 312 |  |  [Sharpness-Aware Minimization for Topic Models with High-Quality Document Representations](https://doi.org/10.18653/v1/2025.naacl-long.231) |  | 0 | Recent advanced frameworks in topic models have significantly enhanced the performance compared to conventional probabilistic approaches. Such models, mostly constructed from neural network architecture together with other advanced techniques such as contextual embedding, optimal transport distance and pre-trained language model, etc. have effectively improved the topic quality and document topic distribution. Despite the improvements, these methods lack considerations of effective optimization... | Tung Nguyen, Tue Le, Hoang Tran Vuong, Quang Duc Nguyen, Duc Anh Nguyen, Linh Ngo Van, Sang Dinh, Thien Huu Nguyen |  |
| 313 |  |  [C²: Scalable Auto-Feedback for LLM-based Chart Generation](https://doi.org/10.18653/v1/2025.naacl-long.232) |  | 0 |  | Woosung Koh, Jang Han Yoon, Minhyung Lee, Youngjin Song, Jaegwan Cho, Jaehyun Kang, Taehyeon Kim, SeYoung Yun, Youngjae Yu, Bongshin Lee |  |
| 314 |  |  [A Top-down Graph-based Tool for Modeling Classical Semantic Maps: A Case Study of Supplementary Adverbs](https://doi.org/10.18653/v1/2025.naacl-long.233) |  | 0 | Semantic map models (SMMs) construct a network-like conceptual space from cross-linguistic instances or forms, based on the connectivity hypothesis. This approach has been widely used to represent similarity and entailment relationships in cross-linguistic concept comparisons. However, most SMMs are manually built by human experts using bottom-up procedures, which are often labor-intensive and time-consuming. In this paper, we propose a novel graph-based algorithm that automatically generates... | Zhu Liu, Cunliang Kong, Ying Liu, Maosong Sun |  |
| 315 |  |  [UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers](https://doi.org/10.18653/v1/2025.naacl-long.234) |  | 0 | Existing information retrieval (IR) models often assume a homogeneous structure for knowledge sources and user queries, limiting their applicability in real-world settings where retrieval is inherently heterogeneous and diverse. In this paper, we introduce UniHGKR, a unified instruction-aware heterogeneous knowledge retriever that (1) builds a unified retrieval space for heterogeneous knowledge and (2) follows diverse user instructions to retrieve knowledge in specified types. UniHGKR consists... | Dehai Min, Zhiyang Xu, Guilin Qi, Lifu Huang, Chenyu You |  |
| 316 |  |  [Improving Model Evaluation using SMART Filtering of Benchmark Datasets](https://doi.org/10.18653/v1/2025.naacl-long.235) |  | 0 | One of the most challenging problems facing NLP today is evaluation. Some of the most pressing issues pertain to benchmark saturation, data contamination, and diversity in the quality of test examples. To address these concerns, we propose Selection Methodology for Accurate, Reduced, and Targeted (SMART) filtering, a novel approach to select a high-quality subset of examples from existing benchmark datasets by systematically removing less informative and lower quality examples. Our approach... | Vipul Gupta, Candace Ross, David Pantoja, Rebecca J. Passonneau, Megan Ung, Adina Williams |  |
| 317 |  |  [Entropy-Based Decoding for Retrieval-Augmented Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.236) |  | 0 | Augmenting Large Language Models (LLMs) with retrieved external knowledge has proven effective in improving the factual accuracy of generated responses. Despite their success, retrieval-augmented LLMs still face the distractibility issue, where the generated responses are negatively influenced by noise from both external and internal knowledge sources. In this paper, we introduce a novel, training-free decoding method guided by entropy considerations to mitigate this issue. Our approach... | Zexuan Qiu, Zijing Ou, Bin Wu, Jingjing Li, Aiwei Liu, Irwin King |  |
| 318 |  |  [What We Talk About When We Talk About LMs: Implicit Paradigm Shifts and the Ship of Language Models](https://doi.org/10.18653/v1/2025.naacl-long.237) |  | 0 | The term Language Models (LMs) as a time-specific collection of models of interest is constantly reinvented, with its referents updated much like the \*Ship of Theseus\* replaces its parts but remains the same ship in essence. In this paper, we investigate this \*Ship of Language Models\* problem, wherein scientific evolution takes the form of continuous, implicit retrofits of key \*existing\* terms. We seek to initiate a novel perspective of scientific progress, in addition to the more... | Shengqi Zhu, Jeffrey M. Rzeszotarski |  |
| 319 |  |  [Diversity Helps Jailbreak Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.238) |  | 0 | We have uncovered a powerful jailbreak technique that leverages large language models’ ability to diverge from prior context, enabling them to bypass safety constraints and generate harmful outputs. By simply instructing the LLM to deviate and obfuscate previous attacks, our method dramatically outperforms existing approaches, achieving up to a 62.83% higher success rate in compromising ten leading chatbots, including GPT-4, Gemini, and Llama, while using only 12.9% of the queries. This... | Weiliang Zhao, Daniel BenLevi, Wei Hao, Junfeng Yang, Chengzhi Mao |  |
| 320 |  |  [Constrained Decoding with Speculative Lookaheads](https://doi.org/10.18653/v1/2025.naacl-long.239) |  | 0 | Constrained decoding with lookahead heuristics (CDLH) is a highly effective method for aligning LLM generations to human preferences. However, the extensive lookahead roll-out operations for each generated token makes CDLH prohibitively expensive, resulting in low adoption in practice. In contrast, common decoding strategies such as greedy decoding are extremely efficient, but achieve very low constraint satisfaction. We propose constrained decoding with speculative lookaheads (CDSL), a... | Nishanth Sridhar Nakshatri, Shamik Roy, Rajarshi Das, Suthee Chaidaroon, Leonid Boytsov, Rashmi Gangadharaiah |  |
| 321 |  |  [DyPCL: Dynamic Phoneme-level Contrastive Learning for Dysarthric Speech Recognition](https://doi.org/10.18653/v1/2025.naacl-long.240) |  | 0 | Dysarthric speech recognition often suffers from performance degradation due to the intrinsic diversity of dysarthric severity and extrinsic disparity from normal speech. To bridge these gaps, we propose a Dynamic Phoneme-level Contrastive Learning (DyPCL) method, which leads to obtaining invariant representations across diverse speakers. We decompose the speech utterance into phoneme segments for phoneme-level contrastive learning, leveraging dynamic connectionist temporal classification... | Wonjun Lee, Solee Im, Heejin Do, Yunsu Kim, Jungseul Ok, Gary Lee |  |
| 322 |  |  [Revisiting Early Detection of Sexual Predators via Turn-level Optimization](https://doi.org/10.18653/v1/2025.naacl-long.241) |  | 0 | Online grooming is a severe social threat where sexual predators gradually entrap child victims with subtle and gradual manipulation. Therefore, timely intervention for online grooming is critical for proactive protection. However, previous methods fail to determine the optimal intervention points (i.e., jump to conclusions) as they rely on chat-level risk labels by causing weak supervision of risky utterances. For timely detection, we propose speed control reinforcement learning (SCoRL),... | Jinmyeong An, Sangwon Ryu, Heejin Do, Yunsu Kim, Jungseul Ok, Gary Lee |  |
| 323 |  |  [StyleTTS-ZS: Efficient High-Quality Zero-Shot Text-to-Speech Synthesis with Distilled Time-Varying Style Diffusion](https://doi.org/10.18653/v1/2025.naacl-long.242) |  | 0 | The rapid development of large-scale text-to-speech (TTS) models has led to significant advancements in modeling diverse speaker prosody and voices. However, these models often face issues such as slow inference speeds, reliance on complex pre-trained neural codec representations, and difficulties in achieving naturalness and high similarity to reference speakers. To address these challenges, this work introduces StyleTTS-ZS, an efficient zero-shot TTS model that leverages distilled... | Yinghao Aaron Li, Xilin Jiang, Cong Han, Nima Mesgarani |  |
| 324 |  |  [Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-long.243) |  | 0 | Large Language Models (LLMs) have demonstrated significant performance improvements across various cognitive tasks. An emerging application is using LLMs to enhance retrieval-augmented generation (RAG) capabilities. These systems require LLMs to understand user queries, retrieve relevant information, and synthesize coherent and accurate responses. Given the increasing real-world deployment of such systems, comprehensive evaluation becomes crucial. To this end, we propose FRAMES (Factuality,... | Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, Manaal Faruqui |  |
| 325 |  |  [ReachAgent: Enhancing Mobile Agent via Page Reaching and Operation](https://doi.org/10.18653/v1/2025.naacl-long.244) |  | 0 | Recently, mobile AI agents have gained increasing attention. Given a task, mobile AI agents can interact with mobile devices in multiple steps and finally form a GUI flow that solves the task. However, existing agents tend to focus on most task-relevant elements at each step, leading to local optimal solutions and ignoring the overall GUI flow. To address this issue, we constructed a training dataset called MobileReach, which breaks the task into page reaching and operation subtasks.... | Qinzhuo Wu, Wei Liu, Jian Luan, Bin Wang |  |
| 326 |  |  [Learning to Solve Domain-Specific Calculation Problems with Knowledge-Intensive Programs Generator](https://doi.org/10.18653/v1/2025.naacl-long.245) |  | 0 | Domain Large Language Models (LLMs) are developed for domain-specific tasks based on general LLMs. But it still requires professional knowledge to facilitate the expertise for some domain-specific tasks. In this paper, we investigate into knowledge-intensive calculation problems. We find that the math problems to be challenging for LLMs, when involving complex domain-specific rules and knowledge documents, rather than simple formulations of terminologies. Therefore, we propose a pipeline to... | Chengyuan Liu, Shihang Wang, Lizhi Qing, Jun Lin, Ji Zhang, Fei Wu, Kun Kuang |  |
| 327 |  |  [SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity Mixture](https://doi.org/10.18653/v1/2025.naacl-long.246) |  | 0 | Despite the recent efforts from the NLP community, balancing the training budget, downstream performance, and general capabilities of large language models (LLM) remains a challenge in many applications. Training the entire model for downstream tasks is expensive, and could easily result in catastrophic forgetting. Parameter-efficient fine-tuning (PEFT) could reduce the training cost, but it still suffers from forgetting, and limits the learning on the downstream tasks. To address the... | Jiayi Han, Liang Du, Hongwei Du, Xiangguo Zhou, Yiwen Wu, Yuanfang Zhang, Weibo Zheng, Donghong Han |  |
| 328 |  |  [MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.247) |  | 0 | Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding and reasoning abilities, often assessed through multiple-choice questions (MCQs) that include an image, a question, and several options. However, many benchmarks used for such evaluations suffer from systematic biases. Remarkably, Large Language Models (LLMs) without any visual perception capabilities achieve non-trivial performance, undermining the credibility of these evaluations. To address this issue while... | Jinsheng Huang, Liang Chen, Taian Guo, Fu Zeng, Yusheng Zhao, Bohan Wu, Ye Yuan, Haozhe Zhao, Zhihui Guo, Yichi Zhang, Jingyang Yuan, Wei Ju, Luchen Liu, Tianyu Liu, Baobao Chang, Ming Zhang |  |
| 329 |  |  [MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning](https://doi.org/10.18653/v1/2025.naacl-long.248) |  | 0 | Efficient finetuning of large language models (LLMs) aims to adapt the LLMs with reduced computational and memory costs. Previous LoRA-based approaches initialize the low-rank matrices with Gaussian distribution and zero values while keeping the original weight matrices frozen. However, the trainable model parameters optimized in an unguided subspace might interfere with the well-learned subspace of the pretrained weight matrices. In this paper, we propose MiLoRA, a simple yet effective LLM... | Hanqing Wang, Yixia Li, Shuo Wang, Guanhua Chen, Yun Chen |  |
| 330 |  |  [Analyzing (In)Abilities of SAEs via Formal Languages](https://doi.org/10.18653/v1/2025.naacl-long.249) |  | 0 | Autoencoders have been used for finding interpretable and disentangled features underlying neural network representations in both image and text domains. While the efficacy and pitfalls of such methods are well-studied in vision, there is a lack of corresponding results, both qualitative and quantitative, for the text domain. We aim to address this gap by training sparse autoencoders (SAEs) on a synthetic testbed of formal languages. Specifically, we train SAEs on the hidden representations of... | Abhinav Menon, Manish Shrivastava, David Krueger, Ekdeep Singh Lubana |  |
| 331 |  |  [Multimodal Cognitive Reframing Therapy via Multi-hop Psychotherapeutic Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.250) |  | 0 | Previous research has revealed the potential of large language models (LLMs) to support cognitive reframing therapy; however, their focus was primarily on text-based methods, often overlooking the importance of non-verbal evidence crucial in real-life therapy. To alleviate this gap, we extend the textual cognitive reframing to multimodality, incorporating visual clues. Specifically, we present a new dataset called Multi Modal-Cognitive Support Conversation (M2CoSC), which pairs each... | Subin Kim, Hoonrae Kim, Heejin Do, Gary Lee |  |
| 332 |  |  [Explanation based In-Context Demonstrations Retrieval for Multilingual Grammatical Error Correction](https://doi.org/10.18653/v1/2025.naacl-long.251) |  | 0 | Grammatical error correction (GEC) aims to correct grammatical, spelling, and semantic errors in natural language text. With the growing of large language models (LLMs), direct text generation has gradually become the focus of the GEC methods, and few-shot in-context learning presents a cost-effective solution. However, selecting effective in-context examples remains challenging, as the similarity between input texts does not necessarily correspond to similar grammatical error patterns. In this... | Wei Li, Wen Luo, Guangyue Peng, Houfeng Wang |  |
| 333 |  |  [A Unified Supervised and Unsupervised Dialogue Topic Segmentation Framework Based on Utterance Pair Modeling](https://doi.org/10.18653/v1/2025.naacl-long.252) |  | 0 | The Dialogue Topic Segmentation task aims to divide a dialogue into different topic paragraphs in order to better understand the structure and content of the dialogue. Due to the short sentences, serious references and non-standard language in the dialogue, it is difficult to determine the boundaries of the topic. Although the unsupervised approaches based on LLMs performs well, it is still difficult to surpass the supervised methods based on classical models in specific domains. To this end,... | Shihao Yang, Ziyi Zhang, Yue Jiang, Chunsheng Qin, Shuhua Liu |  |
| 334 |  |  [Evaluating Small Language Models for News Summarization: Implications and Factors Influencing Performance](https://doi.org/10.18653/v1/2025.naacl-long.253) |  | 0 | The increasing demand for efficient summarization tools in resource-constrained environments highlights the need for effective solutions. While large language models (LLMs) deliver superior summarization quality, their high computational resource requirements limit practical use applications. In contrast, small language models (SLMs) present a more accessible alternative, capable of real-time summarization on edge devices. However, their summarization capabilities and comparative performance... | Borui Xu, Yao Chen, Zeyi Wen, Weiguo Liu, Bingsheng He |  |
| 335 |  |  [Dynamic Fisher-weighted Model Merging via Bayesian Optimization](https://doi.org/10.18653/v1/2025.naacl-long.254) |  | 0 | The fine-tuning of pre-trained language models has resulted in the widespread availability of task-specific models. Model merging offers an efficient way to create multi-task models by combining these fine-tuned models at the parameter level, without the need for training data or joint training on multiple datasets. Existing merging approaches typically involve scaling the parameters model-wise or integrating parameter importance parameter-wise. Both approaches exhibit their own weaknesses,... | Sanwoo Lee, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Yunfang Wu |  |
| 336 |  |  [AI-Assisted Human Evaluation of Machine Translation](https://doi.org/10.18653/v1/2025.naacl-long.255) |  | 0 |  | Vilém Zouhar, Tom Kocmi, Mrinmaya Sachan |  |
| 337 |  |  [MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria](https://doi.org/10.18653/v1/2025.naacl-long.256) |  | 0 | Multimodal large language models (MLLMs) have broadened the scope of AI applications. Existing automatic evaluation methodologies for MLLMs are mainly limited in evaluating objective queries without considering real-world user experiences, inadequately addressing the nuances of creative and associative multimodal tasks. However, the open-ended and subjective nature of such tasks poses a significant challenge to the evaluation methodology, where it is difficult to define the ground-truth answers... | Wentao Ge, Shunian Chen, Hardy Chen, Nuo Chen, Junying Chen, Zhihong Chen, Wenya Xie, Shuo Yan, ChenghaoZhu ChenghaoZhu, Ziyue Lin, Dingjie Song, Xidong Wang, Anningzhe Gao, Zhiyi Zhang, Jianquan Li, Xiang Wan, Benyou Wang |  |
| 338 |  |  [AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios](https://doi.org/10.18653/v1/2025.naacl-long.257) |  | 0 | Large language models (LLMs) are increasingly leveraged to empower autonomous agents to simulate human beings in various fields of behavioral research. However, evaluating their capacity to navigate complex social interactions remains a challenge. Previous studies face limitations due to insufficient scenario diversity, complexity, and a single-perspective focus. To this end, we introduce AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios. Drawing on... | Xinyi Mou, Jingcong Liang, Jiayu Lin, Xinnong Zhang, Xiawei Liu, Shiyue Yang, Rong Ye, Lei Chen, Haoyu Kuang, Xuanjing Huang, Zhongyu Wei |  |
| 339 |  |  [FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data](https://doi.org/10.18653/v1/2025.naacl-long.258) |  | 0 | Prior research on training grounded factuality classification models to detect hallucinations in large language models (LLMs) has relied on public natural language inference (NLI) data and synthetic data. However, conventional NLI datasets are not well-suited for document-level reasoning, which is critical for detecting LLM hallucinations. Recent approaches to document-level synthetic data generation involve iteratively removing sentences from documents and annotating factuality using LLM-based... | Deren Lei, Yaxi Li, Siyao Li, Mengya Hu, Rui Xu, Ken Archer, Mingyu Wang, Emily Ching, Alex Deng |  |
| 340 |  |  [Label Drop for Multi-Aspect Relation Modeling in Universal Information Extraction](https://doi.org/10.18653/v1/2025.naacl-long.259) |  | 0 | Universal Information Extraction (UIE) has garnered significant attention due to its ability to address model explosion problems effectively. Extractive UIE can achieve strong performance using a relatively small model, making it widely adopted. Extractive UIEs generally rely on task instructions for different tasks, including single-target instructions and multiple-target instructions. Single-target instruction UIE enables the extraction of only one type of relation at a time, limiting its... | Lu Yang, Jiajia Li, En Ci, Lefei Zhang, Zuchao Li, Ping Wang |  |
| 341 |  |  [Test-Time Code-Switching for Cross-lingual Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2025.naacl-long.260) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) is a thriving research area with impressive outcomes being achieved on high-resource languages. However, the application of cross-lingual transfer to the ASTE task has been relatively unexplored, and current code-switching methods still suffer from term boundary detection issues and out-of-dictionary problems. In this study, we introduce a novel Test-Time Code-SWitching (TT-CSW) framework, which bridges the gap between the bilingual training phase and... | Dongming Sheng, Kexin Han, Hao Li, Yan Zhang, Yucheng Huang, Jun Lang, Wenqiang Liu |  |
| 342 |  |  [VisCGEC: Benchmarking the Visual Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2025.naacl-long.261) |  | 0 |  | Xiaoman Wang, Dan Yuan, Xin Liu, Yike Zhao, Xiaoxiao Zhang, Xizhi Chen, Yunshi Lan |  |
| 343 |  |  [Are We Done with MMLU?](https://doi.org/10.18653/v1/2025.naacl-long.262) |  | 0 | Maybe not. We identify and analyse errors in the popular Massive Multitask Language Understanding (MMLU) benchmark. Even though MMLU is widely adopted, our analysis demonstrates numerous ground truth errors that obscure the true capabilities of LLMs. For example, we find that 57% of the analysed questions in the Virology subset contain errors. To address this issue, we introduce a comprehensive framework for identifying dataset errors using a novel error annotation protocol. Then, we create... | Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, Pasquale Minervini |  |
| 344 |  |  [MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling](https://doi.org/10.18653/v1/2025.naacl-long.263) |  | 0 | Integrating tools into Large Language Models (LLMs) has facilitated the widespread application. Despite this, in specialized downstream task contexts, reliance solely on tools is insufficient to fully address the complexities of the real world. This particularly restricts the effective deployment of LLMs in fields such as medicine. In this paper, we focus on the downstream tasks of medical calculators, which use standardized tests to assess an individual’s health status. We introduce MeNTi, a... | Yakun Zhu, Shaohang Wei, Xu Wang, Kui Xue, Shaoting Zhang, Xiaofan Zhang |  |
| 345 |  |  [Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering](https://doi.org/10.18653/v1/2025.naacl-long.264) |  | 0 | Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context—this phenomenon, known as context-memory knowledge conflicts, can lead to undesirable model behaviour, such as reliance on outdated or incorrect information. Analysing the internal activations of LLMs, we find that they can internally register the signals of knowledge conflict at mid-layers. Such... | Yu Zhao, Alessio Devoto, Giwon Hong, Xiaotang Du, Aryo Pradipta Gema, Hongru Wang, Xuanli He, KamFai Wong, Pasquale Minervini |  |
| 346 |  |  [MoDification: Mixture of Depths Made Easy](https://doi.org/10.18653/v1/2025.naacl-long.265) |  | 0 | Long-context efficiency has recently become a trending topic in serving large language models (LLMs). And mixture of depths (MoD) is proposed as a perfect fit to bring down both latency and memory. In this paper, however, we discover that MoD can barely transform existing LLMs without costly training over an extensive number of tokens. To enable the transformations from any LLMs to MoD ones, we showcase top-k operator in MoD should be promoted to threshold-p operator, and refinement to... | Chen Zhang, Meizhi Zhong, Qimeng Wang, Xuantao Lu, Zheyu Ye, Chengqiang Lu, Yan Gao, Yao Hu, Kehai Chen, Min Zhang, Dawei Song |  |
| 347 |  |  [On the Vulnerability of Text Sanitization](https://doi.org/10.18653/v1/2025.naacl-long.266) |  | 0 | Text sanitization, which employs differential privacy to replace sensitive tokens with new ones, represents a significant technique for privacy protection. Typically, its performance in preserving privacy is evaluated by measuring the attack success rate (ASR) of reconstruction attacks, where attackers attempt to recover the original tokens from the sanitized ones. However, current reconstruction attacks on text sanitization are developed empirically, making it challenging to accurately assess... | Meng Tong, Kejiang Chen, Xiaojian Yuan, Jiayang Liu, Weiming Zhang, Nenghai Yu, Jie Zhang |  |
| 348 |  |  [Multilingual Needle in a Haystack: Investigating Long-Context Behavior of Multilingual Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.267) |  | 0 | While recent large language models (LLMs) demonstrate remarkable abilities in responding to queries in diverse languages, their ability to handle long multilingual contexts is unexplored. As such, a systematic evaluation of the long-context capabilities of LLMs in multilingual settings is crucial, specifically in the context of information retrieval. To address this gap, we introduce the MultiLingual Needle-in-a-Haystack (MLNeedle) test, designed to assess a model’s ability to retrieve relevant... | Amey Hengle, Prasoon Bajpai, Soham Dan, Tanmoy Chakraborty |  |
| 349 |  |  [Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation](https://doi.org/10.18653/v1/2025.naacl-long.268) |  | 0 | Claim verification is a long-standing and challenging task that demands not only high accuracy but also explainability and thoroughness of the verification process. This task becomes an emerging research issue in the era of large language models (LLMs) since real-world claims are often complex, featuring intricate semantic structures or obfuscated entities. Traditional approaches typically address this by decomposing claims into sub-claims and querying a knowledge base to resolve hidden or... | Hoang Pham, ThanhDo Nguyen, KhacHoai Nam Bui |  |
| 350 |  |  [Exploring the Potential of Large Language Models for Heterophilic Graphs](https://doi.org/10.18653/v1/2025.naacl-long.269) |  | 0 | Large language models (LLMs) have presented significant opportunities to enhance various machine learning applications, including graph neural networks (GNNs). By leveraging the vast open-world knowledge within LLMs, we can more effectively interpret and utilize textual data to better characterize heterophilic graphs, where neighboring nodes often have different labels. However, existing approaches for heterophilic graphs overlook the rich textual data associated with nodes, which could unlock... | Yuxia Wu, Shujie Li, Yuan Fang, Chuan Shi |  |
| 351 |  |  [Exploiting Edited Large Language Models as General Scientific Optimizers](https://doi.org/10.18653/v1/2025.naacl-long.270) |  | 0 | Large language models (LLMs) have been widely adopted in mathematical optimization in scientific scenarios for their extensive knowledge and advanced reasoning capabilities. Existing methods mainly focus on utilizing LLMs to solve optimization problems in a prompt-based manner, which takes observational feedback as additional textual descriptions. However, due to LLM’s \*\*high sensitivity to the prompts\*\* and \*\*tendency to get lost in lengthy prompts\*\*, these methods struggle to... | Qitan Lv, Tianyu Liu, Hong Wang |  |
| 352 |  |  [DIRAS: Efficient LLM Annotation of Document Relevance for Retrieval Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-long.271) |  | 0 | Retrieval Augmented Generation (RAG) is widely employed to ground responses to queries on domain-specific documents. But do RAG implementations leave out important information when answering queries that need an integrated analysis of information (e.g., Tell me good news in the stock market today.)? To address these concerns, RAG developers need to annotate information retrieval (IR) data for their domain of interest, which is challenging because (1) domain-specific queries usually need nuanced... | Jingwei Ni, Tobias Schimanski, Meihong Lin, Mrinmaya Sachan, Elliott Ash, Markus Leippold |  |
| 353 |  |  [Hello Again! LLM-powered Personalized Agent for Long-term Dialogue](https://doi.org/10.18653/v1/2025.naacl-long.272) |  | 0 | Open-domain dialogue systems have seen remarkable advancements with the development of large language models (LLMs). Nonetheless, most existing dialogue systems predominantly focus on brief single-session interactions, neglecting the real-world demands for long-term companionship and personalized interactions with chatbots. Crucial to addressing this real-world need are event summary and persona management, which enable reasoning for appropriate long-term dialogue responses. Recent progress in... | Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, TatSeng Chua |  |
| 354 |  |  [My LLM might Mimic AAE - But When Should It?](https://doi.org/10.18653/v1/2025.naacl-long.273) |  | 0 | We examine the representation of African American English (AAE) in large language models (LLMs), exploring (a) the perceptions Black Americans have of how effective these technologies are at producing authentic AAE, and (b) in what contexts Black Americans find this desirable. Through both a survey of Black Americans (n= 104) and annotation of LLM-produced AAE by Black Americans (n= 228), we find that Black Americans favor choice and autonomy in determining when AAE is appropriate in LLM... | Sandra Sandoval, Christabel Acquaye, Kwesi A. Cobbina, Mohammad Nayeem Teli, Hal Daumé III |  |
| 355 |  |  [High-Dimension Human Value Representation in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.274) |  | 0 | The widespread application of Large Language Models (LLMs) across various tasks and fields has necessitated the alignment of these models with human values and preferences. Given various approaches of human value alignment, such as Reinforcement Learning with Human Feedback (RLHF), constitutional learning, and safety fine-tuning etc., there is an urgent need to understand the scope and nature of human values injected into these LLMs before their deployment and adoption. We propose UniVar, a... | Samuel Cahyawijaya, Delong Chen, Yejin Bang, Leila Khalatbari, Bryan Wilie, Ziwei Ji, Etsuko Ishii, Pascale Fung |  |
| 356 |  |  [Not all Hallucinations are Good to Throw Away When it Comes to Legal Abstractive Summarization](https://doi.org/10.18653/v1/2025.naacl-long.275) |  | 0 | Automatic summarization of legal documents requires a thorough understanding of their specificities, mainly with respect to the vocabulary used by legal experts. Indeed, the latter rely heavily on their external knowledge when writing summaries, in order to contextualize the main entities of the source document. This leads to reference summaries containing many abstractions, that sota models struggle to generate. In this paper, we propose an entity-driven approach aiming at learning the model... | Nihed Bendahman, Karen PinelSauvagnat, Gilles Hubert, Mokhtar Boumedyen Billami |  |
| 357 |  |  [Query-focused Referentiability Learning for Zero-shot Retrieval](https://doi.org/10.18653/v1/2025.naacl-long.276) |  | 0 | Dense passage retrieval enhances Information Retrieval (IR) by encoding queries and passages into representation space. However, passage representations often fail to be referenced by their gold queries under domain shifts, revealing a weakness in representation space. One desirable concept for representations is ”argmaxable”. Being argmaxable ensures that no representations are theoretically excluded from selection due to geometric constraints. To be argmaxable, a notable approach is to... | Jaeyoung Kim, Dohyeon Lee, Seungwon Hwang |  |
| 358 |  |  [A Novel Computational Modeling Foundation for Automatic Coherence Assessment](https://doi.org/10.18653/v1/2025.naacl-long.277) |  | 0 | Coherence is an essential property of well-written texts, that refers to the way textual units relate to one another. In the era of generative AI, coherence assessment is essential for many NLP tasks such as summarization, long-form question-answering, and more.Current NLP approaches for modeling coherence often rely on a proxy task, specifically, sentence reordering. However, such an approach may not capture the full range of factors contributing to coherence.To remedy this, in this work we... | Aviya Maimon |  |
| 359 |  |  [Token-based Decision Criteria Are Suboptimal in In-context Learning](https://doi.org/10.18653/v1/2025.naacl-long.278) |  | 0 | In-Context Learning (ICL) typically utilizes classification criteria from output probabilities of manually selected label tokens. However, we argue that such token-based classification criteria lead to suboptimal decision boundaries, despite delicate calibrations through translation and constrained rotation applied. To address this problem, we propose Hidden Calibration, which renounces token probabilities and uses the nearest centroid classifier on the LM’s last hidden states. In detail, we... | Hakaze Cho, Yoshihiro Sakai, Mariko Kato, Kenshiro Tanaka, Akira Ishii, Naoya Inoue |  |
| 360 |  |  [CSEval: Towards Automated, Multi-Dimensional, and Reference-Free Counterspeech Evaluation using Auto-Calibrated LLMs](https://doi.org/10.18653/v1/2025.naacl-long.279) |  | 0 | Counterspeech has emerged as a popular and effective strategy for combating online hate speech, sparking growing research interest in automating its generation using language models. However, the field still lacks standardised evaluation protocols and reliable automated evaluation metrics that align with human judgement. Current automatic evaluation methods, primarily based on similarity metrics, do not effectively capture the complex and independent attributes of counterspeech quality, such as... | Amey Hengle, Aswini Kumar Padhi, Anil Bandhakavi, Tanmoy Chakraborty |  |
| 361 |  |  [Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study](https://doi.org/10.18653/v1/2025.naacl-long.280) |  | 0 | Large language models (LLMs) have shown continuously improving multilingual capabilities, and even small-scale open-source models have demonstrated rapid performance enhancement. In this paper, we systematically explore the abilities of open LLMs with less than ten billion parameters to handle multilingual machine translation (MT) tasks. We conduct comprehensive evaluations on six popular LLMs and find that models like Gemma2-9B exhibit impressive multilingual translation capabilities. We then... | Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, Bin Wang |  |
| 362 |  |  [RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.281) |  | 0 | Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming.However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model’s safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile.... | Bang An, Shiyue Zhang, Mark Dredze |  |
| 363 |  |  [Evaluating Evidence Attribution in Generated Fact Checking Explanations](https://doi.org/10.18653/v1/2025.naacl-long.282) |  | 0 | Automated fact-checking systems often struggle with trustworthiness, as their generated explanations can include hallucinations. In this work, we explore evidence attribution for fact-checking explanation generation. We introduce a novel evaluation protocol, citation masking and recovery, to assess attribution quality in generated explanations. We implement our protocol using both human annotators and automatic annotators and found that LLM annotation correlates with human annotation,... | Rui Xing, Timothy Baldwin, Jey Han Lau |  |
| 364 |  |  [ETHIC: Evaluating Large Language Models on Long-Context Tasks with High Information Coverage](https://doi.org/10.18653/v1/2025.naacl-long.283) |  | 0 | Recent advancements in large language models (LLM) capable of processing extremely long texts highlight the need for a dedicated evaluation benchmark to assess their long-context capabilities. However, existing methods, like the needle-in-a-haystack test, do not effectively assess whether these models fully utilize contextual information, raising concerns about the reliability of current evaluation techniques. To thoroughly examine the effectiveness of existing benchmarks, we introduce a new... | Taewhoo Lee, Chanwoong Yoon, Kyochul Jang, Donghyeon Lee, Minju Song, Hyunjae Kim, Jaewoo Kang |  |
| 365 |  |  [Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors](https://doi.org/10.18653/v1/2025.naacl-long.284) |  | 0 | In-context Learning (ICL) has become the primary method for performing natural language tasks with Large Language Models (LLMs). The knowledge acquired during pre-training is crucial for this few-shot capability, providing the model with task priors. However, recent studies have shown that ICL predominantly relies on retrieving task priors rather than “learning” to perform tasks. This limitation is particularly evident in complex subjective domains such as emotion and morality, where priors... | Georgios Chochlakis, Alexandros Potamianos, Kristina Lerman, Shrikanth Narayanan |  |
| 366 |  |  [Arabic Dataset for LLM Safeguard Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.285) |  | 0 | The growing use of large language models (LLMs) has raised concerns regarding their safety. While many studies have focused on English, the safety of LLMs in Arabic, with its linguistic and cultural complexities, remains under-explored. Here, we aim to bridge this gap. In particular, we present an Arab-region-specific safety evaluation dataset consisting of 5,799 questions, including direct attacks, indirect attacks, and harmless requests with sensitive words, adapted to reflect the... | Yasser Ashraf, Yuxia Wang, Bin Gu, Preslav Nakov, Timothy Baldwin |  |
| 367 |  |  [Anticipating Future with Large Language Model for Simultaneous Machine Translation](https://doi.org/10.18653/v1/2025.naacl-long.286) |  | 0 | Simultaneous machine translation (SMT) takes streaming input utterances and incrementally produces target text. Existing SMT methods only use the partial utterance that has already arrived at the input and the generated hypothesis. Motivated by human interpreters’ technique to forecast future words before hearing them, we propose Translation by Anticipating Future (TAF), a method to improve translation quality while retaining low latency. Its core idea is to use a large language model (LLM) to... | Siqi Ouyang, Oleksii Hrinchuk, Zhehuai Chen, Vitaly Lavrukhin, Jagadeesh Balam, Lei Li, Boris Ginsburg |  |
| 368 |  |  [GuideLLM: Exploring LLM-Guided Conversation with Applications in Autobiography Interviewing](https://doi.org/10.18653/v1/2025.naacl-long.287) |  | 0 | Although Large Language Models (LLMs) succeed in human-guided conversations such as instruction following and question answering, the potential of LLM-guided conversations—where LLMs direct the discourse and steer the conversation’s objectives—remains under-explored. In this study, we first characterize LLM-guided conversation into three fundamental components: (i) Goal Navigation; (ii) Context Management; (iii) Empathetic Engagement, and propose GuideLLM as an installation. We then implement... | Jinhao Duan, Xinyu Zhao, Zhuoxuan Zhang, Eunhye Grace Ko, Lily Boddy, Chenan Wang, Tianhao Li, Alexander Rasgon, Junyuan Hong, Min Kyung Lee, Chenxi Yuan, Qi Long, Ying Ding, Tianlong Chen, Kaidi Xu |  |
| 369 |  |  [Fine-Tuning Large Language Models with Sequential Instructions](https://doi.org/10.18653/v1/2025.naacl-long.288) |  | 0 | We find that existing instruction-tuned models usually struggle to adhere to a query with multiple intentions, which impairs their performance when the completion of several tasks is demanded by a single command. Hence, this paper teaches models to respond to sequential instructions. Our first attempt stems from a task-driven perspective, manually creating additional intermediate tasks to train multilingual and visual question answering. Next, we develop an automatic and generic process that... | Hanxu Hu, Simon Yu, Pinzhen Chen, Edoardo M. Ponti |  |
| 370 |  |  [Diverse In-Context Example Selection After Decomposing Programs and Aligned Utterances Improves Semantic Parsing](https://doi.org/10.18653/v1/2025.naacl-long.289) |  | 0 | LLMs are increasingly used as seq2seq translators from natural language utterances to structured programs, a process called semantic interpretation. Unlike atomic labels or token sequences, programs are naturally represented as abstract syntax trees (ASTs). Such structured representation raises novel issues related to the design and selection of in-context examples (ICEs) presented to the LLM. We focus on decomposing the pool of available ICE trees into fragments, some of which may be better... | Mayank Kothyari, Sunita Sarawagi, Soumen Chakrabarti, Gaurav Arora, Srujana Merugu |  |
| 371 |  |  [Elevating Legal LLM Responses: Harnessing Trainable Logical Structures and Semantic Knowledge with Legal Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.290) |  | 0 | Large Language Models (LLMs) have achieved impressive results across numerous domains, yet they experience notable deficiencies in legal question-answering tasks. LLMs often generate generalized responses that lack the logical specificity required for expert legal advice and are prone to hallucination, providing answers that appear correct but are unreliable. Retrieval-Augmented Generation (RAG) techniques offer partial solutions to address this challenge, but existing approaches typically... | Rujing Yao, Yang Wu, Chenghao Wang, Jingwei Xiong, Fang Wang, Xiaozhong Liu |  |
| 372 |  |  [Efficient One-shot Compression via Low-Rank Local Feature Distillation](https://doi.org/10.18653/v1/2025.naacl-long.291) |  | 0 | Current structured pruning approaches for large language models typically involve two steps: (1) compression using calibration data and (2) costly continued pretraining on billions of tokens to recover lost performance. This second step is necessary as the first significantly impacts model accuracy. Moreover, prior research suggests that pretrained Transformer weights are not necessarily low-rank, unlike their activations, making one-shot structured pruning challenging. Based on this... | Yaya Sy, Christophe Cerisara, Irina Illina |  |
| 373 |  |  [Waste Not, Want Not; Recycled Gumbel Noise Improves Consistency in Natural Language Generation](https://doi.org/10.18653/v1/2025.naacl-long.292) |  | 0 | Consistency in the output of language models is critical for their reliability and practical utility. Due to their training objective, language models learn to model the full space of possible continuations, leading to outputs that can vary significantly in style, content, and tone, even for similar inputs. To address this, we propose a novel decoding algorithm that enhances response consistency across different prompts with no degradation in response quality. By incorporating a latent variable... | Damien de Mijolla, Hannan Saddiq, Kim Moore |  |
| 374 |  |  [ConQRet: A New Benchmark for Fine-Grained Automatic Evaluation of Retrieval Augmented Computational Argumentation](https://doi.org/10.18653/v1/2025.naacl-long.293) |  | 0 | Computational argumentation, which involves generating answers or summaries for controversial topics like abortion bans and vaccination, has become increasingly important in today’s polarized environment. Sophisticated LLM capabilities offer the potential to provide nuanced, evidence-based answers to such questions through Retrieval-Augmented Argumentation (RAArg), leveraging real-world evidence for high-quality, grounded arguments. However, evaluating RAArg remains challenging, as human... | Kaustubh D. Dhole, Kai Shu, Eugene Agichtein |  |
| 375 |  |  [SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators](https://doi.org/10.18653/v1/2025.naacl-long.294) |  | 0 | Existing approaches to multilingual text detoxification are hampered by the scarcity of parallel multilingual datasets. In this work, we introduce a pipeline for the generation of multilingual parallel detoxification data. We also introduce SynthDetoxM, a manually collected and synthetically generated multilingual parallel text detoxification dataset comprising 16,000 high-quality detoxification sentence pairs across German, French, Spanish and Russian. The data was sourced from different... | Daniil Moskovskiy, Nikita Sushko, Sergey Pletenev, Elena Tutubalina, Alexander Panchenko |  |
| 376 |  |  [BEMEAE: Moving Beyond Exact Span Match for Event Argument Extraction](https://doi.org/10.18653/v1/2025.naacl-long.295) |  | 0 | Event Argument Extraction (EAE) is a key task in natural language processing, focusing on identifying and classifying event arguments in text. However, the widely adopted exact span match (ESM) evaluation metric has notable limitations due to its rigid span constraints, often misidentifying valid predictions as errors and underestimating system performance. In this paper, we evaluate nine state-of-the-art EAE models on the RAMS and GENEVA datasets, highlighting ESM’s limitations. To address... | Enfa Fane, Md Nayem Uddin, Oghenevovwe Ikumariegbe, Daniyal Kashif, Eduardo Blanco, Steven R. Corman |  |
| 377 |  |  [uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in Low-Data Regimes](https://doi.org/10.18653/v1/2025.naacl-long.296) |  | 0 | Recent work on distilling Whisper’s knowledge into small models using pseudo-labels shows promising performance while reducing the size by up to 50%. This results in small, efficient, and dedicated models. However, a critical step of distillation using pseudo-labels involves filtering high-quality predictions and using only those during training. This step requires ground truth labels to compare with and filter low-quality examples, making the process dependent on human labels. Additionally,... | Abdul Waheed, Karima Kadaoui, Bhiksha Raj, Muhammad AbdulMageed |  |
| 378 |  |  [Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities](https://doi.org/10.18653/v1/2025.naacl-long.297) |  | 0 | Recent research has shown that Large Language Models (LLMs) are vulnerable to automated jailbreak attacks, where adversarial suffixes crafted by algorithms appended to harmful queries bypass safety alignment and trigger unintended responses. Current methods for generating these suffixes are computationally expensive and have low Attack Success Rates (ASR), especially against well-aligned models like Llama2 and Llama3. To overcome these limitations, we introduce \*\*ADV-LLM\*\*, an iterative... | ChungEn Sun, Xiaodong Liu, Weiwei Yang, TsuiWei Weng, Hao Cheng, Aidan San, Michel Galley, Jianfeng Gao |  |
| 379 |  |  [VoiceTextBlender: Augmenting Large Language Models with Speech Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning](https://doi.org/10.18653/v1/2025.naacl-long.298) |  | 0 | Recent studies have augmented large language models (LLMs) with speech capabilities, leading to the development of speech language models (SpeechLMs). Earlier SpeechLMs focused on single-turn speech-based question answering (QA), where user input comprised a speech context and a text question. More recent studies have extended this to multi-turn conversations, though they often require complex, multi-stage supervised fine-tuning (SFT) with diverse data. Another critical challenge with SpeechLMs... | Yifan Peng, Krishna C. Puvvada, Zhehuai Chen, Piotr Zelasko, He Huang, Kunal Dhawan, Ke Hu, Shinji Watanabe, Jagadeesh Balam, Boris Ginsburg |  |
| 380 |  |  [Rethinking Word Similarity: Semantic Similarity through Classification Confusion](https://doi.org/10.18653/v1/2025.naacl-long.299) |  | 0 | Word similarity has many applications to social science and cultural analytics tasks like measuring meaning change over time and making sense of contested terms. Yet traditional similarity methods based on cosine similarity between word embeddings cannot capture the context-dependent, asymmetrical, polysemous nature of semantic similarity. We propose a new measure of similarity, Word Confusion, that reframes semantic similarity in terms of feature-based classification confusion. Word Confusion... | Kaitlyn Zhou, Haishan Gao, Sarah Li Chen, Dan Edelstein, Dan Jurafsky, Chen Shani |  |
| 381 |  |  [SUNAR: Semantic Uncertainty based Neighborhood Aware Retrieval for Complex QA](https://doi.org/10.18653/v1/2025.naacl-long.300) |  | 0 | Complex question-answering (QA) systems face significant challenges in retrieving and reasoning over information that addresses multifaceted queries. While large language models (LLMs) have advanced the reasoning capabilities of these systems, the bounded-recall problem persists, where procuring all relevant documents in first-stage retrieval remains a challenge. Missing pertinent documents at this stage leads to performance degradation that cannot be remedied in later stages, especially given... | Venktesh V, Mandeep Rathee, Avishek Anand |  |
| 382 |  |  [Do RAG Systems Cover What Matters? Evaluating and Optimizing Responses with Sub-Question Coverage](https://doi.org/10.18653/v1/2025.naacl-long.301) |  | 0 | Evaluating retrieval-augmented generation (RAG) systems remains challenging, particularly for open-ended questions that lack definitive answers and require coverage of multiple sub-topics. In this paper, we introduce a novel evaluation framework based on sub-question coverage, which measures how well a RAG system addresses different facets of a question. We propose decomposing questions into sub-questions and classifying them into three types—core, background, and follow-up—to reflect their... | Kaige Xie, Philippe Laban, Prafulla Kumar Choubey, Caiming Xiong, ChienSheng Wu |  |
| 383 |  |  [Stronger Universal and Transferable Attacks by Suppressing Refusals](https://doi.org/10.18653/v1/2025.naacl-long.302) |  | 0 | Making large language models (LLMs) safe for mass deployment is a complex and ongoing challenge. Efforts have focused on aligning models to human preferences (RLHF), essentially embedding a “safety feature” into the model’s parameters. The Greedy Coordinate Gradient (GCG) algorithm (Zou et al., 2023b) emerges as one of the most popular automated jailbreaks, an attack that circumvents this safety training. So far, it is believed that such optimization-based attacks (unlike hand-crafted ones) are... | David Huang, Avidan Shah, Alexandre Araujo, David A. Wagner, Chawin Sitawarin |  |
| 384 |  |  [The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models](https://doi.org/10.18653/v1/2025.naacl-long.303) |  | 0 | As language models (LMs) become capable of handling a wide range of tasks, their evaluation is becoming as challenging as their development. Most generation benchmarks currently assess LMs using abstract evaluation criteria-like helpfulness and harmlessness-which often lack the flexibility and granularity of human assessment. Additionally, these benchmarks tend to focus disproportionately on specific capabilities such as instruction following, leading to coverage bias. To overcome these... | Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Choi, Sheikh Shafayat, Jinheon Baek, Sue Hyun Park, Hyeonbin Hwang, Jinkyung Jo, Hyowon Cho, Haebin Shin, Seongyun Lee, Hanseok Oh, Noah Lee, Namgyu Ho, Se June Joo, Miyoung Ko, Yoonjoo Lee, Hyungjoo Chae, Jamin Shin, Joel Jang, Seonghyeon Ye, Bill Yuchen Lin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo |  |
| 385 |  |  [DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback](https://doi.org/10.18653/v1/2025.naacl-long.304) |  | 0 | Despite their widespread success, Text-to-Image models (T2I) still struggle to produce images that are both aesthetically pleasing and faithful to the user’s input text. We introduce DreamSync, a simple yet effective training algorithm that improves T2I models to be faithful to the text input. DreamSync utilizes large vision-language models (VLMs) to effectively identify the fine-grained discrepancies between generated images and the text inputs and enable T2I models to self-improve without... | Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin, DaCheng Juan, Dana Alon, Charles Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, Cyrus Rashtchian |  |
| 386 |  |  [Uncovering Bias in Large Vision-Language Models at Scale with Counterfactuals](https://doi.org/10.18653/v1/2025.naacl-long.305) |  | 0 | With the advent of Large Language Models (LLMs) possessing increasingly impressive capabilities, a number of Large Vision-Language Models (LVLMs) have been proposed to augment LLMs with visual inputs. Such models condition generated text on both an input image and a text prompt, enabling a variety of use cases such as visual question answering and multimodal chat. While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored... | Phillip Howard, Kathleen C. Fraser, Anahita Bhiwandiwalla, Svetlana Kiritchenko |  |
| 387 |  |  [AEGIS2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails](https://doi.org/10.18653/v1/2025.naacl-long.306) |  | 0 | As Large Language Models (LLMs) and generative AI become increasingly widespread, concerns about content safety have grown in parallel. Currently, there is a clear lack of high-quality, human-annotated datasets that address the full spectrum of LLM-related safety risks and are usable for commercial applications. To bridge this gap, we propose a comprehensive and adaptable taxonomy for categorizing safety risks, structured into 12 top-level hazard categories with an extension to 9 fine-grained... | Shaona Ghosh, Prasoon Varshney, Makesh Narsimhan Sreedhar, Aishwarya Padmakumar, Traian Rebedea, Jibin Rajan Varghese, Christopher Parisien |  |
| 388 |  |  [UOREX: Towards Uncertainty-Aware Open Relation Extraction](https://doi.org/10.18653/v1/2025.naacl-long.307) |  | 0 | Open relation extraction (OpenRE) aims to identify relational facts within open-domain corpora without relying on predefined relation types. A significant limitation of current state-of-the-art OpenRE approaches is their inability to accurately self-assess their performance. Which is caused by the reliance on pseudo-labels, that treats all points within a cluster equally, regardless of their actual relative position according to the cluster center. This leads to models that are often... | Rebii Jamal, Mounir Ourekouch, Mohammed Erradi |  |
| 389 |  |  [Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training](https://doi.org/10.18653/v1/2025.naacl-long.308) |  | 0 | Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises... | Yuchen Zhuang, Jingfeng Yang, Haoming Jiang, Xin Liu, Kewei Cheng, Sanket Lokegaonkar, Yifan Gao, Qing Ping, Tianyi Liu, Binxuan Huang, Zheng Li, Zhengyang Wang, Pei Chen, Ruijie Wang, Rongzhi Zhang, Nasser Zalmout, Priyanka Nigam, Bing Yin, Chao Zhang |  |
| 390 |  |  [TinyThinker: Distilling Reasoning through Coarse-to-Fine Knowledge Internalization with Self-Reflection](https://doi.org/10.18653/v1/2025.naacl-long.309) |  | 0 | Large Language Models exhibit impressive reasoning capabilities across diverse tasks, motivating efforts to distill these capabilities into smaller models through generated reasoning data. However, direct training on such synthesized reasoning data may lead to superficial imitation of reasoning process, rather than fostering a genuine integration of reasoning capabilities with underlying knowledge. To address this, we propose TinyThinker, a framework introducing two novel approaches. First, we... | Shengmin Piao, Sanghyun Park |  |
| 391 |  |  [VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-long.310) |  | 0 | Understanding information from a collection of multiple documents, particularly those with visually rich elements, is important for document-grounded question answering. This paper introduces VisDoMBench, the first comprehensive benchmark designed to evaluate QA systems in multi-document settings with rich multimodal content, including tables, charts, and presentation slides. We propose VisDoMRAG, a novel multimodal Retrieval Augmented Generation (RAG) approach that simultaneously utilizes... | Manan Suri, Puneet Mathur, Franck Dernoncourt, Kanika Goswami, Ryan A. Rossi, Dinesh Manocha |  |
| 392 |  |  [VTechAGP: An Academic-to-General-Audience Text Paraphrase Dataset and Benchmark Models](https://doi.org/10.18653/v1/2025.naacl-long.311) |  | 0 | Existing text simplification or paraphrase datasets mainly focus on sentence-level text generation in a general domain. These datasets are typically developed without using domain knowledge. In this paper, we release a novel dataset, VTechAGP, which is the first academic-to-general-audience text paraphrase dataset consisting of document-level these and dissertation academic and general-audience abstract pairs from 8 colleges authored over 25 years. We also propose a novel dynamic soft prompt... | Ming Cheng, Jiaying Gong, Chenhan Yuan, William A. Ingram, Edward A. Fox, Hoda Eldardiry |  |
| 393 |  |  [Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages](https://doi.org/10.18653/v1/2025.naacl-long.312) |  | 0 | Human bilinguals often use similar brain regions to process multiple languages, depending on when they learned their second language and their proficiency. In large language models (LLMs), how are multiple languages learned and encoded? In this work, we explore the extent to which LLMs share representations of morphsyntactic concepts such as grammatical number, gender, and tense across languages. We train sparse autoencoders on Llama-3-8B and Aya-23-8B, and demonstrate that abstract grammatical... | Jannik Brinkmann, Chris Wendler, Christian Bartelt, Aaron Mueller |  |
| 394 |  |  [Examining and Adapting Time for Multilingual Classification via Mixture of Temporal Experts](https://doi.org/10.18653/v1/2025.naacl-long.313) |  | 0 | Time is implicitly embedded in classification process: classifiers are usually built on existing data while to be applied on future data whose distributions (e.g., label and token) may change. However, existing state-of-the-art classification models merely consider the temporal variations and primarily focus on English corpora, which leaves temporal studies less explored, let alone under multilingual settings. In this study, we fill the gap by treating time as domains (e.g., 2024 vs. 2025),... | Weisi Liu, Guangzeng Han, Xiaolei Huang |  |
| 395 |  |  [FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.314) |  | 0 | Sign language translation has historically been peripheral to mainstream machine translation research. In order to help converge the fields, we introduce FLEURS-ASL, an extension of the multiway parallel benchmarks FLORES (for text) and FLEURS (for speech) to support their first sign language (as video), American Sign Language, translated by 5 Certified Deaf Interpreters. FLEURS-ASL can be used to evaluate a variety of tasks—primarily sentence- and discourse-level translation—between ASL and... | Garrett Tanzer |  |
| 396 |  |  [EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms](https://doi.org/10.18653/v1/2025.naacl-long.315) |  | 0 | The rise of powerful large language models (LLMs) has spurred a new trend in building LLM-based autonomous agents for solving complex tasks, especially multi-agent systems. Despite the remarkable progress, we notice that existing works are heavily dependent on human-designed frameworks, which greatly limits the functional scope and scalability of agent systems. How to automatically extend the specialized agent to multi-agent systems to improve task-solving capability still remains a significant... | Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang |  |
| 397 |  |  [EmoCharacter: Evaluating the Emotional Fidelity of Role-Playing Agents in Dialogues](https://doi.org/10.18653/v1/2025.naacl-long.316) |  | 0 | Role-playing agents (RPAs) powered by large language models (LLMs) have been widely utilized in dialogue systems for their capability to deliver personalized interactions. Current evaluations of RPAs mainly focus on personality fidelity, tone imitation, and knowledge consistency, while overlooking emotional fidelity, a key factor that affects user experience. To this end, we propose a benchmark called EmoCharacter to assess emotional fidelity of RPAs in dialogues. EmoCharacter includes two... | Qiming Feng, Qiujie Xie, Xiaolong Wang, Qingqiu Li, Yuejie Zhang, Rui Feng, Tao Zhang, Shang Gao |  |
| 398 |  |  [Language Models can Categorize System Inputs for Performance Analysis](https://doi.org/10.18653/v1/2025.naacl-long.317) |  | 0 | Language model systems are used to process diverse categories of input requests, ranging from improving creative writing to solving programming challenges. It would be useful to know which categories they are good at. However, existing evaluations compare model performance on pre-defined categories, failing to reflect a system’s performance on finer-grained or novel ones. We propose to automatically search for finer-grained categories based on inputs where a system performs well or poorly, and... | Dominic Sobhani, Ruiqi Zhong, Edison MarreseTaylor, Keisuke Sakaguchi, Yutaka Matsuo |  |
| 399 |  |  [FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.318) |  | 0 | Large language models have demonstrated outstanding performance in various natural language processing tasks, but their security capabilities in the financial domain have not been explored, and their performance on complex tasks like financial agent remains unknown. This paper presents FinEval, a benchmark designed to evaluate LLMs’ financial domain knowledge and practical abilities. The dataset contains 8,351 questions categorized into four different key areas: Financial Academic Knowledge,... | Xin Guo, Haotian Xia, Zhaowei Liu, Hanyang Cao, Zhi Yang, Zhiqiang Liu, Sizhe Wang, Jinyi Niu, Chuqi Wang, Yanhui Wang, Xiaolong Liang, Xiaoming Huang, Bing Zhu, Zhongyu Wei, Yun Chen, Weining Shen, Liwen Zhang |  |
| 400 |  |  [Rethinking the Role of LLMs for Document-level Relation Extraction: a Refiner with Task Distribution and Probability Fusion](https://doi.org/10.18653/v1/2025.naacl-long.319) |  | 0 | Document-level relation extraction (DocRE) provides a broad context for extracting one or more relations for each entity pair. Large language models (LLMs) have made great progress in relation extraction tasks. However, one of the main challenges we face is that LLMs have difficulty in multi-label relation prediction tasks. Additionally, another noteworthy challenge and discovery we reveal: the small language models (SLMs) for DocRE tend to classify existing relations as ”no relation” (NA),... | Fu Zhang, Xinlong Jin, Jingwei Cheng, Hongsen Yu, Huangming Xu |  |
| 401 |  |  [Decomposition Dilemmas: Does Claim Decomposition Boost or Burden Fact-Checking Performance?](https://doi.org/10.18653/v1/2025.naacl-long.320) |  | 0 | Fact-checking pipelines increasingly adopt the Decompose-Then-Verify paradigm, where texts are broken down into smaller claims for individual verification and subsequently combined for a veracity decision. While decomposition is widely-adopted in such pipelines, its effects on final fact-checking performance remain underexplored. Some studies have reported improvements from decompostition, while others have observed performance declines, indicating its inconsistent impact. To date, no... | Qisheng Hu, Quanyu Long, Wenya Wang |  |
| 402 |  |  [Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing](https://doi.org/10.18653/v1/2025.naacl-long.321) |  | 0 | Large Language Models (LLMs) have demonstrated great potential as generalist assistants, showcasing powerful task understanding and problem-solving capabilities. To deploy LLMs as AI assistants, it is crucial that these models exhibit desirable behavioral traits, such as non-toxicity and resilience against jailbreak attempts. Current approaches for detoxification or preventing jailbreaking usually involve Supervised Fine-Tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), which... | Huanqian Wang, Yang Yue, Rui Lu, Jingxin Shi, Andrew Zhao, Shenzhi Wang, Shiji Song, Gao Huang |  |
| 403 |  |  [Effective Skill Unlearning through Intervention and Abstention](https://doi.org/10.18653/v1/2025.naacl-long.322) |  | 0 |  | Yongce Li, ChungEn Sun, TsuiWei Weng |  |
| 404 |  |  [CharacterBox: Evaluating the Role-Playing Capabilities of LLMs in Text-Based Virtual Worlds](https://doi.org/10.18653/v1/2025.naacl-long.323) |  | 0 | Role-playing is a crucial capability of Large Language Models (LLMs), enabling a wide range of practical applications, including intelligent non-player characters, digital twins, and emotional companions. Evaluating this capability in LLMs is challenging due to the complex dynamics involved in role-playing, such as maintaining character fidelity throughout a storyline and navigating open-ended narratives without a definitive ground truth. Current evaluation methods, which primarily focus on... | Lei Wang, Jianxun Lian, Yi Huang, Yanqi Dai, Haoxuan Li, Xu Chen, Xing Xie, JiRong Wen |  |
| 405 |  |  [A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision-Language Models](https://doi.org/10.18653/v1/2025.naacl-long.324) |  | 0 | Large Vision-Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the Cookie Theft task in human cognitive tests, we propose a novel evaluation benchmark to evaluate high-level cognitive abilities of LVLMs using images with rich semantics. The benchmark consists of 251 images along with comprehensive annotations. It defines eight reasoning capabilities and comprises an image description task and... | Xiujie Song, Mengyue Wu, Kenny Q. Zhu, Chunhao Zhang, Yanyi Chen |  |
| 406 |  |  [CoME: An Unlearning-based Approach to Conflict-free Model Editing](https://doi.org/10.18653/v1/2025.naacl-long.325) |  | 0 | Large language models (LLMs) often retain outdated or incorrect information from pre-training, which undermines their reliability. While model editing methods have been developed to address such errors without full re-training, they frequently suffer from knowledge conflicts, where outdated information interferes with new knowledge. In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing... | Dahyun Jung, Jaehyung Seo, Jaewook Lee, Chanjun Park, Heuiseok Lim |  |
| 407 |  |  [On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena](https://doi.org/10.18653/v1/2025.naacl-long.326) |  | 0 | Language Models (LMs) have been shown to exhibit a strong preference towards entities associated with Western culture when operating in non-Western languages. In this paper, we aim to uncover the origins of entity-related cultural biases in LMs by analyzing several contributing factors, including the representation of entities in pre-training data and the impact of variations in linguistic phenomena across languages. We introduce CAMeL-2, a parallel Arabic-English benchmark of 58,086 entities... | Tarek Naous, Wei Xu |  |
| 408 |  |  [Adapting Sentence-level Automatic Metrics for Document-level Simplification Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.327) |  | 0 | Text simplification aims to enhance the clarity and comprehensibility of a complex text while preserving its original meaning. Previous research on the automatic evaluation of text simplification has primarily focused on sentence simplification, with commonly used metrics such as SARI and advanced metrics such as LENS being trained and evaluated at the sentence level. However, these metrics often underperform on longer texts. In our study, we propose a novel approach to adapt existing... | Mounica Maddela, Fernando AlvaManchego |  |
| 409 |  |  [Decoding Speculative Decoding](https://doi.org/10.18653/v1/2025.naacl-long.328) |  | 0 | Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without sacrificing quality. When performing inference, speculative decoding uses a smaller draft model to generate speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. In this work, we perform a detailed study comprising over 350 experiments with LLaMA-65B and OPT-66B using... | Minghao Yan, Saurabh Agarwal, Shivaram Venkataraman |  |
| 410 |  |  [Leveraging LLM For Synchronizing Information Across Multilingual Tables](https://doi.org/10.18653/v1/2025.naacl-long.329) |  | 0 | The vast amount of online information today poses challenges for non-English speakers, as much of it is concentrated in high-resource languages such as English and French. Wikipedia reflects this imbalance, with content in low-resource languages frequently outdated or incomplete. Recent research has sought to improve cross-language synchronization of Wikipedia tables using rule-based methods. These approaches can be effective, but they struggle with complexity and generalization. This paper... | Siddharth Khincha, Tushar Kataria, Ankita Anand, Dan Roth, Vivek Gupta |  |
| 411 |  |  [ConMeC: A Dataset for Metonymy Resolution with Common Nouns](https://doi.org/10.18653/v1/2025.naacl-long.330) |  | 0 | Metonymy plays an important role in our daily communication. People naturally think about things using their most salient properties or commonly related concepts. For example, by saying “The bus decided to skip our stop today,” we actually mean that the bus driver made the decision, not the bus. Prior work on metonymy resolution has mainly focused on named entities. However, metonymy involving common nouns (such as desk, baby, and school) is also a frequent and challenging phenomenon. We argue... | Saptarshi Ghosh, Tianyu Jiang |  |
| 412 |  |  [Self-DC: When to Reason and When to Act? Self Divide-and-Conquer for Compositional Unknown Questions](https://doi.org/10.18653/v1/2025.naacl-long.331) |  | 0 | Previous research has typically concentrated on leveraging the internal knowledge of Large Language Models (LLMs) to answer known questions (i.e., internal reasoning such as generate-then-read). In contrast, for questions that fall outside their known scope, these models rely on external knowledge retrieval to provide accurate responses (i.e., external acting such as retrieve-then-read). However, few previous works consider the compositional questions, which consist of several known and unknown... | Hongru Wang, Boyang Xue, Baohang Zhou, Tianhua Zhang, Cunxiang Wang, Huimin Wang, Guanhua Chen, KamFai Wong |  |
| 413 |  |  [TRANSIENTTABLES: Evaluating LLMs' Reasoning on Temporally Evolving Semi-structured Tables](https://doi.org/10.18653/v1/2025.naacl-long.332) |  | 0 | Humans continuously make new discoveries, and understanding temporal sequence of events leading to these breakthroughs is essential for advancing science and society. This ability to reason over time allows us to identify future steps and understand the effects of financial and political decisions on our lives. However, large language models (LLMs) are typically trained on static datasets, limiting their ability to perform effective temporal reasoning. To assess the temporal reasoning... | Abhilash Reddy Shankarampeta, Harsh Mahajan, Tushar Kataria, Dan Roth, Vivek Gupta |  |
| 414 |  |  [AdvisorQA: Towards Helpful and Harmless Advice-seeking Question Answering with Collective Intelligence](https://doi.org/10.18653/v1/2025.naacl-long.333) |  | 0 | As the integration of large language models into daily life is on the rise, there is still a lack of dataset for \*advising on subjective and personal dilemmas\*. To address this gap, we introduce AdvisorQA, which aims to improve LLMs’ capability to offer advice for deeply subjective concerns, utilizing the LifeProTips Reddit forum. This forum features a dynamic interaction where users post advice-seeking questions, receiving an average of 8.9 advice per query, with 164.2 upvotes from hundreds... | Minbeom Kim, Hwanhee Lee, Joonsuk Park, Hwaran Lee, Kyomin Jung |  |
| 415 |  |  [tRAG: Term-level Retrieval-Augmented Generation for Domain-Adaptive Retrieval](https://doi.org/10.18653/v1/2025.naacl-long.334) |  | 0 | Neural retrieval models have emerged as an effective tool for information retrieval, but their performance suffers when there is a domain shift between training and test data distributions. Recent work aims to construct pseudo-training data for the target domain by generating domain-adapted pseudo-queries using large language models (LLMs). However, we identifies that LLMs exhibit a “seen term bias” where the generated pseudo-queries fail to include relevant “unseen” terms as expected for... | Dohyeon Lee, Jongyoon Kim, Jihyuk Kim, Seungwon Hwang, Joonsuk Park |  |
| 416 |  |  [JRE-L: Journalist, Reader, and Editor LLMs in the Loop for Science Journalism for the General Audience](https://doi.org/10.18653/v1/2025.naacl-long.335) |  | 0 | Science journalism reports current scientific discoveries to non-specialists, aiming to enable public comprehension of the state of the art. This task is challenging as the audience often lacks specific knowledge about the presented research. We propose JRE-L, a framework that integrates three LLMs mimicking the writing-reading-feedback-revision loop. In JRE-L, one LLM acts as the journalist, another LLM as the general public reader, and the third LLM as an editor. The journalist’s writing is... | Gongyao Jiang, Xinran Shi, Qiong Luo |  |
| 417 |  |  [Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.336) |  | 0 | Data selection for fine-tuning large language models (LLMs) aims to choose a high-quality subset from existing datasets, allowing the trained model to outperform baselines trained on the full dataset. However, the expanding body of research lacks a clear, unified framework, and the variability in experimental settings complicates systematic comparisons.While existing surveys comprehensively overview the stages and methods of data selection, they often overlook an in-depth exploration of the... | Ziche Liu, Rui Ke, Yajiao Liu, Feng Jiang, Haizhou Li |  |
| 418 |  |  [Graph Neural Network Enhanced Retrieval for Question Answering of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.337) |  | 0 | Retrieval augmented generation has revolutionized large language model (LLM) outputs by providing factual supports. Nevertheless, it struggles to capture all the necessary knowledge for complex reasoning questions. Existing retrieval methods typically divide reference documents into passages, treating them in isolation. These passages, however, are often interrelated, such as passages that are contiguous or share the same keywords. Therefore, it is crucial to recognize such relatedness for... | Zijian Li, Qingyan Guo, Jiawei Shao, Lei Song, Jiang Bian, Jun Zhang, Rui Wang |  |
| 419 |  |  [Pula: Training Large Language Models for Setswana](https://doi.org/10.18653/v1/2025.naacl-long.338) |  | 0 | In this work we present Pula, a suite of bilingual language models proficient in both Setswana and English. Leveraging recent advancements in data availability and efficient fine-tuning, Pula 8B and Pula 14B outperform GPT-4o and Gemini 1.5 Pro on English-Setswana translation tasks and achieve state-of-the-art performance on Setswana reasoning tasks for their size. We release the weights for Pula 1B, 3B, 8B, and 14B as well as training logs and training and evaluation code. Alongside Pula, we... | Nathan Brown, Vukosi Marivate |  |
| 420 |  |  [LegalViz: Legal Text Visualization by Text To Diagram Generation](https://doi.org/10.18653/v1/2025.naacl-long.339) |  | 0 | Legal documents including judgments and court orders require highly sophisticated legal knowledge for understanding. To disclose expert knowledge for non-experts, we explore the problem of visualizing legal texts with easy-to-understand diagrams and propose a novel dataset of LegalViz with 23 languages and 7,010 cases of legal document and visualization pairs, using the DOT graph description language of Graphviz. LegalViz provides a simple diagram from a complicated legal corpus identifying... | Eri Onami, Taiki Miyanishi, Koki Maeda, Shuhei Kurita |  |
| 421 |  |  [Active Few-Shot Learning for Text Classification](https://doi.org/10.18653/v1/2025.naacl-long.340) |  | 0 | The rise of Large Language Models (LLMs) has boosted the use of Few-Shot Learning (FSL) methods in natural language processing, achieving acceptable performance even when working with limited training data. The goal of FSL is to effectively utilize a small number of annotated samples in the learning process. However, the performance of FSL suffers when unsuitable support samples are chosen. This problem arises due to the heavy reliance on a limited number of support samples, which hampers... | Saeed Ahmadnia, Arash Yousefi Jordehi, Mahsa Hosseini Khasheh Heyran, Seyed Abolghasem Mirroshandel, Owen Rambow, Cornelia Caragea |  |
| 422 |  |  [Enhancing Multimodal Entity Linking with Jaccard Distance-based Conditional Contrastive Learning and Contextual Visual Augmentation](https://doi.org/10.18653/v1/2025.naacl-long.341) |  | 0 | Previous research on multimodal entity linking (MEL) has primarily employed contrastive learning as the primary objective. However, using the rest of the batch as negative samples without careful consideration, these studies risk leveraging easy features and potentially overlook essential details that make entities unique. In this work, we propose JD-CCL (Jaccard Distance-based Conditional Contrastive Learning), a novel approach designed to enhance the ability to match multimodal entity linking... | CongDuy T. Nguyen, Xiaobao Wu, Thong Thanh Nguyen, Shuai Zhao, Khoi M. Le, VietAnh Nguyen, Yichao Feng, Anh Tuan Luu |  |
| 423 |  |  [ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.342) |  | 0 | The pace of scientific research, vital for improving human life, is complex, slow, and needs specialized expertise. Meanwhile, novel, impactful research often stems from both a deep understanding of prior work, and a cross-pollination of ideas across domains and fields. To enhance the productivity of researchers, we propose ResearchAgent, which leverages the encyclopedic knowledge and linguistic reasoning capabilities of Large Language Models (LLMs) to assist them in their work. This system... | Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang |  |
| 424 |  |  [Logit Separability-Driven Samples and Multiple Class-Related Words Selection for Advancing In-Context Learning](https://doi.org/10.18653/v1/2025.naacl-long.343) |  | 0 | Effective organization of in-context learning (ICL) demonstrations is key to improving the quality of large language model (LLM) responses. To create better sample-label pairs that instruct LLM understanding, we introduce logit separability, a criterion to assess the clarity of both samples and class-related words at the logit level. This facilitates the optimization of sample and label selection, enhancing the precision of information provided in ICL demonstrations. Additionally, we find that... | Zixiao Zhu, Zijian Feng, Hanzhang Zhou, Junlang Qian, Kezhi Mao |  |
| 425 |  |  [Identifying Emerging Concepts in Large Corpora](https://doi.org/10.18653/v1/2025.naacl-long.344) |  | 0 | We introduce a new method to identify emerging concepts in large text corpora. By analyzing changes in the heatmaps of the underlying embedding space, we are able to detect these concepts with high accuracy shortly after they originate, in turn outperforming common alternatives. We further demonstrate the utility of our approach by analyzing speeches in the U.S. Senate from 1941 to 2015. Our results suggest that the minority party is more active in introducing new concepts into the Senate... | Sibo Ma, Julian Nyarko |  |
| 426 |  |  [CodeSCM: Causal Analysis for Multi-Modal Code Generation](https://doi.org/10.18653/v1/2025.naacl-long.345) |  | 0 | In this paper, we propose CodeSCM, a Structural Causal Model (SCM) for analyzing multi-modal code generation using large language models (LLMs). By applying interventions to CodeSCM, we measure the causal effects of different prompt modalities, such as natural language, code, and input-output examples, on the model. CodeSCM introduces latent mediator variables to separate the code and natural language semantics of a multi-modal code generation prompt. Using the principles of Causal Mediation... | Mukur Gupta, Noopur Bhatt, Suman Jana |  |
| 427 |  |  [From Distributional to Overton Pluralism: Investigating Large Language Model Alignment](https://doi.org/10.18653/v1/2025.naacl-long.346) |  | 0 | The alignment process changes several properties of a large language model’s (LLM’s) output distribution. We analyze two aspects of post-alignment distributional shift of LLM responses. First, we re-examine previously reported reductions in response diversity post-alignment. Our analysis suggests that an apparent drop in the diversity of responses is largely explained by quality control and information aggregation. Alignment suppresses irrelevant and unhelpful content while shifting the output... | Thom Lake, Eunsol Choi, Greg Durrett |  |
| 428 |  |  [Advancing MoE Efficiency: A Collaboration-Constrained Routing (C2R) Strategy for Better Expert Parallelism Design](https://doi.org/10.18653/v1/2025.naacl-long.347) |  | 0 |  | Mohan Zhang, Pingzhi Li, Jie Peng, Mufan Qiu, Tianlong Chen |  |
| 429 |  |  [LibEvolutionEval: A Benchmark and Study for Version-Specific Code Generation](https://doi.org/10.18653/v1/2025.naacl-long.348) |  | 0 | Recent advancements in code completion models have primarily focused on local file contexts. However, these studies do not fully capture the complexity of real-world software development, which often requires the use of rapidly-evolving public libraries. To address this gap, we introduce LibEvolutionEval, a comprehensive study that emphasizes the need to understand library evolution to perform accurate in-line code completions. LibEvolutionEvaloffers a version-specific code-completion task... | Sachit Kuhar, Wasi Uddin Ahmad, Zijian Wang, Nihal Jain, Haifeng Qian, Baishakhi Ray, Murali Krishna Ramanathan, Xiaofei Ma, Anoop Deoras |  |
| 430 |  |  [Evaluating and Mitigating Object Hallucination in Large Vision-Language Models: Can They Still See Removed Objects?](https://doi.org/10.18653/v1/2025.naacl-long.349) |  | 0 | Large Vision-Language Models (LVLMs) have a significant issue with object hallucinations, where researchers have noted that LVLMs often mistakenly determine objects as present in images where they do not actually exist. Some recent studies evaluate the occurrence of object hallucinations by asking LVLMs whether they see objects that do not exist in input images. However, we observe that these evaluation methods have some limitations, such as the objects being questioned potentially having... | Yixiao He, Haifeng Sun, Pengfei Ren, Jingyu Wang, Huazheng Wang, Qi Qi, Zirui Zhuang, Jing Wang |  |
| 431 |  |  [Self-Pluralising Culture Alignment for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.350) |  | 0 | As large language models (LLMs) become increasingly accessible in many countries, it is essential to align them to serve pluralistic human values across cultures. However, pluralistic culture alignment in LLMs remain an open problem. In this paper, we propose CultureSPA, a Self-Pluralising Culture Alignment framework that allows LLMs to simultaneously align to pluralistic cultures. The framework first generates questions on various culture topics, then yields LLM outputs in response to these... | Shaoyang Xu, Yongqi Leng, Linhao Yu, Deyi Xiong |  |
| 432 |  |  [K-COMP: Retrieval-Augmented Medical Domain Question Answering With Knowledge-Injected Compressor](https://doi.org/10.18653/v1/2025.naacl-long.351) |  | 0 |  | Jeonghun Cho, Gary Lee |  |
| 433 |  |  [DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students' Hand-Drawn Math Images](https://doi.org/10.18653/v1/2025.naacl-long.352) |  | 0 | In real-world settings, vision language models (VLMs) should robustly handle naturalistic, noisy visual content as well as domain-specific language and concepts. For example, K-12 educators using digital learning platforms may need to examine and provide feedback across many images of students’ math work. To assess the potential of VLMs to support educators in settings like this one, we introduce DrawEduMath, an English-language dataset of 2,030 images of students’ handwritten responses to K-12... | Sami Baral, Li Lucy, Ryan Knight, Alice Ng, Luca Soldaini, Neil T. Heffernan, Kyle Lo |  |
| 434 |  |  [Knowledge Graph Guided Evaluation of Abstention Techniques](https://doi.org/10.18653/v1/2025.naacl-long.353) |  | 0 | To deploy language models safely, it is crucial that they abstain from responding to inappropriate requests. Several prior studies test the safety promises of models based on their effectiveness in blocking malicious requests. In this work, we focus on evaluating the underlying techniques that cause models to abstain. We create ‘SELECT‘, a benchmark derived from a set of benign concepts (e.g., “rivers”) from a knowledge graph. Focusing on benign concepts isolates the effect of safety training,... | Kinshuk Vasisht, Navreet Kaur, Danish Pruthi |  |
| 435 |  |  [Wav2Prompt: End-to-End Speech Prompt Learning and Task-based Fine-tuning for Text-based LLMs](https://doi.org/10.18653/v1/2025.naacl-long.354) |  | 0 | Wav2Prompt is proposed which allows integrating spoken input with a text-based large language model (LLM). Wav2Prompt uses a straightforward training process with only the same data used to train an automatic speech recognition (ASR) model. After training, Wav2Prompt learns continuous representations from speech and uses them as LLM prompts. To avoid task over-fitting issues found in prior work and preserve the emergent abilities of LLMs, Wav2Prompt takes LLM token embeddings as the training... | Keqi Deng, Guangzhi Sun, Philip C. Woodland |  |
| 436 |  |  [Legal Judgment Prediction based on Knowledge-enhanced Multi-Task and Multi-Label Text Classification](https://doi.org/10.18653/v1/2025.naacl-long.355) |  | 0 | Legal judgment prediction (LJP) is an essential task for legal AI, aiming at predicting judgments based on the facts of a case. Legal judgments can involve multiple law articles and charges. Although recent methods in LJP have made notable progress, most are constrained to single-task settings (e.g., only predicting charges) or single-label settings (e.g., not accommodating cases with multiple charges), diverging from the complexities of real-world scenarios. In this paper, we address the... | Ang Li, Yiquan Wu, Ming Cai, Adam Jatowt, Xiang Zhou, Weiming Lu, Changlong Sun, Fei Wu, Kun Kuang |  |
| 437 |  |  [SPeCtrum: A Grounded Framework for Multidimensional Identity Representation in LLM-Based Agent](https://doi.org/10.18653/v1/2025.naacl-long.356) |  | 0 | Existing methods for simulating individual identities often oversimplify human complexity, which may lead to incomplete or flattened representations. To address this, we introduce SPeCtrum, a grounded framework for constructing authentic LLM agent personas by incorporating an individual’s multidimensional self-concept. SPeCtrum integrates three core components: Social Identity (S), Personal Identity (P), and Personal Life Context (C), each contributing distinct yet interconnected aspects of... | Keyeun Lee, SeoHyeong Kim, Seolhee Lee, Jinsu Eun, Yena Ko, Hayeon Jeon, Esther Hehsun Kim, Seonghye Cho, Soeun Yang, Eunmee Kim, Hajin Lim |  |
| 438 |  |  [Beemo: Benchmark of Expert-edited Machine-generated Outputs](https://doi.org/10.18653/v1/2025.naacl-long.357) |  | 0 | The rapid proliferation of large language models (LLMs) has increased the volume of machine-generated texts (MGTs) and blurred text authorship in various domains. However, most existing MGT benchmarks include single-author texts (human-written and machine-generated). This conventional design fails to capture more practical multi-author scenarios, where the user refines the LLM response for natural flow, coherence, and factual correctness. Our paper introduces the Benchmark of Expert-edited... | Ekaterina Artemova, Jason Samuel Lucas, Saranya Venkatraman, Jooyoung Lee, Sergei Tilga, Adaku Uchendu, Vladislav Mikhailov |  |
| 439 |  |  [SANDWiCH: Semantical Analysis of Neighbours for Disambiguating Words in Context ad Hoc](https://doi.org/10.18653/v1/2025.naacl-long.358) |  | 0 | The rise of generative chat-based Large Language Models (LLMs) over the past two years has spurred a race to develop systems that promise near-human conversational and reasoning experiences. However, recent studies indicate that the language understanding offered by these models remains limited and far from human-like performance, particularly in grasping the contextual meanings of words—an essential aspect of reasoning. In this paper, we present a simple yet computationally efficient framework... | Daniel GuzmanOlivares, Lara Quijano Sánchez, Federico Liberatore |  |
| 440 |  |  [Towards Automatic Evaluation for Image Transcreation](https://doi.org/10.18653/v1/2025.naacl-long.359) |  | 0 | Beyond conventional paradigms of translating speech and text, recently, there has been interest in automated transcreation of images to facilitate localization of visual content across different cultures. Attempts to define this as a formal Machine Learning (ML) problem have been impeded by the lack of automatic evaluation mechanisms, with previous work relying solely on human evaluation. In this paper, we seek to close this gap by proposing a suite of automatic evaluation metrics inspired by... | Simran Khanuja, Vivek Iyer, Xiaoyu He, Graham Neubig |  |
| 441 |  |  [ImgTrojan: Jailbreaking Vision-Language Models with ONE Image](https://doi.org/10.18653/v1/2025.naacl-long.360) |  | 0 | There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By... | Xijia Tao, Shuai Zhong, Lei Li, Qi Liu, Lingpeng Kong |  |
| 442 |  |  [RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement](https://doi.org/10.18653/v1/2025.naacl-long.361) |  | 0 | Existing large language models (LLMs) show exceptional problem-solving capabilities but might struggle with complex reasoning tasks. Despite the successes of chain-of-thought and tree-based search methods, they mainly depend on the internal knowledge of LLMs to search over intermediate reasoning steps, limited to dealing with simple tasks involving fewer reasoning steps. In this paper, we propose RAG-Star, a novel RAG approach that integrates the retrieved information to guide the tree-based... | Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Xin Zhao, Yang Song, Tao Zhang |  |
| 443 |  |  [Mitigating Biases of Large Language Models in Stance Detection with Counterfactual Augmented Calibration](https://doi.org/10.18653/v1/2025.naacl-long.362) |  | 0 | Stance detection is critical for understanding the underlying position or attitude expressed toward a topic. Large language models (LLMs) have demonstrated significant advancements across various natural language processing tasks including stance detection, however, their performance in stance detection is limited by biases and spurious correlations inherent due to their data-driven nature. Our statistical experiment reveals that LLMs are prone to generate biased stances due to sentiment-stance... | Ang Li, Jingqian Zhao, Bin Liang, Lin Gui, Hui Wang, Xi Zeng, Xingwei Liang, KamFai Wong, Ruifeng Xu |  |
| 444 |  |  [Beyond the Next Token: Towards Prompt-Robust Zero-Shot Classification via Efficient Multi-Token Prediction](https://doi.org/10.18653/v1/2025.naacl-long.363) |  | 0 |  | Junlang Qian, Zixiao Zhu, Hanzhang Zhou, Zijian Feng, Zepeng Zhai, Kezhi Mao |  |
| 445 |  |  [Investigating Hallucinations in Simultaneous Machine Translation: Knowledge Distillation Solution and Components Analysis](https://doi.org/10.18653/v1/2025.naacl-long.364) |  | 0 | Simultaneous Machine Translation (SiMT) generates target translation before receiving the whole source sentence and faces a serious hallucination problem. In contrast, traditional offline machine translation (OMT) models exhibit significantly fewer hallucinations. Motivated by this disparity, we propose Knowledge Distillation for SiMT (KD-SiMT), a simple yet effective method that utilizes the OMT model to mitigate hallucinations in SiMT. Experiments on Zh→En and De→En tasks demonstrate that... | Donglei Yu, Xiaomian Kang, Yuchen Liu, Feifei Zhai, Nanchang Cheng, Yu Zhou, Chengqing Zong |  |
| 446 |  |  [Markov Chain of Thought for Efficient Mathematical Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.365) |  | 0 |  | Wen Yang, Minpeng Liao, Kai Fan |  |
| 447 |  |  [Towards Inducing Long-Context Abilities in Multilingual Neural Machine Translation Models](https://doi.org/10.18653/v1/2025.naacl-long.366) |  | 0 | Neural Machine Translation (NMT) models have traditionally used Sinusoidal Positional Embeddings (PEs), which often struggle to capture long-range dependencies and are inefficient for handling extended context or document-level translation tasks. This work addresses the challenge of transitioning pre-trained NMT models from absolute Sinusoidal PEs to Relative PEs, such as RoPE and ALiBi, without compromising performance. We demonstrate that parameter-efficient fine-tuning, using only a small... | Varun Gumma, Pranjal A. Chitale, Kalika Bali |  |
| 448 |  |  [Yeah, Un, Oh: Continuous and Real-time Backchannel Prediction with Fine-tuning of Voice Activity Projection](https://doi.org/10.18653/v1/2025.naacl-long.367) |  | 0 | In human conversations, short backchannel utterances such as “yeah” and “oh” play a crucial role in facilitating smooth and engaging dialogue.These backchannels signal attentiveness and understanding without interrupting the speaker, making their accurate prediction essential for creating more natural conversational agents.This paper proposes a novel method for real-time, continuous backchannel prediction using a fine-tuned Voice Activity Projection (VAP) model.While existing approaches have... | Koji Inoue, Divesh Lala, Gabriel Skantze, Tatsuya Kawahara |  |
| 449 |  |  [Prompt Compression for Large Language Models: A Survey](https://doi.org/10.18653/v1/2025.naacl-long.368) |  | 0 | Leveraging large language models (LLMs) for complex natural language tasks typically requires long-form prompts to convey detailed requirements and information, which results in increased memory usage and inference costs. To mitigate these challenges, multiple efficient methods have been proposed, with prompt compression gaining significant research interest. This survey provides an overview of prompt compression techniques, categorized into hard prompt methods and soft prompt methods. First,... | Zongqian Li, Yinhong Liu, Yixuan Su, Nigel Collier |  |
| 450 |  |  [Goal-Conditioned DPO: Prioritizing Safety in Misaligned Instructions](https://doi.org/10.18653/v1/2025.naacl-long.369) |  | 0 | Large language models (LLMs) undergo extensive safety training to maximize both helpfulness and harmlessness in their responses. However, various jailbreak attacks jeopardize model safety, allowing malicious actors to bypass safety guidelines. Existing defense methods primarily focus on aligning the model’s output towards less harmful responses through post-processing or input perturbation. Consequently, these approaches are prone to general performance degradation and lack the ability to... | Joo Bon Maeng, Seongmin Lee, Seokin Seo, KeeEung Kim |  |
| 451 |  |  [K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.370) |  | 0 | Strategic reasoning is a complex yet essential capability for intelligent agents. It requires Large Language Model (LLM) agents to adapt their strategies dynamically in multi-agent environments. Unlike static reasoning tasks, success in these contexts depends on anticipating other agents’ beliefs and actions while continuously adjusting strategies to achieve individual goals. LLMs and LLM agents often struggle with strategic reasoning due to the absence of a reasoning framework that enables... | Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Man Lan, Furu Wei |  |
| 452 |  |  [SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.371) |  | 0 | Syllogistic reasoning is crucial for Natural Language Inference (NLI). This capability is particularly significant in specialized domains such as biomedicine, where it can support automatic evidence interpretation and scientific discovery. This paper presents SylloBio-NLI, a novel framework that leverages external ontologies to systematically instantiate diverse syllogistic arguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language Models (LLMs) on identifying valid... | Magdalena Wysocka, Danilo S. Carvalho, Oskar Wysocki, Marco Valentino, André Freitas |  |
| 453 |  |  [The State and Fate of Summarization Datasets: A Survey](https://doi.org/10.18653/v1/2025.naacl-long.372) |  | 0 | Automatic summarization has consistently attracted attention due to its versatility and wide application in various downstream tasks. Despite its popularity, we find that annotation efforts have largely been disjointed, and have lacked common terminology. Consequently, it is challenging to discover existing resources or identify coherent research directions. To address this, we survey a large body of work spanning 133 datasets in over 100 languages, creating a novel ontology covering sample... | Noam Dahan, Gabriel Stanovsky |  |
| 454 |  |  [MGM: Global Understanding of Audience Overlap Graphs for Predicting the Factuality and the Bias of News Media](https://doi.org/10.18653/v1/2025.naacl-long.373) |  | 0 | In the current era of rapidly growing digital data, evaluating the political bias and factuality of news outlets has become more important for seeking reliable information online. In this work, we study the classification problem of profiling news media from the lens of political bias and factuality. Traditional profiling methods, such as Pre-trained Language Models (PLMs) and Graph Neural Networks (GNNs) have shown promising results, but they face notable challenges. PLMs focus solely on... | Muhammad Arslan Manzoor, Ruihong Zeng, Dilshod Azizov, Preslav Nakov, Shangsong Liang |  |
| 455 |  |  [A Logical Fallacy-Informed Framework for Argument Generation](https://doi.org/10.18653/v1/2025.naacl-long.374) |  | 0 | Despite the remarkable performance of large language models (LLMs), they still struggle with generating logically sound arguments, resulting in potential risks such as spreading misinformation. An important factor contributing to LLMs’ suboptimal performance in generating coherent arguments is their oversight of logical fallacies. To address this issue, we introduce fallacy-informed preference optimization (FIPO) that helps steer LLMs toward generating logically sound arguments. FIPO includes a... | Luca Mouchel, Debjit Paul, Shaobo Cui, Robert West, Antoine Bosselut, Boi Faltings |  |
| 456 |  |  [LLaMA-Berry: Pairwise Optimization for Olympiad-level Mathematical Reasoning via O1-like Monte Carlo Tree Search](https://doi.org/10.18653/v1/2025.naacl-long.375) |  | 0 | This paper presents LLaMA-Berry, an advanced mathematical reasoning framework to enhance the problem-solving ability of large language models (LLMs). The framework combines Monte Carlo Tree Search with Self-Refine (SR-MCTS) to optimize the reasoning paths and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critique and rewriting capabilities of LLMs, our SR-MCTS overcomes the inefficiencies and limitations of conventional step-wise and greedy search... | Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, Dongzhan Zhou |  |
| 457 |  |  [Generative Prompt Internalization](https://doi.org/10.18653/v1/2025.naacl-long.376) |  | 0 | Prompts used in recent large language model based applications are often fixed and lengthy, leading to significant computational overhead. To address this challenge, we propose Generative Prompt Internalization (GenPI), a lightweight method that employs a joint training approach. GenPI not only replicates the behavior of models with prompt inputs but also generates the content of the prompt along with reasons for why the model’s behavior should change accordingly. We demonstrate that our... | Haebin Shin, Lei Ji, Yeyun Gong, Sungdong Kim, Eunbi Choi, Minjoon Seo |  |
| 458 |  |  [Script-Agnosticism and its Impact on Language Identification for Dravidian Languages](https://doi.org/10.18653/v1/2025.naacl-long.377) |  | 0 | Language identification is used as the first step in many data collection and crawling efforts because it allows us to sort online text into language-specific buckets. However, many modern languages, such as Konkani, Kashmiri, Punjabi etc., are synchronically written in several scripts. Moreover, languages with different writing systems do not share significant lexical, semantic, and syntactic properties in neural representation spaces, which is a disadvantage for closely related languages and... | Milind Agarwal, Joshua Otten, Antonios Anastasopoulos |  |
| 459 |  |  [NAT: Enhancing Agent Tuning with Negative Samples](https://doi.org/10.18653/v1/2025.naacl-long.378) |  | 0 | Interaction trajectories between agents and environments have proven effective in tuning LLMs into task-specific agents. However, constructing these trajectories, especially successful trajectories, is often computationally and time intensive due to the relatively low success rates of even the most advanced LLMs, such as GPT-4 and Claude. Additionally, common training paradigms like supervised fine-tuning (SFT) and reinforcement learning (RL) not only require large volumes of data but also have... | Renxi Wang, Xudong Han, Yixuan Zhang, Timothy Baldwin, Haonan Li |  |
| 460 |  |  [Hazards in Daily Life? Enabling Robots to Proactively Detect and Resolve Anomalies](https://doi.org/10.18653/v1/2025.naacl-long.379) |  | 0 | Existing household robots have made significant progress in performing routine tasks, such as cleaning floors or delivering objects. However, a key limitation of these robots is their inability to recognize potential problems or dangers in home environments. For example, a child may pick up and ingest medication that has fallen on the floor, posing a serious risk. We argue that household robots should proactively detect such hazards or anomalies within the home, and propose the task of anomaly... | Zirui Song, Guangxian Ouyang, Meng Fang, Hongbin Na, Zijing Shi, Zhenhao Chen, Yujie Fu, Zeyu Zhang, Shiyu Jiang, Miao Fang, Ling Chen, Xiuying Chen |  |
| 461 |  |  [How to Make the Most of LLMs' Grammatical Knowledge for Acceptability Judgments](https://doi.org/10.18653/v1/2025.naacl-long.380) |  | 0 | The grammatical knowledge of language models (LMs) is often measured using a benchmark of linguistic minimal pairs, where LMs are presented with a pair of acceptable and unacceptable sentences and required to judge which is more acceptable. Conventional approaches compare sentence probabilities directly, but large language models (LLMs) provide nuanced evaluation methods using prompts and templates. We therefore investigate how to derive the most accurate acceptability judgments from LLMs to... | Yusuke Ide, Yuto Nishida, Justin Vasselli, Miyu Oba, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe |  |
| 462 |  |  [Is Your LLM Outdated? A Deep Look at Temporal Generalization](https://doi.org/10.18653/v1/2025.naacl-long.381) |  | 0 | The rapid advancement of Large Language Models (LLMs) has led to the development of benchmarks that consider temporal dynamics, however, there remains a gap in understanding how well these models can generalize across temporal contexts due to the inherent dynamic nature of language and information. This paper introduces the concept of temporal generalization in LLMs, including bias in past and future generalizations. Then we introduce FreshBench, a new evaluation framework that employs fresh... | ChenghaoZhu ChenghaoZhu, Nuo Chen, Yufei Gao, Yunyi Zhang, Prayag Tiwari, Benyou Wang |  |
| 463 |  |  [Towards a Perspectivist Turn in Argument Quality Assessment](https://doi.org/10.18653/v1/2025.naacl-long.382) |  | 0 | The assessment of argument quality depends on well-established logical, rhetorical, and dialectical properties that are unavoidably subjective: multiple valid assessments may exist, there is no unequivocal ground truth. This aligns with recent paths in machine learning, which embrace the co-existence of different perspectives. However, this potential remains largely unexplored in NLP research on argument quality. One crucial reason seems to be the yet unexplored availability of suitable... | Julia Romberg, Maximilian Maurer, Henning Wachsmuth, Gabriella Lapesa |  |
| 464 |  |  [A Picture is Worth A Thousand Numbers: Enabling LLMs Reason about Time Series via Visualization](https://doi.org/10.18653/v1/2025.naacl-long.383) |  | 0 | Large language models (LLMs), with demonstrated reasoning abilities across multiple domains, have been largely underexplored fortime-series reasoning (TsR), which is ubiquitous in the real world. In this work, wepropose TimerBed, the first comprehensivetestbed for evaluating LLMs’ TsR performance.Specifically, TimerBed includes stratified reasoning patterns with real-world tasks, diversecombinations of LLMs and reasoning strategies, and various supervised models as comparison anchors. We... | Haoxin Liu, Chenghao Liu, B. Aditya Prakash |  |
| 465 |  |  [PlagBench: Exploring the Duality of Large Language Models in Plagiarism Generation and Detection](https://doi.org/10.18653/v1/2025.naacl-long.384) |  | 0 | Recent studies have raised concerns about the potential threats large language models (LLMs) pose to academic integrity and copyright protection. Yet, their investigation is predominantly focused on literal copies of original texts. Also, how LLMs can facilitate the detection of LLM-generated plagiarism remains largely unexplored. To address these gaps, we introduce PlagBench, a dataset of 46.5K synthetic text pairs that represent three major types of plagiarism: verbatim copying, paraphrasing,... | Jooyoung Lee, Toshini Agrawal, Adaku Uchendu, Thai Le, Jinghui Chen, Dongwon Lee |  |
| 466 |  |  [Commonality and Individuality! Integrating Humor Commonality with Speaker Individuality for Humor Recognition](https://doi.org/10.18653/v1/2025.naacl-long.385) |  | 0 | Humor recognition aims to identify whether a specific speaker’s text is humorous. Current methods for humor recognition mainly suffer from two limitations: (1) they solely focus on one aspect of humor commonalities, ignoring the multifaceted nature of humor; and (2) they typically overlook the critical role of speaker individuality, which is essential for a comprehensive understanding of humor expressions. To bridge these gaps, we introduce the Commonality and Individuality Incorporated Network... | Haohao Zhu, Xiaokun Zhang, Zeyuan Zeng, Junyu Lu, Zewen Bai, Liang Yang, Hongfei Lin |  |
| 467 |  |  [CAST: Corpus-Aware Self-similarity Enhanced Topic modelling](https://doi.org/10.18653/v1/2025.naacl-long.386) |  | 0 | Topic modelling is a pivotal unsupervised machine learning technique for extracting valuable insights from large document collections. Existing neural topic modelling methods often encode contextual information of documents, while ignoring contextual details of candidate centroid words, leading to the inaccurate selection of topic words due to the \*contextualization gap\*. In parallel, it is found that functional words are frequently selected over topical words. To address these limitations,... | Yanan Ma, Chenghao Xiao, Chenhan Yuan, Sabine N. van der Veer, Lamiece Hassan, Chenghua Lin, Goran Nenadic |  |
| 468 |  |  [A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding](https://doi.org/10.18653/v1/2025.naacl-long.387) |  | 0 | Dialogue State Tracking (DST) is crucial for understanding user needs and executing appropriate system actions in task-oriented dialogues. Majority of existing DST methods are designed to work within predefined ontologies and assume the availability of gold domain labels, struggling with adapting to new slots values. While Large Language Models (LLMs)-based systems show promising zero-shot DST performance, they either require extensive computational resources or they underperform existing... | Abdulfattah Safa, Gözde Gül Sahin |  |
| 469 |  |  [Navigating the Cultural Kaleidoscope: A Hitchhiker's Guide to Sensitivity in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.388) |  | 0 | Cultural harm stems in LLMs whereby these models fail to align with specific cultural norms, resulting in misrepresentations or violations of cultural values. This work addresses the challenges of ensuring cultural sensitivity in LLMs, especially in small-parameter models that often lack the extensive training data needed to capture global cultural nuances. We present two key contributions: (1) A cultural harm test dataset, created to assess model outputs across different cultural contexts... | Somnath Banerjee, Sayan Layek, Hari Shrawgi, Rajarshi Mandal, Avik Halder, Shanu Kumar, Sagnik Basu, Parag Agrawal, Rima Hazra, Animesh Mukherjee |  |
| 470 |  |  [Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models](https://doi.org/10.18653/v1/2025.naacl-long.389) |  | 0 | Text-to-image (T2I) diffusion models rely on encoded prompts to guide the image generation process. Typically, these prompts are extended to a fixed length by appending padding tokens to the input. Despite being a default practice, the influence of padding tokens on the image generation process has not been investigated. In this work, we conduct the first in-depth analysis of the role padding tokens play in T2I models. We develop two causal techniques to analyze how information is encoded in... | Michael Toker, Ido Galil, Hadas Orgad, Rinon Gal, Yoad Tewel, Gal Chechik, Yonatan Belinkov |  |
| 471 |  |  [In-Context Learning (and Unlearning) of Length Biases](https://doi.org/10.18653/v1/2025.naacl-long.390) |  | 0 | Large language models have demonstrated strong capabilities to learn in-context, where exemplar input-output pairings are appended to the prompt for demonstration. However, existing work has demonstrated the ability of models to learn lexical and label biases in-context, which negatively impacts both performance and robustness of models. The impact of other statistical data biases remains under-explored, which this work aims to address. We specifically investigate the impact of length biases on... | Stephanie Schoch, Yangfeng Ji |  |
| 472 |  |  [AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising](https://doi.org/10.18653/v1/2025.naacl-long.391) |  | 0 | As the fluency of ad texts automatically generated by natural language generation technologies continues to improve, there is an increasing demand to assess the quality of these creatives in real-world setting.We propose \*\*AdTEC\*\*, the first public benchmark to evaluate ad texts from multiple perspectives within practical advertising operations.Our contributions are as follows: (i) Defining five tasks for evaluating the quality of ad texts, as well as constructing a Japanese dataset based... | Peinan Zhang, Yusuke Sakai, Masato Mita, Hiroki Ouchi, Taro Watanabe |  |
| 473 |  |  [Empowering Retrieval-based Conversational Recommendation with Contrasting User Preferences](https://doi.org/10.18653/v1/2025.naacl-long.392) |  | 0 | Conversational recommender systems (CRSs) are designed to suggest the target item that the user is likely to prefer through multi-turn conversations. Recent studies stress that capturing sentiments in user conversations improves recommendation accuracy. However, they employ a single user representation, which may fail to distinguish between contrasting user intentions, such as likes and dislikes, potentially leading to suboptimal performance. To this end, we propose a novel conversational... | Heejin Kook, Junyoung Kim, Seongmin Park, Jongwuk Lee |  |
| 474 |  |  [LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices](https://doi.org/10.18653/v1/2025.naacl-long.393) |  | 0 | With the commercialization of large language models (LLMs), weight-activation quantization has emerged to compress and accelerate LLMs, achieving high throughput while reducing inference costs. However, existing post-training quantization (PTQ) techniques for quantizing weights and activations of LLMs still suffer from non-negligible accuracy drops, especially on massive multitask language understanding. To address this issue, we propose Low-Rank Quantization (LRQ) - a simple yet effective... | Jung Hyun Lee, Jeonghoon Kim, June Yong Yang, Se Jung Kwon, Eunho Yang, Kang Min Yoo, Dongsoo Lee |  |
| 475 |  |  [Towards Robust Knowledge Representations in Multilingual LLMs for Equivalence and Inheritance based Consistent Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.394) |  | 0 | Reasoning and linguistic skills form the cornerstone of human intelligence, facilitating problem-solving and decision-making. Recent advances in Large Language Models (LLMs) have led to impressive linguistic capabilities and emergent reasoning behaviors, fueling widespread adoption across application domains. However, LLMs still struggle with complex reasoning tasks, highlighting their systemic limitations. In this work, we focus on evaluating whether LLMs have the requisite representations to... | Gaurav Arora, Srujana Merugu, Shreya Jain, Vaibhav Saxena |  |
| 476 |  |  [LLMs as Meta-Reviewers' Assistants: A Case Study](https://doi.org/10.18653/v1/2025.naacl-long.395) |  | 0 | One of the most important yet onerous tasks in the academic peer-reviewing process is composing meta-reviews, which involves assimilating diverse opinions from multiple expert peers, formulating one’s self-judgment as a senior expert, and then summarizing all these perspectives into a concise holistic overview to make an overall recommendation. This process is time-consuming and can be compromised by human factors like fatigue, inconsistency, missing tiny details, etc. Given the latest major... | Eftekhar Hossain, Sanjeev Kumar Sinha, Naman Bansal, R. Alexander Knipper, Souvika Sarkar, John Salvador, Yash Mahajan, Sri Guttikonda, Mousumi Akter, Md. Mahadi Hassan, Matthew Freestone, Matthew C. Williams Jr., Dongji Feng, Santu Karmaker |  |
| 477 |  |  [A Survey of NLP Progress in Sino-Tibetan Low-Resource Languages](https://doi.org/10.18653/v1/2025.naacl-long.396) |  | 0 | Despite the increasing effort in including more low-resource languages in NLP/CL development, most of the world’s languages are still absent. In this paper, we take the example of the Sino-Tibetan language family which consists of hundreds of low-resource languages, and we look at the representation of these low-resource languages in papers archived on ACL Anthology. Our findings indicate that while more techniques and discussions on more languages are present in more publication venues over... | Shuheng Liu, Michael Best |  |
| 478 |  |  [Enhancing Language Model Hypernetworks with Restart: A Study on Optimization](https://doi.org/10.18653/v1/2025.naacl-long.397) |  | 0 | Hypernetworks are a class of meta-networks that generate weights for main neural networks. Their unique parameter spaces necessitate exploring suitable optimization strategies to enhance performance, especially for language models. However, a comprehensive investigation into optimization strategies for hypernetworks remains absent. To address this gap, we analyze the loss landscape of hypernetworks and propose that restart optimization strategies can improve their performance for language... | Yihan Zhang, Jie Fu, Rongrong Ji, Jie Chen |  |
| 479 |  |  [Functional Lexicon in Subword Tokenization](https://doi.org/10.18653/v1/2025.naacl-long.398) |  | 0 | The distinction between function and content units of the lexicon has been somewhat neglected in recent NLP work, but it could still be useful when working with low-resource languages, and, in particular, to improve cross-lingual transfer. In this paper, we investigate to what extent BPE subword tokenization can be used to identify units of the functional lexicon in a language without any annotated data. We analyze subword tokens in terms of their productivity and attempt to find thresholds... | Zachary William Hopton, Yves Scherrer, Tanja Samardzic |  |
| 480 |  |  [Getting More Juice Out of Your Data: Hard Pair Refinement Enhances Visual-Language Models Without Extra Data](https://doi.org/10.18653/v1/2025.naacl-long.399) |  | 0 | Contrastive Language-Image Pre-training (CLIP) has become the standard for cross- modal image-text representation learning. Improving CLIP typically requires additional data and retraining with new loss functions, but these demands raise resource and time costs, limiting practical use. In this work, we introduce HELIP, a cost-effective strategy that improves CLIP models by exploiting challenging text-image pairs within existing datasets in continuous training. This eliminates the need for... | Haonan Wang, Minbin Huang, Runhui Huang, Lanqing Hong, Hang Xu, Tianyang Hu, Xiaodan Liang, Zhenguo Li, Hong Cheng, Kenji Kawaguchi |  |
| 481 |  |  [Evaluating the Prompt Steerability of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.400) |  | 0 | Building pluralistic AI requires designing models that are able to be shaped to represent a wide range of value systems and cultures. Achieving this requires first being able to evaluate the degree to which a given model is capable of reflecting various personas. To this end, we propose a benchmark for evaluating the steerability of model personas as a function of prompting. Our design is based on a formal definition of prompt steerability, which analyzes the degree to which a model’s joint... | Erik Miehling, Michael Desmond, Karthikeyan Natesan Ramamurthy, Elizabeth M. Daly, Kush R. Varshney, Eitan Farchi, Pierre Dognin, Jesus Rios, Djallel Bouneffouf, Miao Liu, Prasanna Sattigeri |  |
| 482 |  |  [A Data-Driven Method for Analyzing and Quantifying Lyrics-Dance Motion Relationships](https://doi.org/10.18653/v1/2025.naacl-long.401) |  | 0 | Dancing to music with lyrics is a popular form of expression. While it is generally accepted that there are relationships between lyrics and dance motions, previous studies have not explored these relationships. A major challenge is that the relationships between lyrics and dance motions are not constant throughout a song but are instead localized to specific parts. To address this challenge, we hypothesize that lyrics and dance motions that co-occur across multiple songs are related. Based on... | Kento Watanabe, Masataka Goto |  |
| 483 |  |  [CROPE: Evaluating In-Context Adaptation of Vision and Language Models to Culture-Specific Concepts](https://doi.org/10.18653/v1/2025.naacl-long.402) |  | 0 | As Vision and Language models (VLMs) become accessible across the globe, it is important that they demonstrate cultural knowledge. In his paper, we introduce CROPE, a visual question answering benchmark designed to probe the knowledge of culture-specific concepts and evaluate the capacity for cultural adaptation through contextual information. This allows us to distinguish between parametric knowledge acquired during training and contextual knowledge provided during inference via visual and... | Malvina Nikandrou, Georgios Pantazopoulos, Nikolas Vitsakis, Ioannis Konstas, Alessandro Suglia |  |
| 484 |  |  [PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona](https://doi.org/10.18653/v1/2025.naacl-long.403) |  | 0 | Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests through natural language interactions, yet existing systems often produce generic, monotonic responses that lack individuality and fail to adapt to users’ personal attributes. To address this, we introduce PicPersona-TOD, a novel dataset that incorporates user images as part of the persona, enabling personalized responses tailored to user-specific factors such as age or emotional context. This is facilitated by first... | Jihyun Lee, Yejin Jeon, Seungyeon Seo, Gary Lee |  |
| 485 |  |  [Scaling LLM Inference Efficiently with Optimized Sample Compute Allocation](https://doi.org/10.18653/v1/2025.naacl-long.404) |  | 0 | Sampling is a basic operation for large language models (LLMs). In reinforcement learning rollouts and meta generation algorithms such as Best-of-N, it is essential to sample correct trajectories within a given compute budget. To find an optimal allocation for sample compute budgets, several choices need to be made:Which sampling configurations (model, temperature, language, etc.) to use?How many samples to generate in each configuration?We formulate these choices as a learning problem and... | Kexun Zhang, Shang Zhou, Danqing Wang, William Yang Wang, Lei Li |  |
| 486 |  |  [Large Language Models for Persian-English Idiom Translation](https://doi.org/10.18653/v1/2025.naacl-long.405) |  | 0 | Large language models (LLMs) have shown superior capabilities in translating figurative language compared to neural machine translation (NMT) systems. However, the impact of different prompting methods and LLM-NMT combinations on idiom translation has yet to be thoroughly investigated. This paper introduces two parallel datasets of sentences containing idiomatic expressions for Persian→English and English→Persian translations, with Persian idioms sampled from our PersianIdioms resource, a... | Sara Rezaeimanesh, Faezeh Hosseini, Yadollah Yaghoobzadeh |  |
| 487 |  |  [Follow the Beaten Path: The Role of Route Patterns on Vision-Language Navigation Agents Generalization Abilities](https://doi.org/10.18653/v1/2025.naacl-long.406) |  | 0 | Vision and language navigation (VLN) is a challenging task towards the creation of embodied agents that requires spatial and temporal reasoning over the instructions provided in natural language and aligning them with the visual perception of an environment. Although a number of methods and approaches have been developed, none achieves human level performance in outdoor settings (by up to 75 percent). The contributions of visual and language modalities to the success of VLN have been studied,... | Kourosh T. Baghaei, Dieter Pfoser, Antonios Anastasopoulos |  |
| 488 |  |  [Sneaking Syntax into Transformer Language Models with Tree Regularization](https://doi.org/10.18653/v1/2025.naacl-long.407) |  | 0 | While compositional accounts of human language understanding are based on a hierarchical tree-like process, neural models like transformers lack a direct inductive bias for such tree structures. Introducing syntactic inductive biases could unlock more robust and data-efficient learning in transformer language models (LMs), but existing methods for incorporating such structure greatly restrict models, either limiting their expressivity or increasing inference complexity. This work instead aims... | Ananjan Nandi, Christopher D. Manning, Shikhar Murty |  |
| 489 |  |  [Meta-Cultural Competence: Climbing the Right Hill of Cultural Awareness](https://doi.org/10.18653/v1/2025.naacl-long.408) |  | 0 | Numerous recent studies have shown that Large Language Models (LLMs) are biased towards a Western and Anglo-centric worldview, which compromises their usefulness in non-Western cultural settings. However, “culture” is a complex, multifaceted topic, and its awareness, representation, and modeling in LLMs and LLM-based applications can be defined and measured in numerous ways. In this position paper, we ask what does it mean for an LLM to possess “cultural awareness”, and through a thought... | Sougata Saha, Saurabh Kumar Pandey, Monojit Choudhury |  |
| 490 |  |  [Reading between the Lines: Can LLMs Identify Cross-Cultural Communication Gaps?](https://doi.org/10.18653/v1/2025.naacl-long.409) |  | 0 | In a rapidly globalizing and digital world, content such as book and product reviews created by people from diverse cultures are read and consumed by others from different corners of the world. In this paper, we investigate the extent and patterns of gaps in understandability of book reviews due to the presence of culturally-specific items and elements that might be alien to users from another culture. Our user-study on 57 book reviews from Goodreads reveal that 83% of the reviews had at least... | Sougata Saha, Saurabh Kumar Pandey, Harshit Gupta, Monojit Choudhury |  |
| 491 |  |  [HMT: Hierarchical Memory Transformer for Efficient Long Context Language Processing](https://doi.org/10.18653/v1/2025.naacl-long.410) |  | 0 | Transformer-based large language models (LLM) have been widely used in language processing applications. However, due to the memory constraints of the devices, most of them restrict the context window. Even though recurrent models in previous works can memorize past tokens to enable unlimited context and maintain effectiveness, they have “flat” memory architectures. Such architectures have limitations in selecting and filtering information. Since humans are good at learning and self-adjustment,... | Zifan He, Yingqi Cao, Zongyue Qin, Neha Prakriya, Yizhou Sun, Jason Cong |  |
| 492 |  |  [Faux Polyglot: A Study on Information Disparity in Multilingual Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.411) |  | 0 | Although the multilingual capability of LLMs offers new opportunities to overcome the language barrier, do these capabilities translate into real-life scenarios where linguistic divide and knowledge conflicts between multilingual sources are known occurrences? In this paper, we studied LLM’s linguistic preference in a cross-language RAG-based information search setting. We found that LLMs displayed systemic bias towards information in the same language as the query language in both document... | Nikhil Sharma, Kenton Murray, Ziang Xiao |  |
| 493 |  |  [Teaching Models to Balance Resisting and Accepting Persuasion](https://doi.org/10.18653/v1/2025.naacl-long.412) |  | 0 | Large language models (LLMs) are susceptible to persuasion, which can pose risks when models are faced with an adversarial interlocutor. We take a first step towards defending models against persuasion while also arguing that defense against adversarial (i.e. \*negative\*) persuasion is only half of the equation: models should also be able to accept beneficial (i.e. \*positive\*) persuasion to improve their answers. We show that optimizing models for only one side results in poor performance on... | Elias StengelEskin, Peter Hase, Mohit Bansal |  |
| 494 |  |  [Making Language Models Robust Against Negation](https://doi.org/10.18653/v1/2025.naacl-long.413) |  | 0 | Negation has been a long-standing challenge for language models.Previous studies have shown that they struggle with negation in many natural language understanding tasks.In this work, we propose a self-supervised method to make language models more robust against negation.We introduce a novel task, Next Sentence Polarity Prediction (NSPP), and a variation of the Next Sentence Prediction (NSP) task.We show that BERT and RoBERTa further pre-trained on our tasks outperform the off-the-shelf... | MohammadHossein Rezaei, Eduardo Blanco |  |
| 495 |  |  [Through the Lens of History: Methods for Analyzing Temporal Variation in Content and Framing of State-run Chinese Newspapers](https://doi.org/10.18653/v1/2025.naacl-long.414) |  | 0 | State-run Chinese newspapers are believed to strategically select and frame news articles to align with the shifting political tides of the country. This paper describes methods to quantify these changes in content and framing over time. Looking at more than 50 years of articles from the People’s Daily and Reference News, we analyze differences in name mentions and sentiment in news articles for politicians before and after their deaths, as well as during and not during certain political... | Shijia Liu, David A. Smith |  |
| 496 |  |  [PoisonedParrot: Subtle Data Poisoning Attacks to Elicit Copyright-Infringing Content from Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.415) |  | 0 | As the capabilities of large language models (LLMs) continue to expand, their usage has become increasingly prevalent. However, as reflected in numerous ongoing lawsuits regarding LLM-generated content, addressing copyright infringement remains a significant challenge. In this paper, we introduce PoisonedParrot: the first stealthy data poisoning attack that induces an LLM to generate copyrighted content even when the model has not been directly trained on the specific copyrighted material.... | MichaelAndrei PanaitescuLiess, Pankayaraj Pathmanathan, Yigitcan Kaya, Zora Che, Bang An, Sicheng Zhu, Aakriti Agrawal, Furong Huang |  |
| 497 |  |  [Towards Operationalizing Right to Data Protection](https://doi.org/10.18653/v1/2025.naacl-long.416) |  | 0 | The widespread practice of indiscriminate data scraping to fine-tune language models (LMs) raises significant legal and ethical concerns, particularly regarding compliance with data protection laws such as the General Data Protection Regulation (GDPR). This practice often results in the unauthorized use of personal information, prompting growing debate within the academic and regulatory communities. Recent works have introduced the concept of generating unlearnable datasets (by adding... | Abhinav Java, Simra Shahid, Chirag Agarwal |  |
| 498 |  |  [Learning vs Retrieval: The Role of In-Context Examples in Regression with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.417) |  | 0 | Generative Large Language Models (LLMs) are capable of being in-context learners. However, the underlying mechanism of in-context learning (ICL) is still a major research question, and experimental research results about how models exploit ICL are not always consistent. In this work, we propose a framework for evaluating in-context learning mechanisms, which we claim are a combination of retrieving internal knowledge and learning from in-context examples by focusing on regression tasks. First,... | Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi |  |
| 499 |  |  [GLiREL - Generalist Model for Zero-Shot Relation Extraction](https://doi.org/10.18653/v1/2025.naacl-long.418) |  | 0 | We introduce GLiREL, an efficient architecture and training paradigm for zero-shot relation classification. Identifying relationships between entities is a key task in information extraction pipelines. The zero-shot setting for relation extraction, where a taxonomy of relations is not pre-specified, has proven to be particularly challenging because of the computational complexity of inference, and because of the lack of labeled training data with sufficient coverage. Existing approaches rely... | Jack Boylan, Chris Hokamp, Demian Gholipour Ghalandari |  |
| 500 |  |  [ComPO: Community Preferences for Language Model Personalization](https://doi.org/10.18653/v1/2025.naacl-long.419) |  | 0 | Conventional algorithms for training language models (LMs) with human feedback rely on preferences that are assumed to account for an “average” user, disregarding subjectivity and finer-grained variations. Recent studies have raised concerns that aggregating such diverse and often contradictory human feedback to finetune models results in generic models that generate outputs not preferred by many user groups, as they tend to average out styles and norms. To address this issue, we draw... | Sachin Kumar, Chan Young Park, Yulia Tsvetkov, Noah A. Smith, Hannaneh Hajishirzi |  |
| 501 |  |  [GroundCocoa: A Benchmark for Evaluating Compositional & Conditional Reasoning in Language Models](https://doi.org/10.18653/v1/2025.naacl-long.420) |  | 0 | The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their reasoning to address complex task requirements. However, LLMs are known to unexpectedly falter in simple tasks and under seemingly straightforward circumstances - underscoring the need for better and more diverse evaluation setups to measure their true capabilities. To this end,... | Harsh Kohli, Sachin Kumar, Huan Sun |  |
| 502 |  |  [ALPACA AGAINST VICUNA: Using LLMs to Uncover Memorization of LLMs](https://doi.org/10.18653/v1/2025.naacl-long.421) |  | 0 | In this paper, we investigate the overlooked impact of instruction-tuning on memorization in large language models (LLMs), which has largely been studied in base, pre-trained models. We propose a black-box prompt optimization method where an attacker LLM agent uncovers higher levels of memorization in a victim agent, surpassing traditional approaches that prompt the model directly with training data. Using an iterative rejection-sampling process, we design instruction-based prompts that... | Aly M. Kassem, Omar Mahmoud, Niloofar Mireshghallah, Hyunwoo Kim, Yulia Tsvetkov, Yejin Choi, Sherif Saad, Santu Rana |  |
| 503 |  |  [Evaluating Contextualized Representations of (Spanish) Ambiguous Words: A New Lexical Resource and Empirical Analysis](https://doi.org/10.18653/v1/2025.naacl-long.422) |  | 0 | Lexical ambiguity—where a single wordform takes on distinct, context-dependent meanings–serves as a useful tool to compare across different language models’ (LMs’) ability to form distinct, contextualized representations of the same stimulus. Few studies have systematically compared LMs’ contextualized word embeddings for languages beyond English. Here, we evaluate semantic representations of Spanish ambiguous nouns in context in a suite of Spanish-language monolingual and multilingual... | Pamela D. Rivière, Anne L. BeattyMartínez, Sean Trott |  |
| 504 |  |  [Understanding LLMs' Fluid Intelligence Deficiency: An Analysis of the ARC Task](https://doi.org/10.18653/v1/2025.naacl-long.423) |  | 0 | While LLMs have exhibited strong performance on various NLP tasks, it is noteworthy that most of these tasks rely on utilizing the vast amount of knowledge encoded in LLMs’ parameters, rather than solving new problems without prior knowledge. In cognitive research, the latter ability is referred to as fluid intelligence, which is considered to be critical for assessing human intelligence. Recent research on fluid intelligence assessments has highlighted significant deficiencies in LLMs’... | Junjie Wu, Mo Yu, Lemao Liu, DitYan Yeung, Jie Zhou |  |
| 505 |  |  [FedSpaLLM: Federated Pruning of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.424) |  | 0 | Large Language Models (LLMs) achieve state-of-the-art performance but are challenging to deploy due to their high computational and storage demands. Pruning can reduce model size, yet existing methods assume public access to calibration data, which is impractical for privacy-sensitive applications. To address the challenge of pruning LLMs in privacy-preserving settings, we propose FedSpaLLM, the first federated learning framework designed specifically for pruning LLMs. FedSpaLLM enables clients... | Guangji Bai, Yijiang Li, Zilinghan Li, Liang Zhao, Kibaek Kim |  |
| 506 |  |  [IHEval: Evaluating Language Models on Following the Instruction Hierarchy](https://doi.org/10.18653/v1/2025.naacl-long.425) |  | 0 | The instruction hierarchy, which establishes a priority order from system messages to user messages, conversation history, and tool outputs, is essential for ensuring consistent and safe behavior in language models (LMs). Despite its importance, this topic receives limited attention, and there is a lack of comprehensive benchmarks for evaluating models’ ability to follow the instruction hierarchy. We bridge this gap by introducing IHEval, a novel benchmark comprising 3,538 examples across nine... | Zhihan Zhang, Shiyang Li, Zixuan Zhang, Xin Liu, Haoming Jiang, Xianfeng Tang, Yifan Gao, Zheng Li, Haodong Wang, Zhaoxuan Tan, Yichuan Li, Qingyu Yin, Bing Yin, Meng Jiang |  |
| 507 |  |  [Afrispeech-Dialog: A Benchmark Dataset for Spontaneous English Conversations in Healthcare and Beyond](https://doi.org/10.18653/v1/2025.naacl-long.426) |  | 0 | Speech technologies are transforming interactions across various sectors, from healthcare to call centers and robots, yet their performance on African-accented conversations remains underexplored. We introduce Afrispeech-Dialog, a benchmark dataset of 50 simulated medical and non-medical African-accented English conversations, designed to evaluate automatic speech recognition (ASR) and related technologies. We assess state-of-the-art (SOTA) speaker diarization and ASR systems on long-form,... | Mardhiyah Sanni, Tassallah Abdullahi, Devendra Deepak Kayande, Emmanuel Ayodele, Naome A. Etori, Michael S. Mollel, Moshood Yekini, Chibuzor Okocha, Lukman E. Ismaila, Folafunmi Omofoye, Boluwatife Adeleye Adewale, Tobi Olatunji |  |
| 508 |  |  [THREAD: Thinking Deeper with Recursive Spawning](https://doi.org/10.18653/v1/2025.naacl-long.427) |  | 0 | Large language models (LLMs) have shown impressive capabilities across diverse settings, but still struggle as the length and complexity of the context increases. To address this challenge, we propose Thinking Recursively and Dynamically (ThReaD). THREAD frames model generation as a thread of execution that, based on the context, can run to completion or dynamically spawn new threads. By spawning, threads can offload work (e.g., thinking, retrieving information) to child threads, which only... | Philip Schroeder, Nathaniel Morgan, Hongyin Luo, James R. Glass |  |
| 509 |  |  [CORG: Generating Answers from Complex, Interrelated Contexts](https://doi.org/10.18653/v1/2025.naacl-long.428) |  | 0 | In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single... | Hyunji Lee, Franck Dernoncourt, Trung Bui, Seunghyun Yoon |  |
| 510 |  |  [Generating Diverse Hypotheses for Inductive Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.429) |  | 0 | Inductive reasoning — the process of inferring general rules from a small number of observations — is a fundamental aspect of human intelligence. Recent works suggest that large language models (LLMs) can engage in inductive reasoning by sampling multiple hypotheses about the rules and selecting the one that best explains the observations. However, due to the IID sampling, semantically redundant hypotheses are frequently generated, leading to significant wastage of compute. In this paper, we 1)... | Kangil Lee, Hyukhun Koh, Dongryeol Lee, Seunghyun Yoon, Minsung Kim, Kyomin Jung |  |
| 511 |  |  [On the Analysis and Distillation of Emergent Outlier Properties in Pre-trained Language Models](https://doi.org/10.18653/v1/2025.naacl-long.430) |  | 0 | A small subset of dimensions within language Transformers’ representation spaces emerge as “outliers” during pretraining, encoding critical knowledge sparsely. We extend previous findings on emergent outliers to Encoder-Decoder Transformers and instruction-finetuned models, and tackle the problem of distilling a student Transformer from a larger teacher Transformer. Knowledge distillation reduces model size and cost by transferring knowledge from a larger teacher to a smaller student,... | Tianyang Zhao, Kunwar Yashraj Singh, Srikar Appalaraju, Peng Tang, Ying Nian Wu, Li Erran Li |  |
| 512 |  |  [Open-World Evaluation for Retrieving Diverse Perspectives](https://doi.org/10.18653/v1/2025.naacl-long.431) |  | 0 | We study retrieving a set of documents that covers various perspectives on a complex and contentious question (e.g., will ChatGPT do more harm than good?). We curate a Benchmark for Retrieval Diversity for Subjective questions (BERDS), where each example consists of a question and diverse perspectives associated with the question, sourced from survey questions and debate websites. On this data, retrievers paired with a corpus are evaluated to surface a document set that contains diverse... | HungTing Chen, Eunsol Choi |  |
| 513 |  |  [Analyzing the Inner Workings of Transformers in Compositional Generalization](https://doi.org/10.18653/v1/2025.naacl-long.432) |  | 0 | The compositional generalization abilities of neural models have been sought after for human-like linguistic competence.The popular method to evaluate such abilities is to assess the models’ input-output behavior.However, that does not reveal the internal mechanisms, and the underlying competence of such models in compositional generalization remains unclear.To address this problem, we explore the inner workings of a Transformer model byfinding an existing subnetwork that contributes to the... | Ryoma Kumon, Hitomi Yanaka |  |
| 514 |  |  [Substance Beats Style: Why Beginning Students Fail to Code with LLMs](https://doi.org/10.18653/v1/2025.naacl-long.433) |  | 0 | Although LLMs are increasing the productivity of professional programmers, existing work shows that beginners struggle to prompt LLMs to solve text-to-code tasks (Nguyen et al., 2024; Prather et al., 2024b; Mordechai et al., 2024). Why is this the case? This paper explores two competing hypotheses about the cause of student-LLM miscommunication: (1) students simply lack the technical vocabulary needed to write good prompts, and (2) students do not understand the extent of information that LLMs... | Francesca Lucchetti, Zixuan Wu, Arjun Guha, Molly Q. Feldman, Carolyn Jane Anderson |  |
| 515 |  |  [Reverse Thinking Makes LLMs Stronger Reasoners](https://doi.org/10.18653/v1/2025.naacl-long.434) |  | 0 | Reverse thinking plays a crucial role in human reasoning. Humans can reason not only from a problem to a solution but also in reverse, i.e., start from the solution and reason towards the problem. This often enhances overall reasoning performance as it enables consistency checks between their forward and backward thinking. To enable Large Language Models (LLMs) to perform reverse thinking, we introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data augmentation and learning... | Justin ChihYao Chen, Zifeng Wang, Hamid Palangi, Rujun Han, Sayna Ebrahimi, Long T. Le, Vincent Perot, Swaroop Mishra, Mohit Bansal, ChenYu Lee, Tomas Pfister |  |
| 516 |  |  [Towards Lifelong Dialogue Agents via Timeline-based Memory Management](https://doi.org/10.18653/v1/2025.naacl-long.435) |  | 0 | To achieve lifelong human-agent interaction, dialogue agents need to constantly memorize perceived information and properly retrieve it for response generation (RG). While prior studies focus on getting rid of outdated memories to improve retrieval quality, we argue that such memories provide rich, important contextual cues for RG (e.g., changes in user behaviors) in long-term conversations. We present THEANINE, a framework for LLM-based lifelong dialogue agents. THEANINE discards memory... | Kai Tzuiunn Ong, Namyoung Kim, Minju Gwak, Hyungjoo Chae, Taeyoon Kwon, Yohan Jo, Seungwon Hwang, Dongha Lee, Jinyoung Yeo |  |
| 517 |  |  [StyleDistance: Stronger Content-Independent Style Embeddings with Synthetic Parallel Examples](https://doi.org/10.18653/v1/2025.naacl-long.436) |  | 0 | Style representations aim to embed texts with similar writing styles closely and texts with different styles far apart, regardless of content. However, the contrastive triplets often used for training these representations may vary in both style and content, leading to potential content leakage in the representations. We introduce StyleDistance, a novel approach to training stronger content-independent style embeddings. We use a large language model to create a synthetic dataset of near-exact... | Ajay Patel, Jiacheng Zhu, Justin Qiu, Zachary Horvitz, Marianna Apidianaki, Kathleen McKeown, Chris CallisonBurch |  |
| 518 |  |  [FiNE: Filtering and Improving Noisy Data Elaborately with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.437) |  | 0 | Data is the lifeblood of large language models (LLMs). While the quantity of open-source data available for training LLMs is substantial, its integrity often falls short. For instance, the open-source chat version of Yi-1.5-9B scores 5.20 on AlignBench, while the Chinese Alpaca-GPT4 version scores 4.12. This discrepancy makes it challenging for developers to create models that excel in downstream tasks and instruction following. Therefore, it is essential to improve data integrity. Currently,... | Junliang He, Ziyue Fan, Shaohui Kuang, Li Xiaoqing, Kai Song, Yaqian Zhou, Xipeng Qiu |  |
| 519 |  |  [CAMIEval: Enhancing NLG Evaluation through Multidimensional Comparative Instruction-Following Analysis](https://doi.org/10.18653/v1/2025.naacl-long.438) |  | 0 | With the rapid development of large language models (LLMs), due to their strong performance across various fields, LLM-based evaluation methods (LLM-as-a-Judge) have become widely used in natural language generation (NLG) evaluation. However, these methods encounter the following challenges: (1) distinguishing instruction-following ability, (2) being applicable across diverse NLG tasks, and (3) identifying low-quality outputs. To address these issues, we propose CAMIEval, a multidimensional... | Ziyue Fan, Junliang He, Li Xiaoqing, Shaohui Kuang, Kai Song, Yaqian Zhou, Xipeng Qiu |  |
| 520 |  |  [LongLeader: A Comprehensive Leaderboard for Large Language Models in Long-context Scenarios](https://doi.org/10.18653/v1/2025.naacl-long.439) |  | 0 | Large Language Models (LLMs), exemplified by Claude and LLama, have exhibited impressive proficiency in tackling a myriad of Natural Language Processing (NLP) tasks. Yet, in pursuit of the ambitious goal of attaining Artificial General Intelligence (AGI), there remains ample room for enhancing LLM capabilities. Chief among these is the pressing need to bolster long-context comprehension. Numerous real-world scenarios demand LLMs to adeptly reason across extended contexts, such as multi-turn... | Pei Chen, Hongye Jin, ChengChe Lee, Rulin Shao, Jingfeng Yang, Mingyu Zhao, Zhaoyu Zhang, Qin Lu, Kaiwen Men, Ning Xie, Huasheng Li, Bing Yin, Han Li, Lingyun Wang |  |
| 521 |  |  [Language Models Can Infer Action Semantics for Symbolic Planners from Environment Feedback](https://doi.org/10.18653/v1/2025.naacl-long.440) |  | 0 | Symbolic planners can discover a sequence of actions from initial to goal states given expert-defined, domain-specific logical action semantics. Large Language Models (LLMs) can directly generate such sequences, but limitations in reasoning and state-tracking often result in plans that are insufficient or unexecutable. We propose Predicting Semantics of Actions with Language Models (PSALM), which automatically learns action semantics by leveraging the strengths of both symbolic planners and... | Wang Bill Zhu, Ishika Singh, Robin Jia, Jesse Thomason |  |
| 522 |  |  [SLM-Mod: Small Language Models Surpass LLMs at Content Moderation](https://doi.org/10.18653/v1/2025.naacl-long.441) |  | 0 | Large language models (LLMs) have shown promise in many natural language understanding tasks, including content moderation. However, these models can be expensive to query in real-time and do not allow for a community-specific approach to content moderation. To address these challenges, we explore the use of open-source small language models (SLMs) for community-specific content moderation tasks. We fine-tune and evaluate SLMs (less than 15B parameters) by comparing their performance against... | Xianyang Zhan, Agam Goyal, Yilun Chen, Eshwar Chandrasekharan, Koustuv Saha |  |
| 523 |  |  [On Positional Bias of Faithfulness for Long-form Summarization](https://doi.org/10.18653/v1/2025.naacl-long.442) |  | 0 | Large Language Models (LLMs) often exhibit positional bias in long-context settings, under-attending to information in the middle of inputs. We investigate the presence of this bias in long-form summarization, its impact on faithfulness, and various techniques to mitigate this bias. To consistently evaluate faithfulness, we first compile a benchmark of eight human-annotated long-form summarization datasets and perform a meta-evaluation of faithfulness metrics. We show that LLM-based... | David Wan, Jesse Vig, Mohit Bansal, Shafiq Joty |  |
| 524 |  |  [BPO: Towards Balanced Preference Optimization between Knowledge Breadth and Depth in Alignment](https://doi.org/10.18653/v1/2025.naacl-long.443) |  | 0 | Reinforcement Learning with Human Feedback (RLHF) is the key to the success of large language models (LLMs) in recent years. In this work, we first introduce the concepts of knowledge breadth and knowledge depth, which measure the comprehensiveness and depth of an LLM or knowledge source respectively. We reveal that the imbalance in the number of prompts and responses can lead to a potential disparity in breadth and depth learning within alignment tuning datasets by showing that even a simple... | Sizhe Wang, Yongqi Tong, Hengyuan Zhang, Dawei Li, Xin Zhang, Tianlong Chen |  |
| 525 |  |  [UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.444) |  | 0 | Mitigating the retention of sensitive or private information in large language models is essential for enhancing privacy and safety. Existing unlearning methods, like Gradient Ascent and Negative Preference Optimization, directly tune models to remove unwanted information. However, these methods often become unstable because they fine-tune by maximizing loss, which is the opposite of traditional loss minimization in learning. This reversal creates instability, especially on larger datasets, as... | Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramón Huerta, Ivan Vulic |  |
| 526 |  |  [H-STAR: LLM-driven Hybrid SQL-Text Adaptive Reasoning on Tables](https://doi.org/10.18653/v1/2025.naacl-long.445) |  | 0 | Tabular reasoning involves interpreting natural language queries about tabular data, which presents a unique challenge of combining language understanding with structured data analysis. Existing methods employ either textual reasoning, which excels in semantic interpretation but struggles with mathematical operations, or symbolic reasoning, which handles computations well but lacks semantic understanding. This paper introduces a novel algorithm H-STAR that integrates both symbolic and semantic... | Nikhil Abhyankar, Vivek Gupta, Dan Roth, Chandan K. Reddy |  |
| 527 |  |  [Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations](https://doi.org/10.18653/v1/2025.naacl-long.446) |  | 0 | The growing popularity of large language models has raised concerns regarding the potential to misuse AI-generated text (AIGT). It becomes increasingly critical to establish an excellent AIGT detection method with high generalization and robustness.While, existing methods either focus on model generalization or concentrate on robustness.The unified mechanism, to simultaneously address the challenges of generalization and robustness, is less explored. In this paper, we first empirically reveal... | Yinghan Zhou, Juan Wen, Wanli Peng, Yiming Xue, Ziwei Zhang, Zhengxian Wu |  |
| 528 |  |  [Vision-Language Models Can Self-Improve Reasoning via Reflection](https://doi.org/10.18653/v1/2025.naacl-long.447) |  | 0 | Chain-of-thought (CoT) has proven to improve the reasoning capability of large language models (LLMs). However, due to the complexity of multimodal scenarios and the difficulty in collecting high-quality CoT data, CoT reasoning in multimodal LLMs has been largely overlooked. To this end, we propose a simple yet effective self-training framework, R3V, which iteratively enhances the model’s Vision-language Reasoning by Reflecting on CoT Rationales. Our framework consists of two interleaved parts:... | Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou, Yang Liu |  |
| 529 |  |  [Emergence of Episodic Memory in Transformers: Characterizing Changes in Temporal Structure of Attention Scores During Training](https://doi.org/10.18653/v1/2025.naacl-long.448) |  | 0 | We investigate in-context temporal biases in attention heads and transformer outputs. Using cognitive science methodologies, we analyze attention scores and outputs of the GPT-2 models of varying sizes. Across attention heads, we observe effects characteristic of human episodic memory, including temporal contiguity, primacy and recency. Transformer outputs demonstrate a tendency toward in-context serial recall. Importantly, this effect is eliminated after the ablation of the induction heads,... | Deven Mahesh Mistry, Anooshka Bajaj, Yash Aggarwal, Sahaj Singh Maini, Zoran Tiganj |  |
| 530 |  |  [Knowledge Graph-Guided Retrieval Augmented Generation](https://doi.org/10.18653/v1/2025.naacl-long.449) |  | 0 | Retrieval-augmented generation (RAG) has emerged as a promising technology for addressing hallucination issues in the responses generated by large language models (LLMs). Existing studies on RAG primarily focus on applying semantic-based approaches to retrieve isolated relevant chunks, which ignore their intrinsic relationships. In this paper, we propose a novel Knowledge Graph-Guided Retrieval Augmented Generation (KG2RAG) framework that utilizes knowledge graphs (KGs) to provide fact-level... | Xiangrong Zhu, Yuexiang Xie, Yi Liu, Yaliang Li, Wei Hu |  |
| 531 |  |  [Amphista: Bi-directional Multi-head Decoding for Accelerating LLM Inference](https://doi.org/10.18653/v1/2025.naacl-long.450) |  | 0 | Large Language Models (LLMs) inherently use autoregressive decoding, which lacks parallelism in inference and results in significantly slow inference speed. While methods such as Medusa constructs parallelized heads, they lack adequate information interaction across different prediction positions. To overcome this limitation, we introduce Amphista, an enhanced speculative decoding framework that builds upon Medusa. Specifically, Amphista models an \*Auto-embedding Block\* capable of parallel... | Zeping Li, Xinlong Yang, Ziheng Gao, Ji Liu, Guanchen Li, Zhuang Liu, Dong Li, Jinzhang Peng, Lu Tian, Emad Barsoum |  |
| 532 |  |  [CAVE: Controllable Authorship Verification Explanations](https://doi.org/10.18653/v1/2025.naacl-long.451) |  | 0 | Authorship Verification (AV) (do two documents have the same author?) is essential in many real-life applications. AV is often used in privacy-sensitive domains that require an offline proprietary model that is deployed on premises, making publicly served online models (APIs) a suboptimal choice. Current offline AV models however have lower downstream utility due to limited accuracy (eg: traditional stylometry AV systems) and lack of accessible post-hoc explanations. In this work, we address... | Sahana Ramnath, Kartik Pandey, Elizabeth Boschee, Xiang Ren |  |
| 533 |  |  [Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.452) |  | 0 | In line with the principle of honesty, there has been a growing effort to train large language models (LLMs) to generate outputs containing epistemic markers. However, evaluation in the presence of epistemic markers has been largely overlooked, raising a critical question: Could the use of epistemic markers in LLM-generated outputs lead to unintended negative consequences? To address this, we present EMBER, a benchmark designed to assess the robustness of LLM-judges to epistemic markers in both... | Dongryeol Lee, Yerin Hwang, Yongil Kim, Joonsuk Park, Kyomin Jung |  |
| 534 |  |  [Dynamic Uncertainty Ranking: Enhancing Retrieval-Augmented In-Context Learning for Long-Tail Knowledge in LLMs](https://doi.org/10.18653/v1/2025.naacl-long.453) |  | 0 | Large language models (LLMs) can learn vast amounts of knowledge from diverse domains during pre-training. However, long-tail knowledge from specialized domains is often scarce and underrepresented, rarely appearing in the models’ memorization. Prior work has shown that in-context learning (ICL) with retriever augmentation can help LLMs better capture long-tail knowledge, reducing their reliance on pre-trained data. Despite these advances, we observe that LLM predictions for long-tail questions... | Shuyang Yu, Runxue Bao, Parminder Bhatia, Taha A. KassHout, Jiayu Zhou, Cao Xiao |  |
| 535 |  |  [Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training](https://doi.org/10.18653/v1/2025.naacl-long.454) |  | 0 | Training large language models (LLMs) heavily relies on distributed training strategies, among which pipeline parallelism (PP) plays a crucial role. As training sequences extend to 32k or even 128k tokens, current PP methods face severe bottlenecks, including substantial pipeline bubbles and high memory footprint, greatly hindering training throughput and model scalability. This paper introduces a sequence-level one-forward-one-backward (1F1B) PP method, named Seq1F1B, tailored for training... | Sun Ao, Weilin Zhao, Xu Han, Cheng Yang, Xinrong Zhang, Zhiyuan Liu, Chuan Shi, Maosong Sun |  |
| 536 |  |  [Differentially Private Learning Needs Better Model Initialization and Self-Distillation](https://doi.org/10.18653/v1/2025.naacl-long.455) |  | 0 | Differentially private SGD (DPSGD) enables privacy-preserving training of language models, but often reduces utility, diversity, and linguistic quality. We introduce DPRefine, a three-phase method that initializes a model using data synthesis from a small pre-trained LM with rigorous filtering, applies DP finetuning on private data, and performs self-distillation to refine outputs. This approach significantly outperforms vanilla DPSGD, with AlpacaEval preferring DPRefine’s generations in 78.38%... | Ivoline C. Ngong, Joseph P. Near, Niloofar Mireshghallah |  |
| 537 |  |  [Is a Peeled Apple Still Red? Evaluating LLMs' Ability for Conceptual Combination with Property Type](https://doi.org/10.18653/v1/2025.naacl-long.456) |  | 0 | Conceptual combination is a cognitive process that merges basic concepts, enabling the creation of complex expressions. During this process, the properties of combination (e.g., the whiteness of a peeled apple) can be inherited from basic concepts, newly emerge, or be canceled. However, previous studies have evaluated a limited set of properties and have not examined the generative process.To address this gap, we introduce the Conceptual Combination with Property Type dataset (CCPT), which... | Seokwon Song, Taehyun Lee, Jaewoo Ahn, Jae Hyuk Sung, Gunhee Kim |  |
| 538 |  |  [CRScore: Grounding Automated Evaluation of Code Review Comments in Code Claims and Smells](https://doi.org/10.18653/v1/2025.naacl-long.457) |  | 0 | The task of automated code review has recently gained a lot of attention from the machine learning community. However, current review comment evaluation metrics rely on comparisons with a human-written reference for a given code change (also called a diff ). Furthermore, code review is a one-to-many problem, like generation and summarization, with many “valid reviews” for a diff. Thus, we develop CRScore — a reference-free metric to measure dimensions of review quality like conciseness,... | Atharva Naik, Marcus Alenius, Daniel Fried, Carolyn P. Rosé |  |
| 539 |  |  [KS-Lottery: Finding Certified Lottery Tickets for Multilingual Transfer in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.458) |  | 0 | The lottery ticket hypothesis posits the existence of “winning tickets” within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further... | Fei Yuan, Chang Ma, Shuai Yuan, Qiushi Sun, Lei Li |  |
| 540 |  |  [PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization](https://doi.org/10.18653/v1/2025.naacl-long.459) |  | 0 | The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing... | Jiayi Wu, Hengyi Cai, Lingyong Yan, Hao Sun, Xiang Li, Shuaiqiang Wang, Dawei Yin, Ming Gao |  |
| 541 |  |  [B⁴: A Black-Box Scrubbing Attack on LLM Watermarks](https://doi.org/10.18653/v1/2025.naacl-long.460) |  | 0 | Watermarking has emerged as a prominent technique for LLM-generated content detection by embedding imperceptible patterns. Despite supreme performance, its robustness against adversarial attacks remains underexplored. Previous work typically considers a grey-box attack setting, where the specific type of watermark is already known. Some even necessitates knowledge about hyperparameters of the watermarking method. Such prerequisites are unattainable in real-world scenarios. Targeting at a more... | Baizhou Huang, Xiao Pu, Xiaojun Wan |  |
| 542 |  |  [IMRRF: Integrating Multi-Source Retrieval and Redundancy Filtering for LLM-based Fake News Detection](https://doi.org/10.18653/v1/2025.naacl-long.461) |  | 0 | The widespread use of social networks has significantly accelerated the dissemination of information but has also facilitated the rapid spread of fake news, leading to various negative consequences. Recently, with the emergence of large language models (LLMs), researchers have focused on leveraging LLMs for automated fake news detection. Unfortunately, many issues remain to be addressed. First, the evidence retrieved to verify given fake news is often insufficient, limiting the performance of... | Dayang Li, Fanxiao Li, Bingbing Song, Li Tang, Wei Zhou |  |
| 543 |  |  [Matina: A Large-Scale 73B Token Persian Text Corpus](https://doi.org/10.18653/v1/2025.naacl-long.462) |  | 0 | Text corpora are essential for training models used in tasks like summarization, translation, and large language models (LLMs). While various efforts have been made to collect monolingual and multilingual datasets in many languages, Persian has often been underrepresented due to limited resources for data collection and preprocessing. Existing Persian datasets are typically small and lack content diversity, consisting mainly of weblogs and news articles. This shortage of high-quality, varied... | Sara Bourbour Hosseinbeigi, Fatemeh Taherinezhad, Heshaam Faili, Hamed Baghbani, Fatemeh Nadi, Mostafa Amiri |  |
| 544 |  |  [SMAB: MAB based word Sensitivity Estimation Framework and its Applications in Adversarial Text Generation](https://doi.org/10.18653/v1/2025.naacl-long.463) |  | 0 | To understand the complexity of sequence classification tasks, Hahn et al. (2021) proposed sensitivity as the number of disjoint subsets of the input sequence that can each be individually changed to change the output. Though effective, calculating sensitivity at scale using this framework is costly because of exponential time complexity. Therefore, we introduce a Sensitivity-based Multi-Armed Bandit framework (SMAB), which provides a scalable approach for calculating word-level local... | Saurabh Kumar Pandey, Sachin Vashistha, Debrup Das, Somak Aditya, Monojit Choudhury |  |
| 545 |  |  [ManaTTS Persian: a recipe for creating TTS datasets for lower resource languages](https://doi.org/10.18653/v1/2025.naacl-long.464) |  | 0 | In this study, we introduce ManaTTS, the most extensive publicly accessible single-speaker Persian corpus, and a comprehensive framework for collecting transcribed speech datasets for the Persian language. ManaTTS, released under the open CC-0 license, comprises approximately 86 hours of audio with a sampling rate of 44.1 kHz. The dataset is supported by a fully transparent, MIT-licensed pipeline, a testament to innovation in the field. It includes unique tools for sentence tokenization,... | Mahta Fetrat Qharabagh, Zahra Dehghanian, Hamid R. Rabiee |  |
| 546 |  |  [CultureInstruct: Curating Multi-Cultural Instructions at Scale](https://doi.org/10.18653/v1/2025.naacl-long.465) |  | 0 | Large language models, despite their remarkable success in recent years, still exhibit severe cultural bias. Therefore, in this paper, we introduce CultureInstruct, a large-scale instruction-tuning dataset designed to reduce cultural bias in LLMs. CultureInstruct is constructed with an automatic pipeline, utilizing public web sources and a specialized LLM to generate instruction. Our data comprises 430K instructions, ranging from classic NLP tasks to complex reasoning. CultureInstruct also... | Viet Thanh Pham, Zhuang Li, Lizhen Qu, Gholamreza Haffari |  |
| 547 |  |  [Lost in Inference: Rediscovering the Role of Natural Language Inference for Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.466) |  | 0 | In the recent past, a popular way of evaluating natural language understanding (NLU), was to consider a model’s ability to perform natural language inference (NLI) tasks. In this paper, we investigate if NLI tasks, that are rarely used for LLM evaluation, can still be informative for evaluating LLMs. Focusing on five different NLI benchmarks across six models of different scales, we investigate if they are able to discriminate models of different size and quality and how their accuracies... | Lovish Madaan, David Esiobu, Pontus Stenetorp, Barbara Plank, Dieuwke Hupkes |  |
| 548 |  |  [DenseSSM: State Space Models with Dense Hidden Connection for Efficient Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.467) |  | 0 | Large language models (LLMs) face a significant challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating... | Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang |  |
| 549 |  |  [A Mixed-Language Multi-Document News Summarization Dataset and a Graphs-Based Extract-Generate Model](https://doi.org/10.18653/v1/2025.naacl-long.468) |  | 0 | Existing research on news summarization primarily focuses on single-language single-document (SLSD), single-language multi-document (SLMD) or cross-language single-document (CLSD). However, in real-world scenarios, news about an international event often involves multiple documents in different languages, i.e., mixed-language multi-document (MLMD). Therefore, summarizing MLMD news is of great significance. However, the lack of datasets for MLMD news summarization has constrained the development... | Shengxiang Gao, Fang Nan, Yongbing Zhang, Yuxin Huang, Kaiwen Tan, Zhengtao Yu |  |
| 550 |  |  [Measuring memorization in language models via probabilistic extraction](https://doi.org/10.18653/v1/2025.naacl-long.469) |  | 0 | Large language models (LLMs) are susceptible to memorizing training data, raising concerns about the potential extraction of sensitive information at generation time. Discoverable extraction is the most common method for measuring this issue: split a training example into a prefix and suffix, then prompt the LLM with the prefix, and deem the example extractable if the LLM generates the matching suffix using greedy sampling. This definition yields a yes-or-no determination of whether extraction... | Jamie Hayes, Marika Swanberg, Harsh Chaudhari, Itay Yona, Ilia Shumailov, Milad Nasr, Christopher A. ChoquetteChoo, Katherine Lee, A. Feder Cooper |  |
| 551 |  |  [Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models](https://doi.org/10.18653/v1/2025.naacl-long.470) |  | 0 | Large Multimodal Models (LMMs) have demonstrated the ability to interact with humans under real-world conditions by combining Large Language Models (LLMs) and modality encoders to align multimodal information (visual and auditory) with text. However, such models raise new safety challenges of whether models that are safety-aligned on text also exhibit consistent safeguards for multimodal inputs. Despite recent safety-alignment research on vision LMMs, the safety of audio LMMs remains... | Hao Yang, Lizhen Qu, Ehsan Shareghi, Gholamreza Haffari |  |
| 552 |  |  [EMS-SD: Efficient Multi-sample Speculative Decoding for Accelerating Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.471) |  | 0 | Speculative decoding emerges as a pivotal technique for enhancing the inference speed of Large Language Models (LLMs). Despite recent research aiming to improve prediction efficiency, multi-sample speculative decoding has been overlooked due to varying numbers of accepted tokens within a batch in the verification phase. Vanilla method adds padding tokens in order to ensure that the number of new tokens remains consistent across samples. However, this increases the computational and memory... | Yunsheng Ni, Chuanjian Liu, Yehui Tang, Kai Han, Yunhe Wang |  |
| 553 |  |  [Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for Language Model Alignment](https://doi.org/10.18653/v1/2025.naacl-long.472) |  | 0 | Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking when the accuracy of the reward model is not high enough. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in... | Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, Kenshi Abe |  |
| 554 |  |  [MAPWise: Evaluating Vision-Language Models for Advanced Map Queries](https://doi.org/10.18653/v1/2025.naacl-long.473) |  | 0 | Vision-language models (VLMs) excel at tasks requiring joint understanding of visual and linguistic information. A particularly promising yet under-explored application for these models lies in answering questions based on various kinds of maps. This study investigates the efficacy of VLMs in answering questions based on choropleth maps, which are widely used for data analysis and representation. To facilitate and encourage research in this area, we introduce a novel map-based... | Srija Mukhopadhyay, Abhishek Rajgaria, Prerana Khatiwada, Manish Shrivastava, Dan Roth, Vivek Gupta |  |
| 555 |  |  [Pay More Attention to Images: Numerous Images-Oriented Multimodal Summarization](https://doi.org/10.18653/v1/2025.naacl-long.474) |  | 0 | Existing multimodal summarization approaches struggle with scenarios involving numerous images as input, leading to a heavy load for readers. Summarizing both the input text and numerous images helps readers quickly grasp the key points of multimodal input. This paper introduces a novel task, Numerous Images-Oriented Multimodal Summarization (NIMMS). To benchmark this task, we first construct the dataset based on a public multimodal summarization dataset. Considering that most existing metrics... | Min Xiao, Junnan Zhu, Feifei Zhai, Chengqing Zong, Yu Zhou |  |
| 556 |  |  [S²-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate Efficiency](https://doi.org/10.18653/v1/2025.naacl-long.475) |  | 0 | Large language models (LLMs) have demonstrated remarkable capabilities across various natural language processing (NLP) scenarios, but they still face challenges when handling complex arithmetic and logical reasoning tasks. While Chain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction strategies have attempted to guide models in sequential, multi-step reasoning, Multi-agent Debate (MAD) has emerged as a viable approach for enhancing the reasoning capabilities of LLMs. By... | Yuting Zeng, Weizhe Huang, Lei Jiang, Tongxuan Liu, Xitai Jin, Chen Tianying Tiana, Jing Li, Xiaohua Xu |  |
| 557 |  |  [MASTER: A Multi-Agent System with LLM Specialized MCTS](https://doi.org/10.18653/v1/2025.naacl-long.476) |  | 0 | Large Language Models (LLM) are increasingly being explored for problem-solving tasks. However, their strategic planning capability is often viewed with skepticism. Recent studies have incorporated the Monte Carlo Tree Search (MCTS) algorithm to augment the planning capacity of LLM. Despite its potential, MCTS relies on extensive sampling simulations to approximate the true reward distribution, which leads to two primary issues. Firstly, MCTS is effective for tasks like the Game of Go, where... | Bingzheng Gan, Yufan Zhao, Tianyi Zhang, Jing Huang, Yusu Li, Shu Xian Teo, Changwang Zhang, Wei Shi |  |
| 558 |  |  [ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots](https://doi.org/10.18653/v1/2025.naacl-long.477) |  | 0 | We introduce ScreenQA, a novel benchmarking dataset designed to advance screen content understanding through question answering. The existing screen datasets are focused either on low-level structural and component understanding, or on a much higher-level composite task such as navigation and task completion for autonomous agents. ScreenQA attempts to bridge this gap. By annotating 86k question-answer pairs over the RICO dataset, we aim to benchmark the screen reading comprehension capacity,... | YuChung Hsiao, Fedir Zubach, Gilles Baechler, Srinivas Sunkara, Victor Carbune, Jason Lin, Maria Wang, Yun Zhu, Jindong Chen |  |
| 559 |  |  [Cross-Lingual and Cross-Cultural Variation in Image Descriptions](https://doi.org/10.18653/v1/2025.naacl-long.478) |  | 0 | Do speakers of different languages talk differently about what they see? Behavioural and cognitive studies report cultural effects on perception; however, these are mostly limited in scope and hard to replicate. In this work, we conduct the first large-scale empirical study of cross-lingual variation in image descriptions. Using a multimodal dataset with 31 languages and images from diverse locations, we develop a method to accurately identify entities mentioned in captions and present in the... | Uri Berger, Edoardo M. Ponti |  |
| 560 |  |  [Soft Syntactic Reinforcement for Neural Event Extraction](https://doi.org/10.18653/v1/2025.naacl-long.479) |  | 0 | Recent event extraction (EE) methods rely on pre-trained language models (PLMs) but still suffer from errors due to a lack of syntactic knowledge. While syntactic information is crucial for EE, there is a need for effective methods to incorporate syntactic knowledge into PLMs. To address this gap, we present a novel method to incorporate syntactic information into PLM-based models for EE, which do not require external syntactic parsers to produce syntactic features of task data. Instead, our... | Anran Hao, Jian Su, Shuo Sun, Teo Yong Sen |  |
| 561 |  |  [Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models](https://doi.org/10.18653/v1/2025.naacl-long.480) |  | 0 | Transformer-based large-scale pre-trained models achieve great success. Fine-tuning is the standard practice for leveraging these models in downstream tasks. Among the fine-tuning methods, adapter-tuning provides a parameter-efficient fine-tuning by introducing lightweight trainable modules while keeping most pre-trained parameters frozen. However, existing adapter-tuning methods still impose substantial resource usage. Through our investigation, we show that each adapter unequally contributes... | Hyegang Son, Yonglak Son, Changhoon Kim, Young Geun Kim |  |
| 562 |  |  [Bridging the Gap between Expert and Language Models: Concept-guided Chess Commentary Generation and Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.481) |  | 0 | Deep learning-based expert models have reached superhuman performance in decision-making domains such as chess and Go. However, it is under-explored to explain or comment on given decisions although it is important for model explainability and human education. The outputs of expert models are accurate, but yet difficult to interpret for humans. On the other hand, large language models (LLMs) can produce fluent commentary but are prone to hallucinations due to their limited decision-making... | Jaechang Kim, Jinmin Goh, Inseok Hwang, Jaewoong Cho, Jungseul Ok |  |
| 563 |  |  [TCProF:Time-Complexity Prediction SSL Framework](https://doi.org/10.18653/v1/2025.naacl-long.482) |  | 0 |  | Joonghyuk Hahn, Hyeseon Ahn, Jungin Kim, Soohan Lim, YoSub Han |  |
| 564 |  |  [Culture-TRIP: Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinement](https://doi.org/10.18653/v1/2025.naacl-long.483) |  | 0 |  | Suchae Jeong, Inseong Choi, Youngsik Yun, Jihie Kim |  |
| 565 |  |  [Behavior-SD: Behaviorally Aware Spoken Dialogue Generation with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.484) |  | 0 | Spoken dialogue involves behaviors like turn-taking, interruptions, filler words, and backchannels, which make interactions more natural and engaging but are often overlooked in language models. These models struggle to explicitly model these behavioral traits, resulting in a less natural and personalized communication style that aligns with user needs. To address this challenge, we make two key contributions. First, we introduce Behavior-SD, a large-scale dataset containing over 100K spoken... | Sehun Lee, Kangwook Kim, Gunhee Kim |  |
| 566 |  |  [Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.485) |  | 0 | Large language models (LLMs) have demonstrated multilingual capabilities, yet they are mostly English-centric due to the imbalanced training corpora. While prior works have leveraged this bias to enhance multilingual performance through translation, they have been largely limited to natural language processing (NLP) tasks. In this work, we extend the evaluation to real-world user queries and non-English-centric LLMs, offering a broader examination of multilingual performance. Our key... | Chaoqun Liu, Wenxuan Zhang, Yiran Zhao, Anh Tuan Luu, Lidong Bing |  |
| 567 |  |  [AlgoPuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Algorithmic Multimodal Puzzles](https://doi.org/10.18653/v1/2025.naacl-long.486) |  | 0 | This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models in solving algorithmic puzzles that necessitate both visual understanding, language understanding, and complex algorithmic reasoning. We create the puzzles to encompass a diverse array of mathematical and algorithmic topics such as boolean logic,... | Deepanway Ghosal, Vernon Toh, Yew Ken Chia, Soujanya Poria |  |
| 568 |  |  [Towards Quantifying Commonsense Reasoning with Mechanistic Insights](https://doi.org/10.18653/v1/2025.naacl-long.487) |  | 0 |  | Abhinav Joshi, Areeb Ahmad, Divyaksh Shukla, Ashutosh Modi |  |
| 569 |  |  [Beyond Logit Lens: Contextual Embeddings for Robust Hallucination Detection & Grounding in VLMs](https://doi.org/10.18653/v1/2025.naacl-long.488) |  | 0 | The rapid development of Large Multimodal Models (LMMs) has significantly advanced multimodal understanding by harnessing the language abilities of Large Language Models (LLMs) and integrating modality-specific encoders. However, LMMs are plagued by hallucinations that limit their reliability and adoption. While traditional methods to detect and mitigate these hallucinations often involve costly training or rely heavily on external models, recent approaches utilizing internal model features... | Anirudh Phukan, Divyansh, Harshit Kumar Morj, Vaishnavi, Apoorv Saxena, Koustava Goswami |  |
| 570 |  |  [M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.489) |  | 0 | Collecting instruction fine-tuning (IFT) data is a resource and time intensive task especially in multilingual setting where finding proficient native speakers is challenging. Moreover, traditional data collection is prone to privacy risks, toxicity and lacks scalability. While, fully synthetic datasets are a promising alternative, research on their use in multilingual domain is limited as existing approaches still rely on machine translation to improve multilingual performance. To bridge this... | Rishabh Maheshwary, Vikas Yadav, Hoang Nguyen, Khyati Mahajan, Sathwik Tejaswi Madhusudhan |  |
| 571 |  |  [Multi³Hate: Multimodal, Multilingual, and Multicultural Hate Speech Detection with Vision-Language Models](https://doi.org/10.18653/v1/2025.naacl-long.490) |  | 0 | Hate speech moderation on global platforms poses unique challenges due to the multimodal and multilingual nature of content, along with the varying cultural perceptions. How well do current vision-language models (VLMs) navigate these nuances? To investigate this, we create the first multimodal and multilingual parallel hate speech dataset, annotated by a multiculturally diverse set of annotators, called Multi3Hate. It contains 300 parallel meme samples across 5 languages: English, German,... | Minh Duc Bui, Katharina von der Wense, Anne Lauscher |  |
| 572 |  |  [Grounding Fallacies Misrepresenting Scientific Publications in Evidence](https://doi.org/10.18653/v1/2025.naacl-long.491) |  | 0 | Health-related misinformation claims often falsely cite a credible biomedical publication as evidence. These publications only superficially seem to support the false claim, when logical fallacies are applied. In this work, we aim to detect and to highlight such fallacies, which requires assessing the exact content of the misrepresented publications. To achieve this, we introduce MissciPlus, an extension of the fallacy detection dataset Missci. MissciPlus extends Missci by grounding the applied... | Max Glockner, Yufang Hou, Preslav Nakov, Iryna Gurevych |  |
| 573 |  |  [Has this Fact been Edited? Detecting Knowledge Edits in Language Models](https://doi.org/10.18653/v1/2025.naacl-long.492) |  | 0 | Knowledge editing methods (KEs) can update language models’ obsolete or inaccurate knowledge learned from pre-training. However, KEs can be used for malicious applications, e.g., inserting misinformation and toxic content. Knowing whether a generated output is based on edited knowledge or first-hand knowledge from pre-training can increase users’ trust in generative models and provide more transparency. Driven by this, we propose a novel task: detecting knowledge edits in language models. Given... | Paul Youssef, Zhixue Zhao, Christin Seifert, Jörg Schlötterer |  |
| 574 |  |  [AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging](https://doi.org/10.18653/v1/2025.naacl-long.493) |  | 0 |  | Yiran Zhao, Wenxuan Zhang, Huiming Wang, Kenji Kawaguchi, Lidong Bing |  |
| 575 |  |  [Coverage-based Fairness in Multi-document Summarization](https://doi.org/10.18653/v1/2025.naacl-long.494) |  | 0 | Fairness in multi-document summarization (MDS) measures whether a system can generate a summary fairly representing information from documents with different social attribute values. Fairness in MDS is crucial since a fair summary can offer readers a comprehensive view. Previous works focus on quantifying summary-level fairness using Proportional Representation, a fairness measure based on Statistical Parity. However, Proportional Representation does not consider redundancy in input documents... | Haoyuan Li, Yusen Zhang, Rui Zhang, Snigdha Chaturvedi |  |
| 576 |  |  [Grammar Control in Dialogue Response Generation for Language Learning Chatbots](https://doi.org/10.18653/v1/2025.naacl-long.495) |  | 0 | Chatbots based on large language models offer cheap conversation practice opportunities for language learners. However, they are hard to control for linguistic forms that correspond to learners’ current needs, such as grammar. We control grammar in chatbot conversation practice by grounding a dialogue response generation model in a pedagogical repository of grammar skills. We also explore how this control helps learners to produce specific grammar. We comprehensively evaluate prompting,... | Dominik Glandorf, Peng Cui, Detmar Meurers, Mrinmaya Sachan |  |
| 577 |  |  [Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge](https://doi.org/10.18653/v1/2025.naacl-long.496) |  | 0 | Recent studies have highlighted the presence of cultural biases in Large Language Models (LLMs), yet often lack a robust methodology to dissect these phenomena comprehensively. Our work aims to bridge this gap by delving into the Food domain—a universally relevant yet culturally diverse aspect of human life. We introduce FmLAMA, a multilingual dataset centered on food-related cultural facts and variations in food practices. We analyze LLMs across various architectures and configurations,... | Li Zhou, Taelin Karidi, Wanlong Liu, Nicolas Garneau, Yong Cao, Wenyu Chen, Haizhou Li, Daniel Hershcovich |  |
| 578 |  |  [Palette of Language Models: A Solver for Controlled Text Generation](https://doi.org/10.18653/v1/2025.naacl-long.497) |  | 0 | Recent advancements in large language models have revolutionized text generation with their remarkable capabilities. These models can produce controlled texts that closely adhere to specific requirements when prompted appropriately. However, designing an optimal prompt to control multiple attributes simultaneously can be challenging. A common approach is to linearly combine single-attribute models, but this strategy often overlooks attribute overlaps and can lead to conflicts. Therefore, we... | Zhe Yang, Yi Huang, Yaqin Chen, XiaotingWu XiaotingWu, Junlan Feng, Chao Deng |  |
| 579 |  |  [MAMM-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration](https://doi.org/10.18653/v1/2025.naacl-long.498) |  | 0 | Multi-agent collaboration among models has shown promise in reasoning tasks but is underexplored in long-form generation tasks like summarization and question-answering. We extend multi-agent multi-model reasoning to generation, specifically to improving faithfulness through refinement, i.e., revising model-generated outputs to remove factual inconsistencies. We investigate how iterative collaboration among multiple instances and types of large language models (LLMs) enhances subtasks in the... | David Wan, Justin ChihYao Chen, Elias StengelEskin, Mohit Bansal |  |
| 580 |  |  [MADial-Bench: Towards Real-world Evaluation of Memory-Augmented Dialogue Generation](https://doi.org/10.18653/v1/2025.naacl-long.499) |  | 0 | Long-term memory is important for chatbots and dialogue systems (DS) to create consistent and human-like conversations, evidenced by numerous developed memory-augmented DS (MADS). To evaluate the effectiveness of such MADS, existing commonly used evaluation metrics, like retrieval accuracy and perplexity (PPL), mainly focus on query-oriented factualness and language quality assessment. However, these metrics often lack practical value. Moreover, the evaluation dimensions are insufficient for... | Junqing He, Liang Zhu, Rui Wang, Xi Wang, Gholamreza Haffari, Jiaxing Zhang |  |
| 581 |  |  [Assessing the State of the Art in Scene Segmentation](https://doi.org/10.18653/v1/2025.naacl-long.500) |  | 0 | The detection of scenes in literary texts is a recently introduced segmentation task in computational literary studies. Its goal is to partition a fictional text into segments that are coherent across the dimensions time, space, action and character constellation. This task is very challenging for automatic methods, since it requires a high-level understanding of the text. In this paper, we provide a thorough analysis of the State of the Art and challenges in this task, identifying and solving... | Albin Zehe, Elisabeth Fischer, Andreas Hotho |  |
| 582 |  |  [DCE-LLM: Dead Code Elimination with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.501) |  | 0 | Dead code introduces several challenges in software development, such as increased binary size and maintenance difficulties. It can also obscure logical errors and be exploited for obfuscation in malware. For LLM-based code-related tasks, dead code introduces vulnerabilities that can mislead these models, raising security concerns. Although modern compilers and IDEs offer dead code elimination, sophisticated patterns can bypass these tools. A universal approach that includes classification,... | Minyu Chen, Guoqiang Li, LingI Wu, Ruibang Liu |  |
| 583 |  |  [Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction](https://doi.org/10.18653/v1/2025.naacl-long.502) |  | 0 | Self-reflection for Large LanguageModels (LLMs) has gained significant attention. Existing approaches involve models iterating and improving their previous responses based on LLMs’ internal reflection ability or external feedback. However, recent research has raised doubts about whether intrinsic self-correction without external feedback may even degrade performance. Based on our empirical evidence, we find that current static reflection methods may lead to redundant, drift, and stubborn... | Liping Liu, Chunhong Zhang, Likang Wu, Chuang Zhao, Zheng Hu, Ming He, Jianping Fan |  |
| 584 |  |  [Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment](https://doi.org/10.18653/v1/2025.naacl-long.503) |  | 0 | A binary decision task, like yes-no questions or answer verification, reflects a significant real-world scenario such as where users look for confirmation about the correctness of their decisions on specific issues. In this work, we observe that language models exhibit a negative bias in the binary decisions of complex reasoning tasks. Based on our observations and the rationale about attention-based model dynamics, we propose a negative attention score (NAS) to systematically and... | Sangwon Yu, Jongyoon Song, Bongkyu Hwang, Hoyoung Kang, Sooah Cho, Junhwa Choi, Seongho Joe, Taehee Lee, Youngjune Gwon, Sungroh Yoon |  |
| 585 |  |  [MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps](https://doi.org/10.18653/v1/2025.naacl-long.504) |  | 0 | \*\*Multimodal Chain of Thought (MCoT)\*\* is a popular prompting strategy for improving the performance of multimodal large language models (MLLMs) across a range of complex reasoning tasks. Despite its popularity, there is a notable absence of automated methods for evaluating the quality of reasoning steps in MCoT. To address this gap, we propose \*\*Multimodal Chain-of-Thought Evaluation (MiCEval)\*\*, a framework designed to assess the correctness of reasoning chains by evaluating the... | Xiongtao Zhou, Jie He, Lanyu Chen, Jingyu Li, Haojing Chen, Víctor GutiérrezBasulto, Jeff Z. Pan, Hanjie Chen |  |
| 586 |  |  [CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian Product Routing in Mixture-of-Experts](https://doi.org/10.18653/v1/2025.naacl-long.505) |  | 0 | Large language models (LLM) have been attracting much attention from the community recently, due to their remarkable performance in all kinds of downstream tasks. According to the well-known scaling law, scaling up a dense LLM enhances its capabilities, but also significantly increases the computational complexity. Mixture-of-Experts (MoE) models address that by allowing the model size to grow without substantially raising training or inference costs. Yet MoE models face challenges regarding... | Zhenpeng Su, Xing Wu, Zijia Lin, Yizhe Xiong, Minxuan Lv, Guangyuan Ma, Hui Chen, Songlin Hu, Guiguang Ding |  |
| 587 |  |  [Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language](https://doi.org/10.18653/v1/2025.naacl-long.506) |  | 0 | We are exposed to much information trying to influence us, such as teaser messages, debates, politically framed news, and propaganda — all of which use persuasive language. With the recent interest in Large Language Models (LLMs), we study the ability of LLMs to produce persuasive text. As opposed to prior work which focuses on particular domains or types of persuasion, we conduct a general study across various domains to measure and benchmark to what degree LLMs produce persuasive language -... | Amalie Brogaard Pauli, Isabelle Augenstein, Ira Assent |  |
| 588 |  |  [MILU: A Multi-task Indic Language Understanding Benchmark](https://doi.org/10.18653/v1/2025.naacl-long.507) |  | 0 | Evaluating Large Language Models (LLMs) in low-resource and linguistically diverse languages remains a significant challenge in NLP, particularly for languages using non-Latin scripts like those spoken in India. Existing benchmarks predominantly focus on English, leaving substantial gaps in assessing LLM capabilities in these languages. We introduce MILU, a Multi-task Indic Language Understanding Benchmark, a comprehensive evaluation benchmark designed to address this gap. MILU spans 8 domains... | Sshubam Verma, Mohammed Safi Ur Rahman Khan, Vishwajeet Kumar, Rudra Murthy, Jaydeep Sen |  |
| 589 |  |  [AutoEval-ToD: Automated Evaluation of Task-oriented Dialog Systems](https://doi.org/10.18653/v1/2025.naacl-long.508) |  | 0 | Task-oriented Dialog systems (ToD) are essential in automating user interactions, but their complex design and dynamic nature make evaluation particularly challenging. Current evaluation methodologies heavily depend on human annotators, which can be inefficient, subjective, and expensive to scale. To advance the field, there is a pressing need for a reliable, scalable, and systematic evaluation framework that can provide comprehensive insights into ToD system performance. In this paper, we... | Arihant Jain, Purav Aggarwal, Rishav Sahay, Chaosheng Dong, Anoop Saladi |  |
| 590 |  |  [Self-calibration for Language Model Quantization and Pruning](https://doi.org/10.18653/v1/2025.naacl-long.509) |  | 0 | Quantization and pruning are fundamental approaches for model compression, enabling efficient inference for language models. In a post-training setting, state-of-the-art quantization and pruning methods require calibration data, a small set of unlabeled examples. Conventionally, this is randomly sampled web text, aiming to reflect the model training data. However, this poses two key problems: (1) unrepresentative calibration examples can harm model performance, and (2) organizations... | Miles Williams, George Chrysostomou, Nikolaos Aletras |  |
| 591 |  |  [Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.510) |  | 0 | Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can improve the reasoning ability of LLMs to some extent, they suffer from an unfaithful issue where derived conclusions may not align with the generated reasoning chain. To address this issue, some studies employ the approach of propositional logic to further... | Tongxuan Liu, Wenjiang Xu, Weizhe Huang, Yuting Zeng, Jiaxing Wang, Xingyu Wang, Hailong Yang, Jing Li |  |
| 592 |  |  [IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval](https://doi.org/10.18653/v1/2025.naacl-long.511) |  | 0 | We introduce IFIR, the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature. Each subset addresses one or more domain-specific retrieval tasks, replicating real-world scenarios where customized instructions are critical. IFIR enables a detailed analysis of instruction-following... | Tingyu Song, Guo Gan, Mingsheng Shang, Yilun Zhao |  |
| 593 |  |  [QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models](https://doi.org/10.18653/v1/2025.naacl-long.512) |  | 0 | In typical multimodal tasks, such as Visual Question Answering (VQA), adversarial attacks targeting a specific image and question can lead large vision-language models (LVLMs) to provide incorrect answers. However, it is common for a single image to be associated with multiple questions, and LVLMs may still answer other questions correctly even for an adversarial image attacked by a specific question. To address this, we introduce the query-agnostic visual attack (QAVA), which aims to create... | Yudong Zhang, Ruobing Xie, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, Yu Wang |  |
| 594 |  |  [Evaluating and Improving Graph to Text Generation with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.513) |  | 0 | Large language models (LLMs) have demonstrated immense potential across various tasks. However, research for exploring and improving the capabilities of LLMs in interpreting graph structures remains limited. To address this gap, we conduct a comprehensive evaluation of prompting current open-source LLMs on graph-to-text generation tasks. Although we explored the optimal prompting strategies and proposed a novel and effective diversity-difficulty-based few-shot sample selection method, we found... | Jie He, Yijun Yang, Wanqiu Long, Deyi Xiong, Víctor GutiérrezBasulto, Jeff Z. Pan |  |
| 595 |  |  [The Plagiarism Singularity Conjecture](https://doi.org/10.18653/v1/2025.naacl-long.514) |  | 0 |  | Sriram Ranga, Rui Mao, Erik Cambria, Anupam Chattopadhyay |  |
| 596 |  |  [Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.515) |  | 0 | Despite recent advances in large language models, open-source models often struggle to consistently perform well on complex reasoning tasks. Existing ensemble methods, whether applied at the token or output levels, fail to address these challenges. In response, we present Language model Ensemble with Monte Carlo Tree Search (LE-MCTS), a novel framework for process-level ensembling of language models. LE-MCTS formulates step-by-step reasoning with an ensemble of language models as a Markov... | Sungjin Park, Xiao Liu, Yeyun Gong, Edward Choi |  |
| 597 |  |  [One Unified Model for Diverse Tasks: Emotion Cause Analysis via Self-Promote Cognitive Structure Modeling](https://doi.org/10.18653/v1/2025.naacl-long.516) |  | 0 | Emotion cause analysis is a critical topic in natural language processing. Key tasks include emotion cause extraction (ECE), emotion-cause pair extraction (ECPE), social emotion cause identification (SECI) as well as social emotion mining and its cause identification (SEMCI). While current emotion cause analysis methods often focus on task-specific model design, they tend to overlook the underlying common ground across these tasks rooted in cognitive emotion theories, in particular, the... | Zhaoxin Yu, Xinglin Xiao, Wenji Mao |  |
| 598 |  |  [Soft Language Prompts for Language Transfer](https://doi.org/10.18653/v1/2025.naacl-long.517) |  | 0 | Cross-lingual knowledge transfer, especially between high- and low-resource languages, remains challenging in natural language processing (NLP). This study offers insights for improving cross-lingual NLP applications through the combination of parameter-efficient fine-tuning methods. We systematically explore strategies for enhancing cross-lingual transfer through the incorporation of language-specific and task-specific adapters and soft prompts. We present a detailed investigation of various... | Ivan Vykopal, Simon Ostermann, Marián Simko |  |
| 599 |  |  [PICLe: Pseudo-annotations for In-Context Learning in Low-Resource Named Entity Detection](https://doi.org/10.18653/v1/2025.naacl-long.518) |  | 0 | In-context learning (ICL) enables Large Language Models (LLMs) to perform tasks using few demonstrations, facilitating task adaptation when labeled examples are hard to come by. However, ICL is sensitive to the choice of demonstrations, and it remains unclear which demonstration attributes enable in-context generalization. In this work, we conduct a perturbation study of in-context demonstrations for low-resource Named Entity Detection (NED). Our surprising finding is that in-context... | Sepideh Mamooler, Syrielle Montariol, Alexander Mathis, Antoine Bosselut |  |
| 600 |  |  [Can Large Language Models Invent Algorithms to Improve Themselves?](https://doi.org/10.18653/v1/2025.naacl-long.519) |  | 0 | Large Language Models (LLMs) have shown remarkable performance improvements and are rapidly gaining adoption in industry. However, the methods for improving LLMs are still designed by humans, which restricts the invention of new model-improving algorithms to human expertise and imagination. To address this, we propose the Self-Developing framework, which enables LLMs to autonomously generate and learn model-improvement algorithms. In this framework, the seed model generates, applies, and learns... | Yoichi Ishibashi, Taro Yano, Masafumi Oyamada |  |
| 601 |  |  [Simulating Classroom Education with LLM-Empowered Agents](https://doi.org/10.18653/v1/2025.naacl-long.520) |  | 0 | Large language models (LLMs) have been applied across various intelligent educational tasks to assist teaching. While preliminary studies have focused on task-specific, independent LLM-empowered agents, the potential of LLMs within a multi-agent collaborative framework for classroom simulation with real user participation remains unexplored. In this work, we propose SimClass, a multi-agent classroom simulation teaching framework. We recognize representative class roles and introduce a novel... | Zheyuan Zhang, Daniel ZhangLi, Jifan Yu, Linlu Gong, Jinchang Zhou, Zhanxin Hao, Jianxiao Jiang, Jie Cao, Huiqin Liu, Zhiyuan Liu, Lei Hou, Juanzi Li |  |
| 602 |  |  [A Grounded Typology of Word Classes](https://doi.org/10.18653/v1/2025.naacl-long.521) |  | 0 | In this work, we propose a grounded approach to meaning in language typology. Using images captioned across languages, we can treat the images as an empirical language agnostic representation of meaning, allowing the quantification of language function and semantics. Using principles from information theory, we define “groundedness”, an empirical measure of contextual semantic contentfulness which can be computed using multilingual (vision-and-)language models. As an initial application, we... | Coleman Haley, Sharon Goldwater, Edoardo M. Ponti |  |
| 603 |  |  [SSH: Sparse Spectrum Adaptation via Discrete Hartley Transformation](https://doi.org/10.18653/v1/2025.naacl-long.522) |  | 0 | Low-rank adaptation (LoRA) has been demonstrated effective in reducing the trainable parameter number when fine-tuning a large foundation model (LLM). However, it still encounters computational and memory challenges when scaling to larger models or addressing more complex task adaptation.In this work, we introduce \*\*Sparse Spectrum Adaptation via Discrete Hartley Transformation (SSH)\*\*, a novel approach that significantly reduces the number of trainable parameters while enhancing model... | Yixian Shen, Qi Bi, JiaHong Huang, Hongyi Zhu, Andy D. Pimentel, Anuj Pathania |  |
| 604 |  |  [LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable User Satisfaction Estimation in Dialogue](https://doi.org/10.18653/v1/2025.naacl-long.523) |  | 0 | Understanding user satisfaction with conversational systems, known as User Satisfaction Estimation (USE), is essential for assessing dialogue quality and enhancing user experiences. However, existing methods for USE face challenges due to limited understanding of underlying reasons for user dissatisfaction and the high costs of annotating user intentions. To address these challenges, we propose PRAISE (Plan and Retrieval Alignment for Interpretable Satisfaction Estimation), an interpretable... | Sangyeop Kim, Sohhyung Park, Jaewon Jung, Jinseok Kim, Sungzoon Cho |  |
| 605 |  |  [LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs](https://doi.org/10.18653/v1/2025.naacl-long.524) |  | 0 | While large language models (LLMs) excel in generating coherent and contextually rich outputs, their capacity to efficiently handle long-form contexts is limited by fixed-length position embeddings. Additionally, the computational cost of processing long sequences increases quadratically, making it challenging to extend context length. To address these challenges, we propose Long-form Context Injection with Recurrent Compression (LCIRC), a method that enables the efficient processing long-form... | Sumin An, Junyoung Sung, Wonpyo Park, Chanjun Park, Paul Hongsuck Seo |  |
| 606 |  |  [A Template Is All You Meme](https://doi.org/10.18653/v1/2025.naacl-long.525) |  | 0 | Templatic memes, characterized by a semantic structure adaptable to the creator’s intent, represent a significant yet underexplored area within meme processing literature. With the goal of establishing a new direction for computational meme analysis, here we create a knowledge base composed of more than 5,200 meme templates, information about them, and 54,000 examples of template instances (templatic memes). To investigate the semantic signal of meme templates, we show that we can match memes... | Luke Bates, Peter Ebert Christensen, Preslav Nakov, Iryna Gurevych |  |
| 607 |  |  [LLMs vs Established Text Augmentation Techniques for Classification: When do the Benefits Outweight the Costs?](https://doi.org/10.18653/v1/2025.naacl-long.526) |  | 0 | The generative large language models (LLMs) are increasingly being used for data augmentation tasks, where text samples are LLM-paraphrased and then used for classifier fine-tuning. Previous studies have compared LLM-based augmentations with established augmentation techniques, but the results are contradictory: some report superiority of LLM-based augmentations, while other only marginal increases (and even decreases) in performance of downstream classifiers. A research that would confirm a... | Ján Cegin, Jakub Simko, Peter Brusilovsky |  |
| 608 |  |  [Bridging the Visual Gap: Fine-Tuning Multimodal Models with Knowledge-Adapted Captions](https://doi.org/10.18653/v1/2025.naacl-long.527) |  | 0 | Recent research increasingly focuses on training vision-language models (VLMs) with long, detailed image captions. However, small-scale VLMs often struggle to balance the richness of these captions with the risk of hallucinating content during fine-tuning. In this paper, we explore how well VLMs adapt to such captions. To quantify caption quality, we propose Decomposed NLI (DNLI), an evaluation framework that breaks down generated captions into individual propositions, assessing each in... | Moran Yanuka, Assaf BenKish, Yonatan Bitton, Idan Szpektor, Raja Giryes |  |
| 609 |  |  [Self-Training Meets Consistency: Improving LLMs' Reasoning with Consistency-Driven Rationale Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.528) |  | 0 | Self-training approach for large language models (LLMs) improves reasoning abilities by training the models on their self-generated rationales. Previous approaches have labeled rationales that produce correct answers for a given question as appropriate for training. However, a single measure risks misjudging rationale quality, leading the models to learn flawed reasoning patterns. To address this issue, we propose CREST (Consistency-driven Rationale Evaluation for Self-Training), a... | Jaehyeok Lee, Keisuke Sakaguchi, JinYeong Bak |  |
| 610 |  |  [Evaluating Defeasible Reasoning in LLMs with DEFREASING](https://doi.org/10.18653/v1/2025.naacl-long.529) |  | 0 |  | Emily Allaway, Kathleen McKeown |  |
| 611 |  |  [Evaluating Input Feature Explanations through a Unified Diagnostic Evaluation Framework](https://doi.org/10.18653/v1/2025.naacl-long.530) |  | 0 | Explaining the decision-making process of machine learning models is crucial for ensuring their reliability and transparency for end users. One popular explanation form highlights key input features, such as i) tokens (e.g., Shapley Values and Integrated Gradients), ii) interactions between tokens (e.g., Bivariate Shapley and Attention-based methods), or iii) interactions between spans of the input (e.g., Louvain Span Interactions). However, these explanation types have only been studied in... | Jingyi Sun, Pepa Atanasova, Isabelle Augenstein |  |
| 612 |  |  [From Evidence to Belief: A Bayesian Epistemology Approach to Language Models](https://doi.org/10.18653/v1/2025.naacl-long.531) |  | 0 | This paper investigates the knowledge of language models from the perspective of Bayesian epistemology. We explore how language models adjust their confidence and responses when presented with evidence with varying levels of informativeness and reliability. To study these properties, we create a dataset with various types of evidence and analyze language models’ responses and confidence using verbalized confidence, token probability, and sampling. We observed that language models do not... | Minsu Kim, Sangryul Kim, James Thorne |  |
| 613 |  |  [Private Synthetic Text Generation with Diffusion Models](https://doi.org/10.18653/v1/2025.naacl-long.532) |  | 0 | How capable are diffusion models of generating synthetics texts? Recent research shows their strengths, with performance reaching that of auto-regressive LLMs. But are they also good in generating synthetic data if the training was under differential privacy? Here the evidence is missing, yet the promises from private image generation look strong. In this paper we address this open question by extensive experiments. At the same time, we critically assess (and reimplement) previous works on... | Sebastian Ochs, Ivan Habernal |  |
| 614 |  |  [Mitigating Tail Narrowing in LLM Self-Improvement via Socratic-Guided Sampling](https://doi.org/10.18653/v1/2025.naacl-long.533) |  | 0 | Self-improvement methods enable large language models (LLMs) to generate solutions themselves and iteratively train on filtered, high-quality rationales. This process proves effective and reduces the reliance on human supervision in LLMs’ reasoning, but the performance soon plateaus. We delve into the process and find that models tend to over-sample on easy queries and under-sample on queries they have yet to master. As iterations proceed, this imbalance in sampling is exacerbated, leading to a... | Yiwen Ding, Zhiheng Xi, Wei He, Lizhuoyuan Lizhuoyuan, Yitao Zhai, Shi Xiaowei, Xunliang Cai, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 615 |  |  [FactEval: Evaluating the Robustness of Fact Verification Systems in the Era of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.534) |  | 0 | Whilst large language models (LLMs) have made significant advances in every natural language processing task, studies have shown that these models are vulnerable to small perturbations in the inputs, raising concerns about their robustness in the real-world. Given the rise of misinformation online and its significant impact on society, fact verification is one area in which assessing the robustness of models developed for this task is crucial. However, the robustness of LLMs in fact... | Mamta Mamta, Oana Cocarascu |  |
| 616 |  |  [Analyzing Memorization in Large Language Models through the Lens of Model Attribution](https://doi.org/10.18653/v1/2025.naacl-long.535) |  | 0 | Large Language Models (LLMs) are prevalent in modern applications but often memorize training data, leading to privacy breaches and copyright issues. Existing research has mainly focused on post-hoc analyses—such as extracting memorized content or developing memorization metrics—without exploring the underlying architectural factors that contribute to memorization. In this work, we investigate memorization from an architectural lens by analyzing how attention modules at different layers impact... | Tarun Ram Menta, Susmit Agrawal, Chirag Agarwal |  |
| 617 |  |  [Track-SQL: Enhancing Generative Language Models with Dual-Extractive Modules for Schema and Context Tracking in Multi-turn Text-to-SQL](https://doi.org/10.18653/v1/2025.naacl-long.536) |  | 0 | Generative language models have shown significant potential in single-turn Text-to-SQL. However, their performance does not extend equivalently to multi-turn Text-to-SQL. This is primarily due to generative language models’ inadequacy in handling the complexities of context information and dynamic schema linking in multi-turn interactions. In this paper, we propose a framework named Track-SQL, which enhances generative language models with dual-extractive modules designed to track schema and... | Bingfeng Chen, Shaobin Shi, Yongqi Luo, Boyan Xu, Ruichu Cai, Zhifeng Hao |  |
| 618 |  |  [Prototypical Extreme Multi-label Classification with a Dynamic Margin Loss](https://doi.org/10.18653/v1/2025.naacl-long.537) |  | 0 | Extreme Multi-label Classification (XMC) methods predict relevant labels for a given query in an extremely large label space. Recent works in XMC address this problem using deep encoders that project text descriptions to an embedding space suitable for recovering the closest labels. However, learning deep models can be computationally expensive in large output spaces, resulting in a trade-off between high performing brute-force approaches and efficient solutions. In this paper, we propose... | Kunal Dahiya, Diego Ortego, David JimenezCabello |  |
| 619 |  |  [MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback](https://doi.org/10.18653/v1/2025.naacl-long.538) |  | 0 | Automatic question generation (QG) is essential for AI and NLP, particularly in intelligent tutoring, dialogue systems, and fact verification. Generating multiple-choice questions (MCQG) for professional exams, like the United States Medical Licensing Examination (USMLE), is particularly challenging, requiring domain expertise and complex multi-hop reasoning for high-quality questions. However, current large language models (LLMs) like GPT-4 struggle with professional MCQG due to outdated... | Zonghai Yao, Aditya Parashar, Huixue Zhou, Won Seok Jang, Feiyun Ouyang, Zhichao Yang, Hong Yu |  |
| 620 |  |  [Main Predicate and Their Arguments as Explanation Signals For Intent Classification](https://doi.org/10.18653/v1/2025.naacl-long.539) |  | 0 | Intent classification is crucial for conversational agents (chatbots), and deep learning models perform well in this area. However, little research has been done on the explainability of intent classification due to the absence of suitable benchmark data. Human annotation of explanation signals in text samples is time-consuming and costly. However, from inspection of data on intent classification, we see that, more often than not, the main verb denotes the action, and the direct object... | Sameer Pimparkhede, Pushpak Bhattacharyya |  |
| 621 |  |  [Handling Missing Entities in Zero-Shot Named Entity Recognition: Integrated Recall and Retrieval Augmentation](https://doi.org/10.18653/v1/2025.naacl-long.540) |  | 0 | Zero-shot Named Entity Recognition (ZS-NER) aims to recognize entities in unseen domains without specific annotated data. A key challenge is handling missing entities while ensuring accurate type recognition, hindered by: 1) the pre-training assumption that each entity has a single type, overlooking diversity, and 2) insufficient contextual knowledge for type reasoning. To address this, we propose IRRA (Integrated Recall and Retrieval Augmentation), a novel two-stage framework leveraging large... | Ruichu Cai, Junhao Lu, Zhongjie Chen, Boyan Xu, Zhifeng Hao |  |
| 622 |  |  [KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy](https://doi.org/10.18653/v1/2025.naacl-long.541) |  | 0 | The increasing demand for mental health services has led to the rise of AI-driven mental health chatbots, though challenges related to privacy, data collection, and expertise persist. Motivational Interviewing (MI) is gaining attention as a theoretical basis for boosting expertise in the development of these chatbots. However, existing datasets are showing limitations for training chatbots, leading to a substantial demand for publicly available resources in the field of MI and psychotherapy.... | Hyunjong Kim, Suyeon Lee, Yeongjae Cho, Eunseo Ryu, Yohan Jo, Suran Seong, Sungzoon Cho |  |
| 623 |  |  [Automatic Input Rewriting Improves Translation with Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.542) |  | 0 | Can we improve machine translation (MT) with LLMs by rewriting their inputs automatically? Users commonly rely on the intuition that well-written text is easier to translate when using off-the-shelf MT systems. LLMs can rewrite text in many ways but in the context of MT, these capabilities have been primarily exploited to rewrite outputs via post-editing. We present an empirical study of 21 input rewriting methods with 3 open-weight LLMs for translating from English into 6 target languages. We... | Dayeon Ki, Marine Carpuat |  |
| 624 |  |  [HIGGS: Pushing the Limits of Large Language Model Quantization via the Linearity Theorem](https://doi.org/10.18653/v1/2025.naacl-long.543) |  | 0 | Quantizing large language models has become a standard way to reduce their memory and computational costs. Typically, existing methods focus on breaking down the problem into individual layer-wise sub-problems, and minimizing per-layer error, measured via various metrics. Yet, this approach currently lacks theoretical justification and the metrics employed may be sub-optimal. In this paper, we present a “linearity theorem” establishing a direct relationship between the layer-wise reconstruction... | Vladimir Malinovskii, Andrei Panferov, Ivan Ilin, Han Guo, Peter Richtárik, Dan Alistarh |  |
| 625 |  |  [The LLM Language Network: A Neuroscientific Approach for Identifying Causally Task-Relevant Units](https://doi.org/10.18653/v1/2025.naacl-long.544) |  | 0 | Large language models (LLMs) exhibit remarkable capabilities on not just language tasks, but also various tasks that are not linguistic in nature, such as logical reasoning and social inference. In the human brain, neuroscience has identified a core language system that selectively and causally supports language processing. We here ask whether similar specialization for language emerges in LLMs. We identify language-selective units within 18 popular LLMs, using the same localization approach... | Badr AlKhamissi, Greta Tuckute, Antoine Bosselut, Martin Schrimpf |  |
| 626 |  |  [MixLLM: Dynamic Routing in Mixed Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.545) |  | 0 | Large Language Models (LLMs) exhibit potential artificial generic intelligence recently, however, their usage is costly with high response latency. Given mixed LLMs with their own strengths and weaknesses, LLM routing aims to identify the most suitable model for each query in the stream to maximize response quality and minimize cost and latency. However, the challenges involve: (1) dynamic trade-offs among quality, cost, and latency; (2) enabling continual learning in deployed systems; and (3)... | Xinyuan Wang, Yanchi Liu, Wei Cheng, Xujiang Zhao, Zhengzhang Chen, Wenchao Yu, Yanjie Fu, Haifeng Chen |  |
| 627 |  |  [Continual Learning in Multilingual Sign Language Translation](https://doi.org/10.18653/v1/2025.naacl-long.546) |  | 0 | The field of sign language translation (SLT) is still in its infancy, as evidenced by the low translation quality, even when using deep learn- ing approaches. Probably because of this, many common approaches in other machine learning fields have not been explored in sign language. Here, we focus on continual learning for mul- tilingual SLT. We experiment with three con- tinual learning methods and compare them to four more naive baseline and fine-tuning ap- proaches. We work with four sign... | Shakib Yazdani, Josef van Genabith, Cristina EspañaBonet |  |
| 628 |  |  [Few-Shot Natural Language to First-Order Logic Translation via Code Generation](https://doi.org/10.18653/v1/2025.naacl-long.547) |  | 0 | Translation of natural language to first-order logical formula (NL-FOL) has recently gained significant attention for its critical role in logic-based NLP applications. Some studies attempt to utilize pretrained language models in a sequence-to-sequence manner for the NL-FOL task. However, these methods encounter challenges such as (1) inconsistency between the training and inference phases and (2) the data-intensive and resource-intensive finetuning process. This paper introduces a novel... | Junnan Liu |  |
| 629 |  |  [How Good Are LLMs for Literary Translation, Really? Literary Translation Evaluation with Humans and LLMs](https://doi.org/10.18653/v1/2025.naacl-long.548) |  | 0 | Recent research has focused on literary machine translation (MT) as a new challenge in MT. However, the evaluation of literary MT remains an open problem. We contribute to this ongoing discussion by introducing LITEVAL-CORPUS, a paragraph-level parallel corpus containing verified human translations and outputs from 9 MT systems, which totals over 2k translations and 13k evaluated sentences across four language pairs, costing 4.5k€. This corpus enables us to (i) examine the consistency and... | Ran Zhang, Wei Zhao, Steffen Eger |  |
| 630 |  |  [PORT: Preference Optimization on Reasoning Traces](https://doi.org/10.18653/v1/2025.naacl-long.549) |  | 0 | Preference optimization methods have been successfully applied to improve not only the alignment of large language models (LLMs) with human values, but also specific natural language tasks such as summarization and stylistic continuations. This paper proposes using preference optimization methods on Chain-of-Thought steps in order to improve the mathematical reasoning performances of language models. While the chosen answers are obtained from datasets that include reasoning traces, we propose... | Salem Lahlou, Abdalgader Abubaker, Hakim Hacid |  |
| 631 |  |  [Guiding Through Complexity: What Makes Good Supervision for Hard Reasoning Tasks?](https://doi.org/10.18653/v1/2025.naacl-long.550) |  | 0 | How can “weak teacher models” (Bowman et al., 2022) such as average human annotators or existing AI systems, effectively supervise LLMs to improve performance on hard reasoning tasks, especially those that challenge and requires expertise or daily practice from the teacher models? In this paper, we seek for empirical answers to this question by investigating various data-driven strategies that offer supervision data at different quality levels upon tasks of varying complexity. Two intuitive... | Xuan He, Da Yin, Nanyun Peng |  |
| 632 |  |  [Fine-Grained Transfer Learning for Harmful Content Detection through Label-Specific Soft Prompt Tuning](https://doi.org/10.18653/v1/2025.naacl-long.551) |  | 0 | The spread of harmful content online is a dynamic issue evolving over time. Existing detection models, reliant on static data, are becoming less effective and generalizable. Developing new models requires sufficient up-to-date data, which is challenging. A potential solution is to combine existing datasets with minimal new data. However, detection tasks vary—some focus on hate speech, offensive, or abusive content, which differ in the intent to harm, while others focus on identifying targets of... | Faeze Ghorbanpour, Viktor Hangya, Alexander Fraser |  |
| 633 |  |  [A Systematic Examination of Preference Learning through the Lens of Instruction-Following](https://doi.org/10.18653/v1/2025.naacl-long.552) |  | 0 | In this work we systematically investigate how specific attributes of preference datasets affect the alignment and downstream performance of LLMs in instruction-following tasks. We use a novel synthetic data generation pipeline to generate 48,000 unique instruction-following prompts with combinations of 23 verifiable constraints that enable fine-grained and automated quality assessments of model responses. With our synthetic prompts, we use rejection sampling (RS) and Monte Carlo Tree Search... | Joongwon Kim, Anirudh Goyal, Aston Zhang, Bo Xiong, Rui Hou, Melanie Kambadur, Dhruv Mahajan, Hannaneh Hajishirzi, Liang Tan |  |
| 634 |  |  [Lived Experience Not Found: LLMs Struggle to Align with Experts on Addressing Adverse Drug Reactions from Psychiatric Medication Use](https://doi.org/10.18653/v1/2025.naacl-long.553) |  | 0 | Adverse Drug Reactions (ADRs) from psychiatric medications are the leading cause of hospitalizations among mental health patients. With healthcare systems and online communities facing limitations in resolving ADR-related issues, Large Language Models (LLMs) have the potential to fill this gap. Despite the increasing capabilities of LLMs, past research has not explored their capabilities in detecting ADRs related to psychiatric medications or in providing effective harm reduction strategies. To... | Mohit Chandra, Siddharth Sriraman, Gaurav Verma, Harneet Singh Khanuja, Jose Suarez Campayo, Zihang Li, Michael L. Birnbaum, Munmun De Choudhury |  |
| 635 |  |  [Latent Factor Models Meets Instructions: Goal-conditioned Latent Factor Discovery without Task Supervision](https://doi.org/10.18653/v1/2025.naacl-long.554) |  | 0 | Instruction-following LLMs have recently allowed systems to discover hidden concepts from a collection of unstructured documents based on a natural language description of the purpose of the discovery (i.e., goal). Still, the quality of the discovered concepts remains mixed, as it depends heavily on LLM’s reasoning ability and drops when the data is noisy or beyond LLM’s knowledge. We present Instruct-LF, a goal-oriented latent factor discovery system that integrates LLM’s instruction-following... | Zhouhang Xie, Tushar Khot, Bhavana Dalvi Mishra, Harshit Surana, Julian J. McAuley, Peter Clark, Bodhisattwa Prasad Majumder |  |
| 636 |  |  [LLM-Supported Natural Language to Bash Translation](https://doi.org/10.18653/v1/2025.naacl-long.555) |  | 0 | The Bourne-Again Shell (Bash) command-line interface for Linux systems has complex syntax and requires extensive specialized knowledge. Using the natural language to Bash command (NL2SH) translation capabilities of large language models (LLMs) for command composition circumvents these issues. However, the NL2SH performance of LLMs is difficult to assess due to inaccurate test data and unreliable heuristics for determining the functional equivalence of Bash commands. We present a manually... | Finnian Westenfelder, Erik Hemberg, Stephen Moskal, UnaMay O'Reilly, Silviu Chiricescu |  |
| 637 |  |  [REL-A.I.: An Interaction-Centered Approach To Measuring Human-LM Reliance](https://doi.org/10.18653/v1/2025.naacl-long.556) |  | 0 | The ability to communicate uncertainty and knowledge limitations is crucial for the safety of large language models (LLMs). Current evaluations of these abilities typically examine the correspondence between model accuracy and its internal probabilities or linguistic outputs. However, evaluation of the uncertainty of LLM communication should also focus on the behaviors of their human interlocutors: how much do users rely on what the LLM says? We introduce an interaction-centered evaluation... | Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, Nouha Dziri, Dan Jurafsky, Maarten Sap |  |
| 638 |  |  [Eliciting Critical Reasoning in Retrieval-Augmented Generation via Contrastive Explanations](https://doi.org/10.18653/v1/2025.naacl-long.557) |  | 0 | Retrieval-augmented generation (RAG) have emerged as a critical mechanism in contemporary NLP to support Large Language Models (LLMs) in systematically accessing richer factual context. However, the integration of RAG mechanisms bring its inherent challenges, as LLMs need to integrate potentially noisy contexts. Recent studies have shown that LLMs still struggle to critically analyse RAG-based in-context information, a limitation that may lead to incorrect inferences and hallucinations. In this... | Leonardo Ranaldi, Marco Valentino, André Freitas |  |
| 639 |  |  [A Distributional Perspective on Word Learning in Neural Language Models](https://doi.org/10.18653/v1/2025.naacl-long.558) |  | 0 | Language models (LMs) are increasingly being studied as models of human language learners.Due to the nascency of the field, it is not well-established whether LMs exhibit similar learning dynamics to humans, and there are few direct comparisons between learning trajectories in humans and models.Word learning trajectories for children are relatively well-documented, and recent work has tried to extend these investigations to language models.However, there are no widely agreed-upon metrics for... | Filippo Ficarra, Ryan Cotterell, Alex Warstadt |  |
| 640 |  |  [Disentangling language change: sparse autoencoders quantify the semantic evolution of indigeneity in French](https://doi.org/10.18653/v1/2025.naacl-long.559) |  | 0 | This study presents a novel approach to analyzing historical language change, focusing on the evolving semantics of the French term “indigène(s)” (“indigenous”) between 1825 and 1950. While existing approaches to measuring semantic change with contextual word embeddings (CWE) rely primarily on similarity measures or clustering, these methods may not be suitable for highly imbalanced datasets, and pose challenges for interpretation. For this reason, we propose an interpretable, feature-level... | Jacob Matthews, Laurent Dubreuil, Imane Terhmina, Yunci Sun, Matthew Wilkens, Marten van Schijndel |  |
| 641 |  |  [Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages](https://doi.org/10.18653/v1/2025.naacl-long.560) |  | 0 | Recent works have explored using language models for planning problems. One approach examines translating natural language descriptions of planning tasks into structured planning languages, such as the planning domain definition language (PDDL). Existing evaluation methods struggle to ensure semantic correctness and rely on simple or unrealistic datasets. To bridge this gap, we introduce Planetarium, a benchmark designed to evaluate language models’ ability to generate PDDL code from natural... | Max Zuo, Francisco Piedrahita Velez, Xiaochen Li, Michael Littman, Stephen H. Bach |  |
| 642 |  |  [One fish, two fish, but not the whole sea: Alignment reduces language models' conceptual diversity](https://doi.org/10.18653/v1/2025.naacl-long.561) |  | 0 | Researchers in social science and psychology have recently proposed using large language models (LLMs) as replacements for humans in behavioral research. In addition to arguments about whether LLMs accurately capture population-level patterns, this has raised questions about whether LLMs capture human-like conceptual diversity. Separately, it is debated whether post-training alignment (RLHF or RLAIF) affects models’ internal diversity. Inspired by human studies, we use a new way of measuring... | Sonia K. Murthy, Tomer D. Ullman, Jennifer Hu |  |
| 643 |  |  [Using Text-Based Causal Inference to Disentangle Factors Influencing Online Review Ratings](https://doi.org/10.18653/v1/2025.naacl-long.562) |  | 0 | Online reviews provide valuable insights into the perceived quality of facets of a product or service. While aspect-based sentiment analysis has focused on extracting these facets from reviews, there is less work understanding the impact of each aspect on overall perception. This is particularly challenging given correlations among aspects, making it difficult to isolate the effects of each. This paper introduces a methodology based on recent advances in text-based causal analysis, specifically... | Linsen Li, Aron Culotta, Nicholas Mattei |  |
| 644 |  |  [Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate](https://doi.org/10.18653/v1/2025.naacl-long.563) |  | 0 | Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference algorithm, enabling us to have better control over the trade-off between the objectives, while... | Xiaomeng Jin, Zhiqi Bu, Bhanukiran Vinzamuri, Anil Ramakrishna, KaiWei Chang, Volkan Cevher, Mingyi Hong |  |
| 645 |  |  [REFFLY: Melody-Constrained Lyrics Editing Model](https://doi.org/10.18653/v1/2025.naacl-long.564) |  | 0 | Automatic melody-to-lyric (M2L) generation aims to create lyrics that align with a given melody. While most previous approaches generate lyrics from scratch, revision—editing plain text draft to fit it into the melody—offers a much more flexible and practical alternative. This enables broad applications, such as generating lyrics from flexible inputs (keywords, themes, or full text that needs refining to be singable), song translation (preserving meaning across languages while keeping the... | Songyan Zhao, Bingxuan Li, Yufei Tian, Nanyun Peng |  |
| 646 |  |  [Exploring Safety-Utility Trade-Offs in Personalized Language Models](https://doi.org/10.18653/v1/2025.naacl-long.565) |  | 0 | As large language models (LLMs) become increasingly integrated into daily applications, it is essential to ensure they function fairly across diverse user demographics. In this work, we show that LLMs suffer from personalization bias, where their performance is impacted when they are personalized to a user’s identity. We quantify personalization bias by evaluating the performance of LLMs along two axes - safety and utility. We measure safety by examining how benign LLM responses are to unsafe... | Anvesh Rao Vijjini, Somnath Basu Roy Chowdhury, Snigdha Chaturvedi |  |
| 647 |  |  [MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart Problems](https://doi.org/10.18653/v1/2025.naacl-long.566) |  | 0 | Multimodal Large Language Models (MLLMs) have demonstrated impressive abilities across various tasks, including visual question answering and chart comprehension, yet existing benchmarks for chart-related tasks fall short in capturing the complexity of real-world multi-chart scenarios. Current benchmarks primarily focus on single-chart tasks, neglecting the multi-hop reasoning required to extract and integrate information from multiple charts, which is essential in practical applications. To... | Zifeng Zhu, Mengzhao Jia, Zhihan Zhang, Lang Li, Meng Jiang |  |
| 648 |  |  [It Is Not Only the Negative that Deserves Attention! Understanding, Generation & Evaluation of (Positive) Moderation](https://doi.org/10.18653/v1/2025.naacl-long.567) |  | 0 | Moderation is essential for maintaining and improving the quality of online discussions. This involves: (1) countering negativity, e.g. hate speech and toxicity, and (2) promoting positive discourse, e.g. broadening the discussion to involve other users and perspectives. While significant efforts have focused on addressing negativity, driven by an urgency to address such issues, this left moderation promoting positive discourse (henceforth PositiveModeration) under-studied. With the recent... | Iman Jundi, Eva Maria Vecchi, Carlotta Quensel, Neele Falk, Gabriella Lapesa |  |
| 649 |  |  [Social Norms in Cinema: A Cross-Cultural Analysis of Shame, Pride and Prejudice](https://doi.org/10.18653/v1/2025.naacl-long.568) |  | 0 | Shame and pride are social emotions expressed across cultures to motivate and regulate people’s thoughts, feelings, and behaviors. In this paper, we introduce the first cross-cultural dataset of over 10k shame/pride-related expressions with underlying social expectations from ~5.4K Bollywood and Hollywood movies. We examine \*how\* and \*why\* shame and pride are expressed across cultures using a blend of psychology-informed language analysis combined with large language models. We find... | Sunny Rai, Khushang Jilesh Zaveri, Shreya Havaldar, Soumna Nema, Lyle H. Ungar, Sharath Chandra Guntuku |  |
| 650 |  |  [The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding](https://doi.org/10.18653/v1/2025.naacl-long.569) |  | 0 | In a systematic way, we investigate a widely asked question: Do LLMs really understand what they say?, which relates to the more familiar term Stochastic Parrot. To this end, we propose a summative assessment over a carefully designed physical concept understanding task, P HYSI C O. Our task alleviates the memorization issue via the usage of grid-format inputs that abstractly describe physical phenomena. The grids represents varying levels of understanding, from the core phenomenon, application... | Mo Yu, Lemao Liu, Junjie Wu, Tsz Ting Chung, Shunchi Zhang, Jiangnan Li, DitYan Yeung, Jie Zhou |  |
| 651 |  |  [mHumanEval - A Multilingual Benchmark to Evaluate Large Language Models for Code Generation](https://doi.org/10.18653/v1/2025.naacl-long.570) |  | 0 | Recent advancements in large language models (LLMs) have significantly enhanced code generation from natural language prompts. The HumanEval Benchmark, developed by OpenAI, remains the most widely used code generation benchmark. However, this and other Code LLM benchmarks face critical limitations, particularly in task diversity, test coverage, and linguistic scope. Current evaluations primarily focus on English-to-Python conversion tasks with limited test cases, potentially overestimating... | Md. Nishat Raihan, Antonios Anastasopoulos, Marcos Zampieri |  |
| 652 |  |  [What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for Gaussian-Noise-free Text-Image Corruption and Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.571) |  | 0 | Vision-Language Models (VLMs) have gained prominence due to their success in solving complex cross-modal tasks. However, the internal mechanisms of VLMs, particularly the roles of cross-attention and self-attention in multimodal integration, are not fully understood. To address this gap, we introduce NOTICE, a Gaussian-Noise-free Text-Image Corruption and Evaluation pipeline for mechanistic interpretability in VLMs. NOTICE introduces Semantic Image Pairs (SIP) corruption, the first visual... | Michal Golovanevsky, William Rudman, Vedant Palit, Carsten Eickhoff, Ritambhara Singh |  |
| 653 |  |  [Are explicit belief representations necessary? A comparison between Large Language Models and Bayesian probabilistic models](https://doi.org/10.18653/v1/2025.naacl-long.572) |  | 0 | Large language models (LLMs) have exhibited certain indirect pragmatic capabilities, including interpreting indirect requests and non-literal meanings. Yet, it is unclear whether the success of LLMs on pragmatic tasks generalizes to phenomena that directly probe inferences about the beliefs of others. Indeed, LLMs’ performance on Theory of Mind (ToM) tasks is mixed. To date, the most successful computationally explicit approach to making inferences about others’ beliefs is the Rational Speech... | Dingyi Pan, Benjamin K. Bergen |  |
| 654 |  |  [Self-Generated Critiques Boost Reward Modeling for Language Models](https://doi.org/10.18653/v1/2025.naacl-long.573) |  | 0 | Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce scalar scores and struggle to incorporate critiques in a natural language format. We hypothesize that predicting both critiques and the scalar reward would improve reward modeling ability. Motivated by this, we propose Critic-RM, a framework that improves reward models using self-generated... | Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, Rui Hou |  |
| 655 |  |  [Characterizing the Role of Similarity in the Property Inferences of Language Models](https://doi.org/10.18653/v1/2025.naacl-long.574) |  | 0 | Property inheritance—a phenomenon where novel properties are projected from higher level categories (e.g., birds) to lower level ones (e.g., sparrows)—provides a unique window into how humans organize and deploy conceptual knowledge. It is debated whether this ability arises due to explicitly stored taxonomic knowledge vs. simple computations of similarity between mental representations. How are these mechanistic hypotheses manifested in contemporary language models? In this work, we... | Juan Diego Rodriguez, Aaron Mueller, Kanishka Misra |  |
| 656 |  |  [SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains](https://doi.org/10.18653/v1/2025.naacl-long.575) |  | 0 | Retrieval-augmented generation (RAG) enhances the question answering (QA) abilities of large language models (LLMs) by integrating external knowledge. However, adapting general-purpose RAG systems to specialized fields such as science and medicine poses unique challenges due to distribution shifts and limited access to domain-specific data. To tackle this, we propose SimRAG, a self-training approach that equips LLMs with joint capabilities of question answering and question generation for... | Ran Xu, Hui Liu, Sreyashi Nag, Zhenwei Dai, Yaochen Xie, Xianfeng Tang, Chen Luo, Yang Li, Joyce C. Ho, Carl Yang, Qi He |  |
| 657 |  |  [Learning to Substitute Words with Model-based Score Ranking](https://doi.org/10.18653/v1/2025.naacl-long.576) |  | 0 | Smart word substitution aims to enhance sentence quality by improving word choices, however current benchmarks rely on human-labeled data , which suffers from subjectivity and lacks diversity due to limitations in the number of annotators. Since word choices are inherently subjective, ground-truth word substitutions generated by a small group of annotators are often incomplete and likely not generalizable. To circumvent this issue, we instead employ a model-based scoring (BARTScore) to quantify... | Hongye Liu, Ricardo Henao |  |
| 658 |  |  [Multilingual Reasoning via Self-training](https://doi.org/10.18653/v1/2025.naacl-long.577) |  | 0 | Although reasoning is innately language-agnostic, the multilingual capacities remains a significant challenge for large language models (LLMs). Their ability to generate structured, step-wise explanations is constantly restricted to dominant languages in pre-training data, making cross-lingual generalisation difficult and hindering broader global adoption. Recent works have introduced eclectic strategies to improve reasoning beyond English; however, these methods remain related to specific... | Leonardo Ranaldi, Giulia Pucci |  |
| 659 |  |  [xLAM: A Family of Large Action Models to Empower AI Agent Systems](https://doi.org/10.18653/v1/2025.naacl-long.578) |  | 0 | Autonomous agents powered by large language models (LLMs) have attracted significant research interest. However, the open-source community faces many challenges in developing specialized models for agent tasks, driven by the scarcity of high-quality agent datasets and the absence of standard protocols in this area. We introduce xLAM, a series of large action models designed for AI agent tasks. The xLAM series includes five models with both dense and mixture-of-expert architectures, ranging from... | Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara Prabhakar, Haolin Chen, Zhiwei Liu, Yihao Feng, Tulika Manoj Awalgaonkar, Rithesh R. N., Zeyuan Chen, Ran Xu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong |  |
| 660 |  |  [ProMQA: Question Answering Dataset for Multimodal Procedural Activity Understanding](https://doi.org/10.18653/v1/2025.naacl-long.579) |  | 0 | Multimodal systems have great potential to assist humans in procedural activities, where people follow instructions to achieve their goals. Despite diverse application scenarios, systems are typically evaluated on traditional classification tasks, e.g., action recognition or temporal action localization. In this paper, we present a novel evaluation dataset, ProMQA, to measure the advancement of systems in application-oriented scenarios. ProMQA consists of 401 multimodal procedural QA pairs on... | Kimihiro Hasegawa, Wiradee Imrattanatrai, ZhiQi Cheng, Masaki Asada, Susan Holm, Yuran Wang, Ken Fukuda, Teruko Mitamura |  |
| 661 |  |  [Ethical Concern Identification in NLP: A Corpus of ACL Anthology Ethics Statements](https://doi.org/10.18653/v1/2025.naacl-long.580) |  | 0 | What ethical concerns, if any, do LLM researchers have? We introduce EthiCon, a corpus of 1,580 ethical concern statements extracted from scientific papers published in the ACL Anthology. We extract ethical concern keywords from the statements and show promising results in automating the concern identification process. Through a survey (N=200), we compare the ethical concerns of the corpus to the concerns listed by the general public and professionals in the field. Finally, we compare our... | Antonia Karamolegkou, Sandrine Schiller Hansen, Ariadni Christopoulou, Filippos Stamatiou, Anne Lauscher, Anders Søgaard |  |
| 662 |  |  [AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge](https://doi.org/10.18653/v1/2025.naacl-long.581) |  | 0 | Knowledge conflict arises from discrepancies between information in the context of a large language model (LLM) and the knowledge stored in its parameters. This can hurt performance when using standard decoding techniques, which tend to ignore the context. Existing test-time contrastive methods seek to address this by comparing the LLM’s output distribution with and without the context and adjust the model according to the contrast between them. However, we find that these methods frequently... | Han Wang, Archiki Prasad, Elias StengelEskin, Mohit Bansal |  |
| 663 |  |  [Are Multimodal LLMs Robust Against Adversarial Perturbations? RoMMath: A Systematic Evaluation on Multimodal Math Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.582) |  | 0 | We introduce RoMMath, the first benchmark designed to evaluate the capabilities and robustness of multimodal large language models (MLLMs) in handling multimodal math reasoning, particularly when faced with adversarial perturbations. RoMMath consists of 4,800 expert-annotated examples, including an original set and seven adversarial sets, each targeting a specific type of perturbation at the text or vision levels. We evaluate a broad spectrum of 17 MLLMs on RoMMath and uncover a critical... | Yilun Zhao, Guo Gan, Chen Zhao, Arman Cohan |  |
| 664 |  |  [LBC: Language-Based-Classifier for Out-Of-Variable Generalization](https://doi.org/10.18653/v1/2025.naacl-long.583) |  | 0 | Large Language Models (LLMs) have great success in natural language processing tasks such as response generation. However, their use in tabular data has been limited due to their inferior performance compared to traditional machine learning models (TMLs) such as XGBoost. We find that the pre-trained knowledge of LLMs enables them to interpret new variables that appear in a test without additional training, a capability central to the concept of Out-of-Variable (OOV). From the findings, we... | Kangjun Noh, Baekryun Seong, Hoyoon Byun, Youngjun Choi, Sungjin Song, Kyungwoo Song |  |
| 665 |  |  [On the Impact of Fine-Tuning on Chain-of-Thought Reasoning](https://doi.org/10.18653/v1/2025.naacl-long.584) |  | 0 | Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs’ task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA)... | Elita A. Lobo, Chirag Agarwal, Himabindu Lakkaraju |  |
| 666 |  |  [InfoPO: On Mutual Information Maximization for Large Language Model Alignment](https://doi.org/10.18653/v1/2025.naacl-long.585) |  | 0 | We study the post-training of large language models (LLMs) with human preference data. Recently, direct preference optimization and its variants have shown considerable promise in aligning language models, eliminating the need for reward models and online sampling. Despite these benefits, these methods rely on explicit assumptions about the Bradley-Terry (BT) model, which makes them prone to overfitting and results in suboptimal performance, particularly on reasoning-heavy tasks. To address... | Teng Xiao, Zhen Ge, Sujay Sanghavi, Tian Wang, Julian KatzSamuels, Marc Versage, Qingjun Cui, Trishul Chilimbi |  |
| 667 |  |  [Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse Frequency Effect in Structural Priming](https://doi.org/10.18653/v1/2025.naacl-long.586) |  | 0 | Large language models (LLMs) have shown the emergent capability of in-context learning (ICL). One line of research has claimed that ICL is functionally equivalent to gradient descent, a type of error-driven learning mechanism. In this paper, we introduce a new way of diagnosing whether ICL is functionally performing error-driven learning. Our approach is based on the inverse frequency effect (IFE)—a phenomenon in which an agent’s behavior is influenced to a greater degree when presented with... | Zhenghao Zhou, Robert Frank, R. Thomas McCoy |  |
| 668 |  |  [Guiding Medical Vision-Language Models with Diverse Visual Prompts: Framework Design and Comprehensive Exploration of Prompt Variations](https://doi.org/10.18653/v1/2025.naacl-long.587) |  | 0 | While mainstream vision-language models (VLMs) have advanced rapidly in understanding image-level information, they still lack the ability to focus on specific areas designated by humans. Rather, they typically rely on large volumes of high-quality image-text paired data to learn and generate posterior attention maps. To address this critical issue, we propose leveraging visual prompts—simple visual markers in various forms—to guide and enhance the formation of region-specific attention. Thus,... | Kangyu Zhu, Ziyuan Qin, Huahui Yi, Zekun Jiang, Qicheng Lao, Shaoting Zhang, Kang Li |  |
| 669 |  |  [Analyzing and Improving Coherence of Large Language Models in Question Answering](https://doi.org/10.18653/v1/2025.naacl-long.588) |  | 0 | Large language models (LLMs) have recently revolutionized natural language processing. These models, however, often suffer from instability or lack of coherence, that is the ability of the models to generate semantically equivalent outputs when receiving diverse yet semantically equivalent input variations. In this work, we analyze the behavior of multiple LLMs, including Mixtral-8x7B, Llama2-70b, Smaug-72b, and Phi-3, when dealing with multiple lexical variations of the same info-seeking... | Ivano Lauriola, Stefano Campese, Alessandro Moschitti |  |
| 670 |  |  [ALinFiK: Learning to Approximate Linearized Future Influence Kernel for Scalable Third-Parity LLM Data Valuation](https://doi.org/10.18653/v1/2025.naacl-long.589) |  | 0 | Large Language Models (LLMs) heavily rely on high-quality training data, making data valuation crucial for optimizing model performance, especially when working within a limited budget. In this work, we aim to offer a third-party data valuation approach that benefits both data providers and model developers. We introduce a linearized future influence kernel (LinFiK), which assesses the value of individual data samples in improving LLM performance during training. We further propose ALinFiK, a... | Yanzhou Pan, Huawei Lin, Yide Ran, Jiamin Chen, Xiaodong Yu, Weijie Zhao, Denghui Zhang, Zhaozhuo Xu |  |
| 671 |  |  [E-Gen: Leveraging E-Graphs to Improve Continuous Representations of Symbolic Expressions](https://doi.org/10.18653/v1/2025.naacl-long.590) |  | 0 | Vector representations have been pivotal in advancing natural language processing (NLP), with prior research focusing on embedding techniques for mathematical expressions using mathematically equivalent formulations. While effective, these approaches are constrained by the size and diversity of training data. In this work, we address these limitations by introducing E-Gen, a novel e-graph-based dataset generation scheme that synthesizes large and diverse mathematical expression datasets,... | Hongbo Zheng, Suyuan Wang, Neeraj Gangwar, Nickvash Kani |  |
| 672 |  |  [Robust and Unbounded Length Generalization in Autoregressive Transformer-Based Text-to-Speech](https://doi.org/10.18653/v1/2025.naacl-long.591) |  | 0 | Autoregressive (AR) Transformer-based sequence models are known to have difficulty generalizing to sequences longer than those seen during training. When applied to text-to-speech (TTS), these models tend to drop or repeat words or produce erratic output, especially for longer utterances. In this paper, we introduce enhancements aimed at AR Transformer-based encoder-decoder TTS systems that address these robustness and length generalization issues. Our approach uses an alignment mechanism to... | Eric Battenberg, R. J. SkerryRyan, Daisy Stanton, Soroosh Mariooryad, Matt Shannon, Julian Salazar, David Kao |  |
| 673 |  |  [PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics](https://doi.org/10.18653/v1/2025.naacl-long.592) |  | 0 | Evaluating the quality of machine-generated natural language content is a challenging task in Natural Language Processing (NLP). Recently, large language models (LLMs) like GPT-4 have been employed for this purpose, but they are computationally expensive due to the extensive token usage required by complex evaluation prompts. In this paper, we propose a prompt optimization approach that uses a smaller, fine-tuned language model to compress input data for evaluation prompt, thus reducing token... | Daniil Larionov, Steffen Eger |  |
| 674 |  |  [AutoParLLM: GNN-guided Context Generation for Zero-Shot Code Parallelization using LLMs](https://doi.org/10.18653/v1/2025.naacl-long.593) |  | 0 | In-Context Learning (ICL) has been shown to be a powerful technique to augment the capabilities of LLMs for a diverse range of tasks. This work proposes AutoParLLM, a novel way to generate context using guidance from graph neural networks (GNNs) to generate efficient parallel codes. We evaluate AutoParLLM on 12 applications from two well-known benchmark suites of parallel codes: NAS Parallel Benchmark and Rodinia Benchmark. Our results show that AutoParLLM improves the state-of-the-art LLMs... | Quazi Ishtiaque Mahmud, Ali TehraniJamsaz, Hung D. Phan, Le Chen, Mihai Capota, Theodore L. Willke, Nesreen K. Ahmed, Ali Jannesari |  |
| 675 |  |  [Causally Modeling the Linguistic and Social Factors that Predict Email Response](https://doi.org/10.18653/v1/2025.naacl-long.594) |  | 0 | Email is a vital conduit for human communication across businesses, organizations, and broader societal contexts. In this study, we aim to model the intents, expectations, and responsiveness in email exchanges. To this end, we release SIZZLER, a new dataset containing 1800 emails annotated with nuanced types of intents and expectations. We benchmark models ranging from feature-based logistic regression to zero-shot prompting of large language models. Leveraging the predictive model for intent,... | Yinuo Xu, Hong Chen, Sushrita Rakshit, Aparna Ananthasubramaniam, Omkar Yadav, Mingqian Zheng, Michael Jiang, Lechen Zhang, Bowen Yi, Kenan Alkiek, Abraham Israeli, Bangzhao Shu, Hua Shen, Jiaxin Pei, Haotian Zhang, Miriam Schirmer, David Jurgens |  |
| 676 |  |  [AI-LieDar : Examine the Trade-off Between Utility and Truthfulness in LLM Agents](https://doi.org/10.18653/v1/2025.naacl-long.595) |  | 0 | Truthfulness (adherence to factual accuracy) and utility (satisfying human needs and instructions) are both fundamental aspects of Large Language Models, yet these goals often conflict (e.g., sell a car with known flaws), making it challenging to achieve both in real-world deployments. We propose AI-LieDar, a framework to study how LLM-based agents navigate these scenarios in an multi-turn interactive setting. We design a set of real-world scenarios where language agents are instructed to... | Zhe Su, Xuhui Zhou, Sanketh Rangreji, Anubha Kabra, Julia Mendelsohn, Faeze Brahman, Maarten Sap |  |
| 677 |  |  [Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset](https://doi.org/10.18653/v1/2025.naacl-long.596) |  | 0 | In an effort to mitigate the harms of large language models (LLMs), learning from human feedback (LHF) has been used to steer LLMs towards outputs that are intended to be both less harmful and more helpful. Despite the widespread adoption of LHF in practice, the quality of this feedback and its effectiveness as a safety mitigation technique remain unclear. This study addresses these issues by auditing the widely-used Helpful and Harmless (HH) dataset by Anthropic. Our work includes: (1) a... | Khaoula Chehbouni, Jonathan Colaço Carr, Yash More, Jackie CK Cheung, Golnoosh Farnadi |  |
| 678 |  |  [FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions](https://doi.org/10.18653/v1/2025.naacl-long.597) |  | 0 | Modern Language Models (LMs) are capable of following long and complex instructions that enable a large and diverse set of user requests. While Information Retrieval (IR) models use these LMs as the backbone of their architectures, virtually none of them allow users to provide detailed instructions alongside queries, thus limiting their ability to satisfy complex information needs. In this work, we study the use of instructions in IR systems. First, we introduce our dataset FollowIR, which... | Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn J. Lawrie, Luca Soldaini |  |
| 679 |  |  [Few-shot Personalization of LLMs with Mis-aligned Responses](https://doi.org/10.18653/v1/2025.naacl-long.598) |  | 0 | As the diversity of users increases, the capability of providing personalized responses by large language models (LLMs) has become increasingly important. Existing approaches have only limited successes in LLM personalization, due to the absence of personalized learning or the reliance on shared personal data. This paper proposes a new approach for a few-shot personalization of LLMs with their mis-aligned responses (Fermi). Our key idea is to learn a set of personalized prompts for each user by... | Jaehyung Kim, Yiming Yang |  |
| 680 |  |  [Prompting with Phonemes: Enhancing LLMs' Multilinguality for Non-Latin Script Languages](https://doi.org/10.18653/v1/2025.naacl-long.599) |  | 0 | Multilingual LLMs have achieved remarkable benchmark performance, but we find they continue to underperform on non-Latin script languages across contemporary LLM families. This discrepancy arises from the fact that LLMs are pretrained with orthographic scripts, which are dominated by Latin characters that obscure their shared phonology with non-Latin scripts. We propose leveraging phonemic transcriptions as complementary signals to induce script-invariant representations. Our study demonstrates... | Hoang Nguyen, Khyati Mahajan, Vikas Yadav, Julian Salazar, Philip S. Yu, Masoud Hashemi, Rishabh Maheshwary |  |
| 681 |  |  [SHADES: Towards a Multilingual Assessment of Stereotypes in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.600) |  | 0 | Large Language Models (LLMs) reproduce and exacerbate the social biases present in their training data, and resources to quantify this issue are limited. While research has attempted to identify and mitigate such biases, most efforts have been concentrated around English, lagging the rapid advancement of LLMs in multilingual settings. In this paper, we introduce a new multilingual parallel dataset SHADES to help address this issue, designed for examining culturally-specific stereotypes that may... | Margaret Mitchell, Giuseppe Attanasio, Ioana Baldini, Miruna Clinciu, Jordan Clive, Pieter Delobelle, Manan Dey, Sil Hamilton, Timm Dill, Jad Doughman, Ritam Dutt, Avijit Ghosh, Jessica Zosa Forde, Carolin Holtermann, LucieAimée Kaffee, Tanmay Laud, Anne Lauscher, Roberto L. LopezDavila, Maraim Masoud, Nikita Nangia, Anaelia Ovalle, Giada Pistilli, Dragomir Radev, Beatrice Savoldi, Vipul Raheja, Jeremy Qin, Esther Ploeger, Arjun Subramonian, Kaustubh D. Dhole, Kaiser Sun, Amirbek Djanibekov, Jonibek Mansurov, Kayo Yin, Emilio Villa Cueva, Sagnik Mukherjee, Jerry Huang, Xudong Shen, Jay Gala, Hamdan AlAli, Tair Djanibekov, Nurdaulet Mukhituly, Shangrui Nie, Shanya Sharma, Karolina Stanczak, Eliza Szczechla, Tiago Timponi Torrent, Deepak Tunuguntla, Marcelo Viridiano, Oskar Van Der Wal, Adina Yakefu, Aurélie Névéol, Mike Zhang, Sydney Zink, Zeerak Talat |  |
| 682 |  |  [Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion](https://doi.org/10.18653/v1/2025.naacl-long.601) |  | 0 | Speculative decoding has emerged as a widely adopted method to accelerate large language model inference without sacrificing the quality of the model outputs. While this technique has facilitated notable speed improvements by enabling parallel sequence verification, its efficiency remains inherently limited by the reliance on incremental token generation in existing draft models. To overcome this limitation, this paper proposes an adaptation of speculative decoding which uses discrete diffusion... | Jacob K. Christopher, Brian R. Bartoldson, Tal BenNun, Michael Cardei, Bhavya Kailkhura, Ferdinando Fioretto |  |
| 683 |  |  [Bayelemabaga: Creating Resources for Bambara NLP](https://doi.org/10.18653/v1/2025.naacl-long.602) |  | 0 | Data curation for under-resource languages enables the development of more accurate and culturally sensitive natural language processing models. However, the scarcity of well-structured multilingual datasets remains a challenge for advancing machine translation in these languages, especially for African languages. This paper focuses on creating high-quality parallel corpora that capture linguistic diversity to address this gap. We introduce Bayelemabaga, the most extensive curated multilingual... | Allahsera Auguste Tapo, Kevin Assogba, Christopher M. Homan, M. Mustafa Rafique, Marcos Zampieri |  |
| 684 |  |  [Single Ground Truth Is Not Enough: Adding Flexibility to Aspect-Based Sentiment Analysis Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.603) |  | 0 | Aspect-based sentiment analysis (ABSA) is a challenging task of extracting sentiments along with their corresponding aspects and opinion terms from the text.The inherent subjectivity of span annotation makes variability in the surface forms of extracted terms, complicating the evaluation process.Traditional evaluation methods often constrain ground truths (GT) to a single term, potentially misrepresenting the accuracy of semantically valid predictions that differ in surface form.To address this... | Soyoung Yang, Hojun Cho, Jiyoung Lee, Sohee Yoon, Edward Choi, Jaegul Choo, Won Ik Cho |  |
| 685 |  |  [DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.604) |  | 0 | Multimodal Large Language Models (MLLMs) pose unique safety challenges due to their integration of visual and textual data, thereby introducing new dimensions of potential attacks and complex risk combinations. In this paper, we begin with a detailed analysis aimed at disentangling risks through step-by-step reasoning within multimodal inputs. We find that systematic multimodal risk disentanglement substantially enhances the risk awareness of MLLMs. Via leveraging the strong discriminative... | Jianyu Liu, Hangyu Guo, Ranjie Duan, Xingyuan Bu, Yancheng He, Shilong Li, Hui Huang, Jiaheng Liu, Yucheng Wang, Chenchen Jing, Xingwei Qu, Xiao Zhang, Pei Wang, Yanan Wu, Jihao Gu, Yangguang Li, Jianke Zhu |  |
| 686 |  |  [In-Context Learning with Long-Context Models: An In-Depth Exploration](https://doi.org/10.18653/v1/2025.naacl-long.605) |  | 0 | As model context lengths continue to increase, the number of demonstrations that can be provided in-context approaches the size of entire training datasets. We study the behavior of in-context learning (ICL) at this extreme scale on multiple datasets and models. We show that, for many datasets with large label spaces, performance continues to increase with thousands of demonstrations. We contrast this with example retrieval and finetuning: example retrieval shows excellent performance at low... | Amanda Bertsch, Maor Ivgi, Emily Xiao, Uri Alon, Jonathan Berant, Matthew R. Gormley, Graham Neubig |  |
| 687 |  |  [Preference Consistency Matters: Enhancing Preference Learning in Language Models with Automated Self-Curation of Training Corpora](https://doi.org/10.18653/v1/2025.naacl-long.606) |  | 0 | Inconsistent annotations in training corpora, particularly within preference learning datasets, pose challenges in developing advanced language models. These inconsistencies often arise from variability among annotators and inherent multi-dimensional nature of the preferences. To address these issues, we introduce a self-curation method that preprocesses annotated datasets by leveraging proxy models trained directly on them. Our method enhances preference learning by automatically detecting and... | JoonHo Lee, JuYoun Son, Juree Seok, Wooseok Jang, YeongDae Kwon |  |
| 688 |  |  [TurtleBench: A Visual Programming Benchmark in Turtle Geometry](https://doi.org/10.18653/v1/2025.naacl-long.607) |  | 0 | Humans have the ability to reason about geometric patterns in images and scenes from a young age. However, developing large multimodal models (LMMs) capable of similar reasoning remains a challenge, highlighting the need for robust evaluation methods to assess these capabilities. We introduce TurtleBench, a benchmark designed to evaluate LMMs’ capacity to interpret geometric patterns—given visual examples, textual instructions, or both—and generate precise code outputs. Inspired by turtle... | Sina Rismanchian, Yasaman Razeghi, Sameer Singh, Shayan Doroudi |  |
| 689 |  |  [Automatically Discovering How Misogyny is Framed on Social Media](https://doi.org/10.18653/v1/2025.naacl-long.608) |  | 0 | Misogyny, which is widespread on social media, can be identified not only by recognizing its many forms but also by discovering how misogyny is framed. This paper considers the automatic discovery of misogyny problems and their frames through the Dis-MP&F method, which enables the generation of a data-driven, rich Taxonomy of Misogyny (ToM), offering new insights in the complexity of expressions of misogyny. Furthermore, the Dis-MP&F method, informed by the ToM, is capable of producing very... | Rakshitha Rao Ailneni, Sanda M. Harabagiu |  |
| 690 |  |  [Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.609) |  | 0 | Faithfulness evaluators based on Large Language Models (LLMs) are often fooled by the fluency of the text and struggle with identifying errors in the summaries, usually leading to high false negative rate. We propose an approach to summary faithfulness evaluation in which multiple LLM-based agents are assigned initial stances (regardless of what their belief might be) and forced to come up with a reason to justify the imposed belief, thus engaging in a multi-round debate to reach an agreement.... | Mahnaz Koupaee, Jake W. Vincent, Saab Mansour, Igor Shalyminov, Han He, Hwanjun Song, Raphael Shu, Jianfeng He, Yi Nian, Amy Wingmei Wong, Kyu J. Han, Hang Su |  |
| 691 |  |  [ReIFE: Re-evaluating Instruction-Following Evaluation](https://doi.org/10.18653/v1/2025.naacl-long.610) |  | 0 | The automatic evaluation of instruction following typically involves using large language models (LLMs) to assess response quality. However, there is a lack of comprehensive evaluation of these LLM-based evaluators across two dimensions: the base LLMs and the evaluation protocols. Therefore, we present a thorough meta-evaluation of instruction following, including 25 base LLMs and 15 recently proposed evaluation protocols, on 4 human-annotated datasets, assessing the evaluation accuracy of the... | Yixin Liu, Kejian Shi, Alexander R. Fabbri, Yilun Zhao, Peifeng Wang, ChienSheng Wu, Shafiq Joty, Arman Cohan |  |
| 692 |  |  [Language Models Predict Empathy Gaps Between Social In-groups and Out-groups](https://doi.org/10.18653/v1/2025.naacl-long.611) |  | 0 | Studies of human psychology have demonstrated that people are more motivated to extend empathy to in-group members than out-group members (Cikara et al., 2011). In this study, we investigate how this aspect of intergroup relations in humans is replicated by LLMs in an emotion intensity prediction task. In this task, the LLM is given a short description of an experience a person had that caused them to feel a particular emotion; the LLM is then prompted to predict the intensity of the emotion... | Yu Hou, Hal Daumé III, Rachel Rudinger |  |
| 693 |  |  [HARP: Hesitation-Aware Reframing in Transformer Inference Pass](https://doi.org/10.18653/v1/2025.naacl-long.612) |  | 0 | This paper aims to improve the performance of large language models by addressing the variable computational demands in inference steps, where some tokens require more computational resources than others. We present HARP, a simple modification to “off-the-shelf” Transformer forward pass. Drawing from hesitation and the framing effect in decision-making, HARP selectively applies additional computation when the model encounters uncertainty during token generation. Our method mimics human... | Romain Storaï, Seungwon Hwang |  |
| 694 |  |  [JAWAHER: A Multidialectal Dataset of Arabic Proverbs for LLM Benchmarking](https://doi.org/10.18653/v1/2025.naacl-long.613) |  | 0 | Recent advancements in instruction fine-tuning, alignment methods such as reinforcement learning from human feedback (RLHF), and optimization techniques like direct preference optimization (DPO), have significantly enhanced the adaptability of large language models (LLMs) to user preferences. However, despite these innovations, many LLMs continue to exhibit biases toward Western, Anglo-centric, or American cultures, with performance on English data consistently surpassing that of other... | Samar Mohamed Magdy, Sang Yun Kwon, Fakhraddin Alwajih, Safaa Taher Abdelfadil, Shady Shehata, Muhammad AbdulMageed |  |
| 695 |  |  [EmojiPrompt: Generative Prompt Obfuscation for Privacy-Preserving Communication with Cloud-based LLMs](https://doi.org/10.18653/v1/2025.naacl-long.614) |  | 0 | Cloud-based Large Language Models (LLMs) such as ChatGPT have become increasingly integral to daily operations. Nevertheless, they also introduce privacy concerns: firstly, numerous studies underscore the risks to user privacy posed by jailbreaking cloud-based LLMs; secondly, the LLM service providers have access to all user data, which deters individuals from confidently utilizing such services. To address such concerns, we propose a simple yet effective paradigm, \*\*EmojiPrompt\*\*, to... | Sam Lin, Wenyue Hua, Zhenting Wang, Mingyu Jin, Lizhou Fan, Yongfeng Zhang |  |
| 696 |  |  [MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools](https://doi.org/10.18653/v1/2025.naacl-long.615) |  | 0 | Tool-using agents that act in the world need to be both useful and safe. Well-calibrated model confidences can be used to weigh the risk versus reward of potential actions, but prior work shows that many models are poorly calibrated. Inspired by interpretability literature exploring the internals of models, we propose a novel class of model-internal confidence estimators (MICE) to better assess confidence when calling tools. MICE first decodes from each intermediate layer of the language model... | Nishant Subramani, Jason Eisner, Justin Svegliato, Benjamin Van Durme, Yu Su, Sam Thomson |  |
| 697 |  |  [PAT: Parameter-Free Audio-Text Aligner to Boost Zero-Shot Audio Classification](https://doi.org/10.18653/v1/2025.naacl-long.616) |  | 0 | Audio-Language Models (ALMs) have demonstrated remarkable performance in zero-shot audio classification. In this paper, we introduce PAT (Parameter-free Audio-Text aligner), a simple and training-free method aimed at boosting zero-shot audio classification performance of CLAP-like ALMs. To achieve this, we propose to improve the cross-modal interaction between audio and language modalities by enhancing the representations for both modalities using mutual feedback. Precisely, to enhance textual... | Ashish Seth, Ramaneswaran Selvakumar, Sonal Kumar, Sreyan Ghosh, Dinesh Manocha |  |
| 698 |  |  [Language Model Council: Democratically Benchmarking Foundation Models on Highly Subjective Tasks](https://doi.org/10.18653/v1/2025.naacl-long.617) |  | 0 | As Large Language Models (LLMs) continue to evolve, evaluating them remains a persistent challenge. Many recent evaluations use LLMs as judges to score outputs from other LLMs, often relying on a single large model like GPT-4o. However, using a single LLM judge is prone to intra-model bias, and many tasks – such as those related to emotional intelligence, creative writing, and persuasiveness – may be too subjective for a single model to judge fairly. We introduce the Language Model Council... | Justin Zhao, Flor Miriam Plaza del Arco, Amanda Cercas Curry |  |
| 699 |  |  [SCIURus: Shared Circuits for Interpretable Uncertainty Representations in Language Models](https://doi.org/10.18653/v1/2025.naacl-long.618) |  | 0 | We investigate the mechanistic sources of uncertainty in large language models (LLMs), an area with important implications for language model reliability and trustworthiness. To do so, we conduct a series of experiments designed to identify whether the factuality of generated responses and a model’s uncertainty originate in separate or shared circuits in the model architecture. We approach this question by adapting the well-established mechanistic interpretability techniques of causal tracing... | Carter Teplica, Yixin Liu, Arman Cohan, Tim G. J. Rudner |  |
| 700 |  |  [ProSE: Diffusion Priors for Speech Enhancement](https://doi.org/10.18653/v1/2025.naacl-long.619) |  | 0 | Speech enhancement (SE) is the fundamental task of enhancing the clarity and quality of speech in the presence of non-stationary additive noise. While deterministic deep learning models have been commonly employed for SE, recent research indicates that generative models, such as denoising diffusion probabilistic models (DDPMs), have shown promise. However, different from speech generation, SE has a strong constraint to generate results in accordance with the underlying ground-truth signal.... | Sonal Kumar, Sreyan Ghosh, Utkarsh Tyagi, Anton Jeran Ratnarajah, Chandra Kiran Reddy Evuru, Ramani Duraiswami, Dinesh Manocha |  |
| 701 |  |  [Mastering the Craft of Data Synthesis for CodeLLMs](https://doi.org/10.18653/v1/2025.naacl-long.620) |  | 0 | Large language models (LLMs) have shown impressive performance in code understanding and generation, making coding tasks a key focus for researchers due to their practical applications and value as a testbed for LLM evaluation. Data synthesis and filtering techniques have been widely adopted and shown to be highly effective in this context. In this paper, we present a focused survey and taxonomy of these techniques, emphasizing recent advancements. We highlight key challenges, explore future... | Meng Chen, Philip Arthur, Qianyu Feng, Cong Duy Vu Hoang, YuHeng Hong, Mahdi Kazemi Moghaddam, Omid Nezami, Duc Thien Nguyen, Gioacchino Tangari, Duy Vu, Thanh Vu, Mark Johnson, Krishnaram Kenthapadi, Don Dharmasiri, Long Duong, YuanFang Li |  |
| 702 |  |  [ParaICL: Towards Parallel In-Context Learning](https://doi.org/10.18653/v1/2025.naacl-long.621) |  | 0 | Large language models (LLMs) have become the norm in natural language processing (NLP), excelling in few-shot in-context learning (ICL) with their remarkable abilities. Nonetheless, the success of ICL largely hinges on the choice of few-shot demonstration examples, making the selection process increasingly crucial. Existing methods have delved into optimizing the quantity and semantic similarity of these examples to improve ICL performances. However, our preliminary experiments indicate that... | Xingxuan Li, XuanPhi Nguyen, Shafiq Joty, Lidong Bing |  |
| 703 |  |  [CausalEval: Towards Better Causal Reasoning in Language Models](https://doi.org/10.18653/v1/2025.naacl-long.622) |  | 0 | Causal reasoning (CR) is a crucial aspect of intelligence, essential for problem-solving, decision-making, and understanding the world. While language models (LMs) can generate rationales for their outputs, their ability to reliably perform causal reasoning remains uncertain, often falling short in tasks requiring a deep understanding of causality. In this paper, we introduce CausalEval, a comprehensive review of research aimed at enhancing LMs for causal reasoning, coupled with an empirical... | Longxuan Yu, Delin Chen, Siheng Xiong, Qingyang Wu, Dawei Li, Zhikai Chen, Xiaoze Liu, Liangming Pan |  |
| 704 |  |  [Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense](https://doi.org/10.18653/v1/2025.naacl-long.623) |  | 0 | As large language models (LLMs) are increasingly deployed in diverse applications, including chatbot assistants and code generation, aligning their behavior with safety and ethical standards has become paramount. However, jailbreak attacks, which exploit vulnerabilities to elicit unintended or harmful outputs, threaten LLMs safety significantly. In this paper, we introduce Layer-AdvPatcher, a novel methodology designed to defend against jailbreak attacks by utilizing an unlearning strategy to... | Yang Ouyang, Hengrui Gu, Shuhang Lin, Wenyue Hua, Jie Peng, Bhavya Kailkhura, Meijun Gao, Tianlong Chen, Kaixiong Zhou |  |
| 705 |  |  [DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot Question Answering in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.624) |  | 0 | While Large Language Models (LLMs) excel in zero-shot Question Answering (QA), they tend to expose biases in their internal knowledge when faced with socially sensitive questions, leading to a degradation in performance. Existing zero-shot methods are efficient but failto consider context and prevent bias propagation in the answers. To address this, we propose \*DeCAP\*, a method for debiasing LLMs usingContext-Adaptive Prompt Generation. \*DeCAP\* leverages a \*Question Ambiguity Detection\*... | Suyoung Bae, YunSeok Choi, JeeHyong Lee |  |
| 706 |  |  [Reward-Guided Tree Search for Inference Time Alignment of Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.625) |  | 0 | Inference-time computation methods enhance the performance of Large Language Models (LLMs) by leveraging additional computational resources to achieve superior results. Common techniques, such as Best-of-N sampling, Majority Voting, and variants of tree-search algorithm have proven to be effective in boosting the performance of LLMs. These approaches strategically trade increased computational resource for improved model responses. In this work, we proposed DARWIN, an inference-time alignment... | ChiaYu Hung, Navonil Majumder, Ambuj Mehrish, Soujanya Poria |  |
| 707 |  |  [Typographic Attacks in a Multi-Image Setting](https://doi.org/10.18653/v1/2025.naacl-long.626) |  | 0 | Large Vision-Language Models (LVLMs) are susceptible to typographic attacks, which are misclassifications caused by an attack text that is added to an image. In this paper, we introduce a multi-image setting for studying typographic attacks, broadening the current emphasis of the literature on attacking individual images. Specifically, our focus is on attacking image sets without repeating the attack query. Such non-repeating attacks are stealthier, as they are more likely to evade a gatekeeper... | Xiaomeng Wang, Zhengyu Zhao, Martha A. Larson |  |
| 708 |  |  [Tonguescape: Exploring Language Models Understanding of Vowel Articulation](https://doi.org/10.18653/v1/2025.naacl-long.627) |  | 0 | Vowels are primarily characterized by tongue position. Humans have discovered these features of vowel articulation through their own experience and explicit objective observation such as using MRI. With this knowledge and our experience, we can explain and understand the relationship between tongue positions and vowels, and this knowledge is helpful for language learners to learn pronunciation. Since language models (LMs) are trained on a large amount of data that includes linguistic and... | Haruki Sakajo, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe |  |
| 709 |  |  [CoRAC: Integrating Selective API Document Retrieval with Question Semantic Intent for Code Question Answering](https://doi.org/10.18653/v1/2025.naacl-long.628) |  | 0 | Automatic code question answering aims to generate precise answers to questions about code by analyzing code snippets. To provide an appropriate answer, it is necessary to accurately understand the relevant part of the code and correctly interpret the intent of the question. However, in real-world scenarios, the questioner often provides only a portion of the code along with the question, making it challenging to find an answer. The responder should be capable of providing a suitable answer... | YunSeok Choi, CheolWon Na, JeeHyong Lee |  |
| 710 |  |  [Pipeline Analysis for Developing Instruct LLMs in Low-Resource Languages: A Case Study on Basque](https://doi.org/10.18653/v1/2025.naacl-long.629) |  | 0 | Large language models (LLMs) are typically optimized for resource-rich languages like English, exacerbating the gap between high-resource and underrepresented languages. This work presents a detailed analysis of strategies for developing a model capable of following instructions in a low-resource language, specifically Basque, by focusing on three key stages: pre-training, instruction tuning, and alignment with human preferences. Our findings demonstrate that continual pre-training with a... | Ander Corral, Ixak Sarasua, Xabier Saralegi |  |
| 711 |  |  [How to Make LLMs Forget: On Reversing In-Context Knowledge Edits](https://doi.org/10.18653/v1/2025.naacl-long.630) |  | 0 | In-context knowledge editing (IKE) enables efficient modification of large language model (LLM) outputs without parameter changes and at zero-cost. However, it can be misused to manipulate responses opaquely, e.g., insert misinformation or offensive content. Such malicious interventions could be incorporated into high-level wrapped APIs where the final input prompt is not shown to end-users. To address this issue, we investigate the detection and reversal of IKE-edits. First, we demonstrate... | Paul Youssef, Zhixue Zhao, Jörg Schlötterer, Christin Seifert |  |
| 712 |  |  [PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian](https://doi.org/10.18653/v1/2025.naacl-long.631) |  | 0 | Large language models predominantly reflect Western cultures, largely due to the dominance of English-centric training data. This imbalance presents a significant challenge, as LLMs are increasingly used across diverse contexts without adequate evaluation of their cultural competence in non-English languages, including Persian. To address this gap, we introduce PerCul, a carefully constructed dataset designed to assess the sensitivity of LLMs toward Persian culture. PerCul features story-based,... | Erfan Moosavi Monazzah, Vahid Rahimzadeh, Yadollah Yaghoobzadeh, Azadeh Shakery, Mohammad Taher Pilehvar |  |
| 713 |  |  [Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models](https://doi.org/10.18653/v1/2025.naacl-long.632) |  | 0 | Large language models (LLMs) are increasingly recognized for their exceptional generative capabilities and versatility across various tasks. However, the high inference costs associated with these models have not received adequate attention, particularly when compared to the focus on training costs in existing research. In response to this gap, our study conducts a comprehensive benchmarking of LLM inference energy across a wide range of NLP tasks, where we analyze the impact of different... | Soham Poddar, Paramita Koley, Janardan Misra, Niloy Ganguly, Saptarshi Ghosh |  |
| 714 |  |  [CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science Research Repositories](https://doi.org/10.18653/v1/2025.naacl-long.633) |  | 0 | The increasing complexity of computer science research projects demands more effective tools for deploying code repositories. Large Language Models (LLMs), such as Anthropic Claude and Meta Llama, have demonstrated significant advancements across various fields of computer science research, including the automation of diverse software engineering tasks. To evaluate the effectiveness of LLMs in handling complex code development tasks of research projects, particularly for NLP/CV/AI/ML/DM topics,... | Yijia Xiao, Runhui Wang, Luyang Kong, Davor Golac, Wei Wang |  |
| 715 |  |  [SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data](https://doi.org/10.18653/v1/2025.naacl-long.634) |  | 0 | In various natural language processing (NLP) tasks, fine-tuning Pre-trained Language Models (PLMs) often leads to the issue of spurious correlations, which negatively impacts performance, particularly when dealing with out-of-distribution data.To address this problem, we propose \*\*SALAD\*\* (\*\*S\*\*tructure \*\*A\*\*ware and \*\*L\*\*LM-driven \*\*A\*\*ugmented \*\*D\*\*ata), a novel approach designed to enhance model robustness and generalization by generating structure-aware and... | Suyoung Bae, YunSeok Choi, Hyojun Kim, JeeHyong Lee |  |
| 716 |  |  [Rationale-Guided Retrieval Augmented Generation for Medical Question Answering](https://doi.org/10.18653/v1/2025.naacl-long.635) |  | 0 | Large language models (LLM) hold significant potential for applications in biomedicine, but they struggle with hallucinations and outdated knowledge.While retrieval-augmented generation (RAG) is generally employed to address these issues, it also has its own set of challenges: (1) LLMs are vulnerable to irrelevant or unhelpful context, (2) medical queries are often not well-targeted for helpful information, and (3) retrievers are prone to bias toward the specific source corpus they were trained... | Jiwoong Sohn, Yein Park, Chanwoong Yoon, Sihyeon Park, Hyeon Hwang, Mujeen Sung, Hyunjae Kim, Jaewoo Kang |  |
| 717 |  |  [Prototype Conditioned Generative Replay for Continual Learning in NLP](https://doi.org/10.18653/v1/2025.naacl-long.636) |  | 0 | Generative replay has proven effective in addressing the catastrophic forgetting issue of continual learning (CL) in natural language processing (NLP). However, relying on a single task-specific token or prompt often falls short in generating pseudo-samples that accurately reflect the true data distribution. This leads to issues of semantic inconsistency and scale inconsistency.To tackle these challenges, we propose a Prototype Conditioned Generative Replay (PCGR) method, which enhances... | Xi Chen, Min Zeng |  |
| 718 |  |  [KODIS: A Multicultural Dispute Resolution Dialogue Corpus](https://doi.org/10.18653/v1/2025.naacl-long.637) |  | 0 |  | James Hale, Sushrita Rakshit, Kushal Chawla, Jeanne M. Brett, Jonathan Gratch |  |
