# NAACL2022

## 会议论文列表

本会议共有 732 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Front Matter](https://aclanthology.org/2022.naacl-industry.0) |  | 0 |  |  |  |
| 2 |  |  [Scalable and Robust Self-Learning for Skill Routing in Large-Scale Conversational AI Systems](https://doi.org/10.18653/v1/2022.naacl-industry.1) |  | 0 | Skill routing is an important component in large-scale conversational systems. In contrast to traditional rule-based skill routing, state-of-the-art systems use a model-based approach to enable natural conversations. To provide supervision signal required to train such models, ideas such as human annotation, replication of a rule-based system, relabeling based on user paraphrases, and bandit-based learning were suggested. However, these approaches: (a) do not scale in terms of the number of... | Mohammad Kachuee, Jinseok Nam, Sarthak Ahuja, JinMyung Won, Sungjin Lee |  |
| 3 |  |  [CREATER: CTR-driven Advertising Text Generation with Controlled Pre-Training and Contrastive Fine-Tuning](https://doi.org/10.18653/v1/2022.naacl-industry.2) |  | 0 | This paper focuses on automatically generating the text of an ad, and the goal is that the generated text can capture user interest for achieving higher click-through rate (CTR). We propose CREATER, a CTR-driven advertising text generation approach, to generate ad texts based on high-quality user reviews. To incorporate CTR objective, our model learns from online A/B test data with contrastive learning, which encourages the model to generate ad texts that obtain higher CTR. To make use of... | Penghui Wei, Xuanhua Yang, Shaoguo Liu, Liang Wang, Bo Zheng |  |
| 4 |  |  [Augmenting Poetry Composition with Verse by Verse](https://doi.org/10.18653/v1/2022.naacl-industry.3) |  | 0 | We describe Verse by Verse, our experiment in augmenting the creative process of writing poetry with an AI. We have created a group of AI poets, styled after various American classic poets, that are able to offer as suggestions generated lines of verse while a user is composing a poem. In this paper, we describe the underlying system to offer these suggestions. This includes a generative model, which is tasked with generating a large corpus of lines of verse offline and which are then stored in... | David C. Uthus, Maria Voitovich, R. J. Mical |  |
| 5 |  |  [AB/BA analysis: A framework for estimating keyword spotting recall improvement while maintaining audio privacy](https://doi.org/10.18653/v1/2022.naacl-industry.4) |  | 0 | Evaluation of keyword spotting (KWS) systems that detect keywords in speech is a challenging task under realistic privacy constraints. The KWS is designed to only collect data when the keyword is present, limiting the availability of hard samples that may contain false negatives, and preventing direct estimation of model recall from production data. Alternatively, complementary data collected from other sources may not be fully representative of the real application. In this work, we propose an... | Raphael Petegrosso, Vasistakrishna Baderdinni, Thibaud Senechal, Benjamin L. Bullough |  |
| 6 |  |  [Temporal Generalization for Spoken Language Understanding](https://doi.org/10.18653/v1/2022.naacl-industry.5) |  | 0 | Spoken Language Understanding (SLU) models in industry applications are usually trained offline on historic data, but have to perform well on incoming user requests after deployment. Since the application data is not available at training time, this is formally similar to the domain generalization problem, where domains correspond to different temporal segments of the data, and the goal is to build a model that performs well on unseen domains, e.g., upcoming data. In this paper, we explore... | Judith Gaspers, Anoop Kumar, Greg Ver Steeg, Aram Galstyan |  |
| 7 |  |  [An End-to-End Dialogue Summarization System for Sales Calls](https://doi.org/10.18653/v1/2022.naacl-industry.6) |  | 0 | Summarizing sales calls is a routine task performed manually by salespeople. We present a production system which combines generative models fine-tuned for customer-agent setting, with a human-in-the-loop user experience for an interactive summary curation process. We address challenging aspects of dialogue summarization task in a real-world setting including long input dialogues, content validation, lack of labeled data and quality evaluation. We show how GPT-3 can be leveraged as an offline... | Abedelkadir Asi, Song Wang, Roy Eisenstadt, Dean Geckt, Yarin Kuper, Yi Mao, Royi Ronen |  |
| 8 |  |  [Controlled Data Generation via Insertion Operations for NLU](https://doi.org/10.18653/v1/2022.naacl-industry.7) |  | 0 | Use of synthetic data is rapidly emerging as a realistic alternative to manually annotating live traffic for industry-scale model building. Manual data annotation is slow, expensive and not preferred for meeting customer privacy expectations. Further, commercial natural language applications are required to support continuously evolving features as well as newly added experiences. To address these requirements, we propose a targeted synthetic data generation technique by inserting tokens into a... | Manoj Kumar, Yuval Merhav, Haidar Khan, Rahul Gupta, Anna Rumshisky, Wael Hamza |  |
| 9 |  |  [Easy and Efficient Transformer: Scalable Inference Solution For Large NLP Model](https://doi.org/10.18653/v1/2022.naacl-industry.8) |  | 0 | Recently, large-scale transformer-based models have been proven to be effective over various tasks across many domains. Nevertheless, applying them in industrial production requires tedious and heavy works to reduce inference costs. To fill such a gap, we introduce a scalable inference solution: Easy and Efficient Transformer (EET), including a series of transformer inference optimization at the algorithm and implementation levels. First, we design highly optimized kernels for long inputs and... | Gongzheng Li, Yadong Xi, Jingzhen Ding, Duan Wang, Ziyang Luo, Rongsheng Zhang, Bai Liu, Changjie Fan, Xiaoxi Mao, Zeng Zhao |  |
| 10 |  |  [Aspect-based Analysis of Advertising Appeals for Search Engine Advertising](https://doi.org/10.18653/v1/2022.naacl-industry.9) |  | 0 | Writing an ad text that attracts people and persuades them to click or act is essential for the success of search engine advertising. Therefore, ad creators must consider various aspects of advertising appeals (A3) such as the price, product features, and quality. However, products and services exhibit unique effective A3 for different industries. In this work, we focus on exploring the effective A3 for different industries with the aim of assisting the ad creation process. To this end, we... | Soichiro Murakami, Peinan Zhang, Sho Hoshino, Hidetaka Kamigaito, Hiroya Takamura, Manabu Okumura |  |
| 11 |  |  [Self-supervised Product Title Rewrite for Product Listing Ads](https://doi.org/10.18653/v1/2022.naacl-industry.10) |  | 0 | Product Listing Ads (PLAs) are primary online advertisements merchants pay to attract more customers. However, merchants prefer to stack various attributes to the title and neglect the fluency and information priority. These seller-created titles are not suitable for PLAs as they fail to highlight the core information in the visible part in PLAs titles. In this work, we present a title rewrite solution. Specifically, we train a self-supervised language model to generate high-quality titles in... | Xue Zhao, Dayiheng Liu, Junwei Ding, Liang Yao, Mahone Yan, Huibo Wang, Wenqing Yao |  |
| 12 |  |  [Efficient Semi-supervised Consistency Training for Natural Language Understanding](https://doi.org/10.18653/v1/2022.naacl-industry.11) |  | 0 | Manually labeled training data is expensive, noisy, and often scarce, such as when developing new features or localizing existing features for a new region. In cases where labeled data is limited but unlabeled data is abundant, semi-supervised learning methods such as consistency training can be used to improve model performance, by training models to output consistent predictions between original and augmented versions of unlabeled data. In this work, we explore different data augmentation... | George Leung, Joshua Tan |  |
| 13 |  |  [Distantly Supervised Aspect Clustering And Naming For E-Commerce Reviews](https://doi.org/10.18653/v1/2022.naacl-industry.12) |  | 0 | Product aspect extraction from reviews is a critical task for e-commerce services to understand customer preferences and pain points. While aspect phrases extraction and sentiment analysis have received a lot of attention, clustering of aspect phrases and assigning human readable names to clusters in e-commerce reviews is an extremely important and challenging problem due to the scale of the reviews that makes human review infeasible. In this paper, we propose fully automated methods for... | Prateek Sircar, Aniket Chakrabarti, Deepak Gupta, Anirban Majumdar |  |
| 14 |  |  [Local-to-global learning for iterative training of production SLU models on new features](https://doi.org/10.18653/v1/2022.naacl-industry.13) |  | 0 | In production SLU systems, new training data becomes available with time so that ML models need to be updated on a regular basis. Specifically, releasing new features adds new classes of data while the old data remains constant. However, retraining the full model each time from scratch is computationally expensive. To address this problem, we propose to consider production releases from the curriculum learning perspective and to adapt the local-to-global learning (LGL) schedule (Cheng et. al,... | Yulia Grishina, Daniil Sorokin |  |
| 15 |  |  [CULG: Commercial Universal Language Generation](https://doi.org/10.18653/v1/2022.naacl-industry.14) |  | 0 | Pre-trained language models (PLMs) have dramatically improved performance for many natural language processing (NLP) tasks in domains such as finance and healthcare. However, the application of PLMs in the domain of commerce, especially marketing and advertising, remains less studied. In this work, we adapt pre-training methods to the domain of commerce, by proposing CULG, a large-scale commercial universal language generation model which is pre-trained on a corpus drawn from 10 markets across... | Haonan Li, Yameng Huang, Yeyun Gong, Jian Jiao, Ruofei Zhang, Timothy Baldwin, Nan Duan |  |
| 16 |  |  [Constraining word alignments with posterior regularization for label transfer](https://doi.org/10.18653/v1/2022.naacl-industry.15) |  | 0 | Unsupervised word alignments offer a lightweight and interpretable method to transfer labels from high- to low-resource languages, as long as semantically related words have the same label across languages. But such an assumption is often not true in industrial NLP pipelines, where multilingual annotation guidelines are complex and deviate from semantic consistency due to various factors (such as annotation difficulty, conflicting ontology, upcoming feature launches etc.);We address this... | Thomas Gueudré, Kevin Martin Jose |  |
| 17 |  |  [Explaining the Effectiveness of Multi-Task Learning for Efficient Knowledge Extraction from Spine MRI Reports](https://doi.org/10.18653/v1/2022.naacl-industry.16) |  | 0 | Pretrained Transformer based models finetuned on domain specific corpora have changed the landscape of NLP. However, training or fine-tuning these models for individual tasks can be time consuming and resource intensive. Thus, a lot of current research is focused on using transformers for multi-task learning (Raffel et al., 2020) and how to group the tasks to help a multi-task model to learn effective representations that can be shared across tasks (Standley et al., 2020; Fifty et al., 2021) .... | Arijit Sehanobish, McCullen Sandora, Nabila Abraham, Jayashri Pawar, Danielle Torres, Anasuya Das, Murray Becker, Richard Herzog, Benjamin Odry, Ron Vianu |  |
| 18 |  |  [FPI: Failure Point Isolation in Large-scale Conversational Assistants](https://doi.org/10.18653/v1/2022.naacl-industry.17) |  | 0 | Large-scale conversational assistants such as Cortana, Alexa, Google Assistant and Siri process requests through a series of modules for wake word detection, speech recognition, language understanding and response generation. An error in one of these modules can cascade through the system. Given the large traffic volumes in these assistants, it is infeasible to manually analyze the data, identify requests with processing errors and isolate the source of error. We present a machine learning... | Rinat Khaziev, Usman Shahid, Tobias Röding, Rakesh Chada, Emir Kapanci, Pradeep Natarajan |  |
| 19 |  |  [Asynchronous Convergence in Multi-Task Learning via Knowledge Distillation from Converged Tasks](https://doi.org/10.18653/v1/2022.naacl-industry.18) |  | 0 | Multi-task learning (MTL) aims to solve multiple tasks jointly by sharing a base representation among them. This can lead to more efficient learning and better generalization, as compared to learning each task individually. However, one issue that often arises in MTL is the convergence speed between tasks varies due to differences in task difficulty, so it can be a challenge to simultaneously achieve the best performance on all tasks with a single model checkpoint. Various techniques have been... | Weiyi Lu, Sunny Rajagopalan, Priyanka Nigam, Jaspreet Singh, Xiaodi Sun, Yi Xu, Belinda Zeng, Trishul Chilimbi |  |
| 20 |  |  [Augmenting Training Data for Massive Semantic Matching Models in Low-Traffic E-commerce Stores](https://doi.org/10.18653/v1/2022.naacl-industry.19) |  | 0 | Extreme multi-label classification (XMC) systems have been successfully applied in e-commerce (Shen et al., 2020; Dahiya et al., 2021) for retrieving products based on customer behavior. Such systems require large amounts of customer behavior data (e.g. queries, clicks, purchases) for training. However, behavioral data is limited in low-traffic e-commerce stores, impacting performance of these systems. In this paper, we present a technique that augments behavioral training data via query... | Ashutosh Joshi, Shankar Vishwanath, Choon Hui Teo, Vaclav Petricek, Vishy Vishwanathan, Rahul Bhagat, Jonathan May |  |
| 21 |  |  [Retrieval Based Response Letter Generation For a Customer Care Setting](https://doi.org/10.18653/v1/2022.naacl-industry.20) |  | 0 | Letter-like communications (such as email) are a major means of customer relationship management within customer-facing organizations. These communications are initiated on a channel by requests from customers and then responded to by the organization on the same channel. For decades, the job has almost entirely been conducted by human agents who attempt to provide the most appropriate reaction to the request. Rules have been made to standardize the overall customer service process and make... | Biplob Biswas, Renhao Cui, Rajiv Ramnath |  |
| 22 |  |  [Medical Coding with Biomedical Transformer Ensembles and Zero/Few-shot Learning](https://doi.org/10.18653/v1/2022.naacl-industry.21) |  | 0 | Medical coding (MC) is an essential pre-requisite for reliable data retrieval and reporting. Given a free-text reported term (RT) such as “pain of right thigh to the knee”, the task is to identify the matching lowest-level term (LLT) –in this case “unilateral leg pain”– from a very large and continuously growing repository of standardized medical terms. However, automating this task is challenging due to a large number of LLT codes (as of writing over 80\,000), limited availability of training... | Angelo Ziletti, Alan Akbik, Christoph Berns, Thomas Herold, Marion Legler, Martina Viell |  |
| 23 |  |  [Knowledge extraction from aeronautical messages (NOTAMs) with self-supervised language models for aircraft pilots](https://doi.org/10.18653/v1/2022.naacl-industry.22) |  | 0 | During their pre-flight briefings, aircraft pilots must analyse a long list of NoTAMs (NOtice To AirMen) indicating potential hazards along the flight route, sometimes up to pages for long-haul flights. NOTAM free-text fields typically have a very special phrasing, with lots of acronyms and domain-specific vocabulary, which makes it differ significantly from standard English. In this paper, we pretrain language models derived from BERT on circa 1 million unlabeled NOTAMs and reuse the learnt... | Alexandre Arnold, Fares Ernez, Catherine Kobus, MarionCécile Martin |  |
| 24 |  |  [Intent Discovery for Enterprise Virtual Assistants: Applications of Utterance Embedding and Clustering to Intent Mining](https://doi.org/10.18653/v1/2022.naacl-industry.23) |  | 0 | A key challenge in the creation and refinement of virtual assistants is the ability to mine unlabeled utterance data to discover common intents. We develop an approach to this problem that combines large-scale pre-training and multi-task learning to derive a semantic embedding that can be leveraged to identify clusters of utterances that correspond to unhandled intents. An utterance encoder is first trained with a language modeling objective and subsequently adapted to predict intent labels... | Minhua Chen, Badrinath Jayakumar, Michael Johnston, S. Eman Mahmoodi, Daniel Pressel |  |
| 25 |  |  [ReFinED: An Efficient Zero-shot-capable Approach to End-to-End Entity Linking](https://doi.org/10.18653/v1/2022.naacl-industry.24) |  | 0 | We introduce ReFinED, an efficient end-to-end entity linking model which uses fine-grained entity types and entity descriptions to perform linking. The model performs mention detection, fine-grained entity typing, and entity disambiguation for all mentions within a document in a single forward pass, making it more than 60 times faster than competitive existing approaches. ReFinED also surpasses state-of-the-art performance on standard entity linking datasets by an average of 3.7 F1. The model... | Tom Ayoola, Shubhi Tyagi, Joseph Fisher, Christos Christodoulopoulos, Andrea Pierleoni |  |
| 26 |  |  [Lightweight Transformers for Conversational AI](https://doi.org/10.18653/v1/2022.naacl-industry.25) |  | 0 | To understand how training on conversational language impacts performance of pre-trained models on downstream dialogue tasks, we build compact Transformer-based Language Models from scratch on several large corpora of conversational data. We compare the performance and characteristics of these models against BERT and other strong baselines on dialogue probing tasks. Commercial dialogue systems typically require a small footprint and fast execution time, but recent trends are in the other... | Daniel Pressel, Wenshuo Liu, Michael Johnston, Minhua Chen |  |
| 27 |  |  [NER-MQMRC: Formulating Named Entity Recognition as Multi Question Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.naacl-industry.26) |  | 0 | NER has been traditionally formulated as a sequence labeling task. However, there has been recent trend in posing NER as a machine reading comprehension task (Wang et al., 2020; Mengge et al., 2020), where entity name (or other information) is considered as a question, text as the context and entity value in text as answer snippet. These works consider MRC based on a single question (entity) at a time. We propose posing NER as a multi-question MRC task, where multiple questions (one question... | Anubhav Shrimal, Avi Jain, Kartik Mehta, Promod Yenigalla |  |
| 28 |  |  [What Do Users Care About? Detecting Actionable Insights from User Feedback](https://doi.org/10.18653/v1/2022.naacl-industry.27) |  | 0 | Users often leave feedback on a myriad of aspects of a product which, if leveraged successfully, can help yield useful insights that can lead to further improvements down the line. Detecting actionable insights can be challenging owing to large amounts of data as well as the absence of labels in real-world scenarios. In this work, we present an aggregation and graph-based ranking strategy for unsupervised detection of these insights from real-world, noisy, user-generated feedback. Our proposed... | Kasturi Bhattacharjee, Rashmi Gangadharaiah, Kathleen R. McKeown, Dan Roth |  |
| 29 |  |  [CTM - A Model for Large-Scale Multi-View Tweet Topic Classification](https://doi.org/10.18653/v1/2022.naacl-industry.28) |  | 0 | Automatically associating social media posts with topics is an important prerequisite for effective search and recommendation on many social media platforms. However, topic classification of such posts is quite challenging because of (a) a large topic space (b) short text with weak topical cues, and (c) multiple topic associations per post. In contrast to most prior work which only focuses on post-classification into a small number of topics (10-20), we consider the task of large-scale topic... | Vivek Kulkarni, Kenny Leung, Aria Haghighi |  |
| 30 |  |  [Developing a Production System for Purpose of Call Detection in Business Phone Conversations](https://doi.org/10.18653/v1/2022.naacl-industry.29) |  | 0 | For agents at a contact centre receiving calls, the most important piece of information is the reason for a given call. An agent cannot provide support on a call if they do not know why a customer is calling. In this paper we describe our implementation of a commercial system to detect Purpose of Call statements in English business call transcripts in real time. We present a detailed analysis of types of Purpose of Call statements and language patterns related to them, discuss an approach to... | Elena Khasanova, Pooja Hiranandani, Shayna Gardiner, Cheng Chen, Simon CorstonOliver, XueYong Fu |  |
| 31 |  |  [Adversarial Text Normalization](https://doi.org/10.18653/v1/2022.naacl-industry.30) |  | 0 | Text-based adversarial attacks are becoming more commonplace and accessible to general internet users. As these attacks proliferate, the need to address the gap in model robustness becomes imminent. While retraining on adversarial data may increase performance, there remains an additional class of character-level attacks on which these models falter. Additionally, the process to retrain a model is time and resource intensive, creating a need for a lightweight, reusable defense. In this work, we... | Joanna Bitton, Maya Pavlova, Ivan Evtimov |  |
| 32 |  |  [Constraint-based Multi-hop Question Answering with Knowledge Graph](https://doi.org/10.18653/v1/2022.naacl-industry.31) |  | 0 | The objective of a Question-Answering system over Knowledge Graph (KGQA) is to respond to natural language queries presented over the KG. A complex question answering system typically addresses one of the two categories of complexity: questions with constraints and questions involving multiple hops of relations. Most of the previous works have addressed these complexities separately. Multi-hop KGQA necessitates reasoning across numerous edges of the KG in order to arrive at the correct answer.... | Sayantan Mitra, Roshni R. Ramnani, Shubhashis Sengupta |  |
| 33 |  |  [Fast Bilingual Grapheme-To-Phoneme Conversion](https://doi.org/10.18653/v1/2022.naacl-industry.32) |  | 0 | Autoregressive transformer (ART)-based grapheme-to-phoneme (G2P) models have been proposed for bi/multilingual text-to-speech systems. Although they have achieved great success, they suffer from high inference latency in real-time industrial applications, especially processing long sentence. In this paper, we propose a fast and high-performance bilingual G2P model. For fast and exact decoding, we used a non-autoregressive structured transformer-based architecture and data augmentation for... | HwaYeon Kim, JongHwan Kim, JaeMin Kim |  |
| 34 |  |  [Knowledge Extraction From Texts Based on Wikidata](https://doi.org/10.18653/v1/2022.naacl-industry.33) |  | 0 | This paper presents an effort within our company of developing knowledge extraction pipeline for English, which can be further used for constructing an entreprise-specific knowledge base. We present a system consisting of entity detection and linking, coreference resolution, and relation extraction based on the Wikidata schema. We highlight existing challenges of knowledge extraction by evaluating the deployed pipeline on real-world data. We also make available a database, which can serve as a... | Anastasia Shimorina, Johannes Heinecke, Frédéric Herledan |  |
| 35 |  |  [AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry](https://doi.org/10.18653/v1/2022.naacl-industry.34) |  | 0 | Table Question Answering (Table QA) systems have been shown to be highly accurate when trained and tested on open-domain datasets built on top of Wikipedia tables. However, it is not clear whether their performance remains the same when applied to domain-specific scientific and business documents, encountered in industrial settings, which exhibit some unique characteristics: (a) they contain tables with a much more complex layout than Wikipedia tables (including hierarchical row and column... | Yannis Katsis, Saneem A. Chemmengath, Vishwajeet Kumar, Samarth Bharadwaj, Mustafa Canim, Michael R. Glass, Alfio Gliozzo, Feifei Pan, Jaydeep Sen, Karthik Sankaranarayanan, Soumen Chakrabarti |  |
| 36 |  |  [Parameter-efficient Continual Learning Framework in Industrial Real-time Text Classification System](https://doi.org/10.18653/v1/2022.naacl-industry.35) |  | 0 | Catastrophic forgetting is a challenge for model deployment in industrial real-time systems, which requires the model to quickly master a new task without forgetting the old one. Continual learning aims to solve this problem; however, it usually updates all the model parameters, resulting in extensive training times and the inability to deploy quickly. To address this challenge, we propose a parameter-efficient continual learning framework, in which efficient parameters are selected through an... | Tao Zhu, Zhe Zhao, Weijie Liu, Jiachi Liu, Yiren Chen, Weiquan Mao, Haoyan Liu, Kunbo Ding, Yudong Li, Xuefeng Yang |  |
| 37 |  |  [Self-Aware Feedback-Based Self-Learning in Large-Scale Conversational AI](https://doi.org/10.18653/v1/2022.naacl-industry.36) |  | 0 | Self-learning paradigms in large-scale conversational AI agents tend to leverage user feedback in bridging between what they say and what they mean. However, such learning, particularly in Markov-based query rewriting systems have far from addressed the impact of these models on future training where successive feedback is inevitably contingent on the rewrite itself, especially in a continually updating environment. In this paper, we explore the consequences of this inherent lack of... | Pragaash Ponnusamy, Clint Solomon Mathialagan, Gustavo Aguilar, Chengyuan Ma, Chenlei Guo |  |
| 38 |  |  [Fast and Light-Weight Answer Text Retrieval in Dialogue Systems](https://doi.org/10.18653/v1/2022.naacl-industry.37) |  | 0 | Dialogue systems can benefit from being able to search through a corpus of text to find information relevant to user requests, especially when encountering a request for which no manually curated response is available. The state-of-the-art technology for neural dense retrieval or re-ranking involves deep learning models with hundreds of millions of parameters. However, it is difficult and expensive to get such models to operate at an industrial scale, especially for cloud services that often... | Hui Wan, Siva Sankalp Patel, J. William Murdock, Saloni Potdar, Sachindra Joshi |  |
| 39 |  |  [BLINK with Elasticsearch for Efficient Entity Linking in Business Conversations](https://doi.org/10.18653/v1/2022.naacl-industry.38) |  | 0 | An Entity Linking system aligns the textual mentions of entities in a text to their corresponding entries in a knowledge base. However, deploying a neural entity linking system for efficient real-time inference in production environments is a challenging task. In this work, we present a neural entity linking system that connects the product and organization type entities in business conversations to their corresponding Wikipedia and Wikidata entries. The proposed system leverages Elasticsearch... | Md. Tahmid Rahman Laskar, Cheng Chen, Aliaksandr Martsinovich, Jonathan Johnston, XueYong Fu, Shashi Bhushan TN, Simon CorstonOliver |  |
| 40 |  |  [Q2R: A Query-to-Resolution System for Natural-Language Queries](https://doi.org/10.18653/v1/2022.naacl-industry.39) |  | 0 | We present a system for document retrieval that combines direct classification with standard content-based retrieval approaches to significantly improve the relevance of the retrieved documents. Our system exploits the availability of an imperfect but sizable amount of labeled data from past queries. For domains such as technical support, the proposed approach enhances the system’s ability to retrieve documents that are otherwise ranked very low based on content alone. The system is easy to... | Shiau Hong Lim, Laura Wynter |  |
| 41 |  |  [Identifying Corporate Credit Risk Sentiments from Financial News](https://doi.org/10.18653/v1/2022.naacl-industry.40) |  | 0 | Credit risk management is one central practice for financial institutions, and such practice helps them measure and understand the inherent risk within their portfolios. Historically, firms relied on the assessment of default probabilities and used the press as one tool to gather insights on the latest credit event developments of an entity. However, due to the deluge of the current news coverage for companies, analyzing news manually by financial experts is considered a highly laborious task.... | Noujoud Ahbali, Xinyuan Liu, Albert Nanda, Jamie Stark, Ashit Talukder, Rupinder Paul Khandpur |  |
| 42 |  |  [Frontmatter](https://aclanthology.org/2022.naacl-srw.0) |  | 0 |  |  |  |
| 43 |  |  [Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention](https://doi.org/10.18653/v1/2022.naacl-srw.1) |  | 0 | Systematicity is thought to be a key inductive bias possessed by humans that is lacking in standard natural language processing systems such as those utilizing transformers. In this work, we investigate the extent to which the failure of transformers on systematic generalization tests can be attributed to a lack of linguistic abstraction in its attention mechanism. We develop a novel modification to the transformer by implementing two separate input streams: a role stream controls the attention... | Ayush K. Chakravarthy, Jacob Labe Russin, Randall C. O'Reilly |  |
| 44 |  |  [Grounding in social media: An approach to building a chit-chat dialogue model](https://doi.org/10.18653/v1/2022.naacl-srw.2) |  | 0 | Building open-domain dialogue systems capable of rich human-like conversational ability is one of the fundamental challenges in language generation. However, even with recent advancements in the field, existing open-domain generative models fail to capture and utilize external knowledge, leading to repetitive or generic responses to unseen utterances. Current work on knowledge-grounded dialogue generation primarily focuses on persona incorporation or searching a fact-based structured knowledge... | Ritvik Choudhary, Daisuke Kawahara |  |
| 45 |  |  [ExtraPhrase: Efficient Data Augmentation for Abstractive Summarization](https://doi.org/10.18653/v1/2022.naacl-srw.3) |  | 0 | Neural models trained with large amount of parallel data have achieved impressive performance in abstractive summarization tasks. However, large-scale parallel corpora are expensive and challenging to construct. In this work, we introduce a low-cost and effective strategy, ExtraPhrase, to augment training data for abstractive summarization tasks. ExtraPhrase constructs pseudo training data in two steps: extractive summarization and paraphrasing. We extract major parts of an input text in the... | Mengsay Loem, Sho Takase, Masahiro Kaneko, Naoaki Okazaki |  |
| 46 |  |  [Regularized Training of Nearest Neighbor Language Models](https://doi.org/10.18653/v1/2022.naacl-srw.4) |  | 0 | Including memory banks in a natural language processing architecture increases model capacity by equipping it with additional data at inference time. In this paper, we build upon kNN-LM (CITATION), which uses a pre-trained language model together with an exhaustive kNN search through the training data (memory bank) to achieve state-of-the-art results. We investigate whether we can improve the kNN-LM performance by instead training a LM with the knowledge that we will be using a kNN post-hoc. We... | JeanFrancois Ton, Walter Talbott, Shuangfei Zhai, Joshua M. Susskind |  |
| 47 |  |  ["Again, Dozens of Refugees Drowned": A Computational Study of Political Framing Evoked by Presuppositions](https://doi.org/10.18653/v1/2022.naacl-srw.5) |  | 0 | Earlier NLP studies on framing in political discourse have focused heavily on shallow classification of issue framing, while framing effect arising from pragmatic cues remains neglected. We put forward this latter type of framing as “pragmatic framing”. To bridge this gap, we take presupposition-triggering adverbs such as ‘again’ as a study case, and quantitatively investigate how different German newspapers use them to covertly evoke different attitudinal subtexts in their report on the event... | Qi Yu |  |
| 48 |  |  [Methods for Estimating and Improving Robustness of Language Models](https://doi.org/10.18653/v1/2022.naacl-srw.6) |  | 0 | Despite their outstanding performance, large language models (LLMs) suffer notorious flaws related to their preference for shallow textual relations over full semantic complexity of the problem. This proposal investigates a common denominator of this problem in their weak ability to generalise outside of the training domain. We survey diverse research directions providing estimations of model generalisation ability and find that incorporating some of these measures in the training objectives... | Michal Stefánik |  |
| 49 |  |  [Retrieval-augmented Generation across Heterogeneous Knowledge](https://doi.org/10.18653/v1/2022.naacl-srw.7) |  | 0 | Retrieval-augmented generation (RAG) methods have been receiving increasing attention from the NLP community and achieved state-of-the-art performance on many NLP downstream tasks. Compared with conventional pre-trained generation models, RAG methods have remarkable advantages such as easy knowledge acquisition, strong scalability, and low training cost. Although existing RAG models have been applied to various knowledge-intensive NLP tasks, such as open-domain QA and dialogue systems, most of... | Wenhao Yu |  |
| 50 |  |  [Neural Retriever and Go Beyond: A Thesis Proposal](https://doi.org/10.18653/v1/2022.naacl-srw.8) |  | 0 | Information Retriever (IR) aims to find the relevant documents (e.g. snippets, passages, and articles) to a given query at large scale. IR plays an important role in many tasks such as open domain question answering and dialogue systems, where external knowledge is needed. In the past, searching algorithms based on term matching have been widely used. Recently, neural-based algorithms (termed as neural retrievers) have gained more attention which can mitigate the limitations of traditional... | Man Luo |  |
| 51 |  |  [Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation](https://doi.org/10.18653/v1/2022.naacl-srw.9) |  | 0 | Cognitive distortions are counterproductive patterns of thinking that are one of the targets of cognitive behavioral therapy (CBT). These can be challenging for clinicians to detect, especially those without extensive CBT training or supervision. Text classification methods can approximate expert clinician judgment in the detection of frequently occurring cognitive distortions in text-based therapy messages. However, performance with infrequent distortions is relatively poor. In this study, we... | Xiruo Ding, Kevin Lybarger, Justin Tauscher, Trevor Cohen |  |
| 52 |  |  [Generate, Evaluate, and Select: A Dialogue System with a Response Evaluator for Diversity-Aware Response Generation](https://doi.org/10.18653/v1/2022.naacl-srw.10) |  | 0 | We aim to overcome the lack of diversity in responses of current dialogue systems and to develop a dialogue system that is engaging as a conversational partner. We propose a generator-evaluator model that evaluates multiple responses generated by a response generator and selects the best response by an evaluator. By generating multiple responses, we obtain diverse responses. We conduct human evaluations to compare the output of the proposed system with that of a baseline system. The results of... | Ryoma Sakaeda, Daisuke Kawahara |  |
| 53 |  |  [Impact of Training Instance Selection on Domain-Specific Entity Extraction using BERT](https://doi.org/10.18653/v1/2022.naacl-srw.11) |  | 0 | State of the art performances for entity extraction tasks are achieved by supervised learning, specifically, by fine-tuning pretrained language models such as BERT. As a result, annotating application specific data is the first step in many use cases. However, no practical guidelines are available for annotation requirements. This work supports practitioners by empirically answering the frequently asked questions (1) how many training samples to annotate? (2) which examples to annotate? We... | Eileen Salhofer, Xinglan Liu, Roman Kern |  |
| 54 |  |  [Analysing the Correlation between Lexical Ambiguity and Translation Quality in a Multimodal Setting using WordNet](https://doi.org/10.18653/v1/2022.naacl-srw.12) |  | 0 | Multimodal Neural Machine Translation is focusing on using visual information to translate sentences in the source language into the target language. The main idea is to utilise information from visual modalities to promote the output quality of the text-based translation model. Although the recent multimodal strategies extract the most relevant visual information in images, the effectiveness of using visual information on translation quality changes based on the text dataset. Due to this, this... | Ali Hatami, Paul Buitelaar, Mihael Arcan |  |
| 55 |  |  [Building a Personalized Dialogue System with Prompt-Tuning](https://doi.org/10.18653/v1/2022.naacl-srw.13) |  | 0 | Dialogue systems without consistent responses are not attractive. In this study, we build a dialogue system that can respond based on a given character setting (persona) to bring consistency. Considering the trend of the rapidly increasing scale of language models, we propose an approach that uses prompt-tuning, which has low learning costs, on pre-trained large-scale language models. The results of the automatic and manual evaluations in English and Japanese show that it is possible to build a... | Tomohito Kasahara, Daisuke Kawahara, Nguyen Tung, Shengzhe Li, Kenta Shinzato, Toshinori Sato |  |
| 56 |  |  [MM-GATBT: Enriching Multimodal Representation Using Graph Attention Network](https://doi.org/10.18653/v1/2022.naacl-srw.14) |  | 0 | While there have been advances in Natural Language Processing (NLP), their success is mainly gained by applying a self-attention mechanism into single or multi-modalities. While this approach has brought significant improvements in multiple downstream tasks, it fails to capture the interaction between different entities. Therefore, we propose MM-GATBT, a multimodal graph representation learning model that captures not only the relational semantics within one modality but also the interactions... | Seung Byum Seo, Hyoungwook Nam, Payam Delgosha |  |
| 57 |  |  [Simulating Feature Structures with Simple Types](https://doi.org/10.18653/v1/2022.naacl-srw.15) |  | 0 | Feature structures have been several times considered to enrich categorial grammars in order to build fine-grained grammars. Most attempts to unify both frameworks either model categorial types as feature structures or add feature structures on top of categorial types. We pursue a different approach: using feature structure as categorial atomic types. In this article, we present a procedure to create, from a simplified HPSG grammar, an equivalent abstract categorial grammar (ACG). We represent... | Valentin D. Richard |  |
| 58 |  |  [Dr. Livingstone, I presume? Polishing of foreign character identification in literary texts](https://doi.org/10.18653/v1/2022.naacl-srw.16) |  | 0 | Character identification is a key element for many narrative-related tasks. To implement it, the baseform of the name of the character (or lemma) needs to be identified, so different appearances of the same character in the narrative could be aligned. In this paper we tackle this problem in translated texts (English–Finnish translation direction), where the challenge regarding lemmatizing foreign names in an agglutinative language appears. To solve this problem, we present and compare several... | Aleksandra Konovalova, Antonio Toral, Kristiina TaivalkoskiShilov |  |
| 59 |  |  [Zuo Zhuan Ancient Chinese Dataset for Word Sense Disambiguation](https://doi.org/10.18653/v1/2022.naacl-srw.17) |  | 0 | Word Sense Disambiguation (WSD) is a core task in Natural Language Processing (NLP). Ancient Chinese has rarely been used in WSD tasks, however, as no public dataset for ancient Chinese WSD tasks exists. Creation of an ancient Chinese dataset is considered a significant challenge because determining the most appropriate sense in a context is difficult and time-consuming owing to the different usages in ancient and modern Chinese. Actually, no public dataset for ancient Chinese WSD tasks exists.... | Xiaomeng Pan, Hongfei Wang, Teruaki Oka, Mamoru Komachi |  |
| 60 |  |  [ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation](https://doi.org/10.18653/v1/2022.naacl-srw.18) |  | 0 | We present ViT5, a pretrained Transformer-based encoder-decoder model for the Vietnamese language. With T5-style self-supervised pretraining, ViT5 is trained on a large corpus of high-quality and diverse Vietnamese texts. We benchmark ViT5 on two downstream text generation tasks, Abstractive Text Summarization and Named Entity Recognition. Although Abstractive Text Summarization has been widely studied for the English language thanks to its rich and large source of data, there has been minimal... | Long Phan, Hieu Tran, Hieu Nguyen, Trieu H. Trinh |  |
| 61 |  |  [Compositional Generalization in Grounded Language Learning via Induced Model Sparsity](https://doi.org/10.18653/v1/2022.naacl-srw.19) |  | 0 | We provide a study of how induced model sparsity can help achieve compositional generalization and better sample efficiency in grounded language learning problems. We consider simple language-conditioned navigation problems in a grid world environment with disentangled observations. We show that standard neural architectures do not always yield compositional generalization. To address this, we design an agent that contains a goal identification module that encourages sparse correlations between... | Sam Spilsbury, Alexander Ilin |  |
| 62 |  |  [How do people talk about images? A study on open-domain conversations with images](https://doi.org/10.18653/v1/2022.naacl-srw.20) |  | 0 | This paper explores how humans conduct conversations with images by investigating an open-domain image conversation dataset, ImageChat. We examined the conversations with images from the perspectives of image relevancy and image information. We found that utterances/conversations are not always related to the given image, and conversation topics diverge within three turns about half of the time. Besides image objects, more comprehensive non-object image information is also indispensable. After... | YiPei Chen, Nobuyuki Shimizu, Takashi Miyazaki, Hideki Nakayama |  |
| 63 |  |  [Text Style Transfer for Bias Mitigation using Masked Language Modeling](https://doi.org/10.18653/v1/2022.naacl-srw.21) |  | 0 | It is well known that textual data on the internet and other digital platforms contain significant levels of bias and stereotypes. Various research findings have concluded that biased texts have significant effects on target demographic groups. For instance, masculine-worded job advertisements tend to be less appealing to female applicants. In this paper, we present a text-style transfer model that can be trained on non-parallel data and be used to automatically mitigate bias in textual data.... | Ewoenam Kwaku Tokpo, Toon Calders |  |
| 64 |  |  [Differentially Private Instance Encoding against Privacy Attacks](https://doi.org/10.18653/v1/2022.naacl-srw.22) |  | 0 | TextHide was recently proposed to protect the training data via instance encoding in natural language domain. Due to the lack of theoretic privacy guarantee, such instance encoding scheme has been shown to be vulnerable against privacy attacks, e.g., reconstruction attack. To address such limitation, we revise the instance encoding scheme with differential privacy and thus provide a provable guarantee against privacy attacks. The experimental results also show that the proposed scheme can... | Shangyu Xie, Yuan Hong |  |
| 65 |  |  [A Simple Approach to Jointly Rank Passages and Select Relevant Sentences in the OBQA Context](https://doi.org/10.18653/v1/2022.naacl-srw.23) |  | 0 | In the open book question answering (OBQA) task, selecting the relevant passages and sentences from distracting information is crucial to reason the answer to a question. HotpotQA dataset is designed to teach and evaluate systems to do both passage ranking and sentence selection. Many existing frameworks use separate models to select relevant passages and sentences respectively. Such systems not only have high complexity in terms of the parameters of models but also fail to take the advantage... | Man Luo, Shuguang Chen, Chitta Baral |  |
| 66 |  |  [Multimodal Modeling of Task-Mediated Confusion](https://doi.org/10.18653/v1/2022.naacl-srw.24) |  | 0 | In order to build more human-like cognitive agents, systems capable of detecting various human emotions must be designed to respond appropriately. Confusion, the combination of an emotional and cognitive state, is under-explored. In this paper, we build upon prior work to develop models that detect confusion from three modalities: video (facial features), audio (prosodic features), and text (transcribed speech features). Our research improves the data collection process by allowing for... | Camille Mince, Skye Rhomberg, Cecilia O. Alm, Reynold Bailey, Alexander Ororbia |  |
| 67 |  |  [Probe-Less Probing of BERT's Layer-Wise Linguistic Knowledge with Masked Word Prediction](https://doi.org/10.18653/v1/2022.naacl-srw.25) |  | 0 | The current study quantitatively (and qualitatively for an illustrative purpose) analyzes BERT’s layer-wise masked word prediction on an English corpus, and finds that (1) the layerwise localization of linguistic knowledge primarily shown in probing studies is replicated in a behavior-based design and (2) that syntactic and semantic information is encoded at different layers for words of different syntactic categories. Hypothesizing that the above results are correlated with the number of... | Tatsuya Aoyama, Nathan Schneider |  |
| 68 |  |  [Multimodal large language models for inclusive collaboration learning tasks](https://doi.org/10.18653/v1/2022.naacl-srw.26) |  | 0 | This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to... | Armanda Lewis |  |
| 69 |  |  [Neural Networks in a Product of Hyperbolic Spaces](https://doi.org/10.18653/v1/2022.naacl-srw.27) |  | 0 | Machine learning in hyperbolic spaces has attracted much attention in natural language processing and many other fields. In particular, Hyperbolic Neural Networks (HNNs) have improved a wide variety of tasks, from machine translation to knowledge graph embedding. Although some studies have reported the effectiveness of embedding into the product of multiple hyperbolic spaces, HNNs have mainly been constructed in a single hyperbolic space, and their extension to product spaces has not been... | Jun Takeuchi, Noriki Nishida, Hideki Nakayama |  |
| 70 |  |  [Explicit Use of Topicality in Dialogue Response Generation](https://doi.org/10.18653/v1/2022.naacl-srw.28) |  | 0 | The current chat dialogue systems implicitly consider the topic given the context, but not explicitly. As a result, these systems often generate inconsistent responses with the topic of the moment. In this study, we propose a dialogue system that responds appropriately following the topic by selecting the entity with the highest “topicality.” In topicality estimation, the model is trained through self-supervised learning that regards entities that appear in both context and response as the... | Takumi Yoshikoshi, Hayato Atarashi, Takashi Kodama, Sadao Kurohashi |  |
| 71 |  |  [Automating Human Evaluation of Dialogue Systems](https://doi.org/10.18653/v1/2022.naacl-srw.29) |  | 0 | Automated metrics to evaluate dialogue systems like BLEU, METEOR, etc., weakly correlate with human judgments. Thus, human evaluation is often used to supplement these metrics for system evaluation. However, human evaluation is time-consuming as well as expensive. This paper provides an alternative approach to human evaluation with respect to three aspects: naturalness, informativeness, and quality in dialogue systems. I propose an approach based on fine-tuning the BERT model with three... | Sujan Reddy A |  |
| 72 |  |  [Strong Heuristics for Named Entity Linking](https://doi.org/10.18653/v1/2022.naacl-srw.30) |  | 0 | Named entity linking (NEL) in news is a challenging endeavour due to the frequency of unseen and emerging entities, which necessitates the use of unsupervised or zero-shot methods. However, such methods tend to come with caveats, such as no integration of suitable knowledge bases (like Wikidata) for emerging entities, a lack of scalability, and poor interpretability. Here, we consider person disambiguation in Quotebank, a massive corpus of speaker-attributed quotations from the news, and... | Marko Culjak, Andreas Spitz, Robert West, Akhil Arora |  |
| 73 |  |  [Static and Dynamic Speaker Modeling based on Graph Neural Network for Emotion Recognition in Conversation](https://doi.org/10.18653/v1/2022.naacl-srw.31) |  | 0 | Each person has a unique personality which affects how they feel and convey emotions. Hence, speaker modeling is important for the task of emotion recognition in conversation (ERC). In this paper, we propose a novel graph-based ERC model which considers both conversational context and speaker personality. We model the internal state of the speaker (personality) as Static and Dynamic speaker state, where the Dynamic speaker state is modeled with a graph neural network based encoder. Experiments... | Prakhar Saxena, Yin Jou Huang, Sadao Kurohashi |  |
| 74 |  |  [Few-shot fine-tuning SOTA summarization models for medical dialogues](https://doi.org/10.18653/v1/2022.naacl-srw.32) |  | 0 | Abstractive summarization of medical dialogues presents a challenge for standard training approaches, given the paucity of suitable datasets. We explore the performance of state-of-the-art models with zero-shot and few-shot learning strategies and measure the impact of pretraining with general domain and dialogue-specific text on the summarization performance. | David Fraile Navarro, Mark Dras, Shlomo Berkovsky |  |
| 75 |  |  [Unifying Parsing and Tree-Structured Models for Generating Sentence Semantic Representations](https://doi.org/10.18653/v1/2022.naacl-srw.33) |  | 0 | We introduce a novel tree-based model that learns its composition function together with its structure. The architecture produces sentence embeddings by composing words according to an induced syntactic tree. The parsing and the composition functions are explicitly connected and, therefore, learned jointly. As a result, the sentence embedding is computed according to an interpretable linguistic pattern and may be used on any downstream task. We evaluate our encoder on downstream tasks, and we... | Antoine Simoulin, Benoît Crabbé |  |
| 76 |  |  [Multiformer: A Head-Configurable Transformer-Based Model for Direct Speech Translation](https://doi.org/10.18653/v1/2022.naacl-srw.34) |  | 0 | Transformer-based models have been achieving state-of-the-art results in several fields of Natural Language Processing. However, its direct application to speech tasks is not trivial. The nature of this sequences carries problems such as long sequence lengths and redundancy between adjacent tokens. Therefore, we believe that regular self-attention mechanism might not be well suited for it. Different approaches have been proposed to overcome these problems, such as the use of efficient attention... | Gerard Sant, Gerard I. Gállego, Belen Alastruey, Marta Ruiz Costajussà |  |
| 77 |  |  [Defending Compositionality in Emergent Languages](https://doi.org/10.18653/v1/2022.naacl-srw.35) |  | 0 | Compositionality has traditionally been understood as a major factor in productivity of language and, more broadly, human cognition. Yet, recently some research started to question its status showing that artificial neural networks are good at generalization even without noticeable compositional behavior. We argue some of these conclusions are too strong and/or incomplete. In the context of a two-agent communication game, we show that compositionality indeed seems essential for successful... | Michal Auersperger, Pavel Pecina |  |
| 78 |  |  [Exploring the Effect of Dialect Mismatched Language Models in Telugu Automatic Speech Recognition](https://doi.org/10.18653/v1/2022.naacl-srw.36) |  | 0 | Previous research has found that Acoustic Models (AM) of an Automatic Speech Recognition (ASR) system are susceptible to dialect variations within a language, thereby adversely affecting the ASR. To counter this, researchers have proposed to build a dialect-specific AM while keeping the Language Model (LM) constant for all the dialects. This study explores the effect of dialect mismatched LM by considering three different Telugu regional dialects: Telangana, Coastal Andhra, and Rayalaseema. We... | Aditya Yadavalli, Mirishkar Sai Ganesh, Anil Kumar Vuppala |  |
| 79 |  |  [Frontmatter](https://aclanthology.org/2022.naacl-main.0) |  | 0 |  |  |  |
| 80 |  |  [Social Norms Guide Reference Resolution](https://doi.org/10.18653/v1/2022.naacl-main.1) |  | 0 | Humans use natural language, vision, and context to resolve referents in their environment. While some situated reference resolution is trivial, ambiguous cases arise when the language is underspecified or there are multiple candidate referents. This study investigates howpragmatic modulators external to the linguistic content are critical for the correct interpretation of referents in these scenarios. Inparticular, we demonstrate in a human subjects experiment how the social norms applicable... | Mitchell Abrams, Matthias Scheutz |  |
| 81 |  |  [Learning Natural Language Generation with Truncated Reinforcement Learning](https://doi.org/10.18653/v1/2022.naacl-main.2) |  | 0 | This paper introduces TRUncated ReinForcement Learning for Language (TrufLL), an original approach to train conditional languagemodels without a supervised learning phase, by only using reinforcement learning (RL). As RL methods unsuccessfully scale to large action spaces, we dynamically truncate the vocabulary space using a generic language model. TrufLL thus enables to train a language agent by solely interacting with its environment without any task-specific prior knowledge; it is only... | Alice Martin, Guillaume Quispe, Charles Ollion, Sylvain Le Corff, Florian Strub, Olivier Pietquin |  |
| 82 |  |  [Language Model Augmented Monotonic Attention for Simultaneous Translation](https://doi.org/10.18653/v1/2022.naacl-main.3) |  | 0 | The state-of-the-art adaptive policies for Simultaneous Neural Machine Translation (SNMT) use monotonic attention to perform read/write decisions based on the partial source and target sequences. The lack of sufficient information might cause the monotonic attention to take poor read/write decisions, which in turn negatively affects the performance of the SNMT model. On the other hand, human translators make better read/write decisions since they can anticipate the immediate future words using... | Sathish Reddy Indurthi, Mohd Abbas Zaidi, Beomseok Lee, Nikhil Kumar Lakumarapu, Sangha Kim |  |
| 83 |  |  [What Makes a Good and Useful Summary? Incorporating Users in Automatic Summarization Research](https://doi.org/10.18653/v1/2022.naacl-main.4) |  | 0 | Automatic text summarization has enjoyed great progress over the years and is used in numerous applications, impacting the lives of many. Despite this development, there is little research that meaningfully investigates how the current research focus in automatic summarization aligns with users’ needs. To bridge this gap, we propose a survey methodology that can be used to investigate the needs of users of automatically generated summaries. Importantly, these needs are dependent on the target... | Maartje ter Hoeve, Julia Kiseleva, Maarten de Rijke |  |
| 84 |  |  [ErAConD: Error Annotated Conversational Dialog Dataset for Grammatical Error Correction](https://doi.org/10.18653/v1/2022.naacl-main.5) |  | 0 | Currently available grammatical error correction (GEC) datasets are compiled using essays or other long-form text written by language learners, limiting the applicability of these datasets to other domains such as informal writing and conversational dialog. In this paper, we present a novel GEC dataset consisting of parallel original and corrected utterances drawn from open-domain chatbot conversations; this dataset is, to our knowledge, the first GEC dataset targeted to a human-machine... | Xun Yuan, Derek Pham, Sam Davidson, Zhou Yu |  |
| 85 |  |  [Semantic Diversity in Dialogue with Natural Language Inference](https://doi.org/10.18653/v1/2022.naacl-main.6) |  | 0 | Generating diverse, interesting responses to chitchat conversations is a problem for neural conversational agents. This paper makes two substantial contributions to improving diversity in dialogue generation. First, we propose a novel metric which uses Natural Language Inference (NLI) to measure the semantic diversity of a set of model responses for a conversation. We evaluate this metric using an established framework (Tevet and Berant, 2021) and find strong evidence indicating NLI Diversity... | Katherine Stasaski, Marti A. Hearst |  |
| 86 |  |  [LEA: Meta Knowledge-Driven Self-Attentive Document Embedding for Few-Shot Text Classification](https://doi.org/10.18653/v1/2022.naacl-main.7) |  | 0 | Text classification has achieved great success with the prosperity of deep learning and pre-trained language models. However, we often encounter labeled data deficiency problems in real-world text-classification tasks. To overcome such challenging scenarios, interest in few-shot learning has increased, whereas most few-shot text classification studies suffer from a difficulty of utilizing pre-trained language models. In the study, we propose a novel learning method for learning how to attend,... | S. K. Hong, Tae Young Jang |  |
| 87 |  |  [Enhancing Self-Attention with Knowledge-Assisted Attention Maps](https://doi.org/10.18653/v1/2022.naacl-main.8) |  | 0 | Large-scale pre-trained language models have attracted extensive attentions in the research community and shown promising results on various tasks of natural language processing. However, the attention maps, which record the attention scores between tokens in self-attention mechanism, are sometimes ineffective as they are learned implicitly without the guidance of explicit semantic knowledge. Thus, we aim to infuse explicit external knowledge into pre-trained language models to further boost... | Jiangang Bai, Yujing Wang, Hong Sun, Ruonan Wu, Tianmeng Yang, Pengfei Tang, Defu Cao, Mingliang Zhang, Yunhai Tong, Yaming Yang, Jing Bai, Ruofei Zhang, Hao Sun, Wei Shen |  |
| 88 |  |  [Batch-Softmax Contrastive Loss for Pairwise Sentence Scoring Tasks](https://doi.org/10.18653/v1/2022.naacl-main.9) |  | 0 | The use of contrastive loss for representation learning has become prominent in computer vision, and it is now getting attention in Natural Language Processing (NLP).Here, we explore the idea of using a batch-softmax contrastive loss when fine-tuning large-scale pre-trained transformer models to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss as well as in the overall training... | Anton Chernyavskiy, Dmitry Ilvovsky, Pavel Kalinin, Preslav Nakov |  |
| 89 |  |  [NewsEdits: A News Article Revision Dataset and a Novel Document-Level Reasoning Challenge](https://doi.org/10.18653/v1/2022.naacl-main.10) |  | 0 | News article revision histories provide clues to narrative and factual evolution in news articles. To facilitate analysis of this evolution, we present the first publicly available dataset of news revision histories, NewsEdits. Our dataset is large-scale and multilingual; it contains 1.2 million articles with 4.6 million versions from over 22 English- and French-language newspaper sources based in three countries, spanning 15 years of coverage (2006-2021).We define article-level edit actions:... | Alexander Spangher, Xiang Ren, Jonathan May, Nanyun Peng |  |
| 90 |  |  [Putting the Con in Context: Identifying Deceptive Actors in the Game of Mafia](https://doi.org/10.18653/v1/2022.naacl-main.11) |  | 0 | While neural networks demonstrate a remarkable ability to model linguistic content, capturing contextual information related to a speaker’s conversational role is an open area of research. In this work, we analyze the effect of speaker role on language use through the game of Mafia, in which participants are assigned either an honest or a deceptive role. In addition to building a framework to collect a dataset of Mafia game records, we demonstrate that there are differences in the language... | Samee Ibraheem, Gaoyue Zhou, John DeNero |  |
| 91 |  |  [SUBS: Subtree Substitution for Compositional Semantic Parsing](https://doi.org/10.18653/v1/2022.naacl-main.12) |  | 0 | Although sequence-to-sequence models often achieve good performance in semantic parsing for i.i.d. data, their performance is still inferior in compositional generalization. Several data augmentation methods have been proposed to alleviate this problem. However, prior work only leveraged superficial grammar or rules for data augmentation, which resulted in limited improvement. We propose to use subtree substitution for compositional data augmentation, where we consider subtrees with similar... | Jingfeng Yang, Le Zhang, Diyi Yang |  |
| 92 |  |  [Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks](https://doi.org/10.18653/v1/2022.naacl-main.13) |  | 0 | Labelled data is the foundation of most natural language processing tasks. However, labelling data is difficult and there often are diverse valid beliefs about what the correct data labels should be. So far, dataset creators have acknowledged annotator subjectivity, but rarely actively managed it in the annotation process. This has led to partly-subjective datasets that fail to serve a clear downstream use. To address this issue, we propose two contrasting paradigms for data annotation. The... | Paul Röttger, Bertie Vidgen, Dirk Hovy, Janet B. Pierrehumbert |  |
| 93 |  |  [Do Deep Neural Nets Display Human-like Attention in Short Answer Scoring?](https://doi.org/10.18653/v1/2022.naacl-main.14) |  | 0 | Deep Learning (DL) techniques have been increasingly adopted for Automatic Text Scoring in education. However, these techniques often suffer from their inabilities to explain and justify how a prediction is made, which, unavoidably, decreases their trustworthiness and hinders educators from embracing them in practice. This study aimed to investigate whether (and to what extent) DL-based graders align with human graders regarding the important words they identify when marking short answer... | Zijie Zeng, Xinyu Li, Dragan Gasevic, Guanliang Chen |  |
| 94 |  |  [Knowledge-Grounded Dialogue Generation with a Unified Knowledge Representation](https://doi.org/10.18653/v1/2022.naacl-main.15) |  | 0 | Knowledge-grounded dialogue systems are challenging to build due to the lack of training data and heterogeneous knowledge sources. Existing systems perform poorly on unseen topics due to limited topics covered in the training data. In addition, it is challenging to generalize to the domains that require different types of knowledge sources. To address the above challenges, we present PLUG, a language model that homogenizes different knowledge sources to a unified knowledge representation for... | Yu Li, Baolin Peng, Yelong Shen, Yi Mao, Lars Liden, Zhou Yu, Jianfeng Gao |  |
| 95 |  |  [CERES: Pretraining of Graph-Conditioned Transformer for Semi-Structured Session Data](https://doi.org/10.18653/v1/2022.naacl-main.16) |  | 0 | User sessions empower many search and recommendation tasks on a daily basis. Such session data are semi-structured, which encode heterogeneous relations between queries and products, and each item is described by the unstructured text. Despite recent advances in self-supervised learning for text or graphs, there lack of self-supervised learning models that can effectively capture both intra-item semantics and inter-item interactions for semi-structured sessions. To fill this gap, we propose... | Rui Feng, Chen Luo, Qingyu Yin, Bing Yin, Tuo Zhao, Chao Zhang |  |
| 96 |  |  [Political Ideology and Polarization: A Multi-dimensional Approach](https://doi.org/10.18653/v1/2022.naacl-main.17) |  | 0 | Analyzing ideology and polarization is of critical importance in advancing our grasp of modern politics. Recent research has made great strides towards understanding the ideological bias (i.e., stance) of news media along the left-right spectrum. In this work, we instead take a novel and more nuanced approach for the study of ideology based on its left or right positions on the issue being discussed. Aligned with the theoretical accounts in political science, we treat ideology as a... | Barea Sinno, Bernardo Oviedo, Katherine Atwell, Malihe Alikhani, Junyi Jessy Li |  |
| 97 |  |  [Cooperative Self-training of Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.naacl-main.18) |  | 0 | Pretrained language models have significantly improved the performance of downstream language understanding tasks, including extractive question answering, by providing high-quality contextualized word embeddings. However, training question answering models still requires large amounts of annotated data for specific domains. In this work, we propose a cooperative self-training framework, RGX, for automatically generating more non-trivial question-answer pairs to improve model performance. RGX... | Hongyin Luo, ShangWen Li, Mingye Gao, Seunghak Yu, James R. Glass |  |
| 98 |  |  [GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers](https://doi.org/10.18653/v1/2022.naacl-main.19) |  | 0 | There has been a growing interest in interpreting the underlying dynamics of Transformers. While self-attention patterns were initially deemed as the primary option, recent studies have shown that integrating other components can yield more accurate explanations. This paper introduces a novel token attribution analysis method that incorporates all the components in the encoder block and aggregates this throughout layers. Through extensive quantitative and qualitative experiments, we demonstrate... | Ali Modarressi, Mohsen Fayyaz, Yadollah Yaghoobzadeh, Mohammad Taher Pilehvar |  |
| 99 |  |  [A Robustly Optimized BMRC for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2022.naacl-main.20) |  | 0 | Aspect sentiment triplet extraction (ASTE) is a challenging subtask in aspect-based sentiment analysis. It aims to explore the triplets of aspects, opinions and sentiments with complex correspondence from the context. The bidirectional machine reading comprehension (BMRC), can effectively deal with ASTE task, but several problems remains, such as query conflict and probability unilateral decrease. Therefore, this paper presents a robustly optimized BMRC method by incorporating four... | Shu Liu, Kaiwen Li, Zuhe Li |  |
| 100 |  |  [Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds](https://doi.org/10.18653/v1/2022.naacl-main.21) |  | 0 | Discovering latent topics from text corpora has been studied for decades. Many existing topic models adopt a fully unsupervised setting, and their discovered topics may not cater to users’ particular interests due to their inability of leveraging user guidance. Although there exist seed-guided topic discovery approaches that leverage user-provided seeds to discover topic-representative terms, they are less concerned with two factors: (1) the existence of out-of-vocabulary seeds and (2) the... | Yu Zhang, Yu Meng, Xuan Wang, Sheng Wang, Jiawei Han |  |
| 101 |  |  [Towards Process-Oriented, Modular, and Versatile Question Generation that Meets Educational Needs](https://doi.org/10.18653/v1/2022.naacl-main.22) |  | 0 | NLP-powered automatic question generation (QG) techniques carry great pedagogical potential of saving educators’ time and benefiting student learning. Yet, QG systems have not been widely adopted in classrooms to date. In this work, we aim to pinpoint key impediments and investigate how to improve the usability of automatic QG techniques for educational purposes by understanding how instructors construct questions and identifying touch points to enhance the underlying NLP models. We perform an... | Xu Wang, Simin Fan, Jessica Houghton, Lu Wang |  |
| 102 |  |  [SwahBERT: Language Model of Swahili](https://doi.org/10.18653/v1/2022.naacl-main.23) |  | 0 | The rapid development of social networks, electronic commerce, mobile Internet, and other technologies, has influenced the growth of Web data. Social media and Internet forums are valuable sources of citizens’ opinions, which can be analyzed for community development and user behavior analysis. Unfortunately, the scarcity of resources (i.e., datasets or language models) become a barrier to the development of natural language processing applications in low-resource languages. Thanks to the... | Gati L. Martin, Medard Edmund Mswahili, YoungSeob Jeong, Jiyoung Woo |  |
| 103 |  |  [Deconstructing NLG Evaluation: Evaluation Practices, Assumptions, and Their Implications](https://doi.org/10.18653/v1/2022.naacl-main.24) |  | 0 | There are many ways to express similar things in text, which makes evaluating natural language generation (NLG) systems difficult. Compounding this difficulty is the need to assess varying quality criteria depending on the deployment setting. While the landscape of NLG evaluation has been well-mapped, practitioners’ goals, assumptions, and constraints—which inform decisions about what, when, and how to evaluate—are often partially or implicitly stated, or not stated at all. Combining a... | Kaitlyn Zhou, Su Lin Blodgett, Adam Trischler, Hal Daumé III, Kaheer Suleman, Alexandra Olteanu |  |
| 104 |  |  [TSTR: Too Short to Represent, Summarize with Details! Intro-Guided Extended Summary Generation](https://doi.org/10.18653/v1/2022.naacl-main.25) |  | 0 | Many scientific papers such as those in arXiv and PubMed data collections have abstracts with varying lengths of 50-1000 words and average length of approximately 200 words, where longer abstracts typically convey more information about the source paper. Up to recently, scientific summarization research has typically focused on generating short, abstract-like summaries following the existing datasets used for scientific summarization. In domains where the source text is relatively long-form,... | Sajad Sotudeh, Nazli Goharian |  |
| 105 |  |  [Empathic Machines: Using Intermediate Features as Levers to Emulate Emotions in Text-To-Speech Systems](https://doi.org/10.18653/v1/2022.naacl-main.26) |  | 0 | We present a method to control the emotional prosody of Text to Speech (TTS) systems by using phoneme-level intermediate features (pitch, energy, and duration) as levers. As a key idea, we propose Differential Scaling (DS) to disentangle features relating to affective prosody from those arising due to acoustics conditions and speaker identity. With thorough experimental studies, we show that the proposed method improves over the prior art in accurately emulating the desired emotions while... | Saiteja Kosgi, Sarath Sivaprasad, Niranjan Pedanekar, Anil Nelakanti, Vineet Gandhi |  |
| 106 |  |  [The Why and The How: A Survey on Natural Language Interaction in Visualization](https://doi.org/10.18653/v1/2022.naacl-main.27) |  | 0 | Natural language as a modality of interaction is becoming increasingly popular in the field of visualization. In addition to the popular query interfaces, other language-based interactions such as annotations, recommendations, explanations, or documentation experience growing interest. In this survey, we provide an overview of natural language-based interaction in the research area of visualization. We discuss a renowned taxonomy of visualization tasks and classify 119 related works to... | Henrik Voigt, Özge Alaçam, Monique Meuschke, Kai Lawonn, Sina Zarrieß |  |
| 107 |  |  [Understand before Answer: Improve Temporal Reading Comprehension via Precise Question Understanding](https://doi.org/10.18653/v1/2022.naacl-main.28) |  | 0 | This work studies temporal reading comprehension (TRC), which reads a free-text passage and answers temporal ordering questions. Precise question understanding is critical for temporal reading comprehension. For example, the question “What happened before the victory” and “What happened after the victory” share almost all words except one, while their answers are totally different. Moreover, even if two questions query about similar temporal relations, different varieties might also lead to... | Hao Huang, Xiubo Geng, Guodong Long, Daxin Jiang |  |
| 108 |  |  [User-Driven Research of Medical Note Generation Software](https://doi.org/10.18653/v1/2022.naacl-main.29) |  | 0 | A growing body of work uses Natural Language Processing (NLP) methods to automatically generate medical notes from audio recordings of doctor-patient consultations. However, there are very few studies on how such systems could be used in clinical practice, how clinicians would adjust to using them, or how system design should be influenced by such considerations. In this paper, we present three rounds of user studies, carried out in the context of developing a medical note generation system. We... | Tom Knoll, Francesco Moramarco, Alex PapadopoulosKorfiatis, Rachel Young, Claudia Ruffini, Mark Perera, Christian Perstl, Ehud Reiter, Anya Belz, Aleksandar Savkov |  |
| 109 |  |  [Ask Me Anything in Your Native Language](https://doi.org/10.18653/v1/2022.naacl-main.30) |  | 0 | Cross-lingual question answering is a thriving field in the modern world, helping people to search information on the web more efficiently. One of the important scenarios is to give an answer even there is no answer in the language a person asks a question with. We present a novel approach based on single encoder for query and passage for retrieval from multi-lingual collection, together with cross-lingual generative reader. It achieves a new state of the art in both retrieval and end-to-end... | Nikita Sorokin, Dmitry Abulkhanov, Irina Piontkovskaya, Valentin Malykh |  |
| 110 |  |  [Diversifying Neural Dialogue Generation via Negative Distillation](https://doi.org/10.18653/v1/2022.naacl-main.31) |  | 0 | Generative dialogue models suffer badly from the generic response problem, limiting their applications to a few toy scenarios. Recently, an interesting approach, namely negative training, has been proposed to alleviate this problem by reminding the model not to generate high-frequency responses during training. However, its performance is hindered by two issues, ignoring low-frequency but generic responses and bringing low-frequency but meaningless responses. In this paper, we propose a novel... | Yiwei Li, Shaoxiong Feng, Bin Sun, Kan Li |  |
| 111 |  |  [On Synthetic Data for Back Translation](https://doi.org/10.18653/v1/2022.naacl-main.32) |  | 0 | Back translation (BT) is one of the most significant technologies in NMT research fields. Existing attempts on BT share a common characteristic: they employ either beam search or random sampling to generate synthetic data with a backward model but seldom work studies the role of synthetic data in the performance of BT. This motivates us to ask a fundamental question: what kind of synthetic data contributes to BT performance?Through both theoretical and empirical studies, we identify two key... | Jiahao Xu, Yubin Ruan, Wei Bi, Guoping Huang, Shuming Shi, Lihui Chen, Lemao Liu |  |
| 112 |  |  [Mapping the Design Space of Human-AI Interaction in Text Summarization](https://doi.org/10.18653/v1/2022.naacl-main.33) |  | 0 | Automatic text summarization systems commonly involve humans for preparing data or evaluating model performance, yet, there lacks a systematic understanding of humans’ roles, experience, and needs when interacting with or being assisted by AI. From a human-centered perspective, we map the design opportunities and considerations for human-AI interaction in text summarization and broader text generation tasks. We first conducted a systematic literature review of 70 papers, developing a taxonomy... | Ruijia Cheng, Alison SmithRenner, Ke Zhang, Joel R. Tetreault, Alejandro JaimesLarrarte |  |
| 113 |  |  [Towards Robust and Semantically Organised Latent Representations for Unsupervised Text Style Transfer](https://doi.org/10.18653/v1/2022.naacl-main.34) |  | 0 | Recent studies show that auto-encoder based approaches successfully perform language generation, smooth sentence interpolation, and style transfer over unseen attributes using unlabelled datasets in a zero-shot manner. The latent space geometry of such models is organised well enough to perform on datasets where the style is “coarse-grained” i.e. a small fraction of words alone in a sentence are enough to determine the overall style label. A recent study uses a discrete token-based perturbation... | Sharan Narasimhan, Suvodip Dey, Maunendra Sankar Desarkar |  |
| 114 |  |  [An Exploration of Post-Editing Effectiveness in Text Summarization](https://doi.org/10.18653/v1/2022.naacl-main.35) |  | 0 | Automatic summarization methods are efficient but can suffer from low quality. In comparison, manual summarization is expensive but produces higher quality. Can humans and AI collaborate to improve summarization performance? In similar text generation tasks (e.g., machine translation), human-AI collaboration in the form of “post-editing” AI-generated text reduces human workload and improves the quality of AI output. Therefore, we explored whether post-editing offers advantages in text... | Vivian Lai, Alison SmithRenner, Ke Zhang, Ruijia Cheng, Wenjuan Zhang, Joel R. Tetreault, Alejandro JaimesLarrarte |  |
| 115 |  |  [Automatic Correction of Human Translations](https://doi.org/10.18653/v1/2022.naacl-main.36) |  | 0 | We introduce translation error correction (TEC), the task of automatically correcting human-generated translations. Imperfections in machine translations (MT) have long motivated systems for improving translations post-hoc with automatic post-editing. In contrast, little attention has been devoted to the problem of automatically correcting human translations, despite the intuition that humans make distinct errors that machines would be well-suited to assist with, from typos to inconsistencies... | Jessy Lin, Geza Kovacs, Aditya Shastry, Joern Wuebker, John DeNero |  |
| 116 |  |  [On the Robustness of Reading Comprehension Models to Entity Renaming](https://doi.org/10.18653/v1/2022.naacl-main.37) |  | 0 | We study the robustness of machine reading comprehension (MRC) models to entity renaming—do models make more wrong predictions when the same questions are asked about an entity whose name has been changed? Such failures imply that models overly rely on entity information to answer questions, and thus may generalize poorly when facts about the world change or questions are asked about novel entities. To systematically audit this issue, we present a pipeline to automatically generate test... | Jun Yan, Yang Xiao, Sagnik Mukherjee, Bill Yuchen Lin, Robin Jia, Xiang Ren |  |
| 117 |  |  [Explaining Why: How Instructions and User Interfaces Impact Annotator Rationales When Labeling Text Data](https://doi.org/10.18653/v1/2022.naacl-main.38) |  | 0 | In the context of data labeling, NLP researchers are increasingly interested in having humans select rationales, a subset of input tokens relevant to the chosen label. We conducted a 332-participant online user study to understand how humans select rationales, especially how different instructions and user interface affordances impact the rationales chosen. Participants labeled ten movie reviews as positive or negative, selecting words and phrases supporting their label as rationales. We varied... | Jamar L. Sullivan Jr., William Brackenbury, Andrew McNut, Kevin Bryson, Kwam Byll, Yuxin Chen, Michael L. Littman, Chenhao Tan, Blase Ur |  |
| 118 |  |  [Fine-tuning Pre-trained Language Models for Few-shot Intent Detection: Supervised Pre-training and Isotropization](https://doi.org/10.18653/v1/2022.naacl-main.39) |  | 0 | It is challenging to train a good intent classifier for a task-oriented dialogue system with only a few annotations. Recent studies have shown that fine-tuning pre-trained language models with a small set of labeled utterances from public benchmarks in a supervised manner is extremely helpful. However, we find that supervised pre-training yields an anisotropic feature space, which may suppress the expressive power of the semantic representations. Inspired by recent research in isotropization,... | Haode Zhang, Haowen Liang, Yuwei Zhang, LiMing Zhan, XiaoMing Wu, Xiaolei Lu, Albert Y. S. Lam |  |
| 119 |  |  [Cross-document Misinformation Detection based on Event Graph Reasoning](https://doi.org/10.18653/v1/2022.naacl-main.40) |  | 0 | For emerging events, human readers are often exposed to both real news and fake news. Multiple news articles may contain complementary or contradictory information that readers can leverage to help detect fake news. Inspired by this process, we propose a novel task of cross-document misinformation detection. Given a cluster of topically related news documents, we aim to detect misinformation at both document level and a more fine-grained level, event level. Due to the lack of data, we generate... | Xueqing Wu, KungHsiang Huang, Yi R. Fung, Heng Ji |  |
| 120 |  |  [Disentangled Action Recognition with Knowledge Bases](https://doi.org/10.18653/v1/2022.naacl-main.41) |  | 0 | Action in video usually involves the interaction of human with objects. Action labels are typically composed of various combinations of verbs and nouns, but we may not have training data for all possible combinations. In this paper, we aim to improve the generalization ability of the compositional action recognition model to novel verbs or novel nouns that are unseen during training time, by leveraging the power of knowledge graphs. Previous work utilizes verb-noun compositional action nodes in... | Zhekun Luo, Shalini Ghosh, Devin Guillory, Keizo Kato, Trevor Darrell, Huijuan Xu |  |
| 121 |  |  [Machine-in-the-Loop Rewriting for Creative Image Captioning](https://doi.org/10.18653/v1/2022.naacl-main.42) |  | 0 | Machine-in-the-loop writing aims to build models that assist humans to accomplish their writing tasks more effectively. Prior work has found that providing users a machine-written draft or sentence-level continuations has limited success since the generated text tends to deviate from users’ intention. To allow the user to retain control over the content, we train a rewriting model that, when prompted, modifies specified spans of text within the user’s original draft to introduce descriptive and... | Vishakh Padmakumar, He He |  |
| 122 |  |  [A Word is Worth A Thousand Dollars: Adversarial Attack on Tweets Fools Stock Prediction](https://doi.org/10.18653/v1/2022.naacl-main.43) |  | 0 | More and more investors and machine learning models rely on social media (e.g., Twitter and Reddit) to gather information and predict movements stock prices. Although text-based models are known to be vulnerable to adversarial attacks, whether stock prediction models have similar vulnerability given necessary constraints is underexplored. In this paper, we experiment with a variety of adversarial attack configurations to fool three stock prediction victim models. We address the task of... | Yong Xie, Dakuo Wang, PinYu Chen, Jinjun Xiong, Sijia Liu, Oluwasanmi Koyejo |  |
| 123 |  |  [Building Multilingual Machine Translation Systems That Serve Arbitrary XY Translations](https://doi.org/10.18653/v1/2022.naacl-main.44) |  | 0 | Multilingual Neural Machine Translation (MNMT) enables one system to translate sentences from multiple source languages to multiple target languages, greatly reducing deployment costs compared with conventional bilingual systems. The MNMT training benefit, however, is often limited to many-to-one directions. The model suffers from poor performance in one-to-many and many-to-many with zero-shot setup. To address this issue, this paper discusses how to practically build MNMT systems that serve... | Akiko Eriguchi, Shufang Xie, Tao Qin, Hany Hassan |  |
| 124 |  |  [Non-Autoregressive Neural Machine Translation with Consistency Regularization Optimized Variational Framework](https://doi.org/10.18653/v1/2022.naacl-main.45) |  | 0 | Variational Autoencoder (VAE) is an effective framework to model the interdependency for non-autoregressive neural machine translation (NAT). One of the prominent VAE-based NAT frameworks, LaNMT, achieves great improvements to vanilla models, but still suffers from two main issues which lower down the translation quality: (1) mismatch between training and inference circumstances and (2) inadequacy of latent representations. In this work, we target on addressing these issues by proposing... | Minghao Zhu, Junli Wang, Chungang Yan |  |
| 125 |  |  [User-Centric Gender Rewriting](https://doi.org/10.18653/v1/2022.naacl-main.46) |  | 0 | In this paper, we define the task of gender rewriting in contexts involving two users (I and/or You) – first and second grammatical persons with independent grammatical gender preferences. We focus on Arabic, a gender-marking morphologically rich language. We develop a multi-step system that combines the positive aspects of both rule-based and neural rewriting models. Our results successfully demonstrate the viability of this approach on a recently created corpus for Arabic gender rewriting,... | Bashar Alhafni, Nizar Habash, Houda Bouamor |  |
| 126 |  |  [Reframing Human-AI Collaboration for Generating Free-Text Explanations](https://doi.org/10.18653/v1/2022.naacl-main.47) |  | 0 | Large language models are increasingly capable of generating fluent-appearing text with relatively little task-specific supervision. But can these models accurately explain classification decisions? We consider the task of generating free-text explanations using human-written examples in a few-shot manner. We find that (1) authoring higher quality prompts results in higher quality generations; and (2) surprisingly, in a head-to-head comparison, crowdworkers often prefer explanations generated... | Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark O. Riedl, Yejin Choi |  |
| 127 |  |  [EmRel: Joint Representation of Entities and Embedded Relations for Multi-triple Extraction](https://doi.org/10.18653/v1/2022.naacl-main.48) |  | 0 | Multi-triple extraction is a challenging task due to the existence of informative inter-triple correlations, and consequently rich interactions across the constituent entities and relations. While existing works only explore entity representations, we propose to explicitly introduce relation representation, jointly represent it with entities, and novelly align them to identify valid triples.We perform comprehensive experiments on document-level relation extraction and joint entity and relation... | Benfeng Xu, Quan Wang, Yajuan Lyu, Yabing Shi, Yong Zhu, Jie Gao, Zhendong Mao |  |
| 128 |  |  [Meta Learning for Natural Language Processing: A Survey](https://doi.org/10.18653/v1/2022.naacl-main.49) |  | 0 | Deep learning has been the mainstream technique in the natural language processing (NLP) area. However, deep learning requires many labeled data and is less generalizable across domains. Meta-learning is an arising field in machine learning. It studies approaches to learning better learning algorithms and aims to improve algorithms in various aspects, including data efficiency and generalizability. The efficacy of meta-learning has been shown in many NLP tasks, but there is no systematic survey... | Hungyi Lee, ShangWen Li, Thang Vu |  |
| 129 |  |  [Analyzing Modality Robustness in Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2022.naacl-main.50) |  | 0 | Building robust multimodal models are crucial for achieving reliable deployment in the wild. Despite its importance, less attention has been paid to identifying and improving the robustness of Multimodal Sentiment Analysis (MSA) models. In this work, we hope to address that by (i) Proposing simple diagnostic checks for modality robustness in a trained multimodal model. Using these checks, we find MSA models to be highly sensitive to a single modality, which creates issues in their robustness;... | Devamanyu Hazarika, Yingting Li, Bo Cheng, Shuai Zhao, Roger Zimmermann, Soujanya Poria |  |
| 130 |  |  [Fuse It More Deeply! A Variational Transformer with Layer-Wise Latent Variable Inference for Text Generation](https://doi.org/10.18653/v1/2022.naacl-main.51) |  | 0 | The past several years have witnessed Variational Auto-Encoder’s superiority in various text generation tasks. However, due to the sequential nature of the text, auto-regressive decoders tend to ignore latent variables and then reduce to simple language models, known as the KL vanishing problem, which would further deteriorate when VAE is combined with Transformer-based structures. To ameliorate this problem, we propose Della, a novel variational Transformer framework. Della learns a series of... | Jinyi Hu, Xiaoyuan Yi, Wenhao Li, Maosong Sun, Xing Xie |  |
| 131 |  |  [Easy Adaptation to Mitigate Gender Bias in Multilingual Text Classification](https://doi.org/10.18653/v1/2022.naacl-main.52) |  | 0 | Existing approaches to mitigate demographic biases evaluate on monolingual data, however, multilingual data has not been examined. In this work, we treat the gender as domains (e.g., male vs. female) and present a standard domain adaptation model to reduce the gender bias and improve performance of text classifiers under multilingual settings. We evaluate our approach on two text classification tasks, hate speech detection and rating prediction, and demonstrate the effectiveness of our approach... | Xiaolei Huang |  |
| 132 |  |  [On the Use of External Data for Spoken Named Entity Recognition](https://doi.org/10.18653/v1/2022.naacl-main.53) |  | 0 | Spoken language understanding (SLU) tasks involve mapping from speech signals to semantic labels. Given the complexity of such tasks, good performance is expected to require large labeled datasets, which are difficult to collect for each new task and domain. However, recent advances in self-supervised speech representations have made it feasible to consider learning SLU models with limited labeled data. In this work, we focus on low-resource spoken named entity recognition (NER) and address the... | Ankita Pasad, Felix Wu, Suwon Shon, Karen Livescu, Kyu Jeong Han |  |
| 133 |  |  [Long-term Control for Dialogue Generation: Methods and Evaluation](https://doi.org/10.18653/v1/2022.naacl-main.54) |  | 0 | Current approaches for controlling dialogue response generation are primarily focused on high-level attributes like style, sentiment, or topic. In this work, we focus on constrained long-term dialogue generation, which involves more fine-grained control and requires a given set of control words to appear in generated responses. This setting requires a model to not only consider the generation of these control words in the immediate context, but also produce utterances that will encourage the... | Ramya Ramakrishnan, Hashan Buddhika Narangodage, Mauro Schilman, Kilian Q. Weinberger, Ryan McDonald |  |
| 134 |  |  [Learning Dialogue Representations from Consecutive Utterances](https://doi.org/10.18653/v1/2022.naacl-main.55) |  | 0 | Learning high-quality dialogue representations is essential for solving a variety of dialogue-oriented tasks, especially considering that dialogue systems often suffer from data scarcity. In this paper, we introduce Dialogue Sentence Embedding (DSE), a self-supervised contrastive learning method that learns effective dialogue representations suitable for a wide range of dialogue tasks. DSE learns from dialogues by taking consecutive utterances of the same dialogue as positive pairs for... | Zhihan Zhou, Dejiao Zhang, Wei Xiao, Nicholas Dingwall, Xiaofei Ma, Andrew O. Arnold, Bing Xiang |  |
| 135 |  |  [On the Machine Learning of Ethical Judgments from Natural Language](https://doi.org/10.18653/v1/2022.naacl-main.56) |  | 0 | Ethics is one of the longest standing intellectual endeavors of humanity. In recent years, the fields of AI and NLP have attempted to address issues of harmful outcomes in machine learning systems that are made to interface with humans. One recent approach in this vein is the construction of NLP morality models that can take in arbitrary text and output a moral judgment about the situation described. In this work, we offer a critique of such NLP methods for automating ethical decision-making.... | Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, Adina Williams |  |
| 136 |  |  [NeuroLogic A\*esque Decoding: Constrained Text Generation with Lookahead Heuristics](https://doi.org/10.18653/v1/2022.naacl-main.57) |  | 0 | The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. Drawing inspiration from the A\* search algorithm, we propose NeuroLogic A\*esque, a decoding algorithm that incorporates heuristic estimates of future cost. We develop lookahead heuristics that are efficient for large-scale language models, making... | Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, Yejin Choi |  |
| 137 |  |  [PARADISE: Exploiting Parallel Data for Multilingual Sequence-to-Sequence Pretraining](https://doi.org/10.18653/v1/2022.naacl-main.58) |  | 0 | Despite the success of multilingual sequence-to-sequence pretraining, most existing approaches rely on monolingual corpora and do not make use of the strong cross-lingual signal contained in parallel data. In this paper, we present PARADISE (PARAllel &Denoising Integration in SEquence-to-sequence models), which extends the conventional denoising objective used to train these models by (i) replacing words in the noised sequence according to a multilingual dictionary, and (ii) predicting the... | Machel Reid, Mikel Artetxe |  |
| 138 |  |  [Explaining Toxic Text via Knowledge Enhanced Text Generation](https://doi.org/10.18653/v1/2022.naacl-main.59) |  | 0 | Warning: This paper contains content that is offensive and may be upsetting. Biased or toxic speech can be harmful to various demographic groups. Therefore, it is not only important for models to detect these speech, but to also output explanations of why a given text is toxic. Previous literature has mostly focused on classifying and detecting toxic speech, and existing efforts on explaining stereotypes in toxic speech mainly use standard text generation approaches, resulting in generic and... | Rohit Sridhar, Diyi Yang |  |
| 139 |  |  [Teaching BERT to Wait: Balancing Accuracy and Latency for Streaming Disfluency Detection](https://doi.org/10.18653/v1/2022.naacl-main.60) |  | 0 | In modern interactive speech-based systems, speech is consumed and transcribed incrementally prior to having disfluencies removed. While this post-processing step is crucial for producing clean transcripts and high performance on downstream tasks (e.g. machine translation), most current state-of-the-art NLP models such as the Transformer operate non-incrementally, potentially causing unacceptable delays for the user. In this work we propose a streaming BERT-based sequence tagging model that,... | Angelica Chen, Vicky Zayats, Daniel D. Walker, Dirk Padfield |  |
| 140 |  |  [GRAM: Fast Fine-tuning of Pre-trained Language Models for Content-based Collaborative Filtering](https://doi.org/10.18653/v1/2022.naacl-main.61) |  | 0 | Content-based collaborative filtering (CCF) predicts user-item interactions based on both users’ interaction history and items’ content information. Recently, pre-trained language models (PLM) have been used to extract high-quality item encodings for CCF. However, it is resource-intensive to train a PLM-based CCF model in an end-to-end (E2E) manner, since optimization involves back-propagating through every content encoding within a given user interaction sequence. To tackle this issue, we... | Yoonseok Yang, Kyu Seok Kim, Minsam Kim, Juneyoung Park |  |
| 141 |  |  [Generating Repetitions with Appropriate Repeated Words](https://doi.org/10.18653/v1/2022.naacl-main.62) |  | 0 | A repetition is a response that repeats words in the previous speaker’s utterance in a dialogue. Repetitions are essential in communication to build trust with others, as investigated in linguistic studies. In this work, we focus on repetition generation. To the best of our knowledge, this is the first neural approach to address repetition generation. We propose Weighted Label Smoothing, a smoothing method for explicitly learning which words to repeat during fine-tuning, and a repetition... | Toshiki Kawamoto, Hidetaka Kamigaito, Kotaro Funakoshi, Manabu Okumura |  |
| 142 |  |  [Textless Speech-to-Speech Translation on Real Data](https://doi.org/10.18653/v1/2022.naacl-main.63) |  | 0 | We present a textless speech-to-speech translation (S2ST) system that can translate speech from one language into another language and can be built without the need of any text data. Different from existing work in the literature, we tackle the challenge in modeling multi-speaker target speech and train the systems with real-world S2ST data. The key to our approach is a self-supervised unit-based speech normalization technique, which finetunes a pre-trained speech encoder with paired audios... | Ann Lee, Hongyu Gong, PaulAmbroise Duquenne, Holger Schwenk, PengJen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Miguel Pino, Jiatao Gu, WeiNing Hsu |  |
| 143 |  |  [WALNUT: A Benchmark on Semi-weakly Supervised Learning for Natural Language Understanding](https://doi.org/10.18653/v1/2022.naacl-main.64) |  | 0 | Building machine learning models for natural language understanding (NLU) tasks relies heavily on labeled data. Weak supervision has been proven valuable when large amount of labeled data is unavailable or expensive to obtain. Existing works studying weak supervision for NLU either mostly focus on a specific task or simulate weak supervision signals from ground-truth labels. It is thus hard to compare different approaches and evaluate the benefit of weak supervision without access to a unified... | Guoqing Zheng, Giannis Karamanolakis, Kai Shu, Ahmed Hassan Awadallah |  |
| 144 |  |  [CompactIE: Compact Facts in Open Information Extraction](https://doi.org/10.18653/v1/2022.naacl-main.65) |  | 0 | A major drawback of modern neural OpenIE systems and benchmarks is that they prioritize high coverage of information in extractions over compactness of their constituents. This severely limits the usefulness of OpenIE extractions in many downstream tasks. The utility of extractions can be improved if extractions are compact and share constituents. To this end, we study the problem of identifying compact extractions with neural-based methods. We propose CompactIE, an OpenIE system that uses a... | Farima Fatahi Bayat, Nikita Bhutani, H. V. Jagadish |  |
| 145 |  |  [CoSIm: Commonsense Reasoning for Counterfactual Scene Imagination](https://doi.org/10.18653/v1/2022.naacl-main.66) |  | 0 | As humans, we can modify our assumptions about a scene by imagining alternative objects or concepts in our minds. For example, we can easily anticipate the implications of the sun being overcast by rain clouds (e.g., the street will get wet) and accordingly prepare for that. In this paper, we introduce a new dataset called Commonsense Reasoning for Counterfactual Scene Imagination (CoSIm) which is designed to evaluate the ability of AI systems to reason about scene change imagination. To be... | Hyounghun Kim, Abhay Zala, Mohit Bansal |  |
| 146 |  |  [Abstraction not Memory: BERT and the English Article System](https://doi.org/10.18653/v1/2022.naacl-main.67) |  | 0 | Article prediction is a task that has long defied accurate linguistic description. As such, this task is ideally suited to evaluate models on their ability to emulate native-speaker intuition. To this end, we compare the performance of native English speakers and pre-trained models on the task of article prediction set up as a three way choice (a/an, the, zero). Our experiments with BERT show that BERT outperforms humans on this task across all articles. In particular, BERT is far superior to... | Harish Tayyar Madabushi, Dagmar Divjak, Petar Milin |  |
| 147 |  |  [OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering](https://doi.org/10.18653/v1/2022.naacl-main.68) |  | 0 | The information in tables can be an important complement to text, making table-based question answering (QA) systems of great value. The intrinsic complexity of handling tables often adds an extra burden to both model design and data annotation. In this paper, we aim to develop a simple table-based QA model with minimal annotation effort. Motivated by the fact that table-based QA requires both alignment between questions and tables and the ability to perform complicated reasoning over multiple... | Zhengbao Jiang, Yi Mao, Pengcheng He, Graham Neubig, Weizhu Chen |  |
| 148 |  |  [Provably Confidential Language Modelling](https://doi.org/10.18653/v1/2022.naacl-main.69) |  | 0 | Large language models are shown to memorize privacy information such as social security numbers in training data. Given the sheer scale of the training corpus, it is challenging to screen and filter these privacy data, either manually or automatically. In this paper, we propose Confidentially Redacted Training (CRT), a method to train language generation models while protecting the confidential segments. We borrow ideas from differential privacy (which solves a related but distinct problem) and... | Xuandong Zhao, Lei Li, YuXiang Wang |  |
| 149 |  |  [KAT: A Knowledge Augmented Transformer for Vision-and-Language](https://doi.org/10.18653/v1/2022.naacl-main.70) |  | 0 | The primary focus of recent work with large-scale transformers has been on optimizing the amount of information packed into the model’s parameters. In this work, we ask a complementary question: Can multimodal transformers leverage explicit knowledge in their reasoning? Existing, primarily unimodal, methods have explored approaches under the paradigm of knowledge retrieval followed by answer prediction, but leave open questions about the quality and relevance of the retrieved knowledge used,... | Liangke Gui, Borui Wang, Qiuyuan Huang, Alexander Hauptmann, Yonatan Bisk, Jianfeng Gao |  |
| 150 |  |  [When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it](https://doi.org/10.18653/v1/2022.naacl-main.71) |  | 0 | Understanding longer narratives or participating in conversations requires tracking of discourse entities that have been mentioned. Indefinite noun phrases (NPs), such as ‘a dog’, frequently introduce discourse entities but this behavior is modulated by sentential operators such as negation. For example, ‘a dog’ in ‘Arthur doesn’t own a dog’ does not introduce a discourse entity due to the presence of negation. In this work, we adapt the psycholinguistic assessment of language models paradigm... | Sebastian Schuster, Tal Linzen |  |
| 151 |  |  [On Curriculum Learning for Commonsense Reasoning](https://doi.org/10.18653/v1/2022.naacl-main.72) |  | 0 | Commonsense reasoning tasks follow a standard paradigm of finetuning pretrained language models on the target task data, where samples are introduced to the model in a random order during training. However, recent research suggests that data order can have a significant impact on the performance of finetuned models for natural language understanding. Hence, we examine the effect of a human-like easy-to-difficult curriculum during finetuning of language models for commonsense reasoning tasks. We... | Adyasha Maharana, Mohit Bansal |  |
| 152 |  |  [DocTime: A Document-level Temporal Dependency Graph Parser](https://doi.org/10.18653/v1/2022.naacl-main.73) |  | 0 | We introduce DocTime - a novel temporal dependency graph (TDG) parser that takes as input a text document and produces a temporal dependency graph. It outperforms previous BERT-based solutions by a relative 4-8% on three datasets from modeling the problem as a graph network with path-prediction loss to incorporate longer range dependencies. This work also demonstrates how the TDG graph can be used to improve the downstream tasks of temporal questions answering and NLI by a relative 4-10% with a... | Puneet Mathur, Vlad I. Morariu, Verena KaynigFittkau, Jiuxiang Gu, Franck Dernoncourt, Quan Hung Tran, Ani Nenkova, Dinesh Manocha, Rajiv Jain |  |
| 153 |  |  [FactPEGASUS: Factuality-Aware Pre-training and Fine-tuning for Abstractive Summarization](https://doi.org/10.18653/v1/2022.naacl-main.74) |  | 0 | We present FactPEGASUS, an abstractive summarization model that addresses the problem of factuality during pre-training and fine-tuning: (1) We augment the sentence selection strategy of PEGASUS’s (Zhang et al., 2019) pre-training objective to create pseudo-summaries that are both important and factual; (2) We introduce three complementary components for fine-tuning. The corrector removes hallucinations present in the reference summary, the contrastor uses contrastive learning to better... | David Wan, Mohit Bansal |  |
| 154 |  |  [ScAN: Suicide Attempt and Ideation Events Dataset](https://doi.org/10.18653/v1/2022.naacl-main.75) |  | 0 | Suicide is an important public health concern and one of the leading causes of death worldwide. Suicidal behaviors, including suicide attempts (SA) and suicide ideations (SI), are leading risk factors for death by suicide. Information related to patients’ previous and current SA and SI are frequently documented in the electronic health record (EHR) notes. Accurate detection of such documentation may help improve surveillance and predictions of patients’ suicidal behaviors and alert medical... | Bhanu Pratap Singh Rawat, Samuel Kovaly, Hong Yu, Wilfred R. Pigeon |  |
| 155 |  |  [Socially Aware Bias Measurements for Hindi Language Representations](https://doi.org/10.18653/v1/2022.naacl-main.76) |  | 0 | Language representations are an efficient tool used across NLP, but they are strife with encoded societal biases. These biases are studied extensively, but with a primary focus on English language representations and biases common in the context of Western society. In this work, we investigate the biases present in Hindi language representations such as caste and religion associated biases. We demonstrate how biases are unique to specific language representations based on the history and... | Vijit Malik, Sunipa Dev, Akihiro Nishi, Nanyun Peng, KaiWei Chang |  |
| 156 |  |  [AmbiPun: Generating Humorous Puns with Ambiguous Context](https://doi.org/10.18653/v1/2022.naacl-main.77) |  | 0 | In this paper, we propose a simple yet effective way to generate pun sentences that does not require any training on existing puns. Our approach is inspired by humor theories that ambiguity comes from the context rather than the pun word itself. Given a pair of definitions of a pun word, our model first produces a list of related concepts through a reverse dictionary. We then utilize one-shot GPT3 to generate context words and then generate puns incorporating context words from both concepts.... | Anirudh Mittal, Yufei Tian, Nanyun Peng |  |
| 157 |  |  [EmpHi: Generating Empathetic Responses with Human-like Intents](https://doi.org/10.18653/v1/2022.naacl-main.78) |  | 0 | In empathetic conversations, humans express their empathy to others with empathetic intents. However, most existing empathetic conversational methods suffer from a lack of empathetic intents, which leads to monotonous empathy. To address the bias of the empathetic intents distribution between empathetic dialogue models and humans, we propose a novel model to generate empathetic responses with human-consistent empathetic intents, EmpHi for short. Precisely, EmpHi learns the distribution of... | Mao Yan Chen, Siheng Li, Yujiu Yang |  |
| 158 |  |  [Yes, No or IDK: The Challenge of Unanswerable Yes/No Questions](https://doi.org/10.18653/v1/2022.naacl-main.79) |  | 0 | The Yes/No QA task (Clark et al., 2019) consists of “Yes” or “No” questions about a given context. However, in realistic scenarios, the information provided in the context is not always sufficient in order to answer the question. For example, given the context “She married a lawyer from New-York.”, we don’t know whether the answer to the question “Did she marry in New York?” is “Yes” or “No”. In this paper, we extend the Yes/No QA task, adding questions with an IDK answer, and show its... | Elior Sulem, Jamaal Hay, Dan Roth |  |
| 159 |  |  [Inducing and Using Alignments for Transition-based AMR Parsing](https://doi.org/10.18653/v1/2022.naacl-main.80) |  | 0 | Transition-based parsers for Abstract Meaning Representation (AMR) rely on node-to-word alignments. These alignments are learned separately from parser training and require a complex pipeline of rule-based components, pre-processing, and post-processing to satisfy domain-specific constraints. Parsers also train on a point-estimate of the alignment pipeline, neglecting the uncertainty due to the inherent ambiguity of alignment. In this work we explore two avenues for overcoming these... | Andrew Drozdov, Jiawei Zhou, Radu Florian, Andrew McCallum, Tahira Naseem, Yoon Kim, Ramón Fernandez Astudillo |  |
| 160 |  |  [Masked Part-Of-Speech Model: Does Modeling Long Context Help Unsupervised POS-tagging?](https://doi.org/10.18653/v1/2022.naacl-main.81) |  | 0 | Previous Part-Of-Speech (POS) induction models usually assume certain independence assumptions (e.g., Markov, unidirectional, local dependency) that do not hold in real languages. For example, the subject-verb agreement can be both long-term and bidirectional. To facilitate flexible dependency modeling, we propose a Masked Part-of-Speech Model (MPoSM), inspired by the recent success of Masked Language Models (MLM). MPoSM can model arbitrary tag dependency and perform POS induction through the... | Xiang Zhou, Shiyue Zhang, Mohit Bansal |  |
| 161 |  |  [DREAM: Improving Situational QA by First Elaborating the Situation](https://doi.org/10.18653/v1/2022.naacl-main.82) |  | 0 | When people answer questions about a specific situation, e.g., “I cheated on my mid-term exam last week. Was that wrong?”, cognitive science suggests that they form a mental picture of that situation before answering. While we do not know how language models (LMs) answer such questions, we conjecture that they may answer more accurately if they are also provided with additional details about the question situation, elaborating the “scene”. To test this conjecture, we train a new model, DREAM,... | Yuling Gu, Bhavana Dalvi, Peter Clark |  |
| 162 |  |  [CoSe-Co: Text Conditioned Generative CommonSense Contextualizer](https://doi.org/10.18653/v1/2022.naacl-main.83) |  | 0 | Pre-trained Language Models (PTLMs) have been shown to perform well on natural language tasks. Many prior works have leveraged structured commonsense present in the form of entities linked through labeled relations in Knowledge Graphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static module which limits coverage since KGs contain finite knowledge. Generative methods train PTLMs on KG triples to improve the scale at which knowledge can be obtained. However, training on... | Rachit Bansal, Milan Aggarwal, Sumit Bhatia, Jivat Neet Kaur, Balaji Krishnamurthy |  |
| 163 |  |  [Probing via Prompting](https://doi.org/10.18653/v1/2022.naacl-main.84) |  | 0 | Probing is a popular approach to understand what linguistic information is contained in the representations of pre-trained language models. However, the mechanism of selecting the probe model has recently been subject to intense debate, as it is not clear if the probes are merely extracting information or modelling the linguistic property themselves. To address this challenge, this paper introduces a novel model-free approach to probing via prompting, which formulates probing as a prompting... | Jiaoda Li, Ryan Cotterell, Mrinmaya Sachan |  |
| 164 |  |  [Database Search Results Disambiguation for Task-Oriented Dialog Systems](https://doi.org/10.18653/v1/2022.naacl-main.85) |  | 0 | As task-oriented dialog systems are becoming increasingly popular in our lives, more realistic tasks have been proposed and explored. However, new practical challenges arise. For instance, current dialog systems cannot effectively handle multiplesearch results when querying a database, due to the lack of such scenarios in existing public datasets. In this paper, we propose Database Search Result (DSR) Disambiguation, a novel task that focuses on disambiguating database search results, which... | Kun Qian, Satwik Kottur, Ahmad Beirami, Shahin Shayandeh, Paul A. Crook, Alborz Geramifard, Zhou Yu, Chinnadhurai Sankar |  |
| 165 |  |  [Unsupervised Slot Schema Induction for Task-oriented Dialog](https://doi.org/10.18653/v1/2022.naacl-main.86) |  | 0 | Carefully-designed schemas describing how to collect and annotate dialog corpora are a prerequisite towards building task-oriented dialog systems. In practical applications, manually designing schemas can be error-prone, laborious, iterative, and slow, especially when the schema is complicated. To alleviate this expensive and time consuming process, we propose an unsupervised approach for slot schema induction from unlabeled dialog corpora. Leveraging in-domain language models and unsupervised... | Dian Yu, Mingqiu Wang, Yuan Cao, Izhak Shafran, Laurent El Shafey, Hagen Soltau |  |
| 166 |  |  [Towards a Progression-Aware Autonomous Dialogue Agent](https://doi.org/10.18653/v1/2022.naacl-main.87) |  | 0 | Recent advances in large-scale language modeling and generation have enabled the creation of dialogue agents that exhibit human-like responses in a wide range of conversational scenarios spanning a diverse set of tasks, from general chit-chat to focused goal-oriented discourse. While these agents excel at generating high-quality responses that are relevant to prior context, they suffer from a lack of awareness of the overall direction in which the conversation is headed, and the likelihood of... | Abraham Sanders, Tomek Strzalkowski, Mei Si, Albert Chang, Deepanshu Dey, Jonas Braasch, Dakuo Wang |  |
| 167 |  |  [Cross-Domain Detection of GPT-2-Generated Technical Text](https://doi.org/10.18653/v1/2022.naacl-main.88) |  | 0 | Machine-generated text presents a potential threat not only to the public sphere, but also to the scientific enterprise, whereby genuine research is undermined by convincing, synthetic text. In this paper we examine the problem of detecting GPT-2-generated technical research text. We first consider the realistic scenario where the defender does not have full information about the adversary’s text generation pipeline, but is able to label small amounts of in-domain genuine and synthetic text in... | Juan Diego Rodriguez, Todd Hay, David Gros, Zain Shamsi, Ravi Srinivasan |  |
| 168 |  |  [DISAPERE: A Dataset for Discourse Structure in Peer Review Discussions](https://doi.org/10.18653/v1/2022.naacl-main.89) |  | 0 | At the foundation of scientific evaluation is the labor-intensive process of peer review. This critical task requires participants to consume vast amounts of highly technical text. Prior work has annotated different aspects of review argumentation, but discourse relations between reviews and rebuttals have yet to be examined. We present DISAPERE, a labeled dataset of 20k sentences contained in 506 review-rebuttal pairs in English, annotated by experts. DISAPERE synthesizes label sets from prior... | Neha Nayak Kennard, Tim O'Gorman, Rajarshi Das, Akshay Sharma, Chhandak Bagchi, Matthew Clinton, Pranay Kumar Yelugam, Hamed Zamani, Andrew McCallum |  |
| 169 |  |  [MultiSpanQA: A Dataset for Multi-Span Question Answering](https://doi.org/10.18653/v1/2022.naacl-main.90) |  | 0 | Most existing reading comprehension datasets focus on single-span answers, which can be extracted as a single contiguous span from a given text passage. Multi-span questions, i.e., questions whose answer is a series of multiple discontiguous spans in the text, are common real life but are less studied. In this paper, we present MultiSpanQA, a new dataset that focuses on multi-span questions. Raw questions and contexts are extracted from the Natural Questions dataset. After multi-span... | Haonan Li, Martin Tomko, Maria Vasardani, Timothy Baldwin |  |
| 170 |  |  [Context-Aware Abbreviation Expansion Using Large Language Models](https://doi.org/10.18653/v1/2022.naacl-main.91) |  | 0 | Motivated by the need for accelerating text entry in augmentative and alternative communication (AAC) for people with severe motor impairments, we propose a paradigm in which phrases are abbreviated aggressively as primarily word-initial letters. Our approach is to expand the abbreviations into full-phrase options by leveraging conversation context with the power of pretrained large language models (LLMs). Through zero-shot, few-shot, and fine-tuning experiments on four public conversation... | Shanqing Cai, Subhashini Venugopalan, Katrin Tomanek, Ajit Narayanan, Meredith Ringel Morris, Michael P. Brenner |  |
| 171 |  |  [Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models](https://doi.org/10.18653/v1/2022.naacl-main.92) |  | 0 | NLP models trained on text have been shown to reproduce human stereotypes, which can magnify harms to marginalized groups when systems are deployed at scale. We adapt the Agency-Belief-Communion (ABC) stereotype model of Koch et al. (2016) from social psychology as a framework for the systematic study and discovery of stereotypic group-trait associations in language models (LMs). We introduce the sensitivity test (SeT) for measuring stereotypical associations from language models. To evaluate... | Yang Trista Cao, Anna Sotnikova, Hal Daumé III, Rachel Rudinger, Linda Zou |  |
| 172 |  |  [Sort by Structure: Language Model Ranking as Dependency Probing](https://doi.org/10.18653/v1/2022.naacl-main.93) |  | 0 | Making an informed choice of pre-trained language model (LM) is critical for performance, yet environmentally costly, and as such widely underexplored. The field of Computer Vision has begun to tackle encoder ranking, with promising forays into Natural Language Processing, however they lack coverage of linguistic tasks such as structured prediction. We propose probing to rank LMs, specifically for parsing dependencies in a given language, by measuring the degree to which labeled trees are... | Max MüllerEberstein, Rob van der Goot, Barbara Plank |  |
| 173 |  |  [Quantifying Synthesis and Fusion and their Impact on Machine Translation](https://doi.org/10.18653/v1/2022.naacl-main.94) |  | 0 | Theoretical work in morphological typology offers the possibility of measuring morphological diversity on a continuous scale. However, literature in Natural Language Processing (NLP) typically labels a whole language with a strict type of morphology, e.g. fusional or agglutinative. In this work, we propose to reduce the rigidity of such claims, by quantifying morphological typology at the word and segment level. We consider Payne (2017)’s approach to classify morphology using two indices:... | Arturo Oncevay, Duygu Ataman, Niels van Berkel, Barry Haddow, Alexandra Birch, Johannes Bjerva |  |
| 174 |  |  [Commonsense and Named Entity Aware Knowledge Grounded Dialogue Generation](https://doi.org/10.18653/v1/2022.naacl-main.95) |  | 0 | Grounding dialogue on external knowledge and interpreting linguistic patterns in dialogue history context, such as ellipsis, anaphora, and co-reference is critical for dialogue comprehension and generation. In this paper, we present a novel open-domain dialogue generation model which effectively utilizes the large-scale commonsense and named entity based knowledge in addition to the unstructured topic-specific knowledge associated with each utterance. We enhance the commonsense knowledge with... | Deeksha Varshney, Akshara Prabhakar, Asif Ekbal |  |
| 175 |  |  [Efficient Hierarchical Domain Adaptation for Pretrained Language Models](https://doi.org/10.18653/v1/2022.naacl-main.96) |  | 0 | The remarkable success of large language models has been driven by dense models trained on massive unlabeled, unstructured corpora. These corpora typically contain text from diverse, heterogeneous sources, but information about the source of the text is rarely used during training. Transferring their knowledge to a target domain is typically done by continuing training in-domain. In this paper, we introduce a method to permit domain adaptation to many diverse domains using a computationally... | Alexandra Chronopoulou, Matthew E. Peters, Jesse Dodge |  |
| 176 |  |  [Hatemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-Based Hate](https://doi.org/10.18653/v1/2022.naacl-main.97) |  | 0 | Detecting online hate is a complex task, and low-performing models have harmful consequences when used for sensitive applications such as content moderation. Emoji-based hate is an emerging challenge for automated detection. We present HatemojiCheck, a test suite of 3,930 short-form statements that allows us to evaluate performance on hateful language expressed with emoji. Using the test suite, we expose weaknesses in existing hate detection models. To address these weaknesses, we create the... | Hannah Kirk, Bertie Vidgen, Paul Röttger, Tristan Thrush, Scott Hale |  |
| 177 |  |  [On the Economics of Multilingual Few-shot Learning: Modeling the Cost-Performance Trade-offs of Machine Translated and Manual Data](https://doi.org/10.18653/v1/2022.naacl-main.98) |  | 0 | Borrowing ideas from Production functions in micro-economics, in this paper we introduce a framework to systematically evaluate the performance and cost trade-offs between machine-translated and manually-created labelled data for task-specific fine-tuning of massively multilingual language models. We illustrate the effectiveness of our framework through a case-study on the TyDIQA-GoldP dataset. One of the interesting conclusion of the study is that if the cost of machine translation is greater... | Kabir Ahuja, Monojit Choudhury, Sandipan Dandapat |  |
| 178 |  |  [Learning to Selectively Learn for Weakly Supervised Paraphrase Generation with Model-based Reinforcement Learning](https://doi.org/10.18653/v1/2022.naacl-main.99) |  | 0 | Paraphrase generation is an important language generation task attempting to interpret user intents and systematically generate new phrases of identical meanings to the given ones. However, the effectiveness of paraphrase generation is constrained by the access to the golden labeled data pairs where both the amount and the quality of the training data pairs are restricted. In this paper, we propose a new weakly supervised paraphrase generation approach that extends the success of a recent work... | Haiyan Yin, Dingcheng Li, Ping Li |  |
| 179 |  |  [Quality-Aware Decoding for Neural Machine Translation](https://doi.org/10.18653/v1/2022.naacl-main.100) |  | 0 | Despite the progress in machine translation quality estimation and evaluation in the last years, decoding in neural machine translation (NMT) is mostly oblivious to this and centers around finding the most probable translation according to the model (MAP decoding), approximated with beam search. In this paper, we bring together these two lines of research and propose quality-aware decoding for NMT, by leveraging recent breakthroughs in reference-free and reference-based MT evaluation through... | Patrick Fernandes, António Farinhas, Ricardo Rei, José Guilherme Camargo de Souza, Perez Ogayo, Graham Neubig, André F. T. Martins |  |
| 180 |  |  [Pretrained Models for Multilingual Federated Learning](https://doi.org/10.18653/v1/2022.naacl-main.101) |  | 0 | Since the advent of Federated Learning (FL), research has applied these methods to natural language processing (NLP) tasks. Despite a plethora of papers in FL for NLP, no previous works have studied how multilingual text impacts FL algorithms. Furthermore, multilingual text provides an interesting avenue to examine the impact of non-IID text (e.g. different languages) on FL in naturally occurring data. We explore three multilingual language tasks, language modeling, machine translation, and... | Orion Weller, Marc Marone, Vladimir Braverman, Dawn J. Lawrie, Benjamin Van Durme |  |
| 181 |  |  [AcTune: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models](https://doi.org/10.18653/v1/2022.naacl-main.102) |  | 0 | Although fine-tuning pre-trained language models (PLMs) renders strong performance in many NLP tasks, it relies on excessive labeled data. Recently, researchers have resorted to active fine-tuning for enhancing the label efficiency of PLM fine-tuning, but existing methods of this type usually ignore the potential of unlabeled data. We develop AcTune, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self-training. AcTune... | Yue Yu, Lingkai Kong, Jieyu Zhang, Rongzhi Zhang, Chao Zhang |  |
| 182 |  |  [Label Anchored Contrastive Learning for Language Understanding](https://doi.org/10.18653/v1/2022.naacl-main.103) |  | 0 | Contrastive learning (CL) has achieved astonishing progress in computer vision, speech, and natural language processing fields recently with self-supervised learning. However, CL approach to the supervised setting is not fully explored, especially for the natural language understanding classification task. Intuitively, the class label itself has the intrinsic ability to perform hard positive/negative mining, which is crucial for CL. Motivated by this, we propose a novel label anchored... | Zhenyu Zhang, Yuming Zhao, Meng Chen, Xiaodong He |  |
| 183 |  |  [Go Back in Time: Generating Flashbacks in Stories with Event Temporal Prompts](https://doi.org/10.18653/v1/2022.naacl-main.104) |  | 0 | Stories or narratives are comprised of a sequence of events. To compose interesting stories, professional writers often leverage a creative writing technique called \*flashback\* that inserts past events into current storylines as we commonly observe in novels and plays. However, it is challenging for machines to generate \*flashback\* as it requires a solid understanding of event \*\*temporal order\*\* (e.g. \*feeling hungry\* before \*eat\*, not vice versa), and the creativity to arrange... | Rujun Han, Hong Chen, Yufei Tian, Nanyun Peng |  |
| 184 |  |  [Forecasting COVID-19 Caseloads Using Unsupervised Embedding Clusters of Social Media Posts](https://doi.org/10.18653/v1/2022.naacl-main.105) |  | 0 | We present a novel approach incorporating transformer-based language models into infectious disease modelling. Text-derived features are quantified by tracking high-density clusters of sentence-level representations of Reddit posts within specific US states’ COVID-19 subreddits. We benchmark these clustered embedding features against features extracted from other high-quality datasets. In a threshold-classification task, we show that they outperform all other feature types at predicting upward... | Felix Drinkall, Stefan Zohren, Janet B. Pierrehumbert |  |
| 185 |  |  [Many Hands Make Light Work: Using Essay Traits to Automatically Score Essays](https://doi.org/10.18653/v1/2022.naacl-main.106) |  | 0 | Most research in the area of automatic essay grading (AEG) is geared towards scoring the essay holistically while there has also been little work done on scoring individual essay traits. In this paper, we describe a way to score essays using a multi-task learning (MTL) approach, where scoring the essay holistically is the primary task, and scoring the essay traits is the auxiliary task. We compare our results with a single-task learning (STL) approach, using both LSTMs and BiLSTMs. To find out... | Rahul Kumar, Sandeep Mathias, Sriparna Saha, Pushpak Bhattacharyya |  |
| 186 |  |  [Natural Language Inference with Self-Attention for Veracity Assessment of Pandemic Claims](https://doi.org/10.18653/v1/2022.naacl-main.107) |  | 0 | We present a comprehensive work on automated veracity assessment from dataset creation to developing novel methods based on Natural Language Inference (NLI), focusing on misinformation related to the COVID-19 pandemic. We first describe the construction of the novel PANACEA dataset consisting of heterogeneous claims on COVID-19 and their respective information sources. The dataset construction includes work on retrieval techniques and similarity measurements to ensure a unique set of claims. We... | Miguel AranaCatania, Elena Kochkina, Arkaitz Zubiaga, Maria Liakata, Robert Procter, Yulan He |  |
| 187 |  |  [Beyond Emotion: A Multi-Modal Dataset for Human Desire Understanding](https://doi.org/10.18653/v1/2022.naacl-main.108) |  | 0 | Desire is a strong wish to do or have something, which involves not only a linguistic expression, but also underlying cognitive phenomena driving human feelings. As the most primitive and basic human instinct, conscious desire is often accompanied by a range of emotional responses. As a strikingly understudied task, it is difficult for machines to model and understand desire due to the unavailability of benchmarking datasets with desire and emotion labels. To bridge this gap, we present MSED,... | Ao Jia, Yu He, Yazhou Zhang, Sagar Uprety, Dawei Song, Christina Lioma |  |
| 188 |  |  [Relation-Specific Attentions over Entity Mentions for Enhanced Document-Level Relation Extraction](https://doi.org/10.18653/v1/2022.naacl-main.109) |  | 0 | Compared with traditional sentence-level relation extraction, document-level relation extraction is a more challenging task where an entity in a document may be mentioned multiple times and associated with multiple relations. However, most methods of document-level relation extraction do not distinguish between mention-level features and entity-level features, and just apply simple pooling operation for aggregating mention-level features into entity-level features. As a result, the distinct... | Jiaxin Yu, Deqing Yang, Shuyu Tian |  |
| 189 |  |  [Twitter-COMMs: Detecting Climate, COVID, and Military Multimodal Misinformation](https://doi.org/10.18653/v1/2022.naacl-main.110) |  | 0 | Detecting out-of-context media, such as “miscaptioned” images on Twitter, is a relevant problem, especially in domains of high public significance. In this work we aim to develop defenses against such misinformation for the topics of Climate Change, COVID-19, and Military Vehicles. We first present a large-scale multimodal dataset with over 884k tweets relevant to these topics. Next, we propose a detection method, based on the state-of-the-art CLIP model, that leverages automatically generated... | Giscard Biamby, Grace Luo, Trevor Darrell, Anna Rohrbach |  |
| 190 |  |  [BlonDe: An Automatic Evaluation Metric for Document-level Machine Translation](https://doi.org/10.18653/v1/2022.naacl-main.111) |  | 0 | Standard automatic metrics, e.g. BLEU, are not reliable for document-level MT evaluation. They can neither distinguish document-level improvements in translation quality from sentence-level ones, nor identify the discourse phenomena that cause context-agnostic translations. This paper introduces a novel automatic metric BlonDe to widen the scope of automatic MT evaluation from sentence to document level. BlonDe takes discourse coherence into consideration by categorizing discourse-related spans... | Yuchen Jiang, Tianyu Liu, Shuming Ma, Dongdong Zhang, Jian Yang, Haoyang Huang, Rico Sennrich, Ryan Cotterell, Mrinmaya Sachan, Ming Zhou |  |
| 191 |  |  [Disentangled Learning of Stance and Aspect Topics for Vaccine Attitude Detection in Social Media](https://doi.org/10.18653/v1/2022.naacl-main.112) |  | 0 | Building models to detect vaccine attitudes on social media is challenging because of the composite, often intricate aspects involved, and the limited availability of annotated data. Existing approaches have relied heavily on supervised training that requires abundant annotations and pre-defined aspect categories. Instead, with the aim of leveraging the large amount of unannotated data now available on vaccination, we propose a novel semi-supervised approach for vaccine attitude detection,... | Lixing Zhu, Zheng Fang, Gabriele Pergola, Robert Procter, Yulan He |  |
| 192 |  |  [SKILL: Structured Knowledge Infusion for Large Language Models](https://doi.org/10.18653/v1/2022.naacl-main.113) |  | 0 | Large language models (LLMs) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, it is largely unexplored whether they can better internalize knowledge from a structured data, such as a knowledge graph, or from text. In this work, we propose a method to infuse structured knowledge into LLMs, by directly training T5 models on factual triples of knowledge graphs (KGs). We show that models pre-trained on Wikidata KG with our method outperform the T5... | Fedor Moiseev, Zhe Dong, Enrique Alfonseca, Martin Jaggi |  |
| 193 |  |  [Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models](https://doi.org/10.18653/v1/2022.naacl-main.114) |  | 0 | The success of multilingual pre-trained models is underpinned by their ability to learn representations shared by multiple languages even in absence of any explicit supervision. However, it remains unclear how these models learn to generalise across languages. In this work, we conjecture that multilingual pre-trained models can derive language-universal abstractions about grammar. In particular, we investigate whether morphosyntactic information is encoded in the same subset of neurons in... | Karolina Stanczak, Edoardo M. Ponti, Lucas Torroba Hennigen, Ryan Cotterell, Isabelle Augenstein |  |
| 194 |  |  [Aspect Is Not You Need: No-aspect Differential Sentiment Framework for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2022.naacl-main.115) |  | 0 | Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment classification task. Most recent efforts adopt pre-trained model to classify the sentences with aspects. However, the aspect sentiment bias from pre-trained model brings some noise to the ABSA task. Besides, traditional methods using cross-entropy loss are hard to find the potential associations between sentiment polarities. In this work, we analyze the ABSA task from a novel cognition perspective: humans can often judge the... | Jiahao Cao, Rui Liu, Huailiang Peng, Lei Jiang, Xu Bai |  |
| 195 |  |  [MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation](https://doi.org/10.18653/v1/2022.naacl-main.116) |  | 0 | Pre-trained language models have demonstrated superior performance in various natural language processing tasks. However, these models usually contain hundreds of millions of parameters, which limits their practicality because of latency requirements in real-world applications. Existing methods train small compressed models via knowledge distillation. However, performance of these small models drops significantly compared with the pre-trained models due to their reduced model capacity. We... | Simiao Zuo, Qingru Zhang, Chen Liang, Pengcheng He, Tuo Zhao, Weizhu Chen |  |
| 196 |  |  [Implicit n-grams Induced by Recurrence](https://doi.org/10.18653/v1/2022.naacl-main.117) |  | 0 | Although self-attention based models such as Transformers have achieved remarkable successes on natural language processing (NLP)tasks, recent studies reveal that they have limitations on modeling sequential transformations (Hahn, 2020), which may promptre-examinations of recurrent neural networks (RNNs) that demonstrated impressive results on handling sequential data. Despite manyprior attempts to interpret RNNs, their internal mechanisms have not been fully understood, and the question on how... | Xiaobing Sun, Wei Lu |  |
| 197 |  |  [Guiding Visual Question Generation](https://doi.org/10.18653/v1/2022.naacl-main.118) |  | 0 | In traditional Visual Question Generation (VQG), most images have multiple concepts (e.g. objects and categories) for which a question could be generated, but models are trained to mimic an arbitrary choice of concept as given in their training data. This makes training difficult and also poses issues for evaluation – multiple valid questions exist for most images but only one or a few are captured by the human references. We present Guiding Visual Question Generation - a variant of VQG which... | Nihir Vedd, Zixu Wang, Marek Rei, Yishu Miao, Lucia Specia |  |
| 198 |  |  [OPERA: Operation-Pivoted Discrete Reasoning over Text](https://doi.org/10.18653/v1/2022.naacl-main.119) |  | 0 | Machine reading comprehension (MRC) that requires discrete reasoning involving symbolic operations, e.g., addition, sorting, and counting, is a challenging task. According to this nature, semantic parsing-based methods predict interpretable but complex logical forms. However, logical form generation is nontrivial and even a little perturbation in a logical form will lead to wrong answers. To alleviate this issue, multi-predictor -based methods are proposed to directly predict different types of... | Yongwei Zhou, Junwei Bao, Chaoqun Duan, Haipeng Sun, Jiahui Liang, Yifan Wang, Jing Zhao, Youzheng Wu, Xiaodong He, Tiejun Zhao |  |
| 199 |  |  [Improving Multi-Document Summarization through Referenced Flexible Extraction with Credit-Awareness](https://doi.org/10.18653/v1/2022.naacl-main.120) |  | 0 | A notable challenge in Multi-Document Summarization (MDS) is the extremely-long length of the input. In this paper, we present an extract-then-abstract Transformer framework to overcome the problem. Specifically, we leverage pre-trained language models to construct a hierarchical extractor for salient sentence selection across documents and an abstractor for rewriting the selected contents as summaries. However, learning such a framework is challenging since the optimal contents for the... | YunZhu Song, YiSyuan Chen, HongHan Shuai |  |
| 200 |  |  [Improving Constituent Representation with Hypertree Neural Networks](https://doi.org/10.18653/v1/2022.naacl-main.121) |  | 0 | Many natural language processing tasks involve text spans and thus high-quality span representations are needed to enhance neural approaches to these tasks. Most existing methods of span representation are based on simple derivations (such as max-pooling) from word representations and do not utilize compositional structures of natural language. In this paper, we aim to improve representations of constituent spans using a novel hypertree neural networks (HTNN) that is structured with... | Hao Zhou, Gongshen Liu, Kewei Tu |  |
| 201 |  |  [Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.naacl-main.122) |  | 0 | An increasing awareness of biased patterns in natural language processing resources such as BERT has motivated many metrics to quantify ‘bias’ and ‘fairness’ in these resources. However, comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the literature on fairness metrics for pre-trained language models and experimentally evaluate compatibility, including both biases in language models and in their... | Pieter Delobelle, Ewoenam Kwaku Tokpo, Toon Calders, Bettina Berendt |  |
| 202 |  |  [MuCPAD: A Multi-Domain Chinese Predicate-Argument Dataset](https://doi.org/10.18653/v1/2022.naacl-main.123) |  | 0 | During the past decade, neural network models have made tremendous progress on in-domain semantic role labeling (SRL). However, performance drops dramatically under the out-of-domain setting. In order to facilitate research on cross-domain SRL, this paper presents MuCPAD, a multi-domain Chinese predicate-argument dataset, which consists of 30,897 sentences and 92,051 predicates from six different domains. MuCPAD exhibits three important features. 1) Based on a frame-free annotation methodology,... | Yahui Liu, Haoping Yang, Chen Gong, Qingrong Xia, Zhenghua Li, Min Zhang |  |
| 203 |  |  [Representation Learning for Conversational Data using Discourse Mutual Information Maximization](https://doi.org/10.18653/v1/2022.naacl-main.124) |  | 0 | Although many pretrained models exist for text or images, there have been relatively fewer attempts to train representations specifically for dialog understanding. Prior works usually relied on finetuned representations based on generic text representation models like BERT or GPT-2. But such language modeling pretraining objectives do not take the structural information of conversational text into consideration. Although generative dialog models can learn structural features too, we argue that... | Bishal Santra, Sumegh Roychowdhury, Aishik Mandal, Vasu Gurram, Atharva Naik, Manish Gupta, Pawan Goyal |  |
| 204 |  |  [ValCAT: Variable-Length Contextualized Adversarial Transformations Using Encoder-Decoder Language Model](https://doi.org/10.18653/v1/2022.naacl-main.125) |  | 0 | Adversarial texts help explore vulnerabilities in language models, improve model robustness, and explain their working mechanisms. However, existing word-level attack methods trap in a one-to-one attack pattern, i.e., only a single word can be modified in one transformation round, and they ignore the interactions between several consecutive words. In this paper, we propose ValCAT, a black-box attack framework that misleads the language model by applying variable-length contextualized... | Chuyun Deng, Mingxuan Liu, Yue Qin, Jia Zhang, HaiXin Duan, Donghong Sun |  |
| 205 |  |  [A Study of Syntactic Multi-Modality in Non-Autoregressive Machine Translation](https://doi.org/10.18653/v1/2022.naacl-main.126) |  | 0 | It is difficult for non-autoregressive translation (NAT) models to capture the multi-modal distribution of target translations due to their conditional independence assumption, which is known as the “multi-modality problem”, including the lexical multi-modality and the syntactic multi-modality. While the first one has been well studied, the syntactic multi-modality brings severe challenges to the standard cross entropy (XE) loss in NAT and is understudied. In this paper, we conduct a systematic... | Kexun Zhang, Rui Wang, Xu Tan, Junliang Guo, Yi Ren, Tao Qin, TieYan Liu |  |
| 206 |  |  [CIAug: Equipping Interpolative Augmentation with Curriculum Learning](https://doi.org/10.18653/v1/2022.naacl-main.127) |  | 0 | Interpolative data augmentation has proven to be effective for NLP tasks. Despite its merits, the sample selection process in mixup is random, which might make it difficult for the model to generalize better and converge faster. We propose CIAug, a novel curriculum-based learning method that builds upon mixup. It leverages the relative position of samples in hyperbolic embedding space as a complexity measure to gradually mix up increasingly difficult and diverse samples along training. CIAug... | Ramit Sawhney, Ritesh Soun, Shrey Pandit, Megh Thakkar, Sarvagya Malaviya, Yuval Pinter |  |
| 207 |  |  [Proposition-Level Clustering for Multi-Document Summarization](https://doi.org/10.18653/v1/2022.naacl-main.128) |  | 0 | Text clustering methods were traditionally incorporated into multi-document summarization (MDS) as a means for coping with considerable information repetition. Particularly, clusters were leveraged to indicate information saliency as well as to avoid redundancy. Such prior methods focused on clustering sentences, even though closely related sentences usually contain also non-aligned parts. In this work, we revisit the clustering approach, grouping together sub-sentential propositions, aiming at... | Ori Ernst, Avi Caciularu, Ori Shapira, Ramakanth Pasunuru, Mohit Bansal, Jacob Goldberger, Ido Dagan |  |
| 208 |  |  [Non-Autoregressive Machine Translation: It's Not as Fast as it Seems](https://doi.org/10.18653/v1/2022.naacl-main.129) |  | 0 | Efficient machine translation models are commercially important as they can increase inference speeds, and reduce costs and carbon emissions. Recently, there has been much interest in non-autoregressive (NAR) models, which promise faster translation. In parallel to the research on NAR models, there have been successful attempts to create optimized autoregressive models as part of the WMT shared task on efficient translation. In this paper, we point out flaws in the evaluation methodology... | Jindrich Helcl, Barry Haddow, Alexandra Birch |  |
| 209 |  |  [BAD-X: Bilingual Adapters Improve Zero-Shot Cross-Lingual Transfer](https://doi.org/10.18653/v1/2022.naacl-main.130) |  | 0 | Adapter modules enable modular and efficient zero-shot cross-lingual transfer, where current state-of-the-art adapter-based approaches learn specialized language adapters (LAs) for individual languages. In this work, we show that it is more effective to learn bilingual language pair adapters (BAs) when the goal is to optimize performance for a particular source-target transfer direction. Our novel BAD-X adapter framework trades off some modularity of dedicated LAs for improved transfer... | Marinela Parovic, Goran Glavas, Ivan Vulic, Anna Korhonen |  |
| 210 |  |  [Combining Humor and Sarcasm for Improving Political Parody Detection](https://doi.org/10.18653/v1/2022.naacl-main.131) |  | 0 | Parody is a figurative device used for mimicking entities for comedic or critical purposes. Parody is intentionally humorous and often involves sarcasm. This paper explores jointly modelling these figurative tropes with the goal of improving performance of political parody detection in tweets. To this end, we present a multi-encoder model that combines three parallel encoders to enrich parody-specific representations with humor and sarcasm information. Experiments on a publicly available data... | Xiao Ao, Danae Sanchez Villegas, Daniel PreotiucPietro, Nikolaos Aletras |  |
| 211 |  |  [TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages](https://doi.org/10.18653/v1/2022.naacl-main.132) |  | 0 | Recently, the structural reading comprehension (SRC) task on web pages has attracted increasing research interests. Although previous SRC work has leveraged extra information such as HTML tags or XPaths, the informative topology of web pages is not effectively exploited. In this work, we propose a Topological Information Enhanced model (TIE), which transforms the token-level task into a tag-level task by introducing a two-stage process (i.e. node locating and answer refining). Based on that,... | Zihan Zhao, Lu Chen, Ruisheng Cao, Hongshen Xu, Xingyu Chen, Kai Yu |  |
| 212 |  |  [RSTGen: Imbuing Fine-Grained Interpretable Control into Long-FormText Generators](https://doi.org/10.18653/v1/2022.naacl-main.133) |  | 0 | In this paper, we study the task of improving the cohesion and coherence of long-form text generated by language models. To this end, we propose RSTGen, a framework that utilises Rhetorical Structure Theory (RST), a classical language theory, to control the discourse structure, semantics and topics of generated text. Firstly, we demonstrate our model’s ability to control structural discourse and semantic features of generated text in open generation evaluation. Then we experiment on the two... | Rilwan Adewoyin, Ritabrata Dutta, Yulan He |  |
| 213 |  |  [Intent Detection and Discovery from User Logs via Deep Semi-Supervised Contrastive Clustering](https://doi.org/10.18653/v1/2022.naacl-main.134) |  | 0 | Intent Detection is a crucial component of Dialogue Systems wherein the objective is to classify a user utterance into one of multiple pre-defined intents. A pre-requisite for developing an effective intent identifier is a training dataset labeled with all possible user intents. However, even skilled domain experts are often unable to foresee all possible user intents at design time and for practical applications, novel intents may have to be inferred incrementally on-the-fly from user... | Rajat Kumar, Mayur Patidar, Vaibhav Varshney, Lovekesh Vig, Gautam Shroff |  |
| 214 |  |  [Extending Multi-Text Sentence Fusion Resources via Pyramid Annotations](https://doi.org/10.18653/v1/2022.naacl-main.135) |  | 0 | NLP models that process multiple texts often struggle in recognizing corresponding and salient information that is often differently phrased, and consolidating the redundancies across texts. To facilitate research of such challenges, the sentence fusion task was proposed, yet previous datasets for this task were very limited in their size and scope. In this paper, we revisit and substantially extend previous dataset creation efforts. With careful modifications, relabeling, and employing... | Daniela Brook Weiss, Paul Roit, Ori Ernst, Ido Dagan |  |
| 215 |  |  [The Devil is in the Details: On the Pitfalls of Vocabulary Selection in Neural Machine Translation](https://doi.org/10.18653/v1/2022.naacl-main.136) |  | 0 | Vocabulary selection, or lexical shortlisting, is a well-known technique to improve latency of Neural Machine Translation models by constraining the set of allowed output words during inference. The chosen set is typically determined by separately trained alignment model parameters, independent of the source-sentence context at inference time. While vocabulary selection appears competitive with respect to automatic quality metrics in prior work, we show that it can fail to select the right set... | Tobias Domhan, Eva Hasler, Ke Tran, Sony Trenous, Bill Byrne, Felix Hieber |  |
| 216 |  |  [MultiCite: Modeling realistic citations requires moving beyond the single-sentence single-label setting](https://doi.org/10.18653/v1/2022.naacl-main.137) |  | 0 | Citation context analysis (CCA) is an important task in natural language processing that studies how and why scholars discuss each others’ work. Despite decades of study, computational methods for CCA have largely relied on overly-simplistic assumptions of how authors cite, which ignore several important phenomena. For instance, scholarly papers often contain rich discussions of cited work that span multiple sentences and express multiple intents concurrently. Yet, recent work in CCA is often... | Anne Lauscher, Brandon Ko, Bailey Kuehl, Sophie Johnson, Arman Cohan, David Jurgens, Kyle Lo |  |
| 217 |  |  [DEGREE: A Data-Efficient Generation-Based Event Extraction Model](https://doi.org/10.18653/v1/2022.naacl-main.138) |  | 0 | Event extraction requires high-quality expert human annotations, which are usually expensive. Therefore, learning a data-efficient event extraction model that can be trained with only a few labeled examples has become a crucial challenge. In this paper, we focus on low-resource end-to-end event extraction and propose DEGREE, a data-efficient model that formulates event extraction as a conditional generation problem. Given a passage and a manually designed prompt, DEGREE learns to summarize the... | IHung Hsu, KuanHao Huang, Elizabeth Boschee, Scott Miller, Prem Natarajan, KaiWei Chang, Nanyun Peng |  |
| 218 |  |  [Bridging the Gap between Language Models and Cross-Lingual Sequence Labeling](https://doi.org/10.18653/v1/2022.naacl-main.139) |  | 0 | Large-scale cross-lingual pre-trained language models (xPLMs) have shown effective in cross-lingual sequence labeling tasks (xSL), such as machine reading comprehension (xMRC) by transferring knowledge from a high-resource language to low-resource languages. Despite the great success, we draw an empirical observation that there is an training objective gap between pre-training and fine-tuning stages: e.g., mask language modeling objective requires local understanding of the masked token and the... | Nuo Chen, Linjun Shou, Ming Gong, Jian Pei, Daxin Jiang |  |
| 219 |  |  [Hero-Gang Neural Model For Named Entity Recognition](https://doi.org/10.18653/v1/2022.naacl-main.140) |  | 0 | Named entity recognition (NER) is a fundamental and important task in NLP, aiming at identifying named entities (NEs) from free text. Recently, since the multi-head attention mechanism applied in the Transformer model can effectively capture longer contextual information, Transformer-based models have become the mainstream methods and have achieved significant performance in this task. Unfortunately, although these models can capture effective global context information, they are still limited... | Jinpeng Hu, Yaling Shen, Yang Liu, Xiang Wan, TsungHui Chang |  |
| 220 |  |  [MGIMN: Multi-Grained Interactive Matching Network for Few-shot Text Classification](https://doi.org/10.18653/v1/2022.naacl-main.141) |  | 0 | Text classification struggles to generalize to unseen classes with very few labeled text instances per class. In such a few-shot learning (FSL) setting, metric-based meta-learning approaches have shown promising results. Previous studies mainly aim to derive a prototype representation for each class. However, they neglect that it is challenging-yet-unnecessary to construct a compact representation which expresses the entire meaning for each class. They also ignore the importance to capture the... | Jianhai Zhang, Mieradilijiang Maimaiti, Gao Xing, Yuanhang Zheng, Ji Zhang |  |
| 221 |  |  [All You May Need for VQA are Image Captions](https://doi.org/10.18653/v1/2022.naacl-main.142) |  | 0 | Visual Question Answering (VQA) has benefited from increasingly sophisticated models, but has not enjoyed the same level of engagement in terms of data creation. In this paper, we propose a method that automatically derives VQA examples at volume, by leveraging the abundance of existing image-caption annotations combined with neural models for textual question generation. We show that the resulting data is of high-quality. VQA models trained on our data improve state-of-the-art zero-shot... | Soravit Changpinyo, Doron Kukliansky, Idan Szpektor, Xi Chen, Nan Ding, Radu Soricut |  |
| 222 |  |  [Frustratingly Easy System Combination for Grammatical Error Correction](https://doi.org/10.18653/v1/2022.naacl-main.143) |  | 0 | In this paper, we formulate system combination for grammatical error correction (GEC) as a simple machine learning task: binary classification. We demonstrate that with the right problem formulation, a simple logistic regression algorithm can be highly effective for combining GEC models. Our method successfully increases the F0.5 score from the highest base GEC system by 4.2 points on the CoNLL-2014 test set and 7.2 points on the BEA-2019 test set. Furthermore, our method outperforms the state... | Muhammad Reza Qorib, SeungHoon Na, Hwee Tou Ng |  |
| 223 |  |  [Simple Local Attentions Remain Competitive for Long-Context Tasks](https://doi.org/10.18653/v1/2022.naacl-main.144) |  | 0 | Many NLP tasks require processing long contexts beyond the length limit of pretrained models. In order to scale these models to longer text sequences, many efficient long-range attention variants have been proposed. Despite the abundance of research along this direction, it is still difficult to gauge the relative effectiveness of these models in practical use cases, e.g., if we apply these models following the pretrain-and-finetune paradigm. In this work, we aim to conduct a thorough analysis... | Wenhan Xiong, Barlas Oguz, Anchit Gupta, Xilun Chen, Diana Liskovich, Omer Levy, Scott Yih, Yashar Mehdad |  |
| 224 |  |  [Even the Simplest Baseline Needs Careful Re-investigation: A Case Study on XML-CNN](https://doi.org/10.18653/v1/2022.naacl-main.145) |  | 0 | The power and the potential of deep learning models attract many researchers to design advanced and sophisticated architectures. Nevertheless, the progress is sometimes unreal due to various possible reasons. In this work, through an astonishing example we argue that more efforts should be paid to ensure the progress in developing a new deep learning method. For a highly influential multi-label text classification method XML-CNN, we show that the superior performance claimed in the original... | SiAn Chen, JieJyun Liu, TsungHan Yang, HsuanTien Lin, ChihJen Lin |  |
| 225 |  |  [Multi-Relational Graph Transformer for Automatic Short Answer Grading](https://doi.org/10.18653/v1/2022.naacl-main.146) |  | 0 | The recent transition to the online educational domain has increased the need for Automatic Short Answer Grading (ASAG). ASAG automatically evaluates a student’s response against a (given) correct response and thus has been a prevalent semantic matching task. Most existing methods utilize sequential context to compare two sentences and ignore the structural context of the sentence; therefore, these methods may not result in the desired performance. In this paper, we overcome this problem by... | Rajat Agarwal, Varun Khurana, Karish Grover, Mukesh K. Mohania, Vikram Goyal |  |
| 226 |  |  [Event Schema Induction with Double Graph Autoencoders](https://doi.org/10.18653/v1/2022.naacl-main.147) |  | 0 | Event schema depicts the typical structure of complex events, serving as a scaffolding to effectively analyze, predict, and possibly intervene in the ongoing events. To induce event schemas from historical events, previous work uses an event-by-event scheme, ignoring the global structure of the entire schema graph. We propose a new event schema induction framework using double graph autoencoders, which captures the global dependencies among nodes in event graphs. Specifically, we first extract... | Xiaomeng Jin, Manling Li, Heng Ji |  |
| 227 |  |  [CS1QA: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course](https://doi.org/10.18653/v1/2022.naacl-main.148) |  | 0 | We introduce CS1QA, a dataset for code-based question answering in the programming education domain. CS1QA consists of 9,237 question-answer pairs gathered from chat logs in an introductory programming class using Python, and 17,698 unannotated chat data with code. Each question is accompanied with the student’s code, and the portion of the code relevant to answering the question. We carefully design the annotation process to construct CS1QA, and analyze the collected dataset in detail. The... | Changyoon Lee, Yeon Seonwoo, Alice Oh |  |
| 228 |  |  [Unsupervised Cross-Lingual Transfer of Structured Predictors without Source Data](https://doi.org/10.18653/v1/2022.naacl-main.149) |  | 0 | Providing technologies to communities or domains where training data is scarce or protected e.g., for privacy reasons, is becoming increasingly important. To that end, we generalise methods for unsupervised transfer from multiple input models for structured prediction. We show that the means of aggregating over the input models is critical, and that multiplying marginal probabilities of substructures to obtain high-probability structures for distant supervision is substantially better than... | Kemal Kurniawan, Lea Frermann, Philip Schulz, Trevor Cohn |  |
| 229 |  |  [Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation](https://doi.org/10.18653/v1/2022.naacl-main.150) |  | 0 | Neural text generation models are typically trained by maximizing log-likelihood with the sequence cross entropy (CE) loss, which encourages an exact token-by-token match between a target sequence with a generated sequence. Such training objective is sub-optimal when the target sequence is not perfect, e.g., when the target sequence is corrupted with noises, or when only weak sequence supervision is available. To address the challenge, we propose a novel Edit-Invariant Sequence Loss (EISL),... | Guangyi Liu, Zichao Yang, Tianhua Tao, Xiaodan Liang, Junwei Bao, Zhen Li, Xiaodong He, Shuguang Cui, Zhiting Hu |  |
| 230 |  |  [Modeling Exemplification in Long-form Question Answering via Retrieval](https://doi.org/10.18653/v1/2022.naacl-main.151) |  | 0 | Exemplification is a process by which writers explain or clarify a concept by providing an example. While common in all forms of writing, exemplification is particularly useful in the task of long-form question answering (LFQA), where a complicated answer can be made more understandable through simple examples. In this paper, we provide the first computational study of exemplification in QA, performing a fine-grained annotation of different types of examples (e.g., hypotheticals, anecdotes) in... | Shufan Wang, Fangyuan Xu, Laure Thompson, Eunsol Choi, Mohit Iyyer |  |
| 231 |  |  [D2U: Distance-to-Uniform Learning for Out-of-Scope Detection](https://doi.org/10.18653/v1/2022.naacl-main.152) |  | 0 | Supervised training with cross-entropy loss implicitly forces models to produce probability distributions that follow a discrete delta distribution. Model predictions in test time are expected to be similar to delta distributions if the classifier determines the class of an input correctly. However, the shape of the predicted probability distribution can become similar to the uniform distribution when the model cannot infer properly. We exploit this observation for detecting out-of-scope (OOS)... | Eyup Halit Yilmaz, Cagri Toraman |  |
| 232 |  |  [Reference-free Summarization Evaluation via Semantic Correlation and Compression Ratio](https://doi.org/10.18653/v1/2022.naacl-main.153) |  | 0 | A document can be summarized in a number of ways. Reference-based evaluation of summarization has been criticized for its inflexibility. The more sufficient the number of abstracts, the more accurate the evaluation results. However, it is difficult to collect sufficient reference summaries. In this paper, we propose a new automatic reference-free evaluation metric that compares semantic distribution between source document and summary by pretrained language models and considers summary... | Yizhu Liu, Qi Jia, Kenny Q. Zhu |  |
| 233 |  |  [KroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation](https://doi.org/10.18653/v1/2022.naacl-main.154) |  | 0 | The development of over-parameterized pre-trained language models has made a significant contribution toward the success of natural language processing. While over-parameterization of these models is the key to their generalization power, it makes them unsuitable for deployment on low-capacity devices. We push the limits of state-of-the-art Transformer-based pre-trained language model compression using Kronecker decomposition. We present our KroneckerBERT, a compressed version of the BERT_BASE... | Marzieh S. Tahaei, Ella Charlaix, Vahid Partovi Nia, Ali Ghodsi, Mehdi Rezagholizadeh |  |
| 234 |  |  [Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models](https://doi.org/10.18653/v1/2022.naacl-main.155) |  | 0 | Recent open-domain dialogue models have brought numerous breakthroughs. However, building a chat system is not scalable since it often requires a considerable volume of human-human dialogue data, especially when enforcing features such as persona, style, or safety. In this work, we study the challenge of imposing roles on open-domain dialogue systems, with the goal of making the systems maintain consistent roles while conversing naturally with humans. To accomplish this, the system must satisfy... | Sanghwan Bae, DongHyun Kwak, Sungdong Kim, Donghoon Ham, Soyoung Kang, SangWoo Lee, WooMyoung Park |  |
| 235 |  |  [Sentence-Level Resampling for Named Entity Recognition](https://doi.org/10.18653/v1/2022.naacl-main.156) |  | 0 | As a fundamental task in natural language processing, named entity recognition (NER) aims to locate and classify named entities in unstructured text. However, named entities are always the minority among all tokens in the text. This data imbalance problem presents a challenge to machine learning models as their learning objective is usually dominated by the majority of non-entity tokens. To alleviate data imbalance, we propose a set of sentence-level resampling methods where the importance of... | Xiaochen Wang, Yue Wang |  |
| 236 |  |  [Word Tour: One-dimensional Word Embeddings via the Traveling Salesman Problem](https://doi.org/10.18653/v1/2022.naacl-main.157) |  | 0 | Word embeddings are one of the most fundamental technologies used in natural language processing. Existing word embeddings are high-dimensional and consume considerable computational resources. In this study, we propose WordTour, unsupervised one-dimensional word embeddings. To achieve the challenging goal, we propose a decomposition of the desiderata of word embeddings into two parts, completeness and soundness, and focus on soundness in this paper. Owing to the single dimensionality, WordTour... | Ryoma Sato |  |
| 237 |  |  [On the Diversity and Limits of Human Explanations](https://doi.org/10.18653/v1/2022.naacl-main.158) |  | 0 | A growing effort in NLP aims to build datasets of human explanations. However, it remains unclear whether these datasets serve their intended goals. This problem is exacerbated by the fact that the term explanation is overloaded and refers to a broad range of notions with different properties and ramifications. Our goal is to provide an overview of the diversity of explanations, discuss human limitations in providing explanations, and ultimately provide implications for collecting and using... | Chenhao Tan |  |
| 238 |  |  [Locally Aggregated Feature Attribution on Natural Language Model Understanding](https://doi.org/10.18653/v1/2022.naacl-main.159) |  | 0 | With the growing popularity of deep-learning models, model understanding becomes more important. Much effort has been devoted to demystify deep neural networks for better explainability. Some feature attribution methods have shown promising results in computer vision, especially the gradient-based methods where effectively smoothing the gradients with reference data is the key to a robust and faithful result. However, direct application of these gradient-based methods to NLP tasks is not... | Sheng Zhang, Jin Wang, Haitao Jiang, Rui Song |  |
| 239 |  |  [Generic and Trend-aware Curriculum Learning for Relation Extraction](https://doi.org/10.18653/v1/2022.naacl-main.160) |  | 0 | We present a generic and trend-aware curriculum learning approach that effectively integrates textual and structural information in text graphs for relation extraction between entities, which we consider as node pairs in graphs. The proposed model extends existing curriculum learning approaches by incorporating sample-level loss trends to better discriminate easier from harder samples and schedule them for training. The model results in a robust estimation of sample difficulty and shows sizable... | Nidhi Vakil, Hadi Amiri |  |
| 240 |  |  [On Systematic Style Differences between Unsupervised and Supervised MT and an Application for High-Resource Machine Translation](https://doi.org/10.18653/v1/2022.naacl-main.161) |  | 0 | Modern unsupervised machine translation (MT) systems reach reasonable translation quality under clean and controlled data conditions. As the performance gap between supervised and unsupervised MT narrows, it is interesting to ask whether the different training methods result in systematically different output beyond what is visible via quality metrics like adequacy or BLEU. We compare translations from supervised and unsupervised MT systems of similar quality, finding that unsupervised output... | Kelly Marchisio, Markus Freitag, David Grangier |  |
| 241 |  |  [Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks](https://doi.org/10.18653/v1/2022.naacl-main.162) |  | 0 | Retrieval-augmented generation models have shown state-of-the-art performance across many knowledge-intensive NLP tasks such as open-domain question answering and fact verification. These models are trained to generate a final output given retrieved passages that can be irrelevant to an input query, leading to learning spurious cues or memorization. This work introduces a method to incorporate evidentiality of passages—whether a passage contains correct evidence to support the output—into... | Akari Asai, Matt Gardner, Hannaneh Hajishirzi |  |
| 242 |  |  [Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning](https://doi.org/10.18653/v1/2022.naacl-main.163) |  | 0 | Commonsense reasoning systems should be able to generalize to diverse reasoning cases. However, most state-of-the-art approaches depend on expensive data annotations and overfit to a specific benchmark without learning how to perform general semantic reasoning. To overcome these drawbacks, zero-shot QA systems have shown promise as a robust learning scheme by transforming a commonsense knowledge graph (KG) into synthetic QA-form samples for model training. Considering the increasing type of... | Yu Jin Kim, Beongwoo Kwak, Youngwook Kim, Reinald Kim Amplayo, Seungwon Hwang, Jinyoung Yeo |  |
| 243 |  |  [Learning to Express in Knowledge-Grounded Conversation](https://doi.org/10.18653/v1/2022.naacl-main.164) |  | 0 | Grounding dialogue generation by extra knowledge has shown great potentials towards building a system capable of replying with knowledgeable and engaging responses. Existing studies focus on how to synthesize a response with proper knowledge, yet neglect that the same knowledge could be expressed differently by speakers even under the same context. In this work, we mainly consider two aspects of knowledge expression, namely the structure of the response and style of the content in each part. We... | Xueliang Zhao, Tingchen Fu, Chongyang Tao, Wei Wu, Dongyan Zhao, Rui Yan |  |
| 244 |  |  [End-to-End Chinese Speaker Identification](https://doi.org/10.18653/v1/2022.naacl-main.165) |  | 0 | Speaker identification (SI) in texts aims to identify the speaker(s) for each utterance in texts. Previous studies divide SI into several sub-tasks (e.g., quote extraction, named entity recognition, gender identification, and coreference resolution). However, we are still far from solving these sub-tasks, making SI systems that rely on them seriously suffer from error propagation. End-to-end SI systems, on the other hand, are not limited by individual modules, but suffer from insufficient... | Dian Yu, Ben Zhou, Dong Yu |  |
| 245 |  |  [MINION: a Large-Scale and Diverse Dataset for Multilingual Event Detection](https://doi.org/10.18653/v1/2022.naacl-main.166) |  | 0 | Event Detection (ED) is the task of identifying and classifying trigger words of event mentions in text. Despite considerable research efforts in recent years for English text, the task of ED in other languages has been significantly less explored. Switching to non-English languages, important research questions for ED include how well existing ED models perform on different languages, how challenging ED is in other languages, and how well ED knowledge and annotation can be transferred across... | Amir Pouran Ben Veyseh, Minh Van Nguyen, Franck Dernoncourt, Thien Huu Nguyen |  |
| 246 |  |  [Do Prompt-Based Models Really Understand the Meaning of Their Prompts?](https://doi.org/10.18653/v1/2022.naacl-main.167) |  | 0 | Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompts manually written for natural language inference (NLI). We find that models can learn just as fast with many prompts that are intentionally... | Albert Webson, Ellie Pavlick |  |
| 247 |  |  [GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval](https://doi.org/10.18653/v1/2022.naacl-main.168) |  | 0 | Dense retrieval approaches can overcome the lexical gap and lead to significantly improved search results. However, they require large amounts of training data which is not available for most domains. As shown in previous work (Thakur et al., 2021b), the performance of dense retrievers severely degrades under a domain shift. This limits the usage of dense retrieval approaches to only a few domains with large training datasets. In this paper, we propose the novel unsupervised domain adaptation... | Kexin Wang, Nandan Thakur, Nils Reimers, Iryna Gurevych |  |
| 248 |  |  [Sparse Distillation: Speeding Up Text Classification by Using Bigger Student Models](https://doi.org/10.18653/v1/2022.naacl-main.169) |  | 0 | Distilling state-of-the-art transformer models into lightweight student models is an effective way to reduce computation cost at inference time. The student models are typically compact transformers with fewer parameters, while expensive operations such as self-attention persist. Therefore, the improved inference speed may still be unsatisfactory for real-time or high-volume use cases. In this paper, we aim to further push the limit of inference speed by distilling teacher models into bigger,... | Qinyuan Ye, Madian Khabsa, Mike Lewis, Sinong Wang, Xiang Ren, Aaron Jaech |  |
| 249 |  |  [Towards Understanding Large-Scale Discourse Structures in Pre-Trained and Fine-Tuned Language Models](https://doi.org/10.18653/v1/2022.naacl-main.170) |  | 0 | In this paper, we extend the line of BERTology work by focusing on the important, yet less explored, alignment of pre-trained and fine-tuned PLMs with large-scale discourse structures. We propose a novel approach to infer discourse information for arbitrarily long documents. In our experiments, we find that the captured discourse information is local and general, even across a collection of fine-tuning tasks. We compare the inferred discourse trees with supervised, distantly supervised and... | Patrick Huber, Giuseppe Carenini |  |
| 250 |  |  [SAIS: Supervising and Augmenting Intermediate Steps for Document-Level Relation Extraction](https://doi.org/10.18653/v1/2022.naacl-main.171) |  | 0 | Stepping from sentence-level to document-level, the research on relation extraction (RE) confronts increasing text length and more complicated entity interactions. Consequently, it is more challenging to encode the key information sources—relevant contexts and entity types. However, existing methods only implicitly learn to model these critical information sources while being trained for RE. As a result, they suffer the problems of ineffective supervision and uninterpretable model predictions.... | Yuxin Xiao, Zecheng Zhang, Yuning Mao, Carl Yang, Jiawei Han |  |
| 251 |  |  [LITE: Intent-based Task Representation Learning Using Weak Supervision](https://doi.org/10.18653/v1/2022.naacl-main.172) |  | 0 | Users write to-dos as personal notes to themselves, about things they need to complete, remember or organize. To-do texts are usually short and under-specified, which poses a challenge for current text representation models. Yet, understanding and representing their meaning is the first step towards providing intelligent assistance for to-do management. We address this problem by proposing a neural multi-task learning framework, LITE, which extracts representations of English to-do tasks with a... | Naoki Otani, Michael Gamon, Sujay Kumar Jauhar, Mei Yang, Sri Raghu Malireddi, Oriana Riva |  |
| 252 |  |  [Does Summary Evaluation Survive Translation to Other Languages?](https://doi.org/10.18653/v1/2022.naacl-main.173) |  | 0 | The creation of a quality summarization dataset is an expensive, time-consuming effort, requiring the production and evaluation of summaries by both trained humans and machines. The returns to such an effort would increase significantly if the dataset could be used in additional languages without repeating human annotations. To investigate how much we can trust machine translation of summarization datasets, we translate the English SummEval dataset to seven languages and compare performances... | Spencer Braun, Oleg V. Vasilyev, Neslihan Iskender, John Bohannon |  |
| 253 |  |  [A Shoulder to Cry on: Towards A Motivational Virtual Assistant for Assuaging Mental Agony](https://doi.org/10.18653/v1/2022.naacl-main.174) |  | 0 | Mental Health Disorders continue plaguing humans worldwide. Aggravating this situation is the severe shortage of qualified and competent mental health professionals (MHPs), which underlines the need for developing Virtual Assistants (VAs) that can assist MHPs. The data+ML for automation can come from platforms that allow visiting and posting messages in peer-to-peer anonymous manner for sharing their experiences (frequently stigmatized) and seeking support. In this paper, we propose a VA that... | Tulika Saha, Saichethan Miriyala Reddy, Anindya Sundar Das, Sriparna Saha, Pushpak Bhattacharyya |  |
| 254 |  |  [SueNes: A Weakly Supervised Approach to Evaluating Single-Document Summarization via Negative Sampling](https://doi.org/10.18653/v1/2022.naacl-main.175) |  | 0 | Canonical automatic summary evaluation metrics, such as ROUGE, focus on lexical similarity which cannot well capture semantics nor linguistic quality and require a reference summary which is costly to obtain. Recently, there have been a growing number of efforts to alleviate either or both of the two drawbacks. In this paper, we present a proof-of-concept study to a weakly supervised summary evaluation approach without the presence of reference summaries. Massive data in existing summarization... | Forrest Sheng Bao, Ge Luo, Hebi Li, Minghui Qiu, Yinfei Yang, Youbiao He, Cen Chen |  |
| 255 |  |  [Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations](https://doi.org/10.18653/v1/2022.naacl-main.176) |  | 0 | In this paper, we advocate for using large pre-trained monolingual language models in cross lingual zero-shot word sense disambiguation (WSD) coupled with a contextualized mapping mechanism. We also report rigorous experiments that illustrate the effectiveness of employing sparse contextualized word representations obtained via a dictionary learning procedure. Our experimental results demonstrate that the above modifications yield a significant improvement of nearly 6.5 points of increase in... | Gábor Berend |  |
| 256 |  |  [Cheat Codes to Quantify Missing Source Information in Neural Machine Translation](https://doi.org/10.18653/v1/2022.naacl-main.177) |  | 0 | This paper describes a method to quantify the amount of information H(t\|s) added by the target sentence t that is not present in the source s in a neural machine translation system. We do this by providing the model the target sentence in a highly compressed form (a “cheat code”), and exploring the effect of the size of the cheat code. We find that the model is able to capture extra information from just a single float representation of the target and nearly reproduces the target with two... | Proyag Pal, Kenneth Heafield |  |
| 257 |  |  [WiC = TSV = WSD: On the Equivalence of Three Semantic Tasks](https://doi.org/10.18653/v1/2022.naacl-main.178) |  | 0 | The Word-in-Context (WiC) task has attracted considerable attention in the NLP community, as demonstrated by the popularity of the recent MCL-WiC SemEval shared task. Systems and lexical resources from word sense disambiguation (WSD) are often used for the WiC task and WiC dataset construction. In this paper, we establish the exact relationship between WiC and WSD, as well as the related task of target sense verification (TSV). Building upon a novel hypothesis on the equivalence of sense and... | Bradley Hauer, Grzegorz Kondrak |  |
| 258 |  |  [What do tokens know about their characters and how do they know it?](https://doi.org/10.18653/v1/2022.naacl-main.179) |  | 0 | Pre-trained language models (PLMs) that use subword tokenization schemes can succeed at a variety of language tasks that require character-level information, despite lacking explicit access to the character composition of tokens. Here, studying a range of models (e.g., GPT- J, BERT, RoBERTa, GloVe), we probe what word pieces encode about character-level information by training classifiers to predict the presence or absence of a particular alphabetical character in a token, based on its... | Ayush Kaushal, Kyle Mahowald |  |
| 259 |  |  [AnswerSumm: A Manually-Curated Dataset and Pipeline for Answer Summarization](https://doi.org/10.18653/v1/2022.naacl-main.180) |  | 0 | Community Question Answering (CQA) fora such as Stack Overflow and Yahoo! Answers contain a rich resource of answers to a wide range of community-based questions. Each question thread can receive a large number of answers with different perspectives. One goal of answer summarization is to produce a summary that reflects the range of answer perspectives. A major obstacle for this task is the absence of a dataset to provide supervision for producing such summaries. Recent works propose heuristics... | Alexander R. Fabbri, Xiaojian Wu, Srini Iyer, Haoran Li, Mona T. Diab |  |
| 260 |  |  [Paragraph-based Transformer Pre-training for Multi-Sentence Inference](https://doi.org/10.18653/v1/2022.naacl-main.181) |  | 0 | Inference tasks such as answer sentence selection (AS2) or fact verification are typically solved by fine-tuning transformer-based models as individual sentence-pair classifiers. Recent studies show that these tasks benefit from modeling dependencies across multiple candidate sentences jointly. In this paper, we first show that popular pre-trained transformers perform poorly when used for fine-tuning on multi-candidate inference tasks. We then propose a new pre-training objective that models... | Luca Di Liello, Siddhant Garg, Luca Soldaini, Alessandro Moschitti |  |
| 261 |  |  [Text Style Transfer via Optimal Transport](https://doi.org/10.18653/v1/2022.naacl-main.182) |  | 0 | Text style transfer (TST) is a well-known task whose goal is to convert the style of the text (e.g., from formal to informal) while preserving its content. Recently, it has been shown that both syntactic and semantic similarities between the source and the converted text are important for TST. However, the interaction between these two concepts has not been modeled. In this work, we propose a novel method based on Optimal Transport for TST to simultaneously incorporate syntactic and semantic... | Nasim Nouri |  |
| 262 |  |  [Exploring the Role of Task Transferability in Large-Scale Multi-Task Learning](https://doi.org/10.18653/v1/2022.naacl-main.183) |  | 0 | Recent work has found that multi-task training with a large number of diverse tasks can uniformly improve downstream performance on unseen target tasks. In contrast, literature on task transferability has established that the choice of intermediate tasks can heavily affect downstream task performance. In this work, we aim to disentangle the effect of scale and relatedness of tasks in multi-task representation learning. We find that, on average, increasing the scale of multi-task learning, in... | Vishakh Padmakumar, Leonard Lausen, Miguel Ballesteros, Sheng Zha, He He, George Karypis |  |
| 263 |  |  [Interactive Query-Assisted Summarization via Deep Reinforcement Learning](https://doi.org/10.18653/v1/2022.naacl-main.184) |  | 0 | Interactive summarization is a task that facilitates user-guided exploration of information within a document set. While one would like to employ state of the art neural models to improve the quality of interactive summarization, many such technologies cannot ingest the full document set or cannot operate at sufficient speed for interactivity. To that end, we propose two novel deep reinforcement learning models for the task that address, respectively, the subtask of summarizing salient... | Ori Shapira, Ramakanth Pasunuru, Mohit Bansal, Ido Dagan, Yael Amsterdamer |  |
| 264 |  |  [Data Augmentation with Dual Training for Offensive Span Detection](https://doi.org/10.18653/v1/2022.naacl-main.185) |  | 0 | Recognizing offensive text is an important requirement for every content management system, especially for social networks. While the majority of the prior work formulate this problem as text classification, i.e., if a text excerpt is offensive or not, in this work we propose a novel model for offensive span detection (OSD), whose goal is to identify the spans responsible for the offensive tone of the text. One of the challenges to train a model for this novel setting is the lack of enough... | Nasim Nouri |  |
| 265 |  |  [Training Mixed-Domain Translation Models via Federated Learning](https://doi.org/10.18653/v1/2022.naacl-main.186) |  | 0 | Training mixed-domain translation models is a complex task that demands tailored architec- tures and costly data preparation techniques. In this work, we leverage federated learning (FL) in order to tackle the problem. Our investiga- tion demonstrates that with slight modifications in the training process, neural machine trans- lation (NMT) engines can be easily adapted when an FL-based aggregation is applied to fuse different domains. Experimental results also show that engines built via FL... | Peyman Passban, Tanya G. Roosta, Rahul Gupta, Ankit Chadha, Clement Chung |  |
| 266 |  |  [QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization](https://doi.org/10.18653/v1/2022.naacl-main.187) |  | 0 | Factual consistency is an essential quality of text summarization models in practical settings. Existing work in evaluating this dimension can be broadly categorized into two lines of research, entailment-based and question answering (QA)-based metrics, and different experimental setups often lead to contrasting conclusions as to which paradigm performs the best. In this work, we conduct an extensive comparison of entailment and QA-based metrics, demonstrating that carefully choosing the... | Alexander R. Fabbri, ChienSheng Wu, Wenhao Liu, Caiming Xiong |  |
| 267 |  |  [How Gender Debiasing Affects Internal Model Representations, and Why It Matters](https://doi.org/10.18653/v1/2022.naacl-main.188) |  | 0 | Common studies of gender bias in NLP focus either on extrinsic bias measured by model performance on a downstream task or on intrinsic bias found in models’ internal representations. However, the relationship between extrinsic and intrinsic bias is relatively unknown. In this work, we illuminate this relationship by measuring both quantities together: we debias a model during downstream fine-tuning, which reduces extrinsic bias, and measure the effect on intrinsic bias, which is operationalized... | Hadas Orgad, Seraphina GoldfarbTarrant, Yonatan Belinkov |  |
| 268 |  |  [A Structured Span Selector](https://doi.org/10.18653/v1/2022.naacl-main.189) |  | 0 | Many natural language processing tasks, e.g., coreference resolution and semantic role labeling, require selecting text spans and making decisions about them. A typical approach to such tasks is to score all possible spans and greedily select spans for task-specific downstream processing. This approach, however, does not incorporate any inductive bias about what sort of spans ought to be selected, e.g., that selected spans tend to be syntactic constituents. In this paper, we propose a novel... | Tianyu Liu, Yuchen Eleanor Jiang, Ryan Cotterell, Mrinmaya Sachan |  |
| 269 |  |  [Unified Semantic Typing with Meaningful Label Inference](https://doi.org/10.18653/v1/2022.naacl-main.190) |  | 0 | Semantic typing aims at classifying tokens or spans of interest in a textual context into semantic categories such as relations, entity types, and event types. The inferred labels of semantic categories meaningfully interpret how machines understand components of text. In this paper, we present UniST, a unified framework for semantic typing that captures label semantics by projecting both inputs and labels into a joint semantic embedding space. To formulate different lexical and relational... | James Y. Huang, Bangzheng Li, Jiashu Xu, Muhao Chen |  |
| 270 |  |  [Learning To Retrieve Prompts for In-Context Learning](https://doi.org/10.18653/v1/2022.naacl-main.191) |  | 0 | In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompts). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM. Given an... | Ohad Rubin, Jonathan Herzig, Jonathan Berant |  |
| 271 |  |  [Necessity and Sufficiency for Explaining Text Classifiers: A Case Study in Hate Speech Detection](https://doi.org/10.18653/v1/2022.naacl-main.192) |  | 0 | We present a novel feature attribution method for explaining text classifiers, and analyze it in the context of hate speech detection. Although feature attribution models usually provide a single importance score for each token, we instead provide two complementary and theoretically-grounded scores – necessity and sufficiency – resulting in more informative explanations. We propose a transparent method that calculates these values by generating explicit perturbations of the input text, allowing... | Esma Balkir, Isar Nejadgholi, Kathleen C. Fraser, Svetlana Kiritchenko |  |
| 272 |  |  [Learning to Retrieve Passages without Supervision](https://doi.org/10.18653/v1/2022.naacl-main.193) |  | 0 | Dense retrievers for open-domain question answering (ODQA) have been shown to achieve impressive performance by training on large datasets of question-passage pairs. In this work we ask whether this dependence on labeled data can be reduced via unsupervised pretraining that is geared towards ODQA. We show this is in fact possible, via a novel pretraining scheme designed for retrieval. Our “recurring span retrieval” approach uses recurring spans across passages in a document to create pseudo... | Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, Amir Globerson |  |
| 273 |  |  [Re2G: Retrieve, Rerank, Generate](https://doi.org/10.18653/v1/2022.naacl-main.194) |  | 0 | As demonstrated by GPT-3 and T5, transformers grow in capability as parameter spaces become larger and larger. However, for tasks that require a large amount of knowledge, non-parametric memory allows models to grow dramatically with a sub-linear increase in computational cost and GPU memory requirements. Recent models such as RAG and REALM have introduced retrieval into conditional generation. These models incorporate neural initial retrieval from a corpus of passages. We build on this line of... | Michael R. Glass, Gaetano Rossiello, Md. Faisal Mahbub Chowdhury, Ankita Naik, Pengshan Cai, Alfio Gliozzo |  |
| 274 |  |  [Don't sweat the small stuff, classify the rest: Sample Shielding to protect text classifiers against adversarial attacks](https://doi.org/10.18653/v1/2022.naacl-main.195) |  | 0 | Deep learning (DL) is being used extensively for text classification. However, researchers have demonstrated the vulnerability of such classifiers to adversarial attacks. Attackers modify the text in a way which misleads the classifier while keeping the original meaning close to intact. State-of-the-art (SOTA) attack algorithms follow the general principle of making minimal changes to the text so as to not jeopardize semantics. Taking advantage of this we propose a novel and intuitive defense... | Jonathan Rusert, Padmini Srinivasan |  |
| 275 |  |  [Federated Learning with Noisy User Feedback](https://doi.org/10.18653/v1/2022.naacl-main.196) |  | 0 | Machine Learning (ML) systems are getting increasingly popular, and drive more and more applications and services in our daily life. Thishas led to growing concerns over user privacy, since human interaction data typically needs to be transmitted to the cloud in order to trainand improve such systems. Federated learning (FL) has recently emerged as a method for training ML models on edge devices using sensitive user data and is seen as a way to mitigate concerns over data privacy. However,... | Rahul Sharma, Anil Ramakrishna, Ansel MacLaughlin, Anna Rumshisky, Jimit Majmudar, Clement Chung, Salman Avestimehr, Rahul Gupta |  |
| 276 |  |  [Gender Bias in Masked Language Models for Multiple Languages](https://doi.org/10.18653/v1/2022.naacl-main.197) |  | 0 | Masked Language Models (MLMs) pre-trained by predicting masked tokens on large corpora have been used successfully in natural language processing tasks for a variety of languages. Unfortunately, it was reported that MLMs also learn discriminative biases regarding attributes such as gender and race. Because most studies have focused on MLMs in English, the bias of MLMs in other languages has rarely been investigated. Manual annotation of evaluation data for languages other than English has been... | Masahiro Kaneko, Aizhan Imankulova, Danushka Bollegala, Naoaki Okazaki |  |
| 277 |  |  [Multi-Domain Targeted Sentiment Analysis](https://doi.org/10.18653/v1/2022.naacl-main.198) |  | 0 | Targeted Sentiment Analysis (TSA) is a central task for generating insights from consumer reviews. Such content is extremely diverse, with sites like Amazon or Yelp containing reviews on products and businesses from many different domains. A real-world TSA system should gracefully handle that diversity. This can be achieved by a multi-domain model – one that is robust to the domain of the analyzed texts, and performs well on various domains. To address this scenario, we present a multi-domain... | Orith ToledoRonen, Matan Orbach, Yoav Katz, Noam Slonim |  |
| 278 |  |  [Falsesum: Generating Document-level NLI Examples for Recognizing Factual Inconsistency in Summarization](https://doi.org/10.18653/v1/2022.naacl-main.199) |  | 0 | Neural abstractive summarization models are prone to generate summaries that are factually inconsistent with their source documents. Previous work has introduced the task of recognizing such factual inconsistency as a downstream application of natural language inference (NLI). However, state-of-the-art NLI models perform poorly in this context due to their inability to generalize to the target task. In this work, we show that NLI models can be effective for this task when the training data is... | Prasetya Ajie Utama, Joshua Bambrick, Nafise Sadat Moosavi, Iryna Gurevych |  |
| 279 |  |  [Dynamic Gazetteer Integration in Multilingual Models for Cross-Lingual and Cross-Domain Named Entity Recognition](https://doi.org/10.18653/v1/2022.naacl-main.200) |  | 0 | Named entity recognition (NER) in a real-world setting remains challenging and is impacted by factors like text genre, corpus quality, and data availability. NER models trained on CoNLL do not transfer well to other domains, even within the same language. This is especially the case for multi-lingual models when applied to low-resource languages, and is mainly due to missing entity information. We propose an approach that with limited effort and data, addresses the NER knowledge gap across... | Besnik Fetahu, Anjie Fang, Oleg Rokhlenko, Shervin Malmasi |  |
| 280 |  |  [MetaICL: Learning to Learn In Context](https://doi.org/10.18653/v1/2022.naacl-main.201) |  | 0 | We introduce MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks. This meta-training enables the model to more effectively learn a new task in context at test time, by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP... | Sewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi |  |
| 281 |  |  [Enhancing Knowledge Selection for Grounded Dialogues via Document Semantic Graphs](https://doi.org/10.18653/v1/2022.naacl-main.202) |  | 0 | Providing conversation models with background knowledge has been shown to make open-domain dialogues more informative and engaging. Existing models treat knowledge selection as a sentence ranking or classification problem where each sentence is handled individually, ignoring the internal semantic connection between sentences. In this work, we propose to automatically convert the background knowledge documents into document semantic graphs and then perform knowledge selection over such graphs.... | Sha Li, Mahdi Namazifar, Di Jin, Mohit Bansal, Heng Ji, Yang Liu, Dilek HakkaniTur |  |
| 282 |  |  [Using Natural Sentence Prompts for Understanding Biases in Language Models](https://doi.org/10.18653/v1/2022.naacl-main.203) |  | 0 | Evaluation of biases in language models is often limited to synthetically generated datasets. This dependence traces back to the need of prompt-style dataset to trigger specific behaviors of language models. In this paper, we address this gap by creating a prompt dataset with respect to occupations collected from real-world natural sentences present in Wikipedia.We aim to understand the differences between using template-based prompts and natural sentence prompts when studying gender-occupation... | Sarah Alnegheimish, Alicia Guo, Yi Sun |  |
| 283 |  |  [Robust Conversational Agents against Imperceptible Toxicity Triggers](https://doi.org/10.18653/v1/2022.naacl-main.204) |  | 0 | Warning: this paper contains content that maybe offensive or upsetting. Recent research in Natural Language Processing (NLP) has advanced the development of various toxicity detection models with the intention of identifying and mitigating toxic language from existing systems. Despite the abundance of research in this area, less attention has been given to adversarial attacks that force the system to generate toxic language and the defense against them. Existing work to generate such attacks is... | Ninareh Mehrabi, Ahmad Beirami, Fred Morstatter, Aram Galstyan |  |
| 284 |  |  [Selective Differential Privacy for Language Modeling](https://doi.org/10.18653/v1/2022.naacl-main.205) |  | 0 | With the increasing applications of language models, it has become crucial to protect these models from leaking private information. Previous work has attempted to tackle this challenge by training RNN-based language models with differential privacy guarantees. However, applying classical differential privacy to language models leads to poor model performance as the underlying privacy notion is over-pessimistic and provides undifferentiated protection for all tokens in the data. Given that the... | Weiyan Shi, Aiqi Cui, Evan Li, Ruoxi Jia, Zhou Yu |  |
| 285 |  |  [Do Trajectories Encode Verb Meaning?](https://doi.org/10.18653/v1/2022.naacl-main.206) |  | 0 | Distributional models learn representations of words from text, but are criticized for their lack of grounding, or the linking of text to the non-linguistic world. Grounded language models have had success in learning to connect concrete categories like nouns and adjectives to the world via images and videos, but can struggle to isolate the meaning of the verbs themselves from the context in which they typically occur. In this paper, we investigate the extent to which trajectories (i.e. the... | Dylan Ebert, Chen Sun, Ellie Pavlick |  |
| 286 |  |  [Long Context Question Answering via Supervised Contrastive Learning](https://doi.org/10.18653/v1/2022.naacl-main.207) |  | 0 | Long-context question answering (QA) tasks require reasoning over a long document or multiple documents. Addressing these tasks often benefits from identifying a set of evidence spans (e.g., sentences), which provide supporting evidence for answering the question. In this work, we propose a novel method for equipping long-context QA models with an additional sequence-level objective for better identification of the supporting evidence. We achieve this via an additional contrastive supervision... | Avi Caciularu, Ido Dagan, Jacob Goldberger, Arman Cohan |  |
| 287 |  |  [The USMLE® Step 2 Clinical Skills Patient Note Corpus](https://doi.org/10.18653/v1/2022.naacl-main.208) |  | 0 | This paper presents a corpus of 43,985 clinical patient notes (PNs) written by 35,156 examinees during the high-stakes USMLE® Step 2 Clinical Skills examination. In this exam, examinees interact with standardized patients - people trained to portray simulated scenarios called clinical cases. For each encounter, an examinee writes a PN, which is then scored by physician raters using a rubric of clinical concepts, expressions of which should be present in the PN. The corpus features PNs from 10... | Victoria Yaneva, Janet Mee, Le An Ha, Polina Harik, Michael Jodoin, Alex Mechaber |  |
| 288 |  |  [Learning to Borrow- Relation Representation for Without-Mention Entity-Pairs for Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.naacl-main.209) |  | 0 | Prior work on integrating text corpora with knowledge graphs (KGs) to improve Knowledge Graph Embedding (KGE) have obtained good performance for entities that co-occur in sentences in text corpora. Such sentences (textual mentions of entity-pairs) are represented as Lexicalised Dependency Paths (LDPs) between two entities. However, it is not possible to represent relations between entities that do not co-occur in a single sentence using LDPs. In this paper, we propose and evaluate several... | Huda Hakami, Mona Hakami, Angrosh Mandya, Danushka Bollegala |  |
| 289 |  |  [Improving Entity Disambiguation by Reasoning over a Knowledge Base](https://doi.org/10.18653/v1/2022.naacl-main.210) |  | 0 | Recent work in entity disambiguation (ED) has typically neglected structured knowledge base (KB) facts, and instead relied on a limited subset of KB information, such as entity descriptions or types. This limits the range of contexts in which entities can be disambiguated. To allow the use of all KB facts, as well as descriptions and types, we introduce an ED model which links entities by reasoning over a symbolic knowledge base in a fully differentiable fashion. Our model surpasses... | Tom Ayoola, Joseph Fisher, Andrea Pierleoni |  |
| 290 |  |  [Modal Dependency Parsing via Language Model Priming](https://doi.org/10.18653/v1/2022.naacl-main.211) |  | 0 | The task of modal dependency parsing aims to parse a text into its modal dependency structure, which is a representation for the factuality of events in the text. We design a modal dependency parser that is based on priming pre-trained language models, and evaluate the parser on two data sets. Compared to baselines, we show an improvement of 2.6% in F-score for English and 4.6% for Chinese. To the best of our knowledge, this is also the first work on Chinese modal dependency parsing. | Jiarui Yao, Nianwen Xue, Bonan Min |  |
| 291 |  |  [Document-Level Relation Extraction with Sentences Importance Estimation and Focusing](https://doi.org/10.18653/v1/2022.naacl-main.212) |  | 0 | Document-level relation extraction (DocRE) aims to determine the relation between two entities from a document of multiple sentences. Recent studies typically represent the entire document by sequence- or graph-based models to predict the relations of all entity pairs. However, we find that such a model is not robust and exhibits bizarre behaviors: it predicts correctly when an entire test document is fed as input, but errs when non-evidence sentences are removed. To this end, we propose a... | Wang Xu, Kehai Chen, Lili Mou, Tiejun Zhao |  |
| 292 |  |  [Are All the Datasets in Benchmark Necessary? A Pilot Study of Dataset Evaluation for Text Classification](https://doi.org/10.18653/v1/2022.naacl-main.213) |  | 0 | In this paper, we ask the research question of whether all the datasets in the benchmark are necessary. We approach this by first characterizing the distinguishability of datasets when comparing different systems. Experiments on 9 datasets and 36 systems show that several existing benchmark datasets contribute little to discriminating top-scoring systems, while those less used datasets exhibit impressive discriminative power. We further, taking the text classification task as a case study,... | Yang Xiao, Jinlan Fu, SeeKiong Ng, Pengfei Liu |  |
| 293 |  |  [Triggerless Backdoor Attack for NLP Tasks with Clean Labels](https://doi.org/10.18653/v1/2022.naacl-main.214) |  | 0 | Backdoor attacks pose a new threat to NLP models. A standard strategy to construct poisoned data in backdoor attacks is to insert triggers (e.g., rare words) into selected sentences and alter the original label to a target label. This strategy comes with a severe flaw of being easily detected from both the trigger and the label perspectives: the trigger injected, which is usually a rare word, leads to an abnormal natural language expression, and thus can be easily detected by a defense model;... | Leilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Yi Yang, Shangwei Guo, Chun Fan |  |
| 294 |  |  [PPL-MCTS: Constrained Textual Generation Through Discriminator-Guided MCTS Decoding](https://doi.org/10.18653/v1/2022.naacl-main.215) |  | 0 | Large language models (LM) based on Transformers allow to generate plausible long texts. In this paper, we explore how this generation can be further controlled at decoding time to satisfy certain constraints (e.g. being non-toxic, conveying certain emotions, using a specific writing style, etc.) without fine-tuning the LM.Precisely, we formalize constrained generation as a tree exploration process guided by a discriminator that indicates how well the associated sequence respects the... | Antoine Chaffin, Vincent Claveau, Ewa Kijak |  |
| 295 |  |  [Interpretable Proof Generation via Iterative Backward Reasoning](https://doi.org/10.18653/v1/2022.naacl-main.216) |  | 0 | We present IBR, an Iterative Backward Reasoning model to solve the proof generation tasks on rule-based Question Answering (QA), where models are required to reason over a series of textual rules and facts to find out the related proof path and derive the final answer. We handle the limitations of existed works in two folds: 1) enhance the interpretability of reasoning procedures with detailed tracking, by predicting nodes and edges in the proof path iteratively backward from the question; 2)... | Hanhao Qu, Yu Cao, Jun Gao, Liang Ding, Ruifeng Xu |  |
| 296 |  |  [Domain Confused Contrastive Learning for Unsupervised Domain Adaptation](https://doi.org/10.18653/v1/2022.naacl-main.217) |  | 0 | In this work, we study Unsupervised Domain Adaptation (UDA) in a challenging self-supervised approach. One of the difficulties is how to learn task discrimination in the absence of target labels. Unlike previous literature which directly aligns cross-domain distributions or leverages reverse gradient, we propose Domain Confused Contrastive Learning (DCCL), which can bridge the source and target domains via domain puzzles, and retain discriminative representations after adaptation. Technically,... | Quanyu Long, Tianze Luo, Wenya Wang, Sinno Jialin Pan |  |
| 297 |  |  [Incorporating Centering Theory into Neural Coreference Resolution](https://doi.org/10.18653/v1/2022.naacl-main.218) |  | 0 | In recent years, transformer-based coreference resolution systems have achieved remarkable improvements on the CoNLL dataset. However, how coreference resolvers can benefit from discourse coherence is still an open question. In this paper, we propose to incorporate centering transitions derived from centering theory in the form of a graph into a neural coreference model. Our method improves the performance over the SOTA baselines, especially on pronoun resolution in long documents, formal... | Haixia Chai, Michael Strube |  |
| 298 |  |  [Progressive Class Semantic Matching for Semi-supervised Text Classification](https://doi.org/10.18653/v1/2022.naacl-main.219) |  | 0 | Semi-supervised learning is a promising way to reduce the annotation cost for text-classification. Combining with pre-trained language models (PLMs), e.g., BERT, recent semi-supervised learning methods achieved impressive performance. In this work, we further investigate the marriage between semi-supervised learning and a pre-trained language model. Unlike existing approaches that utilize PLMs only for model parameter initialization, we explore the inherent topic matching capability inside PLMs... | HaiMing Xu, Lingqiao Liu, Ehsan Abbasnejad |  |
| 299 |  |  [Low Resource Style Transfer via Domain Adaptive Meta Learning](https://doi.org/10.18653/v1/2022.naacl-main.220) |  | 0 | Text style transfer (TST) without parallel data has achieved some practical success. However, most of the existing unsupervised text style transfer methods suffer from (i) requiring massive amounts of non-parallel data to guide transferring different text styles. (ii) colossal performance degradation when fine-tuning the model in new domains. In this work, we propose DAML-ATM (Domain Adaptive Meta-Learning with Adversarial Transfer Model), which consists of two parts: DAML and ATM. DAML is a... | Xiangyang Li, Xiang Long, Yu Xia, Sujian Li |  |
| 300 |  |  [Features or Spurious Artifacts? Data-centric Baselines for Fair and Robust Hate Speech Detection](https://doi.org/10.18653/v1/2022.naacl-main.221) |  | 0 | Avoiding to rely on dataset artifacts to predict hate speech is at the cornerstone of robust and fair hate speech detection. In this paper we critically analyze lexical biases in hate speech detection via a cross-platform study, disentangling various types of spurious and authentic artifacts and analyzing their impact on out-of-distribution fairness and robustness. We experiment with existing approaches and propose simple yet surprisingly effective data-centric baselines. Our results on English... | Alan Ramponi, Sara Tonelli |  |
| 301 |  |  [Document-Level Event Argument Extraction by Leveraging Redundant Information and Closed Boundary Loss](https://doi.org/10.18653/v1/2022.naacl-main.222) |  | 0 | In document-level event argument extraction, an argument is likely to appear multiple times in different expressions in the document. The redundancy of arguments underlying multiple sentences is beneficial but is often overlooked. In addition, in event argument extraction, most entities are regarded as class “others”, i.e. Universum class, which is defined as a collection of samples that do not belong to any class of interest. Universum class is composed of heterogeneous entities without... | Hanzhang Zhou, Kezhi Mao |  |
| 302 |  |  [A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African News Translation](https://doi.org/10.18653/v1/2022.naacl-main.223) |  | 0 | Recent advances in the pre-training for language models leverage large-scale datasets to create multilingual models. However, low-resource languages are mostly left out in these datasets. This is primarily because many widely spoken languages that are not well represented on the web and therefore excluded from the large-scale crawls for datasets. Furthermore, downstream users of these models are restricted to the selection of languages originally chosen for pre-training. This work investigates... | David Ifeoluwa Adelani, Jesujoba O. Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter, Dietrich Klakow, Peter Nabende, Ernie Chang, Tajuddeen Gwadabe, Freshia Sackey, Bonaventure F. P. Dossou, Chris Emezue, Colin Leong, Michael Beukman, Shamsuddeen Hassan Muhammad, Guyo Dub Jarso, Oreen Yousuf, Andre Niyongabo Rubungo, Gilles Hacheme, Eric Peter Wairagala, Muhammad Umair Nasir, Benjamin Ajibade, Tunde Ajayi, Yvonne Wambui Gitau, Jade Z. Abbott, Mohamed Ahmed, Millicent Ochieng, Aremu Anuoluwapo, Perez Ogayo, Jonathan Mukiibi, Fatoumata Ouoba Kabore, Godson Kalipe, Derguene Mbaye, Allahsera Auguste Tapo, Victoire Memdjokam Koagne, Edwin MunkohBuabeng, Valencia Wagner, Idris Abdulmumin, Ayodele Awokoya, Happy Buzaaba, Blessing K. Sibanda, Andiswa Bukula, Sam Manthalu |  |
| 303 |  |  [Should We Rely on Entity Mentions for Relation Extraction? Debiasing Relation Extraction with Counterfactual Analysis](https://doi.org/10.18653/v1/2022.naacl-main.224) |  | 0 | Recent literature focuses on utilizing the entity information in the sentence-level relation extraction (RE), but this risks leaking superficial and spurious clues of relations. As a result, RE still suffers from unintended entity bias, i.e., the spurious correlation between entity mentions (names) and relations. Entity bias can mislead the RE models to extract the relations that do not exist in the text. To combat this issue, some previous work masks the entity mentions to prevent the RE... | Yiwei Wang, Muhao Chen, Wenxuan Zhou, Yujun Cai, Yuxuan Liang, Dayiheng Liu, Baosong Yang, Juncheng Liu, Bryan Hooi |  |
| 304 |  |  [Analyzing Encoded Concepts in Transformer Language Models](https://doi.org/10.18653/v1/2022.naacl-main.225) |  | 0 | We propose a novel framework ConceptX, to analyze how latent concepts are encoded in representations learned within pre-trained lan-guage models. It uses clustering to discover the encoded concepts and explains them by aligning with a large set of human-defined concepts. Our analysis on seven transformer language models reveal interesting insights: i) the latent space within the learned representations overlap with different linguistic concepts to a varying degree, ii) the lower layers in the... | Hassan Sajjad, Nadir Durrani, Fahim Dalvi, Firoj Alam, Abdul Rafae Khan, Jia Xu |  |
| 305 |  |  [Boosted Dense Retriever](https://doi.org/10.18653/v1/2022.naacl-main.226) |  | 0 | We propose DrBoost, a dense retrieval ensemble inspired by boosting. DrBoost is trained in stages: each component model is learned sequentially and specialized by focusing only on retrieval mistakes made by the current ensemble. The final representation is the concatenation of the output vectors of all the component models, making it a drop-in replacement for standard dense retrievers at test time. DrBoost enjoys several advantages compared to standard dense retrieval models. It produces... | Patrick Lewis, Barlas Oguz, Wenhan Xiong, Fabio Petroni, Scott Yih, Sebastian Riedel |  |
| 306 |  |  [MuCGEC: a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2022.naacl-main.227) |  | 0 | This paper presents MuCGEC, a multi-reference multi-source evaluation dataset for Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences collected from three Chinese-as-a-Second-Language (CSL) learner sources. Each sentence is corrected by three annotators, and their corrections are carefully reviewed by a senior annotator, resulting in 2.3 references per sentence. We conduct experiments with two mainstream CGEC models, i.e., the sequence-to-sequence model and the... | Yue Zhang, Zhenghua Li, Zuyi Bao, Jiacheng Li, Bo Zhang, Chen Li, Fei Huang, Min Zhang |  |
| 307 |  |  [NeuS: Neutral Multi-News Summarization for Mitigating Framing Bias](https://doi.org/10.18653/v1/2022.naacl-main.228) |  | 0 | Media news framing bias can increase political polarization and undermine civil society. The need for automatic mitigation methods is therefore growing. We propose a new task, a neutral summary generation from multiple news articles of the varying political leaningsto facilitate balanced and unbiased news reading. In this paper, we first collect a new dataset, illustrate insights about framing bias through a case study, and propose a new effective metric and model (NeuS-Title) for the task.... | Nayeon Lee, Yejin Bang, Tiezheng Yu, Andrea Madotto, Pascale Fung |  |
| 308 |  |  [Enhance Incomplete Utterance Restoration by Joint Learning Token Extraction and Text Generation](https://doi.org/10.18653/v1/2022.naacl-main.229) |  | 0 | This paper introduces a model for incomplete utterance restoration (IUR) called JET (Joint learning token Extraction and Text generation). Different from prior studies that only work on extraction or abstraction datasets, we design a simple but effective model, working for both scenarios of IUR. Our design simulates the nature of IUR, where omitted tokens from the context contribute to restoration. From this, we construct a Picker that identifies the omitted tokens. To support the picker, we... | Shumpei Inoue, Tsungwei Liu, Son Nguyen, MinhTien Nguyen |  |
| 309 |  |  [Efficient Constituency Tree based Encoding for Natural Language to Bash Translation](https://doi.org/10.18653/v1/2022.naacl-main.230) |  | 0 | Bash is a Unix command language used for interacting with the Operating System. Recent works on natural language to Bash translation have made significant advances, but none of the previous methods utilize the problem’s inherent structure. We identify this structure andpropose a Segmented Invocation Transformer (SIT) that utilizes the information from the constituency parse tree of the natural language text. Our method is motivated by the alignment between segments in the natural language text... | Shikhar Bharadwaj, Shirish K. Shevade |  |
| 310 |  |  [Privacy-Preserving Text Classification on BERT Embeddings with Homomorphic Encryption](https://doi.org/10.18653/v1/2022.naacl-main.231) |  | 0 | Embeddings, which compress information in raw text into semantics-preserving low-dimensional vectors, have been widely adopted for their efficacy. However, recent research has shown that embeddings can potentially leak private information about sensitive attributes of the text, and in some cases, can be inverted to recover the original input text. To address these growing privacy challenges, we propose a privatization mechanism for embeddings based on homomorphic encryption, to prevent... | Garam Lee, Minsoo Kim, Jai Hyun Park, Seungwon Hwang, Jung Hee Cheon |  |
| 311 |  |  [ITA: Image-Text Alignments for Multi-Modal Named Entity Recognition](https://doi.org/10.18653/v1/2022.naacl-main.232) |  | 0 | Recently, Multi-modal Named Entity Recognition (MNER) has attracted a lot of attention. Most of the work utilizes image information through region-level visual representations obtained from a pretrained object detector and relies on an attention mechanism to model the interactions between image and text representations. However, it is difficult to model such interactions as image and text representations are trained separately on the data of their respective modality and are not aligned in the... | Xinyu Wang, Min Gui, Yong Jiang, Zixia Jia, Nguyen Bach, Tao Wang, Zhongqiang Huang, Kewei Tu |  |
| 312 |  |  [A Dataset for N-ary Relation Extraction of Drug Combinations](https://doi.org/10.18653/v1/2022.naacl-main.233) |  | 0 | Combination therapies have become the standard of care for diseases such as cancer, tuberculosis, malaria and HIV. However, the combinatorial set of available multi-drug treatments creates a challenge in identifying effective combination therapies available in a situation. To assist medical professionals in identifying beneficial drug-combinations, we construct an expert-annotated dataset for extracting information about the efficacy of drug combinations from the scientific literature. Beyond... | Aryeh Tiktinsky, Vijay Viswanathan, Danna Niezni, Dana Meron Azagury, Yosi Shamay, Hillel TaubTabib, Tom Hope, Yoav Goldberg |  |
| 313 |  |  [Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding](https://doi.org/10.18653/v1/2022.naacl-main.234) |  | 0 | In the age of large transformer language models, linguistic evaluation play an important role in diagnosing models’ abilities and limitations on natural language understanding. However, current evaluation methods show some significant shortcomings. In particular, they do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. Thus they fail to effectively map out the aspects of language understanding that remain... | Zeming Chen, Qiyue Gao |  |
| 314 |  |  [Neural Language Taskonomy: Which NLP Tasks are the most Predictive of fMRI Brain Activity?](https://doi.org/10.18653/v1/2022.naacl-main.235) |  | 0 | Several popular Transformer based language models have been found to be successful for text-driven brain encoding. However, existing literature leverages only pretrained text Transformer models and has not explored the efficacy of task-specific learned Transformer representations. In this work, we explore transfer learning from representations learned for ten popular natural language processing tasks (two syntactic and eight semantic) for predicting brain responses from two diverse datasets:... | Subba Reddy Oota, Jashn Arora, Veeral Agarwal, Mounika Marreddy, Manish Gupta, Bapi Raju Surampudi |  |
| 315 |  |  [FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations](https://doi.org/10.18653/v1/2022.naacl-main.236) |  | 0 | Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improvements in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FactGraph, a method that decomposes the... | Leonardo F. R. Ribeiro, Mengwen Liu, Iryna Gurevych, Markus Dreyer, Mohit Bansal |  |
| 316 |  |  [Unsupervised Paraphrasability Prediction for Compound Nominalizations](https://doi.org/10.18653/v1/2022.naacl-main.237) |  | 0 | Commonly found in academic and formal texts, a nominalization uses a deverbal noun to describe an event associated with its corresponding verb. Nominalizations can be difficult to interpret because of ambiguous semantic relations between the deverbal noun and its arguments. Automatic generation of clausal paraphrases for nominalizations can help disambiguate their meaning. However, previous work has not identified cases where it is awkward or impossible to paraphrase a compound nominalization.... | John Sie Yuen Lee, Ho Hung Lim, Carol Webster |  |
| 317 |  |  [Global Entity Disambiguation with BERT](https://doi.org/10.18653/v1/2022.naacl-main.238) |  | 0 | We propose a global entity disambiguation (ED) model based on BERT. To capture global contextual information for ED, our model treats not only words but also entities as input tokens, and solves the task by sequentially resolving mentions to their referent entities and using resolved entities as inputs at each step. We train the model using a large entity-annotated corpus obtained from Wikipedia. We achieve new state-of-the-art results on five standard ED datasets: AIDA-CoNLL, MSNBC, AQUAINT,... | Ikuya Yamada, Koki Washio, Hiroyuki Shindo, Yuji Matsumoto |  |
| 318 |  |  [Clues Before Answers: Generation-Enhanced Multiple-Choice QA](https://doi.org/10.18653/v1/2022.naacl-main.239) |  | 0 | A trending paradigm for multiple-choice question answering (MCQA) is using a text-to-text framework. By unifying data in different tasks into a single text-to-text format, it trains a generative encoder-decoder model which is both powerful and universal. However, a side effect of twisting a generation target to fit the classification nature of MCQA is the under-utilization of the decoder and the knowledge that can be decoded. To exploit the generation capability and underlying knowledge of a... | Zixian Huang, Ao Wu, Jiaying Zhou, Yu Gu, Yue Zhao, Gong Cheng |  |
| 319 |  |  [Towards Efficient NLP: A Standard Evaluation and A Strong Baseline](https://doi.org/10.18653/v1/2022.naacl-main.240) |  | 0 | Supersized pre-trained language models have pushed the accuracy of various natural language processing (NLP) tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless SOTA accuracy, more and more researchers start paying attention to model efficiency and usability. Different from accuracy, the metric for efficiency varies across different studies, making them hard to be fairly compared. To that end, this work presents ELUE (Efficient Language Understanding Evaluation), a... | Xiangyang Liu, Tianxiang Sun, Junliang He, Jiawen Wu, Lingling Wu, Xinyu Zhang, Hao Jiang, Zhao Cao, Xuanjing Huang, Xipeng Qiu |  |
| 320 |  |  [Stylized Knowledge-Grounded Dialogue Generation via Disentangled Template Rewriting](https://doi.org/10.18653/v1/2022.naacl-main.241) |  | 0 | Current Knowledge-Grounded Dialogue Generation (KDG) models specialize in producing rational and factual responses. However, to establish long-term relationships with users, the KDG model needs the capability to generate responses in a desired style or attribute. Thus, we study a new problem: Stylized Knowledge-Grounded Dialogue Generation (SKDG). It presents two challenges: (1) How to train a SKDG model where no <context, knowledge, stylized response> triples are available. (2) How to cohere... | Qingfeng Sun, Can Xu, Huang Hu, Yujing Wang, Jian Miao, Xiubo Geng, Yining Chen, Fei Xu, Daxin Jiang |  |
| 321 |  |  [LUNA: Learning Slot-Turn Alignment for Dialogue State Tracking](https://doi.org/10.18653/v1/2022.naacl-main.242) |  | 0 | Dialogue state tracking (DST) aims to predict the current dialogue state given the dialogue history. Existing methods generally exploit the utterances of all dialogue turns to assign value for each slot. This could lead to suboptimal results due to the information introduced from irrelevant utterances in the dialogue history, which may be useless and can even cause confusion. To address this problem, we propose LUNA, a SLot-TUrN Alignment enhanced approach. It first explicitly aligns each slot... | Yifan Wang, Jing Zhao, Junwei Bao, Chaoqun Duan, Youzheng Wu, Xiaodong He |  |
| 322 |  |  [Crossroads, Buildings and Neighborhoods: A Dataset for Fine-grained Location Recognition](https://doi.org/10.18653/v1/2022.naacl-main.243) |  | 0 | General domain Named Entity Recognition (NER) datasets like CoNLL-2003 mostly annotate coarse-grained location entities such as a country or a city. But many applications require identifying fine-grained locations from texts and mapping them precisely to geographic sites, e.g., a crossroad, an apartment building, or a grocery store. In this paper, we introduce a new dataset HarveyNER with fine-grained locations annotated in tweets. This dataset presents unique challenges and characterizes many... | Pei Chen, Haotian Xu, Cheng Zhang, Ruihong Huang |  |
| 323 |  |  [Tricks for Training Sparse Translation Models](https://doi.org/10.18653/v1/2022.naacl-main.244) |  | 0 | Multi-task learning with an unbalanced data distribution skews model learning towards high resource tasks, especially when model capacity is fixed and fully shared across all tasks. Sparse scaling architectures, such as BASELayers, provide flexible mechanisms for different tasks to have a variable number of parameters, which can be useful to counterbalance skewed data distributions. We find that that sparse architectures for multilingual machine translation can perform poorly out of the box and... | Dheeru Dua, Shruti Bhosale, Vedanuj Goswami, James Cross, Mike Lewis, Angela Fan |  |
| 324 |  |  [Persona-Guided Planning for Controlling the Protagonist's Persona in Story Generation](https://doi.org/10.18653/v1/2022.naacl-main.245) |  | 0 | Endowing the protagonist with a specific personality is essential for writing an engaging story. In this paper, we aim to control the protagonist’s persona in story generation, i.e., generating a story from a leading context and a persona description, where the protagonist should exhibit the specified personality through a coherent event sequence. Considering that personas are usually embodied implicitly and sparsely in stories, we propose a planning-based generation model named ConPer to... | Zhexin Zhang, Jiaxin Wen, Jian Guan, Minlie Huang |  |
| 325 |  |  [CHEF: A Pilot Chinese Dataset for Evidence-Based Fact-Checking](https://doi.org/10.18653/v1/2022.naacl-main.246) |  | 0 | The explosion of misinformation spreading in the media ecosystem urges for automated fact-checking. While misinformation spans both geographic and linguistic boundaries, most work in the field has focused on English. Datasets and tools available in other languages, such as Chinese, are limited. In order to bridge this gap, we construct CHEF, the first CHinese Evidence-based Fact-checking dataset of 10K real-world claims. The dataset covers multiple domains, ranging from politics to public... | Xuming Hu, Zhijiang Guo, Guanyu Wu, Aiwei Liu, Lijie Wen, Philip S. Yu |  |
| 326 |  |  [VGNMN: Video-grounded Neural Module Networks for Video-Grounded Dialogue Systems](https://doi.org/10.18653/v1/2022.naacl-main.247) |  | 0 | Neural module networks (NMN) have achieved success in image-grounded tasks such as Visual Question Answering (VQA) on synthetic images. However, very limited work on NMN has been studied in the video-grounded dialogue tasks. These tasks extend the complexity of traditional visual tasks with the additional visual temporal variance and language cross-turn dependencies. Motivated by recent NMN approaches on image-grounded tasks, we introduce Video-grounded Neural Module Network (VGNMN) to model... | Hung Le, Nancy F. Chen, Steven C. H. Hoi |  |
| 327 |  |  [Multimodal Dialogue State Tracking](https://doi.org/10.18653/v1/2022.naacl-main.248) |  | 0 | Designed for tracking user goals in dialogues, a dialogue state tracker is an essential component in a dialogue system. However, the research of dialogue state tracking has largely been limited to unimodality, in which slots and slot values are limited by knowledge domains (e.g. restaurant domain with slots of restaurant name and price range) and are defined by specific database schema. In this paper, we propose to extend the definition of dialogue state tracking to multimodality. Specifically,... | Hung Le, Nancy F. Chen, Steven C. H. Hoi |  |
| 328 |  |  [On the Use of Bert for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation](https://doi.org/10.18653/v1/2022.naacl-main.249) |  | 0 | In recent years, pre-trained models have become dominant in most natural language processing (NLP) tasks. However, in the area of Automated Essay Scoring (AES), pre-trained models such as BERT have not been properly used to outperform other deep learning models such as LSTM. In this paper, we introduce a novel multi-scale essay representation for BERT that can be jointly learned. We also employ multiple losses and transfer learning from out-of-domain essays to further improve the performance.... | Yongjie Wang, Chuang Wang, Ruobing Li, Hui Lin |  |
| 329 |  |  [Recognition of They/Them as Singular Personal Pronouns in Coreference Resolution](https://doi.org/10.18653/v1/2022.naacl-main.250) |  | 0 | As using they/them as personal pronouns becomes increasingly common in English, it is important that coreference resolution systems work as well for individuals who use personal “they” as they do for those who use gendered personal pronouns. We introduce a new benchmark for coreference resolution systems which evaluates singular personal “they” recognition. Using these WinoNB schemas, we evaluate a number of publicly available coreference resolution systems and confirm their bias toward... | Connor Baumler, Rachel Rudinger |  |
| 330 |  |  [TWEETSPIN: Fine-grained Propaganda Detection in Social Media Using Multi-View Representations](https://doi.org/10.18653/v1/2022.naacl-main.251) |  | 0 | Recently, several studies on propaganda detection have involved document and fragment-level analyses of news articles. However, there are significant data and modeling challenges dealing with fine-grained detection of propaganda on social media. In this work, we present TWEETSPIN, a dataset containing tweets that are weakly annotated with different fine-grained propaganda techniques, and propose a neural approach to detect and categorize propaganda tweets across those fine-grained categories.... | Prashanth Vijayaraghavan, Soroush Vosoughi |  |
| 331 |  |  [UserIdentifier: Implicit User Representations for Simple and Effective Personalized Sentiment Analysis](https://doi.org/10.18653/v1/2022.naacl-main.252) |  | 0 | Global models are typically trained to be as generalizable as possible. Invariance to the specific user is considered desirable since models are shared across multitudes of users. However, these models are often unable to produce personalized responses for individual users, based on their data. Contrary to widely-used personalization techniques based on few-shot and meta-learning, we propose UserIdentifier, a novel scheme for training a single shared model for all users. Our approach produces... | Fatemehsadat Mireshghallah, Vaishnavi Shrivastava, Milad Shokouhi, Taylor BergKirkpatrick, Robert Sim, Dimitrios Dimitriadis |  |
| 332 |  |  [Improving Neural Models for Radiology Report Retrieval with Lexicon-based Automated Annotation](https://doi.org/10.18653/v1/2022.naacl-main.253) |  | 0 | Many clinical informatics tasks that are based on electronic health records (EHR) need relevant patient cohorts to be selected based on findings, symptoms and diseases. Frequently, these conditions are described in radiology reports which can be retrieved using information retrieval (IR) methods. The latest of these techniques utilize neural IR models such as BERT trained on clinical text. However, these methods still lack semantic understanding of the underlying clinical conditions as well as... | Luyao Shi, Tanveer F. SyedaMahmood, Tyler Baldwin |  |
| 333 |  |  [Transparent Human Evaluation for Image Captioning](https://doi.org/10.18653/v1/2022.naacl-main.254) |  | 0 | We establish THumB, a rubric-based human evaluation protocol for image captioning models. Our scoring rubrics and their definitions are carefully developed based on machine- and human-generated captions on the MSCOCO dataset. Each caption is evaluated along two main dimensions in a tradeoff (precision and recall) as well as other aspects that measure the text quality (fluency, conciseness, and inclusive language). Our evaluations demonstrate several critical problems of the current evaluation... | Jungo Kasai, Keisuke Sakaguchi, Lavinia Dunagan, Jacob Morrison, Ronan Le Bras, Yejin Choi, Noah A. Smith |  |
| 334 |  |  [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](https://doi.org/10.18653/v1/2022.naacl-main.255) |  | 0 | Multilingual pre-trained models are known to suffer from the curse of multilinguality, which causes per-language performance to drop as they cover more languages. We address this issue by introducing language-specific modules, which allows us to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant. In contrast with prior work that learns language-specific components post-hoc, we pre-train the modules of our Cross-lingual Modular... | Jonas Pfeiffer, Naman Goyal, Xi Victoria Lin, Xian Li, James Cross, Sebastian Riedel, Mikel Artetxe |  |
| 335 |  |  [DocAMR: Multi-Sentence AMR Representation and Evaluation](https://doi.org/10.18653/v1/2022.naacl-main.256) |  | 0 | Despite extensive research on parsing of English sentences into Abstract Meaning Representation (AMR) graphs, which are compared to gold graphs via the Smatch metric, full-document parsing into a unified graph representation lacks well-defined representation and evaluation. Taking advantage of a super-sentential level of coreference annotation from previous work, we introduce a simple algorithm for deriving a unified graph representation, avoiding the pitfalls of information loss from... | Tahira Naseem, Austin Blodgett, Sadhana Kumaravel, Tim O'Gorman, YoungSuk Lee, Jeffrey Flanigan, Ramón Fernandez Astudillo, Radu Florian, Salim Roukos, Nathan Schneider |  |
| 336 |  |  [Learning to Transfer Prompts for Text Generation](https://doi.org/10.18653/v1/2022.naacl-main.257) |  | 0 | Pretrained language models (PLMs) have made remarkable progress in text generation tasks via fine-tuning. While, it is challenging to fine-tune PLMs in a data-scarce situation. Therefore, it is non-trivial to develop a general and lightweight model that can adapt to various text generation tasks based on PLMs. To fulfill this purpose, the recent prompt-based learning offers a potential solution. In this paper, we improve this technique and propose a novel prompt-based method (PTG) for text... | Junyi Li, Tianyi Tang, JianYun Nie, JiRong Wen, Xin Zhao |  |
| 337 |  |  [ElitePLM: An Empirical Study on General Language Ability Evaluation of Pretrained Language Models](https://doi.org/10.18653/v1/2022.naacl-main.258) |  | 0 | Nowadays, pretrained language models (PLMs) have dominated the majority of NLP tasks. While, little research has been conducted on systematically evaluating the language abilities of PLMs. In this paper, we present a large-scale empirical study on general language ability evaluation of PLMs (ElitePLM). In our study, we design four evaluation dimensions, memory, comprehension, reasoning, and composition, to measure ten widely-used PLMs within five categories. Our empirical results demonstrate... | Junyi Li, Tianyi Tang, Zheng Gong, Lixin Yang, Zhuohao Yu, Zhipeng Chen, Jingyuan Wang, Xin Zhao, JiRong Wen |  |
| 338 |  |  [Bidimensional Leaderboards: Generate and Evaluate Language Hand in Hand](https://doi.org/10.18653/v1/2022.naacl-main.259) |  | 0 | Natural language processing researchers have identified limitations of evaluation methodology for generation tasks, with new questions raised about the validity of automatic metrics and of crowdworker judgments. Meanwhile, efforts to improve generation models tend to depend on simple n-gram overlap metrics (e.g., BLEU, ROUGE). We argue that new advances on models and metrics should each more directly benefit and inform the other. We therefore propose a generalization of leaderboards,... | Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Lavinia Dunagan, Jacob Morrison, Alexander R. Fabbri, Yejin Choi, Noah A. Smith |  |
| 339 |  |  [Improving In-Context Few-Shot Learning via Self-Supervised Training](https://doi.org/10.18653/v1/2022.naacl-main.260) |  | 0 | Self-supervised pretraining has made few-shot learning possible for many NLP tasks. But the pretraining objectives are not typically adapted specifically for in-context few-shot learning. In this paper, we propose to use self-supervision in an intermediate training stage between pretraining and downstream few-shot usage with the goal to teach the model to perform in-context few shot learning. We propose and evaluate four self-supervised objectives on two benchmarks. We find that the... | Mingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor Mihaylov, Srini Iyer, Veselin Stoyanov, Zornitsa Kozareva |  |
| 340 |  |  [Exposing the Limits of Video-Text Models through Contrast Sets](https://doi.org/10.18653/v1/2022.naacl-main.261) |  | 0 | Recent video-text models can retrieve relevant videos based on text with a high accuracy, but to what extent do they comprehend the semantics of the text? Can they discriminate between similar entities and actions? To answer this, we propose an evaluation framework that probes video-text models with hard negatives. We automatically build contrast sets, where true textual descriptions are manipulated in ways that change their semantics while maintaining plausibility. Specifically, we leverage a... | Jae Sung Park, Sheng Shen, Ali Farhadi, Trevor Darrell, Yejin Choi, Anna Rohrbach |  |
| 341 |  |  [Zero-shot Sonnet Generation with Discourse-level Planning and Aesthetics Features](https://doi.org/10.18653/v1/2022.naacl-main.262) |  | 0 | Poetry generation, and creative language generation in general, usually suffers from the lack of large training data. In this paper, we present a novel framework to generate sonnets that does not require training on poems. We design a hierarchical framework which plans the poem sketch before decoding. Specifically, a content planning module is trained on non-poetic texts to obtain discourse-level coherence; then a rhyme module generates rhyme words and a polishing module introduces imagery and... | Yufei Tian, Nanyun Peng |  |
| 342 |  |  [Benchmarking Intersectional Biases in NLP](https://doi.org/10.18653/v1/2022.naacl-main.263) |  | 0 | There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has... | John Lalor, Yi Yang, Kendall Smith, Nicole Forsgren, Ahmed Abbasi |  |
| 343 |  |  [When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer](https://doi.org/10.18653/v1/2022.naacl-main.264) |  | 0 | While recent work on multilingual language models has demonstrated their capacity for cross-lingual zero-shot transfer on downstream tasks, there is a lack of consensus in the community as to what shared properties between languages enable such transfer. Analyses involving pairs of natural languages are often inconclusive and contradictory since languages simultaneously differ in many linguistic aspects. In this paper, we perform a large-scale empirical study to isolate the effects of various... | Ameet Deshpande, Partha Talukdar, Karthik Narasimhan |  |
| 344 |  |  [How Conservative are Language Models? Adapting to the Introduction of Gender-Neutral Pronouns](https://doi.org/10.18653/v1/2022.naacl-main.265) |  | 0 | Gender-neutral pronouns have recently been introduced in many languages to a) include non-binary people and b) as a generic singular. Recent results from psycholinguistics suggest that gender-neutral pronouns (in Swedish) are not associated with human processing difficulties. This, we show, is in sharp contrast with automated processing. We show that gender-neutral pronouns in Danish, English, and Swedish are associated with higher perplexity, more dispersed attention patterns, and worse... | Stephanie Brandl, Ruixiang Cui, Anders Søgaard |  |
| 345 |  |  [Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts](https://doi.org/10.18653/v1/2022.naacl-main.266) |  | 0 | Fine-tuning continuous prompts for target tasks has recently emerged as a compact alternative to full model fine-tuning. Motivated by these promising results, we investigate the feasibility of extracting a discrete (textual) interpretation of continuous prompts that is faithful to the problem they solve. In practice, we observe a “wayward” behavior between the task solved by continuous prompts and their nearest neighbor discrete projections: We can find continuous prompts that solve a task... | Daniel Khashabi, Xinxi Lyu, Sewon Min, Lianhui Qin, Kyle Richardson, Sean Welleck, Hannaneh Hajishirzi, Tushar Khot, Ashish Sabharwal, Sameer Singh, Yejin Choi |  |
| 346 |  |  [Contrastive Representation Learning for Cross-Document Coreference Resolution of Events and Entities](https://doi.org/10.18653/v1/2022.naacl-main.267) |  | 0 | Identifying related entities and events within and across documents is fundamental to natural language understanding. We present an approach to entity and event coreference resolution utilizing contrastive representation learning. Earlier state-of-the-art methods have formulated this problem as a binary classification problem and leveraged large transformers in a cross-encoder architecture to achieve their results. For large collections of documents and corresponding set of n mentions, the... | Benjamin Hsu, Graham Horwood |  |
| 347 |  |  [Learning the Ordering of Coordinate Compounds and Elaborate Expressions in Hmong, Lahu, and Chinese](https://doi.org/10.18653/v1/2022.naacl-main.268) |  | 0 | Coordinate compounds (CCs) and elaborate expressions (EEs) are coordinate constructions common in languages of East and Southeast Asia. Mortensen (2006) claims that (1) the linear ordering of EEs and CCs in Hmong, Lahu, and Chinese can be predicted via phonological hierarchies and (2) that these phonological hierarchies lack a clear phonetic rationale. These claims are significant because morphosyntax has often been seen as in a feed-forward relationship with phonology, and phonological... | Chenxuan Cui, Katherine J. Zhang, David R. Mortensen |  |
| 348 |  |  [FRUIT: Faithfully Reflecting Updated Information in Text](https://doi.org/10.18653/v1/2022.naacl-main.269) |  | 0 | Textual knowledge bases such as Wikipedia require considerable effort to keep up to date and consistent. While automated writing assistants could potentially ease this burden, the problem of suggesting edits grounded in external knowledge has been under-explored. In this paper, we introduce the novel generation task of \*faithfully reflecting updated information in text\* (FRUIT) where the goal is to update an existing article given new evidence. We release the FRUIT-WIKI dataset, a collection... | Robert L. Logan IV, Alexandre Passos, Sameer Singh, MingWei Chang |  |
| 349 |  |  [Multi2WOZ: A Robust Multilingual Dataset and Conversational Pretraining for Task-Oriented Dialog](https://doi.org/10.18653/v1/2022.naacl-main.270) |  | 0 | Research on (multi-domain) task-oriented dialog (TOD) has predominantly focused on the English language, primarily due to the shortage of robust TOD datasets in other languages, preventing the systematic investigation of cross-lingual transfer for this crucial NLP application area. In this work, we introduce Multi2WOZ, a new multilingual multi-domain TOD dataset, derived from the well-established English dataset MultiWOZ, that spans four typologically diverse languages: Chinese, German, Arabic,... | ChiaChien Hung, Anne Lauscher, Ivan Vulic, Simone Paolo Ponzetto, Goran Glavas |  |
| 350 |  |  [ChapterBreak: A Challenge Dataset for Long-Range Language Models](https://doi.org/10.18653/v1/2022.naacl-main.271) |  | 0 | While numerous architectures for long-range language models (LRLMs) have recently been proposed, a meaningful evaluation of their discourse-level language understanding capabilities has not yet followed. To this end, we introduce ChapterBreak, a challenge dataset that provides an LRLM with a long segment from a narrative that ends at a chapter boundary and asks it to distinguish the beginning of the ground-truth next chapter from a set of negative segments sampled from the same narrative. A... | Simeng Sun, Katherine Thai, Mohit Iyyer |  |
| 351 |  |  [ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction](https://doi.org/10.18653/v1/2022.naacl-main.272) |  | 0 | Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by... | Keshav Santhanam, Omar Khattab, Jon SaadFalcon, Christopher Potts, Matei Zaharia |  |
| 352 |  |  [Quantifying Language Variation Acoustically with Few Resources](https://doi.org/10.18653/v1/2022.naacl-main.273) |  | 0 | Deep acoustic models represent linguistic information based on massive amounts of data. Unfortunately, for regional languages and dialects such resources are mostly not available. However, deep acoustic models might have learned linguistic information that transfers to low-resource languages. In this study, we evaluate whether this is the case through the task of distinguishing low-resource (Dutch) regional varieties. By extracting embeddings from the hidden layers of various wav2vec 2.0 models... | Martijn Bartelds, Martijn Wieling |  |
| 353 |  |  [Adaptable Adapters](https://doi.org/10.18653/v1/2022.naacl-main.274) |  | 0 | State-of-the-art pretrained NLP models contain a hundred million to trillion parameters. Adapters provide a parameter-efficient alternative for the full finetuning in which we can only finetune lightweight neural network layers on top of pretrained weights. Adapter layers are initialized randomly. However, existing work uses the same adapter architecture—i.e., the same adapter layer on top of each layer of the pretrained model—for every dataset, regardless of the properties of the dataset or... | Nafise Sadat Moosavi, Quentin Delfosse, Kristian Kersting, Iryna Gurevych |  |
| 354 |  |  [Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants](https://doi.org/10.18653/v1/2022.naacl-main.275) |  | 0 | In Dynamic Adversarial Data Collection (DADC), human annotators are tasked with finding examples that models struggle to predict correctly. Models trained on DADC-collected training data have been shown to be more robust in adversarial and out-of-domain settings, and are considerably harder for humans to fool. However, DADC is more time-consuming than traditional data collection and thus more costly per annotated example. In this work, we examine whether we can maintain the advantages of DADC,... | Max Bartolo, Tristan Thrush, Sebastian Riedel, Pontus Stenetorp, Robin Jia, Douwe Kiela |  |
| 355 |  |  [GMN: Generative Multi-modal Network for Practical Document Information Extraction](https://doi.org/10.18653/v1/2022.naacl-main.276) |  | 0 | Document Information Extraction (DIE) has attracted increasing attention due to its various advanced applications in the real world. Although recent literature has already achieved competitive results, these approaches usually fail when dealing with complex documents with noisy OCR results or mutative layouts. This paper proposes Generative Multi-modal Network (GMN) for real-world scenarios to address these problems, which is a robust multi-modal generation method without predefined label... | Haoyu Cao, Jiefeng Ma, Antai Guo, Yiqing Hu, Hao Liu, Deqiang Jiang, Yinsong Liu, Bo Ren |  |
| 356 |  |  [One Reference Is Not Enough: Diverse Distillation with Reference Selection for Non-Autoregressive Translation](https://doi.org/10.18653/v1/2022.naacl-main.277) |  | 0 | Non-autoregressive neural machine translation (NAT) suffers from the multi-modality problem: the source sentence may have multiple correct translations, but the loss function is calculated only according to the reference sentence. Sequence-level knowledge distillation makes the target more deterministic by replacing the target with the output from an autoregressive model. However, the multi-modality problem in the distilled dataset is still nonnegligible. Furthermore, learning from a specific... | Chenze Shao, Xuanfu Wu, Yang Feng |  |
| 357 |  |  [Can Rationalization Improve Robustness?](https://doi.org/10.18653/v1/2022.naacl-main.278) |  | 0 | A growing line of work has investigated the development of neural NLP models that can produce rationales–subsets of input that can explain their model predictions. In this paper, we ask whether such rationale models can provide robustness to adversarial attacks in addition to their interpretable nature. Since these models need to first generate rationales (“rationalizer”) before making predictions (“predictor”), they have the potential to ignore noise or adversarially added text by simply... | Howard Chen, Jacqueline He, Karthik Narasimhan, Danqi Chen |  |
| 358 |  |  [On the Effectiveness of Sentence Encoding for Intent Detection Meta-Learning](https://doi.org/10.18653/v1/2022.naacl-main.279) |  | 0 | Recent studies on few-shot intent detection have attempted to formulate the task as a meta-learning problem, where a meta-learning model is trained with a certain capability to quickly adapt to newly specified few-shot tasks with potentially unseen intent categories. Prototypical networks have been commonly used in this setting, with the hope that good prototypical representations could be learned to capture the semantic similarity between the query and a few labeled instances. This intuition... | Tingting Ma, Qianhui Wu, Zhiwei Yu, Tiejun Zhao, ChinYew Lin |  |
| 359 |  |  [A Computational Acquisition Model for Multimodal Word Categorization](https://doi.org/10.18653/v1/2022.naacl-main.280) |  | 0 | Recent advances in self-supervised modeling of text and images open new opportunities for computational models of child language acquisition, which is believed to rely heavily on cross-modal signals. However, prior studies has been limited by their reliance on vision models trained on large image datasets annotated with a pre-defined set of depicted object categories. This is (a) not faithful to the information children receive and (b) prohibits the evaluation of such models with respect to... | Uri Berger, Gabriel Stanovsky, Omri Abend, Lea Frermann |  |
| 360 |  |  [Residue-Based Natural Language Adversarial Attack Detection](https://doi.org/10.18653/v1/2022.naacl-main.281) |  | 0 | Deep learning based systems are susceptible to adversarial attacks, where a small, imperceptible change at the input alters the model prediction. However, to date the majority of the approaches to detect these attacks have been designed for image processing systems. Many popular image adversarial detection approaches are able to identify adversarial examples from embedding feature spaces, whilst in the NLP domain existing state of the art detection approaches solely focus on input text... | Vyas Raina, Mark J. F. Gales |  |
| 361 |  |  [Does it Really Generalize Well on Unseen Data? Systematic Evaluation of Relational Triple Extraction Methods](https://doi.org/10.18653/v1/2022.naacl-main.282) |  | 0 | The ability to extract entities and their relations from unstructured text is essential for the automated maintenance of large-scale knowledge graphs. To keep a knowledge graph up-to-date, an extractor needs not only the ability to recall the triples it encountered during training, but also the ability to extract the new triples from the context that it has never seen before. In this paper, we show that although existing extraction models are able to easily memorize and recall already seen... | Juhyuk Lee, MinJoong Lee, June Yong Yang, Eunho Yang |  |
| 362 |  |  [From spoken dialogue to formal summary: An utterance rewriting for dialogue summarization](https://doi.org/10.18653/v1/2022.naacl-main.283) |  | 0 | Due to the dialogue characteristics of unstructured contexts and multi-parties with first-person perspective, many successful text summarization works have failed when dealing with dialogue summarization. In dialogue summarization task, the input dialogue is usually spoken style with ellipsis and co-references but the output summaries are more formal and complete. Therefore, the dialogue summarization model should be able to complete the ellipsis content and co-reference information and then... | Yue Fang, Hainan Zhang, Hongshen Chen, Zhuoye Ding, Bo Long, Yanyan Lan, Yanquan Zhou |  |
| 363 |  |  [EASE: Entity-Aware Contrastive Learning of Sentence Embedding](https://doi.org/10.18653/v1/2022.naacl-main.284) |  | 0 | We present EASE, a novel method for learning sentence embeddings via contrastive learning between sentences and their related entities. The advantage of using entity supervision is twofold: (1) entities have been shown to be a strong indicator of text semantics and thus should provide rich training signals for sentence embeddings; (2) entities are defined independently of languages and thus offer useful cross-lingual alignment supervision. We evaluate EASE against other unsupervised models both... | Sosuke Nishikawa, Ryokan Ri, Ikuya Yamada, Yoshimasa Tsuruoka, Isao Echizen |  |
| 364 |  |  [Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics](https://doi.org/10.18653/v1/2022.naacl-main.285) |  | 0 | Recent work incorporates pre-trained word embeddings such as BERT embeddings into Neural Topic Models (NTMs), generating highly coherent topics. However, with high-quality contextualized document representations, do we really need sophisticated neural models to obtain coherent and interpretable topics? In this paper, we conduct thorough experiments showing that directly clustering high-quality sentence embeddings with an appropriate word selecting method can generate more coherent and diverse... | Zihan Zhang, Meng Fang, Ling Chen, MohammadReza NamaziRad |  |
| 365 |  |  [Dynamic Multistep Reasoning based on Video Scene Graph for Video Question Answering](https://doi.org/10.18653/v1/2022.naacl-main.286) |  | 0 | Existing video question answering (video QA) models lack the capacity for deep video understanding and flexible multistep reasoning. We propose for video QA a novel model which performs dynamic multistep reasoning between questions and videos. It creates video semantic representation based on the video scene graph composed of semantic elements of the video and semantic relations among these elements. Then, it performs multistep reasoning for better answer decision between the representations of... | Jianguo Mao, Wenbin Jiang, Xiangdong Wang, Zhifan Feng, Yajuan Lyu, Hong Liu, Yong Zhu |  |
| 366 |  |  [TRUE: Re-evaluating Factual Consistency Evaluation](https://doi.org/10.18653/v1/2022.naacl-main.287) |  | 0 | Grounded text generation systems often generate text that contains factual inconsistencies, hindering their real-world applicability. Automatic factual consistency evaluation may help alleviate this limitation by accelerating evaluation cycles, filtering inconsistent outputs and augmenting training data. While attracting increasing attention, such evaluation metrics are usually developed and evaluated in silo for a single task or dataset, slowing their adoption. Moreover, previous... | Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansky, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, Yossi Matias |  |
| 367 |  |  [Knowledge Inheritance for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.naacl-main.288) |  | 0 | Recent explorations of large-scale pre-trained language models (PLMs) have revealed the power of PLMs with huge amounts of parameters, setting off a wave of training ever-larger PLMs. However, it requires tremendous computational resources to train a large-scale PLM, which may be practically unaffordable. In addition, existing large-scale PLMs are mainly trained from scratch individually, ignoring that many well-trained PLMs are available. To this end, we explore the question how could existing... | Yujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han, Zhengyan Zhang, Yusheng Su, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou |  |
| 368 |  |  [Bi-SimCut: A Simple Strategy for Boosting Neural Machine Translation](https://doi.org/10.18653/v1/2022.naacl-main.289) |  | 0 | We introduce Bi-SimCut: a simple but effective training strategy to boost neural machine translation (NMT) performance. It consists of two procedures: bidirectional pretraining and unidirectional finetuning. Both procedures utilize SimCut, a simple regularization method that forces the consistency between the output distributions of the original and the cutoff sentence pairs. Without leveraging extra dataset via back-translation or integrating large-scale pretrained model, Bi-SimCut achieves... | Pengzhi Gao, Zhongjun He, Hua Wu, Haifeng Wang |  |
| 369 |  |  [On Transferability of Prompt Tuning for Natural Language Processing](https://doi.org/10.18653/v1/2022.naacl-main.290) |  | 0 | Prompt tuning (PT) is a promising parameter-efficient method to utilize extremely large pre-trained language models (PLMs), which can achieve comparable performance to full-parameter fine-tuning by only tuning a few soft prompts. However, PT requires much more training time than fine-tuning. Intuitively, knowledge transfer can help to improve the efficiency. To explore whether we can improve PT via prompt transfer, we empirically investigate the transferability of soft prompts across different... | Yusheng Su, Xiaozhi Wang, Yujia Qin, ChiMin Chan, Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, Lei Hou, Maosong Sun, Jie Zhou |  |
| 370 |  |  [DocEE: A Large-Scale and Fine-grained Benchmark for Document-level Event Extraction](https://doi.org/10.18653/v1/2022.naacl-main.291) |  | 0 | Event extraction aims to identify an event and then extract the arguments participating in the event. Despite the great success in sentence-level event extraction, events are more naturally presented in the form of documents, with event arguments scattered in multiple sentences. However, a major barrier to promote document-level event extraction has been the lack of large-scale and practical training and evaluation datasets. In this paper, we present DocEE, a new document-level event extraction... | Meihan Tong, Bin Xu, Shuai Wang, Meihuan Han, Yixin Cao, Jiangqi Zhu, Siyu Chen, Lei Hou, Juanzi Li |  |
| 371 |  |  [Towards Debiasing Translation Artifacts](https://doi.org/10.18653/v1/2022.naacl-main.292) |  | 0 | Cross-lingual natural language processing relies on translation, either by humans or machines, at different levels, from translating training data to translating test sets. However, compared to original texts in the same language, translations possess distinct qualities referred to as translationese. Previous research has shown that these translation artifacts influence the performance of a variety of cross-lingual tasks. In this work, we propose a novel approach to reducing translationese by... | Koel Dutta Chowdhury, Rricha Jalota, Cristina EspañaBonet, Josef van Genabith |  |
| 372 |  |  [WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models](https://doi.org/10.18653/v1/2022.naacl-main.293) |  | 0 | Large pretrained language models (LMs) have become the central building block of many NLP applications. Training these models requires ever more computational resources and most of the existing models are trained on English text only. It is exceedingly expensive to train these models in other languages. To alleviate this problem, we introduce a novel method – called WECHSEL – to efficiently and effectively transfer pretrained LMs to new languages. WECHSEL can be applied to any model which uses... | Benjamin Minixhofer, Fabian Paischer, Navid Rekabsaz |  |
| 373 |  |  [A New Concept of Knowledge based Question Answering (KBQA) System for Multi-hop Reasoning](https://doi.org/10.18653/v1/2022.naacl-main.294) |  | 0 | Knowledge based question answering (KBQA) is a complex task for natural language understanding. Many KBQA approaches have been proposed in recent years, and most of them are trained based on labeled reasoning path. This hinders the system’s performance as many correct reasoning paths are not labeled as ground truth, and thus they cannot be learned. In this paper, we introduce a new concept of KBQA system which can leverage multiple reasoning paths’ information and only requires labeled answer... | Yu Wang, Vijay Srinivasan, Hongxia Jin |  |
| 374 |  |  [Bilingual Tabular Inference: A Case Study on Indic Languages](https://doi.org/10.18653/v1/2022.naacl-main.295) |  | 0 | Existing research on Tabular Natural Language Inference (TNLI) exclusively examines the task in a monolingual setting where the tabular premise and hypothesis are in the same language. However, due to the uneven distribution of text resources on the web across languages, it is common to have the tabular premise in a high resource language and the hypothesis in a low resource language. As a result, we present the challenging task of bilingual Tabular Natural Language Inference (bTNLI), in which... | Chaitanya Agarwal, Vivek Gupta, Anoop Kunchukuttan, Manish Shrivastava |  |
| 375 |  |  [Generative Biomedical Entity Linking via Knowledge Base-Guided Pre-training and Synonyms-Aware Fine-tuning](https://doi.org/10.18653/v1/2022.naacl-main.296) |  | 0 | Entities lie in the heart of biomedical natural language understanding, and the biomedical entity linking (EL) task remains challenging due to the fine-grained and diversiform concept names. Generative methods achieve remarkable performances in general domain EL with less memory usage while requiring expensive pre-training. Previous biomedical EL methods leverage synonyms from knowledge bases (KB) which is not trivial to inject into a generative method. In this work, we use a generative... | Hongyi Yuan, Zheng Yuan, Sheng Yu |  |
| 376 |  |  [Robust Self-Augmentation for Named Entity Recognition with Meta Reweighting](https://doi.org/10.18653/v1/2022.naacl-main.297) |  | 0 | Self-augmentation has received increasing research interest recently to improve named entity recognition (NER) performance in low-resource scenarios. Token substitution and mixup are two feasible heterogeneous self-augmentation techniques for NER that can achieve effective performance with certain specialized efforts. Noticeably, self-augmentation may introduce potentially noisy augmented data. Prior research has mainly resorted to heuristic rule-based constraints to reduce the noise for... | Linzhi Wu, Pengjun Xie, Jie Zhou, Meishan Zhang, Chunping Ma, Guangwei Xu, Min Zhang |  |
| 377 |  |  [Unsupervised Stem-based Cross-lingual Part-of-Speech Tagging for Morphologically Rich Low-Resource Languages](https://doi.org/10.18653/v1/2022.naacl-main.298) |  | 0 | Unsupervised cross-lingual projection for part-of-speech (POS) tagging relies on the use of parallel data to project POS tags from a source language for which a POS tagger is available onto a target language across word-level alignments. The projected tags then form the basis for learning a POS model for the target language. However, languages with rich morphology often yield sparse word alignments because words corresponding to the same citation form do not align well. We hypothesize that for... | Ramy Eskander, Cass Lowry, Sujay Khandagale, Judith Klavans, Maria Polinsky, Smaranda Muresan |  |
| 378 |  |  [Optimising Equal Opportunity Fairness in Model Training](https://doi.org/10.18653/v1/2022.naacl-main.299) |  | 0 | Real-world datasets often encode stereotypes and societal biases. Such biases can be implicitly captured by trained models, leading to biased predictions and exacerbating existing societal preconceptions. Existing debiasing methods, such as adversarial training and removing protected information from representations, have been shown to reduce bias. However, a disconnect between fairness criteria and training objectives makes it difficult to reason theoretically about the effectiveness of... | Aili Shen, Xudong Han, Trevor Cohn, Timothy Baldwin, Lea Frermann |  |
| 379 |  |  [Leaner and Faster: Two-Stage Model Compression for Lightweight Text-Image Retrieval](https://doi.org/10.18653/v1/2022.naacl-main.300) |  | 0 | Current text-image approaches (e.g., CLIP) typically adopt dual-encoder architecture using pre-trained vision-language representation. However, these models still pose non-trivial memory requirements and substantial incremental indexing time, which makes them less practical on mobile devices. In this paper, we present an effective two-stage framework to compress large pre-trained dual-encoder for lightweight text-image retrieval. The resulting model is smaller (39% of the original), faster... | Siyu Ren, Kenny Q. Zhu |  |
| 380 |  |  [Joint Learning-based Heterogeneous Graph Attention Network for Timeline Summarization](https://doi.org/10.18653/v1/2022.naacl-main.301) |  | 0 | Previous studies on the timeline summarization (TLS) task ignored the information interaction between sentences and dates, and adopted pre-defined unlearnable representations for them. They also considered date selection and event detection as two independent tasks, which makes it impossible to integrate their advantages and obtain a globally optimal summary. In this paper, we present a joint learning-based heterogeneous graph attention network for TLS (HeterTls), in which date selection and... | Jingyi You, Dongyuan Li, Hidetaka Kamigaito, Kotaro Funakoshi, Manabu Okumura |  |
| 381 |  |  [Early Rumor Detection Using Neural Hawkes Process with a New Benchmark Dataset](https://doi.org/10.18653/v1/2022.naacl-main.302) |  | 0 | Little attention has been paid on EArly Rumor Detection (EARD), and EARD performance was evaluated inappropriately on a few datasets where the actual early-stage information is largely missing. To reverse such situation, we construct BEARD, a new Benchmark dataset for EARD, based on claims from fact-checking websites by trying to gather as many early relevant posts as possible. We also propose HEARD, a novel model based on neural Hawkes process for EARD, which can guide a generic rumor... | Fengzhu Zeng, Wei Gao |  |
| 382 |  |  [Emp-RFT: Empathetic Response Generation via Recognizing Feature Transitions between Utterances](https://doi.org/10.18653/v1/2022.naacl-main.303) |  | 0 | Each utterance in multi-turn empathetic dialogues has features such as emotion, keywords, and utterance-level meaning. Feature transitions between utterances occur naturally. However, existing approaches fail to perceive the transitions because they extract features for the context at the coarse-grained level. To solve the above issue, we propose a novel approach of recognizing feature transitions between utterances, which helps understand the dialogue flow and better grasp the features of... | Wongyu Kim, Youbin Ahn, Donghyun Kim, KyongHo Lee |  |
| 383 |  |  [KCD: Knowledge Walks and Textual Cues Enhanced Political Perspective Detection in News Media](https://doi.org/10.18653/v1/2022.naacl-main.304) |  | 0 | Political perspective detection has become an increasingly important task that can help combat echo chambers and political polarization. Previous approaches generally focus on leveraging textual content to identify stances, while they fail to reason with background knowledge or leverage the rich semantic and syntactic textual labels in news articles. In light of these limitations, we propose KCD, a political perspective detection approach to enable multi-hop knowledge reasoning and incorporate... | Wenqian Zhang, Shangbin Feng, Zilong Chen, Zhenyu Lei, Jundong Li, Minnan Luo |  |
| 384 |  |  [Collective Relevance Labeling for Passage Retrieval](https://doi.org/10.18653/v1/2022.naacl-main.305) |  | 0 | Deep learning for Information Retrieval (IR) requires a large amount of high-quality query-document relevance labels, but such labels are inherently sparse. Label smoothing redistributes some observed probability mass over unobserved instances, often uniformly, uninformed of the true distribution. In contrast, we propose knowledge distillation for informed labeling, without incurring high computation overheads at evaluation time. Our contribution is designing a simple but efficient teacher... | Jihyuk Kim, Minsoo Kim, Seungwon Hwang |  |
| 385 |  |  [COGMEN: COntextualized GNN based Multimodal Emotion recognitioN](https://doi.org/10.18653/v1/2022.naacl-main.306) |  | 0 | Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person’s emotions are influenced by the other speaker’s utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multi- modal Emotion recognitioN (COGMEN) system that leverages local information (i.e., inter/intra... | Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram Singh, Ashutosh Modi |  |
| 386 |  |  [Revisit Overconfidence for OOD Detection: Reassigned Contrastive Learning with Adaptive Class-dependent Threshold](https://doi.org/10.18653/v1/2022.naacl-main.307) |  | 0 | Detecting Out-of-Domain (OOD) or unknown intents from user queries is essential in a task-oriented dialog system. A key challenge of OOD detection is the overconfidence of neural models. In this paper, we comprehensively analyze overconfidence and classify it into two perspectives: over-confident OOD and in-domain (IND). Then according to intrinsic reasons, we respectively propose a novel reassigned contrastive learning (RCL) to discriminate IND intents for over-confident OOD and an adaptive... | Yanan Wu, Keqing He, Yuanmeng Yan, QiXiang Gao, Zhiyuan Zeng, Fujia Zheng, Lulu Zhao, Huixing Jiang, Wei Wu, Weiran Xu |  |
| 387 |  |  [AISFG: Abundant Information Slot Filling Generator](https://doi.org/10.18653/v1/2022.naacl-main.308) |  | 0 | As an essential component of task-oriented dialogue systems, slot filling requires enormous labeled training data in a certain domain. However, in most cases, there is little or no target domain training data is available in the training stage. Thus, cross-domain slot filling has to cope with the data scarcity problem by zero/few-shot learning. Previous researches on zero/few-shot cross-domain slot filling focus on slot descriptions and examples while ignoring the slot type ambiguity and... | Yang Yan, Junda Ye, Zhongbao Zhang, Liwen Wang |  |
| 388 |  |  [Improving negation detection with negation-focused pre-training](https://doi.org/10.18653/v1/2022.naacl-main.309) |  | 0 | Negation is a common linguistic feature that is crucial in many language understanding tasks, yet it remains a hard problem due to diversity in its expression in different types of text. Recent works show that state-of-the-art NLP models underperform on samples containing negation in various tasks, and that negation detection models do not transfer well across domains. We propose a new negation-focused pre-training strategy, involving targeted data augmentation and negation masking, to better... | Thinh Hung Truong, Timothy Baldwin, Trevor Cohn, Karin Verspoor |  |
| 389 |  |  [Practice Makes a Solver Perfect: Data Augmentation for Math Word Problem Solvers](https://doi.org/10.18653/v1/2022.naacl-main.310) |  | 0 | Existing Math Word Problem (MWP) solvers have achieved high accuracy on benchmark datasets. However, prior works have shown that such solvers do not generalize well and rely on superficial cues to achieve high performance. In this paper, we first conduct experiments to showcase that this behaviour is mainly associated with the limited size and diversity present in existing MWP datasets. Next, we propose several data augmentation techniques broadly categorized into Substitution and Paraphrasing... | Vivek Kumar, Rishabh Maheshwary, Vikram Pudi |  |
| 390 |  |  [DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings](https://doi.org/10.18653/v1/2022.naacl-main.311) |  | 0 | We propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings. DiffCSE learns sentence embeddings that are sensitive to the difference between the original sentence and an edited sentence, where the edited sentence is obtained by stochastically masking out the original sentence and then sampling from a masked language model. We show that DiffSCE is an instance of equivariant contrastive learning, which generalizes contrastive learning and learns... | YungSung Chuang, Rumen Dangovski, Hongyin Luo, Yang Zhang, Shiyu Chang, Marin Soljacic, ShangWen Li, Scott Yih, Yoon Kim, James R. Glass |  |
| 391 |  |  [Generative Cross-Domain Data Augmentation for Aspect and Opinion Co-Extraction](https://doi.org/10.18653/v1/2022.naacl-main.312) |  | 0 | As a fundamental task in opinion mining, aspect and opinion co-extraction aims to identify the aspect terms and opinion terms in reviews. However, due to the lack of fine-grained annotated resources, it is hard to train a robust model for many domains. To alleviate this issue, unsupervised domain adaptation is proposed to transfer knowledge from a labeled source domain to an unlabeled target domain. In this paper, we propose a new Generative Cross-Domain Data Augmentation framework for... | Junjie Li, Jianfei Yu, Rui Xia |  |
| 392 |  |  [ProQA: Structural Prompt-based Pre-training for Unified Question Answering](https://doi.org/10.18653/v1/2022.naacl-main.313) |  | 0 | Question Answering (QA) is a longstanding challenge in natural language processing. Existing QA works mostly focus on specific question types, knowledge domains, or reasoning skills. The specialty in QA research hinders systems from modeling commonalities between tasks and generalization for wider applications. To address this issue, we present ProQA, a unified QA paradigm that solves various tasks through a single model. ProQA takes a unified structural prompt as the bridge and improves the... | Wanjun Zhong, Yifan Gao, Ning Ding, Yujia Qin, Zhiyuan Liu, Ming Zhou, Jiahai Wang, Jian Yin, Nan Duan |  |
| 393 |  |  [A Data Cartography based MixUp for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.naacl-main.314) |  | 0 | MixUp is a data augmentation strategy where additional samples are generated during training by combining random pairs of training samples and their labels. However, selecting random pairs is not potentially an optimal choice. In this work, we propose TDMixUp, a novel MixUp strategy that leverages Training Dynamics and allows more informative samples to be combined for generating new data samples. Our proposed TDMixUp first measures confidence, variability, (Swayamdipta et al., 2020), and Area... | Seoyeon Park, Cornelia Caragea |  |
| 394 |  |  [Grapheme-to-Phoneme Conversion for Thai using Neural Regression Models](https://doi.org/10.18653/v1/2022.naacl-main.315) |  | 0 | We propose a novel Thai grapheme-to-phoneme conversion method based on a neural regression model that is trained using neural networks to predict the similarity between a candidate and the correct pronunciation. After generating a set of candidates for an input word or phrase using the orthography rules, this model selects the best-similarity pronunciation from the candidates. This method can be applied to languages other than Thai simply by preparing enough orthography rules, and can reduce... | Tomohiro Yamasaki |  |
| 395 |  |  [Generating Authentic Adversarial Examples beyond Meaning-preserving with Doubly Round-trip Translation](https://doi.org/10.18653/v1/2022.naacl-main.316) |  | 0 | Generating adversarial examples for Neural Machine Translation (NMT) with single Round-Trip Translation (RTT) has achieved promising results by releasing the meaning-preserving restriction. However, a potential pitfall for this approach is that we cannot decide whether the generated examples are adversarial to the target NMT model or the auxiliary backward one, as the reconstruction error through the RTT can be related to either. To remedy this problem, we propose a new definition for NMT... | Siyu Lai, Zhen Yang, Fandong Meng, Xue Zhang, Yufeng Chen, Jinan Xu, Jie Zhou |  |
| 396 |  |  [TVShowGuess: Character Comprehension in Stories as Speaker Guessing](https://doi.org/10.18653/v1/2022.naacl-main.317) |  | 0 | We propose a new task for assessing machines’ skills of understanding fictional characters in narrative stories. The task, TVShowGuess, builds on the scripts of TV series and takes the form of guessing the anonymous main characters based on the backgrounds of the scenes and the dialogues. Our human study supports that this form of task covers comprehension of multiple types of character persona, including understanding characters’ personalities, facts and memories of personal experience, which... | Yisi Sang, Xiangyang Mou, Mo Yu, Shunyu Yao, Jing Li, Jeffrey M. Stanton |  |
| 397 |  |  [Causal Distillation for Language Models](https://doi.org/10.18653/v1/2022.naacl-main.318) |  | 0 | Distillation efforts have led to language models that are more compact and efficient without serious drops in performance. The standard approach to distillation trains a student model against two objectives: a task-specific objective (e.g., language modeling) and an imitation objective that encourages the hidden states of the student model to be similar to those of the larger teacher model. In this paper, we show that it is beneficial to augment distillation with a third objective that... | Zhengxuan Wu, Atticus Geiger, Joshua Rozner, Elisa Kreiss, Hanson Lu, Thomas Icard, Christopher Potts, Noah D. Goodman |  |
| 398 |  |  [FNet: Mixing Tokens with Fourier Transforms](https://doi.org/10.18653/v1/2022.naacl-main.319) |  | 0 | We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that “mix” input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths.... | James LeeThorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontañón |  |
| 399 |  |  [Answer Consolidation: Formulation and Benchmarking](https://doi.org/10.18653/v1/2022.naacl-main.320) |  | 0 | Current question answering (QA) systems primarily consider the single-answer scenario, where each question is assumed to be paired with one correct answer. However, in many real-world QA applications, multiple answer scenarios arise where consolidating answers into a comprehensive and non-redundant set of answers is a more efficient user interface. In this paper, we formulate the problem of answer consolidation, where answers are partitioned into multiple groups, each representing different... | Wenxuan Zhou, Qiang Ning, Heba Elfardy, Kevin Small, Muhao Chen |  |
| 400 |  |  [Informativeness and Invariance: Two Perspectives on Spurious Correlations in Natural Language](https://doi.org/10.18653/v1/2022.naacl-main.321) |  | 0 | Spurious correlations are a threat to the trustworthiness of natural language processing systems, motivating research into methods for identifying and eliminating them. However, addressing the problem of spurious correlations requires more clarity on what they are and how they arise in language data. Gardner et al (2021) argue that the compositional nature of language implies that all correlations between labels and individual “input features” are spurious. This paper analyzes this proposal in... | Jacob Eisenstein |  |
| 401 |  |  [FOAM: A Follower-aware Speaker Model For Vision-and-Language Navigation](https://doi.org/10.18653/v1/2022.naacl-main.322) |  | 0 | The speaker-follower models have proven to be effective in vision-and-language navigation, where a speaker model is used to synthesize new instructions to augment the training data for a follower navigation model. However, in previous work, the speaker model is follower-agnostic and fails to take the state of the follower into consideration. In this paper, we present FOAM, a FOllower-Aware speaker Model that is constantly updated given the follower feedback, so that the generated instructions... | ZiYi Dou, Nanyun Peng |  |
| 402 |  |  [Improving Compositional Generalization with Latent Structure and Data Augmentation](https://doi.org/10.18653/v1/2022.naacl-main.323) |  | 0 | Generic unstructured neural networks have been shown to struggle on out-of-distribution compositional generalization. Compositional data augmentation via example recombination has transferred some prior knowledge about compositionality to such black-box neural models for several semantic parsing tasks, but this often required task-specific engineering or provided limited gains. We present a more powerful data recombination method using a model called Compositional Structure Learner (CSL). CSL... | Linlu Qiu, Peter Shaw, Panupong Pasupat, Pawel Krzysztof Nowak, Tal Linzen, Fei Sha, Kristina Toutanova |  |
| 403 |  |  [Joint Extraction of Entities, Relations, and Events via Modeling Inter-Instance and Inter-Label Dependencies](https://doi.org/10.18653/v1/2022.naacl-main.324) |  | 0 | Event trigger detection, entity mention recognition, event argument extraction, and relation extraction are the four important tasks in information extraction that have been performed jointly (Joint Information Extraction - JointIE) to avoid error propagation and leverage dependencies between the task instances (i.e., event triggers, entity mentions, relations, and event arguments). However, previous JointIE models often assume heuristic manually-designed dependency between the task instances... | Minh Van Nguyen, Bonan Min, Franck Dernoncourt, Thien Huu Nguyen |  |
| 404 |  |  [Linguistic Frameworks Go Toe-to-Toe at Neuro-Symbolic Language Modeling](https://doi.org/10.18653/v1/2022.naacl-main.325) |  | 0 | We examine the extent to which, in principle, different syntactic and semantic graph representations can complement and improve neural language modeling. Specifically, by conditioning on a subgraph encapsulating the locally relevant sentence history, can a model make better next-word predictions than a pretrained sequential language model alone? With an ensemble setup consisting of GPT-2 and ground-truth graphs from one of 7 different formalisms, we find that the graph information indeed... | Jakob Prange, Nathan Schneider, Lingpeng Kong |  |
| 405 |  |  [Imagination-Augmented Natural Language Understanding](https://doi.org/10.18653/v1/2022.naacl-main.326) |  | 0 | Human brains integrate linguistic and perceptual information simultaneously to understand natural language, and hold the critical ability to render imaginations. Such abilities enable us to construct new abstract concepts or concrete objects, and are essential in involving practical knowledge to solve problems in low-resource scenarios. However, most existing methods for Natural Language Understanding (NLU) are mainly focused on textual signals. They do not simulate human visual imagination... | Yujie Lu, Wanrong Zhu, Xin Wang, Miguel P. Eckstein, William Yang Wang |  |
| 406 |  |  [What company do words keep? Revisiting the distributional semantics of J.R. Firth & Zellig Harris](https://doi.org/10.18653/v1/2022.naacl-main.327) |  | 0 | The power of word embeddings is attributed to the linguistic theory that similar words will appear in similar contexts. This idea is specifically invoked by noting that “you shall know a word by the company it keeps,” a quote from British linguist J.R. Firth who, along with his American colleague Zellig Harris, is often credited with the invention of “distributional semantics.” While both Firth and Harris are cited in all major NLP textbooks and many foundational papers, the content and... | Mikael Brunila, Jack LaViolette |  |
| 407 |  |  [Compositional Task-Oriented Parsing as Abstractive Question Answering](https://doi.org/10.18653/v1/2022.naacl-main.328) |  | 0 | Task-oriented parsing (TOP) aims to convert natural language into machine-readable representations of specific tasks, such as setting an alarm. A popular approach to TOP is to apply seq2seq models to generate linearized parse trees. A more recent line of work argues that pretrained seq2seq2 models are better at generating outputs that are themselves natural language, so they replace linearized parse trees with canonical natural-language paraphrases that can then be easily translated into parse... | Wenting Zhao, Konstantine Arkoudas, Weiqi Sun, Claire Cardie |  |
| 408 |  |  [Learning Cross-Lingual IR from an English Retriever](https://doi.org/10.18653/v1/2022.naacl-main.329) |  | 0 | We present DR.DECR (Dense Retrieval with Distillation-Enhanced Cross-Lingual Representation), a new cross-lingual information retrieval (CLIR) system trained using multi-stage knowledge distillation (KD). The teacher of DR.DECR relies on a highly effective but computationally expensive two-stage inference process consisting of query translation and monolingual IR, while the student, DR.DECR, executes a single CLIR step. We teach DR.DECR powerful multilingual representations as well as CLIR by... | Yulong Li, Martin Franz, Md. Arafat Sultan, Bhavani Iyer, YoungSuk Lee, Avirup Sil |  |
| 409 |  |  [Testing the Ability of Language Models to Interpret Figurative Language](https://doi.org/10.18653/v1/2022.naacl-main.330) |  | 0 | Figurative and metaphorical language are commonplace in discourse, and figurative expressions play an important role in communication and cognition. However, figurative language has been a relatively under-studied area in NLP, and it remains an open question to what extent modern language models can interpret nonliteral phrases. To address this question, we introduce Fig-QA, a Winograd-style nonliteral language understanding task consisting of correctly interpreting paired figurative phrases... | Emmy Liu, Chenxuan Cui, Kenneth Zheng, Graham Neubig |  |
| 410 |  |  [Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity](https://doi.org/10.18653/v1/2022.naacl-main.331) |  | 0 | We present a new scientific document similarity model based on matching fine-grained aspects of texts. To train our model, we exploit a naturally-occurring source of supervision: sentences in the full-text of papers that cite multiple papers together (co-citations). Such co-citations not only reflect close paper relatedness, but also provide textual descriptions of how the co-cited papers are related. This novel form of textual supervision is used for learning to match aspects across papers. We... | Sheshera Mysore, Arman Cohan, Tom Hope |  |
| 411 |  |  [CHAI: A CHatbot AI for Task-Oriented Dialogue with Offline Reinforcement Learning](https://doi.org/10.18653/v1/2022.naacl-main.332) |  | 0 | Conventionally, generation of natural language for dialogue agents may be viewed as a statistical learning problem: determine the patterns in human-provided data and generate appropriate responses with similar statistical properties. However, dialogue can also be regarded as a goal directed process, where speakers attempt to accomplish a specific task. Reinforcement learning (RL) algorithms are designed specifically for solving such goal-directed problems, but the most direct way to apply RL,... | Siddharth Verma, Justin Fu, Sherry Yang, Sergey Levine |  |
| 412 |  |  [Connecting the Dots between Audio and Text without Parallel Data through Visual Knowledge Transfer](https://doi.org/10.18653/v1/2022.naacl-main.333) |  | 0 | Machines that can represent and describe environmental soundscapes have practical potential, e.g., for audio tagging and captioning. Prevailing learning paradigms of audio-text connections have been relying on parallel audio-text data, which is, however, scarcely available on the web. We propose VIP-ANT that induces Audio-Text alignment without using any parallel audio-text data. Our key idea is to share the image modality between bi-modal image-text representations and bi-modal image-audio... | Yanpeng Zhao, Jack Hessel, Youngjae Yu, Ximing Lu, Rowan Zellers, Yejin Choi |  |
| 413 |  |  [SURF: Semantic-level Unsupervised Reward Function for Machine Translation](https://doi.org/10.18653/v1/2022.naacl-main.334) |  | 0 | The performance of Reinforcement Learning (RL) for natural language tasks including Machine Translation (MT) is crucially dependent on the reward formulation. This is due to the intrinsic difficulty of the task in the high-dimensional discrete action space as well as the sparseness of the standard reward functions defined for limited set of ground-truth sequences biased towards singular lexical choices. To address this issue, we formulate SURF, a maximally dense semantic-level unsupervised... | Atijit Anuchitanukul, Julia Ive |  |
| 414 |  |  [Disentangling Categorization in Multi-agent Emergent Communication](https://doi.org/10.18653/v1/2022.naacl-main.335) |  | 0 | The emergence of language between artificial agents is a recent focus of computational linguistics, as it offers a synthetic substrate for reasoning about human language evolution. From the perspective of cognitive science, sophisticated categorization in humans is thought to enable reasoning about novel observations, and thus compose old information to describe new phenomena. Unfortunately, the literature to date has not managed to isolate the effect of categorization power in artificial... | Washington Garcia, Hamilton Scott Clouse, Kevin R. B. Butler |  |
| 415 |  |  [Show, Don't Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue](https://doi.org/10.18653/v1/2022.naacl-main.336) |  | 0 | Building universal dialogue systems that operate across multiple domains/APIs and generalize to new ones with minimal overhead is a critical challenge. Recent works have leveraged natural language descriptions of schema elements to enable such systems; however, descriptions only indirectly convey schema semantics. In this work, we propose Show, Don’t Tell, which prompts seq2seq models with a labeled example dialogue to show the semantics of schema elements rather than tell the model through... | Raghav Gupta, Harrison Lee, Jeffrey Zhao, Yuan Cao, Abhinav Rastogi, Yonghui Wu |  |
| 416 |  |  [Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge](https://doi.org/10.18653/v1/2022.naacl-main.337) |  | 0 | Transformer models pre-trained with a masked-language-modeling objective (e.g., BERT) encode commonsense knowledge as evidenced by behavioral probes; however, the extent to which this knowledge is acquired by systematic inference over the semantics of the pre-training corpora is an open question. To answer this question, we selectively inject verbalized knowledge into the pre-training minibatches of BERT and evaluate how well the model generalizes to supported inferences after pre-training on... | Ian Porada, Alessandro Sordoni, Jackie Chi Kit Cheung |  |
| 417 |  |  [Using Paraphrases to Study Properties of Contextual Embeddings](https://doi.org/10.18653/v1/2022.naacl-main.338) |  | 0 | We use paraphrases as a unique source of data to analyze contextualized embeddings, with a particular focus on BERT. Because paraphrases naturally encode consistent word and phrase semantics, they provide a unique lens for investigating properties of embeddings. Using the Paraphrase Database’s alignments, we study words within paraphrases as well as phrase representations. We find that contextual embeddings effectively handle polysemous words, but give synonyms surprisingly different... | Laura Burdick, Jonathan K. Kummerfeld, Rada Mihalcea |  |
| 418 |  |  [Measure and Improve Robustness in NLP Models: A Survey](https://doi.org/10.18653/v1/2022.naacl-main.339) |  | 0 | As NLP models achieved state-of-the-art performances over benchmarks and gained wide applications, it has been increasingly important to ensure the safe deployment of these models in the real world, e.g., making sure the models are robust against unseen or challenging scenarios. Despite robustness being an increasingly studied topic, it has been separately explored in applications like vision and NLP, with various definitions, evaluation and mitigation strategies in multiple lines of research.... | Xuezhi Wang, Haohan Wang, Diyi Yang |  |
| 419 |  |  [Learning to Generate Examples for Semantic Processing Tasks](https://doi.org/10.18653/v1/2022.naacl-main.340) |  | 0 | Even if recent Transformer-based architectures, such as BERT, achieved impressive results in semantic processing tasks, their fine-tuning stage still requires large scale training resources. Usually, Data Augmentation (DA) techniques can help to deal with low resource settings. In Text Classification tasks, the objective of DA is the generation of well-formed sentences that i) represent the desired task category and ii) are novel with respect to existing sentences. In this paper, we propose a... | Danilo Croce, Simone Filice, Giuseppe Castellucci, Roberto Basili |  |
| 420 |  |  [Symbolic Knowledge Distillation: from General Language Models to Commonsense Models](https://doi.org/10.18653/v1/2022.naacl-main.341) |  | 0 | The common practice for training commonsense models has gone from–human–to–corpus–to–machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from–machine–to–corpus–to–machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al. 2015), our approach... | Peter West, Chandra Bhagavatula, Jack Hessel, Jena D. Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, Yejin Choi |  |
| 421 |  |  [GenIE: Generative Information Extraction](https://doi.org/10.18653/v1/2022.naacl-main.342) |  | 0 | Structured and grounded representation of text is typically formalized by closed information extraction, the problem of extracting an exhaustive set of (subject, relation, object) triplets that are consistent with a predefined set of entities and relations from a knowledge base schema. Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations. We introduce GenIE (generative information... | Martin Josifoski, Nicola De Cao, Maxime Peyrard, Fabio Petroni, Robert West |  |
| 422 |  |  [Entity Linking via Explicit Mention-Mention Coreference Modeling](https://doi.org/10.18653/v1/2022.naacl-main.343) |  | 0 | Learning representations of entity mentions is a core component of modern entity linking systems for both candidate generation and making linking predictions. In this paper, we present and empirically analyze a novel training approach for learning mention and entity representations that is based on building minimum spanning arborescences (i.e., directed spanning trees) over mentions and entities across documents to explicitly model mention coreference relationships. We demonstrate the efficacy... | Dhruv Agarwal, Rico Angell, Nicholas Monath, Andrew McCallum |  |
| 423 |  |  [Massive-scale Decoding for Text Generation using Lattices](https://doi.org/10.18653/v1/2022.naacl-main.344) |  | 0 | Conditional neural text generation models generate high-quality outputs, but often concentrate around a mode when what we really want is a diverse set of options. We present a search algorithm to construct lattices encoding a massive number of generation options. First, we restructure decoding as a best-first search, which explores the space differently than beam search and improves efficiency by avoiding pruning paths. Second, we revisit the idea of hypothesis recombination: we can identify... | Jiacheng Xu, Siddhartha Jonnalagadda, Greg Durrett |  |
| 424 |  |  [Disentangling Indirect Answers to Yes-No Questions in Real Conversations](https://doi.org/10.18653/v1/2022.naacl-main.345) |  | 0 | In this paper, we explore the task of determining indirect answers to yes-no questions in real conversations. We work with transcripts of phone conversations in the Switchboard Dialog Act (SwDA) corpus and create SwDA-IndirectAnswers (SwDA-IA), a subset of SwDA consisting of all conversations containing a yes-no question with an indirect answer. We annotate the underlying direct answers to the yes-no questions (yes, probably yes, middle, probably no, or no). We show that doing so requires... | Krishna Chaitanya Sanagavarapu, Jathin Singaraju, Anusha Kakileti, Anirudh Kaza, Aaron Abraham Mathews, Helen Li, Nathan Raul Brito, Eduardo Blanco |  |
| 425 |  |  [Quantifying Adaptability in Pre-trained Language Models with 500 Tasks](https://doi.org/10.18653/v1/2022.naacl-main.346) |  | 0 | When a neural language model (LM) is adapted to perform a new task, what aspects of the task predict the eventual performance of the model? In NLP, systematic features of LM generalization to individual examples are well characterized, but systematic aspects of LM adaptability to new tasks are not nearly as well understood. We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence... | Belinda Z. Li, Jane A. Yu, Madian Khabsa, Luke Zettlemoyer, Alon Y. Halevy, Jacob Andreas |  |
| 426 |  |  [Counterfactually Augmented Data and Unintended Bias: The Case of Sexism and Hate Speech Detection](https://doi.org/10.18653/v1/2022.naacl-main.347) |  | 0 | Counterfactually Augmented Data (CAD) aims to improve out-of-domain generalizability, an indicator of model robustness. The improvement is credited to promoting core features of the construct over spurious artifacts that happen to correlate with it. Yet, over-relying on core features may lead to unintended model bias. Especially, construct-driven CAD—perturbations of core features—may induce models to ignore the context in which core features are used. Here, we test models for sexism and hate... | Indira Sen, Mattia Samory, Claudia Wagner, Isabelle Augenstein |  |
| 427 |  |  [A Study of the Attention Abnormality in Trojaned BERTs](https://doi.org/10.18653/v1/2022.naacl-main.348) |  | 0 | Trojan attacks raise serious security concerns. In this paper, we investigate the underlying mechanism of Trojaned BERT models. We observe the attention focus drifting behavior of Trojaned models, i.e., when encountering an poisoned input, the trigger token hijacks the attention focus regardless of the context. We provide a thorough qualitative and quantitative analysis of this phenomenon, revealing insights into the Trojan mechanism. Based on the observation, we propose an attention-based... | Weimin Lyu, Songzhu Zheng, Tengfei Ma, Chao Chen |  |
| 428 |  |  [EPiDA: An Easy Plug-in Data Augmentation Framework for High Performance Text Classification](https://doi.org/10.18653/v1/2022.naacl-main.349) |  | 0 | Recent works have empirically shown the effectiveness of data augmentation (DA) in NLP tasks, especially for those suffering from data scarcity. Intuitively, given the size of generated data, their diversity and quality are crucial to the performance of targeted tasks. However, to the best of our knowledge, most existing methods consider only either the diversity or the quality of augmented data, thus cannot fully mine the potential of DA for NLP. In this paper, we present an easy and plug-in... | Minyi Zhao, Lu Zhang, Yi Xu, Jiandong Ding, Jihong Guan, Shuigeng Zhou |  |
| 429 |  |  [Partial-input baselines show that NLI models can ignore context, but they don't](https://doi.org/10.18653/v1/2022.naacl-main.350) |  | 0 | When strong partial-input baselines reveal artifacts in crowdsourced NLI datasets, the performance of full-input models trained on such datasets is often dismissed as reliance on spurious correlations. We investigate whether state-of-the-art NLI models are capable of overriding default inferences made by a partial-input baseline. We introduce an evaluation set of 600 examples consisting of perturbed premises to examine a RoBERTa model’s sensitivity to edited contexts. Our results indicate that... | Neha Srikanth, Rachel Rudinger |  |
| 430 |  |  [Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora](https://doi.org/10.18653/v1/2022.naacl-main.351) |  | 0 | Pretrained language models (PTLMs) are typically learned over a large, static corpus and further fine-tuned for various downstream tasks. However, when deployed in the real world, a PTLM-based model must deal with data distributions that deviates from what the PTLM was initially trained on. In this paper, we study a lifelong language model pretraining challenge where a PTLM is continually updated so as to adapt to emerging data. Over a domain-incremental research paper stream and a... | Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, ShangWen Li, Xiaokai Wei, Andrew O. Arnold, Xiang Ren |  |
| 431 |  |  [Learning as Conversation: Dialogue Systems Reinforced for Information Acquisition](https://doi.org/10.18653/v1/2022.naacl-main.352) |  | 0 | We propose novel AI-empowered chat bots for learning as conversation where a user does not read a passage but gains information and knowledge through conversation with a teacher bot. Our information acquisition-oriented dialogue system employs a novel adaptation of reinforced self-play so that the system can be transferred to various domains without in-domain dialogue data, and can carry out conversations both informative and attentive to users. | Pengshan Cai, Hui Wan, Fei Liu, Mo Yu, Hong Yu, Sachindra Joshi |  |
| 432 |  |  [Dynamic Programming in Rank Space: Scaling Structured Inference with Low-Rank HMMs and PCFGs](https://doi.org/10.18653/v1/2022.naacl-main.353) |  | 0 | Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs) are widely used structured models, both of which can be represented as factor graph grammars (FGGs), a powerful formalism capable of describing a wide range of models. Recent research found it beneficial to use large state spaces for HMMs and PCFGs. However, inference with large state spaces is computationally demanding, especially for PCFGs. To tackle this challenge, we leverage tensor rank decomposition (aka. CPD) to... | Songlin Yang, Wei Liu, Kewei Tu |  |
| 433 |  |  [What Factors Should Paper-Reviewer Assignments Rely On? Community Perspectives on Issues and Ideals in Conference Peer-Review](https://doi.org/10.18653/v1/2022.naacl-main.354) |  | 0 | Both scientific progress and individual researcher careers depend on the quality of peer review, which in turn depends on paper-reviewer matching. Surprisingly, this problem has been mostly approached as an automated recommendation problem rather than as a matter where different stakeholders (area chairs, reviewers, authors) have accumulated experience worth taking into account. We present the results of the first survey of the NLP community, identifying common issues and perspectives on what... | Terne Sasha Thorn Jakobsen, Anna Rogers |  |
| 434 |  |  [Reducing Disambiguation Biases in NMT by Leveraging Explicit Word Sense Information](https://doi.org/10.18653/v1/2022.naacl-main.355) |  | 0 | Recent studies have shed some light on a common pitfall of Neural Machine Translation (NMT) models, stemming from their struggle to disambiguate polysemous words without lapsing into their most frequently occurring senses in the training corpus. In this paper, we first provide a novel approach for automatically creating high-precision sense-annotated parallel corpora, and then put forward a specifically tailored fine-tuning strategy for exploiting these sense annotations during training without... | Niccolò Campolungo, Tommaso Pasini, Denis Emelin, Roberto Navigli |  |
| 435 |  |  [Mining Clues from Incomplete Utterance: A Query-enhanced Network for Incomplete Utterance Rewriting](https://doi.org/10.18653/v1/2022.naacl-main.356) |  | 0 | Incomplete utterance rewriting has recently raised wide attention. However, previous works do not consider the semantic structural information between incomplete utterance and rewritten utterance or model the semantic structure implicitly and insufficiently. To address this problem, we propose a QUEry-Enhanced Network(QUEEN) to solve this problem. Firstly, our proposed query template explicitly brings guided semantic structural knowledge between the incomplete utterance and the rewritten... | Shuzheng Si, Shuang Zeng, Baobao Chang |  |
| 436 |  |  [Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization](https://doi.org/10.18653/v1/2022.naacl-main.357) |  | 0 | The most advanced abstractive dialogue summarizers lack generalization ability on new domains and the existing researches for domain adaptation in summarization generally rely on large-scale pre-trainings. To explore the lightweight fine-tuning methods for domain adaptation of dialogue summarization, in this paper, we propose an efficient and generalizable Domain-Oriented Prefix-tuning model, which utilizes a domain word initialized prefix module to alleviate domain entanglement and adopts... | Lulu Zhao, Fujia Zheng, Weihao Zeng, Keqing He, Weiran Xu, Huixing Jiang, Wei Wu, Yanan Wu |  |
| 437 |  |  [Interactive Symbol Grounding with Complex Referential Expressions](https://doi.org/10.18653/v1/2022.naacl-main.358) |  | 0 | We present a procedure for learning to ground symbols from a sequence of stimuli consisting of an arbitrarily complex noun phrase (e.g. “all but one green square above both red circles.”) and its designation in the visual scene. Our distinctive approach combines: a) lazy few-shot learning to relate open-class words like green and above to their visual percepts; and b) symbolic reasoning with closed-class word categories like quantifiers and negation. We use this combination to estimate new... | Rimvydas Rubavicius, Alex Lascarides |  |
| 438 |  |  [Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks](https://doi.org/10.18653/v1/2022.naacl-main.359) |  | 0 | Logical approaches to representing language have developed and evaluated computational models of quantifier words since the 19th century, but today’s NLU models still struggle to capture their semantics. We rely on Generalized Quantifier Theory for language-independent representations of the semantics of quantifier words, to quantify their contribution to the errors of NLU models. We find that quantifiers are pervasive in NLU benchmarks, and their occurrence at test time is associated with... | Ruixiang Cui, Daniel Hershcovich, Anders Søgaard |  |
| 439 |  |  [Exact Paired-Permutation Testing for Structured Test Statistics](https://doi.org/10.18653/v1/2022.naacl-main.360) |  | 0 | Significance testing—especially the paired-permutation test—has played a vital role in developing NLP systems to provide confidence that the difference in performance between two systems (i.e., the test statistic) is not due to luck. However, practitioners rely on Monte Carlo approximation to perform this test due to a lack of a suitable exact algorithm. In this paper, we provide an efficient exact algorithm for the paired-permutation test for a family of structured test statistics. Our... | Ran Zmigrod, Tim Vieira, Ryan Cotterell |  |
| 440 |  |  [A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping the Linguistic Blood Bank](https://doi.org/10.18653/v1/2022.naacl-main.361) |  | 0 | We show that the choice of pretraining languages affects downstream cross-lingual transfer for BERT-based models. We inspect zero-shot performance in balanced data conditions to mitigate data size confounds, classifying pretraining languages that improve downstream performance as donors, and languages that are improved in zero-shot performance as recipients. We develop a method of quadratic time complexity in the number of languages to estimate these relations, instead of an exponential... | Dan Malkin, Tomasz Limisiewicz, Gabriel Stanovsky |  |
| 441 |  |  [SSEGCN: Syntactic and Semantic Enhanced Graph Convolutional Network for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2022.naacl-main.362) |  | 0 | Aspect-based Sentiment Analysis (ABSA) aims to predict the sentiment polarity towards a particular aspect in a sentence. Recently, graph neural networks based on dependency tree convey rich structural information which is proven to be utility for ABSA. However, how to effectively harness the semantic and syntactic structure information from the dependency tree remains a challenging research question. In this paper, we propose a novel Syntactic and Semantic Enhanced Graph Convolutional Network... | Zheng Zhang, Zili Zhou, Yanna Wang |  |
| 442 |  |  [Mitigating Toxic Degeneration with Empathetic Data: Exploring the Relationship Between Toxicity and Empathy](https://doi.org/10.18653/v1/2022.naacl-main.363) |  | 0 | Large pre-trained neural language models have supported the effectiveness of many NLP tasks, yet are still prone to generating toxic language hindering the safety of their use. Using empathetic data, we improve over recent work on controllable text generation that aims to reduce the toxicity of generated text. We find we are able to dramatically reduce the size of fine-tuning data to 7.5-30k samples while at the same time making significant improvements over state-of-the-art toxicity mitigation... | Allison Lahnala, Charles Welch, Béla Neuendorf, Lucie Flek |  |
| 443 |  |  [DUCK: Rumour Detection on Social Media by Modelling User and Comment Propagation Networks](https://doi.org/10.18653/v1/2022.naacl-main.364) |  | 0 | Social media rumours, a form of misinformation, can mislead the public and cause significant economic and social disruption. Motivated by the observation that the user network — which captures who engage with a story — and the comment network — which captures how they react to it — provide complementary signals for rumour detection, in this paper, we propose DUCK (rumour ̲detection with ̲user and ̲comment networ ̲ks) for rumour detection on social media. We study how to leverage transformers... | Lin Tian, Xiuzhen Zhang, Jey Han Lau |  |
| 444 |  |  [Jam or Cream First? Modeling Ambiguity in Neural Machine Translation with SCONES](https://doi.org/10.18653/v1/2022.naacl-main.365) |  | 0 | The softmax layer in neural machine translation is designed to model the distribution over mutually exclusive tokens. Machine translation, however, is intrinsically uncertain: the same source sentence can have multiple semantically equivalent translations. Therefore, we propose to replace the softmax activation with a multi-label classification layer that can model ambiguity more effectively. We call our loss function Single-label Contrastive Objective for Non-Exclusive Sequences (SCONES). We... | Felix Stahlberg, Shankar Kumar |  |
| 445 |  |  [SkillSpan: Hard and Soft Skill Extraction from English Job Postings](https://doi.org/10.18653/v1/2022.naacl-main.366) |  | 0 | Skill Extraction (SE) is an important and widely-studied task useful to gain insights into labor market dynamics. However, there is a lacuna of datasets and annotation guidelines; available datasets are few and contain crowd-sourced labels on the span-level or labels from a predefined skill inventory. To address this gap, we introduce SKILLSPAN, a novel SE dataset consisting of 14.5K sentences and over 12.5K annotated spans. We release its respective guidelines created over three different... | Mike Zhang, Kristian Nørgaard Jensen, Sif Dam Sonniks, Barbara Plank |  |
| 446 |  |  [RAAT: Relation-Augmented Attention Transformer for Relation Modeling in Document-Level Event Extraction](https://doi.org/10.18653/v1/2022.naacl-main.367) |  | 0 | In document-level event extraction (DEE) task, event arguments always scatter across sentences (across-sentence issue) and multipleevents may lie in one document (multi-event issue). In this paper, we argue that the relation information of event arguments is of greatsignificance for addressing the above two issues, and propose a new DEE framework which can model the relation dependencies, calledRelation-augmented Document-level Event Extraction (ReDEE). More specifically, this framework... | Yuan Liang, Zhuoxuan Jiang, Di Yin, Bo Ren |  |
| 447 |  |  [A Double-Graph Based Framework for Frame Semantic Parsing](https://doi.org/10.18653/v1/2022.naacl-main.368) |  | 0 | Frame semantic parsing is a fundamental NLP task, which consists of three subtasks: frame identification, argument identification and role classification. Most previous studies tend to neglect relations between different subtasks and arguments and pay little attention to ontological frame knowledge defined in FrameNet. In this paper, we propose a Knowledge-guided Incremental semantic parser with Double-graph (KID). We first introduce Frame Knowledge Graph (FKG), a heterogeneous graph containing... | Ce Zheng, Xudong Chen, Runxin Xu, Baobao Chang |  |
| 448 |  |  [An Enhanced Span-based Decomposition Method for Few-Shot Sequence Labeling](https://doi.org/10.18653/v1/2022.naacl-main.369) |  | 0 | Few-Shot Sequence Labeling (FSSL) is a canonical paradigm for the tagging models, e.g., named entity recognition and slot filling, to generalize on an emerging, resource-scarce domain. Recently, the metric-based meta-learning framework has been recognized as a promising approach for FSSL. However, most prior works assign a label to each token based on the token-level similarities, which ignores the integrality of named entities or slots. To this end, in this paper, we propose ESD, an Enhanced... | Peiyi Wang, Runxin Xu, Tianyu Liu, Qingyu Zhou, Yunbo Cao, Baobao Chang, Zhifang Sui |  |
| 449 |  |  [A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction](https://doi.org/10.18653/v1/2022.naacl-main.370) |  | 0 | Most previous studies aim at extracting events from a single sentence, while document-level event extraction still remains under-explored. In this paper, we focus on extracting event arguments from an entire document, which mainly faces two critical problems: a) the long-distance dependency between trigger and arguments over sentences; b) the distracting context towards an event in the document. To address these issues, we propose a Two-Stream Abstract meaning Representation enhanced extraction... | Runxin Xu, Peiyi Wang, Tianyu Liu, Shuang Zeng, Baobao Chang, Zhifang Sui |  |
| 450 |  |  [Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning](https://doi.org/10.18653/v1/2022.naacl-main.371) |  | 0 | Controlled table-to-text generation seeks to generate natural language descriptions for highlighted subparts of a table. Previous SOTA systems still employ a sequence-to-sequence generation method, which merely captures the table as a linear structure and is brittle when table layouts change. We seek to go beyond this paradigm by (1) effectively expressing the relations of content pieces in the table, and (2) making our model robust to content-invariant structural transformations. Accordingly,... | Fei Wang, Zhewei Xu, Pedro A. Szekely, Muhao Chen |  |
| 451 |  |  [JointLK: Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering](https://doi.org/10.18653/v1/2022.naacl-main.372) |  | 0 | Existing KG-augmented models for commonsense question answering primarily focus on designing elaborate Graph Neural Networks (GNNs) to model knowledge graphs (KGs). However, they ignore (i) the effectively fusing and reasoning over question context representations and the KG representations, and (ii) automatically selecting relevant nodes from the noisy KGs during reasoning. In this paper, we propose a novel model, JointLK, which solves the above limitations through the joint reasoning of LM... | Yueqing Sun, Qi Shi, Le Qi, Yu Zhang |  |
| 452 |  |  [Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens](https://doi.org/10.18653/v1/2022.naacl-main.373) |  | 0 | Standard pretrained language models operate on sequences of subword tokens without direct access to the characters that compose each token’s string representation. We probe the embedding layer of pretrained language models and show that models learn the internal character composition of whole word and subword tokens to a surprising extent, without ever seeing the characters coupled with the tokens. Our results show that the embedding layers of RoBERTa and GPT2 each hold enough information to... | Itay Itzhak, Omer Levy |  |
| 453 |  |  [A Corpus for Understanding and Generating Moral Stories](https://doi.org/10.18653/v1/2022.naacl-main.374) |  | 0 | Teaching morals is one of the most important purposes of storytelling. An essential ability for understanding and writing moral stories is bridging story plots and implied morals. Its challenges mainly lie in: (1) grasping knowledge about abstract concepts in morals, (2) capturing inter-event discourse relations in stories, and (3) aligning value preferences of stories and morals concerning good or bad behavior. In this paper, we propose two understanding tasks and two generation tasks to... | Jian Guan, Ziqi Liu, Minlie Huang |  |
| 454 |  |  [Modeling Multi-Granularity Hierarchical Features for Relation Extraction](https://doi.org/10.18653/v1/2022.naacl-main.375) |  | 0 | Relation extraction is a key task in Natural Language Processing (NLP), which aims to extract relations between entity pairs from given texts. Recently, relation extraction (RE) has achieved remarkable progress with the development of deep neural networks. Most existing research focuses on constructing explicit structured features using external knowledge such as knowledge graph and dependency tree. In this paper, we propose a novel method to extract multi-granularity features based solely on... | Xinnian Liang, Shuangzhi Wu, Mu Li, Zhoujun Li |  |
| 455 |  |  [Cross-modal Contrastive Learning for Speech Translation](https://doi.org/10.18653/v1/2022.naacl-main.376) |  | 0 | How can we learn unified representations for spoken utterances and their written text? Learning similar representations for semantically similar speech and text is important for speech translation. To this end, we propose ConST, a cross-modal contrastive learning method for end-to-end speech-to-text translation. We evaluate ConST and a variety of previous baselines on a popular benchmark MuST-C. Experiments show that the proposed ConST consistently outperforms the previous methods, and achieves... | Rong Ye, Mingxuan Wang, Lei Li |  |
| 456 |  |  [Meet Your Favorite Character: Open-domain Chatbot Mimicking Fictional Characters with only a Few Utterances](https://doi.org/10.18653/v1/2022.naacl-main.377) |  | 0 | In this paper, we consider mimicking fictional characters as a promising direction for building engaging conversation models. To this end, we present a new practical task where only a few utterances of each fictional character are available to generate responses mimicking them. Furthermore, we propose a new method named Pseudo Dialog Prompting (PDP) that generates responses by leveraging the power of large-scale language models with prompts containing the target character’s utterances. To... | Seungju Han, Beomsu Kim, Jin Yong Yoo, Seokjun Seo, Sangbum Kim, Enkhbayar Erdenee, Buru Chang |  |
| 457 |  |  [DynamicTOC: Persona-based Table of Contents for Consumption of Long Documents](https://doi.org/10.18653/v1/2022.naacl-main.378) |  | 0 | Long documents like contracts, financial documents, etc., are often tedious to read through. Linearly consuming (via scrolling or navigation through default table of content) these documents is time-consuming and challenging. These documents are also authored to be consumed by varied entities (referred to as persona in the paper) interested in only certain parts of the document. In this work, we describe DynamicToC, a dynamic table of content-based navigator, to aid in the task of non-linear,... | Himanshu Maheshwari, Nethraa Sivakumar, Shelly Jain, Tanvi Karandikar, Vinay Aggarwal, Navita Goyal, Sumit Shekhar |  |
| 458 |  |  [KALA: Knowledge-Augmented Language Model Adaptation](https://doi.org/10.18653/v1/2022.naacl-main.379) |  | 0 | Pre-trained language models (PLMs) have achieved remarkable success on various natural language understanding tasks. Simple fine-tuning of PLMs, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains. While adaptive pre-training of PLMs can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the PLM’s performance on the downstream task by causing... | Minki Kang, Jinheon Baek, Sung Ju Hwang |  |
| 459 |  |  [On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model](https://doi.org/10.18653/v1/2022.naacl-main.380) |  | 0 | Many recent studies on large-scale language models have reported successful in-context zero- and few-shot learning ability. However, the in-depth analysis of when in-context learning occurs is still lacking. For example, it is unknown how in-context learning performance changes as the training corpus varies. Here, we investigate the effects of the source and size of the pretraining corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From our in-depth investigation, we... | Seongjin Shin, SangWoo Lee, Hwijeen Ahn, Sungdong Kim, HyoungSeok Kim, Boseop Kim, Kyunghyun Cho, Gichang Lee, WooMyoung Park, JungWoo Ha, Nako Sung |  |
| 460 |  |  [Sketching as a Tool for Understanding and Accelerating Self-attention for Long Sequences](https://doi.org/10.18653/v1/2022.naacl-main.381) |  | 0 | Transformer-based models are not efficient in processing long sequences due to the quadratic space and time complexity of the self-attention modules. To address this limitation, Linformer and Informer reduce the quadratic complexity to linear (modulo logarithmic factors) via low-dimensional projection and row selection, respectively. These two models are intrinsically connected, and to understand their connection we introduce a theoretical framework of matrix sketching. Based on the theoretical... | Yifan Chen, Qi Zeng, Dilek HakkaniTur, Di Jin, Heng Ji, Yun Yang |  |
| 461 |  |  [Partner Personas Generation for Dialogue Response Generation](https://doi.org/10.18653/v1/2022.naacl-main.382) |  | 0 | Incorporating personas information allows diverse and engaging responses in dialogue response generation. Unfortunately, prior works have primarily focused on self personas and have overlooked the value of partner personas. Moreover, in practical applications, the availability of the gold partner personas is often not the case. This paper attempts to tackle these issues by offering a novel framework that leverages automatic partner personas generation to enhance the succeeding dialogue response... | Hongyuan Lu, Wai Lam, Hong Cheng, Helen Meng |  |
| 462 |  |  [Semantically Informed Slang Interpretation](https://doi.org/10.18653/v1/2022.naacl-main.383) |  | 0 | Slang is a predominant form of informal language making flexible and extended use of words that is notoriously hard for natural language processing systems to interpret. Existing approaches to slang interpretation tend to rely on context but ignore semantic extensions common in slang word usage. We propose a semantically informed slang interpretation (SSI) framework that considers jointly the contextual and semantic appropriateness of a candidate interpretation for a query slang. We perform... | Zhewei Sun, Richard S. Zemel, Yang Xu |  |
| 463 |  |  [Dual-Channel Evidence Fusion for Fact Verification over Texts and Tables](https://doi.org/10.18653/v1/2022.naacl-main.384) |  | 0 | Different from previous fact extraction and verification tasks that only consider evidence of a single format, FEVEROUS brings further challenges by extending the evidence format to both plain text and tables. Existing works convert all candidate evidence into either sentences or tables, thus often failing to fully capture the rich context in their original format from the converted evidence, let alone the context information lost during conversion. In this paper, we propose a Dual Channel... | Nan Hu, Zirui Wu, Yuxuan Lai, Xiao Liu, Yansong Feng |  |
| 464 |  |  [TreeMix: Compositional Constituency-based Data Augmentation for Natural Language Understanding](https://doi.org/10.18653/v1/2022.naacl-main.385) |  | 0 | Data augmentation is an effective approach to tackle over-fitting. Many previous works have proposed different data augmentations strategies for NLP, such as noise injection, word replacement, back-translation etc. Though effective, they missed one important characteristic of language–compositionality, meaning of a complex expression is built from its sub-parts. Motivated by this, we propose a compositional data augmentation approach for natural language understanding called TreeMix.... | Le Zhang, Zichao Yang, Diyi Yang |  |
| 465 |  |  [Syn2Vec: Synset Colexification Graphs for Lexical Semantic Similarity](https://doi.org/10.18653/v1/2022.naacl-main.386) |  | 0 | In this paper we focus on patterns of colexification (co-expressions of form-meaning mapping in the lexicon) as an aspect of lexical-semantic organization, and use them to build large scale synset graphs across BabelNet’s typologically diverse set of 499 world languages. We introduce and compare several approaches: monolingual and cross-lingual colexification graphs, popular distributional models, and fusion approaches. The models are evaluated against human judgments on a semantic similarity... | John B. Harvill, Roxana Girju, Mark HasegawaJohnson |  |
| 466 |  |  [On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?](https://doi.org/10.18653/v1/2022.naacl-main.387) |  | 0 | Knowledge-grounded conversational models are known to suffer from producing factually invalid statements, a phenomenon commonly called hallucination. In this work, we investigate the underlying causes of this phenomenon: is hallucination due to the training data, or to the models? We conduct a comprehensive human study on both existing knowledge-grounded conversational benchmarks and several state-of-the-art models. Our study reveals that the standard benchmarks consist of > 60% hallucinated... | Nouha Dziri, Sivan Milton, Mo Yu, Osmar R. Zaïane, Siva Reddy |  |
| 467 |  |  [Is "My Favorite New Movie" My Favorite Movie? Probing the Understanding of Recursive Noun Phrases](https://doi.org/10.18653/v1/2022.naacl-main.388) |  | 0 | Recursive noun phrases (NPs) have interesting semantic properties. For example, “my favorite new movie” is not necessarily my favorite movie, whereas “my new favorite movie” is. This is common sense to humans, yet it is unknown whether language models have such knowledge. We introduce the Recursive Noun Phrase Challenge (RNPC), a dataset of three textual inference tasks involving textual entailment and event plausibility comparison, precisely targeting the understanding of recursive NPs. When... | Qing Lyu, Zheng Hua, Daoxin Li, Li Zhang, Marianna Apidianaki, Chris CallisonBurch |  |
| 468 |  |  [Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance](https://doi.org/10.18653/v1/2022.naacl-main.389) |  | 0 | Human-translated text displays distinct features from naturally written text in the same language. This phenomena, known as translationese, has been argued to confound the machine translation (MT) evaluation. Yet, we find that existing work on translationese neglects some important factors and the conclusions are mostly correlational but not causal. In this work, we collect CausalMT, a dataset where the MT training data are also labeled with the human translation directions. We inspect two... | Jingwei Ni, Zhijing Jin, Markus Freitag, Mrinmaya Sachan, Bernhard Schölkopf |  |
| 469 |  |  [Visual Commonsense in Pretrained Unimodal and Multimodal Models](https://doi.org/10.18653/v1/2022.naacl-main.390) |  | 0 | Our commonsense knowledge about objects includes their typical visual attributes; we know that bananas are typically yellow or green, and not purple. Text and image corpora, being subject to reporting bias, represent this world-knowledge to varying degrees of faithfulness. In this paper, we investigate to what degree unimodal (language-only) and multimodal (image and language) models capture a broad range of visually salient attributes. To that end, we create the Visual Commonsense Tests... | Chenyu Zhang, Benjamin Van Durme, Zhuowan Li, Elias StengelEskin |  |
| 470 |  |  [QuALITY: Question Answering with Long Input Texts, Yes!](https://doi.org/10.18653/v1/2022.naacl-main.391) |  | 0 | To enable building and testing models on long-document comprehension, we introduce QuALITY, a multiple-choice QA dataset with context passages in English that have an average length of about 5,000 tokens, much longer than typical current models can process. Unlike in prior work with passages, our questions are written and validated by contributors who have read the entire passage, rather than relying on summaries or excerpts. In addition, only half of the questions are answerable by annotators... | Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, Samuel R. Bowman |  |
| 471 |  |  [ExSum: From Local Explanations to Model Understanding](https://doi.org/10.18653/v1/2022.naacl-main.392) |  | 0 | Interpretability methods are developed to understand the working mechanisms of black-box models, which is crucial to their responsible deployment. Fulfilling this goal requires both that the explanations generated by these methods are correct and that people can easily and reliably understand them. While the former has been addressed in prior work, the latter is often overlooked, resulting in informal model understanding derived from a handful of local explanations. In this paper, we introduce... | Yilun Zhou, Marco Túlio Ribeiro, Julie Shah |  |
| 472 |  |  [Maximum Bayes Smatch Ensemble Distillation for AMR Parsing](https://doi.org/10.18653/v1/2022.naacl-main.393) |  | 0 | AMR parsing has experienced an unprecendented increase in performance in the last three years, due to a mixture of effects including architecture improvements and transfer learning. Self-learning techniques have also played a role in pushing performance forward. However, for most recent high performant parsers, the effect of self-learning and silver data augmentation seems to be fading. In this paper we propose to overcome this diminishing returns of silver data by combining Smatch-based... | YoungSuk Lee, Ramón Fernandez Astudillo, Hoang Thanh Lam, Tahira Naseem, Radu Florian, Salim Roukos |  |
| 473 |  |  [When Does Syntax Mediate Neural Language Model Performance? Evidence from Dropout Probes](https://doi.org/10.18653/v1/2022.naacl-main.394) |  | 0 | Recent causal probing literature reveals when language models and syntactic probes use similar representations. Such techniques may yield “false negative” causality results: models may use representations of syntax, but probes may have learned to use redundant encodings of the same syntactic information. We demonstrate that models do encode syntactic information redundantly and introduce a new probe design that guides probes to consider all syntactic information present in embeddings. Using... | Mycal Tucker, Tiwalayo Eisape, Peng Qian, Roger Levy, Julie Shah |  |
| 474 |  |  [Modeling Task Interactions in Document-Level Joint Entity and Relation Extraction](https://doi.org/10.18653/v1/2022.naacl-main.395) |  | 0 | We target on the document-level relation extraction in an end-to-end setting, where the model needs to jointly perform mention extraction, coreference resolution (COREF) and relation extraction (RE) at once, and gets evaluated in an entity-centric way. Especially, we address the two-way interaction between COREF and RE that has not been the focus by previous work, and propose to introduce explicit interaction namely Graph Compatibility (GC) that is specifically designed to leverage task... | Liyan Xu, Jinho D. Choi |  |
| 475 |  |  [Few-Shot Semantic Parsing with Language Models Trained on Code](https://doi.org/10.18653/v1/2022.naacl-main.396) |  | 0 | Large language models can perform semantic parsing with little training data, when prompted with in-context examples. It has been shown that this can be improved by formulating the problem as paraphrasing into canonical utterances, which casts the underlying meaning representation into a controlled natural language-like representation. Intuitively, such models can more easily output canonical utterances as they are closer to the natural language used for pre-training. Recently, models also... | Richard Shin, Benjamin Van Durme |  |
| 476 |  |  [CORWA: A Citation-Oriented Related Work Annotation Dataset](https://doi.org/10.18653/v1/2022.naacl-main.397) |  | 0 | Academic research is an exploratory activity to discover new solutions to problems. By this nature, academic research works perform literature reviews to distinguish their novelties from prior work. In natural language processing, this literature review is usually conducted under the “Related Work” section. The task of related work generation aims to automatically generate the related work section given the rest of the research paper and a list of papers to cite. Prior work on this task has... | Xiangci Li, Biswadip Mandal, Jessica Ouyang |  |
| 477 |  |  [Overcoming Catastrophic Forgetting During Domain Adaptation of Seq2seq Language Generation](https://doi.org/10.18653/v1/2022.naacl-main.398) |  | 0 | Seq2seq language generation models that are trained offline with multiple domains in a sequential fashion often suffer from catastrophic forgetting. Lifelong learning has been proposed to handle this problem. However, existing work such as experience replay or elastic weighted consolidation requires incremental memory space. In this work, we propose an innovative framework, RMR_DSEthat leverages a recall optimization mechanism to selectively memorize important parameters of previous tasks via... | Dingcheng Li, Zheng Chen, Eunah Cho, Jie Hao, Xiaohu Liu, Fan Xing, Chenlei Guo, Yang Liu |  |
| 478 |  |  [Extreme Zero-Shot Learning for Extreme Text Classification](https://doi.org/10.18653/v1/2022.naacl-main.399) |  | 0 | The eXtreme Multi-label text Classification (XMC) problem concerns finding most relevant labels for an input text instance from a large label set. However, the XMC setup faces two challenges: (1) it is not generalizable to predict unseen labels in dynamic environments, and (2) it requires a large amount of supervised (instance, label) pairs, which can be difficult to obtain for emerging domains. In this paper, we consider a more practical scenario called Extreme Zero-Shot XMC (EZ-XMC), in which... | Yuanhao Xiong, WeiCheng Chang, ChoJui Hsieh, HsiangFu Yu, Inderjit S. Dhillon |  |
| 479 |  |  [ConfliBERT: A Pre-trained Language Model for Political Conflict and Violence](https://doi.org/10.18653/v1/2022.naacl-main.400) |  | 0 | Analyzing conflicts and political violence around the world is a persistent challenge in the political science and policy communities due in large part to the vast volumes of specialized text needed to monitor conflict and violence on a global scale. To help advance research in political science, we introduce ConfliBERT, a domain-specific pre-trained language model for conflict and political violence. We first gather a large domain-specific text corpus for language modeling from various... | Yibo Hu, Seyyed MohammadSaleh Hosseini, Erick Skorupa Parolin, Javier Osorio, Latifur Khan, Patrick T. Brandt, Vito D'Orazio |  |
| 480 |  |  [Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification](https://doi.org/10.18653/v1/2022.naacl-main.401) |  | 0 | Prompt-based learning (i.e., prompting) is an emerging paradigm for exploiting knowledge learned by a pretrained language model. In this paper, we propose Automatic Multi-Label Prompting (AMuLaP), a simple yet effective method to automatically select label mappings for few-shot text classification with prompting. Our method exploits one-to-many label mappings and a statistics-based algorithm to select label mappings given a prompt template. Our experiments demonstrate that AMuLaP achieves... | Han Wang, Canwen Xu, Julian J. McAuley |  |
| 481 |  |  [Few-shot Subgoal Planning with Language Models](https://doi.org/10.18653/v1/2022.naacl-main.402) |  | 0 | Pre-trained language models have shown successful progress in many text understanding benchmarks. This work explores the capability of these models to predict actionable plans in real-world environments. Given a text instruction, we show that language priors encoded in pre-trained models allow us to infer fine-grained subgoal sequences. In contrast to recent methods which make strong assumptions about subgoal supervision, our experiments show that language models can infer detailed subgoal... | Lajanugen Logeswaran, Yao Fu, Moontae Lee, Honglak Lee |  |
| 482 |  |  [IDPG: An Instance-Dependent Prompt Generation Method](https://doi.org/10.18653/v1/2022.naacl-main.403) |  | 0 | Prompt tuning is a new, efficient NLP transfer learning paradigm that adds a task-specific prompt in each input instance during the model training stage. It freezes the pre-trained language model and only optimizes a few task-specific prompts. In this paper, we propose a conditional prompt generation method to generate prompts for each input instance, referred to as the Instance-Dependent Prompt Generation (IDPG). Unlike traditional prompt tuning methods that use a fixed prompt, IDPG introduces... | Zhuofeng Wu, Sinong Wang, Jiatao Gu, Rui Hou, Yuxiao Dong, V. G. Vinod Vydiswaran, Hao Ma |  |
| 483 |  |  [Embedding Hallucination for Few-shot Language Fine-tuning](https://doi.org/10.18653/v1/2022.naacl-main.404) |  | 0 | Few-shot language learners adapt knowledge from a pre-trained model to recognize novel classes from a few-labeled sentences. In such settings, fine-tuning a pre-trained language model can cause severe over-fitting. In this paper, we propose an Embedding Hallucination (EmbedHalluc) method, which generates auxiliary embedding-label pairs to expand the fine-tuning dataset. The hallucinator is trained by playing an adversarial game with the discriminator, such that the hallucinated embedding is... | Yiren Jian, Chongyang Gao, Soroush Vosoughi |  |
| 484 |  |  [Cryptocurrency Bubble Detection: A New Stock Market Dataset, Financial Task & Hyperbolic Models](https://doi.org/10.18653/v1/2022.naacl-main.405) |  | 0 | The rapid spread of information over social media influences quantitative trading and investments. The growing popularity of speculative trading of highly volatile assets such as cryptocurrencies and meme stocks presents a fresh challenge in the financial realm. Investigating such “bubbles” - periods of sudden anomalous behavior of markets are critical in better understanding investor behavior and market dynamics. However, high volatility coupled with massive volumes of chaotic social media... | Ramit Sawhney, Shivam Agarwal, Vivek Mittal, Paolo Rosso, Vikram Nanda, Sudheer Chava |  |
| 485 |  |  [Nearest Neighbor Knowledge Distillation for Neural Machine Translation](https://doi.org/10.18653/v1/2022.naacl-main.406) |  | 0 | k-nearest-neighbor machine translation (kNN-MT), proposed by Khandelwal et al. (2021), has achieved many state-of-the-art results in machine translation tasks. Although effective, kNN-MT requires conducting kNN searches through the large datastore for each decoding step during inference, prohibitively increasing the decoding cost and thus leading to the difficulty for the deployment in real-world applications. In this paper, we propose to move the time-consuming kNN search forward to the... | Zhixian Yang, Renliang Sun, Xiaojun Wan |  |
| 486 |  |  [DEMix Layers: Disentangling Domains for Modular Language Modeling](https://doi.org/10.18653/v1/2022.naacl-main.407) |  | 0 | We introduce a new domain expert mixture (DEMix) layer that enables conditioning a language model (LM) on the domain of the input text. A DEMix layer includes a collection of expert feedforward networks, each specialized to a domain, that makes the LM modular: experts can be mixed, added, or removed after initial training. Extensive experiments with autoregressive transformer LMs (up to 1.3B parameters) show that DEMix layers reduce test-time perplexity (especially for out-of-domain data),... | Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, Luke Zettlemoyer |  |
| 487 |  |  [Contrastive Learning for Prompt-based Few-shot Language Learners](https://doi.org/10.18653/v1/2022.naacl-main.408) |  | 0 | The impressive performance of GPT-3 using natural language prompts and in-context learning has inspired work on better fine-tuning of moderately-sized models under this paradigm. Following this line of work, we present a contrastive learning framework that clusters inputs from the same class for better generality of models trained with only limited examples. Specifically, we propose a supervised contrastive framework that clusters inputs from the same class under different augmented “views” and... | Yiren Jian, Chongyang Gao, Soroush Vosoughi |  |
| 488 |  |  [Cross-Lingual Event Detection via Optimized Adversarial Training](https://doi.org/10.18653/v1/2022.naacl-main.409) |  | 0 | In this work, we focus on Cross-Lingual Event Detection where a model is trained on data from a source language but its performance is evaluated on data from a second, target, language. Most recent works in this area have harnessed the language-invariant qualities displayed by pre-trained Multi-lingual Language Models. Their performance, however, reveals there is room for improvement as the cross-lingual setting entails particular challenges. We employ Adversarial Language Adaptation to train a... | Luis GuzmanNateras, Minh Van Nguyen, Thien Nguyen |  |
| 489 |  |  [Identifying Implicitly Abusive Remarks about Identity Groups using a Linguistically Informed Approach](https://doi.org/10.18653/v1/2022.naacl-main.410) |  | 0 | We address the task of distinguishing implicitly abusive sentences on identity groups (“Muslims contaminate our planet”) from other group-related negative polar sentences (“Muslims despise terrorism”). Implicitly abusive language are utterances not conveyed by abusive words (e.g. “bimbo” or “scum”). So far, the detection of such utterances could not be properly addressed since existing datasets displaying a high degree of implicit abuse are fairly biased. Following the recently-proposed... | Michael Wiegand, Elisabeth Eder, Josef Ruppenhofer |  |
| 490 |  |  [Label Definitions Improve Semantic Role Labeling](https://doi.org/10.18653/v1/2022.naacl-main.411) |  | 0 | Argument classification is at the core of Semantic Role Labeling. Given a sentence and the predicate, a semantic role label is assigned to each argument of the predicate. While semantic roles come with meaningful definitions, existing work has treated them as symbolic. Learning symbolic labels usually requires ample training data, which is frequently unavailable due to the cost of annotation. We instead propose to retrieve and leverage the definitions of these labels from the annotation... | Li Zhang, Ishan Jindal, Yunyao Li |  |
| 491 |  |  [Shedding New Light on the Language of the Dark Web](https://doi.org/10.18653/v1/2022.naacl-main.412) |  | 0 | The hidden nature and the limited accessibility of the Dark Web, combined with the lack of public datasets in this domain, make it difficult to study its inherent characteristics such as linguistic properties. Previous works on text classification of Dark Web domain have suggested that the use of deep neural models may be ineffective, potentially due to the linguistic differences between the Dark and Surface Webs. However, not much work has been done to uncover the linguistic characteristics of... | Youngjin Jin, Eugene Jang, Yongjae Lee, Seungwon Shin, JinWoo Chung |  |
| 492 |  |  [Conceptualizing Treatment Leakage in Text-based Causal Inference](https://doi.org/10.18653/v1/2022.naacl-main.413) |  | 0 | Causal inference methods that control for text-based confounders are becoming increasingly important in the social sciences and other disciplines where text is readily available. However, these methods rely on a critical assumption that there is no treatment leakage: that is, the text only contains information about the confounder and no information about treatment assignment. When this assumption does not hold, methods that control for text to adjust for confounders face the problem of... | Adel Daoud, Connor T. Jerzak, Richard Johansson |  |
| 493 |  |  [Consistency Training with Virtual Adversarial Discrete Perturbation](https://doi.org/10.18653/v1/2022.naacl-main.414) |  | 0 | Consistency training regularizes a model by enforcing predictions of original and perturbed inputs to be similar. Previous studies have proposed various augmentation methods for the perturbation but are limited in that they are agnostic to the training model. Thus, the perturbed samples may not aid in regularization due to their ease of classification from the model. In this context, we propose an augmentation method of adding a discrete noise that would incur the highest divergence between... | Jungsoo Park, Gyuwan Kim, Jaewoo Kang |  |
| 494 |  |  [CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning](https://doi.org/10.18653/v1/2022.naacl-main.415) |  | 0 | Factual inconsistencies in generated summaries severely limit the practical applications of abstractive dialogue summarization. Although significant progress has been achieved by using pre-trained neural language models, substantial amounts of hallucinated content are found during the human evaluation. In this work, we first devised a typology of factual errors to better understand the types of hallucinations generated by current models and conducted human evaluation on popular dialog... | Xiangru Tang, Arjun Nair, Borui Wang, Bingyao Wang, Jai Desai, Aaron Wade, Haoran Li, Asli Celikyilmaz, Yashar Mehdad, Dragomir R. Radev |  |
| 495 |  |  [CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation](https://doi.org/10.18653/v1/2022.naacl-main.416) |  | 0 | As the use of interactive machines grow, the task of Emotion Recognition in Conversation (ERC) became more important. If the machine-generated sentences reflect emotion, more human-like sympathetic conversations are possible. Since emotion recognition in conversation is inaccurate if the previous utterances are not taken into account, many studies reflect the dialogue context to improve the performances. Many recent approaches show performance improvement by combining knowledge into modules... | Joosung Lee, Wooin Lee |  |
| 496 |  |  [Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries](https://doi.org/10.18653/v1/2022.naacl-main.417) |  | 0 | Current pre-trained models applied for summarization are prone to factual inconsistencies that misrepresent the source text. Evaluating the factual consistency of summaries is thus necessary to develop better models. However, the human evaluation setup for evaluating factual consistency has not been standardized. To determine the factors that affect the reliability of the human evaluation, we crowdsource evaluations for factual consistency across state-of-the-art models on two news... | Xiangru Tang, Alexander R. Fabbri, Haoran Li, Ziming Mao, Griffin Adams, Borui Wang, Asli Celikyilmaz, Yashar Mehdad, Dragomir R. Radev |  |
| 497 |  |  [DialSummEval: Revisiting Summarization Evaluation for Dialogues](https://doi.org/10.18653/v1/2022.naacl-main.418) |  | 0 | Dialogue summarization is receiving increasing attention from researchers due to its extraordinary difficulty and unique application value. We observe that current dialogue summarization models have flaws that may not be well exposed by frequently used metrics such as ROUGE. In our paper, we re-evaluate 18 categories of metrics in terms of four dimensions: coherence, consistency, fluency and relevance, as well as a unified human evaluation of various models for the first time. Some noteworthy... | Mingqi Gao, Xiaojun Wan |  |
| 498 |  |  [Hyperbolic Relevance Matching for Neural Keyphrase Extraction](https://doi.org/10.18653/v1/2022.naacl-main.419) |  | 0 | Keyphrase extraction is a fundamental task in natural language processing that aims to extract a set of phrases with important information from a source document. Identifying important keyphrases is the central component of keyphrase extraction, and its main challenge is learning to represent information comprehensively and discriminate importance accurately. In this paper, to address the above issues, we design a new hyperbolic matching model (HyperMatch) to explore keyphrase extraction in... | Mingyang Song, Yi Feng, Liping Jing |  |
| 499 |  |  [Template-free Prompt Tuning for Few-shot NER](https://doi.org/10.18653/v1/2022.naacl-main.420) |  | 0 | Prompt-based methods have been successfully applied in sentence-level few-shot learning tasks, mostly owing to the sophisticated design of templates and label words. However, when applied to token-level labeling tasks such as NER, it would be time-consuming to enumerate the template queries over all potential entity spans. In this work, we propose a more elegant method to reformulate NER tasks as LM problems without any templates. Specifically, we discard the template construction process while... | Ruotian Ma, Xin Zhou, Tao Gui, Yiding Tan, Linyang Li, Qi Zhang, Xuanjing Huang |  |
| 500 |  |  [Few-Shot Document-Level Relation Extraction](https://doi.org/10.18653/v1/2022.naacl-main.421) |  | 0 | We present FREDo, a few-shot document-level relation extraction (FSDLRE) benchmark. As opposed to existing benchmarks which are built on sentence-level relation extraction corpora, we argue that document-level corpora provide more realism, particularly regarding none-of-the-above (NOTA) distributions. Therefore, we propose a set of FSDLRE tasks and construct a benchmark based on two existing supervised learning data sets, DocRED and sciERC. We adapt the state-of-the-art sentence-level method... | Nicholas Popovic, Michael Färber |  |
| 501 |  |  [LaMemo: Language Modeling with Look-Ahead Memory](https://doi.org/10.18653/v1/2022.naacl-main.422) |  | 0 | Although Transformers with fully connected self-attentions are powerful to model long-term dependencies, they are struggling to scale to long texts with thousands of words in language modeling. One of the solutions is to equip the model with a recurrence memory. However, existing approaches directly reuse hidden states from the previous segment that encodes contexts in a uni-directional way. As a result, this prohibits the memory to dynamically interact with the current context that provides... | Haozhe Ji, Rongsheng Zhang, Zhenyu Yang, Zhipeng Hu, Minlie Huang |  |
| 502 |  |  [Exploiting Inductive Bias in Transformers for Unsupervised Disentanglement of Syntax and Semantics with VAEs](https://doi.org/10.18653/v1/2022.naacl-main.423) |  | 0 | We propose a generative model for text generation, which exhibits disentangled latent representations of syntax and semantics. Contrary to previous work, this model does not need syntactic information such as constituency parses, or semantic information such as paraphrase pairs. Our model relies solely on the inductive bias found in attention-based architectures such as Transformers. In the attention of Transformers, keys handle information selection while values specify what information is... | Ghazi Felhi, Joseph Le Roux, Djamé Seddah |  |
| 503 |  |  [Neighbors Are Not Strangers: Improving Non-Autoregressive Translation under Low-Frequency Lexical Constraints](https://doi.org/10.18653/v1/2022.naacl-main.424) |  | 0 | Lexically constrained neural machine translation (NMT) draws much industrial attention for its practical usage in specific domains. However, current autoregressive approaches suffer from high latency. In this paper, we focus on non-autoregressive translation (NAT) for this problem for its efficiency advantage. We identify that current constrained NAT models, which are based on iterative editing, do not handle low-frequency constraints well. To this end, we propose a plug-in algorithm for this... | Chun Zeng, Jiangjie Chen, Tianyi Zhuang, Rui Xu, Hao Yang, Ying Qin, Shimin Tao, Yanghua Xiao |  |
| 504 |  |  [What do Toothbrushes do in the Kitchen? How Transformers Think our World is Structured](https://doi.org/10.18653/v1/2022.naacl-main.425) |  | 0 | Transformer-based models are now predominant in NLP.They outperform approaches based on static models in many respects. This success has in turn prompted research that reveals a number of biases in the language models generated by transformers. In this paper we utilize this research on biases to investigate to what extent transformer-based language models allow for extracting knowledge about object relations (X occurs in Y; X consists of Z; action A involves using X).To this end, we compare... | Alexander Henlein, Alexander Mehler |  |
| 505 |  |  [Less is More: Learning to Refine Dialogue History for Personalized Dialogue Generation](https://doi.org/10.18653/v1/2022.naacl-main.426) |  | 0 | Personalized dialogue systems explore the problem of generating responses that are consistent with the user’s personality, which has raised much attention in recent years. Existing personalized dialogue systems have tried to extract user profiles from dialogue history to guide personalized response generation. Since the dialogue history is usually long and noisy, most existing methods truncate the dialogue history to model the user’s personality. Such methods can generate some personalized... | Hanxun Zhong, Zhicheng Dou, Yutao Zhu, Hongjin Qian, JiRong Wen |  |
| 506 |  |  [A Holistic Framework for Analyzing the COVID-19 Vaccine Debate](https://doi.org/10.18653/v1/2022.naacl-main.427) |  | 0 | The Covid-19 pandemic has led to infodemic of low quality information leading to poor health decisions. Combating the outcomes of this infodemic is not only a question of identifying false claims, but also reasoning about the decisions individuals make. In this work we propose a holistic analysis framework connecting stance and reason analysis, and fine-grained entity level moral sentiment analysis. We study how to model the dependencies between the different level of analysis and incorporate... | Maria Leonor Pacheco, Tunazzina Islam, Monal Mahajan, Andrey Shor, Ming Yin, Lyle H. Ungar, Dan Goldwasser |  |
| 507 |  |  [Learning to Win Lottery Tickets in BERT Transfer via Task-agnostic Mask Training](https://doi.org/10.18653/v1/2022.naacl-main.428) |  | 0 | Recent studies on the lottery ticket hypothesis (LTH) show that pre-trained language models (PLMs) like BERT contain matching subnetworks that have similar transfer learning performance as the original PLM. These subnetworks are found using magnitude-based pruning. In this paper, we find that the BERT subnetworks have even more potential than these studies have shown. Firstly, we discover that the success of magnitude pruning can be attributed to the preserved pre-training performance, which... | Yuanxin Liu, Fandong Meng, Zheng Lin, Peng Fu, Yanan Cao, Weiping Wang, Jie Zhou |  |
| 508 |  |  [You Don't Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers' Private Personas](https://doi.org/10.18653/v1/2022.naacl-main.429) |  | 0 | Social chatbots, also known as chit-chat chatbots, evolve rapidly with large pretrained language models. Despite the huge progress, privacy concerns have arisen recently: training data of large language models can be extracted via model inversion attacks. On the other hand, the datasets used for training chatbots contain many private conversations between two individuals. In this work, we further investigate the privacy leakage of the hidden states of chatbots trained by language modeling which... | Haoran Li, Yangqiu Song, Lixin Fan |  |
| 509 |  |  [Explaining Dialogue Evaluation Metrics using Adversarial Behavioral Analysis](https://doi.org/10.18653/v1/2022.naacl-main.430) |  | 0 | There is an increasing trend in using neural methods for dialogue model evaluation. Lack of a framework to investigate these metrics can cause dialogue models to reflect their biases and cause unforeseen problems during interactions. In this work, we propose an adversarial test-suite which generates problematic variations of various dialogue aspects, e.g. logical entailment, using automatic heuristics. We show that dialogue metrics for both open-domain and task-oriented settings are biased in... | Baber Khalid, Sungjin Lee |  |
| 510 |  |  [Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection](https://doi.org/10.18653/v1/2022.naacl-main.431) |  | 0 | The perceived toxicity of language can vary based on someone’s identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the \*who\*, \*why\*, and \*what\* behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (\*who\*) and beliefs (\*why\*), drawing from social psychology... | Maarten Sap, Swabha Swayamdipta, Laura Vianna, Xuhui Zhou, Yejin Choi, Noah A. Smith |  |
| 511 |  |  [Non-Autoregressive Chinese ASR Error Correction with Phonological Training](https://doi.org/10.18653/v1/2022.naacl-main.432) |  | 0 | Automatic Speech Recognition (ASR) is an efficient and widely used input method that transcribes speech signals into text. As the errors introduced by ASR systems will impair the performance of downstream tasks, we introduce a post-processing error correction method, PhVEC, to correct errors in text space. For the errors in ASR result, existing works mainly focus on fixed-length corrections, modifying each wrong token to a correct one (one-to-one correction), but rarely consider the... | Zheng Fang, Ruiqing Zhang, Zhongjun He, Hua Wu, Yanan Cao |  |
| 512 |  |  [Hate Speech and Counter Speech Detection: Conversational Context Does Matter](https://doi.org/10.18653/v1/2022.naacl-main.433) |  | 0 | Hate speech is plaguing the cyberspace along with user-generated content. Adding counter speech has become an effective way to combat hate speech online. Existing datasets and models target either (a) hate speech or (b) hate and counter speech but disregard the context. This paper investigates the role of context in the annotation and detection of online hate and counter speech, where context is defined as the preceding comment in a conversation thread. We created a context-aware dataset for a... | Xinchen Yu, Eduardo Blanco, Lingzi Hong |  |
| 513 |  |  [DACSA: A large-scale Dataset for Automatic summarization of Catalan and Spanish newspaper Articles](https://doi.org/10.18653/v1/2022.naacl-main.434) |  | 0 | The application of supervised methods to automatic summarization requires the availability of adequate corpora consisting of a set of document-summary pairs. As in most Natural Language Processing tasks, the great majority of available datasets for summarization are in English, making it difficult to develop automatic summarization models for other languages. Although Spanish is gradually forming part of some recent summarization corpora, it is not the same for minority languages such as... | Encarnación Segarra Soriano, Vicent Ahuir, LluísF. Hurtado, José González |  |
| 514 |  |  [Time Waits for No One! Analysis and Challenges of Temporal Misalignment](https://doi.org/10.18653/v1/2022.naacl-main.435) |  | 0 | When an NLP model is trained on text data from one time period and tested or deployed on data from another, the resulting temporal misalignment can degrade end-task performance. In this work, we establish a suite of eight diverse tasks across different domains (social media, science papers, news, and reviews) and periods of time (spanning five years or more) to quantify the effects of temporal misalignment. Our study is focused on the ubiquitous setting where a pretrained model is optionally... | Kelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, Noah A. Smith |  |
| 515 |  |  [MCSE: Multimodal Contrastive Learning of Sentence Embeddings](https://doi.org/10.18653/v1/2022.naacl-main.436) |  | 0 | Learning semantically meaningful sentence embeddings is an open problem in natural language processing. In this work, we propose a sentence embedding learning approach that exploits both visual and textual information via a multimodal contrastive objective. Through experiments on a variety of semantic textual similarity tasks, we demonstrate that our approach consistently improves the performance across various datasets and pre-trained encoders. In particular, combining a small amount of... | Miaoran Zhang, Marius Mosbach, David Ifeoluwa Adelani, Michael A. Hedderich, Dietrich Klakow |  |
| 516 |  |  [HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction](https://doi.org/10.18653/v1/2022.naacl-main.437) |  | 0 | Unsupervised relation extraction aims to extract the relationship between entities from natural language sentences without prior information on relational scope or distribution. Existing works either utilize self-supervised schemes to refine relational feature signals by iteratively leveraging adaptive clustering and classification that provoke gradual drift problems, or adopt instance-wise contrastive learning which unreasonably pushes apart those sentence pairs that are semantically similar.... | Shuliang Liu, Xuming Hu, Chenwei Zhang, Shu'ang Li, Lijie Wen, Philip S. Yu |  |
| 517 |  |  [Diagnosing Vision-and-Language Navigation: What Really Matters](https://doi.org/10.18653/v1/2022.naacl-main.438) |  | 0 | Vision-and-language navigation (VLN) is a multimodal task where an agent follows natural language instructions and navigates in visual environments. Multiple setups have been proposed, and researchers apply new model architectures or training techniques to boost navigation performance. However, there still exist non-negligible gaps between machines’ performance and human benchmarks. Moreover, the agents’ inner mechanisms for navigation decisions remain unclear. To the best of our knowledge, how... | Wanrong Zhu, Yuankai Qi, Pradyumna Narayana, Kazoo Sone, Sugato Basu, Xin Wang, Qi Wu, Miguel P. Eckstein, William Yang Wang |  |
| 518 |  |  [Aligning to Social Norms and Values in Interactive Narratives](https://doi.org/10.18653/v1/2022.naacl-main.439) |  | 0 | We focus on creating agents that act in alignment with socially beneficial norms and values in interactive narratives or text-based games—environments wherein an agent perceives and interacts with a world through natural language. Such interactive agents are often trained via reinforcement learning to optimize task performance, even when such rewards may lead to agent behaviors that violate societal norms—causing harm either to the agent itself or other entities in the environment. Social value... | Prithviraj Ammanabrolu, Liwei Jiang, Maarten Sap, Hannaneh Hajishirzi, Yejin Choi |  |
| 519 |  |  [MOVER: Mask, Over-generate and Rank for Hyperbole Generation](https://doi.org/10.18653/v1/2022.naacl-main.440) |  | 0 | Despite being a common figure of speech, hyperbole is under-researched in Figurative Language Processing. In this paper, we tackle the challenging task of hyperbole generation to transfer a literal sentence into its hyperbolic paraphrase. To address the lack of available hyperbolic sentences, we construct HYPO-XL, the first large-scale English hyperbole corpus containing 17,862 hyperbolic sentences in a non-trivial way. Based on our corpus, we propose an unsupervised method for hyperbole... | Yunxiang Zhang, Xiaojun Wan |  |
| 520 |  |  [Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference](https://doi.org/10.18653/v1/2022.naacl-main.441) |  | 0 | The task of natural language inference (NLI), to decide if a hypothesis entails or contradicts a premise, received considerable attention in recent years. All competitive systems build on top of contextualized representations and make use of transformer architectures for learning an NLI model. When somebody is faced with a particular NLI task, they need to select the best model that is available. This is a time-consuming and resource-intense endeavour. To solve this practical problem, we... | Emils Kadikis, Vaibhav Srivastav, Roman Klinger |  |
| 521 |  |  [Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics](https://doi.org/10.18653/v1/2022.naacl-main.442) |  | 0 | How reliably an automatic summarization evaluation metric replicates human judgments of summary quality is quantified by system-level correlations. We identify two ways in which the definition of the system-level correlation is inconsistent with how metrics are used to evaluate systems in practice and propose changes to rectify this disconnect. First, we calculate the system score for an automatic metric using the full test set instead of the subset of summaries judged by humans, which is... | Daniel Deutsch, Rotem Dror, Dan Roth |  |
| 522 |  |  [Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022](https://aclanthology.org/volumes/2022.findings-naacl/) |  | 0 |  | Marine Carpuat, MarieCatherine de Marneffe, Iván Vladimir Meza Ruíz |  |
| 523 |  |  [Frontmatter](https://aclanthology.org/2022.findings-naacl.0) |  | 0 |  |  |  |
| 524 |  |  [PubHealthTab: A Public Health Table-based Dataset for Evidence-based Fact Checking](https://doi.org/10.18653/v1/2022.findings-naacl.1) |  | 0 | Inspired by human fact checkers, who use different types of evidence (e.g. tables, images, audio) in addition to text, several datasets with tabular evidence data have been released in recent years. Whilst the datasets encourage research on table fact-checking, they rely on information from restricted data sources, such as Wikipedia for creating claims and extracting evidence data, making the fact-checking process different from the real-world process used by fact checkers. In this paper, we... | Mubashara Akhtar, Oana Cocarascu, Elena Simperl |  |
| 525 |  |  [Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context](https://doi.org/10.18653/v1/2022.findings-naacl.2) |  | 0 | Physical measurements constitute a large portion of numbers in academic papers, engineering reports, and web tables. Current benchmarks fall short of properly evaluating numeracy of pretrained language models on measurements, hindering research on developing new methods and applying them to numerical tasks. To that end, we introduce a novel task, Masked Measurement Prediction (MMP), where a model learns to reconstruct a number together with its associated unit given masked text. MMP is useful... | Daniel Spokoyny, Ivan Lee, Zhao Jin, Taylor BergKirkpatrick |  |
| 526 |  |  [PromptGen: Automatically Generate Prompts using Generative Models](https://doi.org/10.18653/v1/2022.findings-naacl.3) |  | 0 | Recently, prompt learning has received significant attention, where the downstream tasks are reformulated to the mask-filling task with the help of a textual prompt. The key point of prompt learning is finding the most appropriate prompt. This paper proposes a novel model PromptGen, which can automatically generate prompts conditional on the input sentence. PromptGen is the first work considering dynamic prompt generation for knowledge probing, based on a pre-trained generative model. To... | Yue Zhang, Hongliang Fei, Dingcheng Li, Ping Li |  |
| 527 |  |  [Improving Conversational Recommendation Systems' Quality with Context-Aware Item Meta-Information](https://doi.org/10.18653/v1/2022.findings-naacl.4) |  | 0 | A key challenge of Conversational Recommendation Systems (CRS) is to integrate the recommendation function and the dialog generation function smoothly. Previous works employ graph neural networks with external knowledge graphs (KG) to model individual recommendation items and integrate KGs with language models through attention mechanism for response generation. Although previous approaches prove effective, there is still room for improvement. For example, KG-based approaches only rely on... | Bowen Yang, Cong Han, Yu Li, Lei Zuo, Zhou Yu |  |
| 528 |  |  [SEQZERO: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models](https://doi.org/10.18653/v1/2022.findings-naacl.5) |  | 0 | Recent research showed promising results on combining pretrained language models (LMs) with canonical utterance for few-shot semantic parsing. The canonical utterance is often lengthy and complex due to the compositional structure of formal languages. Learning to generate such canonical utterance requires significant amount of data to reach high performance. Fine-tuning with only few-shot samples, the LMs can easily forget pretrained knowledge, overfit spurious biases, and suffer from... | Jingfeng Yang, Haoming Jiang, Qingyu Yin, Danqing Zhang, Bing Yin, Diyi Yang |  |
| 529 |  |  [MultiVerS: Improving scientific claim verification with weak supervision and full-document context](https://doi.org/10.18653/v1/2022.findings-naacl.6) |  | 0 | The scientific claim verification task requires an NLP system to label scientific documents which Support or Refute an input claim, and to select evidentiary sentences (or rationales) justifying each predicted label. In this work, we present MultiVerS, which predicts a fact-checking label and identifies rationales in a multitask fashion based on a shared encoding of the claim and full document context. This approach accomplishes two key modeling goals. First, it ensures that all relevant... | David Wadden, Kyle Lo, Lucy Lu Wang, Arman Cohan, Iz Beltagy, Hannaneh Hajishirzi |  |
| 530 |  |  [An Item Response Theory Framework for Persuasion](https://doi.org/10.18653/v1/2022.findings-naacl.7) |  | 0 | In this paper, we apply Item Response Theory, popular in education and political science research, to the analysis of argument persuasiveness in language. We empirically evaluate the model’s performance on three datasets, including a novel dataset in the area of political advocacy. We show the advantages of separating these components under several style and content representations, including evaluating the ability of the speaker embeddings generated by the model to parallel real-world... | Anastassia Kornilova, Vladimir Eidelman, Daniel Douglass |  |
| 531 |  |  [Self-Supervised Contrastive Learning with Adversarial Perturbations for Defending Word Substitution-based Attacks](https://doi.org/10.18653/v1/2022.findings-naacl.8) |  | 0 | In this paper, we present an approach to improve the robustness of BERT language models against word substitution-based adversarial attacks by leveraging adversarial perturbations for self-supervised contrastive learning. We create a word-level adversarial attack generating hard positives on-the-fly as adversarial examples during contrastive learning. In contrast to previous works, our method improves model robustness without using any labeled data. Experimental results show that our method... | Zhao Meng, Yihan Dong, Mrinmaya Sachan, Roger Wattenhofer |  |
| 532 |  |  [Quiz Design Task: Helping Teachers Create Quizzes with Automated Question Generation](https://doi.org/10.18653/v1/2022.findings-naacl.9) |  | 0 | Question generation (QGen) models are often evaluated with standardized NLG metrics that are based on n-gram overlap. In this paper, we measure whether these metric improvements translate to gains in a practical setting, focusing on the use case of helping teachers automate the generation of reading comprehension quizzes. In our study, teachers building a quiz receive question suggestions, which they can either accept or refuse with a reason. Even though we find that recent progress in QGen... | Philippe Laban, ChienSheng Wu, Lidiya Murakhovs'ka, Wenhao Liu, Caiming Xiong |  |
| 533 |  |  [In-BoXBART: Get Instructions into Biomedical Multi-Task Learning](https://doi.org/10.18653/v1/2022.findings-naacl.10) |  | 0 | Single-task models have proven pivotal in solving specific tasks; however, they have limitations in real-world applications where multi-tasking is necessary and domain shifts are exhibited. Recently, instructional prompts have shown significant improvement towards multi-task generalization; however, the effect of instructional prompts and Multi-Task Learning (MTL) has not been systematically studied in the biomedical domain. Motivated by this, this paper explores the impact of instructional... | Mihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, Murad Mohammad, Chitta Baral |  |
| 534 |  |  [How to Translate Your Samples and Choose Your Shots? Analyzing Translate-train & Few-shot Cross-lingual Transfer](https://doi.org/10.18653/v1/2022.findings-naacl.11) |  | 0 | Translate-train or few-shot cross-lingual transfer can be used to improve the zero-shot performance of multilingual pretrained language models. Few-shot utilizes high-quality low-quantity samples (often manually translated from the English corpus ). Translate-train employs a machine translation of the English corpus, resulting in samples with lower quality that could be scaled to high quantity. Given the lower cost and higher availability of machine translation compared to manual professional... | Iman Jundi, Gabriella Lapesa |  |
| 535 |  |  [Multi-Hop Open-Domain Question Answering over Structured and Unstructured Knowledge](https://doi.org/10.18653/v1/2022.findings-naacl.12) |  | 0 | Open-domain question answering systems need to answer question of our interests with structured and unstructured information. However, existing approaches only select one source to generate answer or only conduct reasoning on structured information. In this paper, we pro- pose a Document-Entity Heterogeneous Graph Network, referred to as DEHG, to effectively integrate different sources of information, and conduct reasoning on heterogeneous information. DEHG employs a graph constructor to... | Yue Feng, Zhen Han, Mingming Sun, Ping Li |  |
| 536 |  |  [FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks](https://doi.org/10.18653/v1/2022.findings-naacl.13) |  | 0 | Increasing concerns and regulations about data privacy and sparsity necessitate the study of privacy-preserving, decentralized learning methods for natural language processing (NLP) tasks. Federated learning (FL) provides promising approaches for a large number of clients (e.g., personal devices or organizations) to collaboratively learn a shared global model to benefit all clients while allowing users to keep their data locally. Despite interest in studying FL methods for NLP tasks, a... | Bill Yuchen Lin, Chaoyang He, Zihang Ze, Hulin Wang, Yufen Hua, Christophe Dupuy, Rahul Gupta, Mahdi Soltanolkotabi, Xiang Ren, Salman Avestimehr |  |
| 537 |  |  [SemAttack: Natural Textual Attacks via Different Semantic Spaces](https://doi.org/10.18653/v1/2022.findings-naacl.14) |  | 0 | Recent studies show that pre-trained language models (LMs) are vulnerable to textual adversarial attacks. However, existing attack methods either suffer from low attack success rates or fail to search efficiently in the exponentially large perturbation space. We propose an efficient and effective framework SemAttack to generate natural adversarial text by constructing different semantic perturbation functions. In particular, SemAttack optimizes the generated perturbations constrained on generic... | Boxin Wang, Chejian Xu, Xiangyu Liu, Yu Cheng, Bo Li |  |
| 538 |  |  [Lacuna Reconstruction: Self-Supervised Pre-Training for Low-Resource Historical Document Transcription](https://doi.org/10.18653/v1/2022.findings-naacl.15) |  | 0 | We present a self-supervised pre-training approach for learning rich visual language representations for both handwritten and printed historical document transcription. After supervised fine-tuning of our pre-trained encoder representations for low-resource document transcription on two languages, (1) a heterogeneous set of handwritten Islamicate manuscript images and (2) early modern English printed documents, we show a meaningful improvement in recognition accuracy over the same supervised... | Nikolai Vogler, Jonathan Parkes Allen, Matthew Thomas Miller, Taylor BergKirkpatrick |  |
| 539 |  |  [FreeTransfer-X: Safe and Label-Free Cross-Lingual Transfer from Off-the-Shelf Models](https://doi.org/10.18653/v1/2022.findings-naacl.16) |  | 0 | Cross-lingual transfer (CLT) is of various applications. However, labeled cross-lingual corpus is expensive or even inaccessible, especially in the fields where labels are private, such as diagnostic results of symptoms in medicine and user profiles in business. Nevertheless, there are off-the-shelf models in these sensitive fields. Instead of pursuing the original labels, a workaround for CLT is to transfer knowledge from the off-the-shelf models without labels. To this end, we define a novel... | Yinpeng Guo, Liangyou Li, Xin Jiang, Qun Liu |  |
| 540 |  |  [Opportunities for Human-centered Evaluation of Machine Translation Systems](https://doi.org/10.18653/v1/2022.findings-naacl.17) |  | 0 | Machine translation models are embedded in larger user-facing systems. Although model evaluation has matured, evaluation at the systems level is still lacking. We review literature from both the translation studies and HCI communities about who uses machine translation and for what purposes. We emphasize an important difference in evaluating machine translation models versus the physical and cultural systems in which they are embedded. We then propose opportunities for improved measurement of... | Daniel Liebling, Katherine A. Heller, Samantha Robertson, Wesley Hanwen Deng |  |
| 541 |  |  [Aligning Generative Language Models with Human Values](https://doi.org/10.18653/v1/2022.findings-naacl.18) |  | 0 | Although current large-scale generative language models (LMs) can show impressive insights about factual knowledge, they do not exhibit similar success with respect to human values judgements (e.g., whether or not the generations of an LM are moral). Existing methods learn human values either by directly mimicking the behavior of human data, or rigidly constraining the generation space to human-chosen tokens. These methods are inherently limited in that they do not consider the contextual and... | Ruibo Liu, Ge Zhang, Xinyu Feng, Soroush Vosoughi |  |
| 542 |  |  [PerKGQA: Question Answering over Personalized Knowledge Graphs](https://doi.org/10.18653/v1/2022.findings-naacl.19) |  | 0 | Previous studies on question answering over knowledge graphs have typically operated over a single knowledge graph (KG). This KG is assumed to be known a priori and is lever- aged similarly for all users’ queries during inference. However, such an assumption is not applicable to real-world settings, such as health- care, where one needs to handle queries of new users over unseen KGs during inference. Furthermore, privacy concerns and high computational costs render it infeasible to query the... | Ritam Dutt, Kasturi Bhattacharjee, Rashmi Gangadharaiah, Dan Roth, Carolyn P. Rosé |  |
| 543 |  |  [Zero-shot Cross-lingual Conversational Semantic Role Labeling](https://doi.org/10.18653/v1/2022.findings-naacl.20) |  | 0 | While conversational semantic role labeling (CSRL) has shown its usefulness on Chinese conversational tasks, it is still under-explored in non-Chinese languages due to the lack of multilingual CSRL annotations for the parser training. To avoid expensive data collection and error-propagation of translation-based methods, we present a simple but effective approach to perform zero-shot cross-lingual CSRL.Our model implicitly learns language-agnostic, conversational structure-aware and semantically... | Han Wu, Haochen Tan, Kun Xu, Shuqi Liu, Lianwei Wu, Linqi Song |  |
| 544 |  |  [A Framework to Generate High-Quality Datapoints for Multiple Novel Intent Detection](https://doi.org/10.18653/v1/2022.findings-naacl.21) |  | 0 | Systems like Voice-command based conversational agents are characterized by a pre-defined set of skills or intents to perform user specified tasks. In the course of time, newer intents may emerge requiring retraining. However, the newer intents may not be explicitly announced and need to be inferred dynamically. Thus, there are two important tasks at hand (a). identifying emerging new intents, (b). annotating data of the new intents so that the underlying classifier can be retrained... | Ankan Mullick, Sukannya Purkayastha, Pawan Goyal, Niloy Ganguly |  |
| 545 |  |  [Design Challenges for a Multi-Perspective Search Engine](https://doi.org/10.18653/v1/2022.findings-naacl.22) |  | 0 | Many users turn to document retrieval systems (e.g. search engines) to seek answers to controversial or open-ended questions. However, classical document retrieval systems fall short at delivering users a set of direct and diverse responses in such cases, which requires identifying responses within web documents in the context of the query, and aggregating the responses based on their different perspectives. The goal of this work is to survey and study the user information needs for building a... | Sihao Chen, Siyi Liu, Xander Uyttendaele, Yi Zhang, William Bruno, Dan Roth |  |
| 546 |  |  [Exploring the Value of Multi-View Learning for Session-Aware Query Representation](https://doi.org/10.18653/v1/2022.findings-naacl.23) |  | 0 | Recent years have witnessed a growing interest towards learning distributed query representations that are able to capture search intent semantics. Most existing approaches learn query embeddings using relevance supervision making them suited only to document ranking tasks. Besides, they generally consider either user’s query reformulations or system’s rankings whereas previous findings show that user’s query behavior and knowledge change depending on the system’s results, intertwine and affect... | Diego Ortiz, José G. Moreno, Gilles Hubert, Karen PinelSauvagnat, Lynda Tamine |  |
| 547 |  |  [Hierarchical Relation-Guided Type-Sentence Alignment for Long-Tail Relation Extraction with Distant Supervision](https://doi.org/10.18653/v1/2022.findings-naacl.24) |  | 0 | Distant supervision uses triple facts in knowledge graphs to label a corpus for relation extraction, leading to wrong labeling and long-tail problems. Some works use the hierarchy of relations for knowledge transfer to long-tail relations. However, a coarse-grained relation often implies only an attribute (e.g., domain or topic) of the distant fact, making it hard to discriminate relations based solely on sentence semantics. One solution is resorting to entity types, but open questions remain... | Yang Li, Guodong Long, Tao Shen, Jing Jiang |  |
| 548 |  |  [PCEE-BERT: Accelerating BERT Inference via Patient and Confident Early Exiting](https://doi.org/10.18653/v1/2022.findings-naacl.25) |  | 0 | BERT and other pretrained language models (PLMs) are ubiquitous in modern NLP. Even though PLMs are the state-of-the-art (SOTA) models for almost every NLP task (CITATION), the significant latency during inference prohibits wider industrial usage. In this work, we propose Patient and Confident Early Exiting BERT (PCEE-BERT), an off-the-shelf sample-dependent early exiting method that can work with different PLMs and can also work along with popular model compression methods. With a multi-exit... | Zhen Zhang, Wei Zhu, Jinfan Zhang, Peng Wang, Rize Jin, TaeSun Chung |  |
| 549 |  |  [Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback](https://doi.org/10.18653/v1/2022.findings-naacl.26) |  | 0 | Large language models (LMs), while powerful, are not immune to mistakes, but can be difficult to retrain. Our goal is for an LM to continue to improve after deployment, without retraining, using feedback from the user. Our approach pairs an LM with (i) a growing memory of cases where the user identified an output error and provided general feedback on how to correct it (ii) a corrector model, trained to translate this general feedback into specific edits to repair the model output. Given a new,... | Niket Tandon, Aman Madaan, Peter Clark, Yiming Yang |  |
| 550 |  |  [One Size Does Not Fit All: The Case for Personalised Word Complexity Models](https://doi.org/10.18653/v1/2022.findings-naacl.27) |  | 0 | Complex Word Identification (CWI) aims to detect words within a text that a reader may find difficult to understand. It has been shown that CWI systems can improve text simplification, readability prediction and vocabulary acquisition modelling. However, the difficulty of a word is a highly idiosyncratic notion that depends on a reader’s first language, proficiency and reading experience. In this paper, we show that personal models are best when predicting word complexity for individual... | Sian Gooding, Manuel Tragut |  |
| 551 |  |  [TEAM: A multitask learning based Taxonomy Expansion approach for Attach and Merge](https://doi.org/10.18653/v1/2022.findings-naacl.28) |  | 0 | Taxonomy expansion is a crucial task. Most of Automatic expansion of taxonomy are of two types, attach and merge. In a taxonomy like WordNet, both merge and attach are integral parts of the expansion operations but majority of study consider them separately. This paper proposes a novel mult-task learning-based deep learning method known as Taxonomy Expansion with Attach and Merge (TEAM) that performs both the merge and attach operations. To the best of our knowledge this is the first study... | Bornali Phukon, Anasua Mitra, Sanasam Ranbir Singh, Priyankoo Sarmah |  |
| 552 |  |  [Extracting Temporal Event Relation with Syntax-guided Graph Transformer](https://doi.org/10.18653/v1/2022.findings-naacl.29) |  | 0 | Extracting temporal relations (e.g., before, after, and simultaneous) among events is crucial to natural language understanding. One of the key challenges of this problem is that when the events of interest are far away in text, the context in-between often becomes complicated, making it challenging to resolve the temporal relationship between them. This paper thus proposes a new Syntax-guided Graph Transformer network (SGT) to mitigate this issue, by (1) explicitly exploiting the connection... | Shuaicheng Zhang, Qiang Ning, Lifu Huang |  |
| 553 |  |  [From Cognitive to Computational Modeling: Text-based Risky Decision-Making Guided by Fuzzy Trace Theory](https://doi.org/10.18653/v1/2022.findings-naacl.30) |  | 0 | Understanding, modelling and predicting human risky decision-making is challenging due to intrinsic individual differences and irrationality. Fuzzy trace theory (FTT) is a powerful paradigm that explains human decision-making by incorporating gists, i.e., fuzzy representations of information which capture only its quintessential meaning. Inspired by Broniatowski and Reyna’s FTT cognitive model, we propose a computational framework which combines the effects of the underlying semantics and... | Jaron Mar, Jiamou Liu |  |
| 554 |  |  [Few-Shot Self-Rationalization with Natural Language Prompts](https://doi.org/10.18653/v1/2022.findings-naacl.31) |  | 0 | Self-rationalization models that predict task labels and generate free-text elaborations for their predictions could enable more intuitive interaction with NLP systems. These models are, however, currently trained with a large amount of human-written free-text explanations for each task which hinders their broader usage. We propose to study a more realistic setting of self-rationalization using few training examples. We present FEB—a standardized collection of four existing English-language... | Ana Marasovic, Iz Beltagy, Doug Downey, Matthew E. Peters |  |
| 555 |  |  [DOCmT5: Document-Level Pretraining of Multilingual Language Models](https://doi.org/10.18653/v1/2022.findings-naacl.32) |  | 0 | In this paper, we introduce DOCmT5, a multilingual sequence-to-sequence language model pretrained with large-scale parallel documents. While previous approaches have focused on leveraging sentence-level parallel data, we try to build a general-purpose pretrained model that can understand and generate long documents. We propose a simple and effective pretraining objective - Document reordering Machine Translation (DrMT), in which the input documents that are shuffled and masked need to be... | ChiaHsuan Lee, Aditya Siddhant, Viresh Ratnakar, Melvin Johnson |  |
| 556 |  |  [Literature-Augmented Clinical Outcome Prediction](https://doi.org/10.18653/v1/2022.findings-naacl.33) |  | 0 | We present BEEP (Biomedical Evidence-Enhanced Predictions), a novel approach for clinical outcome prediction that retrieves patient-specific medical literature and incorporates it into predictive models. Based on each individual patient’s clinical notes, we train language models (LMs) to find relevant papers and fuse them with information from notes to predict outcomes such as in-hospital mortality. We develop methods to retrieve literature based on noisy, information-dense patient notes, and... | Aakanksha Naik, Sravanthi Parasa, Sergey Feldman, Lucy Lu Wang, Tom Hope |  |
| 557 |  |  [Improving Few-Shot Relation Classification by Prototypical Representation Learning with Definition Text](https://doi.org/10.18653/v1/2022.findings-naacl.34) |  | 0 | Few-shot relation classification is difficult because the few instances available may not represent well the relation patterns. Some existing approaches explored extra information such as relation definition, in addition to the instances, to learn a better relation representation. However, the encoding of the extra information has been performed independently from the labeled instances. In this paper, we propose to learn a prototype encoder from relation definition in a way that is useful for... | Zhenzhen Li, Yuyang Zhang, JianYun Nie, Dongsheng Li |  |
| 558 |  |  [Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner](https://doi.org/10.18653/v1/2022.findings-naacl.35) |  | 0 | Large language models have achieved high performance on various question answering (QA) benchmarks, but the explainability of their output remains elusive. Structured explanations, called entailment trees, were recently suggested as a way to explain the reasoning behind a QA system’s answer. In order to better generate such entailment trees, we propose an architecture called Iterative Retrieval-Generation Reasoner (IRGR). Our model is able to explain a given hypothesis by systematically... | Danilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Rui Dong, Xiaokai Wei, Henghui Zhu, Xinchi Chen, Peng Xu, Zhiheng Huang, Andrew O. Arnold, Dan Roth |  |
| 559 |  |  [Multimodal Intent Discovery from Livestream Videos](https://doi.org/10.18653/v1/2022.findings-naacl.36) |  | 0 | Individuals, educational institutions, and businesses are prolific at generating instructional video content such as “how-to” and tutorial guides. While significant progress has been made in basic video understanding tasks, identifying procedural intent within these instructional videos is a challenging and important task that remains unexplored but essential to video summarization, search, and recommendations. This paper introduces the problem of instructional intent identification and... | Adyasha Maharana, Quan Hung Tran, Franck Dernoncourt, Seunghyun Yoon, Trung Bui, Walter Chang, Mohit Bansal |  |
| 560 |  |  [A Question-Answer Driven Approach to Reveal Affirmative Interpretations from Verbal Negations](https://doi.org/10.18653/v1/2022.findings-naacl.37) |  | 0 | This paper explores a question-answer driven approach to reveal affirmative interpretations from verbal negations (i.e., when a negation cue grammatically modifies a verb). We create a new corpus consisting of 4,472 verbal negations and discover that 67.1% of them convey that an event actually occurred. Annotators generate and answer 7,277 questions % converted for 4,000 for the 3,001 negations that convey an affirmative interpretation. We first cast the problem of revealing affirmative... | Md Mosharaf Hossain, Luke Holman, Anusha Kakileti, Tiffany Kao, Nathan Raul Brito, Aaron Abraham Mathews, Eduardo Blanco |  |
| 561 |  |  [Harmless Transfer Learning for Item Embeddings](https://doi.org/10.18653/v1/2022.findings-naacl.38) |  | 0 | Learning embedding layers (for classes, words, items, etc.) is a key component of lots of applications, ranging from natural language processing, recommendation systems to electronic health records, etc. However, the frequency of real-world items follows a long-tail distribution in these applications, causing naive training methods perform poorly on the rare items. A line of previous works address this problem by transferring the knowledge from the frequent items to rare items by introducing an... | Chengyue Gong, Xiaocong Du, Dhruv Choudhary, Bhargav Bhushanam, Qiang Liu, Arun Kejariwal |  |
| 562 |  |  [Fine-grained Image Captioning with CLIP Reward](https://doi.org/10.18653/v1/2022.findings-naacl.39) |  | 0 | Modern image captioning models are usually trained with text similarity objectives. However, since reference captions in public datasets often describe the most salient common objects, models trained with the text similarity objectives tend to ignore specific and detailed aspects of an image that distinguish it from others. Towards more descriptive and distinctive caption generation, we propose to use CLIP, a multimodal encoder trained on huge image-text pairs from the web, to calculate... | Jaemin Cho, Seunghyun Yoon, Ajinkya Kale, Franck Dernoncourt, Trung Bui, Mohit Bansal |  |
| 563 |  |  [Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control](https://doi.org/10.18653/v1/2022.findings-naacl.40) |  | 0 | Abstractive summarization systems leveraging pre-training language models have achieved superior results on benchmark datasets. However, such models have been shown to be more prone to hallucinate facts that are unfaithful to the input context. In this paper, we propose a method to remedy entity-level extrinsic hallucinations with Entity Coverage Control (ECC). We first compute entity coverage precision and prepend the corresponding control code for each training example, which implicitly... | Haopeng Zhang, Semih Yavuz, Wojciech Kryscinski, Kazuma Hashimoto, Yingbo Zhou |  |
| 564 |  |  [Modeling Ideological Salience and Framing in Polarized Online Groups with Graph Neural Networks and Structured Sparsity](https://doi.org/10.18653/v1/2022.findings-naacl.41) |  | 0 | The increasing polarization of online political discourse calls for computational tools that automatically detect and monitor ideological divides in social media. We introduce a minimally supervised method that leverages the network structure of online discussion forums, specifically Reddit, to detect polarized concepts. We model polarization along the dimensions of salience and framing, drawing upon insights from moral psychology. Our architecture combines graph neural networks with structured... | Valentin Hofmann, Xiaowen Dong, Janet B. Pierrehumbert, Hinrich Schütze |  |
| 565 |  |  [On Measuring Social Biases in Prompt-Based Multi-Task Learning](https://doi.org/10.18653/v1/2022.findings-naacl.42) |  | 0 | Large language models trained on a mixture of NLP tasks that are converted into a text-to-text format using prompts, can generalize into novel forms of language and handle novel tasks. A large body of work within prompt engineering attempts to understand the effects of input forms and prompts in achieving superior performance. We consider an alternative measure and inquire whether the way in which an input is encoded affects social biases promoted in outputs. In this paper, we study T0, a... | Afra Feyza Akyürek, Sejin Paik, Muhammed Yusuf Kocyigit, Seda Akbiyik, Serife Leman Runyun, Derry Wijaya |  |
| 566 |  |  [Anti-Overestimation Dialogue Policy Learning for Task-Completion Dialogue System](https://doi.org/10.18653/v1/2022.findings-naacl.43) |  | 0 | A dialogue policy module is an essential part of task-completion dialogue systems. Recently, increasing interest has focused on reinforcement learning (RL)-based dialogue policy. Its favorable performance and wise action decisions rely on an accurate estimation of action values. The overestimation problem is a widely known issue of RL since its estimate of the maximum action value is larger than the ground truth, which results in an unstable learning process and suboptimal policy. This problem... | Chang Tian, Wenpeng Yin, MarieFrancine Moens |  |
| 567 |  |  [Penn-Helsinki Parsed Corpus of Early Modern English: First Parsing Results and Analysis](https://doi.org/10.18653/v1/2022.findings-naacl.44) |  | 0 | The Penn-Helsinki Parsed Corpus of Early Modern English (PPCEME), a 1.7-million-word treebank that is an important resource for research in syntactic change, has several properties that present potential challenges for NLP technologies. We describe these key features of PPCEME that make it challenging for parsing, including a larger and more varied set of function tags than in the Penn Treebank, and present results for this corpus using a modified version of the Berkeley Neural Parser and the... | Seth Kulick, Neville Ryant, Beatrice Santorini |  |
| 568 |  |  [Instilling Type Knowledge in Language Models via Multi-Task QA](https://doi.org/10.18653/v1/2022.findings-naacl.45) |  | 0 | Understanding human language often necessitates understanding entities and their place in a taxonomy of knowledge—their types.Previous methods to learn entity types rely on training classifiers on datasets with coarse, noisy, and incomplete labels. We introduce a method to instill fine-grained type knowledge in language models with text-to-text pre-training on type-centric questions leveraging knowledge base documents and knowledge graphs.We create the WikiWiki dataset: entities and passages... | Shuyang Li, Mukund Sridhar, Chandana Satya Prakash, Jin Cao, Wael Hamza, Julian J. McAuley |  |
| 569 |  |  [StATIK: Structure and Text for Inductive Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.findings-naacl.46) |  | 0 | Knowledge graphs (KGs) often represent knowledge bases that are incomplete. Machine learning models can alleviate this by helping automate graph completion. Recently, there has been growing interest in completing knowledge bases that are dynamic, where previously unseen entities may be added to the KG with many missing links. In this paper, we present StATIK–Structure And Text for Inductive Knowledge Completion. StATIK uses Language Models to extract the semantic information from text... | Elan Markowitz, Keshav Balasubramanian, Mehrnoosh Mirtaheri, Murali Annavaram, Aram Galstyan, Greg Ver Steeg |  |
| 570 |  |  [CoCoA-MT: A Dataset and Benchmark for Contrastive Controlled MT with Application to Formality](https://doi.org/10.18653/v1/2022.findings-naacl.47) |  | 0 | The machine translation (MT) task is typically formulated as that of returning a single translation for an input segment. However, in many cases, multiple different translations are valid and the appropriate translation may depend on the intended target audience, characteristics of the speaker, or even the relationship between speakers. Specific problems arise when dealing with honorifics, particularly translating from English into languages with formality markers. For example, the sentence... | Maria Nadejde, Anna Currey, Benjamin Hsu, Xing Niu, Marcello Federico, Georgiana Dinu |  |
| 571 |  |  [CLEAR: Improving Vision-Language Navigation with Cross-Lingual, Environment-Agnostic Representations](https://doi.org/10.18653/v1/2022.findings-naacl.48) |  | 0 | Vision-and-Language Navigation (VLN) tasks require an agent to navigate through the environment based on language instructions. In this paper, we aim to solve two key challenges in this task: utilizing multilingual instructions for improved instruction-path grounding and navigating through new environments that are unseen during training. To address these challenges, first, our agent learns a shared and visually-aligned cross-lingual language representation for the three languages (English,... | Jialu Li, Hao Tan, Mohit Bansal |  |
| 572 |  |  [Language Models for Code-switch Detection of te reo Māori and English in a Low-resource Setting](https://doi.org/10.18653/v1/2022.findings-naacl.49) |  | 0 | Te reo Māori, New Zealand’s only indigenous language, is code-switched with English. Māori speakers are atleast bilingual, and the use of Māori is increasing in New Zealand English. Unfortunately, due to the minimal availability of resources, including digital data, Māori is under-represented in technological advances. Cloud-based multilingual systems such as Google and Microsoft Azure support Māori language detection. However, we provide experimental evidence to show that the accuracy of such... | Jesin James, Vithya Yogarajan, Isabella Shields, Catherine I. Watson, Peter Keegan, Keoni Mahelona, PeterLucas Jones |  |
| 573 |  |  [Opponent Modeling in Negotiation Dialogues by Related Data Adaptation](https://doi.org/10.18653/v1/2022.findings-naacl.50) |  | 0 | Opponent modeling is the task of inferring another party’s mental state within the context of social interactions. In a multi-issue negotiation, it involves inferring the relative importance that the opponent assigns to each issue under discussion, which is crucial for finding high-value deals. A practical model for this task needs to infer these priorities of the opponent on the fly based on partial dialogues as input, without needing additional annotations for training. In this work, we... | Kushal Chawla, Gale M. Lucas, Jonathan May, Jonathan Gratch |  |
| 574 |  |  [LMTurk: Few-Shot Learners as Crowdsourcing Workers in a Language-Model-as-a-Service Framework](https://doi.org/10.18653/v1/2022.findings-naacl.51) |  | 0 | Vast efforts have been devoted to creating high-performance few-shot learners, i.e., large-scale pretrained language models (PLMs) that perform well with little downstream task training data. Training PLMs has incurred significant cost, but utilizing the few-shot learners is still challenging due to their enormous size. This work focuses on a crucial question: How to make effective use of these few-shot learners? We propose LMTurk, a novel approach that treats few-shotlearners as crowdsourcing... | Mengjie Zhao, Fei Mi, Yasheng Wang, Minglei Li, Xin Jiang, Qun Liu, Hinrich Schütze |  |
| 575 |  |  [Entity Cloze By Date: What LMs Know About Unseen Entities](https://doi.org/10.18653/v1/2022.findings-naacl.52) |  | 0 | Language models (LMs) are typically trained once on a large-scale corpus and used for years without being updated. However, in a dynamic world, new entities constantly arise. We propose a framework to analyze what LMs can infer about new entities that did not exist when the LMs were pretrained. We derive a dataset of entities indexed by their origination date and paired with their English Wikipedia articles, from which we can find sentences about each entity. We evaluate LMs’ perplexity on... | Yasumasa Onoe, Michael J. Q. Zhang, Eunsol Choi, Greg Durrett |  |
| 576 |  |  [Data Augmentation for Low-Resource Dialogue Summarization](https://doi.org/10.18653/v1/2022.findings-naacl.53) |  | 0 | We present DADS, a novel Data Augmentation technique for low-resource Dialogue Summarization. Our method generates synthetic examples by replacing sections of text from both the input dialogue and summary while preserving the augmented summary to correspond to a viable summary for the augmented dialogue. We utilize pretrained language models that produce highly likely dialogue alternatives while still being free to generate diverse alternatives. We applied our data augmentation method to the... | Yongtai Liu, Joshua Maynez, Gonçalo Simões, Shashi Narayan |  |
| 577 |  |  [A Versatile Adaptive Curriculum Learning Framework for Task-oriented Dialogue Policy Learning](https://doi.org/10.18653/v1/2022.findings-naacl.54) |  | 0 | Training a deep reinforcement learning-based dialogue policy with brute-force random sampling is costly. A new training paradigm was proposed to improve learning performance and efficiency by combining curriculum learning. However, attempts in the field of dialogue policy are very limited due to the lack of reliable evaluation of difficulty scores of dialogue tasks and the high sensitivity to the mode of progression through dialogue tasks. In this paper, we present a novel versatile adaptive... | Yang Zhao, Hua Qin, Zhenyu Wang, Changxi Zhu, Shihan Wang |  |
| 578 |  |  [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://doi.org/10.18653/v1/2022.findings-naacl.55) |  | 0 | Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present LongT5, a new model that explores the effects of scaling both the input length and model size at the same time. Specifically, we integrate attention ideas from long-input transformers (ETC), and adopt pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result... | Mandy Guo, Joshua Ainslie, David C. Uthus, Santiago Ontañón, Jianmo Ni, YunHsuan Sung, Yinfei Yang |  |
| 579 |  |  [Challenging America: Modeling language in longer time scales](https://doi.org/10.18653/v1/2022.findings-naacl.56) |  | 0 | The aim of the paper is to apply, for historical texts, the methodology used commonly to solve various NLP tasks defined for contemporary data, i.e. pre-train and fine-tune large Transformer models. This paper introduces an ML challenge, named Challenging America (ChallAm), based on OCR-ed excerpts from historical newspapers collected from the Chronicling America portal. ChallAm provides a dataset of clippings, labeled with metadata on their origin, and paired with their textual contents... | Jakub Pokrywka, Filip Gralinski, Krzysztof Jassem, Karol Kaczmarek, Krzysztof Jurkiewicz, Piotr Wierzchon |  |
| 580 |  |  [LM-CORE: Language Models with Contextually Relevant External Knowledge](https://doi.org/10.18653/v1/2022.findings-naacl.57) |  | 0 | Large transformer-based pre-trained language models have achieved impressive performance on a variety of knowledge-intensive tasks and can capture factual knowledge in their parameters. We argue that storing large amounts of knowledge in the model parameters is sub-optimal given the ever-growing amounts of knowledge and resource requirements. We posit that a more efficient alternative is to provide explicit access to contextually relevant structured knowledge to the model and train it to use... | Jivat Neet Kaur, Sumit Bhatia, Milan Aggarwal, Rachit Bansal, Balaji Krishnamurthy |  |
| 581 |  |  [A Generative Language Model for Few-shot Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2022.findings-naacl.58) |  | 0 | Sentiment analysis is an important task in natural language processing. In recent works, pre-trained language models are often used to achieve state-of-the-art results, especially when training data is scarce. It is common to fine-tune on the downstream task, usually by adding task-specific layers on top of the model. In this paper, we focus on aspect-based sentiment analysis, which involves extracting aspect term, category, and predicting their corresponding polarities. In particular, we are... | Ehsan HosseiniAsl, Wenhao Liu, Caiming Xiong |  |
| 582 |  |  [Permutation Invariant Strategy Using Transformer Encoders for Table Understanding](https://doi.org/10.18653/v1/2022.findings-naacl.59) |  | 0 | Representing text in tables is essential for many business intelligence tasks such as semantic retrieval, data exploration and visualization, and question answering. Existing methods that leverage pretrained Transformer encoders range from a simple construction of pseudo-sentences by concatenating text across rows or columns to complex parameter-intensive models that encode table structure and require additional pretraining. In this work, we introduce a novel encoding strategy for Transformer... | Sarthak Dash, Sugato Bagchi, Nandana Mihindukulasooriya, Alfio Gliozzo |  |
| 583 |  |  [MultiNERD: A Multilingual, Multi-Genre and Fine-Grained Dataset for Named Entity Recognition (and Disambiguation)](https://doi.org/10.18653/v1/2022.findings-naacl.60) |  | 0 | Named Entity Recognition (NER) is the task of identifying named entities in texts and classifying them through specific semantic categories, a process which is crucial for a wide range of NLP applications. Current datasets for NER focus mainly on coarse-grained entity types, tend to consider a single textual genre and to cover a narrow set of languages, thus limiting the general applicability of NER systems. In this work, we design a new methodology for automatically producing NER annotations,... | Simone Tedeschi, Roberto Navigli |  |
| 584 |  |  [Learning to Embed Multi-Modal Contexts for Situated Conversational Agents](https://doi.org/10.18653/v1/2022.findings-naacl.61) |  | 0 | The Situated Interactive Multi-Modal Conversations (SIMMC) 2.0 aims to create virtual shopping assistants that can accept complex multi-modal inputs, i.e. visual appearances of objects and user utterances. It consists of four subtasks, multi-modal disambiguation (MM-Disamb), multi-modal coreference resolution (MM-Coref), multi-modal dialog state tracking (MM-DST), and response retrieval and generation. While many task-oriented dialog systems usually tackle each subtask separately, we propose a... | Haeju Lee, Oh Joon Kwon, Yunseon Choi, Minho Park, Ran Han, Yoonhyung Kim, Jinhyeon Kim, Youngjune Lee, Haebin Shin, Kangwook Lee, KeeEung Kim |  |
| 585 |  |  [Measuring and Improving Compositional Generalization in Text-to-SQL via Component Alignment](https://doi.org/10.18653/v1/2022.findings-naacl.62) |  | 0 | In text-to-SQL tasks — as in much of NLP — compositional generalization is a major challenge: neural networks struggle with compositional generalization where training and test distributions differ. However, most recent attempts to improve this are based on word-level synthetic data or specific dataset splits to generate compositional biases. In this work, we propose a clause-level compositional example generation method. We first split the sentences in the Spider text-to-SQL dataset into... | Yujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver |  |
| 586 |  |  [Empathetic Persuasion: Reinforcing Empathy and Persuasiveness in Dialogue Systems](https://doi.org/10.18653/v1/2022.findings-naacl.63) |  | 0 | Persuasion is an intricate process involving empathetic connection between two individuals. Plain persuasive responses may make a conversation non-engaging. Even the most well-intended and reasoned persuasive conversations can fall through in the absence of empathetic connection between the speaker and listener. In this paper, we propose a novel task of incorporating empathy when generating persuasive responses. We develop an empathetic persuasive dialogue system by fine-tuning a maximum... | Azlaan Mustafa Samad, Kshitij Mishra, Mauajama Firdaus, Asif Ekbal |  |
| 587 |  |  [Attention Fusion: a light yet efficient late fusion mechanism for task adaptation in NLU](https://doi.org/10.18653/v1/2022.findings-naacl.64) |  | 0 | Fine-tuning a pre-trained language model using annotated data has become the de-facto standard for adapting general-purpose pre-trained models like BERT to downstream tasks. However, given the trend of larger pre-trained models, fine-tuning these models for each downstream task is parameter-inefficient and computationally-expensive deeming this approach sub-optimal for adoption by NLU systems. In recent years, various approaches have been proposed for parameter efficient task adaptation such as... | Jin Cao, Chandana Satya Prakash, Wael Hamza |  |
| 588 |  |  [The Limits of Word Level Differential Privacy](https://doi.org/10.18653/v1/2022.findings-naacl.65) |  | 0 | As the issues of privacy and trust are receiving increasing attention within the research community, various attempts have been made to anonymize textual data. A significant subset of these approaches incorporate differentially private mechanims to perturb word embeddings, thus replacing individual words in a sentence. While these methods represent very important contributions, have various advantages over other techniques and do show anonymization capabilities,they have several shortcomings.... | Justus Mattern, Benjamin Weggenmann, Florian Kerschbaum |  |
| 589 |  |  [Efficient Learning of Multiple NLP Tasks via Collective Weight Factorization on BERT](https://doi.org/10.18653/v1/2022.findings-naacl.66) |  | 0 | The Transformer architecture continues to show remarkable performance gains in many Natural Language Processing tasks. However, obtaining such state-of-the-art performance in different tasks requires fine-tuning the same model separately for each task. Clearly, such an approach is demanding in terms of both memory requirements and computing power. In this paper, aiming to improve training efficiency across multiple tasks, we propose to collectively factorize the weighs of the multi-head... | Christos Papadopoulos, Yannis Panagakis, Manolis Koubarakis, Mihalis Nicolaou |  |
| 590 |  |  [Learning Rich Representation of Keyphrases from Text](https://doi.org/10.18653/v1/2022.findings-naacl.67) |  | 0 | In this work, we explore how to train task-specific language models aimed towards learning rich representation of keyphrases from text documents. We experiment with different masking strategies for pre-training transformer language models (LMs) in discriminative as well as generative settings. In the discriminative setting, we introduce a new pre-training objective - Keyphrase Boundary Infilling with Replacement (KBIR), showing large gains in performance (upto 8.16 points in F1) over SOTA, when... | Mayank Kulkarni, Debanjan Mahata, Ravneet Arora, Rajarshi Bhowmik |  |
| 591 |  |  [Improving Contextual Representation with Gloss Regularized Pre-training](https://doi.org/10.18653/v1/2022.findings-naacl.68) |  | 0 | Though achieving impressive results on many NLP tasks, the BERT-like masked language models (MLM) encounter the discrepancy between pre-training and inference. In light of this gap, we investigate the contextual representation of pre-training and inference from the perspective of word probability distribution. We discover that BERT risks neglecting the contextual word similarity in pre-training. To tackle this issue, we propose an auxiliary gloss regularizer module to BERT pre-training... | Yu Lin, Zhecheng An, Peihao Wu, Zejun Ma |  |
| 592 |  |  [An Information-Theoretic Approach and Dataset for Probing Gender Stereotypes in Multilingual Masked Language Models](https://doi.org/10.18653/v1/2022.findings-naacl.69) |  | 0 | Bias research in NLP is a rapidly growing and developing field. Similar to CrowS-Pairs (Nangia et al., 2020), we assess gender bias in masked-language models (MLMs) by studying pairs of sentences with gender swapped person references. Most bias research focuses on and often is specific to English.Using a novel methodology for creating sentence pairs that is applicable across languages, we create, based on CrowS-Pairs, a multilingual dataset for English, Finnish, German, Indonesian and... | Victor Steinborn, Philipp Dufter, Haris Jabbar, Hinrich Schütze |  |
| 593 |  |  [Self-Training with Differentiable Teacher](https://doi.org/10.18653/v1/2022.findings-naacl.70) |  | 0 | Self-training achieves enormous success in various semi-supervised and weakly-supervised learning tasks. The method can be interpreted as a teacher-student framework, where the teacher generates pseudo-labels, and the student makes predictions. The two models are updated alternatingly. However, such a straightforward alternating update rule leads to training instability. This is because a small change in the teacher may result in a significant change in the student. To address this issue, we... | Simiao Zuo, Yue Yu, Chen Liang, Haoming Jiang, Siawpeng Er, Chao Zhang, Tuo Zhao, Hongyuan Zha |  |
| 594 |  |  [SHARP: Search-Based Adversarial Attack for Structured Prediction](https://doi.org/10.18653/v1/2022.findings-naacl.71) |  | 0 | Adversarial attack of structured prediction models faces various challenges such as the difficulty of perturbing discrete words, the sentence quality issue, and the sensitivity of outputs to small perturbations. In this work, we introduce SHARP, a new attack method that formulates the black-box adversarial attack as a search-based optimization problem with a specially designed objective function considering sentence fluency, meaning preservation and attacking effectiveness. Additionally, three... | Liwen Zhang, Zixia Jia, Wenjuan Han, Zilong Zheng, Kewei Tu |  |
| 595 |  |  [MM-Claims: A Dataset for Multimodal Claim Detection in Social Media](https://doi.org/10.18653/v1/2022.findings-naacl.72) |  | 0 | In recent years, the problem of misinformation on the web has become widespread across languages, countries, and various social media platforms. Although there has been much work on automated fake news detection, the role of images and their variety are not well explored. In this paper, we investigate the roles of image and text at an earlier stage of the fake news detection pipeline, called claim detection. For this purpose, we introduce a novel dataset, MM-Claims, which consists of tweets and... | Gullal Singh Cheema, Sherzod Hakimov, Abdul Sittar, Eric MüllerBudack, Christian Otto, Ralph Ewerth |  |
| 596 |  |  [QLEVR: A Diagnostic Dataset for Quantificational Language and Elementary Visual Reasoning](https://doi.org/10.18653/v1/2022.findings-naacl.73) |  | 0 | Synthetic datasets have successfully been used to probe visual question-answering datasets for their reasoning abilities. CLEVR (John- son et al., 2017), for example, tests a range of visual reasoning abilities. The questions in CLEVR focus on comparisons of shapes, colors, and sizes, numerical reasoning, and existence claims. This paper introduces a minimally biased, diagnostic visual question-answering dataset, QLEVR, that goes beyond existential and numerical quantification and focus on more... | Zechen Li, Anders Søgaard |  |
| 597 |  |  [MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving](https://doi.org/10.18653/v1/2022.findings-naacl.74) |  | 0 | Math word problem (MWP) solving faces a dilemma in number representation learning. In order to avoid the number representation issue and reduce the search space of feasible solutions, existing works striving for MWP solving usually replace real numbers with symbolic placeholders to focus on logic reasoning. However, different from common symbolic reasoning tasks like program synthesis and knowledge graph reasoning, MWP solving has extra requirements in numerical reasoning. In other words,... | Zhenwen Liang, Jipeng Zhang, Lei Wang, Wei Qin, Yunshi Lan, Jie Shao, Xiangliang Zhang |  |
| 598 |  |  [Restoring Hebrew Diacritics Without a Dictionary](https://doi.org/10.18653/v1/2022.findings-naacl.75) |  | 0 | We demonstrate that it is feasible to accurately diacritize Hebrew script without any human-curated resources other than plain diacritized text. We present Nakdimon, a two-layer character-level LSTM, that performs on par with much more complicated curation-dependent systems, across a diverse array of modern Hebrew sources. The model is accompanied by a training set and a test set, collected from diverse sources. | Elazar Gershuni, Yuval Pinter |  |
| 599 |  |  [Masked Summarization to Generate Factually Inconsistent Summaries for Improved Factual Consistency Checking](https://doi.org/10.18653/v1/2022.findings-naacl.76) |  | 0 | Despite the recent advances in abstractive summarization systems, it is still difficult to determine whether a generated summary is factual consistent with the source text. To this end, the latest approach is to train a factual consistency classifier on factually consistent and inconsistent summaries. Luckily, the former is readily available as reference summaries in existing summarization datasets. However, generating the latter remains a challenge, as they need to be factually inconsistent,... | Hwanhee Lee, Kang Min Yoo, Joonsuk Park, Hwaran Lee, Kyomin Jung |  |
| 600 |  |  [Probing the Role of Positional Information in Vision-Language Models](https://doi.org/10.18653/v1/2022.findings-naacl.77) |  | 0 | In most Vision-Language models (VL), the understanding of the image structure is enabled by injecting the position information (PI) about objects in the image. In our case study of LXMERT, a state-of-the-art VL model, we probe the use of the PI in the representation and study its effect on Visual Question Answering. We show that the model is not capable of leveraging the PI for the image-text matching task on a challenge set where only position differs. Yet, our experiments with probing confirm... | Philipp Jonas Rösch, Jindrich Libovický |  |
| 601 |  |  ["Diversity and Uncertainty in Moderation" are the Key to Data Selection for Multilingual Few-shot Transfer](https://doi.org/10.18653/v1/2022.findings-naacl.78) |  | 0 | Few-shot transfer often shows substantial gain over zero-shot transfer (CITATION), which is a practically useful trade-off between fully supervised and unsupervised learning approaches for multilingual pretained model-based systems. This paper explores various strategies for selecting data for annotation that can result in a better few-shot transfer. The proposed approaches rely on multiple measures such as data entropy using n-gram language model, predictive entropy, and gradient embedding. We... | Shanu Kumar, Sandipan Dandapat, Monojit Choudhury |  |
| 602 |  |  [A Self-supervised Joint Training Framework for Document Reranking](https://doi.org/10.18653/v1/2022.findings-naacl.79) |  | 0 | Pretrained language models such as BERT have been successfully applied to a wide range of natural language processing tasks and also achieved impressive performance in document reranking tasks. Recent works indicate that further pretraining the language models on the task-specific datasets before fine-tuning helps improve reranking performance. However, the pre-training tasks like masked language model and next sentence prediction were based on the context of documents instead of encouraging... | Xiaozhi Zhu, Tianyong Hao, Sijie Cheng, Fu Lee Wang, Hai Liu |  |
| 603 |  |  [CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training](https://doi.org/10.18653/v1/2022.findings-naacl.80) |  | 0 | Recent years have witnessed increasing interest in code representation learning, which aims to represent the semantics of source code into distributed vectors. Currently, various works have been proposed to represent the complex semantics of source code from different views, including plain text, Abstract Syntax Tree (AST), and several kinds of code graphs (e.g., Control/Data Flow Graph). However, most of them only consider a single view of source code independently, ignoring the... | Xin Wang, Yasheng Wang, Yao Wan, Jiawei Wang, Pingyi Zhou, Li Li, Hao Wu, Jin Liu |  |
| 604 |  |  [RGL: A Simple yet Effective Relation Graph Augmented Prompt-based Tuning Approach for Few-Shot Learning](https://doi.org/10.18653/v1/2022.findings-naacl.81) |  | 0 | Pre-trained language models (PLMs) can provide a good starting point for downstream applications. However, it is difficult to generalize PLMs to new tasks given a few labeled samples. In this work, we show that Relation Graph augmented Learning (RGL) can improve the performance of few-shot natural language understanding tasks. During learning, RGL constructs a relation graph based on the label consistency between samples in the same batch, and learns to solve the resultant node classification... | Yaqing Wang, Xin Tian, Haoyi Xiong, Yueyang Li, Zeyu Chen, Sheng Guo, Dejing Dou |  |
| 605 |  |  [Seeing the wood for the trees: a contrastive regularization method for the low-resource Knowledge Base Question Answering](https://doi.org/10.18653/v1/2022.findings-naacl.82) |  | 0 | Given a context knowledge base (KB) and a corresponding question, the Knowledge Base Question Answering task aims to retrieve correct answer entities from this KB. Despite sophisticated retrieval algorithms, the impact of the low-resource (incomplete) KB is not fully exploited, where contributing components (. key entities and/or relations) may be absent for question answering. To effectively address this problem, we propose a contrastive regularization based method, which is motivated by the... | Junping Liu, Shijie Mei, Xinrong Hu, Xun Yao, Jie (Jack) Yang, Yi Guo |  |
| 606 |  |  [Phrase-level Textual Adversarial Attack with Label Preservation](https://doi.org/10.18653/v1/2022.findings-naacl.83) |  | 0 | Generating high-quality textual adversarial examples is critical for investigating the pitfalls of natural language processing (NLP) models and further promoting their robustness. Existing attacks are usually realized through word-level or sentence-level perturbations, which either limit the perturbation space or sacrifice fluency and textual quality, both affecting the attack effectiveness. In this paper, we propose Phrase-Level Textual Adversarial ATtack (PLAT) that generates adversarial... | Yibin Lei, Yu Cao, Dianqi Li, Tianyi Zhou, Meng Fang, Mykola Pechenizkiy |  |
| 607 |  |  [Prompt Augmented Generative Replay via Supervised Contrastive Learning for Lifelong Intent Detection](https://doi.org/10.18653/v1/2022.findings-naacl.84) |  | 0 | Identifying all possible user intents for a dialog system at design time is challenging even for skilled domain experts. For practical applications, novel intents may have to be inferred incrementally on the fly. This typically entails repeated retraining of the intent detector on both the existing and novel intents which can be expensive and would require storage of all past data corresponding to prior intents. In this paper, the objective is to continually train an intent detector on new... | Vaibhav Varshney, Mayur Patidar, Rajat Kumar, Lovekesh Vig, Gautam Shroff |  |
| 608 |  |  [OTExtSum: Extractive Text Summarisation with Optimal Transport](https://doi.org/10.18653/v1/2022.findings-naacl.85) |  | 0 | Extractive text summarisation aims to select salient sentences from a document to form a short yet informative summary. While learning-based methods have achieved promising results, they have several limitations, such as dependence on expensive training and lack of interpretability. Therefore, in this paper, we propose a novel non-learning-based method by for the first time formulating text summarisation as an Optimal Transport (OT) problem, namely Optimal Transport Extractive Summariser... | Peggy Tang, Kun Hu, Rui Yan, Lei Zhang, Junbin Gao, Zhiyong Wang |  |
| 609 |  |  [Speeding Up Entmax](https://doi.org/10.18653/v1/2022.findings-naacl.86) |  | 0 | Softmax is the de facto standard for normalizing logits in modern neural networks for language processing. However, by producing a dense probability distribution each token in the vocabulary has a nonzero chance of being selected at each generation step, leading to a variety of reported problems in text generation. 𝛼-entmax of Peters et al. (2019) solves this problem, but is unfortunately slower than softmax. In this paper, we propose an alternative to 𝛼-entmax, which keeps its virtuous... | Maxat Tezekbayev, Vassilina Nikoulina, Matthias Gallé, Zhenisbek Assylbekov |  |
| 610 |  |  [Improving Code-Switching Dependency Parsing with Semi-Supervised Auxiliary Tasks](https://doi.org/10.18653/v1/2022.findings-naacl.87) |  | 0 | Code-switching dependency parsing stands as a challenging task due to both the scarcity of necessary resources and the structural difficulties embedded in code-switched languages. In this study, we introduce novel sequence labeling models to be used as auxiliary tasks for dependency parsing of code-switched text in a semi-supervised scheme. We show that using auxiliary tasks enhances the performance of an LSTM-based dependency parsing model and leads to better results compared to an XLM-R-based... | Saziye Betül Özates, Arzucan Özgür, Tunga Gungor, Özlem Çetinoglu |  |
| 611 |  |  [Dangling-Aware Entity Alignment with Mixed High-Order Proximities](https://doi.org/10.18653/v1/2022.findings-naacl.88) |  | 0 | We study dangling-aware entity alignment in knowledge graphs (KGs), which is an underexplored but important problem. As different KGs are naturally constructed by different sets of entities, a KG commonly contains some dangling entities that cannot find counterparts in other KGs. Therefore, dangling-aware entity alignment is more realistic than the conventional entity alignment where prior studies simply ignore dangling entities. We propose a framework using mixed high-order proximities on... | Juncheng Liu, Zequn Sun, Bryan Hooi, Yiwei Wang, Dayiheng Liu, Baosong Yang, Xiaokui Xiao, Muhao Chen |  |
| 612 |  |  [DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks](https://doi.org/10.18653/v1/2022.findings-naacl.89) |  | 0 | Since 2017, the Transformer-based models play critical roles in various downstream Natural Language Processing tasks. However, a common limitation of the attention mechanism utilized in Transformer Encoder is that it cannot automatically capture the information of word order, so explicit position embeddings are generally required to be fed into the target model. In contrast, Transformer Decoder with the causal attention masks is naturally sensitive to the word order. In this work, we focus on... | Ziyang Luo, Yadong Xi, Jing Ma, Zhiwei Yang, Xiaoxi Mao, Changjie Fan, Rongsheng Zhang |  |
| 613 |  |  [Towards Computationally Feasible Deep Active Learning](https://doi.org/10.18653/v1/2022.findings-naacl.90) |  | 0 | Active learning (AL) is a prominent technique for reducing the annotation effort required for training machine learning models. Deep learning offers a solution for several essential obstacles to deploying AL in practice but introduces many others. One of such problems is the excessive computational resources required to train an acquisition model and estimate its uncertainty on instances in the unlabeled pool. We propose two techniques that tackle this issue for text classification and tagging... | Akim Tsvigun, Artem Shelmanov, Gleb Kuzmin, Leonid Sanochkin, Daniil Larionov, Gleb Gusev, Manvel Avetisian, Leonid Zhukov |  |
| 614 |  |  [End-to-end Spoken Conversational Question Answering: Task, Dataset and Model](https://doi.org/10.18653/v1/2022.findings-naacl.91) |  | 0 | In spoken question answering, the systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling the systems to model complex dialogues flow given the speech documents. In this task, our main objective is to build the system to deal with conversational... | Chenyu You, Nuo Chen, Fenglin Liu, Shen Ge, Xian Wu, Yuexian Zou |  |
| 615 |  |  [Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training](https://doi.org/10.18653/v1/2022.findings-naacl.92) |  | 0 | Keyphrase generation is the task of automatically predicting keyphrases given a piece of long text. Despite its recent flourishing, keyphrase generation on non-English languages haven’t been vastly investigated. In this paper, we call attention to a new setting named multilingual keyphrase generation and we contribute two new datasets, EcommerceMKP and AcademicMKP, covering six languages. Technically, we propose a retrieval-augmented method for multilingual keyphrase generation to mitigate the... | Yifan Gao, Qingyu Yin, Zheng Li, Rui Meng, Tong Zhao, Bing Yin, Irwin King, Michael R. Lyu |  |
| 616 |  |  [FAtNet: Cost-Effective Approach Towards Mitigating the Linguistic Bias in Speaker Verification Systems](https://doi.org/10.18653/v1/2022.findings-naacl.93) |  | 0 | Linguistic bias in Deep Neural Network (DNN) based Natural Language Processing (NLP) systems is a critical problem that needs attention. The problem further intensifies in the case of security systems, such as speaker verification, where fairness is essential. Speaker verification systems are intelligent systems that determine if two speech recordings belong to the same speaker. Such human-oriented security systems should be usable by diverse people speaking varied languages. Thus, a speaker... | Divya V. Sharma, Arun Balaji Buduru |  |
| 617 |  |  [A Survey on Stance Detection for Mis- and Disinformation Identification](https://doi.org/10.18653/v1/2022.findings-naacl.94) |  | 0 | Understanding attitudes expressed in texts, also known as stance detection, plays an important role in systems for detecting false information online, be it misinformation (unintentionally false) or disinformation (intentionally false information). Stance detection has been framed in different ways, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right. While there have been prior efforts to contrast... | Momchil Hardalov, Arnav Arora, Preslav Nakov, Isabelle Augenstein |  |
| 618 |  |  [Syntax Controlled Knowledge Graph-to-Text Generation with Order and Semantic Consistency](https://doi.org/10.18653/v1/2022.findings-naacl.95) |  | 0 | The knowledge graph (KG) stores a large amount of structural knowledge, while it is not easy for direct human understanding. Knowledge graph-to-text (KG-to-text) generation aims to generate easy-to-understand sentences from the KG, and at the same time, maintains semantic consistency between generated sentences and the KG. Existing KG-to-text generation methods phrase this task as a sequence-to-sequence generation task with linearized KG as input and consider the consistency issue of the... | Jin Liu, Chongfeng Fan, Fengyu Zhou, Huijuan Xu |  |
| 619 |  |  [To Answer or Not To Answer? Improving Machine Reading Comprehension Model with Span-based Contrastive Learning](https://doi.org/10.18653/v1/2022.findings-naacl.96) |  | 0 | Machine Reading Comprehension with Unanswerable Questions is a difficult NLP task, challenged by the questions which can not be answered from passages. It is observed that subtle literal changes often make an answerable question unanswerable, however, most MRC models fail to recognize such changes. To address this problem, in this paper, we propose a span-based method of Contrastive Learning (spanCL) which explicitly contrast answerable questions with their answerable and unanswerable... | Yunjie Ji, Liangyu Chen, Chenxiao Dou, Baochang Ma, Xiangang Li |  |
| 620 |  |  [Target-Guided Dialogue Response Generation Using Commonsense and Data Augmentation](https://doi.org/10.18653/v1/2022.findings-naacl.97) |  | 0 | Target-guided response generation enables dialogue systems to smoothly transition a conversation from a dialogue context toward a target sentence. Such control is useful for designing dialogue systems that direct a conversation toward specific goals, such as creating non-obtrusive recommendations or introducing new topics in the conversation. In this paper, we introduce a new technique for target-guided response generation, which first finds a bridging path of commonsense knowledge concepts... | Prakhar Gupta, Harsh Jhamtani, Jeffrey P. Bigham |  |
| 621 |  |  [BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource Language Understanding Evaluation in Bangla](https://doi.org/10.18653/v1/2022.findings-naacl.98) |  | 0 | In this work, we introduce BanglaBERT, a BERT-based Natural Language Understanding (NLU) model pretrained in Bangla, a widely spoken yet low-resource language in the NLP literature. To pretrain BanglaBERT, we collect 27.5 GB of Bangla pretraining data (dubbed ‘Bangla2B+’) by crawling 110 popular Bangla sites. We introduce two downstream task datasets on natural language inference and question answering and benchmark on four diverse NLU tasks covering text classification, sequence labeling, and... | Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin Mubasshir, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar |  |
| 622 |  |  [ALLSH: Active Learning Guided by Local Sensitivity and Hardness](https://doi.org/10.18653/v1/2022.findings-naacl.99) |  | 0 | Active learning, which effectively collects informative unlabeled data for annotation, reduces the demand for labeled data. In this work, we propose to retrieve unlabeled samples with a local sensitivity and hardness-aware acquisition function. The proposed method generates data copies through local perturbations and selects data points whose predictive likelihoods diverge the most from their copies. We further empower our acquisition function by injecting the select-worst case perturbation.... | Shujian Zhang, Chengyue Gong, Xingchao Liu, Pengcheng He, Weizhu Chen, Mingyuan Zhou |  |
| 623 |  |  [Low-resource Entity Set Expansion: A Comprehensive Study on User-generated Text](https://doi.org/10.18653/v1/2022.findings-naacl.100) |  | 0 | Entity set expansion (ESE) aims at obtaining a more complete set of entities given a textual corpus and a seed set of entities of a concept. Although it is a critical task in many NLP applications, existing benchmarks are limited to well-formed text (e.g., Wikipedia) and well-defined concepts (e.g., countries and diseases). Furthermore, only a small number of predictions are evaluated compared to the actual size of an entity set. A rigorous assessment of ESE methods warrants more comprehensive... | Yutong Shao, Nikita Bhutani, Sajjadur Rahman, Estevam Hruschka |  |
| 624 |  |  [POLITICS: Pretraining with Same-story Article Comparison for Ideology Prediction and Stance Detection](https://doi.org/10.18653/v1/2022.findings-naacl.101) |  | 0 | Ideology is at the core of political science research. Yet, there still does not exist general-purpose tools to characterize and predict ideology across different genres of text. To this end, we study Pretrained Language Models using novel ideology-driven pretraining objectives that rely on the comparison of articles on the same story written by media of different ideologies. We further collect a large-scale dataset, consisting of more than 3.6M political news articles, for pretraining. Our... | Yujian Liu, Xinliang Frederick Zhang, David Wegsman, Nicholas Beauchamp, Lu Wang |  |
| 625 |  |  [Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention](https://doi.org/10.18653/v1/2022.findings-naacl.102) |  | 0 | The massive amount of trainable parameters in the pre-trained language models (PLMs) makes them hard to be deployed to multiple downstream tasks. To address this issue, parameter-efficient transfer learning methods have been proposed to tune only a few parameters during fine-tuning while freezing the rest. This paper looks at existing methods along this line through the kernel lens. Motivated by the connection between self-attention in transformer-based PLMs and kernel learning, we propose... | Yifan Chen, Devamanyu Hazarika, Mahdi Namazifar, Yang Liu, Di Jin, Dilek HakkaniTur |  |
| 626 |  |  [RAIL-KD: RAndom Intermediate Layer Mapping for Knowledge Distillation](https://doi.org/10.18653/v1/2022.findings-naacl.103) |  | 0 | Intermediate layer knowledge distillation (KD) can improve the standard KD technique (which only targets the output of teacher and student models) especially over large pre-trained language models. However, intermediate layer distillation suffers from excessive computational burdens and engineering efforts required for setting up a proper layer mapping. To address these problems, we propose a RAndom Intermediate Layer Knowledge Distillation (RAIL-KD) approach in which, intermediate layers from... | Md. Akmal Haidar, Nithin Anchuri, Mehdi Rezagholizadeh, Abbas Ghaddar, Philippe Langlais, Pascal Poupart |  |
| 627 |  |  [Unbiased Math Word Problems Benchmark for Mitigating Solving Bias](https://doi.org/10.18653/v1/2022.findings-naacl.104) |  | 0 | In this paper, we revisit the solving bias when evaluating models on current Math Word Problem (MWP) benchmarks. However, current solvers exist solving bias which consists of data bias and learning bias due to biased dataset and improper training strategy. Our experiments verify MWP solvers are easy to be biased by the biased training datasets which do not cover diverse questions for each problem narrative of all MWPs, thus a solver can only learn shallow heuristics rather than deep semantics... | Zhicheng Yang, Jinghui Qin, Jiaqi Chen, Xiaodan Liang |  |
| 628 |  |  [Learn To Remember: Transformer with Recurrent Memory for Document-Level Machine Translation](https://doi.org/10.18653/v1/2022.findings-naacl.105) |  | 0 | The Transformer architecture has led to significant gains in machine translation. However, most studies focus on only sentence-level translation without considering the context dependency within documents, leading to the inadequacy of document-level coherence. Some recent research tried to mitigate this issue by introducing an additional context encoder or translating with multiple sentences or even the entire document. Such methods may lose the information on the target side or have an... | Yukun Feng, Feng Li, Ziang Song, Boyuan Zheng, Philipp Koehn |  |
| 629 |  |  [Improving Few-Shot Image Classification Using Machine- and User-Generated Natural Language Descriptions](https://doi.org/10.18653/v1/2022.findings-naacl.106) |  | 0 | Humans can obtain the knowledge of novel visual concepts from language descriptions, and we thus use the few-shot image classification task to investigate whether a machine learning model can have this capability. Our proposed model, LIDE (Learning from Image and DEscription), has a text decoder to generate the descriptions and a text encoder to obtain the text representations of machine- or user-generated descriptions. We confirmed that LIDE with machine-generated descriptions outperformed... | Kosuke Nishida, Kyosuke Nishida, Shuichi Nishioka |  |
| 630 |  |  [All Information is Valuable: Question Matching over Full Information Transmission Network](https://doi.org/10.18653/v1/2022.findings-naacl.107) |  | 0 | Question matching is the task of identifying whether two questions have the same intent. For better reasoning the relationship between questions, existing studies adopt multiple interaction modules and perform multi-round reasoning via deep neural networks. In this process, there are two kinds of critical information that are commonly employed: the representation information of original questions and the interactive information between pairs of questions. However, previous studies tend to... | Le Qi, Yu Zhang, Qingyu Yin, Guidong Zheng, Wen Junjie, Jinlong Li, Ting Liu |  |
| 631 |  |  [Pathway2Text: Dataset and Method for Biomedical Pathway Description Generation](https://doi.org/10.18653/v1/2022.findings-naacl.108) |  | 0 | Biomedical pathways have been extensively used to characterize the mechanism of complex diseases. One essential step in biomedical pathway analysis is to curate the description of a pathway based on its graph structure and node features. Neural text generation could be a plausible technique to circumvent the tedious manual curation. In this paper, we propose a new dataset Pathway2Text, which contains 2,367 pairs of biomedical pathways and textual descriptions. All pathway graphs are... | Junwei Yang, Zequn Liu, Ming Zhang, Sheng Wang |  |
| 632 |  |  [Exploring Neural Models for Query-Focused Summarization](https://doi.org/10.18653/v1/2022.findings-naacl.109) |  | 0 | Query-focused summarization (QFS) aims to produce summaries that answer particular questions of interest, enabling greater user control and personalization. While recently released datasets, such as QMSum or AQuaMuSe, facilitate research efforts in QFS, the field lacks a comprehensive study of the broad space of applicable modeling methods. In this paper we conduct a systematic exploration of neural approaches to QFS, considering two general classes of methods: two-stage extractive-abstractive... | Jesse Vig, Alexander R. Fabbri, Wojciech Kryscinski, ChienSheng Wu, Wenhao Liu |  |
| 633 |  |  [BitextEdit: Automatic Bitext Editing for Improved Low-Resource Machine Translation](https://doi.org/10.18653/v1/2022.findings-naacl.110) |  | 0 | Mined bitexts can contain imperfect translations that yield unreliable training signals for Neural Machine Translation (NMT). While filtering such pairs out is known to improve final model quality, we argue that it is suboptimal in low-resource conditions where even mined data can be limited. In our work, we propose instead, to refine the mined bitexts via automatic editing: given a sentence in a language xf, and a possibly imperfect translation of it xe, our model generates a revised version... | Eleftheria Briakou, Sida I. Wang, Luke Zettlemoyer, Marjan Ghazvininejad |  |
| 634 |  |  [MixQG: Neural Question Generation with Mixed Answer Types](https://doi.org/10.18653/v1/2022.findings-naacl.111) |  | 0 | Asking good questions is an essential ability for both human and machine intelligence. However, existing neural question generation approaches mainly focus on short factoid type of answers. In this paper, we introduce a neural question generator, MixQG, to bridge this gap. We combine nine question answering datasets with diverse answer types, including yes/no, multiple-choice, extractive, and abstractive answers, to train a single generative model. We show with empirical results that our model... | Lidiya Murakhovs'ka, ChienSheng Wu, Philippe Laban, Tong Niu, Wenhao Liu, Caiming Xiong |  |
| 635 |  |  [Temporal Attention for Language Models](https://doi.org/10.18653/v1/2022.findings-naacl.112) |  | 0 | Pretrained language models based on the transformer architecture have shown great success in NLP.Textual training data often comes from the web and is thus tagged with time-specific information, but most language models ignore this information. They are trained on the textual data alone, limiting their ability to generalize temporally. In this work, we extend the key component of the transformer architecture, i.e., the self-attention mechanism, and propose temporal attention - a time-aware... | Guy D. Rosin, Kira Radinsky |  |
| 636 |  |  [Efficient Few-Shot Fine-Tuning for Opinion Summarization](https://doi.org/10.18653/v1/2022.findings-naacl.113) |  | 0 | Abstractive summarization models are typically pre-trained on large amounts of generic texts, then fine-tuned on tens or hundreds of thousands of annotated samples. However, in opinion summarization, large annotated datasets of reviews paired with reference summaries are not available and would be expensive to create. This calls for fine-tuning methods robust to overfitting on small datasets. In addition, generically pre-trained models are often not accustomed to the specifics of customer... | Arthur Brazinskas, Ramesh Nallapati, Mohit Bansal, Markus Dreyer |  |
| 637 |  |  [Domain-matched Pre-training Tasks for Dense Retrieval](https://doi.org/10.18653/v1/2022.findings-naacl.114) |  | 0 | Pre-training on larger datasets with ever increasing model size isnow a proven recipe for increased performance across almost all NLP tasks.A notable exception is information retrieval, where additional pre-traininghas so far failed to produce convincing results. We show that, with theright pre-training setup, this barrier can be overcome. We demonstrate thisby pre-training large bi-encoder models on 1) a recently released set of 65 millionsynthetically generated questions, and 2) 200 million... | Barlas Oguz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis, Vladimir Karpukhin, Aleksandra Piktus, Xilun Chen, Sebastian Riedel, Scott Yih, Sonal Gupta, Yashar Mehdad |  |
| 638 |  |  [UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering](https://doi.org/10.18653/v1/2022.findings-naacl.115) |  | 0 | We study open-domain question answering with structured, unstructured and semi-structured knowledge sources, including text, tables, lists and knowledge bases. Departing from prior work, we propose a unifying approach that homogenizes all sources by reducing them to text and applies the retriever-reader model which has so far been limited to text sources only. Our approach greatly improves the results on knowledge-base QA tasks by 11 points, compared to latest graph-based methods. More... | Barlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan Peshterliev, Dmytro Okhonko, Michael Sejr Schlichtkrull, Sonal Gupta, Yashar Mehdad, Scott Yih |  |
| 639 |  |  [White-box Testing of NLP models with Mask Neuron Coverage](https://doi.org/10.18653/v1/2022.findings-naacl.116) |  | 0 | Recent literature has seen growing interest in using black-box strategies like for testing the behavior of NLP models. Research on white-box testing has developed a number of methods for evaluatinghow thoroughly the internal behavior of deep models is tested, but they are not applicableto NLP models. We propose a set of white-box testing methods that are customized for transformer-based NLP models. These include MASK NEURON COVERAGE (MNCOVER) that measures how thoroughlythe attention layers in... | Arshdeep Sekhon, Yangfeng Ji, Matthew B. Dwyer, Yanjun Qi |  |
| 640 |  |  [Hierarchical Transformers Are More Efficient Language Models](https://doi.org/10.18653/v1/2022.findings-naacl.117) |  | 0 | Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences, which allows them to produce long coherent outputs: entire paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that... | Piotr Nawrot, Szymon Tworkowski, Michal Tyrolski, Lukasz Kaiser, Yuhuai Wu, Christian Szegedy, Henryk Michalewski |  |
| 641 |  |  [DISARM: Detecting the Victims Targeted by Harmful Memes](https://doi.org/10.18653/v1/2022.findings-naacl.118) |  | 0 | Internet memes have emerged as an increasingly popular means of communication on the web. Although memes are typically intended to elicit humour, they have been increasingly used to spread hatred, trolling, and cyberbullying, as well as to target specific individuals, communities, or society on political, socio-cultural, and psychological grounds. While previous work has focused on detecting harmful, hateful, and offensive memes in general, identifying whom these memes attack (i.e., the... | Shivam Sharma, Md. Shad Akhtar, Preslav Nakov, Tanmoy Chakraborty |  |
| 642 |  |  [KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation](https://doi.org/10.18653/v1/2022.findings-naacl.119) |  | 0 | Self-supervised vision-and-language pretraining (VLP) aims to learn transferable multi-modal representations from large-scale image-text data and to achieve strong performances on a broad scope of vision-language tasks after finetuning. Previous mainstream VLP approaches typically adopt a two-step strategy relying on external object detectors to encode images in a multi-modal Transformer framework, which suffer from restrictive object concept space, limited image context and inefficient... | Yongfei Liu, Chenfei Wu, ShaoYen Tseng, Vasudev Lal, Xuming He, Nan Duan |  |
| 643 |  |  [Dependency Position Encoding for Relation Extraction](https://doi.org/10.18653/v1/2022.findings-naacl.120) |  | 0 | Leveraging the dependency tree of the input sentence is able to improve the model performance for relation extraction. A challenging issue is how to remove confusions from the tree. Efforts have been made to utilize the dependency connections between words to selectively emphasize target-relevant information. However, these approaches are limited in focusing on exploiting dependency types. In this paper, we propose dependency position encoding (DPE), an efficient way of incorporating both... | Qiushi Guo, Xin Wang, Dehong Gao |  |
| 644 |  |  [Good Visual Guidance Make A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction](https://doi.org/10.18653/v1/2022.findings-naacl.121) |  | 0 | Multimodal named entity recognition and relation extraction (MNER and MRE) is a fundamental and crucial branch in information extraction. However, existing approaches for MNER and MRE usually suffer from error sensitivity when irrelevant object images incorporated in texts. To deal with these issues, we propose a novel Hierarchical Visual Prefix fusion NeTwork (HVPNeT) for visual-enhanced entity and relation extraction, aiming to achieve more effective and robust performance. Specifically, we... | Xiang Chen, Ningyu Zhang, Lei Li, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, Huajun Chen |  |
| 645 |  |  [The Role of Context in Detecting Previously Fact-Checked Claims](https://doi.org/10.18653/v1/2022.findings-naacl.122) |  | 0 | Recent years have seen the proliferation of disinformation and fake news online. Traditional approaches to mitigate these issues is to use manual or automatic fact-checking. Recently, another approach has emerged: checking whether the input claim has previously been fact-checked, which can be done automatically, and thus fast, while also offering credibility and explainability, thanks to the human fact-checking and explanations in the associated fact-checking article. Here, we focus on claims... | Shaden Shaar, Firoj Alam, Giovanni Da San Martino, Preslav Nakov |  |
| 646 |  |  [Pruning Adatperfusion with Lottery Ticket Hypothesis](https://doi.org/10.18653/v1/2022.findings-naacl.123) |  | 0 | Pre-trained language models have shown great success in multiple downstream tasks. However, they are computationally expensive to fine-tune. Thus, transfer learning with adapter modules has been introduced to alleviate this problem, helping to extract knowledge of the downstream tasks. Adapterfusion models are an example of the transformers-with-adapter-modules, which merge multiple adapters to incorporate knowledge from different tasks. However, merging multiple adapters will inevitably cause... | Jiarun Wu, Qingliang Chen, Zeguan Xiao, Yuliang Gu, Mengsi Sun |  |
| 647 |  |  [EVI: Multilingual Spoken Dialogue Tasks and Dataset for Knowledge-Based Enrolment, Verification, and Identification](https://doi.org/10.18653/v1/2022.findings-naacl.124) |  | 0 | Knowledge-based authentication is crucial for task-oriented spoken dialogue systems that offer personalised and privacy-focused services. Such systems should be able to enrol (E), verify (V), and identify (I) new and recurring users based on their personal information, e.g. postcode, name, and date of birth. In this work, we formalise the three authentication tasks and their evaluation protocols, and we present EVI, a challenging spoken multilingual dataset with 5,506 dialogues in English,... | Georgios Spithourakis, Ivan Vulic, Michal Lis, Iñigo Casanueva, Pawel Budzianowski |  |
| 648 |  |  [Post-Training Dialogue Summarization using Pseudo-Paraphrasing](https://doi.org/10.18653/v1/2022.findings-naacl.125) |  | 0 | Previous dialogue summarization techniques adapt large language models pretrained on the narrative text by injecting dialogue-specific features into the models. These features either require additional knowledge to recognize or make the resulting models harder to tune. To bridge the format gap between dialogues and narrative summaries in dialogue summarization tasks, we propose to post-train pretrained language models (PLMs) to rephrase from dialogue to narratives. After that, the model is... | Qi Jia, Yizhu Liu, Haifeng Tang, Kenny Q. Zhu |  |
| 649 |  |  [A Dual-Channel Framework for Sarcasm Recognition by Detecting Sentiment Conflict](https://doi.org/10.18653/v1/2022.findings-naacl.126) |  | 0 | Sarcasm employs ambivalence, where one says something positive but actually means negative, and vice versa. The essence of sarcasm, which is also a sufficient and necessary condition, is the conflict between literal and implied sentiments expressed in one sentence. However, it is difficult to recognize such sentiment conflict because the sentiments are mixed or even implicit. As a result, the recognition of sophisticated and obscure sentiment brings in a great challenge to sarcasm detection. In... | Yiyi Liu, Yequan Wang, Aixin Sun, Xuying Meng, Jing Li, Jiafeng Guo |  |
| 650 |  |  [Zero-shot Entity Linking with Less Data](https://doi.org/10.18653/v1/2022.findings-naacl.127) |  | 0 | Entity Linking (EL) maps an entity mention in a natural language sentence to an entity in a knowledge base (KB). The Zero-shot Entity Linking (ZEL) extends the scope of EL to unseen entities at the test time without requiring new labeled data. BLINK (BERT-based) is one of the SOTA models for ZEL. Interestingly, we discovered that BLINK exhibits diminishing returns, i.e., it reaches 98% of its performance with just 1% of the training data and the remaining 99% of the data yields only a marginal... | G. P. Shrivatsa Bhargav, Dinesh Khandelwal, Saswati Dana, Dinesh Garg, Pavan Kapanipathi, Salim Roukos, Alexander G. Gray, L. Venkata Subramaniam |  |
| 651 |  |  [GraphCache: Message Passing as Caching for Sentence-Level Relation Extraction](https://doi.org/10.18653/v1/2022.findings-naacl.128) |  | 0 | Entity types and textual context are essential properties for sentence-level relation extraction (RE). Existing work only encodes these properties within individual instances, which limits the performance of RE given the insufficient features in a single sentence. In contrast, we model these properties from the whole dataset and use the dataset-level information to enrich the semantics of every instance. We propose the GraphCache (Graph Neural Network as Caching) module, that propagates the... | Yiwei Wang, Muhao Chen, Wenxuan Zhou, Yujun Cai, Yuxuan Liang, Bryan Hooi |  |
| 652 |  |  [Revisiting Generative Commonsense Reasoning: A Pre-Ordering Approach](https://doi.org/10.18653/v1/2022.findings-naacl.129) |  | 0 | Pre-trained models (PTMs) have lead to great improvements in natural language generation (NLG). However, it is still unclear how much commonsense knowledge they possess. With the goal of evaluating commonsense knowledge of NLG models, recent work has proposed the problem of generative commonsense reasoning, e.g., to compose a logical sentence given a set of unordered concepts. Existing approaches to this problem hypothesize that PTMs lack sufficient parametric knowledge for this task, which can... | Chao Zhao, Faeze Brahman, Tenghao Huang, Snigdha Chaturvedi |  |
| 653 |  |  [Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models](https://doi.org/10.18653/v1/2022.findings-naacl.130) |  | 0 | Recently, NLP models have achieved remarkable progress across a variety of tasks; however, they have also been criticized for being not robust. Many robustness problems can be attributed to models exploiting “spurious correlations”, or “shortcuts” between the training data and the task labels. Most existing work identifies a limited set of task-specific shortcuts via human priors or error analyses, which requires extensive expertise and efforts. In this paper, we aim to automatically identify... | Tianlu Wang, Rohit Sridhar, Diyi Yang, Xuezhi Wang |  |
| 654 |  |  [$Great Truths are Always Simple: $ A Rather Simple Knowledge Encoder for Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models](https://doi.org/10.18653/v1/2022.findings-naacl.131) |  | 0 | Commonsense reasoning in natural language is a desired ability of artificial intelligent systems. For solving complex commonsense reasoning tasks, a typical solution is to enhance pre-trained language models (PTMs) with a knowledge-aware graph neural network (GNN) encoder that models a commonsense knowledge graph (CSKG).Despite the effectiveness, these approaches are built on heavy architectures, and can’t clearly explain how external knowledge resources improve the reasoning capacity of PTMs.... | Jinhao Jiang, Kun Zhou, JiRong Wen, Wayne Xin Zhao |  |
| 655 |  |  [Analyzing the Intensity of Complaints on Social Media](https://doi.org/10.18653/v1/2022.findings-naacl.132) |  | 0 | Complaining is a speech act that expresses a negative inconsistency between reality and human’s expectations. While prior studies mostly focus on identifying the existence or the type of complaints, in this work, we present the first study in computational linguistics of measuring the intensity of complaints from text. Analyzing complaints from such perspective is particularly useful, as complaints of certain degrees may cause severe consequences for companies or organizations. We first collect... | Ming Fang, Shi Zong, Jing Li, Xinyu Dai, Shujian Huang, Jiajun Chen |  |
| 656 |  |  [Detecting Narrative Elements in Informational Text](https://doi.org/10.18653/v1/2022.findings-naacl.133) |  | 0 | Automatic extraction of narrative elements from text, combining narrative theories with computational models, has been receiving increasing attention over the last few years. Previous works have utilized the oral narrative theory by Labov and Waletzky to identify various narrative elements in personal stories texts. Instead, we direct our focus to informational texts, specifically news stories. We introduce NEAT (Narrative Elements AnnoTation) – a novel NLP task for detecting narrative elements... | Effi Levi, Guy Mor, Tamir Sheafer, Shaul R. Shenhav |  |
| 657 |  |  [When do Contrastive Word Alignments Improve Many-to-many Neural Machine Translation?](https://doi.org/10.18653/v1/2022.findings-naacl.134) |  | 0 | Word alignment has proven to benefit many-to-many neural machine translation (NMT). However, high-quality ground-truth bilingual dictionaries were used for pre-editing in previous methods, which are unavailable for most language pairs. Meanwhile, the contrastive objective can implicitly utilize automatically learned word alignment, which has not been explored in many-to-many NMT. This work proposes a word-level contrastive objective to leverage word alignments for many-to-many NMT. Empirical... | Zhuoyuan Mao, Chenhui Chu, Raj Dabre, Haiyue Song, Zhen Wan, Sadao Kurohashi |  |
| 658 |  |  [Minimally-Supervised Relation Induction from Pre-trained Language Model](https://doi.org/10.18653/v1/2022.findings-naacl.135) |  | 0 | Relation Induction is a very practical task in Natural Language Processing (NLP) area. In practical application scenarios, people want to induce more entity pairs having the same relation from only a few seed entity pairs. Thus, instead of the laborious supervised setting, in this paper, we focus on the minimally-supervised setting where only a couple of seed entity pairs per relation are provided. Although the conventional relation induction methods have made some success, their performance... | Lu Sun, Yongliang Shen, Weiming Lu |  |
| 659 |  |  [Crake: Causal-Enhanced Table-Filler for Question Answering over Large Scale Knowledge Base](https://doi.org/10.18653/v1/2022.findings-naacl.136) |  | 0 | Semantic parsing solves knowledge base (KB) question answering (KBQA) by composing a KB query, which generally involves node extraction (NE) and graph composition (GC) to detect and connect related nodes in a query. Despite the strong causal effects between NE and GC, previous works fail to directly model such causalities in their pipeline, hindering the learning of subtask correlations. Also, the sequence-generation process for GC in previous works induces ambiguity and exposure bias, which... | Minhao Zhang, Ruoyu Zhang, Yanzeng Li, Lei Zou |  |
| 660 |  |  [Exploring the Universal Vulnerability of Prompt-based Learning Paradigm](https://doi.org/10.18653/v1/2022.findings-naacl.137) |  | 0 | Prompt-based learning paradigm bridges the gap between pre-training and fine-tuning, and works effectively under the few-shot setting. However, we find that this learning paradigm inherits the vulnerability from the pre-training stage, where model predictions can be misled by inserting certain triggers into the text. In this paper, we explore this universal vulnerability by either injecting backdoor triggers or searching for adversarial triggers on pre-trained language models using only plain... | Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Zhiyuan Liu |  |
| 661 |  |  [Exploiting Numerical-Contextual Knowledge to Improve Numerical Reasoning in Question Answering](https://doi.org/10.18653/v1/2022.findings-naacl.138) |  | 0 | Numerical reasoning over text is a challenging subtask in question answering (QA) that requires both the understanding of texts and numbers. However, existing language models in these numerical reasoning QA models tend to overly rely on the pre-existing parametric knowledge at inference time, which commonly causes hallucination in interpreting numbers. Our work proposes a novel attention masked reasoning model, the NC-BERT, that learns to leverage the number-related contextual knowledge to... | Jeonghwan Kim, Junmo Kang, KyungMin Kim, Giwon Hong, SungHyon Myaeng |  |
| 662 |  |  [Learn from Relation Information: Towards Prototype Representation Rectification for Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2022.findings-naacl.139) |  | 0 | Few-shot Relation Extraction refers to fast adaptation to novel relation classes with few samples through training on the known relation classes. Most existing methods focus on implicitly introducing relation information (i.e., relation label or relation description) to constrain the prototype representation learning, such as contrastive learning, graphs, and specifically designed attentions, which may bring useless and even harmful parameters. Besides, these approaches are limited in handing... | Yang Liu, Jinpeng Hu, Xiang Wan, TsungHui Chang |  |
| 663 |  |  [HUE: Pretrained Model and Dataset for Understanding Hanja Documents of Ancient Korea](https://doi.org/10.18653/v1/2022.findings-naacl.140) |  | 0 | Historical records in Korea before the 20th century were primarily written in Hanja, an extinct language based on Chinese characters and not understood by modern Korean or Chinese speakers. Historians with expertise in this time period have been analyzing the documents, but that process is very difficult and time-consuming, and language models would significantly speed up the process. Toward building and evaluating language models for Hanja, we release the Hanja Understanding Evaluation dataset... | Haneul Yoo, Jiho Jin, Juhee Son, JinYeong Bak, Kyunghyun Cho, Alice Oh |  |
| 664 |  |  [SeaD: End-to-end Text-to-SQL Generation with Schema-aware Denoising](https://doi.org/10.18653/v1/2022.findings-naacl.141) |  | 0 | On the WikiSQL benchmark, most methods tackle the challenge of text-to-SQL with predefined sketch slots and build sophisticated sub-tasks to fill these slots. Though achieving promising results, these methods suffer from over-complex model structure. In this paper, we present a simple yet effective approach that enables auto-regressive sequence-to-sequence model to robust text-to-SQL generation. Instead of formulating the task of text-to-SQL as slot-filling, we propose to train... | Kuan Xu, Yongbo Wang, Yongliang Wang, Zihao Wang, Zujie Wen, Yang Dong |  |
| 665 |  |  [Cross-Lingual Cross-Modal Consolidation for Effective Multilingual Video Corpus Moment Retrieval](https://doi.org/10.18653/v1/2022.findings-naacl.142) |  | 0 | Existing multilingual video corpus moment retrieval (mVCMR) methods are mainly based on a two-stream structure. The visual stream utilizes the visual content in the video to estimate the query-visual similarity, and the subtitle stream exploits the query-subtitle similarity. The final query-video similarity ensembles similarities from two streams. In our work, we pro- pose a simple and effective strategy termed as Cross-lingual Cross-modal Consolidation (C3 ) to improve mVCMR accuracy. We adopt... | Jiaheng Liu, Tan Yu, Hanyu Peng, Mingming Sun, Ping Li |  |
| 666 |  |  [Delving Deep into Regularity: A Simple but Effective Method for Chinese Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-naacl.143) |  | 0 | Recent years have witnessed the improving performance of Chinese Named Entity Recognition (NER) from proposing new frameworks or incorporating word lexicons. However, the inner composition of entity mentions in character-level Chinese NER has been rarely studied. Actually, most mentions of regular types have strong name regularity. For example, entities end with indicator words such as “公司 (company) ” or “银行 (bank)” usually belong to organization. In this paper, we propose a simple but... | Yingjie Gu, Xiaoye Qu, Zhefeng Wang, Yi Zheng, Baoxing Huai, Nicholas Jing Yuan |  |
| 667 |  |  [CRUSH: Contextually Regularized and User anchored Self-supervised Hate speech Detection](https://doi.org/10.18653/v1/2022.findings-naacl.144) |  | 0 | The last decade has witnessed a surge in the interaction of people through social networking platforms. While there are several positive aspects of these social platforms, their proliferation has led them to become the breeding ground for cyber-bullying and hate speech. Recent advances in NLP have often been used to mitigate the spread of such hateful content. Since the task of hate speech detection is usually applicable in the context of social networks, we introduce CRUSH, a framework for... | Souvic Chakraborty, Parag Dutta, Sumegh Roychowdhury, Animesh Mukherjee |  |
| 668 |  |  [METGEN: A Module-Based Entailment Tree Generation Framework for Answer Explanation](https://doi.org/10.18653/v1/2022.findings-naacl.145) |  | 0 | Knowing the reasoning chains from knowledge to the predicted answers can help construct an explainable question answering (QA) system. Advances on QA explanation propose to explain the answers with entailment trees composed of multiple entailment steps. While current work proposes to generate entailment trees with end-to-end generative models, the steps in the generated trees are not constrained and could be unreliable. In this paper, we propose METGEN, a Module-based Entailment Tree GENeration... | Ruixin Hong, Hongming Zhang, Xintong Yu, Changshui Zhang |  |
| 669 |  |  [A Timestep aware Sentence Embedding and Acme Coverage for Brief but Informative Title Generation](https://doi.org/10.18653/v1/2022.findings-naacl.146) |  | 0 | The title generation task that summarizes article content in recapitulatory words relies heavily on utilizing the corresponding key context. To generate a title with appropriate information in the content and avoid repetition, we propose a title generation framework with two complementary components in this paper. First, we propose a Timestep aware Sentence Embedding (TSE) mechanism, which updates the sentences’ representations by re-locating the critical words in the corresponding sentence for... | Quanbin Wang, Xiexiong Lin, Feng Wang |  |
| 670 |  |  [Make The Most of Prior Data: A Solution for Interactive Text Summarization with Preference Feedback](https://doi.org/10.18653/v1/2022.findings-naacl.147) |  | 0 | For summarization, human preferences is critical to tame outputs of the summarizer in favor of human interests, as ground-truth summaries are scarce and ambiguous. Practical settings require dynamic exchanges between humans and AI agents wherein feedback is provided in an online manner, a few at a time. In this paper, we introduce a new framework to train summarization models with preference feedback interactively. By properly leveraging offline data and a novel reward model, we improve the... | DuyHung Nguyen, NguyenVietDung Nghiem, BaoSinh Nguyen, Dung Tien Le, Shahab Sabahi, MinhTien Nguyen, Hung Le |  |
| 671 |  |  [XLTime: A Cross-Lingual Knowledge Transfer Framework for Temporal Expression Extraction](https://doi.org/10.18653/v1/2022.findings-naacl.148) |  | 0 | Temporal Expression Extraction (TEE) is essential for understanding time in natural language. It has applications in Natural Language Processing (NLP) tasks such as question answering, information retrieval, and causal inference. To date, work in this area has mostly focused on English as there is a scarcity of labeled data for other languages. We propose XLTime, a novel framework for multilingual TEE. XLTime works on top of pre-trained language models and leverages multi-task learning to... | Yuwei Cao, William Groves, Tanay Kumar Saha, Joel R. Tetreault, Alejandro Jaimes, Hao Peng, Philip S. Yu |  |
| 672 |  |  [BehancePR: A Punctuation Restoration Dataset for Livestreaming Video Transcript](https://doi.org/10.18653/v1/2022.findings-naacl.149) |  | 0 | Given the increasing number of livestreaming videos, automatic speech recognition and post-processing for livestreaming video transcripts are crucial for efficient data management as well as knowledge mining. A key step in this process is punctuation restoration which restores fundamental text structures such as phrase and sentence boundaries from the video transcripts. This work presents a new human-annotated corpus, called BehancePR, for punctuation restoration in livestreaming video... | Viet Dac Lai, Amir Pouran Ben Veyseh, Franck Dernoncourt, Thien Huu Nguyen |  |
| 673 |  |  [Event Detection for Suicide Understanding](https://doi.org/10.18653/v1/2022.findings-naacl.150) |  | 0 | Suicide is a serious problem in every society. Understanding life events of a potential patient is essential for successful suicide-risk assessment and prevention. In this work, we focus on the Event Detection (ED) task to identify event trigger words of suicide-related events in public posts of discussion forums. In particular, we introduce SuicideED: a new dataset for the ED task that features seven suicidal event types to comprehensively capture suicide actions and ideation, and general risk... | Luis GuzmanNateras, Viet Dac Lai, Amir Pouran Ben Veyseh, Franck Dernoncourt, Thien Huu Nguyen |  |
| 674 |  |  [Great Power, Great Responsibility: Recommendations for Reducing Energy for Training Language Models](https://doi.org/10.18653/v1/2022.findings-naacl.151) |  | 0 | The energy requirements of current natural language processing models continue to grow at a rapid, unsustainable pace. Recent works highlighting this problem conclude there is an urgent need for methods that reduce the energy needs of NLP and machine learning more broadly. In this article, we investigate techniques that can be used to reduce the energy consumption of common NLP applications. In particular, we focus on techniques to measure energy usage and different hardware and... | Joseph McDonald, Baolin Li, Nathan C. Frey, Devesh Tiwari, Vijay Gadepally, Siddharth Samsi |  |
| 675 |  |  [What kinds of errors do reference resolution models make and what can we learn from them?](https://doi.org/10.18653/v1/2022.findings-naacl.152) |  | 0 | Referring resolution is the task of identifying the referent of a natural language expression, for example “the woman behind the other woman getting a massage”. In this paper we investigate which are the kinds of referring expressions on which current transformer based models fail. Motivated by this analysis we identify the weakening of the spatial natural constraints as one of its causes and propose a model that aims to restore it. We evaluate our proposed model on different datasets for the... | Jorge Sánchez, Mauricio Mazuecos, Hernán Maina, Luciana Benotti |  |
| 676 |  |  [Uncertainty-Aware Cross-Lingual Transfer with Pseudo Partial Labels](https://doi.org/10.18653/v1/2022.findings-naacl.153) |  | 0 | Large-scale multilingual pre-trained language models have achieved remarkable performance in zero-shot cross-lingual tasks. A recent study has demonstrated the effectiveness of self-learning-based approach on cross-lingual transfer, where only unlabeled data of target languages are required, without any efforts to annotate gold labels for target languages. However, it suffers from noisy training due to the incorrectly pseudo-labeled samples. In this work, we propose an uncertainty-aware... | Shuo Lei, Xuchao Zhang, Jianfeng He, Fanglan Chen, ChangTien Lu |  |
| 677 |  |  [NLU++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue](https://doi.org/10.18653/v1/2022.findings-naacl.154) |  | 0 | We present NLU++, a novel dataset for natural language understanding (NLU) in task-oriented dialogue (ToD) systems, with the aim to provide a much more challenging evaluation environment for dialogue NLU models, up to date with the current application and industry requirements. NLU++ is divided into two domains (BANKING and HOTELS) and brings several crucial improvements over current commonly used NLU datasets. 1) NLU++ provides fine-grained domain ontologies with a large set of challenging... | Iñigo Casanueva, Ivan Vulic, Georgios Spithourakis, Pawel Budzianowski |  |
| 678 |  |  [Challenges in Generalization in Open Domain Question Answering](https://doi.org/10.18653/v1/2022.findings-naacl.155) |  | 0 | Recent work on Open Domain Question Answering has shown that there is a large discrepancy in model performance between novel test questions and those that largely overlap with training questions. However, it is unclear which aspects of novel questions make them challenging. Drawing upon studies on systematic generalization, we introduce and annotate questions according to three categories that measure different levels and kinds of generalization: training set overlap, compositional... | Linqing Liu, Patrick Lewis, Sebastian Riedel, Pontus Stenetorp |  |
| 679 |  |  [Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence](https://doi.org/10.18653/v1/2022.findings-naacl.156) |  | 0 | The logical negation property (LNP), which implies generating different predictions for semantically opposite inputs (p is true iff ¬p is false), is an important property that a trustworthy language model must satisfy. However, much recent evidence shows that large-size pre-trained language models (PLMs) do not satisfy this property. In this paper, we perform experiments using probing tasks to assess PLMs’ LNP understanding. Unlike previous studies that only examined negation expressions, we... | Myeongjun Jang, Frank Mtumbuka, Thomas Lukasiewicz |  |
| 680 |  |  [Por Qué Não Utiliser Alla Språk? Mixed Training with Gradient Optimization in Few-Shot Cross-Lingual Transfer](https://doi.org/10.18653/v1/2022.findings-naacl.157) |  | 0 | The current state-of-the-art for few-shot cross-lingual transfer learning first trains on abundant labeled data in the source language and then fine-tunes with a few examples on the target language, termed target-adapting. Though this has been demonstrated to work on a variety of tasks, in this paper we show some deficiencies of this approach and propose a one-step mixed training method that trains on both source and target data with stochastic gradient surgery, a novel gradient-level... | Haoran Xu, Kenton Murray |  |
| 681 |  |  [Learning to Execute Actions or Ask Clarification Questions](https://doi.org/10.18653/v1/2022.findings-naacl.158) |  | 0 | Collaborative tasks are ubiquitous activities where a form of communication is required in order to reach a joint goal. Collaborative building is one of such tasks. We wish to develop an intelligent builder agent in a simulated building environment (Minecraft) that can build whatever users wish to build by just talking to the agent. In order to achieve this goal, such agents need to be able to take the initiative by asking clarification questions when further information is needed. Existing... | Zhengxiang Shi, Yue Feng, Aldo Lipani |  |
| 682 |  |  [Capturing Conversational Interaction for Question Answering via Global History Reasoning](https://doi.org/10.18653/v1/2022.findings-naacl.159) |  | 0 | Conversational Question Answering (ConvQA) is required to answer the current question, conditioned on the observable paragraph-level context and conversation history. Previous works have intensively studied history-dependent reasoning. They perceive and absorb topic-related information of prior utterances in the interactive encoding stage. It yielded significant improvement compared to history-independent reasoning. This paper further strengthens the ConvQA encoder by establishing long-distance... | Jin Qian, Bowei Zou, Mengxing Dong, Xiao Li, AiTi Aw, Yu Hong |  |
| 683 |  |  [Learning Structural Information for Syntax-Controlled Paraphrase Generation](https://doi.org/10.18653/v1/2022.findings-naacl.160) |  | 0 | Syntax-controlled paraphrase generation aims to produce paraphrase conform to given syntactic patterns. To address this task, recent works have started to use parse trees (or syntactic templates) to guide generation.A constituency parse tree contains abundant structural information, such as parent-child relation, sibling relation, and the alignment relation between words and nodes. Previous works have only utilized parent-child and alignment relations, which may affect the generation quality.... | Erguang Yang, Chenglin Bai, Deyi Xiong, Yujie Zhang, Yao Meng, Jinan Xu, Yufeng Chen |  |
| 684 |  |  [Controllable Sentence Simplification via Operation Classification](https://doi.org/10.18653/v1/2022.findings-naacl.161) |  | 0 | Different types of transformations have been used to model sentence simplification ranging from mainly local operations such as phrasal or lexical rewriting, deletion and re-ordering to the more global affecting the whole input sentence such as sentence rephrasing, copying and splitting. In this paper, we propose a novel approach to sentence simplification which encompasses four global operations: whether to rephrase or copy and whether to split based on syntactic or discourse structure. We... | Liam Cripwell, Joël Legrand, Claire Gardent |  |
| 685 |  |  [Balancing Multi-Domain Corpora Learning for Open-Domain Response Generation](https://doi.org/10.18653/v1/2022.findings-naacl.162) |  | 0 | Open-domain conversational systems are assumed to generate equally good responses on multiple domains. Previous work achieved good performance on the single corpus, but training and evaluating on multiple corpora from different domains are less studied. This paper explores methods of generating relevant responses for each of multiple multi-domain corpora. We first examine interleaved learning which intermingles multiple corpora as the baseline. We then investigate two multi-domain learning... | Yujie Xing, Jinglun Cai, Nils Barlaug, Peng Liu, Jon Atle Gulla |  |
| 686 |  |  [Semantic-Preserving Abstractive Text Summarization with Siamese Generative Adversarial Net](https://doi.org/10.18653/v1/2022.findings-naacl.163) |  | 0 | We propose a novel siamese generative adversarial net for abstractive text summarization (SSPGAN), which can preserve the main semantics of the source text. Different from previous generative adversarial net based methods, SSPGAN is equipped with a siamese semantic-preserving discriminator, which can not only be trained to discriminate the machine-generated summaries from the human-summarized ones, but also ensure the semantic consistency between the source text and target summary. As a... | Xin Sheng, Linli Xu, Yinlong Xu, Deqiang Jiang, Bo Ren |  |
| 687 |  |  [Towards Job-Transition-Tag Graph for a Better Job Title Representation Learning](https://doi.org/10.18653/v1/2022.findings-naacl.164) |  | 0 | Works on learning job title representation are mainly based on Job-Transition Graph, built from the working history of talents. However, since these records are usually messy, this graph is very sparse, which affects the quality of the learned representation and hinders further analysis. To address this specific issue, we propose to enrich the graph with additional nodes that improve the quality of job title representation. Specifically, we construct Job-Transition-Tag Graph, a heterogeneous... | Jun Zhu, Céline Hudelot |  |
| 688 |  |  [CL-ReLKT: Cross-lingual Language Knowledge Transfer for Multilingual Retrieval Question Answering](https://doi.org/10.18653/v1/2022.findings-naacl.165) |  | 0 | Cross-Lingual Retrieval Question Answering (CL-ReQA) is concerned with retrieving answer documents or passages to a question written in a different language. A common approach to CL-ReQA is to create a multilingual sentence embedding space such that question-answer pairs across different languages are close to each other. In this paper, we propose a novel CL-ReQA method utilizing the concept of language knowledge transfer and a new cross-lingual consistency training technique to create a... | Peerat Limkonchotiwat, Wuttikorn Ponwitayarat, Can Udomcharoenchaikit, Ekapol Chuangsuwanich, Sarana Nutanong |  |
| 689 |  |  [BORT: Back and Denoising Reconstruction for End-to-End Task-Oriented Dialog](https://doi.org/10.18653/v1/2022.findings-naacl.166) |  | 0 | A typical end-to-end task-oriented dialog system transfers context into dialog state, and upon which generates a response, which usually faces the problem of error propagation from both previously generated inaccurate dialog states and responses, especially in low-resource scenarios. To alleviate these issues, we propose BORT, a back and denoising reconstruction approach for end-to-end task-oriented dialog system. Squarely, to improve the accuracy of dialog states, back reconstruction is used... | Haipeng Sun, Junwei Bao, Youzheng Wu, Xiaodong He |  |
| 690 |  |  [Multi-stage Distillation Framework for Cross-Lingual Semantic Similarity Matching](https://doi.org/10.18653/v1/2022.findings-naacl.167) |  | 0 | Previous studies have proved that cross-lingual knowledge distillation can significantly improve the performance of pre-trained models for cross-lingual similarity matching tasks. However, the student model needs to be large in this operation. Otherwise, its performance will drop sharply, thus making it impractical to be deployed to memory-limited devices. To address this issue, we delve into cross-lingual knowledge distillation and propose a multi-stage distillation framework for constructing... | Kunbo Ding, Weijie Liu, Yuejian Fang, Zhe Zhao, Qi Ju, Xuefeng Yang, Rong Tian, Zhu Tao, Haoyan Liu, Han Guo, Xingyu Bai, Weiquan Mao, Yudong Li, Weigang Guo, Taiqiang Wu, Ningyuan Sun |  |
| 691 |  |  [On the Limitations of Dataset Balancing: The Lost Battle Against Spurious Correlations](https://doi.org/10.18653/v1/2022.findings-naacl.168) |  | 0 | Recent work has shown that deep learning models in NLP are highly sensitive to low-level correlations between simple features and specific output labels, leading to over-fitting and lack of generalization. To mitigate this problem, a common practice is to balance datasets by adding new instances or by filtering out “easy” instances (Sakaguchi et al., 2020), culminating in a recent proposal to eliminate single-word correlations altogether (Gardner et al., 2021). In this opinion paper, we... | Roy Schwartz, Gabriel Stanovsky |  |
| 692 |  |  [Specializing Pre-trained Language Models for Better Relational Reasoning via Network Pruning](https://doi.org/10.18653/v1/2022.findings-naacl.169) |  | 0 | Pretrained masked language models (PLMs) were shown to be inheriting a considerable amount of relational knowledge from the source corpora. In this paper, we present an in-depth and comprehensive study concerning specializing PLMs into relational models from the perspective of network pruning. We show that it is possible to find subnetworks capable of representing grounded commonsense relations at non-trivial sparsity while being more generalizable than original PLMs in scenarios requiring... | Siyu Ren, Kenny Q. Zhu |  |
| 693 |  |  [D2GCLF: Document-to-Graph Classifier for Legal Document Classification](https://doi.org/10.18653/v1/2022.findings-naacl.170) |  | 0 | Legal document classification is an essential task in law intelligence to automate the labor-intensive law case filing process. Unlike traditional document classification problems, legal documents should be classified by reasons and facts instead of topics. We propose a Document-to-Graph Classifier (D2GCLF), which extracts facts as relations between key participants in the law case and represents a legal document with four relation graphs. Each graph is responsible for capturing different... | Qiqi Wang, Kaiqi Zhao, Robert Amor, Benjamin Liu, Ruofan Wang |  |
| 694 |  |  [A Label-Aware Autoregressive Framework for Cross-Domain NER](https://doi.org/10.18653/v1/2022.findings-naacl.171) |  | 0 | Cross-domain named entity recognition (NER) aims to borrow the entity information from the source domain to help the entity recognition in the target domain with limited labeled data. Despite the promising performance of existing approaches, most of them focus on reducing the discrepancy of token representation between source and target domains, while the transfer of the valuable label information is often not explicitly considered or even ignored. Therefore, we propose a novel autoregressive... | Jinpeng Hu, He Zhao, Dan Guo, Xiang Wan, TsungHui Chang |  |
| 695 |  |  [A Dog Is Passing Over The Jet? A Text-Generation Dataset for Korean Commonsense Reasoning and Evaluation](https://doi.org/10.18653/v1/2022.findings-naacl.172) |  | 0 | Recent natural language understanding (NLU) research on the Korean language has been vigorously maturing with the advancements of pretrained language models and datasets. However, Korean pretrained language models still struggle to generate a short sentence with a given condition based on compositionality and commonsense reasoning (i.e., generative commonsense reasoning). The two major challenges are inadequate data resources to develop generative commonsense reasoning regarding Korean... | Jaehyung Seo, Seounghoon Lee, Chanjun Park, Yoonna Jang, Hyeonseok Moon, Sugyeong Eo, Seonmin Koo, Heuiseok Lim |  |
| 696 |  |  [Improve Discourse Dependency Parsing with Contextualized Representations](https://doi.org/10.18653/v1/2022.findings-naacl.173) |  | 0 | Previous works show that discourse analysis benefits from modeling intra- and inter-sentential levels separately, where proper representations for text units of different granularities are desired to capture both the information of the text units and their relation to the context. In this paper, we propose to take advantage of transformers to encode different contextualized representations of units of different levels to dynamically capture the information required for discourse dependency... | Yifei Zhou, Yansong Feng |  |
| 697 |  |  [LiST: Lite Prompted Self-training Makes Parameter-efficient Few-shot Learners](https://doi.org/10.18653/v1/2022.findings-naacl.174) |  | 0 | We present a new method LiST for efficient fine-tuning of large pre-trained language models (PLMs) in few-shot learning settings. LiST improves over recent methods that adopt prompt-based fine-tuning (FN) using two key techniques. The first is the use of self-training to leverage large amounts of unlabeled data for prompt-based FN in few-shot settings. We use self-training in conjunction with meta-learning for re-weighting noisy pseudo-prompt labels. Traditionally, self-training is expensive as... | Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Awadallah, Jianfeng Gao |  |
| 698 |  |  [CLMLF: A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection](https://doi.org/10.18653/v1/2022.findings-naacl.175) |  | 0 | Compared with unimodal data, multimodal data can provide more features to help the model analyze the sentiment of data. Previous research works rarely consider token-level feature fusion, and few works explore learning the common features related to sentiment in multimodal data to help the model fuse multimodal features. In this paper, we propose a Contrastive Learning and Multi-Layer Fusion (CLMLF) method for multimodal sentiment detection. Specifically, we first encode text and image to... | Zhen Li, Bing Xu, Conghui Zhu, Tiejun Zhao |  |
| 699 |  |  [Weakly Supervised Text Classification using Supervision Signals from a Language Model](https://doi.org/10.18653/v1/2022.findings-naacl.176) |  | 0 | Solving text classification in a weakly supervised manner is important for real-world applications where human annotations are scarce. In this paper, we propose to query a masked language model with cloze style prompts to obtain supervision signals. We design a prompt which combines the document itself and “this article is talking about [MASK].” A masked language model can generate words for the [MASK] token. The generated words which summarize the content of a document can be utilized as... | Ziqian Zeng, Weimin Ni, Tianqing Fang, Xiang Li, Xinran Zhao, Yangqiu Song |  |
| 700 |  |  [Analytical Reasoning of Text](https://doi.org/10.18653/v1/2022.findings-naacl.177) |  | 0 | Analytical reasoning is an essential and challenging task that requires a system to analyze a scenario involving a set of particular circumstances and perform reasoning over it to make conclusions. However, current neural models with implicit reasoning ability struggle to solve this task. In this paper, we study the challenge of analytical reasoning of text and collect a new dataset consisting of questions from the Law School Admission Test from 1991 to 2016. We analyze what knowledge... | Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Yining Chen, Jiahai Wang, Jian Yin, Ming Zhou, Nan Duan |  |
| 701 |  |  [Denoising Neural Network for News Recommendation with Positive and Negative Implicit Feedback](https://doi.org/10.18653/v1/2022.findings-naacl.178) |  | 0 | News recommendation is different from movie or e-commercial recommendation as people usually do not grade the news. Therefore, user feedback for news is always implicit (click behavior, reading time, etc). Inevitably, there are noises in implicit feedback. On one hand, the user may exit immediately after clicking the news as he dislikes the news content, leaving the noise in his positive implicit feedback; on the other hand, the user may be recommended multiple interesting news at the same time... | Yunfan Hu, Zhaopeng Qiu, Xian Wu |  |
| 702 |  |  [Continual Machine Reading Comprehension via Uncertainty-aware Fixed Memory and Adversarial Domain Adaptation](https://doi.org/10.18653/v1/2022.findings-naacl.179) |  | 0 | Continual Machine Reading Comprehension aims to incrementally learn from a continuous data stream across time without access the previous seen data, which is crucial for the development of real-world MRC systems. However, it is a great challenge to learn a new domain incrementally without catastrophically forgetting previous knowledge. In this paper, MA-MRC, a continual MRC model with uncertainty-aware fixed Memory and Adversarial domain adaptation, is proposed. In MA-MRC, a fixed size memory... | Zhijing Wu, Hua Xu, Jingliang Fang, Kai Gao |  |
| 703 |  |  [Jointly Learning Guidance Induction and Faithful Summary Generation via Conditional Variational Autoencoders](https://doi.org/10.18653/v1/2022.findings-naacl.180) |  | 0 | Abstractive summarization can generate high quality results with the development of the neural network. However, generating factual consistency summaries is a challenging task for abstractive summarization. Recent studies extract the additional information with off-the-shelf tools from the source document as a clue to guide the summary generation, which shows effectiveness to improve the faithfulness. Unlike these work, we present a novel framework based on conditional variational autoencoders,... | Wang Xu, Tiejun Zhao |  |
| 704 |  |  [Context-Aware Language Modeling for Goal-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2022.findings-naacl.181) |  | 0 | Goal-oriented dialogue systems face a trade-off between fluent language generation and task-specific control. While supervised learning with large language models is capable of producing realistic text, how to steer such responses towards completing a specific task without sacrificing language quality remains an open question. In this work, we formulate goal-oriented dialogue as a partially observed Markov decision process, interpreting the language model as a representation of both the... | Charlie Snell, Sherry Yang, Justin Fu, Yi Su, Sergey Levine |  |
| 705 |  |  [Am I Me or You? State-of-the-Art Dialogue Models Cannot Maintain an Identity](https://doi.org/10.18653/v1/2022.findings-naacl.182) |  | 0 | State-of-the-art dialogue models still often stumble with regards to factual accuracy and self-contradiction. Anecdotally, they have been observed to fail to maintain character identity throughout discourse; and more specifically, may take on the role of their interlocutor. In this work we formalize and quantify this deficiency, and show experimentally through human evaluations that this is indeed a problem. In contrast, we show that discriminative models trained specifically to recognize who... | Kurt Shuster, Jack Urbanek, Arthur Szlam, Jason Weston |  |
| 706 |  |  [Unsupervised Domain Adaptation for Question Generation with DomainData Selection and Self-training](https://doi.org/10.18653/v1/2022.findings-naacl.183) |  | 0 | Question generation (QG) approaches based on large neural models require (i) large-scale and (ii) high-quality training data. These two requirements pose difficulties for specific application domains where training data is expensive and difficult to obtain. The trained QG models’ effectiveness can degrade significantly when they are applied on a different domain due to domain shift. In this paper, we explore an unsupervised domain adaptation approach to combat the lack of training data and... | Peide Zhu, Claudia Hauff |  |
| 707 |  |  [CCQA: A New Web-Scale Question Answering Dataset for Model Pre-Training](https://doi.org/10.18653/v1/2022.findings-naacl.184) |  | 0 | We propose a novel open-domain question-answering dataset based on the Common Crawl project. With a previously unseen number of around 130 million multilingual question-answer pairs (including about 60 million English data-points), we use our large-scale, natural, diverse and high-quality corpus to in-domain pre-train popular language models for the task of question-answering. In our experiments, we find that our Common Crawl Question Answering dataset (CCQA) achieves promising results in... | Patrick Huber, Armen Aghajanyan, Barlas Oguz, Dmytro Okhonko, Scott Yih, Sonal Gupta, Xilun Chen |  |
| 708 |  |  [The Case for a Single Model that can Both Generate Continuations and Fill-in-the-Blank](https://doi.org/10.18653/v1/2022.findings-naacl.185) |  | 0 | The task of inserting text into a specified position in a passage, known as fill in the blank (FitB), is useful for a variety of applications where writers interact with a natural language generation (NLG) system to craft text. While previous work has tackled this problem with models trained specifically to do fill in the blank, a more useful model is one that can effectively perform _both_ FitB and continuation tasks. In this work, we evaluate the feasibility of using a single model to do both... | Daphne Ippolito, Liam Dugan, Emily Reif, Ann Yuan, Andy Coenen, Chris CallisonBurch |  |
| 709 |  |  [Learning Discriminative Representations for Open Relation Extraction with Instance Ranking and Label Calibration](https://doi.org/10.18653/v1/2022.findings-naacl.186) |  | 0 | Open relation extraction is the task to extract relational facts without pre-defined relation types from open-domain corpora. However, since there are some hard or semi-hard instances sharing similar context and entity information but belonging to different underlying relation, current OpenRE methods always cluster them into the same relation type. In this paper, we propose a novel method based on Instance Ranking and Label Calibration strategies (IRLC) to learn discriminative representations... | Shusen Wang, Bin Duan, Yanan Wu, Yajing Xu |  |
| 710 |  |  [Textual Entailment for Event Argument Extraction: Zero- and Few-Shot with Multi-Source Learning](https://doi.org/10.18653/v1/2022.findings-naacl.187) |  | 0 | Recent work has shown that NLP tasks such as Relation Extraction (RE) can be recasted as a Textual Entailment tasks using verbalizations, with strong performance in zero-shot and few-shot settings thanks to pre-trained entailment models. The fact that relations in current RE datasets are easily verbalized casts doubts on whether entailment would be effective in more complex tasks. In this work we show that entailment is also effective in Event Argument Extraction (EAE), reducing the need of... | Oscar Sainz, Itziar GonzalezDios, Oier Lopez de Lacalle, Bonan Min, Eneko Agirre |  |
| 711 |  |  [RCL: Relation Contrastive Learning for Zero-Shot Relation Extraction](https://doi.org/10.18653/v1/2022.findings-naacl.188) |  | 0 | Zero-shot relation extraction aims to identify novel relations which cannot be observed at the training stage. However, it still faces some challenges since the unseen relations of instances are similar or the input sentences have similar entities, the unseen relation representations from different categories tend to overlap and lead to errors. In this paper, we propose a novel Relation Contrastive Learning framework (RCL) to mitigate above two types of similar problems: Similar Relations and... | Shusen Wang, Bosen Zhang, Yajing Xu, Yanan Wu, Bo Xiao |  |
| 712 |  |  [Latent Group Dropout for Multilingual and Multidomain Machine Translation](https://doi.org/10.18653/v1/2022.findings-naacl.189) |  | 0 | Multidomain and multilingual machine translation often rely on parameter sharing strategies, where large portions of the network are meant to capture the commonalities of the tasks at hand, while smaller parts are reserved to model the peculiarities of a language or a domain. In adapter-based approaches, these strategies are hardcoded in the network architecture, independent of the similarities between tasks. In this work, we propose a new method to better take advantage of these similarities,... | Minh Quang Pham, François Yvon, Josep Maria Crego |  |
| 713 |  |  [ATP: AMRize Then Parse! Enhancing AMR Parsing with PseudoAMRs](https://doi.org/10.18653/v1/2022.findings-naacl.190) |  | 0 | As Abstract Meaning Representation (AMR) implicitly involves compound semantic annotations, we hypothesize auxiliary tasks which are semantically or formally related can better enhance AMR parsing. We find that 1) Semantic role labeling (SRL) and dependency parsing (DP), would bring more performance gain than other tasks e.g. MT and summarization in the text-to-AMR transition even with much less data. 2) To make a better fit for AMR, data from auxiliary tasks should be properly “AMRized” to... | Liang Chen, Peiyi Wang, Runxin Xu, Tianyu Liu, Zhifang Sui, Baobao Chang |  |
| 714 |  |  [TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning](https://doi.org/10.18653/v1/2022.findings-naacl.191) |  | 0 | Masked language models (MLMs) such as BERT have revolutionized the field of Natural Language Understanding in the past few years. However, existing pre-trained MLMs often output an anisotropic distribution of token representations that occupies a narrow subset of the entire representation space. Such token representations are not ideal, especially for tasks that demand discriminative semantic meanings of distinct tokens. In this work, we propose TaCL (Token-aware Contrastive Learning), a novel... | Yixuan Su, Fangyu Liu, Zaiqiao Meng, Tian Lan, Lei Shu, Ehsan Shareghi, Nigel Collier |  |
| 715 |  |  [MTG: A Benchmark Suite for Multilingual Text Generation](https://doi.org/10.18653/v1/2022.findings-naacl.192) |  | 0 | We introduce MTG, a new benchmark suite for training and evaluating multilingual text generation. It is the first-proposed multilingual multiway text generation dataset with the largest human-annotated data (400k). It includes four generation tasks (story generation, question generation, title generation and text summarization) across five languages (English, German, French, Spanish and Chinese). The multiway setup enables testing knowledge transfer capabilities for a model across languages and... | Yiran Chen, Zhenqiao Song, Xianze Wu, Danqing Wang, Jingjing Xu, Jiaze Chen, Hao Zhou, Lei Li |  |
| 716 |  |  [Weakly Supervised Text-to-SQL Parsing through Question Decomposition](https://doi.org/10.18653/v1/2022.findings-naacl.193) |  | 0 | Text-to-SQL parsers are crucial in enabling non-experts to effortlessly query relational data. Training such parsers, by contrast, generally requires expertise in annotating natural language (NL) utterances with corresponding SQL queries. In this work, we propose a weak supervision approach for training text-to-SQL parsers. We take advantage of the recently proposed question meaning representation called QDMR, an intermediate between NL and formal query languages. Given questions, their QDMR... | Tomer Wolfson, Daniel Deutch, Jonathan Berant |  |
| 717 |  |  [Detect Rumors in Microblog Posts for Low-Resource Domains via Adversarial Contrastive Learning](https://doi.org/10.18653/v1/2022.findings-naacl.194) |  | 0 | Massive false rumors emerging along with breaking news or trending topics severely hinder the truth. Existing rumor detection approaches achieve promising performance on the yesterday’s news, since there is enough corpus collected from the same domain for model training. However, they are poor at detecting rumors about unforeseen events especially those propagated in minority languages due to the lack of training data and prior knowledge (i.e., low-resource regimes). In this paper, we propose... | Hongzhan Lin, Jing Ma, Liangliang Chen, Zhiwei Yang, Mingfei Cheng, Guang Chen |  |
| 718 |  |  [DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-naacl.195) |  | 0 | Task-oriented dialogue generation is challenging since the underlying knowledge is often dynamic and effectively incorporating knowledge into the learning process is hard. It is particularly challenging to generate both human-like and informative responses in this setting. Recent research primarily focused on various knowledge distillation methods where the underlying relationship between the facts in a knowledge base is not effectively captured. In this paper, we go one step further and... | Md. Rashad Al Hasan Rony, Ricardo Usbeck, Jens Lehmann |  |
| 719 |  |  [Zero-Shot Event Detection Based on Ordered Contrastive Learning and Prompt-Based Prediction](https://doi.org/10.18653/v1/2022.findings-naacl.196) |  | 0 | Event detection is a classic natural language processing task. However, the constantly emerging new events make supervised methods not applicable to unseen types. Previous zero-shot event detection methods either require predefined event types as heuristic rules or resort to external semantic analyzing tools. To overcome this weakness, we propose an end-to-end framework named Zero-Shot Event Detection Based on Ordered Contrastive Learning and Prompt-Based Prediction (ZEOP). By creatively... | Senhui Zhang, Tao Ji, Wendi Ji, Xiaoling Wang |  |
| 720 |  |  [KETOD: Knowledge-Enriched Task-Oriented Dialogue](https://doi.org/10.18653/v1/2022.findings-naacl.197) |  | 0 | Existing studies in dialogue system research mostly treat task-oriented dialogue and chit-chat as separate domains. Towards building a human-like assistant that can converse naturally and seamlessly with users, it is important to build a dialogue system that conducts both types of conversations effectively. In this work, we investigate how task-oriented dialogue and knowledge-grounded chit-chat can be effectively integrated into a single model. To this end, we create a new dataset, KETOD... | Zhiyu Chen, Bing Liu, Seungwhan Moon, Chinnadhurai Sankar, Paul A. Crook, William Yang Wang |  |
| 721 |  |  [TANet: Thread-Aware Pretraining for Abstractive Conversational Summarization](https://doi.org/10.18653/v1/2022.findings-naacl.198) |  | 0 | Although pre-trained language models (PLMs) have achieved great success and become a milestone in NLP, abstractive conversational summarization remains a challenging but less studied task. The difficulty lies in two aspects. One is the lack of large-scale conversational summary data. Another is that applying the existing pre-trained models to this task is tricky because of the structural dependence within the conversation and its informal expression, etc. In this work, we first build a... | Ze Yang, Christian Wang, Zhoujin Tian, Wei Wu, Zhoujun Li |  |
| 722 |  |  [AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks](https://doi.org/10.18653/v1/2022.findings-naacl.199) |  | 0 | Transformer-based pre-trained models with millions of parameters require large storage. Recent approaches tackle this shortcoming by training adapters, but these approaches still require a relatively large number of parameters. In this study, AdapterBias, a surprisingly simple yet effective adapter architecture, is proposed. AdapterBias adds a token-dependent shift to the hidden output of transformer layers to adapt to downstream tasks with only a vector and a linear layer. Extensive... | ChinLun Fu, ZihChing Chen, YunRu Lee, Hungyi Lee |  |
| 723 |  |  [Bridging the Gap between Training and Inference: Multi-Candidate Optimization for Diverse Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-naacl.200) |  | 0 | Diverse NMT aims at generating multiple diverse yet faithful translations given a source sentence. In this paper, we investigate a common shortcoming in existing diverse NMT studies: the model is usually trained with single reference, while expected to generate multiple candidate translations in inference. The discrepancy between training and inference enlarges the confidence variance and quality gap among candidate translations and thus hinders model performance. To deal with this defect, we... | Huan Lin, Baosong Yang, Liang Yao, Dayiheng Liu, Haibo Zhang, Jun Xie, Min Zhang, Jinsong Su |  |
| 724 |  |  [Learning from Bootstrapping and Stepwise Reinforcement Reward: A Semi-Supervised Framework for Text Style Transfer](https://doi.org/10.18653/v1/2022.findings-naacl.201) |  | 0 | Text style transfer is an important task in controllable language generation. Supervised approaches have pushed performance improvement on style-oriented rewriting such as formality conversion. However, challenges remain due to the scarcity of large-scale parallel data in many domains. While unsupervised approaches do not rely on annotated sentence pairs for each style, they are often plagued with instability issues such as mode collapse or quality degradation. To take advantage of both... | Zhengyuan Liu, Nancy F. Chen |  |
| 725 |  |  [EA²E: Improving Consistency with Event Awareness for Document-Level Argument Extraction](https://doi.org/10.18653/v1/2022.findings-naacl.202) |  | 0 | Events are inter-related in documents. Motivated by the one-sense-per-discourse theory, we hypothesize that a participant tends to play consistent roles across multiple events in the same document. However recent work on document-level event argument extraction models each individual event in isolation and therefore causes inconsistency among extracted arguments across events, which will further cause discrepancy for downstream applications such as event knowledge base population, question... | Qi Zeng, Qiusi Zhan, Heng Ji |  |
| 726 |  |  [Label Refinement via Contrastive Learning for Distantly-Supervised Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-naacl.203) |  | 0 | Distantly-supervised named entity recognition (NER) locates and classifies entities using only knowledge bases and unlabeled corpus to mitigate the reliance on human-annotated labels. The distantly annotated data suffer from the noise in labels, and previous works on DSNER have proved the importance of pre-refining distant labels with hand-crafted rules and extra existing semantic information. In this work, we explore the way to directly learn the distant label refinement knowledge by imitating... | Huaiyuan Ying, Shengxuan Luo, Tiantian Dang, Sheng Yu |  |
| 727 |  |  [Negative Sample is Negative in Its Own Way: Tailoring Negative Sentences for Image-Text Retrieval](https://doi.org/10.18653/v1/2022.findings-naacl.204) |  | 0 | Matching model is essential for Image-Text Retrieval framework. Existing research usually train the model with a triplet loss and explore various strategy to retrieve hard negative sentences in the dataset. We argue that current retrieval-based negative sample construction approach is limited in the scale of the dataset thus fail to identify negative sample of high difficulty for every image. We propose our TAiloring neGative Sentences with Discrimination and Correction (TAGS-DC) to generate... | Zhihao Fan, Zhongyu Wei, Zejun Li, Siyuan Wang, Xuanjing Huang, Jianqing Fan |  |
| 728 |  |  [Explore More Guidance: A Task-aware Instruction Network for Sign Language Translation Enhanced with Data Augmentation](https://doi.org/10.18653/v1/2022.findings-naacl.205) |  | 0 | Sign language recognition and translation first uses a recognition module to generate glosses from sign language videos and then employs a translation module to translate glosses into spoken sentences. Most existing works focus on the recognition step, while paying less attention to sign language translation. In this work, we propose a task-aware instruction network, namely TIN-SLT, for sign language translation, by introducing the isntruction module and the learning-based feature fuse strategy... | Yong Cao, Wei Li, Xianzhi Li, Min Chen, Guangyong Chen, Long Hu, Zhengdao Li, Kai Hwang |  |
| 729 |  |  [RoViST: Learning Robust Metrics for Visual Storytelling](https://doi.org/10.18653/v1/2022.findings-naacl.206) |  | 0 | Visual storytelling (VST) is the task of generating a story paragraph that describes a given image sequence. Most existing storytelling approaches have evaluated their models using traditional natural language generation metrics like BLEU or CIDEr. However, such metrics based on n-gram matching tend to have poor correlation with human evaluation scores and do not explicitly consider other criteria necessary for storytelling such as sentence structure or topic coherence. Moreover, a single score... | Eileen Wang, Soyeon Caren Han, Josiah Poon |  |
| 730 |  |  [Query2Particles: Knowledge Graph Reasoning with Particle Embeddings](https://doi.org/10.18653/v1/2022.findings-naacl.207) |  | 0 | Answering complex logical queries on incomplete knowledge graphs (KGs) with missing edges is a fundamental and important task for knowledge graph reasoning. The query embedding method is proposed to answer these queries by jointly encoding queries and entities to the same embedding space. Then the answer entities are selected according to the similarities between the entity embeddings and the query embedding. As the answers to a complex query are obtained from a combination of logical... | Jiaxin Bai, Zihao Wang, Hongming Zhang, Yangqiu Song |  |
| 731 |  |  [ID10M: Idiom Identification in 10 Languages](https://doi.org/10.18653/v1/2022.findings-naacl.208) |  | 0 | Idioms are phrases which present a figurative meaning that cannot be (completely) derived by looking at the meaning of their individual components. Identifying and understanding idioms in context is a crucial goal and a key challenge in a wide range of Natural Language Understanding tasks. Although efforts have been undertaken in this direction, the automatic identification and understanding of idioms is still a largely under-investigated area, especially when operating in a multilingual... | Simone Tedeschi, Federico Martelli, Roberto Navigli |  |
| 732 |  |  [Cross-Domain Classification of Moral Values](https://doi.org/10.18653/v1/2022.findings-naacl.209) |  | 0 | Moral values influence how we interpret and act upon the information we receive. Identifying human moral values is essential for artificially intelligent agents to co-exist with humans. Recent progress in natural language processing allows the identification of moral values in textual discourse. However, domain-specific moral rhetoric poses challenges for transferring knowledge from one domain to another. We provide the first extensive investigation on the effects of cross-domain classification... | Enrico Liscio, Alin Dondera, Andrei Geadau, Catholijn M. Jonker, Pradeep K. Murukannaiah |  |
