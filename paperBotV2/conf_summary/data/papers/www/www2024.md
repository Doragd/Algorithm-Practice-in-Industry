# WWW2024

## 会议论文列表

本会议共有 766 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [PRINT: Personalized Relevance Incentive Network for CTR Prediction in Sponsored Search](https://doi.org/10.1145/3589335.3648316) |  | 0 | Click-Through Rate (CTR) prediction plays a critical role in sponsored search. Modeling the semantic relevance between queries and ads is one of the most crucial factors affecting the performance of CTR prediction. However, different users have different sensitivities to semantic relevance due to their personalized relevance preferences. Therefore, semantic relevance may have different incentives on the user's click probability (i.e., stimulative incentive, inhibitive incentive, or irrelevant incentive). Unfortunately, few works have studied the phenomenon, which ignores the complicated incentive effects of semantic relevance and limits the performance of CTR prediction. To this end, we propose a novel Personalized Relevance Incentive N eTwork (PRINT for short) to explicitly model the personalized incentives of query-ad semantic relevance on user's click probability. Specifically, we introduce a User Relevance Preference Module (usertask) to extract the user's personalized relevance preference from historical query-ad interacted sequence. Then, a RElevance Incentive Module (REIM) is designed to discern three incentive types and model the personalized incentive effects on CTR prediction. Experiments on public datasets and industrial datasets demonstrate the significant improvement of our PRINT. Furthermore, PRINT is also deployed in the sponsored search advertising system in Meituan, obtaining an improvement of 1.94% and 2.29% in CTR and Cost Per Mile (CPM) respectively. We publish the source code at https://anonymous.4open.science/r/PRINT-D365/. | Zhaolin Hong, Haitao Wang, Chengjie Qian, Wei Chen, Tianqi He, Yajie Zou, Qiang Liu, Xingxing Wang |  |
| 2 |  |  [Dynamic Search Results Re-ranking Method by Advertisement Relevance Feedback based on Users' Unconscious Expectations for Listing Advertisement](https://doi.org/10.1145/3589335.3651495) |  | 0 | Search results hold paramount importance for both users and advertisers. However, re-ranking results based on the timing of user clicks on listing advertisements poses a considerable challenge. In this study, we introduce a dynamic re-ranking method to re-rank search results triggered by the viewing of listing advertisements containing search terms after the initial presentation of search results. The proposed method leverages relevance feedback to enhance search results, providing valuable support to users in their searches. We conducted a comparative verification of accuracy rates between methods with and without the added weight. The results suggest that considering the timings of clicking on listing ads for re-ranking results can more effectively reflect user interest compared to approaches that don't take this timing into account. This study contributes to the advancement of search result presentation strategies by incorporating dynamic user behavior considerations. | Da Li, Shigenaga Hamaguchi, Ruman Suyama, Shinsuke Nakajima, Yukiko Kawai |  |
| 3 |  |  [Adversarial-Enhanced Causal Multi-Task Framework for Debiasing Post-Click Conversion Rate Estimation](https://doi.org/10.1145/3589334.3645379) |  | 0 | In real-world industrial scenarios, post-click conversion rate (CVR) prediction models are trained offline based on click events and subsequently applied online to both clicked and unclicked events. Unfortunately, unclicked events are inevitably difficult to estimate due to user self-selection, which leads to a degradation of CVR prediction accuracy. In order to estimate the prediction of unclicked events, the current mainstream Doubly Robust (DR) estimators introduce the concept of imputed errors. However, inaccuracies in imputed errors can increase the uncertainty in the generalization bound of CVR predictions, consequently resulting in a decline in the CVR prediction accuracy. To challenge this issue, we first present a theoretical analysis of the bias and variance inherent in DR estimators and then introduce a novel causal estimator that seeks to strike a balance between bias and variance within the DR framework, thus optimizing the learning of the imputation model in a more robust manner. Additionally, drawing inspiration from adversarial learning techniques, we propose a novel dual adversarial component, which learns from both the space level and the task level to eliminate the causal influence of input features on the CTR task (i.e., the click propensity), with the goal of achieving unbiased estimations. Our extensive experimental evaluations, conducted on both the widely used benchmark and the real-world large-scale Internet giant platform, convincingly demonstrate the effectiveness of our proposed scheme. Besides, we have released a high-quality industrial dataset named Tenc-UnionAds used for selection bias research in the advertising field. | Xinyue Zhang, Cong Huang, Kun Zheng, Hongzu Su, Tianxu Ji, Wei Wang, Hongkai Qi, Jingjing Li |  |
| 4 |  |  [Enhancing Sequential Recommendation via LLM-based Semantic Embedding Learning](https://doi.org/10.1145/3589335.3648307) |  | 0 | Sequential recommendation systems (SRS) are crucial in various applications as they enable users to discover relevant items based on their past interactions. Recent advancements involving large language models (LLMs) have shown significant promise in addressing intricate recommendation challenges. However, these efforts exhibit certain limitations. Specifically, directly extracting representations from an LLM based on items' textual features and feeding them into a sequential model hold no guarantee that the semantic information of texts could be preserved in these representations. Additionally, concatenating textual descriptions of all items in an item sequence into a long text and feeding it into an LLM for recommendation results in lengthy token sequences, which largely diminishes the practical efficiency. In this paper, we introduce SAID, a framework that utilizes LLMs to explicitly learn Semantically Aligned item ID embeddings based on texts. For each item, SAID employs a projector module to transform an item ID into an embedding vector, which will be fed into an LLM to elicit the exact descriptive text tokens accompanied by the item. The item embeddings are forced to preserve fine-grained semantic information of textual descriptions. Further, the learned embeddings can be integrated with lightweight downstream sequential models for practical recommendations. In this way, SAID circumvents lengthy token sequences in previous works, reducing resources required in industrial scenarios and also achieving superior recommendation performance. Experiments on six public datasets demonstrate that SAID outperforms baselines by about 5% to 15% in terms of NDCG@10. Moreover, SAID has been deployed in Alipay's online advertising platform, achieving a 3.07% relative improvement of cost per mille (CPM) over baselines, with an online response time of under 20 milliseconds. | Jun Hu, Wenwen Xia, Xiaolu Zhang, Chilin Fu, Weichang Wu, Zhaoxin Huan, Ang Li, Zuoli Tang, Jun Zhou |  |
| 5 |  |  [OmniSearchSage: Multi-Task Multi-Entity Embeddings for Pinterest Search](https://doi.org/10.1145/3589335.3648309) |  | 0 | In this paper, we present OmniSearchSage, a versatile and scalable system for understanding search queries, pins, and products for Pinterest search. We jointly learn a unified query embedding coupled with pin and product embeddings, leading to an improvement of >8% relevance, >7% engagement, and >5% ads CTR in Pinterest's production search system. The main contributors to these gains are improved content understanding, better multi-task learning, and real-time serving. We enrich our entity representations using diverse text derived from image captions from a generative LLM, historical engagement, and user-curated boards. Our multitask learning setup produces a single search query embedding in the same space as pin and product embeddings and compatible with pre-existing pin and product embeddings. We show the value of each feature through ablation studies, and show the effectiveness of a unified model compared to standalone counterparts. Finally, we share how these embeddings have been deployed across the Pinterest search stack, from retrieval to ranking, scaling to serve 300k requests per second at low latency. Our implementation of this work is available at https://github.com/pinterest/atg-research/tree/main/omnisearchsage. | Prabhat Agarwal, Minhazul Islam SK, Nikil Pancha, Kurchi Subhra Hazra, Jiajing Xu, Chuck Rosenberg |  |
| 6 |  |  [ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation](https://doi.org/10.1145/3589334.3645467) |  | 0 | With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data quality of testing samples, which greatly reduces the difficulty for LLMs to extract the essential knowledge from user behavior sequences. As for few-shot recommendation, we further design retrieval-enhanced instruction tuning (ReiT) by adopting SUBR as a data augmentation technique for training samples. Specifically, we develop a mixed training dataset consisting of both the original data samples and their retrieval-enhanced counterparts. We conduct extensive experiments on three real-world public datasets to demonstrate the superiority of ReLLa compared with existing baseline models, as well as its capability for lifelong sequential behavior comprehension. To be highlighted, with only less than 10 traditional CTR models that are trained on the entire training set (e.g., DCNv2, DIN, SIM). The code is available <https://github.com/LaVieEnRose365/ReLLa>. | Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, Weinan Zhang |  |
| 7 |  |  [Large Language Model based Long-tail Query Rewriting in Taobao Search](https://doi.org/10.1145/3589335.3648298) |  | 0 | In the realm of e-commerce search, the significance of semantic matching cannot be overstated, as it directly impacts both user experience and company revenue. Along this line, query rewriting, serving as an important technique to bridge the semantic gaps inherent in the semantic matching process, has attached wide attention from the industry and academia. However, existing query rewriting methods often struggle to effectively optimize long-tail queries and alleviate the phenomenon of "few-recall" caused by semantic gap. In this paper, we present BEQUE, a comprehensive framework that Bridges the sEmantic gap for long-tail QUEries. In detail, BEQUE comprises three stages: multi-instruction supervised fine tuning (SFT), offline feedback, and objective alignment. We first construct a rewriting dataset based on rejection sampling and auxiliary tasks mixing to fine-tune our large language model (LLM) in a supervised fashion. Subsequently, with the well-trained LLM, we employ beam search to generate multiple candidate rewrites, and feed them into Taobao offline system to obtain the partial order. Leveraging the partial order of rewrites, we introduce a contrastive learning method to highlight the distinctions between rewrites, and align the model with the Taobao online objectives. Offline experiments prove the effectiveness of our method in bridging semantic gap. Online A/B tests reveal that our method can significantly boost gross merchandise volume (GMV), number of transaction (#Trans) and unique visitor (UV) for long-tail queries. BEQUE has been deployed on Taobao, one of most popular online shopping platforms in China, since October 2023. | Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, Derong Xu, Tong Xu, Enhong Chen |  |
| 8 |  |  [An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce](https://doi.org/10.1145/3589335.3648318) |  | 0 | The problem of search relevance in the E-commerce domain is a challenging one since it involves understanding the intent of a user's short nuanced query and matching it with the appropriate products in the catalog. This problem has traditionally been addressed using language models (LMs) and graph neural networks (GNNs) to capture semantic and inter-product behavior signals, respectively. However, the rapid development of new architectures has created a gap between research and the practical adoption of these techniques. Evaluating the generalizability of these models for deployment requires extensive experimentation on complex, real-world datasets, which can be non-trivial and expensive. Furthermore, such models often operate on latent space representations that are incomprehensible to humans, making it difficult to evaluate and compare the effectiveness of different models. This lack of interpretability hinders the development and adoption of new techniques in the field. To bridge this gap, we propose Plug and Play Graph LAnguage Model (PP-GLAM), an explainable ensemble of plug and play models. Our approach uses a modular framework with uniform data processing pipelines. It employs additive explanation metrics to independently decide whether to include (i) language model candidates, (ii) GNN model candidates, and (iii) inter-product behavioral signals. For the task of search relevance, we show that PP-GLAM outperforms several state-of-the-art baselines as well as a proprietary model on real-world multilingual, multi-regional e-commerce datasets. To promote better model comprehensibility and adoption, we also provide an analysis of the explainability and computational complexity of our model. We also provide the public codebase and provide a deployment strategy for practical implementation. | Nurendra Choudhary, Edward W. Huang, Karthik Subbian, Chandan K. Reddy |  |
| 9 |  |  [Hierarchical Query Classification in E-commerce Search](https://doi.org/10.1145/3589335.3648332) |  | 0 | E-commerce platforms typically store and structure product information and search data in a hierarchy. Efficiently categorizing user search queries into a similar hierarchical structure is paramount in enhancing user experience on e-commerce platforms as well as news curation and academic research. The significance of this task is amplified when dealing with sensitive query categorization or critical information dissemination, where inaccuracies can lead to considerable negative impacts. The inherent complexity of hierarchical query classification is compounded by two primary challenges: (1) the pronounced class imbalance that skews towards dominant categories, and (2) the inherent brevity and ambiguity of search queries that hinder accurate classification. To address these challenges, we introduce a novel framework that leverages hierarchical information through (i) enhanced representation learning that utilizes the contrastive loss to discern fine-grained instance relationships within the hierarchy, called ”instance hierarchy”, and (ii) a nuanced hierarchical classification loss that attends to the intrinsic label taxonomy, named ”label hierarchy”. Additionally, based on our observation that certain unlabeled queries share typographical similarities with labeled queries, we propose a neighborhood-aware sampling technique to intelligently select these unlabeled queries to boost the classification performance. Extensive experiments demonstrate that our proposed method is better than state-of-the-art (SOTA) on the proprietary Amazon dataset, and comparable to SOTA on the public datasets of Web of Science and RCV1-V2. These results underscore the efficacy of our proposed solution, and pave the path toward the next generation of hierarchy-aware query classification systems. | Bing He, Sreyashi Nag, Limeng Cui, Suhang Wang, Zheng Li, Rahul Goutam, Zhen Li, Haiyang Zhang |  |
| 10 |  |  [Detecting Generated Native Ads in Conversational Search](https://doi.org/10.1145/3589335.3651489) |  | 0 | Conversational search engines such as YouChat and Microsoft Copilot use large language models (LLMs) to generate answers to queries. It is only a small step to also use this technology to generate and integrate advertising within these answers - instead of placing ads separately from the organic search results. This type of advertising is reminiscent of native advertising and product placement, both of which are very effective forms of subtle and manipulative advertising. It is likely that information seekers will be confronted with such use of LLM technology in the near future, especially when considering the high computational costs associated with LLMs, for which providers need to develop sustainable business models. This paper investigates whether LLMs can also be used as a countermeasure against generated native ads, i.e., to block them. For this purpose we compile a large dataset of ad-prone queries and of generated answers with automatically integrated ads to experiment with fine-tuned sentence transformers and state-of-the-art LLMs on the task of recognizing the ads. In our experiments sentence transformers achieve detection precision and recall values above 0.9, while the investigated LLMs struggle with the task. | Sebastian Schmidt, Ines Zelch, Janek Bevendorff, Benno Stein, Matthias Hagen, Martin Potthast |  |
| 11 |  |  [Recall-Augmented Ranking: Enhancing Click-Through Rate Prediction Accuracy with Cross-Stage Data](https://doi.org/10.1145/3589335.3651551) |  | 0 | Click-through rate (CTR) prediction plays an indispensable role in online platforms. Numerous models have been proposed to capture users' shifting preferences by leveraging user behavior sequences. However, these historical sequences often suffer from severe homogeneity and scarcity compared to the extensive item pool. Relying solely on such sequences for user representations is inherently restrictive, as user interests extend beyond the scope of items they have previously engaged with. To address this challenge, we propose a data-driven approach to enrich user representations. We recognize user profiling and recall items as two ideal data sources within the cross-stage framework, encompassing the u2u (user-to-user) and i2i (item-to-item) aspects respectively. In this paper, we propose a novel architecture named Recall-Augmented Ranking (RAR). RAR consists of two key sub-modules, which synergistically gather information from a vast pool of look-alike users and recall items, resulting in enriched user representations. Notably, RAR is orthogonal to many existing CTR models, allowing for consistent performance improvements in a plug-and-play manner. Extensive experiments are conducted, which verify the efficacy and compatibility of RAR against the SOTA methods. | Junjie Huang, Guohao Cai, Jieming Zhu, Zhenhua Dong, Ruiming Tang, Weinan Zhang, Yong Yu |  |
| 12 |  |  [RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction](https://doi.org/10.1145/3589335.3651550) |  | 0 | Predicting click-through rates (CTR) is a fundamental task for Web applications, where a key issue is to devise effective models for feature interactions. Current methodologies predominantly concentrate on modeling feature interactions within an individual sample, while overlooking the potential cross-sample relationships that can serve as a reference context to enhance the prediction. To make up for such deficiency, this paper develops a Retrieval-Augmented Transformer (RAT), aiming to acquire fine-grained feature interactions within and across samples. By retrieving similar samples, we construct augmented input for each target sample. We then build Transformer layers with cascaded attention to capture both intra- and cross-sample feature interactions, facilitating comprehensive reasoning for improved CTR prediction while retaining efficiency. Extensive experiments on real-world datasets substantiate the effectiveness of RAT and suggest its advantage in long-tail scenarios. The code has been open-sourced at <https://github.com/YushenLi807/WWW24-RAT>. | Yushen Li, Jinpeng Wang, Tao Dai, Jieming Zhu, Jun Yuan, Rui Zhang, ShuTao Xia |  |
| 13 |  |  [Personalized Ordering of Recommendation-Modules on an E-Commerce Homepage](https://doi.org/10.1145/3589335.3651545) |  | 0 | The homepage of an E-Commerce website may accommodate multiple and diverse recommendation modules; with each module is designed to cover some facet of the user's needs. Commonly, the recommendation modules are ordered in the same way for all homepage users, which leads to a sub-optimal user experience. In this work, we present a novel personalized module ordering solution that provides a more educated way to determine an ordering of the homepage modules based on historical user-interactions. Overall, we evaluate our solution and demonstrate its merits. | Haggai Roitman, Alexander Nus, Yotam Eshel |  |
| 14 |  |  [Retrieval-augmented Query Reformulation for Heterogeneous Research Asset Retrieval in Virtual Research Environment](https://doi.org/10.1145/3589335.3651553) |  | 0 | Discovering and reusing research assets such as datasets and computational notebooks is crucial for building research workflows in data-centric studies. The rapid growth of research assets in scientific communities provides scientists with great opportunities to enhance research efficacy but also poses significant challenges in finding suitable materials for specific tasks. Scientists, especially those focusing on cross-disciplinary research, often find it difficult to formulate effective queries to retrieve desired resources. Previous work has proposed query reformulation methods to increase the efficiency of research asset search. However, it relies on existent knowledge graphs and is constrained to computational notebooks only. As research assets utilized by data analytic workflows are in essence heterogeneous, i.e., of distinct kinds and from diversified sources, query reformulation methods in this regard should consider the relationship between different types of research assets. To address the above challenges, we propose a retrieval-augmented query reformulation method for heterogeneous research asset retrieval. It is developed in the context of a Notebook-based virtual research environment (VRE) and offers query reformulation services to other VRE components. We demonstrate the effectiveness of the proposed query reformulation service with experiments on dataset and notebook retrieval. Up till now, we have indexed 8,954 datasets and 18,158 notebooks. The experimental results show that the proposed service can create useful query suggestions. | Peide Zhu, Na Li, Zhiming Zhao |  |
| 15 |  |  [List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation](https://doi.org/10.1145/3589334.3645336) |  | 0 | The results of information retrieval (IR) are usually presented in the form of a ranked list of candidate documents, such as web search for humans and retrieval-augmented generation for large language models (LLMs). List-aware retrieval aims to capture the list-level contextual features to return a better list, mainly including reranking and truncation. Reranking finely re-scores the documents in the list. Truncation dynamically determines the cut-off point of the ranked list to achieve the trade-off between overall relevance and avoiding misinformation from irrelevant documents. Previous studies treat them as two separate tasks and model them separately. However, the separation is not optimal. First, it is hard to share the contextual information of the ranking list between the two tasks. Second, the separate pipeline usually meets the error accumulation problem, where the small error from the reranking stage can largely affect the truncation stage. To solve these problems, we propose a Reranking-Truncation joint model (GenRT) that can perform the two tasks concurrently. GenRT integrates reranking and truncation via generative paradigm based on encoder-decoder architecture. We also design the novel loss functions for joint optimization to make the model learn both tasks. Sharing parameters by the joint model is conducive to making full use of the common modeling information of the two tasks. Besides, the two tasks are performed concurrently and co-optimized to solve the error accumulation problem between separate stages. Experiments on public learning-to-rank benchmarks and open-domain Q&A tasks show that our method achieves SOTA performance on both reranking and truncation tasks for web search and retrieval-augmented LLMs. | Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, Xueqi Cheng |  |
| 16 |  |  [Query in Your Tongue: Reinforce Large Language Models with Retrievers for Cross-lingual Search Generative Experience](https://doi.org/10.1145/3589334.3645701) |  | 0 | In the contemporary digital landscape, search engines play an invaluable role in information access, yet they often face challenges in Cross-Lingual Information Retrieval (CLIR). Though attempts are made to improve CLIR, current methods still leave users grappling with issues such as misplaced named entities and lost cultural context when querying in non-native languages. While some advances have been made using Neural Machine Translation models and cross-lingual representation, these are not without limitations. Enter the paradigm shift brought about by Large Language Models (LLMs), which have transformed search engines from simple retrievers to generators of contextually relevant information. This paper introduces the Multilingual Information Model for Intelligent Retrieval (MIMIR). Built on the power of LLMs, MIMIR directly responds in the language of the user's query, reducing the need for post-search translations. Our model's architecture encompasses a dual-module system: a retriever for searching multilingual documents and a responder for crafting answers in the user's desired language. Through a unique unified training framework, with the retriever serving as a reward model supervising the responder, and in turn, the responder producing synthetic data to refine the retriever's proficiency, MIMIR's retriever and responder iteratively enhance each other. Performance evaluations via CLEF and MKQA benchmarks reveal MIMIR's superiority over existing models, effectively addressing traditional CLIR challenges. | Ping Guo, Yue Hu, Yanan Cao, Yubing Ren, Yunpeng Li, Heyan Huang |  |
| 17 |  |  [UnifiedSSR: A Unified Framework of Sequential Search and Recommendation](https://doi.org/10.1145/3589334.3645427) |  | 0 | In this work, we propose a Unified framework of Sequential Search and Recommendation (UnifiedSSR) for joint learning of user behavior history in both search and recommendation scenarios. Specifically, we consider user-interacted products in the recommendation scenario, user-interacted products and user-issued queries in the search scenario as three distinct types of user behaviors. We propose a dual-branch network to encode the pair of interacted product history and issued query history in the search scenario in parallel. This allows for cross-scenario modeling by deactivating the query branch for the recommendation scenario. Through the parameter sharing between dual branches, as well as between product branches in two scenarios, we incorporate cross-view and cross-scenario associations of user behaviors, providing a comprehensive understanding of user behavior patterns. To further enhance user behavior modeling by capturing the underlying dynamic intent, an Intent-oriented Session Modeling module is designed for inferring intent-oriented semantic sessions from the contextual information in behavior sequences. In particular, we consider self-supervised learning signals from two perspectives for intent-oriented semantic session locating, which encourage session discrimination within each behavior sequence and session alignment between dual behavior sequences. Extensive experiments on three public datasets demonstrate that UnifiedSSR consistently outperforms state-of-the-art methods for both search and recommendation. | Jiayi Xie, Shang Liu, Gao Cong, Zhenzhong Chen |  |
| 18 |  |  [Scaling User Modeling: Large-scale Online User Representations for Ads Personalization in Meta](https://doi.org/10.1145/3589335.3648301) |  | 0 | Effective user representations are pivotal in personalized advertising. However, stringent constraints on training throughput, serving latency, and memory, often limit the complexity and input feature set of online ads ranking models. This challenge is magnified in extensive systems like Meta's, which encompass hundreds of models with diverse specifications, rendering the tailoring of user representation learning for each model impractical. To address these challenges, we present Scaling User Modeling (SUM), a framework widely deployed in Meta's ads ranking system, designed to facilitate efficient and scalable sharing of online user representation across hundreds of ads models. SUM leverages a few designated upstream user models to synthesize user embeddings from massive amounts of user features with advanced modeling techniques. These embeddings then serve as inputs to downstream online ads ranking models, promoting efficient representation sharing. To adapt to the dynamic nature of user features and ensure embedding freshness, we designed SUM Online Asynchronous Platform (SOAP), a latency free online serving system complemented with model freshness and embedding stabilization, which enables frequent user model updates and online inference of user embeddings upon each user request. We share our hands-on deployment experiences for the SUM framework and validate its superiority through comprehensive experiments. To date, SUM has been launched to hundreds of ads ranking models in Meta, processing hundreds of billions of user requests daily, yielding significant online metric gains and infrastructure cost savings. | Wei Zhang, Dai Li, Chen Liang, Fang Zhou, Zhongke Zhang, Xuewei Wang, Ru Li, Yi Zhou, Yaning Huang, Dong Liang, Kai Wang, Zhangyuan Wang, Zhengxing Chen, Fenggang Wu, Minghai Chen, Huayu Li, Yunnan Wu, Zhan Shu, Mindi Yuan, Sri Reddy |  |
| 19 |  |  [Finding What Users Look for by Attribute-Aware Personalized Item Comparison in Relevant Recommendation](https://doi.org/10.1145/3589335.3651508) |  | 0 | Relevant recommendation is a distinctive recommendation scenario in e-commerce platforms, which provides an extended set of items that are relevant to the trigger item (the item that triggers the relevant recommendation). Different from the general recommendations whose item feeds are diversified, relevant recommendation regards the trigger item as a key component. From one perspective, the trigger item reveals users' current interests and determines the range of the recommendation results. From the other perspective, users may have the mindset to look for items that have directional attribute differences from the trigger item. In this paper, we present an attribute-aware personalized item comparison framework. Under this framework, an item subtraction module is first applied over the trigger item and the candidate item, which calculates their directional difference with consideration of their intrinsic similarity. Then two modules are used to estimate users' preference for this current item pair: one learns the collective preference of all users, and the other learns the current user's personal evolutional preference. Experiments on a CTR prediction task over both a public dataset and an industrial dataset from our shopping app show that the proposed method outperforms the state-of-the-art algorithms and also achieves better generalization ability. | Rui Ma, Dike Sun, Jincheng Xu, Jingsong Yuan, Jiandong Zhang |  |
| 20 |  |  [De-Anchor: Mitigating Attention Polarization for Lifelong User Behavior Modeling in Click-Through Rate Prediction](https://doi.org/10.1145/3589335.3651486) |  | 0 | User lifelong behavior sequences are essential for click-through rate (CTR) prediction tasks in industrial recommender systems. Attention-based module, especially multi-head target attention (MHTA), has been proven to be effective in aggregating behavior features given a certain target item. However, we found a common phenomenon that attention weights in MHTA tend to over-concentrate on merely a small subset of a user's historical behaviors, producing a sparse one-hot distributed attention weights in training gradually, which we callAttention Polarization (AP). These polarized weights on certain behaviors (which we call "\textitattention anchor ") could make the model fail to capture a user's diversified interests, and harm the learning of behavior embedding as the gradients on these features are nearly zero. We introduce two indicators:anchor rate andattention entropy to measure the magnitude of AP, and proposeDe-Anchor, a novel method to alleviate it, which can serve as a stand-alone and parameter-efficient plug-in to existing CTR backbones. De-Anchor contains two modules:Anchor-aware gradient dropout (AGD) forcing the model to capture diversified interest information from behavior sequences by discarding gradients of non-behavior features, andTarget-aware attention anchor (TAA) providing a pseudo behavior to offload excessive weights of MHTA. Extensive offline experiments and industrial online A/B tests demonstrate the efficacy of our method. | Hongzun Liu, Kang Yin, Tianyu Sun, Rui Huang, Yunsong Li, Xiao Fang, Zhaojie Liu, Weidong Liu, Guorui Zhou |  |
| 21 |  |  [User Distribution Mapping Modelling with Collaborative Filtering for Cross Domain Recommendation](https://doi.org/10.1145/3589334.3645331) |  | 0 | User cold-start recommendation aims to provide accurate items for the newly joint users and is a hot and challenging problem. Nowadays as people participant in different domains, how to recommend items in the new domain for users in an old domain has become more urgent. In this paper, we focus on the Dual Cold-Start Cross Domain Recommendation (Dual-CSCDR) problem. That is, providing the most relevant items for new users on the source and target domains. The prime task in Dual-CSCDR is to properly model user-item rating interactions and map user expressive embeddings across domains. However, previous approaches cannot solve Dual-CSCDR well, since they separate the collaborative filtering and distribution mapping process, leading to the error superimposition issue. Moreover, most of these methods fail to fully exploit the cross-domain relationship among large number of non-overlapped users, which strongly limits their performance. To fill this gap, we propose User Distribution Mapping model with Collaborative Filtering (UDMCF), a novel end-to-end cold-start cross-domain recommendation framework for the Dual-CSCDR problem. UDMCF includes two main modules, i.e., rating prediction module and distribution alignment module. The former module adopts one-hot ID vectors and multi-hot historical ratings for collaborative filtering via a contrastive loss. The latter module contains overlapped user embedding alignment and general user subgroup distribution alignment. Specifically, we innovatively propose unbalance distribution optimal transport with typical subgroup discovering algorithm to map the whole user distributions. Our empirical study on several datasets demonstrates that UDMCF significantly outperforms the state-of-the-art models under the Dual-CSCDR setting. | Weiming Liu, Chaochao Chen, Xinting Liao, Mengling Hu, Jiajie Su, Yanchao Tan, Fan Wang |  |
| 22 |  |  [Leave No One Behind: Online Self-Supervised Self-Distillation for Sequential Recommendation](https://doi.org/10.1145/3589334.3645590) |  | 0 | Sequential recommendation methods play a pivotal role in modern recommendation systems. A key challenge lies in accurately modeling user preferences in the face of data sparsity. To tackle this challenge, recent methods leverage contrastive learning (CL) to derive self-supervision signals by maximizing the mutual information of two augmented views of the original user behavior sequence. Despite their effectiveness, CL-based methods encounter a limitation in fully exploiting self-supervision signals for users with limited behavior data, as users with extensive behaviors naturally offer more information. To address this problem, we introduce a novel learning paradigm, named Online Self-Supervised Self-distillation for Sequential Recommendation ($S^4$Rec), effectively bridging the gap between self-supervised learning and self-distillation methods. Specifically, we employ online clustering to proficiently group users by their distinct latent intents. Additionally, an adversarial learning strategy is utilized to ensure that the clustering procedure is not affected by the behavior length factor. Subsequently, we employ self-distillation to facilitate the transfer of knowledge from users with extensive behaviors (teachers) to users with limited behaviors (students). Experiments conducted on four real-world datasets validate the effectiveness of the proposed method\footnote{Code is available at https://github.com/xjaw/S4Rec | Shaowei Wei, Zhengwei Wu, Xin Li, Qintong Wu, Zhiqiang Zhang, Jun Zhou, Lihong Gu, Jinjie Gu |  |
| 23 |  |  [Improving Search for New Product Categories via Synthetic Query Generation Strategies](https://doi.org/10.1145/3589335.3648299) |  | 0 | Efficient retrieval and ranking of relevant products in e-commerce product search relies on accurate mapping of queries to product categories. This query classification typically utilizes a combination of textual and customer behavioral signals. However, new product categories often lack customer interaction data leading to poor performance. In this paper, we present a novel approach to mitigate this cold start problem in product ranking via synthetic generation of queries as well as simulation of customer interactions. Specifically we study two strategies for synthetic data generation: (i) fine-tuning a generative language model (LLM) on historical product-query interactions and using it to generate synthetic queries from the product catalog, (ii) Bayesian prompt optimization with an instruction-tuned LLM to directly generate queries from catalog. Empirical evaluation of the proposed approaches on public datasets and real-world customer queries demonstrates significant benefits (+2.96% and +2.34% in PR-AUC on e-commerce queries)1 relative to the baseline approach without synthetic data augmentation. Furthermore, evaluation of the augmented model on live search page results in a substantial increase in highly relevant product results (+3.35%) and reduction (-3.07%) in irrelevant results. | Akshay Jagatap, Srujana Merugu, Prakash Mandayam Comar |  |
| 24 |  |  [MS MARCO Web Search: A Large-scale Information-rich Web Dataset with Millions of Real Click Labels](https://doi.org/10.1145/3589335.3648327) |  | 0 | Recent breakthroughs in large models have highlighted the critical significance of data scale, labels and modals. In this paper, we introduce MS MARCO Web Search, the first large-scale information-rich web dataset, featuring millions of real clicked query-document labels. This dataset closely mimics real-world web document and query distribution, provides rich information for various kinds of downstream tasks and encourages research in various areas, such as generic end-to-end neural indexer models, generic embedding models, and next generation information access system with large language models. MS MARCO Web Search offers a retrieval benchmark with three web retrieval challenge tasks that demand innovations in both machine learning and information retrieval system research domains. As the first dataset that meets large, real and rich data requirements, MS MARCO Web Search paves the way for future advancements in AI and system research. MS MARCO Web Search dataset is available at: https://github.com/microsoft/MS-MARCO-Web-Search. | Qi Chen, Xiubo Geng, Corby Rosset, Carolyn Buractaon, Jingwen Lu, Tao Shen, Kun Zhou, Chenyan Xiong, Yeyun Gong, Paul N. Bennett, Nick Craswell, Xing Xie, Fan Yang, Bryan Tower, Nikhil Rao, Anlei Dong, Wenqi Jiang, Zheng Liu, Mingqin Li, Chuanjie Liu, Zengzhong Li, Rangan Majumder, Jennifer Neville, Andy Oakley, Knut Magne Risvik, Harsha Vardhan Simhadri, Manik Varma, Yujing Wang, Linjun Yang, Mao Yang, Ce Zhang |  |
| 25 |  |  [Enhancing Cross-Domain Click-Through Rate Prediction via Explicit Feature Augmentation](https://doi.org/10.1145/3589335.3648341) |  | 0 | Cross-domain CTR (CDCTR) prediction is an important research topic that studies how to leverage meaningful data from a related domain to help CTR prediction in target domain. Most existing CDCTR works design implicit ways to transfer knowledge across domains such as parameter-sharing that regularizes the model training in target domain. More effectively, recent researchers propose explicit techniques to extract user interest knowledge and transfer this knowledge to target domain. However, the proposed method mainly faces two issues: 1) it usually requires a super domain, i.e. an extremely large source domain, to cover most users or items of target domain, and 2) the extracted user interest knowledge is static no matter what the context is in target domain. These limitations motivate us to develop a more flexible and efficient technique to explicitly transfer knowledge. In this work, we propose a cross-domain augmentation network (CDAnet) being able to perform explicit knowledge transfer between two domains. Specifically, CDAnet contains a designed translation network and an augmentation network which are trained sequentially. The translation network computes latent features from two domains and learns meaningful cross-domain knowledge of each input in target domain by using a designed cross-supervised feature translator. Later the augmentation network employs the explicit cross-domain knowledge as augmented information to boost the target domain CTR prediction. Through extensive experiments on two public benchmarks and one industrial production dataset, we show CDAnet can learn meaningful translated features and largely improve the performance of CTR prediction. CDAnet has been conducted online A/B test in image2product retrieval at Taobao app, bringing an absolute 0.11 point CTR improvement, a relative 0.64 | Xu Chen, Zida Cheng, Jiangchao Yao, Chen Ju, Weilin Huang, Jinsong Lan, Xiaoyi Zeng, Shuai Xiao |  |
| 26 |  |  [Item-Ranking Promotion in Recommender Systems](https://doi.org/10.1145/3589335.3651529) |  | 0 | In this paper, we first define the problem of item-ranking promotion (IRP) in recommender systems as (Goal 1) maintaining a high level of overall recommendation accuracy while (Goal 2) recommending the items with extra values (i.e., RP-items) to as many users as possible. Our novel framework, proposed to address the IRP problem, is based on our own loss function that simultaneously aims to achieve the two goals above and employs a learning-to-rank scheme for training a recommender model. Via extensive experiments, we validate the effectiveness of our framework in terms of the exposure rate of RP-items and the accuracy of recommendation. | HongKyun Bae, HaeRi Jang, YangSae Moon, SangWook Kim |  |
| 27 |  |  [Understanding and Counteracting Feature-Level Bias in Click-Through Rate Prediction](https://doi.org/10.1145/3589335.3651576) |  | 0 | Common click-through rate (CTR) prediction recommender models tend to exhibit feature-level bias, which leads to unfair recommendations among item groups and inaccurate recommendations for users. While existing methods address this issue by adjusting the learning of CTR models, such as through additional optimization objectives, they fail to consider how the bias is caused within these models. To address this research gap, our study performs a top-down analysis on representative CTR models. Through blocking different components of a trained CTR model one by one, we identify the key contribution of the linear component to feature-level bias. We conduct a theoretical analysis of the learning process for the weights in the linear component, revealing how group-wise properties of training data influence them. Our experimental and statistical analyses demonstrate a strong correlation between imbalanced positive sample ratios across item groups and feature-level bias. Based on this understanding, we propose a minimally invasive yet effective strategy to counteract feature-level bias in CTR models by removing the biased linear weights from trained models. Additionally, we present a linear weight adjusting strategy that requires fewer random exposure records than relevant debiasing methods. The superiority of our proposed strategies are validated through extensive experiments on three real-world datasets. | Jinqiu Jin, Sihao Ding, Wenjie Wang, Fuli Feng |  |
| 28 |  |  [A Demonstration of Decentralized Search Over Solid Personal Online Datastores](https://doi.org/10.1145/3589335.3651248) |  | 0 | In the modern Web landscape, data privacy and control are increasingly unattainable for users. Solid 1, a decentralized Web ecosystem, restores individual privacy and control by separating data from applications, allowing integration across applications while enabling users to control access. The growth in Solid's decentralized pods, and the escalating amounts of data stored in them, necessitate a decentralized search mechanism to query data within personal datastores, while respecting varying access constraints. This demo paper presents our decentralized search system (ESPRESSO) for querying RDF and non-RDF data in Solid datastores, tackling challenges like query propagation, data indexing, privacy, and results aggregation. | Mohamed Ragab, Yury Savateev, Helen Oliver, Reza Moosaei, Thanassis Tiropanis, Alexandra Poulovassilis, Adriane Chapman, George Roussos |  |
| 29 |  |  [Neural Contextual Bandits for Personalized Recommendation](https://doi.org/10.1145/3589335.3641241) |  | 0 | In the dynamic landscape of online businesses, recommender systems are pivotal in enhancing user experiences. While traditional approaches have relied on static supervised learning, the quest for adaptive, user-centric recommendations has led to the emergence of the formulation of contextual bandits. This tutorial investigates the contextual bandits as a powerful framework for personalized recommendations. We delve into the challenges, advanced algorithms and theories, collaborative strategies, and open challenges and future prospects within this field. Different from existing related tutorials, (1) we focus on the exploration perspective of contextual bandits to alleviate the \`\`Matthew Effect'' in the recommender systems, i.e., the rich get richer and the poor get poorer, concerning the popularity of items; (2) in addition to the conventional linear contextual bandits, we will also dedicated to neural contextual bandits which have emerged as an important branch in recent years, to investigate how neural networks benefit contextual bandits for personalized recommendation both empirically and theoretically; (3) we will cover the latest topic, collaborative neural contextual bandits, to incorporate both user heterogeneity and user correlations customized for recommender system; (4) we will provide and discuss the new emerging challenges and open questions for neural contextual bandits with applications in the personalized recommendation, especially for large neural models. | Yikun Ban, Yunzhe Qi, Jingrui He |  |
| 30 |  |  [Causality-driven User Modeling for Sequential Recommendations over Time](https://doi.org/10.1145/3589335.3651896) |  | 0 | Contemporary sequential recommendation systems predominantly leverage statistical correlations derived from user interaction histories to predict future preferences. However, these correlations often mask implicit challenges. On the one hand, user data is frequently plagued by implicit, noisy feedback, misdirecting users towards items that fail to align with their actual interests, which is magnified in sequential recommendation contexts. On the other hand, prevalent methods tend to over-rely on similarity-based attention mechanisms across item pairs, which are prone to utilizing heuristic shortcuts, thereby leading to suboptimal recommendation. To tackle these issues, we put forward a causality-driven user modeling approach for sequential recommendation, which pivots towards a causal perspective. Specifically, we involves the application of a causal graph to identify confounding factors that give rise to spurious correlations and to isolate conceptual variables that causally encapsulate user preferences. By learning the representation of these disentangled causal variables at the conceptual level, we can distinguish between causal and non-causal associations while preserving the inherent sequential nature of user behaviors. This enables us to ascertain which elements are critical and which may induce unintended biases. The framework of our method can be compatible with various mainstream sequential models, which offers a robust foundation for reconstructing more accurate and meaningful user and item representations driven by causality. | Xingming Chen, Qing Li |  |
| 31 |  |  [ConvSDG: Session Data Generation for Conversational Search](https://doi.org/10.1145/3589335.3651940) |  | 0 | Conversational search provides a more convenient interface for users to search by allowing multi-turn interaction with the search engine. However, the effectiveness of the conversational dense retrieval methods is limited by the scarcity of training data required for their fine-tuning. Thus, generating more training conversational sessions with relevant labels could potentially improve search performance. Based on the promising capabilities of large language models (LLMs) on text generation, we propose ConvSDG, a simple yet effective framework to explore the feasibility of boosting conversational search by using LLM for session data generation. Within this framework, we design dialogue/session-level and query-level data generation with unsupervised and semi-supervised learning, according to the availability of relevance judgments. The generated data are used to fine-tune the conversational dense retriever. Extensive experiments on four widely used datasets demonstrate the effectiveness and broad applicability of our ConvSDG framework compared with several strong baselines. | Fengran Mo, Bole Yi, Kelong Mao, Chen Qu, Kaiyu Huang, JianYun Nie |  |
| 32 |  |  [How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation](https://doi.org/10.1145/3589335.3651955) |  | 0 | Conversational Recommender System (CRS) interacts with users through natural language to understand their preferences and provide personalized recommendations in real-time. CRS has demonstrated significant potential, prompting researchers to address the development of more realistic and reliable user simulators as a key focus. Recently, the capabilities of Large Language Models (LLMs) have attracted a lot of attention in various fields. Simultaneously, efforts are underway to construct user simulators based on LLMs. While these works showcase innovation, they also come with certain limitations that require attention. In this work, we aim to analyze the limitations of using LLMs in constructing user simulators for CRS, to guide future research. To achieve this goal, we conduct analytical validation on the notable work, iEvaLM. Through multiple experiments on two widely-used datasets in the field of conversational recommendation, we highlight several issues with the current evaluation methods for user simulators based on LLMs: (1) Data leakage, which occurs in conversational history and the user simulator's replies, results in inflated evaluation results. (2) The success of CRS recommendations depends more on the availability and quality of conversational history than on the responses from user simulators. (3) Controlling the output of the user simulator through a single prompt template proves challenging. To overcome these limitations, we propose SimpleUserSim, employing a straightforward strategy to guide the topic toward the target items. Our study validates the ability of CRS models to utilize the interaction information, significantly improving the recommendation results. | Lixi Zhu, Xiaowen Huang, Jitao Sang |  |
| 33 |  |  [Mining Exploratory Queries for Conversational Search](https://doi.org/10.1145/3589334.3645424) |  | 0 | Users' queries are usually vague, and their search intents tend to be ambiguous, thereby needing search clarification to clarify users' current intent by asking a clarifying question and providing several clickable sub-intent items as clarification options. However, in addition to drilling down the current query, users may also have exploratory needs that diverge from their current intent. For example, a user searching for the query "Cartier women watches'' may also potentially want to explore some parallel information by issuing queries such as "Rolex women watches'' or "Cartier women bracelets'', named exploratory queries in this paper. These exploratory needs are common during the search process yet cannot be satisfied by current search clarification approaches which typically stick to the sub-intents of the query. This paper focuses on mining exploratory queries as additional options to meet users' exploratory needs in conversational search systems. Specifically, we first design a rule-based model that generates exploratory queries based on the current query's top retrieved documents. Then, we propose using the data generated by the rule-based model to train a neural generation model through multi-task learning for further generalization. Finally, we borrow the in-context learning ability of the large language model to generate exploratory queries based on prompt engineering. We constructed an evaluation dataset based on human annotations and conduct an extensive set of experiments. The results show that our proposed methods generate higher-quality exploratory queries compared with several baselines. | Wenhan Liu, Ziliang Zhao, Yutao Zhu, Zhicheng Dou |  |
| 34 |  |  [An In-depth Investigation of User Response Simulation for Conversational Search](https://doi.org/10.1145/3589334.3645447) |  | 0 | Conversational search has seen increased recent attention in both the IR and NLP communities. It seeks to clarify and solve users' search needs through multi-turn natural language interactions. However, most existing systems are trained and demonstrated with recorded or artificial conversation logs. Eventually, conversational search systems should be trained, evaluated, and deployed in an open-ended setting with unseen conversation trajectories. A key challenge is that training and evaluating such systems both require a human-in-the-loop, which is expensive and does not scale. One strategy is to simulate users, thereby reducing the scaling costs. However, current user simulators are either limited to only responding to yes-no questions from the conversational search system or unable to produce high-quality responses in general. In this paper, we show that existing user simulation systems could be significantly improved by a smaller finetuned natural language generation model. However, rather than merely reporting it as the new state-of-the-art, we consider it a strong baseline and present an in-depth investigation of simulating user response for conversational search. Our goal is to supplement existing work with an insightful hand-analysis of unsolved challenges by the baseline and propose our solutions. The challenges we identified include (1) a blind spot that is difficult to learn, and (2) a specific type of misevaluation in the standard setup. We propose a new generation system to effectively cover the training blind spot and suggest a new evaluation setup to avoid misevaluation. Our proposed system leads to significant improvements over existing systems and large language models such as GPT-4. Additionally, our analysis provides insights into the nature of user simulation to facilitate future work. | Zhenduo Wang, Zhichao Xu, Vivek Srikumar, Qingyao Ai |  |
| 35 |  |  [Rethinking Cross-Domain Sequential Recommendation under Open-World Assumptions](https://doi.org/10.1145/3589334.3645351) |  | 0 | Cross-Domain Sequential Recommendation (CDSR) methods aim to tackle the data sparsity and cold-start problems present in Single-Domain Sequential Recommendation (SDSR). Existing CDSR works design their elaborate structures relying on overlapping users to propagate the cross-domain information. However, current CDSR methods make closed-world assumptions, assuming fully overlapping users across multiple domains and that the data distribution remains unchanged from the training environment to the test environment. As a result, these methods typically result in lower performance on online real-world platforms due to the data distribution shifts. To address these challenges under open-world assumptions, we design an Adaptive Multi-Interest Debiasing framework for cross-domain sequential recommendation (AMID), which consists of a multi-interest information module (MIM) and a doubly robust estimator (DRE). Our framework is adaptive for open-world environments and can improve the model of most off-the-shelf single-domain sequential backbone models for CDSR. Our MIM establishes interest groups that consider both overlapping and non-overlapping users, allowing us to effectively explore user intent and explicit interest. To alleviate biases across multiple domains, we developed the DRE for the CDSR methods. We also provide a theoretical analysis that demonstrates the superiority of our proposed estimator in terms of bias and tail bound, compared to the IPS estimator used in previous work. | Wujiang Xu, Qitian Wu, Runzhong Wang, Mingming Ha, Qiongxu Ma, Linxun Chen, Bing Han, Junchi Yan |  |
| 36 |  |  [Not All Embeddings are Created Equal: Towards Robust Cross-domain Recommendation via Contrastive Learning](https://doi.org/10.1145/3589334.3645357) |  | 0 | Cross-domain recommendation (CDR) aims to leverage the rich information from the source domain to enhance recommendation performance in the target domain. However, the data imbalance problem inherent across different domains compromises the effectiveness of CDR approaches, posing a significant challenge to CDR. Most current CDR methodologies focus on creating better user embeddings for the target domain, yet usually neglect the inconsistency in user activities due to data imbalance. As a result, the process of creating user embeddings tends to prioritize users with more frequent interactions and leave less active users underserved, leading these CDR methods to struggle in making accurate recommendations for those with fewer interactions. Such bias in creating embeddings reveals the fact that ''not all embeddings are created equal'' in CDR, which serves as the primary motivation of this study. Inspired by the recent development of contrastive learning, this paper proposes User-aware Contrastive Learning for Robust cross-domain recommendation (UCLR), enhancing the robustness of cross-domain recommendation. Specifically, our proposed method consists of two sub-modules: (i) pretrained global embedding, where the global user embeddings are pretrained across all the domains; (ii) contrastive dual-stream collaborative autoencoder, where more equal user embeddings are generated by optimizing contrastive loss with individualized temperatures. To further improve the performance of our method in each domain, we finetune the whole framework of UCLR based on Low-Rank Adaptation (LoRA). Theoretically, our method is equipped with a provable convergence guarantee during the contrastive learning stage. Furthermore, we also conduct comprehensive experiments on real-world datasets to validate the effectiveness of our proposed method. | Wenhao Yang, Yingchun Jian, Yibo Wang, Shiyin Lu, Lei Shen, Bing Wang, Haihong Tang, Lijun Zhang |  |
| 37 |  |  [Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation](https://doi.org/10.1145/3589334.3645380) |  | 0 | In recommendation systems, users frequently engage in multiple types of behaviors, such as clicking, adding to a cart, and purchasing. However, with diversified behavior data, user behavior sequences will become very long in the short term, which brings challenges to the efficiency of the sequence recommendation model. Meanwhile, some behavior data will also bring inevitable noise to the modeling of user interests. To address the aforementioned issues, firstly, we develop the Efficient Behavior Sequence Miner (EBM) that efficiently captures intricate patterns in user behavior while maintaining low time complexity and parameter count. Secondly, we design hard and soft denoising modules for different noise types and fully explore the relationship between behaviors and noise. Finally, we introduce a contrastive loss function along with a guided training strategy to compare the valid information in the data with the noisy signal, and seamlessly integrate the two denoising processes to achieve a high degree of decoupling of the noisy signal. Sufficient experiments on real-world datasets demonstrate the effectiveness and efficiency of our approach in dealing with multi-behavior sequential recommendation. | Yongqiang Han, Hao Wang, Kefan Wang, Likang Wu, Zhi Li, Wei Guo, Yong Liu, Defu Lian, Enhong Chen |  |
| 38 |  |  [Debiasing Recommendation with Personal Popularity](https://doi.org/10.1145/3589334.3645421) |  | 0 | Global popularity (GP) bias is the phenomenon that popular items are recommended much more frequently than they should be, which goes against the goal of providing personalized recommendations and harms user experience and recommendation accuracy. Many methods have been proposed to reduce GP bias but they fail to notice the fundamental problem of GP, i.e., it considers popularity from a global perspective of all users and uses a single set of popular items, and thus cannot capture the interests of individual users. As such, we propose a user-aware version of item popularity named personal popularity (PP), which identifies different popular items for each user by considering the users that share similar interests. As PP models the preferences of individual users, it naturally helps to produce personalized recommendations and mitigate GP bias. To integrate PP into recommendation, we design a general personal popularity aware counterfactual (PPAC) framework, which adapts easily to existing recommendation models. In particular, PPAC recognizes that PP and GP have both direct and indirect effects on recommendations and controls direct effects with counterfactual inference techniques for unbiased recommendations. All codes and datasets are available at <https://github.com/Stevenn9981/PPAC>. | Wentao Ning, Reynold Cheng, Xiao Yan, Ben Kao, Nan Huo, Nur Al Hasan Haldar, Bo Tang |  |
| 39 |  |  [Negative Sampling in Next-POI Recommendations: Observation, Approach, and Evaluation](https://doi.org/10.1145/3589334.3645681) |  | 0 | To recommend the points of interest (POIs) that a user would check-in next, most deep-learning (DL)-based existing studies have employed random negative (RN) sampling during model training. In this paper, we claim and validate that, as the training proceeds, such an RN sampling in reality performs as sampling easy negative (EN) POIs (i.e., EN sampling) that a user was highly unlikely to check-in at her check-in time point. Furthermore, we verify that EN sampling is more disadvantageous in improving the accuracy than sampling hard negative (HN) POIs (i.e., HN sampling) that a user was highly likely to check-in. To address this limitation, we present the novel concept of the Degree of Positiveness (DoP), which can be formulated by two factors: (i) the degree to which a POI has the characteristics preferred by a user; (ii) the geographical distance between a user and a POI. Then, we propose a new model-training scheme based on HN sampling by using DoP. Using real-world datasets (i.e., NYC, TKY, and Brightkite), we demonstrate that all the state-of-the-art models trained by our scheme showed dramatic improvements in accuracy by up to about 82.8%. | HongKyun Bae, Yebeen Kim, Hyunjoon Kim, SangWook Kim |  |
| 40 |  |  [Towards Personalized Privacy: User-Governed Data Contribution for Federated Recommendation](https://doi.org/10.1145/3589334.3645690) |  | 0 | Federated recommender systems (FedRecs) have gained significant attention for their potential to protect user's privacy by keeping user privacy data locally and only communicating model parameters/gradients to the server. Nevertheless, the currently existing architecture of FedRecs assumes that all users have the same 0-privacy budget, i.e., they do not upload any data to the server, thus overlooking those users who are less concerned about privacy and are willing to upload data to get a better recommendation service. To bridge this gap, this paper explores a user-governed data contribution federated recommendation architecture where users are free to take control of whether they share data and the proportion of data they share to the server. To this end, this paper presents a cloud-device collaborative graph neural network federated recommendation model, named CDCGNNFed. It trains user-centric ego graphs locally, and high-order graphs based on user-shared data in the server in a collaborative manner via contrastive learning. Furthermore, a graph mending strategy is utilized to predict missing links in the graph on the server, thus leveraging the capabilities of graph neural networks over high-order graphs. Extensive experiments were conducted on two public datasets, and the results demonstrate the effectiveness of the proposed method. | Liang Qu, Wei Yuan, Ruiqi Zheng, Lizhen Cui, Yuhui Shi, Hongzhi Yin |  |
| 41 |  |  [A Semi-supervised Multi-channel Graph Convolutional Network for Query Classification in E-commerce](https://doi.org/10.1145/3589335.3648302) |  | 0 | Query intent classification is an essential module for customers to quickly find desired products on the e-commerce application. Most existing query intent classification methods rely on the users' click behavior as a supervised signal to construct training samples. However, these methods based entirely on posterior labels may lead to serious category imbalance problems because of the Matthew effect in click samples. Compared with popular categories, it is difficult for products under long-tail categories to obtain traffic and user clicks, which makes the models unable to detect users' intent for products under long-tail categories. This in turn aggravates the problem that long-tail categories cannot obtain traffic, forming a vicious circle. In addition, due to the randomness of the user's click, the posterior label is unstable for the query with similar semantics, which makes the model very sensitive to the input, leading to an unstable and incomplete recall of categories. In this paper, we propose a novel Semi-supervised Multi-channel Graph Convolutional Network (SMGCN) to address the above problems from the perspective of label association and semi-supervised learning. SMGCN extends category information and enhances the posterior label by utilizing the similarity score between the query and categories. Furthermore, it leverages the co-occurrence and semantic similarity graph of categories to strengthen the relations among labels and weaken the influence of posterior label instability. We conduct extensive offline and online A/B experiments, and the experimental results show that SMGCN significantly outperforms the strong baselines, which shows its effectiveness and practicality. | Chunyuan Yuan, Ming Pang, Zheng Fang, Xue Jiang, Changping Peng, Zhangang Lin |  |
| 42 |  |  [Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale](https://doi.org/10.1145/3589335.3648304) |  | 0 | Neural Architecture Search (NAS) has demonstrated its efficacy in computer vision and potential for ranking systems. However, prior work focused on academic problems, which are evaluated at small scale under well-controlled fixed baselines. In industry system, such as ranking system in Meta, it is unclear whether NAS algorithms from the literature can outperform production baselines because of: (1) scale - Meta ranking systems serve billions of users, (2) strong baselines - the baselines are production models optimized by hundreds to thousands of world-class engineers for years since the rise of deep learning, (3) dynamic baselines - engineers may have established new and stronger baselines during NAS search, and (4) efficiency - the search pipeline must yield results quickly in alignment with the productionization life cycle. In this paper, we present Rankitect, a NAS software framework for ranking systems at Meta. Rankitect seeks to build brand new architectures by composing low level building blocks from scratch. Rankitect implements and improves state-of-the-art (SOTA) NAS methods for comprehensive and fair comparison under the same search space, including sampling-based NAS, one-shot NAS, and Differentiable NAS (DNAS). We evaluate Rankitect by comparing to multiple production ranking models at Meta. We find that Rankitect can discover new models from scratch achieving competitive tradeoff between Normalized Entropy loss and FLOPs. When utilizing search space designed by engineers, Rankitect can generate better models than engineers, achieving positive offline evaluation and online A/B test at Meta scale. | Wei Wen, KuangHung Liu, Igor Fedorov, Xin Zhang, Hang Yin, Weiwei Chu, Kaveh Hassani, Mengying Sun, Jiang Liu, Xu Wang, Lin Jiang, Yuxin Chen, Buyun Zhang, Xi Liu, Dehua Cheng, Zhengxing Chen, Guang Zhao, Fangqiu Han, Jiyan Yang, Yuchen Hao, Liang Xiong, WenYen Chen |  |
| 43 |  |  [Aligned Side Information Fusion Method for Sequential Recommendation](https://doi.org/10.1145/3589335.3648308) |  | 0 | Combining contextual information (i.e., side information) of items beyond IDs has become an important way to improve the performance in recommender systems. Existing self-attention-based side information fusion methods can be categorized into early, late, and hybrid fusion. In practice, naive early fusion may interfere with the representation of IDs, resulting in negative effects, while late fusion misses effective interactions between IDs and side information. Some hybrid methods have been proposed to address these issues, but they only utilize side information in calculating attention scores, which may lead to information loss. To harness the full potential of side information without noisy interference, we propose an <u>A</u>ligned <u>S</u>ide <u>I</u>nformation <u>F</u>usion (ASIF) method for sequential recommendation, consisting of two parts: Fused Attention with Untied Positions and Representation Alignment. Specifically, we first decouple the positions to exclude the noisy interference in the attention scores. Secondly, we adopt the contrastive objective to maintain the semantic consistency between IDs and side information and then employ orthogonal decomposition to extract the homogeneous parts. By aligning the representations and fusing them together, ASIF makes full use of the side information without interfering with IDs. Offline experimental results on four datasets demonstrate the superiority of ASIF. Additionally, we successfully deployed the model in Alipay's advertising system and achieved 1.09% and 1.86% improvements on clicks and Cost Per Mille (CPM). | Shuhan Wang, Bin Shen, Xu Min, Yong He, Xiaolu Zhang, Liang Zhang, Jun Zhou, Linjian Mo |  |
| 44 |  |  [Discrete Conditional Diffusion for Reranking in Recommendation](https://doi.org/10.1145/3589335.3648313) |  | 0 | Reranking plays a crucial role in modern multi-stage recommender systems by rearranging the initial ranking list to model interplay between items. Considering the inherent challenges of reranking such as combinatorial searching space, some previous studies have adopted the evaluator-generator paradigm, with a generator producing feasible sequences and a evaluator selecting the best one based on estimated listwise utility. Inspired by the remarkable success of diffusion generative models, this paper explores the potential of diffusion models for generating high-quality sequences in reranking. However, we argue that it is nontrivial to take diffusion models as the generator in the context of recommendation. Firstly, diffusion models primarily operate in continuous data space, differing from the discrete data space of item permutations. Secondly, the recommendation task is different from conventional generation tasks as the purpose of recommender systems is to fulfill user interests. Lastly, real-life recommender systems require efficiency, posing challenges for the inference of diffusion models. To overcome these challenges, we propose a novel Discrete Conditional Diffusion Reranking (DCDR) framework for recommendation. DCDR extends traditional diffusion models by introducing a discrete forward process with tractable posteriors, which adds noise to item sequences through step-wise discrete operations (e.g., swapping). Additionally, DCDR incorporates a conditional reverse process that generates item sequences conditioned on expected user responses. Extensive offline experiments conducted on public datasets demonstrate that DCDR outperforms state-of-the-art reranking methods. Furthermore, DCDR has been deployed in a real-world video app with over 300 million daily active users, significantly enhancing online recommendation quality. | Xiao Lin, Xiaokai Chen, Chenyang Wang, Hantao Shu, Linfeng Song, Biao Li, Peng Jiang |  |
| 45 |  |  [A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model](https://doi.org/10.1145/3589335.3648315) |  | 0 | In online advertising scenario, sellers often create multiple creatives to provide comprehensive demonstrations, making it essential to present the most appealing design to maximize the Click-Through Rate (CTR). However, sellers generally struggle to consider users preferences for creative design, leading to the relatively lower aesthetics and quantities compared to Artificial Intelligence (AI)-based approaches. Traditional AI-based approaches still face the same problem of not considering user information while having limited aesthetic knowledge from designers. In fact that fusing the user information, the generated creatives can be more attractive because different users may have different preferences. To optimize the results, the generated creatives in traditional methods are then ranked by another module named creative ranking model. The ranking model can predict the CTR score for each creative considering user features. However, the two above stages are regarded as two different tasks and are optimized separately. In this paper, we proposed a new automated Creative Generation pipeline for Click-Through Rate (CG4CTR) with the goal of improving CTR during the creative generation stage. Our contributions have 4 parts: 1) The inpainting mode in stable diffusion is firstly applied to creative generation task in online advertising scene. A self-cyclic generation pipeline is proposed to ensure the convergence of training. 2) Prompt model is designed to generate individualized creatives for different user groups, which can further improve the diversity and quality. 3) Reward model comprehensively considers the multimodal features of image and text to improve the effectiveness of creative ranking task, and it is also critical in self-cyclic pipeline. 4) The significant benefits obtained in online and offline experiments verify the significance of our proposed method. | Hao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, Yifan Zeng |  |
| 46 |  |  [NoteLLM: A Retrievable Large Language Model for Note Recommendation](https://doi.org/10.1145/3589335.3648314) |  | 0 | People enjoy sharing "notes" including their experiences within online communities. Therefore, recommending notes aligned with user interests has become a crucial task. Existing online methods only input notes into BERT-based models to generate note embeddings for assessing similarity. However, they may underutilize some important cues, e.g., hashtags or categories, which represent the key concepts of notes. Indeed, learning to generate hashtags/categories can potentially enhance note embeddings, both of which compress key note information into limited content. Besides, Large Language Models (LLMs) have significantly outperformed BERT in understanding natural languages. It is promising to introduce LLMs into note recommendation. In this paper, we propose a novel unified framework called NoteLLM, which leverages LLMs to address the item-to-item (I2I) note recommendation. Specifically, we utilize Note Compression Prompt to compress a note into a single special token, and further learn the potentially related notes' embeddings via a contrastive learning approach. Moreover, we use NoteLLM to summarize the note and generate the hashtag/category automatically through instruction tuning. Extensive validations on real scenarios demonstrate the effectiveness of our proposed method compared with the online baseline and show major improvements in the recommendation system of Xiaohongshu. | Chao Zhang, Shiwei Wu, Haoxin Zhang, Tong Xu, Yan Gao, Yao Hu, Enhong Chen |  |
| 47 |  |  [Lightweight GCN Encoder and Sequential Decoder for Multi-Candidate Carpooling Route Planning in Road Network](https://doi.org/10.1145/3589335.3648328) |  | 0 | Carpooling Route Planning (CRP) has become an important issue with the growth of low-carbon traffic systems. We investigate a meaningful and challenging scenario for CRP in industry, where each passenger may have several potential positions to get on and off the car. Traditional graph search algorithms or indexing methods usually consume a lot of time and space or perform poorly. In this paper, we propose an end-to-end encoder-decoder model to plan a route for each many-to-one carpooling order with various data-driven mechanisms such as graph partitioning and feature crossover. The encoder is a filter-integrated Graph Convolution Network with external information fusion combining a supervised pre-training classification task, while the latter mimics a pointer network with a rule-based mask mechanism and a domain feature crossover module. We validate the effectiveness and efficiency of our model based on both synthetic and real-world datasets. | Yucen Gao, Li Ma, Zhemeng Yu, Songjian Zhang, Jun Fang, Xiaofeng Gao, Guihai Chen |  |
| 48 |  |  [PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction](https://doi.org/10.1145/3589335.3648329) |  | 0 | Click-through rate (CTR) prediction is a core task in recommender systems. Existing methods (IDRec for short) rely on unique identities to represent distinct users and items that have prevailed for decades. On one hand, IDRec often faces significant performance degradation on cold-start problem; on the other hand, IDRec cannot use longer training data due to constraints imposed by iteration efficiency. Most prior studies alleviate the above problems by introducing pre-trained knowledge(e.g. pre-trained user model or multi-modal embeddings). However, the explosive growth of online latency can be attributed to the huge parameters in the pre-trained model. Therefore, most of them cannot employ the unified model of end-to-end training with IDRec in industrial recommender systems, thus limiting the potential of the pre-trained model. To this end, we propose a Pre-trained Plug-in CTR Model, namely PPM. PPM employs multi-modal features as input and utilizes large-scale data for pre-training. Then, PPM is plugged in IDRec model to enhance unified model's performance and iteration efficiency. Upon incorporating IDRec model, certain intermediate results within the network are cached, with only a subset of the parameters participating in training and serving. Hence, our approach can successfully deploy an end-to-end model without causing huge latency increases. Comprehensive offline experiments and online A/B testing at JD E-commerce demonstrate the efficiency and effectiveness of PPM. | Yuanbo Gao, Peng Lin, Dongyue Wang, Feng Mei, Xiwei Zhao, Sulong Xu, Jinghe Hu |  |
| 49 |  |  [Towards Robustness Analysis of E-Commerce Ranking System](https://doi.org/10.1145/3589335.3648335) |  | 0 | Information retrieval (IR) is a pivotal component in various applications. Recent advances in machine learning (ML) have enabled the integration of ML algorithms into IR, particularly in ranking systems. While there is a plethora of research on the robustness of ML-based ranking systems, these studies largely neglect commercial e-commerce systems and fail to establish a connection between real-world and manipulated query relevance. In this paper, we present the first systematic measurement study on the robustness of e-commerce ranking systems. We define robustness as the consistency of ranking outcomes for semantically identical queries. To quantitatively analyze robustness, we propose a novel metric that considers both ranking position and item-specific information that are absent in existing metrics. Our large-scale measurement study with real-world data from e-commerce retailers reveals an open opportunity to measure and improve robustness since semantically identical queries often yield inconsistent ranking results. Based on our observations, we propose several solution directions to enhance robustness, such as the use of Large Language Models. Note that the issue of robustness discussed herein does not constitute an error or oversight. Rather, in scenarios where there exists a vast array of choices, it is feasible to present a multitude of products in various permutations, all of which could be equally appealing. However, this extensive selection may lead to customer confusion. As e-commerce retailers use various techniques to improve the quality of search results, we hope that this research offers valuable guidance for measuring the robustness of the ranking systems. | Ningfei Wang, Yupin Huang, Han Cheng, Jiri Gesi, Xiaojie Wang, Vivek Mittal |  |
| 50 |  |  [Personalized Audiobook Recommendations at Spotify Through Graph Neural Networks](https://doi.org/10.1145/3589335.3648339) |  | 0 | In the ever-evolving digital audio landscape, Spotify, well-known for its music and talk content, has recently introduced audiobooks to its vast user base. While promising, this move presents significant challenges for personalized recommendations. Unlike music and podcasts, audiobooks, initially available for a fee, cannot be easily skimmed before purchase, posing higher stakes for the relevance of recommendations. Furthermore, introducing a new content type into an existing platform confronts extreme data sparsity, as most users are unfamiliar with this new content type. Lastly, recommending content to millions of users requires the model to react fast and be scalable. To address these challenges, we leverage podcast and music user preferences and introduce 2T-HGNN, a scalable recommendation system comprising Heterogeneous Graph Neural Networks (HGNNs) and a Two Tower (2T) model. This novel approach uncovers nuanced item relationships while ensuring low latency and complexity. We decouple users from the HGNN graph and propose an innovative multi-link neighbor sampler. These choices, together with the 2T component, significantly reduce the complexity of the HGNN model. Empirical evaluations involving millions of users show significant improvement in the quality of personalized recommendations, resulting in a +46 a +23 beyond audiobooks, benefiting established products like podcasts. | Marco De Nadai, Francesco Fabbri, Paul Gigioli, Alice Wang, Ang Li, Fabrizio Silvestri, Laura Kim, Shawn Lin, Vladan Radosavljevic, Sandeep Ghael, David Nyhan, Hugues Bouchard, Mounia Lalmas, Andreas Damianou |  |
| 51 |  |  [End-to-End Graph-Sequential Representation Learning for Accurate Recommendations](https://doi.org/10.1145/3589335.3651499) |  | 0 | Recent recommender system advancements have focused on developing sequence-based and graph-based approaches. Both approaches proved useful in modeling intricate relationships within behavioral data, leading to promising outcomes in personalized ranking and next-item recommendation tasks while maintaining good scalability. However, they capture very different signals from data. While the former approach represents users directly through ordered interactions with recent items, the latter aims to capture indirect dependencies across the interactions graph. This paper presents a novel multi-representational learning framework exploiting these two paradigms' synergies. Our empirical evaluation on several datasets demonstrates that mutual training of sequential and graph components with the proposed framework significantly improves recommendations performance. | Vladimir Baikalov, Evgeny Frolov |  |
| 52 |  |  [DiffuRetrieval: A Chain-of-Thought Enhanced Diffusion Retrieval in Sponsored Search](https://doi.org/10.1145/3589335.3651491) |  | 0 | Embedding-based Retrieval (EBR) system is a fundamental component that supplies candidates for downstream ranking mechanisms in the sponsored search system. To enhance search experience and ensure effective retrieval, EBR usually accounts for various objectives including the semantic relevance and personalization of search results. However, traditional multi-task EBR models ignore the intrinsic progressive relationship between relevant and personalized candidates during a search. Recognizing this gap, we make the very first attempt to utilize the representation generation capabilities of Diffusion Models in EBR. In this paper, we present a novel model DiffuRetrieval to address the progressive objectives for high-quality item retrieval. In forward process, DiffuRetrieval incrementally corrupts item representations through controlled noise injection. Conversely, in reverse process, we refine the representations based on query information in a chain-of-thought manner, initially establishing coarse-grained relevance and progressively moving towards fine-grained personalization. Online A/B tests on Meituan sponsored search platform demonstrate that our approach markedly surpasses the baselines, delivering substantial improvements in revenue, relevance and personalization. | Yadong Zhang, Siyu Lu, Qiang Liu, Xingxing Wang |  |
| 53 |  |  [The Impact of Cluster Centroid and Text Review Embeddings on Recommendation Methods](https://doi.org/10.1145/3589335.3651570) |  | 0 | Recommendation systems often neglect global patterns that can be provided by clusters of similar items or even additional information such as text. Therefore, we study the impact of integrating clustering embeddings, review embeddings, and their combinations with embeddings obtained by a recommender system. Our work assesses the performance of this approach across various state-of-the-art recommender system algorithms. Our study highlights the improvement of recommendation performance through clustering, particularly evident when combined with review embeddings, and the enhanced performance of neural methods when incorporating review embeddings. | Peter Dolog, Ylli Sadikaj, Yllka Velaj, Andreas Stephan, Benjamin Roth, Claudia Plant |  |
| 54 |  |  [Rethinking Sequential Relationships: Improving Sequential Recommenders with Inter-Sequence Data Augmentation](https://doi.org/10.1145/3589335.3651552) |  | 0 | Predicting customer preferences for each item is a prerequisite module for most recommender systems in e-commerce. However, the sparsity of behavioral data is often a challenge to learn accurate prediction models. Given millions of items, each customer may only be able to interact with a small subset of them over time. This sparse behavioral data is insufficient to represent item-customer and item-item relations for a machine learning model to digest, resulting in limited prediction accuracy that hinders recommendation performance. To mitigate this issue, this study introduces an inter-sequence data augmentation method, SDAinter, that enhances data density by leveraging cross-customer behavioral patterns to enrich item relations. Tested on three public and one proprietary e-commerce dataset, SDAinter significantly increases data density, leading to notable improvements in both evaluation and business metrics. Our findings demonstrate SDAinter's effectiveness and its potential to complement existing data augmentation strategies in recommender systems. See https://github.com/ML-apollo/SDA_inter. | Yang Jiao, Fan Yang, Yetian Chen, Yan Gao, Jia Liu, Yi Sun |  |
| 55 |  |  [Unsupervised Search Algorithm Configuration using Query Performance Prediction](https://doi.org/10.1145/3589335.3651579) |  | 0 | Search engine configuration can be quite difficult for inexpert developers. Instead, an auto-configuration approach can be used to speed up development time. Yet, such an automatic process usually requires relevance labels to train a supervised model. In this work, we suggest a simple solution based on query performance prediction that requires no relevance labels but only a sample of queries in a given domain. Using two example usecases we demonstrate the merits of our solution. | Haggai Roitman |  |
| 56 |  |  [I-CoSim: Efficient Dynamic CoSimRank Retrieval on Evolving Networks](https://doi.org/10.1145/3589335.3651523) |  | 0 | CoSimRank, a favorable measure for assessing node similarity based on graphs, faces computational challenges on real evolving graphs. The best-of-breed algorithm, D-CoSim, for incremental CoSimRank search evaluates similarity changes by summing dot products between two vectors. These vectors are iteratively generated from scratch in the original high-dimensional space, leading to significant costs. In this paper, we propose I-CoSim, a novel efficient dynamic CoSimRank algorithm for evolving graphs. I-CoSim resorts to two low-dimensional Krylov subspaces and maximally reuses previously computed similarities in the original graph, which substantially expedites CoSimRank search on evolving graphs. We also theoretically provide an error bound on the I-CoSim estimation with guaranteed accuracy. Experimental results on real datasets show that I-CoSim is up to 28 times faster than the best-known competitor, with only a slight compromise in accuracy. | Xiaoyu Xu, Weiren Yu |  |
| 57 |  |  [Unlocking the Potential of Health Data with Decentralised Search in Personal Health Datastores](https://doi.org/10.1145/3589335.3651454) |  | 0 | In the digital age, where health data and digital lives converge, data privacy and control are crucial. The advent of AI and Large Language Models (LLMs) brings advanced data analysis and healthcare predictions, but also privacy concerns. The ESPRESSO project 1 asserts that for AI to be trustworthy and effective in healthcare, it must prioritize user control over corporate interests. The shift towards decentralized personal online datastores (pods) and Solid 2 principles represents a new era of private, controllable Web interactions, balancing AI data protection and machine intelligence. This balance is particularly important for applications involving health data. However, decentralization poses challenges, particularly in secure, efficient data search and data retrieval, that need to be addressed first. We argue that a decentralized search system that provides a large-scale search across Solid pods, while considering data owners' control of their data and users' different access rights, is crucial for this new paradigm. In this paper, we describe how our current decentralized search system's prototype (ESPRESSO) helps to query structured and unstructured personal health data in Solid servers. The paper also describes a search scenario that shows how ESPRESSO can search health data combined with fitness personal data stored in different personal datastores | Mohamed Ragab, Yury Savateev, Helen Oliver, Thanassis Tiropanis, Alexandra Poulovassilis, Adriane Chapman, George Roussos |  |
| 58 |  |  [TATKC: A Temporal Graph Neural Network for Fast Approximate Temporal Katz Centrality Ranking](https://doi.org/10.1145/3589334.3645432) |  | 0 | Numerous real-world networks are represented as temporal graphs, which capture the dynamics of connections over time. Identifying important nodes on temporal graphs has a plethora of real-life applications, such as information propagation and influential user identification, etc. Temporal Katz centrality, a popular temporal metric, gauges the importance of nodes by taking into account both the number of temporal walks and the timespan between the interactions. The computation of traditional temporal Katz centrality is computationally expensive, especially when applied to massive temporal graphs. Therefore, in this paper, we design a temporal graph neural network to approximate temporal Katz centrality computation. To the best of our knowledge, we are the first to address temporal Katz centrality computation purely from a learning-based perspective. We propose a time-injected self-attention model that consists of two phases. In the first phase, we utilize a time-injected self-attention mechanism to acquire node representations that encompass both structural information and temporal relevance. The second phase is structured as a multi-layer perceptron (MLP) which uses the learned node representation to predict node rankings. Furthermore, normalization and neighbor sampling strategies are integrated into the model to enhance its overall performance. Extensive experiments on real-world networks demonstrate the efficiency and accuracy of TATKC. | Tianming Zhang, Junkai Fang, Zhengyi Yang, Bin Cao, Jing Fan |  |
| 59 |  |  [FairSync: Ensuring Amortized Group Exposure in Distributed Recommendation Retrieval](https://doi.org/10.1145/3589334.3645413) |  | 0 | In pursuit of fairness and balanced development, recommender systems (RS) often prioritize group fairness, ensuring that specific groups maintain a minimum level of exposure over a given period. For example, RS platforms aim to ensure adequate exposure for new providers or specific categories of items according to their needs. Modern industry RS usually adopts a two-stage pipeline: stage-1 (retrieval stage) retrieves hundreds of candidates from millions of items distributed across various servers, and stage-2 (ranking stage) focuses on presenting a small-size but accurate selection from items chosen in stage-1. Existing efforts for ensuring amortized group exposures focus on stage-2, however, stage-1 is also critical for the task. Without a high-quality set of candidates, the stage-2 ranker cannot ensure the required exposure of groups. Previous fairness-aware works designed for stage-2 typically require accessing and traversing all items. In stage-1, however, millions of items are distributively stored in servers, making it infeasible to traverse all of them. How to ensure group exposures in the distributed retrieval process is a challenging question. To address this issue, we introduce a model named FairSync, which transforms the problem into a constrained distributed optimization problem. Specifically, FairSync resolves the issue by moving it to the dual space, where a central node aggregates historical fairness data into a vector and distributes it to all servers. To trade off the efficiency and accuracy, the gradient descent technique is used to periodically update the parameter of the dual vector. The experiment results on two public recommender retrieval datasets showcased that FairSync outperformed all the baselines, achieving the desired minimum level of exposures while maintaining a high level of retrieval accuracy. | Chen Xu, Jun Xu, Yiming Ding, Xiao Zhang, Qi Qi |  |
| 60 |  |  [Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism](https://doi.org/10.1145/3589334.3645482) |  | 0 | Traditional search engines usually provide identical search results for all users, overlooking individual preferences. To counter this limitation, personalized search has been developed to re-rank results based on user preferences derived from query logs. Deep learning-based personalized search methods have shown promise, but they rely heavily on abundant training data, making them susceptible to data sparsity challenges. This paper proposes a Cognitive Personalized Search (CoPS) model, which integrates Large Language Models (LLMs) with a cognitive memory mechanism inspired by human cognition. CoPS employs LLMs to enhance user modeling and user search experience. The cognitive memory mechanism comprises sensory memory for quick sensory responses, working memory for sophisticated cognitive responses, and long-term memory for storing historical interactions. CoPS handles new queries using a three-step approach: identifying re-finding behaviors, constructing user profiles with relevant historical information, and ranking documents based on personalized query intent. Experiments show that CoPS outperforms baseline models in zero-shot scenarios. | Yujia Zhou, Qiannan Zhu, Jiajie Jin, Zhicheng Dou |  |
| 61 |  |  [Asking Multimodal Clarifying Questions in Mixed-Initiative Conversational Search](https://doi.org/10.1145/3589334.3645483) |  | 0 | In mixed-initiative conversational search systems, clarifying questions are used to help users who struggle to express their intentions in a single query. These questions aim to uncover user's information needs and resolve query ambiguities. We hypothesize that in scenarios where multimodal information is pertinent, the clarification process can be improved by using non-textual information. Therefore, we propose to add images to clarifying questions and formulate the novel task of asking multimodal clarifying questions in open-domain, mixed-initiative conversational search systems. To facilitate research into this task, we collect a dataset named Melon that contains over 4k multimodal clarifying questions, enriched with over 14k images. We also propose a multimodal query clarification model named Marto and adopt a prompt-based, generative fine-tuning strategy to perform the training of different stages with different prompts. Several analyses are conducted to understand the importance of multimodal contents during the query clarification phase. Experimental results indicate that the addition of images leads to significant improvements of up to 90 images. Extensive analyses are also performed to show the superiority of Marto compared with discriminative baselines in terms of effectiveness and efficiency. | Yifei Yuan, Clemencia Siro, Mohammad Aliannejadi, Maarten de Rijke, Wai Lam |  |
| 62 |  |  [Benchmark and Neural Architecture for Conversational Entity Retrieval from a Knowledge Graph](https://doi.org/10.1145/3589334.3645676) |  | 0 | This paper introduces a novel information retrieval (IR) task of Conversational Entity Retrieval from a Knowledge Graph (CER-KG), which extends non-conversational entity retrieval from a knowledge graph (KG) to the conversational scenario. The user queries in CER-KG dialog turns may rely on the results of the preceding turns, which are KG entities. Similar to the conversational document IR, CER-KG can be viewed as a sequence of interrelated ranking tasks. To enable future research on CER-KG, we created QBLink-KG, a publicly available benchmark that was adapted from QBLink, a benchmark for text-based conversational reading comprehension of Wikipedia. As an initial approach to CER-KG, we experimented with Transformer- and LSTM-based query encoders in combination with the Neural Architecture for Conversational Entity Retrieval (NACER), our proposed feature-based neural architecture for entity ranking in CER-KG. NACER computes the ranking score of a candidate KG entity by taking into account diverse lexical and semantic matching signals between various KG components in its neighborhood, such as entities, categories, and literals, as well as entities in the results of the preceding turns in dialog history. The reported experimental results reveal the key challenges of CER-KG along with the possible directions for new approaches to this task. | Mona Zamiri, Yao Qiang, Fedor Nikolaev, Dongxiao Zhu, Alexander Kotov |  |
| 63 |  |  [Intelligent Model Update Strategy for Sequential Recommendation](https://doi.org/10.1145/3589334.3645316) |  | 0 | Modern online platforms are increasingly employing recommendation systems to address information overload and improve user engagement. There is an evolving paradigm in this research field that recommendation network learning occurs both on the cloud and on edges with knowledge transfer in between (i.e., edge-cloud collaboration). Recent works push this filed further by enabling edge-specific context-aware adaptivity, where model parameters are updated in real-time based on incoming on-edge data. However, we argue that frequent data exchanges between the cloud and edges often lead to inefficiency and waste of communication/computation resources, as considerable parameter updates might be redundant. To investigate this problem, we introduce Intelligent Edge-Cloud Parameter Request Model (IntellectReq). IntellectReq is designed to operate on edge, evaluating the cost-benefit landscape of parameter requests with minimal computation and communication overhead. We formulate this as a novel learning task, aimed at the detection of out-of-distribution data, thereby fine-tuning adaptive communication strategies. Further, we employ statistical mapping techniques to convert real-time user behavior into a normal distribution, thereby employing multi-sample outputs to quantify the model's uncertainty and thus its generalization capabilities. Rigorous empirical validation on four widely-adopted benchmarks evaluates our approach, evidencing a marked improvement in the efficiency and generalizability of edge-cloud collaborative and dynamic recommendation systems. | Zheqi Lv, Wenqiao Zhang, Zhengyu Chen, Shengyu Zhang, Kun Kuang |  |
| 64 |  |  [A Data-Centric Multi-Objective Learning Framework for Responsible Recommendation Systems](https://doi.org/10.1145/3589334.3645324) |  | 0 | Recommendation systems effectively guide users in locating their desired information within extensive content repositories. Generally, a recommendation model is optimized to enhance accuracy metrics from a user utility standpoint, such as click-through rate or matching relevance. However, a responsible industrial recommendation system must address not only user utility (responsibility to users) but also other objectives, including increasing platform revenue (responsibility to platforms), ensuring fairness (responsibility to content creators), and maintaining unbiasedness (responsibility to long-term healthy development). Multi-objective learning is a potent approach for achieving responsible recommendation systems. Nevertheless, current methods encounter two challenges: difficulty in scaling to heterogeneous objectives within a unified framework, and inadequate controllability over objective priority during optimization, leading to uncontrollable solutions. In this paper, we present a data-centric optimization framework, MoRec, which unifies the learning of diverse objectives. MoRec is a tri-level framework: the outer level manages the balance between different objectives, utilizing a proportional-integral-derivative (PID)-based controller to ensure a preset regularization on the primary objective. The middle level transforms objective-aware optimization into data sampling weights using sign gradients. The inner level employs a standard optimizer to update model parameters with the sampled data. Consequently, MoRec can flexibly support various objectives while maintaining the original model intact. Comprehensive experiments on two public datasets and one industrial dataset showcase the effectiveness, controllability, flexibility, and Pareto efficiency of MoRec, making it highly suitable for real-world implementation. | Xu Huang, Jianxun Lian, Hao Wang, Hao Liao, Defu Lian, Xing Xie |  |
| 65 |  |  [Collaborative Large Language Model for Recommender Systems](https://doi.org/10.1145/3589334.3645347) |  | 0 | Recently, there is a growing interest in developing next-generation recommender systems (RSs) based on pretrained large language models (LLMs), fully utilizing their encoded knowledge and reasoning ability. However, the semantic gap between natural language and recommendation tasks is still not well addressed, leading to multiple issues such as spuriously-correlated user/item descriptors, ineffective language modeling on user/item contents, and inefficient recommendations via auto-regression, etc. In this paper, we propose CLLM4Rec, the first generative RS that tightly integrates the LLM paradigm and ID paradigm of RS, aiming to address the above challenges simultaneously. We first extend the vocabulary of pretrained LLMs with user/item ID tokens to faithfully model the user/item collaborative and content semantics. Accordingly, in the pretraining stage, a novel soft+hard prompting strategy is proposed to effectively learn user/item collaborative/content token embeddings via language modeling on RS-specific corpora established from user-item interactions and user/item features, where each document is split into a prompt consisting of heterogeneous soft (user/item) tokens and hard (vocab) tokens and a main text consisting of homogeneous item tokens or vocab tokens that facilitates stable and effective language modeling. In addition, a novel mutual regularization strategy is introduced to encourage the CLLM4Rec to capture recommendation-oriented information from user/item contents. Finally, we propose a novel recommendation-oriented finetuning strategy for CLLM4Rec, where an item prediction head with multinomial likelihood is added to the pretrained CLLM4Rec backbone to predict hold-out items based on the soft+hard prompts established from masked user-item interaction history, where recommendations of multiple items can be generated efficiently. | Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, Jundong Li |  |
| 66 |  |  [Harnessing Large Language Models for Text-Rich Sequential Recommendation](https://doi.org/10.1145/3589334.3645358) |  | 0 | Recent advances in Large Language Models (LLMs) have been changing the paradigm of Recommender Systems (RS). However, when items in the recommendation scenarios contain rich textual information, such as product descriptions in online shopping or news headlines on social media, LLMs require longer texts to comprehensively depict the historical user behavior sequence. This poses significant challenges to LLM-based recommenders, such as over-length limitations, extensive time and space overheads, and suboptimal model performance. To this end, in this paper, we design a novel framework for harnessing Large Language Models for Text-Rich Sequential Recommendation (LLM-TRSR). Specifically, we first propose to segment the user historical behaviors and subsequently employ an LLM-based summarizer for summarizing these user behavior blocks. Particularly, drawing inspiration from the successful application of Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) models in user modeling, we introduce two unique summarization techniques in this paper, respectively hierarchical summarization and recurrent summarization. Then, we construct a prompt text encompassing the user preference summary, recent user interactions, and candidate item information into an LLM-based recommender, which is subsequently fine-tuned using Supervised Fine-Tuning (SFT) techniques to yield our final recommendation model. We also use Low-Rank Adaptation (LoRA) for Parameter-Efficient Fine-Tuning (PEFT). We conduct experiments on two public datasets, and the results clearly demonstrate the effectiveness of our approach. | Zhi Zheng, Wenshuo Chao, Zhaopeng Qiu, Hengshu Zhu, Hui Xiong |  |
| 67 |  |  [ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction](https://doi.org/10.1145/3589334.3645396) |  | 0 | Click-through rate (CTR) prediction has become increasingly indispensable for various Internet applications. Traditional CTR models convert the multi-field categorical data into ID features via one-hot encoding, and extract the collaborative signals among features. Such a paradigm suffers from the problem of semantic information loss. Another line of research explores the potential of pretrained language models (PLMs) for CTR prediction by converting input data into textual sentences through hard prompt templates. Although semantic signals are preserved, they generally fail to capture the collaborative information (e.g., feature interactions, pure ID features), not to mention the unacceptable inference overhead brought by the huge model size. In this paper, we aim to model both the semantic knowledge and collaborative knowledge for accurate CTR estimation, and meanwhile address the inference inefficiency issue. To benefit from both worlds and close their gaps, we propose a novel model-agnostic framework (i.e., ClickPrompt), where we incorporate CTR models to generate interaction-aware soft prompts for PLMs. We design a prompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM has to recover the masked tokens based on the language context, as well as the soft prompts generated by CTR model. The collaborative and semantic knowledge from ID and textual features would be explicitly aligned and interacted via the prompt interface. Then, we can either tune the CTR model with PLM for superior performance, or solely tune the CTR model without PLM for inference efficiency. Experiments on four real-world datasets validate the effectiveness of ClickPrompt compared with existing baselines. | Jianghao Lin, Bo Chen, Hangyu Wang, Yunjia Xi, Yanru Qu, Xinyi Dai, Kangning Zhang, Ruiming Tang, Yong Yu, Weinan Zhang |  |
| 68 |  |  [Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion](https://doi.org/10.1145/3589334.3645404) |  | 0 | Large Language Models (LLMs) excel at tackling various natural language tasks. However, due to the significant costs involved in re-training or fine-tuning them, they remain largely static and difficult to personalize. Nevertheless, a variety of applications could benefit from generations that are tailored to users' preferences, goals, and knowledge. Among them is web search, where knowing what a user is trying to accomplish, what they care about, and what they know can lead to improved search experiences. In this work, we propose a novel and general approach that augments an LLM with relevant context from users' interaction histories with a search engine in order to personalize its outputs. Specifically, we construct an entity-centric knowledge store for each user based on their search and browsing activities on the web, which is then leveraged to provide contextually relevant LLM prompt augmentations. This knowledge store is light-weight, since it only produces user-specific aggregate projections of interests and knowledge onto public knowledge graphs, and leverages existing search log infrastructure, thereby mitigating the privacy, compliance, and scalability concerns associated with building deep user profiles for personalization. We validate our approach on the task of contextual query suggestion, which requires understanding not only the user's current search context but also what they historically know and care about. Through a number of experiments based on human evaluation, we show that our approach is significantly better than several other LLM-powered baselines, generating query suggestions that are contextually more relevant, personalized, and useful. | Jinheon Baek, Nirupama Chandrasekaran, Silviu Cucerzan, Allen Herring, Sujay Kumar Jauhar |  |
| 69 |  |  [Top-Personalized-K Recommendation](https://doi.org/10.1145/3589334.3645417) |  | 0 | The conventional top-K recommendation, which presents the top-K items with the highest ranking scores, is a common practice for generating personalized ranking lists. However, is this fixed-size top-K recommendation the optimal approach for every user's satisfaction? Not necessarily. We point out that providing fixed-size recommendations without taking into account user utility can be suboptimal, as it may unavoidably include irrelevant items or limit the exposure to relevant ones. To address this issue, we introduce Top-Personalized-K Recommendation, a new recommendation task aimed at generating a personalized-sized ranking list to maximize individual user satisfaction. As a solution to the proposed task, we develop a model-agnostic framework named PerK. PerK estimates the expected user utility by leveraging calibrated interaction probabilities, subsequently selecting the recommendation size that maximizes this expected utility. Through extensive experiments on real-world datasets, we demonstrate the superiority of PerK in Top-Personalized-K recommendation task. We expect that Top-Personalized-K recommendation has the potential to offer enhanced solutions for various real-world recommendation scenarios, based on its great compatibility with existing models. | Wonbin Kweon, SeongKu Kang, Sanghwan Jang, Hwanjo Yu |  |
| 70 |  |  [AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems](https://doi.org/10.1145/3589334.3645537) |  | 0 | Recently, there has been an emergence of employing LLM-powered agents as believable human proxies, based on their remarkable decision-making capability. However, existing studies mainly focus on simulating human dialogue. Human non-verbal behaviors, such as item clicking in recommender systems, although implicitly exhibiting user preferences and could enhance the modeling of users, have not been deeply explored. The main reasons lie in the gap between language modeling and behavior modeling, as well as the incomprehension of LLMs about user-item relations. To address this issue, we propose AgentCF for simulating user-item interactions in recommender systems through agent-based collaborative filtering. We creatively consider not only users but also items as agents, and develop a collaborative learning approach that optimizes both kinds of agents together. Specifically, at each time step, we first prompt the user and item agents to interact autonomously. Then, based on the disparities between the agents' decisions and real-world interaction records, user and item agents are prompted to reflect on and adjust the misleading simulations collaboratively, thereby modeling their two-sided relations. The optimized agents can also propagate their preferences to other agents in subsequent interactions, implicitly capturing the collaborative filtering idea. Overall, the optimized agents exhibit diverse interaction behaviors within our framework, including user-item, user-user, item-item, and collective interactions. The results show that these agents can demonstrate personalized behaviors akin to those of real-world individuals, sparking the development of next-generation user behavior simulation. | Junjie Zhang, Yupeng Hou, Ruobing Xie, Wenqi Sun, Julian J. McAuley, Wayne Xin Zhao, Leyu Lin, JiRong Wen |  |
| 71 |  |  [Enhancing Recommendation Accuracy and Diversity with Box Embedding: A Universal Framework](https://doi.org/10.1145/3589334.3645577) |  | 0 | Recommender systems have emerged as an indispensable mean to meet personalized interests of users and alleviate information overload. Despite the great success, accuracy-oriented recommendation models are creating information cocoons, i.e., it is becoming increasingly difficult for users to see other items they might be interested in. Although recent studies start paying attention to enhancing recommendation diversity, models based on point embedding fail to describe the range of user preferences and item features well, which is essential for diversified matching. To this end, we propose LCD-UC, a novel List-Check-Decide framework with UnCertainty masking based on box embedding to improve recommendation diversity with recommendation accuracy maintained. Specifically, LCD-UC creates hypercubes to represent users and items using box embedding for high model flexibility and expressiveness. Then, a hypercube similarity scoring function is designed to measure the similarity between hypercubes representing users and items. To make a balance between the accuracy and diversity of recommendations and achieve personalized diversity needs, we further develop a user-item pairwise attention mechanism as well as a user uncertainty masking mechanism in LCD-UC. Besides, we present two new metrics for better evaluation on recommendation diversity, which address the issue that existing metrics only consider the coverage of categories while ignore the frequency of categories. The extensive experiments on three real-world datasets show that LCD-UC can improve both recommendation accuracy and diversity over three base models, and is superior to six state-of-the-art recommendation models. An online 10-day AB test also demonstrates that LCD-UC can improve the performance of a real-world advertising system. | Cheng Wu, Shaoyun Shi, Chaokun Wang, Ziyang Liu, Wang Peng, Wenjin Wu, Dongying Kong, Han Li, Kun Gai |  |
| 72 |  |  [Is Contrastive Learning Necessary? A Study of Data Augmentation vs Contrastive Learning in Sequential Recommendation](https://doi.org/10.1145/3589334.3645661) |  | 0 | Sequential recommender systems (SRS) are designed to predict users' future behaviors based on their historical interaction data. Recent research has increasingly utilized contrastive learning (CL) to leverage unsupervised signals to alleviate the data sparsity issue in SRS. In general, CL-based SRS first augments the raw sequential interaction data by using data augmentation strategies and employs a contrastive training scheme to enforce the representations of those sequences from the same raw interaction data to be similar. Despite the growing popularity of CL, data augmentation, as a basic component of CL, has not received sufficient attention. This raises the question: Is it possible to achieve superior recommendation results solely through data augmentation? To answer this question, we benchmark eight widely used data augmentation strategies, as well as state-of-the-art CL-based SRS methods, on four real-world datasets under both warm- and cold-start settings. Intriguingly, the conclusion drawn from our study is that, certain data augmentation strategies can achieve similar or even superior performance compared with some CL-based methods, demonstrating the potential to significantly alleviate the data sparsity issue with fewer computational overhead. We hope that our study can further inspire more fundamental studies on the key functional components of complex CL techniques. Our processed datasets and codes are available at https://github.com/AIM-SE/DA4Rec. | Peilin Zhou, YouLiang Huang, Yueqi Xie, Jingqi Gao, Shoujin Wang, Jae Boum Kim, Sunghun Kim |  |
| 73 |  |  [Can Small Language Models be Good Reasoners for Sequential Recommendation?](https://doi.org/10.1145/3589334.3645671) |  | 0 | Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larger teacher model. The rationales generated by the teacher model are then utilized as labels to distill the downstream smaller student model (e.g., LLaMA2-7B). In this way, the student model acquires the step-by-step reasoning capabilities in recommendation tasks. We encode the generated rationales from the student model into a dense vector, which empowers recommendation in both ID-based and ID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of SLIM over state-of-the-art baselines, and further analysis showcasing its ability to generate meaningful recommendation reasoning at affordable costs. | Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Liang Pang, Xiao Wang |  |
| 74 |  |  [Predictive Relevance Uncertainty for Recommendation Systems](https://doi.org/10.1145/3589334.3645689) |  | 0 | Click-through Rate (CTR) module is the foundation block of recommendation system and used for search, content selection, advertising, video streaming etc. CTR is modelled as a classification problem and extensive research is done to improve the CTR models. However, uncertainty method for these models are still an unexplored area. In this work we analyse popular uncertainty methods in the context of recommendation system. We found that popular uncertainty models fails to capture the predictive uncertainty of the CTR model that exist unique to the recommendation models and is not prevalent in the traditional classification models. We empirical show why a different uncertainty measure is required for the recommendation system CTR prediction models. We propose PRU (Predictive Relevance Uncertainty), a single forward pass uncertainty approach for a sample as a distance from the predictive relevance samples of the training data. We show the efficacy of the proposed predictive relevance uncertainty (PRU) on selective prediction. Further, we demonstrate the utility of the proposed framework on the downstream task of OOD detection and active learning while maintaining the latency of a single pass deterministic model. | Charul Paliwal, Anirban Majumder, Sivaramakrishnan Kaveri |  |
| 75 |  |  [Decentralized Collaborative Learning with Adaptive Reference Data for On-Device POI Recommendation](https://doi.org/10.1145/3589334.3645696) |  | 0 | In Location-based Social Networks, Point-of-Interest (POI) recommendation helps users discover interesting places. There is a trend to move from the cloud-based model to on-device recommendations for privacy protection and reduced server reliance. Due to the scarcity of local user-item interactions on individual devices, solely relying on local instances is not adequate. Collaborative Learning (CL) emerges to promote model sharing among users, where reference data is an intermediary that allows users to exchange their soft decisions without directly sharing their private data or parameters, ensuring privacy and benefiting from collaboration. However, existing CL-based recommendations typically use a single reference for all users. Reference data valuable for one user might be harmful to another, given diverse user preferences. Users may not offer meaningful soft decisions on items outside their interest scope. Consequently, using the same reference data for all collaborations can impede knowledge exchange and lead to sub-optimal performance. To address this gap, we introduce the Decentralized Collaborative Learning with Adaptive Reference Data (DARD) framework, which crafts adaptive reference data for effective user collaboration. It first generates a desensitized public reference data pool with transformation and probability data generation methods. For each user, the selection of adaptive reference data is executed in parallel by training loss tracking and influence function. Local models are trained with individual private data and collaboratively with the geographical and semantic neighbors. During the collaboration between two users, they exchange soft decisions based on a combined set of their adaptive reference data. Our evaluations across two real-world datasets highlight DARD's superiority in recommendation performance and addressing the scarcity of available reference data. | Ruiqi Zheng, Liang Qu, Tong Chen, Lizhen Cui, Yuhui Shi, Hongzhi Yin |  |
| 76 |  |  [Towards Efficient Communication and Secure Federated Recommendation System via Low-rank Training](https://doi.org/10.1145/3589334.3645702) |  | 0 | Federated Recommendation (FedRec) systems have emerged as a solution to safeguard users' data in response to growing regulatory concerns. However, one of the major challenges in these systems lies in the communication costs that arise from the need to transmit neural network models between user devices and a central server. Prior approaches to these challenges often lead to issues such as computational overheads, model specificity constraints, and compatibility issues with secure aggregation protocols. In response, we propose a novel framework, called Correlated Low-rank Structure (CoLR), which leverages the concept of adjusting lightweight trainable parameters while keeping most parameters frozen. Our approach substantially reduces communication overheads without introducing additional computational burdens. Critically, our framework remains fully compatible with secure aggregation protocols, including the robust use of Homomorphic Encryption. The approach resulted in a reduction of up to 93.75 recommendation performance across datasets. Code for reproducing our experiments can be found at https://github.com/NNHieu/CoLR-FedRec. | NgocHieu Nguyen, TuanAnh Nguyen, Tuan Nguyen, Vu Tien Hoang, Dung D. Le, KokSeng Wong |  |
| 77 |  |  [A User-State Based Interest Transfer Network for Cross-Domain Recommendation](https://doi.org/10.1145/3589335.3651465) |  | 0 | Cross-domain recommendation (CDR) has emerged as a promising approach to improve click-through rate (CTR) in the target domain by effectively transferring user interests from the source domain. However, existing methods either use a uniform interest transfer function or focus on user-level personalized transfer functions, neglecting the fact that the transition of user states in the target domain also influence the interests in the source domain. To address this issue, we present User-State based Interest Transfer network (USIT), a novel method that takes into account the user state evolution. USIT contains two main components: a User-State Transition module (UST) and a State-Level Interests Transfer module (SLIT). UST models the evolution of user states by predicting the next state in the target domain. As the user's state evolves, SLIT adaptively weights the interests by interest-level mask attention in the source domain. Extensive offline experiments and online A/B tests demonstrate that our proposed USIT method significantly outperforms current state-of-the-art models in CDR scenarios. Currently, we have deployed it on NetEase Cloud Music, affecting millions of users. | Pingjun Pan, Jialu Wang, Tingting Zhou, Wenyu Yang, Hongxiang Chen |  |
| 78 |  |  [Position Bias Estimation with Item Embedding for Sparse Dataset](https://doi.org/10.1145/3589335.3651546) |  | 0 | Estimating position bias is a well-known challenge in Learning to Rank (L2R). Click data in e-commerce applications, such as targeted advertisements and search engines, provides implicit but abundant feedback to improve personalized rankings. However, click data inherently includes various biases like position bias. Based on the position-based click model, Result Randomization and Regression Expectation-Maximization algorithm (REM) have been proposed to estimate position bias, but they require various paired observations of (item, position). In real-world scenarios of advertising, marketers frequently display advertisements in a fixed pre-determined order, which creates difficulties in estimation due to the limited availability of various pairs in the training data, resulting in a sparse dataset. We propose a variant of the REM that utilizes item embeddings to alleviate the sparsity of (item, position). Using a public dataset and internal carousel advertisement click dataset, we empirically show that item embedding with Latent Semantic Indexing (LSI) and Variational Auto-Encoder (VAE) improves the accuracy of position bias estimation and the estimated position bias enhances Learning to Rank performance. We also show that LSI is more effective as an embedding creation method for position bias estimation. | Shion Ishikawa, Yun Ching Liu, Youngjoo Chung, Yu Hirate |  |
| 79 |  |  [When Federated Recommendation Meets Cold-Start Problem: Separating Item Attributes and User Interactions](https://doi.org/10.1145/3589334.3645525) |  | 0 | Federated recommendation system usually trains a global model on the server without direct access to users' private data on their own devices. However, this separation of the recommendation model and users' private data poses a challenge in providing quality service, particularly when it comes to new items, namely cold-start recommendations in federated settings. This paper introduces a novel method called Item-aligned Federated Aggregation (IFedRec) to address this challenge. It is the first research work in federated recommendation to specifically study the cold-start scenario. The proposed method learns two sets of item representations by leveraging item attributes and interaction records simultaneously. Additionally, an item representation alignment mechanism is designed to align two item representations and learn the meta attribute network at the server within a federated learning framework. Experiments on four benchmark datasets demonstrate IFedRec's superior performance for cold-start scenarios. Furthermore, we also verify IFedRec owns good robustness when the system faces limited client participation and noise injection, which brings promising practical application potential in privacy-protection enhanced federated recommendation systems. The implementation code is available | Chunxu Zhang, Guodong Long, Tianyi Zhou, Zijian Zhang, Peng Yan, Bo Yang |  |
| 80 |  |  [Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai](https://doi.org/10.1145/3589335.3648334) |  | 0 | In the recommender system of Meituan Waimai, we are dealing with ever-lengthening user behavior sequences, which pose an increasing challenge to modeling user preference effectively. Existing sequential recommendation models often fail to capture long-term dependencies or are too complex, complicating the fulfillment of Meituan Waimai's unique business needs. To better model user interests, we consider selecting relevant sub-sequences from users' extensive historical behaviors based on their preferences. In this specific scenario, we've noticed that the contexts in which users interact have a significant impact on their preferences. For this purpose, we introduce a novel method called Context-based Fast Recommendation Strategy to tackle the issue of long sequences. We first identify contexts that share similar user preferences with the target context and then locate the corresponding PoIs based on these identified contexts. This approach eliminates the necessity to select a sub-sequence for every candidate PoI, thereby avoiding high time complexity. Specifically, we implement a prototype-based approach to pinpoint contexts that mirror similar user preferences. To amplify accuracy and interpretability, we employ JS divergence of PoI attributes such as categories and prices as a measure of similarity between contexts. A temporal graph integrating both prototype and context nodes helps incorporate temporal information. We then identify appropriate prototypes considering both target contexts and short-term user preferences. Following this, we utilize contexts aligned with these prototypes to generate a sub-sequence, aimed at predicting CTR and CTCVR scores with target attention. Since its inception in 2023, this strategy has been adopted in Meituan Waimai's display recommender system, leading to a 4.6 in CTR and a 4.2 | Zhichao Feng, Junjie Xie, Kaiyuan Li, Yu Qin, Pengfei Wang, Qianzhong Li, Bin Yin, Xiang Li, Wei Lin, Shangguang Wang |  |
| 81 |  |  [Large Language Models as Data Augmenters for Cold-Start Item Recommendation](https://doi.org/10.1145/3589335.3651532) |  | 0 | The reasoning and generalization capabilities of LLMs can help us better understand user preferences and item characteristics, offering exciting prospects to enhance recommendation systems. Though effective while user-item interactions are abundant, conventional recommendation systems struggle to recommend cold-start items without historical interactions. To address this, we propose utilizing LLMs as data augmenters to bridge the knowledge gap on cold-start items during training. We employ LLMs to infer user preferences for cold-start items based on textual description of user historical behaviors and new item descriptions. The augmented training signals are then incorporated into learning the downstream recommendation models through an auxiliary pairwise loss. Through experiments on public Amazon datasets, we demonstrate that LLMs can effectively augment the training signals for cold-start items, leading to significant improvements in cold-start item recommendation for various recommendation models. | Jianling Wang, Haokai Lu, James Caverlee, Ed H. Chi, Minmin Chen |  |
| 82 |  |  [Filter Bubble or Homogenization? Disentangling the Long-Term Effects of Recommendations on User Consumption Patterns](https://doi.org/10.1145/3589334.3645497) |  | 0 | Recommendation algorithms play a pivotal role in shaping our media choices, which makes it crucial to comprehend their long-term impact on user behavior. These algorithms are often linked to two critical outcomes: homogenization, wherein users consume similar content despite disparate underlying preferences, and the filter bubble effect, wherein individuals with differing preferences only consume content aligned with their preferences (without much overlap with other users). Prior research assumes a trade-off between homogenization and filter bubble effects and then shows that personalized recommendations mitigate filter bubbles by fostering homogenization. However, because of this assumption of a tradeoff between these two effects, prior work cannot develop a more nuanced view of how recommendation systems may independently impact homogenization and filter bubble effects. We develop a more refined definition of homogenization and the filter bubble effect by decomposing them into two key metrics: how different the average consumption is between users (inter-user diversity) and how varied an individual's consumption is (intra-user diversity). We then use a novel agent-based simulation framework that enables a holistic view of the impact of recommendation systems on homogenization and filter bubble effects. Our simulations show that traditional recommendation algorithms (based on past behavior) mainly reduce filter bubbles by affecting inter-user diversity without significantly impacting intra-user diversity. Building on these findings, we introduce two new recommendation algorithms that take a more nuanced approach by accounting for both types of diversity. | Md Sanzeed Anwar, Grant Schoenebeck, Paramveer S. Dhillon |  |
| 83 |  |  [Can One Embedding Fit All? A Multi-Interest Learning Paradigm Towards Improving User Interest Diversity Fairness](https://doi.org/10.1145/3589334.3645662) |  | 0 | Recommender systems (RSs) have gained widespread applications across various domains owing to the superior ability to capture users' interests. However, the complexity and nuanced nature of users' interests, which span a wide range of diversity, pose a significant challenge in delivering fair recommendations. In practice, user preferences vary significantly; some users show a clear preference toward certain item categories, while others have a broad interest in diverse ones. Even though it is expected that all users should receive high-quality recommendations, the effectiveness of RSs in catering to this disparate interest diversity remains under-explored. In this work, we investigate whether users with varied levels of interest diversity are treated fairly. Our empirical experiments reveal an inherent disparity: users with broader interests often receive lower-quality recommendations. To mitigate this, we propose a multi-interest framework that uses multiple (virtual) interest embeddings rather than single ones to represent users. Specifically, the framework consists of stacked multi-interest representation layers, which include an interest embedding generator that derives virtual interests from shared parameters, and a center embedding aggregator that facilitates multi-hop aggregation. Experiments demonstrate the effectiveness of the framework in achieving better trade-off between fairness and utility across various datasets and backbones. | Yuying Zhao, Minghua Xu, Huiyuan Chen, Yuzhong Chen, Yiwei Cai, Rashidul Islam, Yu Wang, Tyler Derr |  |
| 84 |  |  [Uplift Modeling for Target User Attacks on Recommender Systems](https://doi.org/10.1145/3589334.3645403) |  | 0 | Recommender systems are vulnerable to injective attacks, which inject limited fake users into the platforms to manipulate the exposure of target items to all users. In this work, we identify that conventional injective attackers overlook the fact that each item has its unique potential audience, and meanwhile, the attack difficulty across different users varies. Blindly attacking all users will result in a waste of fake user budgets and inferior attack performance. To address these issues, we focus on an under-explored attack task called target user attacks, aiming at promoting target items to a particular user group. In addition, we formulate the varying attack difficulty as heterogeneous treatment effects through a causal lens and propose an Uplift-guided Budget Allocation (UBA) framework. UBA estimates the treatment effect on each target user and optimizes the allocation of fake user budgets to maximize the attack performance. Theoretical and empirical analysis demonstrates the rationality of treatment effect estimation methods of UBA. By instantiating UBA on multiple attackers, we conduct extensive experiments on three datasets under various settings with different target items, target users, fake user budgets, victim models, and defense models, validating the effectiveness and robustness of UBA. | Wenjie Wang, Changsheng Wang, Fuli Feng, Wentao Shi, Daizong Ding, TatSeng Chua |  |
| 85 |  |  [TikTok and the Art of Personalization: Investigating Exploration and Exploitation on Social Media Feeds](https://doi.org/10.1145/3589334.3645600) |  | 0 | Recommendation algorithms for social media feeds often function as black boxes from the perspective of users. We aim to detect whether social media feed recommendations are personalized to users, and to characterize the factors contributing to personalization in these feeds. We introduce a general framework to examine a set of social media feed recommendations for a user as a timeline. We label items in the timeline as the result of exploration vs. exploitation of the user's interests on the part of the recommendation algorithm and introduce a set of metrics to capture the extent of personalization across user timelines. We apply our framework to a real TikTok dataset and validate our results using a baseline generated from automated TikTok bots, as well as a randomized baseline. We also investigate the extent to which factors such as video viewing duration, liking, and following drive the personalization of content on TikTok. Our results demonstrate that our framework produces intuitive and explainable results, and can be used to audit and understand personalization in social media feeds. | Karan Vombatkere, Sepehr Mousavi, Savvas Zannettou, Franziska Roesner, Krishna P. Gummadi |  |
| 86 |  |  [Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits](https://doi.org/10.1145/3589334.3645420) |  | 0 | Web-based applications such as chatbots, search engines and news recommendations continue to grow in scale and complexity with the recent surge in the adoption of LLMs. Online model selection has thus garnered increasing attention due to the need to choose the best model among a diverse set while balancing task reward and exploration cost. Organizations faces decisions like whether to employ a costly API-based LLM or a locally finetuned small LLM, weighing cost against performance. Traditional selection methods often evaluate every candidate model before choosing one, which are becoming impractical given the rising costs of training and finetuning LLMs. Moreover, it is undesirable to allocate excessive resources towards exploring poor-performing models. While some recent works leverage online bandit algorithm to manage such exploration-exploitation trade-off in model selection, they tend to overlook the increasing-then-converging trend in model performances as the model is iteratively finetuned, leading to less accurate predictions and suboptimal model selections. In this paper, we propose a time-increasing bandit algorithm TI-UCB, which effectively predicts the increase of model performances due to finetuning and efficiently balances exploration and exploitation in model selection. To further capture the converging points of models, we develop a change detection mechanism by comparing consecutive increase predictions. We theoretically prove that our algorithm achieves a logarithmic regret upper bound in a typical increasing bandit setting, which implies a fast convergence rate. The advantage of our method is also empirically validated through extensive experiments on classification model selection and online selection of LLMs. Our results highlight the importance of utilizing increasing-then-converging pattern for more efficient and economic model selection in the deployment of LLMs. | Yu Xia, Fang Kong, Tong Yu, Liya Guo, Ryan A. Rossi, Sungchul Kim, Shuai Li |  |
| 87 |  |  [Full-stage Diversified Recommendation: Large-scale Online Experiments in Short-video Platform](https://doi.org/10.1145/3589334.3648144) |  | 0 | The recommender systems on online platforms assist users in finding personalized information, yet this also leads to the issue of limited diversity, potentially giving rise to societal issues such as filter bubbles. Despite significant progress in diversified recommendation algorithms, they have not been extensively experimented with and evaluated for effectiveness in large-scale, full-stage industrial recommender systems. Specifically, industrial recommenders usually consist of three stages of matching, ranking, and re-ranking, in which specific characteristics lead to critical challenges for promoting both recommendation diversity and user engagement. First, user interests are partially observed due to only relevance maximization. Second, item-side feature-aware bias causes imbalanced recommendations. Last, the impact of diversity perception on user engagement stresses the necessity of explicit diversity modeling. To address these challenges in industrial systems, in this work, we deploy several existing diversified algorithms in a real-world short-video platform, including exploration-exploitation, feature-aware debiasing, and diversity optimization. We conduct large-scale online A/B testing for evaluation via online metrics of user engagement and recommendation diversity. Performance improvement across full stages demonstrates the effectiveness of these simple solutions. From comparing performance across different stages and algorithms, we identify that the ranking stage is the most suitable for real-world deployment, and the combination of debiasing and diversity optimization is a promising direction in terms of diversified recommendations. This work provides experiential guidance for the large-scale deployment of diversified algorithms and the construction of a more inclusive platform on the Web. | Nian Li, Yunzhu Pan, Chen Gao, Depeng Jin, Qingmin Liao |  |
| 88 |  |  [Improving Item-side Fairness of Multimodal Recommendation via Modality Debiasing](https://doi.org/10.1145/3589334.3648156) |  | 0 | Multimodal recommender systems have acquired applications in broad web scenarios such as e-commerce businesses and short-video platforms. Existing multimodal recommendation methods generally boost performance by introducing item-side multimodal content as supplement information. However, the common training paradigm, i.e., encoding unimodal content respectively and fusing them to fit user preference scores, makes the model biased towards items with prevailing modality content under non-uniform training data. This results in a serious item-side unfairness issue, i.e., some items with prevailing modality content are over-recommended while a large number of items don't receive adequate recommendation opportunities, leaving corresponding content providers at great disadvantage. Aiming to eliminate such modality bias and promote item-side fairness, we propose a fairness-aware modality debiasing framework based on counterfactual inference. In the training stage, we additionally introduce unimodal prediction branches to capture the modality bias. In the inference stage, we conduct a fairness-aware counterfactual inference to adaptively eliminate the modality bias. The proposed framework is model-agnostic and flexible to be implemented in various multimodal recommendation models. Extensive experiments on two datasets demonstrate that the proposed method can significantly enhance item-side fairness while providing competitive recommendation accuracy. Our proposed framework is expected to help mitigate the unfair treatment experienced by vulnerable content providers on multimedia web platforms. Codes are available in https://github.com/tsinghua-fib-lab-WWW2024-Modality-Debiasing. | Yu Shang, Chen Gao, Jiansheng Chen, Depeng Jin, Yong Li |  |
| 89 |  |  [Knowledge Enhanced Multi-intent Transformer Network for Recommendation](https://doi.org/10.1145/3589335.3648296) |  | 0 | Incorporating Knowledge Graphs into Recommendation has attracted growing attention in industry, due to the great potential of KG in providing abundant supplementary information and interpretability for the underlying models. However, simply integrating KG into recommendation usually brings in negative feedback in industry, due to the ignorance of the following two factors: i) users' multiple intents, which involve diverse nodes in KG. For example, in e-commerce scenarios, users may exhibit preferences for specific styles, brands, or colors. ii) knowledge noise, which is a prevalent issue in Knowledge Enhanced Recommendation (KGR) and even more severe in industry scenarios. The irrelevant knowledge properties of items may result in inferior model performance compared to approaches that do not incorporate knowledge. To tackle these challenges, we propose a novel approach named Knowledge Enhanced Multi-intent Transformer Network for Recommendation (KGTN), comprising two primary modules: Global Intents Modeling with Graph Transformer, and Knowledge Contrastive Denoising under Intents. Specifically, Global Intents with Graph Transformer focuses on capturing learnable user intents, by incorporating global signals from user-item-relation-entity interactions with a graph transformer, meanwhile learning intent-aware user/item representations. Knowledge Contrastive Denoising under Intents is dedicated to learning precise and robust representations. It leverages intent-aware representations to sample relevant knowledge, and proposes a local-global contrastive mechanism to enhance noise-irrelevant representation learning. Extensive experiments conducted on benchmark datasets show the superior performance of our proposed method over the state-of-the-arts. And online A/B testing results on Alibaba large-scale industrial recommendation platform also indicate the real-scenario effectiveness of KGTN. | Ding Zou, Wei Wei, Feida Zhu, Chuanyu Xu, Tao Zhang, Chengfu Huo |  |
| 90 |  |  [Modeling User Viewing Flow using Large Language Models for Article Recommendation](https://doi.org/10.1145/3589335.3648305) |  | 0 | This paper proposes the User Viewing Flow Modeling (SINGLE) method for the article recommendation task, which models the user constant preference and instant interest from user-clicked articles. Specifically, we first employ a user constant viewing flow modeling method to summarize the user's general interest to recommend articles. In this case, we utilize Large Language Models (LLMs) to capture constant user preferences from previously clicked articles, such as skills and positions. Then we design the user instant viewing flow modeling method to build interactions between user-clicked article history and candidate articles. It attentively reads the representations of user-clicked articles and aims to learn the user's different interest views to match the candidate article. Our experimental results on the Alibaba Technology Association (ATA) website show the advantage of SINGLE, achieving a 2.4 improvement over previous baseline models in the online A/B test. Our further analyses illustrate that SINGLE has the ability to build a more tailored recommendation system by mimicking different article viewing behaviors of users and recommending more appropriate and diverse articles to match user interests. | Zhenghao Liu, Zulong Chen, Moufeng Zhang, Shaoyang Duan, Hong Wen, Liangyue Li, Nan Li, Yu Gu, Ge Yu |  |
| 91 |  |  [Counterfactual Data Augmentation for Debiased Coupon Recommendations Based on Potential Knowledge](https://doi.org/10.1145/3589335.3648306) |  | 0 | In real-world coupon recommendations, the coupon allocation process is influenced by both the recommendation model trained with historical interaction data and marketing tactics aimed at specific commercial goals. These tactics can cause an imbalance in user-coupon interactions, leading to a deviation from users' natural preferences. We refer to this deviation as the matching bias. Theoretically, unbiased data which is assumed to be collected via a randomized allocating policy (i.e., without model or tactics intervention) is ideal training data because it reflects the user's natural preferences. However, obtaining unbiased data in real-world scenarios is costly and sometimes unfeasible. To address this problem, we propose a novel model-agnostic training paradigm named <u>C</u>ounterfactual <u>D</u>ata <u>A</u>ugmentation for debiased coupon recommendations based on <u>P</u>otential <u>K</u>nowledge (CDAPK) for the marketing scenario that allocates coupons with discounts. We leverage the counterfactual data augmentation technique to answer the following key question: If a user is offered a coupon that he has never seen before in his history, will he use this coupon? By creating the counterfactual interaction data and assigning labels based on the potential knowledge of the given scenario, CDAPK shifts the original data distribution into an unbiased distribution, facilitating model optimization and debiasing. The advantage of CDAPK lies in its ability to approximate the ideal states of the training data without depleting the real-world traffic flow. We implement CDAPK on five representative models: FM, DNN, NCF, MASKNET, and DEEPFM, and conduct extensive offline and online experiments against SOTA debiasing methods to validate the superiority of CDAPK. | Junpeng Fang, Gongduo Zhang, Qing Cui, Lihong Gu, Longfei Li, Jinjie Gu, Jun Zhou |  |
| 92 |  |  [User Response Modeling in Reinforcement Learning for Ads Allocation](https://doi.org/10.1145/3589335.3648310) |  | 0 | User response modeling can enhance the learning of user representations and further improve the reinforcement learning (RL) recommender agent. However, as users' behaviors are influenced by their long-term preferences and short-term stochastic factors (e.g., weather, mood, or fashion trends), it remains challenging for previous works focusing on recurrent neural network-based user response modeling. Meanwhile, due to the dynamic interests of users, it is often unrealistic to assume the dynamics of users are stationary. Drawing inspiration from opponent modeling, we propose a novel network structure, Deep User Q-Network (DUQN), incorporating a user response probabilistic model into the Q-learning ads allocation strategy to capture the effect of the non-stationary user policy on Q-values. Moreover, we utilize the Recurrent State-Space Model (RSSM) to develop the user response model, which includes deterministic and stochastic components, enabling us to fully consider user long-term preferences and short-term stochastic factors. In particular, we design a RetNet version of RSSM (R-RSSM) to support parallel computation. The R-RSSM model can be further used for multi-step predictions to enable bootstrapping over multiple steps simultaneously. Finally, we conduct extensive experiments on a large-scale offline dataset from the Meituan food delivery platform and a public benchmark. Experimental results show that our method yields superior performance to state-of-the-art (SOTA) baselines. Moreover, our model demonstrates a significant improvement in the online A/B test and has been fully deployed on the industrial Meituan platform, serving more than 500 million customers. | Zhiyuan Zhang, Qichao Zhang, Xiaoxu Wu, Xiaowen Shi, Guogang Liao, Yongkang Wang, Xingxing Wang, Dongbin Zhao |  |
| 93 |  |  [Cluster Anchor Regularization to Alleviate Popularity Bias in Recommender Systems](https://doi.org/10.1145/3589335.3648312) |  | 0 | Recommender systems are essential for finding personalized content for users on online platforms. These systems are often trained on historical user interaction data, which collects user feedback on system recommendations. This creates a feedback loop leading to popularity bias; popular content is over-represented in the data, better learned, and thus recommended even more. Less popular content struggles to reach its potential audiences. Popularity bias limits the diversity of content that users are exposed to, and makes it harder for new creators to gain traction. Existing methods to alleviate popularity bias tend to trade off the performance of popular items. In this work, we propose a new method for alleviating popularity bias in recommender systems, called the cluster anchor regularization, which partitions the large item corpus into hierarchical clusters, and then leverages the cluster information of each item to facilitate transfer learning from head items to tail items. Our results demonstrate the effectiveness of the proposed method with offline analyses and live experiments on a large-scale industrial recommendation platform, where it significantly increases tail recommendation without hurting the overall user experience. | Bo Chang, Changping Meng, He Ma, Shuo Chang, Yang Gu, Yajun Peng, Jingchen Feng, Yaping Zhang, Shuchao Bi, Ed H. Chi, Minmin Chen |  |
| 94 |  |  [MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation](https://doi.org/10.1145/3589335.3648319) |  | 0 | Compared to business-to-consumer (B2C) e-commerce systems, consumer-to-consumer (C2C) e-commerce platforms usually encounter the limited-stock problem, that is, a product can only be sold one time in a C2C system. This poses several unique challenges for click-through rate (CTR) prediction. Due to limited user interactions for each product (i.e. item), the corresponding item embedding in the CTR model may not easily converge. This makes the conventional sequence modeling based approaches cannot effectively utilize user history information since historical user behaviors contain a mixture of items with different volume of stocks. Particularly, the attention mechanism in a sequence model tends to assign higher score to products with more accumulated user interactions, making limited-stock products being ignored and contribute less to the final output. To this end, we propose the Meta-Split Network (MSN) to split user history sequence regarding to the volume of stock for each product, and adopt differentiated modeling approaches for different sequences. As for the limited-stock products, a meta-learning approach is applied to address the problem of inconvergence, which is achieved by designing meta scaling and shifting networks with ID and side information. In addition, traditional approach can hardly update item embedding once the product is consumed. Thereby, we propose an auxiliary loss that makes the parameters updatable even when the product is no longer in distribution. To the best of our knowledge, this is the first solution addressing the recommendation of limited-stock product. Experimental results on the production dataset and online A/B testing demonstrate the effectiveness of our proposed method. | Wenhao Wu, Jialiang Zhou, Ailong He, Shuguang Han, Jufeng Chen, Bo Zheng |  |
| 95 |  |  [Knowledge Graph-based Session Recommendation with Session-Adaptive Propagation](https://doi.org/10.1145/3589335.3648324) |  | 0 | Session-based recommender systems (SBRSs) predict users' next interacted items based on their historical activities. While most SBRSs capture purchasing intentions locally within each session, capturing items' global information across different sessions is crucial in characterizing their general properties. Previous works capture this cross-session information by constructing graphs and incorporating neighbor information. However, this incorporation cannot vary adaptively according to the unique intention of each session, and the constructed graphs consist of only one type of user-item interaction. To address these limitations, we propose knowledge graph-based session recommendation with session-adaptive propagation. Specifically, we build a knowledge graph by connecting items with multi-typed edges to characterize various user-item interactions. Then, we adaptively aggregate items' neighbor information considering user intention within the learned session. Experimental results demonstrate that equipping our constructed knowledge graph and session-adaptive propagation enhances session recommendation backbones by 10 study showing our proposed framework achieves 2 existing well-deployed model at The Home Depot e-platform. | Yu Wang, Amin Javari, Janani Balaji, Walid Shalaby, Tyler Derr, Xiquan Cui |  |
| 96 |  |  [Cache-Aware Reinforcement Learning in Large-Scale Recommender Systems](https://doi.org/10.1145/3589335.3648326) |  | 0 | Modern large-scale recommender systems are built upon computation-intensive infrastructure and usually suffer from a huge difference in traffic between peak and off-peak periods. In peak periods, it is challenging to perform real-time computation for each request due to the limited budget of computational resources. The recommendation with a cache is a solution to this problem, where a user-wise result cache is used to provide recommendations when the recommender system cannot afford a real-time computation. However, the cached recommendations are usually suboptimal compared to real-time computation, and it is challenging to determine the items in the cache for each user. In this paper, we provide a cache-aware reinforcement learning (CARL) method to jointly optimize the recommendation by real-time computation and by the cache. We formulate the problem as a Markov decision process with user states and a cache state, where the cache state represents whether the recommender system performs recommendations by real-time computation or by the cache. The computational load of the recommender system determines the cache state. We perform reinforcement learning based on such a model to improve user engagement over multiple requests. Moreover, we show that the cache will introduce a challenge called critic dependency, which deteriorates the performance of reinforcement learning. To tackle this challenge, we propose an eigenfunction learning (EL) method to learn independent critics for CARL. Experiments show that CARL can significantly improve the users' engagement when considering the result cache. CARL has been fully launched in Kwai app, serving over 100 million users. | Xiaoshuang Chen, Gengrui Zhang, Yao Wang, Yulin Wu, Shuo Su, Kaiqiao Zhan, Ben Wang |  |
| 97 |  |  [Enhancing Interpretability and Effectiveness in Recommendation with Numerical Features via Learning to Contrast the Counterfactual samples](https://doi.org/10.1145/3589335.3648345) |  | 0 | We propose a general model-agnostic Contrastive learning framework with Counterfactual Samples Synthesizing (CCSS) for modeling the monotonicity between the neural network output and numerical features which is critical for interpretability and effectiveness of recommender systems. CCSS models the monotonicity via a two-stage process: synthesizing counterfactual samples and contrasting the counterfactual samples. The two techniques are naturally integrated into a model-agnostic framework, forming an end-to-end training process. Abundant empirical tests are conducted on a publicly available dataset and a real industrial dataset, and the results well demonstrate the effectiveness of our proposed CCSS. Besides, CCSS has been deployed in our real large-scale industrial recommender, successfully serving over hundreds of millions users. | Xiaoxiao Xu, Hao Wu, Wenhui Yu, Lantao Hu, Peng Jiang, Kun Gai |  |
| 98 |  |  [Term Importance for Transformer-Based QA Retrieval: A Case Study of StackExchange](https://doi.org/10.1145/3589335.3651568) |  | 0 | Question-answering (QA) retrieval is the task of retrieving the most relevant answer to a given question from a collection of answers. Various approaches to QA retrieval have been developed recently. One successful and popular model is Contextualized Late Interaction over BERT (ColBERT), a transformer-based approach that adopts a query-document scoring mechanism that retains the granularity of transformer matching, whilst improving on efficiency. However, one key limitation is that it requires further fine-tuning for new query or collection types. In this work, we explore and propose several non-parametric retrieval augmentation methods based on explicit signals of term importance that improve over ColBERT's baseline performance. In particular, we consider the QA retrieval task in the context of StackExchange question-answering forum, verifying the effectiveness of our methods in this setting. | Bryan Zhi Yang Tan, Hady W. Lauw |  |
| 99 |  |  [GreenRec: A Large-Scale Dataset for Green Food Recommendation](https://doi.org/10.1145/3589335.3651516) |  | 0 | In response to growing interest in sustainable living from both governmental and public spheres, there is an increased effort to understand environmental implications. Recommendation systems, which are widely applied in various aspects of daily life, are crucial tools in encouraging and guiding users toward sustainable choices. However, existing public recommendation datasets primarily focus on user-item interactions and lack sufficient emphasis on sustainability, posing significant challenges to developing recommendations for sustainable items. In this work, we enrich a public food recommendation dataset by assigning environmental impact, nutritional impact, and health scores to each recipe, following well-recognized sustainability measurements. Through this work, we aim to lay a groundwork for recommending foods that are both healthy and environmentally conscious, all while maintaining recommendation accuracy. | Lingzi Zhang, Yinan Zhang, Xin Zhou, Zhiqi Shen |  |
| 100 |  |  [RimiRec: Modeling Refined Multi-interest in Hierarchical Structure for Recommendation](https://doi.org/10.1145/3589335.3651554) |  | 0 | Industrial recommender systems usually consist of the retrieval stage and the ranking stage, to handle the billion-scale of users and items. The retrieval stage retrieves candidate items relevant to user interests for recommendations and has attracted much attention. Frequently, a user shows refined multi-interests in a hierarchical structure. For example, a user likes Conan and Kuroba Kaito, which are the roles in hierarchical structure "Animation, Japanese Animation, Detective Conan". However, most existing methods ignore this hierarchical nature, and simply average the fine-grained interest information. Therefore, we propose a novel two-stage approach to explicitly modeling refined multi-interest in a hierarchical structure for recommendation. In the first hierarchical multi-interest mining stage, the hierarchical clustering and transformer-based model adaptively generate circles or sub-circles that users are interested in. In the second stage, the partition of retrieval space allows the EBR models to deal only with items within each circle and accurately capture users' refined interests. Experimental results show that the proposed approach achieves state-of-the-art performance. Our framework has also been deployed at Lofter. | Haolei Pei, Yuanyuan Xu, Yangping Zhu, Yuan Nie |  |
| 101 |  |  [Is Cosine-Similarity of Embeddings Really About Similarity?](https://doi.org/10.1145/3589335.3651526) |  | 0 | Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless \`similarities.' For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models; these have implicit and unintended effects when taking cosine-similarities of the resulting embeddings, rendering results opaque and possibly arbitrary. Based on these insights, we caution against blindly using cosine-similarity and outline alternatives. | Harald Steck, Chaitanya Ekanadham, Nathan Kallus |  |
| 102 |  |  [Discrete Semantic Tokenization for Deep CTR Prediction](https://doi.org/10.1145/3589335.3651558) |  | 0 | Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings and then caches them, prioritizes space over time. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user–item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction. | Qijiong Liu, Hengchang Hu, Jiahao Wu, Jieming Zhu, MinYen Kan, XiaoMing Wu |  |
| 103 |  |  [Aligning Language Models for Versatile Text-based Item Retrieval](https://doi.org/10.1145/3589335.3651468) |  | 0 | This paper addresses the gap between general-purpose text embeddings and the specific demands of item retrieval tasks. We demonstrate the shortcomings of existing models in capturing the nuances necessary for zero-shot performance on item retrieval tasks. To overcome these limitations, we propose generate in-domain dataset from ten tasks tailored to unlocking models' representation ability for item retrieval. Our empirical studies demonstrate that fine-tuning embedding models on the dataset leads to remarkable improvements in a variety of retrieval tasks. We also illustrate the practical application of our refined model in a conversational setting, where it enhances the capabilities of LLM-based Recommender Agents like Chat-Rec. Our code is available at https://github.com/microsoft/RecAI. | Yuxuan Lei, Jianxun Lian, Jing Yao, Mingqi Wu, Defu Lian, Xing Xie |  |
| 104 |  |  [Cornac-AB: An Open-Source Recommendation Framework with Native A/B Testing Integration](https://doi.org/10.1145/3589335.3651241) |  | 0 | Recommender systems significantly impact user experience across diverse domains, yet existing frameworks often prioritize offline evaluation metrics, neglecting the crucial integration of A/B testing for forward-looking assessments. In response, this paper introduces a new framework seamlessly incorporating A/B testing into the Cornac recommendation library. Leveraging a diverse collection of model implementations in Cornac, our framework enables effortless A/B testing experiment setup from offline trained models. We introduce a carefully designed dashboard and a robust backend for efficient logging and analysis of user feedback. This not only streamlines the A/B testing process but also enhances the evaluation of recommendation models in an online environment. Demonstrating the simplicity of on-demand online model evaluations, our work contributes to advancing recommender system evaluation methodologies, underscoring the significance of A/B testing and providing a practical framework for implementation. The framework is open-sourced at https://github.com/PreferredAI/cornac-ab. | Darryl Ong, QuocTuan Truong, Hady W. Lauw | Amazon, Seattle, WA, USA; Singapore Management University, Singapore, Singapore |
| 105 |  |  [Towards Reliable and Efficient Long-Term Recommendation with Large Foundation Models](https://doi.org/10.1145/3589335.3651258) |  | 0 | Prioritizing long-term engagement rather than immediate benefits has garnered increasing attention in recent years. However, current research on long-term recommendation faces substantial challenges in terms of model evaluation and design: 1) Traditional evaluation approaches suffer from limitations due to the sparsity and bias in the offline data and fail to capture user psychological influences. 2) Existing recommenders based on Reinforcement Learning (RL) are entirely data-driven and constrained by sparse and long-tail distributed offline data. Fortunately, recent advancements in Large Foundation Models (LFMs), characterized by remarkable simulation and planning capacity, offer significant opportunities for long-term recommendation. Despite potential, due to the substantial scenario divergence between LFM pre-training and recommendation, employing LFMs in long-term recommendation still faces certain challenges. To this end, this research focuses on adapting the remarkable capabilities of LFMs to long-term recommendations to devise reliable evaluation schemes and efficient recommenders. | Wentao Shi |  |
| 106 |  |  [Clickbait vs. Quality: How Engagement-Based Optimization Shapes the Content Landscape in Online Platforms](https://doi.org/10.1145/3589334.3645353) |  | 0 | Online content platforms commonly use engagement-based optimization when making recommendations. This encourages content creators to invest in quality, but also rewards gaming tricks such as clickbait. To understand the total impact on the content landscape, we study a game between content creators competing on the basis of engagement metrics and analyze the equilibrium decisions about investment in quality and gaming. First, we show the content created at equilibrium exhibits a positive correlation between quality and gaming, and we empirically validate this finding on a Twitter dataset. Using the equilibrium structure of the content landscape, we then examine the downstream performance of engagement-based optimization along several axes. Perhaps counterintuitively, the average quality of content consumed by users can decrease at equilibrium as gaming tricks become more costly for content creators to employ. Moreover, engagement-based optimization can perform worse in terms of user utility than a baseline with random recommendations, and engagement-based optimization is also suboptimal in terms of realized engagement relative to quality-based optimization. Altogether, our results highlight the need to consider content creator incentives when evaluating a platform's choice of optimization metric. | Nicole Immorlica, Meena Jagadeesan, Brendan Lucier |  |
| 107 |  |  [Bidder Selection Problem in Position Auctions: A Fast and Simple Algorithm via Poisson Approximation](https://doi.org/10.1145/3589334.3645418) |  | 0 | In the Bidder Selection Problem (BSP) there is a large pool of n potential advertisers competing for ad slots on the user's web page. Due to strict computational restrictions, the advertising platform can run a proper auction only for a fraction k<n of advertisers. We consider the basic optimization problem underlying BSP: given n independent prior distributions, how to efficiently find a subset of k with the objective of either maximizing expected social welfare or revenue of the platform. We study BSP in the classic multi-winner model of position auctions for welfare and revenue objectives using the optimal (respectively, VCG mechanism, or Myerson's auction) format for the selected set of bidders. Previous PTAS results for BSP optimization were only known for single-item auctions and in case of [Segev and Singla 2021] for l-unit auctions. More importantly, all of these PTASes were computational complexity results with impractically large running times, which defeats the purpose of using these algorithms under severe computational constraints. We propose a novel Poisson relaxation of BSP for position auctions that immediately implies that 1) BSP is polynomial-time solvable up to a vanishingly small error as the problem size k grows; 2) there is a PTAS for position auctions after combining our relaxation with the trivial brute force algorithm. Unlike all previous PTASes, we implemented our algorithm and did extensive numerical experiments on practically relevant input sizes. First, our experiments corroborate the previous experimental findings of Mehta et al. that a few simple heuristics used in practice perform surprisingly well in terms of approximation factor. Furthermore, our algorithm outperforms Greedy both in running time and approximation on medium and large-sized instances. | Nikolai Gravin, Yixuan Even Xu, Renfei Zhou |  |
| 108 |  |  [A Fast Hop-Biased Approximation Algorithm for the Quadratic Group Steiner Tree Problem](https://doi.org/10.1145/3589334.3645325) |  | 0 | Knowledge Graph (KG) exploration helps Web users understand the contents of a large and unfamiliar KG and extract relevant insights. The task has recently been formulated as a Quadratic Group Steiner Tree Problem (QGSTP) to search for a semantically cohesive subgraph connecting entities that match query keywords. However, on large graphs, existing algorithms for this NP-hard problem cannot meet the performance need. In this paper, we propose a novel approximation algorithm for QGSTP called HB. It finds and merges an optimal set of paths according to a Hop-Biased objective function, which not only leads to a guaranteed approximation ratio but is also decomposable by paths to enable efficient dynamic programming based search. Accompanied by a set of pruning heuristics, HB outperformed the state of the art by 1-2 orders of magnitude, empirically reducing the average time for answering a query on a million-scale graph from about one minute to one second. | Xiaoqing Wang, Gong Cheng |  |
| 109 |  |  [Link Prediction on Multilayer Networks through Learning of Within-Layer and Across-Layer Node-Pair Structural Features and Node Embedding Similarity](https://doi.org/10.1145/3589334.3645646) |  | 0 | Link prediction has traditionally been studied in the context of simple graphs, although real-world networks are inherently complex as they are often comprised of multiple interconnected components, or layers. Predicting links in such network systems, or multilayer networks, require to consider both the internal structure of a target layer as well as the structure of the other layers in a network, in addition to layer-specific node-attributes when available. This problem poses several challenges, even for graph neural network based approaches despite their successful and wide application to a variety of graph learning problems. In this work, we aim to fill a lack of multilayer graph representation learning methods designed for link prediction. Our proposal is a novel neural-network-based learning framework for link prediction on (attributed) multilayer networks, whose key idea is to combine (i) pairwise similarities of multilayer node embeddings learned by a graph neural network model, and (ii) structural features learned from both within-layer and across-layer link information based on overlapping multilayer neighborhoods. Extensive experimental results have shown that our framework consistently outperforms both single-layer and multilayer methods for link prediction on popular real-world multilayer networks, with an average percentage increase in AUC up to 38%. We make source code and evaluation data available at https://mlnteam-unical.github.io/resources/. | Lorenzo Zangari, Domenico Mandaglio, Andrea Tagarelli |  |
| 110 |  |  [Diffusion-based Negative Sampling on Graphs for Link Prediction](https://doi.org/10.1145/3589334.3645650) |  | 0 | Link prediction is a fundamental task for graph analysis with important applications on the Web, such as social network analysis and recommendation systems, etc. Modern graph link prediction methods often employ a contrastive approach to learn robust node representations, where negative sampling is pivotal. Typical negative sampling methods aim to retrieve hard examples based on either predefined heuristics or automatic adversarial approaches, which might be inflexible or difficult to control. Furthermore, in the context of link prediction, most previous methods sample negative nodes from existing substructures of the graph, missing out on potentially more optimal samples in the latent space. To address these issues, we investigate a novel strategy of multi-level negative sampling that enables negative node generation with flexible and controllable “hardness” levels from the latent space. Our method, called Conditional Diffusion-based Multi-level Negative Sampling (DMNS), leverages the Markov chain property of diffusion models to generate negative nodes in multiple levels of variable hardness and reconcile them for effective graph link prediction. We further demonstrate that DMNS follows the sub-linear positivity principle for robust negative sampling. Extensive experiments on several benchmark datasets demonstrate the effectiveness of DMNS. | TrungKien Nguyen, Yuan Fang |  |
| 111 |  |  [InfoRank: Unbiased Learning-to-Rank via Conditional Mutual Information Minimization](https://doi.org/10.1145/3589334.3645356) |  | 0 | Ranking items regarding individual user interests is a core technique of multiple downstream tasks such as recommender systems. Learning such a personalized ranker typically relies on the implicit feedback from users' past click-through behaviors. However, collected feedback is biased toward previously highly-ranked items and directly learning from it would result in a "rich-get-richer" phenomenon. In this paper, we propose a simple yet sufficient unbiased learning-to-rank paradigm named InfoRank that aims to simultaneously address both position and popularity biases. We begin by consolidating the impacts of those biases into a single observation factor, thereby providing a unified approach to addressing bias-related issues. Subsequently, we minimize the mutual information between the observation estimation and the relevance estimation conditioned on the input features. By doing so, our relevance estimation can be proved to be free of bias. To implement InfoRank, we first incorporate an attention mechanism to capture latent correlations within user-item features, thereby generating estimations of observation and relevance. We then introduce a regularization term, grounded in conditional mutual information, to promote conditional independence between relevance estimation and observation estimation. Experimental evaluations conducted across three extensive recommendation and search datasets reveal that InfoRank learns more precise and unbiased ranking strategies. | Jiarui Jin, Zexue He, Mengyue Yang, Weinan Zhang, Yong Yu, Jun Wang, Julian J. McAuley |  |
| 112 |  |  [Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback](https://doi.org/10.1145/3589334.3645365) |  | 0 | In the rapidly evolving landscape of information retrieval, search engines strive to provide more personalized and relevant results to users. Query suggestion systems play a crucial role in achieving this goal by assisting users in formulating effective queries. However, existing query suggestion systems mainly rely on textual inputs, potentially limiting user search experiences for querying images. In this paper, we introduce a novel Multimodal Query Suggestion (MMQS) task, which aims to generate query suggestions based on user query images to improve the intentionality and diversity of search results. We present the RL4Sugg framework, leveraging the power of Large Language Models (LLMs) with Multi-Agent Reinforcement Learning from Human Feedback to optimize the generation process. Through comprehensive experiments, we validate the effectiveness of RL4Sugg, demonstrating a 18 compared to the best existing approach. Moreover, the MMQS has been transferred into real-world search engine products, which yield enhanced user engagement. Our research advances query suggestion systems and provides a new perspective on multimodal information retrieval. | Zheng Wang, Bingzheng Gan, Wei Shi |  |
| 113 |  |  [A Fast Similarity Matrix Calibration Method with Incomplete Query](https://doi.org/10.1145/3589334.3645456) |  | 0 | The similarity matrix is at the core of similarity search problems. However, incomplete observations are ubiquitous in real scenarios leading to a less accurate similarity matrix. To alleviate this problem, in this paper, based on the key insight that the similarity matrix enjoys both the symmetric and positive semi-definiteness (PSD) properties, we propose a novel similarity matrix calibration method, which is scalable, effective, and sound. Specifically, we establish the PSD property as a constraint for the similarity matrix calibration problem and propose a novel similarity matrix calibration method to estimate the similarity matrix, which approximates the unknown complete ground-truth similarity matrix. To enable a fast optimization process, we further develop a general approximated algorithm that bypasses the computation of singular values. Theoretical analysis ensures stable calibration performance and convergence speed. Extensive experiments of similarity matrix calibration on real-world datasets demonstrate that our proposed method outperforms baseline methods in terms of both accuracy and speed. | Changyi Ma, Runsheng Yu, Youzhi Zhang |  |
| 114 |  |  [Whole Page Unbiased Learning to Rank](https://doi.org/10.1145/3589334.3645474) |  | 0 | The page presentation biases in the information retrieval system, especially on the click behavior, is a well-known challenge that hinders improving ranking models' performance with implicit user feedback. Unbiased Learning to Rank (ULTR) algorithms are then proposed to learn an unbiased ranking model with biased click data. However, most existing algorithms are specifically designed to mitigate position-related bias, e.g., trust bias, without considering biases induced by other features in search result page presentation(SERP), e.g. attractive bias induced by the multimedia. Unfortunately, those biases widely exist in industrial systems and may lead to an unsatisfactory search experience. Therefore, we introduce a new problem, i.e., whole-page Unbiased Learning to Rank(WP-ULTR), aiming to handle biases induced by whole-page SERP features simultaneously. It presents tremendous challenges: (1) a suitable user behavior model (user behavior hypothesis) can be hard to find; and (2) complex biases cannot be handled by existing algorithms. To address the above challenges, we propose a Bias Agnostic whole-page unbiased Learning to rank algorithm, named BAL, to automatically find the user behavior model with causal discovery and mitigate the biases induced by multiple SERP features with no specific design. Experimental results on a real-world dataset verify the effectiveness of the BAL. | Haitao Mao, Lixin Zou, Yujia Zheng, Jiliang Tang, Xiaokai Chu, Jiashu Zhao, Qian Wang, Dawei Yin |  |
| 115 |  |  [Mitigating Exploitation Bias in Learning to Rank with an Uncertainty-aware Empirical Bayes Approach](https://doi.org/10.1145/3589334.3645487) |  | 0 | Ranking is at the core of many artificial intelligence (AI) applications, including search engines, recommender systems, etc. Modern ranking systems are often constructed with learning-to-rank (LTR) models built from user behavior signals. While previous studies have demonstrated the effectiveness of using user behavior signals (e.g., clicks) as both features and labels of LTR algorithms, we argue that existing LTR algorithms that indiscriminately treat behavior and non-behavior signals in input features could lead to suboptimal performance in practice. Particularly because user behavior signals often have strong correlations with the ranking objective and can only be collected on items that have already been shown to users, directly using behavior signals in LTR could create an exploitation bias that hurts the system performance in the long run. To address the exploitation bias, we propose EBRank, an empirical Bayes-based uncertainty-aware ranking algorithm. Specifically, to overcome exploitation bias brought by behavior features in ranking models, EBRank uses a sole non-behavior feature based prior model to get a prior estimation of relevance. In the dynamic training and serving of ranking systems, EBRank uses the observed user behaviors to update posterior relevance estimation instead of concatenating behaviors as features in ranking models. Besides, EBRank additionally applies an uncertainty-aware exploration strategy to explore actively, collect user behaviors for empirical Bayesian modeling and improve ranking performance. Experiments on three public datasets show that EBRank is effective, practical and significantly outperforms state-of-the-art ranking algorithms. | Tao Yang, Cuize Han, Chen Luo, Parth Gupta, Jeff M. Phillips, Qingyao Ai |  |
| 116 |  |  [Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://doi.org/10.1145/3589334.3645574) |  | 0 | Session search involves a series of interactive queries and actions to fulfill user's complex information need. Current strategies typically prioritize sequential modeling for deep semantic understanding, overlooking the graph structure in interactions. While some approaches focus on capturing structural information, they use a generalized representation for documents, neglecting the word-level semantic modeling. In this paper, we propose Symbolic Graph Ranker (SGR), which aims to take advantage of both text-based and graph-based approaches by leveraging the power of recent Large Language Models (LLMs). Concretely, we first introduce a set of symbolic grammar rules to convert session graph into text. This allows integrating session history, interaction process, and task instruction seamlessly as inputs for the LLM. Moreover, given the natural discrepancy between LLMs pre-trained on textual corpora, and the symbolic language we produce using our graph-to-text grammar, our objective is to enhance LLMs' ability to capture graph structures within a textual format. To achieve this, we introduce a set of self-supervised symbolic learning tasks including link prediction, node content generation, and generative contrastive learning, to enable LLMs to capture the topological information from coarse-grained to fine-grained. Experiment results and comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm the superiority of our approach. Our paradigm also offers a novel and effective methodology that bridges the gap between traditional search strategies and modern LLMs. | Songhao Wu, Quan Tu, Hong Liu, Jia Xu, Zhongyi Liu, Guannan Zhang, Ran Wang, Xiuying Chen, Rui Yan |  |
| 117 |  |  [Matching Feature Separation Network for Domain Adaptation in Entity Matching](https://doi.org/10.1145/3589334.3645397) |  | 0 | Entity matching (EM) determines whether two records from different data sources refer to the same real-world entity. It is a fundamental task in knowledge graph construction and data integration. Currently, deep learning (DL) based EM methods have achieved state-of-the-art (SOTA) results. However, apply-ing DL-based EM methods often costs a lot of human efforts to label the data. To address this challenge, we propose a new do-main adaptation (DA) framework for EM called Matching Fea-ture Separation Network (MFSN). We implement DA by sepa-rating private and common matching features. Briefly, MFSN first uses three encoders to explicitly model the private and common matching features in both the source and target do-mains. Then, it transfers the knowledge learned from the source common matching features to the target domain. We also pro-pose an enhanced variant called Feature Representation and Separation Enhanced MFSN (MFSN-FRSE). Compared with MFSN, it has superior feature representation and separation capabilities. We evaluate the effectiveness of MFSN and MFSN-FRSE on twelve DA in EM tasks. The results show that our framework is approximately 7% higher in F1 score on average than the previous SOTA methods. Then, we verify the effec-tiveness of each module in MFSN and MFSN-FRSE by ablation study. Finally, we explore the optimal strategy of each module in MFSN and MFSN-FRSE through detailed tests. | Chenchen Sun, Yang Xu, Derong Shen, Tiezheng Nie |  |
| 118 |  |  [FedUP: Querying Large-Scale Federations of SPARQL Endpoints](https://doi.org/10.1145/3589334.3645704) |  | 0 | Processing SPARQL queries over large federations of SPARQL endpoints is crucial for keeping the Semantic Web decentralized. Despite the existence of hundreds of SPARQL endpoints, current federation engines only scale to dozens. One major issue comes from the current definition of the source selection problem, i.e., finding the minimal set of SPARQL endpoints to contact per triple pattern. Even if such a source selection is minimal, only a few combinations of sources may return results. Consequently, most of the query processing time is wasted evaluating combinations that return no results. In this paper, we introduce the concept of Result-Aware query plans. This concept ensures that every subquery of the query plan effectively contributes to the result of the query. To compute a Result-Aware query plan, we propose FedUP, a new federation engine able to produce Result-Aware query plans by tracking the provenance of query results. However, getting query results requires computing source selection, and computing source selection requires query results. To break this vicious cycle, FedUP computes results and provenances on tiny quotient summaries of federations at the cost of source selection accuracy. Experimental results on federated benchmarks demonstrate that FedUP outperforms state-of-the-art federation engines by orders of magnitude in the context of large-scale federations. | Julien AimonierDavat, Brice Nédelec, Minh Hoang Dang, Pascal Molli, Hala SkafMolli |  |
| 119 |  |  [Prompt-enhanced Federated Content Representation Learning for Cross-domain Recommendation](https://doi.org/10.1145/3589334.3645337) |  | 0 | Cross-domain Recommendation (CDR) as one of the effective techniques in alleviating the data sparsity issues has been widely studied in recent years. However, previous works may cause domain privacy leakage since they necessitate the aggregation of diverse domain data into a centralized server during the training process. Though several studies have conducted privacy preserving CDR via Federated Learning (FL), they still have the following limitations: 1) They need to upload users' personal information to the central server, posing the risk of leaking user privacy. 2) Existing federated methods mainly rely on atomic item IDs to represent items, which prevents them from modeling items in a unified feature space, increasing the challenge of knowledge transfer among domains. 3) They are all based on the premise of knowing overlapped users between domains, which proves impractical in real-world applications. To address the above limitations, we focus on Privacy-preserving Cross-domain Recommendation (PCDR) and propose PFCR as our solution. For Limitation 1, we develop a FL schema by exclusively utilizing users' interactions with local clients and devising an encryption method for gradient encryption. For Limitation 2, we model items in a universal feature space by their description texts. For Limitation 3, we initially learn federated content representations, harnessing the generality of natural language to establish bridges between domains. Subsequently, we craft two prompt fine-tuning strategies to tailor the pre-trained model to the target domain. Extensive experiments on two real-world datasets demonstrate the superiority of our PFCR method compared to the SOTA approaches. | Lei Guo, Ziang Lu, Junliang Yu, Quoc Viet Hung Nguyen, Hongzhi Yin |  |
| 120 |  |  [PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning](https://doi.org/10.1145/3589334.3645359) |  | 0 | Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal recommender systems. These modalities provide intuitive semantics that facilitate modality-aware user preference modeling. However, two key challenges in multi-modal recommenders remain unresolved: i) The introduction of multi-modal encoders with a large number of additional parameters causes overfitting, given high-dimensional multi-modal features provided by extractors (e.g., ViT, BERT). ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference. To tackle these problems, we propose to simplify and empower recommenders through Multi-modal Knowledge Distillation (PromptMM) with the prompt-tuning that enables adaptive quality distillation. Specifically, PromptMM conducts model compression through distilling u-i edge relationship and multi-modal node content from cumbersome teachers to relieve students from the additional feature reduction parameters. To bridge the semantic gap between multi-modal context and collaborative signals for empowering the overfitting teacher, soft prompt-tuning is introduced to perform student task-adaptive. Additionally, to adjust the impact of inaccuracies in multimedia data, a disentangled multi-modal list-wise distillation is developed with modality-aware re-weighting mechanism. Experiments on real-world data demonstrate PromptMM's superiority over existing techniques. Ablation tests confirm the effectiveness of key components. Additional tests show the efficiency and effectiveness. | Wei Wei, Jiabin Tang, Lianghao Xia, Yangqin Jiang, Chao Huang |  |
| 121 |  |  [Cold-start Bundle Recommendation via Popularity-based Coalescence and Curriculum Heating](https://doi.org/10.1145/3589334.3645377) |  | 0 | How can we recommend cold-start bundles to users? The cold-start problem in bundle recommendation is crucial because new bundles are continuously created on the Web for various marketing purposes. Despite its importance, existing methods for cold-start item recommendation are not readily applicable to bundles. They depend overly on historical information, even for less popular bundles, failing to address the primary challenge of the highly skewed distribution of bundle interactions. In this work, we propose CoHeat (Popularity-based Coalescence and Curriculum Heating), an accurate approach for cold-start bundle recommendation. CoHeat first represents users and bundles through graph-based views, capturing collaborative information effectively. To estimate the user-bundle relationship more accurately, CoHeat addresses the highly skewed distribution of bundle interactions through a popularity-based coalescence approach, which incorporates historical and affiliation information based on the bundle's popularity. Furthermore, it effectively learns latent representations by exploiting curriculum learning and contrastive learning. CoHeat demonstrates superior performance in cold-start bundle recommendation, achieving up to 193 | Hyunsik Jeon, Jongeun Lee, Jeongin Yun, U Kang |  |
| 122 |  |  [Scalable and Provably Fair Exposure Control for Large-Scale Recommender Systems](https://doi.org/10.1145/3589334.3645390) |  | 0 | Typical recommendation and ranking methods aim to optimize the satisfaction of users, but they are often oblivious to their impact on the items (e.g., products, jobs, news, video) and their providers. However, there has been a growing understanding that the latter is crucial to consider for a wide range of applications, since it determines the utility of those being recommended. Prior approaches to fairness-aware recommendation optimize a regularized objective to balance user satisfaction and item fairness based on some notion such as exposure fairness. These existing methods have been shown to be effective in controlling fairness, however, most of them are computationally inefficient, limiting their applications to only unrealistically small-scale situations. This indeed implies that the literature does not yet provide a solution to enable a flexible control of exposure in the industry-scale recommender systems where millions of users and items exist. To enable a computationally efficient exposure control even for such large-scale systems, this work develops a scalable, fast, and fair method called exposure-aware ADMM (exADMM). exADMM is based on implicit alternating least squares (iALS), a conventional scalable algorithm for collaborative filtering, but optimizes a regularized objective to achieve a flexible control of accuracy-fairness tradeoff. A particular technical challenge in developing exADMM is the fact that the fairness regularizer destroys the separability of optimization subproblems for users and items, which is an essential property to ensure the scalability of iALS. Therefore, we develop a set of optimization tools to enable yet scalable fairness control with provable convergence guarantees as a basis of our algorithm. | Riku Togashi, Kenshi Abe, Yuta Saito |  |
| 123 |  |  [Causally Debiased Time-aware Recommendation](https://doi.org/10.1145/3589334.3645400) |  | 0 | Time-aware recommendation has been widely studied for modeling the user dynamic preference and a lot of models have been proposed. However, these models often overlook the fact that users may not behave evenly on the timeline, and observed datasets can be biased by user intrinsic preferences or previous recommender systems, leading to degraded model performance. We propose a causally debiased time-aware recommender framework to accurately learn user preference. We formulate the task of time-aware recommendation by a causal graph, identifying two types of biases on the item and time levels. To optimize the ideal unbiased learning objective, we propose a debiased framework based on the inverse propensity score (IPS) and extend it to the doubly robust method. Considering that the user preference can be diverse and complex, which may result in unmeasured confounders, we develop a sensitivity analysis method to obtain more accurate IPS. We theoretically draw a connection between the proposed method and the ideal learning objective, which to the best of our knowledge, is the first time in the research community. We conduct extensive experiments on three real-world datasets to demonstrate the effectiveness of our model. To promote this research direction, we have released our project at https://paitesanshi.github.io/CDTR/. | Lei Wang, Chen Ma, Xian Wu, Zhaopeng Qiu, Yefeng Zheng, Xu Chen |  |
| 124 |  |  [Learning Category Trees for ID-Based Recommendation: Exploring the Power of Differentiable Vector Quantization](https://doi.org/10.1145/3589334.3645484) |  | 0 | Category information plays a crucial role in enhancing the quality and personalization of recommender systems. Nevertheless, the availability of item category information is not consistently present, particularly in the context of ID-based recommendations. In this work, we propose a novel approach to automatically learn and generate entity (i.e., user or item) category trees for ID-based recommendation. Specifically, we devise a differentiable vector quantization framework for automatic category tree generation, namely CAGE, which enables the simultaneous learning and refinement of categorical code representations and entity embeddings in an end-to-end manner, starting from the randomly initialized states. With its high adaptability, CAGE can be easily integrated into both sequential and non-sequential recommender systems. We validate the effectiveness of CAGE on various recommendation tasks including list completion, collaborative filtering, and click-through rate prediction, across different recommendation models. We release the code and data for others to reproduce the reported results. | Qijiong Liu, Jiaren Xiao, Lu Fan, Jieming Zhu, XiaoMing Wu |  |
| 125 |  |  [Poisoning Federated Recommender Systems with Fake Users](https://doi.org/10.1145/3589334.3645492) |  | 0 | Federated recommendation is a prominent use case within federated learning, yet it remains susceptible to various attacks, from user to server-side vulnerabilities. Poisoning attacks are particularly notable among user-side attacks, as participants upload malicious model updates to deceive the global model, often intending to promote or demote specific targeted items. This study investigates strategies for executing promotion attacks in federated recommender systems. Current poisoning attacks on federated recommender systems often rely on additional information, such as the local training data of genuine users or item popularity. However, such information is challenging for the potential attacker to obtain. Thus, there is a need to develop an attack that requires no extra information apart from item embeddings obtained from the server. In this paper, we introduce a novel fake user based poisoning attack named PoisonFRS to promote the attacker-chosen targeted item in federated recommender systems without requiring knowledge about user-item rating data, user attributes, or the aggregation rule used by the server. Extensive experiments on multiple real-world datasets demonstrate that PoisonFRS can effectively promote the attacker-chosen targeted item to a large portion of genuine users and outperform current benchmarks that rely on additional information about the system. We further observe that the model updates from both genuine and fake users are indistinguishable within the latent space. | Ming Yin, Yichang Xu, Minghong Fang, Neil Zhenqiang Gong |  |
| 126 |  |  [Could Small Language Models Serve as Recommenders? Towards Data-centric Cold-start Recommendation](https://doi.org/10.1145/3589334.3645494) |  | 0 | Recommendation systems help users find information that matches their interests based on their historical behaviors. However, generating personalized recommendations becomes challenging in the absence of historical user-item interactions, a practical problem for startups known as the system cold-start recommendation. Current research tackles user or item cold-start scenarios but lacks solutions for system cold-start. To tackle the problem, we initially propose PromptRec, a simple but effective approach based on in-context learning of language models, where we transform the recommendation task into the sentiment analysis task on natural language containing user and item profiles. However, this naive strategy heavily relied on the strong in-context learning ability emerged from large language models, which could suffer from significant latency for online recommendations. To fill this gap, we present a theoretical framework to formalize the connection between in-context recommendation and language modeling. Based on it, we propose to enhance small language models with a data-centric pipeline, which consists of: (1) constructing a refined corpus for model pre-training; (2) constructing a decomposed prompt template via prompt pre-training. They correspond to the development of training data and inference data, respectively. To evaluate our proposed method, we introduce a cold-start recommendation benchmark, and the results demonstrate that the enhanced small language models can achieve comparable cold-start recommendation performance to that of large models with only around 17 time. To the best of our knowledge, this is the first study to tackle the system cold-start recommendation problem. We believe our findings will provide valuable insights for future works. The benchmark and implementations are available at https://github.com/JacksonWuxs/PromptRec. | Xuansheng Wu, Huachi Zhou, Yucheng Shi, Wenlin Yao, Xiao Huang, Ninghao Liu |  |
| 127 |  |  [Macro Graph Neural Networks for Online Billion-Scale Recommender Systems](https://doi.org/10.1145/3589334.3645517) |  | 0 | Predicting Click-Through Rate (CTR) in billion-scale recommender systems poses a long-standing challenge for Graph Neural Networks (GNNs) due to the overwhelming computational complexity involved in aggregating billions of neighbors. To tackle this, GNN-based CTR models usually sample hundreds of neighbors out of the billions to facilitate efficient online recommendations. However, sampling only a small portion of neighbors results in a severe sampling bias and the failure to encompass the full spectrum of user or item behavioral patterns. To address this challenge, we name the conventional user-item recommendation graph as "micro recommendation graph" and introduce a more suitable MAcro Recommendation Graph (MAG) for billion-scale recommendations. MAG resolves the computational complexity problems in the infrastructure by reducing the node count from billions to hundreds. Specifically, MAG groups micro nodes (users and items) with similar behavior patterns to form macro nodes. Subsequently, we introduce tailored Macro Graph Neural Networks (MacGNN) to aggregate information on a macro level and revise the embeddings of macro nodes. MacGNN has already served Taobao's homepage feed for two months, providing recommendations for over one billion users. Extensive offline experiments on three public benchmark datasets and an industrial dataset present that MacGNN significantly outperforms twelve CTR baselines while remaining computationally efficient. Besides, online A/B tests confirm MacGNN's superiority in billion-scale recommender systems. | Hao Chen, Yuanchen Bei, Qijie Shen, Yue Xu, Sheng Zhou, Wenbing Huang, Feiran Huang, Senzhang Wang, Xiao Huang |  |
| 128 |  |  [Recommender Transformers with Behavior Pathways](https://doi.org/10.1145/3589334.3645528) |  | 0 | Sequential recommendation requires the recommender to capture the evolving behavior characteristics from logged user behavior data for accurate recommendations. However, user behavior sequences are viewed as a script with multiple ongoing threads intertwined. We find that only a small set of pivotal behaviors can be evolved into the user's future action. As a result, the future behavior of the user is hard to predict. We conclude this characteristic for sequential behaviors of each user as the Behavior Pathway. Different users have their unique behavior pathways. Among existing sequential models, transformers have shown great capacity in capturing global-dependent characteristics. However, these models mainly provide a dense distribution over all previous behaviors using the self-attention mechanism, making the final predictions overwhelmed by the trivial behaviors not adjusted to each user. In this paper, we build the Recommender Transformer (RETR) with a novel Pathway Attention mechanism. RETR can dynamically plan the behavior pathway specified for each user, and sparingly activate the network through this behavior pathway to effectively capture evolving patterns useful for recommendation. The key design is a learned binary route to prevent the behavior pathway from being overwhelmed by trivial behaviors. We empirically verify the effectiveness of RETR on seven real-world datasets and RETR yields state-of-the-art performance. | Zhiyu Yao, Xinyang Chen, Sinan Wang, Qinyan Dai, Yumeng Li, Tanchao Zhu, Mingsheng Long |  |
| 129 |  |  [Ensuring User-side Fairness in Dynamic Recommender Systems](https://doi.org/10.1145/3589334.3645536) |  | 0 | User-side group fairness is crucial for modern recommender systems, aiming to alleviate performance disparities among user groups defined by sensitive attributes like gender, race, or age. In the ever-evolving landscape of user-item interactions, continual adaptation to newly collected data is crucial for recommender systems to stay aligned with the latest user preferences. However, we observe that such continual adaptation often exacerbates performance disparities. This necessitates a thorough investigation into user-side fairness in dynamic recommender systems, an area that has been unexplored in the literature. This problem is challenging due to distribution shifts, frequent model updates, and non-differentiability of ranking metrics. To our knowledge, this paper presents the first principled study on ensuring user-side fairness in dynamic recommender systems. We start with theoretical analyses on fine-tuning v.s. retraining, showing that the best practice is incremental fine-tuning with restart. Guided by our theoretical analyses, we propose FAir Dynamic rEcommender (FADE), an end-to-end fine-tuning framework to dynamically ensure user-side fairness over time. To overcome the non-differentiability of recommendation metrics in the fairness loss, we further introduce Differentiable Hit (DH) as an improvement over the recent NeuralNDCG method, not only alleviating its gradient vanishing issue but also achieving higher efficiency. Besides that, we also address the instability issue of the fairness loss by leveraging the competing nature between the recommendation loss and the fairness loss. Through extensive experiments on real-world datasets, we demonstrate that FADE effectively and efficiently reduces performance disparities with little sacrifice in the overall recommendation performance. | Hyunsik Yoo, Zhichen Zeng, Jian Kang, Ruizhong Qiu, David Zhou, Zhining Liu, Fei Wang, Charlie Xu, Eunice Chan, Hanghang Tong |  |
| 130 |  |  [Mirror Gradient: Towards Robust Multimodal Recommender Systems via Exploring Flat Local Minima](https://doi.org/10.1145/3589334.3645553) |  | 0 | Multimodal recommender systems utilize various types of information to model user preferences and item features, helping users discover items aligned with their interests. The integration of multimodal information mitigates the inherent challenges in recommender systems, e.g., the data sparsity problem and cold-start issues. However, it simultaneously magnifies certain risks from multimodal information inputs, such as information adjustment risk and inherent noise risk. These risks pose crucial challenges to the robustness of recommendation models. In this paper, we analyze multimodal recommender systems from the novel perspective of flat local minima and propose a concise yet effective gradient strategy called Mirror Gradient (MG). This strategy can implicitly enhance the model's robustness during the optimization process, mitigating instability risks arising from multimodal information inputs. We also provide strong theoretical evidence and conduct extensive empirical experiments to show the superiority of MG across various multimodal recommendation models and benchmarks. Furthermore, we find that the proposed MG can complement existing robust training methods and be easily extended to diverse advanced recommendation models, making it a promising new and fundamental paradigm for training multimodal recommender systems. The code is released at https://github.com/Qrange-group/Mirror-Gradient. | Shanshan Zhong, Zhongzhan Huang, Daifeng Li, Wushao Wen, Jinghui Qin, Liang Lin |  |
| 131 |  |  [Adaptive Neural Ranking Framework: Toward Maximized Business Goal for Cascade Ranking Systems](https://doi.org/10.1145/3589334.3645605) |  | 0 | Cascade ranking is widely used for large-scale top-k selection problems in online advertising and recommendation systems, and learning-to-rank is an important way to optimize the models in cascade ranking. Previous works on learning-to-rank usually focus on letting the model learn the complete order or top-k order, and adopt the corresponding rank metrics (e.g. OPA and NDCG@k) as optimization targets. However, these targets can not adapt to various cascade ranking scenarios with varying data complexities and model capabilities; and the existing metric-driven methods such as the Lambda framework can only optimize a rough upper bound of limited metrics, potentially resulting in sub-optimal and performance misalignment. To address these issues, we propose a novel perspective on optimizing cascade ranking systems by highlighting the adaptability of optimization targets to data complexities and model capabilities. Concretely, we employ multi-task learning to adaptively combine the optimization of relaxed and full targets, which refers to metrics Recall@m@k and OPA respectively. We also introduce permutation matrix to represent the rank metrics and employ differentiable sorting techniques to relax hard permutation matrix with controllable approximate error bound. This enables us to optimize both the relaxed and full targets directly and more appropriately. We named this method as Adaptive Neural Ranking Framework (abbreviated as ARF). Furthermore, we give a specific practice under ARF. We use the NeuralSort to obtain the relaxed permutation matrix and draw on the variant of the uncertainty weight method in multi-task learning to optimize the proposed losses jointly. Experiments on a total of 4 public and industrial benchmarks show the effectiveness and generalization of our method, and online experiment shows that our method has significant application value. | Yunli Wang, Zhiqiang Wang, Jian Yang, Shiyang Wen, Dongying Kong, Han Li, Kun Gai |  |
| 132 |  |  [Doubly Calibrated Estimator for Recommendation on Data Missing Not at Random](https://doi.org/10.1145/3589334.3645617) |  | 0 | Recommender systems often suffer from selection bias as users tend to rate their preferred items. The datasets collected under such conditions exhibit entries missing not at random and thus are not randomized-controlled trials representing the target population. To address this challenge, a doubly robust estimator and its enhanced variants have been proposed as they ensure unbiasedness when accurate imputed errors or predicted propensities are provided. However, we argue that existing estimators rely on miscalibrated imputed errors and propensity scores as they depend on rudimentary models for estimation. We provide theoretical insights into how miscalibrated imputation and propensity models may limit the effectiveness of doubly robust estimators and validate our theorems using real-world datasets. On this basis, we propose a Doubly Calibrated Estimator that involves the calibration of both the imputation and propensity models. To achieve this, we introduce calibration experts that consider different logit distributions across users. Moreover, we devise a tri-level joint learning framework, allowing the simultaneous optimization of calibration experts alongside prediction and imputation models. Through extensive experiments on real-world datasets, we demonstrate the superiority of the Doubly Calibrated Estimator in the context of debiased recommendation tasks. | Wonbin Kweon, Hwanjo Yu |  |
| 133 |  |  [General Debiasing for Graph-based Collaborative Filtering via Adversarial Graph Dropout](https://doi.org/10.1145/3589334.3645667) |  | 0 | Graph neural networks (GNNs) have shown impressive performance in recommender systems, particularly in collaborative filtering (CF). The key lies in aggregating neighborhood information on a user-item interaction graph to enhance user/item representations. However, we have discovered that this aggregation mechanism comes with a drawback, which amplifies biases present in the interaction graph. For instance, a user's interactions with items can be driven by both unbiased true interest and various biased factors like item popularity or exposure. However, the current aggregation approach combines all information, both biased and unbiased, leading to biased representation learning. Consequently, graph-based recommenders can learn distorted views of users/items, hindering the modeling of their true preferences and generalizations. To address this issue, we introduce a novel framework called Adversarial Graph Dropout (AdvDrop). It differentiates between unbiased and biased interactions, enabling unbiased representation learning. For each user/item, AdvDrop employs adversarial learning to split the neighborhood into two views: one with bias-mitigated interactions and the other with bias-aware interactions. After view-specific aggregation, AdvDrop ensures that the bias-mitigated and bias-aware representations remain invariant, shielding them from the influence of bias. We validate AdvDrop's effectiveness on five public datasets that cover both general and specific biases, demonstrating significant improvements. Furthermore, our method exhibits meaningful separation of subgraphs and achieves unbiased representations for graph-based CF models, as revealed by in-depth analysis. Our code is publicly available at https://github.com/Arthurma71/AdvDrop. | An Zhang, Wenchang Ma, Pengbo Wei, Leheng Sheng, Xiang Wang |  |
| 134 |  |  [Online Sequential Decision-Making with Unknown Delays](https://doi.org/10.1145/3589334.3645388) |  | 0 | In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay. Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback. Our proposed algorithms are versatile and applicable to universal norms. Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points. For each type of algorithm, we provide corresponding regret bounds under cases of general convexity and relative strong convexity, respectively. We also demonstrate the efficiency of each algorithm under different norms through concrete examples. Furthermore, our theoretical results are consistent with the current best bounds when degenerated to standard settings. | Ping Wu, Heyan Huang, Zhengyang Liu |  |
| 135 |  |  [Triage of Messages and Conversations in a Large-Scale Child Victimization Corpus](https://doi.org/10.1145/3589334.3648142) |  | 0 | Children are among the most vulnerable online populations. Reports of child sexual exploitation on social media and apps have grown annually at an alarming rate and are overwhelming investigators. Even a single case can require examining millions of messages involving hundreds of victims. Triage and prioritization based on victims' experiences is an unfortunate necessity. Using a chat dataset of more than 3 million messages between victims and perpetrators, we evaluate and contribute tools for analyzing the experiences of victims of sexual exploitation. We develop both supervised and unsupervised methods to classify messages into categories of interest to law enforcement, such as age requests, persuasion, and sexual messages. We also introduce a conversation clustering technique to illuminate differences among victims' experiences based on their chat history. Through a qualitative analysis, we demonstrate that the learned clusters are coherent and represent distinct conversation patterns. For example, we can distinguish groups of users who never comply with sexual requests, comply after a few conversations, or comply immediately after being targeted. We expect this approach and associated visualizations will aid law enforcement, industry moderators, and sociologists who need to analyze massive corpora in this domain. Finally, we validate prior models derived from conversations involving adults pretending to be minors and provide statistics that could help undercover adults more accurately portray minor victims. | Prasanna Lakkur Subramanyam, Mohit Iyyer, Brian Neil Levine |  |
| 136 |  |  [Item-side Fairness of Large Language Model-based Recommendation System](https://doi.org/10.1145/3589334.3648158) |  | 0 | Recommendation systems for Web content distribution intricately connect to the information access and exposure opportunities for vulnerable populations. The emergence of Large Language Models-based Recommendation System (LRS) may introduce additional societal challenges to recommendation systems due to the inherent biases in Large Language Models (LLMs). From the perspective of item-side fairness, there remains a lack of comprehensive investigation into the item-side fairness of LRS given the unique characteristics of LRS compared to conventional recommendation systems. To bridge this gap, this study examines the property of LRS with respect to item-side fairness and reveals the influencing factors of both historical users' interactions and inherent semantic biases of LLMs, shedding light on the need to extend conventional item-side fairness methods for LRS. Towards this goal, we develop a concise and effective framework called IFairLRS to enhance the item-side fairness of an LRS. IFairLRS covers the main stages of building an LRS with specifically adapted strategies to calibrate the recommendations of LRS. We utilize IFairLRS to fine-tune LLaMA, a representative LLM, on MovieLens and Steam datasets, and observe significant item-side fairness improvements. The code can be found in https://github.com/JiangM-C/IFairLRS.git. | Meng Jiang, Keqin Bao, Jizhi Zhang, Wenjie Wang, Zhengyi Yang, Fuli Feng, Xiangnan He |  |
| 137 |  |  [Collaborative-Enhanced Prediction of Spending on Newly Downloaded Mobile Games under Consumption Uncertainty](https://doi.org/10.1145/3589335.3648297) |  | 0 | With the surge in mobile gaming, accurately predicting user spending on newly downloaded games has become paramount for maximizing revenue. However, the inherently unpredictable nature of user behavior poses significant challenges in this endeavor. To address this, we propose a robust model training and evaluation framework aimed at standardizing spending data to mitigate label variance and extremes, ensuring stability in the modeling process. Within this framework, we introduce a collaborative-enhanced model designed to predict user game spending without relying on user IDs, thus ensuring user privacy and enabling seamless online training. Our model adopts a unique approach by separately representing user preferences and game features before merging them as input to the spending prediction module. Through rigorous experimentation, our approach demonstrates notable improvements over production models, achieving a remarkable 17.11% enhancement on offline data and an impressive 50.65% boost in an online A/B test. In summary, our contributions underscore the importance of stable model training frameworks and the efficacy of collaborative-enhanced models in predicting user spending behavior in mobile gaming. | Peijie Sun, Yifan Wang, Min Zhang, Chuhan Wu, Yan Fang, Hong Zhu, Yuan Fang, Meng Wang |  |
| 138 |  |  [HiFI: Hierarchical Fairness-aware Integrated Ranking with Constrained Reinforcement Learning](https://doi.org/10.1145/3589335.3648317) |  | 0 | Integrated ranking is a critical component in industrial recommendation platforms. It combines candidate lists from different upstream channels or sources and ranks them into an integrated list, which will be exposed to users. During this process, to take responsibility for channel providers, the integrated ranking system needs to consider the exposure fairness among channels, which directly affects the opportunities of different channels being displayed to users. Besides, personalization also requires the integrated ranking system to consider the user's diverse preference on different channels besides items. Existing methods are hard to address both problems effectively. In this paper, we propose a <u>Hi</u>erarchical <u>F</u>airness-aware <u>I</u>ntegrated ranking (HiFI) framework. It contains a channel recommender and an item recommender, and the fairness constraint is on channels with constrained RL. We also design a gated attention layer (GAL) to effectively capture users' multi-faceted preferences. We compare HiFI with various baselines on public and industrial datasets, and HiFI achieves the state-of-the-art performance on both utility and fairness metrics. We also conduct an online A/B test to further validate the effectiveness of HiFI. | Yifan Liu, Wei Xia, Weiwen Liu, Menghui Zhu, Weinan Zhang, Ruiming Tang, Yong Yu |  |
| 139 |  |  [On Practical Diversified Recommendation with Controllable Category Diversity Framework](https://doi.org/10.1145/3589335.3648323) |  | 0 | Recommender systems have made significant strides in various industries, primarily driven by extensive efforts to enhance recommendation accuracy. However, this pursuit of accuracy has inadvertently given rise to echo chamber/filter bubble effects. Especially in industry, it could impair user's experiences and prevent user from accessing a wider range of items. One of the solutions is to take diversity into account. However, most of existing works focus on user's explicit preferences, while rarely exploring user's non-interaction preferences. These neglected non-interaction preferences are especially important for broadening user's interests in alleviating echo chamber/filter bubble effects.Therefore, in this paper, we first define diversity as two distinct definitions, i.e., user-explicit diversity (U-diversity) and user-item non-interaction diversity (N-diversity) based on user historical behaviors. Then, we propose a succinct and effective method, named as Controllable Category Diversity Framework (CCDF) to achieve both high U-diversity and N-diversity simultaneously.Specifically, CCDF consists of two stages, User-Category Matching and Constrained Item Matching. The User-Category Matching utilizes the DeepU2C model and a combined loss to capture user's preferences in categories, and then selects the top-K categories with a controllable parameter K.These top-K categories will be used as trigger information in Constrained Item Matching. Offline experimental results show that our proposed DeepU2C outperforms state-of-the-art diversity-oriented methods, especially on N-diversity task. The whole framework is validated in a real-world production environment by conducting online A/B testing. | Tao Zhang, Luwei Yang, Zhibo Xiao, Wen Jiang, Wei Ning |  |
| 140 |  |  [Optimization-Based Budget Pacing in eBay Sponsored Search](https://doi.org/10.1145/3589335.3648331) |  | 0 | In online platforms like eBay, sponsored search advertising has become instrumental for businesses aiming for enhanced visibility. However, in automated ad auctions, the sellers (ad campaigns) run the risk of exhausting their budgets prematurely in the absence of proper pacing strategies. In response to this, online platforms have been prompted to employ budget pacing strategies to maintain consistent spending patterns for their sellers. While numerous budget pacing strategies have been introduced, they predominantly stem from either empirical or theoretical perspectives, often functioning in isolation. This paper aims to bridge this gap by investigating the performance of a theoretically inspired optimization-based bid shading method, AdaptivePacing, within eBay's sponsored search environment and proposing variants of the algorithm tailored to real-world environments. Our findings highlight the benefits of applying theoretical pacing approaches in practical contexts. Specifically, the optimization-based AdaptivePacing method offers the platform flexible control over campaign spending patterns, accounts for business constraints, and suggests tailored strategies for distinct advertisers. Furthermore, when evaluating AdaptivePacing alongside established empirical methods, we demonstrate its practical effectiveness and pinpoint areas for further refinement. | Qinyi Chen, Phuong Ha Nguyen, Djordje Gligorijevic |  |
| 141 |  |  [AutoML for Large Capacity Modeling of Meta's Ranking Systems](https://doi.org/10.1145/3589335.3648336) |  | 0 | Web-scale ranking systems at Meta serving billions of users is complex. Improving ranking models is essential but engineering heavy. Automated Machine Learning (AutoML) can release engineers from labor intensive work of tuning ranking models; however, it is unknown if AutoML is efficient enough to meet tight production timeline in real-world and, at the same time, bring additional improvements to the strong baselines. Moreover, to achieve higher ranking performance, there is an ever-increasing demand to scale up ranking models to even larger capacity, which imposes more challenges on the efficiency. The large scale of models and tight production schedule requires AutoML to outperform human baselines by only using a small number of model evaluation trials (around 100). We presents a sampling-based AutoML method, focusing on neural architecture search and hyperparameter optimization, addressing these challenges in Meta-scale production when building large capacity models. Our approach efficiently handles large-scale data demands. It leverages a lightweight predictor-based searcher and reinforcement learning to explore vast search spaces, significantly reducing the number of model evaluations. Through experiments in large capacity modeling for CTR and CVR applications, we show that our method achieves outstanding Return on Investment (ROI) versus human tuned baselines, with up to 0.09% Normalized Entropy (NE) loss reduction or $25\%$ Query per Second (QPS) increase by only sampling one hundred models on average from a curated search space. The proposed AutoML method has already made real-world impact where a discovered Instagram CTR model with up to -0.36% NE gain (over existing production baseline) was selected for large-scale online A/B test and show statistically significant gain. These production results proved AutoML efficacy and accelerated its adoption in ranking systems at Meta. | Hang Yin, KuangHung Liu, Mengying Sun, Yuxin Chen, Buyun Zhang, Jiang Liu, Vivek Sehgal, Rudresh Rajnikant Panchal, Eugen Hotaj, Xi Liu, Daifeng Guo, Jamey Zhang, Zhou Wang, Shali Jiang, Huayu Li, Zhengxing Chen, WenYen Chen, Jiyan Yang, Wei Wen |  |
| 142 |  |  [OneSparse: A Unified System for Multi-index Vector Search](https://doi.org/10.1145/3589335.3648338) |  | 0 | Multi-index vector search has become the cornerstone for many applications, such as recommendation systems. Efficient search in such a multi-modal hybrid vector space is challenging since no single index design performs well for all kinds of vector data. Existing approaches to processing multi-index hybrid queries either suffer from algorithmic limitations or processing inefficiency. In this paper, we propose OneSparse, a unified multi-vector index query system that incorporates multiple posting-based vector indices, which enables highly efficient retrieval of multi-modal data-sets. OneSparse introduces a novel multi-index query engine design of inter-index intersection push-down. It also optimizes the vector posting format to expedite multi-index queries. Our experiments show OneSparse achieves more than 6x search performance improvement while maintaining comparable accuracy. OneSparse has already been integrated into Microsoft online web search and advertising systems with 5x+ latency gain for Bing web search and 2.0% Revenue Per Mille (RPM) gain for Bing sponsored search. | Yaoqi Chen, Ruicheng Zheng, Qi Chen, Shuotao Xu, Qianxi Zhang, Xue Wu, Weihao Han, Hua Yuan, Mingqin Li, Yujing Wang, Jason Li, Fan Yang, Hao Sun, Weiwei Deng, Feng Sun, Qi Zhang, Mao Yang |  |
| 143 |  |  [LLaCE: Locally Linear Contrastive Embedding](https://doi.org/10.1145/3589335.3651534) |  | 0 | Node embedding is one of the most widely adopted techniques in numerous graph analysis tasks, such as node classification. Methods for node embedding can be broadly classified into three categories: proximity matrix factorization approaches, sampling methods, and deep learning strategies. Among the deep learning strategies, graph contrastive learning has attracted significant interest. Yet, it has been observed that existing graph contrastive learning approaches do not adequately preserve the local topological structure of the original graphs, particularly when neighboring nodes belong to disparate categories. To address this challenge, this paper introduces a novel node embedding approach named Locally Linear Contrastive Embedding (LLaCE). LLaCE is designed to maintain the intrinsic geometric structure of graph data by utilizing locally linear formulation, thereby ensuring that the local topological characteristics are accurately reflected in the embedding space. Experimental results on one synthetic dataset and five real-world datasets validate the effectiveness of our proposed method. | Ruichen Liu, Yang Liu, Jiming Liu |  |
| 144 |  |  [GraphSAGE-based POI Recommendation via Continuous-Time Modeling](https://doi.org/10.1145/3589335.3651515) |  | 0 | With the proliferation of Location-based Social Networks (LBSNs), user check-in data at Points-of-Interest (POIs) has surged, reshaping user-environment interaction. However, POI recommendation remains a challenging task for two primary reasons. First, external incentives often drive users' check-ins, potentially misrepresenting their genuine preferences. Second, while many current research model the temporal dynamics of user preferences in a discrete space, they ignore capturing the continuous evolution of these preferences. To address these challenges, we propose the GraphSAGE-based POI Recommendation via Continuous-Time Modeling (GSA-CTM). We first utilize GraphSAGE to identify real user preferences and filter out noise beyond the user's real preferences. After GraphSAGE captures complex interaction, we use Gated Recurrent Unit (GRU) combined with neural Ordinary Differential Equations (ODEs) to capture the temporal information embedded in the interaction, and then use neural ODEs to model the user's continuous dynamic preferences into continuous space. Experiments on two widely-used public datasets validate the superiority of our method. | Yuwen Liu, Lianyong Qi, Weiming Liu, Xiaolong Xu, Xuyun Zhang, Wanchun Dou |  |
| 145 |  |  [Boost Social Recommendation via Adaptive Denoising Network](https://doi.org/10.1145/3589335.3651473) |  | 0 | Social recommendation aims to integrate social relationships to improve the performance of recommendation, and has attracted increasing attention in the field of recommendation system. Recently, Graph Neural Networks (GNNs) based methods for social recommendation are very competitive, but most of them overlook the fact that social relationships may have potential noises. Through the message passing mechanism of GNNs, these noises could be propagated and amplified, ultimately reducing the performance of recommendation. In view of this, we propose a novel GNN-based Adaptive Denoising Social Recommendation (ADSRec) method. It devises a denoising network, which can alleviate the impact of social relationships noises via the adaptive weight adjustment strategy. By further introducing the contrastive learning, the representations of users and items can be enhanced, leading to better recommendation results. Extensive experiments on three widely used datasets demonstrate the superiority of ADSRec over baselines. | Xinran Chen, Chaobo He, Quanlong Guan |  |
| 146 |  |  [3D Face Reconstruction Using A Spectral-Based Graph Convolution Encoder](https://doi.org/10.1145/3589335.3651460) |  | 0 | Monocular 3D face reconstruction plays a crucial role in avatar generation, with significant demand in web-related applications such as generating virtual financial advisors in FinTech. Current reconstruction methods predominantly rely on deep learning techniques and employ 2D self-supervision as a means to guide model learning. However, these methods encounter challenges in capturing the comprehensive 3D structural information of the face due to the utilization of 2D images for model training purposes. To overcome this limitation and enhance the reconstruction of 3D structural features, we propose an innovative approach that integrates existing 2D features with 3D features to guide the model learning process. Specifically, we introduce the 3D-ID Loss, which leverages the high-dimensional structure features extracted from a Spectral-Based Graph Convolution Encoder applied to the facial mesh. This approach surpasses the sole reliance on the 3D information provided by the facial mesh vertices coordinates. Our model is trained using 2D-3D data pairs from a combination of datasets and achieves state-of-the-art performance on the NoW benchmark. | Haoxin Xu, Zezheng Zhao, Yuxin Cao, Chunyu Chen, Hao Ge, Ziyao Liu |  |
| 147 |  |  [General2Specialized LLMs Translation for E-commerce](https://doi.org/10.1145/3589335.3651510) |  | 0 | Existing Neural Machine Translation (NMT) models mainly handle translation in the general domain, while overlooking domains with special writing formulas, such as e-commerce and legal documents. Taking e-commerce as an example, the texts usually include amounts of domain-related words and have more grammar problems, which leads to inferior performances of current NMT methods. To address these problems, we collect two domain-related resources, including a set of term pairs (aligned Chinese-English bilingual terms) and a parallel corpus annotated for the e-commerce domain. Furthermore, we propose a two-step fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general NMT model to the specialized NMT model for e-commerce. The paradigm can be used for the NMT models based on Large language models (LLMs). Extensive evaluations on real e-commerce titles demonstrate the superior translation quality and robustness of our G2ST approach, as compared with state-of-the-art NMT models such as LLaMA, Qwen, GPT-3.5, and even GPT-4. | Kaidi Chen, Ben Chen, Dehong Gao, Huangyu Dai, Wen Jiang, Wei Ning, Shanqing Yu, Libin Yang, Xiaoyan Cai |  |
| 148 |  |  [Counterfactual Explanations for Visual Recommender Systems](https://doi.org/10.1145/3589335.3651484) |  | 0 | Users rely on clever recommendations for items they might like to buy, and service providers rely on clever recommender systems to ensure that their product is recommended to their target audience. Providing explanations for recommendations helps to increase transparency and the users' overall trust in the system, besides helping practitioners debug their recommendation model. Modern recommendation systems utilize multi-modal data such as reviews and images to provide recommendation. In this work, we propose CAVIAR (Counterfactual explanations for VIsual Recommender systems), a novel method to explain recommender systems that utilize visual features of items. Our explanation is counterfactual and is optimized to be simultaneously simple and effective. Given an item in the user's top-K recommended list, CAVIAR makes a minimal, yet meaningful, perturbation to the item's image-embedding such that it is no longer a part of the list. In this way, CAVIAR aims to find the visual features of the item that were the most relevant for the recommendation. In order to lend meaning to the perturbations, we leverage CLIP model to connect the perturbed image features to textual features. We frame the explanation as a natural language counterfactual by contrasting the observed visual features in the item before and after the perturbation. | Neham Jain, Vibhhu Sharma, Gaurav Sinha |  |
| 149 |  |  [Ad Laundering: How Websites Deceive Advertisers into Rendering Ads Next to Illicit Content](https://doi.org/10.1145/3589335.3651466) |  | 0 | Providing online content monetized via ads to users is a lucrative business. But what if the content is pirated or illicit, thus harming the brand safety of the advertiser? In this paper, we are the first to investigate Ad Laundering: a technique with which bad actors deceive advertisers by hiding illicit content within evidently lawful websites to monetize the generated traffic. We develop a client-side detection methodology to detect and analyze websites performing ad laundering. We describe in detail the techniques these websites use to cloak content, and provide estimations for the ad revenues they are able to collect on a monthly basis. Finally, we attribute the generated revenue to different traffic channels and establish that even popular brands have their ads rendered next to undesirable content. | Emmanouil Papadogiannakis, Panagiotis Papadopoulos, Evangelos P. Markatos, Nicolas Kourtellis |  |
| 150 |  |  [Is the 'Impression Log' Beneficial to Evaluating News Recommender Systems? No, it is Not!](https://doi.org/10.1145/3589335.3651527) |  | 0 | This paper aims to answer the question of whether to use the impression log in evaluating news recommendation models. We start with a claim that the testing with the impression log composed of only hard-negative news (i.e., impression (IMP)-based test) is not beneficial to evaluating the models precisely. Based on the claim, we discuss a way of evaluating models by employing all kinds of negative news articles (i.e., Total test). Also, we propose a more-efficient way of evaluating models by sampling only a small number of negative articles (i.e., random-sampling (RS)-based test). We verify our claim by extensively comparing the evaluation results on six models from the IMP-based, Total, and RS-based tests: the RS-based test shows more accurate results than the IMP-based test in determining the superiority among the models while providing higher efficiency than the Total test. Therefore, our answer to the question above would be "do not employ the impression log in testing models even if it is available." This result is quite meaningful since it enables news recommendation researchers and practitioners, who have been using the impression log thus going to the wrong way, to turn to the right one. | Jeewon Ahn, HongKyun Bae, SangWook Kim | Hanyang University, Seoul, Republic of Korea |
| 151 |  |  [How Good are LLMs in Generating Personalized Advertisements?](https://doi.org/10.1145/3589335.3651520) |  | 0 | In this paper, we explore the potential of large language models (LLMs) in generating personalized online advertisements (ads) tailored to specific personality traits, focusing on openness and neuroticism. We conducted a user study involving two tasks to understand the performance of LLM-generated ads compared to human-written ads in different online environments. Task 1 simulates a social media environment where users encounter ads while scrolling through their feed. Task 2 mimics a shopping website environment where users are presented with multiple sponsored products side-by-side. Our results indicate that LLM-generated ads targeting the openness trait positively impact user engagement and preferences, with performance comparable to human-written ads. Furthermore, in both scenarios, the overall effectiveness of LLM-generated ads was found to be similar to that of human-written ads, highlighting the potential of LLM-generated personalised content to rival traditional advertising methods with the added advantage of scalability. This study underscores the need for cautious consideration in the deployment of LLM-generated content at scale. While our findings confirm the scalability and potential effectiveness of LLM-generated content, there is an equally pressing concern about the ease with which it can be misused. | Elyas Meguellati, Lei Han, Abraham Bernstein, Shazia W. Sadiq, Gianluca Demartini |  |
| 152 |  |  [Compact Interpretable Tensor Graph Multi-Modal News Embeddings](https://doi.org/10.1145/3589335.3651480) |  | 0 | Online news articles encompass a variety of modalities such as text and images. How can we learn a representation that incorporates information from all those modalities in a compact and interpretable manner? In this paper, we propose CITEM (Compact Interpretable Tensor graph multi-modal news EMbedding), a tensor-based framework for compact and interpretable multi-modal news representations. CITEM generates a tensor graph consisting of a news similarity graph for each modality and employs a tensor decomposition to produce compact and interpretable embeddings, each dimension of which is a heterogeneous co-cluster of news articles and corresponding modalities. We extensively validate CITEM compared to baselines on two news classification tasks: misinformation news detection and news categorization. The experimental results show that CITEM performs within the same range of AUC as state-of-the-art baselines while producing 7x to 10.5x more compact embeddings. In addition, each embedding dimension of CITEM is interpretable, representing a latent co-cluster of articles. | Dawon Ahn, William Shiao, Arindam Khaled, Andrew Bauer, Stefanos Poulis, Evangelos E. Papalexakis |  |
| 153 |  |  [Proactive Recommendation with Iterative Preference Guidance](https://doi.org/10.1145/3589335.3651548) |  | 0 | Recommender systems mainly tailor personalized recommendations according to user interests learned from user feedback. However, such recommender systems passively cater to user interests and even reinforce existing interests in the feedback loop, leading to problems like filter bubbles and opinion polarization. To counteract this, proactive recommendation actively steers users towards developing new interests in a target item or topic by strategically modulating recommendation sequences. Existing work for proactive recommendation faces significant hurdles: 1) overlooking the user feedback in the guidance process; 2) lacking explicit modeling of the guiding objective; and 3) insufficient flexibility for integration into existing industrial recommender systems. To address these issues, we introduce an Iterative Preference Guidance (IPG) framework. IPG performs proactive recommendation in a flexible post-processing manner by ranking items according to their IPG scores that consider both interaction probability and guiding value. These scores are explicitly estimated with iteratively updated user representation that considers the most recent user interactions. Extensive experiments validate that IPG can effectively guide user interests toward target interests with a reasonable trade-off in recommender accuracy. The code is available at https://github.com/GabyUSTC/IPG-Rec. | Shuxian Bi, Wenjie Wang, Hang Pan, Fuli Feng, Xiangnan He |  |
| 154 |  |  [FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs](https://doi.org/10.1145/3589335.3651504) |  | 0 | Our society is facing rampant misinformation harming public health and trust. To address the societal challenge, we introduce FACT-GPT, a system leveraging Large Language Models (LLMs) to automate the claim matching stage of fact-checking. FACT-GPT, trained on a synthetic dataset, identifies social media content that aligns with, contradicts, or is irrelevant to previously debunked claims. Our evaluation shows that our specialized LLMs can match the accuracy of larger models in identifying related claims, closely mirroring human judgment. This research provides an automated solution for efficient claim matching, demonstrates the potential of LLMs in supporting fact-checkers, and offers valuable resources for further research in the field. | EunCheol Choi, Emilio Ferrara |  |
| 155 |  |  [Benchmarking News Recommendation in the Era of Green AI](https://doi.org/10.1145/3589335.3651472) |  | 0 | Over recent years, news recommender systems have gained significant attention in both academia and industry, emphasizing the need for a standardized benchmark to evaluate and compare the performance of these systems. Concurrently, Green AI advocates for reducing the energy consumption and environmental impact of machine learning. To address these concerns, we introduce the first Green AI benchmarking framework for news recommendation, known as GreenRec, and propose a metric for assessing the tradeoff between recommendation accuracy and efficiency. Our benchmark encompasses 30 base models and their variants, covering traditional end-to-end training paradigms as well as our proposed efficient only-encode-once (OLEO) paradigm. Through experiments consuming 2000 GPU hours, we observe that the OLEO paradigm achieves competitive accuracy compared to state-of-the-art end-to-end paradigms and delivers up to a 2992% improvement in sustainability metrics. | Qijiong Liu, Jieming Zhu, Quanyu Dai, XiaoMing Wu |  |
| 156 |  |  [Event GDR: Event-Centric Generative Document Retrieval](https://doi.org/10.1145/3589335.3651500) |  | 0 | Generative document retrieval, an emerging paradigm in information retrieval, learns to build connections between documents and identifiers within a single model, garnering significant attention. However, there are still two challenges: (1) neglecting inner-content correlation during document representation; (2) lacking explicit semantic structure during identifier construction. Nonetheless, events have enriched relations and well-defined taxonomy, which could facilitate addressing the above two challenges. Inspired by this, we propose Event GDR, an event-centric generative document retrieval model, integrating event knowledge into this task. Specifically, we utilize an exchange-then-reflection method based on multi-agents for event knowledge extraction. For document representation, we employ events and relations to model the document to guarantee the comprehensiveness and inner-content correlation. For identifier construction, we map the events to well-defined event taxonomy to construct the identifiers with explicit semantic structure. Our method achieves significant improvement over the baselines on two datasets, and also hopes to provide insights for future research. | Yong Guan, Dingxiao Liu, Jinchen Ma, Hao Peng, Xiaozhi Wang, Lei Hou, Ru Li |  |
| 157 |  |  [DIAERESIS: Knowledge Graph Partitioning for Efficient Query Answering](https://doi.org/10.1145/3589335.3651231) |  | 0 | The rapid explosion of linked data demands effective and efficient storage, management, and querying methods. Apache Spark is one of the most widely used engines for big data processing, with more and more systems adopting it for efficient query answering. Existing approaches, exploiting Spark for querying RDF data, adopt partitioning techniques for reducing the data that need to be accessed in order to improve efficiency. However, simplistic methods for data partitioning fail to minimize data access at query answering and effectively improve query efficiency. In this demonstration, we present DIAERESIS, a novel platform that exploits a summary-based partitioning strategy achieving a significant improvement in minimizing data access and as such improving query-answering efficiency. DIAERESIS first identifies the top-k most important schema nodes and distributes the other schema nodes to the centroid they mostly depend on. Then, it allocates the corresponding instance nodes to the schema nodes they are instantiated under, creating vertical sub-partitions and indexes. We allow conference participants to actively identify the impact of our partitioning methodology on data distribution and replication, data accessed for query answering, and query answering efficiency. Further, we contrast our approach with existing partitioning approaches adopted by state-of-the-art systems in the domain, providing a deep understanding of the challenges in the area. | Georgia Troullinou, Kostas Stefanidis, Dimitris Plexousakis, Haridimos Kondylakis |  |
| 158 |  |  [Box2Go: Collaborative Interactive Infobox Filling](https://doi.org/10.1145/3589335.3651235) |  | 0 | Infoboxes can be useful to quickly learn about the contents of text collections, but manually creating them is error-prone and time-consuming, and existing automatic approaches require training data or resources like ontologies that are not available for every domain. Moreover, they lack techniques for adaptation to the user. We therefore propose a system to automatically fill user-defined attributes of infoboxes with the human-in-the-loop which provides this adaptation, and works without training data and domain-specific resources. Our approach generalizes simple user feedback to explore a joint embedding space and find the correct values for the attributes. These structured representations of the texts can be used for collaborative exploration of text collections on the web. We provide a prototypic implementation for such a collaborative web application and demonstrate its usage. | Benjamin Hättasch, Carsten Binnig |  |
| 159 |  |  [HINCare: An Intelligent Helper Recommender System for Elderly Care](https://doi.org/10.1145/3589335.3651236) |  | 0 | In Hong Kong, the number of elderly citizens will reach one-third of the population within the next decade. To mitigate this problem, timebanking has received attention in recent years. In timebanking, an NGO helper earns time credits through providing voluntary services (e.g., household duties) to elders. These time credits can be used to acquire other services. Although timebanking has shown the promise of promoting mutual care in many countries, its potential has not been fully utilized, due to the lack of IT and data support. We thus develop HINCare, a software platform that supports timebanking for multiple NGOs. Besides providing convenience to NGO supervisors, helpers, and elders, HINCare makes use of a heterogeneous information network (HIN) for recommending suitable helpers to elders. This is the first time a graph-based recommender system is used for such purposes. Currently, HINCare is used by 12 NGOs to serve more than 5000 users in Hong Kong. In this demonstration, participants can play the role of helpers and elders in the HINCare environment. | Carrie Wang, Wentao Ning, Xiaoman Wu, Reynold Cheng |  |
| 160 |  |  [Linked Open Literature Review using the Neuro-symbolic Open Research Knowledge Graph](https://doi.org/10.1145/3589335.3651238) |  | 0 | The way scholarly knowledge and in particular literature reviews are communicated today rather resembles static, unstructured, pseudo-digitized articles, which are hardly processable by machines and AI. This demo showcases a novel way to create and publish scholarly literature reviews, also called semantic reviews. The neuro-symbolic approach consists of extracting key insights from scientific papers leveraging neural models and organizing them using a symbolic scholarly knowledge graph. The food information engineering review case study will allow participants to see how this approach is implemented using the Open Research Knowledge Graph (ORKG). The real-time demo will allow participants to play with the ORKG and create their own living, semantic review. | Azanzi Jiomekong, Sören Auer, Allard Oelen |  |
| 161 |  |  [RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems](https://doi.org/10.1145/3589335.3651242) |  | 0 | This paper introduces RecAI, a practical toolkit designed to augment or even revolutionize recommender systems with the advanced capabilities of Large Language Models (LLMs). RecAI provides a suite of tools, including Recommender AI Agent, Recommendation-oriented Language Models, Knowledge Plugin, RecExplainer, and Evaluator, to facilitate the integration of LLMs into recommender systems from multifaceted perspectives. The new generation of recommender systems, empowered by LLMs, are expected to be more versatile, explainable, conversational, and controllable, paving the way for more intelligent and user-centric recommendation experiences. We hope the open-source of RecAI can help accelerate evolution of new advanced recommender systems. The source code of RecAI is available at <https://github.com/microsoft/RecAI>. | Jianxun Lian, Yuxuan Lei, Xu Huang, Jing Yao, Wei Xu, Xing Xie |  |
| 162 |  |  [Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform](https://doi.org/10.1145/3589335.3651243) |  | 0 | Large language model evaluation plays a pivotal role in the enhancement of its capacity. Previously, numerous methods for evaluating large language models have been proposed in this area. Despite their effectiveness, these existing works mainly focus on assessing objective questions, overlooking the capability to evaluate subjective questions which is extremely common for large language models. Additionally, these methods predominantly utilize centralized datasets for evaluation, with question banks concentrated within the evaluation platforms themselves. Moreover, the evaluation processes employed by these platforms often overlook personalized factors, neglecting to consider the individual characteristics of both the evaluators and the models being evaluated. To address these limitations, we propose a novel anonymous crowd-sourcing evaluation platform, BingJian, for large language models that employs a competitive scoring mechanism where users participate in ranking models based on their performance. This platform stands out not only for its support of centralized evaluations to assess the general capabilities of models but also for offering an open evaluation gateway. Through this gateway, users have the opportunity to submit their questions, testing the models on a personalized and potentially broader range of capabilities. Furthermore, our platform introduces personalized evaluation scenarios, leveraging various forms of human-computer interaction to assess large language models in a manner that accounts for individual user preferences and contexts. The demonstration of BingJian can be accessed at https://github.com/Mingyue-Cheng/Bingjian. | Mingyue Cheng, Hao Zhang, Jiqian Yang, Qi Liu, Li Li, Xin Huang, Liwei Song, Zhi Li, Zhenya Huang, Enhong Chen |  |
| 163 |  |  [PKG API: A Tool for Personal Knowledge Graph Management](https://doi.org/10.1145/3589335.3651247) |  | 0 | Personal knowledge graphs (PKGs) offer individuals a way to store and consolidate their fragmented personal data in a central place, improving service personalization while maintaining full user control. Despite their potential, practical PKG implementations with user-friendly interfaces remain scarce. This work addresses this gap by proposing a complete solution to represent, manage, and interface with PKGs. Our approach includes (1) a user-facing PKG Client, enabling end-users to administer their personal data easily via natural language statements, and (2) a service-oriented PKG API. To tackle the complexity of representing these statements within a PKG, we present an RDF-based PKG vocabulary that supports this, along with properties for access rights and provenance. | Nolwenn Bernard, Ivica Kostric, Weronika Lajewska, Krisztian Balog, Petra Galuscáková, Vinay Setty, Martin G. Skjæveland |  |
| 164 |  |  [Brinjal: A Web-Plugin for Collaborative Hate Speech Detection](https://doi.org/10.1145/3589335.3651250) |  | 0 | The proliferation of hate speech (HS) has compromised the safety and trustworthiness of the internet, exacerbating social divides by promoting hatred and discrimination. Although recent studies have produced guidelines and developed advanced technologies for the automated detection of HS, their efficacy and adaptability in real-world applications remain unclear. Furthermore, existing guidelines on what constitutes HS might not reflect the perspectives and beliefs of individuals and communities. This paper introduces Brinjal, a multifaceted web plugin designed for the collaborative detection of HS. Brinjal enables individuals to identify instances of HS and engage in discussions to verify such content, thereby enhancing the collective understanding of HS. Additionally, Brinjal serves as a practical platform for deploying and evaluating advanced HS detection models, facilitating user interaction and performance assessment. Lastly, Brinjal includes an analytical tool for analyzing HS, offering insights based on the crowdsourced instances and discussions about HS across various websites. The video demonstration of Brinjal can be viewed here: https://youtu.be/\_JxziIVWBO4. Disclaimer: This paper contains violent and discriminatory content that may be disturbing to some readers. | Ming Shan Hee, Karandeep Singh, Charlotte Ng Si Min, Kenny Tsu Wei Choo, Roy KaWei Lee |  |
| 165 |  |  [SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores](https://doi.org/10.1145/3589335.3651251) |  | 0 | We present SocialGenPod, a decentralised and privacy-friendly way of deploying generative AI Web applications. Unlike centralised Web and data architectures that keep user data tied to application and service providers, we show how one can use Solid – a decentralised Web specification – to decouple user data from generative AI applications. We demonstrate SocialGenPod using a prototype that allows users to converse with different Large Language Models, optionally leveraging Retrieval Augmented Generation to generate answers grounded in private documents stored in any Solid Pod that the user is allowed to access, directly or indirectly. SocialGenPod makes use of Solid access control mechanisms to give users full control of determining who has access to data stored in their Pods. SocialGenPod keeps all user data (chat history, app configuration, personal documents, etc) securely in the user's personal Pod; separate from specific model or application providers. Besides better privacy controls, this approach also enables portability across different services and applications. Finally, we discuss challenges, posed by the large compute requirements of state-of-the-art models, that future research in this area should address. Our prototype is open-source and available at: https://github.com/Vidminas/socialgenpod/. | Vidminas Vizgirda, Rui Zhao, Naman Goel |  |
| 166 |  |  [Ducho 2.0: Towards a More Up-to-Date Unified Framework for the Extraction of Multimodal Features in Recommendation](https://doi.org/10.1145/3589335.3651440) |  | 0 | In this work, we introduce Ducho 2.0, the latest stable version of our framework. Differently from Ducho, Ducho 2.0 offers a more personalized user experience with the definition and import of custom extraction models fine-tuned on specific tasks and datasets. Moreover, the new version is capable of extracting and processing features through multimodal-by-design large models. Notably, all these new features are supported by optimized data loading and storing to the local memory. To showcase the capabilities of Ducho 2.0, we demonstrate a complete multimodal recommendation pipeline, from the extraction/processing to the final recommendation. The idea is to provide practitioners and experienced scholars with a ready-to-use tool that, put on top of any multimodal recommendation framework, may permit them to run extensive benchmarking analyses. All materials are accessible at: <https://github.com/sisinflab/Ducho>. | Matteo Attimonelli, Danilo Danese, Daniele Malitesta, Claudio Pomo, Giuseppe Gassi, Tommaso Di Noia |  |
| 167 |  |  [SE-PQA: Personalized Community Question Answering](https://doi.org/10.1145/3589335.3651445) |  | 0 | Personalization in Information Retrieval is a topic studied for a long time. Nevertheless, there is still a lack of high-quality, real-world datasets to conduct large-scale experiments and evaluate models for personalized search. This paper contributes to filling this gap by introducing SE-PQA (StackExchange - Personalized Question Answering), a new curated resource to design and evaluate personalized models related to the task of community Question Answering (cQA). The contributed dataset includes more than 1 million queries and 2 million answers, annotated with a rich set of features modeling the social interactions among the users of a popular cQA platform. We describe the characteristics of SE-PQA and detail the features associated with questions and answers. We also provide reproducible baseline methods for the cQA task based on the resource, including deep learning models and personalization approaches. The results of the preliminary experiments conducted show the appropriateness of SE-PQA to train effective cQA models; they also show that personalization remarkably improves the effectiveness of all the methods tested. Furthermore, we show the benefits in terms of robustness and generalization of combining data from multiple communities for personalization purposes. | Pranav Kasela, Marco Braga, Gabriella Pasi, Raffaele Perego |  |
| 168 |  |  [Digital Democracy at Crossroads: A Meta-Analysis of Web and AI Influence on Global Elections](https://doi.org/10.1145/3589335.3652003) |  | 0 | 2024 will be the largest election year in history involving over 50 countries and approximately 4.2 billion people. Since 1996, the Web has been instrumental in political campaigns, enhancing public engagement and creating new communication avenues for elections. Nevertheless, the proliferation of generative AI technologies has made false information dissemination simpler and quicker, posing a substantial threat to election integrity and democratic processes. The 2024 global elections underscore the need to comprehend and tackle the impact of such technologies on democracy. In this paper, we undertake a detailed meta-analysis, scrutinizing 44 papers published in The Web Conference, detailing the influence of the Web on elections. Our research reveals key historical trends on how the Web has impacted elections: first, social media has revolutionized election strategies through direct voter-candidate interactions. Second, big data and algorithm-driven campaigns are commonplace. Third, AI advancements have exacerbated the spread of fake news, risking election fairness. Predominantly from studies published since 2018 among 44 papers, we underscore the necessity for advanced detection tools, policy formulation, and responsible AI use to maintain electoral integrity. This analysis offers an insight into the Web and AI's impact on elections, presenting pointers for addressing challenges and leveraging opportunities in the 2024 and future elections. | Zheng Wei, Xian Xu, Pan Hui |  |
| 169 |  |  [BoxCare: A Box Embedding Model for Disease Representation and Diagnosis Prediction in Healthcare Data](https://doi.org/10.1145/3589335.3651448) |  | 0 | Diagnosis prediction is becoming crucial to develop healthcare plans for patients based on Electronic Health Records (EHRs). Existing works usually enhance diagnosis prediction via learning accurate disease representation, where many of them try to capture inclusive relations based on the hierarchical structures of existing disease ontologies such as those provided by ICD-9 codes. However, they overlook exclusive relations that can reflect different and complementary perspectives of the ICD-9 structures, and thus fail to accurately represent relations among diseases and ICD-9 codes. To this end, we propose to project disease embeddings and ICD-9 code embeddings into boxes, where a box is an axis-aligned hyperrectangle with a geometric region and two boxes can clearly "include" or "exclude" each other. Upon box embeddings, we further obtain patient embeddings via aggregating the disease representations for diagnosis prediction. Extensive experiments on two real-world EHR datasets show significant performance gains brought by our proposed framework, yielding average improvements of 6.04% for diagnosis prediction over state-of-the-art competitors. | Hang Lv, Zehai Chen, Yacong Yang, Guofang Ma, Yanchao Tan, Carl Yang |  |
| 170 |  |  [Enabling Pre-Shock State Detection using Electrogram Signals from Implantable Cardioverter-Defibrillators](https://doi.org/10.1145/3589335.3651450) |  | 0 | Identifying electrical signatures preceding a ventricular arrhythmia from the implantable cardioverter-defibrillators (ICDs) can help predict an upcoming ICD shock. To achieve this, we first deployed a large-scale study (N=326) to continuously monitor the electrogram (EGM) data from the ICDs and select the EGM segments prior to a shock event and under the normal condition. Next, we design a novel cohesive framework that integrates metric learning, prototype learning, and few-shot learning, enabling learning from an imbalanced dataset. We implement metric learning by leveraging a Siamese neural network architecture, which incorporates LSTM units. We innovatively utilize triplet and pair losses in a sequential manner throughout the training process on EGM samples. This approach generates embeddings that significantly enhance the distinction of EGM signals under different conditions. In the inference stage, k-means clustering identifies prototypes representing pre-shock and normal states from these embeddings. In summary, this framework leverages the predictive potential of signals before ICD shocks, addressing the gap in early cardiac arrhythmia detection. Our experimental results show a notable F1 score of 0.87, sensitivity of 0.97, and precision of 0.79. Our framework offers a significant advancement in cardiac care predictive analytics, promising enhanced ICD decision-making for improved patient outcomes. | Runze Yan, Neal K. Bhatia, Faisal M. Merchant, Alex Fedorov, Ran Xiao, Cheng Ding, Xiao Hu |  |
| 171 |  |  [In The Beginning, Let There Be The Word: Challenges and Insights in Applying Sentiment Analysis to Social Research](https://doi.org/10.1145/3589335.3651264) |  | 0 | Sentiment analysis based on lexical corpora is widely employed despite its inherent limitations in capturing nuances such as sarcasm and irony. This research delves into the application of sentiment analysis to political communication. To address the limitations of the Bag of Words methodology, a comparative study of sentiment analysis tools and emotion detection from speech is conducted, using automated speech recognition as a benchmark. Emotion recognition from speech has shown promising results, indicating its potential superiority over other methods [1], [2]. This study uses media material from Polish radio and television broadcasts, focusing on political interviews during a significant period marked by a high-profile assassination attempt. Results indicate challenges at the micro-level, but aggregated data reveals a significant correlation between valence measured from voice and text. While sentiment analysis may lack sensitivity in capturing mourning-related discourse, it proves effective in political communication devoid of such nuances. This suggests that valence in sentiment analysis reflects emotional content derived from intonation fairly accurately. | Andrzej Meler |  |
| 172 |  |  [Modeling Multidimensional Cognitive Search in Creativity with Generalized Additive Model](https://doi.org/10.1145/3589335.3651267) |  | 0 | Creativity is the ability to develop innovative functional ideas through unconventional associations. The consensus view on creativity in the literature involves divergence from stereotypical and habitual thought patterns [29, 39]. Creativity relies on search to explore diverse solutions. Search requires charting the mental terrain, leveraging past experiences and knowledge to manipulate and reconfigure components for new solutions [28]. The generally-accepted and overly-narrow view on creativity, however, neglects the fact that creativity is multidimensional [14]. This one-dimensional view of creativity triggers questions such as "Does one consider an unethical but novel creation to be creative?" and "Does one consider a new iPhone with mainstream functionalities but advanced camera features to be creative?" This research challenges the one-dimensional view of creativity, offering a more all-encompassing conceptualization of creativity [14]. The research examines the multidimensional nature of creativity by building a computational model of a designer's mutual search process across multiple mutually dependent search spaces. The research examines the trajectory of mutual search across multiple cognitive search spaces using a Generalized Additive Model (GAM). The field experiment employs 108 designers who develop their web designs through five iterations, utilizing computer graphics methods to extract the images. Through measuring the distance of search by considering changes in visual and source code in each iteration, the study argues that the search patterns differ in the degree of exploration in these search spaces over time. The research concludes that designers' search processes are non-linear and argues that there are more than one or two search spaces. The research also provides perceptual explanations of the multiple search processes in designs and argues for a more encompassing view of creativity. | Jia Lin Cheoh |  |
| 173 |  |  [Data Augmentation for Conversational AI](https://doi.org/10.1145/3589335.3641238) |  | 0 | Advancements in conversational systems have revolutionized information access, surpassing the limitations of single queries. However, developing dialogue systems requires a large amount of training data, which is a challenge in low-resource domains and languages. Traditional data collection methods like crowd-sourcing are labor-intensive and time-consuming, making them ineffective in this context. Data augmentation (DA) is an affective approach to alleviate the data scarcity problem in conversational systems. This tutorial provides a comprehensive and up-to-date overview of DA approaches in the context of conversational systems. It highlights recent advances in conversation augmentation, open domain and task-oriented conversation generation, and different paradigms of evaluating these models. We also discuss current challenges and future directions in order to help researchers and practitioners to further advance the field in this area. | Heydar Soudani, Roxana Petcu, Evangelos Kanoulas, Faegheh Hasibi | Radboud Univ Nijmegen, Nijmegen, Netherlands; Univ Amsterdam, Amsterdam, Netherlands |
| 174 |  |  [Recent Advances in Generative Information Retrieval](https://doi.org/10.1145/3589335.3641239) |  | 0 | Generative retrieval (GR) has witnessed significant growth recently in the area of information retrieval. Compared to the traditional "index-retrieve-then-rank'' pipeline, the GR paradigm aims to consolidate all information within a corpus into a single model. Typically, a sequence-to-sequence model is trained to directly map a query to its relevant document identifiers (i.e., docids). This tutorial offers an introduction to the core concepts of the GR paradigm and a comprehensive overview of recent advances in its foundations and applications. We start by providing preliminary information covering foundational aspects and problem formulations of GR. Then, our focus shifts towards recent progress in docid design, training approaches, inference strategies, and applications of GR. We end by outlining challenges and issuing a call for future GR research.This tutorial is intended to be beneficial to both researchers and industry practitioners interested in developing novel GR solutions or applying them in real-world scenarios. | Yubao Tang, Ruqing Zhang, Weiwei Sun, Jiafeng Guo, Maarten de Rijke |  |
| 175 |  |  [Large Language Models for Recommendation: Progresses and Future Directions](https://doi.org/10.1145/3589335.3641247) |  | 0 | The powerful large language models (LLMs) have played a pivotal role in advancing recommender systems. Recently, in both academia and industry, there has been a surge of interest in developing LLMs for recommendation, referred to as LLM4Rec. This includes endeavors like leveraging LLMs for generative item retrieval and ranking, as well as the exciting possibility of building universal LLMs for diverse open-ended recommendation tasks. These developments hold the potential to reshape the traditional recommender paradigm, paving the way for the next-generation recommender systems. In this tutorial, we aim to retrospect the evolution of LLM4Rec and conduct a comprehensive review of existing research. In particular, we will clarify how recommender systems benefit from LLMs through a variety of perspectives, including the model architecture, learning paradigm, and the strong abilities of LLMs such as chatting, generalization, planning, and generation. Furthermore, we will discuss the critical challenges and open problems in this emerging field, for instance, the trustworthiness, efficiency, and model retraining issues. Lastly, we will summarize the implications of previous work and outline future research directions. We believe that this tutorial will assist the audience in better understanding the progress and prospects of LLM4Rec, inspiring them for future exploration. This, in turn, will drive the prosperity of LLM4Rec, possibly fostering a paradigm shift in recommendation systems. | Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He | Natl Univ Singapore, Singapore, Singapore; Univ Sci & Technol China, Hefei, Peoples R China |
| 176 |  |  [Multimodal Pretraining and Generation for Recommendation: A Tutorial](https://doi.org/10.1145/3589335.3641248) |  | 0 | Personalized recommendation stands as a ubiquitous channel for users to explore information or items aligned with their interests. Nevertheless, prevailing recommendation models predominantly rely on unique IDs and categorical features for user-item matching. While this ID-centric approach has witnessed considerable success, it falls short in comprehensively grasping the essence of raw item contents across diverse modalities, such as text, image, audio, and video. This underutilization of multimodal data poses a limitation to recommender systems, particularly in the realm of multimedia services like news, music, and short-video platforms. The recent surge in pretraining and generation techniques presents both opportunities and challenges in the development of multimodal recommender systems. This tutorial seeks to provide a thorough exploration of the latest advancements and future trajectories in multimodal pretraining and generation techniques within the realm of recommender systems. The tutorial comprises three parts: multimodal pretraining, multimodal generation, and industrial applications and open challenges in the field of recommendation. Our target audience encompasses scholars, practitioners, and other parties interested in this domain. By providing a succinct overview of the field, we aspire to facilitate a swift understanding of multimodal recommendation and foster meaningful discussions on the future development of this evolving landscape. | Jieming Zhu, Xin Zhou, Chuhan Wu, Rui Zhang, Zhenhua Dong |  |
| 177 |  |  [On-Device Recommender Systems: A Tutorial on The New-Generation Recommendation Paradigm](https://doi.org/10.1145/3589335.3641250) |  | 0 | Given the sheer volume of contemporary e-commerce applications, recommender systems (RSs) have gained significant attention in both academia and industry. However, traditional cloud-based RSs face inevitable challenges, such as resource-intensive computation, reliance on network access, and privacy breaches. In response, a new paradigm called on-device recommender systems (ODRSs) has emerged recently in various industries like Taobao, Google, and Kuaishou. ODRSs unleash the computational capacity of user devices with lightweight recommendation models tailored for resource-constrained environments, enabling real-time inference with users' local data. This tutorial aims to systematically introduce methodologies of ODRSs, including (1) an overview of existing research on ODRSs; (2) a comprehensive taxonomy of ODRSs, where the core technical content to be covered span across three major ODRS research directions, including on-device deployment and inference, on-device training, and privacy/security of ODRSs; (3) limitations and future directions of ODRSs. This tutorial expects to lay the foundation and spark new insights for follow-up research and applications concerning this new recommendation paradigm. | Hongzhi Yin, Tong Chen, Liang Qu, Bin Cui |  |
| 178 |  |  [Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems](https://doi.org/10.1145/3589335.3651905) |  | 0 | When interacting with Retrieval-Augmented Generation (RAG)-based conversational agents, the users must carefully craft their queries to be understood correctly. Yet, understanding the system's capabilities can be challenging for the users, leading to ambiguous questions that necessitate further clarification. This work aims to bridge the gap by developing a suggestion question generator. To generate suggestion questions, our approach involves utilizing dynamic context, which includes both dynamic few-shot examples and dynamically retrieved contexts. Through experiments, we show that the dynamic contexts approach can generate better suggestion questions as compared to other prompting approaches. | Anuja Tayal, Aman Tyagi |  |
| 179 |  |  [TransDrift: Modeling Word-Embedding Drift using Transformer](https://doi.org/10.1145/3589335.3651894) |  | 0 | In modern NLP applications, word embeddings are a crucial backbone that can be readily shared across a number of tasks. However as the text distributions change and word semantics evolve over time, the downstream applications using the embeddings can suffer if the word representations do not conform to the data drift. Thus, maintaining word embeddings to be consistent with the underlying data distribution is a key problem. In this work, we tackle this problem and propose TransDrift, a transformer-based prediction model for word embeddings. Leveraging the flexibility of transformer, our model accurately learns the dynamics of the embedding drift and predicts the future embedding. In experiments, we compare with existing methods and show that our model makes significantly more accurate predictions of the word embedding than the baselines. Crucially, by applying the predicted embeddings as a backbone for downstream classification tasks, we show that our embeddings lead to superior performance compared to the previous methods. | Nishtha Madaan, Prateek Chaudhury, Nishant Kumar, Srikanta Bedathur |  |
| 180 |  |  [Automated Claim Matching with Large Language Models: Empowering Fact-Checkers in the Fight Against Misinformation](https://doi.org/10.1145/3589335.3651910) |  | 0 | In today's digital era, the rapid spread of misinformation poses threats to public well-being and societal trust. As online misinformation proliferates, manual verification by fact checkers becomes increasingly challenging. We introduce FACT-GPT (Fact-checking Augmentation with Claim matching Task-oriented Generative Pre-trained Transformer), a framework designed to automate the claim matching phase of fact-checking using Large Language Models (LLMs). This framework identifies new social media content that either supports or contradicts claims previously debunked by fact-checkers. Our approach employs GPT-4 to generate a labeled dataset consisting of simulated social media posts. This data set serves as a training ground for fine-tuning more specialized LLMs. We evaluated FACT-GPT on an extensive dataset of social media content related to public health. The results indicate that our fine-tuned LLMs rival the performance of larger pre-trained LLMs in claim matching tasks, aligning closely with human annotations. This study achieves three key milestones: it provides an automated framework for enhanced fact-checking; demonstrates the potential of LLMs to complement human expertise; offers public resources, including datasets and models, to further research and applications in the fact-checking domain. | EunCheol Choi, Emilio Ferrara |  |
| 181 |  |  [Decoding YouTube's Recommendation System: A Comparative Study of Metadata and GPT-4 Extracted Narratives](https://doi.org/10.1145/3589335.3651913) |  | 0 | YouTube's recommendation system is integral to shaping user experiences by suggesting content based on past interactions using collaborative filtering techniques. Nonetheless, concerns about potential biases and homogeneity in these recommendations are prevalent, with the danger of leading users into filter bubbles and echo chambers that reinforce their pre-existing beliefs. Researchers have sought to understand and address these biases in recommendation systems. However, traditionally, such research has relied primarily on metadata, such as video titles, which does not always encapsulate the full content or context of the videos. This reliance on metadata can overlook the nuances and substantive content of videos, potentially perpetuating the very biases and echo chambers that the research aims to unravel. This study advances the examination of sentiment, toxicity, and emotion within YouTube content by conducting a comparative analysis across various depths of titles and narratives extracted by leveraging GPT-4. Our analysis reveals a clear trend in sentiment, emotion, and toxicity levels as the depth of content analysis increases. Notably, there is a general shift from neutral to positive sentiments in both YouTube video titles and narratives. Emotion analysis indicates an increase in positive emotions, particularly joy, with a corresponding decrease in negative emotions such as anger and disgust in narratives, while video titles show a steady decrease in anger. Additionally, toxicity analysis presents a contrasting pattern, with video titles displaying an upward trend in toxicity, peaking at the greatest depth analyzed, whereas narratives exhibit a high initial toxicity level that sharply decreases and stabilizes at lower depths. These findings suggest that the depth of engagement with video content significantly influences emotional and sentiment expressions. | Mayor Inna Gurung, Md Monoarul Islam Bhuiyan, Ahmed AlTaweel, Nitin Agarwal |  |
| 182 |  |  [Graph Coarsening via Convolution Matching for Scalable Graph Neural Network Training](https://doi.org/10.1145/3589335.3651920) |  | 0 | Graph summarization as a preprocessing step is an effective and complementary technique for scalable graph neural network (GNN) training. In this work, we propose the Coarsening Via Convolution Matching (CONVMATCH) algorithm and a highly scalable variant, A-CONVMATCH, for creating summarized graphs that preserve the output of graph convolution. We evaluate CONVMATCH on six real-world link prediction and node classification graph datasets, and show it is efficient and preserves prediction performance while significantly reducing the graph size. Notably, CONVMATCH achieves up to 95 performance of GNNs on node classification while trained on graphs summarized down to 1 tasks, CONVMATCH consistently outperforms all baselines, achieving up to a 2x improvement. | Charles Dickens, Edward W. Huang, Aishwarya Reganti, Jiong Zhu, Karthik Subbian, Danai Koutra |  |
| 183 |  |  [FedHLT: Efficient Federated Low-Rank Adaption with Hierarchical Language Tree for Multilingual Modeling](https://doi.org/10.1145/3589335.3651933) |  | 0 | Federated Multilingual Modeling (FMM) has become an essential approach in natural language processing (NLP) due to increasing linguistic diversity and the heightened emphasis on data privacy. However, FMM faces two primary challenges: 1) the high communication costs inherent in network operations, and 2) the complexities arising from parameter interference, as languages exhibit both unique characteristics and shared features. To tackle these issues, we introduce a communication-efficient framework for Multilingual Modeling (MM) that combines low-rank adaptation with a hierarchical language tree structure. Our method maintains the base model's weights while focusing on updating only the Low-rank adaptation (LoRA) parameters, significantly reducing communication costs. Additionally, we mitigate parameter conflicts by organizing languages based on their familial ties rather than merging all LoRA parameters together. Our experimental findings reveal that this novel model surpasses established baseline models in performance and markedly decreases communication overhead. | Zhihan Guo, Yifei Zhang, Zhuo Zhang, Zenglin Xu, Irwin King |  |
| 184 |  |  [Information Retrieval Meets Large Language Models](https://doi.org/10.1145/3589335.3641299) |  | 0 | The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans play a central role of demanders and evaluators to the reliability of information services. Nevertheless, significant challenges exist, including computational costs, credibility concerns, domain-specific limitations, and ethical considerations. To thoroughly discuss the transformative impact of LLMs on IR research, the Chinese IR community conducted a strategic workshop in April 2023, yielding valuable insights. This paper provides a summary of the workshop’s outcomes, including the rethinking of IR’s core values, the mutual enhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and open challenges. | Zheng Liu, Yujia Zhou, Yutao Zhu, Jianxun Lian, Chaozhuo Li, Zhicheng Dou, Defu Lian, JianYun Nie | Renmin University of China, China; Shandong University, China; University of Science and Technology of China, China; Shandong Artificial Intelligence Institute, China; Institute of Computing Technology, Chinese Academy of Sciences, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, China; Zhejiang University, China; Jilin University, China; Huawei Technologies Ltd. Co, China; Tsinghua University, China; Beijing University of Posts and Telecommunications, China; Wuhan University, China; South China University of Technology, China |
| 185 |  |  [Heterogeneous Knowledge Grounding for Medical Question Answering with Retrieval Augmented Large Language Model](https://doi.org/10.1145/3589335.3651941) |  | 0 | The Large Language Model (LLM) is renowned for its ability to encode a vast amount of general domain knowledge, enabling it to excel in question-answering, dialogue systems, and summarization tasks. However, the medical domain presents a unique challenge to LLM due to the distribution of medical knowledge, which follows a long-tail pattern. Existing approaches address this challenge by injecting medical knowledge into LLM through single sources such as medical textbooks or medical knowledge bases. However, medical knowledge is distributed across multiple heterogeneous information sources. A medical question-answering system can enhance answer coverage and confidence by considering these diverse knowledge sources together. To bridge this gap, we propose a novel approach called Heterogeneous Knowledge Retrieval-Augmented LLM for medical domain question answering. Our experiments, conducted on the MedQA-USMLE dataset, demonstrate promising performance improvements. These results underscore the importance of harnessing heterogeneous knowledge sources in the medical domain. | Wenting Zhao, Zhongfen Deng, Shweta Yadav, Philip S. Yu |  |
| 186 |  |  [Weakly Supervised Video Moment Retrieval via Location-irrelevant Proposal Learning](https://doi.org/10.1145/3589335.3651942) |  | 0 | This paper deals with Video Moment Retrieval (VMR) in a weakly-supervised fashion, which aims to retrieve local video clips with only global video-level descriptions. Scrutinizing the recent advances in VMR, we find that the fully-supervised models achieve strong performance, but they are heavily relied on the precise temporal annotations. Weakly-supervised methods do not rely on temporal annotations, however, their performance is much weaker than the fully-supervised ones. To fill such gap, we propose to take advantage of a pretrained video-text model as hitchhiker to generate pseudo temporal labels. The pseudo temporal labels, together with the descriptive labels, are then utilized to guide the training of the proposed VMR model. The proposed Location-irrelevant Proposal Learning (LPL) model is based on a pretrained video-text model with cross-modal prompt learning, together with different strategies to generate reasonable proposals with various lengths. Despite the simplicity, we find that our method performs much better than the previous state-of-the-art methods on standard benchmarks, eg., +4.4% and +1.4% in mIoU on the Charades and ActivityNet-Caption datasets respectively, which benefits from training with fine-grained video-text pairs. Further experiments on two synthetic datasets with shuffled temporal location and longer video length demonstrate our model's robustness towards temporal localization bias as well as its strength in handling long video sequences. | Wei Ji, Ruiqi Shi, Yinwei Wei, Shanshan Zhao, Roger Zimmermann |  |
| 187 |  |  [One-step Reach: LLM-based Keyword Generation for Sponsored Search Advertising](https://doi.org/10.1145/3589335.3651943) |  | 0 | Query keyword matching plays a crucial role in sponsored search advertising by retrieving semantically related keywords of the user query to target relevant advertisements. Conventional technical solutions adopt the retrieve-judge-then-rank retrieval framework structured in cascade funnels. However, it has limitations in accurately depicting the semantic relevance between the query and keyword, and the cumulative funnel losses result in unsatisfactory precision and recall. To address the above issues, this paper proposes a Large Language Model (LLM)-based keyword generation method (LKG) to reach related keywords from the search query in one step. LKG models the query keyword matching as an end-to-end keyword generation task based on the LLM through multi-match prompt tuning. Moreover, it employs the feedback tuning and the prefix tree-based constrained beam search to improve the generation quality and efficiency. Extensive offline experiments and online A/B testing demonstrate the effectiveness and superiority of LKG which is fully deployed in the Baidu sponsored search system bringing significant improvements. | Yang Wang, Zheyi Sha, Kunhai Lin, Chaobing Feng, Kunhong Zhu, Lipeng Wang, Xuewu Jiao, Fei Huang, Chao Ye, Dengwu He, Zhi Guo, Shuanglong Li, Lin Liu |  |
| 188 |  |  [A Case Study of Enhancing Sparse Retrieval using LLMs](https://doi.org/10.1145/3589335.3651945) |  | 0 | While dense retrieval methods have made significant advancements, sparse retrieval techniques continue to offer advantages in terms of interpretability and generalizability. However, query-document term mismatch in sparse retrieval persists, rendering it infeasible for many practical applications. Recent research has shown that Large Language Models (LLMs) hold relevant information that can enhance sparse retrieval through the application of prompt engineering. In this paper, we build upon this concept to explore various strategies employing LLMs for information retrieval purposes. Specifically, we utilize LLMs to enhance sparse retrieval by query rewriting and query expansion. In query rewriting, the original query is refined by creating several new queries. For query expansion, LLMs are employed to generate extra terms, thereby enriching the original query. We conduct experiments on a range of well-known information retrieval datasets, including MSMARCO-passage, TREC2019, TREC2020, Natural Questions, SCIFACT. The experiments show that LLMs can be beneficial for sparse methods since the added information provided by the LLMs can help diminish the discrepancy between the term frequencies of the important terms in a query and the relevant document. In certain domains, we demonstrate that the effectiveness of LLMs is constrained, indicating that they may not consistently perform optimally, which will be explored in future research. | Michael Antonios Kruse Ayoub, Zhan Su, Qiuchi Li |  |
| 189 |  |  [Bi-CAT: Improving Robustness of LLM-based Text Rankers to Conditional Distribution Shifts](https://doi.org/10.1145/3589335.3651947) |  | 0 | Retrieval and ranking lie at the heart of several applications like search, question-answering, and recommendations. The use of Large language models (LLMs) such as BERT in these applications have shown promising results in recent times. Recent works on text-based retrievers and rankers show promising results by using bi-encoders (BE) architecture with BERT like LLMs for retrieval and a cross-attention transformer (CAT) architecture BERT or other LLMs for ranking the results retrieved. Although the use of CAT architecture for re-ranking improves ranking metrics, their robustness to data shifts is not guaranteed. In this work we analyze the robustness of CAT-based rankers. Specifically, we show that CAT rankers are sensitive to item distribution shifts conditioned on a query, we refer to this as conditional item distribution shift (CIDS). CIDS naturally occurs in large online search systems as the retrievers keep evolving, making it challenging to consistently train and evaluate rankers with the same item distribution. In this paper, we formally define CIDS and show that while CAT rankers are sensitive to this, BE models are far more robust to CIDS. We propose a simple yet effective approach referred to as BI-CAT which augments BE model outputs with CAT rankers, to significantly improve the robustness of CAT rankers without any drop in in-distribution performance. We conducted a series of experiments on two publicly available ranking datasets and one dataset from a large e-commerce store. Our results on dataset with CIDS demonstrate that the BI-CAT model significantly improves the robustness of CAT rankers by roughly 100-1000bps in F1 without any reduction in in-distribution model performance. | Sriram Srinivasan, Stephen Sheng, Rishabh Deshmukh, Chen Luo, Yesh Dattatreya, Subhajit Sanyal, S. V. N. Vishwanathan |  |
| 190 |  |  [Deep Learning for Hate Speech Detection: A Personality-based Approach](https://doi.org/10.1145/3589335.3652502) |  | 0 | A crucial element in the combat against hate speech is the development of efficient algorithms for automatically detecting hate speech. Previous research, however, has primarily neglected important insights from the field of psychology literature, particularly the relationship between personality and hate, resulting in suboptimal performance in hate speech detection. To this end, we propose a novel framework for detecting hate speech focusing on people's personality factors reflected in their writing. Our framework has two components: (i) a knowledge distillation model for fully automating the process of personality inference from text and (ii) a personality-based deep learning model for hate speech detection. Our approach is unique in that it incorporates low-level personality factors, which have been largely neglected in prior literature, into automated hate speech detection and proposes novel deep learning components for fully exploiting the intricate relationship between personality and hate (i.e., intermediate personality factors). The evaluation shows that our model significantly outperforms state-of-the-art baselines. Our study paves the way for future research by incorporating personality aspects into the design of automated hate speech detection. In addition, it offers substantial assistance to online social platforms and governmental authorities facing challenges in effectively moderating hate speech. | Kyuhan Lee, Sudha Ram |  |
| 191 |  |  [Requirements and Challenges for Query Execution across Decentralized Environments](https://doi.org/10.1145/3589335.3652523) |  | 0 | Due to the economic and societal problems being caused by the Web's growing centralization, there is an increasing interest in de-centralizing data on the Web. This decentralization does however cause a number of technical challenges. If we want to give users in decentralized environments the same level of user experience as they are used to with centralized applications, we need solutions to these challenges. We discuss how query engines can act as layer between applications on the one hand, and decentralized environments on the other hand, Query engines therefore act as an abstraction layer that hides the complexities of decentralized data management for application developers. In this article, we outline the requirements for query engines over decentralized environments. Furthermore, we show how existing approaches meet these requirements, and which challenges remain. As such, this article offers a high-level overview of a roadmap in the query and decentralization research domains. | Ruben Taelman |  |
| 192 |  |  [Diffusion Recommendation with Implicit Sequence Influence](https://doi.org/10.1145/3589335.3651951) |  | 0 | Sequence recommendation tasks often have performance bottlenecks, mainly reflected in the following two aspects: previous research relied on a single item embedding distribution, resulting in a decrease in overall modeling ability. In addition, the implicit dynamic preferences reflected in user interaction sequences are not distinguished, and the feature representation ability is insufficient. To address these issues, we propose a novel model called Diffusion Recommendation with Implicit Sequence Influence (DiffRIS). Specifically, we establish an implicit feature extraction module, which includes multi-scale CNN and residual LSTM networks that learn local and global features of sequence information, respectively, to explore the length dependence of data features. Subsequently, we use the output of the module as a conditional input for the diffusion model, guiding the denoising process based on historical interactions. Through experiments on two open-source datasets, we find that implicit features of sequences have a positive impact on the diffusion process. The proposed DiffRIS framework performs well compared to multiple baseline models, effectively improving the accuracy of sequential recommendation models. We believe that the proposed DiffRIS can provide some research ideas for diffusion sequence recommendation. | Yong Niu, Xing Xing, Zhichun Jia, Ruidi Liu, Mindong Xin, Jianfu Cui |  |
| 193 |  |  [Multimodal Conditioned Diffusion Model for Recommendation](https://doi.org/10.1145/3589335.3651956) |  | 0 | Multimodal recommendation aims at to modeling the feature distributions of items by using their multi-modal information. Prior efforts typically focus on the denoising of the user-item graph with a degree-sensitive strategy, which may not well-handle the users' consistent preference across modalities. More importantly, it has been observed that existing methods may learn ill-posed item embeddings due to their focus on a specific auxiliary optimization task for multimodal representations rather than explicitly modeling them. This paper therefore presents a solution that takes the advantages of the explicit uncertainty injection ability of Diffusion Model (DM) for the modeling and fusion of multi-modal information. Specifically, we propose a novel Multimodal Conditioned Diffusion Model for Recommendation (MCDRec), which tailors DM with two technical modules to model the high-order multimodal knowledge. The first module is multimodal-conditioned representation diffusion (MRD), which integrates pre-extracted multimodal knowledge into the item representation modeling via a tailored DM. This smoothly bridges the insurmountable gap between the multi-modal content features and the collaborative signals. Secondly, with the diffusion-guided graph denoising (DGD) module, MCDRec may effectively denoise the user-item graph by filtering the occasional interactions in user historical behaviors. This is achieved with the power of DM in aligning the users' collaborative preferences with their shared items' content information. Extensive experiments compared to several SOTA baselines on two real-word datasets demonstrate the effectiveness of MCDRec. The specific visualization also reveals the potential of MRD to precisely handling the high-order representation correlations among the user embeddings and the multi-modal heterogeneous representations of items. | Haokai Ma, Yimeng Yang, Lei Meng, Ruobing Xie, Xiangxu Meng |  |
| 194 |  |  [Universal Knowledge Graph Embeddings](https://doi.org/10.1145/3589335.3651978) |  | 0 | A variety of knowledge graph embedding approaches have been developed. Most of them obtain embeddings by learning the structure of the knowledge graph within a link prediction setting. As a result, the embeddings reflect only the semantics of a single knowledge graph, and embeddings for different knowledge graphs are not aligned, e.g., they cannot be used to find similar entities across knowledge graphs via nearest neighbor search. However, knowledge graph embedding applications such as entity disambiguation require a more global representation, i.e., a representation that is valid across multiple sources. We propose to learn universal knowledge graph embeddings from large-scale interlinked knowledge sources. To this end, we fuse large knowledge graphs based on the owl:sameAs relation such that every entity is represented by a unique identity. We instantiate our idea by computing universal embeddings based on DBpedia and Wikidata yielding embeddings for about 180 million entities, 15 thousand relations, and 1.2 billion triples. Moreover, we develop a convenient API to provide embeddings as a service. Experiments on link prediction show that universal knowledge graph embeddings encode better semantics compared to embeddings computed on a single knowledge graph. For reproducibility purposes, we provide our source code and datasets open access at https://github.com/dice-group/Universal_Embeddings | N'Dah Jean Kouagou, Caglar Demir, Hamada M. Zahera, Adrian Wilke, Stefan Heindorf, Jiayi Li, AxelCyrille Ngonga Ngomo |  |
| 195 |  |  [Towards Graph Foundation Models for Personalization](https://doi.org/10.1145/3589335.3651980) |  | 0 | In the realm of personalization, integrating diverse information sources such as consumption signals and content-based representations is becoming increasingly critical to build state-of-the-art solutions. In this regard, two of the biggest trends in research around this subject are Graph Neural Networks (GNNs) and Foundation Models (FMs). While GNNs emerged as a popular solution in industry for powering personalization at scale, FMs have only recently caught attention for their promising performance in personalization tasks like ranking and retrieval. In this paper, we present a graph-based foundation modeling approach tailored to personalization. Central to this approach is a Heterogeneous GNN (HGNN) designed to capture multi-hop content and consumption relationships across a range of recommendable item types. To ensure the generality required from a Foundation Model, we employ a Large Language Model (LLM) text-based featurization of nodes that accommodates all item types, and construct the graph using co-interaction signals, which inherently transcend content specificity. To facilitate practical generalization, we further couple the HGNN with an adaptation mechanism based on a two-tower (2T) architecture, which also operates agnostically to content type. This multi-stage approach ensures high scalability; while the HGNN produces general purpose embeddings, the 2T component models in a continuous space the sheer size of user-item interaction data. Our comprehensive approach has been rigorously tested and proven effective in delivering recommendations across a diverse array of products within a real-world, industrial audio streaming platform. | Andreas Damianou, Francesco Fabbri, Paul Gigioli, Marco De Nadai, Alice Wang, Enrico Palumbo, Mounia Lalmas |  |
| 196 |  |  [AI for Materials Innovation: Self-Improving Photosensitizer Discovery System via Bayesian Search with First-Principles Simulation](https://doi.org/10.1145/3589334.3649115) |  | 0 | Artificial intelligence (AI) based self-learning or self-improving material discovery systems will enable next-generation material discovery. Herein, we demonstrate how to combine accurate prediction of material performance via first-principles calculation and Bayesian optimization-based active learning to realize a self-improving discovery system for high-performance photosensitizers (PSs). Through self-improving cycles, such a system can improve the model prediction accuracy (best mean absolute error of 0.090 eV for singlet--triplet spitting) and high-performance PS search ability, realizing efficient discovery of PSs. From a molecular space with more than 7 million molecules, 5357 potential high-performance PSs were discovered. Four PSs were further synthesized to show performance comparable with or superior to commercial ones. This work highlights the potential of active learning in first principle-based materials design, and the discovered structures could boost the development of photosensitization-related applications, which is one of the typical examples of how AI can be used to accelerate materials innovation and facilitate science development in general. | Bin Liu |  |
| 197 |  |  [Tight Competitive and Variance Analyses of Matching Policies in Gig Platforms](https://doi.org/10.1145/3589334.3645335) |  | 0 | In this paper, we propose an online-matching-based model to tackle the two fundamental issues, matching and pricing, existing in a wide range of real-world gig platforms, including ride-hailing (matching riders and drivers), crowdsourcing markets (pairing workers and tasks), and online recommendations (offering items to customers). Our model assumes the arriving distributions of dynamic agents (e.g., riders, workers, and buyers) are accessible in advance, and they can change over time, which is referred to as Known Heterogeneous Distributions (KHD). In this paper, we initiate variance analysis for online matching algorithms under KHD. Unlike the popular competitive-ratio (CR) metric, the variance of online algorithms' performance is rarely studied due to inherent technical challenges, though it is well linked to robustness. We focus on two natural parameterized sampling policies, denoted by 𝖠𝖳𝖳(γ) and 𝖲𝖠𝖬𝖯(γ), which appear as foundational bedrock in online algorithm design. We offer rigorous competitive ratio (CR) and variance analyses for both policies. Specifically, we show that 𝖠𝖳𝖳(γ) with γ∈ [0,1/2] achieves a CR of γ and a variance of γ· (1-γ) · B on the total number of matches with B being the total matching capacity. In contrast, 𝖲𝖠𝖬𝖯(γ) with γ∈ [0,1] accomplishes a CR of γ (1-γ) and a variance of γ̅ (1-γ̅)· B with γ̅=min(γ,1/2). All CR and variance analyses are tight and unconditional of any benchmark. As a byproduct, we prove that 𝖠𝖳𝖳(γ=1/2) achieves an optimal CR of 1/2. | Pan Xu |  |
| 198 |  |  [Ad vs Organic: Revisiting Incentive Compatible Mechanism Design in E-commerce Platforms](https://doi.org/10.1145/3589334.3645638) |  | 0 | On typical e-commerce platforms, a product can be displayed to users in two possible forms, as an ad item or an organic item. Usually, ad and organic items are separately selected by the advertising system and recommendation system, and then combined by a content merging mechanism. Although the design of the content merging mechanism has been extensively studied, little attention has been given to a crucial situation where there is an overlap between candidate ad and organic items. Despite its common occurrence, this situation is not correctly handled by almost all existing works, potentially leading to incentive problems for advertisers and the violation of economic constraints. To address these issues, we revisit the design of the content merging mechanism. We introduce a necessary property called form stability, and provide simplification results of the mechanism design problem. Furthermore, we design two simple mechanisms strictly ensuring desired economic properties including incentive compatibility, and demonstrate their guaranteed performance through competitive ratio analysis under certain conditions. | Ningyuan Li, Yunxuan Ma, Yang Zhao, Qian Wang, Zhilin Zhang, Chuan Yu, Jian Xu, Bo Zheng, Xiaotie Deng |  |
| 199 |  |  [Towards Expansive and Adaptive Hard Negative Mining: Graph Contrastive Learning via Subspace Preserving](https://doi.org/10.1145/3589334.3645327) |  | 0 | Graph Neural Networks (GNNs) have emerged as the predominant approach for analyzing graph data on the web and beyond. Contrastive learning (CL), a self-supervised paradigm, not only mitigates reliance on annotations but also has potential in performance. The hard negative sampling strategy that benefits CL in other domains proves ineffective in the context of Graph Contrastive Learning (GCL) due to the message passing mechanism. Embracing the subspace hypothesis in clustering, we propose a method towards expansive and adaptive hard negative mining, referred to as G raph contR astive leA rning via subsP ace prE serving (GRAPE ). Beyond homophily, we argue that false negatives are prevalent over an expansive range and exploring them confers benefits upon GCL. Diverging from existing neighbor-based methods, our method seeks to mine long-range hard negatives throughout subspace, where message passing is conceived as interactions between subspaces. %Empirical investigations back up this strategy. Additionally, our method adaptively scales the hard negatives set through subspace preservation during training. In practice, we develop two schemes to enhance GCL that are pluggable into existing GCL frameworks. The underlying mechanisms are analyzed and the connections to related methods are investigated. Comprehensive experiments demonstrate that our method outperforms across diverse graph datasets and remains competitive across varied application scenarios\footnoteOur code is available at https://github.com/zz-haooo/WWW24-GRAPE. . | Zhezheng Hao, Haonan Xin, Long Wei, Liaoyuan Tang, Rong Wang, Feiping Nie |  |
| 200 |  |  [Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction](https://doi.org/10.1145/3589334.3645372) |  | 0 | Learning positional information of nodes in a graph is important for link prediction tasks. We propose a representation of positional information using representative nodes called landmarks. A small number of nodes with high degree centrality are selected as landmarks, which serve as reference points for the nodes' positions. We justify this selection strategy for well-known random graph models and derive closed-form bounds on the average path lengths involving landmarks. In a model for power-law graphs, we prove that landmarks provide asymptotically exact information on inter-node distances. We apply theoretical insights to practical networks and propose Hierarchical Position embedding with Landmarks and Clustering (HPLC). HPLC combines landmark selection and graph clustering, where the graph is partitioned into densely connected clusters in which nodes with the highest degree are selected as landmarks. HPLC leverages the positional information of nodes based on landmarks at various levels of hierarchy such as nodes' distances to landmarks, inter-landmark distances and hierarchical grouping of clusters. Experiments show that HPLC achieves state-of-the-art performances of link prediction on various datasets in terms of HIT@K, MRR, and AUC. The code is available at <https://github.com/kmswin1/HPLC>. | Minsang Kim, Seung Baek |  |
| 201 |  |  [Collaborative Metapath Enhanced Corporate Default Risk Assessment on Heterogeneous Graph](https://doi.org/10.1145/3589334.3645402) |  | 0 | Default risk assessment for small companies is a tough problem in financial services. Recent efforts utilize advanced Heterogeneous Graph Neural Networks (HGNNs) with metapaths to exploit interactive features in corporate activities for risk analysis. However, few works are proposed for commercial banks. Given a real financial graph, how to detect corporate default risks? We identify two challenges for the task. (1) Massive noisy connections hinder HGNNs to achieve strong results. (2) Multiple semantic connections greatly increase transitive default risk, while existing aggregation schemes do not leverage such connection patterns. In this work, we propose a novel Heterogeneous Graph Co-Attention Network for corporate default risk assessment. Our model takes advantage of collaborative metapaths to distill risky features by a co-attentive aggregation mechanism. First, the local attention score models the importance of neighbors under each metapath by holistic metapath context. Second, the global attention score fuse local attention scores to filter valuable/noisy signals. Then, pairwise importance learning aims to enhance attention scores of multi-metapath neighbors for risky feature distillation. Extensive experiments on large-scale banking datasets demonstrate the effectiveness of our method. | Zheng Zhang, Yingsheng Ji, Jiachen Shen, Yushu Chen, Xi Zhang, Guangwen Yang |  |
| 202 |  |  [Graph Contrastive Learning with Kernel Dependence Maximization for Social Recommendation](https://doi.org/10.1145/3589334.3645412) |  | 0 | Contrastive learning (CL) has recently catalyzed a productive avenue of research for recommendation. The efficacy of most CL methods for recommendation may hinge on their capacity to learn representation uniformity by mapping the data onto a hypersphere. Nonetheless, applying contrastive learning to downstream recommendation tasks remains challenging, as existing CL methods encounter difficulties in capturing the nonlinear dependence of representations in high-dimensional space and struggle to learn hierarchical social dependency among users-essential points for modeling user preferences. Moreover, the subtle distinctions between the augmented representations render CL methods sensitive to noise perturbations. Inspired by the Hilbert-Schmidt independence criterion (HSIC), we propose a graph Contrastive Learning model with Kernel Dependence Maximization CL-KDM for social recommendation to address these challenges. Specifically, to explicitly learn the kernel dependence of representations and improve the robustness and generalization of recommendation, we maximize the kernel dependence of augmented representations in kernel Hilbert space by introducing HSIC into the graph contrastive learning. Additionally, to simultaneously extract the hierarchical social dependency across users while preserving underlying structures, we design a hierarchical mutual information maximization module for generating augmented user representations, which are injected into the message passing of a graph neural network to enhance recommendation. Extensive experiments are conducted on three social recommendation datasets, and the results indicate that CL-KDM outperforms various baseline recommendation methods. | Xuelian Ni, Fei Xiong, Yu Zheng, Liang Wang |  |
| 203 |  |  [MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs](https://doi.org/10.1145/3589334.3645423) |  | 0 | Graphs can inherently model interconnected objects on the Web, thereby facilitating a series of Web applications, such as web analyzing and content recommendation. Recently, Graph Neural Networks (GNNs) have emerged as a mainstream technique for graph representation learning. However, their efficacy within an end-to-end supervised framework is significantly tied to the availabilityof task-specific labels. To mitigate labeling costs and enhance robustness in few-shot settings, pre-training on self-supervised tasks has emerged as a promising method, while prompting has been proposed to further narrow the objective gap between pretext and downstream tasks. Although there has been some initial exploration of prompt-based learning on graphs, they primarily leverage a single pretext task, resulting in a limited subset of general knowledge that could be learned from the pre-training data. Hence, in this paper, we propose MultiGPrompt, a novel multi-task pre-training and prompting framework to exploit multiple pretext tasks for more comprehensive pre-trained knowledge. First, in pre-training, we design a set of pretext tokens to synergize multiple pretext tasks. Second, we propose a dual-prompt mechanism consisting of composed and open prompts to leverage task-specific and global pre-training knowledge, to guide downstream tasks in few-shot settings. Finally, we conduct extensive experiments on six public datasets to evaluate and analyze MultiGPrompt. | Xingtong Yu, Chang Zhou, Yuan Fang, Xinming Zhang |  |
| 204 |  |  [SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding](https://doi.org/10.1145/3589334.3645441) |  | 0 | Knowledge graphs (KGs) have been increasingly employed for link prediction and recommendation using real-world datasets. However, the majority of current methods rely on static data, neglecting the dynamic nature and the hidden spatio-temporal attributes of real-world scenarios. This often results in suboptimal predictions and recommendations. Although there are effective spatio-temporal inference methods, they face challenges such as scalability with large datasets and inadequate semantic understanding, which impede their performance. To address these limitations, this paper introduces a novel framework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing and exploring spatio-temporal KGs. To integrate spatial and temporal data into KGs, our framework exploited through a new 3-step embedding method. Output embeddings can be used for future temporal sequence prediction and spatial information recommendation, providing valuable insights for various applications such as retail sales forecasting and traffic volume prediction. Our framework offers a simple but comprehensive way to understand the underlying patterns and trends in dynamic KG, thereby enhancing the accuracy of predictions and the relevance of recommendations. This work paves the way for more effective utilization of spatio-temporal data in KGs, with potential impacts across a wide range of sectors. | Ruiyi Yang, Flora D. Salim, Hao Xue |  |
| 205 |  |  [Spectral Heterogeneous Graph Convolutions via Positive Noncommutative Polynomials](https://doi.org/10.1145/3589334.3645515) |  | 0 | Heterogeneous Graph Neural Networks (HGNNs) have gained significant popularity in various heterogeneous graph learning tasks. However, most existing HGNNs rely on spatial domain-based methods to aggregate information, i.e., manually selected meta-paths or some heuristic modules, lacking theoretical guarantees. Furthermore, these methods cannot learn arbitrary valid heterogeneous graph filters within the spectral domain, which have limited expressiveness. To tackle these issues, we present a positive spectral heterogeneous graph convolution via positive noncommutative polynomials. Then, using this convolution, we propose PSHGCN, a novel Positive Spectral Heterogeneous Graph Convolutional Network. PSHGCN offers a simple yet effective method for learning valid heterogeneous graph filters. Moreover, we demonstrate the rationale of PSHGCN in the graph optimization framework. We conducted an extensive experimental study to show that PSHGCN can learn diverse heterogeneous graph filters and outperform all baselines on open benchmarks. Notably, PSHGCN exhibits remarkable scalability, efficiently handling large real-world graphs comprising millions of nodes and edges. Our codes are available at https://github.com/ivam-he/PSHGCN. | Mingguo He, Zhewei Wei, Shikun Feng, Zhengjie Huang, Weibin Li, Yu Sun, Dianhai Yu |  |
| 206 |  |  [Densest Subhypergraph: Negative Supermodular Functions and Strongly Localized Methods](https://doi.org/10.1145/3589334.3645624) |  | 0 | Dense subgraph discovery is a fundamental primitive in graph and hypergraph analysis which among other applications has been used for real-time story detection on social media and improving access to data stores of social networking systems. We present several contributions for localized densest subgraph discovery, which seeks dense subgraphs located nearby given seed sets of nodes. We first introduce a generalization of a recent anchored densest subgraph problem, extending this previous objective to hypergraphs and also adding a tunable locality parameter that controls the extent to which the output set overlaps with seed nodes. Our primary technical contribution is to prove when it is possible to obtain a strongly-local algorithm for solving this problem, meaning that the runtime depends only on the size of the input set. We provide a strongly-local algorithm that applies whenever the locality parameter is not too small, and show via counterexample why strongly-local algorithms are impossible below a certain threshold. Along the way to proving our results for localized densest subgraph discovery, we also provide several advances in solving global dense subgraph discovery objectives. This includes the first strongly polynomial time algorithm for the densest supermodular set problem and a flow-based exact algorithm for a heavy and dense subgraph discovery problem in graphs with arbitrary node weights. We demonstrate our algorithms on several web-based data analysis tasks. | Yufan Huang, David F. Gleich, Nate Veldt |  |
| 207 |  |  [Towards Deeper Understanding of PPR-based Embedding Approaches: A Topological Perspective](https://doi.org/10.1145/3589334.3645663) |  | 0 | Node embedding learns low-dimensional vectors for nodes in the graph. Recent state-of-the-art embedding approaches take Personalized PageRank (PPR) as the proximity measure and factorize the PPR matrix or its adaptation to generate embeddings. However, little previous work analyzes what information is encoded by these approaches, and how the information correlates with their superb performance in downstream tasks. In this work, we first show that state-of-the-art embedding approaches that factorize a PPR-related matrix can be unified into a closed-form framework. Then, we study whether the embeddings generated by this strategy can be inverted to better recover the graph topology information than random-walk based embeddings. To achieve this, we propose two methods for recovering graph topology via PPR-based embeddings, including the analytical method and the optimization method. Extensive experimental results demonstrate that the embeddings generated by factorizing a PPR-related matrix maintain more topological information, such as common edges and community structures, than that generated by random walks, paving a new way to systematically comprehend why PPR-based node embedding approaches outperform random walk-based alternatives in various downstream tasks. To the best of our knowledge, this is the first work that focuses on the interpretability of PPR-based node embedding approaches. | Xingyi Zhang, Zixuan Weng, Sibo Wang |  |
| 208 |  |  [Globally Interpretable Graph Learning via Distribution Matching](https://doi.org/10.1145/3589334.3645674) |  | 0 | Graph neural networks (GNNs) have emerged as a powerful model to capture critical graph patterns. Instead of treating them as black boxes in an end-to-end fashion, attempts are arising to explain the model behavior. Existing works mainly focus on local interpretation to reveal the discriminative pattern for each individual instance, which however cannot directly reflect the high-level model behavior across instances. To gain global insights, we aim to answer an important question that is not yet well studied: how to provide a global interpretation for the graph learning procedure? We formulate this problem as globally interpretable graph learning, which targets on distilling high-level and human-intelligible patterns that dominate the learning procedure, such that training on this pattern can recover a similar model. As a start, we propose a novel model fidelity metric, tailored for evaluating the fidelity of the resulting model trained on interpretations. Our preliminary analysis shows that interpretative patterns generated by existing global methods fail to recover the model training procedure. Thus, we further propose our solution, Graph Distribution Matching (GDM), which synthesizes interpretive graphs by matching the distribution of the original and interpretive graphs in the GNN's feature space as its training proceeds, thus capturing the most informative patterns the model learns during training. Extensive experiments on graph classification datasets demonstrate multiple advantages of the proposed method, including high model fidelity, predictive accuracy and time efficiency, as well as the ability to reveal class-relevant structure. | Yi Nian, Yurui Chang, Wei Jin, Lu Lin |  |
| 209 |  |  [Retention Depolarization in Recommender System](https://doi.org/10.1145/3589334.3645485) |  | 0 | Repeated risk minimization is a popular choice in real-world recommender systems driving their recommendation algorithms to adapt to user preferences and trends. However, numerous studies have shown that it exacerbates retention disparities among user groups, resulting in polarization within the user population. Given the primary objective of improving long-term user engagement in most industrial recommender systems and the significant commercial benefits from a diverse user population, enforcing retention fairness across user population is therefore crucial. Nonetheless, this goal is highly challenging due to the unknown dynamics of user retention (e.g., when a user would abandon the system) and the simultaneous aim to maximize the experience of every user. In this paper, we propose ReFair, the first computational framework that continuously improves recommendation algorithms while ensuring long-term retention fairness in the entire user population. ReFair alternates between environment learning (i.e., estimate the user retention dynamics) and fairness constrained policy improvement with respect to the estimated environment, while effectively handling uncertainties in the estimation. Our solution provides strong theoretical guarantees for long-term recommendation performance and retention fairness violation. Empirical experiments on two real-world recommendation datasets also demonstrate its effectiveness in realizing these two goals. | Xiaoying Zhang, Hongning Wang, Yang Liu |  |
| 210 |  |  [Uncovering the Hidden Data Costs of Mobile YouTube Video Ads](https://doi.org/10.1145/3589334.3645496) |  | 0 | Popular video streaming platforms attract a large number of global marketers who use the platform to advertise their services. While benefiting platforms and advertisers, users are burdened with the costs of advertisements. Users not only pay for these ads with their invested time and personal information, but also through a substantial amount of data translating into direct financial cost. The financial cost becomes even more pronounced in developing countries, where the cost of mobile broadband can be disproportionately high relative to average income levels. In this paper, we perform the first independent and empirical analysis of the data costs of mobile video ads on YouTube, the most popular video platform, from the users' perspective. To do so, we collect and analyze a data set of over 46,000 YouTube video ads. We find that streaming video ads have multiplelatent andavoidable sources of data wastage, which can lead to excessive data consumption by users. We also conduct an affordability analysis to quantify the overall impact of data wastage and reveal the specific data costs per country associated with these losses. Our findings highlight the need for video platform providers, such as YouTube, to minimize data wastage linked to ads, to make their services more affordable and inclusive. | Emaan Atique, Saad Sher Alam, Harris Ahmad, Ihsan Ayyub Qazi, Zafar Ayyub Qazi |  |
| 211 |  |  [Perceptions in Pixels: Analyzing Perceived Gender and Skin Tone in Real-world Image Search Results](https://doi.org/10.1145/3589334.3645666) |  | 0 | The results returned by image search engines have the power to shape peoples' perceptions about social groups. Existing work on image search engines leverages hand-selected queries for occupations like "doctor" and "engineer" to quantify racial and gender bias in search results. We complement this work by analyzing peoples' real-world image search queries and measuring the distributions of perceived gender, skin tone, and age in their results. We collect 54,070 unique image search queries and analyze 1,481 open-ended people queries (i.e. not queries for named entities) from a representative sample of 643 US residents. For each query, we analyze the top 15 results returned on both Google and Bing Images. Analysis of real-world image search queries produces multiple insights. First, less than 5% of unique queries are open-ended people queries. Second, fashion queries are, by far, the most common category of open-ended people queries, accounting for over 30% of the total. Third, the modal skin tone on the Monk Skin Tone scale is two out of ten (the second lightest) for images from both search engines. Finally, we observe a bias against older people: eleven of our top fifteen query categories have a median age that is lower than the median age in the US. | Jeffrey L. Gleason, Avijit Ghosh, Ronald E. Robertson, Christo Wilson |  |
| 212 |  |  [Reconciling the Accuracy-Diversity Trade-off in Recommendations](https://doi.org/10.1145/3589334.3645625) |  | 0 | In recommendation settings, there is an apparent trade-off between the goals of accuracy (to recommend items a user is most likely to want) and diversity (to recommend items representing a range of categories). As such, real-world recommender systems often explicitly incorporate diversity separately from accuracy. This approach, however, leaves a basic question unanswered: Why is there a trade-off in the first place? We show how the trade-off can be explained via a user's consumption constraints -- users typically only consume a few of the items they are recommended. In a stylized model we introduce, objectives that account for this constraint induce diverse recommendations, while objectives that do not account for this constraint induce homogeneous recommendations. This suggests that accuracy and diversity appear misaligned because standard accuracy metrics do not consider consumption constraints. Our model yields precise and interpretable characterizations of diversity in different settings, giving practical insights into the design of diverse recommendations. | Kenny Peng, Manish Raghavan, Emma Pierson, Jon M. Kleinberg, Nikhil Garg |  |
| 213 |  |  [MileCut: A Multi-view Truncation Framework for Legal Case Retrieval](https://doi.org/10.1145/3589334.3645349) |  | 0 | In the search process, it is essential to strike a balance between effectiveness and efficiency to improve search experience. Thus, ranking list truncation has become increasingly crucial. Especially in the legal domain, irrelevant cases can severely increase search costs and even compromise the pursuit of legal justice. However, there are truncation challenges that mainly arise from the distinctive structure of legal case documents, where the elements such as fact, reasoning, and judgement in a case serve as different but multi-view texts, which could result in a bad performance if the multi-view texts cannot be well-modeled. Existing approaches are limited due to their inability to handle multi-view elements information and their neglect of semantic interconnections between cases in the ranking list. In this paper, we propose a multi-view truncation framework for legal case retrieval, named MileCut. MileCut employs a case elements extraction module to fully exploit the multi-view information of cases in the ranking list. Then, MileCut applies a multi-view truncation module to select the most informative view and make a more comprehensive cut-off decision, similar to how legal experts look over retrieval results. As a practical evaluation, MileCut is assessed across three datasets, including criminal and civil case retrieval scenarios, and the results show that MileCut outperforms other methods on F1, DCG, and OIE metrics. | Fuda Ye, Shuangyin Li |  |
| 214 |  |  [Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks](https://doi.org/10.1145/3589334.3645363) |  | 0 | Making the content generated by Large Language Model (LLM), accurate, credible and traceable is crucial, especially in complex knowledge-intensive tasks that require multi-step reasoning and each step needs knowledge to solve. Retrieval-augmented generation is good potential to solve this problem. However, where and how to introduce Information Retrieval (IR) to LLM is a big challenge. Previous work has the problems that wrong knowledge retrieved by IR misleads the LLM and interaction between IR and LLM breaks the reasoning chain of LLM. This paper proposes a novel framework named Search-in-the-Chain (SearChain) for the interaction between LLM and IR to solve the challenges. First, LLM generates the reasoning chain named Chain-of-Query (CoQ) where each node consists of an IR-oriented query-answer pair. Second, IR verifies the answer of each node of CoQ. It corrects the answer that is not consistent with the retrieved information when IR gives high confidence, which improves the credibility. Third, LLM can indicate its missing knowledge in CoQ and rely on IR to provide this knowledge to LLM. These operations improve the accuracy in terms of reasoning and knowledge. Finally, SearChain generates the reasoning process and marks references to supporting documents for each reasoning step, which improves traceability. Interaction with IR in SearChain forms a novel reasoning path based on a tree, which enables LLM to dynamically modify the direction of reasoning. Experiments show that SearChain outperforms state-of-the-art baselines on complex knowledge-intensive tasks including multi-hop Q&A, slot filling, fact checking, and long-form Q&A. | Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, TatSeng Chua |  |
| 215 |  |  [Scalable and Effective Generative Information Retrieval](https://doi.org/10.1145/3589334.3645477) |  | 0 | Recent research has shown that transformer networks can be used as differentiable search indexes by representing each document as a sequence of document ID tokens. These generative retrieval models cast the retrieval problem to a document ID generation problem for each query. Despite their elegant design, existing generative retrieval models only perform well on artificially-constructed and small-scale collections. This paper represents an important milestone in generative retrieval research by showing that generative retrieval models can be trained to perform effectively on large-scale standard retrieval benchmarks. In more detail, we propose RIPOR- an optimization framework for generative retrieval that is designed based on two often-overlooked fundamental design considerations. First, RIPOR introduces a novel prefix-oriented ranking optimization algorithm for accurate estimation of relevance score during sequential document ID generation. Second, RIPOR constructs document IDs based on the relevance associations between queries and documents. Evaluation on MSMARCO and TREC Deep Learning Track reveals that RIPOR surpasses state-of-the-art generative retrieval models by a large margin (e.g., 30.5% MRR improvements on MS MARCO Dev Set). | Hansi Zeng, Chen Luo, Bowen Jin, Sheikh Muhammad Sarwar, Tianxin Wei, Hamed Zamani |  |
| 216 |  |  [Metacognitive Retrieval-Augmented Large Language Models](https://doi.org/10.1145/3589334.3645481) |  | 0 | Retrieval-augmented generation have become central in natural language processing due to their efficacy in generating factual content. While traditional methods employ single-time retrieval, more recent approaches have shifted towards multi-time retrieval for multi-hop reasoning tasks. However, these strategies are bound by predefined reasoning steps, potentially leading to inaccuracies in response generation. This paper introduces MetaRAG, an approach that combines the retrieval-augmented generation process with metacognition. Drawing from cognitive psychology, metacognition allows an entity to self-reflect and critically evaluate its cognitive processes. By integrating this, MetaRAG enables the model to monitor, evaluate, and plan its response strategies, enhancing its introspective reasoning abilities. Through a three-step metacognitive regulation pipeline, the model can identify inadequacies in initial cognitive responses and fixes them. Empirical evaluations show that MetaRAG significantly outperforms existing methods. | Yujia Zhou, Zheng Liu, Jiajie Jin, JianYun Nie, Zhicheng Dou |  |
| 217 |  |  [Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy](https://doi.org/10.1145/3589334.3645512) |  | 0 | Document retrieval has greatly benefited from the advancements of large-scale pre-trained language models (PLMs). However, their effectiveness is often limited in theme-specific applications for specialized areas or industries, due to unique terminologies, incomplete contexts of user queries, and specialized search intents. To capture the theme-specific information and improve retrieval, we propose to use a corpus topical taxonomy, which outlines the latent topic structure of the corpus while reflecting user-interested aspects. We introduce ToTER (Topical Taxonomy Enhanced Retrieval) framework, which identifies the central topics of queries and documents with the guidance of the taxonomy, and exploits their topical relatedness to supplement missing contexts. As a plug-and-play framework, ToTER can be flexibly employed to enhance various PLM-based retrievers. Through extensive quantitative, ablative, and exploratory experiments on two real-world datasets, we ascertain the benefits of using topical taxonomy for retrieval in theme-specific applications and demonstrate the effectiveness of ToTER. | SeongKu Kang, Shivam Agarwal, Bowen Jin, Dongha Lee, Hwanjo Yu, Jiawei Han |  |
| 218 |  |  [(In)Security of File Uploads in Node.js](https://doi.org/10.1145/3589334.3645342) |  | 0 | File upload is a critical feature incorporated by a myriad of web applications in an effort to enable users to share and manage their files conveniently. It has been used in many useful services such as file-sharing and social media. While file upload is an essential component of web applications, the lack of rigorous checks on the file name, type, and content of the uploaded files can result in security issues, often referred to as Unrestricted File Upload (UFU). In this study, we analyze the (in)security of popular file upload libraries and real-world applications in the Node.js ecosystem. To automate our analysis, we propose and implement NodeSEC- a tool designed to analyze file upload insecurities in Node.js applications and libraries. NodeSEC generates unique payloads and thoroughly evaluates the application's file upload security against 13 distinct UFU-type attacks. Utilizing NodeSEC, we analyze the most popular file upload libraries and real-world applications in the Node.js ecosystem. Our analysis results reveal that some real-world web applications are vulnerable to UFU attacks and disclose serious security bugs in file upload libraries. As of this writing, we received 19 CVEs and two US-CERT cases for the security issues that we reported. Our findings provide strong evidence that dynamic features of Node.js applications introduce security shortcomings and that web developers should be cautious when implementing file upload features in their applications. Finally, combining our responsible disclosure experience and root cause analysis, we identified the main causes of significant security weaknesses in file uploads in Node.js. | Harun Oz, Abbas Acar, Ahmet Aris, Güliz Seray Tuncay, Amin Kharraz, A. Selcuk Uluagac |  |
| 219 |  |  [IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion](https://doi.org/10.1145/3589334.3645361) |  | 0 | Temporal Knowledge Graphs (TKGs) incorporate a temporal dimension, allowing for a precise capture of the evolution of knowledge and reflecting the dynamic nature of the real world. Typically, TKGs contain complex geometric structures, with various geometric structures interwoven. However, existing Temporal Knowledge Graph Completion (TKGC) methods either model TKGs in a single space or neglect the heterogeneity of different curvature spaces, thus constraining their capacity to capture these intricate geometric structures. In this paper, we propose a novel Integrating Multi-curvature shared and specific Embedding (IME) model for TKGC tasks. Concretely, IME models TKGs into multi-curvature spaces, including hyperspherical, hyperbolic, and Euclidean spaces. Subsequently, IME incorporates two key properties, namely space-shared property and space-specific property. The space-shared property facilitates the learning of commonalities across different curvature spaces and alleviates the spatial gap caused by the heterogeneous nature of multi-curvature spaces, while the space-specific property captures characteristic features. Meanwhile, IME proposes an Adjustable Multi-curvature Pooling (AMP) approach to effectively retain important information. Furthermore, IME innovatively designs similarity, difference, and structure loss functions to attain the stated objective. Experimental results clearly demonstrate the superior performance of IME over existing state-of-the-art TKGC models. | Jiapu Wang, Zheng Cui, Boyue Wang, Shirui Pan, Junbin Gao, Baocai Yin, Wen Gao |  |
| 220 |  |  [Poisoning Attack on Federated Knowledge Graph Embedding](https://doi.org/10.1145/3589334.3645422) |  | 0 | Federated Knowledge Graph Embedding (FKGE) is an emerging collaborative learning technique for deriving expressive representations (i.e., embeddings) from client-maintained distributed knowledge graphs (KGs). However, poisoning attacks in FKGE, which lead to biased decisions by downstream applications, remain unexplored. This paper is the first work to systematize the risks of FKGE poisoning attacks, from which we develop a novel framework for poisoning attacks that force the victim client to predict specific false facts. Unlike centralized KGEs, FKGE maintains KGs locally, making direct injection of poisoned data challenging. Instead, attackers must create poisoned data without access to the victim's KG and inject it indirectly through FKGE aggregation. Specifically, to create poisoned data, the attacker first infers the targeted relations in the victim's local KG via a new KG component inference attack. Then, to accurately mislead the victim's embeddings via aggregation, the attacker locally trains a shadow model using the poisoned data and uses an optimized dynamic poisoning scheme to adjust the model and generate progressive poisoned updates. Our experimental results demonstrate the attack's effectiveness, achieving a remarkable success rate on various KGE models (e.g., 100% on TransE with WN18RR) while keeping the original task's performance nearly unchanged. | Enyuan Zhou, Song Guo, Zhixiu Ma, Zicong Hong, Tao Guo, Peiran Dong |  |
| 221 |  |  [ReliK: A Reliability Measure for Knowledge Graph Embeddings](https://doi.org/10.1145/3589334.3645430) |  | 0 | Can we assess a priori how well a knowledge graph embedding will perform on a specific downstream task and in a specific part of the knowledge graph? Knowledge graph embeddings (KGEs) represent entities (e.g., "da Vinci," "Mona Lisa") and relationships (e.g., "painted") of a knowledge graph (KG) as vectors. KGEs are generated by optimizing an embedding score, which assesses whether a triple (e.g., "da Vinci," "painted," "Mona Lisa") exists in the graph. KGEs have been proven effective in a variety of web-related downstream tasks, including, for instance, predicting relationships among entities. However, the problem of anticipating the performance of a given KGE in a certain downstream task and locally to a specific individual triple, has not been tackled so far. In this paper, we fill this gap with ReliK, a Reliability measure for KGEs. ReliK relies solely on KGE embedding scores, is task- and KGE-agnostic, and requires no further KGE training. As such, it is particularly appealing for semantic web applications which call for testing multiple KGE methods on various parts of the KG and on each individual downstream task. Through extensive experiments, we attest that ReliK correlates well with both common downstream tasks, such as tail or relation prediction and triple classification, as well as advanced downstream tasks, such as rule mining and question answering, while preserving locality. | Maximilian K. Egger, Wenyue Ma, Davide Mottin, Panagiotis Karras, Ilaria Bordino, Francesco Gullo, Aris Anagnostopoulos |  |
| 222 |  |  [A Method for Assessing Inference Patterns Captured by Embedding Models in Knowledge Graphs](https://doi.org/10.1145/3589334.3645505) |  | 0 | Various methods embed knowledge graphs with the goal of predicting missing edges. Inference patterns are the logical relationships that occur in a graph. To make proper predictions, models trained by embedding methods must capture inference patterns. There are several theoretical analyses studying pattern-capturing capabilities. Unfortunately, these analyses are challenging and many embedding methods remain unstudied. Also, they do not quantify how accurately a pattern is captured in real-world datasets. Existing empirical studies have studied a small subset of simple inference patterns, and the analysis methods used have varied depending on the models evaluated. In this paper, we present a model-agnostic method to empirically quantify how patterns are captured by trained embedding models. We collect the most plausible predictions to form a new graph, and use it to globally assess pattern-capturing capabilities. For a given pattern, we study positive and negative evidence, i.e., edges that the pattern deems correct and incorrect based on the partial completeness assumption. As far as we know, it is the first time negative evidence is analyzed. Our experiments show that several models effectively capture the positive evidence of inference patterns. However, the performance is poor for negative evidence, which entails that models fail to learn the partial completeness assumption. We also identify new inference patterns not studied before. Surprisingly, models generally achieve better performance in these new patterns that we introduce. | Narayanan Asuri Krishnan, Carlos R. Rivero |  |
| 223 |  |  [Fact Embedding through Diffusion Model for Knowledge Graph Completion](https://doi.org/10.1145/3589334.3645451) |  | 0 | Knowledge graph embedding (KGE) is an efficient and scalable method for knowledge graph completion tasks. Existing KGE models typically map entities and relations into a unified continuous vector space and define a score function to capture the connectivity patterns among the elements (entities and relations) of facts. The score on a fact measures its plausibility in a knowledge graph (KG). However, since the connectivity patterns are very complex in a real knowledge graph, it is difficult to define an explicit and efficient score function to capture them, which also limits their performance. This paper argues that plausible facts in a knowledge graph come from a distribution in the low-dimensional fact space. Inspired by this insight, this paper proposes a novel framework called Fact Embedding through Diffusion Model (FDM) to address the knowledge graph completion task. Instead of defining a score function to measure the plausibility of facts in a knowledge graph, this framework directly learns the distribution of plausible facts from the known knowledge graph and casts the entity prediction task into the conditional fact generation task. Specifically, we concatenate the elements embedding in a fact as a whole and take it as input. Then, we introduce a Conditional Fact Denoiser to learn the reverse denoising diffusion process and generate the target fact embedding from noised data. Extensive experiments demonstrate that FDM significantly outperforms existing state-of-the-art methods in three benchmark datasets. | Xiao Long, Liansheng Zhuang, Aodi Li, Houqiang Li, Shafei Wang |  |
| 224 |  |  [HaSa: Hardness and Structure-Aware Contrastive Knowledge Graph Embedding](https://doi.org/10.1145/3589334.3645564) |  | 0 | We consider a contrastive learning approach to knowledge graph embedding (KGE) via InfoNCE. For KGE, efficient learning relies on augmenting the training data with negative triples. However, most KGE works overlook the bias from generating the negative triples-false negative triples (factual triples missing from the knowledge graph). We argue that the generation of high-quality (i.e., hard) negative triples might lead to an increase in false negative triples. To mitigate the impact of false negative triples during the generation of hard negative triples, we propose the Hardness and Structure-aware (\textbf{HaSa}) contrastive KGE method, which alleviates the effect of false negative triples while generating the hard negative triples. Experiments show that HaSa improves the performance of InfoNCE-based KGE approaches and achieves state-of-the-art results in several metrics for WN18RR datasets and competitive results for FB15k-237 datasets compared to both classic and pre-trained LM-based KGE methods. | Honggen Zhang, June Zhang, Igor Molybog |  |
| 225 |  |  [Bridging the Space Gap: Unifying Geometry Knowledge Graph Embedding with Optimal Transport](https://doi.org/10.1145/3589334.3645565) |  | 0 | Knowledge Graph Embedding (KGE) is a critical field aiming to transform the elements of knowledge graphs (KGs) into continuous spaces, offering great potential for structured data representation. In contemporary KGE research, the utilization of either hyperbolic or Euclidean space for knowledge graph Embedding is a common practice. However, knowledge graphs encompass diverse geometric data structures, including chains and hierarchies, whose hybrid nature exceeds the capacity of a single embedding space to capture effectively. This paper introduces a novel and highly effective approach called Unified Geometry Knowledge Graph Embedding (UniGE) to address the challenge of representing diverse geometric data in KGs. UniGE stands out as a novel KGE method that seamlessly integrates KGE in both Euclidean and hyperbolic geometric spaces. We introduce an embedding alignment method and fusion strategy, which harnesses optimal transport techniques and the Wasserstein barycenter method. Furthermore, we offer a comprehensive theoretical analysis to substantiate the superiority of our approach, as evident from a more robust error bound. To substantiate the strength of UniGE, we conducted comprehensive experiments on three benchmark datasets. The results consistently demonstrate that UniGE outperforms state-of-the-art methods, aligning with the conclusions drawn from our theoretical analysis. | Yuhan Liu, Zelin Cao, Xing Gao, Ji Zhang, Rui Yan |  |
| 226 |  |  [Query Optimization for Ontology-Mediated Query Answering](https://doi.org/10.1145/3589334.3645567) |  | 0 | Ontology-mediated query answering (OMQA) consists in asking database queries on knowledge bases (KBs); a KB is a set of facts called the KB's database, which is described by domain knowledge called the KB's ontology. A widely-investigated OMQA technique is FO-rewriting: every query asked on a KB is reformulated w.r.t. the KB's ontology, so that its answers are computed by the relational evaluation of the query reformulation on the KB's database. Crucially, because FO-rewriting compiles the domain knowledge relevant to queries into their reformulations, query reformulations may be complex and their optimization is the crux of efficiency. We devise a novel optimization framework for a large set of OMQA settings that enjoy FO-rewriting: conjunctive queries, i.e., the core select-project-join queries, asked on KBs expressed using datalog+/-, description logics, existential rules, OWL, or RDFS. We optimize the query reformulations produced by state-of-the-art FO-rewriting algorithms by computing rapidly, with the help of a KB's database summary, simpler (contained) queries with the same answers that can be evaluated faster by RDBMSs. We show on a well-established OMQA benchmark that time performance is significantly improved by our optimization framework in general, up to three orders of magnitude. | Wafaa El Husseini, Cheikh Brahim El Vaigh, François Goasdoué, Hélène Jaudoin |  |
| 227 |  |  [Query2GMM: Learning Representation with Gaussian Mixture Model for Reasoning over Knowledge Graphs](https://doi.org/10.1145/3589334.3645569) |  | 0 | Logical query answering over Knowledge Graphs (KGs) is a fundamental yet complex task. A promising approach to achieve this is to embed queries and entities jointly into the same embedding space. Research along this line suggests that using multi-modal distribution to represent answer entities is more suitable than uni-modal distribution, as a single query may contain multiple disjoint answer subsets due to the compositional nature of multi-hop queries and the varying latent semantics of relations. However, existing methods based on multi-modal distribution roughly represent each subset without capturing its accurate cardinality, or even degenerate into uni-modal distribution learning during the reasoning process due to the lack of an effective similarity measure. To better model queries with diversified answers, we propose Query2GMM for answering logical queries over knowledge graphs. In Query2GMM, we present the GMM embedding to represent each query using a univariate Gaussian Mixture Model (GMM). Each subset of a query is encoded by its cardinality, semantic center and dispersion degree, allowing for precise representation of multiple subsets. Then we design specific neural networks for each operator to handle the inherent complexity that comes with multi-modal distribution while alleviating the cascading errors. Last, we design a new similarity measure to assess the relationships between an entity and a query's multi-answer subsets, enabling effective multi-modal distribution learning for reasoning. Comprehensive experimental results show that Query2GMM outperforms the best competitor by an absolute average of 6.35%. | Yuhan Wu, Yuanyuan Xu, Wenjie Zhang, Xiwei Xu, Ying Zhang |  |
| 228 |  |  [Enhancing Complex Question Answering over Knowledge Graphs through Evidence Pattern Retrieval](https://doi.org/10.1145/3589334.3645563) |  | 0 | Information retrieval (IR) methods for KGQA consist of two stages: subgraph extraction and answer reasoning. We argue current subgraph extraction methods underestimate the importance of structural dependencies among evidence facts. We propose Evidence Pattern Retrieval (EPR) to explicitly model the structural dependencies during subgraph extraction. We implement EPR by indexing the atomic adjacency pattern of resource pairs. Given a question, we perform dense retrieval to obtain atomic patterns formed by resource pairs. We then enumerate their combinations to construct candidate evidence patterns. These evidence patterns are scored using a neural model, and the best one is selected to extract a subgraph for downstream answer reasoning. Experimental results demonstrate that the EPR-based approach has significantly improved the F1 scores of IR-KGQA methods by over 10 points on ComplexWebQuestions and achieves competitive performance on WebQuestionsSP. | Wentao Ding, Jinmao Li, Liangchuan Luo, Yuzhong Qu |  |
| 229 |  |  [Author Name Disambiguation via Paper Association Refinement and Compositional Contrastive Embedding](https://doi.org/10.1145/3589334.3645596) |  | 0 | Author name disambiguation (AND) is an essential task for online academic retrieval systems. Recent models adopt representation learning in the author's name disambiguation. Despite achieving remarkable success, these methods may be limited in two aspects. First, the heuristically constructed paper association graphs used for representation learning contain uncertainties that may cause negative supervision. Second, existing algorithms, such as binary cross-entropy loss, used to train representation learning models may not produce sufficiently high-quality representations for AND. To tackle the above problems, we propose an association refining and compositional contrasting (ARCC) framework for AND tasks. ARCC first adopts an iterative graph structure refinement process to dynamically reduce the uncertainties in paper graphs. Then, a compositional contrastive learning method is proposed to encourage learning more discriminative representations for AND. Empirical studies on two benchmark datasets suggest that ARCC is effective for AND and outperforms the state-of-the-art models. | Dezhi Liu, Richong Zhang, Junfan Chen, Xinyue Chen |  |
| 230 |  |  [Dual Box Embeddings for the Description Logic EL++](https://doi.org/10.1145/3589334.3645648) |  | 0 | OWL ontologies, whose formal semantics are rooted in Description Logic (DL), have been widely used for knowledge representation. Similar to Knowledge Graphs (KGs), ontologies are often incomplete, and maintaining and constructing them has proved challenging. While classical deductive reasoning algorithms use the precise formal semantics of an ontology to predict missing facts, recent years have witnessed growing interest in inductive reasoning techniques that can derive probable facts from an ontology. Similar to KGs, a promising approach is to learn ontology embeddings in a latent vector space, while additionally ensuring they adhere to the semantics of the underlying DL. While a variety of approaches have been proposed, current ontology embedding methods suffer from several shortcomings, especially that they all fail to faithfully model one-to-many, many-to-one, and many-to-many relations and role inclusion axioms. To address this problem and improve ontology completion performance, we propose a novel ontology embedding method named Box^2EL for the DL EL++, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles), and models inter-concept relationships using a bumping mechanism. We theoretically prove the soundness of Box^2EL and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets on the tasks of subsumption prediction, role assertion prediction, and approximating deductive reasoning. | Mathias Jackermeier, Jiaoyan Chen, Ian Horrocks |  |
| 231 |  |  [Jointly Canonicalizing and Linking Open Knowledge Base via Unified Embedding Learning](https://doi.org/10.1145/3589334.3645700) |  | 0 | Recent years have witnessed increasing attention on the semantic knowledge integration between curated knowledge bases (CKBs) and open knowledge bases (OKBs), which is non-trivial due to the intrinsically heterogeneous features involved in CKBs and OKBs. OKB canonicalization and OKB linking are regarded as two vital tasks to achieve the knowledge integration. Although these two tasks are inherently complementary with each other, previous studies just solve them separately or via superficial interaction. To address this issue, we propose CLUE, a novel framework that jointly encodes the OKB and CKB into a unified embedding space, to tackle OKB canonicalization and OKB linking simultaneously and make them benefit each other reciprocally. We design an expectation-maximization (EM) based approach to iteratively refine the unified embedding space via performing seed generation and embedding refinement alternately, by leveraging the deep interaction between OKB canonicalization and OKB linking. Curriculum learning is employed to yield high-quality canonicalization seeds and linking seeds adaptively, according to two elaborately designed metrics (i.e., a margin-based linking metric and an entropy-based cluster metric). A thorough experimental study over two public benchmark data sets demonstrates that our proposed CLUE consistently outperforms state-of-the-art baselines for the task of OKB canonicalization (resp. OKB linking) in terms of average F1 (resp. accuracy). | Wei Shen, Binhan Yang, Yinan Liu |  |
| 232 |  |  [Efficient Exact and Approximate Betweenness Centrality Computation for Temporal Graphs](https://doi.org/10.1145/3589334.3645438) |  | 0 | Betweenness centrality of a vertex in a graph evaluates how often the vertex occurs in the shortest paths. It is a widely used metric of vertex importance in graph analytics. While betweenness centrality on static graphs has been extensively investigated, many real-world graphs are time-varying and modeled as temporal graphs. Examples include social networks and telecommunication networks, where a relationship between two vertices occurs at a specific time. Hence, in this paper, we target efficient methods for temporal betweenness centrality computation. We firstly propose an exact algorithm with the new notion of time instance graph, based on which, we derive a temporal dependency accumulation theory for iterative computation. To reduce the size of the time instance graph and improve the efficiency, we propose an additional optimization, which compresses the time instance graph with equivalent vertices and edges, and extends the dependency theory to the compressed graph. Since it is theoretically complex to compute temporal betweenness centrality, we further devise a probabilistically guaranteed approximate method to handle massive temporal graphs. Extensive experimental results on real-world temporal networks demonstrate the superior performance of the proposed methods. In particular, our exact and approximate methods outperform the state-of-the-art methods by up to two and five orders of magnitude, respectively. | Tianming Zhang, Yunjun Gao, Jie Zhao, Lu Chen, Lu Jin, Zhengyi Yang, Bin Cao, Jing Fan |  |
| 233 |  |  [PaCEr: Network Embedding From Positional to Structural](https://doi.org/10.1145/3589334.3645516) |  | 0 | Network embedding plays an important role in a variety of social network applications. Existing network embedding methods, explicitly or implicitly, can be categorized into positional embedding (PE) methods or structural embedding (SE) methods. Specifically, PE methods encode the positional information and obtain similar embeddings for adjacent/close nodes, while SE methods aim to learn identical representations for nodes with the same local structural patterns, even if the two nodes are far away from each other. The disparate designs of the two types of methods lead to an apparent dilemma in that no embedding could perfectly capture both positional and structural information. In this paper, we seek to demystify the underlying relationship between positional embedding and structural embedding. We first point out that the positional embedding can produce the structural embedding with simple transformations, while the opposite direction cannot hold. Based on this finding, a novel network embedding model PACER is proposed, which optimizes the positional embedding with the help of random walk with restart (RWR) proximity distribution, and such positional embedding is then used to seamlessly obtain the structural embedding with simple transformations. Furthermore, two variants of PACER are proposed to handle node classification task on homophilic and heterophilic graphs. Extensive experiments on 17 datasets show that PACER achieves comparable or better performance than the state-of-the-arts. | Yuchen Yan, Yongyi Hu, Qinghai Zhou, Lihui Liu, Zhichen Zeng, Yuzhong Chen, Menghai Pan, Huiyuan Chen, Mahashweta Das, Hanghang Tong |  |
| 234 |  |  [Link Recommendation to Augment Influence Diffusion with Provable Guarantees](https://doi.org/10.1145/3589334.3645521) |  | 0 | Link recommendation systems in online social networks (OSNs), such as Facebook's “People You May Know”, Twitter's “Who to Follow”, and Instagram's “Suggested Accounts”, facilitate the formation of new connections among users. This paper addresses the challenge of link recommendation for the purpose of social influence maximization. In particular, given a graph G and the seed set S, our objective is to select k edges that connect seed nodes and ordinary nodes to optimize the influence dissemination of the seed set. This problem, referred to as influence maximization with augmentation (IMA), has been proven to be NP-hard. In this paper, we propose an algorithm, namely , consisting of an efficient estimator for augmented influence estimation and an accelerated sampling approach. provides a (1-1/e-ε)-approximate solution with a high probability of 1-δ, and runs in O(k^2 (m+n) log (n / δ) / ε^2 + k \|E_𝒞\|) time assuming that the influence of any singleton node is smaller than that of the seed set. To the best of our knowledge, this is the first algorithm that can be implemented on large graphs containing millions of nodes while preserving strong theoretical guarantees. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed algorithm. | Xiaolong Chen, Yifan Song, Jing Tang |  |
| 235 |  |  [Fairness Rising from the Ranks: HITS and PageRank on Homophilic Networks](https://doi.org/10.1145/3589334.3645609) |  | 0 | In this paper, we investigate the conditions under which link analysis algorithms prevent minority groups from reaching high ranking slots. We find that the most common link-based algorithms using centrality metrics, such as PageRank and HITS, can reproduce and even amplify bias against minority groups in networks. Yet, their behavior differs: one one hand, we empirically show that PageRank mirrors the degree distribution for most of the ranking positions and it can equalize representation of minorities among the top ranked nodes; on the other hand, we find that HITS amplifies pre-existing bias in homophilic networks through a novel theoretical analysis, supported by empirical results. We find the root cause of bias amplification in HITS to be the level of homophily present in the network, modeled through an evolving network model with two communities. We illustrate our theoretical analysis on both synthetic and real datasets and we present directions for future work. | AnaAndreea Stoica, Nelly Litvak, Augustin Chaintreau |  |
| 236 |  |  [Modeling the Impact of Timeline Algorithms on Opinion Dynamics Using Low-rank Updates](https://doi.org/10.1145/3589334.3645714) |  | 0 | Timeline algorithms are key parts of online social networks, but during recent years they have been blamed for increasing polarization and disagreement in our society. Opinion-dynamics models have been used to study a variety of phenomena in online social networks, but an open question remains on how these models can be augmented to take into account the fine-grained impact of user-level timeline algorithms. We make progress on this question by providing a way to model the impact of timeline algorithms on opinion dynamics. Specifically, we show how the popular Friedkin–Johnsen opinion-formation model can be augmented based on aggregate information, extracted from timeline data. We use our model to study the problem of minimizing the polarization and disagreement; we assume that we are allowed to make small changes to the users' timeline compositions by strengthening some topics of discussion and penalizing some others. We present a gradient descent-based algorithm for this problem, and show that under realistic parameter settings, our algorithm computes a (1+ε)-approximate solution in time Õ(m√(n)(1/ε)), where m is the number of edges in the graph and n is the number of vertices. We also present an algorithm that provably computes an ε-approximation of our model in near-linear time. We evaluate our method on real-world data and show that it effectively reduces the polarization and disagreement in the network. Finally, we release an anonymized graph dataset with ground-truth opinions and more than 27 000 nodes (the previously largest publicly available dataset contains less than 550 nodes). | Tianyi Zhou, Stefan Neumann, Kiran Garimella, Aristides Gionis |  |
| 237 |  |  [PAGE: Equilibrate Personalization and Generalization in Federated Learning](https://doi.org/10.1145/3589334.3645513) |  | 0 | Federated learning (FL) is becoming a major driving force behind machine learning as a service, where customers (clients) collaboratively benefit from shared local updates under the orchestration of the service provider (server). Representing clients' current demands and the server's future demand, local model personalization and global model generalization are separately investigated, as the ill-effects of data heterogeneity enforce the community to focus on one over the other. However, these two seemingly competing goals are of equal importance rather than black and white issues, and should be achieved simultaneously. In this paper, we propose the first algorithm to balance personalization and generalization on top of game theory, dubbed PAGE, which reshapes FL as a co-opetition game between clients and the server. To explore the equilibrium, PAGE further formulates the game as Markov decision processes, and leverages the reinforcement learning algorithm, which simplifies the solving complexity. Extensive experiments on four widespread datasets show that PAGE outperforms state-of-the-art FL baselines in terms of global and local prediction accuracy simultaneously, and the accuracy can be improved by up to 35.20% and 39.91%, respectively. In addition, biased variants of PAGE imply promising adaptiveness to demand shifts in practice. | Qian Chen, Zilong Wang, Jiaqi Hu, Haonan Yan, Jianying Zhou, Xiaodong Lin |  |
| 238 |  |  [MatchNAS: Optimizing Edge AI in Sparse-Label Data Contexts via Automating Deep Neural Network Porting for Mobile Deployment](https://doi.org/10.1145/3589334.3645538) |  | 0 | Recent years have seen the explosion of edge intelligence with powerful Deep Neural Networks (DNNs). One popular scheme is training DNNs on powerful cloud servers and subsequently porting them to mobile devices after being lightweight. Conventional approaches manually specialized DNNs for various edge platforms and retrain them with real-world data. However, as the number of platforms increases, these approaches become labour-intensive and computationally prohibitive. Additionally, real-world data tends to be sparse-label, further increasing the difficulty of lightweight models. In this paper, we propose MatchNAS, a novel scheme for porting DNNs to mobile devices. Specifically, we simultaneously optimise a large network family using both labelled and unlabelled data and then automatically search for tailored networks for different hardware platforms. MatchNAS acts as an intermediary that bridges the gap between cloud-based DNNs and edge-based DNNs. | Hongtao Huang, Xiaojun Chang, Wen Hu, Lina Yao |  |
| 239 |  |  [Temporal Conformity-aware Hawkes Graph Network for Recommendations](https://doi.org/10.1145/3589334.3645354) |  | 0 | Many existing recommender systems (RSs) assume user behavior is governed solely by their interests. However, the peer effect often influences individual decision-making, which leads to conformity behavior. Conventional solutions that eliminate indiscriminately such bias may cause RSs to neglect valuable information and depersonalize the recommendation results. Also, conformity can transform into user interest, e.g., discovering new tastes after a glance at popular music. By better representing different forms of conformity influence, we can do a better job at interest mining and debiasing. In certain extreme circumstances, the herd effect may be exacerbated by user anxiety with uncertainty (e.g., panic buying during the COVID-19 pandemic). RSs may thus fail to respond in time due to sudden and dramatic changes. Moreover, many existing studies potentially conflate conformity bias with popularity bias and lump together various factors responsible for differences in popularity. In this paper, we identify two distinct types of conformity behavior: informational conformity and normative conformity. To address this, we introduce the TCHN model, which utilizes attentional Hawkes processes to disentangle user self-interest and conformity in a personalized manner. Our approach incorporates temporal graph attention networks to capture users' stable and volatile dynamics. We conduct experiments on three real-world datasets, which uncover diverse levels of conformity among users. The results show that TCHN excels in recommendation accuracy, diversity, and fairness across various user groups. | Chenglong Ma, Yongli Ren, Pablo Castells, Mark Sanderson |  |
| 240 |  |  [Hierarchical Graph Signal Processing for Collaborative Filtering](https://doi.org/10.1145/3589334.3645368) |  | 0 | Graph Signal Processing (GSP) has proven to be a highly effective and efficient tool for predicting user future interactions in recommender systems. However, current GSP methods recognize user interaction patterns based on the interactions of all users, so that the recognized interaction patterns are not fully user-matched and easily impacted by other users with different interaction behaviors, resulting in sub-optimal recommendation performance. To this end, we propose a hierarchical graph signal processing method (HiGSP) for collaborative filtering, which consists of two key modules: 1) the cluster-wise filter module that recognizes user unique interaction patterns merely from interactions of users with similar preferences, making the recognized patterns able to reflect user preference without being influenced by other users with different interaction behaviors, and 2) the globally-aware filter module that serves as a complementary to the cluster-wise filter module to recognize user general interaction patterns more effectively from all user interactions. By linearly combining these two modules, HiGSP can recognize user-matched interaction patterns, so as to model user preference and predict user future interactions more accurately. Extensive experiments on six real-world datasets demonstrate the superiority of HiGSP compared to other GCN-based and GSP-based recommendation methods in terms of efficacy and efficiency. | Jiafeng Xia, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, Li Shang, Ning Gu |  |
| 241 |  |  [Lower-Left Partial AUC: An Effective and Efficient Optimization Metric for Recommendation](https://doi.org/10.1145/3589334.3645371) |  | 0 | Optimization metrics are crucial for building recommendation systems at scale. However, an effective and efficient metric for practical use remains elusive. While Top-K ranking metrics are the gold standard for optimization, they suffer from significant computational overhead. Alternatively, the more efficient accuracy and AUC metrics often fall short of capturing the true targets of recommendation tasks, leading to suboptimal performance. To overcome this dilemma, we propose a new optimization metric, Lower-Left Partial AUC (LLPAUC), which is computationally efficient like AUC but strongly correlates with Top-K ranking metrics. Compared to AUC, LLPAUC considers only the partial area under the ROC curve in the Lower-Left corner to push the optimization focus on Top-K. We provide theoretical validation of the correlation between LLPAUC and Top-K ranking metrics and demonstrate its robustness to noisy user feedback. We further design an efficient point-wise recommendation loss to maximize LLPAUC and evaluate it on three datasets, validating its effectiveness and robustness. | Wentao Shi, Chenxu Wang, Fuli Feng, Yang Zhang, Wenjie Wang, Junkang Wu, Xiangnan He |  |
| 242 |  |  [Learning to Rewrite Prompts for Personalized Text Generation](https://doi.org/10.1145/3589334.3645408) |  | 0 | Facilitated by large language models (LLMs), personalized text generation has become a rapidly growing research direction. Most existing studies focus on designing specialized models for a particular domain, or they require fine-tuning the LLMs to generate personalized text. We consider a typical scenario in which the large language model, which generates personalized output, is frozen and can only be accessed through APIs. Under this constraint, all one can do is to improve the input text (i.e., text prompts) sent to the LLM, a procedure that is usually done manually. In this paper, we propose a novel method to automatically revise prompts for personalized text generation. The proposed method takes the initial prompts generated by a state-of-the-art, multistage framework for personalized generation and rewrites a few critical components that summarize and synthesize the personal context. The prompt rewriter employs a training paradigm that chains together supervised learning (SL) and reinforcement learning (RL), where SL reduces the search space of RL and RL facilitates end-to-end training of the rewriter. Using datasets from three representative domains, we demonstrate that the rewritten prompts outperform both the original prompts and the prompts optimized via supervised learning or reinforcement learning alone. In-depth analysis of the rewritten prompts shows that they are not only human readable, but also able to guide manual revision of prompts when there is limited resource to employ reinforcement learning to train the prompt rewriter, or when it is costly to deploy an automatic prompt rewriter for inference. | Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, Michael Bendersky |  |
| 243 |  |  [Physical Trajectory Inference Attack and Defense in Decentralized POI Recommendation](https://doi.org/10.1145/3589334.3645410) |  | 0 | As an indispensable personalized service within Location-Based Social Networks (LBSNs), the Point-of-Interest (POI) recommendation aims to assist individuals in discovering attractive and engaging places. However, the accurate recommendation capability relies on the powerful server collecting a vast amount of users' historical check-in data, posing significant risks of privacy breaches. Although several collaborative learning (CL) frameworks for POI recommendation enhance recommendation resilience and allow users to keep personal data on-device, they still share personal knowledge to improve recommendation performance, thus leaving vulnerabilities for potential attackers. Given this, we design a new Physical Trajectory Inference Attack (PTIA) to expose users' historical trajectories. Specifically, for each user, we identify the set of interacted POIs by analyzing the aggregated information from the target POIs and their correlated POIs. We evaluate the effectiveness of PTIA on two real-world datasets across two types of decentralized CL frameworks for POI recommendation. Empirical results demonstrate that PTIA poses a significant threat to users' historical trajectories. Furthermore, Local Differential Privacy (LDP), the traditional privacy-preserving method for CL frameworks, has also been proven ineffective against PTIA. In light of this, we propose a novel defense mechanism (AGD) against PTIA based on an adversarial game to eliminate sensitive POIs and their information in correlated POIs. After conducting intensive experiments, AGD has been proven precise and practical, with minimal impact on recommendation performance. | Jing Long, Tong Chen, Guanhua Ye, Kai Zheng, Quoc Viet Hung Nguyen, Hongzhi Yin |  |
| 244 |  |  [Towards the Identifiability and Explainability for Personalized Learner Modeling: An Inductive Paradigm](https://doi.org/10.1145/3589334.3645437) |  | 0 | Personalized learner modeling using cognitive diagnosis (CD), which aims to model learners' cognitive states by diagnosing learner traits from behavioral data, is a fundamental yet significant task in many web learning services. Existing cognitive diagnosis models (CDMs) follow the proficiency-response paradigm that views learner traits and question parameters as trainable embeddings and learns them through learner performance prediction. However, we notice that this paradigm leads to the inevitable non-identifiability and explainability overfitting problem, which is harmful to the quantification of learners' cognitive states and the quality of web learning services. To address these problems, we propose an identifiable cognitive diagnosis framework (ID-CDF) based on a novel response-proficiency-response paradigm inspired by encoder-decoder models. Specifically, we first devise the diagnostic module of ID-CDF, which leverages inductive learning to eliminate randomness in optimization to guarantee identifiability and captures the monotonicity between overall response data distribution and cognitive states to prevent explainability overfitting. Next, we propose a flexible predictive module for ID-CDF to ensure diagnosis preciseness. We further present an implementation of ID-CDF, i.e., ID-CDM, to illustrate its usability. Extensive experiments on four real-world datasets with different characteristics demonstrate that ID-CDF can effectively address the problems without loss of diagnosis preciseness. | Jiatong Li, Qi Liu, Fei Wang, Jiayu Liu, Zhenya Huang, Fangzhou Yao, Linbo Zhu, Yu Su |  |
| 245 |  |  [Generative News Recommendation](https://doi.org/10.1145/3589334.3645448) |  | 0 | Most existing news recommendation methods tackle this task by conducting semantic matching between candidate news and user representation produced by historical clicked news. However, they overlook the high-level connections among different news articles and also ignore the profound relationship between these news articles and users. And the definition of these methods dictates that they can only deliver news articles as-is. On the contrary, integrating several relevant news articles into a coherent narrative would assist users in gaining a quicker and more comprehensive understanding of events. In this paper, we propose a novel generative news recommendation paradigm that includes two steps: (1) Leveraging the internal knowledge and reasoning capabilities of the Large Language Model (LLM) to perform high-level matching between candidate news and user representation; (2) Generating a coherent and logically structured narrative based on the associations between related news and user interests, thus engaging users in further reading of the news. Specifically, we propose GNR to implement the generative news recommendation paradigm. First, we compose the dual-level representation of news and users by leveraging LLM to generate theme-level representations and combine them with semantic-level representations. Next, in order to generate a coherent narrative, we explore the news relation and filter the related news according to the user preference. Finally, we propose a novel training method named UIFT to train the LLM to fuse multiple news articles in a coherent narrative. Extensive experiments show that GNR can improve recommendation accuracy and eventually generate more personalized and factually consistent narratives. | Shen Gao, Jiabao Fang, Quan Tu, Zhitao Yao, Zhumin Chen, Pengjie Ren, Zhaochun Ren |  |
| 246 |  |  [MMPOI: A Multi-Modal Content-Aware Framework for POI Recommendations](https://doi.org/10.1145/3589334.3645449) |  | 0 | The Point-of-Interest (POI) recommendation system, designed to recommend potential future visits of users based on their check-in sequences, faces the challenge of data scarcity. This challenge primarily stems from the data sparsity issue, namely users interact with only a small number of POIs. Most existing studies attempt to solve this problem by focusing on POI check-in sequences, without considering the substantial multi-modal content information (e.g. textual and image data) commonly associated with POIs. In this paper, we propose a novel multi-modal content-aware framework for POI recommendation (MMPOI). Our approach addresses the issue of data sparsity by incorporating multi-modal content information about POIs from a new perspective. Specifically, MMPOI leverages pre-trained models for inter-modal conversion and employs a unified pre-trained model to extract modal-specific features from each modality, effectively bridging the semantic gap between different modalities. We propose to build a Multi-Modal Trajectory Flow Graph (MTFG) which combines the multi-modal semantic structure with check-in sequences. Moreover, we design an adaptive multi-task Transformer that models users' multi-modal movement patterns and integrates them for the next POI recommendation tasks. Extensive experiments on four real-world datasets demonstrate that MMPOI outperforms state-of-the-art POI recommendation methods. To facilitate reproducibility, we have released both the code and the multi-modal POI recommendation datasets we collect https://github.com/zzmylq/MMPOI | Yang Xu, Gao Cong, Lei Zhu, Lizhen Cui |  |
| 247 |  |  [Representation Learning with Large Language Models for Recommendation](https://doi.org/10.1145/3589334.3645458) |  | 0 | Recommender systems have seen significant advancements with the influence of deep learning and graph neural networks, particularly in capturing complex user-item relationships. However, these graph-based recommenders heavily depend on ID-based data, potentially disregarding valuable textual information associated with users and items, resulting in less informative learned representations. Moreover, the utilization of implicit feedback data introduces potential noise and bias, posing challenges for the effectiveness of user preference learning. While the integration of large language models (LLMs) into traditional ID-based recommenders has gained attention, challenges such as scalability issues, limitations in text-only reliance, and prompt input constraints need to be addressed for effective implementation in practical recommender systems. To address these challenges, we propose a model-agnostic framework RLMRec that aims to enhance existing recommenders with LLM-empowered representation learning. It proposes a recommendation paradigm that integrates representation learning with LLMs to capture intricate semantic aspects of user behaviors and preferences. RLMRec incorporates auxiliary textual signals, develops a user/item profiling paradigm empowered by LLMs, and aligns the semantic space of LLMs with the representation space of collaborative relational signals through a cross-view alignment framework. This work further establish a theoretical foundation demonstrating that incorporating textual signals through mutual information maximization enhances the quality of representations. In our evaluation, we integrate RLMRec with state-of-the-art recommender models, while also analyzing its efficiency and robustness to noise data. Our implementation codes are available at https://github.com/HKUDS/RLMRec. | Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang |  |
| 248 |  |  [Challenging Low Homophily in Social Recommendation](https://doi.org/10.1145/3589334.3645460) |  | 0 | Social relations are leveraged to tackle the sparsity issue of user-item interaction data in recommendation under the assumption of social homophily. However, social recommendation paradigms predominantly focus on homophily based on user preferences. While social information can enhance recommendations, its alignment with user preferences is not guaranteed, thereby posing the risk of introducing informational redundancy. We empirically discover that social graphs in real recommendation data exhibit low preference-aware homophily, which limits the effect of social recommendation models. To comprehensively extract preference-aware homophily information latent in the social graph, we propose Social Heterophily-alleviating Rewiring (SHaRe), a data-centric framework for enhancing existing graph-based social recommendation models. We adopt Graph Rewiring technique to capture and add highly homophilic social relations, and cut low homophilic (or heterophilic) relations. To better refine the user representations from reliable social relations, we integrate a contrastive learning method into the training of SHaRe, aiming to calibrate the user representations for enhancing the result of Graph Rewiring. Experiments on real-world datasets show that the proposed framework not only exhibits enhanced performances across varying homophily ratios but also improves the performance of existing state-of-the-art (SOTA) social recommendation models. | Wei Jiang, Xinyi Gao, Guandong Xu, Tong Chen, Hongzhi Yin |  |
| 249 |  |  [Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization](https://doi.org/10.1145/3589334.3645463) |  | 0 | Click-Through Rate (CTR) prediction holds paramount significance in online advertising and recommendation scenarios. Despite the proliferation of recent CTR prediction models, the improvements in performance have remained limited, as evidenced by open-source benchmark assessments. Current researchers tend to focus on developing new models for various datasets and settings, often neglecting a crucial question: What is the key challenge that truly makes CTR prediction so demanding? In this paper, we approach the problem of CTR prediction from an optimization perspective. We explore the typical data characteristics and optimization statistics of CTR prediction, revealing a strong positive correlation between the top hessian eigenvalue and feature frequency. This correlation implies that frequently occurring features tend to converge towards sharp local minima, ultimately leading to suboptimal performance. Motivated by the recent advancements in sharpness-aware minimization (SAM), which considers the geometric aspects of the loss landscape during optimization, we present a dedicated optimizer crafted for CTR prediction, named Helen. Helen incorporates frequency-wise Hessian eigenvalue regularization, achieved through adaptive perturbations based on normalized feature frequencies. Empirical results under the open-source benchmark framework underscore Helen's effectiveness. It successfully constrains the top eigenvalue of the Hessian matrix and demonstrates a clear advantage over widely used optimization algorithms when applied to seven popular models across three public benchmark datasets on BARS. Our code locates at github.com/NUS-HPC-AI-Lab/Helen. | Zirui Zhu, Yong Liu, Zangwei Zheng, Huifeng Guo, Yang You |  |
| 250 |  |  [Linear-Time Graph Neural Networks for Scalable Recommendations](https://doi.org/10.1145/3589334.3645486) |  | 0 | In an era of information explosion, recommender systems are vital tools to deliver personalized recommendations for users. The key of recommender systems is to forecast users' future behaviors based on previous user-item interactions. Due to their strong expressive power of capturing high-order connectivities in user-item interaction data, recent years have witnessed a rising interest in leveraging Graph Neural Networks (GNNs) to boost the prediction performance of recommender systems. Nonetheless, classic Matrix Factorization (MF) and Deep Neural Network (DNN) approaches still play an important role in real-world large-scale recommender systems due to their scalability advantages. Despite the existence of GNN-acceleration solutions, it remains an open question whether GNN-based recommender systems can scale as efficiently as classic MF and DNN methods. In this paper, we propose a Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender systems to achieve comparable scalability as classic MF approaches while maintaining GNNs' powerful expressiveness for superior prediction accuracy. Extensive experiments and ablation studies are presented to validate the effectiveness and scalability of the proposed algorithm. Our implementation based on PyTorch is available. | Jiahao Zhang, Rui Xue, Wenqi Fan, Xin Xu, Qing Li, Jian Pei, Xiaorui Liu |  |
| 251 |  |  [Intersectional Two-sided Fairness in Recommendation](https://doi.org/10.1145/3589334.3645518) |  | 0 | Fairness of recommender systems (RS) has attracted increasing attention recently. Based on the involved stakeholders, the fairness of RS can be divided into user fairness, item fairness, and two-sided fairness which considers both user and item fairness simultaneously. However, we argue that the intersectional two-sided unfairness may still exist even if the RS is two-sided fair, which is observed and shown by empirical studies on real-world data in this paper, and has not been well-studied previously. To mitigate this problem, we propose a novel approach called Intersectional Two-sided Fairness Recommendation (ITFR). Our method utilizes a sharpness-aware loss to perceive disadvantaged groups, and then uses collaborative loss balance to develop consistent distinguishing abilities for different intersectional groups. Additionally, predicted score normalization is leveraged to align positive predicted scores to fairly treat positives in different intersectional groups. Extensive experiments and analyses on three public datasets show that our proposed approach effectively alleviates the intersectional two-sided unfairness and consistently outperforms previous state-of-the-art methods. | Yifan Wang, Peijie Sun, Weizhi Ma, Min Zhang, Yuan Zhang, Peng Jiang, Shaoping Ma |  |
| 252 |  |  [Full Stage Learning to Rank: A Unified Framework for Multi-Stage Systems](https://doi.org/10.1145/3589334.3645523) |  | 0 | The Probability Ranking Principle (PRP) has been considered as the foundational standard in the design of information retrieval (IR) systems. The principle requires an IR module's returned list of results to be ranked with respect to the underlying user interests, so as to maximize the results' utility. Nevertheless, we point out that it is inappropriate to indiscriminately apply PRP through every stage of a contemporary IR system. Such systems contain multiple stages (e.g., retrieval, pre-ranking, ranking, and re-ranking stages, as examined in this paper). The selection bias inherent in the model of each stage significantly influences the results that are ultimately presented to users. To address this issue, we propose an improved ranking principle for multi-stage systems, namely the Generalized Probability Ranking Principle (GPRP), to emphasize both the selection bias in each stage of the system pipeline as well as the underlying interest of users. We realize GPRP via a unified algorithmic framework named Full Stage Learning to Rank. Our core idea is to first estimate the selection bias in the subsequent stages and then learn a ranking model that best complies with the downstream modules' selection bias so as to deliver its top ranked results to the final ranked list in the system's output. We performed extensive experiment evaluations of our developed Full Stage Learning to Rank solution, using both simulations and online A/B tests in one of the leading short-video recommendation platforms. The algorithm is proved to be effective in both retrieval and ranking stages. Since deployed, the algorithm has brought consistent and significant performance gain to the platform. | Kai Zheng, Haijun Zhao, Rui Huang, Beichuan Zhang, Na Mou, Yanan Niu, Yang Song, Hongning Wang, Kun Gai |  |
| 253 |  |  [RecDCL: Dual Contrastive Learning for Recommendation](https://doi.org/10.1145/3589334.3645533) |  | 0 | Self-supervised recommendation (SSR) has achieved great success in mining the potential interacted behaviors for collaborative filtering in recent years. As a major branch, Contrastive Learning (CL) based SSR conquers data sparsity in Web platforms by contrasting the embedding between raw data and augmented data. However, existing CL-based SSR methods mostly focus on contrasting in a batch-wise way, failing to exploit potential regularity in the feature-wise dimension, leading to redundant solutions during the representation learning process of users (items) from Websites. Furthermore, the joint benefits of utilizing both Batch-wise CL (BCL) and Feature-wise CL (FCL) for recommendations remain underexplored. To address these issues, we investigate the relationship of objectives between BCL and FCL. Our study suggests a cooperative benefit of employing both methods, as evidenced from theoretical and experimental perspectives. Based on these insights, we propose a dual CL method for recommendation, referred to as RecDCL. RecDCL first eliminates redundant solutions on user-item positive pairs in a feature-wise manner. It then optimizes the uniform distributions within users and items using a polynomial kernel from an FCL perspective. Finally, it generates contrastive embedding on output vectors in a batch-wise objective. We conduct experiments on four widely-used benchmarks and an industrial dataset. The results consistently demonstrate that the proposed RecDCL outperforms the state-of-the-art GNNs-based and SSL-based models (with up to a 5.65\% improvement in terms of Recall@20), thereby confirming the effectiveness of the joint-wise objective. All source codes used in this paper are publicly available at \url{https://github.com/THUDM/RecDCL}}. | Dan Zhang, Yangliao Geng, Wenwen Gong, Zhongang Qi, Zhiyu Chen, Xing Tang, Ying Shan, Yuxiao Dong, Jie Tang |  |
| 254 |  |  [GraphPro: Graph Pre-training and Prompt Learning for Recommendation](https://doi.org/10.1145/3589334.3645546) |  | 0 | GNN-based recommendation systems have been successful in capturing complex user-item interactions using multi-hop message passing. However, these methods often struggle to handle the dynamic nature of user-item interactions, making it challenging to adapt to changes in user preferences and new data distributions. This limits their scalability and performance in real-world dynamic scenarios. In our study, we propose a framework called GraphPro that combines dynamic graph pre-training with prompt learning in an efficient way. This unique approach allows GNNs to effectively capture both long-term user preferences and short-term behavior changes, resulting in accurate and up-to-date recommendations. To address the issue of changing user preferences, we integrate a temporal prompt mechanism and a graph-structural prompt learning mechanism into the pre-trained GNN architecture. The temporal prompt mechanism incorporates time-related information into user-item interactions, enabling the model to naturally incorporate temporal dynamics. The graph-structural prompt learning mechanism allows the model to apply pre-trained insights to new behavior dynamics without the need for continuous retraining. We also introduce a dynamic evaluation framework for recommendations that better reflects real-world scenarios and reduces the offline-online discrepancy. Through comprehensive experiments, including deployment in a large-scale industrial scenario, we demonstrate the seamless scalability of GraphPro with various leading recommenders. Our results highlight the superiority of GraphPro in terms of effectiveness, robustness, and efficiency. We release the model implementation at the link: https://github.com/HKUDS/GraphPro. | Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang |  |
| 255 |  |  [Modeling Balanced Explicit and Implicit Relations with Contrastive Learning for Knowledge Concept Recommendation in MOOCs](https://doi.org/10.1145/3589334.3645559) |  | 0 | The knowledge concept recommendation in Massive Open Online Courses (MOOCs) is a significant issue that has garnered widespread attention. Existing methods primarily rely on the explicit relations between users and knowledge concepts on the MOOC platforms for recommendation. However, there are numerous implicit relations (e.g., shared interests or same knowledge levels between users) generated within the users' learning activities on the MOOC platforms. Existing methods fail to consider these implicit relations, and these relations themselves are difficult to learn and represent, causing poor performance in knowledge concept recommendation and an inability to meet users' personalized needs. To address this issue, we propose a novel framework based on contrastive learning, which can represent and balance the explicit and implicit relations for knowledge concept recommendation in MOOCs (CL-KCRec). Specifically, we first construct a MOOCs heterogeneous information network (HIN) by modeling the data from the MOOC platforms. Then, we utilize a relation-updated graph convolutional network and stacked multi-channel graph neural network to represent the explicit and implicit relations in the HIN, respectively. Considering that the quantity of explicit relations is relatively fewer compared to implicit relations in MOOCs, we propose a contrastive learning with prototypical graph to enhance the representations of both relations to capture their fruitful inherent relational knowledge, which can guide the propagation of students' preferences within the HIN. Based on these enhanced representations, to ensure the balanced contribution of both towards the final recommendation, we propose a dual-head attention mechanism for balanced fusion. Experimental results demonstrate that CL-KCRec outperforms several state-of-the-art baselines on real-world datasets in terms of HR, NDCG and MRR. | Hengnian Gu, Zhiyi Duan, Pan Xie, Dongdai Zhou |  |
| 256 |  |  [A Counterfactual Framework for Learning and Evaluating Explanations for Recommender Systems](https://doi.org/10.1145/3589334.3645560) |  | 0 | In the field of recommender systems, explainability remains a pivotal yet challenging aspect. To address this, we introduce the Learning to eXplain Recommendations (LXR) framework, a post-hoc, model-agnostic approach designed for providing counterfactual explanations. LXR is compatible with any differentiable recommender algorithm and scores the relevance of user data in relation to recommended items. A distinctive feature of LXR is its use of novel self-supervised counterfactual loss terms, which effectively highlight the most influential user data responsible for a specific recommended item. Additionally, we propose several innovative counterfactual evaluation metrics specifically tailored for assessing the quality of explanations in recommender systems. Our code is available on our GitHub repository: https://github.com/DeltaLabTLV/LXR. | Oren Barkan, Veronika Bogina, Liya Gurevitch, Yuval Asher, Noam Koenigstein |  |
| 257 |  |  [Category-based and Popularity-guided Video Game Recommendation: A Balance-oriented Framework](https://doi.org/10.1145/3589334.3645573) |  | 0 | In recent years, the video game industry has experienced substantial growth, presenting players with a vast array of game choices. This surge in options has spurred the need for a specialized recommender system tailored for video games. However, current video game recommendation approaches tend to prioritize accuracy over diversity, potentially leading to unvaried game suggestions. In addition, the existing game recommendation methods commonly lack the ability to establish strict connections between games to enhance accuracy. Furthermore, many existing diversity-focused methods fail to leverage crucial item information, such as item category and popularity during neighbor modeling and message propagation. To address these challenges, we introduce a novel framework, called CPGRec, comprising three modules, namely accuracy-driven, diversity-driven, and comprehensive modules. The first module extends the state-of-the-art accuracy-focused game recommendation method by connecting games in a more stringent manner to enhance recommendation accuracy. The second module connects neighbors with diverse categories within the proposed game graph and harnesses the advantages of popular game nodes to amplify the influence of long-tail games within the player-game bipartite graph, thereby enriching recommendation diversity. The third module combines the above two modules and employs a new negative-sample rating score reweighting method to balance accuracy and diversity. Experimental results on the Steam dataset demonstrate the effectiveness of our proposed method in improving game recommendations. The dataset and source codes are anonymously released at: https://github.com/CPGRec2024/CPGRec.git. | Xiping Li, Jianghong Ma, Kangzhe Liu, Shanshan Feng, Haijun Zhang, Yutong Wang |  |
| 258 |  |  [Unleashing the Power of Knowledge Graph for Recommendation via Invariant Learning](https://doi.org/10.1145/3589334.3645576) |  | 0 | Knowledge graph (KG) demonstrates substantial potential for enhancing the performance of recommender systems. Due to its rich semantic content and associations among interactive entities, it can effectively alleviate inherent limitations in collaborative filtering (CF), such as data sparsity or cold-start issues. However, most existing knowledge-aware recommendation models indiscriminately aggregate all information in KG, without considering information specifically relevant to the recommendation task. Such indiscriminate aggregation could introduce additional noisy knowledge into representation learning, which can distort the understanding of users' genuine preferences, thereby sacrificing the recommendation quality. In this paper, we introduce the principle of invariance to the knowledge-aware recommendation, culminating in our Knowledge Graph Invariant Learning (KGIL) framework. It aims to discern and harness the task-relevant knowledge connections within KG to enhance the recommendation models. Specifically, we employ multiple environment generators to simulate diverse noisy KG-environments. Then we devise a novel attention learning mechanism for KG and user-item interaction graph, aiming to learn environment-invariant subgraphs. Leveraging an adversarial optimization strategy, we enhance the diversity of the environments, meanwhile, promote invariant representation learning across environments. We conduct extensive experiments on three datasets and compare KGIL with state-of-the-art methods. The experimental results further demonstrate the superiority of our approach. | Shuyao Wang, Yongduo Sui, Chao Wang, Hui Xiong |  |
| 259 |  |  [Distributionally Robust Graph-based Recommendation System](https://doi.org/10.1145/3589334.3645598) |  | 0 | With the capacity to capture high-order collaborative signals, Graph Neural Networks (GNNs) have emerged as powerful methods in Recommender Systems (RS). However, their efficacy often hinges on the assumption that training and testing data share the same distribution (a.k.a. IID assumption), and exhibits significant declines under distribution shifts. Distribution shifts commonly arises in RS, often attributed to the dynamic nature of user preferences or ubiquitous biases during data collection in RS. Despite its significance, researches on GNN-based recommendation against distribution shift are still sparse. To bridge this gap, we propose Distributionally Robust GNN (DR-GNN) that incorporates Distributional Robust Optimization (DRO) into the GNN-based recommendation. DR-GNN addresses two core challenges: 1) To enable DRO to cater to graph data intertwined with GNN, we reinterpret GNN as a graph smoothing regularizer, thereby facilitating the nuanced application of DRO; 2) Given the typically sparse nature of recommendation data, which might impede robust optimization, we introduce slight perturbations in the training distribution to expand its support. Notably, while DR-GNN involves complex optimization, it can be implemented easily and efficiently. Our extensive experiments validate the effectiveness of DR-GNN against three typical distribution shifts. The code is available at https://github.com/WANGBohaO-jpg/DR-GNN . | Bohao Wang, Jiawei Chen, Changdong Li, Sheng Zhou, Qihao Shi, Yang Gao, Yan Feng, Chun Chen, Can Wang |  |
| 260 |  |  [Co-clustering for Federated Recommender System](https://doi.org/10.1145/3589334.3645626) |  | 0 | As data privacy and security attract increasing attention, Federated Recommender System (FRS) offers a solution that strikes a balance between providing high-quality recommendations and preserving user privacy. However, the presence of statistical heterogeneity in FRS, commonly observed due to personalized decision-making patterns, can pose challenges. To address this issue and maximize the benefit of collaborative filtering (CF) in FRS, it is intuitive to consider clustering clients (users) as well as items into different groups and learning group-specific models. Existing methods either resort to client clustering via user representations-risking privacy leakage, or employ classical clustering strategies on item embeddings or gradients, which we found are plagued by the curse of dimensionality. In this paper, we delve into the inefficiencies of the K-Means method in client grouping, attributing failures due to the high dimensionality as well as data sparsity occurring in FRS, and propose CoFedRec, a novel Co-clustering Federated Recommendation mechanism, to address clients heterogeneity and enhance the collaborative filtering within the federated framework. Specifically, the server initially formulates an item membership from the client-provided item networks. Subsequently, clients are grouped regarding a specific item category picked from the item membership during each communication round, resulting in an intelligently aggregated group model. Meanwhile, to comprehensively capture the global inter-relationships among items, we incorporate an additional supervised contrastive learning term based on the server-side generated item membership into the local training phase for each client. Extensive experiments on four datasets are provided, which verify the effectiveness of the proposed CoFedRec. | Xinrui He, Shuo Liu, Jacky Keung, Jingrui He |  |
| 261 |  |  [PMG : Personalized Multimodal Generation with Large Language Models](https://doi.org/10.1145/3589334.3645633) |  | 0 | The emergence of large language models (LLMs) has revolutionized the capabilities of text comprehension and generation. Multi-modal generation attracts great attention from both the industry and academia, but there is little work on personalized generation, which has important applications such as recommender systems. This paper proposes the first method for personalized multimodal generation using LLMs, showcases its applications and validates its performance via an extensive experimental study on two datasets. The proposed method, Personalized Multimodal Generation (PMG for short) first converts user behaviors (e.g., clicks in recommender systems or conversations with a virtual assistant) into natural language to facilitate LLM understanding and extract user preference descriptions. Such user preferences are then fed into a generator, such as a multimodal LLM or diffusion model, to produce personalized content. To capture user preferences comprehensively and accurately, we propose to let the LLM output a combination of explicit keywords and implicit embeddings to represent user preferences. Then the combination of keywords and embeddings are used as prompts to condition the generator. We optimize a weighted sum of the accuracy and preference scores so that the generated content has a good balance between them. Compared to a baseline method without personalization, PMG has a significant improvement on personalization for up to 8 | Xiaoteng Shen, Rui Zhang, Xiaoyan Zhao, Jieming Zhu, Xi Xiao |  |
| 262 |  |  [M-scan: A Multi-Scenario Causal-driven Adaptive Network for Recommendation](https://doi.org/10.1145/3589334.3645635) |  | 0 | We primarily focus on the field of multi-scenario recommendation, which poses a significant challenge in effectively leveraging data from different scenarios to enhance predictions in scenarios with limited data. Current mainstream efforts mainly center around innovative model network architectures, with the aim of enabling the network to implicitly acquire knowledge from diverse scenarios. However, the uncertainty of implicit learning in networks arises from the absence of explicit modeling, leading to not only difficulty in training but also incomplete user representation and suboptimal performance. Furthermore, through causal graph analysis, we have discovered that the scenario itself directly influences click behavior, yet existing approaches directly incorporate data from other scenarios during the training of the current scenario, leading to prediction biases when they directly utilize click behaviors from other scenarios to train models. To address these problems, we propose the Multi-Scenario Causal-driven Adaptive Network M-scan). This model incorporates a Scenario-Aware Co-Attention mechanism that explicitly extracts user interests from other scenarios that align with the current scenario. Additionally, it employs a Scenario Bias Eliminator module utilizing causal counterfactual inference to mitigate biases introduced by data from other scenarios. Extensive experiments on two public datasets demonstrate the efficacy of our M-scan compared to the existing baseline models. | Jiachen Zhu, Yichao Wang, Jianghao Lin, Jiarui Qin, Ruiming Tang, Weinan Zhang, Yong Yu |  |
| 263 |  |  [Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation](https://doi.org/10.1145/3589334.3645693) |  | 0 | The heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has emerged as a potent tool for mitigating data sparsity in recommender systems. Existing HIN-based recommender systems operate under the assumption of centralized storage and model training. However, real-world data is often distributed due to privacy concerns, leading to the semantic broken issue within HINs and consequent failures in centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored on the client side and shared HINs on the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which facilitates collaborative training of a recommendation model using distributed HINs while protecting user privacy. Specifically, we first formalize the privacy definition for HIN-based federated recommendation (FedRec) in the light of differential privacy, with the goal of protecting user-item interactions within private HIN as well as users' high-order patterns from shared HINs. To recover the broken meta-path based semantics and ensure proposed privacy measures, we elaborately design a semantic-preserving user interactions publishing method, which locally perturbs user's high-order patterns and related user-item interactions for publishing. Subsequently, we introduce an HGNN model for recommendation, which conducts node- and semantic-level aggregations to capture recovered semantics. Extensive experiments on four datasets demonstrate that our model outperforms existing methods by a substantial margin (up to 34 reasonable privacy budget. | Bo Yan, Yang Cao, Haoyu Wang, Wenchuan Yang, Junping Du, Chuan Shi |  |
| 264 |  |  [Understanding Human Preferences: Towards More Personalized Video to Text Generation](https://doi.org/10.1145/3589334.3645711) |  | 0 | While previous video to text models have achieved remarkable successes, they mostly focus on how to understand the video contents in a general sense, but fail to capture the human personalized preferences, which is highly demanded for an engaging multimodal chatbots. Different from user modeling in collaborative filtering, there is no other user behaviors in inference as a real-time video stream is coming. In this paper, we formally define the task of personalized video commenting task and design an end-to-end personalized framework for solving this task. In specific, we argue that the personalization for video comment generation can be reflected in two aspects, that is, (1) for the same video, different users may comment on different clips, and (2) for the same clip, different people may also express various opinions with diverse commentary styles. Motivated by these considerations, we design our framework based on two components. The first one is a clip selector, which is responsible for predicting the clips that the user may comment in the video. The second one is a text generator, which aims to produce the comment based on the above predicted clips and the user's preference. In our framework, these two components are optimized in an end-to-end manner to mutually enhance each other, where we design confidence-aware scheduled sampling and iterative inference strategies to solve the problem that the ground truth clips are absent in the inference phase. As the absence of personalized video to text dataset, we collect and release a new dataset for studying this problem. We conduct extensive experiments to demonstrate the effectiveness of our model. | Yihan Wu, Ruihua Song, Xu Chen, Hao Jiang, Zhao Cao, Jin Yu |  |
| 265 |  |  [Entity Disambiguation with Extreme Multi-label Ranking](https://doi.org/10.1145/3589334.3645498) |  | 0 | Entity disambiguation is one of the most important natural language tasks to identify entities behind ambiguous surface mentions within a knowledge base. Although many recent studies apply deep learning to achieve decent results, they need exhausting pre-training and mediocre recall in the retrieval stage. In this paper, we propose a novel framework, eXtreme Multi-label Ranking for Entity Disambiguation (XMRED), to address this challenge. An efficient zero-shot entity retriever with auxiliary data is first pre-trained to recall relevant entities based on linear models. Specifically, the retrieval process can be considered as an extreme multi-label ranking (XMR) task. Entities are first clustered at different scales to form a label tree, thereby learning multi-scale entity retrievers over the label tree with high recall. Moreover, XMRED applies deep cross-encoder as a re-ranker to achieve high precision based on high-quality candidates. Extensive experimental results based on the AIDA-CoNLL benchmark and five zero-shot testing datasets demonstrate that XMRED obtains 98% and over 95% recall scores for in-domain and zero-shot datasets with top-10 retrieved entities. With a deep cross-encoder as the re-ranker, XMRED further outperforms the previous state-of-the-art by 1.74% in In-KB micro-F1 scores on average with a significant improvement on the training efficiency from days to 3.48 hours. In addition, XMRED also beats the state-of-the-art for page-level document retrieval by 2.38% in accuracy and 1.90% in recall@5. | JyunYu Jiang, WeiCheng Chang, Jiong Zhang, ChoJui Hsieh, HsiangFu Yu |  |
| 266 |  |  [BOND: Bootstrapping From-Scratch Name Disambiguation with Multi-task Promoting](https://doi.org/10.1145/3589334.3645580) |  | 0 | From-scratch name disambiguation is an essential task for establishing a reliable foundation for academic platforms. It involves partitioning documents authored by identically named individuals into groups representing distinct real-life experts. Canonically, the process is divided into two decoupled tasks: locally estimating the pairwise similarities between documents followed by globally grouping these documents into appropriate clusters. However, such a decoupled approach often inhibits optimal information exchange between these intertwined tasks. Therefore, we present BOND, which bootstraps the local and global informative signals to promote each other in an end-to-end regime. Specifically, BOND harnesses local pairwise similarities to drive global clustering, subsequently generating pseudo-clustering labels. These global signals further refine local pairwise characterizations. The experimental results establish BOND's superiority, outperforming other advanced baselines by a substantial margin. Moreover, an enhanced version, BOND+, incorporating ensemble and post-match techniques, rivals the top methods in the WhoIsWho competition. | Yuqing Cheng, Bo Chen, Fanjin Zhang, Jie Tang |  |
| 267 |  |  [Inductive Graph Alignment Prompt: Bridging the Gap between Graph Pre-training and Inductive Fine-tuning From Spectral Perspective](https://doi.org/10.1145/3589334.3645620) |  | 0 | The "Graph pre-training and fine-tuning" paradigm has significantly improved Graph Neural Networks(GNNs) by capturing general knowledge without manual annotations for downstream tasks. However, due to the immense gap of data and tasks between the pre-training and fine-tuning stages, the model performance is still limited. Inspired by prompt fine-tuning in Natural Language Processing(NLP), many endeavors have been made to bridge the gap in graph domain. But existing methods simply reformulate the form of fine-tuning tasks to the pre-training ones. With the premise that the pre-training graphs are compatible with the fine-tuning ones, these methods typically operate in transductive setting. In order to generalize graph pre-training to inductive scenario where the fine-tuning graphs might significantly differ from pre-training ones, we propose a novel graph prompt based method called Inductive Graph Alignment Prompt(IGAP). Firstly, we unify the mainstream graph pre-training frameworks and analyze the essence of graph pre-training from graph spectral theory. Then we identify the two sources of the data gap in inductive setting: (i) graph signal gap and (ii) graph structure gap. Based on the insight of graph pre-training, we propose to bridge the graph signal gap and the graph structure gap with learnable prompts in the spectral space. A theoretical analysis ensures the effectiveness of our method. At last, we conduct extensive experiments among nodes classification and graph classification tasks under the transductive, semi-inductive and inductive settings. The results demonstrate that our proposed method can successfully bridge the data gap under different settings. | Yuchen Yan, Peiyan Zhang, Zheng Fang, Qingqing Long |  |
| 268 |  |  [Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction](https://doi.org/10.1145/3589334.3645678) |  | 0 | Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in information systems that aims to simultaneously extract entities with semantic relations from a document. Existing methods heavily rely on a substantial amount of fully labeled data. However, collecting and annotating data for newly emerging relations is time-consuming and labor-intensive. Recent advanced Large Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text generation capabilities, inspiring us to explore an alternative approach for obtaining auto-labeled documents with new relations. In this paper, we propose a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework, which generates labeled data by retrieval and denoising knowledge from LLMs, called GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide ChatGPT to generate labeled long-text data step by step. To improve the quality of synthetic data, we propose a denoising strategy based on the consistency of cross-document knowledge. Leveraging our denoised synthetic data, we proceed to fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets. We perform experiments for both zero-shot document-level relation and triplet extraction on two public datasets. The experimental results illustrate that our GenRDK framework outperforms strong baselines. | Qi Sun, Kun Huang, Xiaocui Yang, Rong Tong, Kun Zhang, Soujanya Poria |  |
| 269 |  |  [Detecting Illicit Food Factories from Chemical Declaration Data via Graph-aware Self-supervised Contrastive Anomaly Ranking](https://doi.org/10.1145/3589334.3648138) |  | 0 | In the global food industry, where the line between legitimate and illicit manufacturing is increasingly blurred by the scale and complexity of the supply chain, safeguarding consumer health and trust necessitates innovative detection methods. Addressing this, this paper presents Graph-aware Self-supervised Contrastive Anomaly Ranking (GraphCAR), a novel unsupervised learning model, devised to identify illicit food factories through the scrutiny of chemical declaration data. GraphCAR tackles the scarcity of labeled data and the intricacies inherent in the vast array of declared chemicals, leveraging a Graph Autoencoder fused with a self-supervised contrastive learning mechanism. This fusion not only simplifies the feature space by embedding chemical declarations within a bipartite graph but also adeptly flags subtle, potentially illicit patterns through contrastively inspecting the learned factory representations. Through rigorous evaluations conducted on real-world factory's chemical declaration data, GraphCAR has demonstrated superior performance over conventional methods on unsupervised outlier detection and one-class classification tasks, showcasing its accuracy, robustness and reliability in flagging potential malpractice. With its successful application in food safety, GraphCAR stands as a testament to the potential of AI-driven solutions to address multifaceted challenges for the greater good. | ShengFang Yang, ChengTe Li |  |
| 270 |  |  [Bayesian Iterative Prediction and Lexical-based Interpretation for Disturbed Chinese Sentence Pair Matching](https://doi.org/10.1145/3589334.3648149) |  | 0 | In an era dominated by web-based intelligent customer services, the applications of Sentence Pair Matching are profoundly broad. Web agents, for example, automatically respond to customer queries by finding similar past questions, significantly reducing customer service expenses. While current large language models (LLMs) offer powerful text generation capabilities, they often struggle with opacity, potential text toxicity, and difficulty managing domain-specific and confidential business inquiries. Consequently, the widespread adoption of web-based intelligent customer services in real-world business still greatly relies on query-based interactions. In this paper, we introduce a series of model-agnostic techniques aimed at enhancing both the accuracy and interpretability of Chinese pairwise sentence-matching models. Our contributions include (1) An Edit-distance-weighted fine-tuning method, (2) A Bayesian Iterative Prediction algorithm, (3) A Lexical-based Dual Ranking Interpreter, and (4) A Bi-criteria Denoising strategy. Experimental results on the Large-scale Chinese Question Matching Corpus (LCQMC) with a disturbed test demonstrate that our fine-tuning and prediction methods can steadily improve matching accuracy, building on the current state-of-the-art models. Besides, our interpreter with denoising strategy markedly enhances token-level interpretation in rationality and loyalty. In both matching accuracy and interpretation, our approaches outperform classic methods and even LLMs. | Muzhe Guo, Muhao Guo, Juntao Su, Junyu Chen, Jiaqian Yu, Jiaqi Wang, Hongfei Du, Parmanand Sahu, Ashwin Assysh Sharma, Fang Jin |  |
| 271 |  |  [Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video Recommendation](https://doi.org/10.1145/3589334.3648159) |  | 0 | Filter bubbles have been studied extensively within the context of online content platforms due to their potential to cause undesirable outcomes such as user dissatisfaction or polarization. With the rise of short-video platforms, the filter bubble has been given extra attention because these platforms rely on an unprecedented use of the recommender system to provide relevant content. In our work, we investigate the deep filter bubble, which refers to the user being exposed to narrow content within their broad interests. We accomplish this using one-year interaction data from a top short-video platform in China, which includes hierarchical data with three levels of categories for each video. We formalize our definition of a "deep" filter bubble within this context, and then explore various correlations within the data: first understanding the evolution of the deep filter bubble over time, and later revealing some of the factors that give rise to this phenomenon, such as specific categories, user demographics, and feedback type. We observe that while the overall proportion of users in a filter bubble remains largely constant over time, the depth composition of their filter bubble changes. In addition, we find that some demographic groups that have a higher likelihood of seeing narrower content and implicit feedback signals can lead to less bubble formation. Finally, we propose some ways in which recommender systems can be designed to reduce the risk of a user getting caught in a bubble. | Nicholas Sukiennik, Chen Gao, Nian Li |  |
| 272 |  |  [Mining Interest Diffusion in Online Activity Data Streams](https://doi.org/10.1145/3589335.3651259) |  | 0 | Modeling and forecasting such data is difficult because online activity data is high-dimensional and composed of multiple time-varying dynamics such as trends, seasonality, and diffusion of interest. In this paper, we propose D-Tracker, designed to capture latent dynamics in online activity data streams and forecast future values. Our proposed method has the following properties: (a) Interpretable: it uses interpretable differential equations to model the latent dynamics in online activity data, which enables us to capture trends and interest diffusion among locations; (b) Automatic: it determines the number of latent dynamics and the number of seasonal patterns fully automatically; (c) Scalable: it incrementally and adaptively detects shifting points of patterns for a semi-infinite collection of tensor streams. (c)Scalable : the computation time of D-Tracker is independent of the time series length. Experiments using web search volume data obtained from GoogleTrends show that the proposed method can achieve higher forecasting accuracy in less computation time than existing methods while extracting the patterns of interest diffusion among locations. | Shingo Higashiguchi |  |
| 273 |  |  [Temporal Interest Network for User Response Prediction](https://doi.org/10.1145/3589335.3648340) |  | 0 | User response prediction is essential in industrial recommendation systems, such as online display advertising. Among all the features in recommendation models, user behaviors are among the most critical. Many works have revealed that a user's behavior reflects her interest in the candidate item, owing to the semantic or temporal correlation between behaviors and the candidate. While the literature has individually examined each of these correlations, researchers have yet to analyze them in combination, that is, the semantic-temporal correlation. We empirically measure this correlation and observe intuitive yet robust patterns. We then examine several popular user interest models and find that, surprisingly, none of them learn such correlation well. To fill this gap, we propose a Temporal Interest Network (TIN) to capture the semantic-temporal correlation simultaneously between behaviors and the target. We achieve this by incorporating target-aware temporal encoding, in addition to semantic encoding, to represent behaviors and the target. Furthermore, we conduct explicit 4-way interaction by deploying target-aware attention and target-aware representation to capture both semantic and temporal correlation. We conduct comprehensive evaluations on two popular public datasets, and our proposed TIN outperforms the best-performing baselines by 0.43 GAUC, respectively. During online A/B testing in Tencent's advertising platform, TIN achieves 1.65 It has been successfully deployed in production since October 2023, serving the WeChat Moments traffic. We have released our code at https://github.com/zhouxy1003/TIN. | Haolin Zhou, Junwei Pan, Xinyi Zhou, Xihua Chen, Jie Jiang, Xiaofeng Gao, Guihai Chen |  |
| 274 |  |  [Practical Batch Bayesian Sampling Algorithms for Online Adaptive Traffic Experimentation](https://doi.org/10.1145/3589335.3648347) |  | 0 | To speed up online testing, adaptive traffic experimentation through multi-armed bandit algorithms is rising as an essential complementary alternative to the fixed horizon A/B testing. Based on recent research on best arm identification and statistical inference with adaptively collected data, this paper derives and evaluates four Bayesian batch bandit algorithms (NB-TS, WB-TS, NB-TTTS, WB-TTTS), which are combinations of two ways of weighting batches (Naive Batch and Weighted Batch) and two Bayesian sampling strategies (Thompson Sampling and Top-Two Thompson Sampling) to adaptively determine traffic allocation. These derived Bayesian sampling algorithms are practically based on summary batch statistics of a reward metric for pilot experiments, where one of the combination WB-TTTS in this paper seems to be newly discussed. The comprehensive evaluation on the four Bayesian sampling algorithms covers trustworthiness, sensitivity and regret of a testing methodology. Moreover, the evaluation includes 4 real-world eBay experiments and 40 reproducible synthetic experiments to reveal the learnings, which covers both stationary and non-stationary situations. Our evaluation reveals that, (a) There exist false positives inflation with equivalent best arms, while seldom discussed in literatures; (b) To control false positives, connections between convergence of posterior optimal probabilities and neutral posterior reshaping are discovered; (c) WB-TTTS shows competitive recall, higher precision, and robustness against non-stationary trend; (d) NB-TS outperforms on minimizing regret trials except on precision and robustness; (e) WB-TTTS is a promising alternative if regret of A/B Testing is affordable, otherwise NB-TS is still a powerful choice with regret consideration for pilot experiments. | Zezhong Zhang, Ted Tao Yuan |  |
| 275 |  |  [Exploring Representational Similarity Analysis to Protect Federated Learning from Data Poisoning](https://doi.org/10.1145/3589335.3651503) |  | 0 | As a paradigm that preserves privacy, Federated Learning (FL) enables distributed clients to cooperatively train global models using local datasets. However, this approach also provides opportunities for adversaries to compromise system stability by contaminating local data, such as through Label-Flipping Attacks (LFAs). In addressing these security challenges, most existing defense strategies presume the presence of an independent and identically distributed (IID) environment, resulting in suboptimal performance under Non-IID conditions. This paper introduces RSim-FL, a novel and pragmatic defense mechanism that incorporates Representational Similarity Analysis (RSA) into the detection of malevolent updates. This is achieved by calculating the similarity between uploaded local models and the global model. The evaluation, conducted against five state-of-the-art baselines, demonstrates that RSim-FL can accurately identify malicious local models and effectively mitigate divergent Label-Flipping Attacks (LFAs) in a Non-IID setting. | Gengxiang Chen, Kai Li, Ahmed M. Abdelmoniem, Linlin You |  |
| 276 |  |  [Online Sampling of Summaries from Public SPARQL Endpoints](https://doi.org/10.1145/3589335.3651543) |  | 0 | Collecting statistics from online public SPARQL endpoints is hampered by their fair usage policies. These restrictions hinder several critical operations, such as aggregate query processing, portal development, and data summarization. Online sampling enables the collection of statistics while respecting fair usage policies. However, sampling has not yet been integrated into the SPARQL standard. Although integrating sampling into the SPARQL standard appears beneficial, its effectiveness must be demonstrated in a practical semantic web context. This paper investigates whether online sampling can generate summaries useful in cutting-edge SPARQL federation engines. Our experimental studies indicate that sampling allows the creation and maintenance of summaries by exploring less than 20% of datasets. | Thi Hoang Thi Pham, Hala SkafMolli, Pascal Molli, Brice Nédelec |  |
| 277 |  |  [Coordinated Activity Modulates the Behavior and Emotions of Organic Users: A Case Study on Tweets about the Gaza Conflict](https://doi.org/10.1145/3589335.3651483) |  | 0 | Social media has become a crucial conduit for the swift dissemination of information during global crises. However, this also paves the way for the manipulation of narratives by malicious actors. This research delves into the interaction dynamics between coordinated (malicious) entities and organic (regular) users on Twitter amidst the Gaza conflict. Through the analysis of approximately 3.5 million tweets from over 1.3 million users, our study uncovers that coordinated users significantly impact the information landscape, successfully disseminating their content across the network: a substantial fraction of their messages is adopted and shared by organic users. Furthermore, the study documents a progressive increase in organic users' engagement with coordinated content, which is paralleled by a discernible shift towards more emotionally polarized expressions in their subsequent communications. These results highlight the critical need for vigilance and a nuanced understanding of information manipulation on social media platforms. | Priyanka Dey, Luca Luceri, Emilio Ferrara |  |
| 278 |  |  [From Files to Streams: Revisiting Web History and Exploring Potentials for Future Prospects](https://doi.org/10.1145/3589335.3652001) |  | 0 | Over the last 30 years, the World Wide Web has changed significantly. In this paper, we argue that common practices to prepare web pages for delivery conflict with many efforts to present content with minimal latency, one fundamental goal that pushed changes in the WWW. To bolster our arguments, we revisit reasons that led to changes of HTTP and compare them systematically with techniques to prepare web pages. We found that the structure of many web pages leverages features of HTTP/1.1 but hinders the use of recent HTTP features to present content quickly. To improve the situation in the future, we propose fine-grained content segmentation. This would allow to exploit streaming capabilities of recent HTTP versions and to render content as quickly as possible without changing underlying protocols or web browsers. | Lucas Vogel, Thomas Springer, Matthias Wählisch |  |
| 279 |  |  [Revisiting the Behavioral Foundations of User Modeling Algorithms](https://doi.org/10.1145/3589334.3649114) |  | 0 | One of the fundamental problems that platform algorithms face is the process of inferring user preferences from observed behavior; the vast amounts of data a platform collects become much less useful if they cannot effectively inform this type of inference. Traditional approaches to this problem rely on an often unstated revealed-preference assumption: that choice reveals preference. Yet a long line of work in psychology and behavioral economics reveals the gaps that can open up between choice and preference, and experience with platform dynamics makes clear how it can arise in some of the most basic online settings; for example, we might choose content to consume in the present and then later regret the time we spent on it. More generally, behavioral biases and inconsistent preferences make it highly challenging to appropriately interpret the user data that we observe. We discuss a set of models and algorithms that address this challenge through a process of "inversion", in which an algorithm must try inferring mental states that are not directly measured in the data. The talk is based on joint work with Jens Ludwig, Sendhil Mullainathan, and Manish Raghavan. | Jon M. Kleinberg |  |
| 280 |  |  [User Response in Ad Auctions: An MDP Formulation of Long-term Revenue Optimization](https://doi.org/10.1145/3589334.3645495) |  | 0 | We propose a new Markov Decision Process (MDP) model for ad auctions to capture the user response to the quality of ads, with the objective of maximizing the long-term discounted revenue. By incorporating user response, our model takes into consideration all three parties involved in the auction (advertiser, auctioneer, and user). The state of the user is modeled as a user-specific click-through rate (CTR) with the CTR changing in the next round according to the set of ads shown to the user in the current round. We characterize the optimal mechanism for this MDP as a Myerson's auction with a notion of modified virtual value, which relies on the value distribution of the advertiser, the current user state, and the future impact of showing the ad to the user. Leveraging this characterization, we design a sample-efficient and computationally-efficient algorithm which outputs an approximately optimal policy that requires only sample access to the true MDP and the value distributions of the bidders. Finally, we propose a simple mechanism built upon second price auctions with personalized reserve prices and show it can achieve a constant-factor approximation to the optimal long term discounted revenue. | Yang Cai, Zhe Feng, Christopher Liaw, Aranyak Mehta, Grigoris Velegkas |  |
| 281 |  |  [Exploring Neural Scaling Law and Data Pruning Methods For Node Classification on Large-scale Graphs](https://doi.org/10.1145/3589334.3645571) |  | 0 | Recently, how the model performance scales with the training sample size has been extensively studied for large models on vision and language related domains. Nevertheless, the ubiquitous node classification tasks on web-scale graphs were ignored, where the traits of these tasks, such as non-IIDness and transductive setting, are likely to cause different scaling laws and motivate novel techniques to beat the law. Therefore, we first explore the neural scaling law for node classification tasks on three large-scale graphs. Then, we benchmark several state-of-the-art data pruning methods on these tasks, not only validating the possibility of improving the original unsatisfactory power law but also gaining insights into a hard-and-representative principle on picking an effective subset of training nodes. Moreover, we leverage the transductive setting to propose a novel data pruning method, which instantiates our principle in a test set-targeted manner. Our method consistently outperforms related methods on all three datasets. Meanwhile, we utilize a PAC-Bayesian framework to analyze our method, extending prior results to account for both hardness and representativeness. In addition to a promising way to ease GNN training on web-scale graphs, our study offers knowledge of the relationship between training nodes and GNN generalization. | Zhen Wang, Yaliang Li, Bolin Ding, Yule Li, Zhewei Wei |  |
| 282 |  |  [Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised Semantic Hashing](https://doi.org/10.1145/3589334.3645440) |  | 0 | Unsupervised semantic hashing has emerged as an indispensable technique for fast image search, which aims to convert images into binary hash codes without relying on labels. Recent advancements in the field demonstrate that employing large-scale backbones (e.g., ViT) in unsupervised semantic hashing models can yield substantial improvements. However, the inference delay has become increasingly difficult to overlook. Knowledge distillation provides a means for practical model compression to alleviate this delay. Nevertheless, the prevailing knowledge distillation approaches are not explicitly designed for semantic hashing. They ignore the unique search paradigm of semantic hashing, the inherent necessities of the distillation process, and the property of hash codes. In this paper, we propose an innovative Bit-mask Robust Contrastive knowledge Distillation (BRCD) method, specifically devised for the distillation of semantic hashing models. To ensure the effectiveness of two kinds of search paradigms in the context of semantic hashing, BRCD first aligns the semantic spaces between the teacher and student models through a contrastive knowledge distillation objective. Additionally, to eliminate noisy augmentations and ensure robust optimization, a cluster-based method within the knowledge distillation process is introduced. Furthermore, through a bit-level analysis, we uncover the presence of redundancy bits resulting from the bit independence property. To mitigate these effects, we introduce a bit mask mechanism in our knowledge distillation objective. Finally, extensive experiments not only showcase the noteworthy performance of our BRCD method in comparison to other knowledge distillation methods but also substantiate the generality of our methods across diverse semantic hashing models and backbones. The code for BRCD is available at https://github.com/hly1998/BRCD. | Liyang He, Zhenya Huang, Jiayu Liu, Enhong Chen, Fei Wang, Jing Sha, Shijin Wang |  |
| 283 |  |  [Perennial Semantic Data Terms of Use for Decentralized Web](https://doi.org/10.1145/3589334.3645631) |  | 0 | In today's digital landscape, the Web has become increasingly centralized, raising concerns about user privacy violations. Decentralized Web architectures, such as Solid, offer a promising solution by empowering users with better control over their data in their personal \`Pods'. However, a significant challenge remains: users must navigate numerous applications to decide which application can be trusted with access to their data Pods. This often involves reading lengthy and complex Terms of Use agreements, a process that users often find daunting or simply ignore. This compromises user autonomy and impedes detection of data misuse. We propose a novel formal description of Data Terms of Use (DToU), along with a DToU reasoner. Users and applications specify their own parts of the DToU policy with local knowledge, covering permissions, requirements, prohibitions and obligations. Automated reasoning verifies compliance, and also derives policies for output data. This constitutes a “perennial” DToU language, where the policy authoring only occurs once, and we can conduct ongoing automated checks across users, applications and activity cycles. Our solution is built on Turtle, Notation 3 and RDF Surfaces, for the language and the reasoning engine. It ensures seamless integration with other semantic tools for enhanced interoperability. We have successfully integrated this language into the Solid framework, and conducted performance benchmark. We believe this work demonstrates a practicality of a perennial DToU language and the potential of a paradigm shift to how users interact with data and applications in a decentralized Web, offering both improved privacy and usability. | Rui Zhao, Jun Zhao |  |
| 284 |  |  [Enhancing Fairness in Meta-learned User Modeling via Adaptive Sampling](https://doi.org/10.1145/3589334.3645369) |  | 0 | Meta-learning has been widely employed to tackle the cold-start problem in user modeling. Similar to a guidebook for a new traveler, meta-learning significantly affects decision-making for new users in crucial scenarios, such as career recommendations. Consequently, the issue of fairness in meta-learning has gained paramount importance. Several methods have been proposed to mitigate unfairness in meta-learning and have shown promising results. However, a fundamental question remains unexplored: What is the critical factor leading to unfairness in meta-learned user modeling? Through the theoretical analysis that integrates the meta-learning paradigm with group fairness metrics, we identify group proportion imbalance as a critical factor. Subsequently, in order to mitigate the impact of this factor, we introduce a novel Fairness-aware Adaptive Sampling framework for meTa-learning, abbreviated as FAST. Its core concept involves adaptively adjusting the sampling distribution for different user groups during the interleaved training process of meta-learning. Furthermore, we provide theoretical guarantees demonstrating the convergence of FAST. Finally, empirical experiments conducted on three datasets reveal that FAST effectively enhances fairness while maintaining high accuracy. The code for FAST is available at https://github.com/zhengz99/FAST. | Zheng Zhang, Qi Liu, Zirui Hu, Yi Zhan, Zhenya Huang, Weibo Gao, Qingyang Mao |  |
| 285 |  |  [MMLSCU: A Dataset for Multi-modal Multi-domain Live Streaming Comment Understanding](https://doi.org/10.1145/3589334.3645677) |  | 0 | With the increasing popularity of live streaming, the interactions from viewers during a live streaming can provide more specific and constructive feedback for both the streamer and platform. In such scenario, the primary and most direct feedback method from the audience is through comments. Thus, mining these live streaming comments to unearth the intentions behind them and, in turn, aiding streamers to enhance their live streaming quality is significant for the well development of live streaming ecosystem. To this end, we introduce the MMLSCU dataset, containing 50,129 intention-annotated comments across multiple modalities (text, images, vi-deos, audio) from eight streaming domains. Using multimodal pretrained large model and drawing inspiration from the Chain of Thoughts (CoT) concept, we implement an end-to-end model to sequentially perform the following tasks: viewer comment intent detection ➛ intent cause mining ➛ viewer comment explanation ➛ streamer policy suggestion. We employ distinct branches for video and audio to process their respective modalities. After obtaining the video and audio representations, we conduct a multimodal fusion with the comment. This integrated data is then fed into the large language model to perform inference across the four tasks following the CoT framework. Experimental results indicate that our model outperforms three multimodal classification baselines on comment intent detection and streamer policy suggestion, and one multimodal generation baselines on intent cause mining and viewer comment explanation. Compared to the models using only text, our multimodal setting yields superior outcomes. Moreover, incorporating CoT allows our model to enhance comment interpretation and more precise suggestions for the streamers. Our proposed dataset and model will bring new research attention on multimodal live streaming comment understanding. | Zixiang Meng, Qiang Gao, Di Guo, Yunlong Li, Bobo Li, Hao Fei, Shengqiong Wu, Fei Li, Chong Teng, Donghong Ji |  |
| 286 |  |  [Entire Chain Uplift Modeling with Context-Enhanced Learning for Intelligent Marketing](https://doi.org/10.1145/3589335.3648320) |  | 0 | Uplift modeling, vital in online marketing, seeks to accurately measure the impact of various strategies, such as coupons or discounts, on different users by predicting the Individual Treatment Effect (ITE). In an e-commerce setting, user behavior follows a defined sequential chain, including impression, click, and conversion. Marketing strategies exert varied uplift effects at each stage within this chain, impacting metrics like click-through and conversion rate. Despite its utility, existing research has neglected to consider the inter-task across all stages impacts within a specific treatment and has insufficiently utilized the treatment information, potentially introducing substantial bias into subsequent marketing decisions. We identify these two issues as the chain-bias problem and the treatment-unadaptive problem. This paper introduces the Entire Chain UPlift method with context-enhanced learning (ECUP), devised to tackle these issues. ECUP consists of two primary components: 1) the Entire Chain-Enhanced Network, which utilizes user behavior patterns to estimate ITE throughout the entire chain space, models the various impacts of treatments on each task, and integrates task prior information to enhance context awareness across all stages, capturing the impact of treatment on different tasks, and 2) the Treatment-Enhanced Network, which facilitates fine-grained treatment modeling through bit-level feature interactions, thereby enabling adaptive feature adjustment. Extensive experiments on public and industrial datasets validate ECUPs effectiveness. Moreover, ECUP has been deployed on the Meituan food delivery platform, serving millions of daily active users, with the related dataset released for future research. | Yinqiu Huang, Shuli Wang, Min Gao, Xue Wei, Changhao Li, Chuan Luo, Yinhua Zhu, Xiong Xiao, Yi Luo |  |
| 287 |  |  [Large Multimodal Model Compression via Iterative Efficient Pruning and Distillation](https://doi.org/10.1145/3589335.3648321) |  | 0 | The deployment of Large Multimodal Models (LMMs) within Ant Group has significantly advanced multimodal tasks in payment, security, and advertising, notably enhancing advertisement audition tasks in Alipay. However, the deployment of such sizable models introduces challenges, particularly in increased latency and carbon emissions, which are antithetical to the ideals of Green AI. This paper introduces a novel multi-stage compression strategy for our proprietary LLM, AntGMM. Our methodology pivots on three main aspects: employing small training sample sizes, addressing multi-level redundancy through multi-stage pruning, and introducing an advanced distillation loss design. In our research, we constructed a dataset, the Multimodal Advertisement Audition Dataset (MAAD), from real-world scenarios within Alipay, and conducted experiments to validate the reliability of our proposed strategy. Furthermore, the effectiveness of our strategy is evident in its operational success in Alipay's real-world multimodal advertisement audition for three months from September 2023. Notably, our approach achieved a substantial reduction in latency, decreasing it from 700ms to 90ms, while maintaining online performance with only a slight performance decrease. Moreover, our compressed model is estimated to reduce electricity consumption by approximately 75 million kWh annually compared to the direct deployment of AntGMM, demonstrating our commitment to green AI initiatives. | Maolin Wang, Yao Zhao, Jiajia Liu, Jingdong Chen, Chenyi Zhuang, Jinjie Gu, Ruocheng Guo, Xiangyu Zhao |  |
| 288 |  |  [Mystique: A Budget Pacing System for Performance Optimization in Online Advertising](https://doi.org/10.1145/3589335.3648342) |  | 0 | Online advertising plays a pivotal role in sustaining the accessibility of free content on the Internet, serving as a primary revenue source for websites and online services. This dynamic marketplace sees advertisers allocating budgets and competing for the opportunity to present ads to users engaging with web pages, online services, and mobile apps. Modern online advertising often employs first-price auctions to determine ad placements. Yet, conducting auctions as isolated events in a greedy manner, may lead to sub-optimal results, necessitating some form of budget pacing. Traditionally, budget pacing has been achieved through hard throttling, where ads or campaigns are selectively made eligible for each auction using a biased coin-toss with a specified probability (or pacing-signal). More recently, the pacing signal has been leveraged to soft throttle ads, and is used as a multiplicative factor on their bids, thus enabling participation in all auctions but with potentially modified bids. In this study, we introduce Mystique, a "soft" throttling-based budget pacing system. Mystique operates on two levels: it utilizes spending data to establish a daily target spending curve for each campaign, and continuously updates a pacing signal to align the actual spending with this curve. Our offline evaluation in a complex simulated marketplace, demonstrates Mystique's ability to outperform several baseline algorithms, enabling budget depletion while securing more opportunities. Mystique has been in production for several years now, serving a major native advertising marketplace, and successfully pacing over one billion USD annually. | Rotem Stram, Rani Abboud, Alex Shtoff, Oren Somekh, Ariel Raviv, Yair Koren |  |
| 289 |  |  [HBIAS FedAvg: Smooth Federated Learning Transition for In-use Edge Models](https://doi.org/10.1145/3589335.3651518) |  | 0 | Federated learning is an approach for privacy preserving machine learning. It is increasingly being used in a number of classification as well as ranking tasks. Protocols for federated learning involve model update at the edge devices and aggregation at the central servers over multiple rounds. In practice, most deep learning models deployed on the edge are already trained and in-use. Federated learning protocols lead to an oscillation in the performance of these local models over the epochs. The drop in accuracy is more prominent in the early phases. In this article, we study such effects for the popular FedAvg federated learning algorithm and suggest the modified HBIAS FedAvg algorithm. The algorithm proposes a heuristic based initialization adoption strategy for this purpose. We find that this protocol leads to smoother performance variation for experiments on benchmark datasets. | Anupam Gupta, Pabitra Mitra, Sudip Misra |  |
| 290 |  |  [The Effect of Alter Ego Accounts on A/B Tests in Social Networks](https://doi.org/10.1145/3589335.3651569) |  | 0 | Social network users often maintain multiple active accounts, sometimes referred to as alter egos. Examples of alter egos include personal and professional accounts or named and anonymous accounts. If alter egos are common on a platform, they can affect the results of A/B testing because a user's alter egos can influence each other. For a single user, one account may be assigned treatment, while another is assigned control. Alter-ego bias is relevant when the treatment affects the individual user rather than the account. Through experimentation and theoretical analysis, we examine the worst and expected case bias for different numbers of alter egos and for a variety of network structures and peer effect strengths. We show that alter egos moderately bias the results of simulated A/B tests on several network structures, including a real-world Facebook subgraph and several types of synthetic networks: small world networks, forest fire networks, stochastic block models, and a worst-case structure. We also show that bias increases with the number of alter egos and that different network structures have different upper bounds on bias. | Katherine Avery, Amir Houmansadr, David D. Jensen | College of Information and Computer Sciences, University of Massachusetts Amherst, Amherst, MA, USA |
| 291 |  |  [CardiO: Predicting Cardinality from Online Sources](https://doi.org/10.1145/3589335.3651477) |  | 0 | Count questions are an important type of information need, though often present in noisy, contradictory, or semantically not fully aligned form on the Web. In this work, we propose CardiO, a lightweight and modular framework for searching entity counts on the Web. CardiO extracts all counts from a set of relevant Web snippets, and infers the most central count based on semantic and numeric distances from other candidates. In the absence of supporting evidence, the system relies on peer sets of similar size, to provide an estimate. Experiments show that CardiO can produce accurate and traceable counts better than small LLM-only methods. Although larger models have higher precision, when used to enhance CardiO components, they do not contribute to the final precision or recall. | Shrestha Ghosh, Simon Razniewski, Damien Graux, Gerhard Weikum |  |
| 292 |  |  [An Identity Alignment Method based on Online Tracking](https://doi.org/10.1145/3589335.3651469) |  | 0 | Companies track user data and sell it to advertisers. They claim to protect user privacy by anonymization, but our research shows that significant risks are still involved. Even with anonymous data, attackers can identify users on other websites from tracking records. We propose an identity alignment method of deanonymization attack, which analyzes tracker data to align identities. We explore the key factors affecting the effectiveness of identity alignment and analyze its impact on user privacy. We use crawling data to create tracker data close to ground-truth scenarios and propose an evaluation framework for online tracking based identity alignment. | Ruisheng Shi, Zhiyuan Peng, Tong Fu, Lina Lan, Jiaqi Zeng, Yuyang Shi, Jinqiao Shi, Shenwen Lin, Lin Li |  |
| 293 |  |  [Semantic interlinking of Immigration Data using LLMs for Knowledge Graph Construction](https://doi.org/10.1145/3589335.3651557) |  | 0 | The challenge of managing immigration data is exacerbated by its reliance on paper-based, evidence-driven records maintained by legal professionals, creating obstacles for efficient processing and analysis due to inherent trust issues with AI-based systems. This paper introduces a cutting-edge framework to surmount these hurdles by synergizing Large Language Models (LLMs) with Knowledge Graphs (KGs), revolutionizing traditional data handling methods. Our method transforms archaic, paper-based immigration records into a structured, interconnected knowledge network that intricately mirrors the legal and procedural nuances of immigration, ensuring a dynamic and trustworthy platform for data analysis. Utilizing LLMs, we extract vital entities and relationships from diverse legal documents to forge a comprehensive knowledge graph, encapsulating the complex legalities and procedural disparities in immigration processes and mapping the multifaceted interactions among stakeholders like applicants, sponsors, and legal experts. This graph not only facilitates a deep dive into the legal stipulations but also incorporates them, significantly boosting the system's reliability and precision. With the integration of Retrieval Augmented Generation (RAG) for exact, context-aware data retrieval and Augmented Knowledge Creation for developing a conversational interface via LLMs, our framework offers a scalable, adaptable solution to immigration data management. This innovative amalgamation of LLMs, KGs, and RAG techniques marks a paradigm shift towards more informed, efficient, and trustworthy decision-making in the sphere of global migration, setting a new benchmark for legal technology and data source management. | Radhakrishnan Venkatakrishnan, Emrah Tanyildizi, M. Abdullah Canbaz |  |
| 294 |  |  [Why Deeper Layers May Introduce More Bias](https://doi.org/10.1145/3589335.3651583) |  | 0 | One of the byproducts of message passing neural networks (MPNNs) is their potential bias towards weakly connected nodes, which can result in degraded performance. This paper confirms that as the number of layers increases, this bias becomes more closely associated with an imbalance in the distribution of eigenvector centrality, known as localization, which further amplifies the discrepancy in label influence on nodes, resulting in a performance gap. Therefore, we explore the effectiveness of non-backtracking centrality and PageRank centrality in mitigating this bias in MPNNs. | Rui Xia, Lisong Wang, Pingping Shi |  |
| 295 |  |  [Shock! Quantifying the Impact of Core Developers' Dropout on the Productivity of OSS Projects](https://doi.org/10.1145/3589335.3651559) |  | 0 | Open Source Software (OSS) projects play a critical role in the digital infrastructure of companies and services provided to millions of people. Given their importance, understanding the resilience of OSS projects is paramount. A primary reason for OSS project failure is the shock caused by the dropout of a core developer, which can jeopardize productivity and project survival. Using a difference-in-differences (DiD) analysis, this study investigates the repercussions of this shock on the productivity of 8,234 developers identified among 9,573 OSS GitHub projects. Our findings reveal the indirect impact of the core developer's dropout. The remaining developers experienced a 20% productivity drop. This observation is troubling because it suggests that the shock might push other developers to drop out, putting the collaboration structure of the project at risk. Also, projects with higher productivity before the shock experienced a larger drop-down after the shock. This points to a tradeoff between productivity and resilience, i.e., the ability of OSS projects to recover from the dropout of a core developer. Our findings underscore the importance of a balanced approach in OSS project management, harmonizing productivity goals with resilience considerations. | Giuseppe Russo Latona, Christoph Gote, Christian Zingg, Giona Casiraghi, Luca Verginer, Frank Schweitzer |  |
| 296 |  |  [Over-Sampling Strategy in Feature Space for Graphs based Class-imbalanced Bot Detection](https://doi.org/10.1145/3589335.3651544) |  | 0 | The presence of a large number of bots in Online Social Networks (OSN) leads to undesirable social effects. Graph neural networks (GNNs) have achieved state-of-the-art performance in bot detection since they can effectively utilize user interaction. In most scenarios, the distribution of bots and humans is imbalanced, resulting in under-represent minority class samples and sub-optimal performance. However, previous GNN-based methods for bot detection seldom consider the impact of class-imbalanced issues. In this paper, we propose an over-sampling strategy for GNN (OS-GNN) that can mitigate the effect of class imbalance in bot detection. Compared with previous over-sampling methods for GNNs, OS-GNN does not call for edge synthesis, eliminating the noise inevitably introduced during the edge construction. Specifically, node features are first mapped to a feature space through neighborhood aggregation and then generated samples for the minority class in the feature space. Finally, the augmented features are fed into GNNs to train the classifiers. This framework is general and can be easily extended into different GNN architectures. The proposed framework is evaluated using three real-world bot detection benchmark datasets, and it consistently exhibits superiority over the baselines. | Shuhao Shi, Kai Qiao, Chen Chen, Jie Yang, Jian Chen, Bin Yan |  |
| 297 |  |  [Dual Graph Networks with Synthetic Oversampling for Imbalanced Rumor Detection on Social Media](https://doi.org/10.1145/3589335.3651494) |  | 0 | Rumor detection is to identify and mitigate potentially damaging falsehoods, thereby shielding the public from misleading information. However, existing methods fall short of tackling class imbalance, meaning rumor is less common than true messages, as they lack specific adaptation for the context of rumor dissemination. In this work, we propose Dual Graph Networks with Synthetic Oversampling (SynDGN), a novel method that can determine whether a claim made on social media is rumor or not in the presence of class imbalance. SynDGN properly utilizes dual graphs to integrate social media contexts and user characteristics to make accurate predictions. Experiments conducted on two well-known datasets verify that SynDGN consistently outperforms state-of-the-art models, regardless of whether the data is balanced or not. | YenWen Lu, ChihYao Chen, ChengTe Li |  |
| 298 |  |  [Travel Demand Prediction with Application to Commuter Demand Estimation on Urban Railways](https://doi.org/10.1145/3589335.3651574) |  | 0 | Travel demand forecasting is a vital problem in the development of smart cities, infrastructure planning, and transportation management. The advent of contactless smart card systems has enabled the collection of data regarding daily transit and purchasing activities, providing a rich source of insights into citizen behavior. In this paper, we introduce a new problem of predicting changes in travel demand resulting from the installation of a new facility while preserving privacy. To address this problem, we propose a simple but effective supervised learning method that can capture the relationships between residential areas and existing facility locations, and exploit spatial features to forecast future demand in response to a new facility location. As a workable example, we employ real-world data to predict the future travel demand triggered by the installation of a new station in a railway system. Through extensive experiments, we demonstrate that our method improves the prediction accuracy. | Yohei Kodama, Yuki Akeyama, Yusuke Miyazaki, Koh Takeuchi |  |
| 299 |  |  [Burstiness-aware Bipartite Graph Neural Networks for Fraudulent User Detection on Rating Platforms](https://doi.org/10.1145/3589335.3651475) |  | 0 | As the digital commerce landscape continues to expand with rating platforms, the consumer base has similarly grown, marking a pivotal reliance on user ratings and reviews. However, the rise of fraudulent users leveraging deceitful conduct, such as manipulation of product rankings, challenges the credibility of these platforms and compels the necessity for an effective detection model. Amid the challenges of evolving fraudulent patterns and label scarcity, this paper presents a novel model, Burstiness-aware Bipartite Graph Neural Networks (BurstBGN), which combats fraud by exploiting user-product bipartite graphs and timestamped rating activities. BurstBGN encapsulates two key ideas: the modeling of user-product interaction via historical rating data through an Edge-time GNN module and the exhaustive mapping of bursty fraudulent user activities. The performance of BurstBGN is demonstrated through rigorous benchmarking against established methods across three datasets. Our results show that BurstBGN consistently outperforms these methods under both transductive and inductive settings, confirming its effectiveness in detecting fraudulent users from limited annotated data, and thereby providing a safeguard for maintaining user trust in e-commerce platforms. | YenWen Lu, YuChe Tsai, ChengTe Li |  |
| 300 |  |  [A Tale of Two Communities: Exploring Academic References on Stack Overflow](https://doi.org/10.1145/3589335.3651464) |  | 0 | Stack Overflow is widely recognized by software practitioners as the go-to resource for addressing technical issues and sharing practical solutions. While it is not typically seen as a forum for scholarly discourse, users on Stack Overflow often refer to academic sources in their discussions. Yet, little is known about these referenced works from the academic community and how they intersect the needs and interests of the Stack Overflow community. To bridge this gap, we conducted a large-scale study on academic references in Stack Overflow. Our findings reveal that Stack Overflow communities with different domains of interest engage with academic literature at varying frequencies and speeds. The contradicting patterns suggest that some disciplines may have diverged in their interests and development trajectories from the corresponding practitioner community. Finally, we discuss the potential of Stack Overflow in gauging the real-world relevance of academic research. | Run Huang, Souti Chattopadhyay |  |
| 301 |  |  [Efficient Location Sampling Algorithms for Road Networks](https://doi.org/10.1145/3589335.3651497) |  | 0 | Many geographic information systems applications rely on data provided by user devices in the road network, including traffic monitoring, driving navigation, and road closure detection. The underlying signal is generally collected by sampling locations from user trajectories. The sampling process, though critical for various applications, has not been studied sufficiently in the literature. While the most natural way to sample a trajectory may be to use a frequency based algorithm, e.g., sampling locations every x seconds, such a sampling strategy can be quite wasteful in resources (e.g., server-side processing, user battery) as well as stored user data. In this work, we conduct a horizontal study of various location sampling algorithms (based on frequency, road geography, reservoir sampling, etc.) and assess their trade-offs in terms of the size of the stored data and the induced quality of training for prediction tasks (specifically predicting speeds on road segments). | Sara Ahmadian, Sreenivas Gollapudi, Kostas Kollias, Vivek Kumar, Ameya Velingker, Santhoshini Velusamy |  |
| 302 |  |  [A Category-agnostic Graph Attention-based Approach for Determining Notability of Articles for Wikipedia](https://doi.org/10.1145/3589335.3651461) |  | 0 | Wikipedia is a highly essential platform because of its informative, dynamic, and easily accessible nature. To identify topics/titles warranting their own Wikipedia article, editors of Wikipedia defined "Notability" guidelines. So far notability is enforced by humans, which makes scalability an issue. There has been no significant work on Notability determination for titles with complex category dependencies. We design a mechanism to identify such titles. We construct a dataset with 9k such titles and propose a category-agnostic approach utilizing Graph neural networks, for their notability determination. Our system outperforms machine learning-based, transformer-based classifiers and entity salience methods. It provides a scalable alternative for notability detection. | Gokul Thota, Vasudeva Varma |  |
| 303 |  |  [Concentration of Power and Participation in Online Governance: the Ecosystem of Decentralized Autonomous Organizations](https://doi.org/10.1145/3589335.3651481) |  | 0 | Blockchain technology enables a new form of online community: Decentralized Autonomous Organizations (DAOs), where members typically vote on proposals using tokens. Enthusiasts claim DAOs provide new opportunities for openness, horizontality, and democratization. However, this phenomenon is still under research, especially given the lack of quantitative studies. This paper presents the first census-like quantitative analysis of the whole ecosystem of DAOs, including 30K DAO communities on the main DAO platforms. This enables us to provide insights into the allegedly "democratic'' nature of DAOs, building metrics concerning their lifespan, participation, and power concentration. Most DAOs have a short lifespan and low participation. There is also a positive correlation between community size and voting power concentration. Like other online communities, DAOs seem to follow the iron law: becoming increasingly oligarchic as they grow. Still, a significant amount of DAOs of varying sizes defy this idea by being egalitarian by design. | Andrea PeñaCalvin, Javier Arroyo, Andrew Schwartz, Samer Hassan |  |
| 304 |  |  ["All of Me": Mining Users' Attributes from their Public Spotify Playlists](https://doi.org/10.1145/3589335.3651459) |  | 0 | In the age of digital music streaming, playlists on platforms like Spotify have become an integral part of individuals' musical experiences. People create and publicly share their own playlists to express their musical tastes, promote the discovery of their favorite artists, and foster social connections. In this work, we aim to address the question: can we infer users' private attributes from their public Spotify playlists? To this end, we conducted an online survey involving 739 Spotify users, resulting in a dataset of 10,286 publicly shared playlists comprising over 200,000 unique songs and 55,000 artists. Then, we utilize statistical analyses and machine learning algorithms to build accurate predictive models for users' attributes. | Pier Paolo Tricomi, Luca Pajola, Luca Pasa, Mauro Conti | Spritz Matter Srl & University of Padova, Padova, Italy; Department of Mathematics, University of Padova & Spritz Matter Srl, Padova, Italy; Department of Mathematics, University of Padova, Padova, Italy |
| 305 |  |  [PyGDebias: A Python Library for Debiasing in Graph Learning](https://doi.org/10.1145/3589335.3651239) |  | 0 | Graph-structured data is ubiquitous among a plethora of real-world applications. However, as graph learning algorithms have been increasingly deployed to help decision-making, there has been rising societal concern in the bias these algorithms may exhibit. In certain high-stake decision-making scenarios, the decisions made may be life-changing for the involved individuals. Accordingly, abundant explorations have been made to mitigate the bias for graph learning algorithms in recent years. However, there still lacks a library to collectively consolidate existing debiasing techniques and help practitioners to easily perform bias mitigation for graph learning algorithms. In this paper, we present PyGDebias, an open-source Python library for bias mitigation in graph learning algorithms. As the first comprehensive library of its kind, PyGDebias covers 13 popular debiasing methods under common fairness notions together with 26 commonly used graph datasets. In addition, PyGDebias also comes with comprehensive performance benchmarks and well-documented API designs for both researchers and practitioners. To foster convenient accessibility, PyGDebias is released under a permissive BSD-license together with performance benchmarks, API documentation, and use examples at https://github.com/yushundong/PyGDebias. | Yushun Dong, Zhenyu Lei, Zaiyi Zheng, Song Wang, Jing Ma, Alex Jing Huang, Chen Chen, Jundong Li |  |
| 306 |  |  [Synslator: An Interactive Machine Translation Tool with Online Learning](https://doi.org/10.1145/3589335.3651240) |  | 0 | Interactive machine translation (IMT) has emerged as a progression of the computer-aided translation paradigm, where the machine translation system and the human translator collaborate to produce high-quality translations. This paper introduces Synslator, a user-friendly computer-aided translation (CAT) tool that not only supports IMT, but is adept at online learning with real-time translation memories. To accommodate various deployment environments for CAT services, Synslator integrates two different neural translation models to handle translation memories for online learning. Additionally, the system employs a language model to enhance the fluency of translations in an interactive mode. In evaluation, we have confirmed the effectiveness of online learning through the translation models, and have observed a 13% increase in post-editing efficiency with the interactive functionalities of Synslator. A tutorial video is available at:https://youtu.be/K0vRsb2lTt8. | Jiayi Wang, Ke Wang, Fengming Zhou, Chengyu Wang, Zhiyong Fu, Zeyu Feng, Yu Zhao, Yuqi Zhang |  |
| 307 |  |  [ACCORD: Constraint-driven Mediation of Multi-user Conflicts in Cloud Services](https://doi.org/10.1145/3589335.3651244) |  | 0 | When multiple users adopt collaborative cloud services like Google Drive to work on a shared resource, incorrect or missing permis- sions may cause conflicting or inconsistent access or use privileges. These issues (or conflicts) compromise resources confidentiality, integrity, or availability leading to a lack of trust in cloud services. An example conflict is when a user with editor permissions changes the permissions on a shared resource without consent from the orig- inal resource owner. In this demonstration, we introduce ACCORD, a web application built on top of Google Drive able to detect and resolve multi-user conflicts. ACCORD employs a simulator to help users preemptively identify potential conflicts and assists them in defining action constraints. Using these constraints, ACCORD can automatically detect and resolve any future conflicts. | Abhiroop Tippavajjula, Primal Pappachan, Anna Cinzia Squicciarini, Jose Such |  |
| 308 |  |  [Fediscount: Shopping Online at a Federated Store Using FedUP as SPARQL Federation Engine](https://doi.org/10.1145/3589335.3651249) |  | 0 | Processing SPARQL queries over large federations of SPARQL endpoints is essential for maintaining the Semantic Web decentralized. However, existing federation engines struggle to query more than a dozen of endpoints. We recently proposed FedUP, a new type of federation engine based on unions-over-joins query plans that outperforms state-of-the-art federation engines by orders of magnitude on large federations. This demonstration paper introduces Fediscount, a federated online shopping application based on the FedShop benchmark, illustrating the capabilities of FedUP. The application is based on standard Semantic Web technologies, enabling end-users to shop online in a virtual federated store comprising 20, 100, or even 200 SPARQL endpoints. This breakthrough opens up promising new avenues for developing and deploying federated applications. | Julien AimonierDavat, Minh Hoang Dang, Pascal Molli, Brice Nédelec, Hala SkafMolli |  |
| 309 |  |  [Online Disinformation and Generative Language Models: Motivations, Challenges, and Mitigations](https://doi.org/10.1145/3589335.3651254) |  | 0 | Disinformation refers to the deliberate dissemination of fake or misleading information, which significantly threatens the modern social stability by undermining trust, intensifying polarization and manipulating public opinion. With the advances of generative AI, the landscape of modern disinformation is changing following the rise of Large Language Models. Recent studies has revealed the capability of generative language models to create convincing and misleading content against the truth and warned the availability of such models to be maliciously abused for deceptive generation. However, AI-driven disinformation is a human-centered societal issue in nature, the realization of which requires not only the in- depth discussion on the latest trends from both sides of generative AI and disinformation, but a critical analysis on the uncertainty of their potential interaction in practice as well. The paper introduces the new vision of AI-driven disinformation campaigns from the perspectives of human-centered AI, proposes a framework of core research questions based on the existing research gap, discusses the preliminary discovery in literature and initial experiments, and elaborates the main lines of research in the future work. | Ziyi Guo |  |
| 310 |  |  [Quantifying Governance of Online Communities at Web Scale](https://doi.org/10.1145/3589335.3651266) |  | 0 | Online communities are powerful tools to connect people and are used worldwide by billions of people. Nearly all online communities rely upon moderators or admins to govern the community in order to mitigate potential harms such as harassment, polarization, and deleterious effects on mental health. However, online communities are complex systems, and studying the impact of community governance empirically at scale is challenging because of the many aspects of community governance and outcomes that must be quantified. In this work, we develop methods to quantify the governance of online communities at web scale. We survey community members to build a comprehensive understanding of what it means to make communities 'better,' then assess existing governance practices and associate them with important outcomes to inform community moderators. We collaborate with communities to deploy our governance interventions to maximize the positive impact of our work, and, at every step of the way, we make our datasets and methods public to support further research on this important topic. | Galen Cassebeer Weld |  |
| 311 |  |  [Tutorial on User Simulation for Evaluating Information Access Systems on the Web](https://doi.org/10.1145/3589335.3641243) |  | 0 | Users routinely interact with the Web via information access systems such as search engines and recommender systems. How to accurately evaluate such interactive systems with reproducible experiments is an important, yet difficult challenge. To address this challenge, user simulation has emerged as a promising solution. This half-day tutorial focuses on providing a thorough introduction to user simulation techniques designed specifically for evaluating information access systems on the Web. We systematically review major research progress, covering both general frameworks for designing user simulators, and specific models and algorithms for simulating user interactions with search engines, recommender systems, and conversational assistants. We also highlight some important future research directions. | Krisztian Balog, ChengXiang Zhai |  |
| 312 |  |  [Archiving and Temporal Analysis of Behavioral Web Data - Tales from the Inside](https://doi.org/10.1145/3589335.3641260) |  | 0 | Behavioral web data such as social web activity streams, query logs or behavioral traces from web search and navigation are crucial to understand the temporal evolution of the web and the human interactions that produce web data and models trained on such data. Thus, behavioral web data empowers research in various fields, such as (temporal) information retrieval, computational social science or cognitive and behavioral modeling of users over time. On the other hand, archiving and using such kind of data is associated with a number of technical and non-technical challenges, e.g. legal and ethical concerns, fast decay of data over time, as well as dependency on 3rd party gatekeepers, such as Twitter/X or Google. We present case studies from many years of research into archiving and temporal analysis of behavioral web data, including large-scale social web archives such as TweetsKB, based on an archive of 14 bn tweets harvested continuously since 2013, and web search and navigation behavior tracked through user studies, longitudinal panels or crowdsourcing-based quasi experiments. | Stefan Dietze |  |
| 313 |  |  [A Case Study Comparing Twitter Communities Detected by the Louvain and Leiden Algorithms During the 2022 War in Ukraine](https://doi.org/10.1145/3589335.3651892) |  | 0 | This paper presents a case study regarding a comparative examination of the Louvain and Leiden community detection algorithms. The case study was conducted on a real-world communication network consisting of 3,222,623 nodes and 27,423,553 edges. In particular, the network in our case study models the communication between Twitter users during the initial four weeks of the 2022 war in Ukraine. In addition, we also applied dynamic topic modeling in order to examine differences in the detected communities. | Karolina Sliwa, Ema Kusen, Mark Strembeck |  |
| 314 |  |  [AI Driven Online Advertising: Market Design, Generative AI, and Ethics](https://doi.org/10.1145/3589335.3641295) |  | 0 | Online advertising contributes a considerable part of the tech sector's revenue, and has been remarkably influencing the public agenda. With evolving developments, AI is playing an increasingly significant role in online advertising. We propose to create a forum for researchers, developers, users, ventures, policymakers, and other stakeholders to exchange ideas, research, innovations, etc. with emphasis on (1) AI driven mechanism design for distributing advertisements, (2) generative AI for creating content in advertisements, such as the promotion images/videos, and (3) ethics issues, especially in political advertisements, such as user privacy, fairness, hating speech, misinformation, etc. Relevant but not mentioned areas are also much encouraged. We plan to organize a half-day workshop. | Fengxiang He, Mengnan Du, Aris FilosRatsikas, Lu Cheng, Qingquan Song, Min Lin, John Vines |  |
| 315 |  |  [Comparative Analysis of Discussion Intensity and Semantic Diversity in Early vs. Late Engagers: A Study of Japanese Tweets about ChatGPT](https://doi.org/10.1145/3589335.3651908) |  | 0 | This study investigates engagement patterns related to OpenAI's ChatGPT on Japanese Twitter, focusing on two distinct user groups - early and late engagers, inspired by the Innovation Theory. Early engagers are defined as individuals who initiated conversations about ChatGPT during its early stages, whereas late engagers are those who began participating at a later date. To examine the nature of the conversations, we conduct a dual methodology, encompassing both quantitative and qualitative analyses. The quantitative analysis reveals that early engagers often engage with more forward-looking and speculative topics, emphasizing the technological advancements and potential transformative impact of ChatGPT. Conversely, the late engagers intereact more with contemporary topics, focusing on the optimization of existing AI capabilities and considering their inherent limitations. Through our qualitative analysis, we propose a method to measure the proportion of shared or unique viewpoints within topics across both groups. We found that early engagers generally concentrate on a more limited range of perspectives, whereas late engagers exhibit a wider range of viewpoints. Interestingly, a weak correlation was found between the volume of tweets and the diversity of discussed topics in both groups. These findings underscore the importance of identifying semantic diversity, rather than relying solely on the volume of tweets, for understanding differences in communication styles between groups within a given topic. Moreover, our versatile dual methodology holds potential for broader applications, such as studying online discourse patterns within different user groups, or in contexts beyond ChatGPT. | Tomoki Fukuma, Koki Noda, Yuta Yamamoto, Takaya Hoshi, Yoshiharu Ichikawa, Kyosuke Kambe, Yu Masubuchi, Fujio Toriumi |  |
| 316 |  |  [Analysis of the Effect between the Information Type on SNSs and User Attributes during Disaster](https://doi.org/10.1145/3589335.3652500) |  | 0 | In the event of a disaster, many people post a lot of information on SNS that promotes or inhibits action we designate this information "behavioral facilitation information". Such information is likely to have various effects on user behavior. A wide variety of people browse SNSs, and different readers perceive the same information in different ways. Therefore, in this study, we focus on users' attributes which are personality traits, age, and gender, and analyze how different users perceive information that promotes behavior. Specifically, we extract behavioral facilitation information from SNSs at the time of a disaster using deep learning, and classify the information into four user's personality traits: "suggestion," "inhibition," "encouragement," and "wish." Then, we conduct an experiment in which subjects classified by user's attributes which are personality traits, age, and gender read and judge how they feel about behavioral facilitation information. We then analyze the results, to determine the relationship between the behavioral facilitation information and the reader's attributes. | Kosuke Wakasugi, Yu Suzuki, Akiyo Nadamoto |  |
| 317 |  |  [Understanding the Impact of COVID-19 on Online Eating Disorder Communities on Reddit](https://doi.org/10.1145/3589335.3652506) |  | 0 | The social restrictions, disruptions in daily activities, and psychological stressors arising from the COVID-19 pandemic constitute a psychological burden for people worldwide, which can be especially detrimental for individuals with mental disorders like Eating Disorders (ED). In this research, we aim to comprehend how COVID-19 has affected individuals with eating disorders through a comparative analysis of data obtained from online communities. We collected data spanning two years before and after the declaration of the pandemic from the subreddits r/AnorexiaNervosa, r/BingeEatingDisorder, and r/EatingDisorders. The research presents multi-faceted tasks where we analyze the content of each of the subreddits by applying a strategy that combines topic modeling, social network analysis, and time series modeling for a better understanding of these communities on both content and network levels. Through a comparative analysis, we address the discussion topic changes based on users' content and determine how COVID-19 leads to changes in communication patterns within the communities. Finally, we implement time series models like ARIMA, Prophet, LSTM, and Transformer on daily posts and comments count to forecast users' activities within the subreddit and establish a performance comparison of these time series models. The findings indicate that both the content of users' discussions and the level of communication and online support-seeking related to eating disorders on Reddit underwent significant changes during the pandemic. The data of this study is available at this GitHub https://github.com/alamincse32/Reddit-Data-for-Eating-Disoder-Community-During-Covid-Pandemic | Md Al Amin, Lu Liu |  |
| 318 |  |  [Statistical Confidence in Mining Power Estimates for PoW Blockchains](https://doi.org/10.1145/3589335.3651960) |  | 0 | The security of blockchain systems depends on the distribution of mining power across participants. If sufficient mining power is controlled by one entity, they can force their own version of events. This may allow them to double spend coins, for example. For Proof of Work (PoW) blockchains, however, the distribution of mining power cannot be read directly from the blockchain and must instead be inferred from the number of blocks mined in a specific sample window. We introduce a framework to quantify this statistical uncertainty for the Nakamoto coefficient, which is a commonly-used measure of blockchain decentralization. We show that aggregating blocks over a day can lead to considerable uncertainty, with Bitcoin failing more than half the hypothesis tests (α = 0.05) when using a daily granularity. For these reasons, we recommend that blocks are aggregated over a sample window of at least 7 days. Instead of reporting a single value, our approach produces a range of possible Nakamoto coefficient values that have statistical support at a particular significance level α. | Mary Milad, Christina Ovezik, Dimitris Karakostas, Daniel W. Woods |  |
| 319 |  |  [From Bollywood Son Preference to Moral Policing on Women in Iran - A 360° View of Gender Bias](https://doi.org/10.1145/3589335.3653010) |  | 0 | Do longitudinal studies reveal a skewed gender distribution among newborn babies depicted in Bollywood movies? Who dominates the speaking time in political conversations on 24x7 news networks in the United States-men or women? How does Twitter discourse on gender equality evolve when a woman dies in police custody in Iran after being arrested (reportedly) due to improper headscarf-wearing? What is the representation of women in divorce court proceedings in India? This broad talk, where cutting-edge AI intersects with social science research questions, encompasses a diverse array of studies that unveil gender bias in various forms. In this presentation, I will describe the substantive findings, social impact, methodological challenges, scope for multimodal investigations, and the novelties entailed in this research. I will conclude the talk with our findings on worrisome gender bias in several large language models. | Ashiqur R. KhudaBukhsh |  |
| 320 |  |  [On Truthful Item-Acquiring Mechanisms for Reward Maximization](https://doi.org/10.1145/3589334.3645345) |  | 0 | In this research, we study the problem that a collector acquires items from the owner based on the item qualities the owner declares and an independent appraiser's assessments. The owner is interested in maximizing the probability that the collector acquires the items and is the only one who knows the items' factual quality. The appraiser performs her duties with impartiality, but her assessment may be subject to random noises, so it may not accurately reflect the factual quality of the items. The main challenge lies in devising mechanisms that prompt the owner to reveal accurate information, thereby optimizing the collector's expected reward. We consider the menu size of mechanisms as a measure of their practicability and study its impact on the attainable expected reward. For the single-item setting, we design optimal mechanisms with a monotone increasing menu size. Although the reward gap between the simplest and optimal mechanisms is bounded, we show that simple mechanisms with a small menu size cannot ensure any positive fraction of the optimal reward of mechanisms with a larger menu size. For the multi-item setting, we show that an ordinal mechanism that only takes the owner's ordering of the items as input is not incentive-compatible. We then propose a set of Union mechanisms that combine single-item mechanisms. Moreover, we run experiments to examine these mechanisms' robustness against the independent appraiser's assessment accuracy and the items' acquiring rate. | Liang Shan, Shuo Zhang, Jie Zhang, Zihe Wang |  |
| 321 |  |  [Barter Exchange with Shared Item Valuations](https://doi.org/10.1145/3589334.3645632) |  | 0 | In barter exchanges agents enter seeking to swap their items for other items on their wishlist. We consider a centralized barter exchange with a set of agents and items where each item has a positive value. The goal is to compute a (re)allocation of items maximizing the agents' collective utility subject to each agent's total received value being comparable to their total given value. Many such centralized barter exchanges exist and serve crucial roles; e.g., kidney exchange programs, which are often formulated as variants of directed cycle packing. We show finding a reallocation where each agent's total given and total received values are equal is NP-hard. On the other hand, we develop a randomized algorithm that achieves optimal utility in expectation and where, i) for any agent, with probability 1 their received value is at least their given value minus v^\* where v^\* is said agent's most valuable owned and wished-for item, and ii) each agent's given and received values are equal in expectation. | Juan Luque, Sharmila Duppala, John P. Dickerson, Aravind Srinivasan |  |
| 322 |  |  [Efficiency of Non-Truthful Auctions in Auto-bidding with Budget Constraints](https://doi.org/10.1145/3589334.3645636) |  | 0 | We study the efficiency of non-truthful auctions for auto-bidders with both return on spend (ROS) and budget constraints. The efficiency of a mechanism is measured by the price of anarchy (PoA), which is the worst case ratio between the liquid welfare of any equilibrium and the optimal (possibly randomized) allocation. Our first main result is that the first-price auction (FPA) is optimal, among deterministic mechanisms, in this setting. Without any assumptions, the PoA of FPA is n which we prove is tight for any deterministic mechanism. However, under a mild assumption that a bidder's value for any query does not exceed their total budget, we show that the PoA is at most 2. This bound is also tight as it matches the optimal PoA without a budget constraint. We next analyze two randomized mechanisms: randomized FPA (rFPA) and "quasi-proportional" FPA. We prove two results that highlight the efficacy of randomization in this setting. First, we show that the PoA of rFPA for two bidders is at most 1.8 without requiring any assumptions. This extends prior work which focused only on an ROS constraint. Second, we show that quasi-proportional FPA has a PoA of 2 for any number of bidders, without any assumptions. Both of these bypass lower bounds in the deterministic setting. Finally, we study the setting where bidders are assumed to bid uniformly. We show that uniform bidding can be detrimental for efficiency in deterministic mechanisms while being beneficial for randomized mechanisms, which is in stark contrast with the settings without budget constraints. | Christopher Liaw, Aranyak Mehta, Wennan Zhu |  |
| 323 |  |  [Individual Welfare Guarantees in the Autobidding World with Machine-learned Advice](https://doi.org/10.1145/3589334.3645660) |  | 0 | Online advertising channels commonly focus on maximizing total advertiser welfare to enhance channel health, and previous literature has studied augmenting ad auctions with machine learning predictions on advertiser values (also known asmachine-learned advice ) to improve total welfare. Yet, such improvements could come at the cost of individual bidders' welfare and do not shed light on how particular advertiser bidding strategies impact welfare. Motivated by this, we present an analysis on an individual bidder's welfare loss in the autobidding world for auctions with and without machine-learned advice, and also uncover how advertiser strategies relate to such losses. In particular, we demonstrate how ad platforms can utilize ML advice to improve welfare guarantee on the aggregate and individual bidder level by setting ML advice as personalized reserve prices when the platform consists ofautobidders who maximize value while respecting a return on ad spend (ROAS) constraint. Under parallel VCG auctions with such ML advice-based reserves, we present a worst-case welfare lower-bound guarantee for an individual autobidder, and show that the lower-bound guarantee is positively correlated with ML advice quality as well as the scale of bids induced by the autobidder's bidding strategies. Further, we show that no truthful, and possibly randomized mechanism with anonymous allocations can achieve universally better individual welfare guarantees than VCG, in the presence of personalized reserves based on ML-advice of equal quality. Moreover, we extend our individual welfare guarantee results to generalized first price (GFP) and generalized second price (GSP) auctions. Finally, we present numerical studies using semi-synthetic data derived from ad auction logs of a search ad platform to showcase improvements in individual welfare when setting personalized reserve prices with ML-advice. | Yuan Deng, Negin Golrezaei, Patrick Jaillet, Jason Cheuk Nam Liang, Vahab Mirrokni |  |
| 324 |  |  [Non-uniform Bid-scaling and Equilibria for Different Auctions: An Empirical Study](https://doi.org/10.1145/3589334.3645659) |  | 0 | In recent years, the growing adoption of autobidding has motivated the study of auction design with value-maximizing auto-bidders. It is known that under mild assumptions, uniform bid-scaling is an optimal bidding strategy in truthful auctions, e.g., Vickrey-Clarke-Groves auction (VCG), and the price of anarchy for VCG is $2$. However, for other auction formats like First-Price Auction (FPA) and Generalized Second-Price auction (GSP), uniform bid-scaling may not be an optimal bidding strategy, and bidders have incentives to deviate to adopt strategies with non-uniform bid-scaling. Moreover, FPA can achieve optimal welfare if restricted to uniform bid-scaling, while its price of anarchy becomes $2$ when non-uniform bid-scaling strategies are allowed. All these price of anarchy results have been focused on welfare approximation in the worst-case scenarios. To complement theoretical understandings, we empirically study how different auction formats (FPA, GSP, VCG) with different levels of non-uniform bid-scaling perform in an autobidding world with a synthetic dataset for auctions. Our empirical findings include: \* For both uniform bid-scaling and non-uniform bid-scaling, FPA is better than GSP and GSP is better than VCG in terms of both welfare and profit; \* A higher level of non-uniform bid-scaling leads to lower welfare performance in both FPA and GSP, while different levels of non-uniform bid-scaling have no effect in VCG. Our methodology of synthetic data generation may be of independent interest. | Yuan Deng, Jieming Mao, Vahab Mirrokni, Yifeng Teng, Song Zuo |  |
| 325 |  |  [A Similarity-based Approach for Efficient Large Quasi-clique Detection](https://doi.org/10.1145/3589334.3645374) |  | 0 | Identifying dense subgraphs called quasi-cliques is pivotal in various graph mining tasks across domains like biology, social networks, and e-commerce. However, recent algorithms still suffer from efficiency issues when mining large quasi-cliques in massive and complex graphs. Our key insight is that vertices within a quasi-clique exhibit similar neighborhoods to some extent. Based on this, we introduce NBSim and FastNBSim, efficient algorithms that find near-maximum quasi-cliques by exploiting vertex neighborhood similarity. FastNBSim further uses MinHash approximations to reduce the time complexity for similarity computation. Empirical evaluation on 10 real-world graphs shows that our algorithms deliver up to three orders of magnitude speedup versus the state-of-the-art algorithms, while ensuring high-quality quasi-clique extraction. | Jiayang Pang, Chenhao Ma, Yixiang Fang |  |
| 326 |  |  [GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning](https://doi.org/10.1145/3589334.3645439) |  | 0 | Graph-structured data is ubiquitous in the world which models complex relationships between objects, enabling various Web applications. Daily influxes of unlabeled graph data on the Web offer immense potential for these applications. Graph self-supervised algorithms have achieved significant success in acquiring generic knowledge from abundant unlabeled graph data. These pre-trained models can be applied to various downstream Web applications, saving training time and improving downstream (target) performance. However, different graphs, even across seemingly similar domains, can differ significantly in terms of attribute semantics, posing difficulties, if not infeasibility, for transferring the pre-trained models to downstream tasks. Concretely speaking, for example, the additional task-specific node information in downstream tasks (specificity) is usually deliberately omitted so that the pre-trained representation (transferability) can be leveraged. The trade-off as such is termed as "transferability-specificity dilemma" in this work. To address this challenge, we introduce an innovative deployment module coined as GraphControl, motivated by ControlNet, to realize better graph domain transfer learning. Specifically, by leveraging universal structural pre-trained models and GraphControl, we align the input space across various graphs and incorporate unique characteristics of target data as conditional inputs. These conditions will be progressively integrated into the model during fine-tuning or prompt tuning through ControlNet, facilitating personalized deployment. Extensive experiments show that our method significantly enhances the adaptability of pre-trained models on target attributed datasets, achieving 1.4-3x performance gain. Furthermore, it outperforms training-from-scratch methods on target data with a comparable margin and exhibits faster convergence. | Yun Zhu, Yaoke Wang, Haizhou Shi, Zhenshuo Zhang, Dian Jiao, Siliang Tang |  |
| 327 |  |  [Rethinking Node-wise Propagation for Large-scale Graph Learning](https://doi.org/10.1145/3589334.3645450) |  | 0 | Scalable graph neural networks (GNNs) have emerged as a promising technique, which exhibits superior predictive performance and high running efficiency across numerous large-scale graph-based web applications. However, (i) Most scalable GNNs tend to treat all nodes in graphs with the same propagation rules, neglecting their topological uniqueness; (ii) Existing node-wise propagation optimization strategies are insufficient on web-scale graphs with intricate topology, where a full portrayal of nodes' local properties is required. Intuitively, different nodes in web-scale graphs possess distinct topological roles, and therefore propagating them indiscriminately or neglect local contexts may compromise the quality of node representations. This intricate topology in web-scale graphs cannot be matched by small-scale scenarios. To address the above issues, we propose Adaptive Topology-aware Propagation (ATP), which reduces potential high-bias propagation and extracts structural patterns of each node in a scalable manner to improve running efficiency and predictive performance. Remarkably, ATP is crafted to be a plug-and-play node-wise propagation optimization strategy, allowing for offline execution independent of the graph learning process in a new perspective. Therefore, this approach can be seamlessly integrated into most scalable GNNs while remain orthogonal to existing node-wise propagation optimization strategies. Extensive experiments on 12 datasets, including the most representative large-scale ogbn-papers100M, have demonstrated the effectiveness of ATP. Specifically, ATP has proven to be efficient in improving the performance of prevalent scalable GNNs for semi-supervised node classification while addressing redundant computational costs. | Xunkai Li, Jingyuan Ma, Zhengyu Wu, Daohan Su, Wentao Zhang, RongHua Li, Guoren Wang |  |
| 328 |  |  [Collaborate to Adapt: Source-Free Graph Domain Adaptation via Bi-directional Adaptation](https://doi.org/10.1145/3589334.3645507) |  | 0 | Unsupervised Graph Domain Adaptation (UGDA) has emerged as a practical solution to transfer knowledge from a label-rich source graph to a completely unlabelled target graph. However, most methods require a labelled source graph to provide supervision signals, which might not be accessible in the real-world settings due to regulations and privacy concerns. In this paper, we explore the scenario of source-free unsupervised graph domain adaptation, which tries to address the domain adaptation problem without accessing the labelled source graph. Specifically, we present a novel paradigm called GraphCTA, which performs model adaptation and graph adaptation collaboratively through a series of procedures: (1) conduct model adaptation based on node's neighborhood predictions in target graph considering both local and global information; (2) perform graph adaptation by updating graph structure and node attributes via neighborhood contrastive learning; and (3) the updated graph serves as an input to facilitate the subsequent iteration of model adaptation, thereby establishing a collaborative loop between model adaptation and graph adaptation. Comprehensive experiments are conducted on various public datasets. The experimental results demonstrate that our proposed model outperforms recent source-free baselines by large margins. | Zhen Zhang, Meihan Liu, Anhui Wang, Hongyang Chen, Zhao Li, Jiajun Bu, Bingsheng He |  |
| 329 |  |  [Graph Contrastive Learning Reimagined: Exploring Universality](https://doi.org/10.1145/3589334.3645480) |  | 0 | Real-world graphs exhibit diverse structures, including homophilic and heterophilic patterns, necessitating the development of a universal Graph Contrastive Learning (GCL) framework. Nonetheless, the existing GCLs, especially those with a local focus, lack universality due to the mismatch between the input graph structure and the homophily assumption for two primary components of GCLs. Firstly, the encoder, commonly Graph Convolution Network (GCN), operates as a low-pass filter, which assumes the input graph to be homophilic. This makes it challenging to aggregate features from neighbor nodes of the same class on heterophilic graphs. Secondly, the local positive sampling regards neighbor nodes as positive samples, which is inspired by the homophily assumption. This results in feature similarity amplification for the samples from the different classes (i.e., FALSE positive samples). Therefore, it is crucial to feed the encoder and positive sampling of GCLs with homophilic graph structures. This paper presents a novel GCL framework, named gRaph cOntraStive Exploring uNiversality (ROSEN), designed to achieve this objective. Specifically, ROSEN equips a local graph structure inference module, utilizing the Block Diagonal Property (BDP) of the affinity matrix extracted from node ego networks. This module can generate the homophilic graph structure by selectively removing disassortative edges. Extensive evaluations validate the effectiveness and universality of ROSEN across node classification and node clustering tasks. | Jiaming Zhuo, Can Cui, Kun Fu, Bingxin Niu, Dongxiao He, Chuan Wang, Yuanfang Guo, Zhen Wang, Xiaochun Cao, Liang Yang |  |
| 330 |  |  [MuGSI: Distilling GNNs with Multi-Granularity Structural Information for Graph Classification](https://doi.org/10.1145/3589334.3645542) |  | 0 | Recent works have introduced GNN-to-MLP knowledge distillation (KD) frameworks to combine both GNN's superior performance and MLP's fast inference speed. However, existing KD frameworks are primarily designed for node classification within single graphs, leaving their applicability to graph classification largely unexplored. Two main challenges arise when extending KD for node classification to graph classification: (1) The inherent sparsity of learning signals due to soft labels being generated at the graph level; (2) The limited expressiveness of student MLPs, especially in datasets with limited input feature spaces. To overcome these challenges, we introduce MuGSI, a novel KD framework that employs Multi-granularity Structural Information for graph classification. Specifically, we propose multi-granularity distillation loss in MuGSI to tackle the first challenge. This loss function is composed of three distinct components: graph-level distillation, subgraph-level distillation, and node-level distillation. Each component targets a specific granularity of the graph structure, ensuring a comprehensive transfer of structural knowledge from the teacher model to the student model. To tackle the second challenge, MuGSI proposes to incorporate a node feature augmentation component, thereby enhancing the expressiveness of the student MLPs and making them more capable learners. We perform extensive experiments across a variety of datasets and different teacher/student model architectures. The experiment results demonstrate the effectiveness, efficiency, and robustness of MuGSI. Codes are publicly available at: <https://github.com/tianyao-aka/MuGSI>. | Tianjun Yao, Jiaqi Sun, Defu Cao, Kun Zhang, Guangyi Chen |  |
| 331 |  |  [Efficient Computation for Diagonal of Forest Matrix via Variance-Reduced Forest Sampling](https://doi.org/10.1145/3589334.3645578) |  | 0 | The forest matrix of a graph, particularly its diagonal elements, has far-reaching implications in network science and machine learning. The state-of-the-art algorithms for the diagonal of forest matrix computation are based on the fast Laplacian solver. However, these algorithms encounter limitations when applied to digraphs due to the incapacity of the Laplacian solver. To overcome the issue, in this paper, we propose three novel sampling-based algorithms:SCF,SCFV,and SCFV+. Our first algorithm SCF leverages a probability interpretation of the diagonal of the forest matrix and utilizes an extension of Wilson's algorithm to sample spanning converging forests. To reduce the variance in the forest sampling, we develop two novel variance-reduced techniques. The first technique, leading to the proposal of the SCFV algorithm, is inspired by opinion dynamics in graphs and applies matrix-vector iteration to the spanning forest sampling. While SCFV achieves reduced variance compared to SCF, the cross-product term in its variance expression can be complex and potentially large in certain graphs. Therefore, we develop another technique, leading to a new iteration equation and the SCFV+ algorithm. SCFV+ achieves further reduced variance without the cross-product term in the variance of SCFV. We prove that SCFV+ can achieve a relative error guarantee with high probability and maintain a linear time complexity relative to the number of nodes in the graph, presenting a superior theoretical result compared to state-of-the-art algorithms. Finally, we conduct extensive experiments on various real-world networks, showing that our algorithms achieve better estimation accuracy and are more time-efficient than the state-of-the-art algorithms. Particularly, our algorithms are scalable to massive graphs with more than twenty million nodes in both undirected and directed graphs. | Haoxin Sun, Zhongzhi Zhang |  |
| 332 |  |  [Decoupled Variational Graph Autoencoder for Link Prediction](https://doi.org/10.1145/3589334.3645601) |  | 0 | Link prediction is an important learning task for graph-structured data, and has become increasingly popular due to its wide application areas. Graph Neural Network (GNN)-based approaches including Variational Graph Autoencoder (VGAE) have achieved promising performance on link prediction outperforming conventional models which use hand-crafted features. VGAE learns latent node representations and predicts links based on the similarities between nodes. While the inner product based decoder effectively utilizes the node representations for link prediction, it exhibits sub-optimal performance due to the intrinsic limitation of the inner product. We found that the the cosine similarity and norm simultaneously try to explain the link probability, which hinders the gradient flow during training. We also point out the message passing scheme is unexpectedly dominated by the nodes with large norm values. In this paper, we propose a stochastic VGAE-based method that can effectively decouple the norm and angle in the embeddings. Specifically, we relate the cosine similarity and norm to two fundamental principles in graph: homophily and node popularity respectively. Our learning scheme is based on a hard expectation maximization learning method; we infer which of the two has been exerted for link formation, and subsequently optimize based on this guess. Through extensive experiments on real-world datasets, we demonstrate our model outperforms the existing state-of-the-art methods on link prediction and achieves comparable performances on other downstream tasks such as node classification and clustering. Our code is at https://github.com/yoonsikcho/d-vgae. | YoonSik Cho |  |
| 333 |  |  [ModelGo: A Practical Tool for Machine Learning License Analysis](https://doi.org/10.1145/3589334.3645520) |  | 0 | Productionizing machine learning projects is inherently complex, involving a multitude of interconnected components that are assembled like LEGO blocks and evolve throughout development lifecycle. These components encompass software, databases, and models, each subject to various licenses governing their reuse and redistribution. However, existing license analysis approaches for Open Source Software (OSS) are not well-suited for this context. For instance, some projects are licensed without explicitly granting sublicensing rights, or the granted rights can be revoked, potentially exposing their derivatives to legal risks. Indeed, the analysis of licenses in machine learning projects grows significantly more intricate as it involves interactions among diverse types of licenses and licensed materials. To the best of our knowledge, no prior research has delved into the exploration of license conflicts within this domain. In this paper, we introduce ModelGo, a practical tool for auditing potential legal risks in machine learning projects to enhance compliance and fairness. With ModelGo, we present license assessment reports based on five use cases with diverse model-reusing scenarios, rendered by real-world machine learning components. Finally, we summarize the reasons behind license conflicts and provide guidelines for minimizing them. Our code is publicly available at https://github.com/Xtra-Computing/ModelGo. | Moming Duan, Qinbin Li, Bingsheng He |  |
| 334 |  |  [Content Moderation and the Formation of Online Communities: A Theoretical Framework](https://doi.org/10.1145/3589334.3645490) |  | 0 | We study the impact of content moderation policies in online communities. In our theoretical model, a platform chooses a content moderation policy and individuals choose whether or not to participate in the community according to the fraction of user content that aligns with their preferences. The effects of content moderation, at first blush, might seem obvious: it restricts speech on a platform. However, when user participation decisions are taken into account, its effects can be more subtle $\unicode{x2013}$ and counter-intuitive. For example, our model can straightforwardly demonstrate how moderation policies may increase participation and diversify content available on the platform. In our analysis, we explore a rich set of interconnected phenomena related to content moderation in online communities. We first characterize the effectiveness of a natural class of moderation policies for creating and sustaining stable communities. Building on this, we explore how resource-limited or ideological platforms might set policies, how communities are affected by differing levels of personalization, and competition between platforms. Our model provides a vocabulary and mathematically tractable framework for analyzing platform decisions about content moderation. | Cynthia Dwork, Chris Hays, Jon M. Kleinberg, Manish Raghavan |  |
| 335 |  |  [Getting Bored of Cyberwar: Exploring the Role of Low-level Cybercrime Actors in the Russia-Ukraine Conflict](https://doi.org/10.1145/3589334.3645401) |  | 0 | There has been substantial commentary on the role of cyberattacks carried by low-level cybercrime actors in the Russia-Ukraine conflict. We analyse 358k web defacement attacks, 1.7M reflected DDoS attacks, 1764 Hack Forums posts mentioning the two countries, and 441 announcements (with 58k replies) of a volunteer hacking group for two months before and four months after the invasion. We find the conflict briefly but notably caught the attention of low-level cybercrime actors, with significant increases in online discussion and both types of attack targeting Russia and Ukraine. However, there was little evidence of high-profile actions; the role of these players in the ongoing hybrid warfare is minor, and they should be separated from persistent and motivated 'hacktivists' in state-sponsored operations. Their involvement in the conflict appears to have been short-lived and fleeting, with a clear loss of interest in discussing the situation and carrying out both defacement and DDoS attacks against either Russia or Ukraine after a few weeks. | Anh V. Vu, Daniel R. Thomas, Ben Collier, Alice Hutchings, Richard Clayton, Ross J. Anderson |  |
| 336 |  |  [The Double Edged Sword: Identifying Authentication Pages and their Fingerprinting Behavior](https://doi.org/10.1145/3589334.3645493) |  | 0 | Browser fingerprinting is often associated with cross-site user tracking, a practice that many browsers (e.g., Safari, Brave, Edge, Firefox, and Chrome) want to block. However, less is publicly known about its uses to enhance online safety, where it can provide an additional security layer against service abuses (e.g., in combination with CAPTCHAs) or during user authentication. To the best of our knowledge, no fingerprinting defenses deployed thus far consider this important distinction when blocking fingerprinting attempts, so they might negatively affect website functionality and security. To address this issue we make three main contributions. First, we introduce a novel machine learning-based method to automatically identify authentication pages (i.e. login and sign-up pages). Our supervised algorithm achieves 96-98% precision and recall on a manually-labelled dataset of almost 1,000 popular sites. Second, we compare our algorithm with methods from prior works on the same dataset, showing that it significantly outperforms all of them. Third, we quantify the prevalence of fingerprinting scripts across login and sign-up pages (10.2%) versus those executed on other pages (9.2%); while the rates of fingerprinting are similar, home pages and authentication pages differ in the third-party scripts they include and how often these scripts are labeled as tracking. We also highlight the substantial differences in fingerprinting on login and sign-up pages. Our work sheds light on the complicated reality that fingerprinting is used to both protect user security and invade user privacy; this dual nature must be considered by fingerprinting mitigations. | Asuman Senol, Alisha Ukani, Dylan Cutler, Igor Bilogrevic |  |
| 337 |  |  ["Are Adversarial Phishing Webpages a Threat in Reality?" Understanding the Users' Perception of Adversarial Webpages](https://doi.org/10.1145/3589334.3645502) |  | 0 | Machine learning based phishing website detectors (ML-PWD) are a critical part of today's anti-phishing solutions in operation. Unfortunately, ML-PWD are prone to adversarial evasions, evidenced by both academic studies and analyses of real-world adversarial phishing webpages. However, existing works mostly focused on assessing adversarial phishing webpages against ML-PWD, while neglecting a crucial aspect: investigating whether they can deceive the actual target of phishing – the end users. In this paper, we fill this gap by conducting two user studies (n=470) to examine how human users perceive adversarial phishing webpages, spanning both synthetically crafted ones (which we create by evading a state-of-the-art ML-PWD) as well as real adversarial webpages (taken from the wild Web) that bypassed a production-grade ML-PWD. Our findings confirm that adversarial phishing is a threat to both users and ML-PWD, since most adversarial phishing webpages have comparable effectiveness on users w.r.t. unperturbed ones. However, not all adversarial perturbations are equally effective. For example, those with added typos are significantly more noticeable to users, who tend to overlook perturbations of higher visual magnitude (such as replacing the background). We also show that users' self-reported frequency of visiting a brand's website has a statistically negative correlation with their phishing detection accuracy, which is likely caused by overconfidence. We release our resources. | Ying Yuan, Qingying Hao, Giovanni Apruzzese, Mauro Conti, Gang Wang |  |
| 338 |  |  [Hyperlink Hijacking: Exploiting Erroneous URL Links to Phantom Domains](https://doi.org/10.1145/3589334.3645510) |  | 0 | Web users often follow hyperlinks hastily, expecting them to be correctly programmed. However, it is possible those links contain typos or other mistakes. By discovering active but erroneous hyperlinks, a malicious actor can spoof a website or service, impersonating the expected content and phishing private information. In 'typosquatting,' misspellings of common domains are registered to exploit errors when users mistype a web address. Yet, no prior research has been dedicated to situations where the linking errors of web publishers (i.e. developers and content contributors) propagate to users. We hypothesize that these 'hijackable hyperlinks' exist in large quantities with the potential to generate substantial traffic. Analyzing large-scale crawls of the web using high-performance computing, we show the web currently contains active links to more than 572,000 dot-com domains that have never been registered, what we term 'phantom domains.' Registering 51 of these, we see 88% of phantom domains exceeding the traffic of a control domain, with up to 10 times more visits. Our analysis shows that these links exist due to 17 common publisher error modes, with the phantom domains they point to free for anyone to purchase and exploit for under 20, representing a low barrier to entry for potential attackers. | Kevin Saric, Felix Savins, Gowri Sankar Ramachandran, Raja Jurdak, Surya Nepal |  |
| 339 |  |  [Fake Resume Attacks: Data Poisoning on Online Job Platforms](https://doi.org/10.1145/3589334.3645524) |  | 0 | While recent studies have exposed various vulnerabilities incurred from data poisoning attacks in many web services, little is known about the vulnerability on online professional job platforms (e.g., LinkedIn and Indeed). In this work, first time, we demonstrate the critical vulnerabilities found in the common Human Resources (HR) task of matching job seekers and companies on online job platforms. Capitalizing on the unrestricted format and contents of job seekers' resumes and easy creation of accounts on job platforms, we demonstrate three attack scenarios: (1) company promotion attack to increase the likelihood of target companies being recommended, (2) company demotion attack to decrease the likelihood of target companies being recommended, and (3) user promotion attack to increase the likelihood of certain users being matched to certain companies. To this end, we develop an end-to-end "fake resume" generation framework, titled FRANCIS, that induces systematic prediction errors via data poisoning. Our empirical evaluation on real-world datasets reveals that data poisoning attacks can markedly skew the results of matchmaking between job seekers and companies, regardless of underlying models, with vulnerability amplified in proportion to poisoning intensity. These findings suggest that the outputs of various services from job platforms can be potentially hacked by malicious users. | Michiharu Yamashita, Thanh Tran, Dongwon Lee |  |
| 340 |  |  [Identifying VPN Servers through Graph-Represented Behaviors](https://doi.org/10.1145/3589334.3645552) |  | 0 | Identifying VPN servers is a crucial task in various situations, such as geo-fraud detection, bot traffic analysis and network attack identification. Although numerous studies that focus on network traffic detection have achieved excellent performance in closed-world scenarios, particularly those methods based on deep learning, they may exhibit significant performance degradation due to changes in network environment. To mitigate this issue, a few studies have attempted to use methods based on active probing to detect VPN servers. However, these methods still have two limitations. They cannot handle situations without probing responses and are limited in applicability due to their focus on specific VPNs. In this work, we propose VPNChecker, which utilizes the graph-represented behaviors to detect VPN servers in real-world scenarios. VPNChecker outperforms existing methods in four offline datasets. The results from our datasets, containing multiple different VPNs, indicate that VPNChecker has better applicability. Furthermore, we deploy VPNChecker in an Internet Service Provider's (ISP) environment to evaluate its effectiveness. The results show that VPNChecker can improve the coverage of sophisticated detection engines and serve as a complement to existing methods. | Chenxu Wang, Jiangyi Yin, Zhao Li, Hongbo Xu, Zhongyi Zhang, Qingyun Liu |  |
| 341 |  |  [Discovering and Measuring CDNs Prone to Domain Fronting](https://doi.org/10.1145/3589334.3645656) |  | 0 | Domain fronting is a network communication technique that involves leveraging (or abusing) content delivery networks (CDNs) to disguise the final destination of network packets by presenting them as if they were intended for a different domain than their actual endpoint. This technique can be used for both benign and malicious purposes, such as circumventing censorship or hiding malware-related communications from network security systems. Since domain fronting has been known for a few years, some popular CDN providers have implemented traffic filtering approaches to curb its use at their CDN infrastructure. However, it remains unclear to what extent domain fronting has been mitigated. To better understand whether domain fronting can still be effectively used, we propose a systematic approach to discover CDNs that are still prone to domain fronting. To this end, we leverage passive and active DNS traffic analysis to pinpoint domain names served by CDNs and build an automated tool that can be used to discover CDNs that allow domain fronting in their infrastructure. Our results reveal that domain fronting is feasible in 22 out of 30 CDNs that we tested, including some major CDN providers like Akamai and Fastly. This indicates that domain fronting remains widely available and can be easily abused for malicious purposes. | Karthika Subramani, Roberto Perdisci, PierrosChristos Skafidas, Manos Antonakakis |  |
| 342 |  |  [Exploring Unconfirmed Transactions for Effective Bitcoin Address Clustering](https://doi.org/10.1145/3589334.3645684) |  | 0 | The development of clustering heuristics has demonstrated that Bitcoin is not completely anonymous. Currently, existing clustering heuristics only consider confirmed transactions recorded in the Bitcoin blockchain. However, unconfirmed transactions in the mempool have yet to be utilized to improve the performance of the clustering heuristics. In this paper, we bridge this gap by combining unconfirmed and confirmed transactions for clustering Bitcoin addresses effectively. First, we present a data collection system for capturing unconfirmed transactions. Two case studies are performed to show the presence of user behaviors in unconfirmed transactions not present in confirmed transactions. Next, we apply the state-of-the-art clustering heuristics to unconfirmed transactions, and the clustering results can reduce the number of entities after applying, for example, the co-spend heuristics in confirmed transactions by 2.3%. Finally, we propose three novel clustering heuristics to capture specific behavior patterns in unconfirmed transactions, which further reduce the number of entities after the application of the co-spend heuristics by 9.8%. Our results demonstrate the utility of unconfirmed transactions in address clustering and further shed light on the limitations of anonymity in cryptocurrencies. To the best of our knowledge, this paper is the first to apply the unconfirmed transactions in Bitcoin to cluster addresses. | Kai Wang, Yakun Cheng, Michael Wen Tong, Zhenghao Niu, Jun Pang, Weili Han |  |
| 343 |  |  [AdFlush: A Real-World Deployable Machine Learning Solution for Effective Advertisement and Web Tracker Prevention](https://doi.org/10.1145/3589334.3645698) |  | 0 | Conventional ad blocking and tracking prevention tools often fall short in addressing web content manipulation. Machine learning approaches have been proposed to enhance detection accuracy, yet aspects of practical deployment have frequently been overlooked. This paper introduces AdFlush, a novel machine learning model for real-world browsers. To develop AdFlush, we evaluated the effectiveness of 883 features, ultimately selecting 27 key features for optimal performance. We tested AdFlush on a dataset of 10,000 real-world websites, achieving an F1 score of 0.98, thereby outperforming AdGraph (F1 score: 0.93), WebGraph (F1 score: 0.90), and WTAgraph (F1 score: 0.84). Additionally, AdFlush significantly reduces computational overhead, requiring 56% less CPU and 80% less memory than AdGraph. We also assessed AdFlush's robustness against adversarial manipulations, demonstrating superior resilience with F1 scores ranging from 0.89 to 0.98, surpassing the performance of AdGraph and WebGraph, which recorded F1 scores between 0.81 and 0.87. A six-month longitudinal study confirmed that AdFlush maintains a high F1 score above 0.97 without the need for retraining, underscoring its effectiveness. | Kiho Lee, Chaejin Lim, Beomjin Jin, Taeyoung Kim, Hyoungshick Kim |  |
| 344 |  |  [Fingerprinting the Shadows: Unmasking Malicious Servers with Machine Learning-Powered TLS Analysis](https://doi.org/10.1145/3589334.3645719) |  | 0 | Over the last few years, the adoption of encryption in network traffic has been constantly increasing. The percentage of encrypted communications worldwide is estimated to exceed 90%. Although network encryption protocols mainly aim to secure and protect users' online activities and communications, they have been exploited by malicious entities that hide their presence in the network. It was estimated that in 2022, more than 85% of the malware used encrypted communication channels. In this work, we examine state-of-the-art fingerprinting techniques and extend a machine learning pipeline for effective and practical server classification. Specifically, we actively contact servers to initiate communication over the TLS protocol and through exhaustive requests, we extract communication metadata. We investigate which features favor an effective classification, following state-of-the-art approaches. Our extended pipeline can indicate whether a server is malicious or not with 91% precision and 95% recall, while it can specify the botnet family with 99% precision and 99% recall. | Andreas Theofanous, Eva Papadogiannaki, Alexander Shevtsov, Sotiris Ioannidis |  |
| 345 |  |  [Efficient Computation of Signature-Restricted Views for Semantic Web Ontologies](https://doi.org/10.1145/3589334.3645317) |  | 0 | Uniform Interpolation (UI) is an advanced reasoning service used to narrow down an ontology to a restricted view. This new ontology, known as a uniform interpolant, will only consist of the ''relevant names'', yet it will retain their original meanings. UI is immensely promising due to its applicability across various domains where custom views of ontologies are essential. Nonetheless, to unlock its full potential, we need optimized techniques to generate these tailored views. Previous studies suggest that creating uniform interpolants for EL-ontologies is notably challenging. In some instances, it is not even feasible to compute a uniform interpolant; when feasible, the size of the uniform interpolant can be up to triple exponentially larger than the source ontology. Despite these challenges, our paper introduces an improved ''forgetting'' technique specifically designed for computing uniform interpolants of ELI-ontologies. We demonstrate that, with good normalization and inference strategies, such uniform interpolants can be efficiently computed, just as quickly as computing ''modules''. A comprehensive evaluation with a prototypical implementation of the method shows superb success rates over two popular benchmark datasets, demonstrating a clear computational advantage over state-of-the-art approaches. | Yizheng Zhao |  |
| 346 |  |  [Follow the Path: Hierarchy-Aware Extreme Multi-Label Completion for Semantic Text Tagging](https://doi.org/10.1145/3589334.3645558) |  | 0 | Extreme Multi Label (XML) problems, and in particular XML completion -- the task of prediction the missing labels of an entity -- have attracted significant attention in the past few years. Most XML completion problems can organically leverage a label hierarchy, which can be represented as a tree that encodes the relations between the different labels. In this paper, we propose a new algorithm, HECTOR - Hierarchical Extreme Completion for Text based on TransfORmer, to solve XML Completion problems more effectively. HECTOR operates by directly predicting paths in the label tree rather than individual labels, thus taking advantage of information encoded in the hierarchy. Due to the sequential aspect of these paths, HECTOR can leverage the effectiveness and performance of the Transformer architecture to outperform state-of-the-art of XML completion methods. Extensive evaluations on three real-world datasets demonstrate the effectiveness of our approach for XML completion. We compare HECTOR with several state-of-the-art XML completion methods for various completion problems, and in particular for label refinement, i.e., the scenario where only the coarse labels (i.e. the first few top levels in a taxonomy) are observed. Empirical results on three different datasets show that our method significantly outperforms the state of the art, with HECTOR frequently outperforming previous techniques by more than 10% according to multiple metrics. | Natalia Ostapuk, Julien Audiffren, Ljiljana Dolamic, Alain Mermoud, Philippe CudréMauroux |  |
| 347 |  |  [Multi-Label Zero-Shot Product Attribute-Value Extraction](https://doi.org/10.1145/3589334.3645649) |  | 0 | E-commerce platforms should provide detailed product descriptions (attribute values) for effective product search and recommendation. However, attribute value information is typically not available for new products. To predict unseen attribute values, large quantities of labeled training data are needed to train a traditional supervised learning model. Typically, it is difficult, time-consuming, and costly to manually label large quantities of new product profiles. In this paper, we propose a novel method to efficiently and effectively extract unseen attribute values from new products in the absence of labeled data (zero-shot setting). We propose HyperPAVE, a multi-label zero-shot attribute value extraction model that leverages inductive inference in heterogeneous hypergraphs. In particular, our proposed technique constructs heterogeneous hypergraphs to capture complex higher-order relations (i.e. user behavior information) to learn more accurate feature representations for graph nodes. Furthermore, our proposed HyperPAVE model uses an inductive link prediction mechanism to infer future connections between unseen nodes. This enables HyperPAVE to identify new attribute values without the need for labeled training data. We conduct extensive experiments with ablation studies on different categories of the MAVE dataset. The results demonstrate that our proposed HyperPAVE model significantly outperforms existing classification-based, generation-based large language models for attribute value extraction in the zero-shot setting. | Jiaying Gong, Hoda Eldardiry |  |
| 348 |  |  [Aligning Out-of-Distribution Web Images and Caption Semantics via Evidential Learning](https://doi.org/10.1145/3589334.3645653) |  | 0 | Vision-language models, pre-trained on web-scale datasets, have the potential to greatly enhance the intelligence of web applications (e.g., search engines, chatbots, and art tools). Precisely, these models align disparate domains into a co-embedding space, achieving impressive zero-shot performance on multi-modal tasks (e.g., image-text retrieval, VQA). However, existing methods often rely on well-prepared data that less frequently contain noise and variability encountered in real-world scenarios, leading to severe performance drops in handling out-of-distribution (OOD) samples. This work first comprehensively analyzes the performance drop between in-distribution (ID) and OOD retrieval. Based on empirical observations, we introduce a novel approach, Evidential Language-Image Posterior (ELIP), to achieve robust alignment between web images and semantic knowledge across various OOD cases by leveraging evidential uncertainties. The proposed ELIP can be seamlessly integrated into general image-text contrastive learning frameworks, providing an efficient fine-tuning approach without exacerbating the need for additional data. To validate the effectiveness of ELIP, we systematically design a series of OOD cases (e.g., image distortion, spelling errors, and a combination of both) on two benchmark datasets to mimic noisy data in real-world web applications. Our experimental results demonstrate that ELIP improves the performance and robustness of mainstream pre-trained vision-language models facing OOD samples in image-text retrieval tasks. | Guohao Sun, Yue Bai, Xueying Yang, Yi Fang, Yun Fu, Zhiqiang Tao |  |
| 349 |  |  [Deliberate Exposure to Opposing Views and Its Association with Behavior and Rewards on Political Communities](https://doi.org/10.1145/3589334.3645375) |  | 0 | Engaging with diverse political views is important for reaching better collective decisions, however, users online tend to remain confined within ideologically homogeneous spaces. In this work, we study users who are members of these spaces but who also show a willingness to engage with diverse views, as they have the potential to introduce more informational diversity into their communities. Across four Reddit communities (r/Conservative, r/The_Donald, r/ChapoTrapHouse, r/SandersForPresident), we find that these users tend to use less hostile and more advanced and personable language, but receive fewer social rewards from their peers compared to others. We also find that social sanctions on the discussion community r/changemyview are insufficient to drive them out in the short term, though they may play a role over the longer term. | Alexandros Efstratiou |  |
| 350 |  |  [Unmasking the Web of Deceit: Uncovering Coordinated Activity to Expose Information Operations on Twitter](https://doi.org/10.1145/3589334.3645529) |  | 0 | Social media platforms, particularly Twitter, have become pivotal arenas for influence campaigns, often orchestrated by state-sponsored information operations (IOs). This paper delves into the detection of key players driving IOs by employing similarity graphs constructed from behavioral pattern data. We unveil that well-known, yet underutilized network properties can help accurately identify coordinated IO drivers. Drawing from a comprehensive dataset of 49 million tweets from six countries, which includes multiple verified IOs, our study reveals that traditional network filtering techniques do not consistently pinpoint IO drivers across campaigns. We first propose a framework based on node pruning that emerges superior, particularly when combining multiple behavioral indicators across different networks. Then, we introduce a supervised machine learning model that harnesses a vector representation of the fused similarity network. This model, which boasts a precision exceeding 0.95, adeptly classifies IO drivers on a global scale and reliably forecasts their temporal engagements. Our findings are crucial in the fight against deceptive influence campaigns on social media, helping us better understand and detect them. | Luca Luceri, Valeria Pantè, Keith Burghardt, Emilio Ferrara |  |
| 351 |  |  [Invariant Graph Learning for Causal Effect Estimation](https://doi.org/10.1145/3589334.3645549) |  | 0 | Causal effect estimation from networked observational data encounters notable challenges, primarily hidden confounders arising from network structure, or spillover effects that influence unit's outcomes based on neighboring treatment assignments. Existing graph neural network (GNN)-based methods have endeavored to address these challenges, utilizing the GNN's message-passing mechanism to capture hidden confounders or model spillover effects. However, they mainly focus on transductive causal effect learning on a single networked data, limiting their efficacy in inductive settings for real-world applications where networked data often originates from multiple environments influenced by potentially varying time or geographical regions. In light of this, we introduce the principle of invariance to the task of causal effect estimation on networked data, culminating in our Invariant Graph Learning (IGL) framework. Specifically, it first generates multiple networked data to simulate diverse environments from a given observational data. Then it further encourages the model to learn environment-invariant representations for confounders and spillover effects. Such a design enables the model to extrapolate beyond a single observed environment, thereby improving the performance of causal effect estimation in potential new environments. Extensive experiments on two real-world datasets demonstrates the superiority of our approach. | Yongduo Sui, Caizhi Tang, Zhixuan Chu, Junfeng Fang, Yuan Gao, Qing Cui, Longfei Li, Jun Zhou, Xiang Wang |  |
| 352 |  |  [Sublinear-Time Opinion Estimation in the Friedkin-Johnsen Model](https://doi.org/10.1145/3589334.3645572) |  | 0 | Online social networks are ubiquitous parts of modern societies and the discussions that take place in these networks impact people's opinions on diverse topics, such as politics or vaccination. One of the most popular models to formally describe this opinion formation process is the Friedkin–Johnsen (FJ) model, which allows to define measures, such as the polarization and the disagreement of a network. Recently, Xu, Bao and Zhang (WebConf'21) showed that all opinions and relevant measures in the FJ model can be approximated in near-linear time. However, their algorithm requires the entire network and the opinions of all nodes as input. Given the sheer size of online social networks and increasing data-access limitations, obtaining the entirety of this data might, however, be unrealistic in practice. In this paper, we show that node opinions and all relevant measures, like polarization and disagreement, can be efficiently approximated in time that is sublinear in the size of the network. Particularly, our algorithms only require query-access to the network and do not have to preprocess the graph. Furthermore, we use a connection between FJ opinion dynamics and personalized PageRank, and show that in d-regular graphs, we can deterministically approximate each node's opinion by only looking at a constant-size neighborhood, independently of the network size. We also experimentally validate that our estimation algorithms perform well in practice. | Stefan Neumann, Yinhao Dong, Pan Peng |  |
| 353 |  |  [Bots, Elections, and Controversies: Twitter Insights from Brazil's Polarised Elections](https://doi.org/10.1145/3589334.3645651) |  | 0 | From 2018 to 2023, Brazil experienced its most fiercely contested elections in history, resulting in the election of far-right candidate Jair Bolsonaro followed by the left-wing, Lula da Silva. This period was marked by a murder attempt, a coup attempt, the pandemic, and a plethora of conspiracy theories and controversies. This paper analyses 437 million tweets originating from 13 million accounts associated with Brazilian politics during these two presidential election cycles. We focus on accounts' behavioural patterns. We noted a quasi-monotonic escalation in bot engagement, marked by notable surges both during COVID-19 and in the aftermath of the 2022 election. The data revealed a strong correlation between bot engagement and the number of replies during a single day ($r=0.66$, $p<0.01$). Furthermore, we identified a range of suspicious activities, including an unusually high number of accounts being created on the same day, with some days witnessing over 20,000 new accounts and super-prolific accounts generating close to 100,000 tweets. Lastly, we uncovered a sprawling network of accounts sharing Twitter handles, with a select few managing to utilise more than 100 distinct handles. This work can be instrumental in dismantling coordinated campaigns and offer valuable insights for the enhancement of bot detection algorithms. | Diogo Pacheco |  |
| 354 |  |  [Scalable Continuous-time Diffusion Framework for Network Inference and Influence Estimation](https://doi.org/10.1145/3589334.3645652) |  | 0 | The study of continuous-time information diffusion has been an important area of research for many applications in recent years. When only the diffusion traces (cascades) are accessible, cascade-based network inference and influence estimation are two essential problems to explore. Alas, existing methods exhibit limited capability to infer and process networks with more than a few thousand nodes, suffering from scalability issues. In this paper, we view the diffusion process as a continuous-time dynamical system, based on which we establish a continuous-time diffusion model. Subsequently, we instantiate the model to a scalable and effective framework (FIM) to approximate the diffusion propagation from available cascades, thereby inferring the underlying network structure. Furthermore, we undertake an analysis of the approximation error of FIM for network inference. To achieve the desired scalability for influence estimation, we devise an advanced sampling technique and significantly boost the efficiency. We also quantify the effect of the approximation error on influence estimation theoretically. Experimental results showcase the effectiveness and superior scalability of FIM on network inference and influence estimation. | Keke Huang, Ruize Gao, Bogdan Cautis, Xiaokui Xiao |  |
| 355 |  |  [Friend or Foe? Mining Suspicious Behavior via Graph Capsule Infomax Detector against Fraudsters](https://doi.org/10.1145/3589334.3645706) |  | 0 | Anomaly detection on graphs has recently attracted considerable attention due to its broad range of high-impact applications, including cybersecurity, financial transactions, and recommendation systems. Although many efforts have thus far been made, how to effectively handle the high inconsistency between users' behavior and labels, a fundamental issue in anomaly detection, has not yet received sufficient concern. Moreover, the inconsistency problem is hard to investigate and even deteriorates the performance of anomaly detectors. To this end, we propose a novel graph self-supervised learning framework, Capsule Graph Infomax (termed CapsGI), to overcome the inconsistency of anomaly detection. Inspired by the recent advances of capsules on images, we explore another possibility of reforming the node embedding by capsule ideas to represent the unique node's properties. Concretely, by disentangling heterogeneous factors underlying each node representation, we can establish node capsules such that their representation can reflect intrinsic node properties. To strengthen the connection among normal nodes, CapsGI further represents the part-whole contrastive learning between lower-level capsules (part) and higher-level capsules (whole) by explicitly considering the context graph relations. Extensive experiments on multiple real-world datasets demonstrate that our model significantly outperforms state-of-the-art models. | Xiangping Zheng, Bo Wu, Xun Liang, Wei Li |  |
| 356 |  |  [PASS: Predictive Auto-Scaling System for Large-scale Enterprise Web Applications](https://doi.org/10.1145/3589334.3645330) |  | 0 | We confront two challenges in the management of a vast and diverse array of online web applications deployed on enterprise-grade auto-scaling infrastructure, primarily focused on ensuring Quality of Service (QoS) for large-scale applications and optimizing resource costs. Firstly, reacting to increased load with a response-based approach can temporarily degrade QoS because many web applications need a few minutes to warm up. Therefore, precise workload prediction is critical for predictive scaling. However, our analysis of real-world applications underscores the substantial challenges arising from the limited precision and robustness of existing single prediction algorithms in the context of predictive auto-scaling. Secondly, guaranteeing the QoS of online applications within a cost-effective structure is crucial, as it is inherently linked to corporate profitability. Nevertheless, our study shows that mainstream auto-scaling methods exhibit various limitations, either being unsuitable for online environments or inadequately ensuring QoS. To address these issues, we introduce PASS, a Predictive Auto-Scaling System tailored for large-scale online web applications in enterprise settings. Our highly robust and accurate prediction framework dynamically integrates and calibrates appropriate prediction algorithms based on the unique characteristics of each application to effectively manage workload diversity. We further establish a performance model derived from online historical logs, enhancing auto-scaling to ensure diverse QoS without adverse impacts on online applications. Additionally, we implement a reactive strategy grounded in queuing theory to promptly address QoS violations resulting from inaccurate predictions or unexpected events. Across a wide spectrum of applications and real-world workloads, PASS outperforms state-of-the-art methods, achieving higher workload prediction accuracy and a superior QoS guarantee rate with less resource cost. | Yunda Guo, Jiake Ge, Panfeng Guo, Yunpeng Chai, Tao Li, Mengnan Shi, Yang Tu, Jian Ouyang |  |
| 357 |  |  [Unlocking the Non-deterministic Computing Power with Memory-Elastic Multi-Exit Neural Networks](https://doi.org/10.1145/3589334.3645340) |  | 0 | With the increasing demand for Web of Things (WoT) and edge computing, the efficient utilization of limited computing power on edge devices is becoming a crucial challenge. Traditional neural networks (NNs) as web services rely on deterministic computational resources. However, they may fail to output the results on non-deterministic computing power which could be preempted at any time, degrading the task performance significantly. Multi-exit NNs with multiple branches have been proposed as a solution, but the accuracy of intermediate results may be unsatisfactory. In this paper, we propose MEEdge, a system that automatically transforms classic single-exit models into heterogeneous and dynamic multi-exit models which enables Memory-Elastic inference at the Edge with non-deterministic computing power. To build heterogeneous multi-exit models, MEEdge uses efficient convolutions to form a branch zoo and High Priority First (HPF)-based branch placement method for branch growth. To adapt models to dynamically varying computational resources, we employ a novel on-device scheduler for collaboration. Further, to reduce the memory overhead caused by dynamic branches, we propose neuron-level weight sharing and few-shot knowledge distillation(KD) retraining. Our experimental results show that models generated by MEEdge can achieve up to 27.31% better performance than existing multi-exit NNs. | Jiaming Huang, Yi Gao, Wei Dong |  |
| 358 |  |  [ZipZap: Efficient Training of Language Models for Large-Scale Fraud Detection on Blockchain](https://doi.org/10.1145/3589334.3645352) |  | 0 | Language models (LMs) have demonstrated superior performance in detecting fraudulent activities on Blockchains. Nonetheless, the sheer volume of Blockchain data results in excessive memory and computational costs when training LMs from scratch, limiting their capabilities to large-scale applications. In this paper, we present ZipZap, a framework tailored to achieve both parameter and computational efficiency when training LMs on large-scale transaction data. First, with the frequency-aware compression, an LM can be compressed down to a mere 7.5% of its initial size with an imperceptible performance dip. This technique correlates the embedding dimension of an address with its occurrence frequency in the dataset, motivated by the observation that embeddings of low-frequency addresses are insufficiently trained and thus negating the need for a uniformly large dimension for knowledge representation. Second, ZipZap accelerates the speed through the asymmetric training paradigm: It performs transaction dropping and cross-layer parameter-sharing to expedite the pre-training process, while revert to the standard training paradigm for fine-tuning to strike a balance between efficiency and efficacy, motivated by the observation that the optimization goals of pre-training and fine-tuning are inconsistent. Evaluations on real-world, large-scale datasets demonstrate that ZipZap delivers notable parameter and computational efficiency improvements for training LMs. Our implementation is available at: https://github.com/git-disl/ZipZap. | Sihao Hu, Tiansheng Huang, KaHo Chow, Wenqi Wei, Yanzhao Wu, Ling Liu |  |
| 359 |  |  [Cold Start or Hot Start? Robust Slow Start in Congestion Control with A Priori Knowledge for Mobile Web Services](https://doi.org/10.1145/3589334.3645393) |  | 0 | Mobile web services value a quick loading of contents in the first page, which is quantified by the above-the-fold time of the first page (first AFT) and is likely to fall into the slow start phase in congestion control. However, the widely deployed slow start mechanism is "cold start", which manually hardcodes the parameters and is not suitable for the first AFT of heterogeneous mobile web services. We revisit the slow start mechanism and find that it could be optimized with a priori knowledge. However, blindly relying on a priori knowledge is not robust enough to handle the fluctuating mobile networks and unpredictable application traffic. In this paper, we propose WiseStart, a "hot-start-based" slow start mechanism. WiseStart utilizes the priori knowledge to set the initial parameters, continuously probes the new connection to handle the fluctuating network conditions, and carefully adapts to the application-limit scenarios. We implement WiseStart in a popular mobile web service online in production. Comprehensive experiments demonstrate that WiseStart reduces the First AFT by 25.43% and the average RCT at connection establishment by 16.15% compared to the default slow start mechanism and other state-of-the-art baselines. | Jia Zhang, Haixuan Tong, Enhuan Dong, Xin Qian, Mingwei Xu, Xiaotian Li, Zili Meng |  |
| 360 |  |  [Investigations of Top-Level Domain Name Collisions in Blockchain Naming Services](https://doi.org/10.1145/3589334.3645459) |  | 0 | Traditionally, top-level domains (TLDs) are managed by the Internet corporation for assigned names and numbers (ICANN), and the domain names under them are managed by registrars. Against such a centralized management, a blockchain naming service (BNS) has been proposed to manage TLDs on blockchains without authority intervention. BNS users can register TLD strings as non-fungible tokens and manage the TLD root zone. However, such a decentralized management results in the introduction of a new security issue, BNS TLD name collision, wherein the same TLD is registered in several different BNSs. In this study, we investigated BNS TLD name collisions by analyzing TLDs registered on two BNSs: Handshake and Decentraweb. Specifically, we collected TLDs registered in Handshake and Decentraweb and the associated data, and analyzed the data registration status of BNS TLDs and BNS TLD name collisions. The analysis of 11,595,406 Handshake and 11,889 Decentraweb TLDs revealed 6,973 BNS TLD name collisions. In particular, lastname TLDs, which are intended for use as person names, yielded a large number of registered domain names. In addition, the analysis identified 10 name collisions between the BNS and operational ICANN TLDs. Further, the ICANN TLD candidates under review also had name collisions against the BNS TLDs. Consequently, based on the characteristics of these name collisions and discussions in BNS communities, we considered countermeasures against BNS TLD name collisions. For the further development of BNSs, we believe that it is essential to discuss with the existing Internet communities and coexist with the existing Internet. | Daiki Ito, Yuta Takata, Hiroshi Kumagai, Masaki Kamizono |  |
| 361 |  |  [Don't Bite Off More than You Can Chew: Investigating Excessive Permission Requests in Trigger-Action Integrations](https://doi.org/10.1145/3589334.3645721) |  | 0 | Web-based trigger-action platforms (TAP) allow users to integrate Internet of Things (IoT) systems and online services into trigger-action integrations (TAIs), facilitating rich automation tasks known as applets. Despite their benefits, these integrations~(typically involving the TAP, trigger, and action service providers) pose significant security and privacy challenges, such as mis-triggering and data leakage. This work investigates cross-entity permission management within TAIs to address the underlying causes of these security and privacy issues, emphasizing permission-functionality consistency to ensure fairness in permission requests. We introduce PFCon, a system that leverages GPT-based language models for analyzing required and requested permissions, revealing excessive permission requests in a large-scale study of IFTTT TAP. Our findings highlight the need for service providers to enforce permission-functionality consistency, raising awareness of the importance of security and privacy in TAI. | Liuhuo Wan, Kailong Wang, Kulani Mahadewa, Haoyu Wang, Guangdong Bai |  |
| 362 |  |  [Off-Policy Evaluation of Slate Bandit Policies via Optimizing Abstraction](https://doi.org/10.1145/3589334.3645343) |  | 0 | We study off-policy evaluation (OPE) in the problem of slate contextual bandits where a policy selects multi-dimensional actions known as slates. This problem is widespread in recommender systems, search engines, marketing, to medical applications, however, the typical Inverse Propensity Scoring (IPS) estimator suffers from substantial variance due to large action spaces, making effective OPE a significant challenge. The PseudoInverse (PI) estimator has been introduced to mitigate the variance issue by assuming linearity in the reward function, but this can result in significant bias as this assumption is hard-to-verify from observed data and is often substantially violated. To address the limitations of previous estimators, we develop a novel estimator for OPE of slate bandits, called Latent IPS (LIPS), which defines importance weights in a low-dimensional slate abstraction space where we optimize slate abstractions to minimize the bias and variance of LIPS in a data-driven way. By doing so, LIPS can substantially reduce the variance of IPS without imposing restrictive assumptions on the reward function structure like linearity. Through empirical evaluation, we demonstrate that LIPS substantially outperforms existing estimators, particularly in scenarios with non-linear rewards and large slate spaces. | Haruka Kiyohara, Masahiro Nomura, Yuta Saito |  |
| 363 |  |  [Long-term Off-Policy Evaluation and Learning](https://doi.org/10.1145/3589334.3645446) |  | 0 | Short- and long-term outcomes of an algorithm often differ, with damaging downstream effects. A known example is a click-bait algorithm, which may increase short-term clicks but damage long-term user engagement. A possible solution to estimate the long-term outcome is to run an online experiment or A/B test for the potential algorithms, but it takes months or even longer to observe the long-term outcomes of interest, making the algorithm selection process unacceptably slow. This work thus studies the problem of feasibly yet accurately estimating the long-term outcome of an algorithm using only historical and short-term experiment data. Existing approaches to this problem either need a restrictive assumption about the short-term outcomes called surrogacy or cannot effectively use short-term outcomes, which is inefficient. Therefore, we propose a new framework called Long-term Off-Policy Evaluation (LOPE), which is based on reward function decomposition. LOPE works under a more relaxed assumption than surrogacy and effectively leverages short-term rewards to substantially reduce the variance. Synthetic experiments show that LOPE outperforms existing approaches particularly when surrogacy is severely violated and the long-term reward is noisy. In addition, real-world experiments on large-scale A/B test data collected on a music streaming platform show that LOPE can estimate the long-term outcome of actual algorithms more accurately than existing feasible methods. | Yuta Saito, Himan Abdollahpouri, Jesse Anderton, Ben Carterette, Mounia Lalmas |  |
| 364 |  |  [Unified Uncertainty Estimation for Cognitive Diagnosis Models](https://doi.org/10.1145/3589334.3645488) |  | 0 | Cognitive diagnosis models have been widely used in different areas, especially intelligent education, to measure users' proficiency levels on knowledge concepts, based on which users can get personalized instructions. As the measurement is not always reliable due to the weak links of the models and data, the uncertainty of measurement also offers important information for decisions. However, the research on the uncertainty estimation lags behind that on advanced model structures for cognitive diagnosis. Existing approaches have limited efficiency and leave an academic blank for sophisticated models which have interaction function parameters (e.g., deep learning-based models). To address these problems, we propose a unified uncertainty estimation approach for a wide range of cognitive diagnosis models. Specifically, based on the idea of estimating the posterior distributions of cognitive diagnosis model parameters, we first provide a unified objective function for mini-batch based optimization that can be more efficiently applied to a wide range of models and large datasets. Then, we modify the reparameterization approach in order to adapt to parameters defined on different domains. Furthermore, we decompose the uncertainty of diagnostic parameters into data aspect and model aspect, which better explains the source of uncertainty. Extensive experiments demonstrate that our method is effective and can provide useful insights into the uncertainty of cognitive diagnosis. | Fei Wang, Qi Liu, Enhong Chen, Chuanren Liu, Zhenya Huang, Jinze Wu, Shijin Wang |  |
| 365 |  |  [A Cross Domain Method for Customer Lifetime Value Prediction in Supply Chain Platform](https://doi.org/10.1145/3589334.3645391) |  | 0 | Accurate customer LifeTime Value (LTV) predictions are crucial for customer relationship management, especially in Supply Chain Platforms (SCP), which involve effectively managing the service resources in business decision-making. Previous LTV prediction methods usually rely on ample historical customer data, which is not available in the early stages of a customer's lifecycle. It makes the modeling of the historical customer data a difficult task due to the data sparsity. Besides, the long-tail distribution of customer LTV also brings new challenges to the prediction of LTV. To tackle the above issues, we propose CDLtvS, a novel Cross Domain method for customer Lifetime value prediction in SCP. It leverages rich cross-domain information from upstream platforms to enhance LTV predictions in downstream platforms. Firstly, CDLtvS pre-trains the customer representations by an LTV modeling framework named LtvS in source and target domains separately. Specifically, LtvS incorporates the Expert Mask Network (ExMN), which not only effectively models the long-tail distribution of LTV in single-domain but also resolves cross-domain learning model bias resulting from this distribution. Then, the various-level alignment mechanism is introduced to keep the consistency of knowledge transferring from source to target domains on both sparse and non-sparse data. Comprehensive experiments on real-world data from JD, one of the world's largest supply chain platforms, demonstrate that CDLtvS achieves a normalized mean average error of 0.3378 in LTV prediction, outperforming 16.3% to the baseline. Additionally, the improvements of ≥2.3% across various data sparsity levels (0% -- 80%) provide valuable insights into cross-domain LTV modeling. | Zhiyuan Zhou, Li Lin, Hai Wang, Xiaolei Zhou, Gong Wei, Shuai Wang |  |
| 366 |  |  [UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting](https://doi.org/10.1145/3589334.3645434) |  | 0 | Multivariate time series forecasting plays a pivotal role in contemporary web technologies. In contrast to conventional methods that involve creating dedicated models for specific time series application domains, this research advocates for a unified model paradigm that transcends domain boundaries. However, learning an effective cross-domain model presents the following challenges. First, various domains exhibit disparities in data characteristics, e.g., the number of variables, posing hurdles for existing models that impose inflexible constraints on these factors. Second, the model may encounter difficulties in distinguishing data from various domains, leading to suboptimal performance in our assessments. Third, the diverse convergence rates of time series domains can also result in compromised empirical performance. To address these issues, we propose UniTime for effective cross-domain time series learning. Concretely, UniTime can flexibly adapt to data with varying characteristics. It also uses domain instructions and a Language-TS Transformer to offer identification information and align two modalities. In addition, UniTime employs masking to alleviate domain convergence speed imbalance issues. Our extensive experiments demonstrate the effectiveness of UniTime in advancing state-of-the-art forecasting performance and zero-shot transferability. | Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang, Bryan Hooi, Roger Zimmermann |  |
| 367 |  |  [Semantic Evolvement Enhanced Graph Autoencoder for Rumor Detection](https://doi.org/10.1145/3589334.3645478) |  | 0 | Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Recently, numerous rumor detection models which utilize textual information and the propagation structure of events have been proposed. However, these methods overlook the importance of semantic evolvement information of event in propagation process, which is often challenging to be truly learned in supervised training paradigms and traditional rumor detection methods. To address this issue, we propose a novel semantic evolvement enhanced Graph Autoencoder for Rumor Detection (GARD) model in this paper. The model learns semantic evolvement information of events by capturing local semantic changes and global semantic evolvement information through specific graph autoencoder and reconstruction strategies. By combining semantic evolvement information and propagation structure information, the model achieves a comprehensive understanding of event propagation and perform accurate and robust detection, while also detecting rumors earlier by capturing semantic evolvement information in the early stages. Moreover, in order to enhance the model's ability to learn the distinct patterns of rumors and non-rumors, we introduce a uniformity regularizer to further improve the model's performance. Experimental results on three public benchmark datasets confirm the superiority of our GARD method over the state-of-the-art approaches in both overall performance and early rumor detection. | Xiang Tao, Liang Wang, Qiang Liu, Shu Wu, Liang Wang |  |
| 368 |  |  [Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding](https://doi.org/10.1145/3589334.3645534) |  | 0 | In online advertising, advertisers participate in ad auctions to acquire ad opportunities, often by utilizing auto-bidding tools provided by demand-side platforms (DSPs). The current auto-bidding algorithms typically employ reinforcement learning (RL). However, due to safety concerns, most RL-based auto-bidding policies are trained in simulation, leading to a performance degradation when deployed in online environments. To narrow this gap, we can deploy multiple auto-bidding agents in parallel to collect a large interaction dataset. Offline RL algorithms can then be utilized to train a new policy. The trained policy can subsequently be deployed for further data collection, resulting in an iterative training framework, which we refer to as iterative offline RL. In this work, we identify the performance bottleneck of this iterative offline RL framework, which originates from the ineffective exploration and exploitation caused by the inherent conservatism of offline RL algorithms. To overcome this bottleneck, we propose Trajectory-wise Exploration and Exploitation (TEE), which introduces a novel data collecting and data utilization method for iterative offline RL from a trajectory perspective. Furthermore, to ensure the safety of online exploration while preserving the dataset quality for TEE, we propose Safe Exploration by Adaptive Action Selection (SEAS). Both offline experiments and real-world experiments on Alibaba display advertising platform demonstrate the effectiveness of our proposed method. | Haoming Li, Yusen Huo, Shuai Dou, Zhenzhe Zheng, Zhilin Zhang, Chuan Yu, Jian Xu, Fan Wu |  |
| 369 |  |  [Stable-Sketch: A Versatile Sketch for Accurate, Fast, Web-Scale Data Stream Processing](https://doi.org/10.1145/3589334.3645581) |  | 0 | Data stream processing plays a pivotal role in various web-related applications, including click fraud detection, anomaly identification, and recommendation systems. Accurate and fast detection of items relevant to such tasks within data streams, e.g., heavy hitters, heavy changers, and persistent items, is however non-trivial. This is due to growing streaming speeds, limited fast memory (L1 cache) available in current systems, and highly skewed item distributions encountered in practice. In effect, items of interest that are tracked only based on their features (e.g., item frequency or persistence value) are susceptible to replacement by non-relevant ones, leading to modest detection accuracy, as we reveal. In this work, we introduce the notion of bucket stability, which quantifies the degree of recorded item variation, and show that this is a powerful metric for identifying distinct item types. We propose Stable-Sketch, an elegant and versatile sketch that exploits multidimensional information, including item statistics and bucket stability, and adopts a stochastic approach to drive replacement decisions. We present a theoretical analysis of the error bounds of Stable-Sketch, and conduct extensive experiments to demonstrate that our solution achieves substantially higher accuracy and faster processing speeds than state-of-the-art sketches in a range of item detection tasks, even with tight memories. We further enhance Stable-Sketch's update throughput with Single Instruction Multiple Data (SIMD) instructions and implement our solution with P4, demonstrating real world deployment viability. | Weihe Li, Paul Patras |  |
| 370 |  |  [Self-Paced Pairwise Representation Learning for Semi-Supervised Text Classification](https://doi.org/10.1145/3589334.3645664) |  | 0 | Text classification is one vital tool assisting web content mining. Semi-supervised text classification (SSTC) offers an approach to alleviate the burden of annotation costs by training on a few labeled texts alongside many unlabeled texts. Unsolved challenges in SSTC are the overfitting problem caused by the limited labeled data and the mislabeling problem of unlabeled texts. To address these issues, this paper proposes a Self-Paced PairWise representation learning (SPPW) model. Concretely, SPPW alleviates the overfitting problem by replacing the overfitting-prone learning of a parameterized classifier with representation learning in a pair-wise manner. Besides, we propose a novel self-paced text filtering method that effectively integrates both label confidence and text hardness to reduce mislabeled texts synergistically. Extensive experiments on 3 benchmark SSTC datasets show that SPPW outperforms baselines and is effective in mitigating overfitting and mislabeling problems. | Junfan Chen, Richong Zhang, Jiarui Wang, Chunming Hu, Yongyi Mao |  |
| 371 |  |  [Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering](https://doi.org/10.1145/3589334.3645670) |  | 0 | Open-domain question answering (ODQA) has emerged as a pivotal research spotlight in information systems. Existing methods follow two main paradigms to collect evidence: (1) The retrieve-then-read paradigm retrieves pertinent documents from an external corpus; and (2) the generate-then-read paradigm employs large language models (LLMs) to generate relevant documents. However, neither can fully address multifaceted requirements for evidence. To this end, we propose LLMQA, a generalized framework that formulates the ODQA process into three basic steps: query expansion, document selection, and answer generation, combining the superiority of both retrieval-based and generation-based evidence. Since LLMs exhibit their excellent capabilities to accomplish various tasks, we instruct LLMs to play multiple roles as generators, rerankers, and evaluators within our framework, integrating them to collaborate in the ODQA process. Furthermore, we introduce a novel prompt optimization algorithm to refine role-playing prompts and steer LLMs to produce higher-quality evidence and answers. Extensive experimental results on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate that LLMQA achieves the best performance in terms of both answer accuracy and evidence quality, showcasing its potential for advancing ODQA research and applications. | Hongda Sun, Yuxuan Liu, Chengwei Wu, Haiyu Yan, Cheng Tai, Xin Gao, Shuo Shang, Rui Yan |  |
| 372 |  |  [POLISH: Adaptive Online Cross-Modal Hashing for Class Incremental Data](https://doi.org/10.1145/3589334.3645716) |  | 0 | In recent years, hashing-based online cross-modal retrieval has garnered growing attention. This trend is motivated by the fact that web data is increasingly delivered in a streaming manner as opposed to batch processing. Simultaneously, the sheer scale of web data sometimes makes it impractical to fully load for the training of hashing models. Despite the evolution of online cross-modal hashing techniques, several challenges remain: 1) Most existing methods learn hash codes by considering the relevance among newly arriving data or between new data and the existing data, often disregarding valuable global semantic information. 2) A common but limiting assumption in many methods is that the label space remains constant, implying that all class labels should be provided within the first data chunk. This assumption does not hold in real-world scenarios, and the presence of new labels in incoming data chunks can severely degrade or even break these methods. To tackle these issues, we introduce a novel supervised online cross-modal hashing method named adaPtive Online cLass-Incremental haSHing (POLISH). Leveraging insights from language models, POLISH generates representations for new class label from multiple angles. Meanwhile, POLISH treats label embeddings, which remain unchanged once learned, as stable global information to produce high-quality hash codes. POLISH also puts forward an efficient optimization algorithm for hash code learning. Extensive experiments on two real-world benchmark datasets show the effectiveness of the proposed POLISH for class incremental data in the cross-modal hashing domain. | YuWei Zhan, Xin Luo, ZhenDuo Chen, Yongxin Wang, Yinwei Wei, XinShun Xu |  |
| 373 |  |  [How Contentious Terms About People and Cultures are Used in Linked Open Data](https://doi.org/10.1145/3589334.3648140) |  | 0 | Web resources in linked open data (LOD) are comprehensible to humans through literal textual values attached to them, such as labels, notes, or comments. Word choices in literals may not always be neutral. When outdated and culturally stereotyping terminology is used in literals, they may appear as offensive to users in interfaces and propagate stereotypes to algorithms trained on them. We study how frequently and in which literals contentious terms about people and cultures occur in LOD and whether there are attempts to mark the usage of such terms. For our analysis, we reuse English and Dutch terms from a knowledge graph that provides opinions of experts from the cultural heritage domain about terms' contentiousness. We inspect occurrences of these terms in four widely used datasets: Wikidata, The Getty Art & Architecture Thesaurus, Princeton WordNet, and Open Dutch WordNet. Some terms are ambiguous and contentious only in particular senses. Applying word sense disambiguation, we generate a set of literals relevant to our analysis. We found that outdated, derogatory, stereotyping terms frequently appear in descriptive and labelling literals, such as preferred labels that are usually displayed in interfaces and used for indexing. In some cases, LOD contributors mark contentious terms with words and phrases in literals (implicit markers) or properties linked to resources (explicit markers). However, such marking is rare and non-consistent in all datasets. Our quantitative and qualitative insights could be helpful in developing more systematic approaches to address the propagation of stereotypes via LOD. | Andrei Nesterov, Laura Hollink, Jacco van Ossenbruggen |  |
| 374 |  |  [MMAdapt: A Knowledge-guided Multi-source Multi-class Domain Adaptive Framework for Early Health Misinformation Detection](https://doi.org/10.1145/3589334.3648152) |  | 0 | This paper studies a critical problem of emergent health misinformation detection, aiming to mitigate the spread of misinformation in emergent health domains to support well-informed healthcare decisions towards a Web for good health. Our work is motivated by the lack of timely resources (e.g., medical knowledge, annotated data) during the initial phases of an emergent health event or topic. In this paper, we develop a multi-source domain adaptive framework that jointly exploits medical knowledge and annotated data from different high-resource source domains (e.g., cancer, COVID-19) to detect misleading posts in an emergent target domain (e.g., mpox, polio). Two important challenges exist in developing our solution: 1) how to accurately detect the partially misleading and unverifiable content in an emergent target domain? 2) How to identify the conflicting knowledge facts from different source domains to accurately detect emergent misinformation in the target domain? To address these challenges, we develop MMAdapt, a multi-source multi-class domain adaptive misinformation detection framework that effectively explores diverse knowledge facts from different source domains to accurately detect not only the outright misleading but also the partially misleading or unverifiable posts on the Web. Extensive experimental results on four real-world misinformation datasets demonstrate that MMAdapt substantially outperforms state-of-the-art baselines in accurately detecting misinformation in an emergent health domain. | Lanyu Shang, Yang Zhang, Bozhang Chen, Ruohan Zong, Zhenrui Yue, Huimin Zeng, Na Wei, Dong Wang |  |
| 375 |  |  [LightCS: Selecting Quadratic Feature Crosses in Linear Complexity](https://doi.org/10.1145/3589335.3648300) |  | 0 | Feature crosses, which represent joint features synthesized by two single features, are critical for deep recommender systems to model sophisticated feature relations. In practice, only a tiny fraction of feature crosses among massive possible ones are informative, while introducing irrelevant or noisy ones may increase online service latency and boost the risk of overfitting. Therefore, picking high-quality feature crosses is essential in practical recommender systems. However, even for selecting quadratic feature crosses, existing algorithms still incur either o(n2) time complexity or o(n2) space complexity, which is inefficient and unscalable in industrial scenarios. In this paper, we present an efficient and accurate quadratic feature cross selection method with both linear time and space complexity. Motivated by the idea of Quasi-Newton methods, we propose to use 2nd-order derivative matrix to evaluate all theoretically possible feature crosses concurrently without the need of constructing them explicitly, where an approximation of 2nd-order gradient is applied to guarantee both low time and space complexity. Furthermore, we decouple the feature crosses' novelty from single features' joint importance. Experiments on two public recommendation datasets and a private dataset validate the efficiency and effectiveness of our method, and it has also become a fundamental feature cross selection tool used by Huawei Ads Platform. | Zhaocheng Du, Junhao Chen, Qinglin Jia, Chuhan Wu, Jieming Zhu, Zhenhua Dong, Ruiming Tang |  |
| 376 |  |  [SOIL: Score Conditioned Diffusion Model for Imbalanced Cloud Failure Prediction](https://doi.org/10.1145/3589335.3648303) |  | 0 | Cloud failure prediction (e.g., disk failure prediction, memory failure prediction, node failure prediction, etc.) is a crucial task for ensuring the reliability and performance of cloud systems.However, the problem of class imbalance poses a huge challenge for accurate prediction as the number of healthy components (majority class) in a cloud system is much larger than the number of failed components (minority class). The consequences of this class imbalance include biased model performance and insufficient learning, as the model may lack adequate information to learn the characteristics associated with cloud failure effectively. Moreover, current methods for addressing the class imbalance problem, such as SMOTE and its variants, exhibit certain drawbacks, such as generating noisy samples and struggling to maintain sample diversity, which limit their effectiveness in addressing the challenges presented by the class imbalance in cloud failure prediction. In this paper, we propose a novel oversampling method for imbalanced classification, named SOIL (Score cOnditioned dIffusion modeL), which employs a score-conditioned diffusion model to generate high-quality synthetic samples for the minority class, more accurately representing real-world cloud failure patterns. By incorporating classification probabilities as conditional scores, SOIL offers supervision to the generation process, effectively limiting noise production while maintaining sample diversity. Through extensive experiments on various public and industrial datasets, upon adopting our method, the cloud failure prediction model's F1-score is improved by an average of 5.39% and consistently outperforms state-of-the-art competitors in addressing the class imbalance problem, which confirm the effectiveness and robustness of SOIL. In addition, SOIL has been successfully applied to a global large-scale cloud platform serving billions of customers, demonstrating its practicability. | Chiming Duan, Fangkai Yang, Pu Zhao, Lingling Zheng, Yash Dagli, Yudong Liu, Qingwei Lin, Dongmei Zhang |  |
| 377 |  |  [Dependency Aware Incident Linking in Large Cloud Systems](https://doi.org/10.1145/3589335.3648311) |  | 0 | Despite significant reliability efforts, large-scale cloud services inevitably experience production incidents that can significantly impact service availability and customer's satisfaction. Worse, in many cases one incident can lead to multiple downstream failures due to cascading effects that creates several related incidents across different dependent services. Often time On-call Engineers (OCEs) examine these incidents in silos that lead to significant amount of manual toil and increase the overall time-to-mitigate incidents. Therefore, developing efficient incident linking models is of paramount importance for grouping related incidents into clusters so as to quickly resolve major outages and reduce on-call fatigue. Existing incident linking methods mostly leverages textual and contextual information of incidents (e.g., title, description, severity, impacted components), thus failing to leverage the inter-dependencies between services. In this paper, we propose the dependency-aware incident linking (DiLink) framework which leverages both textual and service dependency graph information to improve the accuracy and coverage of incident links not only coming from same service, but also from different services and workloads. Furthermore, we propose a novel method to align the embeddings of multi-modal (i.e., textual and graphical) data using Orthogonal Procrustes. Extensive experimental results on real-world incidents from 5 workloads of Microsoft demonstrate that our alignment method has an F1-score of 0.96 (14 are also in the process of deploying this solution across 610 services from these 5 workloads for continuously supporting OCEs improving incident management and reducing manual toil. | Supriyo Ghosh, Karish Grover, Jimmy Wong, Chetan Bansal, Rakesh Namineni, Mohit Verma, Saravan Rajmohan |  |
| 378 |  |  [Assessing Web Fingerprinting Risk](https://doi.org/10.1145/3589335.3648322) |  | 0 | Modern Web APIs allow developers to provide extensively customized experiences for website visitors, but the richness of the device information they provide also make them vulnerable to being abused to construct browser fingerprints, device-specific identifiers that enable covert tracking of users even when cookies are disabled. Previous research has established entropy, a measure of information, as the key metric for quantifying fingerprinting risk. However, earlier studies had two major limitations. First, their entropy estimates were based on either a single website or a very small sample of devices. Second, they did not adequately consider correlations among different Web APIs, potentially grossly overestimating their fingerprinting risk. We provide the first study of browser fingerprinting which addresses the limitations of prior work. Our study is based on actual visited pages and Web APIs reported by tens of millions of real Chrome browsers in-the-wild. We accounted for the dependencies and correlations among Web APIs, which is crucial for obtaining more realistic entropy estimates. We also developed a novel experimental design that accurately and efficiently estimates entropy while never observing too much information from any single user. Our results provide an understanding of the distribution of entropy for different website categories, confirm the utility of entropy as a fingerprinting proxy, and offer a method for evaluating browser enhancements which are intended to mitigate fingerprinting. | Enrico Bacis, Igor Bilogrevic, Róbert BusaFekete, Asanka Herath, Antonio Sartori, Umar Syed |  |
| 379 |  |  [A Graph-based Framework for Reducing False Positives in Authentication Alerts in Security Systems](https://doi.org/10.1145/3589335.3648325) |  | 0 | The high false positive (FP) rate of authentication alerts remains to be a prominent challenge in cybersecurity nowadays. We identify two problems that cause this issue, which are unaddressed in existing learning-based anomaly detection methods. First, in industrial applications, ground-truth labels for malicious authentication events are extremely scarce. Therefore, learning-based methods must optimize their procedures for auto-generating high-quality training instances, an aspect that existing works have overlooked. Second, every existing model is based on a single form of data representation, either stream or graph snapshot, which may not be expressive enough to identify heterogeneity in behaviors of networked entities. This results in misclassifying a legitimate but differently-behaved authentication event into an anomalous one. We address these problems by proposing a new framework based on self-supervised link prediction on dynamic authentication networks, with two highlighted features: (1) our framework is based on the unification of two most popular views of dynamic interconnected systems: graph snapshots and link stream, ensuring the best coverage of behavioral heterogeneity; (2) to generate high-quality training samples, we propose a carefully designed negative sampling procedure called filtered rewiring, to ensure that the negative samples used for training are both truly negative and instructive. We validate our framework on 4 months of authentication data of 125 randomly selected, real organizations that subscribe to Microsoft's defense services. | Yanbang Wang, Karl Hallgren, Jonathan Larson |  |
| 380 |  |  [FinReport: Explainable Stock Earnings Forecasting via News Factor Analyzing Model](https://doi.org/10.1145/3589335.3648330) |  | 0 | The task of stock earnings forecasting has received considerable attention due to the demand investors in real-world scenarios. However, compared with financial institutions, it is not easy for ordinary investors to mine factors and analyze news. On the other hand, although large language models in the financial field can serve users in the form of dialogue robots, it still requires users to have financial knowledge to ask reasonable questions. To serve the user experience, we aim to build an automatic system, FinReport, for ordinary investors to collect information, analyze it, and generate reports after summarizing. Specifically, our FinReport is based on financial news announcements and a multi-factor model to ensure the professionalism of the report. The FinReport consists of three modules: news factorization module, return forecasting module, risk assessment module. The news factorization module involves understanding news information and combining it with stock factors, the return forecasting module aim to analysis the impact of news on market sentiment, and the risk assessment module is adopted to control investment risk. Extensive experiments on real-world datasets have well verified the effectiveness and explainability of our proposed FinReport. Our codes and datasets are available at https://github.com/frinkleko/FinReport. | Xiangyu Li, Xinjie Shen, Yawen Zeng, Xiaofen Xing, Jin Xu |  |
| 381 |  |  [DISKCO : Disentangling Knowledge from Cross-Encoder to Bi-Encoder](https://doi.org/10.1145/3589335.3648333) |  | 0 | In the field of Natural Language Processing (NLP), sentence pair classification is important in various real-world applications. Bi-encoders are commonly used to address these problems due to their low-latency requirements, and their ability to act as effective retrievers. However, bi-encoders often under-perform compared to cross-encoders by a significant margin. To address this gap, many Knowledge Distillation (KD) techniques have been proposed. Most existing KD methods focus solely on utilizing the prediction scores of cross-encoder models and overlook the fact that cross-encoders and bi-encoders have fundamentally different input structures. In this work, we introduce a novel knowledge distillation approach called DISKCO, which DISentangles the Knowledge learned in Cross-encoder models especially from multi-head cross-attention models and transfers it to bi-encoder models. DISKCO leverages the information encoded in the cross-attention weights of the trained cross-encoder model, and provide it as contextual cues for the student bi-encoder model during training and inference. DISKCO combines the benefits of independent encoding for low-latency applications with the knowledge acquired from cross-encoders, resulting in improved performance. Empirically, we demonstrate the effectiveness of DISKCO on proprietary and on various publicly available datasets. Our experiments show that DISKCO outperforms traditional knowledge distillation methods by upto 2%. | Ankith M. S, Arindam Bhattacharya, Ankit Gandhi, Vijay Huddar, Atul Saroop, Rahul Bhagat |  |
| 382 |  |  [Information Diffusion Meets Invitation Mechanism](https://doi.org/10.1145/3589335.3648337) |  | 0 | The dissemination of information is a complex process that plays a crucial role in real-world applications, especially when intertwined with friend invitations and their ensuing responses. Traditional diffusion models, however, often do not adequately capture this invitation-aware diffusion (IAD), rendering inferior results. These models typically focus on describing the social influence process, i.e., how a user is informed by friends, but tend to overlook the subsequent behavioral changes that invitations might precipitate. To this end, we present the Independent Cascade with Invitation (ICI) model, which incorporates both the social influence process and multi-stage behavior conversions in IAD. We validate our design through an empirical study on in-game IAD. Furthermore, we conduct extensive experiments to evaluate the effectiveness of our proposal against 6 state-of-the-art models on 6 real-world datasets. In particular, we demonstrate that our solution can outperform the best competitor by up to 5× in cascade estimation and 17.2% in diffusion prediction. We deploy our proposal in the seed selection and friend ranking scenarios of Tencent's online games, where it achieves improvements of up to 170% and 20.3%, respectively. | Shiqi Zhang, Jiachen Sun, Wenqing Lin, Xiaokui Xiao, Yiqian Huang, Bo Tang |  |
| 383 |  |  [The MMO Economist: AI Empowers Robust, Healthy, and Sustainable P2W MMO Economies](https://doi.org/10.1145/3589335.3648344) |  | 0 | Massively Multiplayer Online Games (MMOs) feature intricate virtual economies that permeate various in-game activities. However, the balancing act between profitability and equality in MMO economic design proves to be a persistent conundrum, especially in nascent business models like Pay-to-Win (P2W). Conventional efforts are curtailed by two primary constraints: the inability to verify and the provision of suboptimal solutions. In light of these predicaments, this paper delves into MMO economies and explores the promising potential of integrating emerging AI methodologies into economic design. Specifically, we introduce a novel hierarchical Reinforcement Learning (RL) solution for achieving Pareto optimality between profitability and equality in P2W economies. Leveraging our substantial industrial acumen and expertise, we establish an economic simulation environment that facilitates authentic and realistic assessments of MMO economic evolution. Building upon this foundation, we reconceptualize the P2W economic design process within the paradigm of a Markov Decision Process (MDP) and tackle it as a standard RL problem. Comprehensive evaluations corroborate that our solution demonstrates consistent personality specialization in economic simulations akin to real-world MMOs and significantly outperforms other baselines in economic design. Further discussions highlight its superiority in both frontier research and practical applications within the game industry. | Shiwei Zhao, Xi Yuan, Runze Wu, Zhipeng Hu, Haoyu Liu, Kai Wang, Yujing Hu, Tangjie Lv, Changjie Fan, Xin Tong, Jiangze Han, Yan Zheng, Jianye Hao |  |
| 384 |  |  [Skewness-aware Boosting Regression Trees for Customer Contribution Prediction in Financial Precision Marketing](https://doi.org/10.1145/3589335.3648346) |  | 0 | In an era of digital evolution, banking sectors face the dual challenge of nurturing a digitally savvy demographic and managing potential dormant account holders. This study delves deep into the prediction of customer contributions, particularly considering the skewed nature of such data. The inherent skewness in customer contribution data, highlighted by the substantial low-value contribution group and the vast variability among high-value contributors, necessitates an advanced prediction model. Addressing this, we present the Skewness-aware Boosting Regression Trees (SBRT) framework to predict customer contributions whose distribution exhibit high skewness. SBRT seamlessly combines the strength of Gradient Boosted Decision Trees with a novel mechanism of random tree deactivation, adeptly tackling distribution skewness. The model's effectiveness is rooted in four principles: cross-feature extraction, a percentile-based calibration and rebalancing method, tree deactivation during the boosting phase, and the utilization of Huber loss. Extensive real-world bank data testing underscores SBRT's promising capability in managing skewed distributions, setting it a cut above in predicting customer contributions. The culmination of this work lies in its practical validation, where online A/B tests highlight SBRT's tangible industrial applicability. | HsinYu Chen, ChengTe Li, TingYu Chen |  |
| 385 |  |  [Can we Soft Prompt LLMs for Graph Learning Tasks?](https://doi.org/10.1145/3589335.3651476) |  | 0 | Graph plays an important role in representing complex relationships in real-world applications such as social networks, biological data and citation networks. In recent years, Large Language Models (LLMs) have achieved tremendous success in various domains, which makes applying LLMs to graphs particularly appealing. However, directly applying LLMs to graph modalities presents unique challenges due to the discrepancy and mismatch between the graph and text modalities. Hence, to further investigate LLMs' potential for comprehending graph information, we introduce GraphPrompter, a novel framework designed to align graph information with LLMs via soft prompts. Specifically, GraphPrompter consists of two main components: a graph neural network to encode complex graph information and an LLM that effectively processes textual information. Comprehensive experiments on various benchmark datasets under node classification and link prediction tasks demonstrate the effectiveness of our proposed method. The GraphPrompter framework unveils the substantial capabilities of LLMs as predictors in graph-related tasks, enabling researchers to utilize LLMs across a spectrum of real-world graph scenarios more effectively. | Zheyuan Liu, Xiaoxin He, Yijun Tian, Nitesh V. Chawla |  |
| 386 |  |  [Everything Perturbed All at Once: Enabling Differentiable Graph Attacks](https://doi.org/10.1145/3589335.3651501) |  | 0 | As powerful tools for representation learning on graphs, graph neural networks (GNNs) have played an important role in applications including social networks, recommendation systems, and online web services. However, GNNs have been shown to be vulnerable to adversarial attacks, which can significantly degrade their effectiveness. Recent state-of-the-art approaches in adversarial attacks rely on gradient-based meta-learning to selectively perturb a single edge with the highest attack score until they reach the budget constraint. While effective in identifying vulnerable links, these methods are plagued by high computational costs. By leveraging continuous relaxation and parameterization of the graph structure, we propose a novel attack method called Differentiable Graph Attack (DGA) to efficiently generate effective attacks and meanwhile eliminate the need for costly retraining. Compared to the state-of-the-art, DGA achieves nearly equivalent attack performance with 6 times less training time and 11 times smaller GPU memory footprint on different benchmark datasets. Additionally, we provide extensive experimental analyses of the transferability of the DGA among different graph models, as well as its robustness against widely-used defense mechanisms. | Haoran Liu, Bokun Wang, Jianling Wang, Xiangjue Dong, Tianbao Yang, James Caverlee |  |
| 387 |  |  [Unlink to Unlearn: Simplifying Edge Unlearning in GNNs](https://doi.org/10.1145/3589335.3651578) |  | 0 | As concerns over data privacy intensify, unlearning in Graph Neural Networks (GNNs) has emerged as a prominent research frontier in academia. This concept is pivotal in enforcing the right to be forgotten, which entails the selective removal of specific data from trained GNNs upon user request. Our research focuses on edge unlearning, a process of particular relevance to real-world applications, owing to its widespread applicability. Current state-of-the-art approaches like GNNDelete can eliminate the influence of specific edges, yet our research has revealed a critical limitation in these approaches, termed over-forgetting. It occurs when the unlearning process inadvertently removes excessive information beyond specific data, leading to a significant decline in prediction accuracy for the remaining edges. To address this issue, we have identified the loss functions of GNNDelete as the primary source of the over-forgetting phenomenon. Furthermore, our analysis also suggests that loss functions may not be essential for effective edge unlearning. Building on these insights, we have simplified GNNDelete to develop Unlink-to-Unlearn (UtU), a novel method that facilitates unlearning exclusively through unlinking the forget edges from graph structure. Our extensive experiments demonstrate that UtU delivers privacy protection on par with that of a retrained model while preserving high accuracy in downstream tasks. Specifically, UtU upholds over 97.3 link prediction accuracy. Meanwhile, UtU requires only constant computational demands, underscoring its advantage as a highly lightweight and practical edge unlearning solution. | Jiajun Tan, Fei Sun, Ruichen Qiu, Du Su, Huawei Shen |  |
| 388 |  |  [Near-duplicate Question Detection](https://doi.org/10.1145/3589335.3651538) |  | 0 | Suggesting relevant questions to users is an important task in various applications, such as community Q&A or e-commerce websites. To ensure that there is no redundancy in the selected set of candidate questions, it is essential to filter out any near-duplicate questions. Identifying near-duplicate questions has another use case in light of the adoption of Large Language Models (LLMs) - fetching pre-computed answers for similar questions. However, identifying the similarity of questions is a bit more complex in comparison to generic text, as questions entail open-ended information that is not explicitly contained within the wording of the question itself. We introduce a taxonomy that accounts for the subtle intricacies characteristic of near-duplicate questions and propose a method for detecting them utilizing the capabilities of LLMs. | Preetam Prabhu Srikar Dammu, Omar Alonso |  |
| 389 |  |  [Tackling Long-Tail Entities for Temporal Knowledge Graph Completion](https://doi.org/10.1145/3589335.3651565) |  | 0 | Most Temporal Knowledge Graphs (TKGs) exhibit a long-tail entity distribution, where the majority of entities have sparse connections. Existing TKG completion methods struggle with managing new or unseen entities that often lack sufficient connections. In this paper, we introduce a model-agnostic enhancement layer that can be integrated with any existing TKG completion method to improve its performance. This enhancement layer employs a broader, global definition of entity similarity, transcending the limitations of local neighborhood proximity found in Graph Neural Network (GNN) based methods. Additionally, we conduct our evaluations in a novel, realistic setup that treats the TKG as a stream of evolving data. Evaluations on two benchmark datasets demonstrate that our framework surpasses existing methods in overall link prediction, inductive link prediction, and in addressing long-tail entities. Notably, our approach achieves a 10% improvement in MRR on one dataset and a 15% increase on another. | Mehrnoosh Mirtaheri, Ryan A. Rossi, Sungchul Kim, Kanak Mahadik, Tong Yu, Xiang Chen, Mohammad Rostami |  |
| 390 |  |  [From Creation to Clarification: ChatGPT's Journey Through the Fake News Quagmire](https://doi.org/10.1145/3589335.3651509) |  | 0 | The rampant spread of fake news has adversely affected society, resulting in extensive research on curbing its spread. As a notable milestone in large language models (LLMs), ChatGPT has gained significant attention due to its exceptional capabilities. In this study, we present an exploration of ChatGPT's proficiency in generating, explaining, and detecting fake news as follows.Generation -- We employ different prompt methods to generate fake news and prove the high quality of these instances through both self-assessment and human evaluation.Explanation -- We obtain nine features to characterize fake news based on ChatGPT's explanations and analyze the distribution of these factors across multiple public datasets.Detection -- We examine ChatGPT's capacity to identify fake news. We propose a reason-aware prompt method to improve its performance. We further probe into the potential extra information that could bolster its effectiveness in detecting fake news. | Yue Huang, Kai Shu, Philip S. Yu, Lichao Sun |  |
| 391 |  |  [The Invisible Game on the Internet: A Case Study of Decoding Deceptive Patterns](https://doi.org/10.1145/3589335.3651571) |  | 0 | Deceptive patterns are design practices embedded in digital platforms to manipulate users, representing a widespread and long-standing issue in the web and mobile software development industry. Legislative actions highlight the urgency of globally regulating deceptive patterns. However, despite advancements in detection tools, a significant gap exists in assessing deceptive pattern risks. In this study, we introduce a comprehensive approach involving the interactions between the Adversary, Watchdog (e.g., detection tools), and Challengers (e.g., users) to formalize and decode deceptive pattern threats. Based on this, we propose a quantitative risk assessment system. Representative cases are analyzed to showcase the practicability of the proposed risk scoring system, emphasizing the importance of involving human factors in deceptive pattern risk assessment. | Zewei Shi, Ruoxi Sun, Jieshan Chen, Jiamou Sun, Minhui Xue |  |
| 392 |  |  [Improving Model Robustness against Adversarial Examples with Redundant Fully Connected Layer](https://doi.org/10.1145/3589335.3651524) |  | 0 | Recent studies show that deep neural networks are extremely vulnerable, especially for adversarial examples of image classification models. However, the current defense technologies exhibit a series of limitations in terms of the adaptability of different attacks, the trade-off between clean-instance accuracy and robust one, as well as efficiency for train time overhead. To tackle these problems, we present a novel component, named redundant fully connected layer, which can be combined with existing model backbones in a pluggable manner. Specifically, we design a tailor-made loss function for it that leverages cosine similarity to maximize the difference and diversity of multiple fully connected parts. We conduct extensive experiments against 12 representative attacks (white-box and black-box), based on the popular dataset. The empirical evaluations show that our scheme realizes significant outcomes against various attacks with negligible additional training overhead, while hardly bringing collateral damage for clean-instance accuracy. | Ziming Zhao, Zhaoxuan Li, Tingting Li, Jiongchi Yu, Fan Zhang, Rui Zhang |  |
| 393 |  |  [Thought Graph: Generating Thought Process for Biological Reasoning](https://doi.org/10.1145/3589335.3651572) |  | 0 | We present the Thought Graph as a novel framework to support complex reasoning and use gene set analysis as an example to uncover semantic relationships between biological processes. Our framework stands out for its ability to provide a deeper understanding of gene sets, significantly surpassing GSEA by 40.28 to human annotations. Our analysis further provides insights into future directions of biological processes naming, and implications for bioinformatics and precision medicine. | ChiYang Hsu, Kyle Cox, Jiawei Xu, Zhen Tan, Tianhua Zhai, Mengzhou Hu, Dexter Pratt, Tianlong Chen, Ziniu Hu, Ying Ding |  |
| 394 |  |  [On the Scale-Free Property of Citation Networks: An Empirical Study](https://doi.org/10.1145/3589335.3651541) |  | 0 | Citation networks have been thought to exhibit scale-free property for many years; however, this assertion has been doubted recently. In this paper, we conduct extensive experiments to resolve this controversial issue. We firstly demonstrate the scale-free property in scale-free networks sampled from the popular Barabasi-Albert (BA) model. To this end, we employ a merged rank distribution, which is divided into outliers, power-law segment, and non-power-law data, to characterize network degrees, and propose a random sample consensus (RANSAC)-based method to identify power-law segments from merged rank distributions, and use the Kolmogorov-Smirnov (KS) test to examine the scale-free property in power-law segments. Subsequently, we apply the same methods to examine the scale-free property in real-world citation networks. Experimental results confirm the scale-free property in citation networks and attribute previous skepticism to the presence of outliers. | Xiaoshi Zhong, Huizhi Liang |  |
| 395 |  |  [Automatic Construction of Expiration Time Expression Dataset from Retweets](https://doi.org/10.1145/3589335.3651471) |  | 0 | Documents describing information with expiration time often include time expressions specifying the expiration time. To train a classifier determining if a time expression represents an expiration time, we need a labeled dataset. We propose a method of automatically constructing such a dataset. Our method collects tweets including time expressions, and automatically determines whether the time expressions represent expiration times based on temporal changes in the frequency of retweets. Our experimental result shows that our method produces an effective dataset. | Hirotaka Nagashima, Keishi Tajima |  |
| 396 |  |  [Finding Dense and Persistently Expansive Subgraphs](https://doi.org/10.1145/3589335.3651507) |  | 0 | How can we detect a group of individuals whose connectivity persists and even strengthens over time? Despite extensive research on temporal networks, this practically pertinent question has been scantily investigated. In this paper, we formulate the problem of selecting a subset of nodes whose induced subgraph maximizes the overall edge count while abiding by time-aware spectral connectivity constraints. We solve the problem via a semidefinite programming (SDP) relaxation. Our experiments on a broad array of synthetic and real-world data establish the effectiveness of our method and deliver key insights on real-world temporal graphs. | Petros Petsinis, Charalampos E. Tsourakakis, Panagiotis Karras |  |
| 397 |  |  [Hyperbolic Heterogeneous Graph Attention Networks](https://doi.org/10.1145/3589335.3651522) |  | 0 | Most previous heterogeneous graph embedding models represent elements in a heterogeneous graph as vector representations in a low-dimensional Euclidean space. However, because heterogeneous graphs inherently possess complex structures, such as hierarchical or power-law structures, distortions can occur when representing them in Euclidean space. To overcome this limitation, we propose Hyperbolic Heterogeneous Graph Attention Networks (HHGAT) that learn vector representations in hyperbolic spaces with meta-path instances. We conducted experiments on three real-world heterogeneous graph datasets, demonstrating that HHGAT outperforms state-of-the-art heterogeneous graph embedding models in node classification and clustering tasks. | Jongmin Park, Seunghoon Han, Soohwan Jeong, Sungsu Lim |  |
| 398 |  |  [Task-Driven Quantum Device Fingerprint Identification via Modeling QNN Outcome Shift Induced by Quantum Noise](https://doi.org/10.1145/3589335.3651567) |  | 0 | Quantum computing (QC) has recently achieved significant technological advancements, attracting widespread attention. Current users mainly access QC resources through cloud services. However, cloud-based quantum services provide convenience while also introducing security risks. For example, attackers could steal private information or inject malicious programs into quantum devices, while quantum device fingerprinting may be the first step for these malicious intents. In this paper, we propose a novel Task-Driven Quantum Device Fingerprinting (TD-QDF) identification method based on quantum neural network (QNN) task outcomes. Unlike previous research, our method does not require any hardware details, resulting in high availability in practice. Extensive experiments involving 3 QNN circuits on 10 real IBM quantum computers show that our method can effectively identify quantum devices. This research contributes to advancing quantum fingerprinting technologies and holds promising implications for enhancing the security and accountability of quantum computing systems. | Tingting Li, Ziming Zhao, Jianwei Yin |  |
| 399 |  |  [News-Driven Price Movement Forecasting with Label-Prior Graph Attention](https://doi.org/10.1145/3589335.3651539) |  | 0 | This paper introduces a novel approach to stock movement prediction using multi-label classification, leveraging the interconnections between news articles and related company stocks. We present the Label-Prior Graph Attention (LPGA) model, which significantly enhances the performance of news-driven stock price movement forecasting. This model is comprised of a unique graph attention architecture, incorporating a label encoder and a text encoder, designed to effectively capture and utilize the relationships between labels in a graph-based context. Our model demonstrates superior performance over several benchmark models. The LPGA model's efficacy is further validated through experiments on two multi-label datasets. The model outperforms established baseline models across various evaluation metrics. The success of the LPGA model in both stock movement prediction and general multi-label classification tasks indicates its potential as a versatile tool in the realm of machine learning and financial analysis. | YiTing Liu, ChungChi Chen, HenHsen Huang, HsinHsi Chen |  |
| 400 |  |  [Enabling Patient-side Disease Prediction via the Integration of Patient Narratives](https://doi.org/10.1145/3589335.3651498) |  | 0 | Disease prediction holds considerable significance in modern healthcare, because of its crucial role in facilitating early intervention and implementing effective prevention measures. However, most recent disease prediction approaches heavily rely on laboratory test outcomes (e.g., blood tests and medical imaging from X-rays). Gaining access to such data for precise disease prediction is often a complex task from the standpoint of a patient and is always only available post-patient consultation. To make disease prediction available from patient-side, we propose Personalized Medical Disease Prediction (PoMP), which predicts diseases using patient health narratives including textual descriptions and demographic information. By applying PoMP, patients can gain a clearer comprehension of their conditions, empowering them to directly seek appropriate medical specialists and thereby reducing the time spent navigating healthcare communication to locate suitable doctors. We conducted extensive experiments using real-world data from Haodf to showcase the effectiveness of PoMP. | Zhixiang Su, Yinan Zhang, Jiazheng Jing, Jie Xiao, Zhiqi Shen |  |
| 401 |  |  [LinkGuard: Link Locally Privacy-Preserving Graph Neural Networks with Integrated Denoising and Private Learning](https://doi.org/10.1145/3589335.3651533) |  | 0 | Recent studies have introduced privacy-preserving graph neural networks to safeguard the privacy of sensitive link information in graphs. However, existing link protection mechanisms in GNNs, particularly over decentralized nodes, struggle to strike an optimal balance between privacy and utility. We argue that a pivotal issue is the separation of noisy topology denoising and GNN private learning into distinct phases at the server side, leading to an under-denoising problem in the noisy topology. To address this, we propose a dynamic, adaptive Link LDP framework that performs noisy topology denoising on the server side in a dynamic manner. This approach aims to mitigate the impact of local noise on the GNN training process, reducing the uncertainty introduced by local noise. Furthermore, we integrate the noise generation and private training processes across all existing Link LDP GNNs into a unified framework. Experimental results demonstrate that our method surpasses existing approaches, obtaining around a 7% performance improvement under strong privacy strength and achieving a better trade-off between utility and privacy. | Yuxin Qi, Xi Lin, Ziyao Liu, Gaolei Li, Jingyu Wang, Jianhua Li |  |
| 402 |  |  [Generator-Guided Crowd Reaction Assessment](https://doi.org/10.1145/3589335.3651512) |  | 0 | In the realm of social media, understanding and predicting post reach is a significant challenge. This paper presents a Crowd Reaction AssessMent (CReAM) task designed to estimate if a given social media post will receive more reaction than another, a particularly essential task for digital marketers and content writers. We introduce the Crowd Reaction Estimation Dataset (CRED), consisting of pairs of tweets from The White House with comparative measures of retweet count. The proposed Generator-Guided Estimation Approach (GGEA) leverages generative Large Language Models (LLMs), such as ChatGPT, FLAN-UL2, and Claude, to guide classification models for making better predictions. Our results reveal that a fine-tuned FLANG-RoBERTa model, utilizing a cross-encoder architecture with tweet content and responses generated by Claude, performs optimally. We further use a T5-based paraphraser to generate paraphrases of a given post and demonstrate GGEA's ability to predict which post will elicit the most reactions. We believe this novel application of LLMs provides a significant advancement in predicting social media post reach. | Sohom Ghosh, ChungChi Chen, Sudip Kumar Naskar |  |
| 403 |  |  [Hierarchical Tensor Clustering for Multiple Graphs Representation](https://doi.org/10.1145/3589335.3651519) |  | 0 | Graph clustering is a challenging task, especially when there is a hierarchical structure. The availability of multiple graphs (or relational graphs), in the multi-graph setting, provides additional information that can be leveraged to improve clustering results. This paper aims to develop a new hierarchical clustering algorithm for multi-graphs, the HTGM algorithm. This algorithm represents the set of graphs in the multi-graph as a 3-way tensor, and maximizes a modularity measure, extending the modularity-based graph clustering algorithm to multi-graphs and tensor structures. We evaluate the proposed algorithm over synthetic and real-world datasets and show the effectiveness of the proposed algorithm by benchmarking it to alternative clustering algorithms. | Karima Boutalbi, Rafika Boutalbi, Hervé Verjus, Kavé Salamatian |  |
| 404 |  |  [Smooth Anonymity for Sparse Graphs](https://doi.org/10.1145/3589335.3651561) |  | 0 | When working with user data providing well-defined privacy guarantees is paramount. In this work, we aim to manipulate and share an entire sparse dataset with a third party privately. In fact, differential privacy has emerged as the gold standard of privacy, however, when it comes to sharing sparse datasets, e.g. sparse networks, as one of our main results, we prove that any differentially private mechanism that maintains a reasonable similarity with the initial dataset is doomed to have a very weak privacy guarantee. In such situations, we need to look into other privacy notions such as k-anonymity. In this work, we consider a variation of k-anonymity, which we call smooth-k-anonymity, and design simple large-scale algorithms that efficiently provide smooth-k-anonymity. We further perform an empirical evaluation to back our theoretical guarantees and show that our algorithm improves the performance in downstream machine learning tasks on anonymized data. | Alessandro Epasto, Hossein Esfandiari, Vahab Mirrokni, Andrés Muñoz Medina |  |
| 405 |  |  [Predicting Node Influence in Complex Networks by the K-Shell Entropy and Degree Centrality](https://doi.org/10.1145/3589335.3651547) |  | 0 | Currently, with the expansion of the use of social networks, the topic of information dissemination has achieved significant importance. The spread of rumors and efforts to stop them have led researchers to pay more attention than ever to predicting the impact of each individual on the network. Various methods have been proposed for this purpose, such as the Hybrid global structure model (HGSM), Generalized Gravity centrality (GGC), and Degree and neighborhood centrality (DNC). However, alongside their advantages, they have drawbacks, such as high time complexity, low accuracy, or inefficiency in distinguishing between the dissemination abilities of different individuals. Therefore, this paper focuses on a method based on degree, K-shell, and K-shell diversity in the neighborhood of each individual. Simulations were conducted using the Susceptible-Infected-Recovered (SIR) model and compared with 9 recent methods. Evaluations of 7 different networks in terms of resolution, accuracy, time complexity, and correlation exhibit the superiority of the proposed method. | Shima Esfandiari, Mostafa Fakhrahmad |  |
| 406 |  |  [Knowledge Induced Transformer Network for Causality Prediction](https://doi.org/10.1145/3589335.3651531) |  | 0 | Causal extraction from text plays a crucial role in various downstream analytical and predictive tasks, such as constructing repositories of causal insights for reasoning. However, existing models often overlook the rich contextual commonsense knowledge that could enhance the reasoning process and evaluate underlying causal mechanisms. In this study, we introduce a knowledge-induced transformer architecture for predicting causality. Our model accepts an antecedent and a set of contextual knowledge as input, then ranks plausible consequences from a given set of hypotheses. To enhance semantic understanding, we augment the transformer with a relational graph network, which computes fine-grained semantic information between the antecedent, knowledge, and hypotheses using a similarity matrix that quantifies word-to-word similarity. We evaluate the proposed architecture against state-of-the-art models using openly available datasets and demonstrate its superior performance. | Tirthankar Dasgupta, Manjira Sinha, Abir Naskar |  |
| 407 |  |  [MetroGNN: Metro Network Expansion with Reinforcement Learning](https://doi.org/10.1145/3589335.3651536) |  | 0 | Selecting urban regions for metro network expansion to meet maximal transportation demands is crucial for urban development, while computationally challenging to solve. The expansion process relies not only on complicated features like urban demographics and origin-destination (OD) flow but is also constrained by the existing metro network and urban geography. In this paper, we introduce a reinforcement learning framework to address a Markov decision process within an urban heterogeneous multi-graph. Our approach employs an attentive policy network that intelligently selects nodes based on information captured by a graph neural network. Experiments on real-world urban data demonstrate that our proposed methodology substantially improve the satisfied transportation demands by over 30% when compared with state-of-the-art methods. Codes are published at https://github.com/tsinghua-fib-lab/MetroGNN. | Hongyuan Su, Yu Zheng, Jingtao Ding, Depeng Jin, Yong Li |  |
| 408 |  |  [Breaking the Bot Barrier: Evaluating Adversarial AI Techniques Against Multi-Modal Defense Models](https://doi.org/10.1145/3589335.3651474) |  | 0 | Websites utilize several approaches to detect automated agents. The agents are deployed either for beneficial purposes such as search engine crawlers, or to perform tasks on behalf of the adversary such as scanning for vulnerabilities. Recent methods in detecting such agents include the analysis of the behavior that the agents show when visiting the website. In this paper, I) we describe a deep learning framework that analyzes the triggered browser events to classify the visitor. II) We develop two adversarial attacks in order to bypass the defense by generating adversarial vectors that are misclassified by the model. III) We discuss how applicable the attacks are by reviewing the limitations of the popular tools (i.e., Selenium and Puppeteer) used for the development of automated agents based on full-fledged browsers. | Behzad Ousat, Dongsheng Luo, Amin Kharraz |  |
| 409 |  |  [RealGraphGPU++: A High-Performance GPU-Based Graph Engine with Direct Storage-to-DM IO](https://doi.org/10.1145/3589335.3651549) |  | 0 | Recently, with the increasing size of real-world networks, graph engines have been studied extensively for efficient graph analysis. As one of the state-of-the-art single-machine-based graph engines, \textRealGraph ^\textGPU processes large-scale graphs very efficiently thanks to its well-designed architecture and the strong parallel-computing power of GPU. Via a preliminary analysis, we first observe \textRealGraph ^\textGPU has a good chance for more performance improvement in IOs between storage and GPU's device memory. This motivates us to present \textRealGraph ^\textGPU++, a solution that substantially reduces IO time by establishing adirect data path between storage and device memory. Additionally, it employsasynchronous processing of CPU and GPU tasks to issue IO requests more frequently, thereby improving overall performance by achieving higher IO bandwidth. Experimental results on real-world datasets show that \textRealGraph ^\textGPU++ outperforms dramatically existing 11 state-of-the-art graph engines including \textRealGraph ^\textGPU . | JeongMin Park, MyungHwan Jang, DuckHo Bae, SangWook Kim |  |
| 410 |  |  [A Study of Vulnerability Repair in JavaScript Programs with Large Language Models](https://doi.org/10.1145/3589335.3651463) |  | 0 | In recent years, JavaScript has become the most widely used programming language, especially in web development. However, writing secure JavaScript code is not trivial, and programmers often make mistakes that lead to security vulnerabilities in web applications. Large Language Models (LLMs) have demonstrated substantial advancements across multiple domains, and their evolving capabilities indicate their potential for automatic code generation based on a required specification, including automatic bug fixing. In this study, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding and fixing security vulnerabilities in JavaScript programs. We also investigate the impact of context in a prompt on directing LLMs to produce a correct patch of vulnerable JavaScript code. Our experiments on real-world software vulnerabilities show that while LLMs are promising in automatic program repair of JavaScript code, achieving a correct bug fix often requires an appropriate amount of context in the prompt. | Tan Khang Le, Saba Alimadadi, Steven Y. Ko |  |
| 411 |  |  [Deanonymizing Transactions Originating from Monero Tor Hidden Service Nodes](https://doi.org/10.1145/3589335.3651487) |  | 0 | Monero is a privacy-focused cryptocurrency that incorporates anonymity networks (such as Tor and I2P) and deploys the Dandelion++ protocol to prevent malicious attackers from linking transactions with their source IPs. However, this paper highlights a vulnerability in Monero's integration of the Tor network, which allows an attacker to successfully deanonymize transactions originating from Monero Tor hidden service nodes at the network-layer level. Our approach involves injecting malicious Monero Tor hidden service nodes into the Monero P2P network to correlate the onion addresses of incoming Monero Tor hidden service peers with their originating transactions. And by sending a signal watermark embedded with the onion address to the Tor circuit, we establish a correlation between the onion address and IP address of a Monero Tor hidden service node. Ultimately, we correlate transactions and IPs of Monero Tor hidden service nodes. Through experimentation on the Monero testnet, we provide empirical evidence of the effectiveness of our approach in successfully deanonymizing transactions originating from Monero Tor hidden service nodes. | Ruisheng Shi, Yulian Ge, Lina Lan, Zhiyuan Peng, Shenwen Lin, Lin Li |  |
| 412 |  |  [WebGraph: The Next Generation (Is in Rust)](https://doi.org/10.1145/3589335.3651581) |  | 0 | We report the results of a yearlong effort at the Laboratory for Web Algorithmics and Inria to port the WebGraph framework [4] from Java to Rust. For two decades WebGraph has been instrumental in the analysis and distribution of large graphs for the research community of TheWebConf, but the intrinsic limitations of the Java Virtual Machine had become a bottleneck for very large use cases, such as the Software Heritage Merkle graph [2] with its half a trillion arcs. As part of this clean-slate implementation of WebGraph in Rust, we developed a few ancillary projects bringing to the Rust ecosystem some missing features of independent interest, such as easy, consistent and zero-cost memory mapping of data structures. WebGraph in Rust offers impressive performance improvements over the previous implementation, enabling open-source graph analytics on very large datasets like Common Crawl, on top of a modern systems programming language. | Tommaso Fontana, Sebastiano Vigna, Stefano Zacchiroli |  |
| 413 |  |  [GradFilt: Class-wise Targeted Data Reconstruction from Gradients in Federated Learning](https://doi.org/10.1145/3589335.3651514) |  | 0 | Gradient Inversion Attacks (GIAs) have shown that private training data can be recovered from gradient updates in Federated Learning (FL). However, these GIAs can only recover the entire batch of data with limited performance or stochastically restore some random instances. In this paper, we propose a class-wise targeted attack, named GradFilt, which can reconstruct the training data of some specified class(es) from the batch-averaged gradients. By modifying the parameters of the classification layer, we create a filter within the FL model that eliminates the gradients of non-target data while preserving the gradients of target data. We evaluate GradFilt with image datasets on popular FL model architectures. The results show that GradFilt can effectively reconstruct the desired samples with higher accuracies than the existing GIAs. Moreover, we can also achieve 100% success rate in restoring the batch labels. We hope this work can raise awareness of the privacy risks in FL and inspire effective defense mechanisms. | Rui Zhang, Song Guo, Ping Li |  |
| 414 |  |  [Advancing Stance Detection of Political Fan Pages: A Multimodal Approach](https://doi.org/10.1145/3589335.3651467) |  | 0 | The evolution of political campaigns is evident with the ascent of social media. Ideological beliefs are increasingly disseminated through political-affiliated fan pages. The interaction between politicians and the general public on these platforms plays a pivotal role in election outcomes. In this study, we utilize a multimodal approach to explore and quantify similarities of ideologies among political fan pages. we employed visualization techniques to demonstrate the political stance of each fan page. To validate our proposal, we concentrated on an analysis of the 2021 national referendums in Taiwan, encompassing a collection of fan pages and their corresponding posts that were related to these referendums. Through a qualitative analysis of the content of these fan pages, the efficacy of our multimodal framework in clustering fan pages according to their respective political ideologies was evaluated. The findings of this study underscore the significant enhancement in the accuracy of stance detection when integrating multiple modalities of data, namely textual content, visual imagery, and user interactions. | KuanHung Kuo, MingHung Wang, HungYu Kao, YuChen Dai |  |
| 415 |  |  [Structural Podcast Content Modeling with Generalizability](https://doi.org/10.1145/3589335.3651563) |  | 0 | Podcast content modeling is crucial for a variety of practical web uses, such as the recommendation and classification of podcasts. However, previous studies on podcast content modeling rely on task-specific datasets to train dedicated models for each downstream application, which are labels heavily dependent and the learned representations are non-generalizable across different tasks. In addition, the rich and intricate structural information among users, podcasts, and topics are neglected. In this paper, we propose to model podcast content without labels and learn general podcast representations without prior knowledge of downstream tasks. Moreover, the learned podcast representations encode crucial structural information, complementary to the independent content information of each podcast. In particular, we first collect a new and large-scale podcast graph from Spotify. Then, we propose Podcast2Vec, a novel self-supervised podcast content modeling method to learn podcast representations. Podcast2Vec captures general transferable knowledge across different tasks and complex structures via a metapath-based neighbor sampling strategy and a multi-view relational modeling framework. Thorough experiments demonstrate the superiority of our method on four real-world podcast content modeling tasks. | Yijun Tian, Maryam Aziz, Alice Wang, Enrico Palumbo, Hugues Bouchard |  |
| 416 |  |  [Detecting Poisoning Attacks on Federated Learning Using Gradient-Weighted Class Activation Mapping](https://doi.org/10.1145/3589335.3651490) |  | 0 | This paper proposes a new defense mechanism, namely, GCAMA, against model poisoning attacks on Federated learning (FL), which integrates <u>G</u>radient-weighted <u>C</u>lass <u>A</u>ctivation <u>M</u>apping (GradCAM) and <u>A</u>utoencoder to offer a scientifically more powerful detection capability compared to existing Euclidean distance-based approaches. Particularly, GCAMA generates a heat map for each uploaded local model update, transforming each local model update into a lower-dimensional, visual representation, thereby accentuating the hidden features of the heat maps and increasing the success rate of identifying anomalous heat maps and malicious local models. We test ResNet-18 and MobileNetV3-Large deep learning models with CIFAR-10 and GTSRB datasets under Non-Independent and Identically Distributed (Non-IID) setting, respectively. The results demonstrate that GCAMA offers superior test accuracy of FL global model compared to the state-of-the-art methods. Our code is available at: https://github.com/jjzgeeks/GradCAM-AE | Jingjing Zheng, Kai Li, Xin Yuan, Wei Ni, Eduardo Tovar |  |
| 417 |  |  [Fighting against Fake News on Newly-Emerging Crisis: A Case Study of COVID-19](https://doi.org/10.1145/3589335.3651506) |  | 0 | As social media users can easily access, generate, and spread information regardless of its authenticity, the proliferation of fake news related to public health has become a serious problem. Since these rumors have caused severe social issues, detecting them in the early stage is imminent. Therefore, in this paper, we propose a deep learning model that can debunk fake news on COVID-19, as a case study, at the initial stage of emergence. The evaluation with a newly-collected dataset consisting of both the COVID-19 and Non-COVID-19 fake news claims demonstrates that the proposed model achieves high performance, indicating that the model can identify fake news on COVID-19 in the early stage with a small amount of data. We believe that our methodology and findings can be applied to detect fake news on newly-emerging and critical topics, which should be performed with insufficient resources. | Migyeong Yang, Chaewon Park, Jiwon Kang, Daeun Lee, Daejin Choi, Jinyoung Han |  |
| 418 |  |  [Targeted Filter Bubbles Mitigating via Edges Insertion](https://doi.org/10.1145/3589335.3651566) |  | 0 | The emergence of filter bubbles leads to various harms. To mitigate filter bubbles, some recent works select the seeds for different viewpoints to minimize the formation of bubbles under the influence propagation model. Different from these works where the diffusion networks remain unchanged, in this paper, we conduct the first attempt to mitigate filter bubbles via edge insertion. Besides, to be more generalized, we focus on mitigating filter bubbles for the given target node set since the audiences can be different for different scenarios. Specifically, we propose the concept of openness score for each target node, which serves as a metric to assess the likelihood of this node being influenced by multiple viewpoints simultaneously. Given a directed graph G, two seed sets, a positive integer k and a target node set, we aim to find k edges incident to the given seeds such that the total openness score is maximized. We prove the NP-hardness of problem studied. A baseline method is first presented by extending the greedy framework. To handle large graphs efficiently, we develop a sampling-based strategy. A data-dependent approximation method is developed with theoretical guarantees. Experiments over real social networks are conducted to demonstrate the advantages of proposed techniques. | Yanping Wu, Jinghao Wang, Renjie Sun, Chen Chen, Xiaoyang Wang, Ying Zhang |  |
| 419 |  |  [Unveiling Wash Trading in Popular NFT Markets](https://doi.org/10.1145/3589335.3651580) |  | 0 | As emerging digital assets, NFTs are susceptible to anomalous trading behaviors due to the lack of stringent regulatory mechanisms, potentially causing economic losses. In this paper, we conduct the first systematic analysis of four non-fungible tokens (NFT) markets. Specifically, we analyze more than 25 million transactions within these markets, to explore the evolution of wash trade activities. Furthermore, we propose a heuristic algorithm that integrates the network characteristics of transactions with behavioral analysis, to detect wash trading activities in NFT markets. Our findings indicate that NFT markets with incentivized structures exhibit higher proportions of wash trading volume compared to those without incentives. Notably, the LooksRare and X2Y2 markets are detected with wash trading volume proportions as high as 94.5 | Yuanzheng Niu, Xiaoqi Li, Hongli Peng, Wenkai Li |  |
| 420 |  |  [Understanding Deployment Experience of 5G](https://doi.org/10.1145/3589335.3651577) |  | 0 | The global rollout of 5G mobile networks has prompted discussions on deployment strategies. Given the knowledge gap in the current deployment strategies of 5G base stations, understanding the deployment experience from regions with widespread 5G base stations is valuable for guiding future deployments elsewhere. In this study, based on a large data set collected from a metropolitan city in China, we discover the misalignment between 5G traffic demand and the number of base stations. Then we introduce a factor to quantify the misalignment. Our analysis indicates the following important observations. Firstly, unique traffic patterns of functional areas contribute to different misalignment factors, i.e., transport areas exhibit a positive factor, in contrast to the negative factor observed in urban comprehensive and residential areas. Secondly, regions with a high density of base stations still suffer from low energy and resource utilization efficiency due to their high energy consumption. Thirdly, our analysis reveals that 5G base stations are frequently located in areas with large 4G traffic, yet the incomplete migration of traffic to 5G results in misalignment. This understanding of the 5G deployment experience can help further studies on optimizing energy efficiency and network utilization rate of the mobile networks. | Ziyi Liu, Huandong Wang, Yong Li |  |
| 421 |  |  [FaST: Accelerating Web Front-end Data Binding with Compiler and Visible Anchor](https://doi.org/10.1145/3589335.3651505) |  | 0 | Data binding in web front-end development has made a significant contribution to removing complexity from development and simplifying programming. However, data binding has caused a degradation of website performance at the cost of reducing the burden on programmers. In this paper, we propose Visible Anchor to solve the performance degradation caused by data binding. We develop a compiler called FaST that implements the method. Then, We compared the rendering time among websites built by existing methods and FaST compiler. The evaluation result revealed that the websites built by FaST compiler are at minimum 2.9 times faster to be rendered than the ones built by the existing methods. FaST made a significant contribution to improving the performance of web front-end data binding. Consequently, data binding with FaST can be a better choice for web front-end development. | Tatsuru Tomizawa, Seiki Makino, Taiga Kume, Satoki Hamanaka, Tadashi Okoshi, Jin Nakazawa |  |
| 422 |  |  [How We Refute Claims: Automatic Fact-Checking through Flaw Identification and Explanation](https://doi.org/10.1145/3589335.3651521) |  | 0 | Automated fact-checking is a crucial task in the governance of internet content. Although various studies utilize advanced models to tackle this issue, a significant gap persists in addressing complex real-world rumors and deceptive claims. To address this challenge, this paper explores the novel task of flaw-oriented fact-checking, including aspect generation and flaw identification. We also introduce RefuteClaim, a new framework designed specifically for this task. Given the absence of an existing dataset, we present FlawCheck, a dataset created by extracting and transforming insights from expert reviews into relevant aspects and identified flaws. The experimental results underscore the efficacy of RefuteClaim, particularly in classifying and elucidating false claims. | WeiYu Kao, AnZi Yen |  |
| 423 |  |  [Characterizing the Solana NFT Ecosystem](https://doi.org/10.1145/3589335.3651478) |  | 0 | Non-Fungible Tokens (NFTs) are digital assets recorded on the blockchain, providing cryptographic proof of ownership over digital or physical items. Although Solana has only begun to gain popularity in recent years, its NFT market has seen substantial transaction volumes. In this paper, we conduct the first systematic research on the characteristics of Solana NFTs from two perspectives: longitudinal measurement and wash trading security audit. We gathered 132,736 Solana NFT from Solscan and analyzed the sales data within these collections. Investigating users' economic activity and NFT owner information reveals that the top users in Solana NFT are skewed toward a higher distribution of purchases. Subsequently, we employ the Local Outlier Factor algorithm to conduct a wash trading audit on 2,175 popular Solana NFTs. We discovered that 138 NFT pools are involved in wash trading, with 8 of these NFTs having a wash trading rate exceeding 50 have been entirely washed out. | Dechao Kong, Xiaoqi Li, Wenkai Li |  |
| 424 |  |  [Multi-round Counterfactual Generation: Interpreting and Improving Models of Text Classification](https://doi.org/10.1145/3589335.3651537) |  | 0 | In recent years, natural language processing (NLP) models have demonstrated remarkable performance in text classification tasks. However, trust in the decision-making process requires a deeper understanding of the operational principles of these networks. Therefore, there is an urgent need to enhance transparency and the interpretability of these "black boxes". Aligned with this, we propose a model-agnostic interpretability method named MCG. This method generates counterfactual interpretations that are more faithful to the original models' performance through a multi-round dialogue, in which a new template is generated based on the evaluation of the previous counterfactual interpretation. In addition, MCG proposes a solution to improve model performance through counterfactual data augmentation for cases where the model to be interpreted is misclassified, which is rarely covered by existing counterfactual methods. Extensive experiments on three datasets demonstrate that our MCG outperforms current state-of-the-art methods in counterfactual generation for interpretability. | Huajie Zhang, Yuxin Ying, Fuzhen Zhuang, Haiqin Weng, Sun Ying, Zhao Zhang, Yiqi Tong, Yan Liu |  |
| 425 |  |  [iSpLib: A Library for Accelerating Graph Neural Networks using Auto-tuned Sparse Operations](https://doi.org/10.1145/3589335.3651528) |  | 0 | Core computations in Graph Neural Network (GNN) training and inference are often mapped to sparse matrix operations such as sparse-dense matrix multiplication (SpMM). These sparse operations are harder to optimize by manual tuning because their performance depends significantly on the sparsity of input graphs, GNN models, and computing platforms. To address this challenge, we present iSpLib, a PyTorch-based C++ library equipped with auto-tuned sparse operations. iSpLib expedites GNN training with a cache-enabled backpropagation that stores intermediate matrices in local caches. The library offers a user-friendly Python plug-in that allows users to take advantage of our optimized PyTorch operations out-of-the-box for any existing linear algebra-based PyTorch implementation of popular GNNs (Graph Convolution Network, GraphSAGE, Graph Inference Network, etc.) with only two lines of additional code. We demonstrate that iSpLib obtains up to 27x overall training speedup compared to the equivalent PyTorch 2.1.0 and PyTorch Geometric 2.4.0 implementations on the CPU. Our library is publicly available at https://github.com/HipGraph/iSpLib (https://doi.org/10.5281/zenodo.10806511). | Md Saidul Hoque Anik, Pranav Badhe, Rohit Gampa, Ariful Azad |  |
| 426 |  |  [DeFiTail: DeFi Protocol Inspection through Cross-Contract Execution Analysis](https://doi.org/10.1145/3589335.3651488) |  | 0 | Decentralized finance (DeFi) protocols are crypto projects developed on the blockchain to manage digital assets. Attacks on DeFi have been frequent and have resulted in losses exceeding $77 billion. However, detection methods for malicious DeFi events are still lacking. In this paper, we propose DeFiTail, the first framework that utilizes deep learning to detect access control and flash loan exploits that may occur on DeFi. Since the DeFi protocol events involve invocations with multi-account transactions, which requires execution path unification with different contracts. Moreover, to mitigate the impact of mistakes in Control Flow Graph (CFG) connections, we validate the data path by employing the symbolic execution stack. Furthermore, we feed the data paths through our model to achieve the inspection of DeFi protocols. Experimental results indicate that DeFiTail achieves the highest accuracy, with 98.39 access control and 97.43 enhanced capability to detect malicious contracts, identifying 86.67 from the CVE dataset. | Wenkai Li, Xiaoqi Li, Yuqing Zhang, Zongwei Li |  |
| 427 |  |  [Are we Making Much Progress? Revisiting Chemical Reaction Yield Prediction from an Imbalanced Regression Perspective](https://doi.org/10.1145/3589335.3651470) |  | 0 | The yield of a chemical reaction quantifies the percentage of the target product formed in relation to the reactants consumed during the chemical reaction. Accurate yield prediction can guide chemists toward selecting high-yield reactions during synthesis planning, offering valuable insights before dedicating time and resources to wet lab experiments. While recent advancements in yield prediction have led to overall performance improvement across the entire yield range, an open challenge remains in enhancing predictions for high-yield reactions, which are of greater concern to chemists. In this paper, we argue that the performance gap in high-yield predictions results from the imbalanced distribution of real-world data skewed towards low-yield reactions, often due to unreacted starting materials and inherent ambiguities in the reaction processes. Despite this data imbalance, existing yield prediction methods continue to treat different yield ranges equally, assuming a balanced training distribution. Through extensive experiments on three real-world yield prediction datasets, we emphasize the urgent need to reframe reaction yield prediction as an imbalanced regression problem. Finally, we demonstrate that incorporating simple cost-sensitive re-weighting methods can significantly enhance the performance of yield prediction models on underrepresented high-yield regions. | Yihong Ma, Xiaobao Huang, Bozhao Nan, Nuno Moniz, Xiangliang Zhang, Olaf Wiest, Nitesh V. Chawla |  |
| 428 |  |  [Simple Multigraph Convolution Networks](https://doi.org/10.1145/3589335.3651560) |  | 0 | Existing multigraph convolution methods either ignore the cross-view interaction among multiple graphs, or induce extremely high computational cost due to standard cross-view polynomial operators. To alleviate this problem, this paper proposes a Simple MultiGraph Convolution Networks (SMGCN) which first extracts consistent cross-view topology from multigraphs including edge-level and subgraph-level topology, then performs polynomial expansion based on raw multigraphs and consistent topologies. In theory, SMGCN utilizes the consistent topologies in polynomial expansion rather than standard cross-view polynomial expansion, which performs credible cross-view spatial message-passing, follows the spectral convolution paradigm, and effectively reduces the complexity of standard polynomial expansion. In the simulations, experimental results demonstrate that SMGCN achieves state-of-the-art performance on ACM and DBLP multigraph benchmark datasets. Our codes are available at https://github.com/frinkleko/SMGCN. | Danyang Wu, Xinjie Shen, Jitao Lu, Jin Xu, Feiping Nie |  |
| 429 |  |  [Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks](https://doi.org/10.1145/3589335.3651555) |  | 0 | Recent studies have revealed that federated learning (FL), once considered secure due to clients not sharing their private data with the server, is vulnerable to attacks such as client-side training data distribution inference, where a malicious client can recreate the victim's data. While various countermeasures exist, they are not practical, often assuming server access to some training data or knowledge of label distribution before the attack. In this work, we bridge the gap by proposing InferGuard, a novel Byzantine-robust aggregation rule aimed at defending against client-side training data distribution inference attacks. In our proposed InferGuard, the server first calculates the coordinate-wise median of all the model updates it receives. A client's model update is considered malicious if it significantly deviates from the computed median update. We conduct a thorough evaluation of our proposed InferGuard on five benchmark datasets and perform a comparison with ten baseline methods. The results of our experiments indicate that our defense mechanism is highly effective in protecting against client-side training data distribution inference attacks, even against strong adaptive attacks. Furthermore, our method substantially outperforms the baseline methods in various practical FL scenarios. | Yichang Xu, Ming Yin, Minghong Fang, Neil Zhenqiang Gong |  |
| 430 |  |  [Group-wise K-anonymity meets (ε, δ) Differentially Privacy Scheme](https://doi.org/10.1145/3589335.3651517) |  | 0 | We studied the link between K-anonymity and differential privacy as the basis for deriving a novel method for noise estimation. Hence, we provide threefold contributions: First, we use the birthday-bound paradox for uniqueness to estimate the noise level, ε in (ε, δ) differentially privacy scheme. Second, our group-aware formulation provides resilience to a series of inference attacks by using the group privacy property in our unique group-centric formulation. Third, draw a connection between the attacker advantage, δ, and ε for univariate and multivariate cases. Finally, we demonstrate applicability in Laplacian, Gaussian, and Exponential mechanisms. | Kenneth Odoh |  |
| 431 |  |  [Generating Privacy-preserving Educational Data Records with Diffusion Model](https://doi.org/10.1145/3589335.3651511) |  | 0 | Educational Data Records (EDR) are crucial for capturing teaching behavior and student information, forming the basis for achieving educational intelligence. However, ensuring educational privacy has become a pressing concern, posing practical challenges to the use and sharing of educational data. To address the issue of EDR privacy preserving, we present EduSyn, a privacy data release scheme that utilizes generative diffusion models and differential privacy methods. Specifically, we adopt a diffusion modeling scheme that can be applied to both discrete and continuous types of data to accommodate the data characteristics of EDR, while an invariant Post Randomization (PRAM) perturbation method that satisfies local differential privacy is applied for data attributes that need to be specially protected before model training. We conduct comprehensive validation of this scheme within the domain of education applications, showcasing that EduSyn generates a superior private EDR dataset compared to similar generative methods and strikes a better privacy-utility trade-off. | Quanlong Guan, Yanchong Yu, Xiujie Huang, Liangda Fang, Chaobo He, Lusheng Wu, Weiqi Luo, Guanliang Chen |  |
| 432 |  |  [StateGuard: Detecting State Derailment Defects in Decentralized Exchange Smart Contract](https://doi.org/10.1145/3589335.3651562) |  | 0 | Decentralized Exchanges (DEXs), leveraging blockchain technology and smart contracts, have emerged in decentralized finance. However, the DEX project with multi-contract interaction is accompanied by complex state logic, which makes it challenging to solve state defects. In this paper, we conduct the first systematic study on state derailment defects of DEXs. These defects could lead to incorrect, incomplete, or unauthorized changes to the system state during contract execution, potentially causing security threats. We propose StateGuard, a deep learning-based framework to detect state derailment defects in DEX smart contracts. StateGuard constructs an Abstract Syntax Tree (AST) of the smart contract, extracting key features to generate a graph representation. Then, it leverages a Graph Convolutional Network (GCN) to discover defects. Evaluating StateGuard on 46 DEX projects with 5,671 smart contracts reveals its effectiveness, with a precision of 92.24 we used StateGuard to audit real-world smart contracts and successfully authenticated multiple novel CVEs. | Zongwei Li, Wenkai Li, Xiaoqi Li, Yuqing Zhang |  |
| 433 |  |  [Rumor Mitigation in Social Media Platforms with Deep Reinforcement Learning](https://doi.org/10.1145/3589335.3651556) |  | 0 | Social media platforms have become one of the main channels where people disseminate and acquire information, of which the reliability is severely threatened by rumors widespread in the network. Existing approaches such as suspending users or broadcasting real information to combat rumors are either with high cost or disturbing users. In this paper, we introduce a novel rumor mitigation paradigm, where only a minimal set of links in the social network are intervened to decelerate the propagation of rumors, countering misinformation with low business cost and user awareness. A knowledge-informed agent embodying rumor propagation mechanisms is developed, which intervenes the social network with a graph neural network for capturing information flow in the social media platforms and a policy network for selecting links. Experiments on real social media platforms demonstrate that the proposed approach can effectively alleviate the influence of rumors, substantially reducing the affected populations by over 25 released at https://github.com/tsinghua-fib-lab/DRL-Rumor-Mitigation. | Hongyuan Su, Yu Zheng, Jingtao Ding, Depeng Jin, Yong Li |  |
| 434 |  |  [SWATTING Spambots: Real-time Detection of Malicious Bots on X](https://doi.org/10.1145/3589335.3651564) |  | 0 | Spambot activity has become increasingly pervasive on social media platforms, such as X (formerly known as Twitter), leading to concerns over information quality and user experience. This study presents an innovative approach for real-time detection and reporting of spambots on Twitter platform. Using data analytics technique, we adapted a comprehensive framework capable of accurately identifying and categorizing spambot accounts based on their behavioral patterns and characteristics. By providing an efficient solution to this growing issue, our research aims to enhance user trust in social media communication channels and promote a more transparent and authentic online environment for users to engage with each other and share information. | Cristian Brokate, Manon Richard, Lisa Giordani, Jean Liénard |  |
| 435 |  |  [One-shot Pairing and Authentication Using Moms Secret](https://doi.org/10.1145/3589335.3651542) |  | 0 | The existing pairing and authentication mechanisms adopt either fuzzy commitment or fuzzy password-authenticated key exchange for device fingerprint generation, detecting and correcting multiple symbol errors, leading to guessing attacks and increased pairing time. In this study, we propose a one-shot pairing and authentication approach that generates a device fingerprint from the selected contextual data using Median-of-medians (Moms), ensuring randomness and preventing guessing attacks. Moreover, we integrate the Moms secret into Password Authenticated Key Exchange (PAKE) to reduce the pairing time and improve security. The evaluation demonstrates that our proposed one-shot pairing and authentication approach ensures strong resistance against information gain, reduces the probability of guessing attacks, and significantly decreases the pairing time compared to state-of-the-art approaches. | Ubaid Ur Rehman, Sungyoung Lee |  |
| 436 |  |  [GPT-generated Text Detection: Benchmark Dataset and Tensor-based Detection Method](https://doi.org/10.1145/3589335.3651513) |  | 0 | As natural language models like ChatGPT become increasingly prevalent in applications and services, the need for robust and accurate methods to detect their output is of paramount importance. In this paper, we present GPT Reddit Dataset (GRiD), a novel Generative Pretrained Transformer (GPT)-generated text detection dataset designed to assess the performance of detection models in identifying generated responses from ChatGPT. The dataset consists of a diverse collection of context-prompt pairs based on Reddit, with human-generated and ChatGPT-generated responses. We provide an analysis of the dataset's characteristics, including linguistic diversity, context complexity, and response quality. To showcase the dataset's utility, we benchmark several detection methods on it, demonstrating their efficacy in distinguishing between human and ChatGPT-generated responses. This dataset serves as a resource for evaluating and advancing detection techniques in the context of ChatGPT and contributes to the ongoing efforts to ensure responsible and trustworthy AI-driven communication on the internet. Finally, we propose GpTen, a novel tensor-based GPT text detection method that is semi-supervised in nature since it only has access to human-generated text and performs on par with fully-supervised baselines. | Zubair Qazi, William Shiao, Evangelos E. Papalexakis |  |
| 437 |  |  [Knowledge Guided Conditional Diffusion Model for Controllable Mobile Traffic Generation](https://doi.org/10.1145/3589335.3651530) |  | 0 | Generating mobile traffic in urban contexts is important for network optimization. However, existing solutions show weakness in capturing complex temporal features of mobile traffic. In this paper, we propose a Knowledge-Guided Conditional Diffusion model (KGDiff) for controllable mobile traffic generation, where a customized denoising network of diffusion model is designed to explore the temporal features of mobile traffic. Specifically, we design a frequency attention mechanism that incorporates an Urban Knowledge Graph (UKG) to adaptively capture implicit correlations between mobile traffic and urban environments in the frequency domain. This approach enables the model to generate network traffic corresponding to different environments in a controlled manner, enhancing the model's controllability. Experiments on one real-world dataset show that the proposed framework has good controllability and can improve generation fidelity with gains surpassing 19%. | Haoye Chai, Tong Li, Fenyu Jiang, Shiyuan Zhang, Yong Li |  |
| 438 |  |  [Critical Nodes Detection: Node Merging Approach](https://doi.org/10.1145/3589335.3651485) |  | 0 | Various cohesive models are widely employed for the analysis of social networks to identify critical users or key relationships, with the k-core being a particularly popular approach. Existing works, such as the anchor k-core problem, aim to maximize k-core by anchoring nodes (the degree of anchor nodes are set as infinity). However, we find that node merging can also enlarge the k-core size. Different from anchoring nodes, nodes merging can cause both degree increase and decrease which brings more challenges. In this paper, we study the <u>c</u>ore <u>m</u>aximization by <u>n</u>ode <u>m</u>erging problem (CMNM) and prove its hardness. A greedy framework is first presented due to its hardness. To scale for large networks, we categorize potentially influential nodes and provide a detailed analysis of all node merging pairs. Then, based on these analyses, a fast and effective algorithm is developed. Finally, we conduct comprehensive experiments on real-world networks to evaluate the effectiveness and efficiency of the proposed method. | Hongbo Qiu, Renjie Sun, Chen Chen, Xiaoyang Wang, Ying Zhang |  |
| 439 |  |  [Dual-level Hypergraph Contrastive Learning with Adaptive Temperature Enhancement](https://doi.org/10.1145/3589335.3651493) |  | 0 | Inspired by the success of graph contrastive learning, researchers have begun exploring the benefits of contrastive learning over hypergraphs. However, these works have the following limitations in modeling the high-order relationships over unlabeled data: (i) They primarily focus on maximizing the agreements among individual node embeddings while neglecting the capture of group-wise collective behaviors within hypergraphs; (ii) Most of them disregard the importance of the temperature index in discriminating contrastive pairs during contrast optimization. To address these limitations, we propose a novel dual-level Hy perG raph C ontrastive L earning framework with Ad aptive T emperature (HyGCL-AdT ) to boost contrastive learning over hypergraphs. Specifically, unlike most works that merely maximize the agreement of node embeddings in hypergraphs, we propose a dual-level contrast mechanism that not only captures the individual node behaviors in a local context but also models the group-wise collective behaviors of nodes within hyperedges from a community perspective. Besides, we design an adaptive temperature-enhanced contrastive optimization to improve the discrimination ability between contrastive pairs. Empirical experiments conducted on seven benchmark hypergraphs demonstrate that HyGCL-AdT exhibits excellent effectiveness compared to state-of-the-art baseline models. The source code is available at \hrefhttps://github.com/graphprojects/HyGCL-AdT https://github.com/graphprojects/HyGCL-AdT. | Yiyue Qian, Tianyi Ma, Chuxu Zhang, Yanfang Ye |  |
| 440 |  |  [Towards Understanding Crypto-Asset Risks on Ethereum Caused by Key Leakage on the Internet](https://doi.org/10.1145/3589335.3651573) |  | 0 | In public blockchains, leaking secret keys can cause the permanent loss of crypto assets. It is imperative to understand the illicit activities on blockchains related to leaked keys. This paper presents the first measurement study that uncovers, quantifies, and characterizes the actual misuses of the leaked keys from top websites on the Internet to withdraw assets on Ethereum. By finding key-leaking web pages and joining them with transactions, the study reveals 7.29\*10^6/0.59\*10^6 USD worth of assets on Ethereum mainnet/Binance Smart Chain (BSC) are withdrawn from 1421/1514 leaked secret keys. Mitigations are proposed to avoid the financial loss caused by leaked keys. | Yuxuan Zhou, Jiaqi Chen, Yibo Wang, Yuzhe Tang, Guofei Gu |  |
| 441 |  |  [Turning A Curse into A Blessing: Data-Aware Memory-Efficient Training of Graph Neural Networks by Dynamic Exiting](https://doi.org/10.1145/3589335.3651575) |  | 0 | Training Graph Neural Networks (GNNs) efficiently remains a challenge due to the high memory demands, especially during recursive neighborhood aggregation. Traditional sampling-based GNN training methods often overlook the data's inherent structure, such as the power-law distribution observed in most real-world graphs, which results in inefficient memory usage and processing. We introduce a novel framework, M emory-A ware D ynamic E xiting GNN (MADE-GNN )), which capitalizes on the power-law nature of graph data to enhance training efficiency. MADE-GNN is designed to be data-aware, dynamically adjusting the depth of feature aggregation based on the connectivity of each node. Specifically, it routes well-connected "head'' nodes through extensive aggregation while allowing sparsely connected "tail'' nodes to exit early, thus reducing memory consumption without sacrificing model performance. This approach not only addresses the challenge of memory-intensive GNN training but also turns the power-law distribution from a traditional "curse'' into a strategic "blessing''. By enabling partial weight sharing between the early-exit mechanism and the full model, MADE-GNN effectively improves the representation of cold-start nodes, leveraging the structural information from head nodes to enhance generalization across the network. Our extensive evaluations across multiple public benchmarks, including industrial-level graphs, show that MADE-GNN outperforms existing GNN training methods in both memory efficiency and performance, offering significant improvements particularly for tail nodes. This demonstrates MADE-GNN's potential as a versatile solution for GNN applications facing similar scalability and distribution challenges. | Yan Han, Kaiqi Chen, Shan Li, Ji Yan, Baoxu Shi, Lei Zhang, Fei Chen, Jaewon Yang, Yunpeng Xu, Xiaoqiang Luo, Qi He, Ying Ding, Zhangyang Wang |  |
| 442 |  |  [A Heterogeneous Network fused with Context-aware Contrastive Learning for Sarcasm Topic-Target Pair Identification](https://doi.org/10.1145/3589335.3651462) |  | 0 | Sarcastic comments are often used to express dissatisfaction with products or events. Mining the topics and targets can provide clues for analyzing the underlying reasons behind the sarcasm, which helps understand user demands and improve products service. Existing research mainly focuses on mining single facet of sarcasm, such as topic or target, ignoring the complex interrelations between them. To overcome the above challenges, this paper proposes a Heterogeneous Information Network fused with Context-Aware Contrastive Learning (HINCCL) method. This approach aims to model multi-view features including syntactic style, domain knowledge, and textual semantics through a hierarchical attention aggregation mechanism. Furthermore, a context-aware negative contrastive training strategy is designed to learn the differentiated representations between different topic-target pairs. The effectiveness of the proposed method is validated on a dataset constructed in the digital domain. | Minjie Yuan, Mengyu Xiang, Yuxuan Song, Qiudan Li, Jinye Fu, Daniel Dajun Zeng |  |
| 443 |  |  [Disentangled Anomaly Detection For Multivariate Time Series](https://doi.org/10.1145/3589335.3651492) |  | 0 | Anomaly detection in time series that aims to identify unusual patterns has attracted a lot of attention recently. However, the representation of abnormal and normal data is difffcult to be distinguished because they are usually entangled. Recently, disentanglement theory based on variational auto-encoder (VAE) has shown great potential in machine learning and achieved great success in computer vision and natural language processing. In this paper, we propose a novel disentangled anomaly detection approach that adopts VAE-based disentanglement networks for anomaly detection in multivariate time series. The proposed method learns highquality disentangled latent factors in a continuous representation space to facilitate the identiffcation of anomalies from normal data. Extensive experiments demonstrate that our proposed lightweight model DA-VAE achieves state-of-the-art performance. | Xin Jie, Xixi Zhou, Chanfei Su, Zijun Zhou, Yuqing Yuan, Jiajun Bu, Haishuai Wang |  |
| 444 |  |  [Content Moderation on Social Media in the EU: Insights From the DSA Transparency Database](https://doi.org/10.1145/3589335.3651482) |  | 0 | The Digital Services Act (DSA) requires large social media platforms in the EU to provide clear and specific information whenever they remove or restrict access to certain content. These "Statements of Reasons" (SoRs) are collected in the DSA Transparency Database to ensure transparency and scrutiny of content moderation decisions of the providers of online platforms. In this work, we empirically analyze 156 million SoRs within an observation period of two months to provide an early look at content moderation decisions of social media platforms in the EU. Our empirical analysis yields the following main findings: (i) There are vast differences in the frequency of content moderation across platforms. For instance, TikTok performs more than 350 times more content moderation decisions per user than X/Twitter. (ii) Content moderation is most commonly applied for text and videos, whereas images and other content formats undergo moderation less frequently. (ii) The primary reasons for moderation include content falling outside the platform's scope of service, illegal/harmful speech, and pornography/sexualized content, with moderation of misinformation being relatively uncommon. (iii) The majority of rule-breaking content is detected and decided upon via automated means rather than manual intervention. However, X/Twitter reports that it relies solely on non-automated methods. (iv) There is significant variation in the content moderation actions taken across platforms. Altogether, our study implies inconsistencies in how social media platforms implement their obligations under the DSA -- resulting in a fragmented outcome that the DSA is meant to avoid. Our findings have important implications for regulators to clarify existing guidelines or lay out more specific rules that ensure common standards on how social media providers handle rule-breaking content on their platforms. | Chiara Patricia Drolsbach, Nicolas Pröllochs |  |
| 445 |  |  [Object-level Copy-Move Forgery Image Detection based on Inconsistency Mining](https://doi.org/10.1145/3589335.3651540) |  | 0 | In copy-move tampering operations, perpetrators often employ techniques, such as blurring, to conceal tampering traces, posing significant challenges to the detection of object-level targets with intact structures. Focus on these challenges, this paper proposes an Object-level Copy-Move Forgery Image Detection based on Inconsistency Mining (IMNet). To obtain complete object-level targets, we customize prototypes for both the source and tampered regions and dynamically update them. Additionally, we extract inconsistent regions between coarse similar regions obtained through self-correlation calculations and regions composed of prototypes. The detected inconsistent regions are used as supplements to coarse similar regions to refine pixel-level detection. We operate experiments on three public datasets which validate the effectiveness and the robustness of the proposed IMNet. | Jingyu Wang, Niantai Jing, Ziyao Liu, Jie Nie, Yuxin Qi, ChiHung Chi, KwokYan Lam |  |
| 446 |  |  [Efficacy of Large Language Models in Predicting Hindi Movies' Attributes: A Comprehensive Survey and Content-Based Analysis](https://doi.org/10.1145/3589335.3651496) |  | 0 | This research explores the efficacy of four state-of-the-art Large Language Models (LLMs): GPT-3.5-turbo-0301, Vicuna, PaLM 2, and Dolly in predicting (i) movie genres using audio transcripts of movie trailers and (ii) meta-information such as director and cast details using movie name and its year-of-release (YoR) for Hindi movies. In the contemporary landscape, training models for movie meta-information prediction often demand extensive data and parameters, posing significant challenges. We aim to discern whether LLMs mitigate these challenges. Focusing on Hindi movies within the Flickscore dataset, our study concentrates on trailer data. Preliminary findings reveal that GPT-3.5 stands out as the most effective LLM in predicting movie meta-information. Despite the inherent complexities of predicting diverse aspects such as genres and user preferences, GPT-3.5 exhibits promising capabilities. This research not only contributes to advancing our understanding of LLMs in the context of movie-related tasks but also sheds light on their potential application in Recommendation Systems (RS), indicating a notable leap forward in user preference comprehension and personalized content recommendations. | Prabir Mondal, Siddharth Singh, Kushum, Sriparna Saha, Jyoti Prakash Singh, Brijraj Singh, Niranjan Pedanekar |  |
| 447 |  |  [Interpretation-Empowered Neural Cleanse for Backdoor Attacks](https://doi.org/10.1145/3589335.3651525) |  | 0 | Backdoor attacks have posed a significant threat to deep neural networks, highlighting the need for robust defense strategies. Previous research has demonstrated that attribution maps change substantially when exposed to attacks, suggesting the potential of interpreters in detecting adversarial examples. However, most existing defense methods against backdoor attacks overlook the untapped capabilities of interpreters, failing to fully leverage their potential. In this paper, we propose a novel approach called interpretation-empowered neural cleanse (IENC ) for defending backdoor attacks. Specifically, integrated gradient (IG) is adopted to bridge the interpreters and classifiers to reverse and reconstruct the high-quality backdoor trigger. Then, an interpretation-empowered adaptative pruning strategy (IEAPS) is proposed to cleanse the backdoor-related neurons without the pre-defined threshold. Additionally, a hybrid model patching approach is employed to integrate the IEAPS and preprocessing techniques to enhance the defense performance. Comprehensive experiments are constructed on various datasets, demonstrating the potential of interpretations in defending backdoor attacks and the superiority of the proposed method. | Liangbo Ning, Zeyu Dai, Jingran Su, Chao Pan, Luning Wang, Wenqi Fan, Qing Li |  |
| 448 |  |  [Who is Creating Malware Repositories on GitHub and Why?](https://doi.org/10.1145/3589335.3651582) |  | 0 | Recent studies have found thousands of malware source code repositories on GitHub. For the first time, we propose to understand the origins and motivations behind the creation of such malware repositories. For that, we collect and profile the authors of malware repositories using a three-fold systematic approach. First, we identify 14K users in GitHub who have authored at least one malware repository. Second, we leverage a pretrained large language model (LLM) to estimate the likelihood of malicious intent of these authors. This innovative approach led us to categorize 3339 as Malicious, 3354 as Likely Malicious, and 7574 as Benign authors. Further, to validate the accuracy and reliability of our classification, we conduct a manual review of 200 randomly selected authors. Third, our analysis provides insights into the authors' profiles and motivations. We find that Malicious authors often have sparse profiles and focus on creating and spreading malware, while Benign authors typically have complete profiles with a focus on cybersecurity research and education. Likely Malicious authors show varying levels of engagement and ambiguous intentions. We see our study as a key step towards understanding the ecosystem of malware authorship on GitHub. | Nishat Ara Tania, Md Rayhanul Masud, Md Omar Faruk Rokon, Qian Zhang, Michalis Faloutsos |  |
| 449 |  |  [Zero-shot Explainable Mental Health Analysis on Social Media by Incorporating Mental Scales](https://doi.org/10.1145/3589335.3651584) |  | 0 | Traditional discriminative approaches in mental health analysis are known for their strong capacity but lack interpretability and demand large-scale annotated data. The generative approaches, such as those based on large language models (LLMs), have the potential to get rid of heavy annotations and provide explanations but their capabilities still fall short compared to discriminative approaches, and their explanations may be unreliable due to the fact that the generation of explanation is a black-box process. Inspired by the psychological assessment practice of using scales to evaluate mental states, our method which is called Mental Analysis by Incorporating Mental Scales (MAIMS), incorporates two procedures via LLMs. First, the patient completes mental scales, and second, the psychologist interprets the collected information from the mental scales and makes informed decisions. Experimental results show that MAIMS outperforms other zero-shot methods. MAIMS can generate more rigorous explanation based on the outputs of mental scales | Wenyu Li, Yinuo Zhu, Xin Lin, Ming Li, Ziyue Jiang, Ziqian Zeng |  |
| 450 |  |  [MART: Learning Hierarchical Music Audio Representations with Part-Whole Transformer](https://doi.org/10.1145/3589335.3651535) |  | 0 | Recent research in self-supervised contrastive learning of music representations has demonstrated remarkable results across diverse downstream tasks. However, a prevailing trend in existing methods involves representing equally-sized music clips in either waveform or spectrogram formats, often overlooking the intrinsic part-whole hierarchies within music. In our quest to comprehend the bottom-up structure of music, we introduce MART, a hierarchical music representation learning approach that facilitates feature interactions among cropped music clips while considering their part-whole hierarchies. Specifically, we propose a hierarchical part-whole transformer to capture the structural relationships between music clips in a part-whole hierarchy. Furthermore, a hierarchical contrastive learning objective is crafted to align part-whole music representations at adjacent levels, progressively establishing a multi-hierarchy representation space. The effectiveness of our music representation learning from part-whole hierarchies has been empirically validated across multiple downstream tasks, including music classification and cover song identification. | Dong Yao, Jieming Zhu, Jiahao Xun, Shengyu Zhang, Zhou Zhao, Liqun Deng, Wenqiao Zhang, Zhenhua Dong, Xin Jiang |  |
| 451 |  |  [Exploiting Associations among Multi-Aspect Node Properties in Heterogeneous Graphs for Link Prediction](https://doi.org/10.1145/3589335.3651502) |  | 0 | Recent years have witnessed the abundant emergence of heterogeneous graph neural networks (HGNNs) for link prediction. In heterogeneous graphs, different meta-paths connected to nodes reflect different aspects of the nodes' properties. Existing work fuses the multi-aspect properties of each node into a single vector representation, which makes them fail to capture fine-grained associations between multiple node properties. To this end, we propose a heterogeneous graph neural network with Multi-Aspect Node Association awareness, namely MANA. MANA leverages key associations among multi-aspect node properties to achieve link prediction. Specifically, to avoid the loss of effective association information for link prediction, we design a transformer-based Multi-Aspect Association Mining module to capture multi-aspect associations between nodes. Then, we introduce the Multi-Aspect Link Prediction module, empowering MANA to focus on the key associations among all, thus avoiding the negative impact of ineffective associations on the model's performance. We conduct extensive experiments on three widely used datasets from Heterogeneous Graph Benchmark (HGB). Experimental results show that our proposed method outperforms state-of-the-art baselines. | Chenguang Du, Hao Geng, Deqing Wang, Fuzhen Zhuang, Zhiqiang Zhang, Lanshan Zhang |  |
| 452 |  |  [Automating the Information Extraction from Semi-Structured Interview Transcripts](https://doi.org/10.1145/3589335.3651230) |  | 0 | This paper explores the development and application of an automated system designed to extract information from semi-structured interview transcripts. Given the labor-intensive nature of traditional qualitative analysis methods, such as coding, there exists a significant demand for tools that can facilitate the analysis process. Our research investigates various topic modeling techniques and concludes that the best model for analyzing interview texts is a combination of BERT embeddings and HDBSCAN clustering. We present a user-friendly software prototype that enables researchers, including those without programming skills, to efficiently process and visualize the thematic structure of interview data. This tool not only facilitates the initial stages of qualitative analysis but also offers insights into the interconnectedness of topics revealed, thereby enhancing the depth of qualitative analysis. | Angelina Parfenova Lucerne |  |
| 453 |  |  [FashionReGen: LLM-Empowered Fashion Report Generation](https://doi.org/10.1145/3589335.3651232) |  | 0 | Fashion analysis refers to the process of examining and evaluating trends, styles, and elements within the fashion industry to understand and interpret its current state, generating fashion reports. It is traditionally performed by fashion professionals based on their expertise and experience, which requires high labour cost and may also produce biased results for relying heavily on a small group of people. In this paper, to tackle the Fashion Report Generation (FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting system based the advanced Large Language Models (LLMs), debbed as GPT-FAR. Specifically, it tries to deliver FashionReGen based on effective catwalk analysis, which is equipped with several key procedures, namely, catwalk understanding, collective organization and analysis, and report generation. By posing and exploring such an open-ended, complex and domain-specific task of FashionReGen, it is able to test the general capability of LLMs in fashion domain. It also inspires the explorations of more high-level tasks with industrial significance in other domains. Video illustration and more materials of GPT-FAR can be found in https://github.com/CompFashion/FashionReGen. | Yujuan Ding, Yunshan Ma, Wenqi Fan, Yige Yao, TatSeng Chua, Qing Li |  |
| 454 |  |  [Tender Document Analyzer with the Combination of Supervised Learning and LLM-based Improver](https://doi.org/10.1145/3589335.3651233) |  | 0 | Bidders often take a long time to read and understand tender documents because they require specialized knowledge, and tender documents are generally long. Bidders first overview the specific items, such as payment and warranty, in a tender document and then check the overall document. Therefore, the function that can extract specific items (i.e., item extractor) and the function that can highlight words or phrases related to specific items (i.e., word-phrase highlighter) are in great demand. To develop the above two types of functions, we need to solve two problems. The first problem is the problem related to the annotated data set. The second problem concerns the BERT NER-based prediction approach in a small training dataset setting. To solve the first problem, we created two types of sequence labeling datasets related to Item Extractor and Word-Phrase Highlighter. To solve the second problem, we propose the Information Extraction (IE) method, which combines (1) a supervised learning approach using Bidirectional Encoder Representations from Transformers (BERT) and (2) a large language model (LLM)-based improver. We then developed the web application system called Tender Document Analyzer (TDDA), which includes "Item Extractor" and "Word-Phrase Highlighter". Experimental evaluation shows that our approach is practical. Firstly, the evaluation for extraction ability shows that the performance of our proposed method is much higher than the baseline approach that uses GPT 3.5, as well as demonstrates that the proposed LLM-based improver can improve the IE ability. In addition, the usability evaluation shows that bidders can solve the task in less time using our system. | Tomoki Ito, Shun Nakagawa |  |
| 455 |  |  [GRACE: Generating Cause and Effect of Disaster Sub-Events from Social Media Text](https://doi.org/10.1145/3589335.3651234) |  | 0 | In recent years, social media has emerged as a pivotal source of emergency response for natural disasters. Causal analysis of disaster sub-events is one of crucial concerns. However, the design and implementation of its application scenario present significant challenges, due to the intricate nature of events and information overload. In this work, we introduce GRACE, a system designed for generating the cause and effect of disaster sub-events from social media text. GRACE aims to provide a rapid, comprehensive, and real-time analysis of disaster intelligence. Different from conventional information digestion systems, GRACE employs event evolution reasoning by constructing a causal knowledge graph for disaster sub-events (referred to as DSECG) and fine-tuning GPT-2 on DSECG. This system offers users a comprehensive understanding of disaster events and supports human organizations in enhancing response efforts during disaster situations. Moreover, an online demo is accessible, allowing user interaction with GRACE and providing a visual representation of the cause and effect of disaster sub-events. | Xinxi Jiang, Xiang Li, Qifeng Zhou, Qing Wang |  |
| 456 |  |  [RealGraphGPUWeb: A Convenient and Efficient GPU-Based Graph Analysis Platform on the Web](https://doi.org/10.1145/3589335.3651237) |  | 0 | In this demo paper, we present RealGraphGPUWeb a web-based graph analysis platform with the following features: (1) easy to use user-friendly GUI, (2) high processing performance, (3) various graph algorithms and data formats supported, (4) high accessibility anywhere on the web, and (5) no coding requirements. In our demo, we show how a naive user (e.g., non-CS researcher) gets graph analysis results conveniently and efficiently with a few clicks through the web-based GUI inRealGraphGPUWeb. We also make the user feel the effect of performance improvement obtained by our optimization strategies employed in RealGraphGPUWeb. We believe that RealGraphGPUWeb could be a good platform not only for CS users but also for non-CS users who want to analyze big graphs for their applications easily and efficiently. | JeongMin Park, MyungHwan Jang, SangWook Kim |  |
| 457 |  |  [BoostER: Leveraging Large Language Models for Enhancing Entity Resolution](https://doi.org/10.1145/3589335.3651245) |  | 0 | Entity resolution, which involves identifying and merging records that refer to the same real-world entity, is a crucial task in areas like Web data integration. This importance is underscored by the presence of numerous duplicated and multi-version data resources on the Web. However, achieving high-quality entity resolution typically demands significant effort. The advent of Large Language Models (LLMs) like GPT-4 has demonstrated advanced linguistic capabilities, which can be a new paradigm for this task. In this paper, we propose a demonstration system named BoostER that examines the possibility of leveraging LLMs in the entity resolution process, revealing advantages in both easy deployment and low cost. Our approach optimally selects a set of matching questions and poses them to LLMs for verification, then refines the distribution of entity resolution results with the response of LLMs. This offers promising prospects to achieve a high-quality entity resolution result for real-world applications, especially to individuals or small companies without the need for extensive model training or significant financial investment. | Huahang Li, Shuangyin Li, Fei Hao, Chen Jason Zhang, Yuanfeng Song, Lei Chen |  |
| 458 |  |  [Scenario-Driven Cyber-Physical-Social System: Intelligent Workflow Generation Based on Capability](https://doi.org/10.1145/3589335.3651246) |  | 0 | Cyber-Physical-Social System (CPSS) is a pioneering solution in Crowd Computing, which integrates heterogeneous resources from cyber, physical, and social spaces, possessing collaborative capabilities in perception, computation, and control. However, existing CPSSs usually confine their functionality to rigid scenarios or tasks, and often oversimplify human resource modeling that fails to dynamically recognize human capabilities. In this work, we propose a Scenario-Driven CPSS to enable an adaptive resource choreography across scenarios. More concretely, we leverage temporal environments to identify events and disassemble these events into workflows, triggering the execution of corresponding capability units, where the capability units abstract the shared functionality of heterogeneous resource groups. Meanwhile, we improve the pre-assuming human capabilities to construct the relationship between human and their capabilities during the execution of workflows, sufficiently promoting human intelligence. Our real-world demo on fire rescue demonstrates the effectiveness of the solution. | Yi Li, Xinkui Zhao, Chen Chen, Shengye Pang, Zhengyang Zhou, Jianwei Yin |  |
| 459 |  |  [REAL-UP: Urban Perceptions From LBSNs Helping Moving Real-Estate Market to the Next Level](https://doi.org/10.1145/3589335.3651252) |  | 0 | Finding the best short- or long-term accommodation is troublesome in unknown areas. Current tools provided by the real-estate market offer valuable information regarding the property, such as price, photos, and descriptions of the space; however, this market has little explored other relevant information regarding the surrounding area, such as what is nearby and users' subjective perception of the property's area. To address this gap, we propose REAL-UP, an interactive tool designed to enrich real-estate marketplaces. In addition to information commonly provided by such applications, e.g., rent price, REAL-UP also provides subjective neighborhood information based on Location-Based Social Networks (LBSNs) messages. This novel tool helps to represent complex users' subjective perceptions of urban areas, which could ease the process of finding the best accommodation. | Frances Albert Santos, Thiago H. Silva, Leandro A. Villas |  |
| 460 |  |  [The Web Data Commons Schema.org Table Corpora](https://doi.org/10.1145/3589335.3651441) |  | 0 | The research on table representation learning, data retrieval, and data integration in the context of data lakes requires large table corpora for the training and evaluation of the developed methods. Over the years, several large table corpora such as WikiTables, GitTables, or the Dresden Web Table Corpus have been published and are used by the research community. This paper complements the set of public table corpora with the Web Data Commons Schema.org table corpora, two table corpora consisting of 4.2 (Release 2020) and 5 million (Release 2023) relational tables describing products, events, local businesses, job postings, recipes, movies, books, as well as 37 further types of entities. The feature that distinguishes the corpora from all other publicly available large table corpora is that all tables that describe entities of a specific type use the same attributes to describe these entities, i.e. all tables use a shared schema, the schema.org vocabulary. The shared schema eases the integration of data from different sources and allows training processes to focus on specific types of entities or specific attributes. Altogether the tables contain ~653 million rows of data which have been extracted from the Common Crawl web corpus and have been grouped into separate tables for each class/host combination, i.e. all records of a specific class that originate from a specific website are put into a single table. This paper describes the creation of the WDC Schema.org Table Corpora, gives an overview of the content of the corpora, and discusses their use cases. | Ralph Peeters, Alexander Brinkmann, Christian Bizer |  |
| 461 |  |  [Tel2Veh: Fusion of Telecom Data and Vehicle Flow to Predict Camera-Free Traffic via a Spatio-Temporal Framework](https://doi.org/10.1145/3589335.3651442) |  | 0 | Vehicle flow, a crucial indicator for transportation, is often limited by detector coverage. With the advent of extensive mobile network coverage, we can leverage mobile user activities, or cellular traffic, on roadways as a proxy for vehicle flow. However, as counts of cellular traffic may not directly align with vehicle flow due to data from various user types, we present a new task: predicting vehicle flow in camera-free areas using cellular traffic. To uncover correlations within multi-source data, we deployed cameras on selected roadways to establish the Tel2Veh dataset, consisting of extensive cellular traffic and sparse vehicle flows. Addressing this challenge, we propose a framework that independently extracts features and integrates them with a graph neural network (GNN)-based fusion to discern disparities, thereby enabling the prediction of unseen vehicle flows using cellular traffic. This work advances the use of telecom data in transportation and pioneers the fusion of telecom and vision-based data, offering solutions for traffic management. | ChungYi Lin, ShenLung Tung, HungTing Su, Winston H. Hsu |  |
| 462 |  |  [An Open Platform for Quality Measures in a Linked Data Index](https://doi.org/10.1145/3589335.3651443) |  | 0 | There is a great diversity of RDF datasets publicly available on the web. Choosing among them requires assessing their "fitness for use'' for a particular use case, and thus, finding the right quality measures and evaluating data sources according to them. However, this is not an easy task due to the large number of possible quality measures, and the multiplicity of implementation and assessment platforms. Therefore, there is a need for a common way to define measures and evaluate RDF datasets, using open standards and tools. IndeGx is a SPARQL-based framework to design indexes of Knowledge Graphs declaratively. We extend it to support more advanced data quality measures. We demonstrate our approach by reproducing two existing measures, showing how one can formalize and add measures using such an open declarative framework. | Pierre Maillot, Jennie Andersen, Sylvie Cazalens, Catherine Faron, Fabien Gandon, Philippe Lamarre, Franck Michel |  |
| 463 |  |  [CompMix: A Benchmark for Heterogeneous Question Answering](https://doi.org/10.1145/3589335.3651444) |  | 0 | Fact-centric question answering (QA) often requires access to multiple, heterogeneous, information sources. By jointly considering several sources like a knowledge base (KB), a text collection, and tables from the web, QA systems can enhance their answer coverage and confidence. However, existing QA benchmarks are mostly constructed with a single source of knowledge in mind. This limits capabilities of these benchmarks to fairly evaluate QA systems that can tap into more than one information repository. To bridge this gap, we release CompMix, a crowdsourced QA benchmark which naturally demands the integration of a mixture of input sources. CompMix has a total of 9,410 questions, and features several complex intents like joins and temporal conditions. Evaluation of a range of QA systems on CompMix highlights the need for further research on leveraging information from heterogeneous sources. | Philipp Christmann, Rishiraj Saha Roy, Gerhard Weikum |  |
| 464 |  |  [Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery](https://doi.org/10.1145/3589335.3651446) |  | 0 | We present CNER-UAV, a fine-grained Chinese Name Entity Recognition dataset specifically designed for the task of address resolution in Unmanned Aerial Vehicle delivery systems. The dataset encompasses a diverse range of five categories, enabling comprehensive training and evaluation of NER models. To construct this dataset, we sourced the data from a real-world UAV delivery system and conducted a rigorous data cleaning and desensitization process to ensure privacy and data integrity. The resulting dataset, consisting of around 12,000 annotated samples, underwent human experts and Large Language Model annotation. We evaluated classical NER models on our dataset and provided in-depth analysis. The dataset and models are publicly available at <https://github.com/zhhvvv/CNER-UAV>. | Yuxuan Yao, Sichun Luo, Haohan Zhao, Guanzhi Deng, Linqi Song |  |
| 465 |  |  [Graphameleon: Relational Learning and Anomaly Detection on Web Navigation Traces Captured as Knowledge Graphs](https://doi.org/10.1145/3589335.3651447) |  | 0 | User and Entity Behavior Analytics (UEBA) is key for managing security risks on information systems and comprehending user activities' impact on the network infrastructure. However, accessing network traffic and Web logs is challenging due to encryption or decentralized systems. Qualifying activities also requires contextualizing them according to the network's topology, as it determines potential exchanges and carries information about which services are used. This complexity hinders learning behavioral patterns when precise user action sequences are needed. We propose to tackle these challenges with Graphameleon, an open-source Web extension for capturing Web navigation traces. We model user activities in an RDF Knowledge Graph (KG), drawing from the UCO and NORIA-O ontologies. With this approach, we are able to distinguish analytics strategies implemented across different websites. | Lionel Tailhardat, Benjamin Stach, Yoan Chabot, Raphaël Troncy |  |
| 466 |  |  [Revisiting 30 years of the Network Time Protocol](https://doi.org/10.1145/3589335.3651998) |  | 0 | Since the inception of the Internet and WWW, providing the time among multiple nodes on the Internet has been one of the most critical challenges. David Mills is the pioneer to provide time on the Internet, inventing the Network Time Protocol (NTP), and synchronizing the clocks in computer systems. Now, the NTP is predominantly used on the Internet and WWW. In this paper, we revisit the NTP, and present the overview of the NTP. In particular, we highlight the advanced research effort, the SpaceNTP, to synchronize the clocks among assets and entities in space. The SpaceNTP designed for space environments will be the fundamental medium and enabling block to provide the future web services in space. | Simon S. Woo |  |
| 467 |  |  [Toward Making Opaque Web Content More Accessible: Accessibility From Adobe Flash to Canvas-Rendered Apps](https://doi.org/10.1145/3589335.3651999) |  | 0 | Adobe Flash, once a ubiquitous multimedia platform, played a pivotal role in shaping the digital landscape for nearly two decades. Its capabilities ranged from animated banners and immersive websites to complex online applications and games. Flash content was embedded on websites with the embed or the object element. To the browser, the embedded content is opaque by default, which means Flash content can't be used for the accessibility tree that the browser creates based on the DOM tree, which is used by platform-specific accessibility APIs to provide a representation that can be understood by assistive technologies, such as screen readers. With Flash losing out on popularity, HTML 5 introduced the canvas element, which for the first time allowed developers to draw graphics and animations with either the canvas scripting API or the WebGL API directly natively in the browser. Similar to Flash, such canvas-rendered content is opaque by default and unusable for the accessibility tree. Lastly, the implementation of the WebAssembly Garbage Collection (WasmGC) standard in browsers allowed developers to port applications, written in non-Web programming languages like Kotlin for non-Web platforms like Android, to the Web by compiling them to WasmGC and rendering the entire app into a canvas. In the most extreme of cases, this means that the entire HTML code of an application can consist of a sole canvas tag, which evidently is opaque to the browser and impossible to leverage for the accessibility tree. Without judging their quality, this paper focuses on documenting approaches then and now for making such opaque Web content more accessible to users of assistive technologies. | Thomas Steiner |  |
| 468 |  |  [History in Making: Political Campaigns in the Era of Artificial Intelligence-Generated Content](https://doi.org/10.1145/3589335.3652000) |  | 0 | Web 2.0 provided impactful tools, based on user-generated content, for political campaigns and opinion engineering. However, in recent months, AI advances and the ease of access to AI-generated content (AIGC) have led to a paradigm shift in political participation by politicians and electorates alike. This paper aims to explore a historical analysis of this shift. We provide anecdotal evidence of new trends, potential impact, and challenges. We discuss the usage of AIGC in political campaigns, and how AIGC is used as a substitute for incarcerated politicians. Such a usage presents novel ways for leaders to reach the public and keep them politically active. However, AIGC also has risks when used for disinformation, such as DeepFake media and caller bots, to undermine and malign the opponents. On the other hand, the evidence shows that governments can nudge AIGC content by censoring Internet services. We also report challenges facing AIGC usage, such as model bias and hallucinations, along with a governance perspective on ethics and regulations. | Ehsan ul Haq, Yiming Zhu, Pan Hui, Gareth Tyson |  |
| 469 |  |  [Me, the Web and Digital Accessibility](https://doi.org/10.1145/3589335.3652002) |  | 0 | This essay will briefly narrate the relationship between my professional growth and the evolution of web technology and the internet, especially in Brazil, where I live. I start by briefly discussing my background and how I came to be involved with technology, from my earliest computer experiences to getting online and continuing my schooling. At this time, several significant Web and Internet events occurred, influencing my career path choice. I also write about how my work has focused on digital accessibility and how my engagement has impacted the Web in my country. I close the article by examining my present work environment and how the Web has impacted my life, motivating me to use free and open technology. | Reinaldo Ferraz |  |
| 470 |  |  [A Non-Intrusive Approach to Assessing Dysarthria Severity: Advancing Clinical Diagnosis](https://doi.org/10.1145/3589335.3651449) |  | 0 | AI-driven severity assessment techniques for dysarthric disorders show promise in aiding speech-language pathologists with diagnostics and therapeutic follow-ups for patients. Existing solutions generally focus on the average intelligibility and hoarseness of the individual speaker's speech (i.e., speaker-level classification). This potentially ignores the slight variations in pronunciation attributed to the speaker's dysarthric disorders, e.g., /t/ and /d/. To address this issue, we rethink the inherent differences in the dysarthria speech, and propose a non-intrusive severity assessment approach called DysarNet. Specifically, we first design a prosodic emphasis module based on frame-level speech features to highlight the fine-grained temporal changes including pronunciation content, rhythm, and timing. Second, we design a multi-scale aggregation strategy to collect statistical cues on articulatory information at different scales, i.e., frame-level and utterance-level. By doing so, multi-scale prosody and articulatory cues are directly assist the prediction network for assessing dysarthria severity from multiple views, and naturally achieve speaker-independent generalization ability. Experimental results on VCC 2018 and TORGO datasets show that our DysarNet excels in assessing dysarthria severity. | Ganjun Liu, Xiaohui Hou, Meng Ge, Tao Zhang, Haizhou Li |  |
| 471 |  |  [AI-based Prediction of Catheter-related Thrombosis Risk for Cancer Patients](https://doi.org/10.1145/3589335.3651452) |  | 0 | Cancer patients face a heightened risk of venous thromboembolism (VTE), emerging as the second most prevalent cause of death within this population. Central venous catheterization (CVC), a routine procedure in cancer care, amplifies the VTE risk, leading to catheter-related thrombosis (CRT). Although traditional risk-assessment models and certain AI methods exist for VTE prediction, their capability and application in CRT risk prediciton for cancer patients remains limited. This paper addresses the shortcomings of current models (RAMs) by crafting a dedicated AI model to predict CRT risks for cancer patients. Leveraging a dataset encompassing 10,512 cancer patients undergoing catheterization over a decade, we meticulously select nine specific features for model construction, resulting in an impressive 0.794 AUROC in prediction, 54.9% higher than baseline. Furthermore, we estimate CRT-free probability using the Kaplan-Meier method. We also develop a WeChat Mini Program designed for efficient data collection and risk prediction, enhancing the efficiency of CRT risk detection for both doctors and patients. | Yaoqi Guo, Yun Ma, Zijian Shao, Weichen Bi, Yanfeng Wang |  |
| 472 |  |  [Health CLIP: Depression Rate Prediction Using Health Related Features in Satellite and Street View Images](https://doi.org/10.1145/3589335.3651451) |  | 0 | Mental health is a state of mental well-being that enables people to cope with the stresses of life, realize their abilities, learn well and work well, and contribute to their community. It has intrinsic and instrumental value and is integral to our well-being, and its correlation with environmental factors has been a subject of growing interest. As the pressure of society keeps growing, depression has become a severe problem in modern cities, and finding a way to estimate depression rate is of significance to relieve the problem. In this study, we introduce a Contrastive Language-Image Pretraining (CLIP) based novel approach to predict mental health indicators, especially depression rate, through satellite and street view images. Our methodology uses state-of-the-art Multimodal Large Language Model (MLLM), GPT4-vision, to generate health related captions for satellite and street view images, then we use the generated image-text pairs to fine-tune the CLIP model, making its image encoder extract health related features such as green spaces, sports fields, and infrastructral characteristics. The fine-tuning process is employed to bridge the semantic gap between textual descriptions and visual representations, enabling a comprehensive analysis of geo-tagged images. Consequently, our methodology achieves a notable R2 value of 0.565 on prediction of depression rate in New York City with the combination of satellite and street view images. The successful deployment of Health CLIP in a real-world scenario underscores the practical applicability of our approach. | Tianjian Ouyang, Xin Zhang, Zhenyu Han, Yu Shang, Yong Li |  |
| 473 |  |  [AI in Health and Social Care: A Methodology for Privacy Risk Modeling and Simulation](https://doi.org/10.1145/3589335.3651453) |  | 0 | As health and social care data networks evolve and adapt to greater digitalization and datafication of health, data and analytics systems are developing and bringing forward new ways to share, access and analyze data. Organizations and individuals making data sharing decisions for AI-enabled health and social care services need to be able to balance the benefits of such uses with the possible risks that may ensue - including those related to issues of privacy and security. In this paper, we provide an overview of our approach to privacy risk assessment for cross-domain access and re-use of sensitive data for research purposes using Spyderisk - an automated risk assessment tool. We apply Spyderisk to a real AI research scenario and consider the ways in which such techniques could support multiple stakeholders to assess privacy and security risks. | Laura Carmichael, Steve Taylor, Adriane Chapman, Michael J. Boniface |  |
| 474 |  |  [Sociotechnical Considerations for Accessibility and Equity in AI for Healthcare](https://doi.org/10.1145/3589335.3651455) |  | 0 | As AI systems are built and deployed to support mental health services, it is imperative to fully understand the stakeholder acceptability of such systems so that these concerns can be taken into account in system design. As such, we undertook a consultation with staff (therapists) and service-users at Adferiad Recovery (a large mental health charity). The aim was to capture insights about their understanding of trust, and different trust factors for AI in mental health care. Surveys, interviews and focus groups were conducted with service users and therapists. Key takeaways for computer scientists and the developers of AI systems are presented. | Adriane Chapman, Chloe L. Harrison, Caroline Jones, James Thornton, Rose Worley, Jeremy C. Wyatt |  |
| 475 |  |  [Uncertainty-Aware Pre-Trained Foundation Models for Patient Risk Prediction via Gaussian Process](https://doi.org/10.1145/3589335.3651456) |  | 0 | Patient risk prediction models are crucial as they enable healthcare providers to proactively identify and address potential health risks. Large pre-trained foundation models offer remarkable performance in risk prediction tasks by analyzing multimodal patient data. However, a notable limitation of pre-trained foundation models lies in their deterministic predictions (i.e., lacking the ability to acknowledge uncertainty). We propose Gaussian Process-based foundation models to enable the generation of accurate predictions with instance-level uncertainty quantification, thus allowing healthcare professionals to make more informed and cautious decisions. Our proposed approach is principled and architecture-agnostic. Experimental results show that our proposed approach achieves competitive performance on classical classification metrics. Moreover, we observe that the accuracy of certain predictions is much higher than that of the uncertain ones, which validates the uncertainty awareness of our proposed method. Therefore, healthcare providers can trust low-uncertainty predictions and conduct more comprehensive investigations on high-uncertainty predictions, ultimately enhancing patient outcomes with less expert intervention. | Jiaying Lu, Shifan Zhao, Wenjing Ma, Hui Shao, Xiao Hu, Yuanzhe Xi, Carl Yang |  |
| 476 |  |  [Enhancing Progressive Diagnosis Prediction in Healthcare with Continuous Normalizing Flows](https://doi.org/10.1145/3589335.3651457) |  | 0 | Progressive diagnosis prediction in healthcare is a promising yet challenging task. Existing studies usually assume a pre-defined prior for generating patient distributions (e.g., Gaussian). However, the inferred approximate posterior can deviate from the real-world distribution, which further affects the modeling of continuous disease progression over time. To alleviate such inference bias, we propose an enhanced progressive diagnostic prediction model (i.e., ProCNF), which integrates continuous normalizing flows (CNF) and neural ordinary differential equations (ODEs) to achieve more accurate approximations of patient health trajectories while capturing the continuity underlying disease progression. We first learn patient embeddings with CNF to construct a complex posterior approximation of patient distributions. Then, we devise a CNF-enhanced neural ODE module for progressive diagnostic prediction, which aims to improve the modeling of disease progression for individual patients. Extensive experiments on two real-world longitudinal EHR datasets show significant performance gains brought by our method over state-of-the-art competitors. | Yanchao Tan, Hengyu Zhang, Zihao Zhou, Guofang Ma, Fan Wang, Weiming Liu, Xinting Liao, Vicki Stover Hertzberg, Carl Yang |  |
| 477 |  |  [Improving Prostate Cancer Risk Prediction through Partial AUC Optimization](https://doi.org/10.1145/3589335.3651458) |  | 0 | Prostate cancer risk prediction (PCRP) is crucial in guiding clinical decision-making and ensuring accurate diagnoses. The area under the receiver operating characteristic curve (AUC) is typically used for the evaluation of PCRP models. However, AUC considers regions with high false positive rates (FPRs), which are not applicable in clinical practice. To address this concern, we propose to use partial AUC (pAUC) as a more clinically meaningful metric which evaluates PCRP models with restricted FPR. Moreover, we propose a new PCRP framework named pAUCP, which optimizes pAUC to train PCRP models and adopts model ensemble to further enhance its usability. We construct clinical datasets obtained from two medical centers over an extended period to evaluate the proposed pAUCP framework. Extensive experiments demonstrate the rationality and superiority of the pAUCP framework, especially the cross-time and cross-center transferability of the obtained PCRP model. | Xinyuan Zhu, Xiaohan Ren, Wentao Shi, Changming Wang, Xuehan Liu, Yuqing Liu, Tao Tao, Fuli Feng |  |
| 478 |  |  [Distributed Transparent Data Layer for Next Generation Blockchains](https://doi.org/10.1145/3589335.3651255) |  | 0 | Distributed Transparent Data Layer (DTDL) aims to overcome the significant storage inefficiencies in blockchain technology. The proposed scheme enhances scalability and enables broader adoption by allowing nodes to store only portions of the blockchain history, diverging from traditional methods like sharding. It consists of three main components: a transparent authentication scheme, a verifiable search tree, and a data availability sampling scheme, supporting diverse applications including zero-knowledge machine learning. This approach not only maintains transparency to the blockchain's upper layer but also offers seamless integration with existing systems without requiring forks. Additionally, the paper introduces an innovative transparent authentication method for Luby Transform (LT) codes using KZG commitments, enabling efficient and secure verification of encoded symbols without decoding. Addressing the challenges of data outsourcing in blockchain, our proposed model ensures data integrity and robust security in a potentially malicious publisher environment, marking a significant advancement in blockchain storage and data integrity solutions. | Li Quan |  |
| 479 |  |  [Temporal Knowledge Graph Extraction and Modeling across Multiple Documents for Health Risk Prediction](https://doi.org/10.1145/3589335.3651256) |  | 0 | Clinical text in electronic health records (EHR) holds vital cues into a patient's journey, often absent in structured EHR data. Evidence-based healthcare decisions demand accurate extraction and modeling of these cues. The goal of our study is to predict Type-II Diabetes by utilizing concept-based models of visit sequences from longitudinal EHR data. We undertake the challenging task of fine-grained temporal information extraction from clinical text using a recent span-based approach with pre-trained transformers. We achieve a new state-of-the-art in end-to-end relation extraction from 2012 clinical temporal relations corpus. We propose to apply our model to a new dataset and extract patient-centric temporal knowledge graphs from their visits-fusing temporal orderings within documents and across visits. Beyond the current focus of our work on Type-II Diabetes risk prediction from EHR, our versatile framework can be extended to other domains including web-based healthcare systems for personalized medicine. It can not only model health outcomes having long progression timelines but also various socio-economic outcomes such as conflict, natural disasters, and financial markets by leveraging news, reports, and social-media text for extracting and modeling irregular time-series and help inform a variety of web-based applications and policies. | Rochana Chaturvedi |  |
| 480 |  |  [When Crypto Economics Meet Graph Analytics and Learning](https://doi.org/10.1145/3589335.3651257) |  | 0 | Utilizing graph analytics and learning has proven to be an effective method for exploring aspects of crypto economics such as network effects, decentralization, tokenomics, and fraud detection. However, the majority of existing research predominantly focuses on leading cryptocurrencies, namely Bitcoin (BTC) and Ethereum (ETH), overlooking the vast diversity among the more than 10,000 cryptocurrency projects. This oversight may result in skewed insights. In our paper, we aim to broaden the scope of investigation to encompass the entire spectrum of cryptocurrencies, examining various coins across their entire life cycles. Furthermore, we intend to pioneer advanced methodologies, including graph transfer learning and the innovative concept of "graph of graphs". By extending our research beyond the confines of BTC and ETH, our goal is to enhance the depth of our understanding of crypto economics and to advance the development of more intricate graph-based techniques. | Bingqiao Luo |  |
| 481 |  |  [Comprehensively Auditing the TikTok Mobile App](https://doi.org/10.1145/3589335.3651260) |  | 0 | TikTok has become a dominant force in the social media landscape of the United States, and has spawned other social media sites emulating their algorithmically-driven short form content recommendation platform (e.g. Youtube Shorts and Instagram Reels). The short-form vertical content is designed to be consumed on mobile phones, but existing audits have predominantly, and to a limited degree, investigated TikTok using the web application. Additionally, there are no advertisements on the web version of TikTok, and as such the advertising ecosystem of the platform has thusfar largely gone unstudied. In this work we propose a technique for auditing TikTok's recommendation algorithm through interfacing with emulators and intercepting network traffic. In this way we are able to measure the personalization that comes from user-specified demographics such as gender and age and better understand how ads are delivered to these groups. Future work will investigate personalization from user interaction such as liking posts and following creators based on their interest, and will study the role that algorithmic personalization plays in ad targeting. | Levi Kaplan, Piotr Sapiezynski |  |
| 482 |  |  [Outgroup Dehumanisation in Telegram - the Role of Ingroup Identity and Perception](https://doi.org/10.1145/3589335.3651261) |  | 0 | Intergroup dehumanisation represents a pressing concern for today's society. It hinders empathy, prosocial behaviour, and contributes to between-group aggression. Its consequences are particularly dangerous in the context of international military conflicts as dehumanisation contributes to support for war, war-related violence, and usually accompanies genocidal conflicts. This motivated the focus of this study on the blatant forms of dehumanisation towards an outgroup defined in political or national terms, with a specific focus on the relations between Ukrainians, Russians, and Belarusians around the time of the Russian invasion of Ukraine in 2022. The study draws attention to previously under-researched aspect in outgroup dehumanisation, specifically the role of ingroup perception in it. Outgroup dehumanisation involves excluding the outgroup from the community one identifies with, thus reinforcing the boundary between ingroup and outgroup. This highlights the comparative nature of dehumanisation, suggesting its basis might lie more in comparative ingroup superiority bias rather than in outgroup inferiority bias. Existing research however generally concentrates solely on negative aspects of outgroup perception in dehumanising attitudes. While some studies have gauged dehumanisation through ingroup-outgroup perception differences, they lacked a ground truth measure for dehumanisation, leaving its comparative nature largely unexamined. Employing generative Large Language Model, we develop a dataset of Telegram channels posts, classified as dehumanising or neutral. Utilising NLP tools we analyse the role of ingroup-outgroup perception disparities in dehumanisation, specifically addressing its relation to affective polarisation. | Elizaveta Chernenko |  |
| 483 |  |  [Leveraging Knowledge-aware Methodologies for Multi-document Summarization](https://doi.org/10.1145/3589335.3651262) |  | 0 | With the development of information technology, a large amount of information and corpora has been incrementally sparked from the Web, stimulating an increasingly high demand for summarizing. Document Summarization is one of Natural Language Processing tasks, which aims to generate abridged versions of a given single or multiple documents as concise and coherent as possible while preserving salient information from the source texts. Recent research in the area has started to use knowledge graphs as they can capture more factual and applicable information from more facets along with source information, benefiting fact consistency and informativeness of generated summaries, rather than just from a linguistic perspective. However, there is no explicit investigation of the effects of different kinds of knowledge graphs on document summarization. The proposed method is to use structured informative and knowledgeable auxiliary information, especially knowledge graphs, into pre-trained summarization models, advancing summary qualities. Expected outcomes are exploring knowledge and knowledge graph incorporation for multi-document summarization, and achieving more informative, coherent, and factually consistent summaries. | Yutong Qu |  |
| 484 |  |  [Knowledge Enabled Relation Extraction](https://doi.org/10.1145/3589335.3651263) |  | 0 | Relation extraction is the task of extracting relationships from input text, where input can be a sentence, document, or multiple documents. This task has been popular for decades and is still of keen interest. Various techniques have been proposed to solve the relation extraction problem, among which the most popular are using distant supervision, deep learning-based models, reasoning-based models, and transformer-based models. We propose three approaches (named ReOnto, DocRE-CLip, and KDocRE) for relation extraction from text at three levels of granularity (sentence, document and across documents). These approaches embed knowledge in a deep learning based model to improve performance. ReOnto and DocRE-CLip have been evaluated and the source code is publicly available. We are currently implementing and evaluating KDocRE. | Monika Jain |  |
| 485 |  |  [Graph Unlearning with Efficient Partial Retraining](https://doi.org/10.1145/3589335.3651265) |  | 0 | Graph Neural Networks (GNNs) have achieved remarkable success in various real-world applications. However, GNNs may be trained on undesirable graph data, which can degrade their performance and reliability. To enable trained GNNs to efficiently unlearn unwanted data, a desirable solution is retraining-based graph unlearning, which partitions the training graph into subgraphs and trains sub-models on them, allowing fast unlearning through partial retraining. However, the graph partition process causes information loss in the training graph, resulting in the low model utility of sub-GNN models. In this paper, we propose GraphRevoker, a novel graph unlearning framework that better maintains the model utility of unlearnable GNNs. Specifically, we preserve the graph property with graph property-aware sharding and effectively aggregate the sub-GNN models for prediction with graph contrastive sub-model aggregation. We conduct extensive experiments to demonstrate the superiority of our proposed approach. | Jiahao Zhang |  |
| 486 |  |  [Incentives in the Ether: Practical Cryptocurrency Economics & Security](https://doi.org/10.1145/3589335.3651268) |  | 0 |  | Aviv Yaish |  |
| 487 |  |  [Large Language Model Powered Agents in the Web](https://doi.org/10.1145/3589335.3641240) |  | 0 | Web applications serve as vital interfaces for users to access information, perform various tasks, and engage with content. Traditional web designs have predominantly focused on user interfaces and static experiences. With the advent of large language models (LLMs), there's a paradigm shift as we integrate LLM-powered agents into these platforms. These agents bring forth crucial human capabilities like memory and planning to make them behave like humans in completing various tasks, effectively enhancing user engagement and offering tailored interactions in web applications. In this tutorial, we delve into the cutting-edge techniques of LLM-powered agents across various web applications, such as web mining, social networks, recommender systems, and conversational systems. We will also explore the prevailing challenges in seamlessly incorporating these agents and hint at prospective research avenues that can revolutionize the way we interact with web platforms. | Yang Deng, An Zhang, Yankai Lin, Xu Chen, JiRong Wen, TatSeng Chua |  |
| 488 |  |  [Social Psychology Meets Social Computing: State of the Art and Future Directions](https://doi.org/10.1145/3589335.3641242) |  | 0 | Social computing platforms typically deal with data that are either related to humans or generated by humans. Consequently, effective design of these platforms needs to be cognizant ofsocial psychology theories. In this tutorial, we review and summarize the research thus far into the paradigm ofpsychology theory-informed design of social computing platforms where the design is guided by theories from social psychology in addition to theories from computer science. Specifically, we review techniques and frameworks that embrace this paradigm in the arena of social influence. In addition, we suggest open problems and new research directions. | Sourav S. Bhowmick, Hui Li, S. H. Annabel Chen, Yining Zhao |  |
| 489 |  |  [Discrete Choice and Applications](https://doi.org/10.1145/3589335.3641244) |  | 0 | This paper presents a framework for estimating and updating user preferences in the context of app-based recommender systems. We specifically consider recommender systems which provide personalized menus of options to users. A Hierarchical Bayes procedure is applied in order to account for inter- and intra-consumer heterogeneity, representing random taste variations among individuals and among choice situations (menus) for a given individual, respectively. Three levels of preference parameters are estimated: population-level, individual-level and menu-specific. In the context of a recommender system, the estimation of these parameters is repeated periodically in an offline process in order to account for trends, such as changing market conditions. Furthermore, the individual-level parameters are updated in real-time as users make choices in order to incorporate the latest information from the users. This online update is computationally efficient which makes it feasible to embed it in a real-time recommender system. The estimated individual-level preferences are stored for each user and retrieved as inputs to a menu optimization model in order to provide recommendations. The proposed methodology is applied to both Monte-Carlo and real data. It is observed that the online update of the parameters is successful in improving the parameter estimates in real-time. This framework is relevant to various recommender systems that generate personalized recommendations ranging from transportation to e-commerce and online marketing, but is particularly useful when the attributes of the alternatives vary over time. | Flavio Chierichetti, Ravi Kumar, Andrew Tomkins | Massachusetts Institute of Technology, Department of Civil and Environmental Engineering, 77 Massachusetts Avenue, Room 1-181, Cambridge, MA 02139, United States of America; Massachusetts Institute of Technology, Department of Civil and Environmental Engineering, Room 181, United States of America; Massachusetts Institute of Technology, Department of Civil and Environmental Engineering, United States of America; Delft University of Technology, Department of Maritime and Transport Technology, Netherlands |
| 490 |  |  [Mining Temporal Networks](https://doi.org/10.1145/3589335.3641245) |  | 0 | Networks (or graphs) are used to represent and analyze large datasets of objects and their relations. Naturally, real-world networks have a temporal component: for instance, interactions between objects have a timestamp and a duration. In this tutorial we present models and algorithms for mining temporal networks, i.e., network data with temporal information. We overview different models used to represent temporal networks. We highlight the main differences between static and temporal networks, and discuss the challenges arising from introducing the temporal dimension in the network representation. We present recent papers addressing the most well-studied problems in the setting of temporal networks, including computation of centrality measures, motif detection and counting, community detection and monitoring, event and anomaly detection, analysis of epidemic processes and influence spreading, network summarization, and structure prediction. | Aristides Gionis, Lutz Oettershagen, Ilie Sarpe | Nordea Data Science Lab, Helsinki, Finland; Aalto University, Helsinki, Finland |
| 491 |  |  [Lecture-style Tutorial: Towards Graph Foundation Models](https://doi.org/10.1145/3589335.3641246) |  | 0 | Emerging as fundamental building blocks for diverse artificial intelligence applications, foundation models have achieved notable success across natural language processing and many other domains. Concurrently, graph machine learning has gradually evolved from shallow methods to deep models to leverage the abundant graph-structured data that constitute an important pillar in the data ecosystem for artificial intelligence. Naturally, the emergence and homogenization capabilities of foundation models have piqued the interest of graph machine learning researchers. This has sparked discussions about developing a next-generation graph learning paradigm, one that is pre-trained on broad graph data and can be adapted to a wide range of downstream graph-based tasks. However, there is currently no clear definition or systematic analysis for this type of work. In this tutorial, we will introduce the concept of graph foundation models (GFMs), and provide a comprehensive exposition on their key characteristics and underpinning technologies. Subsequently, we will thoroughly review existing works that lay the groundwork towards GFMs, which are summarized into three primary categories based on their roots in graph neural networks, large language models, or a hybrid of both. Beyond providing a comprehensive overview and in-depth analysis of the current landscape and progress towards graph foundation models, this tutorial will also explore potential avenues for future research in this important and dynamic field. Finally, to help the audience gain a systematic understanding of the topics covered in this tutorial, we present further details in our recent preprint paper, "Towards Graph Foundation Models: A Survey and Beyond"[4], available at https://arxiv.org/pdf/2310.11829.pdf. | Chuan Shi, Cheng Yang, Yuan Fang, Lichao Sun, Philip S. Yu |  |
| 492 |  |  [Understanding (Dark) Humour with Internet Meme Analysis](https://doi.org/10.1145/3589335.3641249) |  | 0 | Internet memes, in their ubiquitous spread across the digital landscape, have transformed into a potent communicative force. Their significance beckons keen interest from researchers and practitioners alike, necessitating a deep comprehension of their nuanced forms and functions. Recent studies have honed in on diverse facets of memes, particularly in detecting offensive material and discerning sarcasm, yet comprehensive instructional resources remain sparse. Addressing this void, our tutorial delivers an integrated framework for dissecting the complex humor of memes. It weaves together disciplines such as natural language processing, computer vision, and multimodal modeling, empowering participants to decode meanings, analyze sentiments, and identify offensive content within memes. Attendees will engage in hands-on exercises and observe demonstrations, tapping into established datasets and cutting-edge algorithms. This equips them with the expertise to navigate the intricacies of meme analysis and to contribute substantively to this dynamic domain. For more information, please check out our tutorial teaser video. | Ming Shan Hee, Rui Cao, Tanmoy Chakraborty, Roy KaWei Lee |  |
| 493 |  |  [Privacy in Web Advertising: Analytics and Modeling](https://doi.org/10.1145/3589335.3641252) |  | 0 | Privacy in general, and differential privacy (DP) in particular, have become important topics in data mining and machine learning. Digital advertising is a critical component of the internet and is powered by large-scale data analytics and machine learning models; privacy concerns around these are on the rise. Despite the central importance of private ad analytics and training privacy-preserving ad prediction models, there has been relatively little exposure of this subject to the broader Web community. In the past three years, the interest in privacy and the interest in online advertising have been steadily growing. The aim of this tutorial is to provide researchers with an introduction to the problems that arise in private analytics and modeling in advertising, survey recent results, and describe the main research challenges in the space. | Badih Ghazi, Ravi Kumar, Pasin Manurangsi |  |
| 494 |  |  [Large Language Models for Graphs: Progresses and Directions](https://doi.org/10.1145/3589335.3641251) |  | 0 | Graph neural networks (GNNs) have emerged as fundamental methods for handling structured graph data in various domains, including citation networks, molecule prediction, and recommender systems. They enable the learning of informative node or graph representations, which are crucial for tasks such as link prediction and node classification in the context of graphs. To achieve high-quality graph representation learning, certain essential factors come into play: clean labels, accurate graph structures, and sufficient initial node features. However, real-world graph data often suffer from noise and sparse labels, while different datasets have unique feature constructions. These factors significantly impact the generalization capabilities of graph neural networks, particularly when faced with unseen tasks. Recently, due to the efficent text processing and task generalization capability of large language models (LLMs), there has been a promising approach to address the challenges mentioned above by combining large language models with graph data. This tutorial offers an overview of incorporating large language models into the graph domain, accompanied by practical examples. The methods are categorized into three dimensions: utilizing LLMs as augmenters, predictors, and agents for graph learning tasks. We will delve into the current progress and future directions within this field. By introducing this emerging topic, our aim is to enhance the audience's understanding of LLM-based graph learning techniques, foster idea exchange, and encourage discussions that drive continuous advancements in this domain. | Chao Huang, Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh V. Chawla |  |
| 495 |  |  [New Frontiers of Knowledge Graph Reasoning: Recent Advances and Future Trends](https://doi.org/10.1145/3589335.3641254) |  | 0 | Knowledge graph reasoning plays an important role in data mining, AI, Web, and social science. These knowledge graphs serve as intuitive repositories of human knowledge, allowing for the inference of new information. However, traditional symbolic reasoning, while powerful in its own right, faces challenges posed by incomplete and noisy data in the knowledge graphs. In contrast, recent years have witnessed the emergence of Neural Symbolic AI, an exciting development that fuses the capabilities of deep learning and symbolic reasoning. It aims to create AI systems that are not only highly interpretable and explainable but also incredibly versatile, effectively bridging the gap between symbolic and neural approaches. Furthermore, with the advent of large language models, the integration of LLMs with knowledge graph reasoning has emerged as a prominent frontier, offering the potential to unlock unprecedented capabilities. This tutorial aims to comprehensively review different aspects of knowledge graph reasoning applications and also introduce the recent advances about Neural Symbolic reasoning and combining knowledge graph reasoning with large language models. It is intended to benefit researchers and practitioners in the fields of data mining, AI, Web, and social science. | Lihui Liu, Zihao Wang, Jiaxin Bai, Yangqiu Song, Hanghang Tong |  |
| 496 |  |  [Simulating Human Society with Large Language Model Agents: City, Social Media, and Economic System](https://doi.org/10.1145/3589335.3641253) |  | 0 | This tutorial will delve into the fascinating realm of simulating human society using Large Language Model (LLM)-driven agents, exploring their applications in cities, social media, and economic systems. Through this tutorial, participants will gain insights into the integration of LLMs into human society simulation, providing a comprehensive understanding of how these models can accurately represent human interactions, decision-making processes, and societal dynamics from cities to social media and to economic systems. The tutorial will introduce the essential background, discuss the motivation and challenges, and elaborate on the recent advances. | Chen Gao, Fengli Xu, Xu Chen, Xiang Wang, Xiangnan He, Yong Li |  |
| 497 |  |  [Text-Attributed Graph Representation Learning: Methods, Applications, and Challenges](https://doi.org/10.1145/3589335.3641255) |  | 0 | Text documents are usually connected in a graph structure, resulting in an important class of data named text-attributed graph, e.g., paper citation graph and Web page hyperlink graph. On the one hand, Graph Neural Networks (GNNs) consider text in each document as general vertex attribute and do not specifically deal with text data. On the other hand, Pre-trained Language Models (PLMs) and Topic Models (TMs) learn effective document embeddings. However, most models focus on text content in each single document only, ignoring link adjacency across documents. The above two challenges motivate the development of text-attributed graph representation learning, combining GNNs with PLMs and TMs into a unified model and learning document embeddings preserving both modalities, which fulfill applications, e.g., text classification, citation recommendation, question answering, etc. In this lecture-style tutorial, we will provide a systematic review of text-attributed graph, including its formal definition, recent methods, diverse applications, and challenges. Specifically, i) we will formally define text-attributed graph and briefly review GNNs, PLMs, and TMs, which are the fundamentals of some existing methods. ii) We will then revisit the technical details of text-attributed graph models, which are generally split into two categories, PLM-based and TM-based. iii) Besides, we will show diverse applications built on text-attributed graph. iv) Finally, we will discuss some challenges of existing models and propose solutions for future research. | Delvin Ce Zhang, Menglin Yang, Rex Ying, Hady W. Lauw |  |
| 498 |  |  [Toward Mitigating Misinformation and Social Media Manipulation in LLM Era](https://doi.org/10.1145/3589335.3641256) |  | 0 | The pervasive abuse of misinformation to influence public opinion on social media has become increasingly evident in various domains, encompassing politics, as seen in presidential elections, and healthcare, most notably during the recent COVID-19 pandemic. This threat has grown in severity as the development of Large Language Models (LLMs) empowers manipulators to generate highly convincing deceptive content with greater efficiency. Furthermore, the recent strides in chatbots integrated with LLMs, such as ChatGPT, have enabled the creation of human-like interactive social bots, posing a significant challenge to both human users and the social-bot-detection systems of social media platforms.These challenges motivate researchers to develop algorithms to mitigate misinformation and social media manipulations. This tutorial introduces the advanced machine learning researches that are helpful for this goal, including (1) detection of social manipulators, (2) learning causal models of misinformation and social manipulation, and (3) LLM-generated misinformation detection. In addition, we also present possible future directions. | Yizhou Zhang, Karishma Sharma, Lun Du, Yan Liu |  |
| 499 |  |  [Curriculum Learning: Theories, Approaches, Applications, Tools, and Future Directions in the Era of Large Language Models](https://doi.org/10.1145/3589335.3641257) |  | 0 | This tutorial focuses on curriculum learning (CL), an important topic in machine learning, which gains an increasing amount of attention in the research community. CL is a learning paradigm that enables machines to learn from easy data to hard data, imitating the meaningful procedure of human learning with curricula. As an easy-to-use plug-in, CL has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision, natural language processing, data mining, reinforcement learning, etc. Therefore, it is essential introducing CL to more scholars and researchers in the machine learning community. However, there have been no tutorials on CL so far, motivating the organization of our tutorial on CL at WWW 2024. To give a comprehensive tutorial on CL, we plan to organize it from the following aspects: (1) theories, (2) approaches, (3) applications, (4) tools and (5) future directions. First, we introduce the motivations, theories and insights behind CL. Second, we advocate novel, high-quality approaches, as well as innovative solutions to the challenging problems in CL. Then we present the applications of CL in various scenarios, followed by some relevant tools. In the end, we discuss open questions and the future direction in the era of large language models. We believe this topic is at the core of the scope of WWW and is attractive to the audience interested in machine learning from both academia and industry. | Xin Wang, Yuwei Zhou, Hong Chen, Wenwu Zhu |  |
| 500 |  |  [English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts](https://doi.org/10.1145/3589335.3651902) |  | 0 | Emotion classification in text is a challenging and subjective task, due to the involved cognitive inference processes that are required to interpret a textual stimulus. In addition, the set of emotion categories is highly domain-specific. For instance, literature analysis might require the use of aesthetic emotions (e.g., finding something beautiful), and social media analysis could benefit from fine-grained sets (e.g., separating anger from annoyance) in contrast to basic emotion categories. This renders the task an interesting field for zero-shot classifications, in which the label set is not known at model development time. Unfortunately, most resources for emotion analysis are English, and therefore, most studies on emotion analysis have been performed in English, including those that involve prompting language models for text labels. This leaves us with a research gap that we address in this paper: In which language should we prompt for emotion labels on non-English texts? This is particularly of interest when we have access to a multilingual large language model, because we could request labels with English prompts even for non-English data. Our experiments with natural language inference-based language models show that it is consistently better to use English prompts even if the data is in a different language. | Patrick Bareiß, Roman Klinger, Jeremy Barnes |  |
| 501 |  |  [Automatic Design Summary Generation with Generative AI](https://doi.org/10.1145/3589335.3651901) |  | 0 | In the private residential sales market, obtaining orders for exterior design requires a proposal considering the constraints of the location and customer. The design summary is a proposal document that describes concepts at the earliest stage of exterior design. It not only appeals to the customer, but also is referenced in the design drawing. However, the quality varies depending on the skill of the creator because the construction of a design summary requires the knowledge of human experts. This paper aims to generate the design summary using generative AI. Firstly, we analyze the characteristics of the design summary to identify the essential elements. Then, we propose a sequence of prompts to generate the design summary. Finally, we conducted a comparative evaluation between design summaries created by experts and those generated by generative AI. | Daisuke Ikoma, Eisuke Aoki, Tomoki Taniguchi, Shinya Suzuki, Tomoko Ohkuma |  |
| 502 |  |  [Data Augmentation for Smishing Detection: A Theory-based Prompt Engineering Approach](https://doi.org/10.1145/3589335.3651903) |  | 0 | Smishing, which refers to social engineering attacks delivered through mobile devices such as smartphones, poses significant threats, yet limited data hinder the development of effective countermeasures. To tackle this, we propose a novel prompt engineering method for data augmentation in smishing detection. Distinguished by its utilization of insights from social science on smishing mechanisms, our approach offers a promising avenue for improving machine learning models in combating smishing attacks. | Ho Sung Shim, Hyoungjun Park, Kyuhan Lee, JangSun Park, Seonhye Kang |  |
| 503 |  |  [Prompt-Eng: Healthcare Prompt Engineering: Revolutionizing Healthcare Applications with Precision Prompts](https://doi.org/10.1145/3589335.3651904) |  | 0 | Prompt Engineering has emerged as a pivotal technique in Natural Language Processing, providing a flexible approach for leveraging pre-trained language models. Particularly, a prompt is used to instruct the model to adopt the nature of given prompts, which became a well-adoptable approach in wide areas of domains. Yet, existing prompt-guided frameworks are experiencing various challenges, such as crafting prompts for specific tasks to achieve clarity and conciseness and avoid ambiguity, which requires time and computational resources. Further existing methods heavily rely on the extensive labelled datasets, yet many domain-specific challenges exist, particularly in healthcare. This study presents Prompt-Eng, a novel framework emphasizing its wide-ranging applications in healthcare, where we design precise prompts with positive and negative aspects; we hypothesize that designing prompts in pairs helps models to generalize effectively. We delve into the significance of quick design and optimization, highlighting its influence in shaping model responses. In addition, we explore the increasing demand for prompts that are aware of the context in multimodal data analysis and the incorporation of prompt engineering in new machine-learning approaches. The essence of our approach is in creating tailored prompts, which serve as instructive guidelines for the models during the prediction procedure. The proposed methodology emphasizes utilizing context-aware prompt pairs to facilitate interpreting and extracting healthcare information from a health corpus by models. The study uses the medical MIMIC-III \footnotehttps://physionet.org/content/mimiciii/1.4/ corpus to predict medicine prescriptions. The paper also explores visual and textual prompts for X-ray image analysis for pneumonia prediction on the MIMIC-CXR \footnote\urlhttps://physionet.org/content/mimic-cxr/2.0.0/ dataset. This approach stands out from existing methods by addressing challenges such as clarity, conciseness, and context awareness, thereby enabling improved interpretation and extraction of healthcare information from diverse data sources. | Awais Ahmed, Mengshu Hou, Rui Xi, Xiaoyang Zeng, Syed Attique Shah |  |
| 504 |  |  [Towards Invariant Time Series Forecasting in Smart Cities](https://doi.org/10.1145/3589335.3651897) |  | 0 | In the transformative landscape of smart cities, the integration of the cutting-edge web technologies into time series forecasting presents a pivotal opportunity to enhance urban planning, sustainability, and economic growth. The advancement of deep neural networks has significantly improved forecasting performance. However, a notable challenge lies in the ability of these models to generalize well to out-of-distribution (OOD) time series data. The inherent spatial heterogeneity and domain shifts across urban environments create hurdles that prevent models from adapting and performing effectively in new urban environments. To tackle this problem, we propose a solution to derive invariant representations for more robust predictions under different urban environments instead of relying on spurious correlation across urban environments for better generalizability. Through extensive experiments on both synthetic and real-world data, we demonstrate that our proposed method outperforms traditional time series forecasting models when tackling domain shifts in changing urban environments. The effectiveness and robustness of our method can be extended to diverse fields including climate modeling, urban planning, and smart city resource management. | Ziyi Zhang, Shaogang Ren, Xiaoning Qian, Nick Duffield |  |
| 505 |  |  [Open Metaverse: Issues, Evolution, and Future](https://doi.org/10.1145/3589335.3651898) |  | 0 | With the evolution of content on the web and the Internet, there is a need for cyberspace that can be used to work, live, and play in digital worlds regardless of geography. The Metaverse provides the possibility of future Internet and represents a future trend. In the future, the Metaverse will be a space where the real and the virtual are combined. In this article, we have a comprehensive survey of the compelling Metaverse. We introduce computer technology, the history of the Internet, and the promise of the Metaverse as the next generation of the Internet. In addition, we briefly introduce the related concepts of the Metaverse, including novel terms like trusted Metaverse, human-intelligence Metaverse, personalized Metaverse, AI-enabled Metaverse, Metaverse-as-a-service, etc. Moreover, we present the challenges of the Metaverse such as limited resources and ethical issues. We also present Metaverse's promising directions, including lightweight Metaverse and autonomous Metaverse. We hope this survey will provide some helpful prospects and insightful directions about the Metaverse to related developments. | Zefeng Chen, Wensheng Gan, Jiayi Sun, Jiayang Wu, Philip S. Yu |  |
| 506 |  |  [Implementing Sustainable Urban Mobility Transitions in Positive Energy Districts](https://doi.org/10.1145/3589335.3651899) |  | 0 | This paper examines Smart Cities transition with the focus on urban mobility. We demonstrate how a multi-stakeholder, multi-disciplinary approach can support integration of systems, data, people, and organisations with a case study on new integrated mobility solutions and urban decarbonisation. | Dirk Ahlers, Bjørn Ove Berthelsen, Tor Rune Skoglund, Kelly Riedesel |  |
| 507 |  |  [Homogenizing Data Flows in Smart Cities: Value-driven use Cases in the Era of Citiverse](https://doi.org/10.1145/3589335.3651900) |  | 0 | Context based data receive an increasing attention for Smart City (SC) flow homogenization and standardization. SC hubness can be a solution that can simplify these flows and transform them to standardized message exchanges, which can be easily retrieved. Several use cases can justify the SC hubness' potential and will be summarized in this paper. However, the aim of this article is to analyze and explain their potential in the era of metaverse in cities, the so-called "citiverse". The role of data in citiverse gains an additional importance since it is a data-oriented ecosystem, but, it has to be seen around the new capabilities and values that citiverse may bring. | Leonidas G. Anthopoulos, Ioannis Nikolaou |  |
| 508 |  |  [Spatio-Temporal Challenges in Understanding your (Smart) City](https://doi.org/10.1145/3589335.3652580) |  | 0 | In this talk, we will explore challenges and opportunities of spatio-temporal information access as connecting temporal and spatial dimensions of mining and analysis. We focus on use cases and examples in the development of systems and services in smart sustainable cities, and in urban energy and climate transitions. | Dirk Ahlers |  |
| 509 |  |  [A Longitudinal Study of Content Control Mechanisms](https://doi.org/10.1145/3589335.3651893) |  | 0 | As generative AI continues to evolve, it becomes increasingly important for site owners to effectively communicate their conditions and preferences to web agents to maintain data sovereignty. This necessity underscores the importance of an ecosystem where the technical means to prevent unauthorized data mining and to set conditions on the usage of web resources are readily available. Our research focuses on the temporal development of such technical content control methods, examining two primary mechanisms: the regulation of web robots via the Robots Exclusion Protocol and the semantic annotation of web documents with licensing information. Through a longitudinal study, we analyze the implementation and recent modifications of robots.txt files, robot directives (such as noindex, nofollow, etc.), and license-related HTML annotations. This study is driven by the growing awareness among site owners regarding the control over their content in the face of the progression of AI, highlighting the critical need for effective web content control strategies to protect and appropriately manage the wealth of texts, images, videos, and other content populating the internet. | Michael Dinzinger, Michael Granitzer |  |
| 510 |  |  [TIQ: A Benchmark for Temporal Question Answering with Implicit Time Constraints](https://doi.org/10.1145/3589335.3651895) |  | 0 | Temporal question answering (QA) involves explicit (e.g., "...before 2024") or implicit (e.g., "...during the Cold War period") time constraints. Implicit constraints are more challenging; yet benchmarks for temporal QA largely disregard such questions. This shortcoming spans three aspects. First, implicit questions are scarce in existing benchmarks. Second, questions are created based on hand-crafted rules, thus lacking diversity in formulations. Third, the source for answering is either a KB or a text corpus, disregarding cues from multiple sources. We propose a benchmark, called TIQ (Temporal Implicit Questions), based on novel techniques for constructing questions with implicit time constraints. First, questions are created automatically, with systematic control of topical diversity, timeframe, head vs. tail entities, etc. Second, questions are formulated using diverse snippets and further paraphrasing by a large language model. Third, snippets for answering come from a variety of sources including KB, text, and infoboxes. The TIQ benchmark contains 10,000 questions with ground-truth answers and underlying snippets as supporting evidence. | Zhen Jia, Philipp Christmann, Gerhard Weikum |  |
| 511 |  |  [Attentive Partial Convolution for RGBD Image Inpainting](https://doi.org/10.1145/3589335.3651906) |  | 0 | In this work, we demonstrated the use of Partial Convolutions for RGBD image inpainting. We proposed the two models L-PConv and Attn-PConv. The baseline partial convolution model is outperformed by both of our proposed models, with the Attn-PConv model performing the best. The proposed Attn-PConv model is able to infill missing pixels with relatively less training time when compared to some other GAN-based models. As far as we know this is the first time a partial convolution model has been used successfully for RGBD image inpainting. The results( Image SSIM: 0.9787, Image PSNR: 30.9665, Depth SSIM: 0.9818, Depth PSNR: 35.7311) indicate that our model is successful in RGBD image inpainting. The addition of the additional loss terms and the Attentive Normalization techniques help improve the performance of the model significantly. We believe our model can be successfully used in AR-related applications where infilling missing pixels is performed frequently especially for both RGB and Depth images together. Beyond the scope of this study, we envision practical applications for our model in augmented reality, particularly in scenarios where frequent pixel infilling is required for both RGB and Depth images. In the future our research trajectory aims to incorporate higher resolution, aligning with the capabilities of modern cameras capable of capturing images at 4k resolution. | Ankan Dash, Guiling Wang, Tao Han |  |
| 512 |  |  [GPT Assisted Annotation of Rhetorical and Linguistic Features for Interpretable Propaganda Technique Detection in News Text](https://doi.org/10.1145/3589335.3651909) |  | 0 | While the use of machine learning for the detection of propaganda techniques in text has garnered considerable attention, most approaches focus on "black-box" solutions with opaque inner workings. Interpretable approaches provide a solution, however, they depend on careful feature engineering and costly expert annotated data. Additionally, language features specific to propagandistic text are generally the focus of rhetoricians or linguists, and there is no data set labeled with such features suitable for machine learning. This study codifies 22 rhetorical and linguistic features identified in literature related to the language of persuasion for the purpose of annotating an existing data set labeled with propaganda techniques. To help human experts annotate natural language sentences with these features, RhetAnn, a web application, was specifically designed to minimize an otherwise considerable mental effort. Finally, a small set of annotated data was used to fine-tune GPT-3.5, a generative large language model (LLM), to annotate the remaining data while optimizing for financial cost and classification accuracy. This study demonstrates how combining a small number of human annotated examples with GPT can be an effective strategy for scaling the annotation process at a fraction of the cost of traditional annotation relying solely on human experts. The results are on par with the best performing model at the time of writing, namely GPT-4, at 10x less the cost. Our contribution is a set of features, their properties, definitions, and examples in a machine-readable format, along with the code for RhetAnn and the GPT prompts and fine-tuning procedures for advancing state-of-the-art interpretable propaganda technique detection. | Kyle Hamilton, Luca Longo, Bojan Bozic |  |
| 513 |  |  [A Bayesian Framework for Measuring Association and Its Application to Emotional Dynamics in Web Discourse](https://doi.org/10.1145/3589335.3651911) |  | 0 | This paper introduces a Bayesian framework designed to measure the degree of association between categorical random variables. The method is grounded in the formal definition of variable independence and is implemented using Markov Chain Monte Carlo (MCMC) techniques. Unlike commonly employed techniques in Association Rule Learning, this approach enables a clear and precise estimation of confidence intervals and the statistical significance of the measured degree of association. We applied the method to non-exclusive emotions identified by annotators in 4,613 tweets written in Portuguese. This analysis revealed pairs of emotions that exhibit associations and mutually opposed pairs. Moreover, the method identifies hierarchical relations between categories, a feature observed in our data, and is utilized to cluster emotions into basic-level groups. | Henrique S. Xavier, Diogo Cortiz, Mateus Silvestrin, Ana Luísa Freitas, Letícia Yumi Nakao Morello, Fernanda Naomi Pantaleão, Gabriel Gaudencio do Rêgo |  |
| 514 |  |  [Leveraging Large Language Models to Detect Influence Campaigns on Social Media](https://doi.org/10.1145/3589335.3651912) |  | 0 | Social media influence campaigns pose significant challenges to public discourse and democracy. Traditional detection methods fall short due to the complexity and dynamic nature of social media. Addressing this, we propose a novel detection method using Large Language Models (LLMs) that incorporates both user metadata and network structures. By converting these elements into a text format, our approach effectively processes multilingual content and adapts to the shifting tactics of malicious campaign actors. We validate our model through rigorous testing on multiple datasets, showcasing its superior performance in identifying influence efforts. This research not only offers a powerful tool for detecting campaigns, but also sets the stage for future enhancements to keep up with the fast-paced evolution of social media-based influence tactics. | Luca Luceri, Eric Boniardi, Emilio Ferrara |  |
| 515 |  |  [Towards Fact-check Summarization Leveraging on Argumentation Elements Tied to Entity Graphs](https://doi.org/10.1145/3589335.3651914) |  | 0 | Fact-check consumers can have different preferences regarding the amount of text being used for explaining the claim veracity verdict. Dynamically adapting the size of a fact-check report is thus an important functionality for systems designed to convey claim verification explainability. Recent works have experimented with applying transformers-based or LLM-based text summarization methods in a zero-shot or few-shot manner, making use of some existing texts available in the summary parts of fact-check reports (e.g., called "justification'' in PolitiFact). However, for complex fact-checks, the purely sub-symbolic summarizers tend to either omit some elements of the fact-checker's argumentation chains or include contextual statements that may not be essential at the given level of granularity. In this paper, we propose a new method for enhancing fact-check summarization with the aim of injecting elements of structured fact-checker argumentation. This argumentation is, in turn, not only captured at the discourse level but tied to an entity graph representing the fact-check, for which we employ the PURO diagrammatic language. We have empirically performed a manual analysis of fact-check reports from two fact-checker websites, yielding (1) textual snippets containing the argumentation essence of the fact-check report and (2) categorized argumentation elements tied to entity graphs. These snippets are then fed to a state-of-the-art hybrid summarizer which has previously produced accurate fact-check summaries, as an additional input. We observe mild improvements on various ROUGE metrics, even if the validity of the results is limited given the small size of the dataset. We also compare the human-provided argumentation element categories with those returned, for the given fact-check ground truth summary, using a pre-trained language model upon both basic and augmented prompting. This yields a moderate accuracy as the model often fails to comply with the explicit given instructions. | Katerina Haniková, David Chudán, Vojtech Svátek, Peter Vajdecka, Raphaël Troncy, Filip Vencovský, Jana Syrovátková |  |
| 516 |  |  [DCAI: Data-centric Artificial Intelligence](https://doi.org/10.1145/3589335.3641297) |  | 0 | The emergence of Data-centric AI (DCAI) represents a pivotal shift in AI development, redirecting focus from model refinement to prioritizing data quality. This paradigmatic transition emphasizes the critical role of data in AI. While past approaches centered on refining models, they often overlooked potential data imperfections, raising questions about the true potential of enhanced model performance. DCAI advocates the systematic engineering of data, complementing existing efforts and playing a vital role in driving AI success. This transition has spurred innovation in various machine learning and data mining algorithms and their applications on the Web. Therefore, we propose the DCAI Workshop at WWW'24, which offers a platform for academic researchers and industry practitioners to showcase the latest advancements in DCAI research and their practical applications in the real world. | Wei Jin, Haohan Wang, Daochen Zha, Qiaoyu Tan, Yao Ma, Sharon Li, SuIn Lee |  |
| 517 |  |  [Robust Data-centric Graph Structure Learning for Text Classification](https://doi.org/10.1145/3589335.3651915) |  | 0 | Over the past decades, text classification underwent remarkable evolution across diverse domains. Despite these advancements, most existing model-centric methods in text classification cannot generalize well on class-imbalanced datasets that contain high-similarity textual information. Instead of developing new model architectures, data-centric approaches enhance the performance by manipulating the data structure. In this study, we aim to investigate robust data-centric approaches that can help text classification in our collected dataset, the metadata of survey papers about Large Language Models (LLMs). In the experiments, we explore four paradigms and observe that leveraging arXiv's co-category information on graphs can help robustly classify the text data over the other three paradigms, conventional machine-learning algorithms, pre-trained language models' fine-tuning, and zero-shot / few-shot classifications using LLMs. | Jun Zhuang |  |
| 518 |  |  [Data Quality-based Gradient Optimization for Recurrent Neural Networks](https://doi.org/10.1145/3589335.3651918) |  | 0 | Time series forecasting holds significant value in various application scenarios. However, existing forecasting methods primarily focus on optimizing model architecture while neglecting the substantial impact of data quality on model learning. In this study, we aim to enhance model performance by optimizing data utilization based on data quality and propose a Data Quality-based Gradient Optimization (DQGO) method to facilitate training of recurrent neural networks. Firstly, we define sample quality as the matching degree between samples and model, and suggest using the attention entropy to calculate the sample quality through an attention mechanism. Secondly, we optimize the model's gradient vector by giving different weights to samples with different quality. Through experiments conducted on six datasets, the results demonstrate that DQGO significantly improves LSTM's performance. In certain cases, it even surpasses the state-of-the-art models. | Feihu Huang, Peiyu Yi, Shan Li, Haiwen Xu |  |
| 519 |  |  [CFinDEE: A Chinese Fine-Grained Financial Dataset for Document-Level Event Extraction](https://doi.org/10.1145/3589335.3651921) |  | 0 | Document-level event extraction faces numerous challenges in accurately modeling real-world financial scenarios, particularly due to the inadequacies in existing datasets regarding data scale and fine-grained annotations. The development of datasets is a crucial factor in driving research progress; therefore, we present a high-quality Chinese document-level event extraction dataset, CFinDEE. This dataset, grounded in real-world financial news, defines 22 event types and 116 argument roles, annotating 26,483 events and 107,096 event arguments. CFinDEE aims to address these shortcomings by providing more comprehensive annotations and data augmentation, offering richer resources for document-level event extraction in the financial domain. CFinDEE extends data both horizontally and vertically, where horizontal expansion enriches the types of financial events, enhancing the diversity of the dataset; vertical expansion, by increasing the scale of the data, effectively boosts the practical value of the dataset. Experiments conducted on multiple advanced models have validated the high applicability and effectiveness of the CFinDEE dataset for document-level event extraction tasks in the financial field. | Tian Zhang, Maofu Liu, Bingying Zhou |  |
| 520 |  |  [FASETS: Discovering Faceted Sets of Entities](https://doi.org/10.1145/3589335.3651924) |  | 0 | Computing related entities for a given seed entity is an important task in exploratory search and comparative data analysis.Prior works, using the seed-based set expansion paradigm, have focused on the single aspect of identifying homogeneous sets with high pairwise relatedness. A few recent works discuss cluster-based approaches to tackle multi-faceted set expansion, however, they fail in harnessing the specificity of the clusters and generating an explanation for them. This paper poses the multi-faceted set expansion as an optimization problem, where the goal is to compute multiple groups of entities that convey different aspects in an explainable manner, with high similarity within each group and diversity across groups. To extend a seed entity, we collect a large pool of candidate entities and facets (e.g., categories)from Wikipedia and knowledge bases, and construct a candidate graph. We propose FASETS, an efficient algorithm for computing faceted groups of bounded size, based on random walks over the candidate graph. Our extensive evaluation shows the superiority of FASETS against prior baselines, with regard to ground-truth collected from crowdsourcing. | Koninika Pal, Hiba Arnaout, Simon Razniewski, Gerhard Weikum |  |
| 521 |  |  [Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept Understanding](https://doi.org/10.1145/3589335.3651927) |  | 0 | The rapid evolution of text-to-image diffusion models has opened the door of generative AI, enabling the translation of textual descriptions into visually compelling images with remarkable quality. However, a persistent challenge within this domain is the optimization of prompts to effectively convey abstract concepts into concrete objects. For example, text encoders can hardly express "peace", while can easily illustrate olive branches and white doves. This paper introduces a novel approach named Prompt Optimizer for Abstract Concepts (POAC) specifically designed to enhance the performance of text-to-image diffusion models in interpreting and generating images from abstract concepts. We propose a Prompt Language Model (PLM), which is initialized from a pre-trained language model, and then fine-tuned with a curated dataset of abstract concept prompts. The dataset is created with GPT-4 to extend the abstract concept to a scene and concrete objects. Our framework employs a Reinforcement Learning (RL)-based optimization strategy, focusing on the alignment between the generated images by a stable diffusion model and optimized prompts. Through extensive experiments, we demonstrate that our proposed POAC significantly improves the accuracy and aesthetic quality of generated images, particularly in the description of abstract concepts and alignment with optimized prompts. We also present a comprehensive analysis of our model's performance across diffusion models under different settings, showcasing its versatility and effectiveness in enhancing abstract concept representation. | Zezhong Fan, Xiaohan Li, Kaushiki Nag, Chenhao Fang, Topojoy Biswas, Jianpeng Xu, Kannan Achan |  |
| 522 |  |  [LLM-Guided Counterfactual Data Generation for Fairer AI](https://doi.org/10.1145/3589335.3651929) |  | 0 | With the widespread adoption of deep learning-based models in practical applications, concerns about their fairness have become increasingly prominent. Existing research indicates that both the model itself and the datasets on which they are trained can contribute to unfair decisions. In this paper, we address the data-related aspect of the problem, aiming to enhance the data to guide the model towards greater trustworthiness. Due to their uncontrolled curation and limited understanding of fairness drivers, real-world datasets pose challenges in eliminating unfairness. Recent findings highlight the potential of Foundation Models in generating substantial datasets. We leverage these foundation models in conjunction with state-of-the-art explainability and fairness platforms to generate counterfactual examples. These examples are used to augment the existing dataset, resulting in a more fair learning model. Our experiments were conducted on the CelebA and UTKface datasets, where we assessed the quality of generated counterfactual data using various bias-related metrics. We observed improvements in bias mitigation across several protected attributes in the fine-tuned model when utilizing counterfactual data. | Ashish Mishra, Gyanaranjan Nayak, Suparna Bhattacharya, Tarun Kumar, Arpit Shah, Martin Foltin |  |
| 523 |  |  [Only Send What You Need: Learning to Communicate Efficiently in Federated Multilingual Machine Translation](https://doi.org/10.1145/3589335.3651931) |  | 0 | Federated learning (FL) is a promising approach for solving multilingual tasks, potentially enabling clients with their own language-specific data to collaboratively construct a high-quality neural machine translation (NMT) model. However, communication constraints in practical network systems present challenges for exchanging large-scale NMT engines between FL parties. In this paper, we propose a meta-learning-based adaptive parameter selection methodology, MetaSend, that improves the communication efficiency of model transmissions from clients during FL-based multilingual NMT training. Our approach learns a dynamic threshold for filtering parameters prior to transmission without compromising the NMT model quality, based on the tensor deviations of clients between different FL rounds. Through experiments on two NMT datasets with different language distributions, we demonstrate that MetaSend obtains substantial improvements over baselines in translation quality in the presence of a limited communication budget. | YunWei Chu, DongJun Han, Christopher G. Brinton |  |
| 524 |  |  [Federated Learning in Large Model Era: Vision-Language Model for Smart City Safety Operation Management](https://doi.org/10.1145/3589335.3651939) |  | 0 | With the tremendous success of large language models such as ChatGPT, artificial intelligence has entered a new era of large models. Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. In this paper, we tackle the problem of building large vision-language intelligent models for specific industrial domains by leveraging the general large models and federated learning. We compare the challenges faced by federated learning in the era of small models and large models from different dimensions, and propose a technical framework for federated learning in the era of large models.Specifically, our framework mainly considers three aspects: heterogeneous model fusion, flexible aggregation methods, and data quality improvement. Based on this framework, we conduct a case study of leading enterprises contributing vision-language data and expert knowledge to city safety operation management. The preliminary experiments show that enterprises can enhance and accumulate their intelligence capabilities through federated learning, and jointly create an intelligent city model that provides high-quality intelligent services covering energy infrastructure security, residential community security and urban operation management. | Zengxiang Li, Zhaoxiang Hou, Hui Liu, Tongzhi Li, Chengyi Yang, Ying Wang, Chao Shi, Longfei Xie, Weishan Zhang, Liang Xu, Zelei Liu |  |
| 525 |  |  [Phoenix: A Federated Generative Diffusion Model](https://doi.org/10.1145/3589335.3651935) |  | 0 | Generative AI has made impressive strides in enabling users to create diverse and realistic visual content such as images, videos, and audio. However, training generative models on large centralized datasets can pose challenges in terms of data privacy, security, and accessibility. Federated learning (FL) is an approach that uses decentralized techniques to collaboratively train a shared deep learning model while retaining the training data on individual edge devices to preserve data privacy. This paper proposes a novel method for training a Denoising Diffusion Probabilistic Model (DDPM) across multiple data sources using FL techniques. Diffusion models, a newly emerging generative model, show promising results in achieving superior quality images than Generative Adversarial Networks (GANs). Our proposed method Phoenix is an unconditional diffusion model that leverages strategies to improve the data diversity of generated samples even when trained on data with statistical heterogeneity or Non-IID (Non-Independent and Identically Distributed) data. We demonstrate how our approach outperforms the default diffusion model in an FL setting. These results indicate that high-quality samples can be generated by maintaining data diversity, preserving privacy, and reducing communication between data sources, offering exciting new possibilities in the field of generative AI. | Fiona Victoria Stanley Jothiraj, Afra Mashhadi |  |
| 526 |  |  [LLM Driven Web Profile Extraction for Identical Names](https://doi.org/10.1145/3589335.3651946) |  | 0 | The number of individuals having identical names on the internet is increasing. Thus making the task of searching for a specific individual tedious. The user must vet through many profiles with identical names to get to the actual individual of interest. The online presence of an individual forms the profile of the individual. We need a solution that helps users by consolidating the profiles of such individuals by retrieving factual information available on the web and providing the same as a single result. We present a novel solution that retrieves web profiles belonging to those bearing identical Full Names through an end-to-end pipeline. Our solution involves information retrieval from the web (extraction), LLM-driven Named Entity Extraction (retrieval), and standardization of facts using Wikipedia, which returns profiles with fourteen multi-valued attributes. After that, profiles that correspond to the same real-world individuals are determined. We accomplish this by identifying similarities among profiles based on the extracted facts using a Prefix Tree inspired data structure (validation) and utilizing ChatGPT's contextual comprehension (revalidation). The system offers varied levels of strictness while consolidating these profiles, namely strict, relaxed, and loose matching. The novelty of our solution lies in the innovative use of GPT -- a highly powerful yet an unpredictable tool, for such a nuanced task. A study involving twenty participants, along with other results, found that one could effectively retrieve information for a specific individual. | Prateek Sancheti, Kamalakar Karlapalem, Kavita Vemuri |  |
| 527 |  |  [Contrastive Disentanglement for Authorship Attribution](https://doi.org/10.1145/3589335.3652501) |  | 0 | Authorship Attribution (AA) seeks to determine the authorship of texts by examining distinctive writing styles. Although current AA methods have shown promising results, they often underperform in scenarios with significant topic shifts. This limitation arises from their inability to effectively separate topical content from the author's stylistic elements. Furthermore, most studies have focused on individual-level AA, overlooking the potential of regional-level AA to uncover linguistic patterns influenced by cultural and geographical factors. To bridge these gaps, this paper introduces ContrastDistAA, a novel framework that leverages contrastive learning and mutual information maximization to disentangle content and stylistic features in latent representations for AA. Our extensive experiments demonstrate that ContrastDistAA surpasses existing state-of-the-art models in both individual and regional-level AA tasks. This breakthrough not only improves the accuracy of authorship attribution but also broadens its applicability to include regional linguistic analysis, making a substantial contribution to the field of computational linguistics. | Zhiqiang Hu, Thao Thanh Nguyen, Yujia Hu, ChiaYu Hung, Ming Shan Hee, ChunWei Seah, Roy KaWei Lee |  |
| 528 |  |  [Large Language Models for Graph Learning](https://doi.org/10.1145/3589335.3641300) |  | 0 | Graphs are widely applied to encode entities with various relations in web applications such as social media and recommender systems. Meanwhile, graph learning-based technologies, such as graph neural networks, are demanding to support the analysis, understanding, and usage of the data in graph structures. Recently, the boom of language foundation models, especially Large Language Models (LLMs), has advanced several main research areas in artificial intelligence, such as natural language processing, graph mining, and recommender systems. The synergy between LLMs and graph learning holds great potential to prompt the research in both areas. For example, LLMs can facilitate existing graph learning models by providing high-quality textual features for entities and edges, or enhancing the graph data with encoded knowledge and information. It may also innovate with novel problem formulations on graph-related tasks. Due to the research significance as well as the potential, the convergent area of LLMs and graph learning has attracted considerable research attention. Therefore, we propose to hold the workshop Large Language Models for Graph Learning at WWW'24, in order to provide a venue to gather researchers in academia and practitioners in the industry to present the recent progress on relevant topics and exchange their critical insights. | Yujuan Ding, Wenqi Fan, Xiao Huang, Qing Li |  |
| 529 |  |  [Multi-Granularity Tibetan Textual Adversarial Attack Method Based on Masked Language Model](https://doi.org/10.1145/3589335.3652503) |  | 0 | In social media, neural network models have been applied to hate speech detection, sentiment analysis, etc., but neural network models are susceptible to adversarial attacks. For instance, in a text classification task, the attacker elaborately introduces perturbations to the original texts that hardly alter the original semantics in order to trick the model into making different predictions. By studying textual adversarial attack methods, the robustness of language models can be evaluated and then improved. Currently, most of the research in this field focuses on English, and there is also a certain amount of research on Chinese. However, there is little research targeting Chinese minority languages. With the rapid development of artificial intelligence technology and the emergence of Chinese minority language models, textual adversarial attacks become a new challenge for the information processing of Chinese minority languages. In response to this situation, we propose a multi-granularity Tibetan textual adversarial attack method based on masked language models called TSTricker. We utilize the masked language models to generate candidate substitution syllables or words, adopt the scoring mechanism to determine the substitution order, and then conduct the attack method on several fine-tuned victim models. The experimental results show that TSTricker reduces the accuracy of the classification models by more than 28.70% and makes the classification models change the predictions of more than 90.60% of the samples, which has an evidently higher attack effect than the baseline method. | Xi Cao, Nuo Qun, Quzong Gesang, Yulei Zhu, Trashi Nyima |  |
| 530 |  |  [Decoding Memes: A Comprehensive Analysis of Late and Early Fusion Models for Explainable Meme Analysis](https://doi.org/10.1145/3589335.3652504) |  | 0 | Memes are important because they serve as conduits for expressing emotions, opinions, and social commentary online, providing valuable insight into public sentiment, trends, and social interactions. By combining textual and visual elements, multi-modal fusion techniques enhance meme analysis, enabling the classification of offensive and sentimental memes effectively. Early and late fusion methods effectively integrate multi-modal data but face limitations. Early fusion integrates features from different modalities before classification. Late fusion combines classification outcomes from each modality after individual classification and reclassifies the combined results. This paper compares early and late fusion models in meme analysis. It showcases their efficacy in extracting meme concepts and classifying meme reasoning. Pre-trained vision encoders, including ViT and VGG-16, and language encoders such as BERT, AlBERT, and DistilBERT, were employed to extract image and text features. These features were subsequently utilized for performing both early and late fusion techniques. This paper further compares the explainability of fusion models through SHAP analysis. In comprehensive experiments, various classifiers such as XGBoost and Random Forest, along with combinations of different vision and text features across multiple sentiment scenarios, showcased the superior effectiveness of late fusion over early fusion. | Faseela Abdullakutty, Usman Naseem |  |
| 531 |  |  [SigBart: Enhanced Pre-training via Salient Content Representation Learning for Social Media Summarization](https://doi.org/10.1145/3589335.3652505) |  | 0 | Our approach to automatically summarizing online mental health posts could help counselors by reducing their reading time, enabling quicker and more effective support for individuals seeking mental health assistance. Neural text summarization methods demonstrate promising performance owing to their strong pre-training procedure. Random token/span masking technique is often relied upon by existing pre-trained language models; an approach that overlooks the importance of content when learning word representations. In an attempt to rectify this, we propose using source and summary alignments as a saliency signal to enhance the pre-training strategy of language model for better representation learning of important content, paving the way for a positive impact on the model fine-tuning phase. Our experiments on a mental health-related dataset for user post summarization MentSum reveal improved performance, as evidenced by human evaluation metrics, surpassing the current state-of-the-art system. | Sajad Sotudeh, Nazli Goharian |  |
| 532 |  |  [An Investigation into the Feasibility of Performing Federated Learning on Social Linked Data Servers](https://doi.org/10.1145/3589335.3651950) |  | 0 | Federated Learning (FL) and the Social Linked Data (\textttSolid ~\footnotehttps://solidproject.org/ ) framework represent decentralized approaches to machine learning and web development, respectively, with a focus on preserving privacy. Federated learning enables the distributed training of machine learning models across datasets partitioned across multiple clients, whereas applications developed with the Solid approach store data inPersonal Online Data Stores (pods) under the control of individual users. This paper discusses the merits and challenges of executing Federated Learning on Solid pods and the readiness of the Solid server architecture to support this. We aim to detail these challenges, in addition to identifying avenues for further work to fully harness the benefits of Federated Learning in Solid environments, where users retain sovereignty over their data. | Nayil Arana, Mohamed Ragab, Thanassis Tiropanis |  |
| 533 |  |  [Detecting Financial Bots on the Ethereum Blockchain](https://doi.org/10.1145/3589335.3651959) |  | 0 | The integration of bots in Distributed Ledger Technologies (DLTs) fosters efficiency and automation. However, their use is also associated with predatory trading and market manipulation, and can pose threats to system integrity. It is therefore essential to understand the extent of bot deployment in DLTs; despite this, current detection systems are predominantly rule-based and lack flexibility. In this study, we present a novel approach that utilizes machine learning for the detection of financial bots on the Ethereum platform. First, we systematize existing scientific literature and collect anecdotal evidence to establish a taxonomy for financial bots, comprising 7 categories and 24 subcategories. Next, we create a ground-truth dataset consisting of 133 human and 137 bot addresses. Third, we employ both unsupervised and supervised machine learning algorithms to detect bots deployed on Ethereum. The highest-performing clustering algorithm is a Gaussian Mixture Model with an average cluster purity of 82.6 classification is a Random Forest with an accuracy of 83 learning-based detection mechanism contributes to understanding the Ethereum ecosystem dynamics by providing additional insights into the current bot landscape. | Thomas Niedermayer, Pietro Saggese, Bernhard Haslhofer |  |
| 534 |  |  [Measuring Arbitrage Losses and Profitability of AMM Liquidity](https://doi.org/10.1145/3589335.3651961) |  | 0 | This paper presents the results of a comprehensive empirical study of losses to arbitrageurs (following the formalization of loss-versus-rebalancing by [Milionis et al., 2022]) incurred by liquidity on automated market makers (AMMs). Through a systematic comparison between historical earnings from trading fees and losses to arbitrageurs, our findings indicate an insufficient compensation from fees for arbitrage losses across many of the largest AMM liquidity pools (on Uniswap). Remarkably, we identify a higher profitability among less capital-efficient Uniswap v2 pools compared to their Uniswap v3 counterparts. Moreover, we investigate a possible LVR mitigation by quantifying how arbitrage losses reduce with shorter block times. We observe notable variations in the manner of decline of arbitrage losses across different trading pairs. For instance, when comparing 100ms block times to Ethereum's current 12-second block times, the decrease in losses to arbitrageurs ranges between 20 | Robin Fritsch, Andrea Canidio |  |
| 535 |  |  [Anonymity Analysis of the Umbra Stealth Address Scheme on Ethereum](https://doi.org/10.1145/3589335.3651963) |  | 0 | Stealth addresses are a privacy-enhancing technology that provides recipient anonymity on blockchains. In this work, we investigate the recipient anonymity and unlinkability guarantees of Umbra, the most widely used implementation of the stealth address scheme on Ethereum, and its three off-chain scalability solutions, e.g., Arbitrum, Optimism, and Polygon. We define and evaluate four heuristics to uncover the real recipients of stealth payments. We find that for the majority of Umbra payments, it is straightforward to establish the recipient, hence nullifying the benefits of using Umbra. Specifically, we find the real recipient of $48.5\%$, $25.8\%$, $65.7\%$, and $52.6\%$ of all Umbra transactions on the Ethereum main net, Polygon, Arbitrum, and Optimism networks, respectively. Finally, we suggest easily implementable countermeasures to evade our deanonymization and linking attacks. | Alex Márk Kovács, István András Seres |  |
| 536 |  |  [Seamlessly Transferring Assets through Layer-0 Bridges: An Empirical Analysis of Stargate Bridge's Architecture and Dynamics](https://doi.org/10.1145/3589335.3651964) |  | 0 | The increasing number of distinct blockchains has led to a growing need for data exchange and asset transfer across various isolated blockchains. To address this, cross-chain bridges have emerged as a critical mechanism for enabling interoperability and facilitating data and asset exchange across diverse blockchains. Among these bridges, the Layer-0 bridge stands out as a scalability solution that enhances blockchain performance at the foundational layer of data transition, without altering the blockchain's structure. Stargate is a notable Layer-0 Lock-and-Unlock cross-chain bridge that supports transactions across various EVM-based blockchains, with the highest Total Value Locked (TVL) among cross-chain bridges of the same kind. While previous cross-chain research has primarily focused on Layer-2 bridges, this study specifically examines Stargate and analyzes its dynamics as well as potential vulnerabilities. We collect transaction data of Stargate on six blockchains including Ethereum, Polygon, Binance Smart Chain, Avalanche, Arbitrum and Optimism. Our findings reveal the transaction patterns and evidence of exploitations of Stargate by investigating its transaction dynamics over time. | Chuanshan Huang, Tao Yan, Claudio J. Tessone |  |
| 537 |  |  [Understanding, Leveraging, and Improving Large Language Models](https://doi.org/10.1145/3589335.3653009) |  | 0 | The emergence of Large Language Models (LLMs) has marked a substantial advancement in Natural Language Processing (NLP), contributing significantly to enhanced task performance both within and outside specific domains. However, amidst these achievements, three key questions remain unanswered: 1) The mechanism through which LLMs accomplish their tasks and their limitations, 2) Effectively harnessing the power of LLMs across diverse domains, and 3) Strategies for enhancing the performance of LLMs. This talk aims to delve into our research group's endeavors to address these pivotal questions. Firstly, I will outline our approach, which involves utilizing ontology-guided prompt perturbations to unravel the primary limitations of LLMs in solving mathematical problems. Moving on to the second question, we will explore the utilization of synthetic data generated by LLMs to bolster challenging downstream tasks, particularly focusing on structured prediction where LLMs face persistent challenges. I will elaborate on our initiatives aimed at improving LLMs by incorporating highly effective retrieval strategies, specifically addressing the prevalent challenge of hallucinations that often plagues contemporary LLMs. Finally, I will present a technique on LLM realignment to restore safety lost during fine-tuning. | Soujanya Poria |  |
| 538 |  |  [Love-Hate Dataset: A Multi-Modal Multi-Platform Dataset Depicting Emotions in the 2023 Israel-Hamas War](https://doi.org/10.1145/3589335.3651966) |  | 0 | War brings about strong feelings of hate, and showcases the love of humanity. During war, social media is utilized for citizen journalism, supply organization and activism, but also for people to express their emotions. In this paper, we present a multi-modal multi-platform dataset that depicts the expression of love and hate towards both sides of the Israel-Hamas War. This dataset presents posts in English from Facebook and Instagram that contain the terms "love" or "hate" in the context of the war during 7 October 2023 (onset of war) to 31 December 2023. We find that over time, the number of posts on the war decreased, suggesting interest in the war has waned; posts about Love reference religion while posts about Hate references hostility; and emojis in Love posts represent hearts, peace and listening while emojis in Hate posts represent being watched, sadness and warning. Finally, we generated Instagram posts with GPT4-V using our dataset as a reference, and the model returned posts of generic love messages with art-form images. We hope our dataset is useful to researchers studying multi-modal and multi-platform information and emotions on social media during a war. | Lynnette Hui Xian Ng, Adrian Xuan Wei Lim, Roy KaWei Lee |  |
| 539 |  |  [Textual Context guided Vision Transformer with Rotated Multi-Head Attention for Sentiment Analysis](https://doi.org/10.1145/3589335.3651968) |  | 0 | Social media multimodal sentiment analysis has proliferated the research attention of the research community, as it opens up various paradigms for social issues such as cyberbullying, hate speech, healthcare, politics, business analysis and many more. At the same time, it is an open problem to learn the intrinsic representation of multiple modalities in order to identify correlated patterns. In the proposed work a sentiment analysis framework is presented that defines Textual Context guided Vision Transformer with Rotated Multi-Head Attention, in order to exploit correlation between image-text pair and mine rich discriminatory features for multimodal sentiment analysis. A novel Rotated-Multi-head attention mechanism is defined that translates the visual or text embeddings in distinct feature space resulting in Adaptively Rotated Refined Embedding (ARREmb). To exhibit the performance of the proposed work, extensive experiments are carried out on three publicly available datasets-BG, Twitter and MVSA-single dataset, in terms of Precision, Recall, F1-score and accuracy. The experiments support superior performance of the proposed approach by laying out comparison with SOTA followed by ablation study. | Chhavi Dhiman, Gaurav Kumar |  |
| 540 |  |  [A Novel Dual-Pipeline based Attention Mechanism for Multimodal Social Sentiment Analysis](https://doi.org/10.1145/3589335.3651967) |  | 0 | Traditionally, sentiment analysis methods rely solely on text or image data. However, most user-generated social media content includes both textual and image content. In this study, we propose a novel Dual-Pipeline based Attentional method that uses different modalities of data, including text and images, to analyse and interpret emotions and sentiments expressed in tweets. Our proposed method simultaneously extracts meaningful local and global contextual features from multiple modalities. Local fusion layers within each pipeline combine modality-specific features using an attention mechanism to enrich the joint multimodal representation. A global fusion layer consolidates the collective sentiment representation by seamlessly intermixing the outputs of both pipelines. We evaluate our proposed method using performance metrics such as accuracy and F1-score. Through extensive experimentation on the MVSA dataset, our method demonstrates superior performance compared to state-of-the-art techniques in identifying the sentiment conveyed in social media data. | Ali Braytee, Andy ShuehChih Yang, Ali Anaissi, Kunal Chaturvedi, Mukesh Prasad |  |
| 541 |  |  [Contextualizing Internet Memes Across Social Media Platforms](https://doi.org/10.1145/3589335.3651970) |  | 0 | Internet memes have emerged as a novel format for communication and expressing ideas on the web. Their fluidity and creative nature are reflected in their widespread use, often across platforms and occasionally for unethical or harmful purposes. While computational work has already analyzed their high-level virality over time and developed specialized classifiers for hate speech detection, there have been no efforts to date that aim to holistically track, identify, and map internet memes posted on social media. To bridge this gap, we investigate whether internet memes across social media platforms can be contextualized by using a semantic repository of knowledge, namely, a knowledge graph. We collect thousands of potential internet meme posts from two social media platforms, namely Reddit and Discord, and perform an extract-transform-load procedure to create a data lake with candidate meme posts. By using vision transformer-based similarity, we match these candidates against the memes cataloged in a recently released knowledge graph of internet memes, IMKG. We provide evidence that memes published online can be identified by mapping them to IMKG. We leverage this grounding to study the prevalence of memes on different platforms, discover popular memes, and select common meme channels and subreddits. Finally, we illustrate how the grounding can enable users to get context about memes on social media thanks to their link to the knowledge graph. | Saurav Joshi, Filip Ilievski, Luca Luceri |  |
| 542 |  |  [Unraveling the Tangle of Disinformation: A Multimodal Approach for Fake News Identification on Social Media](https://doi.org/10.1145/3589335.3651972) |  | 0 | The growth of interactive and multimedia content on the Internet has made it an essential news source for people worldwide. Social media is a platform for sharing information and facilitates the spread of fake news. The dissemination of disinformation on social media has a significant impact on society. Conventional methods used in the identification of fake news often struggle to analyze textual, visual, and combined aspects of news shared on social media. Therefore, we propose the Multimodal Approach for Fake News Identification (MuAFaNI), which uses a combined representation of text and images to assess news authenticity as fake or real. MuAFaNI uses the RoBERTa language model for text analysis and ResNet-50 for image analysis. Experiments on two prominent social media datasets, Twitter and Weibo, showed that MuAFaNI performed better than state-of-the-art fake news techniques in terms of accuracy, precision, recall and F1 score. | Junaid Rashid, Jungeun Kim, Anum Masood |  |
| 543 |  |  [RUHate-MM: Identification of Hate Speech and Targets using Multimodal Data from Russia-Ukraine Crisis](https://doi.org/10.1145/3589335.3651973) |  | 0 | During the conflict between Ukraine and Russia, hate speech targeted toward specific groups was widespread on different social media platforms. With most social platforms allowing multimodal content, the use of multimodal content to express hate speech is widespread on the Internet. Although there has been considerable research in detecting hate speech within unimodal content, the investigation into multimodal content remains insufficient. The limited availability of annotated multimodal datasets further restricts our ability to explore new methods to interpret and identify hate speech and its targets. The availability of annotated datasets for hate speech detection during political events, such as invasions, are even limited. To fill this gap, we introduce a comprehensive multimodal dataset consisting of 20,675 posts related to the Russia-Ukraine crisis, which were manually annotated as either 'Hate Speech' or 'No Hate Speech'. Additionally, we categorize the hate speech data into three targets: 'Individual', 'Organization', and 'Community'. Our benchmarked evaluations show that there is still room for improvement in accurately identifying hate speech and its targets. We hope that the availability of this dataset and the evaluations performed on it will encourage the development of new methods for identifying hate speech and its targets during political events like invasions and wars. The dataset and resources are made available at https://github.com/Farhan-jafri/Russia-Ukraine. | Surendrabikram Thapa, Farhan Ahmad Jafri, Kritesh Rauniyar, Mehwish Nasim, Usman Naseem |  |
| 544 |  |  [Unveiling Misogyny Memes: A Multimodal Analysis of Modality Effects on Identification](https://doi.org/10.1145/3589335.3651974) |  | 0 | In today's digital era, memes have become a popular means of communication that often reflect societal attitudes as well as prejudices. Misogyny memes are a form of memes that explicitly discriminate against women in various aspects, such as shaming or stereotyping. This research aims to identify misogynous memes through deep learning multimodal analysis and determine which modality, text or image, plays a more significant role in fairness considerations. To achieve this, we utilized the dataset GOAT-benchmarks, which comprises over 6,000 diverse memes covering topics like implicit hate speech, sexism, and cyberbullying. Furthermore, we evaluated the fairness of these models by assessing their performance across different demographic groups. Our findings revealed that while both text and image modalities contribute to identifying misogynous memes, text plays a significant role in misogyny identification, while image contributes further in terms of fairness. This study emphasizes the importance of multimodal analysis in recognizing and mitigating biases in online content. Disclaimer: This paper contains content that may be disturbing to some readers. | Shijing Chen, Usman Naseem, Imran Razzak, Flora D. Salim |  |
| 545 |  |  [Ensemble Pretrained Models for Multimodal Sentiment Analysis using Textual and Video Data Fusion](https://doi.org/10.1145/3589335.3651971) |  | 0 | We introduce an ensemble model approach for multimodal sentiment analysis, focusing on the fusion of textual and video data to enhance the accuracy and depth of emotion interpretation. By integrating three foundational models-IFFSA, BFSA, and TBJE-using advanced ensemble techniques, we achieve a significant improvement in sentiment analysis performance across diverse datasets, including MOSI and MOSEI. Specifically, we propose two novel models-IFFSA and BFSA, which utilise the large language models BERT and GPT-2 to extract the features from text modality and ResNet and VGG for video modality. Our work uniquely contributes to the field by demonstrating the synergistic potential of combining different modal analytical strengths, thereby addressing the intricate challenge of nuanced emotion detection in multimodal contexts. Through comprehensive experiments and an extensive ablation study, we not only validate the superior performance of our ensemble model against current state-of-the-art benchmarks but also reveal critical insights into the model's capability to discern complex emotional states. Our findings underscore the strategic advantage of ensemble methods in multimodal sentiment analysis and set a new precedent for future research in effectively integrating multimodal data sources. | Zhicheng Liu, Ali Braytee, Ali Anaissi, Guifu Zhang, Lingyun Qin, Junaid Akram |  |
| 546 |  |  [AI Deepfakes on the Web: The 'Wicked' Challenges for AI Ethics, Law and Technology](https://doi.org/10.1145/3589334.3649116) |  | 0 | Advances in generative AI and the increasingly easy availability of tools for creating text, code, audio, and images have impacted almost all industry sectors, promising new efficiencies and changing work patterns. The darker side of this same technology is the problematic case of deepfakes created by AI and spread online to humiliate, manipulate, trick, or defraud ordinary individuals and public figures. Transparency, fairness, and beneficence are vital values of responsible and ethical AI. All of these values would preclude harmful uses of AI deep fakes. However, harmful deepfakes are usually the work of fraudsters with little regard for ethics and beyond the reach of the law. So, who should be responsible? Arguably, principles of responsible AI require tech companies and digital platforms to take responsibility for reducing harmful uses of deepfakes. These entities are gatekeepers to the creation and distribution of deepfakes. Therefore, they are ethically obligated to respond to the foreseeable consequential harms arising from generative AI. Increasingly, this is the response of lawmakers. Gatekeeper responsibility envisages that tech producers and platforms will proactively invest in technical solutions to harmful deepfakes, such as watermarking, finetuning, red teaming or automated content moderation, and proactive take-down responses. This response is compelling and might seem straightforward. As always, the details are more complex. The efficiency of the proposed technical responses is still emerging. They raise as yet unaddressed implications for smaller providers and the relations between tech companies and digital platforms. Moreover, even beginning to respond to online deepfakes requires social policy decisions that assess and weigh incommensurable considerations, including retaining trust on the Web, keeping vulnerable groups safe, preserving free speech and creativity, and not stifling the development of potentially beneficial technology. This presentation addresses these problematic choices in responding to the 'wicked' challenge of AI deepfakes on the Web. It proposes a networked response to the problem, embracing multiple relevant actors and influences. | Jeannie Marie Paterson |  |
| 547 |  |  [Challenges Toward AGI and Its Impact to the Web](https://doi.org/10.1145/3589334.3649113) |  | 0 | Large language models have substantially advanced the state of the art in various AI tasks, such as natural language understanding and text generation, and image processing, and multimodal modeling. In this talk, we will first introduce the development of AI in the past decades, in particular from the angle of China. We will also talk about the opportunities, challenges, and risks of AGI in the future, and its impact on the Web. In the second part of the talk, we will use ChatGLM, an alternative but open sourced model to ChatGPT, as an example to explain our understandings and insights derived during the implementation of the model. | Bo Zhang, Jie Tang |  |
| 548 |  |  [Budget-Constrained Auctions with Unassured Priors: Strategic Equivalence and Structural Properties](https://doi.org/10.1145/3589334.3645344) |  | 0 | In today's online advertising markets, it is common for advertisers to set long-term budgets. Correspondingly, advertising platforms adopt budget control methods to ensure that advertisers' payments lie within their budgets. Most budget control methods rely on the value distributions of advertisers. However, due to the complex advertising landscape and potential privacy concerns, the platform hardly learns advertisers' true priors. Thus, it is crucial to understand how budget control auction mechanisms perform under unassured priors. This work answers this problem from multiple aspects. We consider the unassured prior game among the seller and all buyers induced by different mechanisms in the stochastic model. We restrict the parameterized mechanisms to satisfy the budget-extracting condition, which maximizes the seller's revenue by extracting buyers' budgets as effectively as possible. Our main result shows that the Bayesian revenue-optimal mechanism and the budget-extracting bid-discount first-price mechanism yield the same set of Nash equilibrium outcomes in the unassured prior game. This implies that simple mechanisms can be as robust as the optimal mechanism under unassured priors in the budget-constrained setting. In the symmetric case, we further show that all these five (budget-extracting) mechanisms share the same set of possible outcomes. We further dig into the structural properties of these mechanisms. We characterize sufficient and necessary conditions on the budget-extracting parameter tuple for bid-discount/pacing first-price auctions. Meanwhile, when buyers do not take strategic behaviors, we exploit the dominance relationships of these mechanisms by revealing their intrinsic structures. | Zhaohua Chen, Mingwei Yang, Chang Wang, Jicheng Li, Zheng Cai, Yukun Ren, Zhihua Zhu, Xiaotie Deng |  |
| 549 |  |  [Efficiency of the Generalized Second-Price Auction for Value Maximizers](https://doi.org/10.1145/3589334.3645360) |  | 0 | We study the price of anarchy of the generalized second-price auction where bidders are value maximizers (i.e., autobidders). We show that in general the price of anarchy can be as bad as $0$. For comparison, the price of anarchy of running VCG is $1/2$ in the autobidding world. We further show a fined-grained price of anarchy with respect to the discount factors (i.e., the ratios of click probabilities between lower slots and the highest slot in each auction) in the generalized second-price auction, which highlights the qualitative relation between the smoothness of the discount factors and the efficiency of the generalized second-price auction. | Yuan Deng, Mohammad Mahdian, Jieming Mao, Vahab Mirrokni, Hanrui Zhang, Song Zuo |  |
| 550 |  |  [Data Exchange Markets via Utility Balancing](https://doi.org/10.1145/3589334.3645364) |  | 0 | This paper explores the design of a balanced data-sharing marketplace for entities with heterogeneous datasets and machine learning models that they seek to refine using data from other agents. The goal of the marketplace is to encourage participation for data sharing in the presence of such heterogeneity. Our market design approach for data sharing focuses on interim utility balance, where participants contribute and receive equitable utility from refinement of their models. We present such a market model for which we study computational complexity, solution existence, and approximation algorithms for welfare maximization and core stability. We finally support our theoretical insights with simulations on a mean estimation task inspired by road traffic delay estimation. | Aditya Bhaskara, Sreenivas Gollapudi, Sungjin Im, Kostas Kollias, Kamesh Munagala, Govind S. Sankar |  |
| 551 |  |  [Fine-Tuning Games: Bargaining and Adaptation for General-Purpose Models](https://doi.org/10.1145/3589334.3645366) |  | 0 | Major advances in Machine Learning (ML) and Artificial Intelligence (AI) increasingly take the form of developing and releasing general-purpose models. These models are designed to be adapted by other businesses and agencies to perform a particular, domain-specific function. This process has become known as adaptation or fine-tuning. This paper offers a model of the fine-tuning process where a Generalist brings the technological product (here an ML model) to a certain level of performance, and one or more Domain-specialist(s) adapts it for use in a particular domain. Both entities are profit-seeking and incur costs when they invest in the technology, and they must reach a bargaining agreement on how to share the revenue for the technology to reach the market. For a relatively general class of cost and revenue functions, we characterize the conditions under which the fine-tuning game yields a profit-sharing solution. We observe that any potential domain-specialization will either contribute, free-ride, or abstain in their uptake of the technology, and we provide conditions yielding these different strategies. We show how methods based on bargaining solutions and sub-game perfect equilibria provide insights into the strategic behavior of firms in these types of interactions, and we find that profit-sharing can still arise even when one firm has significantly higher costs than another. We also provide methods for identifying Pareto-optimal bargaining arrangements for a general set of utility functions. | Benjamin Laufer, Jon M. Kleinberg, Hoda Heidari |  |
| 552 |  |  [Robust Decision Aggregation with Second-order Information](https://doi.org/10.1145/3589334.3645384) |  | 0 | We consider a decision aggregation problem with two experts who each make a binary recommendation after observing a private signal about an unknown binary world state. An agent, who does not know the joint information structure between signals and states, sees the experts' recommendations and aims to match the action with the true state. Under the scenario, we study whether supplemented additionally with second-order information (each expert's forecast on the other's recommendation) could enable a better aggregation. We adopt a minimax regret framework to evaluate the aggregator's performance, by comparing it to an omniscient benchmark that knows the joint information structure. With general information structures, we show that second-order information provides no benefit. No aggregator can improve over a trivial aggregator, which always follows the first expert's recommendation. However, positive results emerge when we assume experts' signals are conditionally independent given the world state. When the aggregator is deterministic, we present a robust aggregator that leverages second-order information, which can significantly outperform counterparts without it. Second, when two experts are homogeneous, by adding a non-degenerate assumption on the signals, we demonstrate that random aggregators using second-order information can surpass optimal ones without it. In the remaining settings, the second-order information is not beneficial. We also extend the above results to the setting when the aggregator's utility function is more general. | Yuqi Pan, Zhaohua Chen, Yuqing Kong |  |
| 553 |  |  [Identifying Risky Vendors in Cryptocurrency P2P Marketplaces](https://doi.org/10.1145/3589334.3645475) |  | 0 | Peer-to-Peer (P2P) cryptocurrency exchanges are two-sided marketplaces, similar to eBay, where individuals can offer to sell cryptocurrencies in exchange for payment. Due to disintermediation, these marketplaces trade off increased privacy for higher risk (e.g., scams/fraud). Although these marketplaces use feedback systems to encourage healthier transactions, anecdotal evidence suggests that feedback often fails to capture vendor-associated risks. This work documents the online safety of cryptocurrency P2P marketplaces, identifies underlying issues in feedback-based reputation systems, and proposes improved mechanisms for predicting/monitoring risky accounts. We collect data from two cryptocurrency marketplaces, Paxful and LocalCoinSwap (LCS) for 12 months (06/2022--06/2023). The data includes over 396,000 listings, 67,000 vendors, and 4.7 million feedback for Paxful; and about 52,000 listings, 14,000 users, and 146,000 feedback for LCS.First, we show that the current feedback system does not sufficiently convey enough information about risky vendors, and is susceptible to reputation manipulation through user collusion and automation. Second, combining various publicly available information, we build machine learning models to predict account suspension, and achieve a 0.86 F1-score and 0.93 AUC for Paxful. Third, while our models appear to have limited transferability across markets, we identify which features most help account suspension across platforms. Finally, we perform a month-long online evaluation to show that our models are significantly more successful than mere feedback-based reputation schemes at predicting which users will be suspended in the future. | Taro Tsuchiya, Alejandro Cuevas, Nicolas Christin |  |
| 554 |  |  [Prior-Free Mechanism with Welfare Guarantees](https://doi.org/10.1145/3589334.3645500) |  | 0 | We consider the problem of designing prior-free revenue-maximizing mechanisms for allocating items to n buyers when the mechanism is additionally provided with an estimate for the optimal welfare (which is guaranteed to be correct to within a multiplicative factor of 1/α). In the digital goods setting (where we can allocate items to an arbitrary subset of the buyers), we demonstrate a mechanism that achieves revenue that is O(log n/α)-competitive with the optimal welfare. In the public goods setting (where we either must allocate the item to all buyers or to no buyers), we demonstrate a mechanism which is O(n log 1/α) competitive. In both settings, we show the dependence on α and n is tight. Finally, we discuss generalizations to broader classes of allocation constraints. | Guru Guruganesh, Jon Schneider, Joshua R. Wang |  |
| 555 |  |  [Mechanism Design for Large Language Models](https://doi.org/10.1145/3589334.3645511) |  | 0 | We investigate auction mechanisms to support the emerging format of AI-generated content. We in particular study how to aggregate several LLMs in an incentive compatible manner. In this problem, the preferences of each agent over stochastically generated contents are described/encoded as an LLM. A key motivation is to design an auction format for AI-generated ad creatives to combine inputs from different advertisers. We argue that this problem, while generally falling under the umbrella of mechanism design, has several unique features. We propose a general formalism -- the token auction model -- for studying this problem. A key feature of this model is that it acts on a token-by-token basis and lets LLM agents influence generated contents through single dimensional bids. We first explore a robust auction design approach, in which all we assume is that agent preferences entail partial orders over outcome distributions. We formulate two natural incentive properties, and show that these are equivalent to a monotonicity condition on distribution aggregation. We also show that for such aggregation functions, it is possible to design a second-price auction, despite the absence of bidder valuation functions. We then move to designing concrete aggregation functions by focusing on specific valuation forms based on KL-divergence, a commonly used loss function in LLM. The welfare-maximizing aggregation rules turn out to be the weighted (log-space) convex combination of the target distributions from all participants. We conclude with experimental results in support of the token auction formulation. | Paul Dütting, Vahab Mirrokni, Renato Paes Leme, Haifeng Xu, Song Zuo |  |
| 556 |  |  [Core-Competitiveness in Partially Observable Networked Market](https://doi.org/10.1145/3589334.3645555) |  | 0 | In auction theory, a core is a stable outcome where no subgroup of participants can achieve better results for themselves. Core-competitive auctions aim to generate revenue that is achievable in a core. They are particularly important because they not only generate optimized revenue for the seller, but also provide an efficient and stable environment for participants. We generalize the design of core-competitive auctions to encompass partially observable networked markets (PONM). Unlike traditional auctions, which often deal with scenarios of limited trading activity, our approach to core-competitive auctions for PONM captures the nature of real-world transaction markets, which is a large linking world for the economic entities and commodities circulate among the entities in the market. Our generalizing the auction market to PONM can much improve the liquidity of the auction, and is especially meaningful for the web economics. Specifically, we quantify the upper and lower bounds of the minimum core revenue in PONM, and further prove that there does not exist any truthful auction for PONM which is efficient and core-competitive. Governed by this impossible result, we identify the criteria that the allocation rule for PONM should meet. Based on these criteria, we propose a new class of auction mechanisms for PONM that is individually rational, incentive-compatible, and core-competitive. | Bin Li, Dong Hao |  |
| 557 |  |  [Unveiling the Paradox of NFT Prosperity](https://doi.org/10.1145/3589334.3645566) |  | 0 | Unlike fungible tokens (e.g., cryptocurrency), a Non-Fungible Token (NFT) is unique and indivisible. As such, they can be used to authenticate ownership of digital assets (e.g., a photo) in a decentralized fashion. Given that NFTs have generated significant media attention since 2021, we perform a large-scale measurement study of the NFT ecosystem. We collect over 242M transfer logs and over 97M marketplace transactions until Aug 1st, 2023, by far the largest NFT dataset, to the best of our knowledge. We characterize the on-chain behavior of NFTs and their trading across five major marketplaces. We find that, although the NFT ecosystem is growing rapidly, it is driven by a relatively small set of dominant centralized players, with suspicious trade activities, e.g., over 23% of the monetary volume is generated by malicious wash trading and the ecosystem has experienced over 157K cases of NFT arbitrage, with a total sum of over \25M profit. Our observations motivate the need for more research efforts in the NFT security analysis. | Jintao Huang, Pengcheng Xia, Jiefeng Li, Kai Ma, Gareth Tyson, Xiapu Luo, Lei Wu, Yajin Zhou, Wei Cai, Haoyu Wang |  |
| 558 |  |  [Fair Surveillance Assignment Problem](https://doi.org/10.1145/3589334.3645613) |  | 0 | Monitoring a specific set of locations serves multiple purposes, such as infrastructure inspection and safety surveillance. We study a generalization of the surveillance problem, where the monitoring area, represented by a graph, is divided and assigned to a set of agents with personalized cost functions. In this paper, each agent's patrolling cost towards receiving a subgraph is measured by the weight of the minimum vertex cover therein, and our objective is to design algorithms to compute fair assignments of the surveillance tasks. The fairness is assessed using maximin share (MMS) fairness proposed by Budish [J. Political Econ., 2011]. Our main result is an algorithm which ensures a 4.562-approximate MMS allocation for any number of agents with arbitrary vertex weights. We then prove that no algorithm can be better than 2-approximate MMS. For scenarios involving no more than four agents, we improve the approximation ratio to 2, which is thus the optimal achievable ratio. | Fangxiao Wang, Bo Li |  |
| 559 |  |  [Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools](https://doi.org/10.1145/3589334.3645615) |  | 0 | Portfolio management (PM) is a fundamental financial trading task, which explores the optimal periodical reallocation of capitals into different stocks to pursue long-term profits. Reinforcement learning (RL) has recently shown its potential to train profitable agents for PM through interacting with financial markets. However, existing work mostly focuses on fixed stock pools, which is inconsistent with investors' practical demand. Specifically, the target stock pool of different investors varies dramatically due to their discrepancy on market states and individual investors may temporally adjust stocks they desire to trade (e.g., adding one popular stocks), which lead to customizable stock pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny change of the stock pool, which leads to high computational cost and unstable performance. To tackle this challenge, we propose EarnMore, a rEinforcement leARNing framework with Maskable stOck REpresentation to handle PM with CSPs through one-shot training in a global stock pool (GSP). Specifically, we first introduce a mechanism to mask out the representation of the stocks outside the target pool. Second, we learn meaningful stock representations through a self-supervised masking and reconstruction process. Third, a re-weighting mechanism is designed to make the portfolio concentrate on favorable stocks and neglect the stocks outside the target pool. Through extensive experiments on 8 subset stock pools of the US stock market, we demonstrate that EarnMore significantly outperforms 14 state-of-the-art baselines in terms of 6 popular financial metrics with over 40 | Wentao Zhang, Yilei Zhao, Shuo Sun, Jie Ying, Yonggang Xie, Zitao Song, Xinrun Wang, Bo An |  |
| 560 |  |  [Exit Ripple Effects: Understanding the Disruption of Socialization Networks Following Employee Departures](https://doi.org/10.1145/3589334.3645634) |  | 0 | Amidst growing uncertainty and frequent restructurings, the impacts of employee exits are becoming one of the central concerns for organizations. Using rich communication data from a large holding company, we examine the effects of employee departures on socialization networks among the remaining coworkers. Specifically, we investigate how network metrics change among people who historically interacted with departing employees. We find evidence of “breakdown" in communication among the remaining coworkers, who tend to become less connected with fewer interactions after their coworkers' departure. This effect appears to be moderated by both external factors, such as periods of high organizational stress, and internal factors, such as the characteristics of the departing employee. At the external level, periods of high stress correspond to greater communication breakdown; at the internal level, however, we find patterns suggesting individuals may end up better positioned in their networks after a network neighbor's departure. Overall, our study provides critical insights into managing workforce changes and preserving communication dynamics in the face of employee exits. | David Gamba, Yulin Yu, Yuan Yuan, Grant Schoenebeck, Daniel M. Romero |  |
| 561 |  |  [APT-Pipe: A Prompt-Tuning Tool for Social Data Annotation using ChatGPT](https://doi.org/10.1145/3589334.3645642) |  | 0 | Recent research has highlighted the potential of LLM applications, like ChatGPT, for performing label annotation on social computing text. However, it is already well known that performance hinges on the quality of the input prompts. To address this, there has been a flurry of research into prompt tuning – techniques and guidelines that attempt to improve the quality of prompts. Yet these largely rely on manual effort and prior knowledge of the dataset being annotated. To address this limitation, we propose APT-Pipe, an automated prompt-tuning pipeline. APT-Pipe aims to automatically tune prompts to enhance ChatGPT's text classification performance on any given dataset. We implement APT-Pipe and test it across twelve distinct text classification datasets. We find that prompts tuned by APT-Pipe help ChatGPT achieve higher weighted F1-score on nine out of twelve experimented datasets, with an improvement of 7.01 a framework by showing how it can be extended to support additional tuning mechanisms. | Yiming Zhu, Zhizhuo Yin, Gareth Tyson, Ehsan ul Haq, LikHang Lee, Pan Hui |  |
| 562 |  |  [Spot Check Equivalence: An Interpretable Metric for Information Elicitation Mechanisms](https://doi.org/10.1145/3589334.3645679) |  | 0 | Because high-quality data is like oxygen for AI systems, effectively eliciting information from crowdsourcing workers has become a first-order problem for developing high-performance machine learning algorithms. Two prevalent paradigms, spot-checking and peer prediction, enable the design of mechanisms to evaluate and incentivize high-quality data from human labelers. So far, at least three metrics have been proposed to compare the performances of these techniques [33, 8, 3]. However, different metrics lead to divergent and even contradictory results in various contexts. In this paper, we harmonize these divergent stories, showing that two of these metrics are actually the same within certain contexts and explain the divergence of the third. Moreover, we unify these different contexts by introducing Spot Check Equivalence, which offers an interpretable metric for the effectiveness of a peer prediction mechanism. Finally, we present two approaches to compute spot check equivalence in various contexts, where simulation results verify the effectiveness of our proposed metric. | Shengwei Xu, Yichi Zhang, Paul Resnick, Grant Schoenebeck |  |
| 563 |  |  [Optimal Engagement-Diversity Tradeoffs in Social Media](https://doi.org/10.1145/3589334.3645713) |  | 0 | Social media platforms are known to optimize user engagement with the help of algorithms. It is widely understood that this practice gives rise to echo chambers\emdash users are mainly exposed to opinions that are similar to their own. In this paper, we ask whether echo chambers are an inevitable result of high engagement; we address this question in a novel model. Our main theoretical results establish bounds on the maximum engagement achievable under a diversity constraint, for suitable measures of engagement and diversity; we can therefore quantify the worst-case tradeoff between these two objectives. Our empirical results, based on real data from Twitter, chart the Pareto frontier of the engagement-diversity tradeoff. | Fabian Baumann, Daniel Halpern, Ariel D. Procaccia, Iyad Rahwan, Itai Shapira, Manuel Wüthrich |  |
| 564 |  |  [MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning](https://doi.org/10.1145/3589334.3645322) |  | 0 | In this work, we investigate the problem of out-of-distribution (OOD) generalization for unsupervised learning methods on graph data. This scenario is particularly challenging because graph neural networks (GNNs) have been shown to be sensitive to distributional shifts, even when labels are available. To address this challenge, we propose a \underline{M}odel-\underline{A}gnostic \underline{R}ecipe for \underline{I}mproving \underline{O}OD generalizability of unsupervised graph contrastive learning methods, which we refer to as MARIO. MARIO introduces two principles aimed at developing distributional-shift-robust graph contrastive methods to overcome the limitations of existing frameworks: (i) Information Bottleneck (IB) principle for achieving generalizable representations and (ii) Invariant principle that incorporates adversarial data augmentation to obtain invariant representations. To the best of our knowledge, this is the first work that investigates the OOD generalization problem of graph contrastive learning, with a specific focus on node-level tasks. Through extensive experiments, we demonstrate that our method achieves state-of-the-art performance on the OOD test set, while maintaining comparable performance on the in-distribution test set when compared to existing approaches. The source code for our method can be found at: https://github.com/ZhuYun97/MARIO | Yun Zhu, Haizhou Shi, Zhenshuo Zhang, Siliang Tang |  |
| 565 |  |  [Cooperative Classification and Rationalization for Graph Generalization](https://doi.org/10.1145/3589334.3645332) |  | 0 | Graph Neural Networks (GNNs) have achieved impressive results in graph classification tasks, but they struggle to generalize effectively when faced with out-of-distribution (OOD) data. Several approaches have been proposed to address this problem. Among them, one solution is to diversify training distributions in vanilla classification by modifying the data environment, yet accessing the environment information is complex. Besides, another promising approach involves rationalization, extracting invariant rationales for predictions. However, extracting rationales is difficult due to limited learning signals, resulting in less accurate rationales and diminished predictions. To address these challenges, in this paper, we propose a Cooperative Classification and Rationalization (C2R) method, consisting of the classification and the rationalization module. Specifically, we first assume that multiple environments are available in the classification module. Then, we introduce diverse training distributions using an environment-conditional generative network, enabling robust graph representations. Meanwhile, the rationalization module employs a separator to identify relevant rationale subgraphs while the remaining non-rationale subgraphs are de-correlated with labels. Next, we align graph representations from the classification module with rationale subgraph representations using the knowledge distillation methods, enhancing the learning signal for rationales. Finally, we infer multiple environments by gathering non-rationale representations and incorporate them into the classification module for cooperative learning. Extensive experimental results on both benchmarks and synthetic datasets demonstrate the effectiveness of C2R. Code is available at https://github.com/yuelinan/Codes-of-C2R. | Linan Yue, Qi Liu, Ye Liu, Weibo Gao, Fangzhou Yao, Wenfeng Li |  |
| 566 |  |  [Cost-effective Data Labelling for Graph Neural Networks](https://doi.org/10.1145/3589334.3645339) |  | 0 | Active learning (AL), that aims to label limited data samples to effectively train the model, stands as a very cost-effective data labelling strategy in machine learning. Given the state-of-the-art performance GNNs have achieved in graph-based tasks, it is critical to design proper AL methods for graph neural networks (GNNs). However, existing GNN-based AL methods require considerable supervised information to guide the AL process, such as the GNN model to use, and initially labelled nodes and labels of newly selected nodes. Such dependency on supervised information limits both flexibility and scalabilty. In this paper, we propose an unsupervised, scalable and flexible AL method - it incurs low memory footprints and time cost, is flexible to the choice of underlying GNNs, and operates without requiring GNN-model-specific knowledge or labels of selected nodes. Specifically, we leverage the commonality of existing GNNs to reformulate the unsupervised AL problem as the Aggregation Involvement Maximization (AIM) problem. The objective of AIM is to maximize the involvement or participation of all nodes during the feature aggregation process of GNNs for nodes to be labelled. In this way, the aggregated features of labelled nodes can be diversified to a large extent, thereby benefiting the training of feature transformation matrices which are major trainable components in GNNs. We prove that the AIM problem is NP-hard and propose an efficient solution with theoretical guarantees. Extensive experiments on public datasets demonstrate the effectiveness, scalability and flexibility of our method. Our study is highly relevant to the track "Graph Algorithms and Modeling for the Web" since we focus one of the major listed topics "Graph Embedding and GNNs for the Web" and AL for GNNs, as an important research problem, is faced by aforementioned challenges to be tackled in this paper. | Shixun Huang, Ge Lee, Zhifeng Bao, Shirui Pan |  |
| 567 |  |  [Masked Graph Autoencoder with Non-discrete Bandwidths](https://doi.org/10.1145/3589334.3645370) |  | 0 | Masked graph autoencoders have emerged as a powerful graph self-supervised learning method that has yet to be fully explored. In this paper, we unveil that the existing discrete edge masking and binary link reconstruction strategies are insufficient to learn topologically informative representations, from the perspective of message propagation on graph neural networks. These limitations include blocking message flows, vulnerability to over-smoothness, and suboptimal neighborhood discriminability. Inspired by these understandings, we explore non-discrete edge masks, which are sampled from a continuous and dispersive probability distribution instead of the discrete Bernoulli distribution. These masks restrict the amount of output messages for each edge, referred to as "bandwidths". We propose a novel, informative, and effective topological masked graph autoencoder using bandwidth masking and a layer-wise bandwidth prediction objective. We demonstrate its powerful graph topological learning ability both theoretically and empirically. Our proposed framework outperforms representative baselines in both self-supervised link prediction (improving the discrete edge reconstructors by at most 20%) and node classification on numerous datasets, solely with a structure-learning pretext. Our implementation is available at https://github.com/Newiz430/Bandana. | Ziwen Zhao, Yuhua Li, Yixiong Zou, Jiliang Tang, Ruixuan Li |  |
| 568 |  |  [Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks](https://doi.org/10.1145/3589334.3645367) |  | 0 | Graph Neural Networks (GNNs) have become popular in Graph Representation Learning (GRL). One fundamental application is few-shot node classification. Most existing methods follow the meta learning paradigm, showing the ability of fast generalization to few-shot tasks. However, recent works indicate that graph contrastive learning combined with fine-tuning can significantly outperform meta learning methods. Despite the empirical success, there is limited understanding of the reasons behind it. In our study, we first identify two crucial advantages of contrastive learning compared to meta learning, including (1) the comprehensive utilization of graph nodes and (2) the power of graph augmentations. To integrate the strength of both contrastive learning and meta learning on the few-shot node classification tasks, we introduce a new paradigm: Contrastive Few-Shot Node Classification (COLA). Specifically, COLA employs graph augmentations to identify semantically similar nodes, which enables the construction of meta-tasks without the need for label information. Therefore, COLA can utilize all nodes to construct meta-tasks, further reducing the risk of overfitting. Through extensive experiments, we validate the essentiality of each component in our design and demonstrate that COLA achieves new state-of-the-art on all tasks. | Hao Liu, Jiarui Feng, Lecheng Kong, Dacheng Tao, Yixin Chen, Muhan Zhang |  |
| 569 |  |  [Local Centrality Minimization with Quality Guarantees](https://doi.org/10.1145/3589334.3645382) |  | 0 | Centrality measures, quantifying the importance of vertices or edges, play a fundamental role in network analysis. To date, triggered by some positive approximability results, a large body of work has been devoted to studying centrality maximization, where the goal is to maximize the centrality score of a target vertex by manipulating the structure of a given network. On the other hand, due to the lack of such results, only very little attention has been paid to centrality minimization, despite its practical usefulness. In this study, we introduce a novel optimization model for local centrality minimization, where the manipulation is allowed only around the target vertex. We prove the NP-hardness of our model and that the most intuitive greedy algorithm has a quite limited performance in terms of approximation ratio. Then we design two effective approximation algorithms: The first algorithm is a highly-scalable algorithm that has an approximation ratio unachievable by the greedy algorithm, while the second algorithm is a bicriteria approximation algorithm that solves a continuous relaxation based on the Lovász extension, using a projected subgradient method. To the best of our knowledge, ours are the first polynomial-time algorithms with provable approximation guarantees for centrality minimization. Experiments using a variety of real-world networks demonstrate the effectiveness of our proposed algorithms: Our first algorithm is applicable to million-scale graphs and obtains much better solutions than those of scalable baselines, while our second algorithm is rather strong against adversarial instances. | Atsushi Miyauchi, Lorenzo Severini, Francesco Bonchi |  |
| 570 |  |  [Fast Inference of Removal-Based Node Influence](https://doi.org/10.1145/3589334.3645389) |  | 0 | Graph neural networks (GNNs) are widely utilized to capture the information spreading patterns in graphs. While remarkable performance has been achieved, there is a new trending topic of evaluating node influence. We propose a new method of evaluating node influence, which measures the prediction change of a trained GNN model caused by removing a node. A real-world application is, "In the task of predicting Twitter accounts' polarity, had a particular account been removed, how would others' polarity change?". We use the GNN as a surrogate model whose prediction could simulate the change of nodes or edges caused by node removal. To obtain the influence for every node, a straightforward way is to alternately remove every node and apply the trained GNN on the modified graph. It is reliable but time-consuming, so we need an efficient method. The related lines of work, such as graph adversarial attack and counterfactual explanation, cannot directly satisfy our needs, since they do not focus on the global influence score for every node. We propose an efficient and intuitive method, NOde-Removal-based fAst GNN inference (NORA), which uses the gradient to approximate the node-removal influence. It only costs one forward propagation and one backpropagation to approximate the influence score for all nodes. Extensive experiments on six datasets and six GNN models verify the effectiveness of NORA. Our code is available at https://github.com/weikai-li/NORA.git. | Weikai Li, Zhiping Xiao, Xiao Luo, Yizhou Sun |  |
| 571 |  |  [Memory Disagreement: A Pseudo-Labeling Measure from Training Dynamics for Semi-supervised Graph Learning](https://doi.org/10.1145/3589334.3645398) |  | 0 | In the realm of semi-supervised graph learning, pseudo-labeling is a pivotal strategy to utilize both labeled and unlabeled nodes for model training. Currently, confidence score is the most frequently used pseudo-labeling measure, however, it suffers from poor calibration and issues in out-of-distribution data. In this paper, we propose memory disagreement (MoDis for short), a novel uncertainty measure for pseudo-labeling. We uncover that training dynamics offer significant insights into prediction uncertainty --- if a graph model makes consistent predictions for an unlabeled node throughout training, the corresponding predicted label is likely to be correct. Thus, the node should be suitable for pseudo-labeling. The basic idea is supported by recent studies on training dynamics. We implement MoDis as the entropy of an accumulated distribution that summarizes the disagreement of the model's predictions throughout training. We further enhance and analyze MoDis in case studies, which show nodes with low MoDis are suitable for pseudo-labeling as these nodes tend to be distant from boundaries in both graph and representation space. We design MoDis based pseudo-label selection algorithm and corresponding pseudo-labeling algorithm, which are applicable to various graph neural networks. We empirically validate MoDis on eight benchmark graph datasets. The experimental results show that pseudo labels given by MoDis have better quality in correctness and information gain, and the algorithm benefits various graph neural networks, achieving an average relative improvement of 3.11% and reaching up to 30.24% when compared to the wildly-used uncertainty measure, confidence score. Moreover, we demonstrate the efficacy of MoDis on out-of-distribution nodes. | Hongbin Pei, Yuheng Xiong, Pinghui Wang, Jing Tao, Jialun Liu, Huiqi Deng, Jie Ma, Xiaohong Guan |  |
| 572 |  |  [Descriptive Kernel Convolution Network with Improved Random Walk Kernel](https://doi.org/10.1145/3589334.3645405) |  | 0 | Graph kernels used to be the dominant approach to feature engineering for structured data, which are superseded by modern GNNs as the former lacks learnability. Recently, a suite of Kernel Convolution Networks (KCNs) successfully revitalized graph kernels by introducing learnability, which convolves input with learnable hidden graphs using a certain graph kernel. The random walk kernel (RWK) has been used as the default kernel in many KCNs, gaining increasing attention. In this paper, we first revisit the RWK and its current usage in KCNs, revealing several shortcomings of the existing designs, and propose an improved graph kernel RWK^+, by introducing color-matching random walks and deriving its efficient computation. We then propose RWK^+ CN, a KCN that uses RWK^+ as the core kernel to learn descriptive graph features with an unsupervised objective, which can not be achieved by GNNs. Further, by unrolling RWK^+, we discover its connection with a regular GCN layer, and propose a novel GNN layer RWK^+ Conv. In the first part of experiments, we demonstrate the descriptive learning ability of RWK^+ CN with the improved random walk kernel RWK^+ on unsupervised pattern mining tasks; in the second part, we show the effectiveness of RWK^+ for a variety of KCN architectures and supervised graph learning tasks, and demonstrate the expressiveness of RWK^+ Conv layer, especially on the graph-level tasks. RWK^+ and RWK^+ Conv adapt to various real-world applications, including web applications such as bot detection in a web-scale Twitter social network, and community classification in Reddit social interaction networks. | MengChieh Lee, Lingxiao Zhao, Leman Akoglu |  |
| 573 |  |  [Dynamic Graph Information Bottleneck](https://doi.org/10.1145/3589334.3645411) |  | 0 | Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB_MS and DGIB_C, in which the DGIB_MS channel aims to learn the minimal and sufficient representations, with the DGIB_MS channel guarantees the predictive consensus. Extensive experiments on real-world and synthetic dynamic graph datasets demonstrate the superior robustness of DGIB against adversarial attacks compared with state-of-the-art baselines in the link prediction task. To the best of our knowledge, DGIB is the first work to learn robust representations of dynamic graphs grounded in the information-theoretic IB principle. | Haonan Yuan, Qingyun Sun, Xingcheng Fu, Cheng Ji, Jianxin Li |  |
| 574 |  |  [Extracting Small Subgraphs in Road Networks](https://doi.org/10.1145/3589334.3645415) |  | 0 | Online navigation platforms are well optimized to solve the standard objective of minimizing travel time and typically require precomputation-based architectures (such as Contraction Hierarchies and Customizable Route Planning) to do so in a fast manner. The reason for this dependence is the size of the graph that represents the road network, which is large. The need to go beyond minimizing the travel time and introduce various types of customizations has led to approaches that rely on alternative route computation or, more generally, small subgraph extraction. On a small subgraph, one can run computationally expensive algorithms at query time and compute optimal solutions for multiple routing problems. In this framework, it is critical for the subgraph to (a) be small and (b) include (near) optimal routes for a collection of customizations. This is precisely the setting that we study in this work. We design algorithms that extract a subgraph connecting designated terminals with the objective of minimizing the subgraph's size and the constraint of including near-optimal routes for a set of predefined cost functions. We provide theoretical guarantees for our algorithms and evaluate them empirically using real-world road networks. | Sara Ahmadian, Sreenivas Gollapudi, Gregory Hutchins, Kostas Kollias, Xizhi Tan |  |
| 575 |  |  [Game-theoretic Counterfactual Explanation for Graph Neural Networks](https://doi.org/10.1145/3589334.3645419) |  | 0 | Graph Neural Networks (GNNs) have been a powerful tool for node classification tasks in complex networks. However, their decision-making processes remain a black-box to users, making it challenging to understand the reasoning behind their predictions. Counterfactual explanations (CFE) have shown promise in enhancing the interpretability of machine learning models. Prior approaches to compute CFE for GNNS often are learning-based approaches that require training additional graphs. In this paper, we propose a semivalue-based, non-learning approach to generate CFE for node classification tasks, eliminating the need for any additional training. Our results reveals that computing Banzhaf values requires lower sample complexity in identifying the counterfactual explanations compared to other popular methods such as computing Shapley values. Our empirical evidence indicates computing Banzhaf values can achieve up to a fourfold speed up compared to Shapley values. We also design a thresholding method for computing Banzhaf values and show theoretical and empirical results on its robustness in noisy environments, making it superior to Shapley values. Furthermore, the thresholded Banzhaf values are shown to enhance efficiency without compromising the quality (i.e., fidelity) in the explanations in three popular graph datasets. | Chirag Chhablani, Sarthak Jain, Akshay Channesh, Ian A. Kash, Sourav Medya |  |
| 576 |  |  [Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph](https://doi.org/10.1145/3589334.3645452) |  | 0 | Due to the ubiquity of graph data on the web, web graph mining has become a hot research spot. Nonetheless, the prevalence of largescale web graphs in real applications poses significant challenges to storage, computational capacity and graph model design. Despite numerous studies to enhance the scalability of graph models, a noticeable gap remains between academic research and practical web graph mining applications. One major cause is that in most industrial scenarios, only a small part of nodes in a web graph are actually required to be analyzed, where we term these nodes as target nodes, while others as background nodes. In this paper, we argue that properly fetching and condensing the background nodes from massive web graph data might be a more economical shortcut to tackle the obstacles fundamentally. To this end, we make the first attempt to study the problem of massive background nodes compression for target nodes classification. Through extensive experiments, we reveal two critical roles played by the background nodes in target node classification: enhancing structural connectivity between target nodes, and feature correlation with target nodes. Following this, we propose a novel Graph-Skeleton model, which properly fetches the background nodes, and further condenses the semantic and topological information of background nodes within similar target-background local structures. Extensive experiments on various web graph datasets demonstrate the effectiveness and efficiency of the proposed method. In particular, for MAG240M dataset with 0.24 billion nodes, our generated skeleton graph achieves highly comparable performance while only containing 1.8% nodes of the original graph. | Linfeng Cao, Haoran Deng, Yang Yang, Chunping Wang, Lei Chen | The Ohio State University, Columbus, USA; FinVolution Group, Shanghai, China; Zhejiang University, Hangzhou, China |
| 577 |  |  [GAUSS: GrAph-customized Universal Self-Supervised Learning](https://doi.org/10.1145/3589334.3645453) |  | 0 | To make Graph Neural Networks (GNNs) meet the requirements of the Web, the universality and the generalization become two important research directions. On one hand, many universal GNNs are presented for semi-supervised tasks on both homophilic and non-homophilic graphs by distinguishing homophilic and heterophilic edges with the help of labels. On the other hand, self-supervised learning (SSL) algorithms on graphs are presented by leveraging the self-supervised learning schemes from computer vision and natural language processing. Unfortunately, graph universal self-supervised learning remains resolved. Most existing SSL methods on graphs, which often employ two-layer GCN as the encoder and train the mapping functions, can't alter the low-passing filtering characteristic of GCN. Therefore, to be universal, SSL must becustomized for the graph, i.e., learning the graph. However, learning the graph via universal GNNs is disabled in SSL, since their distinguishability on homophilic and heterophilic edges disappears without the labels. To overcome this difficulty, this paper proposes novel GrAph-customized Universal Self-Supervised Learning (GAUSS) by exploiting local attribute distribution. The main idea is to replace the global parameters with locally learnable propagation. To make the propagation matrix demonstrate the affinity between the nodes, the self-representative learning framework is employed with k-block diagonal regularization. Extensive experiments on synthetic and real-world datasets demonstrate its effectiveness, universality and robustness to noises. | Liang Yang, Weixiao Hu, Jizhong Xu, Runjie Shi, Dongxiao He, Chuan Wang, Xiaochun Cao, Zhen Wang, Bingxin Niu, Yuanfang Guo |  |
| 578 |  |  [Optimizing Network Resilience via Vertex Anchoring](https://doi.org/10.1145/3589334.3645465) |  | 0 | Network resilience is a critical ability of a network to maintain its functionality against disturbances. A network is resilient/robust when a large portion of the nodes are to be better engaged in the network, i.e., they are less likely to leave given the changes on the network. Existing studies validate that the engagement of a node can be well captured by its coreness on network topology. Therefore, it is promising to maximize the number of nodes with increasing coreness values. In this paper, we propose and study thefollower maximization problem: maximizing the resilience gain (the number of coreness-increased vertices) via anchoring a set of vertices within a given budget. We prove that the problem is NP-hard and W[2]-hard, and it is NP-hard to approximate within an O(n^1-ε ) factor. We first propose an advanced greedy approach, followed by a time-dependent framework designed to quickly find high-quality results. The framework is initialized by the advanced greedy algorithm and incorporates novel techniques for optimizing the search space. The effectiveness and efficiency of our solution are verified with extensive experiments on 8 real-life datasets. Our source codes are available at https://github.com/Tsyxxxka/Follower-Maximization. | Siyi Teng, Jiadong Xie, Fan Zhang, Can Lu, Juntao Fang, Kai Wang |  |
| 579 |  |  [VilLain: Self-Supervised Learning on Homogeneous Hypergraphs without Features via Virtual Label Propagation](https://doi.org/10.1145/3589334.3645454) |  | 0 | Group interactions arise in various scenarios in real-world systems: collaborations of researchers, co-purchases of products, and discussions in online Q&A sites, to name a few. Such higher-order relations are naturally modeled as hypergraphs, which consist of hyperedges (i.e., any-sized subsets of nodes). For hypergraphs, the challenge to learn node representation when features or labels are not available is imminent, given that (a) most real-world hypergraphs are not equipped with external features while (b) most existing approaches for hypergraph learning resort to additional information. Thus, in this work, we propose VilLain, a novel self-supervised hypergraph representation learning method based on the propagation of virtual labels (v-labels). Specifically, we learn for each node a sparse probability distribution over v-labels as its feature vector, and we propagate the vectors to construct the final node embeddings. Inspired by higher-order label homogeneity, which we discover in real-world hypergraphs, we design novel self-supervised loss functions for the v-labels to reproduce the higher-order structure-label pattern. We demonstrate that VilLain is: (a) Requirement-free: learning node embeddings without relying on node labels and features, (b) Versatile: giving embeddings that are not specialized to specific tasks but generalizable to diverse downstream tasks, and (c) Accurate: more accurate than its competitors for node classification, hyperedge prediction, node clustering, and node retrieval tasks. Our code and dataset are available at https://github.com/geon0325/VilLain. | Geon Lee, Soo Yong Lee, Kijung Shin |  |
| 580 |  |  [SMUG: Sand Mixing for Unobserved Class Detection in Graph Few-Shot Learning](https://doi.org/10.1145/3589334.3645466) |  | 0 | Graph few-shot learning (GFSL) has achieved great success in node classification tasks with rare labels. However, graph few-shot classification (GFSC) models often encounter the problem of classifying test samples with unobserved (or unknown) classes due to the rareness of labels. We formulate this problem as out-of-distribution (OOD) sample detection in inductive graph few-shot learning. This paper presents SMUG, a novel GFSL framework that can detect unobserved classes. Since we have no ground-truth OOD samples in a practical training dataset, it is challenging for the GFSC model to retrieve knowledge about unknown classes from labeled samples. To address this difficulty, we propose a sand mixing scheme to introduce observed classes as artificial OOD samples into meta-tasks. We also develop two unsupervised OOD discriminators to identify OOD samples. Thus, we can assess the performance of OOD discriminators since we know the true classes of these artificial OOD samples. Subsequently, we design a novel training procedure to optimize the encoder based on the performance of the OOD discriminators and the GFSC model. It not only enables the GFSL model to distinguish OOD samples but also promotes the classification accuracy of normal samples. We conduct extensive experiments to evaluate the effectiveness of SMUG based on four benchmark datasets. Experimental results demonstrate that SMUG achieves superior performance over state-of-the-art approaches in OOD detection and node classification. The source code of this paper is available at https://github.com/Memepp/SMUG. | Chenxu Wang, Xichan Nie, Jinfeng Chen, Pinghui Wang, Junzhou Zhao, Xiaohong Guan |  |
| 581 |  |  [Graph Contrastive Learning with Cohesive Subgraph Awareness](https://doi.org/10.1145/3589334.3645470) |  | 0 | Graph contrastive learning (GCL) has emerged as a state-of-the-art strategy for learning representations of diverse graphs including social and biomedical networks. GCL widely uses stochastic graph topology augmentation, such as uniform node dropping, to generate augmented graphs. However, such stochastic augmentations may severely damage the intrinsic properties of a graph and deteriorate the following representation learning process. We argue that incorporating an awareness of cohesive subgraphs during the graph augmentation and learning processes has the potential to enhance GCL performance. To this end, we propose a novel unified framework called CTAug, to seamlessly integrate cohesion awareness into various existing GCL mechanisms. In particular, CTAug comprises two specialized modules: topology augmentation enhancement and graph learning enhancement. The former module generates augmented graphs that carefully preserve cohesion properties, while the latter module bolsters the graph encoder's ability to discern subgraph patterns. Theoretical analysis shows that CTAug can strictly improve existing GCL mechanisms. Empirical experiments verify that CTAug can achieve state-of-the-art performance for graph representation learning, especially for graphs with high degrees. The code is available at https://doi.org/10.5281/zenodo.10594093, or https://github.com/wuyucheng2002/CTAug. | Yucheng Wu, Leye Wang, Xiao Han, HanJia Ye |  |
| 582 |  |  [GNNFingers: A Fingerprinting Framework for Verifying Ownerships of Graph Neural Networks](https://doi.org/10.1145/3589334.3645489) |  | 0 | Graph neural networks (GNNs) have emerged as the state of the art for a variety of graph-related tasks and have been widely commercialized in real-world scenarios. Behind its revolutionary representation capability, the huge training costs also expose GNNs to the risks of potential model piracy attacks which threaten the intellectual property (IP) of GNNs. In this work, we design a novel and effective ownership verification framework for GNNs called GNNFingers to safeguard the IP of GNNs. The key design of the proposed framework is two-fold: graph fingerprint construction and robust verification module. With GNNFingers, a GNN model owner can verify if a deployed model is stolen from the source GNN simply by querying with graph inputs. Besides, GNNFingers could be applied to various GNN models and graph-related tasks. We extensively evaluate the proposed framework on various GNNs designed for multiple graph-related tasks including graph classification, graph matching, node classification, and link prediction. Our results show that GNNFingers can robustly distinguish post-processed surrogate GNNs from irrelevant GNNs, e.g., GNNFingers achieves 100% true positives and 100% true negatives on the test of 200 suspect GNNs of both graph classification and node classification tasks. | Xiaoyu You, Youhe Jiang, Jianwei Xu, Mi Zhang, Min Yang |  |
| 583 |  |  [Graph Fairness Learning under Distribution Shifts](https://doi.org/10.1145/3589334.3645508) |  | 0 | Graph neural networks (GNNs) have achieved remarkable performance on graph-structured data. However, GNNs may inherit prejudice from the training data and make discriminatory predictions based on sensitive attributes, such as gender and race. Recently, there has been an increasing interest in ensuring fairness on GNNs, but all of them are under the assumption that the training and testing data are under the same distribution, i.e., training data and testing data are from the same graph. Will graph fairness performance decrease under distribution shifts? How does distribution shifts affect graph fairness learning? All these open questions are largely unexplored from a theoretical perspective. To answer these questions, we first theoretically identify the factors that determine bias on a graph. Subsequently, we explore the factors influencing fairness on testing graphs, with a noteworthy factor being the representation distances of certain groups between the training and testing graph. Motivated by our theoretical analysis, we propose our framework FatraGNN. Specifically, to guarantee fairness performance on unknown testing graphs, we propose a graph generator to produce numerous graphs with significant bias and under different distributions. Then we minimize the representation distances for each certain group between the training graph and generated graphs. This empowers our model to achieve high classification and fairness performance even on generated graphs with significant bias, thereby effectively handling unknown testing graphs. Experiments on real-world and semi-synthetic datasets demonstrate the effectiveness of our model in terms of both accuracy and fairness. | Yibo Li, Xiao Wang, Yujie Xing, Shaohua Fan, Ruijia Wang, Yaoqi Liu, Chuan Shi |  |
| 584 |  |  [Self-Guided Robust Graph Structure Refinement](https://doi.org/10.1145/3589334.3645522) |  | 0 | Recent studies have revealed that GNNs are vulnerable to adversarial attacks. To defend against such attacks, robust graph structure refinement (GSR) methods aim at minimizing the effect of adversarial edges based on node features, graph structure, or external information. However, we have discovered that existing GSR methods are limited by narrowassumptions, such as assuming clean node features, moderate structural attacks, and the availability of external clean graphs, resulting in the restricted applicability in real-world scenarios. In this paper, we propose a self-guided GSR framework (SG-GSR), which utilizes a clean sub-graph found within the given attacked graph itself. Furthermore, we propose a novel graph augmentation and a group-training strategy to handle the two technical challenges in the clean sub-graph extraction: 1) loss of structural information, and 2) imbalanced node degree distribution. Extensive experiments demonstrate the effectiveness of SG-GSR under various scenarios including non-targeted attacks, targeted attacks, feature attacks, e-commerce fraud, and noisy node labels. Our code is available at https://github.com/yeonjun-in/torch-SG-GSR. | Yeonjun In, Kanghoon Yoon, Kibum Kim, Kijung Shin, Chanyoung Park |  |
| 585 |  |  [DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning](https://doi.org/10.1145/3589334.3645561) |  | 0 | We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD) approach to consider both the class representativeness and the diversity within each class of the replayed nodes. Moreover, we adopt graph structure learning (GSL) to ensure that the replayed nodes are connected to truly informative neighbors. Extensive experimental results demonstrate the effectiveness and efficiency of DSLR. | Seungyoon Choi, Wonjoong Kim, Sungwon Kim, Yeonjun In, Sein Kim, Chanyoung Park |  |
| 586 |  |  [Calibrating Graph Neural Networks from a Data-centric Perspective](https://doi.org/10.1145/3589334.3645562) |  | 0 | Graph neural networks (GNNs) have gained popularity in modeling various complex networks, e.g., social network and webpage network. Despite the promising accuracy, the confidences of GNNs are shown to be miscalibrated, indicating limited awareness of prediction uncertainty and harming the reliability of model decisions. Existing calibration methods primarily focus on improving GNN models, e.g., adding regularization during training or introducing temperature scaling after training. In this paper, we argue that the miscalibration of GNNs may stem from the graph data and can be alleviated through topology modification. To support this motivation, we conduct data observations by examining the impacts ofdecisive andhomophilic edges on calibration performance, where decisive edges play a critical role in GNN predictions and homophilic edges connect nodes of the same class. By assigning larger weights to these edges in the adjacency matrix, we observe an improvement in calibration performance without sacrificing classification accuracy. This suggests the potential of a data-centric approach for calibrating GNNs. Motivated by our observations, we propose Data-centric Graph Calibration (DCGC), which uses two edge weighting modules to adjust the input graph for GNN calibration. The first module learns the weights of decisive edges by parameterizing the adjacency matrix and enabling backpropagation of the prediction loss to edge weights. This emphasizes critical edges that fit the prediction needs. The second module computes weights for homophilic edges based on predicted label distributions, assigning larger weights to edges with stronger homophily. These modifications operate at the data level and can be easily integrated with temperature scaling-based methods for better calibration. Experimental results on 8 benchmark datasets demonstrate that DCGC achieves state-of-the-art calibration performance, with an average relative improvement of 36.4% in ECE, while maintaining or even slightly improving classification accuracy. Ablation studies and hyper-parameter analysis further validate the effectiveness and robustness of our proposed method DCGC. Code and data are available at https://github.com/BUPT-GAMMA/DCGC. | Cheng Yang, Chengdong Yang, Chuan Shi, Yawen Li, Zhiqiang Zhang, Jun Zhou |  |
| 587 |  |  [EXGC: Bridging Efficiency and Explainability in Graph Condensation](https://doi.org/10.1145/3589334.3645551) |  | 0 | Graph representation learning on vast datasets, like web data, has made significant strides. However, the associated computational and storage overheads raise concerns. In sight of this, Graph condensation (GCond) has been introduced to distill these large real datasets into a more concise yet information-rich synthetic graph. Despite acceleration efforts, existing GCond methods mainly grapple with efficiency, especially on expansive web data graphs. Hence, in this work, we pinpoint two major inefficiencies of current paradigms: (1) the concurrent updating of a vast parameter set, and (2) pronounced parameter redundancy. To counteract these two limitations correspondingly, we first (1) employ the Mean-Field variational approximation for convergence acceleration, and then (2) propose the objective of Gradient Information Bottleneck (GDIB) to prune redundancy. By incorporating the leading explanation techniques (e.g., GNNExplainer and GSAT) to instantiate the GDIB, our EXGC, the Efficient and eXplainable Graph Condensation method is proposed, which can markedly boost efficiency and inject explainability. Our extensive evaluations across eight datasets underscore EXGC's superiority and relevance. Code is available at https://github.com/MangoKiller/EXGC. | Junfeng Fang, Xinglin Li, Yongduo Sui, Yuan Gao, Guibin Zhang, Kun Wang, Xiang Wang, Xiangnan He |  |
| 588 |  |  [Fast and Accurate Fair k-Center Clustering in Doubling Metrics](https://doi.org/10.1145/3589334.3645568) |  | 0 | We study the classic k-center clustering problem under the additional constraint that each cluster should be fair. In this setting, each point is marked with one or more colors, which can be used to model protected attributes (e.g., gender or ethnicity). A cluster is deemed fair if, for every color, the fraction of its points marked with that color is within some prespecified range. We present a coreset-based approach to fair k-center clustering for general metric spaces which attains almost the best approximation quality of the current state of the art solutions, while featuring running times which can be orders of magnitude faster for large datasets of low doubling dimension. We devise sequential, streaming and MapReduce implementations of our approach and conduct a thorough experimental analysis to provide evidence of their practicality, scalability, and effectiveness. | Matteo Ceccarello, Andrea Pietracaprina, Geppino Pucci |  |
| 589 |  |  [Graph Principal Flow Network for Conditional Graph Generation](https://doi.org/10.1145/3589334.3645570) |  | 0 | Conditional graph generation is crucial and challenging since the conditional distribution of graph topology and feature is complicated and the semantic information is hard to capture by the generative model. In this work, we propose a novel graph conditional generative model, Graph Principal Flow Network (GPrinFlowNet), which enables us to progressively generate high-quality graphs from low- to high-frequency components for a given graph label. We show that GPrinFlowNet follows a coarse-to-fine resolution generation curriculum, which enables it to capture subtle semantic information by generating intermediate graphs with high mutual information relative to the graph label. Extensive experiments and ablation studies showcase that our model achieves state-of-the-art performance compared to existing conditional graph generation models. | Zhanfeng Mo, Tianze Luo, Sinno Jialin Pan |  |
| 590 |  |  [Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem](https://doi.org/10.1145/3589334.3645583) |  | 0 | The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to extract low-frequency signals from graph topology, which may lead to the over-smoothing problem when GCN goes deep. To this end, various methods have been proposed to create an adaptive filter by incorporating an extra filter (e.g., a high-pass filter) extracted from the graph topology. However, these methods heavily rely on topological information and ignore the node attribute space, which severely sacrifices the expressive power of the deep GCNs, especially when dealing with disassortative graphs. In this paper, we propose a cross-space adaptive filter, called CSF, to produce the adaptive-frequency information extracted from both the topology and attribute spaces. Specifically, we first derive a tailored attribute-based high-pass filter that can be interpreted theoretically as a minimizer for semi-supervised kernel ridge regression. Then, we cast the topology-based low-pass filter as a Mercer's kernel within the context of GCNs. This serves as a foundation for combining it with the attribute-based filter to capture the adaptive-frequency information. Finally, we derive the cross-space filter via an effective multiple-kernel learning strategy, which unifies the attribute-based high-pass filter and the topology-based low-pass filter. This helps to address the over-smoothing problem while maintaining effectiveness. Extensive experiments demonstrate that CSF not only successfully alleviates the over-smoothing problem but also promotes the effectiveness of the node classification task. | Chen Huang, Haoyang Li, Yifan Zhang, Wenqiang Lei, Jiancheng Lv |  |
| 591 |  |  [A Quasi-Wasserstein Loss for Learning Graph Neural Networks](https://doi.org/10.1145/3589334.3645586) |  | 0 | When learning graph neural networks (GNNs) in node-level prediction tasks, most existing loss functions are applied for each node independently, even if node embeddings and their labels are non-i.i.d. because of their graph structures. To eliminate such inconsistency, in this study we propose a novel Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on graphs, leading to new learning and prediction paradigms of GNNs. In particular, we design a "Quasi-Wasserstein" distance between the observed multi-dimensional node labels and their estimations, optimizing the label transport defined on graph edges. The estimations are parameterized by a GNN in which the optimal label transport may determine the graph edge weights optionally. By reformulating the strict constraint of the label transport to a Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein loss associated with two efficient solvers learning the GNN together with optimal label transport. When predicting node labels, our model combines the output of the GNN with the residual component provided by the optimal label transport, leading to a new transductive prediction paradigm. Experiments show that the proposed QW loss applies to various GNNs and helps to improve their performance in node-level classification and regression tasks. | Minjie Cheng, Hongteng Xu |  |
| 592 |  |  [GNNShap: Scalable and Accurate GNN Explanation using Shapley Values](https://doi.org/10.1145/3589334.3645599) |  | 0 | Graph neural networks (GNNs) are popular machine learning models for graphs with many applications across scientific domains. However, GNNs are considered black box models, and it is challenging to understand how the model makes predictions. Game theoric Shapley value approaches are popular explanation methods in other domains but are not well-studied for graphs. Some studies have proposed Shapley value based GNN explanations, yet they have several limitations: they consider limited samples to approximate Shapley values; some mainly focus on small and large coalition sizes, and they are an order of magnitude slower than other explanation methods, making them inapplicable to even moderate-size graphs. In this work, we propose GNNShap, which provides explanations for edges since they provide more natural explanations for graphs and more fine-grained explanations. We overcome the limitations by sampling from all coalition sizes, parallelizing the sampling on GPUs, and speeding up model predictions by batching. GNNShap gives better fidelity scores and faster explanations than baselines on real-world datasets. The code is available at https://github.com/HipGraph/GNNShap. | Selahattin Akkas, Ariful Azad |  |
| 593 |  |  [Graph Out-of-Distribution Generalization via Causal Intervention](https://doi.org/10.1145/3589334.3645604) |  | 0 | Out-of-distribution (OOD) generalization has gained increasing attentions for learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation with distribution shifts. The challenge is that distribution shifts on graphs involve intricate interconnections between nodes, and the environment labels are often absent in data. In this paper, we adopt a bottom-up data-generative perspective and reveal a key observation through causal analysis: the crux of GNNs' failure in OOD generalization lies in the latent confounding bias from the environment. The latter misguides the model to leverage environment-sensitive correlations between ego-graph features and target nodes' labels, resulting in undesirable generalization on new unseen nodes. Built upon this analysis, we introduce a conceptually simple yet principled approach for training robust GNNs under node-level distribution shifts, without prior knowledge of environment labels. Our method resorts to a new learning objective derived from causal inference that coordinates an environment estimator and a mixture-of-expert GNN predictor. The new approach can counteract the confounding bias in training data and facilitate learning generalizable predictive relations. Extensive experiment demonstrates that our model can effectively enhance generalization with various types of distribution shifts and yield up to 27.4% accuracy improvement over state-of-the-arts on graph OOD generalization benchmarks. Source codes are available at https://github.com/fannie1208/CaNet. | Qitian Wu, Fan Nie, Chenxiao Yang, Tianyi Bao, Junchi Yan |  |
| 594 |  |  [Adversarial Mask Explainer for Graph Neural Networks](https://doi.org/10.1145/3589334.3645608) |  | 0 | The Graph Neural Networks (GNNs) model is a powerful tool for integrating node information with graph topology to learn representations and make predictions. However, the complex graph structure of GNNs has led to a lack of clear explainability in the decision-making process. Recently, there has been a growing interest in seeking instance-level explanations of the GNNs model, which aims to uncover the decision-making process of the GNNs model and provide insights into how it arrives at its final output. Previous works have focused on finding a set of weights (masks) for edges/nodes/node features to determine their importance. These works have adopted a regularization term and a hyperparameter K to control the explanation size during the training process and keep only the top-K weights as the explanation set. However, the true size of the explanation is typically unknown to users, making it difficult to provide reasonable values for the regularization term and K. In this work, we propose a novel framework AMExplainer which leverages the concept of adversarial networks to achieve a dual optimization objective in the target function. This approach ensures both accurate prediction of the mask and sparsity of the explanation set. In addition, we devise a novel scaling function to automatically sense and amplify the weights of the informative part of the graph, which filters out insignificant edges/nodes/node features for expediting the convergence of the solution during training. Our extensive experiments show that AMExplainer yields a more compelling explanation by generating a sparse set of masks while simultaneously maintaining fidelity. | Wei Zhang, Xiaofan Li, Wolfgang Nejdl |  |
| 595 |  |  [On the Feasibility of Simple Transformer for Dynamic Graph Modeling](https://doi.org/10.1145/3589334.3645622) |  | 0 | Dynamic graph modeling is crucial for understanding complex structures in web graphs, spanning applications in social networks, recommender systems, and more. Most existing methods primarily emphasize structural dependencies and their temporal changes. However, these approaches often overlook detailed temporal aspects or struggle with long-term dependencies. Furthermore, many solutions overly complicate the process by emphasizing intricate module designs to capture dynamic evolutions. In this work, we harness the strength of the Transformer's self-attention mechanism, known for adeptly handling long-range dependencies in sequence modeling. Our approach offers a simple Transformer model tailored for dynamic graph modeling without complex modifications. We re-conceptualize dynamic graphs as a sequence modeling challenge and introduce an innovative temporal alignment technique. This technique not only captures the inherent temporal evolution patterns within dynamic graphs but also streamlines the modeling process of their evolution. As a result, our method becomes versatile, catering to an array of applications. Our model's effectiveness is underscored through rigorous experiments on four real-world datasets from various sectors, solidifying its potential in dynamic graph modeling. | Yuxia Wu, Yuan Fang, Lizi Liao |  |
| 596 |  |  [Can GNN be Good Adapter for LLMs?](https://doi.org/10.1145/3589334.3645627) |  | 0 | Recently, large language models (LLMs) have demonstrated superior capabilities in understanding and zero-shot learning on textual data, promising significant advances for many text-related domains. In the graph domain, various real-world scenarios also involve textual data, where tasks and node features can be described by text. These text-attributed graphs (TAGs) have broad applications in social media, recommendation systems, etc. Thus, this paper explores how to utilize LLMs to model TAGs. Previous methods for TAG modeling are based on million-scale LMs. When scaled up to billion-scale LLMs, they face huge challenges in computational costs. Additionally, they also ignore the zero-shot inference capabilities of LLMs. Therefore, we propose GraphAdapter, which uses a graph neural network (GNN) as an efficient adapter in collaboration with LLMs to tackle TAGs. In terms of efficiency, the GNN adapter introduces only a few trainable parameters and can be trained with low computation costs. The entire framework is trained using auto-regression on node text (next token prediction). Once trained, GraphAdapter can be seamlessly fine-tuned with task-specific prompts for various downstream tasks. Through extensive experiments across multiple real-world TAGs, GraphAdapter based on Llama 2 gains an average improvement of approximately 5% in terms of node classification. Furthermore, GraphAdapter can also adapt to other language models, including RoBERTa, GPT-2. The promising results demonstrate that GNNs can serve as effective adapters for LLMs in TAG modeling. | Xuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei Chai, Qi Zhu |  |
| 597 |  |  [When Imbalance Meets Imbalance: Structure-driven Learning for Imbalanced Graph Classification](https://doi.org/10.1145/3589334.3645629) |  | 0 | Graph Neural Networks (GNNs) can learn representative graph-level features to achieve efficient graph classification. But GNNs usually assume an environment where both class and structure distribution are balanced. Although previous works have considered the graph classification problem under the scenario of class imbalance or structure imbalance, they habitually ignored the obvious fact that class imbalance and structural imbalance are often intertwined in the real world. In this paper, we propose a carefully designed structure-driven learning framework called ImbGNN to address the potential intertwined class imbalance and structural imbalance in graph classification. Specifically, we find that feature-oriented augmentation (e.g., feature masking) and structure-oriented augmentation (e.g., edge perturbation) will have differential impacts when applied to different graphs. Therefore, we design optional augmentation based on the average degree distribution to alleviate structural imbalance. Furthermore, based on the imbalance of graph size distribution, we utilize a similarity-friendly graph random walk to extract a core subgraph to improve the accuracy of graph kernel similarity calculation, and then construct a more reasonable kernel-based graph of graphs, thereby alleviating the class imbalance and size imbalance. Extensive experiments on multiple benchmark datasets demonstrate that our proposed ImbGNN framework outperforms previous baselines on imbalanced graph classification tasks. The code of ImbGNN is available in~https://github.com/Xiaovy/ImbGNN. | Wei Xu, Pengkun Wang, Zhe Zhao, Binwu Wang, Xu Wang, Yang Wang |  |
| 598 |  |  [Disambiguated Node Classification with Graph Neural Networks](https://doi.org/10.1145/3589334.3645637) |  | 0 | Graph Neural Networks (GNNs) have demonstrated significant success in learning from graph-structured data across various domains. Despite their great successful, one critical challenge is often overlooked by existing works, i.e., the learning of message propagation that can generalize effectively to underrepresented graph regions. These minority regions often exhibit irregular homophily/heterophily patterns and diverse neighborhood class distributions, resulting in ambiguity. In this work, we investigate the ambiguity problem within GNNs, its impact on representation learning, and the development of richer supervision signals to fight against this problem. We conduct a fine-grained evaluation of GNN, analyzing the existence of ambiguity in different graph regions and its relation with node positions. To disambiguate node embeddings, we propose a novel method, , which exploits additional optimization guidance to enhance representation learning, particularly for nodes in ambiguous regions. identifies ambiguous nodes based on temporal inconsistency of predictions and introduces a disambiguation regularization by employing contrastive learning in a topology-aware manner. promotes discriminativity of node representations and can alleviating semantic mixing caused by message propagation, effectively addressing the ambiguity problem. Empirical results validate the efficiency of and highlight its potential to improve GNN performance in underrepresented graph regions. | Tianxiang Zhao, Xiang Zhang, Suhang Wang |  |
| 599 |  |  [Finding Densest Subgraphs with Edge-Color Constraints](https://doi.org/10.1145/3589334.3645647) |  | 0 | We consider a variant of the densest subgraph problem in networks with single or multiple edge attributes. For example, in a social network, the edge attributes may describe the type of relationship between users, such as friends, family, or acquaintances, or different types of communication. For conceptual simplicity, we view the attributes as edge colors. The new problem we address is to find a diverse densest subgraph that fulfills given requirements on the numbers of edges of specific colors. When searching for a dense social network community, our problem will enforce the requirement that the community is diverse according to criteria specified by the edge attributes. We show that the decision versions for finding exactly, at most, and at least h colored edges densest subgraph, where h is a vector of color requirements, are NP-complete, for already two colors. For the problem of finding a densest subgraph with at least h colored edges, we provide a linear-time constant-factor approximation algorithm when the input graph is sparse. On the way, we introduce the related at least h (non-colored) edges densest subgraph problem, show its hardness, and also provide a linear-time constant-factor approximation. In our experiments, we demonstrate the efficacy and efficiency of our new algorithms. | Lutz Oettershagen, Honglian Wang, Aristides Gionis |  |
| 600 |  |  [Low Mileage, High Fidelity: Evaluating Hypergraph Expansion Methods by Quantifying the Information Loss](https://doi.org/10.1145/3589334.3645657) |  | 0 | In this paper, we first define information loss that occurs in the hypergraph expansion and then propose a novel framework, named MILEAGE, to evaluate hypergraph expansion methods by measuring their degree of information loss. MILEAGE employs the following four steps: (1) expanding a hypergraph; (2) performing the unsupervised representation learning on the expanded graph; (3) reconstructing a hypergraph based on vector representations obtained; and (4) measuring MILEAGE-score (i.e., mileage) by comparing the reconstructed and the original hypergraphs. To demonstrate the usefulness of MILEAGE, we conduct experiments via downstream tasks on three levels (i.e., node, hyperedge, and hypergraph): node classification, hyperedge prediction, and hypergraph classification on eight real-world hypergraph datasets. Through the extensive experiments, we observe that information loss through hypergraph expansion has a negative impact on downstream tasks and MILEAGE can effectively evaluate hypergraph expansion methods through the information loss and recommend a new method that resolves the problems of existing ones. | David Y. Kang, Qiaozhu Mei, SangWook Kim |  |
| 601 |  |  [Learning Scalable Structural Representations for Link Prediction with Bloom Signatures](https://doi.org/10.1145/3589334.3645672) |  | 0 | Graph neural networks (GNNs) have shown great potential in learning on graphs, but they are known to perform sub-optimally on link prediction tasks. Existing GNNs are primarily designed to learn node-wise representations and usually fail to capture pairwise relations between target nodes, which proves to be crucial for link prediction. Recent works resort to learning more expressive edge-wise representations by enhancing vanilla GNNs with structural features such as labeling tricks and link prediction heuristics, but they suffer from high computational overhead and limited scalability. To tackle this issue, we propose to learn structural link representations by augmenting the message-passing framework of GNNs with Bloom signatures. Bloom signatures are hashing-based compact encodings of node neighborhoods, which can be efficiently merged to recover various types of edge-wise structural features. We further show that any type of neighborhood overlap-based heuristic can be estimated by a neural network that takes Bloom signatures as input. GNNs with Bloom signatures are provably more expressive than vanilla GNNs and also more scalable than existing edge-wise models. Experimental results on five standard link prediction benchmarks show that our proposed model achieves comparable or better performance than existing edge-wise GNN models while being 3-200 × faster and more memory-efficient for online inference. | Tianyi Zhang, Haoteng Yin, Rongzhe Wei, Pan Li, Anshumali Shrivastava |  |
| 602 |  |  [GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks](https://doi.org/10.1145/3589334.3645682) |  | 0 | Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and instruction-following capabilities, have catalyzed a revolutionary transformation across diverse research fields of artificial intelligence, especially for open-ended tasks. While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor. To break this dilemma, we propose to bridge the pretrained GM and LLM by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of LLMs to offer various open-ended tasks for GM. To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along node information, neighbor information and model information. By treating the node representation as a type of language, the proposed GraphTranslator empowers an LLM to make predictions based on node representation and language instructions, providing a unified perspective for both pre-defined and open-ended tasks. Extensive results show that the proposed GraphTranslator effectively improves the results of zero-shot node classification. The graph question answering experiments reveal our GraphTranslator potential across a broad spectrum of open-ended applications through language instructions. | Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, Chuan Shi |  |
| 603 |  |  [HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks](https://doi.org/10.1145/3589334.3645685) |  | 0 | Graphs have emerged as a natural choice to represent and analyze the intricate patterns and rich information of the Web, enabling applications such as online page classification and social recommendation. The prevailing "pre-train, fine-tune" paradigm has been widely adopted in graph machine learning tasks, particularly in scenarios with limited labeled nodes. However, this approach often exhibits a misalignment between the training objectives of pretext tasks and those of downstream tasks. This gap can result in the "negative transfer" problem, wherein the knowledge gained from pre-training adversely affects performance in the downstream tasks. The surge in prompt-based learning within Natural Language Processing (NLP) suggests the potential of adapting a "pre-train, prompt" paradigm to graphs as an alternative. However, existing graph prompting techniques are tailored to homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To bridge this gap, we propose HetGPT, a general post-training prompting framework to improve the predictive performance of pre-trained heterogeneous graph neural networks (HGNNs). The key is the design of a novel prompting function that integrates a virtual class prompt and a heterogeneous feature prompt, with the aim to reformulate downstream tasks to mirror pretext tasks. Moreover, HetGPT introduces a multi-view neighborhood aggregation mechanism, capturing the complex neighborhood structure in heterogeneous graphs. Extensive experiments on three benchmark datasets demonstrate HetGPT's capability to enhance the performance of state-of-the-art HGNNs on semi-supervised node classification. | Yihong Ma, Ning Yan, Jiayu Li, Masood S. Mortazavi, Nitesh V. Chawla |  |
| 604 |  |  [Graph Contrastive Learning via Interventional View Generation](https://doi.org/10.1145/3589334.3645687) |  | 0 | Graph contrastive learning (GCL), as a popular self-supervised learning technique, has demonstrated promising capability in learning discriminative representations for diverse downstream tasks. A large body of GCL frameworks mainly work on graphs formed under homophily effect, i.e., similar nodes tend to connect with each other. In their design, the augmentation and aggregation are usually conducted indiscriminately on edges, ignoring the existence of heterophilic edges that connect dissimilar nodes. Therefore, the efficacy of GCL could greatly deteriorate on heterophilic graphs, verified by our analysis: GCL on a mixture of homophilic and heterophilic edges will generate representations that are indistinguishable across different classes in the embedding space. To address this challenge, we propose a novel GCL framework via interventional view generation. Specifically, we generate homophilic and heterophilic views through counterfactual intervention, which targets on disentangling homophilic and heterophilic structure from the original graph, such that we can capture their corresponding information using separate filters in the contrastive learning process. Since the homophilic view and the heterophilic view present different frequency signals, they are further encoded via a low-pass and a high-pass filter respectively. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our design. Our proposed framework achieves a remarkably improved downstream performance on graphs with high heterophily while maintaining a comparable ability in learning homophilic graphs. A comprehensive study also verifies the necessity of individual designs in our framework. | Zengyi Wo, Minglai Shao, Wenjun Wang, Xuan Guo, Lu Lin |  |
| 605 |  |  [Endowing Pre-trained Graph Models with Provable Fairness](https://doi.org/10.1145/3589334.3645703) |  | 0 | Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks. Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications. The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs. However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient. Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario. To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained Graph models with Provable fAiRness (called GraphPAR). GraphPAR freezes the parameters of PGMs and trains a parameter-efficient adapter to flexibly improve the fairness of PGMs in downstream tasks. Specifically, we design a sensitive semantic augmenter on node representations, to extend the node representations with different sensitive attribute semantics for each node. The extended representations will be used to further train an adapter, to prevent the propagation of sensitive attribute semantics from PGMs to task predictions. Furthermore, with GraphPAR, we quantify whether the fairness of each node is provable, i.e., predictions are always fair within a certain range of sensitive attribute semantics. Experimental evaluations on real-world datasets demonstrate that GraphPAR achieves state-of-the-art prediction performance and fairness on node classification task. Furthermore, based on our GraphPAR, around 90% nodes have provable fairness. | Zhongjian Zhang, Mengmei Zhang, Yue Yu, Cheng Yang, Jiawei Liu, Chuan Shi |  |
| 606 |  |  [Unveiling Delay Effects in Traffic Forecasting: A Perspective from Spatial-Temporal Delay Differential Equations](https://doi.org/10.1145/3589334.3645688) |  | 0 | Traffic flow forecasting is a fundamental research issue for transportation planning and management, which serves as a canonical and typical example of spatial-temporal predictions. In recent years, Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) have achieved great success in capturing spatial-temporal correlations for traffic flow forecasting. Yet, two non-ignorable issues haven't been well solved: 1) The message passing in GNNs is immediate, while in reality the spatial message interactions among neighboring nodes can be delayed. The change of traffic flow at one node will take several minutes, i.e., time delay, to influence its connected neighbors. 2) Traffic conditions undergo continuous changes. The prediction frequency for traffic flow forecasting may vary based on specific scenario requirements. Most existing discretized models require retraining for each prediction horizon, restricting their applicability. To tackle the above issues, we propose a neural Spatial-Temporal Delay Differential Equation model, namely STDDE. It includes both delay effects and continuity into a unified delay differential equation framework, which explicitly models the time delay in spatial information propagation. Furthermore, theoretical proofs are provided to show its stability. Then we design a learnable traffic-graph time-delay estimator, which utilizes the continuity of the hidden states to achieve the gradient backward process. Finally, we propose a continuous output module, allowing us to accurately predict traffic flow at various frequencies, which provides more flexibility and adaptability to different scenarios. Extensive experiments show the superiority of STDDE. Both quantitative and qualitative experiments are conducted to validate the concept of a delay-aware module. Also, the flexibility validation shows the effectiveness of the continuous output module. | Qingqing Long, Zheng Fang, Chen Fang, Chong Chen, Pengfei Wang, Yuanchun Zhou |  |
| 607 |  |  [Optimizing Polynomial Graph Filters: A Novel Adaptive Krylov Subspace Approach](https://doi.org/10.1145/3589334.3645705) |  | 0 | Graph Neural Networks (GNNs), known as spectral graph filters, find a wide range of applications in web networks. To bypass eigendecomposition, polynomial graph filters are proposed to approximate graph filters by leveraging various polynomial bases for filter training. However, no existing studies have explored the diverse polynomial graph filters from a unified perspective for optimization. In this paper, we first unify polynomial graph filters, as well as the optimal filters of identical degrees into the Krylov subspace of the same order, thus providing equivalent expressive power theoretically. Next, we investigate the asymptotic convergence property of polynomials from the unified Krylov subspace perspective, revealing their limited adaptability in graphs with varying heterophily degrees. Inspired by those facts, we design a novel adaptive Krylov subspace approach to optimize polynomial bases with provable controllability over the graph spectrum so as to adapt various heterophily graphs. Subsequently, we propose AdaptKry, an optimized polynomial graph filter utilizing bases from the adaptive Krylov subspaces. Meanwhile, in light of the diverse spectral properties of complex graphs, we extend AdaptKry by leveraging multiple adaptive Krylov bases without incurring extra training costs. As a consequence, extended AdaptKry is able to capture the intricate characteristics of graphs and provide insights into their inherent complexity. We conduct extensive experiments across a series of real-world datasets. The experimental results demonstrate the superior filtering capability of AdaptKry, as well as the optimized efficacy of the adaptive Krylov basis. | Keke Huang, Wencai Cao, Hoang Ta, Xiaokui Xiao, Pietro Liò |  |
| 608 |  |  [Full-Attention Driven Graph Contrastive Learning: with Effective Mutual Information Insight](https://doi.org/10.1145/3589334.3645717) |  | 0 | Graph contrastive learning often faces challenges when data augmentations compromise the graph's critical attributes, introducing the risk of generating noise-positive pairs. Although recent methods have attempted to address these issues, they either fall short of ensuring effective data augmentation or suffer from excessive computational demands. The advent of full-attention graph Transformers, with their enhanced capacity for graph representation learning, has sparked significant interest. Despite their potential, employing full-attention graph Transformers for contrastive learning can introduce issues such as noisy redundancies. In this work, we propose the Graph Attention Contrastive Learning (GACL) model, which innovatively combines a full-attention transformer with a message-passing graph neural network as its encoder. To mitigate the noise associated with full-attention mechanisms, we apply a denoising modification. Our GACL model effectively tackles the challenges associated with full-attention mechanisms and introduces a novel approach for data augmentation. Moreover, we propose the concept of effective mutual information to theoretically underpin our methodology. Utilizing this framework, we explore the impact of the denoising matrix within GACL's contrastive learning process and delve into comprehensive discussions on its implications. Empirical assessments underscore GACL's exceptional performance, establishing it as a state-of-the-art solution in graph contrastive learning. | Long Li, Zemin Liu, Chenghao Liu, Jianling Sun |  |
| 609 |  |  [Understanding GDPR Non-Compliance in Privacy Policies of Alexa Skills in European Marketplaces](https://doi.org/10.1145/3589334.3645409) |  | 0 | Amazon Alexa is one of the largest Voice Personal Assistant (VPA) platforms and it allows third-party developers to publish their voice apps, named skills, to the Alexa skill store. To satisfy the needs of European users, Amazon Alexa has established multiple skill marketplaces in Europe and allows developers to publish skills in their native languages. Skills in European marketplaces are required to comply with GDPR (General Data Protection Regulation), which imposes strict obligations on data collection and processing. Skills that involve data collection should provide a privacy policy to disclose the data practice to users and meet GDPR requirements. In this work, we analyze the privacy policies of skills in European marketplaces, focusing on whether skills' privacy policies and data collection behaviors comply with GDPR. We collect a large-scale dataset that includes skills in all European marketplaces with privacy policies. To classify whether a sentence in a privacy policy provides GDPR information, we gather a labeled dataset including skills' privacy policy sentences and use it to train a BERT model. Then, we analyze the GDPR compliance of European skills. Using a dynamic testing tool based on ChatGPT, we check whether skills' privacy policies comply with GDPR and are consistent with the actual data collection behaviors. Surprisingly, we find that 67% of the privacy policies fail to comply with GDPR and don't provide necessary GDPR-related information. For 1,187 skills with data collection behaviors, we observe that 603 skills (50.8%) don't provide a complete privacy policy and 1,128 skills (95%) have GDPR non-compliance issues in their privacy policies. Meanwhile, we find that the GDPR has a positive influence on European privacy policies. | Song Liao, Mohammed Aldeen, Jingwen Yan, Long Cheng, Xiapu Luo, Haipeng Cai, Hongxin Hu |  |
| 610 |  |  [Differentially Private Selection from Secure Distributed Computing](https://doi.org/10.1145/3589334.3645435) |  | 0 | Given a collection of vectors $x^{(1)},\dots,x^{(n)} \in \{0,1\}^d$, the selection problem asks to report the index of an "approximately largest" entry in $x=\sum_{j=1}^n x^{(j)}$. Selection abstracts a host of problems--in machine learning it can be used for hyperparameter tuning, feature selection, or to model empirical risk minimization. We study selection under differential privacy, where a released index guarantees privacy for each vectors. Though selection can be solved with an excellent utility guarantee in the central model of differential privacy, the distributed setting lacks solutions. Specifically, strong privacy guarantees with high utility are offered in high trust settings, but not in low trust settings. For example, in the popular shuffle model of distributed differential privacy, there are strong lower bounds suggesting that the utility of the central model cannot be obtained. In this paper we design a protocol for differentially private selection in a trust setting similar to the shuffle model--with the crucial difference that our protocol tolerates corrupted servers while maintaining privacy. Our protocol uses techniques from secure multi-party computation (MPC) to implement a protocol that: (i) has utility on par with the best mechanisms in the central model, (ii) scales to large, distributed collections of high-dimensional vectors, and (iii) uses $k\geq 3$ servers that collaborate to compute the result, where the differential privacy holds assuming an honest majority. Since general-purpose MPC techniques are not sufficiently scalable, we propose a novel application of integer secret sharing, and evaluate the utility and efficiency of our protocol theoretically and empirically. Our protocol is the first to demonstrate that large-scale differentially private selection is possible in a distributed setting. | Ivan Damgård, Hannah Keller, Boel Nelson, Claudio Orlandi, Rasmus Pagh |  |
| 611 |  |  [The Dynamics of (Not) Unfollowing Misinformation Spreaders](https://doi.org/10.1145/3589334.3645445) |  | 0 | Many studies explore how people 'come into' misinformation exposure. But much less is known about how people 'come out of' misinformation exposure. Do people organically sever ties to misinformation spreaders? And what predicts doing so? Over six months, we tracked the frequency and predictors of 1M followers unfollowing 5K health misinformation spreaders on Twitter. We found that misinformation ties are persistent. Monthly unfollowing rates are just 0.52 Users are also 31 they are to unfollow misinformation spreaders. Although generally infrequent, the factors most associated with unfollowing misinformation spreaders are (1) redundancy and (2) ideology. First, users initially following many spreaders, or who follow spreaders that tweet often, are most likely to unfollow later. Second, liberals are more likely to unfollow than conservatives. Overall, we observe strong persistence of misinformation ties. The fact that users rarely unfollow misinformation spreaders suggests a need for external nudges and the importance of preventing exposure from arising in the first place. | Joshua Ashkinaze, Eric Gilbert, Ceren Budak |  |
| 612 |  |  [Federated Learning Vulnerabilities: Privacy Attacks with Denoising Diffusion Probabilistic Models](https://doi.org/10.1145/3589334.3645514) |  | 0 | Federal Learning (FL) is highly respected for protecting data privacy in a distributed environment. However, the correlation between the updated gradient and the training data opens up the possibility of data reconstruction for malicious attackers, thus threatening the basic privacy requirements of FL. Previous research on such attacks mainly focuses on two main perspectives: one exclusively relies on gradient attacks, which performs well on small-scale data but falter with large-scale data; the other incorporates images prior but faces practical implementation challenges. So far, the effectiveness of privacy leakage attacks in FL is still far from satisfactory. In this paper, we introduce the Gradient Guided Diffusion Model (GGDM), a novel learning-free approach based on a pre-trained unconditional Denoising Diffusion Probabilistic Models (DDPM), aimed at improving the effectiveness and reducing the difficulty of implementing gradient based privacy attacks on complex networks and high-resolution images. To the best of our knowledge, this is the first work to employ the DDPM for privacy leakage attacks of FL. GGDM capitalizes on the unique nature of gradients and guides DDPM to ensure that reconstructed images closely mirror the original data. In addition, in GGDM, we elegantly combine the gradient similarity function with the Stochastic Differential Equation (SDE) to guide the DDPM sampling process based on theoretical analysis, and further reveal the impact of common similarity functions on data reconstruction. Extensive evaluation results demonstrate the excellent generalization ability of GGDM. Specifically, compared with state-of-the-art methods, GGDM shows clear superiority in both quantitative metrics and visualization, significantly enhancing the reconstruction quality of privacy attacks. | Hongyan Gu, Xinyi Zhang, Jiang Li, Hui Wei, Baiqi Li, Xinli Huang |  |
| 613 |  |  [Fair Graph Representation Learning via Sensitive Attribute Disentanglement](https://doi.org/10.1145/3589334.3645532) |  | 0 | Group fairness for Graph Neural Networks (GNNs), which emphasizes algorithmic decisions neither favoring nor harming certain groups defined by sensitive attributes (e.g., race and gender), has gained considerable attention. In particular, the objective of group fairness is to ensure that the decisions made by GNNs are independent of the sensitive attribute. To achieve this objective, most existing approaches involve eliminating sensitive attribute information in node representations or algorithmic decisions. However, such ways may also eliminate task-related information due to its inherent correlation with the sensitive attribute, leading to a sacrifice in utility. In this work, we focus on improving the fairness of GNNs while preserving task-related information and propose a fair GNN framework named FairSAD. Instead of eliminating sensitive attribute information, FairSAD enhances the fairness of GNNs via Sensitive Attribute Disentanglement (SAD), which separates the sensitive attribute-related information into an independent component to mitigate its impact. Additionally, FairSAD utilizes a channel masking mechanism to adaptively identify the sensitive attribute-related component and subsequently decorrelates it. Overall, FairSAD minimizes the impact of the sensitive attribute on GNN outcomes rather than eliminating sensitive attributes, thereby preserving task-related information associated with the sensitive attribute. Furthermore, experiments conducted on several real-world datasets demonstrate that FairSAD outperforms other state-of-the-art methods by a significant margin in terms of both fairness and utility performance. Our source code is available at https://github.com/ZzoomD/FairSAD. | Yuchang Zhu, Jintang Li, Zibin Zheng, Liang Chen |  |
| 614 |  |  [DPAR: Decoupled Graph Neural Networks with Node-Level Differential Privacy](https://doi.org/10.1145/3589334.3645531) |  | 0 | Graph Neural Networks (GNNs) have achieved great success in learning with graph-structured data. Privacy concerns have also been raised for the trained models which could expose the sensitive information of graphs including both node features and the structure information. In this paper, we aim to achieve node-level differential privacy (DP) for training GNNs so that a node and its edges are protected. Node DP is inherently difficult for GNNs because all direct and multi-hop neighbors participate in the calculation of gradients for each node via layer-wise message passing and there is no bound on how many direct and multi-hop neighbors a node can have, so existing DP methods will result in high privacy cost or poor utility due to high node sensitivity. We propose a Decoupled GNN with Differentially Private Approximate Personalized PageRank (DPAR) for training GNNs with an enhanced privacy-utility tradeoff. The key idea is to decouple the feature projection and message passing via a DP PageRank algorithm which learns the structure information and uses the top-K neighbors determined by the PageRank for feature aggregation. By capturing the most important neighbors for each node and avoiding the layer-wise message passing, it bounds the node sensitivity and achieves improved privacy-utility tradeoff compared to layer-wise perturbation based methods. We theoretically analyze the node DP guarantee for the two processes combined together and empirically demonstrate better utilities of DPAR with the same level of node DP compared with state-of-the-art methods. | Qiuchen Zhang, HongKyu Lee, Jing Ma, Jian Lou, Carl Yang, Li Xiong |  |
| 615 |  |  [A Worldwide View on the Reachability of Encrypted DNS Services](https://doi.org/10.1145/3589334.3645539) |  | 0 | To protect user DNS privacy, four DNS over Encryption (DoE) protocols have been proposed, including DNS over TLS (DoT), DNS over HTTPS (DoH), DNS over QUIC (DoQ), and DNS over HTTP/3 (DoH3). Ensuring reachability stands as a prominent prerequisite for the proper functionality of these DoE protocols, driving considerable efforts in this domain. However, existing studies predominantly concentrate on a limited number of DoT/DoH domains or employ a restricted subset of vantage points (VPs). In this paper, we present the first comprehensive worldwide view of DoE service reachability. By collecting data from our 15-month-long scan, we elaborately built a list of 1302 operational DoE domains as measurement targets, 448 of which support IPv6. Then we performed 10M DoE over IPv4 (DoEv4) and 570K DoE over IPv6 (DoEv6) queries from 5K VPs over two months, encompassing 102 countries/regions. Our results reveal that the reachability of DoE services is poor in some countries/regions. Specifically, 592K (5.92%) DoEv4 queries and 28K (4.91%) DoEv6 queries are blocked. In countries/regions with strict Internet control, DoEv4 service blocking often occurs during TCP connection and QUIC version negotiation. Compared to DoEv4, the reachability of DoEv6 services is better. In particular, some DoE blocking policies target only specific IP addresses or DoE protocols, providing clients with the opportunity to access blocked DoE domains. Our study highlights the need for the DNS community to pay attention and improve the reachability of DoE services. | Ruixuan Li, Baojun Liu, Chaoyi Lu, Haixin Duan, Jun Shao |  |
| 616 |  |  [Contrastive Fingerprinting: A Novel Website Fingerprinting Attack over Few-shot Traces](https://doi.org/10.1145/3589334.3645575) |  | 0 | Website Fingerprinting (WF) attacks enable passive adversaries to identify the website a user visits over encrypted or anonymized network connections. WF attacks based on deep learning have achieved high accuracy in identifying websites based on abundant training traffic traces per website. However, collecting large-scale and fresh traces is quite cost-consuming and unrealistic. Morevoer, these deep-learning-based WF attacks lack flexibility because they require a long bootstrap time for retraining when facing new traffic traces with different distributions or newly added monitored websites. This paper proposes a high-accuracy WF attack named Contrastive Fingerprinting (CF), which leverages contrastive learning and data augmentation over a few training traces. The results of extensive experiments on challenging datasets over few-shot traces demonstrate the high accuracy of the CF attack and its robustness against WF defenses. For example, when each monitored website only has 20 training traces, CF identifies monitored websites with a high accuracy of 90.4% in the closed-world scenario and distinguishes monitored websites with a high True Positive Rate of 91.2% in the open-world scenario. The experimental results also show that CF outperforms two existing WF attacks with few-shot traces under different network conditions in real-world applications. | Yi Xie, Jiahao Feng, Wenju Huang, Yixi Zhang, Xueliang Sun, Xiaochou Chen, Xiapu Luo |  |
| 617 |  |  [Analyzing Ad Exposure and Content in Child-Oriented Videos on YouTube](https://doi.org/10.1145/3589334.3645585) |  | 0 | As a popular choice for video and entertainment streaming, YouTube hosts a large audience, including children, who form a growing proportion of its users. Despite separate "made for kids" labelling and stricter moderation of these videos, inappropriate advertising remains a concern as it threatens the safety of YouTube for young viewers. This paper is the first comparative measurement study that explores how advertisement exposure and content vary across child-oriented videos on YouTube. We do this by conducting a cross-regional advertisement analysis on highly viewed "made for kids" labelled content across a total of ten countries with varying regulation. A second front of comparison is carried out between ad patterns on unlabelled and labelled child-oriented videos. Our analysis reveals that the safety of a child's YouTube experience is shaped significantly by their external environment. There also appears to be lax enforcement of YouTube ad and child protection policies, indicated by the presence of unlabelled child-oriented content with weak ad regulation. We discuss the implications of inappropriate exposure on children and suggest policy and implementation measures to mitigate this threat. | Emaan Bilal Khan, Nida Tanveer, Aima Shahid, Mohammad Jaffer Iqbal, Haashim Ali Mirza, Armish Javed, Ihsan Ayyub Qazi, Zafar Ayyub Qazi |  |
| 618 |  |  [A Study of GDPR Compliance under the Transparency and Consent Framework](https://doi.org/10.1145/3589334.3645618) |  | 0 | This paper presents a study of GDPR compliance under the Interactive Advertising Bureau Europe's Transparency and Consent Framework (TCF). This framework provides digital advertising market participants a standard for sharing users' privacy consent choices. TCF is widely used across the Internet, and this paper presents a thorough experimental evaluation of both the compliance of websites with TCF and its impact on user privacy. We reviewed 2,230 websites that use TCF and accepted the automatic decline of user consent by our data collection system. Unlike previous work on GDPR compliance, we found that most websites using TCF properly record the user's consent choice. However, we found that 72.8% of the websites that were TCF compliant claimed legitimate interest as a rationale for overriding the consent choice. While legitimate interest is legal under GDPR, previous studies have shown that most users disagreed with how it is being used to collect data. Additionally, analysis of cookies set to the browsers indicates that TCF may not fully protect user privacy even when websites are compliant. Our research provides regulators and publishers with a data collection and analysis system to monitor compliance, detect non-compliance, and examine questionable practices of circumventing user consent choices using legitimate interest. | Michael Smith, Antonio TorresAgüero, Riley Grossman, Pritam Sen, Yi Chen, Cristian Borcea |  |
| 619 |  |  [Breaking the Trilemma of Privacy, Utility, and Efficiency via Controllable Machine Unlearning](https://doi.org/10.1145/3589334.3645669) |  | 0 | Machine Unlearning (MU) algorithms have become increasingly critical due to the imperative adherence to data privacy regulations. The primary objective of MU is to erase the influence of specific data samples on a given model without the need to retrain it from scratch. Accordingly, existing methods focus on maximizing user privacy protection. However, there are different degrees of privacy regulations for each real-world web-based application. Exploring the full spectrum of trade-offs between privacy, model utility, and runtime efficiency is critical for practical unlearning scenarios. Furthermore, designing the MU algorithm with simple control of the aforementioned trade-off is desirable but challenging due to the inherent complex interaction. To address the challenges, we present Controllable Machine Unlearning (ConMU), a novel framework designed to facilitate the calibration of MU. The ConMU framework contains three integral modules: an important data selection module that reconciles the runtime efficiency and model generalization, a progressive Gaussian mechanism module that balances privacy and model generalization, and an unlearning proxy that controls the trade-offs between privacy and runtime efficiency. Comprehensive experiments on various benchmark datasets have demonstrated the robust adaptability of our control mechanism and its superiority over established unlearning methods. ConMU explores the full spectrum of the Privacy-Utility-Efficiency trade-off and allows practitioners to account for different real-world regulations. Source code available at: https://github.com/guangyaodou/ConMU | Zheyuan Liu, Guangyao Dou, Eli Chien, Chunhui Zhang, Yijun Tian, Ziwei Zhu |  |
| 620 |  |  [Heterogeneous Subgraph Transformer for Fake News Detection](https://doi.org/10.1145/3589334.3645680) |  | 0 | Fake news is pervasive on social media, inflicting substantial harm on public discourse and societal well-being. We investigate the explicit structural information and textual features of news pieces by constructing a heterogeneous graph concerning the relations among news topics, entities, and content. Through our study, we reveal that fake news can be effectively detected in terms of the atypical heterogeneous subgraphs centered on them, which encapsulate the essential semantics and intricate relations between news elements. However, suffering from the heterogeneity, exploring such heterogeneous subgraphs remains an open problem. To bridge the gap, this work proposes a heterogeneous subgraph transformer (HeteroSGT) to exploit subgraphs in our constructed heterogeneous graph. In HeteroSGT, we first employ a pre-trained language model to derive both word-level and sentence-level semantics. Then the random walk with restart (RWR) is applied to extract subgraphs centered on each news, which are further fed to our proposed subgraph Transformer to quantify the authenticity. Extensive experiments on five real-world datasets demonstrate the superior performance of HeteroSGT over five baselines. Further case and ablation studies validate our motivation and demonstrate that performance improvement stems from our specially designed components. | Yuchen Zhang, Xiaoxiao Ma, Jia Wu, Jian Yang, Hao Fan |  |
| 621 |  |  [Experimental Security Analysis of Sensitive Data Access by Browser Extensions](https://doi.org/10.1145/3589334.3645683) |  | 0 | Browser extensions offer a variety of valuable features and functionalities. They also pose a significant security risk if not properly designed or reviewed. Prior works have shown that browser extensions can access and manipulate data fields, including sensitive data such as passwords, credit card numbers, and Social Security numbers. In this paper, we present an empirical study of the security risks posed by browser extensions. Specifically, we first build a proof-of-concept extension that can steal sensitive user information. We find that the extension passes the Chrome Webstore review process. We then perform a measurement study on the top 10K website login pages to check if the extension access to password fields via JS. We find that none of the password fields are actively protected, and can be accessed using JS. Moreover, we found that 1K websites store passwords in plaintext in their page source, including popular websites like Google.com and Cloudflare.com. We also analyzed over 160K Chrome Web Store extensions for malicious behavior, finding that 28K have permission to access sensitive fields and 190 store password fields in variables. To analyze the behavioral workflow of the potentially malicious extensions, we propose an LLM-driven framework, Extension Reviewer. Finally, we discuss two countermeasures to address these risks: a bolt-on JavaScript package for immediate adoption by website developers allowing them to protect sensitive input fields, and a browser-level solution that alerts users when an extension accesses sensitive input fields. Our research highlights the urgent need for improved security measures to protect sensitive user information online. | Asmit Nayak, Rishabh Khandelwal, Earlence Fernandes, Kassem Fawaz |  |
| 622 |  |  [Automating Website Registration for Studying GDPR Compliance](https://doi.org/10.1145/3589334.3645709) |  | 0 | Investigating how websites use sensitive user data is an active research area. However, research based on automated measurements has been limited to those websites that do not require user authentication. To overcome this limitation, we developed a crawler that automates website registrations and newsletter subscriptions and detects both security and privacy threats at scale. We demonstrate our crawler's capabilities by running it on 660k websites. We use this to identify security and privacy threats and to contextualize them within EU laws, namely the General Data Protection Regulation and ePrivacy Directive. Our methods detect private data collection over insecure HTTP connections and websites sending emails with user-provided passwords. We are also the first to apply machine learning to web forms, assessing violations of marketing consent collection requirements. Overall, we find that 37.2% of websites send marketing emails without proper user consent. This is mostly caused by websites failing both to verify and store consent adequately. Additionally, 1.8% of websites share users' email addresses with third parties without a transparent disclosure. | Karel Kubicek, Jakob Merane, Ahmed Bouhoula, David A. Basin |  |
| 623 |  |  [Generating Multi-turn Clarification for Web Information Seeking](https://doi.org/10.1145/3589334.3645712) |  | 0 | Asking multi-turn clarifying questions has been applied in various conversational search systems to help recommend people, commodities, and images to users. However, its importance is still not emphasized in the Web search. In this paper, we make a step to extend the multi-turn clarification generation to Web search for clarifying users' ambiguous or faceted intents. Compared with other conversational search scenarios, Web search queries are more complicated, so clarification should be generated instead of being selected which is commonly applied in current studies. To this end, we first define the whole process of multi-turn Web search clarification composed of clarification candidate generation, optimal clarification selection, and document retrieval. Due to the lack of multi-turn open-domain clarification data, we first design a simple yet effective rule-based method to fit the above three components. After that, by utilizing the in-context learning and zero-shot instruction ability of large language models (LLMs), we implement clarification generation and selection by prompting LLMs with demonstrations and declarations, further improving the clarification effectiveness. To evaluate our proposed methods, we first measure whether our methods can improve the ability to retrieve documents. We also evaluate the quality of generated candidate facets. Experimental results show that, compared with existing single-turn methods for Web search clarification, our proposed framework is more suitable for open-domain Web search systems in asking multi-turn clarification questions to clarify users' ambiguous or faceted intents. | Ziliang Zhao, Zhicheng Dou |  |
| 624 |  |  [Advancing Web 3.0: Making Smart Contracts Smarter on Blockchain](https://doi.org/10.1145/3589334.3645319) |  | 0 | Blockchain and smart contracts are one of the key technologies promoting Web 3.0. However, due to security considerations and consistency requirements, smart contracts currently only support simple and deterministic programs, which significantly hinders their deployment in intelligent Web 3.0 applications. To enhance smart contracts intelligence on the blockchain, we propose SMART, a plug-in smart contract framework that supports efficient AI model inference while being compatible with existing blockchains. To handle the high complexity of model inference, we propose an on-chain and off-chain joint execution model, which separates the SMART contract into two parts: the deterministic code still runs inside an on-chain virtual machine, while the complex model inference is offloaded to off-chain compute nodes. To solve the non-determinism brought by model inference, we leverage Trusted Execution Environments (TEEs) to endorse the integrity and correctness of the off-chain execution. We also design distributed attestation and secret key provisioning schemes to further enhance the system security and model privacy. We implement a SMART prototype and evaluate it on a popular Ethereum Virtual Machine (EVM)-based blockchain. Theoretical analysis and prototype evaluation show that SMART not only achieves the security goals of correctness, liveness, and model privacy, but also has approximately 5 orders of magnitude faster inference efficiency than existing on-chain solutions. | Junqin Huang, Linghe Kong, Guanjie Cheng, Qiao Xiang, Guihai Chen, Gang Huang, Xue Liu |  |
| 625 |  |  [From Promises to Practice: Evaluating the Private Browsing Modes of Android Browser Apps](https://doi.org/10.1145/3589334.3645320) |  | 0 | Private browsing is a common feature of web browsers on desktop platforms. This feature protects the privacy of users browsing the Internet and, therefore, is widely welcomed by users. In recent years, with the popularity of smartphones, the private browsing mode has been introduced into mobile browsers. However, its deployment on mobile platforms has not been well evaluated. To bridge the gap, in this work, we systemically studied the private browsing modes of Android browser apps. Specifically, we proposed six private rules for mobile browsers to follow by combining the mobile browsing features with the previous research on private browsing. Furthermore, we designed an automated analysis framework, BroDroid, to detect whether mobile browsers violate these rules. Also, with BroDroid, we evaluated 49 popular browser apps crawled from Google Play. Finally, BroDroid successfully identified 58 violations, some of which come from the promised capabilities of the browser. We reported our discovered issues to the corresponding developers, and four of them (Yandex Browser, Mint Browser, Web Explorer, and Net Fast Web Browser) have acknowledged our findings. Our observation may be the tip of the iceberg, and more efforts should be put into improving the privacy protections of mobile browsers. | Xiaoyin Liu, Wenzhi Li, Qinsheng Hou, Shishuai Yang, Lingyun Ying, Wenrui Diao, Yanan Li, Shanqing Guo, Haixin Duan |  |
| 626 |  |  [Interface Illusions: Uncovering the Rise of Visual Scams in Cryptocurrency Wallets](https://doi.org/10.1145/3589334.3645348) |  | 0 | Cryptocurrencies, while revolutionary, have become a magnet for malicious actors. With numerous reports underscoring cyberattacks and scams in this domain, our paper takes the lead in characterizing visual scams associated with cryptocurrency wallets---a fundamental component of Web3. Specifically, scammers capitalize on the omission of vital wallet interface details, such as token symbols, wallet addresses, and smart contract function names, to mislead users, potentially resulting in unintended financial losses. Analyzing Ethereum blockchain transactions from July 2022 to June 2023, we uncovered a total of 24,901,115 visual scam incidents, which include 3,585,493 counterfeit token attacks, 21,281,749 zero-transfer attacks, and 33,873 function name attacks, orchestrated by 6,768 distinct attackers. Shockingly, over 28,414 victims fell prey to these scams, with losses surpassing 27 million USD. This alarming data underscores the pressing need for robust protective measures. By profiling the typical victims and attackers, we are able to propose mitigation strategies informed by our findings. | Guoyi Ye, Geng Hong, Yuan Zhang, Min Yang |  |
| 627 |  |  [Trident: A Universal Framework for Fine-Grained and Class-Incremental Unknown Traffic Detection](https://doi.org/10.1145/3589334.3645407) |  | 0 | To detect unknown attack traffic, anomaly-based network intrusion detection systems (NIDSs) are widely used in Internet infrastructure. However, the security communities realize some limitations when they put most existing proposals into practice. The challenges are mainly concerned with (i) fine-grained emerging attack detection and (ii) incremental updates/adaptations. To tackle these problems, we propose to decouple the need for model capabilities by transforming known/new class identification issues into multiple independent one-class learning tasks. Based on the above core ideas, we develop Trident, a universal framework for fine-grained unknown encrypted traffic detection. It consists of three main modules, i.e., tSieve, tScissors, and tMagnifier are used for profiling traffic, determining outlier thresholds, and clustering respectively, each of which supports custom configuration. Using four popular datasets of network traces, we show that Trident significantly outperforms 16 state-of-the-art (SOTA) methods. Furthermore, a series of experiments (concept drift, overhead/parameter evaluation) demonstrate the stability, scalability, and practicality of Trident. | Ziming Zhao, Zhaoxuan Li, Zhuoxue Song, Wenhao Li, Fan Zhang |  |
| 628 |  |  [SSI, from Specifications to Protocol? Formally Verify Security!](https://doi.org/10.1145/3589334.3645426) |  | 0 | We evaluate a bundle of specifications from the Self-Sovereign Identity (SSI) paradigm to construct an authentication protocol for the Web. We demonstrate how relevant standards such as W3C Verifiable Credentials (VC), W3C Decentralised Identifiers (DIDs), and components of the Hyperledger Aries Framework are to be assembled methodologically into a protocol. We make those assumptions from standard trust models explicit that underlie the derived protocol, and verify security and privacy properties, notably secrecy, authentication, and unlinkability. This enables us to formally justify the additional precision that we urge these specifications to consider, to ensure that implementors of SSI-based systems do not neglect security-critical controls. | Christoph H.J. Braun, Ross Horne, Tobias Käfer, Sjouke Mauw | University of Strathclyde, Glasgow, United Kingdom; Karlsruhe Institute of Technology, Karlsruhe, Germany; University of Luxemburg, Esch-sur-Alzette, Luxembourg |
| 629 |  |  [Blockchain Censorship](https://doi.org/10.1145/3589334.3645431) |  | 0 | Permissionless blockchains promise to be resilient against censorship by a single entity. This suggests that deterministic rules, and not third-party actors, are responsible for deciding if a transaction is appended to the blockchain or not. In 2022, the U.S. Office of Foreign Assets Control (OFAC) sanctioned a Bitcoin mixer and an Ethereum application, putting the neutrality of permissionless blockchains to the test. In this paper, we formalize quantify and analyze the security impact of blockchain censorship. We start by defining censorship, followed by a quantitative assessment of current censorship practices. We find that 46% of Ethereum blocks were made by censoring actors that intend to comply with OFAC sanctions, indicating the significant impact of OFAC sanctions on the neutrality of public blockchains. We further uncover that censorship not only impacts neutrality, but also security. We show how after Ethereum's move to Proof-of-Stake (PoS) and adoption of Proposer-Builder Separation (PBS) the inclusion of censored transactions was delayed by an average of 85%. Inclusion delays compromise a transaction's security by, e.g., strengthening a sandwich adversary. Finally we prove a fundamental limitation of PoS and Proof-of-Work (PoW) protocols against censorship resilience. | Anton Wahrstätter, Jens Ernstberger, Aviv Yaish, Liyi Zhou, Kaihua Qin, Taro Tsuchiya, Sebastian Steinhorst, Davor Svetinovic, Nicolas Christin, Mikolaj Barczentewicz, Arthur Gervais |  |
| 630 |  |  [GRASP: Hardening Serverless Applications through Graph Reachability Analysis of Security Policies](https://doi.org/10.1145/3589334.3645436) |  | 0 | Serverless computing is supplanting past versions of cloud computing as the easiest way to rapidly prototype and deploy applications. However, the reentrant and ephemeral nature of serverless functions only exacerbates the challenge of correctly specifying security policies. Unfortunately, with role-based access control solutions like Amazon Identity and Access Management (IAM) already suffering from pervasive misconfiguration problems, the likelihood of policy failures in serverless applications is high. In this work, we introduce GRASP, a graph-based analysis framework for modeling serverless access control policies as queryable reachability graphs. GRASP generates reusable models that represent the principals of a serverless application and the interactions between those principals. We implement GRASP for Amazon IAM in Prolog, then deploy it on a corpus of 731 open source Amazon Lambda applications. We find that serverless policies tend to be short and highly permissive, e.g., 92% of surveyed policies are comprised of just 10 statements and 30% exhibit full reachability between all application functions and resources. We then use GRASP to identify potential attack vectors permitted by these policies, including hundreds of sensitive access channels, a dozen publicly-exposed resources, and four channels that may permit an attacker to exfiltrate an application's private resources through one of its public resources. These findings demonstrate GRASP's utility as a means of identifying opportunities for hardening application policies and highlighting potential exfiltration channels. | Isaac Polinsky, Pubali Datta, Adam Bates, William Enck |  |
| 631 |  |  [Divide, Conquer, and Coalesce: Meta Parallel Graph Neural Network for IoT Intrusion Detection at Scale](https://doi.org/10.1145/3589334.3645457) |  | 0 | This paper proposes Meta Parallel Graph Neural Network (MPGNN) to establish a scalable Network Intrusion Detection System (NIDS) for large-scale Internet of Things (IoT) networks. MPGNN leverages a meta-learning framework to optimize the parallelism of GNN-based NIDS. The core of MPGNN is a coalition formation policy that generates meta-knowledge for partitioning a massive graph into multiple coalitions/subgraphs in a way that maximizes the performance and efficiency of parallel coalitional NIDSs. We propose an offline reinforcement learning algorithm, called Graph-Embedded Adversarially Trained Actor-Critic (G-ATAC), to learn a coalition formation policy that jointly optimizes intrusion detection accuracy, communication overheads, and computational complexities of coalitional NIDSs. In particular, G-ATAC learns to capture the temporal dependencies of network states and coalition formation decisions over offline data, eliminating the need for expensive online interactions with large IoT networks. Given generated coalitions, MPGNN employs E-GraphSAGE to establish coalitional NIDSs which then collaborate via ensemble prediction to accomplish intrusion detection for the entire network. We evaluate MPGNN on two real-world datasets. The experimental results demonstrate the superiority of our method with substantial improvements in F1 score, surpassing the state-of-the-art methods by 0.38 and 0.29 for the respective datasets. Compared to the centralized NIDS, MPGNN reduces the training time of NIDS by 41.63% and 22.11%, while maintaining an intrusion detection performance comparable to centralized NIDS. | Hua Ding, Lixing Chen, Shenghong Li, Yang Bai, Pan Zhou, Zhe Qu |  |
| 632 |  |  [Medusa: Unveil Memory Exhaustion DoS Vulnerabilities in Protocol Implementations](https://doi.org/10.1145/3589334.3645476) |  | 0 | Web services have brought great convenience to our daily lives. Meanwhile, they are vulnerable to Denial-of-Service (DoS) attacks. DoS attacks launched via vulnerabilities in the services can cause great harm. The vulnerabilities in protocol implementations are especially important because they are the keystones of web services. One vulnerable protocol implementation can affect all the web services built on top of it. Compared to the vulnerabilities that cause the target service to crash, resource exhaustion vulnerabilities are equally if not more important. This is because such vulnerabilities can deplete the system resources, leading to the unavailability of not only the vulnerable service but also other services running on the same machine. Despite the significance of this type of vulnerability, there has been limited research in this area. In this paper, we propose Medusa, a dynamic analysis framework to detect memory exhaustion vulnerabilities in protocol implementations, which are the most common type of resource exhaustion vulnerabilities. Medusa works in two phases: exploration phase and verification. In the exploration phase, a protocol property graph (PPG) is constructed to embed the states with relevant properties including memory consumption information. In the verification phase, the PPG is used to simulate DoS attacks to verify the vulnerabilities. We implemented Medusa and evaluated its performance on 21 implementations of five protocols. The results demonstrate that Medusa outperforms the state-of-the-art techniques by discovering overall 127× maximum memory consumption. Lastly, Medusa has discovered six 0-day vulnerabilities in six protocol implementations for three protocols. Particularly, one of the vulnerabilities was found in Eclipse Mosquitto, which can affect thousands of services and it has been assigned with a CVE ID. | Zhengjie Du, Yuekang Li, Yaowen Zheng, Xiaohan Zhang, Cen Zhang, Yi Liu, Sheikh Mahbub Habib, Xinghua Li, Linzhang Wang, Yang Liu, Bing Mao |  |
| 633 |  |  [ContraMTD: An Unsupervised Malicious Network Traffic Detection Method based on Contrastive Learning](https://doi.org/10.1145/3589334.3645479) |  | 0 | Malicious traffic detection has been a focal point in the field of network security, and deep learning-based approaches are emerging as a new paradigm. However, most of them are supervised methods, which highly depend on well-labeled data, and fail to handle unknown or continuously evolving attacks. Unsupervised methods alleviate the need for labeled data, but existing methods are often limited to detecting anomalies either in vertical perspective through historical comparisons or in horizontal perspective by comparing with concurrent entities. Relying on data from a single perspective is unreliable, and it limits the model's accuracy and generalizability. In this paper, we propose a novel method ContraMTD based on contrastive learning, which comprehensively considers both vertical and horizontal perspectives. ContraMTD extracts local behavior features and global interaction features from normal network traffic by proposed SEC and DE-GAT respectively, then employs contrastive learning to learn the relationship, especially consistency between them, and finally detects malicious traffic through a multi-round scoring approach. We conduct extensive experiments on three datasets, including a self-collected dataset, and the results demonstrate that our method outperforms many state-of-the-art methods in the domain of unsupervised malicious traffic detection. | Xueying Han, Susu Cui, Jian Qin, Song Liu, Bo Jiang, Cong Dong, Zhigang Lu, Baoxu Liu |  |
| 634 |  |  [Unfiltered: Measuring Cloud-based Email Filtering Bypasses](https://doi.org/10.1145/3589334.3645499) |  | 0 | Email service has increasingly been outsourced to cloud-based providers and so too has the task of filtering such messages for potential threats. Thus, customers will commonly direct that their incoming email is first sent to a third-party email filtering service (e.g., Proofpoint or Barracuda) and only the "clean" messages are then sent on to their email hosting provider (e.g., Gmail or Microsoft Exchange Online). However, this loosely coupled approach can, in theory, be bypassed if the email hosting provider is not configured to only accept messages that arrive from the email filtering service. In this paper we demonstrate that such bypasses are commonly possible. We document a multi-step methodology to infer if an organization has correctly configured its email hosting provider to guard against such scenarios. Then, using an empirical measurement of edu and com domains as a case study, we show that 80% of such organizations making use of popular cloud-based email filtering services can be bypassed in this manner. We also discuss reasons that lead to such misconfigurations and outline challenges in hardening the binding between email filtering and hosting providers. | Sumanth Rao, Enze Liu, Grant Ho, Geoffrey M. Voelker, Stefan Savage |  |
| 635 |  |  [RecurScan: Detecting Recurring Vulnerabilities in PHP Web Applications](https://doi.org/10.1145/3589334.3645530) |  | 0 | Detecting recurring vulnerabilities has become a popular means of static vulnerability detection in recent years because they do not require labor-intensive vulnerability modeling. Recently, a body of work, with HiddenCPG as a representative, has redefined the problem of statically identifying recurring vulnerabilities as the subgraph isomorphism problem. More specifically, these approaches represent known vulnerable code as graph-based structures (e.g., PDG or CPG), and then identify subgraphs within target applications that match the vulnerable graphs. However, since these methods are highly sensitive to changes in the code graph, they may miss a significant number of recurring vulnerabilities with slight code differences from known vulnerabilities. In this paper, we propose a novel approach, namely RecurScan, which can accurately detect recurring vulnerabilities with resilience to code differences. To achieve this goal, RecurScan works around security patches and symbolic tracking techniques, detecting recurring vulnerabilities by comparing symbolic expressions and selective constraints between the target applications and known vulnerabilities. Benefiting from this design, RecurScan can tolerate the code differences arising from complex data or control flows within the applications. We evaluated RecurScan on 200 popular PHP web applications using 184 known vulnerability patches. The results demonstrate that RecurScan discovered 232 previously unknown vulnerabilities, 174 of which were assigned CVE identifiers, outperforming state-of-the-art approach (i.e., HiddenCPG) by 25.98% in precision and 87.09% in recall. | Youkun Shi, Yuan Zhang, Tianhao Bai, Lei Zhang, Xin Tan, Min Yang |  |
| 636 |  |  [Phishing Vs. Legit: Comparative Analysis of Client-Side Resources of Phishing and Target Brand Websites](https://doi.org/10.1145/3589334.3645535) |  | 0 | Phishing attacks have persistently remained a prevalent and widespread cybersecurity threat for several years. This leads to numerous endeavors aimed at comprehensively understanding the phishing attack ecosystem, with a specific focus on presenting new attack tactics and defense mechanisms against phishing attacks. Unfortunately, little is known about how client-side resources (e.g., JavaScript libraries) are used in phishing websites, compared to those in their corresponding legitimate target brand websites. This understanding can help us gain insights into the construction and techniques of phishing websites and phishing attackers' behaviors when building phishing websites. In this paper, we gain a deeper understanding of how client-side resources (especially, JavaScript libraries) are used in phishing websites by comparing them with the resources used in the legitimate target websites. For our study, we collect both client-side resources from phishing websites and their corresponding legitimate target brand websites for 25 months: 3.4M phishing websites (1.1M distinct phishing domains). Our study reveals that phishing websites tend to employ more diverse JavaScript libraries than their legitimate websites do. However, these libraries in phishing websites are older (nearly 21.2 months) and distinct in comparison. For example, Socket.IO is uniquely used in phishing websites to send victims' information to an external server in real time. Furthermore, we find that a considerable portion of them still maintain a basic and simplistic structure (e.g., simply displaying a login form or image), while phishing websites have significantly evolved to bypass anti-phishing measures. Finally, through HTML structure and style similarities, we can identify specific target webpages of legitimate brands that phishing attackers reference and use to mimic for their phishing attacks. | Kyungchan Lim, Jaehwan Park, Doowon Kim |  |
| 637 |  |  [Detecting and Understanding Self-Deleting JavaScript Code](https://doi.org/10.1145/3589334.3645540) |  | 0 | Self-deletion is a well-known strategy frequently utilized by malware to evade detection. Recently, this technique has found its way into client-side JavaScript code, significantly raising the complexity of JavaScript analysis. In this work, we systematically study the emerging client-side JavaScript self-deletion behavior on the web. We tackle various technical challenges associated with JavaScript dynamic analysis and introduce JSRay, a browser-based JavaScript runtime monitoring system designed to comprehensively study client-side script deletion. We conduct a large-scale measurement of one million popular websites, revealing that script self-deletion is prevalent in the real world. While our findings indicate that most developers employ self-deletion for legitimate purposes, we also discover that self-deletion has already been employed together with other anti-analysis techniques for cloaking suspicious operations in client-side JavaScript. | Xinzhe Wang, Zeyang Zhuang, Wei Meng, James Cheng |  |
| 638 |  |  [Malicious Package Detection using Metadata Information](https://doi.org/10.1145/3589334.3645543) |  | 0 | Protecting software supply chains from malicious packages is paramount in the evolving landscape of software development. Attacks on the software supply chain involve attackers injecting harmful software into commonly used packages or libraries in a software repository. For instance, JavaScript uses Node Package Manager (NPM), and Python uses Python Package Index (PyPi) as their respective package repositories. In the past, NPM has had vulnerabilities such as the event-stream incident, where a malicious package was introduced into a popular NPM package, potentially impacting a wide range of projects. As the integration of third-party packages becomes increasingly ubiquitous in modern software development, accelerating the creation and deployment of applications, the need for a robust detection mechanism has become critical. On the other hand, due to the sheer volume of new packages being released daily, the task of identifying malicious packages presents a significant challenge. To address this issue, in this paper, we introduce a metadata-based malicious package detection model, MeMPtec. This model extracts a set of features from package metadata information. These extracted features are classified as either easy-to-manipulate (ETM) or difficult-to-manipulate (DTM) features based on monotonicity and restricted control properties. By utilising these metadata features, not only do we improve the effectiveness of detecting malicious packages, but also we demonstrate its resistance to adversarial attacks in comparison with existing state-of-the-art. Our experiments indicate a significant reduction in both false positives (up to 97.56 negatives (up to 91.86 | Sajal Halder, Michael Bewong, Arash Mahboubi, Yinhao Jiang, Md Rafiqul Islam, Md Zahidul Islam, Ryan HL Ip, Muhammad Ejaz Ahmed, Gowri Sankar Ramachandran, Muhammad Ali Babar |  |
| 639 |  |  [Unveiling the Invisible: Detection and Evaluation of Prototype Pollution Gadgets with Dynamic Taint Analysis](https://doi.org/10.1145/3589334.3645579) |  | 0 | For better or worse, JavaScript is the cornerstone of modern Web. Prototype-based languages like JavaScript are susceptible to prototype pollution vulnerabilities, enabling an attacker to inject arbitrary properties into an object's prototype. The attacker can subsequently capitalize on the injected properties by executing otherwise benign pieces of code, so-called gadgets, that perform security-sensitive operations. The success of an attack largely depends on the presence of gadgets, leading to high-profile exploits such as privilege escalation and arbitrary code execution (ACE). This paper proposes Dasty, the first semi-automated pipeline to help developers identify gadgets in their applications' software supply chain. Dasty targets server-side Node.js applications and relies on an enhancement of dynamic taint analysis which we implement with the dynamic AST-level instrumentation. Moreover, Dasty provides support for visualization of code flows with an IDE, thus facilitating the subsequent manual analysis for building proof-of-concept exploits. To illustrate the danger of gadgets, we use Dasty in a study of the most dependent-upon NPM packages to analyze the presence of gadgets leading to ACE. Dasty identifies 1,269 server-side packages, of which 631 have code flows that may reach dangerous sinks. We manually prioritize and verify the candidate flows to build proof-of-concept exploits for 49 NPM packages, including popular packages such as ejs, nodemailer and workerpool. To investigate how Dasty integrates with existing tools to find end-to-end exploits, we conduct an in-depth analysis of a popular data visualization dashboard to find one high-severity CVE-2023-31415 leading to remote code execution. For the first time, our results systematically demonstrate the dangers of server-side gadgets and call for further research to solve the problem. | Mikhail Shcherbakov, Paul Moosbrugger, Musard Balliu |  |
| 640 |  |  [ARTEMIS: Detecting Airdrop Hunters in NFT Markets with a Graph Learning System](https://doi.org/10.1145/3589334.3645597) |  | 0 | As Web3 projects leverage airdrops to incentivize participation, airdrop hunters tactically amass wallet addresses to capitalize on token giveaways. This poses challenges to the decentralization goal. Current detection approaches tailored for cryptocurrencies overlook non-fungible tokens (NFTs) nuances. We introduce ARTEMIS, an optimized graph neural network system for identifying airdrop hunters in NFT transactions. ARTEMIS captures NFT airdrop hunters through: (1) a multimodal module extracting visual and textual insights from NFT metadata using Transformer models; (2) a tailored node aggregation function chaining NFT transaction sequences, retaining behavioral insights; (3) engineered features based on market manipulation theories detecting anomalous trading. Evaluated on decentralized exchange Blur's data, ARTEMIS significantly outperforms baselines in pinpointing hunters. This pioneering computational solution for an emergent Web3 phenomenon has broad applicability for blockchain anomaly detection. The data and code for the paper are accessible at the following link: \hrefhttps://doi.org/10.5281/zenodo.10676801 doi.org/10.5281/zenodo.10676801. | Chenyu Zhou, Hongzhou Chen, Hao Wu, Junyu Zhang, Wei Cai |  |
| 641 |  |  [HSDirSniper: A New Attack Exploiting Vulnerabilities in Tor's Hidden Service Directories](https://doi.org/10.1145/3589334.3645591) |  | 0 | Tor hidden services (HSs) are used to provide anonymous services to users on the Internet without revealing the location of the servers. However, existing approaches have proven ineffective in mitigating the misuse of hidden services. Our investigation reveals that the latest iteration of Tor hidden services still exhibits vulnerabilities related to Hidden Service Directories (HSDirs). Building upon this identified weakness, we introduce the HSDirSniper attack, which leverages a substantial volume of descriptors to inundate the HSDir's descriptor cache. This results in the HSDir purging all stored descriptors, thereby blocking arbitrary hidden services. Notably, our attack represents the most practical means of blocking hidden services within the current high-adversarial context. The advantage of the HSDirSniper attack lies in its covert nature, as the targeted hidden service remains unaware of the attack. Additionally, the successful execution of this attack does not require the introduction of a colluding routing node within the Tor Network. We conducted comprehensive experiments in the real-world Tor Network, and the experimental results show that an attacker equipped with a certain quantity of hidden servers can render arbitrary hidden services inaccessible up to 90% of the time. To ascertain the potential scope of damage that the HSDirSniper attack can inflict upon hidden services, we provide a formal analytical framework for quantifying the cost of the HSDirSniper attack. Finally, we discuss the ethical concerns and countermeasures. | Qingfeng Zhang, Zhiyang Teng, Xuebin Wang, Yue Gao, Qingyun Liu, Jinqiao Shi |  |
| 642 |  |  [The Matter of Captchas: An Analysis of a Brittle Security Feature on the Modern Web](https://doi.org/10.1145/3589334.3645619) |  | 0 | The web ecosystem is a fast-paced environment. In this dynamic landscape, new security features are offered one after another to enhance the security and robustness of web applications and the operations they handle. This paper focuses on a fragile but still in-use security feature, text-based CAPTCHAs, that had been wildly used by web applications in the past to protect against automated attacks such as credential stuffing and account hijacking. The paper first investigates what it takes to develop automated scanners that can solve previously unseen text-based CAPTCHAs. We evaluated the possibility of developing and integrating a pre-trained CAPTCHA solver in the automated web scanning process without using a significantly large training dataset. We also perform an analysis of the impact of such autonomous scanners on CAPTCHA-enabled websites. Our analysis shows that solvable text-based CAPTCHAs on login, contact, and comment pages of websites are not uncommon. In particular, we identified over 3,100 text-based CAPTCHA websites in critical sectors such as finance, government, and health with hundreds of thousands of users. We showed that a web scanner with a pre-trained solver could solve more than 20% of previously unseen CAPTCHAs in just one single attempt. This result is worrisome considering the substantial potential to autonomously run the operation across thousands of websites on a daily basis with minimal training. The findings suggest that the integration of autonomous scanning with pre-training and local optimization of models can significantly increase adversaries' asymmetric power to launch their attacks cheaper and faster. | Behzad Ousat, Esteban Schafir, Duc C. Hoang, Mohammad Ali Tofighi, Cuong V. Nguyen, Sajjad Arshad, A. Selcuk Uluagac, Amin Kharraz |  |
| 643 |  |  [Characterizing Ethereum Upgradable Smart Contracts and Their Security Implications](https://doi.org/10.1145/3589334.3645640) |  | 0 | Upgradeable smart contracts (USCs) have been widely adopted to enable modifying deployed smart contracts. While USCs bring great flexibility to developers, improper usage might introduce new security issues, potentially allowing attackers to hijack USCs and their users. In this paper, we conduct a large-scale measurement study to characterize USCs and their security implications in the wild. We summarize six commonly used USC patterns and develop a tool, USCDetector, to identify USCs without needing source code. Particularly, USCDetector collects various information such as bytecode and transaction information to construct upgrade chains for USCs and disclose potentially vulnerable ones. We evaluate USCDetector using verified smart contracts (i.e., with source code) as ground truth and show that USCDetector can achieve high accuracy with a precision of 96.26 to conduct a large-scale study on Ethereum, covering a total of 60,251,064 smart contracts. USCDetecor constructs 10,218 upgrade chains and discloses multiple real-world USCs with potential security issues. | Xiaofan Li, Jin Yang, Jiaqi Chen, Yuzhe Tang, Xing Gao |  |
| 644 |  |  [IDEA-DAC: Integrity-Driven Editing for Accountable Decentralized Anonymous Credentials via ZK-JSON](https://doi.org/10.1145/3589334.3645658) |  | 0 | Decentralized Anonymous Credential (DAC) systems are increasingly relevant, especially when enhancing revocation mechanisms in the face of complex traceability challenges. This paper introduces IDEA-DAC a paradigm shift from the conventional revoke-and-reissue methods, promoting direct and Integrity-Driven Editing (IDE) for Accountable DACs, which results in better integrity accountability, traceability, and system simplicity. We further incorporate an Edit-bound Conformity Check that ensures tailored integrity standards during credential amendments using R1CS-based ZK-SNARKs. Delving deeper, we propose ZK-JSON, a unique R1CS circuit design tailored for IDE over generic JSON documents. This design imposes strictly O(N) rank-1 constraints for variable-length JSON documents of up to N bytes in length, encompassing serialization, encryption, and edit-bound conformity checks. Additionally, our circuits only necessitate a one-time compilation, setup, and smart contract deployment for homogeneous JSON documents up to a specified size. While preserving core DAC features such as selective disclosure, anonymity, and predicate provability, IDEA-DAC achieves precise data modification checks without revealing private content, ensuring only authorized edits are permitted. In summary, IDEA-DAC offers an enhanced methodology for large-scale JSON-formatted credential systems, setting a new standard in decentralized identity management efficiency and precision. | Shuhao Zheng, Zonglun Li, Junliang Luo, Ziyue Xin, Xue Liu |  |
| 645 |  |  [Is It Safe to Share Your Files? An Empirical Security Analysis of Google Workspace](https://doi.org/10.1145/3589334.3645697) |  | 0 | The increasing demand for remote work and virtual interactions has heightened the usage of business collaboration platforms~(BCPs), with Google Workspace as a prominent example. These platforms enhance team collaboration by integrating Google Docs, Slides, Calendar, and feature-rich third-party applications (add-ons). However, such integration of multiple users and entities has inadvertently introduced new and complex attack surfaces, elevating security and privacy risks in resource management to unprecedented levels. In this study, we conduct a systematic study on the effectiveness of the cross-entity resource management in Google Workspace, the most popular BCP. Our study unveils the access control enforcement in real-world BCPs for the first time. Based on this, we formulate the attack surfaces inherent in BCPs and conduct a comprehensive assessment, pinpointing three vulnerability types leading to distinct attacks. An analysis of 4,732 marketplace add-ons reveals that approximately 70% are potentially vulnerable to these attacks. We propose robust countermeasures to improve BCP security, urging immediate action and setting a foundation for future research. | Liuhuo Wan, Kailong Wang, Haoyu Wang, Guangdong Bai |  |
| 646 |  |  [PanoptiChrome: A Modern In-browser Taint Analysis Framework](https://doi.org/10.1145/3589334.3645699) |  | 0 | Taint tracking in web browsers is a problem of profound interest because it allows developers to accurately understand the flow of sensitive data across JavaScript (JS) functions. Modern websites load JS functions from either the web server or other third-party sites, hence this problem has acquired a much more complex and pernicious dimension. Sadly, for the latest version of the Chromium browser (used by 75% of users), there is no dynamic taint propagation engine primarily because it is incredibly complex to build one. The nearest contending work in this space was published in 2018 for version 57; at the time of writing, we are at Chromium version 117, and the current version is very different from the 2018 version. We outline the details of a multi-year effort in this paper that led to PanoptiChrome, which accurately tracks information flow across an arbitrary number of sources and sinks and is, to a large extent, portable across platforms. As an example use case of the platform, we experimentally show that we can discover fingerprinting APIs that can uniquely identify the browser and sometimes the user, which are missed by state-of-the-art tools, owing to our comprehensive dynamic analysis methodology. For the top 20,000 most popular websites, we discovered a total of 362 APIs that have the potential to be used for fingerprinting -- out of these, 208 APIs were previously not reported by state-of-the-art tools. | Rahul Kanyal, Smruti R. Sarangi |  |
| 647 |  |  [PhishinWebView: Analysis of Anti-Phishing Entities in Mobile Apps with WebView Targeted Phishing](https://doi.org/10.1145/3589334.3645708) |  | 0 | Despite the relentless efforts on developing anti-phishing techniques, phishing attacks continue to proliferate, often incorporating evasion techniques to bypass detection. While recent studies have continuously enhanced our understanding of their evasion techniques in desktop environments, few studies have been conducted to explore how the phishing attack is being handled in mobile environments, specifically WebView. In this study, we systematically evaluate the blocking processes of anti-phishing entities in individual apps in the real world by designing the phishing attack tailored to WebView. Specifically, we select eight well-known apps using WebView, and report 80 typical phishing sites (without evasion techniques) and 130 user-agent-specific phishing sites (accessible exclusively via each app's WebView). For scalable analysis, we develop an autonomous evaluation framework and investigate accessibility of both apps and Safe Browsing entities. As a result, we find that user-agent-specific (UA-specific) phishing sites successfully evade blocking across all of the eight Android apps. We also investigate accessing strategies of anti-phishing crawlers of both the apps and Safe Browsing entities; and find that only two apps' crawlers can access UA-specific phishing sites without any subsequent actions such as blocking the link. Based on our experiment results, we present security recommendations to take proactive phishing cautions using link preview bots. To the best of our knowledge, this is the first study that explores how the WebView environments handle phishing attacks and disclose their limitation in the real world. | Yoonjung Choi, Woonghee Lee, Junbeom Hur |  |
| 648 |  |  [Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models](https://doi.org/10.1145/3589334.3645376) |  | 0 | Temporal reasoning is a crucial NLP task, providing a nuanced understanding of time-sensitive contexts within textual data. Although recent advancements in LLMs have demonstrated their potential in temporal reasoning, the predominant focus has been on tasks such as temporal expression and temporal relation extraction. These tasks are primarily designed for the extraction of direct and past temporal cues and to engage in simple reasoning processes. A significant gap remains when considering complex reasoning tasks such as event forecasting, which requires multi-step temporal reasoning on events and prediction on the future timestamp. Another notable limitation of existing methods is their incapability to provide an illustration of their reasoning process, hindering explainability. In this paper, we introduce the first task of explainable temporal reasoning, to predict an event's occurrence at a future timestamp based on context which requires multiple reasoning over multiple events, and subsequently provide a clear explanation for their prediction. Our task offers a comprehensive evaluation of both the LLMs' complex temporal reasoning ability, the future event prediction ability, and explainability-a critical attribute for AI applications. To support this task, we present the first multi-source instruction-tuning dataset of explainable temporal reasoning (ExpTime) with 26k derived from the temporal knowledge graph datasets and their temporal reasoning paths, using a novel knowledge-graph-instructed-generation strategy. Based on the dataset, we propose the first open-source LLM series TimeLlaMA based on the foundation LlaMA2, with the ability of instruction following for explainable temporal reasoning. We compare the performance of our method and a variety of LLMs, where our method achieves the state-of-the-art performance of temporal prediction and explanation. | Chenhan Yuan, Qianqian Xie, Jimin Huang, Sophia Ananiadou | The University of Manchester Manchester; Wuhan University Wuhan |
| 649 |  |  [A Knowledge-Injected Curriculum Pretraining Framework for Question Answering](https://doi.org/10.1145/3589334.3645406) |  | 0 | Knowledge-based question answering (KBQA) is a key task in NLP research, andalso an approach to access the web data and knowledge, which requiresexploiting knowledge graphs (KGs) for reasoning. In the literature, onepromising solution for KBQA is to incorporate the pretrained language model(LM) with KGs by generating KG-centered pretraining corpus, which has shown itssuperiority. However, these methods often depend on specific techniques andresources to work, which may not always be available and restrict itsapplication. Moreover, existing methods focus more on improving languageunderstanding with KGs, while neglect the more important human-like complexreasoning. To this end, in this paper, we propose a general Knowledge-InjectedCurriculum Pretraining framework (KICP) to achieve comprehensive KG learningand exploitation for KBQA tasks, which is composed of knowledge injection (KI),knowledge adaptation (KA) and curriculum reasoning (CR). Specifically, the KImodule first injects knowledge into the LM by generating KG-centeredpretraining corpus, and generalizes the process into three key steps that couldwork with different implementations for flexible application. Next, the KAmodule learns knowledge from the generated corpus with LM equipped with anadapter as well as keeps its original natural language understanding ability toreduce the negative impacts of the difference between the generated and naturalcorpus. Last, to enable the LM with complex reasoning, the CR module followshuman reasoning patterns to construct three corpora with increasingdifficulties of reasoning, and further trains the LM from easy to hard in acurriculum manner. We provide an implementation of the general framework, andevaluate the proposed KICP on four real-word datasets. The results demonstratethat our framework can achieve higher performances. | Xin Lin, Tianhuang Su, Zhenya Huang, Shangzi Xue, Haifeng Liu, Enhong Chen |  |
| 650 |  |  [Using Model Calibration to Evaluate Link Prediction in Knowledge Graphs](https://doi.org/10.1145/3589334.3645506) |  | 0 | Link prediction models assign scores to predict new, plausible edges to complete knowledge graphs. In link prediction evaluation, the score of an existing edge (positive) is ranked w.r.t. the scores of its synthetically corrupted counterparts (negatives). An accurate model ranks positives higher than negatives, assuming ascending order. Since the number of negatives are typically large for a single positive, link prediction evaluation is computationally expensive. As far as we know, only one approach has proposed to replace rank aggregations by a distance between sample positives and negatives. Unfortunately, the distance does not consider individual ranks, so edges in isolation cannot be assessed. In this paper, we propose an alternative protocol based on posterior probabilities of positives rather than ranks. A calibration function assigns posterior probabilities to edges that measure their plausibility. We propose to assess our alternative protocol in various ways, including whether expected semantics are captured when using different strategies to synthetically generate negatives. Our experiments show that posterior probabilities and ranks are highly correlated. Also, the time reduction of our alternative protocol is quite significant: more than 77% compared to rank-based evaluation. We conclude that link prediction evaluation based on posterior probabilities is viable and significantly reduces computational costs. | Aishwarya Rao, Narayanan Asuri Krishnan, Carlos R. Rivero |  |
| 651 |  |  [Faithful Temporal Question Answering over Heterogeneous Sources](https://doi.org/10.1145/3589334.3645547) |  | 0 | Temporal question answering (QA) involves time constraints, with phrases suchas "... in 2019" or "... before COVID". In the former, time is an explicitcondition, in the latter it is implicit. State-of-the-art methods havelimitations along three dimensions. First, with neural inference, timeconstraints are merely soft-matched, giving room to invalid or inexplicableanswers. Second, questions with implicit time are poorly supported. Third,answers come from a single source: either a knowledge base (KB) or a textcorpus. We propose a temporal QA system that addresses these shortcomings.First, it enforces temporal constraints for faithful answering with tangibleevidence. Second, it properly handles implicit questions. Third, it operatesover heterogeneous sources, covering KB, text and web tables in a unifiedmanner. The method has three stages: (i) understanding the question and itstemporal conditions, (ii) retrieving evidence from all sources, and (iii)faithfully answering the question. As implicit questions are sparse in priorbenchmarks, we introduce a principled method for generating diverse questions.Experiments show superior performance over a suite of baselines. | Zhen Jia, Philipp Christmann, Gerhard Weikum | Max Planck Institute for Informatics Saarland Informatics Campus; Southwest Jiaotong University Chengdu |
| 652 |  |  [From Shapes to Shapes: Inferring SHACL Shapes for Results of SPARQL CONSTRUCT Queries](https://doi.org/10.1145/3589334.3645550) |  | 0 | SPARQL CONSTRUCT queries allow for the specification of data processingpipelines that transform given input graphs into new output graphs. It is nowcommon to constrain graphs through SHACL shapes allowing users to understandwhich data they can expect and which not. However, it becomes challenging tounderstand what graph data can be expected at the end of a data processingpipeline without knowing the particular input data: Shape constraints on theinput graph may affect the output graph, but may no longer apply literally, andnew shapes may be imposed by the query template. In this paper, we study thederivation of shape constraints that hold on all possible output graphs of agiven SPARQL CONSTRUCT query. We assume that the SPARQL CONSTRUCT query isfixed, e.g., being part of a program, whereas the input graphs adhere to inputshape constraints but may otherwise vary over time and, thus, are mostlyunknown. We study a fragment of SPARQL CONSTRUCT queries (SCCQ) and a fragmentof SHACL (Simple SHACL). We formally define the problem of deriving the mostrestrictive set of Simple SHACL shapes that constrain the results fromevaluating a SCCQ over any input graph restricted by a given set of SimpleSHACL shapes. We propose and implement an algorithm that statically analysesinput SHACL shapes and CONSTRUCT queries and prove its soundness andcomplexity. | Philipp Seifer, Daniel Hernández, Ralf Lämmel, Steffen Staab |  |
| 653 |  |  [Zero-shot Image Classification with Logic Adapter and Rule Prompt](https://doi.org/10.1145/3589334.3645554) |  | 0 | Zero-shot image classification, which aims to predict unseen classes whose samples have never appeared during the training phase, is crucial in the Web domain because many new web images appear on various websites. Attributes, as annotations for class-level characteristics, are widely used semantic information for this task. However, most current methods often fail to capture discriminative image features between similar images from different classes, leading to unsatisfactory zero-shot image classification results. This is because they solely focus on limited visual-attribute feature alignment. Therefore, we propose a Zero-Shot image Classification with Logic adapter and Rule prompt method called ZSCLR, which utilizes logic adapter and rule prompts to encourage the model to capture discriminative image features and achieve reasoning. Specifically, ZSCLR consists of a visual perception module and a logic adapter. The visual perception module extracts image features from training data. At the same time, the logic adapter utilizes the Markov logic network to encode the extracted image features and rule prompts for refining the discriminative image features. Due to predicates of rule prompts representing symbolic discriminative features, the proposed model can focus more on these discriminative features and achieve more precise image classification. Additionally, the logic adapter enables the model to adapt from recognizing images in seen classes to those in unseen classes through the reasoning of the Markov logic networks. We implement experiments on three standard zero-shot image classification benchmarks, and ZSCLR achieves competitive performance. Furthermore, ZSCLR can provide explanations for its predictions through rule prompts. | Dongran Yu, Xueyan Liu, Bo Yang |  |
| 654 |  |  [NPCS: Native Provenance Computation for SPARQL](https://doi.org/10.1145/3589334.3645557) |  | 0 | The popularity of Knowledge Graphs (KGs) both in industry and academia owes credit to their flexible data model, suitable for data integration from multiple sources. Several KG-based applications such as trust assessment or view maintenance on dynamic data rely on the ability to compute provenance explanations for query results. The how-provenance of a query result is an expression that encodes the records (triples or facts) that explain its inclusion in the result set. This article proposes NPCS, a Native Provenance Computation approach for SPARQL queries. NPCS annotates query results with their how-provenance. By building upon spm-provenance semirings, NPCS supports both monotonic and non-monotonic SPARQL queries. Thanks to its reliance on query rewriting techniques, the approach is directly applicable to already deployed SPARQL engines using different reification schemes - including RDF-star. Our experimental evaluation on two popular SPARQL engines (GraphDB and Stardog) shows that our novel query rewriting brings a significant runtime improvement over existing query rewriting solutions, scaling to RDF graphs with billions of triples. | Zubaria Asma, Daniel Hernández, Luis Galárraga, Giorgos Flouris, Irini Fundulaki, Katja Hose |  |
| 655 |  |  [Taxonomy Completion via Implicit Concept Insertion](https://doi.org/10.1145/3589334.3645584) |  | 0 | \beginabstract High quality taxonomies play a critical role in various domains such as e-commerce, web search and ontology engineering. While there has been extensive work on expanding taxonomies from externally mined data, there has been less attention paid to enriching taxonomies by exploiting existing concepts and structure within the taxonomy. In this work, we show the usefulness of this kind of enrichment, and explore its viability with a new taxonomy completion system ICON (I mplicit CON cept Insertion). ICON generates new concepts by identifying implicit concepts based on the existing concept structure, generating names for such concepts and inserting them in appropriate positions within the taxonomy. ICON integrates techniques from entity retrieval, text summary, and subsumption prediction; this modular architecture offers high flexibility while achieving state-of-the-art performance. We have evaluated ICON on two e-commerce taxonomies, and the results show that it offers significant advantages over strong baselines including recent taxonomy completion models and the large language model, ChatGPT. | Jingchuan Shi, Hang Dong, Jiaoyan Chen, Zhe Wu, Ian Horrocks |  |
| 656 |  |  [UniLP: Unified Topology-aware Generative Framework for Link Prediction in Knowledge Graph](https://doi.org/10.1145/3589334.3645592) |  | 0 | Link prediction (LP) in knowledge graph (KG) is a crucial task that has received increasing attention recently. Due to the heterogeneous structures of KGs, various application scenarios, and demand-specific downstream objectives, there exist multiple subtasks in LP. Most studies only focus on designing a dedicated architecture for a specific subtask, which results in various complicated LP models. The isolated architectures and chaotic situations make it significant to construct a unified model that can handle multiple LP subtasks simultaneously. However, unifying all subtasks in LP presents numerous challenges, including unified input forms, task-specific context modeling, and topological information encoding. To address these challenges, we propose a topology-aware generative framework, namely UniLP, which utilizes a generative pre-trained language model to accomplish different LP subtasks universally. Specifically, we introduce a context demonstration template to convert task-specific context into a unified generative formulation. Based on the unified formulation, to address the limitation of transformer architecture that may overlook important structural signals in KGs, we design novel topology-aware soft prompts to deeply couple topology and text information in a contextualized manner. Extensive experiment results demonstrate that our framework achieves substantial performance gain and provides a real unified end-to-end solution for the whole LP subtasks. We also perform comprehensive ablation studies to support in-depth analysis of each component in UniLP. | Ben Liu, Miao Peng, Wenjie Xu, Xu Jia, Min Peng |  |
| 657 |  |  [A Symbolic Rule Integration Framework with Logic Transformer for Inductive Relation Prediction](https://doi.org/10.1145/3589334.3645594) |  | 0 | Relation prediction in knowledge graphs (KGs) aims at predicting missing relations in incomplete triples, whereas the dominant paradigm by KG embeddings has a limitation to predict the relation between unseen entities. This situation is called an inductive setting, which is more common in the real-world scenario. To handle this issue, implicit symbolic rules have shown great potential in capturing the inductive capability. However, it is still challenging to obtain precise representations of logic rules from KGs. The argument variability and predicate non-commutativity in symbolic rule integration make the modeling of component symbols difficult. To this end, we propose a novel inductive relation prediction model named SymRITa with a logic transformer integrating rules. SymRITa firstly extracts the subgraph, whose embeddings are captured by a graph network. Meanwhile, symbolic rule graphs in the subgraph can be generated. Then, the symbolic rules are modeled by a proposed logic transformer. Specifically, the input format based on the subgraph-based embeddings is to focus on the argument variability in symbolic rules. In addition, a conjunction attention mechanism in the logic transformer can resolve predicate non-commutativity in the symbolic rule integration process. Finally, the subgraph-based and symbol-based embeddings obtained from the previous steps are combined for the training regime, and prediction results as well as rules explaining the reasoning process are explicitly output. Extensive experiments on twelve inductive datasets show that SymRITa achieves outstanding effectiveness compared to state-of-the-art inductive baselines. Moreover, the logic rules with corresponding confidences provide an interpretable paradigm. | Yudai Pan, Jun Liu, Tianzhe Zhao, Lingling Zhang, Yun Lin, Jin Song Dong |  |
| 658 |  |  [Causal Question Answering with Reinforcement Learning](https://doi.org/10.1145/3589334.3645610) |  | 0 | Causal questions inquire about causal relationships between different events or phenomena. They are important for a variety of use cases, including virtual assistants and search engines. However, many current approaches to causal question answering cannot provide explanations or evidence for their answers. Hence, in this paper, we aim to answer causal questions with a causality graph, a large-scale dataset of causal relations between noun phrases along with the relations' provenance data. Inspired by recent, successful applications of reinforcement learning to knowledge graph tasks, such as link prediction and fact-checking, we explore the application of reinforcement learning on a causality graph for causal question answering. We introduce an Actor-Critic-based agent which learns to search through the graph to answer causal questions. We bootstrap the agent with a supervised learning procedure to deal with large action spaces and sparse rewards. Our evaluation shows that the agent successfully prunes the search space to answer binary causal questions by visiting less than 30 nodes per question compared to over 3,000 nodes by a naive breadth-first search. Our ablation study indicates that our supervised learning strategy provides a strong foundation upon which our reinforcement learning agent improves. The paths returned by our agent explain the mechanisms by which a cause produces an effect. Moreover, for each edge on a path, our causality graph provides its original source allowing for easy verification of paths. | Lukas Blübaum, Stefan Heindorf | Paderborn University |
| 659 |  |  [DRAM-like Architecture with Asynchronous Refreshing for Continual Relation Extraction](https://doi.org/10.1145/3589334.3645621) |  | 0 | Continual Relation Extraction (CRE) has found widespread web applications (e.g., search engines) in recent times. One significant challenge in this task is the phenomenon of catastrophic forgetting, where models tend to forget earlier information. Existing approaches in this field predominantly rely on memory-based methods to alleviate catastrophic forgetting, which overlooks the inherent challenge posed by the varying memory requirements of different relations and the need for a suitable memory refreshing strategy. Drawing inspiration from the mechanisms of Dynamic Random Access Memory (DRAM), our study introduces a novel CRE architecture with an asynchronous refreshing strategy to tackle these challenges. We first design a DRAM-like architecture, comprising three key modules: perceptron, controller, and refresher. This architecture dynamically allocates memory, enabling the consolidation of well-remembered relations while allocating additional memory for revisiting poorly learned relations. Furthermore, we propose a compromising asynchronous refreshing strategy to find the pivot between over-memorization and overfitting, which focuses on the current learning task and mixed-memory data asynchronously. Additionally, we explain the existing refreshing strategies in CRE from the DRAM perspective. Our proposed method has experimented on two benchmarks and overall outperforms ConPL (the SOTA method) by an average of 1.50% on accuracy, which demonstrates the efficiency of the proposed architecture and refreshing strategy. | Tianci Bu, Kang Yang, Wenchuan Yang, Jiawei Feng, Xiaoyu Zhang, Xin Lu |  |
| 660 |  |  [KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large Language Models](https://doi.org/10.1145/3589334.3645623) |  | 0 | Large language models (LLMs) demonstrate remarkable performance onknowledge-intensive tasks, suggesting that real-world knowledge is encoded intheir model parameters. However, besides explorations on a few probing tasks inlimited knowledge domains, it is not well understood how to evaluate LLMs'knowledge systematically and how well their knowledge abilities generalize,across a spectrum of knowledge domains and progressively complex task formats.To this end, we propose KGQuiz, a knowledge-intensive benchmark tocomprehensively investigate the knowledge generalization abilities of LLMs.KGQuiz is a scalable framework constructed from triplet-based knowledge, whichcovers three knowledge domains and consists of five tasks with increasingcomplexity: true-or-false, multiple-choice QA, blank filling, factual editing,and open-ended knowledge generation. To gain a better understanding of LLMs'knowledge abilities and their generalization, we evaluate 10 open-source andblack-box LLMs on the KGQuiz benchmark across the five knowledge-intensivetasks and knowledge domains. Extensive experiments demonstrate that LLMsachieve impressive performance in straightforward knowledge QA tasks, whilesettings and contexts requiring more complex reasoning or employingdomain-specific facts still present significant challenges. We envision KGQuizas a testbed to analyze such nuanced variations in performance across domainsand task formats, and ultimately to understand, evaluate, and improve LLMs'knowledge abilities across a wide spectrum of knowledge domains and tasks. | Yuyang Bai, Shangbin Feng, Vidhisha Balachandran, Zhaoxuan Tan, Shiqi Lou, Tianxing He, Yulia Tsvetkov |  |
| 661 |  |  [Robust Link Prediction over Noisy Hyper-Relational Knowledge Graphs via Active Learning](https://doi.org/10.1145/3589334.3645686) |  | 0 | Modern Knowledge Graphs (KGs) are inevitably noisy due to the nature of their construction process. Existing robust learning techniques for noisy KGs mostly focus on triple facts, where the fact-wise confidence is straightforward to evaluate. However, hyper-relational facts, where an arbitrary number of key-value pairs are associated with a base triplet, have become increasingly popular in modern KGs, but significantly complicate the confidence assessment of the fact. Against this background, we study the problem of robust link prediction over noisy hyper-relational KGs, and propose NYLON, a \underlineN oise-resistant h\underlineY per-re\underlineL ati\underlineON al link prediction technique via active crowd learning. Specifically, beyond the traditional fact-wise confidence, we first introduce element-wise confidence measuring the fine-grained confidence of each entity or relation of a hyper-relational fact. We connect the element- and fact-wise confidences via a "least confidence'' principle to allow efficient crowd labeling. NYLON is then designed to systematically integrate three key components, where a hyper-relational link predictor uses the fact-wise confidence for robust prediction, a cross-grained confidence evaluator predicts both element- and fact-wise confidences, and an effort-efficient active labeler selects informative facts for crowd annotators to label using an efficient labeling mechanism guided by the element-wise confidence under the "least confidence'' principle and further followed by data augmentation. We evaluate NYLON on three real-world KG datasets against a sizeable collection of baselines. Results show that NYLON achieves superior and robust performance in both link prediction and error detection tasks on noisy KGs, and outperforms best baselines by 2.42-10.93% and 3.46-10.65% in the two tasks, respectively. | Weijian Yu, Jie Yang, Dingqi Yang |  |
| 662 |  |  [OODREB: Benchmarking State-of-the-Art Methods for Out-Of-Distribution Generalization on Relation Extraction](https://doi.org/10.1145/3589334.3645695) |  | 0 | Relation extraction (RE) methods have achieved striking performance when training and test data are independently and identically distributed (i.i.d). However, in real-world scenarios where RE models are trained to acquire knowledge in the wild, the assumption can hardly be satisfied due to the different and unknown testing distributions. In this paper, we serve as the first effort to study out-of-distribution (OOD) problems in RE by constructing an out-of-distribution relation extraction benchmark (OODREB) and then investigating the abilities of state-of-the-art (SOTA) RE methods on OODREB in both i.i.d. and OOD settings. Our proposed benchmark and analysis reveal new findings and insights: (1) Existing SOTA RE methods struggle to achieve satisfying performance on OODREB in both i.i.d. and OOD settings due to the complex training data and biased model selection method. Rethinking the developing protocols of RE methods is of great urgency. (2) The SOTA RE methods fail to learn causality due to the diverse linguistic expressions of causal information. The failure limits their robustness and generalization ability; (3) Current RE methods based on language models are far away from being deployed in real-world applications. We appeal to future work to take the OOD generalization and causality learning ability into consideration. We make our annotation and code publicly available at https://github.com/Hytn/OODREB. | Haotian Chen, Houjing Guo, Bingsheng Chen, Xiangdong Zhou |  |
| 663 |  |  [Toward Practical Entity Alignment Method Design: Insights from New Highly Heterogeneous Knowledge Graph Datasets](https://doi.org/10.1145/3589334.3645720) |  | 0 | The flourishing of knowledge graph (KG) applications has driven the need for entity alignment (EA) across KGs. However, the heterogeneity of practical KGs, characterized by differing scales, structures, and limited overlapping entities, greatly surpasses that of existing EA datasets. This discrepancy highlights an oversimplified heterogeneity in current EA datasets, which obstructs the exploration of the EA application. In this paper, we study the performance of EA methods on the alignment of highly heterogeneous KGs (HHKGs). Firstly, we address the oversimplified heterogeneity settings of current datasets and propose two new HHKG datasets that closely mimic practical EA scenarios. Then, based on these datasets, we conduct extensive experiments to evaluate previous representative EA methods. Our findings reveal that, in aligning HHKGs, valuable structure information can hardly be exploited, which leads to inferior performance of existing EA methods, especially those based on GNNs. These findings shed light on the potential problems associated with the conventional application of GNN-based methods as a panacea for all EA datasets. Consequently, to elucidate what EA methodology is genuinely beneficial in practical scenarios, we undertake an in-depth analysis by implementing a simple but effective approach: Simple-HHEA. Our experiment results conclude that the key to the future EA model design in practice lies in their adaptability and efficiency to varying information quality conditions, as well as their capability to capture patterns across HHKGs. The datasets and source code are available at https://github.com/IDEA-FinAI/Simple-HHEA. | Xuhui Jiang, Chengjin Xu, Yinghan Shen, Yuanzhuo Wang, Fenglong Su, Zhichao Shi, Fei Sun, Zixuan Li, Jian Guo, Huawei Shen | IDEA Research, International Digital Economy Academy, Shenzhen, China; National University of Defense Technology Institute of Computing Technology; International Digital Economy Academy; Chinese Academy of Sciences Big Data Academy of Zhongke; Chinese Academy of Sciences Institute of Computing Technology; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Chinese Academy of Sciences School of Computer Science and Technology |
| 664 |  |  [Social Media Discourses on Interracial Intimacy: Tracking Racism and Sexism through Chinese Geo-located Social Media Data](https://doi.org/10.1145/3589334.3645334) |  | 0 | We examine the social media discourse surrounding interracial relationships in China, specifically on the popular platform Douyin. By analyzing comments on short video posts, the study focuses on four types of interracial relationships: Black men and Chinese women, Black women and Chinese men, White men and Chinese women, and White women and Chinese men. The study also explores potential regional differences in these discourses, using IP geolocation data made available to the public since April 2022. Our content analysis revealed that the Black men and Chinese women couples attracted the most negative comments and the White women and Chinese men couples received the least negative comments. We also observed substantial regional differences in the discourses towards these interracial relationships. We investigated several regional socioeconomic development indicators and noted that local GDP, population sizes, and the level of openness to Western cultures explained the variation in the negative sentiment level. This work advances our understanding of the interplay of race, gender, and immigration in constructing public discourses on social media and offers important insights into how these discourses evolve along with socioeconomic development. | Zheng Wei, Yixuan Xie, Danyun Xiao, Simin Zhang, Pan Hui, Muzhi Zhou |  |
| 665 |  |  [Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models](https://doi.org/10.1145/3589334.3645381) |  | 0 | The age of social media is flooded with Internet memes, necessitating a cleargrasp and effective identification of harmful ones. This task presents asignificant challenge due to the implicit meaning embedded in memes, which isnot explicitly conveyed through the surface text and image. However, existingharmful meme detection methods do not present readable explanations that unveilsuch implicit meaning to support their detection decisions. In this paper, wepropose an explainable approach to detect harmful memes, achieved throughreasoning over conflicting rationales from both harmless and harmful positions.Specifically, inspired by the powerful capacity of Large Language Models (LLMs)on text generation and reasoning, we first elicit multimodal debate betweenLLMs to generate the explanations derived from the contradictory arguments.Then we propose to fine-tune a small language model as the debate judge forharmfulness inference, to facilitate multimodal fusion between the harmfulnessrationales and the intrinsic multimodal information within memes. In this way,our model is empowered to perform dialectical reasoning over intricate andimplicit harm-indicative patterns, utilizing multimodal explanationsoriginating from both harmless and harmful arguments. Extensive experiments onthree public meme datasets demonstrate that our harmful meme detection approachachieves much better performance than state-of-the-art methods and exhibits asuperior capacity for explaining the meme harmfulness of the model predictions. | Hongzhan Lin, Ziyang Luo, Wei Gao, Jing Ma, Bo Wang, Ruichao Yang | Hong Kong Baptist University; Jilin University; Singapore Management University |
| 666 |  |  [What News Do People Get on Social Media? Analyzing Exposure and Consumption of News through Data Donations](https://doi.org/10.1145/3589334.3645399) |  | 0 | Understanding how exposure to news on social media impacts public discourse and exacerbates political polarization is a significant endeavor in both computer and social sciences.Unfortunately, progress in this area is hampered by limited access to data due to the closed nature of social media platforms.Consequently, prior studies have been constrained to considering only fragments of users' news exposure and reactions.To overcome this obstacle, we present an innovative measurement approach centered on donating personal data for scientific purposes, facilitated through a privacy-preserving tool that captures users' interactions with news on Facebook.This approach offers a nuanced perspective on users' news exposure and consumption, encompassing different types of news exposure: selective, incidental, algorithmic, and targeted, driven by the diverse underlying mechanisms governing news appearance on users' feeds.Our analysis of data from 472 participants based in the U.S. reveals several interesting findings.For instance, users are more prone to encountering misinformation because of their active selection of low-quality news sources rather than being exposed solely due to friends or platform algorithms.Furthermore, our study uncovers that users are open to engaging with news sources with opposite political ideology as long as these interactions are not visible to their immediate social circles.Overall, our study showcases the viability of data donation as a means to provide clarity to longstanding questions in this field, offering new perspectives on the intricate dynamics of social media news consumption and its effects. | Salim Chouaki, Abhijnan Chakraborty, Oana Goga, Savvas Zannettou |  |
| 667 |  |  [Euphemism Identification via Feature Fusion and Individualization](https://doi.org/10.1145/3589334.3645433) |  | 0 | Euphemisms are widely used on social media and darknet markets to evade supervision. For instance, "ice" serves as a euphemism for the target keyword "methamphetamine" in illicit transactions. Thus, euphemism identification which aims to map the euphemism to its secret meaning (target keyword) is a crucial task in ensuring social network security. However, this task poses significant challenges, including resource limitations due to the unavailable of annotated datasets and linguistic challenges arising from subtle differences in meaning between target keywords. Existing methods employed self-supervised schemes to automatically construct labeled training data, addressing the resource limitations. Yet, these methods rely on static embedding methods that fail to distinguish between target keywords with similar meanings. In addition, we observe that different euphemisms in similar contexts confuse the identification results. To overcome these obstacles, we propose a feature fusion and individualization (FFI) method for euphemism identification. First, we reformulate the task as a cloze task, making it more feasible. Next, we develop a feature fusion module to capture both dynamic global and static local features, enhancing discrimination between different euphemisms in similar contexts. Additionally, we employ a feature individualization module to ensure each target keyword has a unique feature representation by projecting features into their orthogonal space. As a result, FFI can effectively identify similar euphemisms that refer to target keywords with similar meanings. Experimental results demonstrate that our method outperforms state-of-the-art methods and large language models, providing robust support for its effectiveness. | Yuxue Hu, Mingmin Wu, Zhongqiang Huang, Junsong Li, Xing Ge, Ying Sha |  |
| 668 |  |  [Team Formation amidst Conflicts](https://doi.org/10.1145/3589334.3645444) |  | 0 | In this work, we formulate the problem of team formation amidst conflicts.The goal is to assign individuals to tasks, with given capacities, taking intoaccount individuals' task preferences and the conflicts between them. Usingdependent rounding schemes as our main toolbox, we provide efficientapproximation algorithms. Our framework is extremely versatile and can modelmany different real-world scenarios as they arise in educational settings andhuman-resource management. We test and deploy our algorithms on real-worlddatasets and we show that our algorithms find assignments that are better thanthose found by natural baselines. In the educational setting we also show howour assignments are far better than those done manually by human experts. Inthe human resource management application we show how our assignments increasethe diversity of teams. Finally, using a synthetic dataset we demonstrate thatour algorithms scale very well in practice. | Iasonas Nikolaou, Evimaria Terzi | Boston University |
| 669 |  |  [T3RD: Test-Time Training for Rumor Detection on Social Media](https://doi.org/10.1145/3589334.3645443) |  | 0 |  | Huaiwen Zhang, Xinxin Liu, Qing Yang, Yang Yang, Fan Qi, Shengsheng Qian, Changsheng Xu |  |
| 670 |  |  [ESCNet: Entity-enhanced and Stance Checking Network for Multi-modal Fact-Checking](https://doi.org/10.1145/3589334.3645455) |  | 0 | Recently, misinformation incorporating both texts and images has been disseminated more effectively than those containing text alone on social media, raising significant concerns for multi-modal fact-checking. Existing research makes contributions to multi-modal feature extraction and interaction, but fails to fully enhance the valuable semantic representations or excavate the intricate entity information. Besides, existing multi-modal fact-checking datasets are primarily focused on English and merely concentrate on a single type of misinformation, thereby neglecting a comprehensive summary and coverage of various types of misinformation. Taking these factors into account, we construct the first large-scale Chinese Multi-modal Fact-Checking (CMFC) dataset which encompasses 46,000 claims. The CMFC covers all types of misinformation for fact-checking and is divided into two sub-datasets, Collected Chinese Multi-modal Fact-Checking (CCMF) and Synthetic Chinese Multi-modal Fact-Checking (SCMF). To establish baseline performance, we propose a novel Entity-enhanced and Stance Checking Network (ESCNet), which includes Multi-modal Feature Extraction Module, Stance Transformer, and Entity-enhanced Encoder. The ESCNet jointly models stance semantic reasoning features and knowledge-enhanced entity pair features, in order to simultaneously learn effective semantic-level and knowledge-level claim representations. Our work offers the first step and establishes a benchmark for evidence-based, multi-type, multi-modal fact-checking. | Fanrui Zhang, Jiawei Liu, Jingyi Xie, Qiang Zhang, Yongchao Xu, ZhengJun Zha |  |
| 671 |  |  [Labor Space: A Unifying Representation of the Labor Market via Large Language Models](https://doi.org/10.1145/3589334.3645464) |  | 0 | The labor market is a complex ecosystem comprising diverse, interconnectedentities, such as industries, occupations, skills, and firms. Due to the lackof a systematic method to map these heterogeneous entities together, eachentity has been analyzed in isolation or only through pairwise relationships,inhibiting comprehensive understanding of the whole ecosystem. Here, weintroduce Labor Space, a vector-space embedding of heterogeneouslabor market entities, derived through applying a large language model withfine-tuning. Labor Space exposes the complex relational fabric of various labormarket constituents, facilitating coherent integrative analysis of industries,occupations, skills, and firms, while retaining type-specific clustering. Wedemonstrate its unprecedented analytical capacities, including positioningheterogeneous entities on an economic axes, such as\`Manufacturing–Healthcare'. Furthermore, by allowing vector arithmetic ofthese entities, Labor Space enables the exploration of complex inter-unitrelations, and subsequently the estimation of the ramifications of economicshocks on individual units and their ripple effect across the labor market. Weposit that Labor Space provides policymakers and business leaders with acomprehensive unifying framework for labor market analysis and simulation,fostering more nuanced and effective strategic decision-making. | Seongwoon Kim, YongYeol Ahn, Jaehyuk Park |  |
| 672 |  |  [Explainable Fake News Detection with Large Language Model via Defense Among Competing Wisdom](https://doi.org/10.1145/3589334.3645471) |  | 0 | Most fake news detection methods learn latent feature representations based on neural networks, which makes them black boxes to classify a piece of news without giving any justification. Existing explainable systems generate veracity justifications from investigative journalism, which suffer from debunking delayed and low efficiency. Recent studies simply assume that the justification is equivalent to the majority opinions expressed in the wisdom of crowds. However, the opinions typically contain some inaccurate or biased information since the wisdom of crowds is uncensored. To detect fake news from a sea of diverse, crowded and even competing narratives, in this paper, we propose a novel defense-based explainable fake news detection framework. Specifically, we first propose an evidence extraction module to split the wisdom of crowds into two competing parties and respectively detect salient evidences. To gain concise insights from evidences, we then design a prompt-based module that utilizes a large language model to generate justifications by inferring reasons towards two possible veracities. Finally, we propose a defense-based inference module to determine veracity via modeling the defense among these justifications. Extensive experiments conducted on two real-world benchmarks demonstrate that our proposed method outperforms state-of-the-art baselines in terms of fake news detection and provides high-quality justifications. | Bo Wang, Jing Ma, Hongzhan Lin, Zhiwei Yang, Ruichao Yang, Yuan Tian, Yi Chang |  |
| 673 |  |  [Navigating the Post-API Dilemma](https://doi.org/10.1145/3589334.3645503) |  | 0 | Recent decisions to discontinue access to social media APIs are having detrimental effects on Internet research and the field of computational social science as a whole. This lack of access to data has been dubbed the Post-API era of Internet research. Fortunately, popular search engines have the means to crawl, capture, and surface social media data on their Search Engine Results Pages (SERP) if provided the proper search query, and may provide a solution to this dilemma. In the present work we ask: does SERP provide a complete and unbiased sample of social media data? Is SERP a viable alternative to direct API-access? To answer these questions, we perform a comparative analysis between (Google) SERP results and nonsampled data from Reddit and Twitter/X. We find that SERP results are highly biased in favor of popular posts; against political, pornographic, and vulgar posts; are more positive in their sentiment; and have large topical gaps. Overall, we conclude that SERP is not a viable alternative to social media API access. | Amrit Poudel, Tim Weninger |  |
| 674 |  |  [Unraveling the Dynamics of Stable and Curious Audiences in Web Systems](https://doi.org/10.1145/3589334.3645473) |  | 0 | We propose the Burst-Induced Poisson Process (BPoP), a model designed to analyze time series data such as feeds or search queries. BPoP can distinguish between the slowly-varying regular activity of a stable audience and the bursty activity of a curious audience, often seen in viral threads. Our model consists of two hidden, interacting processes: a self-feeding process (SFP) that generates bursty behavior related to viral threads, and a non-homogeneous Poisson process (NHPP) with step function intensity that is influenced by the bursts from the SFP. The NHPP models the normal background behavior, driven solely by the overall popularity of the topic among the stable audience. Through extensive empirical work, we have demonstrated that our model fits and characterizes a large number of real datasets more effectively than state-of-the-art models. Most importantly, BPoP can quantify the stable audience of media channels over time, serving as a valuable indicator of their popularity. | Rodrigo Alves, Antoine Ledent, Renato Assunção, Pedro O. S. Vaz de Melo, Marius Kloft |  |
| 675 |  |  [SymLearn: A Symbiotic Crowd-AI Collective Learning Framework to Web-based Healthcare Policy Adherence Assessment](https://doi.org/10.1145/3589334.3645519) |  | 0 | This paper develops a symbiotic human-AI collective learning framework that explores the complementary strengths of both AI and crowdsourced human intelligence to address a novel Web-based healthcare-policy-adherence assessment (WebHA) problem. In particular, the objective of the WebHA problem is to automatically assess people's public health policy adherence during emergent global health crisis events (e.g., COVID-19, MonkeyPox) by exploring massive social media imagery data. Recent advances in human-AI systems exhibit a significant potential in addressing the intricate imagery-based classification problems like WebHA by leveraging the collective intelligence of both humans and AI. This paper aims to address the limitation of existing human-AI systems that often rely heavily on human intelligence to improve AI model performance while overlooking the fact that humans themselves can be fallible and prone to errors. To address the above limitation, this paper develops SymLearn, a symbiotic human-AI co-learning framework that leverages human intelligence to troubleshoot and fine-tune the AI model while using AI models to guide human crowd workers to reduce the inherent human errors in their labels. Extensive experiments on two real-world WebHA applications show that SymLearn clearly outperforms the state-of-the-art baselines by improving WebHA performance and reducing crowd response delay. | Yang Zhang, Ruohan Zong, Lanyu Shang, Huimin Zeng, Zhenrui Yue, Dong Wang |  |
| 676 |  |  [An Efficient Automatic Meta-Path Selection for Social Event Detection via Hyperbolic Space](https://doi.org/10.1145/3589334.3645526) |  | 0 | Social events reflect changes in communities, such as natural disasters and emergencies. Detection of these situations can help residents and organizations in the community avoid danger and reduce losses. The complex nature of social messages makes social event detection on social media challenging. The challenges that have a greater impact on social media detection models are as follows: (1) the amount of social media data is huge but its availability is small; (2) social media data is a tree structure and traditional Euclidean space embedding will distort embedded features; and (3) the heterogeneity of social media networks makes existing models unable to capture rich information well. To solve the above challenges, we propose a Heterogeneous Information Graph representation via Hyperbolic space combined with an Automatic Meta-path selection (GraphHAM) model, an efficient framework that automatically selects the meta-path's weight and combines hyperbolic space to learn information on social media. In particular, we apply an efficient automatic meta-path selection technique and convert the selected meta-path into a vector, thereby reducing the requisite amount of labeled data for the model. We also design a novel Hyperbolic Multi-Layer Perceptron (HMLP) to further learn the semantic and structural information of social information. Extensive experiments show that GraphHAM can achieve outstanding performance on real-world data using only 20% of the whole dataset as the training set. Our code can be found on GitHub https://github.com/ZITAIQIU/GraphHAM. | Zitai Qiu, Congbo Ma, Jia Wu, Jian Yang |  |
| 677 |  |  [NETEVOLVE: Social Network Forecasting using Multi-Agent Reinforcement Learning with Interpretable Features](https://doi.org/10.1145/3589334.3647982) |  | 0 | Predicting how social networks change in the future is important in many applications. Results in social network research have shown that the change in the network can be explained by a small number of concepts, such as "homophily" and "transitivity". However, existing prediction methods require many latent features that are not connected to such concepts, making the methods' black boxes and their prediction results difficult to interpret, making them harder to derive scientific knowledge about social networks. In this study, we propose NetEvolve a novel multi-agent reinforcement learning-based method that predicts changes in a given social network. Given a sequence of changes as training data, NetEvolve learns the characteristics of the nodes with interpretable features, such as how the node feels rewards for connecting with similar people and the cost of the connection itself. Based on the learned feature, NetEvolve makes a forecast based on multi-agent simulation. The method achieves comparable or better accuracy than existing methods in predicting network changes in real-world social networks while keeping the prediction results interpretable. | Kentaro Miyake, Hiroyoshi Ito, Christos Faloutsos, Hirotomo Matsumoto, Atsuyuki Morishima |  |
| 678 |  |  [Analysis and Detection of "Pink Slime" Websites in Social Media Posts](https://doi.org/10.1145/3589334.3645588) |  | 0 | Local news outlets play a vital role in providing trusted and relevant information to communities and addressing their specific needs and concerns. The emergence of news outlets posing as local sources and their spread on social media present a significant challenge in the digital information landscape. This paper presents a comprehensive study investigating posts featuring "pink slime'' news, which is a term that has been used to refer to these news outlets due to its deceptive nature. By analyzing a large dataset of posts, we gain valuable insights into the patterns of these posts and the origin of these posts. We show in this work that extracting syntactical features proves valuable in developing a classification approach for detecting such posts and that the approach achieves 92.5% accuracy. We also show that our approach achieves near-perfect detection when grouping the posts by URL. | Abdullah Aljebreen, Weiyi Meng, Eduard C. Dragut |  |
| 679 |  |  [Navigating Multidimensional Ideologies with Reddit's Political Compass: Economic Conflict and Social Affinity](https://doi.org/10.1145/3589334.3645606) |  | 0 | The prevalent perspective in quantitative research on opinion dynamics flattens the landscape of the online political discourse into a traditional left--right dichotomy. While this approach helps simplify the analysis and modeling effort, it also neglects the intrinsic multidimensional richness of ideologies. In this study, we analyze social interactions on Reddit, under the lens of a multi-dimensional ideological framework: the political compass. We examine over 8 million comments posted on the subreddits /r/PoliticalCompass and /r/PoliticalCompassMemes during 2020--2022. By leveraging their self-declarations, we disentangle the ideological dimensions of users into economic (left--right) and social (libertarian--authoritarian) axes. In addition, we characterize users by their demographic attributes (age, gender, and affluence). We find significant homophily for interactions along the social axis of the political compass and demographic attributes. Compared to a null model, interactions among individuals of similar ideology surpass expectations by 6%. In contrast, we uncover a significant heterophily along the economic axis: left/right interactions exceed expectations by 10%. Furthermore, heterophilic interactions are characterized by a higher language toxicity than homophilic interactions, which hints at a conflictual discourse between every opposite ideology. Our results help reconcile apparent contradictions in recent literature, which found a superposition of homophilic and heterophilic interactions in online political discussions. By disentangling such interactions into the economic and social axes we pave the way for a deeper understanding of opinion dynamics on social media. | Ernesto Colacrai, Federico Cinus, Gianmarco De Francisci Morales, Michele Starnini |  |
| 680 |  |  [Unifying Local and Global Knowledge: Empowering Large Language Models as Political Experts with Knowledge Graphs](https://doi.org/10.1145/3589334.3645616) |  | 0 | Large Language Models (LLMs) have revolutionized solutions for general natural language processing (NLP) tasks. However, deploying these models in specific domains still faces challenges like hallucination. While existing knowledge graph retrieval-based approaches offer partial solutions, they cannot be well adapted to the political domain. On one hand, existing generic knowledge graphs lack vital political context, hindering deductions for practical tasks. On the other hand, the nature of political questions often renders the direct facts elusive, necessitating deeper aggregation and comprehension of retrieved evidence. To address these challenges, we propose a Political Experts through Knowledge Graph Integration (PEG) framework. PEG entails the creation and utilization of a multi-view political knowledge graph (MVPKG), which integrates U.S. legislative, election, and diplomatic data, as well as conceptual knowledge from Wikidata. With MVPKG as its foundation, PEG enhances existing methods through knowledge acquisition, aggregation, and injection. This process begins with refining evidence through semantic filtering, followed by its aggregation into global knowledge via implicit or explicit methods. The integrated knowledge is then utilized by LLMs through prompts. Experiments on three real-world datasets across diverse LLMs confirm PEG's superiority in tackling political modeling tasks. | Xinyi Mou, Zejun Li, Hanjia Lyu, Jiebo Luo, Zhongyu Wei |  |
| 681 |  |  [Not All Asians are the Same: A Disaggregated Approach to Identifying Anti-Asian Racism in Social Media](https://doi.org/10.1145/3589334.3645630) |  | 0 | Recent policy initiatives have acknowledged the importance of disaggregatingdata pertaining to diverse Asian ethnic communities to gain a morecomprehensive understanding of their current status and to improve theiroverall well-being. However, research on anti-Asian racism has thus far fallenshort of properly incorporating data disaggregation practices. Our studyaddresses this gap by collecting 12-month-long data from X (formerly known asTwitter) that contain diverse sub-ethnic group representations within Asiancommunities. In this dataset, we break down anti-Asian toxic messages based onboth temporal and ethnic factors and conduct a series of comparative analysesof toxic messages, targeting different ethnic groups. Using temporalpersistence analysis, n-gram-based correspondence analysis, and topicmodeling, this study provides compelling evidence that anti-Asian messagescomprise various distinctive narratives. Certain messages targeting sub-ethnicAsian groups entail different topics that distinguish them from those targetingAsians in a generic manner or those aimed at major ethnic groups, such asChinese and Indian. By introducing several techniques that facilitatecomparisons of online anti-Asian hate towards diverse ethnic communities, thisstudy highlights the importance of taking a nuanced and disaggregated approachfor understanding racial hatred to formulate effective mitigation strategies. | Fan Wu, Sanyam Lakhanpal, Qian Li, Kookjin Lee, Doowon Kim, Heewon Chae, Kyounghee Hazel Kwon | Arizona State University Tempe; University of Tennessee Knoxville |
| 682 |  |  [Global News Synchrony and Diversity During the Start of the COVID-19 Pandemic](https://doi.org/10.1145/3589334.3645645) |  | 0 | News coverage profoundly affects how countries and individuals behave ininternational relations. Yet, we have little empirical evidence of how newscoverage varies across countries. To enable studies of global news coverage, wedevelop an efficient computational methodology that comprises three components:(i) a transformer model to estimate multilingual news similarity; (ii) a globalevent identification system that clusters news based on a similarity network ofnews articles; and (iii) measures of news synchrony across countries and newsdiversity within a country, based on country-specific distributions of newscoverage of the global events. Each component achieves state-of-the artperformance, scaling seamlessly to massive datasets of millions of newsarticles. We apply the methodology to 60 million news articles publishedglobally between January 1 and June 30, 2020, across 124 countries and 10languages, detecting 4357 news events. We identify the factors explainingdiversity and synchrony of news coverage across countries. Our study revealsthat news media tend to cover a more diverse set of events in countries withlarger Internet penetration, more official languages, larger religiousdiversity, higher economic inequality, and larger populations. Coverage of newsevents is more synchronized between countries that not only activelyparticipate in commercial and political relations – such as, pairs ofcountries with high bilateral trade volume, and countries that belong to theNATO military alliance or BRICS group of major emerging economies – but alsocountries that share certain traits: an official language, high GDP, and highdemocracy indices. | Xi Chen, Scott A. Hale, David Jurgens, Mattia Samory, Ethan Zuckerman, Przemyslaw A. Grabowicz | University of Oxford & Meedan Oxford; University of Massachusetts Amherst Amherst; University of Michigan Ann Arbor; Sapienza University of Rome Rome |
| 683 |  |  [Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries](https://doi.org/10.1145/3589334.3645643) |  | 0 | Large language models (LLMs) are transforming the ways the general public accesses and consumes information. Their influence is particularly pronounced in pivotal sectors like healthcare, where lay individuals are increasingly appropriating LLMs as conversational agents for everyday queries. While LLMs demonstrate impressive language understanding and generation proficiencies, concerns regarding their safety remain paramount in these high-stake domains. Moreover, the development of LLMs is disproportionately focused on English. It remains unclear how these LLMs perform in the context of non-English languages, a gap that is critical for ensuring equity in the real-world use of these systems.This paper provides a framework to investigate the effectiveness of LLMs as multi-lingual dialogue systems for healthcare queries. Our empirically-derived framework XlingEval focuses on three fundamental criteria for evaluating LLM responses to naturalistic human-authored health-related questions: correctness, consistency, and verifiability. Through extensive experiments on four major global languages, including English, Spanish, Chinese, and Hindi, spanning three expert-annotated large health Q A datasets, and through an amalgamation of algorithmic and human-evaluation strategies, we found a pronounced disparity in LLM responses across these languages, indicating a need for enhanced cross-lingual capabilities. We further propose XlingHealth, a cross-lingual benchmark for examining the multilingual capabilities of LLMs in the healthcare context. Our findings underscore the pressing need to bolster the cross-lingual capacities of these models, and to provide an equitable information ecosystem accessible to all. | Yiqiao Jin, Mohit Chandra, Gaurav Verma, Yibo Hu, Munmun De Choudhury, Srijan Kumar |  |
| 684 |  |  [Bridging or Breaking: Impact of Intergroup Interactions on Religious Polarization](https://doi.org/10.1145/3589334.3645675) |  | 0 | While exposure to diverse viewpoints may reduce polarization, it can also have a backfire effect and exacerbate polarization when the discussion is adversarial. Here, we examine the question whether intergroup interactions around important events affect polarization between majority and minority groups in social networks. We compile data on the religious identity of nearly 700,000 Indian Twitter users engaging in COVID-19-related discourse during 2020. We introduce a new measure for an individual's group conformity based on contextualized embeddings of tweet text, which helps us assess polarization between religious groups. We then use a meta-learning framework to examine heterogeneous treatment effects of intergroup interactions on an individual's group conformity in the light of communal, political, and socio-economic events. We find that for political and social events, intergroup interactions reduce polarization. This decline is weaker for individuals at the extreme who already exhibit high conformity to their group. In contrast, during communal events, intergroup interactions can increase group conformity. Finally, we decompose the differential effects across religious groups in terms of emotions and topics of discussion. The results show that the dynamics of religious polarization are sensitive to the context and have important implications for understanding the role of intergroup interactions. | Rochana Chaturvedi, Sugat Chaturvedi, Elena Zheleva |  |
| 685 |  |  [ARES: Predictable Traffic Engineering under Controller Failures in SD-WANs](https://doi.org/10.1145/3589334.3645321) |  | 0 | Emerging web applications (e.g., video streaming and Web of Things applications) account for a large share of traffic in Wide Area Networks (WANs) and provide traffic with various Quality of Service (QoS) requirements. Software-Defined Wide Area Networks (SD-WANs) offer a promising opportunity to enhance the performance of Traffic Engineering (TE), which aims to enable differentiable QoS for numerous web applications. Nevertheless, SD-WANs are managed by controllers, and unpredictable controller failures may undermine flexible network management. Switches previously controlled by the failed controllers may become offline, and flows traversing these offline switches lose the path programmability to route flows on available forwarding paths. Thus, these offline flows cannot be routed/rerouted on previous paths to accommodate potential traffic variations, leading to severe TE performance degradation. Existing recovery solutions reassign offline switches to other active controllers to recover the degraded path programmability but fail to promise good TE performance since higher path programmability does not necessarily guarantee satisfactory TE performance. In this paper, we propose ARES to provide predictable TE performance under controller failures. We formulate an optimization problem to maintain predictable TE performance by jointly considering fine-grained flow-controller reassignment using P4 Runtime and flow rerouting and propose ARES to efficiently solve this problem. Extensive simulation results demonstrate that our problem formulation exhibits comparable load balancing performance to optimal TE solution without controller failures, and the proposed ARES significantly improves average load balancing performance by up to 43.36% with low computation time compared with existing solutions. | Songshi Dou, Li Qi, Zehua Guo |  |
| 686 |  |  [QUIC is not Quick Enough over Fast Internet](https://doi.org/10.1145/3589334.3645323) |  | 0 | QUIC is a multiplexed transport-layer protocol over UDP and comes with enforced encryption. It is expected to be a game-changer in improving web application performance. Together with the network layer and layers below, UDP, QUIC, and HTTP/3 form a new protocol stack for future network communication, whose current counterpart is TCP, TLS, and HTTP/2. In this study, to understand QUIC's performance over high-speed networks and its potential to replace the TCP stack, we carry out a series of experiments to compare the UDP+QUIC+HTTP/3 (QUIC) stack and the TCP+TLS+HTTP/2 (HTTP/2) stack. Preliminary measurements on file download reveal that QUIC suffers from a data rate reduction compared to HTTP/2 across different hosts. | Xumiao Zhang, Shuowei Jin, Yi He, Ahmad Hassan, Z. Morley Mao, Feng Qian, ZhiLi Zhang | University of Minnesota; University of Michigan; University of Southern California |
| 687 |  |  [A Multifaceted Look at Starlink Performance](https://doi.org/10.1145/3589334.3645328) |  | 0 | In recent years, Low-Earth Orbit (LEO) mega-constellations have emerged as a promising network technology and have ushered in a new era for democratizing Internet access. The Starlink network from SpaceX stands out as the only consumer-facing LEO network with over 2M+ customers and more than 4000 operational satellites. In this paper, we conduct the first-of-its-kind extensive multi-faceted analysis of Starlink network performance leveraging several measurement sources. First, based on 19.2M crowdsourced M-Lab speed test measurements from 34 countries since 2021, we analyze Starlink global performance relative to terrestrial cellular networks. Second, we examine Starlink's ability to support real-time web-based latency and bandwidth-critical applications by analyzing the performance of (i) Zoom video conferencing, and (ii) Luna cloud gaming, comparing it to 5G and terrestrial fiber. Third, we orchestrate targeted measurements from Starlink-enabled RIPE Atlas probes to shed light on the last-mile Starlink access and other factors affecting its performance globally. Finally, we conduct controlled experiments from Starlink dishes in two countries and analyze the impact of globally synchronized "15-second reconfiguration intervals" of the links that cause substantial latency and throughput variations. Our unique analysis provides revealing insights on global Starlink functionality and paints the most comprehensive picture of the LEO network's operation to date. | Nitinder Mohan, Andrew E. Ferguson, Hendrik Cech, Rohan Bose, Prakita Rayyan Renatin, Mahesh K. Marina, Jörg Ott | The University of Edinburgh, Edinburgh, United Kingdom; Technical University of Munich, Munich, Germany; Technical University of Munich; University of Edinburgh United Kingdom |
| 688 |  |  [GEES: Enabling Location Privacy-Preserving Energy Saving in Multi-Access Edge Computing](https://doi.org/10.1145/3589334.3645329) |  | 0 | The global deployment of the 5G network has led to a substantial increase in the deployment of edge servers to host web applications, catering to the growing demand for low service latency by edge web users. Yet, running edge servers 24/7 leads to enormous energy consumption and excessive carbon emissions. Energy-efficient edge resource provision is desired to achieve sustainable development goals in the new multi-access edge computing (MEC) architecture. Recently, several approaches have been proposed to solve the demand response problem for energy saving in cloud computing and MEC. However, accurate location information of edge web users should always be provided, which sacrifices users' privacy. To protect edge web users' location privacy while saving energy in MEC, we systematically formulate this location privacy-preserving edge demand response (LEDR) problem. To solve the LEDR problem effectively and efficiently, we propose a system named GEES by incorporating differential geo-obfuscation to secure user privacy while maximizing system utility and energy efficiency through inferences with theoretical analysis. Extensive and comprehensive experiments are conducted based on a synthetic real-world dataset, and the results demonstrate that GEES outperforms representative approaches by 23.02%, 31.47%, and 17.29% on average in terms of energy efficiency, user privacy and system utility. | Ziqi Wang, Xiaoyu Xia, Minhui Xue, Ibrahim Khalil, Minghui Liwang, Xun Yi |  |
| 689 |  |  [DirectFaaS: A Clean-Slate Network Architecture for Efficient Serverless Chain Communications](https://doi.org/10.1145/3589334.3645333) |  | 0 | Serverless computing, also known as Function-as-a-Service (FaaS), triggers web applications in the form of function chains. It uses a central orchestrator to route all requests from end-users and internal functions. Such architecture simplifies application deployment for developers. However, the convenient centralized network architecture compromises the efficiency of function chain communications. Specifically, (i) a centralized API gateway assists in routing requests between functions. This indirect routing scheme raises invocation latency. (ii) The control flow for invoking functions and the data flow for passing function data packets are both forwarded by the API gateway. This results in the API gateway consuming a significant amount of resources. (iii) All data packets of internal function communications go through the same API gateway. This expands the additional attack surface in multi-tenant scenarios. In this paper, we propose DirectFaaS, a clean-slate network architecture to improve the function chain communication performance. By separating coupled control flow and data flow, DirectFaaS releases the API gateway from heavy traffic forwarding, reducing its resource consumption. For this goal, DirectFaaS exploits the network control capabilities of Software-Defined Networking (SDN) to establish direct data forwarding channels to accelerate function chain invocations. In addition, the data flow constrained by fine-grained network policies consolidates multi-tenant traffic security. We implement the DirectFaaS prototype on the popular OpenFaaS platform. Evaluations under real-world serverless applications show that DirectFaaS achieves a reduction in application execution time by up to 30.9% and CPU consumption by up to 30.1% compared to the current architecture. | Qingyang Zeng, Kaiyu Hou, Xue Leng, Yan Chen |  |
| 690 |  |  [Meet Challenges of RTT Jitter, A Hybrid Internet Congestion Control Algorithm](https://doi.org/10.1145/3589334.3645338) |  | 0 | Congestion control has been a fundamental research focus in web transmission for over 30 years. However, with diverse network scenarios like cellular networks and WiFi, traditional models might no longer accurately describe current network conditions -- we empirically observe that the minimum round-trip time (RTTmin) still varies under different network conditions, challenging the assumption of its constancy in traditional models. In this paper, we model it as a normal distribution based on our measurements and propose a novel congestion control algorithm LingBo. LingBo consists of two phases: an offline trained decision model to achieve goals under different RTTmin distributions, and an online perception scheme to detect the current RTTmin distribution. We evaluate LingBo in various network environments and find it consistently performs well in terms of power metric and throughput compared to recent state-of-the-art baselines. Our code is available at https://github.com/thumedia/LingBo. | Lianchen Jia, Chao Zhou, Tianchi Huang, Chaoyang Li, Lifeng Sun |  |
| 691 |  |  [Towards Energy-efficient Federated Learning via INT8-based Training on Mobile DSPs](https://doi.org/10.1145/3589334.3645341) |  | 0 | AI is making the Web an even cooler place, but also introduces serious privacy risks due to the extensive user data collection. Federated learning (FL), as a privacy-preserving machine learning paradigm, enables mobile devices to collaboratively learn a shared prediction model while keeping all training data on devices. However, a key obstacle towards practical cross-device FL training is huge energy consumption, especially for lightweight mobile devices. In this work, we perform the first-of-its-kind analysis of improving FL performance through low-precision training with an energy-friendly Digital Signal Processor (DSP) on mobile devices. We first demonstrate that directly integrating the state-of-the-art INT8 (8-bit integer) training algorithm and classic FL protocols will significantly degrade the model accuracy. Moreover, we observe that there are still unavoidable frequent quantization operations on devices that cause extreme load stress on DSP-enabled INT8 training. To address the above challenges, we present Q-FedUpdate, an FL framework that efficiently preserves model accuracy with ultra-low energy consumption. It maintains a global full-precision model and allows the tiny model updates to be continuously accumulated, instead of being erased by the quantization. Furthermore, it introduces pipelining technology to parallel CPU-based quantization and DSP-enabled training, which reduces the floating-point computation overhead of frequent data quantization. Extensive experiments show that Q-FedUpdate can effectively reduce the on-device energy consumption by 21×, and accelerate the FL convergence by 6.1× with only 2% accuracy loss. | Jinliang Yuan, Shangguang Wang, Hongyu Li, Daliang Xu, Yuanchun Li, Mengwei Xu, Xuanzhe Liu |  |
| 692 |  |  [FreqMAE: Frequency-Aware Masked Autoencoder for Multi-Modal IoT Sensing](https://doi.org/10.1145/3589334.3645346) |  | 0 | This paper presents FreqMAE, a novel self-supervised learning framework that synergizes masked autoencoding (MAE) with physics-informed insights to capture feature patterns in multi-modal IoT sensor data. FreqMAE enhances latent space representation of sensor data, reducing reliance on data labeling and improving accuracy for AI tasks. Differing from data augmentation-based methods like contrastive learning, FreqMAE's approach eliminates the need for handcrafted transformations. Adapting MAE for IoT sensing signals, we present three contributions from frequency domain insights: First, a Temporal-Shifting Transformer (TS-T) encoder that enables temporal interactions while distinguishing different frequency bands; Second, a factorized multi-modal fusion mechanism for leveraging cross-modal correlations and preserving unique modality features; Third, a hierarchically weighted loss function that emphasizes important frequency components and high Signal-to-Noise Ratio (SNR) samples. Comprehensive evaluations on two sensing applications validate FreqMAE's proficiency in reducing labeling needs and enhancing resilience against domain shifts. | Denizhan Kara, Tomoyoshi Kimura, Shengzhong Liu, Jinyang Li, Dongxin Liu, Tianshi Wang, Ruijie Wang, Yizhuo Chen, Yigong Hu, Tarek F. Abdelzaher |  |
| 693 |  |  [Air-CAD: Edge-Assisted Multi-Drone Network for Real-time Crowd Anomaly Detection](https://doi.org/10.1145/3589334.3645362) |  | 0 | Drones connected via the web are increasingly being used for crowd anomaly detection (CAD). Existing solutions, however, face many challenges, such as low accuracy and high latency due to drones' dynamic shooting distances and angles as well as limited computing and networking capabilities. In this paper, we propose Air-CAD, an edge-assisted multi-drone network that uses air-ground cooperation to achieve fast and accurate CAD. Air-CAD consists of two stages: person detection and multi-feature analysis. To improve CAD accuracy, Air-CAD dynamically adjusts the inference of person detection model based on drones' shooting distances and assigns appropriate feature analysis tasks to drones shooting at variable angles. To achieve fast CAD, edge devices connected to drones are deployed to offload assigned feature analysis tasks from drones. Air-CAD schedules the connection between each drone and edge to accelerate processing based on drone's assigned task and the computing/network resources of the edge device. To validate the performance of Air-CAD, we generate a new simulated human stampede dataset captured from various drone-view recordings. We deploy and evaluate Air-CAD in both simulation and real-world testbed. Experimental results show that Air-CAD achieves 95.33% AUROC and real-time inference latency within 0.47 seconds. | Yuanzheng Tan, Qing Li, Junkun Peng, Zhenhui Yuan, Yong Jiang |  |
| 694 |  |  [λGrapher: A Resource-Efficient Serverless System for GNN Serving through Graph Sharing](https://doi.org/10.1145/3589334.3645383) |  | 0 | Graph Neural Networks (GNNs) have been increasingly adopted for graph analysis in web applications such as social networks. Yet, efficient GNN serving remains a critical challenge due to high workload fluctuations and intricate GNN operations. Serverless computing, thanks to its flexibility and agility, offers on-demand serving of GNN inference requests. Alas, the request-centric serverless model is still too coarse-grained to avoid resource waste. Observing the significant data locality in computation graphs of requests, we propose λGrapher, a serverless system for GNN serving that achieves resource efficiency through graph sharing and fine-grained resource allocation. "Grapher features the following designs: (1) adaptive timeout for request buffering to balance resource efficiency and inference latency, (2) graph-centric scheduling to minimize computation and memory redundancy, and (3) resource-centric function management with fine-grained resource allocation catered to the resource sensitivities of GNN operations and function orchestration optimized to hide communication latency. We implement a prototype of λGrapher based on the representative open-source serverless platform Knative and evaluate it with real-world traces from various web applications. Our results show that λGrapher can achieve an average savings of 61.5% in memory resource and 47.2% in computing resource compared with the state of the arts while ensuring GNN inference latency. | Haichuan Hu, Fangming Liu, Qiangyu Pei, Yongjie Yuan, Zichen Xu, Lin Wang |  |
| 695 |  |  [SPRING: Improving the Throughput of Sharding Blockchain via Deep Reinforcement Learning Based State Placement](https://doi.org/10.1145/3589334.3645386) |  | 0 | Sharding provides an opportunity to overcome the inherent scalability challenges of the blockchain, which is the infrastructure for the next generation of the Web. In a sharding blockchain, the state is partitioned into smaller groups known as "shards." Since the states are placed on different shards, cross-shard transactions are inevitable, which is detrimental to the performance of the sharding blockchain. Existing solutions place states based on heuristic algorithms or redistribute states via graph-partitioning-based methods, which are either less effective or costly. In this paper, we present SPRING, the first deep-reinforcement-learning(DRL)-based sharding framework for state placement. SPRING formulates the state placement as a Markov Decision Process, which considers the cross-shard transaction ratio and workload balancing and employs DRL to learn the effective state placement policy. Experimental results based on real Ethereum transaction data demonstrate the superiority of SPRING compared to other state placement solutions. In particular, it decreases the cross-shard transaction ratio by up to 26.63% and boosts throughput by up to 36.03%, all without unduly sacrificing the workload balance among shards. Moreover, updating the training model and making decisions takes only 0.1s and 0.002s, respectively, which shows the overhead is acceptable. | Pengze Li, Mingxuan Song, Mingzhe Xing, Zhen Xiao, Qiuyu Ding, Shengjie Guan, Jieyi Long |  |
| 696 |  |  [Supervised Fine-Tuning for Unsupervised KPI Anomaly Detection for Mobile Web Systems](https://doi.org/10.1145/3589334.3645392) |  | 0 | With the rapid development of cellular networks, wireless base stations (WBSes) have become crucial infrastructure for mobile web systems. To ensure service quality, operators constantly monitor the operation status of WBSes and deploy anomaly detection methods to identify anomalies promptly. After the deployment of anomaly detection methods, operators periodically collect feedback, which holds significant value in improving anomaly detection performance. In real-world industrial environments, the frequency of false negative feedback is usually very low, and the newly generated data's distribution can differ significantly from that of the original training data. Therefore, the feedback-based performance improvement of the previously proposed methods is limited. In this paper, we propose AnoTuner, which incorporates a false negative augmentation mechanism to generate similar false negative feedback cases, effectively compensating for the low feedback frequency. Additionally, we introduce a Two-Stage Active Learning (TSAL) mechanism that minimizes data contamination issues caused by the difference between the distribution of feedback data and that of the training data. Experiments conducted on the real-world data collected from a top-tier global Internet Service Provider (ISP) demonstrate that the performance improvement of AnoTuner after feedback-based fine-tuning is significantly higher than that of the best baseline method. | Zhaoyang Yu, Shenglin Zhang, Mingze Sun, Yingke Li, Yankai Zhao, Xiaolei Hua, Lin Zhu, Xidao Wen, Dan Pei |  |
| 697 |  |  [NCTM: A Novel Coded Transmission Mechanism for Short Video Deliveries](https://doi.org/10.1145/3589334.3645387) |  | 0 | With the rapid popularity of short video applications, a large number of short video transmissions occupy the bandwidth, placing a heavy load on the Internet. Due to the extensive number of short videos and the predominant service for mobile users, traditional approaches (e.g., CDN delivery, edge caching) struggle to achieve the expected performance, leading to a significant number of redundant transmissions. In order to reduce the amount of traffic, we design a Novel Coded Transmission Mechanism (NCTM), which transmits XOR-coded data instead of the original video content. NCTM caches the short videos that users have already watched in user devices, and encodes, multicasts, and decodes XOR-coded files separately at the server, edge nodes, and clients, with the assistance of cached content. This approach enables NCTM to deliver more short video data given the limited bandwidth. Our extensive trace-driven simulations show how NCTM reduces network load by 3.02%-14.75%, cuts peak traffic by 23.01%, and decreases rebuffering events by 43%-85% in comparison to a CDN-supported scheme and a naive edge caching scheme. Additionally, NCTM also increases the user's buffered video duration by 1.21x-13.53x, ensuring improved playback smoothness. | Zhenge Xu, Qing Li, Wanxin Shi, Yong Jiang, Zhenhui Yuan, Peng Zhang, GabrielMiro Muntean |  |
| 698 |  |  [InArt: In-Network Aggregation with Route Selection for Accelerating Distributed Training](https://doi.org/10.1145/3589334.3645394) |  | 0 | Deep learning has brought about a revolutionary transformation in network applications, particularly in domains like e-commerce and online advertising. Distributed training (DT), as a critical means to expedite model training, has progressively emerged as a key foundational infrastructure for such applications. However, with the rapid advancement of hardware accelerators, the performance bottleneck in DT has shifted from computation to communication. In-network aggregation (INA) solutions have shown promise in alleviating the communication bottleneck. Regrettably, current INA solutions primarily focus on improving efficiency under the traditional parameter server (PS) architecture and do not fully address the communication bottleneck caused by limited PS ingress bandwidth. To bridge this gap, we propose InArt, the first work to introduce INA with routing selection in a multi-PS architecture. InArt employs a multi-PS architecture to split DT tasks among multiple PSs, and selects appropriate routing schemes to fully harness INA capabilities. To accommodate traffic dynamics, InArt adopts a two-phase approach: splitting the training model among multiple parameter servers and selecting routing paths for INA. We propose Lagrange multiplier and randomized rounding algorithms for these phases, respectively. We implement InArt and evaluate its performance through experiments on physical platforms (Tofino switches) and Mininet emulation (P4 Software Switches). Experimental results show that InArt can reduce communication time by 48%\!\sim57\!% compared with state-of-the-art solutions. | Jiawei Liu, Yutong Zhai, Gongming Zhao, Hongli Xu, Jin Fang, Zhen Zeng, Ying Zhu |  |
| 699 |  |  [FusionRender: Harnessing WebGPU's Power for Enhanced Graphics Performance on Web Browsers](https://doi.org/10.1145/3589334.3645395) |  | 0 | Graphics rendering on web browsers serves as the foundation for numerous web applications. Compared with the widely employed WebGL, the next-generation web graphics API, WebGPU, demonstrates an enhanced capacity to adapt to modern GPU features, boasting more significant potential. However, our experiment shows that the performance of current graphics rendering frameworks based on WebGPU lags behind those built on WebGL. Such discrepancy primarily arises from an incomplete alignment with WebGPU's distinctive features. The individual rendering of each graphic leads to redundant communication between the CPU and GPU. To enhance the graphics performance on the web, we introduce the FusionRender to harness the power of WebGPU. To mitigate redundant communication, FusionRender assigns a unique signature to each object and employs these signatures for grouping, enabling the consolidation of graphics rendering whenever possible. In simulated experiments involving the rendering of multiple objects, FusionRender improves the rendering performance by 29.3%-122.1% compared with the existing optimal baseline. In real cases with more complex features, performance improvement ranges from 9.4% to 39.7%. Additionally, FusionRender exhibits robust performance enhancement across various devices and browsers. | Weichen Bi, Yun Ma, Yudong Han, Yifan Chen, Deyu Tian, Jiaqi Du |  |
| 700 |  |  [FedDSE: Distribution-aware Sub-model Extraction for Federated Learning over Resource-constrained Devices](https://doi.org/10.1145/3589334.3645416) |  | 0 | Sub-model extraction based federated learning has emerged as a popular strategy for training models on resource-constrained devices. However, existing methods treat all clients equally and extract sub-models using predetermined rules, which disregard the statistical heterogeneity across clients and may lead to fierce competition among them. Specifically, this paper identifies that when making predictions, different clients tend to activate different neurons of the entire model related to their respective distributions. If highly activated neurons from some clients with one distribution are incorporated into the sub-model allocated to other clients with different distributions, they will be forced to fit the new distributions, which can hinder their activation over the previous clients and result in a performance reduction. Motivated by this finding, we propose a novel method called FedDSE, which can reduce the conflicts among clients by extracting sub-models based on the data distribution of each client. The core idea of FedDSE is to empower each client to adaptively extract neurons from the entire model based on their activation over the local dataset. We theoretically show that FedDSE can achieve an improved classification score and convergence over general neural networks with the ReLU activation function. Experimental results on various datasets and models show that FedDSE outperforms all state-of-the-art baselines. | Haozhao Wang, Yabo Jia, Meng Zhang, Qinghao Hu, Hao Ren, Peng Sun, Yonggang Wen, Tianwei Zhang |  |
| 701 |  |  [BlockDFL: A Blockchain-based Fully Decentralized Peer-to-Peer Federated Learning Framework](https://doi.org/10.1145/3589334.3645425) |  | 0 | Federated learning (FL) enables collaborative training of machine learningmodels without sharing training data. Traditional FL heavily relies on atrusted centralized server. Although decentralized FL eliminates the centraldependence, it may worsen the other inherit problems faced by FL such aspoisoning attacks and data representation leakage due to insufficientrestrictions on the behavior of participants, and heavy communication cost,especially in fully decentralized scenarios, i.e., peer-to-peer (P2P) settings.In this paper, we propose a blockchain-based fully decentralized P2P frameworkfor FL, called BlockDFL. It takes blockchain as the foundation, leveraging theproposed PBFT-based voting mechanism and two-layer scoring mechanism tocoordinate FL among peer participants without mutual trust, while effectivelydefending against poisoning attacks. Gradient compression is introduced tolowering communication cost and prevent data from being reconstructed fromtransmitted model updates. Extensive experiments conducted on two real-worlddatasets exhibit that BlockDFL obtains competitive accuracy compared tocentralized FL and can defend poisoning attacks while achieving efficiency andscalability. Especially when the proportion of malicious participants is ashigh as 40existing fully decentralized P2P FL frameworks based on blockchain. | Zhen Qin, Xueqiang Yan, Mengchu Zhou, Shuiguang Deng | • S. Dustdar is with Distributed Systems Group; Zhejiang University College of Computer Science and Technology; Huawei Technologies Wireless Technology Lab |
| 702 |  |  [Incentive and Dynamic Client Selection for Federated Unlearning](https://doi.org/10.1145/3589334.3645462) |  | 0 | With the development of AI-Generated Content (AIGC), data is becoming increasingly important, while the right of data to be forgotten, which is defined in the General Data Protection Regulation (GDPR) and permits data owners to remove information from AIGC models, is also arising. To protect this right in a distributed manner corresponding to federated learning, federated unlearning is employed to eliminate history model updates and unlearn the global model to mitigate data effects from the targeted clients intending to withdraw from training tasks. To diminish centralization failures, the hierarchical federated framework that is distributed and collaborative can be integrated into the unlearning process, wherein each cluster can support multiple AIGC tasks. However, two issues remain unexplored in current federated unlearning solutions: 1) getting remaining clients, those not withdraw from the task, to join the unlearning process, which demands additional resources and notably has fewer benefits than federated learning, particularly in achieving the original performance via alternative unlearning processes and 2) exploring mechanisms for dynamic unlearning in the selection of remaining clients possessing unbalanced data to avoid starting the unlearning from scratch. We initially consider a two-level incentive and unlearning mechanism to address the aforementioned challenges. At the lower level, we utilize evolutionary game theory to model the dynamic participation process, aiming to attract remaining clients to participate in retraining tasks. At the upper level, we integrate deep reinforcement learning into federated unlearning to dynamically select remaining clients to join the unlearning process to mitigate the bias introduced by the unbalanced data distribution among clients. Experimental results demonstrate that the proposed mechanisms outperform comparative methods, enhancing utilities and improving accuracy. | Yijing Lin, Zhipeng Gao, Hongyang Du, Dusit Niyato, Jiawen Kang, Xiaoyuan Liu |  |
| 703 |  |  [Accelerating the Decentralized Federated Learning via Manipulating Edges](https://doi.org/10.1145/3589334.3645509) |  | 0 | Federated learning enables collaborative AI training across organizations without compromising data privacy. Decentralized federated learning (DFL) improves this by offering enhanced reliability and security through peer-to-peer (P2P) model sharing. However, DFL faces challenges in terms of slow convergence rate due to complex P2P graphs. To address this issue, we propose an efficient algorithm to accelerate DFL by introducing a limited number of k of edges into the P2P graphs. Specifically, we establish a connection between the convergence rate and the second smallest eigenvalue of the laplacian matrix of the P2P graph. We prove that finding the optimal set of edges to maximize this eigenvalue is an NP-complete problem. Our quantitative analysis shows the positive effect of strategic edge additions on improving this eigenvalue. Based on the analysis, we then propose an efficient algorithm to compute the best set of candidate edges to maximize the second smallest eigenvalue, and consequently the convergence rate is maximized. Our algorithm has a low time complexity of O(krn^2). Experimental results on diverse datasets validate the effectiveness of our proposed algorithms in accelerating DFL convergence. | Mingyang Zhou, Gang Liu, Kezhong Lu, Rui Mao, Hao Liao |  |
| 704 |  |  [How Few Davids Improve One Goliath: Federated Learning in Resource-Skewed Edge Computing Environments](https://doi.org/10.1145/3589334.3645544) |  | 0 | Real-world deployment of federated learning requires orchestrating clients with widely varied compute resources, from strong enterprise-grade devices in data centers to weak mobile and Web-of-Things devices. Prior works have attempted to downscale large models for weak devices and aggregate shared parts among heterogeneous models. A typical architectural assumption is that there are equally many strong and weak devices. In reality, however, we often encounter resource skew where a few (1 or 2) strong devices hold substantial data resources, alongside many weak devices. This poses challenges-the unshared portion of the large model rarely receives updates or gains benefits from weak collaborators. We aim to facilitate reciprocal benefits between strong and weak devices in resource-skewed environments. We propose RecipFL, a novel framework featuring a server-side graph hypernetwork. This hypernetwork is trained to produce parameters for personalized client models adapted to device capacity and unique data distribution. It effectively generalizes knowledge about parameters across different model architectures by encoding computational graphs. Notably, RecipFL is agnostic to model scaling strategies and supports collaboration among arbitrary neural networks. We establish the generalization bound of RecipFL through theoretical analysis and conduct extensive experiments with various model architectures. Results show that RecipFL improves accuracy by 4.5% and 7.4% for strong and weak devices respectively, incentivizing both devices to actively engage in federated learning. | Jiayun Zhang, Shuheng Li, Haiyu Huang, Zihan Wang, Xiaohan Fu, Dezhi Hong, Rajesh K. Gupta, Jingbo Shang |  |
| 705 |  |  [Privacy-Preserving and Fairness-Aware Federated Learning for Critical Infrastructure Protection and Resilience](https://doi.org/10.1145/3589334.3645545) |  | 0 | The energy industry is undergoing significant transformations as it strives to achieve net-zero emissions and future-proof its infrastructure, where every participant in the power grid has the potential to both consume and produce energy resources. Federated learning -- which enables multiple participants to collaboratively train a model without aggregating the training data -- becomes a viable technology. However, the global model parameters that have to be shared for optimization are still susceptible to training data leakage. In this work, we propose confined gradient descent (CGD) that enhances the privacy of federated learning by eliminating the sharing of global model parameters. CGD exploits the fact that a gradient descent optimization can start with a set of discrete points and converges to another set in the neighborhood of the global minimum of the objective function. As such, each participant can independently initiate its own private global model~(referred to as the confined model ), and collaboratively learn it towards the optimum. The updates to their own models are worked out in a secure collaborative way during the training process.In such a manner, CGD retains the ability of learning from distributed data but greatly diminishes information sharing. Such a strategy also allows the proprietary confined models to adapt to the heterogeneity in federated learning, providing inherent benefits of fairness. We theoretically and empirically demonstrate that decentralized CGD øne provides a stronger differential privacy (DP) protection; \two is robust against the state-of-the-art poisoning privacy attacks; þree results in bounded fairness guarantee among participants; and \four provides high test accuracy (comparable with centralized learning) with a bounded convergence rate over four real-world datasets. | Yanjun Zhang, Ruoxi Sun, Liyue Shen, Guangdong Bai, Minhui Xue, Mark Huasong Meng, Xue Li, Ryan K. L. Ko, Surya Nepal |  |
| 706 |  |  [Making Cloud Spot Instance Interruption Events Visible](https://doi.org/10.1145/3589334.3645548) |  | 0 | Public cloud computing providers offer a surplus of computing resources at a lower price with a service of a spot instance. Despite the possible great cost savings from using spot instances, sudden resource interruption can occur as resource demand changes. To help users estimate cost savings and the possibility of interruption when using spot instances, vendors provide diverse datasets. However, the effectiveness of using the datasets has not yet been quantitatively evaluated, and many users still rely on the guess when choosing spot instances. To help users lower the chance of interruption of the spot instance for reliable usage, in this paper, we thoroughly analyze various datasets of the spot instance and present the feasibility for value prediction. Then, to measure how the public datasets reflect real-world spot instance interruption events, we conduct real-world experiments for spot instances of AWS, Azure, and Google Cloud. Combining the dataset analysis, modeling, and the real-world spot instance interruption experiment, we present a significant improvement in reducing the possibility of interruption events. | Kyunghwan Kim, Kyungyong Lee |  |
| 707 |  |  [E2Usd: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series](https://doi.org/10.1145/3589334.3645593) |  | 0 | We propose E2USD that enables efficient-yet-accurate unsupervised MTS statedetection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor(FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that togetherencode input MTSs at low computational overhead. Additionally, we propose aFalse Negative Cancellation Contrastive Learning method (FNCCLearning) tocounteract the effects of false negatives and to achieve more cluster-friendlyembedding spaces. To reduce computational overhead further in streamingsettings, we introduce Adaptive Threshold Detection (ADATD). Comprehensiveexperiments with six baselines and six datasets offer evidence that E2USD iscapable of SOTA accuracy at significantly reduced computational overhead. Ourcode is available at https://github.com/AI4CTS/E2Usd. | Zhichen Lai, Huan Li, Dalin Zhang, Yan Zhao, Weizhu Qian, Christian S. Jensen |  |
| 708 |  |  [Robust Route Planning under Uncertain Pickup Requests for Last-mile Delivery](https://doi.org/10.1145/3589334.3645595) |  | 0 | Empowered by the widespread adoption of Internet of Things (IoT) devices and smartphones, last-mile delivery services have evolved to accommodate both delivery and pickup tasks. An essential challenge in last-mile delivery is efficiently planning routes for couriers to handle pre-scheduled delivery requests as well as stochastic pickup requests. Existing work approaches this problem by either adjusting routes on the fly when new requests arise or preplanning routes based on predicted future pickup requests. However, these methods either compromise the optimality of planned routes or heavily rely on the accuracy of predictions. In this work, we take conformal prediction as an opportunity to address the issue of prediction uncertainty. We design ROPU, a novel courier route planning framework for logistics systems that incorporates conformal prediction into reinforcement learning. Our work advances the existing work from two aspects: (i) Pickup request prediction utilizes spatial-temporal conformal prediction to capture historical pickup request patterns, providing a unified spatial-temporal conformal interval with high confidence (ii) A spatial-temporal attention network assesses location importance from various perspectives and enables the actor to perceive time and integrate the spatial-temporal conformal interval. We implement and evaluate ROPU on one of the largest logistics platforms. Extensive experiment results demonstrate that our method outperforms other state-of-the-art methods with improvements of at least 30.49% in the pickup overdue rate, 25.00% in the delivery overdue rate, and 5.49% in the traveling distance metric. | Hua Yan, Heng Tan, Haotian Wang, Desheng Zhang, Yu Yang |  |
| 709 |  |  [Unity is Strength? Benchmarking the Robustness of Fusion-based 3D Object Detection against Physical Sensor Attack](https://doi.org/10.1145/3589334.3645612) |  | 0 | As a safety-critical application, Autonomous Driving (AD) has received growing attention from security researchers. AD heavily relies on sensors for perception. However, sensors themselves are susceptible to various threats since they are exposed to the environments and vulnerable to malicious or interfering signals. To cope with situations where a sensor might malfunction, Multi Sensor Fusion (MSF) was proposed as a general strategy to enhance the robustness of perception models. In this paper, we focus on investigating MSF security under various sensor attacks and wish to answer the following research questions: (1)Does fusion enhance robustness or not? (2)How does the architecture of the fusion model influence robustness? To this end, we establish a rigorous benchmark for fusion-based 3D object detection robustness. Our new benchmark features 5 types of LiDAR attacks and 6 types of camera attacks. Different from traditional benchmarks, we take the physical sensor attacks into consideration during the corruption construction. Then, we systematically investigate 7 MSF-based and 5 single-modality 3D object detection models with different fusion architectures. We release the benchmarks and codes to facilitate future studies: \textcolorblue \hrefhttps://github.com/Jinzizhisir/PSA-Fusion https://github.com/Jinzizhisir/PSA-Fusion . | Zizhi Jin, Xuancun Lu, Bo Yang, Yushi Cheng, Chen Yan, Xiaoyu Ji, Wenyuan Xu |  |
| 710 |  |  [WEFix: Intelligent Automatic Generation of Explicit Waits for Efficient Web End-to-End Flaky Tests](https://doi.org/10.1145/3589334.3645628) |  | 0 | Web end-to-end (e2e) testing evaluates the workflow of a web application. Itsimulates real-world user scenarios to ensure the application flows behave asexpected. However, web e2e tests are notorious for being flaky, i.e., the testscan produce inconsistent results despite no changes to the code. One commontype of flakiness is caused by nondeterministic execution orders between thetest code and the client-side code under test. In particular, UI-basedflakiness emerges as a notably prevalent and challenging issue to fix becausethe test code has limited knowledge about the client-side code execution. Inthis paper, we propose WEFix, a technique that can automatically generate fixcode for UI-based flakiness in web e2e testing. The core of our approach is toleverage browser UI changes to predict the client-side code execution andgenerate proper wait oracles. We evaluate the effectiveness and efficiency ofWEFix against 122 web e2e flaky tests from seven popular real-world projects.Our results show that WEFix dramatically reduces the overhead (from 3.7×to 1.25×) while achieving a high correctness (98 | Xinyue Liu, Zihe Song, Weike Fang, Wei Yang, Weihang Wang |  |
| 711 |  |  [SatGuard: Concealing Endless and Bursty Packet Losses in LEO Satellite Networks for Delay-Sensitive Web Applications](https://doi.org/10.1145/3589334.3645639) |  | 0 | Delay-sensitive Web services are crucial applications in emerging low-earth orbit (LEO) satellite networks (LSNs). However, our real-world measurement study based on SpaceX's Starlink, the most widely used commercial LSN today, reveals that the endless and bursty packet losses over unstable LEO satellite links impose significant challenges on guaranteeing the quality of experience (QoE) of Web applications. We propose SatGuard, a distributed in-orbit loss recovery mechanism that can reduce user-perceived delay by completely concealing packet losses in the unstable and lossy LSN environment from endpoints. Specifically, SatGuard adopts a series of techniques to: (i) correctly migrate on-board packet buffer to support link-local retransmission under LEO dynamics; (ii) efficiently detect packet losses on satellite links; and (iii) ensure packet ordering for endpoints. We implement a SatGuard prototype, and conduct extensive trace-driven evaluations guided by public constellation information and real-world measurements. Our experiments demonstrate that, in comparison with other state-of-the-art approaches, SatGuard can significantly improve Web-based QoE, by reducing: (i) up to 48.3% of page load time for Web browsing; and (ii) up to 57.4% end-to-end communication delay for WebRTC. | Jihao Li, Hewu Li, Zeqi Lai, Qian Wu, Yijie Liu, Qi Zhang, Yuanjie Li, Jun Liu |  |
| 712 |  |  [More Than Routing: Joint GPS and Route Modeling for Refine Trajectory Representation Learning](https://doi.org/10.1145/3589334.3645644) |  | 0 | Trajectory representation learning plays a pivotal role in supporting variousdownstream tasks. Traditional methods in order to filter the noise in GPStrajectories tend to focus on routing-based methods used to simplify thetrajectories. However, this approach ignores the motion details contained inthe GPS data, limiting the representation capability of trajectoryrepresentation learning. To fill this gap, we propose a novel representationlearning framework that Joint GPS and Route Modelling based on self-supervisedtechnology, namely JGRM. We consider GPS trajectory and route as the two modesof a single movement observation and fuse information through inter-modalinformation interaction. Specifically, we develop two encoders, each tailoredto capture representations of route and GPS trajectories respectively. Therepresentations from the two modalities are fed into a shared transformer forinter-modal information interaction. Eventually, we design threeself-supervised tasks to train the model. We validate the effectiveness of theproposed method on two real datasets based on extensive experiments. Theexperimental results demonstrate that JGRM outperforms existing methods in bothroad segment representation and trajectory representation tasks. Our sourcecode is available at Anonymous Github. | Zhipeng Ma, Zheyan Tu, Xinhai Chen, Yan Zhang, Deguo Xia, Guyue Zhou, Yilun Chen, Yu Zheng, Jiangtao Gong |  |
| 713 |  |  [Cardinality Counting in "Alcatraz": A Privacy-aware Federated Learning Approach](https://doi.org/10.1145/3589334.3645655) |  | 0 | The task of cardinality counting, pivotal for data analysis, endeavors to quantify unique elements within datasets and has significant applications across various sectors like healthcare, marketing, cybersecurity, and web analytics. Current methods, categorized into deterministic and probabilistic, often fail to prioritize data privacy. Given the fragmentation of datasets across various organizations, there is an elevated risk of inadvertently disclosing sensitive information during collaborative data studies using state-of-the-art cardinality counting techniques. This study introduces an innovative privacy-centric solution for the cardinality counting dilemma, leveraging a federated learning framework. Our approach involves employing a locally differentially private data encoding for initial processing, followed by a privacy-aware federated K-means clustering strategy, ensuring that cardinality counting occurs across distinct datasets without necessitating data amalgamation. The efficacy of our methodology is underscored by promising results from tests on both real-world and simulated datasets, pointing towards a transformative approach to privacy-sensitive cardinality counting in contemporary data science. | Nan Wu, Xin Yuan, Shuo Wang, Hongsheng Hu, Minhui Xue | CSIRO's Data61, Sydney, NSW, Australia |
| 714 |  |  [GAMMA: Graph Neural Network-Based Multi-Bottleneck Localization for Microservices Applications](https://doi.org/10.1145/3589334.3645665) |  | 0 | Microservices architecture is quickly replacing monolithic and multi-tier architectures as the implementation choice for large-scale web applications as it allows independent development, scalability, and maintenance. However, even with careful node scheduling and scaling, the microservices applications are still vulnerable to performance degradation due to unexpected (dependent or independent) events like anomalous node behavior, workload interference, or sudden spikes in requests or retries. These events can adversely affect the performance of one or more microservices (bottlenecks), degrading the overall application performance. To ensure a good customer experience and avoid revenue loss, it is crucial to detect and mitigate all bottlenecks swiftly. This work introduces GAMMA, a novel, explainable graph learning model that integrates a mixture of experts to detect multiple bottlenecks. We evaluated GAMMA using a popular open-source benchmarking application deployed on Kubernetes under various practical bottleneck scenarios. Our experimental evaluation results show that GAMMA provides significantly better performance (46% higher F1 score) than existing works that employ deep learning, machine learning, and statistical techniques, demonstrating its ability to detect multiple bottlenecks by learning complex interactions in a microservices architecture. The dataset is made publicly available [49] for reproducibility and further research in the field. | Gagan Somashekar, Anurag Dutt, Mainak Adak, Tania LoridoBotran, Anshul Gandhi |  |
| 715 |  |  [Revisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective](https://doi.org/10.1145/3589334.3645710) |  | 0 | Time series Anomaly Detection (AD) plays a crucial role for web systems.Various web systems rely on time series data to monitor and identify anomaliesin real time, as well as to initiate diagnosis and remediation procedures.Variational Autoencoders (VAEs) have gained popularity in recent decades due totheir superior de-noising capabilities, which are useful for anomaly detection.However, our study reveals that VAE-based methods face challenges in capturinglong-periodic heterogeneous patterns and detailed short-periodic trendssimultaneously. To address these challenges, we propose Frequency-enhancedConditional Variational Autoencoder (FCVAE), a novel unsupervised AD method forunivariate time series. To ensure an accurate AD, FCVAE exploits an innovativeapproach to concurrently integrate both the global and local frequency featuresinto the condition of Conditional Variational Autoencoder (CVAE) tosignificantly increase the accuracy of reconstructing the normal data. Togetherwith a carefully designed "target attention" mechanism, our approach allows themodel to pick the most useful information from the frequency domain for bettershort-periodic trend construction. Our FCVAE has been evaluated on publicdatasets and a large-scale cloud system, and the results demonstrate that itoutperforms state-of-the-art methods. This confirms the practical applicabilityof our approach in addressing the limitations of current VAE-based anomalydetection models. | Zexin Wang, Changhua Pei, Minghua Ma, Xin Wang, Zhihan Li, Dan Pei, Saravan Rajmohan, Dongmei Zhang, Qingwei Lin, Haiming Zhang, Jianhui Li, Gaogang Xie | University of Chinese Academy of Sciences Also with |
| 716 |  |  [Interpretable Knowledge Tracing with Multiscale State Representation](https://doi.org/10.1145/3589334.3645373) |  | 0 | Knowledge Tracing (KT) is vital for education, continuously monitoring students' knowledge states (mastery of knowledge) as they interact with online education materials. Despite significant advancements in deep learning-based KT models, existing approaches often struggle to strike the right balance in granularity, leading to either overly coarse or excessively fine tracing and representation of students' knowledge states, thereby limiting their performance. Additionally, achieving a high-performing model while ensuring interpretability presents a challenge. Therefore, in this paper, we propose a novel approach called Multiscale-state-based Interpretable Knowledge Tracing (MIKT). Specifically, MIKT traces students' knowledge states on two scales: a coarse-grained representation to trace students' domain knowledge state, and a fine-grained representation to monitor their conceptual knowledge state. Furthermore, the classical psychological measurement model, IRT (Item Response Theory), is introduced to explain the prediction process of MIKT, enhancing its interpretability without sacrificing performance. Additionally, we extended the Rasch representation method to effectively handle scenarios where questions are associated with multiple concepts, making it more applicable to real-world situations. We extensively compared MIKT with 20 state-of-the-art KT models on four widely-used public datasets. Experimental results demonstrate that MIKT outperforms other models while maintaining its interpretability. Moreover, experimental observations have revealed that our proposed extended Rasch representation method not only benefits MIKT but also significantly improves the performance of other KT baseline models. The code can be found at https://github.com/lilstrawberry/MIKT. | Jianwen Sun, Fenghua Yu, Qian Wan, Qing Li, Sannyuya Liu, Xiaoxuan Shen |  |
| 717 |  |  [COLA: Cross-city Mobility Transformer for Human Trajectory Simulation](https://doi.org/10.1145/3589334.3645469) |  | 0 | Human trajectory data produced by daily mobile devices has proven itsusefulness in various substantial fields such as urban planning and epidemicprevention. In terms of the individual privacy concern, human trajectorysimulation has attracted increasing attention from researchers, targeting atoffering numerous realistic mobility data for downstream tasks. Nevertheless,the prevalent issue of data scarcity undoubtedly degrades the reliability ofexisting deep learning models. In this paper, we are motivated to explore theintriguing problem of mobility transfer across cities, grasping the universalpatterns of human trajectories to augment the powerful Transformer withexternal mobility data. There are two crucial challenges arising in theknowledge transfer across cities: 1) how to transfer the Transformer to adaptfor domain heterogeneity; 2) how to calibrate the Transformer to adapt forsubtly different long-tail frequency distributions of locations. To addressthese challenges, we have tailored a Cross-city mObiLity trAnsformer (COLA)with a dedicated model-agnostic transfer framework by effectively transferringcross-city knowledge for human trajectory simulation. Firstly, COLA divides theTransformer into the private modules for city-specific characteristics and theshared modules for city-universal mobility patterns. Secondly, COLA leverages alightweight yet effective post-hoc adjustment strategy for trajectorysimulation, without disturbing the complex bi-level optimization ofmodel-agnostic knowledge transfer. Extensive experiments of COLA compared tostate-of-the-art single-city baselines and our implemented cross-city baselineshave demonstrated its superiority and effectiveness. The code is available athttps://github.com/Star607/Cross-city-Mobility-Transformer. | Yu Wang, Tongya Zheng, Yuxuan Liang, Shunyu Liu, Mingli Song |  |
| 718 |  |  [Off-Policy Evaluation for Large Action Spaces via Policy Convolution](https://doi.org/10.1145/3589334.3645501) |  | 0 | Developing accurate off-policy estimators is crucial for both evaluating and optimizing for new policies. The main challenge in off-policy estimation is the distribution shift between the logging policy that generates data and the target policy that we aim to evaluate. Typically, techniques for correcting distribution shift involve some form of importance sampling. This approach results in unbiased value estimation but often comes with the trade-off of high variance, even in the simpler case of one-step contextual bandits. Furthermore, importance sampling relies on the common support assumption, which becomes impractical when the action space is large. To address these challenges, we introduce the Policy Convolution (PC) family of estimators. These methods leverage latent structure within actions -- made available through action embeddings -- to strategically convolve the logging and target policies. This convolution introduces a unique bias-variance trade-off, which can be controlled by adjusting the amount of convolution. Our experiments on synthetic and benchmark datasets demonstrate remarkable mean squared error (MSE) improvements when using PC, especially when either the action space or policy mismatch becomes large, with gains of up to 5 - 6 orders of magnitude over existing estimators. | Noveen Sachdeva, Lequn Wang, Dawen Liang, Nathan Kallus, Julian J. McAuley | Netflix & Cornell University, Los Gatos, CA, USA; Netflix, Los Gatos, CA, USA; University of California, San Diego, La Jolla, CA, USA |
| 719 |  |  [Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice](https://doi.org/10.1145/3589334.3645504) |  | 0 | Adaptive experimental design (AED) methods are increasingly being used in industry as a tool to boost testing throughput or reduce experimentation cost relative to traditional A/B/N testing methods. However, the behavior and guarantees of such methods are not well-understood beyond idealized stationary settings. This paper shares lessons learned regarding the challenges of naively using AED systems in industrial settings where non-stationarity is prevalent, while also providing perspectives on the proper objectives and system specifications in such settings. We developed an AED framework for counterfactual inference based on these experiences, and tested it in a commercial environment. | Tanner Fiez, Houssam Nassif, YuCheng Chen, Sergio Gamez, Lalit Jain | Houssam Nassif \* Meta Seattle |
| 720 |  |  [LFDe: A Lighter, Faster and More Data-Efficient Pre-training Framework for Event Extraction](https://doi.org/10.1145/3589334.3645318) |  | 0 | Pre-training Event Extraction (EE) models on unlabeled data is an effective strategy that frees researchers from costly and labor-intensive data annotation. However, existing pre-training methods necessitate substantial computational resources, requiring high-performance hardware infrastructure and extensive training duration. In response to these challenges, this paper proposes a Lighter, Faster, and more Data-efficient pre-training framework for EE, named LFDe. Distinct from existing methods that strive to establish a comprehensive representation space during pre-training, our framework focuses on quickly familiarizing with the task format from a small amount of automatically constructed pseudo-events. It comprises three stages: weak-label data construction, pre-training, and fine-tuning. Specifically, during the first stage, LFDe first automatically designates pseudo-triggers and arguments based on the characteristics of real events to form pre-training samples. In the processes of pre-training and fine-tuning, the framework reframes EE as the identification of tokens semantically closest to the prompt within the given sentence. This paper also introduces a novel prompt-based sequence labeling model for EE to accommodate this reframing. Experiments on real-world datasets show that compared to similar models, our framework requires fewer pre-training data (only about 0.04%), a shorter pre-training period (about 0.03%), and lower memory requirements (about 57.6%). Simultaneously, our framework significantly improves performance in various data-scarce scenarios. | Zhigang Kan, Liwen Peng, Yifu Gao, Ning Liu, Linbo Qiao, Dongsheng Li |  |
| 721 |  |  [Multi-Scenario Pricing for Hotel Revenue Management](https://doi.org/10.1145/3589334.3645350) |  | 0 | Dynamic pricing algorithms have been widely studied to manage hotel and platform revenue over online travel platforms (OTPs). For better dynamic pricing, the accurate estimation of the market demand and the market competitiveness are crucial. However, the existing approaches obtain a pricing strategy tailored to each specific scenario using data only from that scenario. They are not considering the shared information between different scenarios, i.e., the data from different scenarios are not fully utilized. So we propose a Multi Scenario Pricing model (MSP) with a novel sharing structure design that leverages cross-scenario and specific information to capture more accurate market demand and competitiveness. Specifically, the model structure explicitly separates information into shared components as market demand and specific information as scenario-wise price competitiveness to prevent domain seesaw. To capture the inherent correlation between listings in different scenarios, an attention network named Price Competitiveness Representation Extraction (PCRE) is well-designed. Meanwhile, traditional metrics are skewed towards model that tends to reduce the price regardless of sample distribution. Thus we propose new offline evaluation metrics that shift attention with sample distribution to avoid biased pricing strategies, which is proved to be more closely related to actual business revenue. Our proposed MSP shows superiority under both offline and online experiments on real-world datasets. The multi-scenario industry dataset and our code are available. To the best of our knowledge, it will be the first real-industry multi-scenario pricing data. | Wendong Xiao, Shuqi Zhang, Zhiyi Huang, Yao Yu |  |
| 722 |  |  [Collaboration-Aware Hybrid Learning for Knowledge Development Prediction](https://doi.org/10.1145/3589334.3645326) |  | 0 | In recent years, the rise of online Knowledge Management Systems (KMSs) has significantly improved work efficiency in enterprises. Knowledge development prediction, as a critical application within these online platforms, enables organizations to proactively address knowledge gaps and align their learning initiatives with evolving job requirements. However, it still confronts challenges in exploring the influence of collaborative networks on knowledge development and adapting to ecological situations in working environment. To this end, in this paper, we propose a Collaboration-Aware Hybrid Learning approach (CAHL) for predicting the future knowledge acquisition of employees and quantifying the impact of various knowledge learning patterns. Specifically, to fully harness the inherent rules of knowledge development, we first learn the knowledge co-occurrence and prerequisite relationships with an association prompt attention mechanism to generate effective knowledge representations through a specially-designed Job Knowledge Embedding module. Then, we aggregate the features of mastering knowledge and work collaborators for employee representations in another Employee Embedding module. Moreover, we propose to model the process of employee knowledge development via a Hybrid Learning Simulation module that integrates both collaborative learning and self learning to predict future-acquired job knowledge of employees. Finally, extensive experiments conducted on a real-world dataset clearly validate the effectiveness of CAHL. | Liyi Chen, Chuan Qin, Ying Sun, Xin Song, Tong Xu, Hengshu Zhu, Hui Xiong |  |
| 723 |  |  [Span-Pair Interaction and Tagging for Dialogue-Level Aspect-Based Sentiment Quadruple Analysis](https://doi.org/10.1145/3589334.3645355) |  | 0 | The Dialogue-level Aspect-based Sentiment Quadruple analysis (DiaASQ) task has recently received attention in the Aspect-Based Sentiment Analysis (ABSA) field. It aims to extract(target, aspect, opinion, sentiment) quadruples from multi-turn and multi-party dialogues. Compared to previous ABSA tasks focusing on text such as sentences, the DiaASQ task involves more complex contextual information and corresponding relations between terms, as well as longer sequences. These characteristics challenge existing methods that struggle to model explicit span-level interactions or have high computational costs. In this paper, we propose a span-pair interaction and tagging method to solve these issues, which includes a novel Span-pair Tagging Scheme (STS) and a simple and efficient Multi-level Representation Model (MRM). STS simplifies the DiaASQ task to a span-pair tagging task and explicitly captures complete span-level semantics by tagging span pairs. MRM efficiently models the dialogue structure information and span-level interactions by constructing multi-level contextual representation. Besides, we train a span ranker to improve the running efficiency of MRM. Extensive experiments on multilingual datasets demonstrate that our method outperforms existing state-of-the-art methods. | Changzhi Zhou, Zhijing Wu, Dandan Song, Linmei Hu, Yuhang Tian, Jing Xu |  |
| 724 |  |  [UrbanCLIP: Learning Text-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining from the Web](https://doi.org/10.1145/3589334.3645378) |  | 0 | Urban region profiling from web-sourced data is of utmost importance forurban planning and sustainable development. We are witnessing a rising trend ofLLMs for various fields, especially dealing with multi-modal data research suchas vision-language learning, where the text modality serves as a supplementinformation for the image. Since textual modality has never been introducedinto modality combinations in urban region profiling, we aim to answer twofundamental questions in this paper: i) Can textual modality enhance urbanregion profiling? ii) and if so, in what ways and with regard to which aspects?To answer the questions, we leverage the power of Large Language Models (LLMs)and introduce the first-ever LLM-enhanced framework that integrates theknowledge of textual modality into urban imagery profiling, named LLM-enhancedUrban Region Profiling with Contrastive Language-Image Pretraining (UrbanCLIP).Specifically, it first generates a detailed textual description for eachsatellite image by an open-source Image-to-Text LLM. Then, the model is trainedon the image-text pairs, seamlessly unifying natural language supervision forurban visual representation learning, jointly with contrastive loss andlanguage modeling loss. Results on predicting three urban indicators in fourmajor Chinese metropolises demonstrate its superior performance, with anaverage improvement of 6.1Our code and the image-language dataset will be released upon papernotification. | Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong Wen, Roger Zimmermann, Yuxuan Liang | The Hong Kong University of Science and Technology (Guangzhou; National University of Singapore; Beijing Jiaotong University; Northwest Polytechnical University 4 DAMO Academy |
| 725 |  |  [MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection](https://doi.org/10.1145/3589334.3645385) |  | 0 | The prevalence of fake news across various online sources has had asignificant influence on the public. Existing Chinese fake news detectiondatasets are limited to news sourced solely from Weibo. However, fake newsoriginating from multiple sources exhibits diversity in various aspects,including its content and social context. Methods trained on purely one singlenews source can hardly be applicable to real-world scenarios. Our pilotexperiment demonstrates that the F1 score of the state-of-the-art method thatlearns from a large Chinese fake news detection dataset, Weibo-21, dropssignificantly from 0.943 to 0.470 when the test data is changed to multi-sourcenews data, failing to identify more than one-third of the multi-source fakenews. To address this limitation, we constructed the first multi-sourcebenchmark dataset for Chinese fake news detection, termed MCFEND, which iscomposed of news we collected from diverse sources such as social platforms,messaging apps, and traditional online news outlets. Notably, such news hasbeen fact-checked by 14 authoritative fact-checking agencies worldwide. Inaddition, various existing Chinese fake news detection methods are thoroughlyevaluated on our proposed dataset in cross-source, multi-source, and unseensource ways. MCFEND, as a benchmark dataset, aims to advance Chinese fake newsdetection approaches in real-world scenarios. | Yupeng Li, Haorui He, Jin Bai, Dacheng Wen |  |
| 726 |  |  [LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty](https://doi.org/10.1145/3589334.3645414) |  | 0 | Named Entity Recognition (NER) serves as a fundamental task in natural language understanding, bearing direct implications for web content analysis, search engines, and information retrieval systems. Fine-tuned NER models exhibit satisfactory performance on standard NER benchmarks. However, due to limited fine-tuning data and lack of knowledge, it performs poorly on unseen entity recognition. As a result, the usability and reliability of NER models in web-related applications are compromised. Instead, Large Language Models (LLMs) like GPT-4 possess extensive external knowledge, but research indicates that they lack specialty for NER tasks. Furthermore, non-public and large-scale weights make tuning LLMs difficult. To address these challenges, we propose a framework that combines small fine-tuned models with LLMs (LinkNER) and an uncertainty-based linking strategy called RDC that enables fine-tuned models to complement black-box LLMs, achieving better performance. We experiment with both standard NER test sets and noisy social media datasets. LinkNER enhances NER task performance, notably surpassing SOTA models in robustness tests. We also quantitatively analyze the influence of key components like uncertainty estimation methods, LLMs, and in-context learning on diverse NER tasks, offering specific web-related recommendations. | Zhen Zhang, Yuhua Zhao, Hang Gao, Mengting Hu | Nankai University Tianjin College of Software |
| 727 |  |  [RicciNet: Deep Clustering via A Riemannian Generative Model](https://doi.org/10.1145/3589334.3645428) |  | 0 | In recent years, deep clustering has achieved encouraging results. However, existing deep clustering methods work with the traditional Euclidean space and thus present deficiency on clustering complex structures. On the contrary, Riemannian geometry provides an elegant framework to model complex structures as well as a powerful tool for clustering, i.e., the Ricci flow. In this paper, we rethink the problem of deep clustering, and introduce the Riemannian geometry to deep clustering for the first time. Deep clustering in Riemannian manifold still faces significant challenges: (1) Ricci flow itself is unaware of cluster membership, (2) Ricci curvature prevents the gradient backpropagation, and (3) learning the flow largely remains open in the manifold. To bridge these gaps, we propose a novel Riemannian generative model (RicciNet), a neural Ricci flow with several theoretical guarantees. The novelty is that we model the dynamic self-clustering process of Ricci flow: data points move to the respective clusters in the manifold, influenced by Ricci curvatures. The point's trajectory is characterized by a parametric velocity, taking the form of Ordinary Differential Equation (ODE). Specifically, we encode data points as samples of Gaussian mixture in the manifold where we propose two types of reparameterization approaches: Gumbel reparameterization, and geometric trick. We formulate a differentiable Ricci curvature parameterized by a Riemannian graph convolution. Thereafter, we propose a geometric learning approach in which we study the geometric regularity of the point's trajectory, and learn the flow via distance matching and velocity matching. Consequently, data points go along the shortest Ricci flow to complete clustering. Extensive empirical results show RicciNet outperforms Euclidean deep methods. | Li Sun, Jingbin Hu, Suyang Zhou, Zhenhao Huang, Junda Ye, Hao Peng, Zhengtao Yu, Philip S. Yu |  |
| 728 |  |  [Weakly Supervised Anomaly Detection via Knowledge-Data Alignment](https://doi.org/10.1145/3589334.3645429) |  | 0 | Anomaly detection (AD) plays a pivotal role in numerous web-basedapplications, including malware detection, anti-money laundering, devicefailure detection, and network fault analysis. Most methods, which rely onunsupervised learning, are hard to reach satisfactory detection accuracy due tothe lack of labels. Weakly Supervised Anomaly Detection (WSAD) has beenintroduced with a limited number of labeled anomaly samples to enhance modelperformance. Nevertheless, it is still challenging for models, trained on aninadequate amount of labeled data, to generalize to unseen anomalies. In thispaper, we introduce a novel framework Knowledge-Data Alignment (KDAlign) tointegrate rule knowledge, typically summarized by human experts, to supplementthe limited labeled data. Specifically, we transpose these rules into theknowledge space and subsequently recast the incorporation of knowledge as thealignment of knowledge and data. To facilitate this alignment, we employ theOptimal Transport (OT) technique. We then incorporate the OT distance as anadditional loss term to the original objective function of WSAD methodologies.Comprehensive experimental results on five real-world datasets demonstrate thatour proposed KDAlign framework markedly surpasses its state-of-the-artcounterparts, achieving superior performance across various anomaly types. | Haihong Zhao, Chenyi Zi, Yang Liu, Chen Zhang, Yan Zhou, Jia Li |  |
| 729 |  |  [MULAN: Multi-modal Causal Structure Learning and Root Cause Analysis for Microservice Systems](https://doi.org/10.1145/3589334.3645442) |  | 0 | Effective root cause analysis (RCA) is vital for swiftly restoring services, minimizing losses, and ensuring the smooth operation and management of complex systems. Previous data-driven RCA methods, particularly those employing causal discovery techniques, have primarily focused on constructing dependency or causal graphs for backtracking the root causes. However, these methods often fall short as they rely solely on data from a single modality, thereby resulting in suboptimal solutions. In this work, we propose Mulan, a unified multi-modal causal structure learning method designed to identify root causes in microservice systems. We leverage a log-tailored language model to facilitate log representation learning, converting log sequences into time-series data. To explore intricate relationships across different modalities, we propose a contrastive learning-based approach to extract modality-invariant and modality-specific representations within a shared latent space. Additionally, we introduce a novel key performance indicator-aware attention mechanism for assessing modality reliability and co-learning a final causal graph. Finally, we employ random walk with restart to simulate system fault propagation and identify potential root causes. Extensive experiments on three real-world datasets validate the effectiveness of our proposed method. | Lecheng Zheng, Zhengzhang Chen, Jingrui He, Haifeng Chen |  |
| 730 |  |  [Dynamic Multi-Network Mining of Tensor Time Series](https://doi.org/10.1145/3589334.3645461) |  | 0 | Subsequence clustering of time series is an essential task in data mining,and interpreting the resulting clusters is also crucial since we generally donot have prior knowledge of the data. Thus, given a large collection of tensortime series consisting of multiple modes, including timestamps, how can weachieve subsequence clustering for tensor time series and provide interpretableinsights? In this paper, we propose a new method, Dynamic Multi-network Mining(DMM), that converts a tensor time series into a set of segment groups ofvarious lengths (i.e., clusters) characterized by a dependency networkconstrained with l1-norm. Our method has the following properties. (a)Interpretable: it characterizes the cluster with multiple networks, each ofwhich is a sparse dependency network of a corresponding non-temporal mode, andthus provides visible and interpretable insights into the key relationships.(b) Accurate: it discovers the clusters with distinct networks from tensor timeseries according to the minimum description length (MDL). (c) Scalable: itscales linearly in terms of the input data size when solving a non-convexproblem to optimize the number of segments and clusters, and thus it isapplicable to long-range and high-dimensional tensors. Extensive experimentswith synthetic datasets confirm that our method outperforms thestate-of-the-art methods in terms of clustering accuracy. We then use realdatasets to demonstrate that DMM is useful for providing interpretable insightsfrom tensor time series. | Kohei Obata, Koki Kawabata, Yasuko Matsubara, Yasushi Sakurai | SANKEN |
| 731 |  |  [MSynFD: Multi-hop Syntax Aware Fake News Detection](https://doi.org/10.1145/3589334.3645468) |  | 0 | The proliferation of social media platforms has fueled the rapiddissemination of fake news, posing threats to our real-life society. Existingmethods use multimodal data or contextual information to enhance the detectionof fake news by analyzing news content and/or its social context. However,these methods often overlook essential textual news content (articles) andheavily rely on sequential modeling and global attention to extract semanticinformation. These existing methods fail to handle the complex, subtle twistsin news articles, such as syntax-semantics mismatches and prior biases, leadingto lower performance and potential failure when modalities or social contextare missing. To bridge these significant gaps, we propose a novel multi-hopsyntax aware fake news detection (MSynFD) method, which incorporatescomplementary syntax information to deal with subtle twists in fake news.Specifically, we introduce a syntactical dependency graph and design amulti-hop subgraph aggregation mechanism to capture multi-hop syntax. Itextends the effect of word perception, leading to effective noise filtering andadjacent relation enhancement. Subsequently, a sequential relativeposition-aware Transformer is designed to capture the sequential information,together with an elaborate keyword debiasing module to mitigate the prior bias.Extensive experimental results on two public benchmark datasets verify theeffectiveness and superior performance of our proposed MSynFD overstate-of-the-art detection models. | Liang Xiao, Qi Zhang, Chongyang Shi, Shoujin Wang, Usman Naseem, Liang Hu | Tongji University School of Computer Science Shanghai; University of Technology Sydney School of Computer Science Sydney; Beijing Institute of Technology School of Computer Science; Macquarie University School of Computing Sydney |
| 732 |  |  [LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Time Series Anomaly Detection](https://doi.org/10.1145/3589334.3645472) |  | 0 | Most of current anomaly detection models assume that the normal patternremains same all the time. However, the normal patterns of Web services changedramatically and frequently. The model trained on old-distribution data isoutdated after such changes. Retraining the whole model every time isexpensive. Besides, at the beginning of normal pattern changes, there is notenough observation data from the new distribution. Retraining a large neuralnetwork model with limited data is vulnerable to overfitting. Thus, we proposea Light and Anti-overfitting Retraining Approach (LARA) for deep variationalauto-encoder based time series anomaly detection methods (VAEs). This work aimsto make three novel contributions: 1) the retraining process is formulated as aconvex problem and can converge at a fast rate as well as prevent overfitting;2) designing a ruminate block, which leverages the historical data without theneed to store them; 3) mathematically proving that when fine-tuning the latentvector and reconstructed data, the linear formations can achieve the leastadjusting errors between the ground truths and the fine-tuned ones. Moreover, we have performed many experiments to verify that retraining LARAwith even 43 time slots of data from new distribution can result in itscompetitive F1 Score in comparison with the state-of-the-art anomaly detectionmodels trained with sufficient data. Besides, we verify its light overhead. | Feiyi Chen, Zhen Qin, Mengchu Zhou, Yingying Zhang, Shuiguang Deng, Lunting Fan, Guansong Pang, Qingsong Wen | Singapore Management University, Singapore, Singapore; Zhejiang Gongshang University, Hangzhou, China; Zhejiang University & Alibaba Group, Hangzhou, China; Zhejiang University, Hangzhou, China; Alibaba Group, Hangzhou, China; Squirrel AI, Bellevue, WA, USA |
| 733 |  |  [Markovletics: Methods and A Novel Application for Learning Continuous-Time Markov Chain Mixtures](https://doi.org/10.1145/3589334.3645491) |  | 0 | Sequential data naturally arises from user engagement on digital platformslike social media, music streaming services, and web navigation, encapsulatingevolving user preferences and behaviors through continuous information streams.A notable unresolved query in stochastic processes is learning mixtures ofcontinuous-time Markov chains (CTMCs). While there is progress in learningmixtures of discrete-time Markov chains with recovery guarantees[GKV16,ST23,KTT2023], the continuous scenario uncovers unique unexploredchallenges. The intrigue in CTMC mixtures stems from their potential to modelintricate continuous-time stochastic processes prevalent in various fieldsincluding social media, finance, and biology. In this study, we introduce a novel framework for exploring CTMCs,emphasizing the influence of observed trails' length and mixture parameters onproblem regimes, which demands specific algorithms. Through thoroughexperimentation, we examine the impact of discretizing continuous-time trailson the learnability of the continuous-time mixture, given that these processesare often observed via discrete, resource-demanding observations. Ourcomparative analysis with leading methods explores sample complexity and thetrade-off between the number of trails and their lengths, offering crucialinsights for method selection in different problem instances. We apply ouralgorithms on an extensive collection of Lastfm's user-generated trailsspanning three years, demonstrating the capability of our algorithms todifferentiate diverse user preferences. We pioneer the use of CTMC mixtures ona basketball passing dataset to unveil intricate offensive tactics of NBAteams. This underscores the pragmatic utility and versatility of our proposedframework. All results presented in this study are replicable, and we providethe implementations to facilitate reproducibility. | Fabian Spaeh, Charalampos E. Tsourakakis | Boston University |
| 734 |  |  [NAT4AT: Using Non-Autoregressive Translation Makes Autoregressive Translation Faster and Better](https://doi.org/10.1145/3589334.3645527) |  | 0 | With the increasing number of web documents, the demand for translation has increased dramatically. Non-autoregressive translation (NAT) models can significantly reduce decoding latency to meet the growing translation needs, but they sacrifice translation quality. And there is still an irreparable performance gap between NAT models and strong autoregressive translation (AT) models at the corpus level. However, more fine-grained comparative experiments on AT and NAT are currently lacking. Therefore, in this paper, we first conducted analysis experiments at the sentence level and found complementarity and high similarity between the translations generated by AT and NAT. Then, based on this observation, we propose a general and effective method called NAT4AT, which can not only use NAT to speed up the inference speed of AT significantly but also improve its final translation quality. Specifically, NAT4AT first uses a NAT model to generate an original translation in parallel and then uses an AT model as a correction model to revise errors in the original translation. In this way, the AT model no longer needs to predict the entire translation but only needs to predict a small number of error parts in the NAT result. Extensive experimental results on major WMT benchmarks verify the generality and effectiveness of our method, whose translation quality is superior to the strong AT model and achieves a 5.0x speedup. | Huanran Zheng, Wei Zhu, Xiaoling Wang |  |
| 735 |  |  [Breaking the Time-Frequency Granularity Discrepancy in Time-Series Anomaly Detection](https://doi.org/10.1145/3589334.3645556) |  | 0 | In light of the remarkable advancements made in time-series anomaly detection(TSAD), recent emphasis has been placed on exploiting the frequency domain as well as the time domain to address the difficulties in precisely detecting pattern-wise anomalies. However, in terms of anomaly scores, the window granularity of the frequency domain is inherently distinct from the data-point granularity of the time domain. Owing to this discrepancy, the anomaly information in the frequency domain has not been utilized to its full potential for TSAD. In this paper, we propose a TSAD framework, Dual-TF, that simultaneously uses both the time and frequency domains while breaking the time-frequency granularity discrepancy. To this end, our framework employs nested-sliding windows, with the outer and inner windows responsible for the time and frequency domains, respectively, and aligns the anomaly scores of the two domains. As a result of the high resolution of the aligned scores, the boundaries of pattern-wise anomalies can be identified more precisely. In six benchmark datasets, our framework outperforms state-of-the-art methods by 12.0--147%, as demonstrated by experimental results. | Youngeun Nam, Susik Yoon, Yooju Shin, Minyoung Bae, Hwanjun Song, JaeGil Lee, Byung Suk Lee |  |
| 736 |  |  [Question Difficulty Consistent Knowledge Tracing](https://doi.org/10.1145/3589334.3645582) |  | 0 | Knowledge tracing aims to estimate knowledge states of students over a set of skills based on students' past learning activities. Deep learning based knowledge tracing models show superior performance to traditional knowledge tracing approaches. Early works like DKT use skill IDs and student responses only. Recent works also incorporate questions IDs into their models and achieve much improved performance in the next question correctness prediction task. However, predictions made by these models are thus on specific questions, and it is not straightforward to translate them to estimation of students' knowledge states over skills. In this paper, we propose to replace question IDs with question difficulty levels in deep knowledge tracing models. The predictions made by our model can be more readily translated to students' knowledge states over skills. Furthermore, by using question difficulty levels to replace question IDs, we can also alleviate the cold-start problem in knowledge tracing as online learning platforms are updated frequently with new questions. We further use two techniques to smooth the predicted scores. One is to combine embeddings of nearby difficulty levels using the Hann function. The other is to constrain the predicted probabilities to be consistent with question difficulties by imposing a penalty if they are not consistent. We conduct extensive experiments to study the performance of the proposed model. Our experimental results show that our model outperforms the state-of-the-art knowledge tracing models in terms of both accuracy and consistency with question difficulty levels. | Guimei Liu, Huijing Zhan, Jungjae Kim |  |
| 737 |  |  [A Simple but Effective Approach for Unsupervised Few-Shot Graph Classification](https://doi.org/10.1145/3589334.3645587) |  | 0 | Graphs, as a fundamental data structure, have proven efficacy in modeling complex relationships between objects and are therefore found in wide web applications. Graph classification is an essential task in graph data analysis, which can effectively assist in extracting information and mining content from the web. Recently, few-shot graph classification, a more realistic and challenging task, has garnered great research interest. Existing few-shot graph classification models are all supervised, assuming abundant labeled data in base classes for meta-training. However, sufficient annotation is often challenging to obtain in practice due to high costs or demand for expertise. Moreover, they commonly adopt complicated meta-learning algorithms via episodic training to transfer prior knowledge from base classes. To break free from these constraints, in this paper, we propose a simple yet effective approach named SMART for unsupervised few-shot graph classification without using any labeled data. SMART employs transfer learning philosophy instead of the previously prevailing meta-learning paradigm, avoiding the need for sophisticated meta-learning algorithms. Additionally, we adopt a novel mixup strategy to augment the original graph data and leverage unsupervised pretraining on these data to obtain the expressive graph encoder. We also utilize the prompt tuning technique to alleviate the overfitting and low fine-tuning efficiency caused by the limited support samples of novel classes. Extensive experimental results demonstrate the superiority of our proposed approach, significantly surpassing even leading supervised few-shot graph classification models. Our code is available here. | Yonghao Liu, Lan Huang, Bowen Cao, Ximing Li, Fausto Giunchiglia, Xiaoyue Feng, Renchu Guan |  |
| 738 |  |  [Inductive Cognitive Diagnosis for Fast Student Learning in Web-Based Intelligent Education Systems](https://doi.org/10.1145/3589334.3645589) |  | 0 | Cognitive diagnosis aims to gauge students' mastery levels based on theirresponse logs. Serving as a pivotal module in web-based online intelligenteducation systems (WOIESs), it plays an upstream and fundamental role indownstream tasks like learning item recommendation and computerized adaptivetesting. WOIESs are open learning environment where numerous new studentsconstantly register and complete exercises. In WOIESs, efficient cognitivediagnosis is crucial to fast feedback and accelerating student learning.However, the existing cognitive diagnosis methods always employ intrinsicallytransductive student-specific embeddings, which become slow and costly due toretraining when dealing with new students who are unseen during training. Tothis end, this paper proposes an inductive cognitive diagnosis model (ICDM) forfast new students' mastery levels inference in WOIESs. Specifically, in ICDM,we propose a novel student-centered graph (SCG). Rather than inferring masterylevels through updating student-specific embedding, we derive the inductivemastery levels as the aggregated outcomes of students' neighbors in SCG.Namely, SCG enables to shift the task from finding the most suitablestudent-specific embedding that fits the response logs to finding the mostsuitable representations for different node types in SCG, and the latter ismore efficient since it no longer requires retraining. To obtain thisrepresentation, ICDM consists of aconstruction-aggregation-generation-transformation process to learn the finalrepresentation of students, exercises and concepts. Extensive experimentsacross real-world datasets show that, compared with the existing cognitivediagnosis methods that are always transductive, ICDM is much more faster whilemaintains the competitive inference performance for new students. | Shuo Liu, Junhao Shen, Hong Qian, Aimin Zhou | Shanghai Institute of AI for Education and School of Computer Science and Technology East China Normal University |
| 739 |  |  [RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules](https://doi.org/10.1145/3589334.3645602) |  | 0 | Weakly supervised text classification (WSTC), also called zero-shot ordataless text classification, has attracted increasing attention due to itsapplicability in classifying a mass of texts within the dynamic and open Webenvironment, since it requires only a limited set of seed words (label names)for each category instead of labeled data. With the help of recently popularprompting Pre-trained Language Models (PLMs), many studies leveraged manuallycrafted and/or automatically identified verbalizers to estimate the likelihoodof categories, but they failed to differentiate the effects of thesecategory-indicative words, let alone capture their correlations and realizeadaptive adjustments according to the unlabeled corpus. In this paper, in orderto let the PLM effectively understand each category, we at first propose anovel form of rule-based knowledge using logical expressions to characterizethe meanings of categories. Then, we develop a prompting PLM-based approachnamed RulePrompt for the WSTC task, consisting of a rule mining module and arule-enhanced pseudo label generation module, plus a self-supervisedfine-tuning module to make the PLM align with this task. Within this framework,the inaccurate pseudo labels assigned to texts and the imprecise logical rulesassociated with categories mutually enhance each other in an alternativemanner. That establishes a self-iterative closed loop of knowledge (rule)acquisition and utilization, with seed words serving as the starting point.Extensive experiments validate the effectiveness and robustness of ourapproach, which markedly outperforms state-of-the-art weakly supervisedmethods. What is more, our approach yields interpretable category rules,proving its advantage in disambiguating easily-confused categories. | Miaomiao Li, Jiaqi Zhu, Yang Wang, Yi Yang, Yilin Li, Hongan Wang |  |
| 740 |  |  [Multimodal Relation Extraction via a Mixture of Hierarchical Visual Context Learners](https://doi.org/10.1145/3589334.3645603) |  | 0 | Multimodal relation extraction is a fundamental task of multimodal information extraction. Recent studies have shown promising results by integrating hierarchical visual features from local regions, like image patches, to the broader global regions that form the entire image. However, research to date has largely ignored the understanding of how hierarchical visual semantics are represented and the characteristics that can benefit relation extraction. To bridge this gap, we propose a novel two-stage hierarchical visual context fusion transformer incorporating the mixture of multimodal experts framework to effectively represent and integrate hierarchical visual features into textual semantic representations. In addition, we introduce the concept of hierarchical tracking maps to facilitate the understanding of the intrinsic mechanisms of image information processing involved in multimodal models. We thoroughly investigate the implications of hierarchical visual contexts through four dimensions: performance evaluation, the nature of auxiliary visual information, the patterns observed in the image encoding hierarchy, and the significance of various visual encoding levels. Empirical studies show that our approach achieves new state-of-the-art performance on the MNRE dataset. | Xiyang Liu, Chunming Hu, Richong Zhang, Kai Sun, Samuel Mensah, Yongyi Mao |  |
| 741 |  |  [Diagrammatic Reasoning for ALC Visualization with Logic Graphs](https://doi.org/10.1145/3589334.3645607) |  | 0 | User studies show the demand for diagrammatic reasoning techniques for knowledge representation formats. OWL ontologies are highly relevant for Web 3.0, however, existing ontology visualization tools do not support diagrammatic reasoning, while existing diagrammatic reasoning systems utilize suboptimal visual languages. The purpose of this research is to facilitate the usage of OWL ontologies by providing a diagrammatic reasoning system over their visual representations. We focus on the ALC description logic, which covers most of the expressivity of the ontologies. As a visual language to reason about, we utilize Logic Graphs, which provide the simplest visualizations regarding graph- and information-theoretic properties. We adapt the tableau algorithm to LGs to reason about concept satisfiability, prove the correctness of the proposed system and illustrate it with examples. The proposed diagrammatic reasoning system allows reasoning over ontologies, reducing complex concepts step by step, and identifying elements that produce a contradiction. | Ildar Baimuratov |  |
| 742 |  |  [Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models](https://doi.org/10.1145/3589334.3645611) |  | 0 | Explaining stock predictions is generally a difficult task for traditionalnon-generative deep learning models, where explanations are limited tovisualizing the attention weights on important texts. Today, Large LanguageModels (LLMs) present a solution to this problem, given their knowncapabilities to generate human-readable explanations for their decision-makingprocess. However, the task of stock prediction remains challenging for LLMs, asit requires the ability to weigh the varying impacts of chaotic social texts onstock prices. The problem gets progressively harder with the introduction ofthe explanation component, which requires LLMs to explain verbally why certainfactors are more important than the others. On the other hand, to fine-tuneLLMs for such a task, one would need expert-annotated samples of explanationfor every stock movement in the training set, which is expensive andimpractical to scale. To tackle these issues, we propose ourSummarize-Explain-Predict (SEP) framework, which utilizes a self-reflectiveagent and Proximal Policy Optimization (PPO) to let a LLM teach itself how togenerate explainable stock predictions in a fully autonomous manner. Thereflective agent learns how to explain past stock movements throughself-reasoning, while the PPO trainer trains the model to generate the mostlikely explanations from input texts. The training samples for the PPO trainerare also the responses generated during the reflective process, whicheliminates the need for human annotators. Using our SEP framework, we fine-tunea LLM that can outperform both traditional deep-learning and LLM methods inprediction accuracy and Matthews correlation coefficient for the stockclassification task. To justify the generalization capability of our framework,we further test it on the portfolio construction task, and demonstrate itseffectiveness through various portfolio metrics. | Kelvin J. L. Koa, Yunshan Ma, Ritchie Ng, TatSeng Chua |  |
| 743 |  |  [High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text Attributed Graphs](https://doi.org/10.1145/3589334.3645614) |  | 0 | We investigate node representation learning on text-attributed graphs (TAGs),where nodes are associated with text information. Although recent studies ongraph neural networks (GNNs) and pretrained language models (PLMs) haveexhibited their power in encoding network and text signals, respectively, lessattention has been paid to delicately coupling these two types of models onTAGs. Specifically, existing GNNs rarely model text in each node in acontextualized way; existing PLMs can hardly be applied to characterize graphstructures due to their sequence architecture. To address these challenges, wepropose HASH-CODE, a High-frequency Aware Spectral Hierarchical ContrastiveSelective Coding method that integrates GNNs and PLMs into a unified model.Different from previous "cascaded architectures" that directly add GNN layersupon a PLM, our HASH-CODE relies on five self-supervised optimizationobjectives to facilitate thorough mutual enhancement between network and textsignals in diverse granularities. Moreover, we show that existing contrastiveobjective learns the low-frequency component of the augmentation graph andpropose a high-frequency component (HFC)-aware contrastive learning objectivethat makes the learned embeddings more distinctive. Extensive experiments onsix real-world benchmarks substantiate the efficacy of our proposed approach.In addition, theoretical analysis and item embedding visualization provideinsights into our model interoperability. | Peiyan Zhang, Chaozhuo Li, Liying Kang, Feiran Huang, Senzhang Wang, Xing Xie, Sunghun Kim | Hong Kong Polytechnic University Hong Kong; Hong Kong University of Science and Technology Hong Kong; Central South University; Microsoft Research Asia; Jinan University |
| 744 |  |  [Distributed Data Placement and Content Delivery in Web Caches with Non-Metric Access Costs](https://doi.org/10.1145/3589334.3645654) |  | 0 | Motivated by applications in web caches and content delivery in peer-to-peer networks, we consider the non-metric data placement problem and develop distributed algorithms for computing or approximating its optimal solutions. In this problem, the goal is to store copies of the data points among a set of cache-capacitated servers to minimize overall data storage and clients' access costs. We first show that the non-metric data placement problem is inapproximable up to a logarithmic factor. We then provide a game-theoretic decomposition of the objective function and show that a natural type of Glauber dynamics in which servers update their cache contents with probability proportional to the utility they receive from caching those data will converge to an optimal global solution for a sufficiently large noise parameter. In particular, we establish the polynomial mixing time of the Glauber dynamics for a certain range of noise parameters. Such a game-theoretic decomposition not only provides a good performance guarantee in terms of content delivery but also allows the system to operate in a fully distributed manner, hence reducing its computational load and improving its robustness to failures. Moreover, we provide another auction-based distributed algorithm, which allows us to approximate the optimal solution with a performance guarantee that depends on the ratio of the revenue vs. social welfare obtained from the underlying auction. |  |  |
| 745 |  |  [DualCL: Principled Supervised Contrastive Learning as Mutual Information Maximization for Text Classification](https://doi.org/10.1145/3589334.3645668) |  | 0 | Text classification is a fundamental task in web content mining. Although the existing supervised contrastive learning (SCL) approach combined with pre-trained language models (PLMs) has achieved leading performance in text classification, it lacks fundamental principles. Theoretically motivated by a derived lower bound of mutual information maximization, we propose a dual contrastive learning framework DualCL that satisfies three properties, i.e., parameter-free, augmentation-easy and label-aware. DualCL generates classifier parameters from the PLM and simultaneously uses them for classification and as augmented views of the input text for supervised contrastive learning. Extensive experiments conclusively demonstrate that DualCL excels in learning superior text representations and consistently outperforms baseline models. | Junfan Chen, Richong Zhang, Yaowei Zheng, Qianben Chen, Chunming Hu, Yongyi Mao |  |
| 746 |  |  [Graph Anomaly Detection with Bi-level Optimization](https://doi.org/10.1145/3589334.3645673) |  | 0 | Graph anomaly detection (GAD) has various applications in finance, healthcare, and security. Graph Neural Networks (GNNs) are now the primary method for GAD, treating it as a task of semi-supervised node classification (normal vs. anomalous). However, most traditional GNNs aggregate and average embeddings from all neighbors, without considering their labels, which can hinder detecting actual anomalies. To address this issue, previous methods try to selectively aggregate neighbors. However, the same selection strategy is applied regardless of normal and anomalous classes, which does not fully solve this issue. This study discovers that nodes with different classes yet similar neighbor label distributions (NLD) tend to have opposing loss curves, which we term it as "loss rivalry". By introducing Contextual Stochastic Block Model (CSBM) and defining NLD distance, we explain this phenomenon theoretically and propose a Bi-level optimization Graph Neural Network (BioGNN), based on these observations. In a nutshell, the lower level of BioGNN segregates nodes based on their classes and NLD, while the upper level trains the anomaly detector using separation outcomes. Our experiments demonstrate that BioGNN outperforms state-of-the-art methods on four benchmarks and effectively mitigates "loss rivalry". | Yuan Gao, Junfeng Fang, Yongduo Sui, Yangyang Li, Xiang Wang, Huamin Feng, Yongdong Zhang |  |
| 747 |  |  [AN-Net: an Anti-Noise Network for Anonymous Traffic Classification](https://doi.org/10.1145/3589334.3645691) |  | 0 | Anonymous networks employ a triple proxy to transmit packets to enhance user privacy, causing traffic packets from all applications and web services to form a unified flow. The traditional approach of applying flow-level encrypted traffic classification methods to anonymous traffic (i.e., treating consecutive packets as a single flow) is hindered by irrelevant packet noise. Moreover, fluctuations in the network environment can introduce per-packet attribute noise and discrepancies between training and test data. How to extract robust patterns from consecutive packets replete with noise remains a key challenge. In this paper, we propose the Anti-Noise Network (AN-Net) to construct robust short-term representations for a single modality, effectively countering irrelevant packet noise. We also incorporate an enhanced multi-modal fusion approach to combat per-packet attribute noise. AN-Net achieves state-of-the-art performance across two anonymous traffic classification tasks and one VPN traffic classification task, notably elevating the F1 score of SJTU-AN21 to 94.39% (6.24%↑). Our code and dataset are available on https://github.com/SJTU-dxw/AN-Net. | Xianwen Deng, Yijun Wang, Zhi Xue |  |
| 748 |  |  [Fast Graph Condensation with Structure-based Neural Tangent Kernel](https://doi.org/10.1145/3589334.3645694) |  | 0 | The rapid development of Internet technology has given rise to a vast amountof graph-structured data. Graph Neural Networks (GNNs), as an effective methodfor various graph mining tasks, incurs substantial computational resource costswhen dealing with large-scale graph data. A data-centric manner solution isproposed to condense the large graph dataset into a smaller one withoutsacrificing the predictive performance of GNNs. However, existing effortscondense graph-structured data through a computational intensive bi-leveloptimization architecture also suffer from massive computation costs. In thispaper, we propose reforming the graph condensation problem as a Kernel RidgeRegression (KRR) task instead of iteratively training GNNs in the inner loop ofbi-level optimization. More specifically, We propose a novel datasetcondensation framework (GC-SNTK) for graph-structured data, where aStructure-based Neural Tangent Kernel (SNTK) is developed to capture thetopology of graph and serves as the kernel function in KRR paradigm.Comprehensive experiments demonstrate the effectiveness of our proposed modelin accelerating graph condensation while maintaining high predictionperformance. The source code is available onhttps://github.com/WANGLin0126/GCSNTK. | Lin Wang, Wenqi Fan, Jiatong Li, Yao Ma, Qing Li |  |
| 749 |  |  [DenseFlow: Spotting Cryptocurrency Money Laundering in Ethereum Transaction Graphs](https://doi.org/10.1145/3589334.3645692) |  | 0 | In recent years, money laundering crimes on blockchain, especially on Ethereum, have become increasingly rampant, resulting in substantial losses. The unique features of money laundering on Ethereum, such as decentralization and pseudonymity, pose new challenges for Ethereum anti-money laundering. Specifically, the existence of dense and extensive laundering gangs and intricate multilayered laundering pathways makes it exceptionally challenging for regulators to identify suspicious accounts and trace money flows. To address this issue, we propose an innovative DenseFlow framework that effectively identifies and traces money laundering activities by finding dense subgraphs and applying the maximum flow idea. We conduct multiple experiments on four datasets from Ethereum to validate the effectiveness of our approach. The precision of our DenseFlow is 16.34% higher than the start-of-the-art comparison methods on average, highlighting its distinctive contribution to tackling money laundering issues on blockchain. | Dan Lin, Jiajing Wu, Yunmei Yu, Qishuang Fu, Zibin Zheng, Changlin Yang |  |
| 750 |  |  [Towards Cross-Table Masked Pretraining for Web Data Mining](https://doi.org/10.1145/3589334.3645707) |  | 0 | Tabular data pervades the landscape of the World Wide Web, playing afoundational role in the digital architecture that underpins onlineinformation. Given the recent influence of large-scale pretrained models likeChatGPT and SAM across various domains, exploring the application ofpretraining techniques for mining tabular data on the web has emerged as ahighly promising research direction. Indeed, there have been some recent worksaround this topic where most (if not all) of them are limited in the scope of afixed-schema/single table. Due to the scale of the dataset and the parametersize of the prior models, we believe that we have not reached the ”BERTmoment” for the ubiquitous tabular data. The development on this linesignificantly lags behind the counterpart research domains such as naturallanguage processing. In this work, we first identify the crucial challengesbehind tabular data pretraining, particularly overcoming the cross-tablehurdle. As a pioneering endeavor, this work mainly (i)-contributes ahigh-quality real-world tabular dataset, (ii)-proposes an innovative, generic,and efficient cross-table pretraining framework, dubbed as CM2, where the coreto it comprises a semantic-aware tabular neural network that uniformly encodesheterogeneous tables without much restriction and (iii)-introduces a novelpretraining objective – prompt Masked Table Modeling (pMTM) – inspired by NLPbut intricately tailored to scalable pretraining on tables. Our extensiveexperiments demonstrate CM2's state-of-the-art performance and validate thatcross-table pretraining can enhance various downstream tasks. | Chao Ye, Guoshan Lu, Haobo Wang, Liyao Li, Sai Wu, Gang Chen, Junbo Zhao | Zhejiang University Hangzhou |
| 751 |  |  [Beyond Labels and Topics: Discovering Causal Relationships in Neural Topic Modeling](https://doi.org/10.1145/3589334.3645715) |  | 0 | Topic models that can take advantage of labels are broadly used in identifying interpretable topics from textual data. However, existing topic models tend to merely view labels as names of topic clusters or as categories of texts, thereby neglecting the potential causal relationships between supervised information and latent topics, as well as within these elements themselves. In this paper, we focus on uncovering possible causal relationships both between and within the supervised information and latent topics to better understand the mechanisms behind the emergence of the topics and the labels. To this end, we propose Causal Relationship-Aware Neural Topic Model (CRNTM), a novel neural topic model that can automatically uncover interpretable causal relationships between and within supervised information and latent topics, while concurrently discovering high-quality topics. In CRNTM, both supervised information and latent topics are treated as nodes, with the causal relationships represented as directed edges in a Directed Acyclic Graph (DAG). A Structural Causal Model (SCM) is employed to model the DAG. Experiments are conducted on three public corpora with different types of labels. Experimental results show that the discovered causal relationships are both reliable and interpretable, and the learned topics are of high quality comparing with eight start-of-the-art topic model baselines. | YiKun Tang, Heyan Huang, Xuewen Shi, XianLing Mao |  |
| 752 |  |  [HD-KT: Advancing Robust Knowledge Tracing via Anomalous Learning Interaction Detection](https://doi.org/10.1145/3589334.3645718) |  | 0 | Knowledge tracing (KT) is a crucial task in online learning, aimed at tracing and predicting each student's knowledge states throughout their learning process. Over the past decade, it has garnered widespread attention due to it provides the potential for more tailored and adaptive online learning experiences. Although most current KT methodologies emphasize optimizing network structures to enhance predictive accuracy for future student performance, they often neglect anomalous interactions in students' learning processes, which may arise from low data quality (i.e., inferior question quality) and abnormal student behaviors (i.e., guessing and mistakes). To this end, in this paper, we propose a novel framework, termed HD-KT, designed to enhance the robustness of existing KT methodologies with Hybrid learning interactions Denoising approach. Specifically, we introduce two detectors for anomalous learning interactions, namely knowledge state-guided anomaly detector and student profile-guided anomaly detector. In the first detection module, we design a sequential autoencoder to identify anomalous learning interactions by detecting atypical student knowledge states. In the second module, we incorporate an attention mechanism by modeling a student's long-term profile to capture irregular interactions. Extensive experiments on four real-world benchmark datasets have decisively shown our HD-KT markedly boosts the robustness of numerous prevailing KT models, consequently increasing the accuracy of future student performance predictions. Additionally, our case studies highlight the versatility of HD-KT in addressing diverse downstream tasks, such as exercise quality analysis and learning behavior-based student clustering. | Haiping Ma, Yong Yang, Chuan Qin, Xiaoshan Yu, Shangshang Yang, Xingyi Zhang, Hengshu Zhu |  |
| 753 |  |  [MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models](https://doi.org/10.1145/3589334.3648137) |  | 0 | The latest large language models (LLMs) such as ChatGPT, exhibit strong capabilities in automated mental health analysis. However, existing relevant studies bear several limitations, including inadequate evaluations, lack of prompting strategies, and ignorance of exploring LLMs for explainability. To bridge these gaps, we comprehensively evaluate the mental health analysis and emotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore the effects of different prompting strategies with unsupervised and distantly supervised emotional information. Based on these prompts, we explore LLMs for interpretable mental health analysis by instructing them to generate explanations for each of their decisions. We convey strict human evaluations to assess the quality of the generated explanations, leading to a novel dataset with 163 human-assessed explanations. We benchmark existing automatic evaluation metrics on this dataset to guide future related works. According to the results, ChatGPT shows strong in-context learning ability but still has a significant gap with advanced task-specific methods. Careful prompt engineering with emotional cues and expert-written few-shot examples can also effectively improve performance on mental health analysis. In addition, ChatGPT generates explanations that approach human performance, showing its great potential in explainable mental health analysis. | Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, Sophia Ananiadou | Jiangxi Normal University, Nanchang, China; Wuhan University, Wuhan, China; The University of Manchester |
| 754 |  |  [Message Injection Attack on Rumor Detection under the Black-Box Evasion Setting Using Large Language Model](https://doi.org/10.1145/3589334.3648139) |  | 0 | Recent analyses have disclosed that existing rumor detection techniques, despite playing a pivotal role in countering the dissemination of misinformation on social media, are vulnerable to both white-box and surrogate-based black-box adversarial attacks. However, such attacks depend heavily on unrealistic assumptions, e.g., modifiable user data and white-box access to the rumor detection models, or appropriate selections of surrogate models, which are impractical in the real world. Thus, existing analyses fail to uncover the robustness of rumor detectors in practice. In this work, we take a further step towards the investigation about the robustness of existing rumor detection solutions. Specifically, we focus on the state-of-the-art rumor detectors, which leverage graph neural network based models to predict whether a post is rumor based on the Message Propagation Tree (MPT), a conversation tree with the post as its root and the replies to the post as the descendants of the root. We propose a novel black-box attack method, HMIA-LLM, against these rumor detectors, which uses the Large Language Model to generate malicious messages and inject them into the targeted MPTs. Our extensive evaluation conducted across three rumor detection datasets, four target rumor detectors, and three baselines for comparison demonstrates the effectiveness of our proposed attack method in compromising the performance of the state-of-the-art rumor detectors. | Yifeng Luo, Yupeng Li, Dacheng Wen, Liang Lan |  |
| 755 |  |  [Human vs ChatGPT: Effect of Data Annotation in Interpretable Crisis-Related Microblog Classification](https://doi.org/10.1145/3589334.3648141) |  | 0 | Recent studies have exploited the vital role of microblogging platforms, such as Twitter, in crisis situations. Various machine-learning approaches have been proposed to identify and prioritize crucial information from different humanitarian categories for preparation and rescue purposes. In crisis domain, the explanation of models' output decisions is gaining significant research momentum. Some previous works focused on human annotations of rationales to train and extract supporting evidence for model interpretability. However, such annotations are usually expensive, require much effort, and are not always available in real-time situations of a new crisis event. In this paper, we investigate the recent advances in large language models (LLMs) as data annotators on informal tweet text. We perform a detailed qualitative and quantitative evaluation of ChatGPT rationale annotations over a few-shot setup. ChatGPT annotations are quite close to humans but less precise in nature. Further, we propose an active learning-based interpretable classification model from a small set of annotated data. Our experiments show that (a). ChatGPT has the potential to extract rationales for the crisis tweet classification tasks, but the performance is slightly less than the model trained on human-annotated rationale data (\sim3-6%), (b). active learning setup can help reduce the burden of manual annotations and maintain a trade-off between performance and data size. | Thi Huyen Nguyen, Koustav Rudra |  |
| 756 |  |  [Contrastive Learning for Multimodal Classification of Crisis related Tweets](https://doi.org/10.1145/3589334.3648143) |  | 0 | Multimodal tasks require learning a joint representation of the constituent modalities of data. Contrastive learning learns a joint representation by using a contrastive loss. For example, CLIP takes as input image-caption pairs and is trained to maximize the similarity between an image and its corresponding caption in actual image-caption pairs, while minimizing the similarity for arbitrary image-caption pairs. This approach operates on the premise that the caption depicts the image's content. However, this assumption does not always hold true for tweets that contain both text and images. Previous studies have indicated that the connection between the image and the text in a tweet is more intricate and complex. We study the effectiveness of pre-trained multimodal contrastive learning models, specifically, CLIP, and ALIGN, on the task of classifying multimodal crisis related tweets. Our experiments using two publicly available datasets, CrisisMMD and DMD, show that despite the intricate relationships in tweets, pre-trained contrastive learning models fine-tuned with task-specific data produce better results than prior approaches used for the multimodal classification of crisis related tweets. Additionally, the experiments show that the contrastive learning models are effective in low-data few-shot and cross-domain settings. | Bishwas Mandal, Sarthak Khanal, Doina Caragea |  |
| 757 |  |  [Modularized Networks for Few-shot Hateful Meme Detection](https://doi.org/10.1145/3589334.3648145) |  | 0 | In this paper, we address the challenge of detecting hateful memes in thelow-resource setting where only a few labeled examples are available. Ourapproach leverages the compositionality of Low-rank adaptation (LoRA), a widelyused parameter-efficient tuning technique. We commence by fine-tuning largelanguage models (LLMs) with LoRA on selected tasks pertinent to hateful memedetection, thereby generating a suite of LoRA modules. These modules arecapable of essential reasoning skills for hateful meme detection. We then usethe few available annotated samples to train a module composer, which assignsweights to the LoRA modules based on their relevance. The model's learnableparameters are directly proportional to the number of LoRA modules. Thismodularized network, underpinned by LLMs and augmented with LoRA modules,exhibits enhanced generalization in the context of hateful meme detection. Ourevaluation spans three datasets designed for hateful meme detection in afew-shot learning context. The proposed method demonstrates superiorperformance to traditional in-context learning, which is also morecomputationally intensive during inference.We then use the few availableannotated samples to train a module composer, which assigns weights to the LoRAmodules based on their relevance. The model's learnable parameters are directlyproportional to the number of LoRA modules. This modularized network,underpinned by LLMs and augmented with LoRA modules, exhibits enhancedgeneralization in the context of hateful meme detection. Our evaluation spansthree datasets designed for hateful meme detection in a few-shot learningcontext. The proposed method demonstrates superior performance to traditionalin-context learning, which is also more computationally intensive duringinference. | Rui Cao, Roy KaWei Lee, Jing Jiang | Singapore Management University Singapore |
| 758 |  |  [CapAlign: Improving Cross Modal Alignment via Informative Captioning for Harmful Meme Detection](https://doi.org/10.1145/3589334.3648146) |  | 0 | Harmful memes detection is challenging due to the semantic gap between different modalities. Previous studies mainly focus on feature extraction and fusion to learn discriminative information from memes. However, they ignore the misalignment of the modalities caused by the modality gap and suffer from data scarcity, resulting in insufficient learning of fusion-based models. Recently, researchers transformed images into textual captions and used language models for predictions, resulting in non-informative image captions. To address these gaps, this paper proposes an instructions-based abstracting approach CapAlign, in zero-shot visual question-answering settings. Precisely, we prompt a large language model (LLM) to ask informative questions to a pre-trained vision-language model and use the dialogues to generate a high-quality image caption. Further, to align the generated caption with the textual content of a meme, we used an LLM with instructions to generate informative captions of the meme and then prepend it with the attributes of the visual content of a meme to a prompt-based LM for prediction. Experimental findings on two benchmark datasets show that our approach produces informative captions and outperforms state-of-the-art methods for detecting harmful memes. | Junhui Ji, Xuanrui Lin, Usman Naseem |  |
| 759 |  |  [Unveiling Climate Drivers via Feature Importance Shift Analysis in New Zealand](https://doi.org/10.1145/3589334.3648147) |  | 0 | In the face of rising surface temperatures from climate change, impacting biodiversity, extreme weather events, and agricultural productivity, understanding the drivers behind temperature changes is imperative. Traditional global climate models (GCMs) are computationally expensive, limiting their applicability, while machine learning approaches, though promising, face interpretability challenges due to their "black box" nature, especially in a dynamic setting where the data is constantly evolving. We propose DUO, a framework to identify shifts in important features and feature combinations as the data distribution changes over time. Our model independently assesses the importance of features and their interactions while also evaluating their relevance when combined with additional features, contributing to the target class. As a case study, we apply DUO to assess the shifts in climate drivers for station-level temperatures in six locations across New Zealand from 1980 to 2020, we identify specific humidity, geopotential height, and air temperature at high atmospheric pressure levels as the most important features for describing temperature variability. By revealing how climate drivers change over time, DUO contributes to a deeper understanding of temperature change patterns, enabling practitioners to develop targeted and adaptive mitigation strategies. | Bowen Chen, Gillian Dobbie, Neelesh Rampal, Yun Sing Koh |  |
| 760 |  |  [Causal Graph ODE: Continuous Treatment Effect Modeling in Multi-agent Dynamical Systems](https://doi.org/10.1145/3589334.3648148) |  | 0 | Real-world multi-agent systems are often dynamic and continuous, where theagents co-evolve and undergo changes in their trajectories and interactionsover time. For example, the COVID-19 transmission in the U.S. can be viewed asa multi-agent system, where states act as agents and daily population movementsbetween them are interactions. Estimating the counterfactual outcomes in suchsystems enables accurate future predictions and effective decision-making, suchas formulating COVID-19 policies. However, existing methods fail to model thecontinuous dynamic effects of treatments on the outcome, especially whenmultiple treatments (e.g., "stay-at-home" and "get-vaccine" policies) areapplied simultaneously. To tackle this challenge, we propose Causal GraphOrdinary Differential Equations (CAG-ODE), a novel model that captures thecontinuous interaction among agents using a Graph Neural Network (GNN) as theODE function. The key innovation of our model is to learn time-dependentrepresentations of treatments and incorporate them into the ODE function,enabling precise predictions of potential outcomes. To mitigate confoundingbias, we further propose two domain adversarial learning-based objectives,which enable our model to learn balanced continuous representations that arenot affected by treatments or interference. Experiments on two datasets (i.e.,COVID-19 and tumor growth) demonstrate the superior performance of our proposedmodel. | Zijie Huang, Jeehyun Hwang, Junkai Zhang, Jinwoo Baik, Weitong Zhang, Dominik Wodarz, Yizhou Sun, Quanquan Gu, Wei Wang | University of California Computer Science Department; University of California Department of Ecology, Behavior & Evolution |
| 761 |  |  [SceneDAPR: A Scene-Level Free-Hand Drawing Dataset for Web-based Psychological Drawing Assessment](https://doi.org/10.1145/3589334.3648150) |  | 0 | Sketch-based drawing assessments are useful in understanding individuals' cognitive and psychological states, such as cognitive impairment or mental disorders. Hence, these assessments have been developed and applied on a large scale, such as in schools and workplaces, to screen individuals who may require further clinical examination. However, the interpretation of a large number of drawing assessments solely relies on human experts, requiring much time and cost. To address this issue, we introduce a novel scene-level sketch dataset, SceneDAPR, which can be used to automatically analyze the drawing assessment, Draw-A-Person-in-the-Rain (DAPR), a popular psychological drawing assessment used for identifying stressful experiences and coping behavior. The proposed dataset consists of 6,420 objects depicted in 1,399 scene sketches drawn by humans, along with detailed supplementary information about the participants. SceneDAPR includes free-hand drawings from different age groups: children & adolescents, adults, and seniors. Leveraging the proposed SceneDAPR, we develop a web-based drawing assessment system. The extensive experiments demonstrate that our system shows a robust performance across the different age groups in the object detection task as well as a considerable performance compared to human experts. We believe that the proposed new sketch dataset can be used to develop an automatic system for psychological drawing assessments, which can support human experts by reducing the time and cost of analyzing the drawing assessments for a large population. SceneDAPR and experimental code are available at https://github.com/DSAIL-SKKU/SceneDAPR. | Jiwon Kang, Jiwon Kim, Migyeong Yang, Chaehee Park, Taeeun Kim, Hayeon Song, Jinyoung Han |  |
| 762 |  |  [MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation](https://doi.org/10.1145/3589334.3648151) |  | 0 | Online memes have emerged as powerful digital cultural artifacts in the age of social media, offering not only humor but also platforms for political discourse, social critique, and information dissemination.Their extensive reach and influence in shaping online communities' sentiments make them invaluable tools for campaigning and promoting ideologies.Despite the development of several memegeneration tools, there remains a gap in their systematic evaluation and their ability to effectively communicate ideologies.Addressing this, we introduce MemeCraft, an innovative meme generator that leverages large language models (LLMs) and visual language models (VLMs) to produce memes advocating specific social movements.MemeCraft presents an end-to-end pipeline, transforming user prompts into compelling multimodal memes without manual intervention.Conscious of the misuse potential in creating divisive content, an intrinsic safety mechanism is embedded to curb hateful meme production.Our assessment, focusing on two UN Sustainable Development Goals-Climate Action and Gender Equality-shows MemeCraft's prowess in creating memes that are both funny and supportive of advocacy goals.This paper highlights how generative AI can promote social good and pioneers the use of LLMs and VLMs in meme generation. | Han Wang, Roy KaWei Lee | Singapore University of Technology Information Systems Technology and Design |
| 763 |  |  [Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response](https://doi.org/10.1145/3589334.3648153) |  | 0 | Current research concentrates on studying discussions on social media related to structural failures to improve disaster response strategies. However, detecting social web posts discussing concerns about anticipatory failures is under-explored. If such concerns are channeled to the appropriate authorities, it can aid in the prevention and mitigation of potential infrastructural failures. In this paper, we develop an infrastructure ombudsman -- that automatically detects specific infrastructure concerns. Our work considers several recent structural failures in the US. We present a first-of-its-kind dataset of 2,662 social web instances for this novel task mined from Reddit and YouTube. | Md Towhidul Absar Chowdhury, Soumyajit Datta, Naveen Sharma, Ashiqur R. KhudaBukhsh | Rochester Institute of Technology; Maulana Abul Kalam Azad University of Technology |
| 764 |  |  [Susceptibility to Unreliable Information Sources: Swift Adoption with Minimal Exposure](https://doi.org/10.1145/3589334.3648154) |  | 0 | Misinformation proliferation on social media platforms is a pervasive threat to the integrity of online public discourse. Genuine users, susceptible to others' influence, often unknowingly engage with, endorse, and re-share questionable pieces of information, collectively amplifying the spread of misinformation. In this study, we introduce an empirical framework to investigate users' susceptibility to influence when exposed to unreliable and reliable information sources. Leveraging two datasets on political and public health discussions on Twitter, we analyze the impact of exposure on the adoption of information sources, examining how the reliability of the source modulates this relationship. Our findings provide evidence that increased exposure augments the likelihood of adoption. Users tend to adopt low-credibility sources with fewer exposures than high-credibility sources, a trend that persists even among non-partisan users. Furthermore, the number of exposures needed for adoption varies based on the source credibility, with extreme ends of the spectrum (very high or low credibility) requiring fewer exposures for adoption. Additionally, we reveal that the adoption of information sources often mirrors users' prior exposure to sources with comparable credibility levels. Our research offers critical insights for mitigating the endorsement of misinformation by vulnerable users, offering a framework to study the dynamics of content exposure and adoption on social media platforms. | Jinyi Ye, Luca Luceri, Julie Jiang, Emilio Ferrara | EMILIO FERRARA; Information Sciences Institute, University of Southern California, Marina Del Rey, CA, USA; Thomas Lord Department of Computer Science, University of Southern California, Los Angeles, CA, USA |
| 765 |  |  [Predicting and Presenting Task Difficulty for Crowdsourcing Food Rescue Platforms](https://doi.org/10.1145/3589334.3648155) |  | 0 | Food waste and food insecurity are two problems that co-exist worldwide. A major force to combat food waste and insecurity, food rescue platforms (FRP) match food donations to low-resource communities. Since they rely on external volunteers to deliver the food, communicating rescue task difficulty to volunteers is very important for volunteer engagement and retention. We develop a hybrid model with tabular and natural language data to predict the difficulty of a given rescue trip, which significantly outperforms baselines in identifying easy and hard rescues. Furthermore, using storyboards, we conducted interviews with different stakeholders to understand their perspectives on how to integrate such predictions into volunteers' workflow. Motivated by our findings, we developed three explanation methods to generate interpretable insights for volunteers to better understand the predictions. The results from this study are in the process of being adopted at Food Rescue Hero, a large FRP serving over 25 cities across the United States. | Zheyuan Ryan Shi, Jiayin Zhi, Siqi Zeng, Zhicheng Zhang, Ameesh Kapoor, Sean Hudson, Hong Shen, Fei Fang |  |
| 766 |  |  [GraphLeak: Patient Record Leakage through Gradients with Knowledge Graph](https://doi.org/10.1145/3589334.3648157) |  | 0 | In real clinics, the medical data are scattered over multiple hospitals. Due to security and privacy concerns, it is almost impossible to gather all the data together and train a unified model. Therefore, multi-node machine learning systems are currently the mainstream form of model training in healthcare systems. Nevertheless, distributed training relies on the exchange of gradients, which has been proved under the risk of privacy leakage. That means malicious attackers can restore the user's sensitive data by utilizing the publicly shared gradients, which is a serious problem for extremely private data such as Electronic Healthcare Records (EHRs). The performance of the previous gradient attack method will drop rapidly when the batch size of training data increases, which makes it less threatening in practice. However, in this paper, we found in the medical domain, by leveraging prior knowledge like the medical knowledge graph, the leakage risk can be significantly amplified. In particular, we present GraphLeak, which incorporates the medical knowledge graph in gradient leakage attacks. GraphLeak can improve the restoration effect of gradient attacks even under large batches of data. We conduct experimental verification on electronic healthcare record datasets, including eICU and MIMIC-III. Our method has achieved state-of-the-art attack performance compared with previous works. Code is available at https://github.com/anonymous4ai/GraphLeak. | Xi Sheryl Zhang, Weifan Guan, Jiahao Lu, Zhaopeng Qiu, Jian Cheng, Xian Wu, Yefeng Zheng |  |
